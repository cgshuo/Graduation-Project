 Reputation analysis is naturally li nked to a sentiment analysis task of the targeted entities. This analysis leverages on a sentiment lexicon that includes general se ntiment words and domain specific jargon. However, in most cases target entities are themselves part of the sentiment lexicon, creatin g a loop from which it is difficult to infer an entity reputation. Sometimes, the entity became a reference in the domain and is vastly cited as an example of a highly reputable entity. For exampl e, in the movies domain it is not uncommon to see reviews citing Batman or Anthony Hopkins as esteemed references. In this paper we describe an unsupervised method for performing a simultane ous-analysis of the reputation of multiple named-entities. Our method jointly extracts named entities reputation and a domain sp ecific sentiment lexicon. The objective is two-fold: (1) named-entities are naturally ranked by our method and (2) we can build a reputation graph of the domain X  X  named entities. This framework has immediate applications in terms of visualization or se arch by reputation.  X  Information systems~Information extraction.  X  Information systems~Sentiment analysis Reputation analysis, sen timent lexicons, LDA. When searching for opinions, an IR system must deal with the domain named-entities and with sp ecific sentiment lexicons. In some cases, these named-entities (e .g., the actors or film titles in the movies domain), are so importa nt that they become a synonym of high-quality (or low-quality). Th ey become references in their area. It is not uncommon to find reviews where multiple citations to actors or movies occur. Thus , it becomes fundamental that an IR system identifies these named-entities and infers its reputation. Reputation analysis for entities has been a topic of recent research, since the new trend of micro blogging has propelled an accumulation of data that reflects public opinion on various subjects. Go et al. [3] used various machine learning algorithms (Na X ve Bayes, Maximum Entrop y and SVM) to classify the overall sentiment of Twitter messages towards specific keywords, representing various distinct entiti es likes movies, famous people, locations and companies. Later, Chen et al. [1] proposed the extraction of sentiment polarit y on tweets towards movies and people as a constrained optimiz ation problem. Chen et al. X  X  approach used a lexicon contai ning both formal and slang words to accommodate Twitter vocabulary, built by collecting words from dictionaries such as SentiW ordNet [2] and Urban Dictionary (www.urbandictionary.com ). Here, however, we argue that static-lexicons are too coarse-grained. As a consequence fail to capture relevant sentiment words that target numerous entities. Also, static-lexicons assign fixed weights to sentiment words. In this paper we capture the relevant se ntiment words and weight them according to their sentiment relevance. The proposed method detects sentiment words fluctua tions through an LDA generative model from users X  sentences extracted from reviews of different ratings. Moreover, the method also weights the overall sentiment level associated to an entity, corresponding to the reputation of that entity. A sentiment analysis challenge begins with the nature of text opinions: opinions are inherently subjective and written in natural language, which is also an ambiguous way of representing knowledge. One of the earliest approaches that contributed on identifying subjective sentences, hence opinionated sentences, was proposed by Hatzivassiloglou et al. [4]. In this work was used a seed of adjectives with a set of linguistic constraints to capture adjectives. Later, Turney et al. [11, 12] used a seed of adjectives from Hatzivassiloglou et al. [4] and the General Inquirer dictionary in a sentiment classification task. Recent work on capturing relevant sentiment words has focused at identifying sentiment words by exploring the usage of slang or domain-specific sentiment words [1]. For instance, Urban Dictionary (UD) and Twittrat X  X  (twitter.com/twitrratr) are dictionaries that aim at capturing sentiment words that more traditional dictionaries fail to capture (e.g. Multi-perspective Question Answering (MPQA) [14], General Inquirer (www.wjh.harvard.edu/~inquirer/) and SentiWordNet [2]). In the present work we stress on the need to capture relevant sentiment words which are strongly associated to a given context. Hence it is proposed a ranked based LDA to capture the most relevant sentiment words. Reputation analysis have focused not only for summarizing the overall reputation but also to predict the reputation of other instances or events [5, 8]. Joshi et al. [5] explored the popularity of old movies among online critic reviews to predict opening weekend revenues for new movies, by comparing the similarity in metadata of old highly rated movies with new ones. More recently, Oghina et al. [8] predicted the IMDb movie ratings by analyzing their popularity on social media, namely Youtube and Twitter. Oghina et al. used textual tweets, comments and likes on videos related to specific movies to predict their reputation, then translated into a numeric rating scale from 1 to 10. In our proposal we aim at identifying the most relevant sentiment words that characterize the reputation of a given entity. Hence, movie reviews were extracted from the IMDb (www.imdb.com ) movie database and, the actors and characters (e.g. batman ) names which represent the entities. The potential marketing usefulness of reputation analysis has led res earch on last years to focus extensively on monitoring and prof iling relevant emerging topics for market brands and organiza tions on Twitter, such as Apple and Windows [6, 7, 10, 13]. Mart X n et al. [7] explored different approaches for identifying emerging topics that are relevant for an organization X  X  reputation, such as representing each tweet as a set of Wikipedia entries that are related to it and extending the LDA generative model to capture rele vant topics on tweets. More recently, Spina et al. [10] tested a variety of different techniques with the same goals as Mart X n et al and others[13]. For obtaining aggregated sentiment regarding an organization Spina generated domain-specific semantic graphs to expand a sentiment lexicon. Also, for monitoring relevant topi cs, tested approaches included filtering relevant or irrelevant tweets to a certain company by discovering filter keywords and us age of wikified and LDA model in a similar way to Mart X n X  X  et al. approach. The problem we address in this paper aims at measuring an entity reputation by creating a sentime nt lexicon and weighting its X  relevance towards the entity. The proposed sentiment lexicon is created with user sentences w ithout human supervision. To identify the sentiment words it is proposed a multilevel generative model of users X  reviews. Ther efore we propose a generative probabilistic model that ties words to different sentiment relevance levels and evaluate within each subjective sentence the sentiment word proximity to an entity. Problem definition. Consider a set of M sentences  X  X  X   X   X  containing user opinions towards a given movie. Each review  X  is represented by a tuple  X   X   X   X ,  X   X  , where  X  sentiment value quantifying the user opinion about the product (it corresponds to the user rating). An entity is any movie, actor, character, director, etc of the domain. Entities are usually mentioned in reviews and are part of the domain taxonomy. Our goal is twofold: first we compute a fine-grain lexicon of sentiment words that best captures the varyi ng level of user satisfaction, and second, we determine the reputation of an entity by measuring its impact in the domain, i.e., the most relevant sentiment words associated to an entity. The reputation analysis framework is divided into different steps, Figure 1 illustrates the process. First, to best capture complex sentiment expressions, we comput e bigrams with a maximum of 3 words between every word pair. Stop words removal and lemmatization are also part of the initial step. Sec ond, we compute the sentiment lexicons after removing the entities from the corpus to determine the influence of these entities in the domain sentiment characteristics. Finally, to ascertain the reputation of named entities, we propose two wa ys: the number of citations and the context in which they are cited. LDA is a generative model that explores word co-occurrences at document-level and at the level of K latent topics. It samples a word distribution from a prior Dirichlet distribution for each latent topic. The probability of a sequence of words and its hidden topics the random parameter of a multi nomial over topics. With ranked LDA the goal is to identify which words are used to express a sentiment. Figure 2 presents the graphical model of the proposed Rank-LDA method. At its core, th e Rank-LDA links the latent topics to the sentiment relevance of each sentence. For each relevance there will be a set of hidden topics that will be activated. Rank-LDA is structured as follows:  X  is the per-corpus topic Dirichlet  X   X | X   X  distribution,  X  is the per-sentence topic distribution, z is the per-word topic assignment following a words observed on each sentence. Finally,  X   X   X   X   X ,...,1  X  is the per-sentence sentiment relevance and sw is the per-word random variable corresponding to its sentiment distributions across the different sentiment levels of relevance. The random variables  X  ,  X  and  X  are the distribution priors. Th e sentiment word distributions are given by the density distribution where we compute the marginal distribution of a word given a sentiment level, over the K latent topics of the Rank-LDA model. The variable  X  is a smoothing parameter that we set to 0.01. The sentiment word distribution function can also be used to rank words by its positive/negative weight or to calculate a word X  X  cross-sentiment occurrences. To achieve such conversion is through a normalization function such as where  X   X   X  X   X |sw  X  and  X   X   X  X   X |sw  X  contain the word  X  relevance values in rating  X  and  X  . IMDb movie reviews are in a rating scale from 1 to 10, thus, Table I presents the words distributions p (see equation 1) obtained with the lower and higher rating, 1 and 10. The words distributions for the opposite ratings is clearly depicted for the words horror , garbage and excellent , as for the words television and pilot the weights do not differ. The word batman represents a word highly related to a specific set of movies and intuitively would not be observed as a sentiment word. Nonetheless, in Table I the word batman depicts a high relevance value. Reasoning on this observation, in reviews from rating 1 and 10 the word batman is mentioned with a frequency of 1,286 and 5,107, in 212 and 251 di fferent movies respectively. For example, 1. (...) it took a non-batman movie to finally make a decent-2. Go watch crash, Capote, walk the line, sideways even hence, words like batman enclose a sentiment weight. The proposed ranked LDA sentiment le xicon captures these relevant sentiment words. Considering batman as a sentiment word the reputation of entities as watchmen , equilibrium and others is weighted with the sentiment word weight of the entity batman , as in this case, is also viewed as relevant sent iment word (Figure 3). 
Figure 3: Reputation given by the sentiment word batman IMDb-Extracted: This dataset contains 1,007,926 million movie reviews, corresponding to a tota l of 10,651,052 million sentences. Reviews are rated in a scale of 1 to 10. For evaluation purposes the dataset was evenly split into three disjoint subsets (A, B and C). Following Pang et al. [9] approach the subset A was used to build a subjective classifier, thus, sentences from movie plots are labeled as objective and sentences from users reviews as subjective. Moreover, to pe rform a balanced subjective classification the number of subj ective sentences was reduced to match the number of objective sent ences. For the subset B and C, 693,349 and 1,449,546 were classifi ed as objective sentences respectively. The ranked LDA le xicon is built with subjective sentences from split B. The subjective sentences from split C are used for evaluation purposes. Table II presents the detailed information of the IMDb-Extracted.This dataset is available https://novasearch.org/datasets/. To evaluate the quality of the ra nked sentiment lexicon for entity reputation two crowdsourcing (www.crowdflower.com) tasks are performed. First, given a sentence it is asked for the annotator to judge if a specified sentiment word is relevant to characterize the entity reputation ( Table IV ); and secondly, given 5 sentiment words it is asked for the annotator to judge if the entity described by those words has a positive, negative or neutral reputation. Therefore the first task evaluates if the captured sentiment words are relevant to measure the ent ity reputation and the second task evaluates the proposed method ability to correctly weight the sentiment word polarity. For the first task it was used 3,000 sentences, the sentences were obtained randomly from the split C. And, for the second task 3,000 combinations of sentiment words in which roughly one third were bi grams. For both experiments, it was created a gold standard by selecting the units where workers had an agreement of 75% or more, resulting in 2036 gold units for the first task and 943 gold units for the second task. Table III compares the obtained annotati ons with methods based on our lexicon. Task REL refers to the first task, POL-UNI and POL-BI refers to the second task for sentiment words obtained from unigrams and bigrams, respectivel y. The obtained results for the relevance task suggests that a very high percentage of the captured sentiment words with our lexicon are relevant for reputation analysis of entities. In parallel, results for the polarity task shows that the associated weights for the sentiment words perform well on standard binary polarity evaluation. 
Table III: Crowdsourcing for Entities Reputation measured with In Figure 4 is examined the number of different entities associated to a sentiment word. For instance, the sentiment word batman is related to 1,557 entities and indiana to 812 entities. In Figure 5 is shown the entities association to domain related sentiment words (e.g. drama , trailer and oscar ). Moreover, Figure 6 presents the top positive and negative sentiment words. This illustrates how rank rlda ca ptures both general and domain specific sentiment words. Also, characters and actor names are frequently used as positive, or negative, reference (Figure 3), thus, the relevance of using these sentiment words when measuring an en tity reputation. In this paper we have proposed a method to measure entities reputation. This was performed by selecting a set of sentiment words that best represent the op inions targeting a given entity. More, specifically, by exploring the words distributions in a generative ranked based LDA model, it is built a sentiment lexicon. The obtained lexicon was able to capture domain specific sentiment words that traditional static sentiment lexicons fail to capture or infer a generic sentiment weight. However, we stress that sentiment words that are usually overlooked as relevant sentiment words by traditional methods (e.g. batman or hannibal ) enclose a relevant sentiment weight and we have shown that for se veral entities these sentiment words are frequently used. Furthe rmore, we have successfully evaluated the proposed approach in two crowdsource tasks. Acknowledgements. This paper was partially funded by the Portuguese Science F oundation (projects UTA-
Est/MAI/0010/2009 and PTDC/EIA-EIA/111518/2009). [1] Chen, L. et al.. Extracting Di verse Sentiment Expressions with [2] Esuli, A. and Sebastiani, F. Sentiwordnet: A publicly available [3] Go, A. et al. Twitter Sentim ent Classification using Distant [4] Hatzivassiloglou, V. and McKeown, K.R.. Predicting the [5] Joshi, M. et al. 2010. Movie Reviews and Revenues: An [6] Mart X n-Wanton, T. et al. An unsupervised transfer learning [7] Mart X n-Wanton, T. et al. UNED at RepLab 2012: Monitoring [8] Oghina, A. et al.. Predicting IMDB Movie Ratings Using Social [9] Pang, B. and Lee, L. Seeing stars: Exploiting class relationships [10] Spina, D. et al. UNED Online Reputation Monitoring Team at [11] Turney, P. Thumbs up or t humbs down? Semantic orientation [12] Turney, P.D. and Littman, M. L. Measuring praise and criticism: [13] Villena-Rom X n, J. et al. DAEDALUS at RepLab 2012: Polarity [14] Wiebe, J. and Cardie, C. 2005. Annotating expressions of 
