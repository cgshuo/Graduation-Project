 Attackers may seek to manipulate recommender systems in order to promote or suppress certain items. Existing de-fenses based on analysis of ratings also discard useful in-formation from honest raters. In this paper, we show that this is unavoidable and provide a lower bound on how much information must be discarded. We use an information-theoretic framework to exhibit a fundamental tradeoff be-tween manipulation-resistance and optimal use of genuine ratings in recommender systems. We define a recommender system to be ( n, c )-robust if an attacker with n sybil iden-tities cannot cause more than a limited amount c units of damage to predictions. We prove that any robust recom-mender system must also discard  X (log n c ) units of useful information from each genuine rater.
 I.2.6 [ Computing Methodologies ]: Artificial Intelligence X  Learning Algorithms, Reliability Recommender systems, manipulation-resistance, shilling, in-formation loss
Content posted on the Internet is not of uniform quality, nor is it equally interesting to different audiences. Recom-mender systems guide people to items they are likely to like, based on their own and other people X  X  subjective reactions.
Authors and other parties often want to direct attention to particular items. Google, Yahoo!, and others channel this into a multi-billion dollar advertising marketplace. But to the extent that people rely on recommender systems to guide Copyright 2008 ACM 978-1-60558-093-7/08/10 ... $ 5.00. their attention, there are also natural incentives for promot-ers to manipulate the recommendations. An attacker may rate strategically rather than honestly and may introduce multiple entities, sometimes called sybils or shills [11, 6], to rate on behalf of the attacker.

The general scenario we analyze is that a sequence of raters rate an item, and then a prediction is made about a target person X  X  reaction to the item. We will refer to people X  X  opinions generically as ratings, whether users ex-plicitly enter them, in the form of ratings or tags, or the system infers them from implicit behavioral indicators such as purchases, read times, bookmarks, or links.

Some of the raters are honest; they acquire private infor-mation about each item and report it honestly. Some are sybils under the control of an attacker, who reports fake ratings to manipulate the predictions. A recommender sys-tem combines the ratings it has received so far to predict whether a target person will like each item. For example, an additional positive rating might increase the predicted prob-ability that the target will like the item, while an additional negative rating might reduce the predicted probability. In order to personalize predictions and to resist manipulation, the recommender may allow some raters to influence the prediction more than others, depending on each rater X  X  pat-tern of ratings of other items and how useful those ratings have been.

Resnick and Sami [14] presented a particular manipulation-resistance algorithm, the Influence Limiter, that can be over-laid on any recommender. It gives only a tiny influence to a new entity, then increases that influence as the entity provides informative ratings. The algorithm is provably re-sistant to any attack involving a bounded number of sybils. The influence limits, however, create an inefficiency: the rec-ommender throws away information from new raters who are honest and informative but who have not yet proven them-selves to be so.

Several other authors have suggested using statistical met-rics on ratings to distinguish  X  X ttack X  identities from  X  X eg-ular X  identities, and eliminate the former [3, 12, 7, 15, 8]. Mobasher et al. [9] survey this literature and classify at-tack strategies. Any process of weeding out attackers based on the distribution of their ratings, however, risks throwing away information from informative raters who are misclas-sified as attackers.

This paper asks whether the information losses incurred in these approaches are necessary. Is there a fundamental tradeoff between resistance to manipulation by an attacker with a large but bounded number of sybils, and use of infor-mation from informative raters? The answer is yes. Indeed, preventing damage by an attacker who merely injects noise in the form of random guesses requires discarding the same order or magnitude of information as that discarded by the Influence Limiter algorithm.

We evaluate a recommender X  X  predictions by applying a scoring rule or loss function that compares the predictions made to the eventual evaluations that a target user gives to the items. The information loss of a recommender is the expected increase in loss from using it rather than an ideal recommender.
 We prove two variants of an information loss lower bound. The first describes how many ratings a manipulation-resistant algorithm must monitor before it allows a rater to have a large influence on a person X  X  recommendation. The second describes the minimum information loss per honest rater that a manipulation-resistant algorithm must incur in the worst case. It is a function of the maximum number of sybils that the algorithm will be resistant to, and of the maximum damage they can be allowed to cause. Thus, an immediate consequence is that no scheme can be resistant to manipu-lation by an unbounded number of sybils without throwing away all information from honest raters.
In this section, we detail and justify a formal model that enables the analysis of manipulation-resistance as well as in-formation efficiency of recommender algorithms. We present the model in three stages. In section 2.1, we introduce a for-mal model of a recommender X  X  predictions and an informa-tion-theoretic measure of the error in those predictions. Sec-tion 2.2 provides a formal model of the information that hon-est raters acquire and reveal to the recommender through their ratings. Section 2.3 introduces an attack model, a measure of the damage that an attacker causes in terms of increased error of the predictions the recommender gener-ates, and a formal definition of ( n, c )-robustness.
The output of the recommender is a prediction about the reaction that a particular target person will have to an item. For simplicity, we assume that a prediction is expressed as a probability that the target will like an item, and a tar-get eventually makes a binary report of liking the item or not. The binary reports do not limit the generalizability of our results: any scheme that is resistant to manipula-tion of predictions about finer-grained reports from targets must also be resistant to manipulation of predictions over more-restricted binary reports. 1
More formally, there is a space  X  of possible states (item types) and a label space L = { HI, LO } of possible responses to an item from a target. As we shall see in the next section, we can, without losing generality, confine all uncertainty in the model to uncertainty about the state; the state deter-
Our model generalizes naturally to systems where targets select from a fixed set of labels, such as 1-5 stars. In that case, a recommender would have to predict the probability of each label being chosen (e.g., 30% chance of 5 stars; 20% chance of 4; 50% chance of 3). Many existing recommenders predict only the mean (e.g., 3.8), which could be extended in a variety of ways to predict probabilities for each of the individual labels; our lower bound implies a limit on the effectiveness of any such extension. mines the reactions of all raters and potential targets with certainty. In particular, for each  X   X   X  there is a corre-sponding value l (  X  )  X  HI, LO that describes the target X  X  reaction to items in that state. A prediction expresses the probability q  X  [0 , 1] of a HI label.

Since predictions are probabilistic, they are not simply right or wrong. A prediction that assigns a higher probabil-ity to the outcome that occurs is more correct, or has less error, than one that assigns a lower probability. We employ the quadratic scoring rule to assign a loss or error score to a prediction, after the target reports liking the item or not. Formally:
Note that the loss or error is 0 when an extreme prediction of 0 or 1 is made and the target agrees with the prediction. The worst error of  X  1 occurs when an extreme prediction is made that the target disagrees with. This error measure corresponds to using the expected squared error (the vari-ance) as a measure of uncertainty, which has a long history in statistics. It also corresponds to the use of mean squared error as a measure of prediction accuracy in recommender systems, though only for systems that make predictions on a 0-1 scale and have targets report binary outcomes.
We now proceed to define the damage of an attacker and the information loss of a recommender system. Intuitively, the damage is the increase in expected error score of the predictions made with the attacker present over those made without the attacker. The information loss is the increase in the expected error score of the predictions made by the actual recommender over those made by an ideal recom-mender.

More precise definitions, however, depend on a specifica-tion of the environment of a sequence of item ratings. How much contribution or damage a rater will make may depend on how much information other raters will provide. We need a model of the partial information provided by each addi-tional rater. We use a standard model of information par-titions, consistent with the model in [14], though we have simplified some of the notation and adjusted the exposition.
There is a set I of items to be rated. The state  X   X   X  of each item i  X  I is drawn independently according to a distribution defined by probability mass function p :  X   X  [0 , 1] that gives the relative likelihood of different states.
There is a set J of raters. Each rater r who evaluates an item receives a signal, some partial information about the item. The state  X  determines with certainty the signal that rater r receives, but many states may produce the same signal. 2 Thus, each realized signal picks out a subset of  X , the set of states that would cause that rater to receive that signal. Subsets of states yielding different signals are non-overlapping and exhaustive (in each state the rater sees exactly one signal) so they form a partition.

For the purposes of analyzing the information available to
Some readers may be more familiar with a model of partial information where a state determines a probability distribu-tion over signals for each rater, rather than a fixed signal. In our model, there would instead be more states, each corre-sponding to one of the possible realized signals; the relative likelihood of the states would correspond to the probability distribution over signals in the other model. an ideal recommender, we assume that a rater X  X  report fully reveals the component  X . We do not model explicitly the mechanics of how a rater reports (e.g., on a 1-5 scale). In practice, the inability for a rating to reflect all the private information the rater has acquired may be one source of information loss for a recommender.

Different raters may notice different things about items or have different tastes. We model this as each rater r having a type  X  ( r ), where  X  ( r ) is a partition of  X  into components corresponding to the different signals that rater r could re-ceive. Let  X  denote the space of all possible rater types. A rater sequence X = ( r 1 , i 1 ) , ( r 2 , i 2 ) , , ( r fies, for each time period t , the unique identifier for a rater r who will rate which item i t . Note that the same item may appear multiple times ( i t 1 = i t 2 ), and the same rater may appear multiple times ( r t 1 = r t 2 ), but each pair is distinct (i.e., each rater examines each item only once). We denote the subsequence consisting of the first t time steps by X
A rating history Y = ( X, { y 1 , , y T } ) is a combination of a rater sequence and a realization of the raters X  signals. The rating y t reveals the signal that r t received for item i , and thus the component s i t ( y t ) consisting of those item types that are still possible given r t  X  X  realized signal.
Each realized rating of item i provides information about whether the target will like the item, by eliminating some of the previously possible states of the item. Given the set of ratings on item i in a rating history Y , by raters with known types given by  X  , item i  X  X  state must lie in a subset  X  s ( i, Y,  X  ) that is the intersection of the components identified by each of the ratings of i .

All the possible subsets that could arise through different realized ratings for i in X t form a partition of  X , which we denote by  X   X  i ( X t ). Note that each component of  X   X  i either a component of  X   X  i ( X t  X  1 ) (if the final rating reveals no new information) or is a strict subset of of a component of  X   X  ( X t  X  1 ) (if the final rating eliminates some possible states). Thus, we say that  X   X  i ( X t ) is a refinement of  X   X  i ( X
We will refer to a hypothetical recommender that makes the best predictions possible, given full information about the rater types  X  ( X ) and the entire rating history Y , as an ideal recommender . The best prediction of whether the target will like item i is the probability of a HI label, con-ditional on the rating history and the rater types: that is, conditional on the state being in  X  s ( i, Y,  X  ). We denote this optimal prediction by The prior probability that the target will like an item i , before receiving ratings, is q ( i, Y 0 ,  X  ) = P  X   X   X  ,l (  X  )= HI
Any joint distribution of rater and target preferences can be represented within this model. For example, suppose the system has a single rater r 1 in addition to the target. Fur-ther, suppose that r 1 can discern two kinds of items, which r model this with a state space  X  = { + H, + L,  X  H,  X  L } , prior partition  X  1 = {{ + H, + L } , { X  H,  X  L }} , and state labels l (+ H ) = l (  X  H ) = HI , l (+ L ) = l (  X  L ) = LO . The prior q ( i,  X  +  X  ,  X  ) = . 3 / ( . 3 + . 2) = . 6.

We summarize our model of the underlying information setting with the following definition:
Definition 1. A partial information structure P = ( X  , p, I, J,  X , X ) consists of a state space  X  , a prior proba-bility distribution p , a set of items I , a set of rater ids J , a function  X  that maps rater ids to types, and a sequence of (item id, rater id) pairs X specifying the ratings to be acquired.
We now introduce attacks into our model of the recom-mendation process. We model attackers as creating sybil identities and injecting strategically chosen ratings into the rating sequence. We will prove lower bounds on the infor-mation loss suffered by any recommender algorithm that is robust in the face of such attacks. That is, we will show that, for any algorithm, there exists a partial information struc-ture and an attack strategy such that either the expected damage for the attacker is high or the expected information loss absent an attacker is high. In fact, our results only require robustness against a special class of attacks, ran-dom guessing attacks , which are defined below. Naturally, the lower bound applies a fortiori to recommenders that are resistant to all possible attack types.

Definition 2. An n -sybil random noise attack strat-egy A = ( J A ,  X   X  , X  X  ) on an information structure P = ( X  , p, I, J,  X , X ) is comprised of three elements.
For each time t such that r t  X  J A , the attacker receives no signal but generates a fake rating. It selects randomly among the signals that the real type  X   X  ( r t ) could have re-ceived, if there were such a real type acting at this point in the sequence. That is, the ratings from previous non-attacker entities in the rating sequence determine the set of states that are still possible. One of these states is selected at random, according to the relative likelihood of those states given by the original probability distribution p . The rating reported is then the one that identifies the component of  X   X  ( r t contains the selected state.
 Expected Damage : Next we quantify the damage caused by an attack A on an information structure P . Intuitively, it is the expected difference in the loss between the predictions that a recom-mender would make with and without the addition of the attacker X  X  ratings.

Up to this point, we have concentrated on the best pos-sible prediction that an ideal recommender system could make given a sequence of ratings and information provided by knowledge of the raters X  types, ie., the structure of the raters X  partitions. An actual recommender system sees the ratings and target labels, but it does not know anything about the underlying partition structure except information that can be inferred from the rating history. Let  X   X   X  denote a vector of the target X  X  ultimate labels, one for each item i . We then model an actual recommender system R by a pre-diction function q R ( i, Y,  X   X   X  ), which processes a realized rating history and target labels to predict, for each item i , a prob-ability of the target labeling item i  X  X I X . We assume that the target labels each item right after its last rating in the sequence Y , and that the recommender R cannot access the value of a label l i until it is labeled. The quality of the prediction for item i is assessed just before the target labels item i ; the recommender may condition its prediction on the rating history and revealed labels up to that time.
Let  X   X   X  = {  X  i } be a vector of states for all items in I . Given the states  X   X   X  , the signals of all genuine raters are determined; let Y (  X   X   X  ) be the realized history for the genuine raters. As the states are independently drawn, p (  X   X   X  ) = Q p (  X 
Let Z be any realization of the random ratings made by an attacker in X  X  . The attack ratings Z may depend on earlier ratings by genuine raters. However, these raters X  ratings are completely determined by  X   X   X  , and so the conditional prob-ability P ( Z |  X   X   X  ) is determined by the type  X   X  the attacker is posing as. We use p  X   X  , X   X   X  ( Z ) to denote this conditional probability. Let Y  X  (  X   X   X , Z ) be the rating history obtained by merging a genuine history Y (  X   X   X  ) with the attack ratings Z .
Definition 3. The expected damage ED ( R, P , A ) of an attack strategy A on a recommender algorithm R for an information structure P is defined as: Robustness : We are now ready to state our definition of robustness.
Definition 4. A recommender system R is ( n , c ) -robust against random noise attacks iff, for any partial information structure P , and any n -sybil random noise attack strategy A on that structure, ED ( R, A, P )  X  c .
 Information Loss : Finally, we define the expected information loss IL( R, P , A ) that a given recommender R incurs for a rater sequence X  X  that may include some attacker sybils. Intuitively, it is the expected difference between the loss on the predictions q made by the recommender and the loss on predictions q that an ideal recommender would make, knowing the types of all the honest raters and which raters were attacker sybils.
Definition 5. The information loss IL ( R, P , A ) of a recommender algorithm R on an information structure P together with attack A is defined as:
Information loss for a practical recommender may arise for several reasons. In order to provide a simple user interface, a recommender may not elicit ratings in a form that fully reveals the rater X  X  partition. It may have an incorrect way of interpreting a rater X  X  ratings, because it does not yet have enough experience with that rater to infer its type. Or the recommender may include features intended to resist ma-nipulation that also cause it to inefficiently use information from some raters.
The key intuition behind our lower bounds is the obser-vation that no algorithm that has observed only a small number of ratings can distinguish (with high probability) a genuine rater from a random-noise sybil. Only after a suffi-ciently long sequence of observations can we reliably detect that the former moves the predictions in the right direction more often. A manipulation-resistant algorithm must keep the expected influence of a random-noise sybil very small to prevent an attacker from using n of them to cause significant damage. Thus, it must also limit the influence of a genuine rater for a number of rounds.

We use a simple family of information structures to con-struct our proofs. This family of instances is sufficient for the worst-case information loss result. In section 4, we discuss generalizations of this result to other information structures, as well as other loss functions. We begin by proving a lower bound in the context of 1-sybil attacks; in section 3.2, we use this to prove our main result, on n -sybil attacks.
In this section, we prove a lower bound on information loss in 1-sybil attacks. Throughout this section, suppose that we have been given a damage bound d . We consider a family of information structures P ( b ), where the param-eter b denotes the magnitude of information the informed raters have. We begin by defining the base structure P ( b ) =  X  = { + H, + L,  X  H,  X  L } ; p b (+ H ) = p b (  X  L ) = 0 . 25 + b/ 2, p (  X  H ) = p b (+ L ) = 0 . 25  X  b/ 2. The target assign labels l (+ H ) = l (  X  H ) = HI , l (+ L ) = l (  X  L ) = LO . I is a set of 2 m b items, where m b will be specified later. There is only one rater r and thus each item gets only one rating. The The rater X  X  partition  X  is such that she receives the signal y i =  X  +  X  when the state is either + H or + L . Note that p ( l (  X  ) = HI | y i =  X  +  X  ) = p b (+ H ) / ( p b (+ H ) + p probability that the target X  X  label is HI is 0.5, as is the probability that y i =  X  +  X  .

We next introduce a corresponding family of attacks A ( b )= ( J
A ,  X   X  b , X  X  ( b )). The set J A consists of only one sybil identity r  X  . The partition  X   X  b ( r  X  ) =  X  b ( r ) is as described above. In other words, every time r  X  rates an item i  X  , she will randomly report y i  X   X  X  + ,  X  X  , each with probability 0 . 5.
The sybil rates the other m b items not rated by the gen-uine rater. The rater sequence X  X  ( b ) is constructed ran-domly, as follows: The attacker flips a fair coin, and if it is heads, she inserts all her m b ratings before r  X  X  first rating. If the coin comes up tails, she instead inserts all her ratings after r  X  X  last rating.

We will now prove that, for any recommender R , we can set a value m b such that either ED( R, P ( b ) , A ( b ))  X  d , or IL( R, P ( b ) , A ( b )) is  X (log 1 d ).

Consider two extreme options for the recommender in making predictions for an item, given the single rating avail-able, which might be from a genuine rater or from an at-tacker. One extreme option is to ignore the rating and pre-dict 0 . 5. This avoids any expected damage, since the rating has no effect. When the rater is a sybil, ignoring the rating also adds nothing to the expected information loss. When the rater is genuine, however, there is an expected informa-tion loss from not moving to the correct prediction of either 0 . 5 + b or 0 . 5  X  b .

The other extreme option is for the recommender to treat the rater as genuine, predicting 0 . 5+ b or 0 . 5  X  b depending on the rating. Here, if the rater is honest, there is no damage or information loss, since the prediction matches both the ideal prediction and the prediction that would have been made in a scenario where all the raters were genuine. If, however, the rater is a sybil, and the rating is just noise, there will be expected information loss and expected damage.

Of course, the recommender need not make only these ex-treme choices. It can partially incorporate a rating, moving the prediction only partway from 0 . 5 to 0 . 5 + b or 0 . 5  X  b . This will increase the information loss when the rater is gen-uine, and increase both damage and loss when it is not.
A key ingredient of our lower bound is that we bound the influence a rater can have after a sequence of ratings. The following definition is specialized to our setting, in which each item is rated by a single rater:
Definition 6. Consider any realized sequences Y  X  and  X   X   X  of ratings and labels in the structure P ( b ) with attack A ( b ) , and suppose that ( r t , i t ) is the rating at time t . Let Y  X  denote the realized ratings upto time t  X  1 , and  X   X   X  t  X  1 the subset of labels that have been revealed before the t rating. Then, we define the influence of rater r t on item i as the value  X  given by:  X  = max Hereafter, we assume that each rater can effect the same change to the predicted probability in either direction, i.e. , upward or downward. This allows for a simpler statement of the lower bound. Dropping this assumption would not alter our result in any significant way.

We first show that the influence affects both the damage (for the attacker r  X  ) and the loss (for the genuine rater r ).
Lemma 1. Suppose ( r  X  , i ) is a rating by the attacker, and suppose the influence of r  X  on item i is  X  (for given R and Y  X  ). Then, the expected damage on this item is  X  2 .
Proof. As this rating is entered by the random-guessing attacker, if the rating is  X  +  X  ,the item will be labeled HI with probability 0 . 5, and LO with probability 0 . 5. By definition of the influence  X  , the recommender predicted a value 0 . 5+  X  on item i . Thus, the expected loss is On the other hand, without the attacker X  X  rating, there would have been no ratings on i , and so the recommender would have predicted 0 . 5, for an expected loss of 0 . 25. Thus, the expected damage of this rating is 0 . 25 +  X  2  X  0 . 25 =  X 
Lemma 2. Suppose ( r, i ) is a rating by the genuine rater, and suppose the influence of r on item i is  X  &lt; b (for given R and Y  X  ). Then, the information loss on this item is ( b  X   X  )
Proof. The additional damage caused due to the re-stricted influence can be calculated as the difference between the expected score of the restricted predictions and the ex-pected score of the optimal predictions. Observe that when rater r reports signal  X  +  X  , the probability that the label is HI is 0 . 5 + b (as is the optimal prediction q ( i, Y  X  ,  X  )), but on this item is: The case in which r reports  X   X   X  is symmetric.

Now, because of the randomized sequence X  X  , the recom-mender algorithm R cannot tell beforehand if the first rater r is the sybil r  X  or the genuine rater r . Moreover, for the first m b items, the recommender has no useful information about the second rater (because all its ratings come after the first rater X  X  ratings.)
Now, consider the time step just after m &lt; m b items have been rated. At any point of time, the only information about the first rater r 1 that is available to the recommender algo-rithm is the past record of its ratings, and the corresponding target labels on the items. The information u i available to the algorithm about each item i  X  m can thus be represented by one of the four symbols { + H, + L,  X  H,  X  L } , where + H denotes that r 1 rated + and the target ultimately labeled the item HI , etc. Note that when the rater is honest, the information u i is exactly the state  X  , but when the rating is from a sybil, the information u i = + H could occur either in state + H or  X  H .

For any item i , the probability of each outcome will be different depending on whether r 1 was the informative rater r , or the uninformative rater r  X  . We use P b () to denote the probability mass function for rater r , and P 0 () to de-note the probability mass function for the random-guesser r  X  . Likewise, we use E 0 () and E b () to denote expectations with respect to probabilities P 0 and P b respectively, and Var 0 and Cov 0 to denote variance and covariance with re-spect to probabilities P 0 . We note that
We allow for the recommender algorithms to operate in a path-dependent way. That is, a rater whose first rating is in the correct direction and second in the wrong direction may be assigned a different influence than one whose first rating is in the wrong direction and second in the right di-rection. Thus, we cannot work with the expected number of guesses in the right or wrong direction; instead, we analyze the probabilities of individual paths of observations.
Fix a recommender algorithm. Suppose rater r 1 has a prediction-outcome history u = ( u 1 , u 2 , . . . , u m ). Just be-fore item ( m + 1) is labeled, the recommender algorithm determines what to predict on that item, depending on r 1 rating. Thus, the recommender implicitly prescribes the in-fluence of r 1 on item ( m + 1). This can depend on the entire path u , but nothing else, and hence we use the no-tation Inf( u ) to denote this quantity. If the recommender algorithm is itself randomized, we can let Inf( u ) denote the expected value (over the recommender X  X  randomization) of r  X  X  influence on item ( m + 1), given history u .

We prove a lower bound on the number of ratings needed for a rater to build her expected influence to a given level  X  , for any (1 , d )-robust algorithm. To do this, we first fix a number m of items rated, and bound the expected influ-ence of the honest rater after m rounds. The robustness property gives us an upper bound on E 0 (Inf( u )), the ex-pected influence of the impersonator. We need to extend this to a bound on E b (Inf( u )), the expected influence of an honest rater. The two expectations can be quite different because certain sequences u are much more likely to have come from an informed rater than an impersonator, i.e. , P ( u )  X  P 0 ( u ). To link them, we use the likelihood ratio function g ( u ) defined as follows: The following relation is immediate: Lemma 3.
 Proof. This follows from the definition of g ( u ). We now seek to prove an upper bound on E 0 ( g ( u )Inf( u )). To do this, we bound E 0 ( g ( u )) and E 0 (Inf( u )) separately, and then bound the covariance Cov 0 ( g ( u ) , Inf( u )). Lemma 4. E 0 ( g ( u )) = 1 .

Proof. First, note that E 0 ( g i ( u )) = 0 . 5(1+2 b )+0 . 5(1  X  2 b ) = 1. Using the independence of different items i , we have E 0 ( g ( u )) = Q i ( E 0 ( g i ( u ))) = 1 Lemma 5. For any (1 , d ) -robust recommender algorithm, E (Inf( u ))  X 
Proof. By Lemma 1, if Inf( u ) =  X  , and r 1 = r  X  , the expected damage on item i is  X  2 . Thus, conditional on r r  X  , the expected damage on the m th item is E 0 ([Inf( u )] Taking into account the probability that r 1 = r  X  is 0 . 5, the expected damage on the i th item is 0 . 5 E 0 ([Inf( u )] the standard inequality [ E ( x )] 2  X  E ( x 2 ), and the fact that the expected damage is no more than d , gives the result.
Lemma 6. The covariance of g ( u ) and Inf( u ) is bounded by:
Proof. We use the standard relationship Cov( X, Y )  X  p Var( X )Var( Y ). As Inf( u )  X  [0 , 1], we have We now bound Var 0 ( g ( u )). First, note that E 0 ([ g i have Thus, we have: This leads to the required bound on the variance: This leads to our main result on the influence growth rate: Theorem 7. In information structure P ( b ) , with attack A ( b ) , and any (1 , d ) -robust recommender algorithm R , for less than  X  .

Proof. We seek to bound E b (Inf( u )) after m items have been rated. From Lemma 3, this is equivalent to bounding E ( g ( u )Inf( u )).

From Lemma 5, we know that E 0 (Inf( u ))  X  from Lemma 4 we know that E 0 ( g ( u )) = 1.

Now, consider any m  X  log( From Lemma 6, we have: Now, we are finally ready to bound the expected influence: E ( g ( u )Inf( u )) = E 0 ( g ( u )) E 0 (Inf( u )) + Cov 0 E ( g ( u )Inf( u )) &lt; This completes the proof.
 Remark: An alternative way to derive the asymptotic form of this lower bound is by reduction to a widely studied prob-lem in statistics: bounding the number of samples (items) required to test a hypothesis (the rater is informative) against an alternative (the rater is a random guesser) while achiev-ing the desired limits on false positive ( d ) and false negative (1 / 2) rates. We accept the hypothesis whenever the ac-tual influence is more than half the expected influence of an informative rater. The (1 , d )-robustness condition implies a bound on the expected influence of the random guesser (Lemma 5), which limits the false positive rate (the frac-tion of random guessers who would have influence above the threshold). Likewise, honest raters must pass the test at least half the time. Results on hypothesis testing (see [4, Sec. 11.8]) lead to a lower bound on the number of samples. Total information loss : Theorem 7 bounds the number of rounds required for the rater r 1 to gain influence  X  in any (1 , d )-robust algorithm. In order to get an information loss bound, we set  X  to b/ 2 in the statement of Theorem 7. Until this influence has been reached, we know that the rater will be effectively restricted. This leads to a lower bound on the total information loss due to limiting the influence of a rater:
Theorem 8. Any (1 , d ) -robust recommender algorithm R must have IL ( R, P ( b ) , A ( b ))  X  log 1 d +log( b
Proof. With probability 0 . 5, the first rater is the in-formed rater r . We seek to prove a lower bound on the total additional information loss due to the restricted influence of rater r . Using the linearity of expectation, we have
E ( total additional loss) = X
First, we observe that for b 2 &lt; 32 d , the stated bound is negative and hence trivially true. Thus, it is sufficient to consider the case in which b 2  X  32 d  X  b  X  4  X  = b/ 2, we know from theorem 7 that the honest rater must have influence less than  X  for the first m = log(  X   X  rounds. For any i  X  m , by Lemma 2, the expected ad-ditional loss the target incurs on item i due to the rater X  X  restricted influence is at least b 2 / 4. Thus, the total addi-tional loss, conditioned on rater r rating first, is at least 4 . Taking into account the 0 . 5 probability that rater r is indeed first, we have an expected loss of mb 2 8 . Noting that  X  = b 2 and Remark: The value of m in this bound gives us the required setting for the number of items m b in P ( b ), in terms of the parameter d .
We can extend P ( b ) and A ( b ) to prove a lower bound for n -sybil attacks; this extension is outlined in this section. The structure has n genuine raters J = { r 1 , . . . , r n with an identical partition; each rates a disjoint set of items. The attacker also creates n sybils J A = { r  X  1 , r  X  2 , . . . , r  X  key is that the rating sequence X  X  is now constructed by an iterative random process: In each iteration, the attacker flips a coin, and if it is heads, he adds the next rater from r onto the list (as long as there are some remaining); otherwise, he adds the next rater from J A . This construction ensures that, at least until the first n raters X  ratings have been labeled, the probability that the next rater is an attacker is exactly 0 . 5, even conditioned on knowing the types of the raters up to that point . Thus, when deciding how much influence to give a rater, the sequence of ratings up to his first rating is irrelevant, and we can apply theorem 8 with our chosen value of d .

In principle, R could allow a different amount of expected damage on each rater, as long as the sum was no more than c . Given the  X (log(1 /d )) form of the bound, which is convex in d , dividing the expected damage equally minimizes the overall bound, and hence we assume without loss of general-ity that the expected damage on each of these first n items is be limited to d = c n .Putting this value into the statement of theorem 8 gives us:
Theorem 9. For each b  X  (0 , 0 . 5) , there is an infor-mation structure P ( b, n ) with n informed raters and attack A ( b, n ) such that any ( n, c ) -robust recommender algorithm R incurs an information loss of at least n log n c +log( b
Proof. Follows by substituting d = c n in theorem 8
In our lower bound, we constructed a simple family of instances P ( b ), and showed that any recommender system must incur an information loss (expected increase in predic-tion error) on these instances. We thus proved that there must be information loss in the worst case instances, where an instance denotes a particular distribution of tastes, spe-cific correlations with the target, and a specific rating order. It is natural to ask if this information loss is a rare problem that occurs only in pathological instances, or if it is likely to occur in common situations as well. In this section, we out-line several reasons to believe that, for any reasonable set of instances, the average-case loss is not likely to be much bet-ter than the worst-case lower bound: The bound is robust to more complex instances and different scoring rules; the attack model is fairly simple; and, the bounds would hold under alternative definitions of robustness. We also identify promising directions for future work in light of these results.
In our family of instances, the recommender prediction is always 0 . 5 before the rater arrives, and 0 . 5  X  b after optimal use of the rater X  X  information. However, the same form of bound can be derived for other settings. The key step in the lower bound proof is equation 1, through which the variance of the likelihood ratio g ( u ) is bounded in terms of b 2 can consider different instances in which the starting point is v 6 = 0 . 5; equation 1 can be shown to hold, with a different constant 1 v (1  X  v ) in place of 4. As long as v is bounded away from 0 and 1, this will yield a similar lower bound on infor-mation loss. We can also consider information structures in which the base prediction r is not the same for all items, but follows some distribution; or, in which the change magnitude b is not the same for all items, but follows some distribution. Provided the distributions have suitably bounded support, a similar  X (log( n/c )) loss bound will follow.

Another generalization is to look at alternative loss func-tions. In particular, the log-loss function is attractive be-cause the informativeness would be quantified in terms of the standard information-theoretic entropy. In this model, too, the lower bound holds; indeed, it holds even more gen-erally, without the restriction mentioned above that v be bounded away from 0 and 1.

Our attacks consist of simple random-guessing strategies, and our bounds thus hold for algorithms robust against any class of attacks that include these attacks. However, one critical feature of the attacks is that they mimic the rat-ing distributions of a plausible genuine rater. One class of manipulation-resistance algorithms identify suspicious raters by matching their rating histories to a set of attack pro-files [3, 12, 9, 7, 8]. Some empirical studies have examined the impact on predictions of removing suspicious raters. If there is no noticeable loss in prediction accuracy, our results implies that either the attack profiles are simplistic (i.e., easy to distinguish from genuine raters) or there is signif-icant information redundancy among the genuine raters so that information discarded from some genuine raters can be compensated by information from others. A fruitful direc-tion for future research is to incorporate such redundancy into our formal model.

Our definition of manipulation-resistance states that the expected net damage inflicted by the attacker on the target should not be too large. Other notions of robustness are also reasonable. One possibility would be to require that an informationless attacker cannot cause a large movement on any one item. Our lower bound extends to this model as well, because the y -guessing attacks we study are infor-mationless, and we only account for the damage caused on a single item rated. Another alternative is to require that the net damage is never too large (not just in expectation); this is the notion of manipulation-resistance that the Influ-ence Limiter satisfies. Our lower bound applies a fortiori to this definition as well. Another reasonable definition is to require that the aggregate damage across all legitimate targets be limited in some way. O X  X onovan and Smyth [10] suggest using accuracy information from multiple targets to judge credibility. Our bound of  X (log( n/c )) loss applies to this model, but is very weak as an aggregate bound. This is an interesting direction for future research.

In [14], it is shown that the Influence Limiter is ( n, c )-robust against a very broad class of strategies, and that the worst-case information loss it induces is O ( log n c sidered as a function of n alone, the  X (log n ) lower bound matches the O (log n ) upper bound. However, the constants in the two bounds differ significantly. The constants in the lower bound proof are probably not optimal, as our bounding technique required approximations that may not be tight. Nevertheless, it is an important challenge for fu-ture work to devise manipulation-resistant algorithms that move closer to the lower bound.
As mentioned, there are a number of papers on the topic of shilling attacks in recommender systems [11, 6, 3, 12, 7, 15, 8, 9, 14]. In this section, we discuss other literatures that are related to our approach.

We defined and analyzed an influence metric that mea-sures the change that a rater can cause to the predicted probability of an item. Rashid et al. [13] propose other algorithm-independent measures of rater influence.
The literature on bounded-regret online learning deals with combining predictions from multiple forecasters and proving worst-case bounds on the error relative to the best predictor that could be chosen in hindsight (see Cesa-Bianchi and Lugosi [2] and references therein). Awerbuch and Klein-berg [1] study manipulation in a different model of the rec-ommendation process: a user samples items and recommen-dations until he likes an item, at which point he recommends that item to others. They describe an online learning scheme and prove bounds on the number of samples required, even in the presence of adversaries.

There are parallels between our lower bound and the re-sults of Friedman and Resnick [5] on the social costs of cheap pseudonyms. At a high level, both results demonstrate that the possibility of creating false identities forces newcomers to be trusted less, thus leading to a loss of system performance or efficiency. However, the result of Friedman and Resnick relies on a characterization of equilibria in an economic gain model, whereas our model is information-theoretic rather than economic, and does not use equilibrium arguments.
In this paper, we have presented an information-theoretic model of informativeness and manipulation in recommender systems, and used it to shed light on the tradeoff between using all available information and resisting manipulation. The first insight, based on our lower bound, is that some amount of information loss is unavoidable in a recommender system that resists manipulation. Further, we note that, keeping other parameters fixed, the lower bound on required information loss is unbounded as the number of sybils n is increased towards  X  . This shows that no useful recom-mender system can be resistant to manipulation by an at-tacker with an unbounded number of sybils: to achieve this, the recommender would need to essentially throw away all rating information from genuine raters for indefinitely long, thus rendering it useless.
 This work was partially supported by the NSF under awards IIS-0812042, CCF-0728768, and IIS-0308006. [1] B. Awerbuch and R. D. Kleinberg. Competitive [2] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, [3] P.-A. Chirita, W. Nejdl, and C. Zamfir. Preventing [4] T. M. Cover and J. A. Thomas. Elements of [5] E. Friedman and P. Resnick. The social cost of cheap [6] S. K. Lam and J. Riedl. Shilling recommender systems [7] B. Mehta, T. Hoffman, and P. Fankhauser. Lies and [8] B. Mehta and W. Nejdl. Attack resistant collaborative [9] B. Mobasher, R. Burke, R. Bhaumik, and [10] J. O X  X onovan and B. Smyth. Trust no one: [11] M. O X  X ahony, N. Hurley, and G. Silvestre. Promoting [12] M. P. O X  X ahony, N. J. Hurley, and G. C. M. Silvestre. [13] A. M. Rashid, G. Karypis, and J. Riedl. Influence in [14] P. Resnick and R. Sami. The influence limiter: [15] J. Sandvig, B. Mobasher, and R. Burke. Robustness of
