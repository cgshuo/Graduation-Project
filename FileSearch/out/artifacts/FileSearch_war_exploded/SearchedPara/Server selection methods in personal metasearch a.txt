 Paul Thomas  X  David Hawking Abstract Server selection is an important subproblem in distributed information retrieval (DIR) but has commonly been studied with collections of more or less uniform size and with more or less homogeneous content. In contrast, realistic DIR applications may feature much more varied collections. In particular, personal metasearch X  X  novel application of DIR which includes all of a user X  X  online resources X  X ay involve collections which vary in size by several orders of magnitude, and which have highly varied data. We describe a number of algorithms for server selection, and consider their effectiveness when collec-tions vary widely in size and are represented by imperfect samples. We compare the algorithms on a personal metasearch testbed comprising calendar, email, mailing list and web collections, where collection sizes differ by three orders of magnitude. We then explore the effect of collection size variations using four partitionings of the TREC ad hoc data used in many other DIR experiments. Kullback-Leibler divergence, previously con-sidered poorly effective, performs better than expected in this application; other techniques thought to be effective perform poorly and are not appropriate for this problem. A strong correlation with size-based rankings for many techniques may be responsible.
 Keywords Server selection Distributed information retrieval 1 Introduction Distributed information retrieval (DIR) systems, or metasearchers, present an alternative to centralised indexes when searches cover several independent collections. A broker for-wards users X  queries to each collection independently, collates the results, and presents a single result set.

Personal metasearch is a novel application of DIR techniques which aims to incorporate all of a user X  X  online resources including, for example: email, calendars, local databases, and private or public Web sites. A personal metasearch tool would have several advan-tages: it could provide much greater coverage than any single system, could allow per-collection optimisations, and could significantly reduce user effort.

Unified search of the information sources routinely consulted by a knowledge worker is whole Web or the Dialog database from their personal computer; it is equally infeasible for external search engines like Google, Yahoo! and Live to crawl, and include in their index, the documents and email from everyone X  X  personal computers. In contrast to earlier DIR ogenous, varying from small private collections such as calendars to large public collec-tions such as the public Web. This contrasts with systems such as FedLemur (Avrahami et al. 2006 ), AllInOneNews (Liu et al. 2007 ), or  X  X  X ertical X  X  selection by web search well-known.

Brokers in personal metasearch or other DIR applications may include server selec-tion as a normal part of query processing. It may not be feasible to forward every query increased risk of unavailability. Server selection aims to choose the server or servers most useful for answering each query. As well as minimising costs and increasing reliability, there is some evidence that a selection algorithm can improve the quality of a Xu and Croft 1999 ).

In implementing personal metasearch, we observed collections which varied in size by several orders of magnitude and which are in some cases hard to sample. However, previous evaluations have typically used testbeds based on TREC ad hoc or Web Track data (French et al. 1998 , 1999 ; Powell and French 2003 ). This may not represent real DIR scenarios. Past evaluations have also assumed perfect knowledge of collection contents, which is not generally possible. This paper presents experiments evaluating server selection with varied collections and with imperfect knowledge of collections based on current sampling techniques. Twelve selection methods are considered in these experiments. Of these, CORI and extensions, CVV, vGlOSS, Kullback-Leibler diver-gence and extension, and ReDDE have been well-tested in other scenarios, but never all at once and never on a highly heterogenous testbed. bGlOSS, Zobel X  X   X  X  X  X  X , and CRCS are less well-studied. 2 Selection techniques From the large number of server selection algorithms in the literature, our experiments consider twelve which we consider applicable to a variety of tasks. Each selects one or more collections from C ; the set of collections available, for each user query q . They do collection c in C : A broker can rank the collections by this score, and choose either those collections with scores above a threshold or some number of top-scoring collections. The servers handling these collections are selected to receive the query. 2.1 The GlOSS family The  X  X  X lossary of servers server X  X  (GlOSS) family of algorithms estimates the number of documents in each collection which a user will find interesting, and ranks collections accordingly. We consider two versions. 2.1.1 bGlOSS The original GlOSS server selection algorithm, later called the Boolean version or bGlOSS (Gravano et al. 1994 ), assumes a Boolean retrieval model and that the interesting docu-ments in a collection are those which match all terms in the query q . If these terms, T q , appear independently, the expected number of interesting documents can be estimated by where df c ( t ) is the document frequency of term t in collection c and N c is the size of c .As originally described, bGlOSS selects only the collection with the highest score. In the collections.

Although the servers used in the experiments below do not use a Boolean model, bGlOSS is similar to other selection techniques based on different assumptions and is therefore included for comparison. 2.1.2 vGlOSS The vector-space version of GlOSS, vGlOSS (Gravano et al. 1999 ), assumes users are interested not in documents containing all query terms but in those documents whose similarity to the query is more than a threshold l . vGlOSS uses either of two algorithms X  MAX and SUM  X  X o estimate the number of interesting documents. A common application l = 0. In this case, MAX and SUM are identical: where w q ( t ) and w c ( t ) are the weight of term t in the query and in the collection respec-tively. In the original presentation, w c ( t ) is calculated at the server, for example using term frequency and inverse document frequency, and is then made available to the broker. In a DIR application without cooperative servers, however, this weight is more likely to be estimated from a sample. 2.2 CORI The CORI algorithm (Callan et al. 1995 ) adapts INQUERY document ranking. It treats each server as a compound  X  X  X ocument X  X , using document frequency (df) instead of term frequency and collection frequency (cf) in place of document frequency. Each term in the query is scored separately, according to document frequency ( T ) and inverse collection frequency ( I ): where cw c is the number of terms in collection c and cw is the mean number of terms taining t . The other terms are constants: b = 0.4, df _ base = 50, and df _ factor = 150. The score for a query is the mean of the per-term scores, s c  X  q  X  X 
CORI has been widely used and has proven effective in selection tasks based on TREC ad hoc data and Web data, although performance appears poorer when collection sizes are highly variable (D X  X ouza et al. 2004; Si and Callan 2003a ).

Extended CORI In practice, the frequency data used by CORI will be estimated from language models which may be of widely varying size both compared with each other and compared with the underlying collections. Later work (Si and Callan 2003a ) suggested two extensions to the basic CORI algorithm, both of which make use of the N m documents sampled from the collection in the course of building a language model. The first extension similarly. The second extension uses both these modifications, and also scales df_base and df_factor . Since these variants expressly adjust for estimated size, they might be expected to perform better over heterogenous collections. 2.3 Lexicon inspection Zobel X  X  algorithm  X  X  X  X  X  (Zobel 1997 ) is a simple inner product without length normalisation: where the weights are based on analogues of term frequency and inverse document fre-quency: the query weight w q ( t ) = log(tf q ( t ) ? 1) w ( t ); the collection weight w taining a term t across all collections in C : The function in Eq. 3 is identical to MAX (0) and SUM (0) from vGlOSS (Eq. 2), although in the case of vGlOSS weights are calculated with collection-specific data only and may vary, even for the same term, from collection to collection. In the experiments described below, this difference in weights results in a significant difference in performance.

One of Zobel X  X  testbeds included collections which varied in size by three orders of magnitude, from 14 documents to 23,000. Although the testbed was in part randomly generated, this wide range of sizes suggests that algorithm  X  X  X  X  X  may be of general use. 2.4 CVV Cue validity variance (CVV) (Yuwono and Lee 1997 ) is a server selection technique which weights terms according to their power to discriminate between collections. The technique begins by calculating the cue validity of each term t at each collection c , CV( t , c ), which measures the extent to which t distinguishes c from other collections: probability of t occurring in a  X  X  X ontrasting concept X  X , is the relative frequency of t in all other collections: (The implementation used in the experiments described below assigns CV( t , c ) = 0 when t is not present in any collection, meaning the term will not contribute to any collection X  X  score.)
The cue validity of t at c gives an indication of how well t distinguishes documents in c from documents from all other collections. CVV( t ), the cue validity variance of term t ,isa measure of how useful a term is in distinguishing collections in C in general. CVV( t ) is just the variance of CV( t , c ). Finally, collections are scored according to the frequency of each query term, each weighted so that terms with more discriminative power are considered more important: s c  X  q  X  X  2.5 Kullback-Leibler divergence Kullback-Leibler (KL) divergence was suggested for server selection by Xu and Croft ( 1999 ) and interpreted in a language modelling framework Si et al. ( 2002 ). The language modelling interpretation ranks each collection c according to its probability of being generated given a model of query q : i.e., s c ( q ) = Pr( c | q ). By Bayes X  X  theorem therefore, Note that Pr( q ), the prior probability of the query, does not depend on the collection and can therefore be ignored. Similarly, Pr( c ), the prior probability of collection c , is normally considered constant and can also be ignored.

With these two simplifications, and assuming that terms occur independently, we can say Pr  X  q j c  X  X  to rank collections equivalently to KL divergence (Thomas 2008 ).

Smoothing can be used to account for infrequent terms. Si et al. ( 2002 ) use a global model, m g , which captures term occurrences across all collections. Term data from this global model is then combined with data from each collection in a modification of the above:
The parameter k controls the mixing and is typically 0.5 (Si and Callan 2003a ; Si et al. 2002 ). Note that when k = 1, KL divergence ranks equivalently to bGlOSS.

Extended Kullback-Leibler divergence There is no explicit control for collection size in the method described above. Si and Callan X  X  extended algorithm (Si and Callan 2003a ) assumes that larger collections are more likely to be relevant and assigns the prior prob-N = 2.6 ReDDE The relevant document distribution estimation method, or ReDDE, is also due to Si and Callan ( 2003b ). As with the GlOSS family, ReDDE attempts to estimate the distribution of relevant documents across all collections based on a simple approximation of relevance. The algorithm computes an estimate of | REL c ( q )|, the number of documents in c which are relevant to the query q ; this forms the basis of the eventual ranking.
If m , the model of c , is representative, it is possible to estimate Pr  X  d j c  X  by assuming a can also estimate Pr  X  relevant j d  X  using the documents sampled when building the model, D m :
Note that this is the expected number of relevant documents; the formulation is similar to that used by bGlOSS (Eq. 1). The remaining task is to estimate the probability that each document d in D m is relevant to q .
 ReDDE estimates this from a hypothetical ranking of all documents in all collections. Each of the top-ranked documents in this complete ( X  X  X entral X  X ) list has a fixed chance of relevance and contributes to the score of the collection it comes from: where RANK _ CENTRAL ( d ) is the rank, over all documents in all collections, of d ; and N all is the total number of documents in all collections,
To compute this complete ranking would require a copy of every document in every collection, which is of course infeasible. RANK _ CENTRAL ( d ) can instead itself be estimated using the samples collected in the course of building models for the collections. All documents sampled from all collections are indexed by the DIR tool itself, and ranked for each query by some effective method; the ranking of sample document d is RANK _ SAM-( d ). With this sample rank, RANK _ CENTRAL ( d ) can be estimated from the total number of documents ranked ahead of d : the intuition is that each document sampled from c stands for N / N m documents in the complete ranking. where c d 0 is the collection from which d 0 is drawn, and m d 0 the sample from which d 0 is drawn.

Substituting the estimates in Eqs. 7 and 8 into Eq. 6 lets us estimate the total number of documents in each collection which are relevant to q . The final score is a normalised version of this total: 2.7 Central-rank-based The central-rank-based collection selection (CRCS) algorithms (Shokouhi 2007 ) are similar to ReDDE, and also make use of an index of sample documents. As for ReDDE, these documents are assumed to be representative of the collections they are drawn from; they are ranked for each query, and those collections which contribute highly-ranked sample documents are selected.

ReDDE awards collections a fixed score for each highly-ranked sample document (Eq. 7). Shokouhi notes that this does not reflect the documents X  likely utility: when ordered by an effective system, top-ranked documents are generally more useful than those of lower ranks. CRCS therefore allocates a rank-based, rather than fixed, score to each of the top c sample documents. Shokouhi gives two variations. In the linear version, CRCS(l), version, CRCS(e), These per-document scores are summed for each collection, in the same manner as ReDDE, and a final score is calculated with N max is the size of the largest collection, and is used to normalise the scores. Besides this normalisation, CRCS is very similar to ReDDE. Where ReDDE expressly rewards col-lections with a large number of high-ranked documents, however, CRCS rewards collec-tions with both large numbers of documents and documents with particularly high ranks. 2.8 Decision theoretic models Starting from assumptions regarding the independence of relevance judgements and the cost of retrieving documents, Fuhr ( 1999 ) derives a decision-theoretic formulation of the server selection problem. This method aims to minimise the total cost of retrieval, including costs of searching at each server, retrieving results, and presenting documents. Experiments used 100 collections of uniform size drawn from TREC data (Nottelmann and Fuhr 2003 , 2004 ); it showed greater precision than CORI, but depends both on training collections in the most general case.

These methods seem to offer advantages when coping with the wide variety of servers found in personal metasearch, since they expressly model per-server attributes such as effectiveness and retrieval cost, and can potentially be used to retrieve more results from a server estimated to have a larger set of relevant documents.

The major limitation of the decision-theoretic framework in practical applications is the need to specify cost functions in advance. Our (admittedly anecdotal) observations of the use of personal metasearch tools suggest that:  X  Cost functions for each personal metasearch installation would have to be specified  X  The appropriate cost functions are likely to change from task to task and even from
It is our belief that individual users would find it difficult to specify cost functions and a burden to do so. In the future it may be possible to provide personal metasearch tools with effective query classification capabilities and to enable the tool to adaptively learn appropriate cost functions from user behaviour and feedback.

Following similar lines, Wu and Crestani ( 2002 ) considered the decision-theoretic model  X  X  X oo abstract X  X , and have instead suggested a simpler  X  X  X ulti-objective X  X  model. The multi-objective method has not been compared with other selection methods, although absent an agreed set of costs and weights it is not clear how this could be done. 2.9 Other methods As well as those above, a large number of server selection methods have been described in the literature (including contributions from Craswell et al. ( 2000 ); Hawking and Thist-lewaite ( 1999 ), Larson ( 2003 ), Rasolofo et al. ( 2001 ), Shen and Lee ( 2002 ), Si and Callan ( 2005 ), Wu et al. ( 2001 ), and many others). In our experiments we have retricted ourselves to those methods which seem most generally applicable: those which make few demands on individual servers, have few or no parameters that require tuning, and do not need bootstrapping with sample queries or known-good relevance judgements. 3 Selection experiments In general, the performance of the selection techniques above has been evaluated using TREC ad hoc or Web Track data X  X ften partitioned with an eye to producing collections of approximately equal size (Table 1 ). The availability of extensive relevance judgments for the TREC ad hoc collections has made them the basis for most DIR testbeds. However, such collections do not resemble those likely to be used in working DIR applications: the Web is extremely large and highly diverse, while TREC collections are all of one data type.

The experiments reported below consider the likely performance of these techniques in size. We consider three questions: 1. How does selection performance compare across methods, and across queries? 2. How does selection performance vary, given samples and size estimates of different 3. Which, if any, of the methods is appropriate for selection in such an environment?
We also examine the performance of these techniques on common testbeds derived from TREC data. Although these testbeds do not correspond to any particular application of selection techniques, we are able to confirm the trends seen on our personal metasearch testbed.

Previous work has presented performance figures for CORI and extensions, KL divergence and extension, and ReDDE on a common testbed (Si and Callan 2003a ). The implementation of these algorithms used in these experiments was validated against this data. One further method is included as a baseline: the  X  X  X andom X  X  method ranks each server randomly for each query, without using any information about each server. 3.1 Personal metasearch testbed range of resources which are likely to be used in personal metasearch applications: sizes range over three orders of magnitude, data types are varied, and topic areas range from the very focussed to the very broad. Each collection is mostly English-language. None are on the scale of the largest likely collections, such as the Web or Dialog. However, rather than ask each user X  X  tool to estimate the size of (for example) the public Web, it is likely that a broker would use pre-computed characteristics for very large collections. The collections used here span the likely size range of local, private, or enterprise collections, where pre-computed data is infeasible.  X  The  X  X  X alendar X  X  collection contains 1049 appointments from a calendar application.  X  The  X  X  X sh-list X  X  and  X  X  X rocmail X  X  collections are archives of public mailing lists discussing  X  The  X  X  X mail X  X  collection includes around 25,000 documents from a personal email  X   X  X  X SJ X  X  incorporates around 99,000 articles from several years of the Wall Street  X   X  X .GOV X  X , the largest collection used here, is a 1.2 million page crawl of Web hosts in
Compared with other testbeds, the personal metasearch testbed used here has fewer from Zobel ( 1997 )), and a much wider variety of document topics and sources. 3.2 Queries Queries for these experiments were created with the intention of representing the variety of topics covered in the testbed. They were generated in a number of ways and formed six sets of 20 queries, each based on one of the six collections.

Past work has often used long queries; for example, Xu and Croft ( 1999 ) report a series of experiments with a mean 34.5 terms per query. This is evidently much larger than the 1.7 X 2.6 terms typical of queries to web search services (Beitzel et al. 2004 ; Jansen et al. 2000 ; Silverstein et al. 1999 ; Spink et al. 2002 ; Zhang and Moffat 2006 ), and the queries used here were accordingly kept short.  X  Subject and location fields were extracted from calendar entries, stopwords were  X  A similar process was used to construct queries from subject lines in the email  X  The procmail and zsh-list collections represent mailing lists on narrow technical  X  The  X  X  X opic X  X  field of twenty topics from the TREC ad hoc track, all of which had at  X  The  X  X  X opic X  X  field of twenty topics from the TREC Web track made up the final set of
Queries in other test collections have typically been generated manually. The first four sets of queries used in these experiments were instead created automatically from iden-tifying fields; the intention was to mimic the terms a user might type if they were familiar with the contents of each collection. It is not clear how well these automatically-generated queries match the queries DIR users would really type, but nor is this clear for the manually-generated queries of the last two sets.

Our testbed also differs in the density of relevant documents. In most previous work, collections are of approximately unform document density, so that for example a collection ten times larger will have about ten times as many relevant documents. The testbed used here instead is constructed under the assumption that collections are approximately equally useful, in the long run, and therefore collections have approximately the same number of relevant documents regardless of size. Over all 120 queries, the number of relevant doc-uments per collection varied from 45 for the calendar collection (1049 documents) to 101 for WSJ (99,000 documents). This difference in density means algorithms should not simply select the largest collection. 3.3 Models and size estimates The selection techniques used in these experiments rely on language models and size estimates of the collections involved. This data was drawn from three sources.

As a baseline, models were built using all documents in each collection, and  X  X  X stimates X  X  of collection size were entirely accurate. Although not possible in practice, this provides a best case for comparison. Two further sets of models represented the best-performing and the most-used of the present techniques. These are discussed further in Sect. 4.2 below. 3.4 Measures Let each collection c have an associated merit, denoted MERIT c ( q ), which is a measure of how good a choice c is for this query. R n is the proportion of this merit captured by the n top-ranked collections (Gravano and Garc X   X  a-Molina 1995 ): collection in the optimal ranking. R n ranges from 0, meaning there is no merit in the first n collections selected, to 1, meaning the first n collections selected are as good as possible.
MERIT c ( q ), the (real) utility of a collection c for a query q , has been defined in a number of ways. Gravano and Garc X   X  a-Molina ( 1995 ) define MERIT in terms of the total similarity between c and q : the sum, over all documents d 2 D c , of the similarity between d and q .If servers work on a vector-space model, this is an approximation of what an effective server might retrieve in response to a question. This has the effect of including an aspect of server performance, but as well as assuming a vector-space model at the servers it assumes that documents similar to the query are likely to be useful. This definition has also been used by French et al. ( 1998 ) and French et al. ( 1999 ) (who also use the relevance-based definition discussed below), and Gravano et al. ( 1999 ).

An alternative definition of MERIT simply counts the number of relevant documents at may not be returned by a server, this measure is independent of server performance. It has been commonly used for this reason (French et al. 1998 , 1999 ; Hawking and Thistlewaite 1999 ; Si and Callan 2003a , b ), and it is this definition we use in the results reported below. (Where servers vary in the cost of retrieving results, this may not be appropriate. A measure of selection quality that properly accounts for this, but which can be compared across testbeds, remains for future work.)
For the testbed used here, a random ordering has an expected R n of around n /6 since there are few queries where more than one collection holds relevant documents. The same is true for a size-based ordering, since each collection holds around the same number of relevant documents. The best ordering, ranking servers according to the number of relevant documents they manage, has a score of R n  X  1 for all n .

We chose to use the R n measure in order to focus on the effect of variations in collection size on selection effectiveness, using the measure most commonly used in previous studies of the algorithms we have evaluated. We wanted to see whether previous observations on these algorithms could be confirmed over a significantly wider range of collection sizes than previously studied, and in a simulation of a realistic DIR application.
We recognize that R n may not correlate well with searcher satisfaction (the gold standard) and that extensive user observation is needed to choose the evaluation measure (and methodology) and to determine to what extent the choice is dependent upon the query or search task. Readers are referred to Thomas and Hawking ( 2008 ) for the results of a small scale pilot study of real personal metasearch users, designed to shed light on whether user judgments tend to confirm testbed evaluations of selection methods. 4 Results We discuss the results of our experiments when selection algorithms were given  X  X  X erfect X  X , estimated from document samples. We also report the observed correlation between data quality and performance of two promising selection algorithms. 4.1 Selection with perfect models and sizes The first experiment provided a baseline for later comparisons. Each method was run with parameters set as described by the original authors; each method was also allowed a perfectly  X  X  X stimate X  X . In the case of tied scores s c ( q ), collections were ordered arbitrarily.
The bGlOSS algorithm has no tunable parameters. Other algorithms used the following:  X  vGlOSS used threshold l = 0, so any documents at all similar to the query were Any query terms which did not appear at all in c were assigned a collection weight of zero. Weights were normalised by dividing through by the Euclidean norm.  X  CORI used b = 0.4, df _ base = 50, and df _ factor = 150 (Callan et al. 1995 ).  X  Zobel X  X  algorithm  X  X  X  X  X  used weights as described in Sect. 2.3 . If a query term did not  X  The KL divergence implementation used mixes in a global language model as in Eq. 5 .  X  ReDDE is considered robust with values of r from 0.002 to 0.005 (the top 0.2% to 0.5%  X  Both CRCS methods give no points to any document not in the top 50 (so c = 50). For
Figure 1 summarises the performance of each method given this baseline data. (Note that lines are interpolated for convenience; n must be integral.) Since models are built from all documents, N m = N c and so the three variants of CORI are identical.

In this case the KL divergence method is clearly the best-performing of the techniques tested; R n scores from this technique are significantly higher than those from any other method for n = 1 to 5 (one-sided t test, a = 0.05). vGlOSS, on the other hand, does relatively poorly and has significantly lower scores than all other methods for n = 2or3. bGlOSS, CORI and variants, extended KL divergence, and CRCS variants also perform well. 4.1.1 Correlation with size-based ranking French et al. noted a very strong correlation between collection ranks from vGlOSS and ranks based on collection size alone (French et al. 1999 ). Collection sizes in DIR appli-cations may range from a few hundred documents (for small email archives, for example) documents, any tendency to select larger collections could result in poor overall perfor-mance. A second set of experiments investigated this bias.

Figure 2 plots the correlation between rankings based on server scores s c ( q ) and those based on collection size N c , over all 120 queries for each selection method. Spearman X  X  largest to smallest.
 It is clear that most methods do in fact correlate highly with a size-based ranking. From Fig. 2 , it seems only vGlOSS and KL divergence are relatively weakly correlated, while bGlOSS, CORI and extensions,  X  X  X  X  X , CVV, extended KL divergence, and the CRCS variants seem prone to ranking large collections highly regardless of the query. For example, bGlOSS ranks .GOV highest for 80 of the 120 test queries despite there only being 20 queries for which this collection actually contains the largest number of relevant documents. Most other methods are similar, ranking .GOV first for between 58 and 119 of the 120 queries. vGlOSS, which is less prone to ranking by size, ranks .GOV first for only 4 of the 120 queries, and KL divergence does the same for only 37.
A straightforward examination of the methods demonstrates why large collections are favoured. bGlOSS and extended KL divergence include an explicit adjustment for N c , the size of the collection; CORI and extensions,  X  X  X  X  X , and CVV use either df c or tf c , which may be expected to correlate highly with collection size. Although ReDDE and CRCS do not explicitly adjust for collection size in this instance (since N m = N c ), the sample index will be dominated by documents from larger collections. In both vGlOSS and KL divergence frequency information is normalised on a collection-by-collection basis and larger col-lections are not favoured.

For example, consider that subset of queries for which .GOV (the largest collection, with 1.2M documents) is the best answer, and that subset for which calendar (the smallest, with 1k documents) is the best answer. Performance on the first set, where success cor-responds to choosing the largest collection, is near-perfect for most methods: this is expected, since most methods are highly correlated with a size-based ranking which would rank .GOV first. vGlOSS, however, which is less strongly correlated, is only significantly better than random selection for n = 5. The situation is reversed for the second subset of queries, where the task is to choose the smallest collection (Fig. 3 ); only KL divergence and vGlOSS, the two methods least prone to size-based ranking, are significantly better than random selection for any n . Other methods tend to rank the small calendar collection low despite it being the best choice in these instances.

French et al. ( 1999 ) report much higher correlation between a size-based ranking and vGlOSS (mean r s = 0.97) than is seen here (mean r s =-0.06). This may be due to differences in the distribution of relevant documents: French et al. observe that the larger collections in their testbed tended to have more relevant documents, which would lead them to be highly ranked by any effective selection technique, and a relevance-based ranking of collections in their testbed correlates moderately well with a size-based ranking ( r s = 0.54 on average, and r s C 0 for all queries). The distribution of relevant documents in the testbed used here, however, is much more uniform. A difference in calculating the across a single collection at a time, could also give rise to this disagreement. Data from French et al. and these experiments are in broad agreement however on the correlation between CORI and a size-based ranking, and this correlation was also mooted by D X  X ouza et al. (2004). 4.2 Selection with samples and estimated sizes The experiments described above use perfect models, or complete samples, and exact size  X  X  X stimates X  X . This provides a baseline, but is of course impractical in most real DIR applications; a third set of experiments therefore considered selection performance with language models and size estimates built from sampled documents.

Two sets of samples and size estimates were used. The first set used samples of 300 documents per collection generated by the multiple queries sampler (Thomas and Hawking 2007 ) and size estimates from multiple capture-recapture (Shokouhi et al. 2006 ); these were the best-performing sampler and size estimator in previous experiments (Thomas and Hawking 2007 ; Thomas 2008 ). (The multiple queries sampler could only sample 200 documents from the calendar collection.) The second used samples of 300 documents generated by query-based sampling (Callan et al. 1999 ) and size estimates from capture-recapture (Liu et al. 2001 ), which have been commonly used in previous work (Hawking and Thomas 2005 ; Shokouhi et al. 2007 ; Si and Callan 2003a , b ).
 The choice of a 300-document sample size follows previous work (Callan et al. 1999 ; Hawking and Thomas 2005 ; Nottelmann and Fuhr 2003 ; Si et al. 2002 ; Si and Callan 2003a , b ) but is essentially arbitrary. A more sophisticated approach may attempt to sample documents until, for example, the learned language model appears stable (Baillie et al. 2006 ); on these measures, and on the testbed used here, early experiments suggest this is at about 300 X 400 documents per collection.

Selection methods needed only minor adaptations to operate with models built from sampled documents. Term and document frequencies for bGlOSS, vGlOSS, CORI,  X  X  X  X  X , CVV, and KL divergence were estimated from models and scaled according to the ratio ^ N = N m : ReDDE, extended CORI, and the two CRCS variants explicitly adjust for differ-ences in model size. Otherwise, parameters were as described in Sect. 4.1 .

Most methods performed worse with these lower-quality models than with the perfect models of Sect. 4.1 for at least some values of n . KL divergence was previously the best-performing method; with models built from 300-document samples from the multiple queries sampler and size estimates from multiple capture-recapture, it is significantly worse for n = 1 to 5 (one-tailed Wilcoxon test, a = 0.05). bGlOSS, another method which performed well with correct data, is significantly worse for n = 1 and 3 to 5; extended KL divergence is significantly worse for n = 1 to 5; and both CRCS variants are significantly worse for n = 1 and 2. CORI, which performed relatively well in earlier experiments, is not significantly worse for any n .

Similar trends held for the second set of runs, which used data from the query-based sampler and (single) capture-recapture. Again KL divergence, its extension, bGlOSS, and CRCS were significantly worse for several values of n , although KL divergence and the CRCS variants still performed well; CORI was not significantly worse than before at any point, and in fact was significantly better for some n . Taken with the observations on size-based ranking above, this suggests that KL divergence is an appropriate server selection method when collections vary greatly. CORI, CRCS(l), or CRCS(e) may also be appro-priate, although they do a poorer job of selecting smaller collections when needed. Choice of selection method may also be influenced by the quality of samples and size estimates.
The extensions to CORI,  X  X  X ORI-ext1 X  X  and  X  X  X ORI-ext2 X  X  in Figs. 4 , and 5 , have little effect on CORI X  X  overall performance. This is consistent with earlier results from Si and Callan ( 2003a ) and Hawking and Thomas ( 2005 ). 4.3 Measures of model quality Results from earlier experiments suggested that the quality of a language model has a significant effect on the performance of selection algorithms. A further set of experiments language model and the performance of these algorithms? If so, any technique which improves the quality of models should have an impact in improved selection as well as in any other applications.
Ninety sets of samples were generated, ten each of 100 X 900 documents (in steps of 100), chosen randomly from each collection. Models of each collection were built from each of these ninety sets, and each model X  X  quality was calculated according to three measures: ctf ratio (Callan et al. 1999 ), which measures the proportion of term occurrences in the collection which are accounted for by terms in a model; Spearman X  X  r s ; and Kull-back-Leibler divergence (D KL ). The mean measure, over each set of six collections, was treated as an indication of the quality of that set of samples. In each case, the mean measure varied from set to set but improved as more documents were included.

To investigate the correlation between these quality measures and the performance of server selection methods, the CORI and KL divergence algorithms X  X wo of the most promising selection algorithms from earlier experiments X  X ere run with each set of models over the same 120 queries as before. (Size  X  X  X stimates X  X  in these runs were correct, performance of the algorithm (measured as mean R 1 ; or the mean recall at one collection selected over all 120 queries) against ctf ratio. Similar results were seen for r s and D KL . All three measures X  X tf ratio, r s , and D KL  X  X orrelate highly with R 1 for both CORI and KL divergence, with absolute coefficients of correlation (Spearman X  X  r s ) of between 0.64 and 0.82 ( p 0.05 in each case). This suggests that selection performance, at least for the CORI and Kullback-Leibler algorithms, will increase if language models are improved and degrade if they are made worse. Further, all three measures are useful; all are good predictors of selection performance. 4.4 Other testbeds The results presented above are derived from our testbed, which represents a particular application of distributed IR techniques. Our final set of experiments investigated the performance of server selection algorithms on alternative testbeds.
 We consider four testbeds generated from TREC ad-hoc documents and queries. Although these testbeds are essentially arbitrary and do not represent any particular application, the availability of relevance judgements means they have been widely used in earlier selection experiments (for example Powell et al. 2000 ; Powell and French 2003 ;Si and Callan 2003b , c ; Shokouhi 2007 ). Each is comprised of the same documents, divided to give four different distributions of those which are relevant.  X  The  X  X  X niform X  X  testbed takes documents from TREC CDs 1, 2, and 3 and divides them
Other testbeds are derived from the uniform testbed by aggregating some smaller collections into new, larger ones.  X  In the  X  X  X elevant X  X  testbed, documents from the Associated Press which comprised 24  X  Finally, in the  X  X  X epresentative X  X  testbed two large collections are built each with 20% of
Queries 51 to 100, and the associated TREC relevance judgements, were used in each case. Results for each testbed are illustrated in Fig. 7 .

Results are clearly different with the TREC testbeds than with the personal metasearch testbed; further, the results from the uniform and nonrelevant testbeds are similar as are those from the relevant and representative.

This observation is explained by characteristics of the testbeds. The relevant testbed, for example, is constructed such that the large collections are more often the best choice for any query; the two largest collections have around 120 relevant documents per query, on average, while the remaining collections typically have four or fewer, and there is a strong correlation between size and mean number of relevant documents (Spearman X  X  q = 0.95, p 0.01). In the representative testbed, although the density of relevant documents is roughly equal the absolute number is not and again the larger collections are a better choice (and here again q = 0.95, p 0.01). In these cases, selection techniques which correlate well with a size-based ranking do well and those which do not (vGlOSS and Kullback-Leibler) fare poorly. A simple size-based ranking is almost indistinguishable from the best of the algorithms.

The correlation between size and number of relevant documents still holds in the nonrelevant testbed, but is much less pronounced ( q = 0.55, p 0.01), and there is no significant correlation in the uniform testbed. 4.5 Summary of results Selection experiments based on perfectly accurate models and size estimates demonstrated that KL divergence performed well, as to a lesser extent did CORI and variants, extended KL divergence, and bGlOSS. Many methods are prone to ranking larger collections highly, regardless of their usefulness for any particular query; only KL divergence and vGlOSS were found to be able to select small collections better than chance.

With less accurate models and size estimates, almost all methods were significantly poorer. CORI, which is less affected by inaccuracies in the model, and KL divergence, which is less prone to size-based ranking, seem appropriate choices for selection algo-rithms in real-world tools.

Comparing results from the TREC testbed with those from the personal metasearch testbed, it is clear the measured performance of selection algorithms varies greatly depending on the characteristics of the testbed used. In particular, the density of relevant documents in each collection is significant. For the personal metasearch application, or others where size varies greatly but the larger collections are not always a good choice, this means conclusions based upon earlier evaluations may not apply. 5 Discussion and conclusions Personal metasearch is a novel but real IR application, where a broker provides a single search interface over all of a user X  X  online resources. Since many disparate sources must be little understood.

Most DIR tools, including personal metasearch tools, need a query-time process of server selection to identify which servers may be useful. This process can reduce costs, especially if servers charge for access, can improve reliability and efficiency, and if done well enough can improve results even compared to a single index. Many selection methods have been sug-gested; twelve of the most generally useful have been tested here, in a testbed with widely varied collection sizes which is representative of personal metasearch applications. (a) (b) (c) (d)
Overall, the CORI algorithm is promising when collection sizes vary, and is robust to poorer quality models; Kullback-Leibler divergence is also robust and is much more likely to select smaller collections when appropriate. Experiments in this paper have shown several further trends, and corresponding directions for further work.

In the case of personal metasearch, collections will vary greatly in size from the fairly small (individual folders, addressbooks, calendars) to the very large (the web, Dialog). However, we do not expect the size of a collection to correlate with the number of times it proportion of queries despite being several orders of magnitude smaller than the largest collections. This is not true of some other DIR applications, and is certainly not assumed by the common TREC-based testbeds.

Correcting for collection size substantially reduces performance when smaller collec-tions are required, and such corrections are unlikely to improve performance overall unless most user needs are in fact answered well by larger collections. A bias towards large (or indeed small) collections is therefore undesirable in personal metasearch, but most of the methods tested exhibit a strong bias (Fig. 2 ). A working broker in this application would need to choose small collections when appropriate.

Personal metasearch is also distinguished by its coverage of a wide variety of data types; work to date has generally considered the small range of data in TREC-based testbeds. In most cases, the methods tested here perform more poorly on our application-especially true when few servers are selected.

Sampling and selection methods currently take no account of the size, type, genre or structure of documents within collections. These aspects could potentially be exploited. For example, if a searcher is known to be seeking movie reviews or media releases, type-or genre-aware selection techniques could easily determine that calendars, staff databases, dictionaries or source code repositories are poor candidates for inclusion. Investigation of these dimensions of sampling and selection is left for future work.

The quality of language models and collection size estimates used in selection tasks has been shown to be important: almost all methods suffered a drop in performance when given models inferred from sampled documents. Further, for the Kullback-Leibler diver-gence and CORI techniques there is a strong correlation between measures of model quality and early performance. This suggests that techniques for building representative models will be of importance to any real-world tool using any of the most promising selection techniques.

Unfortunately current size estimation techniques are impractical for collections on the scale encountered by personal metasearch tools (up to 10 6 documents in our testbeds, and several orders of magnitude larger in practice), and sampling techniques and hence lan-guage models are poor at many scales X  X ompare for example Figs. 4 and 5 with Fig. 1 .
Although it is not yet clear what impact selection may have on final user satisfaction it appears that these DIR techniques, tested largely on artificial testbeds, are inappropriate for the real application of personal metasearch. Techniques are needed which are better able to cope with wide varieties of collection size, poor quality language models, and a variety of subject matter.

The decision-theoretic or multi-objective frameworks are theoretically general enough to accommodate this problem but seem too difficult to apply in a practical broker, due to the dependence of cost functions not only on users and collections but on individual queries; it is so far impractical to specify them on this basis. Much more observation is needed of personal metasearch in practice to identify modes of use which could be associated with particular clusters of cost functions. To date it is not known how well these methods work when given approximate input such as is derived from sampling.

Finally, we note that a personal metasearch tool receiving frequent use is in an ideal position to observe its owner X  X  behaviour. If it included an effective task/query classifier, it could almost certainly learn to improve its performance over time, and would certainly provide an insight into effective techniques for server selection as well as other DIR tasks. these directions is continuing.
 References
