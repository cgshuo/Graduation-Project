 Formulating and processing phrases and other term dependencies to improve query effectiveness is an important problem in informa-tion retrieval. However, accessing these types of statistics using standard inverted indexes requires unreasonable processing time or incurs a substantial space overhead. Establishing a balance be-tween these competing space and time trade-offs can dramatically improve system performance.

In this paper, we present and analyze a new index structure de-signed to improve query efficiency in term dependency retrieval models, with bounded space requirements. By adapting a class of ( ,  X  ) -approximation algorithms originally proposed for sketch summarization in networking applications, we show how to ac-curately estimate various statistics important in term dependency models with low, probabilisti cally bounded error rates. The space requirements of the sketch index structure is largely independent of this size and the number of phrase term dependencies.

Empirically, we show that the sketch index can reduce the space requirements of the vocabulary component of an index of all n -grams consisting of between 1 and 5 words extracted from the Clueweb-Part-B collection to less than 0 . 2% of the requirements of an equivalent full index. We show that n -gram queries of 5 words can be processed more efficiently than in current alternatives, such as next-word indexes. We show retrieval using the sketch index to be up to 400 times faster than with positional indexes, and 15 times faster than next-word indexes.
 H.3.4 [ Information Storage and Retrieval ]: Systems and soft-ware X  performance evaluation .; H.3.1 [ Content Analysis and In-dexing ]: Indexing methods; H.3.3 [ Information Search and Re-trieval ]: Information filtering Sketching, Indexing, Scalability, Term Dependency Models
Term dependency models are a compelling new approach to im-proving the effectiveness in ranked document retrieval. A term de-pendency is any relationship between two or more terms. Examples of term dependencies include noun phrases, verb phrases, ordered windows, unordered windows, spans of text, or any sequence of n words. Many recently developed retrieval models depend on statis-tics extracted for query term dependencies [1, 7, 9, 10, 16]. While these methods have been shown to significantly improve the effec-tiveness of the retrieval model, prior work has not addressed how to efficiently generate the necessary statistics at query time.
We investigate the problem of providing access to document level statistics for all n -words in the collection in a space and time ef-ficient manner. We define the term n -word as any sequence of n sequential words. In order to avoid confusion with character level n -grams, or linguistic definitions of various types of phrases, we will use the term n -word throughout this paper. This type of term dependency is often used as a surrogate for more complex, linguis-tic term dependencies. Improving the efficiency of the calculation of this type of retrieval model feature can improve the efficiency of each of the above retrieval models.

Previous approaches have used a variety of different index struc-tures. Positional indexes [15] support the calculation of n -word statistics by processing n term posting lists simultaneously, and comparing position offset information for each term in each doc-ument. Next-word indexing [14] was proposed as an attractive trade-off between index space and retrieval efficiency for process-ing phrase queries. A next-word index stores position data for pairs of terms. Similar to the positional index, statistics for longer phrases are computed at query time by comparing positional data.
Direct indexes of all n -words dramatically reduces the cost of processing positional information at query time, however these in-dexes have unacceptably large space requirements. Filtered in-dexes [8] reduces this problem by discarding a large fraction of the vocabulary. Alternatively, query-log analysis could be used to guide the filtering of direct indexes, similar to a cache of intersected posting lists [12].

In this paper, we present an indexing structure using data stream sketching techniques to estimate n -word statistics. Our sketch in-dex is derived from a C OUNT M IN sketch [5], and designed to minimize space usage while still pr oducing accurate statistical es-timates. This strategy also ensures that the space required by the index is independent of the number of indexed n -word terms, while still supporting efficient query processing.

Conceptually, our summary sketch is an ( ,  X  ) -approximation of a full inverted index structure. The index representation is capable of estimating statistics for specific n -word with bounded perfor-mance. We show that the relative error of the term dependency statistics being estimated is within the theoretic bounds of similar data streaming applications, and describe how the bounds minimize the space requirement in practice. We show that the retrieval effi-ciency of the sketch index is comparable to full indexes, and many times faster than positional and next-word indexes.

This paper is structured as follows: Section 2 presents the nec-essary background on data stream sketching techniques; Section 3 presents the algorithmic framework for our term dependency statis-tics estimator, and outlines the p robabilistic error bounds ensured by the representation; Section 4 evaluates the performance of our new estimator empirically. We conclude in Section 5.
Algorithms for approximating frequency moments have advanced dramatically in the last twenty years [3, 4]. This line of research is based on the streaming model of computation, and has widespread applications in networking, databases, and data mining [11]. Much of the work in the networking community using these tools has focused on identifying  X  X eavy-hitters X , or top-k items (see [2] or [3]). If only the k most frequent items must be accurately esti-mated, counter-based approaches work well in practice. However, counter-based methods are generally not sufficient if estimates for all the items in a stream are desirable, since the number of counters must be significantly fewer than the number of unique items in the stream. For frequency estimation of any item in a stream, various  X  X ketching X  methods are an appropriate alternative.

We focus our discussion on the C OUNT M IN sketch, recently proposed by Cormode and Muthukrishnan [5]. The key idea of a C
OUNT M IN sketch is to create an array of r  X  w counters, with independent hash functions for each row r . The hash functions map each update to set of counters, one in each row r . In streams that do not support deletions, this ensures that the frequency of any item f ( i ) in the sketch is an overestimate. The collisions for i on any row is 1  X  i  X   X ,i = i f ( i ) /w . C OUNT M IN can be used to estimate  X  f i with error at most n with probability at least 1 using O ( 1 log 1  X  ) . The time per update is O ( r ) where r =log and w = O ( 1 ) . Figure 1: Example C OUNT M IN sketch containing frequency data for 3 items: f ( i 1 )=1 , f ( i 2 )=5 and f ( i 3 )=2 .Where i 2 is hashed to each of the highlighted cells. The frequency of i 2 can be estimated as the minimum value of the highlighted cells f ( i 2 )=5 .
An example C OUNT M IN sketch is shown in Figure 1. Updates are performed as follows: when an item, i , is added to the sketch, one counter is incremented in each row of the C OUNT M IN The correct cell is determined by the corresponding hash function. Formally: If the stream contains only positive frequencies, the frequency i can be estimated by returning the minimum count in the set. For streams allowing positive or negative frequencies, the frequency of i is estimated by returning the median of the r counts. Let C be a text collection partitioned into d documents { D ,..., D d } containing at most  X  n terms. Here, a term t can be n -words, a sequence of n adjacent  X  X ords X . So,  X  1 represents the total number of 1 -words in a collection. An inverted index , the number of times each term t appears in each document D 1  X  t  X   X  n and 1  X  j  X  d . Conceptually, this can be represented as a  X  n  X  d matrix, M , as every term t appears in all d documents in the worst case.

The following statistical notation will apply for our discussion:
In practice, M is sparse, and I is often represented as a com-pressed adjacency list. An inverted index is a mapping of keys to a list of document counters. For each document identifier d , f is maintained. Traditionally, each term in the vocabulary is stored explicitly in a lookup table.

Now, consider the case of constructing an inverted index of n -words. The collection C can contain at most ( |C|  X  n distinct n -words. This number is often less than the  X  n 1 possibilities, but still much larger than  X  1 , thus increasing the number of rows in In this paper, we investigate how to apply the ideas presented by Cormode and Muthukrishnan [5] to fix the number of rows in and still provide accurate statisti cal information. Interestingly, d is already static for a given collection, and d |C| . But, the num-ber of  X  n rows increases with n , and we would like to minimize this overhead. Note that the total number of rows required in the sketch is proportional to r  X  f t . So, if we reduce M to a linear pro-jection of f t , we can use C OUNT M IN to accurately approximate  X  f . Recall that the number collisions for t on any row in the sketch is 1  X  t  X   X ,t = t f t /w . Using a Markov inequality argument, Cor-mode and Muthukrishnan [ 5] show that by setting w =2 / and r =log1 / X  in the sketch, the estimate  X  f t is at most F 1 with probability at least 1  X   X  ,where F 1 is the first frequency moment 1  X  t  X   X  n f t , the sum of all of the frequencies.

In a sketch representation of an inverted index, each distinct term is replaced with a hash value where each hash value may represent more that one term. This reduction means that the vocabulary of n -words no longer needs to be stored with the index. If a sim-ple hashing representation were used, then there is no mechanism available to resolve collisions unless each term string is accessible to the table. However, using the collision mitigation strategy of a sketch, such as the method described for C OUNT M IN ,weare able to reduce the probability that hashing collisions will result in incorrect results.

Our new indexing structure is composed of an r  X  w matrix of pointers to r  X  w postings lists. Conceptually, this matrix is equiva-lent to a C OUNT M IN sketch designed to estimate  X  f t with one twist: we do not simply use a single counter to aggregate  X  f t ,butrather Figure 2: Example extraction of the statistics for a single term de-pendency in our sketch representation. The first two sketch posting lists are processed to produce the intersected posting list for the term abc . Colors are used for each output document posting. Table 1: Global statistics for TREC Collections Robust-04 and ClueWeb-B. allow multiple document counters a ttached in list-wise fashion to each cell in the C OUNT M IN sketch. These document counters are then used to aggregate  X  f d,t .

This approach allows us to fix the size of the lookup table inde-pendent of the vocabulary of n -words being indexed. We do not at-tempt to fix the number of  X  f d,t counters. As in a standard inverted index, every term could appear in every document, producing a maximum of d  X |  X  n | counters in the worst case. But, in practice, the distribution is skewed, and many terms have very few non-zero  X  f d,t counters. Note that since the width of |  X  n | is fixed in our ap-proach, the number of counters is largely independent of the order of n , but rather some percentage of the counters are redistributed in the redundant postings lists.

We now discuss how to estimate point queries using our ap-proach. By using the biased estimation of a C OUNT M IN sketch of only positive counts, our estimates of  X  f t , and subsequently guaranteed to be an overestimate of the true term counts. Further-more, the same formal arguments using the Markov inequality and Chernoff bounds can be made for bounding  X  f t , and subsequently  X  f d,t , we could reasonably expect for each cell. So, to estimate using C OUNT M IN , we would take min j count [ j, h j ( x each counter count [] is actually a pointer to a postings list, con-taining approximately  X  f t counters. When the posting list for any t is requested, the intersection of the r posting lists is performed us-ing the minimum frequency from the w counters representing each sketched posting list. Figure 2 shows an example of the intersection process, which represents the min j for a given t .
We compare the performance of our new term dependency statis-tical estimator over two TREC collections: Robust-04 and Clueweb-B. Statistical properties for each collection is shown in Table 1. As a pre-processing step, all terms in each collection were mapped to 32 -bit integers, where an integer corresponds to a unique word in the collection. This is a standard technique to improve index and vocabulary compression, and ensures uniform space requirements across all of the baselines used in this study.

In each of our experiments, we measure index properties and retrieval performance on n -word data. An n -word is defined as any sequence of n sequential words. In the literature, each distinct sequence is sometimes referred to as a phrase, a shingle, or an n -gram.

We compare the performance of our statistical n -word estima-tor with five other index structures capable of generating the true statistics of n -word term dependencies. We compare our approach with positional indexes [15], full indexes of n -words, filtered in-dexes [8], query-log-based indexes [12] and next-word indexes [14]. The query-log-based indexes are created by ordering the query log by timestamps, then indexing all n -words extracted from the first 90% of queries. The remaining 10% are used to test the retrieval efficiency.

To ensure a fair comparison, all index structures are implemented as disk based b-tree indexes that implement a variety of modern index compression techniques, including d -gap and vbyte integer compression for posting list data, and prefix-based vocabulary com-pression for b-tree blocks. We use 32 kB b-tree blocks. Witten et al. [15] provides a good description of standard compression techniques amenable to text indexing and retrieval. Hash functions used in the sketch indexes are pair-wise universal hash functions.
Our experiments focus on three key aspects of our new statistical term dependency estimator: relative statistical error, space usage, and retrieval efficiency. The sketch index can only estimate var-ious statistical values, and so we assess the relationship between and  X  parameter selection and the resulting relative error. We measure statistical error in the collection for a random sample of term dependencies. Space requirements of our approach are com-pared with each comparable index structure. We compare retrieval efficiency of our estimator with the same set of comparable index structures using n -words extracted from the last 10% of the AOL query log.

Due to space constraits, we omit the details of experiments test-ing the effect of using a sketch index on information retrieval ef-fectiveness. These experiments show that using sketch index with conservative parameters resulted in no change in the retrieval effec-tiveness.

Each index structure we investigate in this section is implemented as an extension to the Galago package, provided by the Lemur Toolkit [6]. All timing experiments were run on a machine with 8-core Intel Xeon processors, with 16 GB of RAM, running the Cen-tos distribution of Linux, Using a distributed, network-attached, 4-node Luster file system to store index data.
As discussed previously, sketch indexes provide an attractive trade-off between space usage and accuracy. In this section, we investigate the relationship between the ( ,  X  ) parameters and the quality of approximation by comparing the relative error of our approach to the true collection statistics. We show results com-puted on indexes created over n -words, extracted from the TREC Robust-04 collection. Average Relative Error (ARE) is defined as the average of the absolute difference between the true value and the estimated value. In our case: ARE = 1 | C | t  X  C | f t
Figure 3 shows ARE values grouped by f t for 3 -words using our approach with a variety of parameters. Other sized n -words were tested, and produce similar results. Data shown in this graph is ag-gregated from 10 instances of sketch indexes with each parameter setting.

The graph shows that conservative settings for ( ,  X  ) can ensure Figure 3: Average relative error of 3 -word frequency statistics ex-tracted from 10 instances of sketch indexes over Robust-04 data, using each set of parameters. Sketch index parameters shown are  X   X  X  0 . 15 , 0 . 05 , 0 . 02 } ,and  X  X  4  X  10  X  5 , 2  X  10 { 1 , 000 / | C | , 5 , 000 / | C | , 10 , 000 / | C |} . Figure 4: Space requirements for sketch and full indexes across a range of n -word sizes over Robust-04 data. The full index shown on this graph is an inverted index storing frequency data for all n -words. The filtered index stores frequency data for n -words that occur at least 32 times. The parameters used for the sketch indexes are =4  X  10  X  5  X  10 , 000 / | C | and  X   X  X  0 . 05 , 0 . 15 a very low error rate in the estimation of collection statistics. Ad-ditionally, we can see that using overly restrictive values of and  X  can degrade our estimates, particularly for infrequent items. This insight is not surprising, since summary sketching is primarily used in networking scenarios that require only the top-k items in a set to be accurately estimated.

Our experiments suggest that  X  4  X  10  X  6 and  X   X  0 . 05 produce a very low relative error for all n -words tested in the Robust-04 collection. All other experiments are based on these parameters.
We now compare space requirements for our estimator with the space requirements of the baseline index structures. Each index structure presented is capable of producing statistics for n -words present in the collection. Figure 4 shows space requirements for the sketch index, as compared to the baselines using the Robust04 Collection. As expected, the sketch estimator requires only a frac-Figure 5: Query processing times for indexes over the Clueweb-B collection. Each data point is the average of 5 runs of 10 , 000 phrase queries. Figure 6: Query processing time versus space usage. Query pro-cessing time as a function of index space requirements. Each data point is the average of 5 runs of 10 , 000 of n -word queries. The size of the query for each data point is indicated on the graph. Colored lines are used to link data points for each index structure. tion of the space of a full index. For n =5 , a sketch index requires less than 20% of the space required by a full five-word index. We can also see from the graph that sketching representations for any n require only slightly more space than other baseline indexes: po-sitional, next-word, and filtered index structures. Query log cache structures are omitted from this graph as we do not have an appro-priate query log.

An important prediction for the sketch index is supported by this data; the space requirement for sketch indexes do not increase sig-nificantly as n increases. This is because the number of rows in our estimator is not bound by n or by the size of the vocabulary, but rather by . This means that a sketch representation of much larger term dependencies are feasible on commodity computing hardware.
We now evaluate the retrieval efficiency of our statistical estima-tor relative to the other index structures for n -word queries. We use the ClueWeb-B collection and queries sampled from the AOL query log. By using a web collection to target the queries, no query translation [13] methods are required.

Recall that we use the first 90% of the time-ordered query log to create the query log cache index structure. Test queries are sampled from the remaining 10% of the log. From this subset of the query log, we uniformly at random sample 10 , 000 n -word queries for each size 1  X  n  X  5 .

The query processing speed for each index structure over a fixed query length is measured as the average of 5 timed runs of the cor-responding sample of queries. In each run the query order is ran-domized. The retrieval system is initialized for each experiment by running a randomly selected sub-sample of 2 , 000 queries. Initial-ization ensures that a portion of the index data is held in memory-based file buffers, as it would be in a live retrieval system. Figure 5 shows query processing time as the length of the query increases for each index structure. Note that times shown in this graph are displayed in log scale. All data points in the graph are significantly different from each of the other index structures, ( p&lt; 0 . 05 using the unpaired t-test).

We observe that query processing time of the sketch index data structure is significantly faster that each of the positional, filtered, query-log and next-word indexes. Unlike position based indexes, we can see that the sketch index is scalable as n increases, the time to process n -word queries does not increase with n .Thisdata shows that sketch indexes are over 300 times faster than a positional index, and 15 times faster than next-word indexes, for processing 5 -word indexes.

Figure 6 shows the trade-off between query processing speed and space usage. Data shown represents the total space requirements to process 1 -to-5 -word queries for each index structure, and the av-erage time to process all of the sampled queries used in the timed experiments above. Query processing times are shown in log scale. Data structures that provide the best trade-off between space re-quirement and retrieval efficiency will approach the origin. This graph clearly demonstrates that the our n -word statistical estimator offers an attractive trade-off between space usage and query effi-ciency when compared with all other baselines.
In this paper, we have investigated the problem of accurately es-timating n -word statistics in large data collections. Existing so-lutions for this problem require large space, or are inefficient for query processing in practice. We have presented a novel approach to estimating n -word statistics for information retrieval tasks. By using frequency sketching techniques developed for data stream-ing applications, we can accurately estimate collection statistics, and provide an attractive trade-off between space and relative er-ror. Furthermore, we show how to bound the space usage of the data structure. Importantly, the number of distinct n -words does not directly influence the space bounds, allowing us to accurately estimate a wide variety statistics efficiently.

We have demonstrated in this paper that the sketch index data structure provides a new and useful trade-off between query pro-cessing time and space requirements for n -word queries. Impor-tantly, we also have shown that this index structure is both scalable in both query processing time and space requirements for the size of the query, n .
This work was supported in part by the Center for Intelligent In-formation Retrieval, in part by NSF CLUE IIS-0844226, in part by NSF grant #CNS-0934322 and in part by the Australian Research Council. Any opinions, findings and conclusions or recommenda-tions expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
 [1] M . Bendersky and W. B. Croft. Discovering key concepts in [2] R. Berinde, G. Cormode, P. Indyk, and M. J. Strauss. [3] G. Cormode and M. Hadjieleftheriou. Finding frequent items [4] G. Cormode and M. Hadjieleftheriou. Methods for finding [5] G. Cormode and S. Muthukrishnan. An improved data [6] W.B. Croft, J. Callan, and [7] B. He, Huang J. X., and Zhou X. Modeling term proximity [8] S. Huston, A. Moffat, and W. B. Croft. Efficient indexing of [9] W. Lu, S. Robertson, and A. MacFarlane. Field-weighted [10] D. Metzler and W.B. Croft. A markov random field model [11] S. Muthukrishnan. Data streams: Algorithms and [12] R. Ozcan, I.S. Altingovde, B.B . Cambazoglu, F.P. Junqueira, [13] W. Webber and A. Moffat. In search of reliable retrieval [14] H. E. Williams, J. Zobel, and P. Anderson. What X  X  next? [15] I.H. Witten, A. Moffat, and T.C. Bell. Managing gigabytes: [16] X. Xue and W.B. Croft. Representing queries as
