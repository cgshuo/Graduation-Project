 The speech understanding component in a spoken dialogue system consists of an automatic speech recognition (ASR) component and a language un-derstanding (LU) component. To develop a speech understanding component, we need to prepare an ASR language model (LM) and a language under-standing model (LUM) for the dialogue domain of the system. There are many types of LMs such as finite-state grammars and N-grams, and many types of LUMs such as finite-state transduc-ers (FST), weighted finite-state transducers (WFST), and keyphrase-extractors (extractor). Selecting a suitable combination of LM and LUM is necessary for robust speech understanding against various user utterances.

Conventional studies of speech understanding have investigated which LM and LUM give the best performance by using fixed training and test data such as the Air Travel Information System (ATIS) corpus. However, in real system development, re-sources such as training data for statistical models and efforts to write finite-state grammars vary ac-cording to the available human resources or budgets. Domain-dependent training data are particularly dif-ficult to obtain. Therefore, in conventional system development, system developers determine the types of LM and LUM by trial and error. Every LM and LUM has some advantages and disadvantages, so it is difficult for a single combination of LM and LUM to gain high accuracy except in a situation involv-ing a lot of training data and effort. Therefore, using multiple speech understanding methods is a more ef-fective approach.

In this paper, we propose a speech understand-ing framework called  X  X ultiple Language models and Multiple Understanding models (MLMU) X , in which multiple LMs and LUMs are used, to achieve better performance under the various development situations. It selects the best speech understanding result from the multiple results generated by arbi-trary combinations of LMs and LUMs.

So far there have been several attempts to im-prove ASR and speech understanding using mul-tiple speech recognizers and speech understanding modules. ROVER (Fiscus, 1997) tried to improve ASR accuracy by integrating the outputs of multi-ple ASRs with different acoustic and language mod-els. The work is different from our study in the fol-lowing two points: it does not deal with speech un-derstanding, and it assumes that each ASR is well-developed and achieves high accuracy for a variety of speech inputs. Eckert et al. (1996) used multiple LMs to deal with both in-grammar utterances and out-of-grammar utterances, but did not mention lan-guage understanding. Hahn et al. (2008) used mul-tiple LUMs, but just a single language model. MLMU is a framework by which system developers can use multiple speech understanding methods by preparing multiple LMs and multiple LUMs. Fig-ure 1 illustrates the flow of speech understanding in MLMU. System developers list available LMs and LUMs for each system X  X  domain, and the system understands utterances by using these models. The framework selects one understanding result from multiple results or calculates a confidence score of the result by using the generated multiple under-standing results.

MLMU can improve speech understanding for the following reason. The performance of each speech understanding (a combination of LM and LUM) might not be very high when either training data for the statistical model or available expertise and ef-fort for writing grammar are insufficient. In such cases, some utterances might not be covered by the system X  X  finite-state grammar LM, and probability estimation in the statistical models may not be very good. Using multiple speech understanding mod-els is expected to solve this problem because each model has different specialities. For example, finite-state grammar LMs and FST-based LUMs achieve high accuracy in recognizing and understanding in-grammar utterances, whereas out-of-grammar utter-ances are covered by N-gram models and LUMs based on WFST and keyphrase-extractors. There-fore it is more possible that the understanding results of MLMU will include the correct result than a case when a single understanding model is used.

The understanding results of MLMU will be help-ful in many ways. We used them to achieve better understanding accuracy by selecting the most reli-able one. This selection is based on features con-cerning ASR results and language understanding re-sults. It is also possible to delay the selection, hold-ing multiple understanding result candidates that will be disambiguated as the dialogue proceeds (Bo-hus, 2004). Furthermore, confidence scores, which enable an efficient dialogue management (Komatani and Kawahara, 2000), can be calculated by ranking these results or by voting on them, by using multi-ple speech understanding results. The understanding results can be used in the discourse understanding module and the dialogue management module. They can choose one of the understanding results depend-ing on the dialogue situation. 3.1 Available Language Models and Language We implemented MLMU as a library of RIME-TK, which is a toolkit for building multi-domain spoken dialogue systems (Nakano et al., 2008). With the current implementation, developers can use the following LMs: 1. A LM based on finite-state grammar (FSG) 2. A domain-dependent statistical N-gram model and the following LUMs: 1. Finite-state transducer (FST) 2. Weighted FST (WFST) 3. Keyphrase-extractor (extractor).
 System developers can use multiple finite-state-grammar-based LMs or N-gram-based LMs, and also multiple FSTs and WFSTs. They can specify the combination for each domain by preparing LMs and LUMs. They can specify grammar models when sufficient human labor is available for writing gram-mar, and specify statistical models when a corpus for training models is available. 3.2 Selecting Understanding Result based on We also implemented a mechanism for selecting one of the understanding results as the best hypothesis. The mechanism chooses the result with the highest estimated probability of correctness. Probabilities are estimated for each understanding result by using logistic regression, which uses several ASR and LU features.

We define P i as the probability that speech under-standing result i is correct, and we select one result based on argmax derstanding result as i ( i = 1,...,6). We constructed a logistic regression model for P i . The regression function can be written as: The coefficients a i 1 ,...,a im ,b i were fitted us-ing training data. The independent variables F i 1 ,F i 2 ,...,F im are listed in Table 1. In the table, n indicates the number of understanding results, that is, n = 6 in this paper X  X  experiment. Here, we denote the features as F i 1 ,F i 2 ,...,F im .

Features from F i 1 to F i 3 represent characteristics of ASR results. The acoustic scores were normal-ized by utterance durations in seconds. These fea-tures are used for verifying its ASR result. Features from F i 4 to F i 9 represent characteristics of LU re-sults. Features from F i 4 to F i 6 are defined on the basis of the concept-based confidence scores (Ko-matani and Kawahara, 2000). We conducted a preliminary experiment to show the potential of the framework by using the two LMs and three LUMs noted in Section 3.1.
 4.1 Preparing LMs and LUMs The finite-state grammar rules were written in sen-tence units manually. A domain-dependent statisti-cal N-gram model was trained on 10,000 sentences randomly generated from the grammar. The vocab-ulary sizes of the grammar LM and the domain-dependent statistical LM were both 278. We also used a domain-independent statistical N-gram model for obtaining acoustic scores for utterance verification, which was trained on Web texts (Kawa-hara et al., 2004). Its vocabulary size was 60,250. The grammar used in the FST was the same as the FSG used as one of the LMs, which was manually written by a system developer. The WFST-based LU was based on a method to estimate WFST parame-ters with a small amount of data (Fukubayashi et al., 2008). Its parameters were estimated by using 105 utterances of just one user. The keyphrase extrac-tor extracts as many concepts as possible from an ASR result on the basis of a grammar while ignor-ing words that do not match the grammar. 4.2 Target Data for Evaluation We used 3,055 utterances in the rent-a-car reserva-tion domain (Nakano et al., 2007). We used Julius (ver. 4.0.2) as the speech recognizer and a 3000-state phonetic tied-mixture (PTM) triphone model as the acoustic model 1 . ASR accuracy in mora ac-curacy when using the FSG and the N-gram model were 71.9% and 75.5% respectively. We used con-cept error rates (CERs) to represent the speech un-derstanding accuracy, which is calculated as fol-lows: We manually annotated whether an understanding result of each utterance was correct or not, and used them as training data to fit the coefficients a 4.3 Evaluation in Concept Error Rates We fitted the coefficients of regression functions and selected understanding results with a 10-fold cross validation. Table 2 lists the CERs based on combi-nations of single LM and LUM and by our method. Of all combinations of single LM and LUM, the best accuracy was obtained with (5) (N-gram + WFST). Our method improved by 2.6 points over (5). Al-though we achieved a lower CER, we used a lot of data to estimate logistic regression coefficients. Such a large amount of data may not be available in a real situation. We will conduct more experiments by changing the amount of training data. Table 2 also shows the accuracy of the oracle selection, which selected the best speech understanding result man-ually. The CER of the oracle selection was 13.5%, a significant improvement compared to all combina-tions of a LM and LUM. There is no combination of a LM and LUM whose understanding results were not selected at all in the oracle selection and our method X  X  selection. These results show that using multiple LMs and multiple LUMs can potentially improve speech understanding accuracy. We will conduct more experiments in other domains or with other resources to evaluate the effectiveness of our framework. We plan to investigate the case in which a smaller amount of the training data is used to estimate the coefficients of the logistic re-gressions. Furthermore, finding a way to calculate confidence scores of speech understanding results is on our agenda.

