 Grammatical relations (GRs) represent functional relationships between language units in a sen-tence. They are exemplified in traditional gram-mars by the notions of subject, direct/indirect object, etc. GRs have assumed an important role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several com-putational grammar formalisms, such as Lexi-cal Function Grammar ( LFG ; Bresnan and Ka-plan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the depen-dency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Cur-ran, 2007a; Miyao et al., 2007).
In this paper, we address the question of an-alyzing Chinese sentences with deep GRs. To acquire high-quality GR corpus, we propose a linguistically-motivated algorithm to translate a Government and Binding ( GB ; Chomsky, 1981; Carnie, 2007) grounded phrase structure treebank, i.e. Chinese Treebank (CTB; Xue et al., 2005) to a deep dependency bank where GRs are ex-plicitly represented. Different from popular shal-low dependency parsing that focus on tree-shaped structures, our GR annotations are represented as general directed graphs that express not only lo-cal but also various long-distance dependencies, such as coordinations, control/raising construc-tions, topicalization, relative clauses and many other complicated linguistic phenomena that goes beyond shallow syntax (see Fig. 1 for example.). Manual evaluation highlights the reliability of our linguistically-motivated GR extraction algorithm: The overall dependency-based precision and recall are 99.17 and 98.87. The automatically-converted corpus would be of use for a wide variety of NLP tasks.

Recent years have seen the introduction of a number of treebank-guided statistical parsers ca-pable of generating considerably accurate parses for Chinese. With the high-quality GR resource at hand, we study data-driven GR parsing. Previ-ous work on dependency parsing mainly focused on structures that can be represented in terms of directed trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individ-ually studied two transition systems that can gen-erate more general graphs rather than trees. In-spired by their work, we study transition-based models for building deep dependency structures. The existence of a large number of crossing arcs in GR graphs makes left-to-right, incremental graph spanning computationally hard. Applied to our data, the two existing systems cover only 51.0% and 76.5% GR graphs respectively. To better suit adjuncts of the coordinated verbs, namely  X   X   X  /issue X  and  X   X  tively linked the two heads. our problem, we extend Titov et al. X  X  work and study what we call K -permutation transition sys-tem. The key idea is to introduce a new type of transition that reorders top k ( 2  X  k  X  K ) el-ements in the memory module of a stack-based transition system. With the increase of K , the ex-pressiveness of the corresponding system strictly increases. We propose an oracle deriving method which is guaranteed to find a sound transition se-quence if one exits. Moreover, we introduce an effective approximation of that oracle, which de-creases decoding ambiguity but practically covers almost exactly the same graphs for our data.
Based on the stronger transition system, we build a GR parser with a discriminative model for disambiguation and a beam decoder for inference. We conduct experiments on CTB 6.0 to profile this parser. With the increase of the K , the parser is able to utilize more GR graphs for training and the numeric performance is improved. Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models. Detailed analysis reveal some important factors that may possibly boost the performance. To our knowl-edge, this work provides the first result of exten-sive experiments of parsing Chinese with GRs.
We release our GR processing kit and gold-standard annotations for research purposes. These resources can be downloaded at http://www. icst.pku.edu.cn/lcwm/omg . In this section, we discuss the construction of the GR annotations. Basically, the annotations are au-tomatically converted from a GB -grounded phrase-structure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep gram-mar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB , which is the linguistic ba-sis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, ex-traction, topicalization, and many other compli-cated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. 2.1 Linguistic Basis GRs are encoded in different ways in different lan-guages. In some languages, e.g. Turkish, gram-matical function is encoded by means of morpho-logical marking, while in highly configurational languages, e.g. Chinese, the grammatical function of a phrase is heavily determined by its constituent structure position. Dominant Chomskyan theo-ries, including GB , have defined GRs as configu-rations at phrase structures. Following this princi-ple, CTB groups words into constituents through the use of a limited set of fundamental grammat-ical functions. Transformational grammar utilizes empty categories (ECs) to represent long-distance dependencies. In CTB, traces are provided by relating displaced linguistic material to where it should be interpreted semantically. By exploiting configurational information, traces and functional tag annotations, GR information can be hopefully example. derived from CTB trees with high accuracy. 2.2 The Extraction Algorithm Our treebank conversion algorithm borrows key insights from Lexical Functional Grammar ( LFG ; Bresnan and Kaplan, 1982; Dalrymple, 2001). LFG posits two levels of representation: c(onstituent)-structure and f(unctional)-structure minimally. C-structure is represented by phrase-structure trees, and captures surface syntactic con-figurations such as word order, while f-structure encodes grammatical functions. It is easy to ex-tract a dependency backbone which approximates basic predicate-argument-adjunct structures from f-structures. The construction of the widely used PARC DepBank (King et al., 2003) is a good ex-ample.

LFG relates c-structure and f-structure through f-structure annotations, which compositionally map every constituent to a corresponding f-structure. Borrowing this key idea, we translate CTB trees to dependency graphs by first augment-ing each constituency with f-structure annotations, then propagating the head words of the head or conjunct daughter(s) upwards to their parents, and finally creating a dependency graph. The follow-ing presents details step-by-step.
 Tapping implicit information. Xue (2007) in-troduced a systematic study to tap the implicit functional information of CTB. This gives us a very good start to extract GRs. We slightly modify their method to enrich a CTB tree with f-structure annotations: Each node in a resulting tree is anno-tated with one and only one corresponding equa-tion. See Fig. 2 for example. Comparing the orig-inal annotation and enriched one, we can see that the functionality of this step is to explicitly repre-sent and regulate grammatical functions.
 Beyond CTB annotations: tracing more. Nat-ural languages do not always interpret linguistic material locally. In order to obtain accurate and complete GR, predicate-argument, or logical form representations, a hallmark of deep grammars is that they usually involve a non-local dependency resolution mechanism. CTB trees utilize ECs and coindexed materials to represent long-distance de-pendencies. An EC is a nominal element that does not have any phonological content and is therefore unpronounced. Two kinds of anaphoric ECs, i.e. big PRO and trace, are annotated in CTB. Theo-retically speaking, only trace is generated as the result of movement and therefore annotated with antecedents in CTB. We carefully check the anno-tation and find that a considerable amount of an-tecedents are not labeled, and hence a lot of impor-Figure 3: An example of lexicalized tree after head word upward passing. Only partial result is shown. The long-distance dependency between  X   X   X  /involve X  and  X   X   X  /document X  is created through copying the dependent to a coindexed anaphoric EC position. tant non-local information is missing. In addition, since the big PRO is also anaphoric, it is possible to find coindexed components sometimes. Such non-local information is also very valuable.
Beyond CTB annotations, we introduce a num-ber of phrase-structure patterns to extract more non-local dependencies. The method heavily leverages linguistic rules to exploit structural in-formation. We take into account both theoreti-cal assumptions and analyzing practices to enrich coindexation information according to phrase-structure patterns. In particular, we try to link an anaphoric EC e with its c-commonders if no non-empty antecedent has already been coindexed with e . Because the CTB is influenced deeply by the X-bar syntax, which regulates constituent anal-ysis much, the number of our linguistic rules is quite modest. For the development of conversion rules, we used the first 9 files of CTB, which con-tains about 100 sentences. Readers can refer to the well-documented Perl script for details. See Fig. 2 for example. The noun phrase  X   X   X   X   X   X  /regulatory documents X  is related to the trace  X *T*. X  This coindexation is not labeled by the original annotation.
 Passing head words and linking ECs. Based on an enriched tree, our algorithm propagates the head word of the head daughter upwards to their parents, linking coindexed units, and finally creat-ing a GR graph. The partial result after head word passing of the running example is shown in Fig. 3. There are two differences of the head word passing between our GR extraction and a  X  X ormal X  depen-dency tree extraction. First, the GR extraction pro-cedure may pass multiple head words to its parent, especially in a coordination construction. Second,
Table 1: Manual evaluation of 209 sentences. long-distance dependencies are created by linking ECs and their coindexed phrases. 2.3 Manual Evaluation To have a precise understanding of whether our ex-traction algorithm works well, we have selected 20 files that contains 209 sentences in total for man-ual evaluation. Linguistic experts carefully exam-ine the corresponding GR graphs derived by our extraction algorithm and correct all errors. In other words, a gold standard GR annotation set is cre-ated. The measure for comparing two dependency graphs is precision/recall of GR tokens which are defined as  X  w h ,w d ,l  X  tuples, where w h is the head, w d is the dependent and l is the relation. Labeled precision/recall (LP/LR) is the ratio of tuples cor-rectly identified by the automatic generator, while unlabeled precision/recall (UP/UR) is the ratio re-gardless of l . F-score is a harmonic mean of pre-cision and recall. These measures correspond to attachment scores (LAS/UAS) in dependency tree parsing. To evaluate our GR parsing models that will be introduced later, we also report these met-rics.

The overall performance is summarized in Tab. 1. We can see that the automatical GR extraction achieves relatively high performance. There are two sources of errors in treebank conversion: (1) inadequate conversion rules and (2) wrong or in-consistent original annotations. During the cre-ation of the gold standard corpus, we find that the former is mainly caused by complicated un-bounded dependencies and the lack of internal structure for some kinds of phrases. Such prob-lems are very hard to solve through rules only, if not possible, since original annotations do not pro-vide sufficient information. The latter problem is more scattered and unpredictable. 2.4 Statistics Allowing non-projective dependencies generally makes parsing either by graph-based or transition-based dependency parsing harder. Substantial re-search effort has been devoted in recent years to the design of elegant solutions for this problem. There are much more crossing arcs in the GR graphs than syntactic dependency trees. In the training data (defined in Section 4.1), there are 558132 arcs and 86534 crossing pairs, About half of the sentences have crossing arcs (10930 out of 22277). The wide existence of crossing arcs poses an essential challenge for GR parsing, namely, to find methods for handling crossing arcs without a significant loss in accuracy and efficiency. The availability of large-scale treebanks has con-tributed to the blossoming of statistical approaches to build accurate shallow constituency and depen-dency parsers. With high-quality GR resources at hand, it is possible to study statistical approaches to automatically parse GR graphs. In this section, we investigate the feasibility of applying a data-driven, grammar-free approach to build GRs di-rectly. In particular, transition-based dependency parsing method is studied. 3.1 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Tor-res Martins et al., 2009) models have attracted the most attention of dependency parsing in re-cent years. Transition-based parsers utilize tran-sition systems to derive dependency trees together with treebank-induced statistical models for pre-dicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on sur-face syntactic structures, and the majority of ex-isting approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsu-jii (2008) and Titov et al. (2009) individually in-troduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 Transition Systems Following (Nivre, 2008), we define a transition system for dependency parsing as a quadruple S = ( C ,T,c s , C t ) , where 1. C is a set of configurations, each of which 2. T is a set of transitions, each of which is a 3. c s is an initialization function, mapping a 4. C t  X  X  is a set of terminal configurations. Given a sentence x = w 1 ,...,w n and a graph G = ( V,A ) on it, if there is a sequence of tran-sitions t 1 ,...,t m and a sequence of configura-tions c 0 ,...,c m such that c 0 = c s ( x ) , t i ( c i  X  1 c ( i = 1 ,...,m ) , c m  X  X  t , and A c the sequence of transitions is an oracle sequence. And we define  X  A c built in c i . In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of S HIFT /R EDUCE actions are performed se-quentially to consume words from the queue and update the partial parsing results. 3.3 Online Reordering Among existing systems, Sagae and Tsujii X  X  is de-signed for projective graphs (denoted by G 1 in Definition 1), and Titov et al. X  X  handles only a specific subset of non-projective graphs as well as projective graphs ( G 2 ). Applied to our data, only 51.0% and 76.5% of the extracted graphs are parsable with their systems. Obviously, it is nec-essary to investigate new transition systems for the parsing task in our study. To deal with crossing arcs, Titov et al. (2009) and Nivre (2009) designed a S WAP transition that switches the position of the two topmost nodes on the stack. Inspired by their work, we extend this approach to parse more gen-eral graphs. The basic idea is to provide our new system with an ability to reorder more nodes dur-ing decoding in an online fashion, which we refer to as online reordering. 3.4 K -Permutation System We define a K -permutation transition system S
K = ( C ,T,c s , C t ) , where a configuration c = (  X , X ,A )  X  C contains a stack  X  of nodes be-sides  X  and A . We set the initial configuration for a sentence x = w 1 ,...,w n to be c s ( x ) = ([] , [1 ,...,n ] , {} ) , and take C t to be the set of all configurations of the form c t = (  X , [] ,A ) (for any arc set A ). The set of transitions T contains five types of actions, as shown in Tab. 2: 1. S HIFT removes the front element from  X  and 2. L EFT -A RC l /R IGHT -A RC l updates a configu-3. P OP deletes the top element of  X  . 4. R OTATE k updates a configuration with stack We refer to this system as K -permutation because by rotating the top k (2  X  k  X  K ) nodes in the stack, we can obtain all the permutations of the top K nodes. Note that S 2 is identical to Titov et al. X  X ; S  X  is complete with respect to the class of all directed graphs without self-loop, since we can arbitrarily permute the nodes in the stack. The K -permutation system exhibits a nice property: The sets of corresponding graphs are strictly mono-tonic with respect to the  X  operation.
 Definition 1. If a graph G can be parsed with tran-sition system S K , we say G is a K -perm graph . We use G K to denote the set of all k -perm graphs. Specially, G 0 =  X  , G 1 is the set of all projective graphs, and G  X  = Theorem 1. G i ( G i +1 ,  X  i  X  0 .
 Proof. It is obvious that G i  X  G i +1 and G 0 ( G 1 . Fig. 4 gives an example which is in G i +1 but not in G i for all i &gt; 0 , indicating G i 6 = G i +1 . Theorem 2. G  X  is the set of all graphs without self-loop.
 Proof. It follows immediately from the fact that G  X  X  | V | ,  X  G =  X  V,E  X  .
 The transition systems introduced in (Sagae and Tsujii, 2008) and (Titov et al., 2009) can be viewed as S 1 1 and S 2 . Figure 4: A graph which is in G i +1 , but not in G i . 3.5 Normal Form Oracle The K -permutation transition system may allow multiple oracle transition sequences on one graph, but trying to sum all the possible oracles is usu-ally computational expensive. Here we give a con-struction procedure which is guaranteed to find an oracle sequence if one exits. We refer it as normal form oracle (NFO).

Let L ( j ) be the ordered list of nodes connected to j in  X  A c we set t i to S HIFT ; if there is no arc linked to j a  X   X  A c A
RC or R IGHT -A RC correspondingly. When there are only S HIFT and R OTATE left, we first apply a sequence of R OTATE  X  X  to make L K (  X  ) com-plete ordered by lexicographical order, then apply a S HIFT . Let c i = t i ( c i  X  1 ) , we continue to com-pute t i +1 , until  X  c Theorem 3. If a graph is parsable with the transi-tion system S K then the construction procedure is guaranteed to find an oracle transition sequence. Proof. During the construction, all the arcs are built by L EFT -A RC or R IGHT -A RC , which links the top of the stack and the front of the buffer. Therefore, we prefer L (  X  ) to be as orderly as pos-sible, to make the words to be linked sooner on the top of the stack. the construction procedure above does best within the power of the system S K . 3.6 An Approximation for NFO In the construction of NFO transitions, we ex-haustively use the R OTATE  X  X  to make L (  X  ) com-plete ordered. We also observed that the tran-sition L EFT -A RC , R IGHT -A RC and S HIFT only change the relative order between the first element of L (  X  ) and the rest elements. Therefore we ex-plored an approximate procedure to determine the R
OTATE  X  X , based on the observation. We call it ap-proximate NFO (ANFO). Using notation defined in Section 3.5, the approximate procedure goes as follows. When it comes to the determination of Figure 5: A graph that can be parsed with S 3 with a transition sequence SSSSR 3 SR 3 APAP R 2 R 3 SR 3 SR 3 APAPAPAPAP, where S stands for S HIFT , R for R OTATE , A for L
EFT -A RC , and P for P OP . But the approximate procedure fails to find the oracle, since R 2 R 3 in bold in the sequence are not to be applied. the R OTATE sequence, let k be the largest m such that 0  X  m  X  min { K,l } and L ( j m ) strictly pre-cedes L ( j 1 ) by the lexicographical order (here we assume L ( j 0 ) strictly precedes any L ( j ) ,j  X   X  ). If k &gt; 0 , we set t i to R OTATE k ; else we set t i to S
HIFT . The approximation assumes L (  X  ) is com-pletely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define  X  G K as the graphs the ora-cle of which can be extracted by S K with the ap-proximation procedure.
 It can be inferred similarly that Theorem 1 and Theorem 2 also hold for  X  G  X  X . However, the  X  G K is not equal to G K in non-trivial cases.
 Theorem 4.  X  G i ( G i ,  X  i  X  3 .
 Proof. It is trivial that  X  G i  X  X  i . An example graph that is in G 3 but not in  X  G 3 is shown in Figure 5, examples for arbitrary i &gt; 3 can be constructed similarly.

The above theorem indicates the inadequacy of the ANFO deriving procedure. Nevertheless, em-pirical evaluation (Section 4.2) shows that the cov-erage of AFO and ANFO deriving procedures are almost identical when applying to linguistic data. 3.7 Statistical Parsing When we parse a sentence w 1 w 2  X  X  X  w n , we start with the initial configuration c 0 = c s ( x ) , and choose next transition t i = C ( c i  X  1 ) iteratively ac-cording to a discriminative classifier trained on or-acle sequences. To build a parser, we use a struc-tured classifier to approximate the oracle, and ap-ply the Passive-Aggressive (PA) algorithm (Cram-mer et al., 2006) for parameter estimation. The PA algorithm is similar to the Perceptron algo-rithm, the difference from which is the update of weight vector. We also use parameter averaging and early update to achieve better training. Devel-oping features has been shown crucial to advanc-ing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary ma-terial for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency for-malism, and very popular to evaluate fundamen-tal NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets ac-cording to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS tag-ing results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to de-pendency tokens is reported. To evaluate the abil-ity to recover non-local dependencies, the recall of such dependencies are reported too. 4.2 Coverage and Accuracy There is a dual effect of the increase of the param-eter k to our transition-based dependency parser. On one hand, the higher k is, the more expres-sivity the corresponding transition system has. A system with higher k covers more structures and allows to use more data for training. On the other hand, higher k brings more ambiguities to the cor-responding parser, and the parsing performance may thus suffer. Note that the ambiguity exists not only in each step for transition decision, but also in selecting the training oracle.

The left-most columns of Tab. 3 shows the cov-erage of K -permutation transition system with re-spect to different K and different oracle deriving algorithms. Readers may be surprised that the coverage of NFO and ANFO deriving procedures is the same. Actually, all the covered graphs by the two oracle deriving procedures are exactly the same, except for S 3 . Only 1 from 22277 sen-tences can find a NFO but not an ANFO. This number demonstrates the effectiveness of ANFO. In the following experiments, we use the ANFO X  X  to train our parser.

Applied to our data, S 2 , i.e. the exact system in-troduced by Titov et al. (2009), only covers 76.5% GR graphs. This is very different from the re-sult obtained on the CoNLL shared task data for English semantic role labeling (SRL). According to (Titov et al., 2009), 99% semantic-role-labelled graphs can be generated by S 2 . We think there are two main reasons accounting for the differences, and highlight the importance of the expressiveness of transition systems to solve deep dependency parsing problems. First, the SRL task only focuses on finding arguments and adjuncts of verbal (and nominal) predicates, while dependencies headed by other words are not contained in its graph rep-resentation. On contrast, a deep dependency struc-ture, like GR graph, approximates deep syntactic or semantic information of a sentence as a whole, and therefore is more dense. As a result, permuta-tion system with a very low k is incapable to han-dle more cases. Another reason is about the Chi-nese language. Some language-specific properties result in complex crossing arcs. For example, se-rial verb constructions are widely used in Chinese to describe several separate events without con-junctions. The verbal heads in such constructions share subjects and adjuncts, both of which are be-fore the heads. The distributive dependencies be-tween verbal heads and subjects/adjuncts usually produce crossing arcs (see Fig. 6). To test our as-sumption, we evaluate the coverage of S 2 over the functor-argument dependency graphs provided by the English and Chinese CCGBank (Hockenmaier and Steedman, 2007; Tse and Curran, 2010). The result is 96.9% vs. 89.0% , which confirms our linguistic intuition under another grammar formal-ism.

Tab. 3 summarizes the performance of the transition-based parser with different configura-tions to reveal how well data-driven parsing can Figure 6: A simplified example to illustrate cross-ing arcs in serial verbal constructions. be performed in realistic situations. We can see that with the increase of K , the overall parsing ac-curacy incrementally goes up. The high complex-ity of Chinese deep dependency structures demon-strates the importance of the expressiveness of a transition system, while the improved numeric ac-curacies practically certify the benefits. The two points merit further exploration to more expressive transition systems for deep dependency parsing, at least for Chinese. The labeled evaluation scores on the final test data are presented in Tab. 4. Test UP UR UF LR L LR NL S 5 83.93 79.82 81.82 80.94 54.38 4.3 Precision vs. Recall A noteworthy thing about the overall performance is that the precision is promising but the recall is too low behind. This difference is consistent with the result obtained by a shift-reduce CCG parser (Zhang and Clark, 2011). The functor-argument dependencies generated by that parser also has a relatively high precision but considerably low re-call. There are two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extrac-tion, systems upon GR parsing, such property mer-its attention. A good trade-off between the preci-sion and the recall may have a great impact on final results. 4.4 Local vs. Non-local Although the micro accuracy of all dependencies are considerably good, the ability of current state-of-the-art statistical parsers to find difficult non-local materials is far from satisfactory, even for English (Rimell et al., 2009; Bender et al., 2011). We report the accuracy in terms of local and non-local dependencies respectively to show the diffi-culty of the recovery of non-local dependencies. The last four columns of Tab. 3 demonstrates the labeled/unlabeled recall of local (UR L /LR L ) and non-local dependencies (UR NL /LR NL ). We can clearly see that non-local dependency recovery is extremely difficult for Chinese parsing. 4.5 Deep vs. Deep CCG and HPSG parsers also favor the dependency-based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). Previous work on Chinese CCG and HPSG parsing unanimously agrees that obtaining the deep analysis of Chinese is more challenging (Yu et al., 2011; Tse and Cur-ran, 2012). The successful C&amp;C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical eval-uation indicates that the state-of-the-art of deep linguistic processing for Chinese lag behind very much. Wide-coverage in-depth and accurate linguistic processing is desirable for many practical NLP ap-plications, such as machine translation (Wu et al., 2010) and information extraction (Miyao et al., 2008). Parsing in deep formalisms, e.g. CCG , HPSG , LFG and TAG , provides valuable, richer linguistic information, and researchers thus draw more and more attention to it. Very recently, study on deep linguistic processing for Chinese has been initialized. Our work is one of them.

To quickly construct deep annotations, corpus-driven grammar engineering has been studied. Phrase structure trees in CTB have been semi-automatically converted to deep derivations in the CCG (Tse and Curran, 2010), LFG (Guo et al., 2007), TAG (Xia, 2001) and HPSG (Yu et al., 2010) formalisms. Our GR extraction work is sim-ilar, but grounded in GB , which is more consistent with the construction of the original annotations.
Based on converted fine-grained linguistic an-notations, successful English deep parsers, such as C&amp;C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntac-tic or semantic parsing for English. In particular, Sagae and Tsujii (2008) X  X  and Titov et al. (2009) X  X  studies on transition-based deep dependency pars-ing motivated our work very much. However, sim-ple adoption of their systems does not resolve Chi-nese GR parsing well because the GR graphs are much more complicated. Our investigation on the K -permutation transition system advances the ca-pacity of existing methods. Recent years witnessed rapid progress made on deep linguistic processing for English, and ini-tial attempts for Chinese. Our work stands in between traditional dependency tree parsing and deep linguistic processing. We introduced a sys-tem for automatically extracting grammatical rela-tions of Chinese sentences from GB phrase struc-ture trees. The present work remedies the re-source gap by facilitating the accurate extraction of GR annotations from GB trees. Manual evalua-tion demonstrate the effectiveness of our method. With the availability of high-quality GR resources, transition-based methods for GR parsing was stud-ied. A new formal system, namely K -permutation system, is well theoretically discussed and prac-tically implemented as the core module of a deep dependency parser. Empirical evaluation and anal-ysis were presented to give better understanding of the Chinese GR parsing problem. Detailed anal-ysis reveals some important directions for future investigation.
 The work was supported by NSFC (61300064, 61170166 and 61331011) and National High-Tech R&amp;D Program (2012AA011101).

