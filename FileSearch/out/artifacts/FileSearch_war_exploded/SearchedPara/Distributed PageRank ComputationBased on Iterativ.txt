 PageRank has been widely used as a major factor in search engine ranking systems. However, global link graph infor-mation is required when computing PageRank, which causes prohibitive communication cost to achieve accurate results in distributed solution. In this paper, we propose a dis-tributed PageRank computation algorithm based on iter-ative aggregation-disaggregation (IAD) method with Block Jacobi smoothing. The basic idea is divide-and-conquer. We treat each web site as a node to explore the block structure of hyperlinks. Local PageRank is computed by each node itself and then updated with a low communication cost with a coordinator. We prove the global convergence of the Block Jacobi method and then analyze the communication over-head and major advantages of our algorithm. Experiments on three real web graphs show that our method converges 5 X 7 times faster than the traditional Power method. We be-lieve our work provides an efficient and practical distributed solution for PageRank on large scale Web graphs. H.3.3 [ Information Search and Retrieval ]: Search pro-cess Algorithms, Performance PageRank, distributed search engines, iterative aggregation-disaggregation, Block Jacobi  X 
This work was conducted when this author was with Ts-inghua University.
 Copyright 2005 ACM 1-59593-140-6/05/0010 ... $ 5.00. The World Wide Web keeps growing. In April 2005, Google 1 announced to have indexed about 8 billion web pages. Only several giants can afford the prohibitive cost for maintaining and updating the index of billions of pages. Moreover, a considerable part of the high-quality Deep Web, which is estimated about 500 times larger than the static Web [20], is exclusive from crawlers.

Motivated by above reasons, distributed and collaborative search engines have been extensively studied. In a typical distributed search system, each node maintains the index of local-stored pages. Usually there are also some nodes serving as coordinators to provide global information for the other nodes.

A major challenge to distributed search engines is how to rank the query results on different nodes. The ranking factors in web search can be divided into two categories. The first is content-based relevance from traditional infor-mation retrieval, which can be easily handled by each node itself. The second is link-based authority rising in recent years. Among the most popular link analysis algorithms are PageRank [26] and HITS [16], both of which have been demonstrated to be successful in many web information re-trieval applications, especially for large scale web search.
However, despite of their simple forms, both PageRank and HITS require the knowledge of the whole link graph for computation, which causes prohibitive communication overhead to achieve accurate results for distributed compu-tation on large graphs. For example, even after compres-sion, a web graph consisting of 118M vertices and 1G edges is 385M Bytes large [1]. Extensive studies have been con-ducted to shape PageRank suitable for distributed compu-tation [7, 31].

In this paper, we propose a distributed PageRank com-putation (DPC) algorithm. The general idea is divide-and-conquer. We treat each web site as a node to make use of the underlying block structure of the Web [12]. Each node computes a PageRank vector for its local pages by links within sites and then updates its local PageRank through low volume communication with a given coordinator.
In a mathematical perspective, we prove that the DPC algorithm is equivalent to the classic iterative aggregation-http://www.google.com disaggregation (IAD) method with Block Jacobi smoothing. We further present the proof for global convergence of the Block Jacobi method and make thorough analysis on the communication overhead and major advantages of our algo-rithm.

We use three real web graphs with several ten million vertices in our experiments. L 1 distance and Kendall X  X   X  -distance are adopted for evaluation. The experimental re-sults show that the DPC algorithm achieves better approx-imation than a recent work in [31]. And it converges 5 X 7 times faster than the traditional Power method. We be-lieve that our work provides an efficient and practical dis-tributed solution for PageRank computation on large scale Web graphs.

The remainder of this paper is organized as follows. Sec-tion 2 briefly reviews the PageRank algorithm. Section 3 proposes our DPC algorithm with theoretical analysis of its convergence properties and communication overhead. The experimental results on three real web graphs with several ten million pages are presented in Section 4. We discuss re-lated work in Section 5 and conclude the whole paper with Section 6.
PageRank has emerged as one of the dominant models for exploring link structures, partly due to its query-independence and immunity to spamming. We briefly re-view PageRank algorithm in this section. A reader familiar with PageRank may skip this section. The basic idea of PageRank assumes that a link from page A to page B indicates that the author of A recommends page B . Thus a page linked by many other pages is important. Furthermore, a page linked by an important page is also important.

To formulate the original intuition with a mathematical model, the Web is viewed as a directed graph G with web pages as vertices, and hyperlinks as edges. Then PageRank is the stationary distribution of a random walk on the graph. In this random walk, a user visits web pages following hy-perlinks or jumps to a random page with certain probability. Such a random walk is essentially a homogeneous first-order Markov chain.

Before we formulate the PageRank algorithm, we intro-duce here some notations used in this paper. k v k 1 denotes the 1-norm of vector v .  X  ( M ) denotes the spectral radius of matrix M . We say M &gt; a if and only if M ij &gt; a,  X  i, j . Other binary relations (e.g.  X  ,  X  , &lt; , =) between a matrix form vector and I is the identity matrix. The size of e and I changes according to the context.

Assume the transition probability matrix of a random walk on directed graph G is P . Let N be the number of states in the Markov chain.  X  denotes the stationary prob-ability vector of P . So  X  satisfies Thus,  X  is the principle eigenvector of P , corresponding to eigenvalue one.
Spectral radius is the largest module of eigenvalues of M .
A homogeneous finite Markov Chains has a unique pos-itive stationary probability distribution if and only if it is irreducible and aperiodic. However, the existence of dan-gling nodes 3 makes the chain reducible. A simple remedy is to modify the model that when random walkers reach a dangling node, they pick a random page for the next state. Suppose that random walkers follow links with a probability d and jump to a random page with a probability 1  X  d . d is called the damping factor, which is 0.85 in this paper. Then the transition matrix satisfies where C j denotes the out-degree of page j . j  X  i denotes there is a link from j to i, and j 9 i denotes there is no link from j to i. Finally we have P  X  (1  X  d ) /N , and the existence of  X  is guaranteed.

A fairly straightforward way to obtain  X  is through Power methods [8], which employs iterative multiplication as fol-lows:
The eigenvector problem in (2.1) can also be formulated as a linear system:
There are many alternative solutions for the linear sys-tem, such as Jacobi method, Gauss-Seidel method, Succes-sive Overrelaxation method (SOR), Symmetric Successive Overrelaxation method (SSOR). There is a unified formu-lation for these algorithms. Split the coefficient matrix as I  X  P = M  X  N , where M is nonsingular and the splitting is weak regular 4 . Let T = M  X  1 N be the iteration matrix. The general PageRank algorithm can be written as: Algorithm 1 (PageRank ( P,  X  0 ,  X  ) ).
 Step 1. Let the initial approximation be  X  0 . Set k = 0 . Step 2. Compute Step 3. Normalize with Step 2 with k increased by 1.

Note that when M = I and N = P , the iteration matrix is identical to that of Power method. Refer to [18] for a comprehensive review of PageRank.
In this section, we propose our distributed PageRank com-putation (DPC) algorithm.
A dangling node is a page with zero out-degree, i.e. an absorbing state.
If M  X  1  X  0 and M  X  1 N  X  0, the splitting is called weak regular [21]. The basic idea of our algorithm is divide-and-conquer. Each node in the distributed system computes PageRank vector for local pages. Unlike parallel computation algo-rithms performed by a cluster of machines connected with gigabit Ethernet [7], distributed algorithms require simple mechanism of interaction between nodes and low volume of communication traffic. Taking these constrains, we propose our Distributed PageRank Computation (DPC) algorithm here.

The web link graph has a natural block structure: the majority of hyperlinks are intra-host ones [12]. Therefore, the random walk on the web can be viewed as a nearly com-pletely decomposable (NCD) Markov chain [24]. This prop-erty opens the door for the iterative aggregation-disaggregation (IAD) methods [29]. Before presenting the DPC algorithm, we introduce the classic IAD methods here. The notations and terminologies adopted in this section fol-low those used in [22].
Let G be a set of integers { 1 , . . . , N } . Let G 1 , . . . , G N be the aggregated groups of elements in G . The sets G , i = 1 , . . . , n , are mutually disjoint and  X  n i =1 G N i be the order of set G i , i.e. the number of elements in G Let R be the n  X  N aggregation matrix , which satisfies We partition the positive vector  X  as (  X  T 1 ,  X  T 2 , . . . ,  X  according to { G i } .  X  i is a subvector with dimension N
Then we define the N  X  n disaggregation matrix S (  X  ) as follows: censored stationary distribution of pages in node G i . Note that RS (  X  ) = I .

Let T = M  X  1 N be a matrix arising from some splitting of I  X  P = M  X  N . To solve the linear system ( I  X  P )  X  = 0, we can adopt the following algorithm: Algorithm 2 (IAD method).
 Step 1. Select a positive initial approximation  X  0 , k  X  Set k = 0 .
 Step 2. Construct the aggregated matrix RPS (  X  k ) and solve the linear system where k z k = 1 .
 Step 3. Compute Step 4. Normalize continues with Step 2 with k increased by 1. First, we define some notations for following discussion. The transition matrix P is partitioned into blocks according to { G i } : We denote the i th block row with and denote the i th block column with
Each diagonal block P ii is square and stands for the intra-node link matrix of node G i , while the off-diagonal blocks stand for the inter-node link structure. Moreover, the n  X  n aggregated matrix A = RPS (  X  ) is the transition matrix between nodes. It is straightforward that A satisfies irre-ducibility and aperiodicity when P does.
 We now present our new algorithm: Algorithm 3 (DPC algorithm).
 Step 1. Each node G i constructs its local transition matrix Q , which contains only the pages in G i . Let the initial approximation be Set k = 0 .
 Step 2. Construct the aggregated matrix A k = RPS (  X  k ) . Solve the associated linear system. This step can be called the solution on the coarse level.
Step 3. Each node G i constructs an ( N i + 1)  X  ( N i + 1) extended local transition matrix where the scalar  X  k ensures the column sum of B k i is one.
Compute the extended local PageRank vector where  X  k +1 i is a scalar. This step can be called the smoothing on the fine granularity.

Before being sent to the center, the local vector is multi-plied by a factor.
Step 4. Normalize If k  X  k +1  X   X  k k &lt;  X  , the algorithm quit with  X  k +1 continue with Step 2 with k increased by 1.
Remark 1. The DPC algorithm is essentially equivalent to an IAD method with T being a Block Jacobi iteration matrix. Proof in exact arithmetic is given in Appendix A .
The IAD method was first proposed by Takahashi in 1975 [29]. It has been widely used to accelerate convergence of iterative methods for solving linear systems and mini-mization problems. After thirty years, the global conver-gence of IAD method is still an open problem. The difficulty partly comes from that the disaggregation step S (  X  ) z is non-linear [4]. Some convergence properties of IAD method are analyzed in [22, 23, 27]. In order to justify the DPC algo-rithm to some extent, we prove the convergence of the Block Jacobi method in PageRank scenario.
 First we present a lemma from [5]:
Lemma 1. The iteration scheme converges when the following conditions are satisfied: ( C 1 )  X  ( T ) = 1 ( C 2 ) T is irreducible ( C 3 ) T is acyclic Neumann and Plemmons [25] proves that the iteration ma-trix derived from any weak regular splitting of the matrix I  X  P satisfies condition ( C 1 ) and ( C 2 ) if the matrix P is stochastic and irreducible.

Let D be the block diagonal of I  X  P . Let L be the block strictly lower triangular part of P , and U be the block strictly upper triangular part of P . Arising from the split-ting I  X  P = D  X  ( L + U ), the iteration matrix of Block Jacobi methods is
Since ( I  X  P ii )  X  1  X  0 [24] and ( L + U )  X  0, the split-ting above is weak regular . Because P is stochastic and irreducible, T satisfies ( C 1 ) and ( C 2 ).

However, the acyclicity of P is not sufficient to guaran-tee the acyclicity of T [14]. Fortunately, in the PageRank scenario we have the following lemma:
Lemma 2. If P &gt; 0 is the transition matrix of a Markov chain and is partitioned according to (3.6) . Let T be the iter-ation matrix defined in (3.16) , i.e. the Block Jacobi matrix. T is acyclic if and only if n &gt; 2 .
 Proof. See Appendix B .

Now ( C 1 ), ( C 2 ) and ( C 3 ) in Lemma 1 are all satisfied when n &gt; 2. Consequently, we have:
Theorem 1. If P &gt; 0 is the transition matrix of a Markov chain and is partitioned according to (3.6) . Let T be the iter-ation matrix defined in (3.16) , i.e. the Block Jacobi matrix. If n &gt; 2 , The iterative scheme (3.15) always converges to the fix point  X  x of P  X  x =  X  x
Courtois [5] proves that when T is cyclic, the iteration scheme (3.15) ultimately converges to a vector composed of subvectors which are parallel to the corresponding subvec-tors of  X  x . Therefore  X  x can be achieved by an additional single iteration of IAD methods.
In this section, we analyze the communication overhead of our algorithm. All messages are in the form of a vector, no matrix is transferred. Because the vector v is usually sparse, In practice, the index is a combination of the node ID and a hash value of the URL string. Let Pos(  X  ) denote the number of positive elements in a vector or a matrix. So the size of message is proportional to Pos( v ). In practice, we use sparse matrix  X  P instead of P . Let  X  L and  X  U be the block strictly lower and upper triangular part of  X  P separately. Step 1 of DPC algorithm needs trivial communication.
In Step 2, node G i sends the coordinator a vector  X  P  X  i subvector of  X  P  X  i  X  i is sent as a scalar e T  X  P ii which is much smaller than Pos(  X  L +  X  U ). Table 1 shows the comparison in real web graphs.

In Step 3, the coordinator sends the i th subvector of  X  PS (  X  ) z to node G i . So the communication cost is Pos((  X  U ) S (  X  ) z ), which is much smaller than N .

In Step 4, local nodes send the vector  X   X  k +1 i to the coordi-nator, who performs the normalization. The communication cost is O
To sum up, the entire communication overhead is of the magnitude O equivalent to that of the LPR-Ref-2 algorithm in [31]. Table 1: Comparison of number of positive elements
The DPC algorithm has three major advantages over stan-dard PageRank algorithm. 1. As most of the computation in DPC is solving PageR-2. The aggregated matrix A and the local transition ma-3. In DPC algorithm, the local PageRank vectors for Figure 1: Histogram of distribution over number of iterations for local PageRank computation. The x -axis gives the number of iterations, and the y -axis shows the fraction of nodes completing their local PageRank computation within x iterations (  X  = 1 e
We use three real web graphs to evaluate the proposed algorithm. We also implement the classic Power method and the LPR-Ref-2 algorithm in [31] for comparison.
The CN04 graph is from our two-week crawling in Aug. 2004, starting from thousands of well-known web sites in China. In order to obtain pages of high-quality, the crawl was performed in breath-first fashion.

The other two graphs, ST01 5 and ST03 6 , come from Stan-ford WebBase project. Table 2 summarizes these three data sets. The three graphs vary in density of links and degree of inter-sites coupling. Figure 2 shows the distribution of the size of web sites, and Figure 3 shows the distribution of pages hosted by nodes of different size. Both figures are based on the data set ST01, the other two data sets have similar distributions.
 The simulation is carried out on one machine. First, all URLs are sorted lexicographically. Anchors in the tail of URLs are ignored. For example,  X  X ttp://a.edu/b.htm#c X  and  X  X ttp://a.edu/b.htm#d X  are considered to be identical. Then we partition web pages into groups according to sites.
L 1 distance and Kendall X  X   X  -distance [15] are adopted to ftp://db.stanford.edu/pub/webbase/Links2001.tar.gz ftp://db.stanford.edu/pub/webbase/Crawl-2003-04.tar.gz Figure 2: Histogram of distribution over size of sites. The x -axis gives the magnitude of the number of pages hosted by a site, and the y -axis shows the fraction of sites of the size. Figure 3: Histogram of distribution over number of pages hosted by sites of different size. The x -axis gives the magnitude of the number of pages hosted by a site, and the y -axis shows the fraction of pages hosted by all sites of that size. evaluate our algorithm. Both measure certain kind of simi-larity between  X  and  X   X  .

The first metric is the L 1 distance k  X   X   X   X  k 1 . We have 0 &lt; k  X   X   X   X  k 1 &lt; 2.

Then we introduce the Kendall X  X   X  metric. Let K (  X ,  X   X  ) be an N  X  N matrix, whose elements are order in  X  and  X   X  .
 Kendall X  X   X  -distance is defined as follows: where 0  X  KDist(  X ,  X   X  )  X  1.
The distributed computation heavily relies on the commu-nication between nodes. When the network is under heavy load, it is possible for the iteration to be executed only once and wait for a long time to reconnect the coordinator. Then the accuracy of  X  1 is virtually important. In fact, the LPR-Ref-2 algorithm proposed in [31] can be performed only once. Table 3 shows the L 1 distance between  X  1 and  X   X  . Table 3: Accuracy after One Iteration ( k  X  1  X   X   X  k 1 )
It is costly to compute Kendall X  X   X  -distance when N is large, because the computational complexity is O ( N 2 ). On a box with Intel Pentium 4 2.4GHz processor, it takes about 4 minutes to compute the Kendall X  X   X  -distance between two vectors when N = 10 5 . Since the N in our experiments are of the order of magnitude of 10 8 , we have to adopt Monte Carlo method to estimate the true Kendall X  X   X  -distance. KDist(  X ,  X   X  ) can be viewed as the probability of two ran-domly picked pages with different order in  X  and  X   X  . So we can estimate the probability by random sampling. In prac-tice, we pick 10 10 random pairs as a sample. Repeated run-ning of the Monte Carlo method empirically demonstrates that the variance of approximations is small. Thus, the sam-pling method is reliable. Table 4 presents the KDist(  X  1 of different methods.
 Table 4: Accuracy of One Iteration (KDist (  X  1 ,  X   X  ) )
When the network condition is acceptable for communica-tion, iterations of the DPC Algorithm are carried out until convergence is reached. Because the LPR-Ref-2 algorithm in [31] can be run only once, its convergence rate cannot be evaluated. Table 5 shows the number of iterations needed to achieve convergence. The DPC algorithm converges 5 X  7 times faster than Power Method. Figure 4 compares the convergence rate of Power method and that of DPC algo-rithm on graph ST01. The results on the other two graphs are similar.
 Table 5: Number of Iterations for Convergence (  X  = 10 Figure 4: Convergence rate for Power method vs.
 DPC algorithm. The x -axis is the number of itera-tions, and the y -axis is the magnitude of L 1 residual
Much work has been done on PageRank acceleration. Kam-var et al. [12] uses Step 1 and 2 of IAD to obtain an initial vector for subsequent iterations. Lee et al. [19] presents a fast PageRank algorithm which lumps dangling nodes into a single state. Broder et al. [2] proposes graph aggregation as an efficient PageRank approximation method. Eiron et al. [6] exploits the hierarchical structure on different levels of granularity to rank the web pages seen while not crawled, which they named as web frontier. Gleich et al. [7] imple-ments a class of iterative algorithms for PageRank compu-tation on a parallel computer. Langville and Meyer [17] employs a modified two-block IAD to accelerate the updat-ing of PageRank vector. Ipsen and Kirkland [9] analyzes the asymptotic convergence rate of the method proposed in [17]. Wang and DeWitt [31] proposes a framework for distributed search system and a distributed algorithm for PageRank ap-proximation.

The most relevant work to our algorithm is probably the one proposed by Vantilborgh [30]. Cao and Stewart [3] es-tablishes conditions for local convergence of IAD with Block Jacobi smoothing. Stewart et al. [27] proposes an IAD method with Block Gauss-Seidel smoothing and establishes some regularity conditions to guarantee the convergence. Kafeety et al. [10] outlines a general framework for IAD. Recent work by Marek et al. [23] analyzes some local and global convergence properties of IAD.
This paper proposes a distributed PageRank computa-tion algorithm based on iterative aggregation-disaggregation (IAD) methods with Block Jacobi smoothing. The basic idea is divide-and-conquer. We group web pages by sites to make use of the block structure of link graphs. To reduce the com-munication cost, nodes are organized in star topology. Each node computes its PageRank vector of local-stored pages and communicates with a coordinator for global updating information. We prove the global convergence of the Block Jacobi method and then analyze communication overhead of our algorithm. Three primary advantages of our algorithm are presented. Experiments on three real web graphs demon-strate that our method achieves better approximation than LPR-Ref-2 in [31] and accelerates convergence by a factor of 5 X 7. We believe our work provides an efficient and prac-tical distributed solution for PageRank on large scale Web graphs.

Several questions remain to be investigated in our future work: 1. How to update PageRank vectors efficiently within our 2. Since  X  ( T ) = 1, is it possible to compute PageRank
The authors are grateful to Stanford Database Group for sharing ST01 and ST03 data sets. We also thank Dr. Yuan Wang and Dr. David J. DeWitt for offering their data sets used in [31]. We would like to thank the anonymous review-ers for their insightful comments. [1] P. Boldi and S. Vigna. The webgraph framework i: [2] A. Broder, R. Lempel, F. Maghoul, and J. Pedersen. [3] W. Cao and W. Stewart. Iterative aggregation/ [4] F. Chatelin. Iterative aggregation/disaggregation [5] P. Courtois and P. Semal. Block iterative algorithm [6] N. Eiron, K. McCurley, and J. Tomlin. Ranking the [7] D. Gleich, L. Zhukov, and P. Berkhin. Fast parallel [8] G. Golub and C. V. Loan. Matrix computations (3rd [9] I. Ipsen and S. Kirkland. Convergence analysis of a [10] H. Kafeety, C. Meyer, and W. Stewart. A general [11] S. Kamvar, T. Haveliwala, and G. Golub. Adaptive [12] S. Kamvar, T. Haveliwala, C. Manning, and G. Golub. [13] S. Kamvar, T. Haveliwala, C. Manning, and G. Golub. [14] L. Kaufman. Matrix methods for queueing problems. [15] M. Kendall and J. Gibbons. Rank Correlation [16] J. Kleinberg. Authoritative sources in a hyperlinked [17] A. Langville and C. Meyer. Updating pagerank with [18] A. Langville and C. Meyer. Deeper inside pagerank. [19] C. Lee, G. Golub, and S. Zenios. A fast two-stage [20] P. Lyman, H. Varian, J. Dunn, A. Strygin, and [21] I. Marek and P. Mayer. Iterative aggregation/ [22] I. Marek and P. Mayer. Convergence theory of some [23] I. Marek and I. Pultarova. A note on local and global [24] C. Meyer. Stochastic complementation, uncoupling [25] M. Neumann and R. Plemmons. Convergent [26] L. Page, S. Brin, R. Motwani, and T. Winograd. The [27] G. Stewart, W. Stewart, and D. McAllister. A two [28] D. Szyld. The mystery of asynchronous iterations [29] Y. Takahashi. A lumping method for numerical [30] H. Vantilborgh. The error of aggregation in [31] Y. Wang and D. DeWitt. Computing pagerank in a
Here we show that DPC Algorithm is essentially identi-cal to an IAD method with Step 3 being a Block Jacobi smoothing.

Comparing Algorithm 3 with Algorithm 2, Step 1,2 and 4 are identical. Now we prove that the Step 3 of Algorithm 3 is equivalent to that of Algorithm 2 with T = D  X  1 ( L + U ).
In Step 3 of Algorithm 2, from  X   X  k +1 = D  X  1 ( L + U ) S (  X  we have where  X   X  i is the i th subvector of  X   X  .

In Step 3 of Algorithm 3, from (3.12), we have: From (3.13), we have: Comparing (A.3) with (A.1), we conclude that Algorithm 3 is theoretically an IAD method with Block Jacobi smooth-ing.

Partition T into blocks according to G i . The diagonal Consider T 2 as the square of T , which is also partitioned accordingly. When n &gt; 2,  X  i, j,  X   X  k that satisfies  X   X  k 6 = j . Consequently, we have It can be easily verified that T m &gt; 0 when m  X  2. Thus T is acyclic.
 When n = 2,
T when m is odd. And
T when m is even. Thus, T is cyclic.
