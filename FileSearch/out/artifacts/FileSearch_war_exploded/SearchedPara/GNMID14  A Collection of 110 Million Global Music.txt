 A new dataset is presented composed of music identifica-tion matches from Gracenote, a leading global music meta-data company. Matches from January 1, 2014 to Decem-ber 31, 2014 have been curated and made available as a public dataset called Gracenote Music Identification 2014, or GNMID14, at the following address: https://developer. gracenote.com/mid2014. This collection is the first signifi-cant music identification dataset and one of the largest mu-sic related datasets available containing more than 110M matches in 224 countries for 3M unique tracks, and 509K unique artists. It features geotemporal information (i.e. country and match date), genre and mood metadata. In this paper, we characterize the dataset and demonstrate its utility for Information Retrieval (IR) research.
 dataset; collection; music; geotemporal; content identifica-tion; fingerprint; mood; genre; music similarity; music con-sumption behavior
The marriage of the internet and audio fingerprinting tech-nology allowed, for the first time, massive numbers of people to identify a recorded music track. Using a device such as a smartphone, a person can easily identify a recorded track being played and discover information like the artist X  X  home country or even purchase the recording. A byproduct of this identification process is a record that includes informa-tion such as timestamp, location on geo-enabled devices, and track metadata of the identified music, and in aggregation, these records create a unique music consumption signal.
Datasets of other music consumption signals, such as in-ternet radio or microblogs (e.g. Twitter), have been success-fully used in a variety of information retrieval applications [9, 5]. They link contextual information with music content, enriching retrieval interactions in search and recommenda-tion systems, and they enable personalization when user in-information has fueled great strides in general information retrieval tasks[11], and we hope that this dataset can simi-larly move research forward in the music domain.
A music identification event consists of a query against a database containing music fingerprints. A typical query scenario involves a user hearing music being played aloud, recording that music with a device such as cell phone, send-ing the query for identification, and then reviewing informa-tion about the identified music. The data in GNMID14 is the resulting positive matches from these music identifica-tion queries for the whole year 2014. We refer the reader to [3] for a closer review audio fingerprinting technology.
GNMID14 was created through curation of a raw export of matches from 2014 of the music identification database. As this data is not a current product of the company, it is provided to the research community with fewer curation steps than is typically applied when data is licensed to an-other company. However, the authors believe that the re-sulting data is still cleaner than what is typically available to researchers using large datasets in the music domain. The initial export included the following fields: Date, Country, Track ID, User ID, and from this several cura-tion steps were performed. First, entries with incomplete or missing fields were removed. All entries from users with greater than 1000 total matches were also removed to reduce the likelihood of non-human users. Next, the User ID was dropped in its entirety to avoid privacy issues encountered in other large datasets [7]. Next, duplicate tracks were removed as extensively as possible using a combination of text and fingerprint analysis. Finally metadata -track name, artist name, 25 artist-level genres and 26 track-level moods -was mapped to each Track ID.
Table 1 provides some basic statistics on GNMID14. As outlined in the table, cover for genre and mood 1 are less than 100% of the total tracks. While this is typical of music metadata at this scale, this coverage is relatively high.
Figure 1 and Figure 2 show the distribution of genre and mood across matches. Additionally, Figure 3 and Figure 4 show the distribution of genre and mood across tracks. Lastly, Table 2 shows the top 10 countries with the most matches in the dataset.
 Genre is editorially labeled and mood is machine generated. artist distributions may yield useful results for IR applica-tions. The temporal nature of the data allows for prediction of artist or genre popularity or geographical movement of a track. Also, an understanding of the relationship between a music identification signal and a user listening signal would help the IR community select the appropriate music con-sumption dataset for their research.
To demonstrate the utility of GNMID14 we present a novel similarity metric for retrieving tracks in the dataset. Due to various characteristics such as language, origin, and genre, a track will have a distinct geographic popularity distribution. For example, a track sung in Japanese by a Japanese artist may have many listens in Japan and fewer listens elsewhere. It can be useful and interesting in a search or recommendation system to find songs with a similar pop-ularity distribution across countries. This provides another way of exploring the data beyond that of a more traditional similarity metric such as retrieving tracks of the same genre.
To find similar tracks we first represent each track in a vectorized  X  X ag-of-countries X  form and use a vector similar-ity metric, in this case Euclidean distance, to compute the similarity of the tracks.

Specifically, for track x i with total listens in countries [ c , c 2 , ..., c n ], we calculate a normalized geographic listening distribution,  X  x i , to account for differences in track popular-ity in (1).
We then calculate the Euclidean distance, d ij , between track  X  x i and  X  x j as .

For a given seed track x s , we rank the remaining tracks in ascending order by distance to the seed track d sj and retrieve the N highest ranking results.
Three seed tracks were chosen from GNMID14 to show a variety of behaviors of retrieval using the similarity metric in (2). Figure 5 shows a comparison of sparse country listening Figure 7: Geographical distributions of  X  X (JR GROOVE Remix) X  and most similar track Table 3: Top 5 Similar Tracks for  X  X ark Horse X  by Katy Perry feat. Juicy J Table 4: Top 5 Similar Tracks for  X  X amulemau X  by Joe Arroyo Table 5: Top 5 Similar Tracks for  X  X (JR GROOVE Remix) X  by MBLAQ
This paper presented a novel dataset of music identifica-tion matches that fills a gap in available datasets for music information retrieval. The results of a small investigation into a geographical similarity metric for music tracks showed the utility of the dataset for IR related tasks. Moving for-ward, additional metadata may be added to enhance the utility of the dataset, and pending interest from the research community, additional data may be released.
