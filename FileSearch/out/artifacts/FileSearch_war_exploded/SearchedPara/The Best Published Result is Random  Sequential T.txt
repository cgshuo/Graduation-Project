 Reusable test collections allow researchers to rapidly test different algorithms to find the one that works  X  X est X . But because of randomness in the topic sample, or in relevance judgments, or in interactions among system components, ex-treme results can be seen entirely due to chance, particularly when a collection becomes very popular. We argue that the best known published effectiveness on any given collection could be measured as much as 20% higher than its  X  X rue X  in-trinsic effectiveness, and that there are many other systems with lower measured effectiveness that could have substan-tially higher intrinsic effectiveness.
 Categories and Subject Descriptors: H.3.4 [Informa-tion Storage and Retrieval] Performance Evaluation General Terms: Experimentation, Measurement Keywords: information retrieval; test collections; evalua-tion; statistical analysis
Statistical significance testing is an important aspect of experimentation in IR. Without it, differences in effective-ness on the order of 5% would be difficult to interpret: they could represent a  X  X eal X  improvement in effectiveness, or they could be the product of random noise. Significance testing helps us differentiate between the two [4, 5, 3].

That we use significance testing implies that we accept there is randomness in measuring effectiveness. There is randomness due to a topic sample, due to documents in a collection, due to relevance judgments, and other factors of a test collection. Significance testing asks whether the variance in effectiveness that can be ascribed directly to dif-ferences in the systems being tested outweighs those other sources of variance [1].
 A full accounting of variance (such as that done for an ANOVA) could compute the total variance due to collection factors, suggesting that there is a  X  X aseline X  level of effective-ness for a given collection close to which we should expect Figure 1: The distribution of the maximum of 100 samples from a normal distribution with mean 0.20 and standard deviation 0.027. to find most reasonable IR systems. It is the systems that are well above that baseline that we are most interested in, and in particular, the one with the maximum measured ef-fectiveness is widely considered the best possible system to compare against.

But just as two systems can have different measured ef-fectiveness due to chance, a system can have effectiveness higher than the expected collection baseline due to chance. This randomness in turn means that there must be random-ness in our determination of which system has produced the maximum value. It is actually likely that whatever method produces the largest measured effectiveness on a given collec-tion has lower intrinsic effectiveness than reported. This is because of sequential testing : the more tests are done with a given collection, the more likely it becomes that an extreme effectiveness value will be observed.

Rather than take the maximum value at face value, we ar-gue that we should analyze it in the context of other known results and the likelihood that such a value could be pro-duced by a system whose intrinsic effectiveness is much lower simply due to random factors. Of course, the opposite is true as well: any given result could be produced by a sys-tem whose intrinsic effectiveness is much higher . We inves-tigate both sides to argue that the intrinsic effectiveness of the best known system could be as much as 20% lower than reported, and systems as much as 20% less effective than the best could have intrinsic effectiveness much higher. In Section 2 we introduce extreme value theory, and in Sections 3 and 4 we show how we can use it for deeper anal-ysis of IR experiments. We conclude in Section 5 with some discussion about the implications of this work. Figure 2: Increase in the expected maximum as the number of samples increases.
Suppose we have 100 normal distributions, each of which has an identical population mean  X  and population standard deviation  X  . From each of these we sample 50 values and average them so that we have 100 sample means b  X  1  X  X  X  b We can think of these as 100 measures of mean effectiveness over 50 topics from systems with the same intrinsic effective-ness. Each of them will be  X  X lose to X  the population mean. But some will be further away than others, and in particu-lar, one of them must be the maximum of the 100. Given that we X  X e sampled 100 values, it is likely that whatever the maximum is, it will be more than two standard deviations above the population mean; that is, it will  X  X ook like X  a sig-nificant improvement over the population mean even though it was sampled from exactly the same distribution.
Figure 1 shows the distribution of maximum MAP when 100 values are sampled from normal distributions with mean 0.20 and standard deviation 0.027. The mean of the distri-bution of maximums is 0.267, which, if taken at face value, would look like a 34% improvement over the mean of 0.2! Furthermore, 0.267 is outside of the 95% confidence interval around 0.2; in fact only 0.6% of the distribution is greater than 0.267. A full 99% of the distribution of maximums is greater than the upper bound of the 95% confidence interval, meaning it is a near-certainty that out of 100 systems with identical intrinsic effectiveness, at least one will be measured above the 95% confidence interval around the mean.
Figure 2 shows the increase in expected maximum MAP as the number of samples increases. From this we argue that as the number of experiments performed on a particular collec-tion increases, the expected maximum effectiveness reported on that collection will increase logarithmically, even if the  X  X eal X  effectiveness of the systems being experimented on is not significantly different from the overall mean.
Extreme value theory is the area of statistics devoted to distributions of maximum and minimum sampled values [2]. The Gumbel distribution is an example of an extreme value distribution (EVD) that is useful for normally-distributed random variables. Its cumulative density function is: This could be the distribution of random variable X rep-resenting the maximum value of N samples from a normal distribution with mean  X  and variance  X  2 . In that case, the parameter  X  is equivalent to  X  , and the parameter  X  is an increasing function of both  X  2 and N . The expected value of the maximum is then given as  X  +  X  X  , where  X  is Euler X  X  constant, which has a value of about 0 . 5772. Since  X  =  X  ,  X  is constant, and  X  is an increasing function of N , this means that the expected maximum increases with the number of identically-distributed samples.
The exact relationship between  X , X  2 ,N has no closed form 1 . It is easy enough to estimate a Gumbel distribu-tion using sampling, however. Given a population mean  X  and population standard deviation  X  , we sample n values from a normal distribution with those parameters N times ( n represents the number of topics; N the number of sys-tems), average those n values for each of the N samples, and take the maximum average. Over many trials, this produces an approximation of the Gumbel distribution.

We can simplify this further by just taking N samples from a normal distribution with mean  X  and standard devi-ation  X / sampling distribution of the mean. Figure 1 was generated this way, as was Figure 2.
We typically answer statistical questions such as  X  X s one algorithm better than another? X  using statistical hypothesis testing . Procedures for hypothesis testing start by forming a reference distribution for the statistic in question (say, difference in mean effectiveness), then checking whether the measured value is in the tail of that distribution. If so, we say the systems are significantly different.

The extreme value question is  X  X s this algorithm X  X  effec-tiveness better than the maximum expected among N al-gorithms with the same intrinsic effectiveness on the same collection? X  2 Note that the question includes the number of samples N ; this is a key difference between a one-off test of significance versus a test that accounts for the history of experiments done.

We would answer that question by forming a reference distribution for maximum effectiveness rather than mean ef-fectiveness. That distribution must be based on variation across a sample of systems as well as topics, and must also be based on the number of systems N . It must be spe-cific to a collection and an effectiveness measure, since dif-ferent collections and measures exhibit different variability  X  X verage precision typically has low variance compared to other measures, while P@10 has high variance; more recent collections (which tend to be larger and more heterogeneous) like ClueWeb12 tend to exhibit higher variance than older (smaller and less heterogeneous) collections like WSJ or AP.
To form a reference distribution for a given collection and measure, we will first need to obtain a set of mean effec-tiveness values. Once obtained, we will assume that all of those values came from the same distribution: a normal dis-tribution centered at their means, with variance equal to the variance of those means divided by the number of topics in
Closed forms exist only for N  X  5 [7].
We use the phrase  X  X ntrinsic effectiveness X  as a shorthand for  X  X opulation effectiveness X , which refers to the system X  X  effectiveness measured over the full population of queries. In practice there may not be a finite population of queries that could be measured even in principle. the collection (this is the variance of the sampling distribu-tion). Then the maximum mean effectiveness has a Gumbel distribution, parameterized by  X  (the mean of the original normal distribution) and  X  (which is a function of the orig-inal variance as well as the number of means in the set); we estimate that distribution as described above.

Though we have described how to estimate a reference distribution that could be used in a significance test, we are not actually going to propose a significance test. Instead, we will use a reference distribution to analyze results reported using different collections.
There were 103 submissions to TREC-7, so we have N = 103 mean average precisions (MAPs). The mean of means, which we will use for  X  , is about 0.2; we consider this the  X  X aseline X  effectiveness by MAP for the collection. The stan-dard deviation among means, which we will use for  X  , is 0.08. We assume that each mean is drawn from a normal distribu-tion with mean 0.2 and standard deviation 0 . 08 / is the standard deviation of the sampling distribution of the mean, also known as the standard error. To generate the reference maximum value distribution (MxVD), we repeat-edly sample 103 values from a normal distribution with those parameters and take the maximum of those values.

One possible analysis similar to a significance test is as follows: identify the MAP in the MxVD such that 5% of the distribution is greater than or equal to that value. That represents the minimum MAP a system would need for us to conclude with high confidence that it is not just a random extreme value from a distribution with mean 0.2. In TREC-7, that value is about 0.2375. 35 of the 103 submitted runs have a MAP greater than 0.2375; we would say that it is likely 33 (95% of 35, since we expect 5% to be false positives) of those have intrinsic effectiveness above the overall mean. We could do the same for minimum MAP. We find that 5% of the minimum value distribution (MnVD) is less than 0.1625, and 34 TREC-7 submissions have MAPs lower than that. This leaves 34 systems with MAPs within the bounds of what would be expected given that we X  X e sampled 103 total MAPs, the highest and lowest of which are nearly 20% different from the mean.

For the systems outside those bounds, we might also ask what distribution they could have reasonably come from. What is the minimum mean that could generate the maxi-mum observed MAP with high enough probability that we do not consider it significant? Let us take the maximum MAP of any automatic TREC-7 run, since we expect a pri-ori that manual runs will have higher MAPs. That is 0.303 for the ok7ax run [6]. Then the question is how low a pop-ulation mean could produce an extreme value of 0.303 or higher (with the same variance and N ) with probability 0.2. Applying a linear search, we find that value to be 0.2705, which is 11% lower than 0.303.

We might also ask how low a MAP we could measure when 103 are sampled from a distribution centered at 0.2705 rather than 0.2, or, what is the minimum MAP that we could observe with probability greater than 0.2 sampled from that distribution? It turns out that it could be as low as 0.2378. Therefore any MAP between 0.2378 and 0.303 could have come from a distribution centered at 0.27 if 103 values are sampled, and the probability of observing a MAP between those values is 60%. Figure 3: A normal distribution with mean 0.2705 and standard deviation 0.0114 (solid line), along with its maximum value distribution and minimum value distribution for N = 103 . Blue lines show the 95% confidence interval of the normal distri-bution; red lines show the 60% interval in which non-extreme values are likely to fall. venue years papers stats SIGIR 1995 X 2014 2,413 1,159 short, 1,254 long ECIR 2005 X 2014 759 346 short, 413 long CIKM 2005 X 2014 2,620 1,015 short, 1,605 long;
To summarize, if we sample 103 mean average precisions from a distribution centered at 0.2705, there is a 20% chance the maximum sampled MAP would be greater than 0.303, and a 20% chance the minimum sampled MAP would be less than 0.2378. These bounds are outside of the 95% confidence interval around 0.2705, so would likely be considered statisti-cally significantly different than 0.2705, even if the systems turned out to be equivalent over a much larger sample of topics. Figure 3 illustrates this, comparing the normal dis-tribution and its 95% confidence interval to its extreme value distributions and the corresponding 60% interval in which non-extreme values are likely to fall.

The conclusion of this example is that there is a wide range of possible MAPs that are likely to be observed when sampling 100 from this distribution, significantly wider than is implied from its 95% confidence interval. The fact that the largest of them is 0.303 is random; under slightly different conditions that same system could have produced a MAP closer to 0.2378, and a system with a MAP of 0.24 could have produced a MAP of 0.3. Yet the change from 0.2378 to 0.303 represents a 27% improvement in effectiveness.
In this section we analyze the IR literature to find dis-tributions of effectiveness for different standard collections. We have a corpus of IR conference papers from 1995 X 2014, some statistics of which are shown in Table 1.We searched this corpus for papers using some standard collections: the Wall Street Journal (WSJ) and Associate Press (AP) col-lections on TREC disks, the GOV2 collection, the WT10g collection, and the TREC Robust 2004 track collection. We transcribed results from these papers, specifically mean ef-fectiveness results. Then for a collection and a measure, we have a set of mean effectiveness values that we can analyze. Table 2: Summary statistics on mean average preci-sions reported in published IR papers for different standard reusable collections. Table 3: Means of distributions that could produce the maximum values in Table 2 along with the 60% confidence interval for non-extreme values. The fi-nal column is the number of MAPs greater than the lower bound.

Table 2 shows summary statistics about data sets. Each row gives the number of results we transcribed ( N ), the mean of those results (  X  ), their standard deviation (  X  ), and the maximum MAP in our sample. Our N s are fairly low, and necessarily a lower bound on the actual value of N . (In fact N cannot be known, since it includes experiments done but never published.)
Table 3 shows results of the analysis we described in Sec-tion 3.1. For each collection we report the mean  X  0 of the normal distribution for which the maximum value reported in Table 2 has 80% cumulative probability in the MxVD X  X o the upper 20th percentile value in this table is the same as the maximum in Table 2. The lower 20th percentile is the value of MAP at the 20th percentile in the MnVD for the distibution centered at  X  0 . These numbers are essentially a mean and 60% confidence bound (like the one in Figure 3) for a distribution that reasonably could have produced the maximum observed value for each collection.

The lower limit of the confidence interval is the value we are most interested in, as it gives an idea of how low mea-sured effectiveness could be while intrinsic effectiveness is still competitive with the best observed effectiveness. Note that the ranges are wide, with the upper bound up to 20% higher than the lower (for GOV2) and over 15% for three of the collections (WSJ, AP, GOV2). The range is lowest for Robust  X 04, because that collection has a much larger number of topics (249) and therefore lower standard error.
The last column in Table 3 gives the number of systems in our sample with effectiveness greater than or equal to the minimum value. In all but WT10g there is more than one system that could reasonably be a candidate for  X  X est performing X  on the collection. If one performs slightly worse than another, it is most likely due to randomness.
We have argued that the best known result on any given test collection has a component of randomness due to the number of times the collection has been experimented with X  something that is out of the control of and unknown to most researchers. Moreover, we have shown that the best known result could come from a system whose intrinsic effective-ness is as much as 20% lower than its observed mean, while a system with much lower observed effectiveness could have intrinsic effectiveness up to 20% greater. This means that there is a wide range of possible overlap in effectiveness, more than what is implied by the standard deviation nor-mally computed for significance testing, due solely to the effect of reusing the collection, and enough that results that are statistically significantly different may actually not be once the extreme value distributions are taken into account.
This means there are extra considerations when reusing test collections and when comparing to best known results. The danger of reusable test collections is that the longer they are used, the more likely it is that an extreme value will be observed by chance alone. This implies that we must men-tally adjust reported results downward some, particularly for older or very popular collections, and especially for methods that have not been shown to consistently work across collec-tions. A relatively simple retrieval approach like BM25 that we know to work well in many different settings, is likely to be a better point of comparison than a much more complex model that happens to have the highest effectiveness on a single collection.

It also suggests that it is not always beneficial to rely on reusable test collections to advance the field. Proprietary collections can be beneficial in that they will not be used by as many different researchers, and thus their N may remain relatively small. Strictly non-reusable collections can never have N &gt; 1, and therefore will never have an issue with ex-treme values being observed due to large N . Therefore it is probably best for the field to publish a portfolio of results across reusable test collections (which will always be good for unit testing, for prototyping, for training, and for failure analysis), proprietary collections (which can include data unavailable outside of the group that owns it, and therefore suggest new avenues of discovery), and non-reusable collec-tions (which should be considered the true  X  X est set X ). Acknowledgments This material is based upon work sup-ported by the National Science Foundation under Grant No. IIS-1350799. Any opinions, findings, and conclusions or rec-ommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Na-tional Science Foundation.
