 In this paper, we investigate a problem of predicting what images are likely to appear on the Web at a future time point , given a query word and a database of historical image streams that potentiates learning of uploading patterns of previous user images and associated metadata. We address such a Web image prediction problem at both a collective group level and an individual user level. We develop a pre-dictive framework based on the multivariate point process, which employs a stochastic parametric model to solve the relations between image occurrence and the covariates that influence it, in a flexible, scalable, and globally optimal way. Using Flickr datasets of more than ten million images of 40 topics, our empirical results show that the proposed algo-rithm is more successful in predicting unseen Web images than other candidate methods, including forecasting on se-mantic meanings only, a PageRank-based image retrieval, and a generative author-time topic model.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Web image prediction, multivariate point processes, penal-ized Poisson regression, personalization
The prevalence of digital cameras and smartphones has led to an explosion of pictures being uploaded and shared online, across websites, platforms and social networks. This phe-nomenon poses great challenges and opportunities in multi-media data mining research. In this paper, we address an interesting problem along this line  X  predicting likely images to appear on the Web at a future time point and retrieving images similar to them from the database, after learning patterns of previous user images and associated metadata.
Fig.1 shows our problem statement with an example of a query world+cup . Suppose that we have the image database downloaded from Flickr for the world+cup up to 12 / 31 / 2008 Can we then estimate what would be the most likely pictures that are taken in a future query time 2 6 / 6 / 2009, and retrieve images similar to them from the database? As Fig.1.(c) has shown, the pictures actually taken at 6 / 6 / 2009 and shared on Flickr are not necessarily about the best possible world cup pictures (if the definition of best is even possible). In-stead, they are the pictures that not only reflect the semantic meaning of the keyword, but also people X  X  intends at that given moment of time. Furthermore, if a user cue is supple-mented, the image prediction becomes highly personalized as shown in Fig.1.(d), given that individual users have their own preferences and photo-taking styles.

The problem in this paper is closely related to one active area of research in information retrieval: exploring the tem-poral dynamics of user behaviors on Web queries [5, 16, 18, 22]. The popularity of queries and their best search results change over time as people X  X  interests evolve. For exam-ple, in [18], it is reported that more than 7% of queries are the ones that do not actually contain a year, but the user implicitly formulate with a specific year in mind ( e.g . miss universe, Olympics). Moreover, many of them are connected to the events that have occurred with predictable periodic-ity. This line of research aims to improve search relevance by identifying what search terms are sensitive to time, what documents should be retrieved to the query time, and what webpages are likely to be clicked by a user at a particular time point. However, much of previous work has targeted at the search of documents such as blogs and news archives by analyzing the query log data; modeling and predicting tem-poral dynamics of Web user images has yet received little attention, even though photos are another popular modal-ity to share the information on the Web.

Consequently, one important application of our image pre-
Strictly speaking, we address a varient of image re-ranking ; we assume that a text-based image search engine ( e.g . Flickr search engine) provides a large-scale pool of unordered Web images for a given query word. Then, our goal is to re-rank those images to be fit for a query time. Image re-ranking following a text-based image search is the de facto pipeline for major image search engines such as Google and Bing [4].
We are interested in future as a query time point rather than past or present because it is most interesting and chal-lenging. For a query time in the past, we may trivially retrieve the images taken at that time from the database. However, if the query time is in future, we have to learn users X  photo-taking patterns and extrapolate likely images to appear for the query time.
 diction is time and user sensitive Web image re-ranking . Suppose that a user submits a query world+cup into Google and Bing image search , which then invariably retrieve re-dundant photos of soccer in the first page. Although the term world+cup usually refers to the international soccer event, it is also commonly used in other international sports and competitions ( e.g . ski, skate, bicycle, or horse riding, as shown in Fig.1.(a)). Therefore, if the world+cup is submit-ted in winter by a user who likes skiing, it is more desirable to include ski world cup photos in the retrieved result. Our image prediction framework can enable the re-ranking of the retrieved images, so that various views of the query word are shown, according to who searches, and when the search takes place. With the majority of Web photos now coming from hundreds of millions of general users with different ex-periences and preferences, the contents of images that are associated even with the same keyword can be highly vari-able according to owners and temporal information.

On the technical aspect, we develop an image prediction algorithm using a multivariate point process, which is a stochastic process that consists of a series of random events occurring at points in time and space [6]. In our method, an observed image stream is viewed as an instance of the multivariate point process. Although this well-established statistical model has been employed for analysis of neural spiking activities [27], and for event detection in video [21], no attempt has been made for image retrieval or re-ranking so far. Nonetheless, we adapt it to offer several key advan-tages for large-scale image prediction as follows: (i) Flexi-bility : The image occurrence on the Web is correlated with a wide range of factors or covariates ( e.g . season, time, user preference, and other external events). A parametric model can be easily set up to relate the image occurrence probabil-ity with any number of factors that influence it (section 3.3). (ii) Optimality : The sparse globally-optimal MLE solution is computed to identify only a small number of key factors and their relative weights (section 3.2). (iii) Scalability : The learning and prediction are performed in a linear time with respect to all parameters, including time steps and the num-ber of covariates (section 3.4). (iv) Prediction accuracies : Our experiments on more than ten millions of Flickr images have demonstrated compelling results on both collective and personalized image forecast over various 40 topic keywords. Indeed we show that our approach outperforms other meth-ods including a PageRank-based image retrieval [15] and a generative author-time topic model [23] (section 4).
The problem of image prediction using large-scale Web photo collections remains an under-addressed topic in the image retrieval literature. Our work is remotely related to following four lines of research, but is significantly different on the task, utility and methodology. Due to vast volume of literatures on these topics, we introduce only some selected papers that are most closely related to our work.

Web content dynamics : This research aims at large-scale analyses to describe how the Web content changes over time. Most previous work [1, 28] has dealt with the textual content on the Web such as news articles and scientific li-braries. In the image domain, the most related work to ours may be [15] in that both involve studying topic evolution in large-scale Flickr photos. However, the main tasks of [15] were subtopic outbreak detection and classification of noisy web images. They did not address the image prediction, which is our main task here. Also, they did not explore any issues regarding personalization, as done in this work.
Similar image retrieval : The image prediction prob-lem is also related to similar image retrieval, a well-studied topic in computer vision [8, 20, 26]. They are related in a way that in both cases, given a query, relevant images are returned from the database. Yet, there are a number of key differences. Traditional similar image retrieval tends to fo-cus solely on the semantic meaning of the query word and feature-wise image similarity, whereas our image prediction additionally emphasizes the temporal trends and user histo-ries associated with the images.

Image based collaborative filtering : The goal of this research is to mine the trends of people X  X  interests from com-munity photos such as Flickr. Examples include the so-cial trends in politics and market [13], and spatio-temporal events [24]. However, most existing work has used images as the source of information to infer other phenomena rather than taking themselves as a subject to be forecasted.
Leveraging Web photos to infer missing informa-tion : The final related work is on inferring missing in-formation by leveraging a large-scale Web image corpus. Some notable examples include scene completion [12], geo-location estimation of a photo sequence [14], 3-D models of landmarks [25], semantic image hierarchy [17], and people matching [11]. However, future image occurrence has not been explored as missing information to be inferred.
Departing from the literatures reviewed above, the main contributions of our work can be summarized as follows: (1) We develop a method for collective and personalized image prediction. To the best of our knowledge, there have been few attempts so far on such prediction tasks using large-scale Web photos. Our work can be used in several interesting data mining applications, such as time and user based image suggestion and re-ranking. (2) We design our algorithm using multivariate point pro-cesses. We are not aware of any prior instances of multivari-ate point process in image re-ranking applications; here we adapt this well-founded statistical model to address a num-ber of key challenges of Web image prediction, including flexibility, optimality, scalability, and prediction accuracies.
We define the image prediction as a variant of the time and user sensitive image re-ranking problem. As an input, an image database consists of Flickr photos in [0 ,T ) that are downloaded by a topic keyword, together with their meta-data including timestamps and user IDs. Then, given a fu-ture time point t q &gt;T in the form of (M/D/Y), we retrieve L number of the most likely images from the database. Actual Web images to be predicted at any days are usually hun-dreds or more in volume and extremely diverse in content. Therefore, we first predict the trends of image clusters, and sample multiple L images as output accordingly, in order to cover various aspects of the topic.

We address both collective and personalized image predic-tion. The former refers to a generic prediction for arbitrary individuals using all collected information; and the latter concerns a customized forecast for a particular individual u q to be specified at test time. The personalized prediction focuses on an individual user X  X  history, whereas the collective prediction deals with societally aggregated trends.
Our problem involves learning a model of the image occur-rences with related factors or covariates, and then building a forecast algorithm to sample the likely images based on the learned model. Multivariate point processes are a unified statistical framework to solve these problems, which will be discussed in detail in the next section.

In this paper, we exploit three information modalities based on which a prediction is made: image description, user description, and timestamps at which photos are taken. For clarity, we explain below the preprocessing steps of the first two modalities, and the third one is self-explanatory.
Image Description : All images are clustered into M different groups, which is called as visual clusters in this paper. We first extract two types of features for each image, spatial pyramids of dense HSV SIFT and HOG 3 . Then, we construct a visual dictionary of M clusters ( e.g . M = 500) for each topic by applying K-means to randomly selected 100 K features. Finally, each image is assigned to the nearest visual cluster in the feature space.

User Description : Measuring user propensity is impor-tant in collaborative filtering [7] because a user X  X  future be-havior is likely to be correlated with those of users who are similar to her. Intuitively, each user can be represented by a set of images that she has posted. We first compute an M -dimensional histogram for each user where each bin rep-resents the count of images belonging to the corresponding visual cluster. Instead of directly using the user descrip-tor, we perform the pLSI-based user clustering proposed by Google News personalization [7]. In pLSI, the distribution of visual cluster v in a user u i  X  X  images p ( v | u i ) is given by the following generative model: The latent variable z  X  X  is assumed to represent the cluster of user propensity. Thus, p ( z | u i ) is proportional to the frac-tional membership of user i to cluster z . We denote p ( z | u by u i , which is used as the descriptor of a user u i . The u is an L -1 normalized |Z| -dimensional vector ( i.e . |Z| = 50).
For simplicity and better exposition of our point process framework, we use relatively simple image and user descrip-tors, and assume that the images in the same visual cluster are interchangeable. However, it is straightforward to en-hance our method by replacing them with richer descriptors ( e.g . soft assignment of visual clusters) or by adding other types of information ( e.g . text tags).
We employ a multivariate point process to model a stream of input images, as illustrated in Fig.2. Formally, a multi-variate point process can be described by a counting process N ( t ) = ( N 1 ( t ) ,  X  X  X  ,N M ( t )) T where M is the number of vi-sual clusters and N i ( t ) is the total number of observed im-ages assigned to the i -th visual cluster in the interval (0 ,t ]. Then, N i ( t +  X )  X  N i ( t ) represents the number of images in a small interval  X . By letting  X   X  0, we obtain the inten-sity function  X  i ( t ) ( i.e . image occurrence rate) at t , which indicates the infinitesimal expected occurrence rate of the
The codes for dense SIFT and HOG are available at http://www.vlfeat.org/ and http://www.robots.ox.ac.uk/  X  vgg/software/, respectively. images of the i -th visual cluster at time t [6]:  X  i ( t ) = lim
Data likelihood : Suppose that we partition the interval (0 ,T ] by a sufficiently large number K ( i.e .  X  = T/K ) so that in each time bin  X  only one or zero image occurs. In other words, if we let  X  N i k = N i k  X  N i k  X  1 be the number of images of the i -th visual cluster occurred between t k  X  1 t , then  X  N i k can be zero or one. Here, t k is the k -th interval ( t k = k  X ). Now we can denote the sequence of images up to T by N i 1: K = ( X  N i 1 ,  X  X  X  ,  X  N i K ). This discretization induces that the log-likelihood function is represented by [27] where  X  i ( t k |  X  ) is the parametric form of the intensity func-tion at k -th interval.
In order to relate the image occurrence with covariates, we model the intensity function as the exponential of a linear combination of functions f i j of the covariates x k : log  X  i ( t k |  X  i ) = where  X  i = (  X  i 1 ,  X  X  X  , X  i J ) is a vector of model parameters and J is the number of covariate functions. It is shown in [27] that the likelihood of a point process in Eq.(3) along with  X  of Eq.(4) is identical to the likelihood of a generalized linear model (GLM) under a Poisson probability model and a log link function, which is also known as the Poisson regression .
L1 regularized likelihood : It is reasonable to assume that although numerous factors affect image occurrences, each visual cluster depends on only a small subset of them. Hence, it is important to detect a small number of strong covariates by encouraging a sparse estimator of  X  i , and we maximize the likelihood with ` 1 penalty:
We can efficiently solve the MLE solution to Eq.(5) ( i.e . generalized linear models with ` 1 norm regularization) by using the cyclical coordinate descent in [10]. We use the regularized path to find the best regularization parameter  X  ; we perform a 10-fold cross validation procedure and choose  X  that minimizes the mean cross-validated error.

Example : Here, we introduce a toy example to intuitively show how the proposed model predicts the image occurrence. For simplicity of the example, we assume that the intensity function is affected by only year and month covariates: log  X  i ( t k |  X  i ) =  X  i 0 + where the parameter set comprises seven  X  i y and twelve  X  I ( t k ) is an indicator function that is 1 if the year of t y , and 0 otherwise ( e.g . I y ( t k ) = 1 if y = 2009 and t 03/01/2009). I m ( t k ) is an indicator for the month.
Fig.3 shows the learned intensity functions  X  i ( t ) of two visual clusters N 1 and N 2 with respect to years and months. Figure 3: An example of the Poisson regression model Fig.3.(b) presents the observed image sequences. For both N 1 and N 2 , the intensity functions increase every year (See Fig.3.(c)). The rates decrease in 2009 because the shark dataset was gathered up to mid 2009. Interestingly, N 1 and N 2 show different monthly behaviors (See Fig.3.(d)). The N 1 for dolphins in the sea has a higher intensity value ( i.e . more frequently occurred) in summer, whereas the N 2 for the ice hockey team peaks around January. This observation is reasonable because sea tours are popular in summer and the ice hockey season takes place during winter.

The learned intensity functions can be used for a simple image prediction. For example, if the month of query time t sample the images from N 2 six times more than from N 1 .
Now we introduce the full model of the intensity function  X  that can be used in image prediction. Note that any prob-able factors can be flexibly included into the model with-out any performance loss, because our objective function in Eq.(5) encourages a sparse solution in which the weights of irrelevant covariates are zeros.

We assume that the occurrence of each visual cluster is affected by three types of covariates: (i) its own history, (ii) behaviors of other visual clusters, and (iii) external covari-ates. It leads to the following composite intensity function:  X  ( t k |  X  i ) =  X  i h ( t k |  X  i h )  X  i e ( t k |  X  i e where the  X  i h ,  X  i e , and  X  i x are called the components of in-tensity functions for history, correlation, and external covari-ates, respectively. They are described in Eq.(8)-(10) with an example of Fig.4. For brevity, we omit the superscript i in the following equations.

The first history component is modeled as a linear autore-gressive (AR) process of order P with  X  h = {  X  0 ,  X  X  X  , X  log  X  h ( t k |  X  h ) =  X  0 +  X  N ( t 1 ,t 2 ) denotes the number of images during [ t 1 d is the width of the time window ( e.g . if  X  = 1 day and d = 7, then  X  N ( t k  X  d,t k ) is the number of images occurred during previous one week from t k ). The history component reflects the dynamic behavior of a visual cluster. As shown in Fig.4.(c), the learned parameters of N 1 (top) and N (middle) show the typical patterns for yearly periodic behav-iors, whereas the parameters of N 3 (bottom) are biphasic, which indicates a bursty occurrence.

The second correlation component models the influence from the history of other visual clusters: log  X  e ( t k |  X  e ) =  X  0 + where the parameter  X  e consists of ( M  X  1)  X  R +1 parame-ters of  X  in the full model. This correlation component is quite useful for the actual prediction in the Flickr dataset; we observe that there are strong correlations between visual clusters, and thus the existence or absence of a particular visual cluster gives a strong clue for others X  prediction. The learned parameters  X  in Fig.4.(d) clearly present the corre-lations observed in Fig.4.(b). For example, the subfigures of  X  12 and  X  21 in Fig.4.(d) show that the occurrence of N and N 2 are highly synchronized, whereas the subfigures of  X  13 and  X  23 illustrate the occurrence of N 3 precedes those of N 1 and N 2 by about four months. For fast computation, instead of using the full pairwise model, we learn the corre-lations of each N i with top K most frequent visual clusters.
The extrinsic component incorporates any types of factors that are likely to influence the image occurrence. In this paper, we use months and user descriptors as covariates: log  X  x ( t k |  X  x ) =  X  0 +
We use g ( t k  X  m )  X  exp(  X   X  ( t k  X  m ) 2 ) for month covari-ates. The idea is that if an image occurs in June, some contributions are also given on nearby months like May and July, assuming that images are smoothly changed as time goes. The user covariate is the average of user preferences elements of user descriptors for the images in [ t k  X  d,t
In this paper, we introduce only three types of covariates for modeling of image occurrences, but one can freely add or remove covariate functions according to the characteris-tics of image topics unless they contradict the definition of Eq.(4). For example, other textual or social factors may be supplemented as covariates; or the AR model can be replaced by a more general linear temporal model such as ARMA (AutoRegressive Moving-Average). The learning corresponds to obtain MLE solution  X  i  X  of Eq.(5) from the observe image sequences N i i : K by solving max where  X  i ( t k |  X  ) has the form of Eq.(7). We use the cyclical coordinate descent in [10].

In the prediction step, given a query time t q , we first ob-cates the occurrence rates of each visual cluster for t q computed by gathering covariate values at t q , and plugging them with  X   X  into the Eq.(7). The final output is L number of most likely images for t q , which is sampled according to  X  ( t q |  X   X  ). The images of visual clusters with higher  X  are more likely to be chosen for t q . We use the thinning al-gorithm [19], which is a rejection sampling to simulate new samples from intensity functions.
 Our parametric model is scalable. The learning time is O ( MTJ ) where T is the number of time steps ( e.g . dis-cretized by day) and J is the number of covariates. Our code written in Matlab takes about 30 minutes to learn the model for the 810K of soccer images with M = 200, T = 1 , 500, and J = 118. The complexity of prediction time is O ( MJ ). In the same experiment, it takes far less than one second.
Given a query user u q , the idea of personalization is to weight more the history of pictures taken by u q or similar users to u q during learning. For collective prediction, one Figure 5: Flickr datasets. (a) The numbers of images image occurrence is counted by equally one ( e.g . the occur-rence data in Fig.3.(b) simply count the number of images taken in each day). On the other hand, for personalized prediction, an image by u q is weighted by a larger value so that the model fitting is more biased to the images of u q Likewise, the weight of an image occurrence can be assigned according to the similarity between its owner and u q .
We implemented this idea by using the locally weighted learning framework [2], which is a form of lazy learning for a regression to adjust the weighting of data samples according to a query. More specifically, the weights of image data of any user u x are assigned by w x = p K ( d ( u q , u x )) where u is the user descriptor of a query user u q , d is the distance function d ( u q , u x )=( u q  X  u x ) 2 , and K is the Gaussian kernel function K ( d ) = exp(  X  d 2 / X  ).

In our lazy learning personalization, the training is de-ferred until a query user is assigned. For fast processing, if the number of users to be considered is very large, we can first perform user clustering ( e.g . [7]), and learn the person-alized prediction model offline for each group of users.
We evaluate the performance of our algorithm for collec-tive and personalized image prediction using Flickr datasets. A simplified MATLAB demo code is available at our web-page http://www.cs.cmu.edu/  X  gunhee .
Datasets : Our dataset consists of 10,284,945 images of 40 topics from Flickr. Some topics are re-used from the datasets of [15] and others are newly downloaded. Both datasets are collected by the same protocol, in which the topic name is used as a search word and all queried images are downloaded without any filtering. For the timestamp, the date taken field is used. Fig.5 summarizes some statis-tics of our Flickr dataset. Fig.5.(a) shows the numbers of images and users of all 40 topics, which are roughly clas-sified into { nations , places , animals , objects , activities , ab-stract , hot topics } . Fig.5.(b) shows a seasonal variation of the soccer dataset; the photo-taking peaks in autumn but falls in winter. Fig.5.(c) is a log-log plot between the num-bers of images (x-axis) and users (y-axis). The number of images per user follows Zipf X  X  law in almost all topics. That is, only a few users contribute the majority of images, and most users have only a small number of images.

Tasks : We first divide all image sets into training and test sets by time; training sets I T consist of the images taken up to 12/31/2008 and test sets are the others. In the following experiments, I T is used as both the database for retrieval and training data to learn the image occurrence patterns.
The collective image prediction is performed as follows; a topic name and a future time point t q are given in a form of (M/D/Y) ( i.e . t q is a time point in 2009 or 2010). The images that are actually taken in [ t q  X  1 days] are the positive test set I + to be estimated. The goal is to select L images I e from I T so that I e and I + are as similar as possible to each other. We set L = 200 in our experiments. In almost all test cases, the sizes of positive test sets are larger than L ( i.e . |I + | &gt; L ).

The personalized image prediction is tested similarly only except that a query user u q is additionally specified at test time. The goal of is to predict L likely images so that they are similar to the actual images taken by u q at t q , which are used as the positive test set I + .

For each topic, we randomly generate 20 t q values and 20 ( t q ,u q ) pairs as test cases of collective and personalized image forecast, respectively. A user is considered as u q has a sufficiently large number of images in both training and test sets ( i.e . at least 500 images in the training set and at least 100 images in the test set).

Baselines and Competitors : Since the Web image pre-diction is relatively novel, there are few existing methods to be compared. Hence, we come up with three alternatives for image prediction, and quantitatively compare them with our algorithm. Table 1 summarizes the baselines.

The ( SemIN ) [9] represents the prediction based on seman-tic meaning only. It is compared to show that the semantic meaning of a topic word is not enough to predict the user images on the Web. The ( RetPR ) [15] and ( TopAT ) [23] the state-of-the-art methods for PageRank-based image re-trieval and generative topic modeling, respectively.
In the personalized forecast, the locally weighted learning is also applied to all the other competitors except SemIN , which is a random sampling from the ImageNet dataset.
Evaluation Measures : We evaluated the performance of all algorithms by measuring the similarity between the estimated images I e and actual images I + at t q . Due to lack of a perfect measure for image similarity, we calculate three popular metrics in image retrieval according to infor-mation levels: L2 , Tiny , and average precision ( AP ), as shown in Table 2. L2 is the most low-level metric by feature-wise comparison, and AP is the most high-level one based on clas-sification ability. No single measure may be perfect, but one algorithm can be fairly said better if it constantly outper-forms others in all three metrics. For L2 and Tiny , we first find the one-to-one correspondences between estimated I and actual I + , and then calculate average distances. Since L2 and Tiny are distance measures, a lower value means a better prediction, whereas in the AP , the higher is the better.
We modified the toolbox at http://psiexp.ss.uci.edu/ research/programs data/toolbox.htm. Table 2: Description of three evaluation metrics. For
Fig.6 summarizes the quantitative comparison between our method and three baselines. In each figure, the leftmost bar set is the average performance of 40 topics, and the re-sults of 15 sampled topics follow. Our algorithm significantly outperformed all the competitors in most measures. In the average performance, the L2 ( tiny ) measure of our method is smaller by 5.1(8.8)%, 17.1(6.5)%, and 34.2(15.5)% over the RetPR , TopAT , and SemIN , respectively. Our AP is also higher than the best of baselines by 8 %. Among the baselines, the RetPR was the best, and the SemIN was the worst.

Fig.7 shows several examples of collective prediction for the topics of world+cup , cardinals , shark , and the penguin in several months. Each figure is obtained as follows. We first find the one-to-one correspondences between the estimated I and the actual I + by the L2 measure. Out of L matched pairs ( L = 200), we only sample five matches and show the predicted images by our algorithm in the first row, and their matched actual images in the second row. The five examples may be too few compared to 200 matched pairs, but here we focus on qualitative analysis, since we already discuss the quantitative superiority of our algorithm in Fig.6.
The term world+cup is commonly used in different sports and competitions ( e.g . soccer, ski, skating, cycling, horse riding, and even cocktail competitions), and the popular sports in the images are changed according to the query times. The topic cardinal is also used in different meanings, including a bird, a baseball team, and an American football team. The football images are frequent from fall to winter, whereas the baseball images are dominant from spring to fall. They agree with the scheduled seasons of correspond-ing sports. The bird images are also varied according to the query times. The popular backgrounds of bird images are snowy fields in winter and leafy trees in summer. The im-ages about eggs and baby cardinals also appear in summer. Figure 6: Quantitative comparison between our method Figure 8: Quantitative comparison between our method Similar observations can be made as well in the shark and penguin topics, as shown in Fig.7.(g)-(l).

This observation concludes that indeed the Web image collections are extremely diverse, but some topics follow pre-dictable patterns. Specifically, our predictive model works well for polysemous topics that show strong annual or peri-odic trends, and is promisingly applicable to time-sensitive image suggestion or re-ranking.
Fig.8 shows the quantitative results of personalized image prediction for the same 15 topics in Fig.6. In the average performance, our method is far better than all the base-lines. The personalized prediction is more accurate than the collective forecast, because knowing the user at query time provides a strong clue to predict the images.
Fig.9 delivers a clear evidence for the importance of per-sonalization in image prediction tasks. Even with the same keyword, users show various preferences. As shown in Fig.9. (a)-(c), the meanings of the term fine+art are differently recognized according to the users, such as paintings, classes, and photography. Other examples in Fig.9, including Brazil-ian , apple , and picnic , also show that this personal variation is quite common in online user photo sets, but it can be correctly treated once the user history is learned.
This observation presents that our method can be em-ployed in the image search where a query word has a board range of concepts, which are varied much according to peo-ple X  X  thoughts and interests. This personalized search has been actively studied in textual information retrieval, but our analysis reveals that images can convey more delicate information about user preferences that are hardly captured by text descriptions. For example, in the fine+art topic, fol-lowing questions can be effectively addressed: What styles of paintings does the user like? How does the class presen-tation look like?
We studied the collective and personalized image predic-tion tasks, which can be applicable to a time and user sen-sitive Web image re-ranking using large-scale online images. The multivariate point process model was successfully tai-lored to achieve the flexibility, optimality, scalability, and prediction accuracy. As a promising direction of future work, it is interesting to incorporate other meta data surrounding Flickr photos such as comments or favs for better forecast. This work is supported by Google, ONR N000140910758, AFOSR FA9550010247, NSF IIS-0713379, and NSF DBI-0640543. L.F-F is supported by grant from NSF-IIS-1115313. [1] A. Ahmed, Q. Ho, C. H. Teo, J. Eisenstein, A. J. [2] C. G. Atkeson, A. W. Moore, and S. Schaal. Locally [3] S. Brin and L. Page. The Anatomy of a Large-scale [4] J. Cui, F. Wen, and X. Tang. Real Time Google and [5] W. Dakka, L. Gravano, and P. G. Ipeirotis. Answering [6] D. Daley and D. Vere-Jones. An Introduction to the [7] A. S. Das, M. Datar, , A. Garg, and S. Rajaram. [8] J. Deng, A. C. Berg, and L. Fei-Fei. Hierarchical [9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and [10] J. Friedman, T. Hastie, and R. Tibshirani.
 [11] R. Garg, D. Ramanan, S. Seitz, and N. Snavely. [12] J. Hays and A. A. Efros. Scene Completion Using [13] X. Jin, A. Gallagher, L. Cao, J. Luo, and J. Han. The [14] E. Kalogerakis, O. Vesselova, J. Hays, A. A. Efros, [15] G. Kim, E. P. Xing, and A. Torralba. Modeling and [16] A. Kulkarni, J. Teevan, K. M. Svore, and S. T. [17] L.-J. Li, C. Wang, Y. Lim, D. Blei, and L. Fei-Fei. [18] D. Metzler, R. Jones, F. Peng, and R. Zhang.
 [19] Y. Ogata. On Lewis X  Simulation Method for Point [20] J. Philbin, O. Chum, M. Isard, J. Sivic, and [21] K. Prabhakar, S. Oh, P. Wang, G. Abowd, and J. M. [22] K. Radinsky, K. Svore, S. Dumais, J. Teevan, [23] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and [24] V. K. Singh, M. Gao, and R. Jain. Social Pixels: [25] N. Snavely, I. Simon, M. Goesele, R. Szeliski, and [26] A. Torralba and W. T. F. Rob Fergus. 80 Million Tiny [27] W. Truccolo, U. T. Eden, M. R. Fellows, J. P. [28] X. Wang and A. McCallum. Topics Over Time: a
