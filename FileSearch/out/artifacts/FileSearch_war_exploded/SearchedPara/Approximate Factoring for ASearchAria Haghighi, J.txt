 } @cs.berkeley.edu Inference tasks in NLP often involve searching for an optimal output from a large set of structured out-puts. For many complex models, selecting the high-est scoring output for a given observation is slow or even intractable. One general technique to increase search is challenging in practice. The design of ad-missible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open prob-lem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases ad-missible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and estimates, but their application is limited to models which have a very restrictive kind of score decom-position. In this work, we broaden their projection-which do not factor in this restricted way.

Like Klein and Manning (2003), we focus on search problems where there are multiple projec-tions or  X  X iews X  of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use general optimization techniques (Boyd and Van-denberghe, 2005) to approximately factor a model over these projections. Solutions to the projected problems yield heuristics for the original model. This approach is flexible, providing either admissi-ble or nearly admissible heuristics, depending on the details of the optimization problem solved. Further-more, our approach allows a modeler explicit control over the trade-off between the tightness of a heuris-tic and its degree of inadmissibility (if any). We de-scribe our technique in general and then apply it to two concrete NLP search tasks: bitext parsing and lexicalized monolingual parsing. Many inference problems in NLP can be solved with agenda-based methods, in which we incremen-tally build hypotheses for larger items by combining smaller ones with some local configurational struc-ture. We can formalize such tasks as graph search problems, where states encapsulate partial hypothe-example, in HMM decoding, the states are anchored labels, e.g. VBD [5] , and edges correspond to hidden transitions, e.g. VBD [5]  X  DT [6] .

The search problem is to find a minimal cost path from the start state to a goal state, where the path cost is the sum of the costs of the edges in the path. For probabilistic inference problems, the cost of an edge is typically a negative log probability which de-pends only on some local configuration type. For instance, in PCFG parsing, the (hyper)edges refer-ence anchored spans X [ i, j ] , but the edge costs de-pend only on the local rule type X  X  Y Z . We will use a to refer to a local configuration and use c ( a ) to refer to its cost. Because edge costs are sensi-tive only to local configurations, the cost of a path is
P which is an estimate h ( s ) of the completion cost , the cost of a best path from state s to a goal.

In this work, following Klein and Manning (2003), we consider problems with projections or  X  X iews, X  which define mappings to simpler state and configuration spaces. For instance, suppose that we are using an HMM to jointly model part-of-speech (POS) and named-entity-recognition (NER) tagging. There might be one projection onto the NER com-ponent and another onto the POS component. For-mally, a projection  X  is a mapping from states to some coarser domain. A state projection induces projections of edges and of the entire graph  X  ( G ) .
We are particularly interested in search problems with multiple projections {  X  projection,  X  projections induce well-defined projections of the local configurations  X  projected search problem admits a simpler infer-ence. For instance, the POS projection in our NER-POS HMM is a simpler HMM, though the gains from this method are greater when inference in the projections have lower asymptotic complexity than the original problem (see sections 3 and 4).
In defining projections, we have not yet dealt with the projected scoring function. Suppose that the cost of local configurations decomposes along pro-jections as well. In this case, where A is the set of local configurations and c represents the cost of configuration a under projec-tion  X  tion in the context of a Markov process over two-part states is shown in figure 1(b), where the costs of the joint transitions equal the sum of costs of their pro-jections. Under the strong assumption of equation (1), Klein and Manning (2003) give an admissible poses as a sum of projected path costs. Hence, the following is an admissible additive heuristic (Felner et al., 2004), where h  X  the projected search graph  X  pletion cost of a state bounds the sum of the comple-tion costs in each projection.

In virtually all cases, however, configuration costs will not decompose over projections, nor would we expect them to. For instance, in our joint POS-NER task, this assumption requires that the POS and NER transitions and observations be generated indepen-dently. This independence assumption undermines the motivation for assuming a joint model. In the central contribution of this work, we exploit the pro-jection structure of our search problem without mak-ing any assumption about cost decomposition.
Rather than assuming decomposition, we propose to find scores  X  for the projected configurations which are pointwise admissible : Here,  X   X  ( a ) , the  X  i projection of configuration a . Given pointwise admissible  X  heuristic recipe of equation (2). An example of factored projection costs are shown in figure 1(c), where no exact decomposition exists, but a point-wise admissible lower bound is easy to find. Claim. If a set of factored projection costs {  X  1 , . . . ,  X  ` } Proof. Assume a to optimally reach the goal from state s . Then,
The first inequality follows from pointwise admis-sibility. The second inequality follows because each inner sum is a completion cost for projected problem  X  we can see two sources of slack in such projection heuristics. First, there may be slack in the pointwise admissible scores. Second, the best paths in the pro-jections will be overly optimistic because they have been decoupled (see figure 5 for an example of de-coupled best paths in projections). 2.1 Finding Factored Projections for We can find factored costs  X  wise admissible by solving an optimization problem. We think of our unknown factored costs as a block vector  X  = [  X  of the factored costs,  X  a  X  A . We can then find admissible factored costs by solving the following optimization problem,
We can think of each  X  the cost of configuration a exceeds the factored pro- X  a  X  0 ing the norm of the  X  bounds; indeed if k  X  k = 0 , the solution corresponds to an exact factoring of the search problem. In the case where we minimize the 1-norm or  X  -norm, the problem above reduces to a linear program, which can be solved efficiently for a large number of vari-
Viewing our procedure decision-theoretically, by minimizing the norm of the pointwise gaps we are effectively choosing a loss function which decom-poses along configuration types and takes the form of the norm (i.e. linear or squared losses). A com-plete investigation of the alternatives is beyond the scope of this work, but it is worth pointing out that in the end we will care only about the gap on entire structures, not configurations, and individual config-uration factored costs need not even be pointwise ad-missible for the overall heuristic to be admissible.
Notice that the number of constraints is |A| , the number of possible local configurations. For many search problems, enumerating the possible configu-rations is not feasible, and therefore neither is solv-ing an optimization problem with all of these con-straints. We deal with this situation in applying our technique to lexicalized parsing models (section 4).
Sometimes, we might be willing to trade search optimality for efficiency. In our approach, we can explicitly make this trade-off by designing an alter-native optimization problem which allows for slack in the admissibility constraints. We solve the follow-ing soft version of problem (4): where  X  + = max { 0 ,  X  } and  X   X  = max { 0 ,  X   X  } represent the componentwise positive and negative elements of  X  respectively. Each  X   X  a configuration where our factored projection esti-mate is not pointwise admissible. Since this situa-tion may result in our heuristic becoming inadmis-sible if used in the projected completion costs, we more heavily penalize overestimating the cost by the constant C . 2.2 Bounding Search Error In the case where we allow pointwise inadmissibil-ity, i.e. variables  X   X  ror. Suppose  X   X  the length of the longest optimal solution for the original problem. Then, h ( s )  X  h  X  ( s ) + L  X   X   X   X  s  X  S . This -admissible heuristic (Ghallab and Allard, 1982) bounds our search error by L  X   X   X  In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence w translation w Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surro-gate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G .

Parsing optimally relative to a synchronous gram-mar using a dynamic program requires time O ( n 6 ) in the length of the sentence (Wu, 1997). This high degree of complexity makes exhaustive bitext pars-ing infeasible for all but the shortest sentences. In contrast, monolingual CFG parsing requires time O ( n 3 ) in the length of the sentence. Alternatively, we can search for an optimal parse ing are rooted bispans, denoted X [ i, j ] :: Y [ k, l ] States represent a joint parse over subspans [ i, j ] of w Y respectively.

Given a WSG G , the algorithm prioritizes a state (or edge) e by the sum of its inside cost  X  negative log of its inside probability) and its outside estimate h ( e ) , or completion cost. 4 We are guaran-teed the optimal parse if our heuristic h ( e ) is never greater than  X 
We now consider a heuristic combining the com-pletion costs of the monolingual projections of G , and guarantee admissibility by enforcing point-wise admissibility. Each state e = X [ i, j ] :: Y [ k, l ] projects a pair of monolingual rooted spans. The heuristic we propose sums independent outside costs of these spans in each monolingual projection. These monolingual outside scores are computed rel-ative to a pair of monolingual WCFG grammars G and G into its components  X  Y  X   X  X  and weighting them via optimized  X  s ( r ) and  X 
To learn pointwise admissible costs for the mono-lingual grammars, we formulate the following opti-
Figure 2 diagrams the two bounds that enforce the admissibility of h ( e ) . For any outside cost  X  there is a corresponding optimal completion struc-ture o under G , which is an outer shell of a syn-chronous tree. o projects monolingual completions o c ( o t ) under G s and G t respectively. Their sum c ( o s ) + c t ( o t ) will underestimate  X  G ( e ) by point-wise admissibility.

Furthermore, the heuristic we compute underesti-mates this sum. Recall that the monolingual outside score  X  pletion of the edge. Hence,  X  and  X  3.2 Experiments We demonstrate our technique using the syn-chronous grammar formalism of tree-to-tree trans-ducers (Knight and Graehl, 2004). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. Figure 3(a) shows an example rule.

Following Galley et al. (2004), we learn a gram-mar by projecting English syntax onto a foreign lan-
We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of while searching for the optimal parse with our op-timization heuristic. The exhaustive curve shows edge expansions using the null heuristic. The in-termediate result, labeled English only , used only the English monolingual outside score as a heuris-tic. Similar results using only Spanish demonstrate that both projections contribute to parsing efficiency. All three curves in figure 4 represent running times for finding the optimal parse.

Zhang and Gildea (2006) offer a different heuris-forward estimate of the cost of aligning the unparsed words in both sentences. We cannot directly apply this technique to our grammar because tree-to-tree transducers only align non-terminals. Instead, we can augment our synchronous grammar model to in-clude a lexical alignment component, then employ both heuristics. We learned the following two-stage generative model: a tree-to-tree transducer generates trees whose leaves are parts of speech. Then, the words of each sentence are generated, either jointly from aligned parts of speech or independently given a null alignment. The cost of a complete parse un-der this new model decomposes into the cost of the synchronous tree over parts of speech and the cost of generating the lexical items.

Given such a model, both our optimization heuris-tic and the lexical heuristic of Zhang and Gildea (2006) can be computed independently. Crucially, the sum of these heuristics is still admissible. Re-sults appear in figure 4(b). Both heuristics ( lexi-cal and optimization ) alone improve parsing perfor-mance, but their sum opt+lex substantially improves upon either one. (a) (b) We next apply our technique to lexicalized pars-ing (Charniak, 1997; Collins, 1999). In lexical-ized parsing, the local configurations are lexicalized rules of the form X [ h, t ]  X  Y [ h 0 , t 0 ] Z [ h, t ] h gument word, and argument tag, respectively. We will use r = X  X  Y Z to refer to the CFG back-bone of a lexicalized rule. As in Klein and Man-ning (2003), we view each lexicalized rule, ` , as having a CFG projection,  X  pendency projection,  X  stituency structure, while the dependency projection encodes lexical selection, and both projections are asymptotically more efficient than the original prob-lem. Klein and Manning (2003) present a factored model where the CFG and dependency projections are generated independently (though with compati-ble bracketing): In this work, we explore the following non-factored model, which allows correlations between the CFG and dependency projections: This model is broadly representative of the suc-cessful lexicalized models of Charniak (1997) and 4.1 Choosing Constraints and Handling Ideally we would like to be able to solve the op-timization problem in (4) for this task. Unfortu-nately, exhaustively listing all possible configura-tions (lexical rules) yields an impractical number of constraints. We therefore solve a relaxed problem in which we enforce the constraints for only a subset of the possible configurations, A 0  X  A . Once we start dropping constraints, we can no longer guaran-tee pointwise admissibility, and therefore there is no reason not to also allow penalized violations of the constraints we do list, so we solve (5) instead.
To generate the set of enforced constraints, we first include all configurations observed in the gold training trees. We then sample novel configurations by choosing ( X, h, t ) from the training distribution and then using the model to generate the rest of the configuration. In our experiments, we ended up with 434,329 observed configurations, and sampled the same number of novel configurations. Our penalty multiplier C was 10.

Even if we supplement our training set with many sample configurations, we will still see new pro-jected dependency configurations at test time. It is therefore necessary to generalize scores from train-ing configurations to unseen ones. We enrich our procedure by expressing the projected configuration costs as linear functions of features. Specifically, we define feature vectors f the CFG and dependency projections, and intro-duce corresponding weight vectors w weight vectors are learned by solving the following optimization problem: minimize Our CFG feature vector has only indicator features for the specific rule. However, our dependency fea-ture vector consists of an indicator feature of the tu-the part-of-speech type ( t, t 0 ) (also including direc-tion), as well as a bias feature. 4.2 Experimental Results We tested our approximate projection heuristic on two lexicalized parsing models. The first is the fac-tored model of Klein and Manning (2003), given by equation (6), and the second is the non-factored model described in equation (7). Both models use the same parent-annotated head-binarized CFG backbone and a basic dependency projection which mate projection heuristics to exhaustive search. We measure efficiency in terms of the number of ex-tially outperforms exhaustive search. For the fac-tored model of Klein and Manning (2003), we can also compare our reconstructed bound to the known tight bound which would result from solving the pointwise admissible problem in (4) with all con-straints. As figure 6 shows, the exact factored heuristic does outperform our approximate factored heuristic, primarily because of many looser, backed-off cost estimates for unseen dependency tuples. For the non-factored model, we compared our approxi-mate factored heuristic to one which only bounds the CFG projection as suggested by Klein and Manning (2003). They suggest, where we obtain factored CFG costs by minimizing over dependency projections. As figure 6 illustrates, this CFG only heuristic is substantially less efficient than our heuristic which bounds both projections.
Since our heuristic is no longer guaranteed to be admissible, we evaluated its effect on search in sev-eral ways. The first is to check for search errors, where the model-optimal parse is not found. In the case of the factored model, we can find the optimal parse using the exact factored heuristic and compare it to the parse found by our learned heuristic. In our test set, the approximate projection heuristic failed to return the model optimal parse in less than 1% of sentences. Of these search errors, none of the costs were more than 0.1% greater than the model optimal cost in negative log-likelihood. For the non-factored model, the model optimal parse is known only for shorter sentences which can be parsed exhaustively. For these sentences up to length 15, there were no search errors. We can also check for violations of pointwise admissibility for configurations encoun-(a) (b) tered during search. For both the factored and non-factored model, less than 2% of the configurations scored by the approximate projection heuristic dur-ing search violated pointwise admissibility.
While this is a paper about inference, we also measured the accuracy in the standard way, on sen-tences of length up to 40, using EVALB. The fac-tored model with the approximate projection heuris-tic achieves an F with the exact factored heuristic, though slower. The non-factored model, using the approximate projec-tion heuristic, achieves an F We note that the CFG and dependency projections are as similar as possible across models, so the in-crease in accuracy is likely due in part to the non-factored model X  X  coupling of CFG and dependency projections. timates for inference in complex models. Our tech-nique can be used to generate provably admissible estimates when all search transitions can be enumer-ated, and an effective heuristic even for problems where all transitions cannot be efficiently enumer-ated. In the future, we plan to investigate alterna-tive objective functions and error-driven methods for learning heuristic bounds.
 Acknowledgments We would like to thank the anonymous reviewers for their comments. This work is supported by a DHS fellowship to the first author and a Microsoft new faculty fellowship to the third author.

