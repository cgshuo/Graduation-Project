 For online reviews, sentiment explanations refer to the sentences that may suggest detailed reasons of sentiment, which are very important for applications in review mining like opinion summa-rization. In this paper, we address the problem of ranking senti-ment explanations by formulating the process as two subproblems: sentence informativeness ranking and structural sentiment analysis. Tractable inference in joint prediction is performed through dual decomposition. Preliminary experiments on publicly available data demonstrate that our approach obtains promising performance. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; I.2.7 [ Artificial Intelligence ]: Natural language processing X  Text Analysis Algorithms, Experimentation Opinion Mining; Sentiment Explanation; Dual Decomposition
With the ongoing increasing amount of user-generated reviews on the web, many people consider online reviews as guidelines for decision making. However, few websites provide brief summaries, which makes it difficult for users to find what they focus on, par-ticularly when the size of reviews is very large. On the other hand, for a single review, not every part is equally informative. It would be important to highlight the informative part of each review before review summarization. We term the informative part as  X  X entiment explanations X .

From our point of view,  X  X entiment explanations X  may be several sentences that suggest the detailed reasons of sentiment. Sentiment explanations are valuable for abstracting a single review, which would also benefit the performance of summarizing a collection of reviews. We propose that good sentiment explanations of a single review should have the following properties: 1) from the summarization perspective, they best represent the 2) from the sentiment perspective, they best represent the key The second property ensures that sentiment explanations should represent original reviews in terms of sentiment polarity, because a review might consist of various opinions. It is an advanced prop-erty compared with traditional single or multi document summa-rization.

In this paper, we propose to rank the sentences of a single re-view such that sentiment explanations rank higher. We formulate the ranking process as two subproblems: sentence informativeness ranking and multi-level sentiment analysis, which also echo the two properties for sentiment explanations. For sentence informa-tiveness ranking, we train a simple ranking model from unlabeled data with several heuristic rules; for multi-level sentiment analy-sis, we employ the approach proposed by Yessenalina et al .[16] which aims to select sentences that best represent the original re-view in terms of polarity. Tractable inference in joint models is performed through dual decomposition [14]. Preliminary experi-ments on publicly available data set demonstrate that our approach of joint modeling obtains promising performance.
Recent years, there has been many studies focused on sentiment analysis [12]. Pang and Lee [11] and Yessenalina et al .[16] shown that not every part of the review was equally informative, they ob-tained improved sentiment classification performance as they con-sidered that subjective part was more important for inferring the review rating.

For review summarization, generally it is considered as a sen-tence or review selection problem [1, 7]. Other studies performed review summarization in cascade approaches[3, 4, 10, 18] with first opinion extraction and then document summarization. However, these approaches don X  X  highlight on sentiment explanations.
One similar work with this paper is [6], they scored the explana-toriness for each sentence, and then ranked explanatory sentences for opinion summarization. Though their approach was unsuper-vised, their formulation was based on the assumption that exiting technique can be used to classify the aspect and sentiment of each review. Our setting is more fundamental and our approach is closer to pragmatic needs.
We first propose two subproblems for each property of sentiment explanation: sentence informativeness ranking and multi-level sen-timent analysis. After that, joint inference of two subproblems will benefit the sentence informativeness ranking model such that sen-timent explanations rank higher. For our task here, tractable in-ference in joint prediction is performed through dual decomposi-tion [14].

Dual decomposition is a general approach for combinatorial op-timization, with each sub-problem can be solved separately. With the help of dual decomposition, it makes the task of sentiment ex-planation ranking much more easier. We first present the setting for sentence informativeness ranking and multi-level sentiment analy-sis, respectively; then we give details for joint inference on a new review using dual decomposition. Table 1 presents notations we will use throughout this paper.
We propose the following heuristic rules for sentence informa-tiveness ranking: Aspects can be considered as certain properties of a product or ser-vice. For example, the aspects are  X  X tory X ,  X  X usic X ,  X  X cting X ,  X  X ic-ture X  and  X  X irector X  for movie reviews;  X  X aste X ,  X  X mbience X ,  X  X er-vice X ,  X  X rice X  and  X  X ocation X  for restaurant reviews. We extract aspect terms using a bootstrapping algorithm based on Chi-Square (  X  2 ) statistics shown in Algorithm 1, which is similar with[15].
The  X  2 statistic to compute the dependencies between word v and aspect a j is  X  2 ( v,a j ) = C  X  ( C 1 C 4  X  C 2 C 3 ) where C 1 is the number of times v occurs in sentences with aspect label a j , C 2 is the number of times v occurs in sentences not la-beled with a j , C 3 is the number of sentences with aspect a not contain v , C 4 is the number of sentences that neither belong to aspect a j nor contain word v , and C is the total number of word occurrences.

After extraction of aspect words, we generate the rank of each sentence for a collection of unlabeled sentences based on the afore-mentioned two rules. Then, we are able to train a ranking model using some learning to rank [8] techniques. In this work, we choose a pairwise ranking approach: SVM rank [5], and use bag-of-words features to train the ranking model.
We use the sentiment lexicon from http://www.cs.uic. edu/~liub/FBS/sentiment-analysis.html Algorithm 1 Bootstrapping Framework.
 Input: Output: 1: Initialize T i = A i for all aspects 2: repeat 3: for all sentence x i  X  X do 4: Match aspect words for x i , and record the matching hits 5: Assign aspect label a j to x i if a j = argmax 6: end for 7: for all aspect a j do 8: for all word v  X  V do 9: Calculate  X  2 ( v,a j ) 10: end for 11: T j = T j S { Top ranked n words } 12: end for 13: until No new aspect words are identified or iteration exceeds l 14: return T 1 ,T 2 ,... ;
Suppose G is the learnt ranking model parameterized by ~w a new sentence x i ,  X  ( x i ) denotes the corresponding bag-of-words features vector, we calculate the ranking score as Then for all the reviews, our model outputs a ranking score for each sentences.
For sentiment analysis, we adopt the approach for multi-level sentiment analysis proposed by Yessenalina et al .[16]. A benefit of this approach is that it extracts a set of sentences that best rep-resent the polarity of original review only with the supervision of document-level review polarity, which can be easily obtained since many online review websites provide semi-structural reviews with overall ratings. Here, we give a brief description of this approach. A review document is represented by x with corresponding polar-ity y  X  { +1 ,  X  1 } . The quality of a sentence with polarity y is computed as where  X  pol ( x j ) and  X  subj ( x j ) denote the polarity and subjectiv-ity features of sentence x j , ~w pol and ~w subj are learnt weights for polarity and subjectivity features, respectively. It can be seen that the polarity part captures the quality of sentence x j with polarity y , and the subjective part captures the quality of x j as a subjective sentence.

Suppose s is a set of sentences that best represents the key opin-ion of original review x . We define F parameterized by ~w function that jointly predicts the document polarity y  X  and extracts sentence set s  X  , we have where P ( x ) is the power set of all the sentences in x . Clearly, F has the form F ( x, ( y,s ); ~w s ) = 1 where N ( x ) is a normalizing factor. As  X  pol ( x j ) and  X  are disjoint by construction, we have For simplicity, let  X ( x, ( y,s )) denote the joint feature map, F can be written as F = ~w s  X ( x, ( y,s )) . The training process is to optimize the following problem using latent variable structural SVMs [17]: Optimization Problem 1: where C is the regularization parameter, N is the number of train-ing instances. Then we employ the model to jointly predict sen-polarity of original review using Equation 1.
Dual decomposition is a general approach for combinatorial op-timization, and has been successfully applied to many tasks in natu-ral language processing [13]. For our task here, we expect that sen-tences in s modeled by multi-level sentiment analysis rank higher in informativeness ranking, i.e., suppose h is the top | s | ranked sentences by the sentence ranking model, our goal is to make an alignment between h and s such that there are as many sentences in common as possible. The joint inference problem is Joint Inference Problem 1: where f and g are linear functions that map the output s and h to two vectors of length |x|, with 1 for the chosen sentences and 0 elsewhere. To solve the joint inference problem, we introduce a vector of Lagrange multipliers, ~u  X  R | x | to obtain the Lagrangian with the dual objective The optimization can be solved using subgradient algorithm. We initialize the Lagrange multipliers to ~u (0) = 0 . For k = 1 , 2 ,... , and perform the following steps: followed by where  X  is the step size. It can be verified that Equation 2 can be solved easily using dual decomposition.

For each review x , we obtain a new ranking model G 0 with up-dated parameters that encoding sentiment information benefitted from dual decomposition. Then we apply the ranking function G to rank all the sentences in x , which will naturally make the  X  X enti-ment explanations X  rank higher.
We use the data for explanatory sentence extraction 2 [6], which is based on a collection of Amazon product reviews 3 used in[2] and [4]. Kim et al .[6] asked 4 labelers to make explanatoriness labels for each sentence with 0 for  X  X o explanation X , 1 for  X  X eak explana-tion X  and 2 for  X  X trong explanation X . Further, for sentences that are labeled as sentiment explanation, an additional label is introduced with 1 for  X  X ess than/equal to half of the text provides good expla-nation X  and 2 for  X  X ost of the text provides good explanation X . We employ the results of all the labelers, therefore, each sentence has a score ranging from 0 to 16.

Since the test input of our approach is a review, to make evalua-tions, we ensure that at least one sentence of the review for testing is labeled as sentiment explanation, filtering out those reviews with no sentence labeled as sentiment explanation. Our approach needs training data for the subproblem of multi-level sentiment analysis, we then sample training reviews published from 2004 to 2008 from Amazon product reviews used in [9] 4 . For each product domain, we sample 2000 positive (rating greater than or equal to 4) and 2000 negative (rating less than or equal to 2) reviews for training, 500 positive and 500 negative reviews for development. Table 2 presents the statistics of evaluation data 5 where  X #. X  means number of.
To make comparisons, we use the Normalized Discounted Cu-mulative Gain (nDCG) as the measure to calculate the score of each review, and we choose the following baselines: http://sifaka.cs.uiuc.edu/~hkim277/expSum/ http://www.cs.uic.edu/~liub/FBS/ sentiment-analysis.html http://snap.stanford.edu/data/web-Amazon.html
We only use the following product reviews: Canon G3, Nikon coolpix4300, Canon S100, Nokia 6600, Nokia 6600, Creative Labs Nomad Jukebox Zen Xtra 40GB and MicroMP3. For other prod-ucts, either the reviews are in forms of sentences or the correspond-ing category can not be easily recognized by product names. Our approach can be considered as joint inference with rank(asp+op) and multi-level sentiment analysis, and we then employ the ranking model with updated parameters of the last iteration in dual decom-position to rank the sentences for a given review.
Table 3 presents the averaged nDCG score of all the reviews for each product domain. It can be seen that rank(asp+op) has a slightly better performance over purely rule based approach rule (asp+op) , and our approach achieves a relative high performance compared with baselines.

In this paper, we address the problem of ranking sentiment ex-planations by formulating the process as two subproblems: sen-tence informativeness ranking and structural sentiment analysis. Tractable inference is performed through dual decomposition. Pre-liminary experiments on publicly available data-set demonstrate that our approach is effective and obtains promising performance. For future work, we plan to encode aspect information for fine gran-ular opinion summarization using dual decomposition.
 This work was partly supported by the following grants from: the National Basic Research Program (973 Program) under grant No. 2012CB316301 &amp; 2013CB329403, the National Science Founda-tion of China project under grant No. 61332007 and No. 61272227, and the Beijing Higher Education Young Elite Teacher Project. [1] M. Bonzanini, M. Martinez-Alvarez, and T. Roelleke. [2] X. Ding, B. Liu, and P. S. Yu. A holistic lexicon-based [3] A. Glaser and H. Sch X tze. Automatic generation of short [4] M. Hu and B. Liu. Mining and summarizing customer [5] T. Joachims. Optimizing search engines using clickthrough [6] H. D. Kim, M. G. Castellanos, M. Hsu, C. Zhai, U. Dayal, [7] T. Lappas, M. Crovella, and E. Terzi. Selecting a [8] T.-Y. Liu. Learning to rank for information retrieval. [9] J. McAuley and J. Leskovec. Hidden factors and hidden [10] X. Meng and H. Wang. Mining user reviews: From [11] B. Pang and L. Lee. A sentimental education: Sentiment [12] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [13] A. M. Rush and M. Collins. A tutorial on dual decomposition [14] A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On dual [15] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating analysis on [16] A. Yessenalina, Y. Yue, and C. Cardie. Multi-level structured [17] C.-N. J. Yu and T. Joachims. Learning structural svms with [18] L. Zhuang, F. Jing, and X.-Y. Zhu. Movie review mining and
