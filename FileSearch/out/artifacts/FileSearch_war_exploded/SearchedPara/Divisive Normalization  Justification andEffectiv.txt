 properties of the signals in the natural environments. Among different ways such may be achieved, the efficient coding hypothesis [2, 3] asserts that a sensory system might be understood as a transform that reduces redundancies in its responses to the input sensory stimuli (e.g., odor, sounds, and time varying images). Such signal transforms, termed as efficient coding transforms , are also important to applications in engineering  X  with the reduced statistical dependencies, sensory signals can be more efficiently stored, transmitted and processed. Over the years, many works, most notably the ICA methodology, have aimed to find linear efficient coding transforms for natural sensory signals [20, 4, 15]. These efforts were widely regarded as a confirmation of the efficient coding hypothesis, as they lead to localized linear basis that are similar to receptive fields found physiologically in the cortex. Nonetheless, it has also been noted that there are statistical dependencies in natural images or sounds, to which linear transforms are not effective to reduce or eliminate [5, 17]. This motivates the study of nonlinear efficient coding transforms.
 Divisive normalization (DN) is perhaps the most simple nonlinear efficient coding transform that has been extensively studied recently. The output of the DN transform is obtained from the response of a linear basis function divided by the square root of a biased and weighted sum of the squared responses of neighboring basis functions of adjacent spatial locations, orientations and scales. In biology, initial interests in DN focused on its ability to model dynamic gain control in retina [24] and the  X  X asking X  behavior in perception [11, 33], and to fit neural recordings from the mammalian visual cortex [12, 19]. In image processing, nonlinear image representations based on DN have been applied to image compression and contrast enhancement [18, 16] showing improved performance over linear representations.
 As an important nonlinear transform with such a ubiquity, it has been of great interest to find the underlying principle from which DN originates. Based on empirical observations, Schwartz and Simoncelli [23] suggested that DN can reduce statistical dependencies in natural sensory signals and is thus justified by the efficient coding hypothesis. More recent works on statistical models and efficient coding transforms of natural sensory signals (e.g., [17, 26]) have also hinted that DN may be an approximation to the optimal efficient coding transform. However, this claim needs to be rigorously validated based on statistical properties of natural sensory signals, and quantitatively evaluated with DN X  X  performance in reducing statistical dependencies of natural sensory signals. In this work, we aim to establish a connection between the DN transform and the statistical proper-ties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justifies DN as an approximation to the transform that completely eliminates its statistical dependency. Further-more, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We com-pare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations confirm DN as an ef-fective efficient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small. Sensory signals in natural environments are highly structured and non-random. Their regularities exhibit as statistical properties that distinguish them from the rest of the ensemble of all possible signals. Over the years, many distinct statistical properties of natural sensory signals have been observed. Particularly, in band-pass filtered domains where local means are removed, three statistical characteristics have been commonly observed across different signal ensembles 1 : -symmetric and sparse non-Gaussian marginal distributions with high kurtosis [7, 10], Fig.1(a); -joint densities of neighboring responses that have elliptically symmetric (spherically symmetric -conditional distributions of one response given neighboring responses that exhibit a  X  X ow-tie X  It has been noted that higher order statistical dependencies in the joint and conditional densities (Fig.1 (b) and (d)) cannot be effectively reduced with linear transform [17]. the multivariate Student X  X  t model. Formally, the probability density function of a d dimensional t random vector x is defined as 2 : where  X  &gt; 0 is the scale parameter and  X  &gt; 1 is the shape parameter.  X  is a symmetric and positive definite matrix, and  X (  X  ) is the Gamma function. From data of neighboring responses of natural sensory signals in the band-pass domain, the parameters (  X , X  ) in the multivariate t model can be obtained numerically with maximum likelihood, the details of which are given in the supplementary material.. The joint density of the fitted multivariate t model has elliptically symmetric level curves of equal probability, and its marginals are 1D Student X  X  t densities that are non-Gaussian and kurtotic [14], all resembling those of the natural sensory signals, Fig.1(a) and (c). It is due to its heavy tail property that the multivariate t model has been used as models of natural images [35, 22]. Furthermore, we provide another property of the multivariate t model that captures the bow-tie dependency exhibited by the conditional distributions of natural sensory signals.
 Lemma 1 Denote x \ i as the vector formed by excluding the i th element from x . For a d -dimensional isotropic t vector x (i.e.,  X  = I ), we have where E(  X  ) and v ar (  X  ) denote expectation and variance, respectively.
 This is proved in the supplementary material. Lemma 1 can be extended to anisotropic t models by incorporating a non-diagonal  X  using a linear  X  X n-whitening X  procedure, the result of which p v ar ( x i | x \ i ) for pairs of adjacent band-pass filtered responses of a natural image, and the three blue dashed curves are the same quantities of the optimally fitted t model. The bow-tie phenomenon comes directly from the dependencies in the conditional variances, which is precisely captured by the fitted multivariate t model 3 . Using the multivariate t model as a compact representation of statistical properties of natural sensory signals in linear band-pass domains, our aim is to find an efficient coding transform that can effec-tively reduce its statistical dependencies. This is based on an important property of the multivariate t model  X  it is a special case of the Gaussian scale mixture (GSM) [1]. More specifically, the joint density p t ( x ;  X , X  ) can be written as an infinite mixture of Gaussians with zero mean and covariance matrix  X  , as d dimensional t vector x , we can decompose it into the product of two independent variables u and  X  z , as x = u  X   X  , and z &gt; 0 is a scalar variable of an inverse Gamma law with parameter (  X , X  ) . To simplify the discussion, hereafter we will assume that the signals have been whitened so that there is no second-order dependencies in x . Correspondingly, the Gaussian vector u has a covariance  X  = I . According to the GSM equivalence of the multivariate t model, we have u = x / Gaussian vector has mutually independent components, there is no statistical dependency among elements of u . In other words, x / dependencies in x . Unfortunately, this optimal efficient coding transform is not realizable, because z is a latent variable that we do not have direct access to.
 To overcome this difficulty, we can use an estimator of z based on the visible data vector x ,  X  z , transform as x / the estimators z , namely, the maximum a posterior (MAP) and the Bayesian least square (BLS) estimators, and a third estimator all have similar forms, a result formally stated in the following lemma (a proof is given in the supplementary material).
 Lemma 2 For the d -dimensional isotropic t vector x with parameters (  X , X  ) , we consider three the inverse of the conditional mean of 1 /z , as  X  z 3 = E z | x (1 /z | x )  X  1 , which are: If we drop the irrelevant scaling factors from each of these estimators and plug them in x / obtain a nonlinear transform of x as, This is the standard form of divisive normalization that will be used throughout this paper. Lemma 2 shows that the DN transform is justified as an approximate to the optimal efficient coding transform given a multivariate t model of natural sensory signals. Our result also shows that the DN transform approximately  X  X aussianizes X  the input data, a phenomenon that has been empirically observed by several authors (e.g., [6, 23]). 3.1 Properties of DN Transform The standard DN transform given by Eq.(2) has some nice and important properties. Particularly, the following Lemma shows that it is invertible and its Jacobian determinant has closed form. Lemma 3 For the standard DN transform given in Eq. (2), its inversion for y  X  R d with k y k &lt; 1 is  X   X  1 ( y ) = Further, the DN transform of a multivariate t vector also has a closed form density function. Lemma 4 If x  X  R d has an isotropic t density with parameter (  X , X  ) , then its DN transform, y =  X  ( x ) , follows an isotropic r model, whose probability density function is Lemma 4 suggests a duality between t and r models with regards to the DN transform. Proofs of Lemma 3 and Lemma 4 can be found in [8]. For completeness, we also provide our proofs in the supplementary material. 3.2 Equivalent Forms of DN Transform Eq.(2). However, if we are merely interested in their ability to reduce statistical dependencies, many of the different forms of DN transform based on l 2 norm of the input vector x become equivalent. To be more specific, we quantify statistical statistical dependency of a random vector x using the multi-information (MI) [27], defined as where H (  X  ) denotes the Shannon differential entropy. MI is non-negative, and is zero if and only if the components of x are mutually independent. MI is a generalization of mutual information, and the two become identical when measures dependency for two dimensional x . Furthermore, MI is invariant to any operation that operates on individual components of x (e.g., element-wise rescaling) since such operations produce an equal effect on the two terms P d k =1 H ( x k ) and H ( x ) (see [27]). Now consider four different definitions of the DN transform expressed in terms of the individual element of the output vector as Here x \ i denotes the vector formed from x without its i th component. Specifically, y i is the output of Eq.(2). s i is the output of the original DN transform used by Heeger [12]. v i corresponds to the DN transform used by Schwartz and Simoncelli [23]. The main difference with Eq.(2) is that the denominator is formed without element x i . Last, t i is the output of the DN transform used in [31]. These forms of DN 4 related with each other by element-wise operations, as we have As element-wise operations do not affect MI, in terms of dependency reduction, all three transforms are equivalent to the standard form in terms of reducing statistical dependencies. Therefore, the subsequent analysis applies to all these equivalent forms of the DN transform. We have set up a relation between the DN transform with statistical properties of natural sensory signals through the multivariate t model. However, its effectiveness as an efficient coding transform for natural sensory signals needs yet to be quantified for two reasons. First, DN is only an approx-imation to the optimal transform that eliminates statistical dependencies in a multivariate t model. Further, the multivariate t model itself is a surrogate of the true statistical model of natural sensory signals. It is our goal in this section to quantify the effectiveness of the DN transform in reduc-ing statistical dependencies. We start with a study of applying DN to the multivariate t model, the closed form density of which permits us a theoretical analysis of DN X  X  performance in dependency reduction. We then appy DN to real natural sensory signal data, and compare its effectiveness as an efficient coding transform with the theoretical prediction obtained with the multivariate t model. 4.1 Results with Multivariate t Model For simplicity, we consider isotropic models whose second order dependencies are removed with whitening. The density functions of multivariate t and r models lead to closed form solutions for MI, as formally stated in the following lemma (proved in the supplementary material).
 Lemma 5 The MI of a d -dimensional isotropic t vector x is Similarly, the MI of a d -dimensional r vector y =  X  ( x ) , which is the DN transform of x , is In both cases,  X (  X  ) denotes the Digamma function which is defined as  X (  X  ) = d d X  log  X (  X  ) . Note that  X  does not appear in these formulas, as it can be removed by re-scaling data and has no effect on MI. Using Lemma 5, for a d -dimensional t vector, if we have I ( x ) &gt; I ( y ) , the DN both Gamma function and Digamma function can be computed to high numerical precision, we can evaluate  X  I = I ( x )  X  I ( y ) corresponding to different shape parameter  X  and data dimensionality d . The left panel of Fig.2 illustrates the surface of  X  I/I ( x ) , which measures the relative change in MI between an isotropic t vector and its DN transform. The right panel of Fig.2 shows one dimensional curves of  X  I/I ( x ) corresponding to different d values with varying  X  .
 These plots illustrate several interesting aspects of the DN transform as an approximate efficient coding transform of the multivariate t models. First, with data dimensionality d &gt; 4 , using DN leads to significant reduction of statistical dependency, but such reductions become weaker as  X  increases. On the other hand, our experiment also showed an unexpected behavior that has not been reported before, for d  X  4 , the change of MI caused by the use of DN is negative, i.e., DN increases statistical dependency for such cases. Therefore, though effective for high dimensional models, DN is not an efficient coding transform for low dimensional multivariate t models. 4.2 Results with Natural Sensory Signals As mentioned previously, the multivariate t model is an approximation to the source model of natural sensory signals. Therefore, we would like to compare our analysis in the previous section with the actual dependency reduction performance of the DN transform on real natural sensory signal data. 4.2.1 Non-parametric Estimating MI Changes To this end, we need to evaluate MI changes after applying DN without relying on any specific para-metric density model. This has been achieved previously for two dimensional data using straightfor-ward nonparametric estimation of MI based on histograms [28]. However, the estimations obtained this way are prone to strong bias due to the binning scheme in generating the histograms [21], and cannot be generalized to higher data dimensions due to the  X  X urse of dimensionality X , as the number of bins increases exponentially with regards to the data dimension.
 Instead, in this work, we directly compute the difference of MI after DN is applied without explicitly binning data. To see how this is possible, we first express the computation of the MI change as Next, the entropy of y =  X  ( x ) is related to the entropy of x , as H ( y ) = H ( x )  X  R det  X  X  ( x )  X  x has closed form (Lemma 3), and replacing it in Eq.(5) yields Once we determine  X  , the last term in Eq.(6) can be approximated with the average of function of scalar random variables, H ( y k ) and H ( x k ) . For a more reliable estimation, we use the nonpara-metric  X  X in-less X  m -spacing estimator [30]. As a simple sanity check, Fig.3(a) shows the theoretical evaluation of ( I ( y )  X  I ( x )) /d obtained with Lemma 5 for isotropic t models with  X  = 1 . 10 and varying d (blue solid curve). The red dashed curve shows the same quantity computed using Eq.(6) with 10 , 000 random samples drawn from the same multivariate t models. The small difference between the two curves in this plot confirms the quality of the non-parametric estimation. 4.2.2 Experimental Evaluation and Comparison We next experiment with natural audio and image data. For audio, we used 20 sound clips of animal vocalization and recordings in natural environments, which have a sampling frequency of 44 . 1 kHz and typical length of 15  X  20 seconds. These sound clips were filtered with a bandpass gamma-tone filter of 3 kHz center frequency [13]. For image data, we used eight images in the van Hateren database [29]. These images have contents of natural scenes such as woods and greens with linearized intensity values. Each image was first cropped to the central 1024  X  1024 region and then subject to a log transform. The log pixel intensities are further adjusted to have a zero mean. We further processed the log transformed pixel intensities by convolving with an isotropic bandpass filter that captures an annulus of frequencies in the Fourier domain ranging from  X / 4 to  X  radians/pixel. Finally, data used in our experiments are obtained by extracting adjacent samples in localized 1D temporal (for audios) or 2D spatial (for images) windows of different sizes. We further whiten the data to remove second order dependencies.
 With these data, we first fit multivariate t models using maximum likelihood (detailed procedure given in the supplementary material), from which we compute the theoretical prediction of MI dif-ference using Lemma 5. Shown in the top row of Fig.3 (b) and (c) are the means and standard deviations of the estimated shape parameters of different sizes of local windows for audio and im-age data, respectively. These plots suggest two properties of the fitted multivariate t model. First, the estimated  X  values are typically close to one due to the high kurtosis of these signal ensembles. Second, the shape parameter in general decreases as the data dimension increases.
 Using the same data, we obtain the optimal DN transform by searching for optimal  X  in Eq.(2) that maximizes the change in MI given by Eq.(6). However, as entropy is estimated non-parametrically, we cannot use gradient based optimization for  X  . Instead, with a range of possible  X  values, we perform a binary search, at each step of which we evaluate Eq.(6) using the current  X  and the non-parametric estimation of entropy based on the data set.
 In the bottom rows of Fig.3 (b) (for audios) and (c) (for images), we show MI changes of using DN on natural sensory data that are predicted by the optimally fitted t model (blue solid curves) and that obtained with optimized DN parameters using nonparametric estimation of Eq.(6) (red dashed curve). For robustness, these results are the averages over data sets from the 20 audio signals and 8 images, respectively. In general, changes in statistical dependencies obtained with the optimal DN transforms are in accordance with those predicted by the multivariate t model. The model-based predictions also tend to be upper-bounds of the actual DN performance. Some discrepancies between the two start to show as dimensionality increases, as the dependency reductions achieved with DN become smaller even though the model-based predictions tend to keep increasing. This may be caused by the approximation nature of the multivariate t model to natural sensory data. As such, more complex structures in the natural sensory signals, especially with larger local windows, cannot be effectively captured by the multivariate t models, which renders DN less effective. On the other hand, our observation based on the multivariate t model that the DN transform tends to increase statistical dependency for small pooling size also holds to real data. Indeed, the increment of MI becomes more severe for d  X  4 . On the surface, our finding seems to be in contradiction with [23], where it was empirically shown that applying an equivalent form of the DN transform as Eq.(2) (see Section 3.2) over a pair of input neurons can reduce statistical dependencies. However, one key yet subtle difference is that statistical dependency is defined as the correlations in the con-ditional variances in [23], i.e., the bow-tie behavior as in Fig.1(d). The observation made in [23] is then based on the empirical observations that after applying DN transform, such dependencies in the transformed variables become weaker, while our results show that the statistical dependency measured by MI in that case actually increases . In this work, based on the use of the multivariate t model of natural sensory signals, we have pre-sented a theoretical analysis showing that DN emerges as an approximate efficient coding transform. Furthermore, we provide a quantitative analysis of the effectiveness of DN as an efficient coding transform for the multivariate t model and natural sensory signal data. These analyses confirm the ability of DN in reducing statistical dependency of natural sensory signals. More interestingly, we observe a previously unreported result that DN can actually increase statistical dependency when the size of pooling is small. As a future direction, we would like to extend this study to a generalized DN transform where the denominator and numerator can have different degrees.
 Acknowledgement The author would like to thank Eero Simoncelli for helpful discussions, and the three anonymous reviewers for their constructive comments.

