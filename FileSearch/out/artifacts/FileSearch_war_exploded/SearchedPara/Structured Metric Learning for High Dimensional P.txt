 The success of popular algorithms such as k -means clustering or nearest neighbor searches depend on the assumption that the un-derlying distance functions reflect domain-specific notions of sim-ilarity for the problem at hand. The distance metric learning prob-lem seeks to optimize a distance function subject to constraints that arise from fully-supervised or semi-supervised information. Sev-eral recent algorithms have been proposed to learn such distance functions in low dimensional settings. One major shortcoming of these methods is their failure to scale to high dimensional prob-lems that are becoming increasingly ubiquitous in modern data mining applications. In this paper, we present metric learning algo-rithms that scale linearly with dimensionality, permitting efficient optimization, storage, and evaluation of the learned metric. This is achieved through our main technical contribution which pro-vides a framework based on the log-determinant matrix divergence which enables efficient optimization of structured, low-parameter Mahalanobis distances. Experimentally, we evaluate our methods tical software analysis, and collaborative filtering, showing that our methods scale to data sets with tens of thousands or more features. We show that our learned metric can achieve excellent quality with respect to various criteria. For example, in the context of metric learning for nearest neighbor classification, we show that our meth-ods achieve 24% higher accuracy over the baseline distance. Ad-ditionally, our methods yield very good precision while providing recall measures up to 20% higher than other baseline methods such as latent semantic analysis.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Distance Metric Learning, High Dimensional Learning
The problem of comparing examples is a fundamental issue as popular algorithms such as k -means clustering and nearest neigh-bor searches rely on an underlying distance funciton. While com-mon practice has traditionally appealed to off-the-shelf distance functions or hand-tuned metrics, the distance metric learning prob-lem instead seeks to automatically optimize a distance function in either semi-supervised or fully supervised settings. The goal of metric learning is to optimize a distance function that reflects the domain-specific notion for the problem at hand.

Metric learning algorithms typically work by optimizing a tar-get distance under various types of constraints. In semi-supervised clustering applications, constraints are typically either "must-link" (two examples should be in the same cluster), or "cannot-link" (two examples should be in different clusters). In information-retrieval settings, constraints relating pairs of examples can be inferred from click-streams. For example, a click on a second search result with-out a click on the first indicates that the former result should be closer to the search target than the latter. Finally, constraints can be directly inferred in fully-supervised settings, where examples in the same classes can be constrained to be similar if they share the same class label and dissimilar otherwise.

One class of distance functions that has shown good generaliza-tion properties is the Mahalanobis distance [3, 10, 6]. The Ma-halanobis distance generalizes the standard squared Euclidean dis-tance commonly used by algorithms such as the k -nearest neighbor classifier. Intuitively, the Mahalanobis distance works by scaling and rotating the feature space, gi ving certain features more weight while also incorporating correlations between features. Mathemat-ically, the function is defined over a d -dimensional vector space parametrized by a d  X  d positive definite matrix. Recently, sev-eral papers have proposed methods for learning Mahalanobis ma-trices subject to a given set of constraints [3, 10, 11, 6]. Overall, algorithms proposed in these papers have resulted in learned dis-tance functions with excellent generalization performance for low dimensional problems.

However, in high dimensional settings, the problem of learning and evaluating a Mahalanobis distance function with its associated d  X  d matrix becomes quickly intractable due to the quadratic de-pendency on d . This quadratic dependency affects not only the running time for both training and testing, but also poses tremen-dous challenges in estimating a quadratic number of parameters. For example, a data set with 10,000 dimensions requires estimation of a symmetric positive definite matrix with about 50 million pa-rameters! This represents a fundamental limitation of existing ap-proaches, as many modern data mining problems possess relatively high dimensionality. For example, in text-analysis domains, stan-dard bag-of-words models can reach the size of thousands or even tens of thousands of features. Statistical software analysis applica-tions that monitor program paths or method counts similarly have features sets with sizes of thousands or more. Finally, in collab-orative filtering domains, objects are typically rated by thousands or even millions of reviewers. Me thods in these domains typically compare content (e.g. movies, songs, etc.) using a representation in which each reviewer can be viewed as a single feature.
In this paper, we present algorithms for learning structured Ma-halanobis distance functions that scale linearly with the dimension-ality. Instead of searching for a full d  X  d matrix with O ( d rameters, our methods search for compressed representations that typically have O ( d ) parameters. This enables the Mahalanobis dis-tance function to be learned, stored, and evaluated efficiently in the context of high dimensionality.

In particular, the technical contributions of our paper are the problem formulations and resulting algorithms that compute two types of structured low parameter matrices: a low-rank represen-tation, and a diagonal plus low-rank representation. The low-rank representation, HDLR, results in a distance measure which is simi-lar to that used by latent semantic analysis (LSA) [4]. This distance projects data into a low dimensional factor space, and the resulting distance between two examples is the distance between their pro-jections. Our low-rank method can be viewed as a semi-supervised variant of LSA, and is well suited for applications in which higher recall is desired. The second method, HDILR , learns a diagonal plus low-rank matrix, and is well suited for problems where both high recall as well as high precision are important. This is achieved by comparing examples at both the factor level in addition to a component that compares examples at a much finer, feature-level resolution.

Computationally, our algorithms are based on the information-theoretic metric learning method presented in [3]. The problem is formulated as one of learning a  X  X aximum entropy X  Mahalanobis distance that satisfies a given set of constraints. Mathematically, this results in a convex programming problem with a matrix-valued objective function called the log-determinant (LogDet) divergence. We provide two new algorithms based on the LogDet divergence that enable learning Mahalanobis distances in high dimensions. Both of these algorithms scale linearly with dimensionality as O ( d ) .
Experimentally, we evaluate our methods in the context of sev-eral modern domains, including text, statistical software analysis, and collaborative filtering. We provide experimental evidence to show that existing metric learning algorithms do not scale to high dimensional data sets, while demonstrating that our methods can easily handle data with upwards of ten thousand dimensions. We compare our methods in the context of learning metrics for nearest neighbor searches on the basis of several criteria, including accu-racy, precision and recall. As baseline measures, we use the stan-dard Euclidean distance and LSA. Additionally, we compare our methods against a heuristic in which an existing full-rank metric learning algorithm LMNN [10] is used to learn a low-rank distance function. In general, we show that our low-parameter metric learn-ing algorithms can learn high quality distance functions. For exam-ple, classification accuracy for the Classic3 data set is improved by 24% over standard Euclidean distance measures. Additionally, our methods achieve precision as high as all other methods while yield-ing recall values up to 20% higher than a baseline LSA approach.
The paper is organized as follows. Section 2 introduces the prob-lem and provides background on related low-dimensional metric learning methods. Section 3 provides two low-parameter Maha-lanobis distance forms for learning in high dimensions. Section 4 formalizes our problems, and we provide efficient and scalable al-gorithms. Finally, we present experimental results in section 5.
One class of distance measures that has shown good generaliza-tion potential is the Mahalanobis distance. This distance function generalizes the standard squared Euclidean distance and is param-eterized by a positive (semi)-definite matrix A : Learning such a distance function has been the focus of much re-cent research [3, 10, 11, 6], and has proven to be quite successful in low dimensional domains. These distance functions are typi-cally learned given supervised or semi-supervised constraint data. For example, the information-theoretic metric learning algorithm presented by Davis et. al. [3] learns a distance function subject to similarity and dissimilarity constraints. The large-margin nearest neighbor method (LMNN) presented by Weinberger et. al. [10] takes a related approach in comparing three examples at a time (i.e. x is more similar to y than to z ).

A common theme in existing methods is the regularization term found in the problem objective. In Xing et. al., a method is pre-sented in which the learned matrix A is optimized with respect to a sum-of-squares Frobenius objective [11]. LMNN is formulated as a semi-definite programming problem with a linear objective opti-mizing the trace of the matrix A . The information-theoretic metric learning (ITML) method seeks a matrix that minimizes the differ-ential relative entropy between a baseline Gaussian parametrized by A 0 to a target Gaussian parametrized by A . Mathematically, this entropic objective results in a convex programming problem that minimizes the log-determinant (LogDet) divergence with re-spect to the baseline matrix A 0 .

The methods we present in this paper attempt to learn a struc-tured positive semi-definite matrix using the LogDet problem frame-work, and are similar to ITML, which we now describe in detail. The problem assumes a given set of similarity constraints S and dissimilarity constraints D between pairs of examples. Constraints may be inferred from true labels (where examples in the same class are constrained to be similar and examples from different classes are constrained to be dissimilar), or constraints may be explicitly provided. Other constraints that are linear in the entries of A can also be easily incorporated. Additionally, ITML assumes a base-line Mahalanobis distance function parametrized by a positive def-inite matrix A 0 . The formal goal is to learn a Mahalanobis distance parametrized by A that has minimum LogDet divergence to a given baseline matrix A 0 while satisfying the given constraints:
The LogDet objective function D d ( A | A 0 ) is a non-negative, convex function that in the absence of constraints is minimized when A = A 0 . It is defined over d  X  d positive definite matri-ces A and A 0 : where | X | denotes the determinant of the matrix X . In practice, slack variables for the constraints can be incorporated into the above formulation but are omitted for the sake of clarity.
A major shortcoming of ITML and other existing approaches is their quadratic (or even cubic) dependency on the dimensional-ity. Learning full-rank Mahalanobis matrices for problems where
A) Metric learning is an important problem in data mining .

B) High dimensional problems are common in data mining .

C) Optimizing distance functions in high dimensions is challenging .
 Figure 1: The upper left corner lists three sentences. The table on the right shows word counts for each word from the sen-tences. We can see from the inner product matrix on the lower left that even though all three sentences are about metric learn-ing, the distance between documents A and C is large. This toy example illustrates that term frequency models are quite accurate when inner products are larger, yet can be inaccurate when inner products are small or zero. dimensionality is large is prohibitive for several reasons. First, optimizing via algorithms with quadratic dependencies on the di-mensionality can be quite expensiv e. Second, learning a full-rank d  X  d matrix can be viewed as a statistical inference procedure over O ( d 2 ) parameters. For example, in text analysis applications, data sets with thousands of dimensions result in full-rank Mahalanobis matrices parametrized by millions of values. For even the most ro-bust methods, learning under suc h conditions is prone to overfitting and requires a very large amount of supervision. Finally, comput-ing the distance between two points with respect to a Mahalanobis distance function parametrized by a dense, full-rank matrix is an O ( d 2 ) operation. To overcome these problems, we now present Mahalanobis distance functions that are parametrized by O ( d ) val-ues.
Term frequency models represent text documents in terms of in-dividual words and their respective frequencies and are standard representations used in many text analysis applications [1]. These models typically compute the distance between two examples and y using the cosine similarity, cos( x , y )= x T y x y that when x and y are normalized to have unit L 2 norm, the co-sine similarity is equivalent to the standard Euclidean distance: d ( x , y )=2  X  2  X  cos( x , y ) . In many high dimensional do-mains, feature representations tend to be very sparse, and term frequency models are no exception. This poses several problems for standard Euclidean measures. In term frequency models, two documents can have very similar contextual meaning, yet may not necessarily share many of the same words. Hence, the inner prod-uct between two documents can be quite small or even zero, re-sulting in large Euclidean distances. An example of this is shown in Figure 1, where we have three sentences about metric learning. Sentence pairs (A,B) and (B,C) share several common words. Sen-tences A and C, however, share no common words and the Eu-clidean distance between them will therefore be quite large. Thus, even though A and C are contextually similar, the model does not reflect their similarity.

Latent factor models work by representing objects in terms of their context or underlying topics [5]. Instead of representing an object x in its original high dimensional space, latent factor mod-els provide a mapping f that transforms x into some lower k -dimensional space. In Figure 1, we saw that if examples A and C are compared using the Euclidean distance via their original full-dimension representations, the resulting distances will be large. This is in spite of the fact that the two examples are in fact quite similar. The goal of a latent factor model is to learn a mapping f such that f ( A ) and f ( C ) are close to each other.

A popular class of latent factor models such as latent seman-tic analysis (LSA) [4] are those that are parametrized by a d projection matrix R . Here, the factor model X  X  mapping function is f ( x )= R T x . Consider the Euclidean distance between the latent factors of two points x and y : where d A is a low-rank Mahalanobis distance defined by the low-rank matrix A = RR T : Whereas a full-rank Mahalanobis matrix is parametrized by O ( d values, this low-rank matrix is parametrized by the O ( dk ) parame-ters in R .
 Computationally, we can see from equation (3.1) that low-rank Mahalanobis distances can also be computed efficiently in O ( dk ) time, as the distance between two d -dimensional instances y can be computed by first mapping them to a lower dimensional space by computing R T x and R T y , and then computing the stan-dard squared Euclidean distance between the low-dimensional points.
In Figure 1, we saw an example of two sentences that were of similar context but had large Euclidean distances due to zero over-lap (i.e. zero inner product) between their respective feature sets. Thus, it would be incorrect to conclude that in the context of term frequency models, if two objects have zero or small inner prod-ucts, then they are contextually different. However, the converse of this statement is much more likely to hold true. That is, if two documents share many commons words, then they are likely to be contextually similar. However, in traditional low-rank models such as LSA this overlap is largely ignored when data is mapped into a low dimensional space.

We now examine this observation in the context of two standard measures used in information retrieval, precision and recall: Precision is measured as the number of relevant documents re-turned, divided by the total number of documents returned: precision = Number of Relevant Documents Returned Term frequency models tend to result in higher precision, whereas low-rank factor models provide better recall.
 Figure 2 illustrates this behavior for the Classic3 text data set. More details regarding this data set will be presented in Section 5. Precision is plotted for various recall values, comparing nearest neighbor searches using a Tf-Idf Euclidean distance model with that of LSA. We see that for relatively low recall values (i.e. when a relatively small number of documents is returned), the term fre-quency model significantly outperforms a ten-dimensional LSA fac-tor model in terms of precision. However, as recall increases, the Precision Figure 2: Precision-recall curve for the Classic3 text data set. term frequency models using Euclidean distance yield rel-atively high precision, while the low-rank LSA method has higher recall. precision for the Euclidean distance model starts to sharply de-crease, after which LSA eventually achieves noticeably higher pre-cision.

In domains where both high recall and high precision are needed, we propose a second Mahalanobis distance that incorporates ben-efits of both the Euclidean distance as well as a low-rank compo-nent. We propose a Mahalanobis distance parametrized by a matrix I + A ,where A is low-rank: d
I + A ( x , y )=( x  X  y ) T ( I + A )( x  X  y ) Since this proposed measure compares vectors in both the original feature space as well as in a projected low-rank factor feature space, one would expect it to achieve both high recall as well as high preci-sion. Revisiting Figure 1, we can see that the Euclidean component is a good predictor for two of the th ree distances, resulting in rela-tively small distances when comparing (A,B) and (B,C). However, the Euclidean distance between A and C is large, and the low-rank component is needed here to effectively compare sentences A and C.
 So far, we have proposed and motivated two forms of low-parameter Mahalanobis distances. In the next section, we formalize these two problems and provide efficient algorithms to optimize them.
We now extend the full-rank ITML algorithm to learn low-rank matrices. Let R be the d  X  k factor matrix for the rank-k regu-larization matrix A 0 ,i.e. A 0 = RR T . We formulate our high dimensional low-rank (HDLR ) metric learning problem as: Comparing this to the full-rank ITML formulation (2.2), we see that enforcing the rank of the optimal Mahalanobis matrix A . Recent work by Kulis et. al. [9] considers a related problem of learning low-rank kernel matrices subject to linear constraints on the ma-trix. In [9], the LogDet divergence was extended to the positive semi -definite cone, and it was shown that two matrices have a finite LogDet divergence if and only if they share the same range space. The following was shown in [9]:
L EMMA 1. The objective D d ( A | RR T ) of problem (4.1) is fi-nite if and only if A is positive semi-definite with range space equal to the range space of R .
 So, if the baseline Mahalanobis distance function is parametrized by a rank-k matrix, the optimal solution to the HDLR metric learn-ing problem (4.1) without the rank constraint will necessarily have rank k (assuming that the optimization problem (4.1) is feasible). Therefore, the rank constraint rank ( A )  X  k need not be explicitly enforced. In section 4.5, we present methods that can be used to choose an appropriate baseline matrix.
We now present Algorithm 1 that solves our HDLR formula-tion (4.1). The algorithm optimizes a slightly modified version of problem (4.1) that incorporates slack variables to allow constraint violation in the case of incorrect or noisy constraints. The slack penalty parameter  X  determines the relative weighting given to the LogDet component of the objective as opposed to the slack penalty component of the objective. When  X  is large, more weighting is given to the slack terms, and the final solution will more closely satisfy the constraints. When  X  is small, more emphasis is given to the LogDet objective, yielding smoother solutions which are closer to the regularization matrix A 0 . In practice,  X  is chosen via cross-validation.

The algorithm uses the method of cyclic projections and works by iteratively projecting the current solution onto a single con-straint. Instead of directly working on the d  X  d matrix A ,the algorithm instead optimizes its d  X  k factor matrix B .Themain loop starting in line 2 iterates over each constraint until conver-gence. In practice, convergence can be checked by monitoring the change in the dual variables  X  . Steps 5-10 compute the projection parameter  X  . In step 11, this parameter is then used to update B via a rank-one update. Each projection can be computed in closed form and requires O ( dk ) computation, where k is the rank of A Finally, the optimal solution is A = BB T . Note that the latter step may not be needed since as shown in (3.1), the low-rank Ma-halanobis distance between two points can be computed in O ( dk ) time without the need to explicitly compute A .
We now formulate the high-dimensional identity plus low-rank (HDILR ) metric learning problem. As in the formulation presented in the Section 4.1, let R be a d  X  k factor matrix of our data. We will constrain the low-rank portion of the HDILR formulation to span the columns of R , i.e., the learned low-rank portion of our metric resides in the same range space as the original factor matrix. Addi-tionally, whereas the HDLR method has only the low-rank term, the HDILR method has an additional ide ntity matrix te rm (correspond-ing to the baseline squared Euclidean distance). Adding a positive semi-definite component to this baseline measure would result in increasing all distances, an undesirable property for enforcing sim-ilarity constraints. To overcome this, we offset the low-rank com-ponent by subtracting the baseline distance measure projected onto the factor space.

Formally, let U be an orthogonal representation of the columns Algorithm 1 High Dim. Low-Rank (HDLR ) Metric Learning Require: A 0 = RR T : baseline Mahalanobis matrix,  X  : slack 1: B = R,  X  ij =0  X  i, j, b ij =0  X  i, j 2: while not converged do 3: ( i, j )  X  similarity or dissimilarity constraint 4:  X   X  1 if similarity constraint,  X  1 if dissimilarity constraint 5: d  X  ( x i  X  x j ) T B T B ( x i  X  x j ) 6:  X   X  min 7:  X   X   X  X / (1 +  X  X d ) 9: b ij =  X b ij / (  X  +  X  X b ij ) 10:  X  = 11: B  X  B +  X  ( x i  X  x j )( x i  X  x j ) T B 12: end while 13: A  X  BB T of R (i.e. U = R ( R T R )  X  1 2 ,sothat U T U = I ). We construct the low-rank component A of our identity plus low-rank matrix A given in (3.5) as the difference of two matrices, UU T AUU UU T A 0 UU T = UU T ( A  X  A 0 ) UU T . The first term is a function of the learned matrix A , and the second term is a low-rank offset provided by the regularization matrix A 0 .If A = A 0 , the low-rank term A will always be zero, and the distance function will reduce to that of the standard Euclidean distance. As A diverges from A the low-rank term starts to dominate, giving more emphasis to the underlying factor model.

We will now present the HDILR metric learning problem that learns a full-rank Mahalanobis matrix with the constraint A = I + UU T ( A  X  I ) UU T . The analysis and algorithms presented below can be generalized to arbitrary regularizers A 0 (i.e. A = I + UU T ( A  X  A 0 ) UU T ). The HDILR problem is: Consider the distance constraints used in this formulation: d ) .
 The first two terms are independent of the learned matrix A and are therefore constants in the context of the optimization problem. Moving these to the right hand side, we can rewrite the problem: where c ij = d I ( x i , x j )  X  d UU T ( x i , x j ) . Note that c the training points x i and x j lie in the range space of R (and hence, U ). Recall that the original problem constrains the full-rank dis-tance between points x i and x j . In (4.3), a low-rank approximation of the learned Mahalanobis distance is constrained. Recall that for the low-rank HDLR formulation presented in the previous section, if A 0 is low-rank, then the optimal solution is also low-rank. The next theorem characterizes the solution for problem (4.3), showing that the optimal solution satisfies A  X  = I + UU T ( A  X   X  thereby obviating the need to explicitly enforce this identity plus low-rank constraint.

T HEOREM 1. Let A  X  be the optimal solution to an instance of the information-theoretic metric learning problem (4.3) with sim-ilarity constraints S , dissimilarity constraints D , and orthogonal projection matrix U .Then A  X  satisfies I + UU T ( A  X   X  I ) UU
P ROOF . For appropriately defined constants  X  c ij , the Lagrangian of problem (4.3) can be written as
L ( A,  X  )=tr( A )  X  log | A | +
X where  X  ij are dual variables with  X  ij  X  0 ,  X  ij =+1 for similarity constraints and  X  ij =  X  1 for dissimilarity constraints. Using the fact that  X  A log | A | = A  X  1 [2], the gradient of the Lagrangian is,  X 
A L ( A,  X  )= I  X  A Setting the gradient to zero and solving for A  X  1 , ( A Thus, the inverse of the optimal solution has the form identity plus low-rank. To see that the solution A  X  also has this form, we can use the Sherman-Morrison-Woodbury formula [7], which states that for any invertible matrix Y and d  X  k complex matrix Z : Note that Z may be complex, and we denote Z H as its conjugate transpose. Applying the above equation for Y = I ,andfor Z = UU T C ,where C is d  X  d and is defined such that CC H = P ,we have: for  X  A = C ( I + C H UU T UU T C )  X  1 C H . Finally, using the fact that U T U = I k ,wehave
The metric learning problem HDILR formulated in the previous section learns a full-rank d  X  d matrix. Using algorithms presented in [3], both running time and storage requirements for this algo-rithm would still be quadratic in the dimensionality. In this section, we show that by exploiting the identity plus low-rank structure of formulation (4.3), we can transform the problem to an equivalent problem in k dimensions. Via this transformation, the problem can then be solved in time quadratic in k . The solution can then by mapped back to the optimal solution of the original problem via a simple matrix operation.

Consider the following k -dimensional metric learning problem: subject to d M ( U T x i ,U T x j )  X  u  X  c ij ( i, j )  X  where the superscript on I k is used to emphasize the dimensionality of the matrix. We now show how to construct the optimal solution to problem (4.3) given the optimal solution M  X  to problem (4.4). L EMMA 2. Let M  X  be an optimal solution to problem (4.4). Then the optimal solution to problem (4.3) can be constructed as A  X  = I d + U ( M  X   X  I k ) U T .

P ROOF . We first show that problems (4.4) and (4.3) are equiv-alent. Specifically, we show that for any two matrices M and A satisfying M = U T AU , (a) problem (4.4) is feasible if and only if problem (4.3) is feasible, and (b) the problem objectives differ by at most a constant. To see equivalence with respect to feasibility, To see that the objective functions of the two problems differ by at most a constant, we consider the trace term: Since the matrix U is orthogonal, U T U = I k ,so tr( I d  X  d  X  k . Next consider the log det term. Let W be the orthogonal complement to U ,i.e.a d  X  ( d  X  k ) matrix such that I d  X  WW T and U T W =0 .
 The determinant of a matrix is invariant under the orthogonal simi-larity transformation by [ UW ] ,so Finally, we have
Algorithm 2 shows how the above lemma can be used to effi-ciently solve the HDILR metric learning problem (4.2). Step 1 projects the original d -dimensional data onto a k -dimensional sub-space using the low-rank basis U . Next, step 2 solves the (full-rank) ITML problem in this much lower, k -dimensional space, returning a k  X  k matrix M  X  . The optimal solution can be constructed us-ing Lemma 2 as A  X  = I + UM  X  U T . Note that this matrix never needs to be explicitly constructe d, since a Mahalanobis distance parametrized by A  X  can be expressed as the sum of two Maha-lanobis distances as shown in equation (3.5). Finally, it is possible that the right hand side of problem (4.3) is negative. This presents problems for the slack variables used in Algorithm 2. However, in practice, the goal is to learn a metric in which constraints are satis-fied relatively , and this issue can be solved by adding a scalar c&gt; 0 to the right hand side of (4.3) in order to ensure positivity. Algorithm 2 Identity Plus Low-Rank (HDILR ) Algorithm Require: U : low-rank basis,  X  : slack penalty, X =[ x 1 , ..., 1: Form the projected data set  X  X =[ U T x 1 , ..., U T x 2: Compute optimal solution M  X  to full-rank ITML prob-3: Return optimal solution A  X  = I + UM  X  U T
The metric learning algorithms presented in the previous section are parametrized by a low-rank matrix. Algorithms 1 and 2 work by optimizing with respect to a given basis. In order to maximize the quality of the learned metric, an appropriate basis should be chosen.

A standard basis used in unsupervised settings such as latent se-mantic analysis is the left singular vectors of the singular value decomposition (SVD) [7]. LSA works by taking the SVD of the data matrix. Let X be a d  X  n data matrix, where each column represents a d -dimensional instance, and let the SVD of this ma-trix be UDV T ,where U and V are orthogonal matrices, and D is diagonal. Let U k denote the first k columns of U .Thema-trix U k is typically referred to as the top k principal components, and the matrix U k ( U k ) T represents a projection from the original space onto a rank-k subspace. LSA uses this projection in comput-ing distances (or cosine similarities) between points, d L D LSA basis is A 0 = UU T .

While the use of the SVD in methods such as LSA has been shown to achieve good results in many information retrieval set-tings, it is fundamentally an unsupervised method. When used with our metric learning algorithms along with similarity and dissimilar-ity constraint information, the result is a semi-supervised form of LSA.

In cases where data is fully supervised, we propose a method which chooses a low-rank basis according to the class labels. Whereas LSA chooses vectors based on the SVD, the class-mean method forms vectors directly using the class labels. Let c be the num-ber of distinct classes and let k be the size of the desired basis. If k = c , then each class mean r i is computed to form the basis matrix R =[ r 1 ... r c ] .If k&lt;c a similar process is used but restricted to a randomly selected subset of k classes. If k&gt;c , instances within each class are clustered into approximately k c clusters. Each clus-ter X  X  mean vector is then computed to form the low-rank matrix R . LSA X  X  use of the SVD results in an orthogonal low-rank basis. The supervised class means method presented here will not gener-ally result in an orthogonal basis. As a final step in forming a class means basis, we orthogonalize R , resulting in a regularization ma-trix A 0 = R ( R T R )  X  1 R T . Orthogonalization reduces distortion of the low-rank distance. For example, distances between class means are preserved. Let r i and r j be two class mean vectors (i.e. columns i and j of R ). Then where e i is a vector of all zeros with a one in the i th
We now present sample results for our methods from a variety of high-dimensional domains: text analysis, statistical software anal-ysis, and collaborative filtering. These datasets can all be charac-terized by relatively high dimensionality (from 5,000 to more than 100,000 features) and represent a broad sample of modern, high dimensional problems.

We evaluate performance of our learned distance metrics in the context of both classification accuracy for the k -nearest neighbor algorithm, as well as in the context of precision/recall performance for general nearest neighbor searches. The k -nearest neighbor clas-sifier uses k =10 nearest neighbors, breaking ties arbitrarily. Ac-curacy is defined as the number of correctly classified examples divided by the total number of classified examples. Recall and pre-cision are computed as defined in equations (3.3) and (3.4).
Our experiments compare our two algorithms, HDLR and HDILR, given in Algorithms 1 and 2. We also compare these algorithms to a heuristic based on the Large-Margin Nearest Neighbor (LMNN) metric learning algorithm. In [10], LMNN is presented as a method for learning full-rank Mahalanobis distance matrices. Here, we use a heuristic in which data is first projected onto some low-rank, r -dimensional basis U . LMNN is then run over this r -dimensional problem, yielding a ( r  X  r ) matrix A . Finally, this matrix is trans-formed back to the original, high-dimensional space as UAU We emphasize that this procedure does not optimize a well-formed global objective, whereas our approaches optimize a log-determinant objective function. The LMNN implementation used is a Matlab implementation provided by Weinberger, one of the authors of [10].
For our proposed algorithms, pairwise constraints are inferred from true labels. For each class 100 pairs of points are randomly chosen from within the class and are constrained to be similar, and 100 pairs of points are drawn from different classes to form dis-similarity constraints. Given c classes, this results in 100 c simi-larity constraints, and 100 c dissimilarity constraints, for a total of 200 c constraints. The upper and lower bounds for the similarity and dissimilarity constraints are determined empirically as the 1 and 99 th percentiles of the distribution of distances computed us-ing a baseline Mahalanobis distance parametrized by A 0 . Finally, the slack penalty parameter  X  used by Algorithms 1 and 2 is cross-validated using values { . 01 ,. 1 , 1 , 10 , 100 , 1000 }
All metrics are trained using data only in the training set. Test instances are drawn from the test set and are compared to examples in the training set using the learned distance function. The test and training sets are established using a standard two-fold cross validation approach. For experiments in which a baseline distance metric is evaluated (for example, the squared Euclidean distance), nearest neighbor searches are again computed from test instances to only instances in the training set.
We first compare the computational speed of our low-parameter algorithms as compared to existing full-rank methods, LMNN and ITML. Figure 3 shows the time taken to learn metrics of dimension-ality 50 to 2000 over a synthetic data set with 900 instances and 3 classes. All implementations are in Matlab and are run on an Intel Pentium processor with 4 GB of RAM. Noting that the time axis is displayed on a log-scale, we can see that our HDLR algorithm scales roughly linearly with dimensionality, whereas existing full-rank methods scale quadratically as dimensionality increases. Fur-ther, LMNN ran out of memory when learning a 3000-dimensional metric. Running times of the HDILR method are comparable to those shown for the low-rank HDLR algorithm in Figure 3. Time in Seconds Figure 3: Running times of our high-dimensional algorithms compared to existing full-rank methods. Full-rank Maha-lanobis distance learning algorithms do not scale well to high dimensionality, whereas our method HDLR does.
Our text data sets are created by standard bag-of-words Tf-Idf representations. Words are stemmed using a standard Porter stem-mer and common stop words are removed. The text models are lim-ited to the 5,000 words with the largest document frequency counts. We provide experiments over two data sets: CMU X  X  20-newsgroup data set, and the Classic3 data set. Classic3 is a relatively small 3 class problem with 3,891 instances. The newsgroup data set is much larger, having 20 different classes from various newsgroup categories and 20,000 instances.

Figure 4 shows classification accuracy across various ranks for the Classic3 dataset, along with the full newsgroup data set and a subset of the data restricted to the three politics related classes: talk.politics.guns, talk.politics.mid east, and talk.politics.misc. Here, the diagonal plus low-rank method HDILR uses the class means basis as described in section 4.5. Comparing this to the baseline Euclidean measure, we can see that for low dimensionality, the accuracy of the two algorithms is similar, with HDILR having a slightly higher value. For larger ranks, the accuracy of the HDILR method slowly increases, while the accuracy of the low-rank HDLR class means method increases much more quickly. In fact, for the largest 20-class Newsgroup data set (b), we can see that for larger ranks, the HDLR method outperforms the HDILR method. Here, the HDLR method achieves accuracy 27% higher than the baseline Euclidean distance.

The HDILR and HDLR class means methods require full super-vision in order to form the low-rank basis. In semi-supervised set-tings, forming this basis from the similarity and dissimilarity con-straints used in our low-rank metric learning algorithms is not pos-sible and a low-rank LSA basis may be used instead. Recall that the LSA basis described in section 4.5 requires no supervision. In Figure 4, we see that our HDLR method outperforms the baseline unsupervised LSA method across all data sets for most dimensions. This is compared to LMNN using the same LSA basis U ,which generally performs only comparably to LSA.

Figure 5 shows recall-precision curves for four methods: stan-dard Euclidean distance Tf-Idf measure, LSA, and our two meth-ods using a class means basis. The rank used for LSA and our two methods is ten. We can see that for low recall values, both the Euclidean distance and the HDILR method achieve significantly higher precision values than the other two low-rank methods. As desired recall levels increase, the precision of the Euclidean dis-tance decreases rather quickly, while the HDILR continues to achieve higher precision values that are comparable to or better than the low-rank methods. For the Classic3 data set, HDILR outperforms all other methods for all recall values, marking an improvement over LSA of up to 20%. For the n ewsgroups Politic s subset, the HDILR method outperforms HDLR for low recall values, yet it achieves slightly worse precision for higher recall values.
We now present results from the Clarify system [8] which at-tempt to improve software error messaging via nearest neighbor software support. The basis of the Clarify system lies in the fact that modern software design promotes modularity and abstraction. When a program terminates abnormally, it is often unclear which component should be responsible for providing an error report. Clarify works by monitoring a set of predefined program features (the data sets used here represent function counts) during program run-time which are then analyzed in the event of abnormal program termination. In order to troubleshoot problems, Clarify uses near-est neighbor searches to find simila r failing executions from other users. Ideally, the neighbors returned should not only have the cor-rect class label, but should also represent those with similar pro-gram configurations or program inputs. The four data sets shown here are Mpg321 (an mp3 player, 4 classes, 128 dimensions), Fox-pro (a database manager, 4 classes, 12,670 dimensions), and two Linux kernel applications, Iptables and Iproute (having 4-5 classes and 50-250 dimensions). As described in [8], these data sets were created by running each program many times with various inputs exposing each of the error classes. Program features are collected from each program run and each run corresponds to a single train-ing instance.
 Figure 6 provides accuracy results for a k -NN classifier used Figure 6: Classification accuracy for four statistical soft-ware analysis datasets across six different algorithms: HDLR, HD 2 LR, a baseline class means method, latent semantic analy-sis (LSA), and the Euclidean distance. with our learned rank-10 distance metrics HDLR and HDILR . We compare this against four baseline methods. The class means method is a supervised method in which the class mean basis is used to parametrize a low-rank Mahalanobis distance without per-forming any additional learning. Confidence intervals intervals shown are computed for the 5 th and 95 th percentiles. Overall, we see that our HDLR and HDILR methods outperform the other four meth-ods.
Finally, we present experiments over a set of Yahoo song re-views. Here, 14,596 songs are reviewed by a total of 120,397 re-viewers. Each review has a score ranging from  X 1 X  (the reviewer does not like the song) to  X 5 X  (the reviewer liked the song). Further, each song is categorized into one of five genres: Rock, R&amp;B, Pop, Rap, and Country.

Many of today X  X  recommender systems work by performing near-est neighbor searches over such collaborative data in order to help users find similar songs, movies, or products to the ones that he or she already enjoys. Here, we consider the problem of learn-ing a distance function over the Yahoo song data that respects gen-res. Such a distance function is important as often people prefer songs from a limited number of genres, yet genre information is not known for all songs. In fact, the 14,596 labelled songs used in 0.2 0.4 0.6 0.8
Precision this data set represent a very small subset ( &lt; 1% ) of the entire set of all songs in the Yahoo music data set, most of which have the genre type  X  X nknown X .

Figure 7 shows classification accuracy for this data set for four different methods across a varying number of dimensions. Here, we see that the low-rank method HDLR (64.5% accuracy) performed significantly better than HDILR (53.9% accuracy). This data set is extremely sparse, with an average of 33 reviews per song (99.97% sparse). Accuracy Figure 7: Classification accuracy for distance functions of var-ious ranks for the Yahoo music data set. Here, the HDLR method significantly outperforms the HDILR method. This suggests that HDLR method may yield better results in prob-lems that are extremely sparse.
As seen in the previous section, the quality of a nearest neighbor search can be greatly improved by l earning an appropriate distance measure. In this paper, we have proposed formulations and algo-rithms that are well suited for learning metrics in high dimensional settings. Nearest neighbor searches are very simple algorithms, yet at the same time are quite flexible as they can be used for both clas-sification tasks as well as for standard information retrieval applica-tions. In this paper, we presented algorithms that provide both bet-ter classification performance in terms of accuracy, precision and recall over existing baseline methods.
 Acknowledgements We would like to thank Yahoo! for the use of their collaborative filtering data set. This research was supported by NSF grant CCF-0431257, NSF-ITR award IIS-0325116 and NSF grant IIS-0713142. [1] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [2] S. Boyd and L. Vandenberghe. Convex Optimization .
 [3] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. [4] S.C.Deerwester,S.T.Dumais,T.K.Landauer,G.W.
 [5] R.O.Duda,P.E.Hart,andD.G.Stork. Pattern [6] A. Globerson and S. Roweis. Metric Learning by Collapsing [7] G.H. Golub and C.F. Van Loan. Matrix Computations . Johns [8] J.Ha,C.Rossbach,J.Davis,I.Roy,D.Chen,H.Ramadan, [9] B. Kulis, M. Sustik, and I. S. Dhillon. Learning Low-rank [10] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance Metric [11] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance
