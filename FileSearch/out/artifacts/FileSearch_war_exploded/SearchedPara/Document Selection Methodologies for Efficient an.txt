 Learning-to-rank has attracted great attention in the IR community. Much thought and research has been placed on query-document feature extraction and development of so-phisticated learning-to-rank algorithms. However, relatively little research has been conducted on selecting documents for learning-to-rank data sets nor on the effect of these choices on the efficiency and effectiveness of learning-to-rank algo-rithms.

In this paper, we employ a number of document selection methodologies, widely used in the context of evaluation  X  depth-k pooling, sampling (infAP, statAP), active-learning (MTC), and on-line heuristics (hedge). Certain method-ologies, e.g. sampling and active-learning, have been shown to lead to efficient and effective evaluation. We investigate whether they can also enable efficient and effective learning-to-rank. We compare them with the document selection methodology used to create the LETOR datasets.

Further, all of the utilized methodologies are different in nature, and thus they construct training data sets with dif-ferent properties, such as the proportion of relevant docu-ments in the data or the similarity among them. We study how such properties affect the efficiency, effectiveness, and robustness of learning-to-rank collections.
 Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 In-formation Search and Retrieval:Retrieval models General Terms: Experimentation, Measurement, Theory Keywords: Learning-to-Rank, Document Selection Method-ologies, Sampling, Evaluation
Ranking is a central problem in information retrieval. Mod-ern search engines, especially those designed for the World Wide Web, commonly analyze and combine hundreds of features extracted from the submitted query and underly-ing documents in order to assess the relative relevance of a document to a given query and thus rank the underlying collection. The sheer size of this problem has led to the de-velopment of learning-to-rank algorithms that can automate the construction of such ranking functions: Given a training set of (feature vector, relevance) pairs, a machine learning procedure learns how to combine the query and document features in such a way so as to effectively assess the relevance of any document to any query and thus rank a collection in response to a user input.

Much thought and research has been placed on feature extraction and the development of sophisticated learning-to-rank algorithms. However, relatively little research has been conducted on the choice of queries and documents for learning-to-rank data sets nor on the effect of these choices on the ability of a learning-to-rank algorithm to  X  X earn X , ef-fectively and efficiently.

Constructing data sets for learning-to-rank tasks requires assembling a document corpus, selecting user information requests (queries), extracting features from query-document pairs and annotating documents in terms of their relevance to these queries (annotations are used as labels for training). Over the past decades, document corpora have been increas-ing in size from thousands of documents in the early TREC collections to billions of documents/pages in the World Wide Web. Due to the large size of document corpora it is prac-tically infeasible (1) to extract features from all document-query pairs, (2) to judge each document as relevant or irrel-evant to each query, and (3) to train learning-to-rank algo-rithms over such a vast data set.

The main bottleneck in constructing learning-to-rank col-lections is annotating documents with relevance grades. It is essential therefore, both for the efficiency of the construction methodology and for the efficiency of the training algorithm, that only a small subset of documents be selected. The doc-ument selection, though, should be done in a way that does not harm the effectiveness of learning.

LETOR [14] is the only attempt made to construct a publicly available learning-to-rank collection. Documents, queries and relevance judgments were obtained from the OHSUMED and TREC test collections. Since there are many documents in these collections, in order to reduce the computational effort required to extract features and train ranking functions over these data sets, only a subset of them was chosen in the following way: Documents were first ranked by their BM25 [12] score, which is known to correlate well with the relevance of a document to a query. Features then were extracted only from the corresponding top 1000 documents, in an effort to include as many rele-vant documents as possible in the learning-to-rank dataset. Features were also extracted from documents that were not ranked in this top 1000 but were judged as relevant in the corresponding TREC collections.

Even though LETOR has been widely used by many re-searchers, recent work demonstrated bias in this document selection methodology that could harm learning-to-rank al-gorithms [15, 17], hence the collection has been criticized. When the LETOR collection was built, the fact that docu-ments with low BM25 score were selected only if they were relevant resulted in BM25 being negatively correlated with relevance in the LETOR collection. This is a highly counter-intuitive outcome. To avoid the aforementioned implication, these extra documents with low BM25 scores were dropped in the latest LETOR release [13].

For the OHSUMED learning-to-rank collection only judged documents were selected for feature extraction. As pointed out by Minka and Robertson [15], this selection methodology results in an atypical proportion of relevant and non-relevant documents in the collection. Further, the nature of the non-relevant documents in the learning-to-rank collection is not representative of that in the entire OHSUMED collection.
These issues clearly manifest the effect a document se-lection methodology may have on the effectiveness of the learning-to-rank algorithms, and thus, on the performance of the resulting retrieval systems. Furthermore, the conclu-sions about the relative quality of different learning-to-rank algorithms may not be reliable.

Unlike document selection for learning-to-rank, where lit-tle work has been done, a significant volume of work has appeared in the literature regarding document selection for efficient and effective evaluation of retrieval systems [2, 5, 6, 21]. Most of these methods are based on sampling doc-uments in an intelligent way to minimize judgment effort and/or using statistical techniques to compute the values of traditional evaluation measures efficiently.

In this work, we explore the duality between document se-lection methodologies for evaluation and document selection methodologies for learning-to-rank. The main question we ask is :  X  X an any of the methodologies designed for efficient evaluation also be used for constructing effective learning collections? If yes, which one of these methods is better for this purpose? X 
We employ five different document selection methodolo-gies that are well studied in the context of evaluation, along with the method used in LETOR for comparison purposes. Subsets of documents are chosen according to the six meth-ods at different percentages of the complete document col-lection (in our case the depth-100 pool), and features are extracted from the selected query-document pairs. State of the art learning-to-rank algorithms are used then to train ranking functions over each one of the data sets the six se-lection methods have produced, and the resulting functions are compared with each other in terms of their performance.
In particular, (1) we explore whether certain document se-lection methodologies are better than others in terms of both efficiency and effectiveness; that is, how fast, in terms of doc-uments, can ranking functions learn to combine features in a meaningful way over the corresponding data sets such that their performance is not significantly worse than the per-formance of functions trained over the complete collection (depth-100 pool), and (2) we isolate certain characteristics of the selected subsets of documents (e.g. the percentage of relevant documents, or the similarity among relevant docu-ments in the subsets) and study their effect on the efficiency and effectiveness of learning.
In order to investigate the effect of document selection on the ability of learning-to-rank algorithms to effectively and efficiently learn a ranking function, five different document selection methodologies, widely used in retrieval evaluation, are studied.

Our complete document collection consists of the depth-100 pools from TREC 6, 7 and 8 adhoc tracks 1 . This collection consists of 150 queries in total, along with the corresponding relevance judgments. Features are extracted from all query-document pairs. Using different document se-lection methodologies, for each query, documents from the complete collection are selected with different percentages from 0.6% to 60%, forming different sized subsets of the complete collection for each methodology.

Features and relevance judgments pairs are then parti-tioned into five parts in order to conduct five-fold cross val-idation. For each fold, three parts are used for training, one part for validation and one part for testing. The documents in the training and validation sets are samples of the com-plete collection, as described above. The testing set consists of the complete set of documents.

State of the art learning-to-rank algorithms are then trained and the quality of the resulting ranking models is assessed by mean average precision (MAP).
The document corpus, the queries and the relevance judg-ments are obtained from TREC 6, 7 and 8 ad-hoc retrieval track.

The document corpus consists of approximately half a mil-lion documents (528,155) from the Financial Times, the Fed-eral Register, the Foreign Broadcast Information Service and the LA Times document collections [20].

The queries were developed by NIST assessors and they were chosen based on their estimated number of relevant documents in the corpus. Collectively, 150 such queries were developed. A number of retrieval systems were run over these queries and for each query the depth-100 pools of the returned documents were judged as relevant or irrelevant by the same NIST assessor that issued the query.

We extract features from all query-document pairs. The features extracted are shown in Table 2.1 and they are a subset of the LETOR3.0 features [13]. The description of these features along with their exact formulas can be found in the LETOR3.0 documentation [13]. Each feature is com-puted over the document text (without the title) and over the document text combined with the title, resulting in 22 features in total. Note that web features such as PageRank are not computed since the document corpus is not a web corpus. The language modeling features are implemented according to Zhai and Lafferty [22] while the BM25 feature is implemented according to Singhal [18].
In the sections that follow we use the  X  X epth-100 pools X  and  X  X omplete collection X  interchangeably sampling percentages.
For each query, six different document selection method-ologies are employed to choose documents from the complete collection:
When the properties of the above document selection method-ologies are considered, one can see that infAP creates a representative selection of documents, statAP and depth-k pooling aim at identifying more relevant documents uti-lizing the knowledge that retrieval systems return relevant documents at higher ranks, the LETOR-like method aims at selecting as many relevant documents according to BM25 as possible, hedge aims at selecting only relevant documents, and MTC greedily selects discriminative documents.
For all these strategies, the precision, computed as the ratio of the number of relevant documents in the document sample to the total number of documents in the document sample, and the recall, computed as the ratio of relevant documents in the document sample to the total number of relevant documents in the complete collection, are illus-trated in the left and right plots of Figure 1, respectively. As expected, hedge achieves the largest values of precision and recall, followed by MTC. Pooling and statAP follow by achieving similar values of precision and recall. Since infAP is based on uniform random sampling, the precision of in-fAP stays constant while the recall grows linearly with the sample percentage. The LETOR-like selection achieves both high precision and recall at small percentages of data used for training (up to 5%) and then it drops to the levels of statAP and depth pooling.

Further, the discrepancy among the selected relevant doc-uments, along with the discrepancy among the selected rel-evant and non-relevant documents for the different selec-tion methods is illustrated in Figures 2. The discrepancy is measured for each pair of documents by the symmetrized Kullback-Leibler divergence between the documents X  (smoothed) language models, then averaged over all pairs in a set. As it can be observed at the leftmost plot, for all methods except infAP, the selected documents are very similar to each other. For small percentages, it can be seen that the relevant doc-uments picked by hedge are very similar to each other. As more documents are selected according this algorithm, rele-vant documents with different properties can be identified. Depth-pooling and statAP select similar relevant documents due to the underlying retrieval systems that return simi-lar relevant documents at the top-end of their ranked lists, while hedge picks similar relevant documents by design. In particular, hedge selects very similar documents regardless of their relevance, as it can be observed in the right-most plot. At the other end of the discrepancy scale, infAP for small sampling percentages selects the most diverse relevant documents while it converges fast to the average discrep-ancy between documents in the complete collection. The LETOR-like selection methodology also selects very similar documents, since the documents selected are those that give high BM25 values and thus have similar characteristics.
We employ five different learning-to-rank algorithms to test the document selection methodologies, http://svmlight.joachims.org/svm perf.html
As a summary, RankBoost optimizes for pairwise pref-erences, Regression optimizes for classification error in the relevance judgments, and SVM optimizes for the area under the ROC curve. RankNet aims to optimize for the prob-ability that two documents are ranked in correct order in the ranking. Finally, LambdaRank directly optimizes for nDCG and even though the gradients are virtually defined, the method is shown to find the local optimum for the target metric.

All the algorithms, with the exception of Regression, are  X  X air-wise X  because they consider pairs of documents while training, either directly in the learning mechanism or indi-rectly in the loss function.
The performance of the learning-to-rank algorithms when trained over the different data sets produced by the six docu-ment selection methodologies is illustrated in Figure 3. The x -axis on the plots is the percentage of documents sampled from the complete document collection (depth-100 pool). The performance of the rankers ( y -axis) is measured by the mean average precision (MAP) of the ranked list of docu-ments returned by the rankers in response to the queries in the testing data sets. Each one of the document selection methods employed corresponds to a curve in the plot.
As one can observe in Figure 3, for most of the cases, the learning-to-rank algorithms reach almost optimal per-formance with as little training data as 1% of the complete collection. The Student X  X  t statistical test was employed in order to test whether the difference among the achieved MAP scores of the ranking function for different sampling percentages and the maximum MAP score the ranking func-tions obtain (MAP using full training data) are statistically significant. The t -test exhibits no significant differences for ranking functions trained over infAP, statAP, depth-pooling and MTC at any of document sampling percentage above 2%.

Therefore, training data sets whose sizes are as small as 1% to 2% of the complete collection are just as effective for learning-to-rank purposes as the complete collection. Thus, one can train much more efficiently over a smaller (though effectively equivalent) data set or at equal cost one can train over a far X  X arger X  X nd more representative data set either by increasing the number of queries of by selecting documents deeper in the rankings. Note that the number of features used by the learning-to-rank algorithms may well affect the efficiency of these algorithms to learn an effective ranking function [19]. In our experiments only twenty two (22) fea-tures where used, most of which are ranking functions of their own (e.g. BM25 or language models). Therefore, in the case where hundreds of (raw) features are employed, ranking functions may need more than 1% of the complete collection to achieve optimal performance. Nevertheless, in a setup similar to LETOR setup, as in our experiments, we show that substantially less documents than the ones used in LETOR can lead to similar performance of the trained ranking functions.

Furthermore, it is apparent from Figure 3 that the ef-fectiveness of small data sets for learning-to-rank purposes eminently depends on the document selection methodology employed. The most striking example of an inefficient docu-ment selection methodology is that of hedge. Ranking func-tions trained on data sets constructed according to the hedge methodology only reach their optimal performance when trained over data sets that are at least 20% of the com-plete collection, while in the worst case, the performance of some ranking functions is significantly lower than the opti-mal one even when trained over 40% to 50% of the complete collection (e.g. the performance of RankBoost, Regression and RankNet with the hidden layer).

Ranking functions exhibit their second worst performance when trained over data sets constructed according to the LETOR-like document selection methodology. Even though LETOR data sets have been widely used by researchers, our results show that the document selection methodology em-ployed in LETOR is neither the most effective nor the most efficient way to construct learning-to-rank collections.
The deficiency of learning-to-rank data sets produced ac-cording to hedge and LETOR-like document selection method-ologies may seem counterintuitive. One would expect rele-vant documents to be much more  X  X nformative X  than non-relevant documents for the purpose of learning-to-rank, and both hedge and LETOR-like document selection methodolo-gies are designed to choose as many relevant documents as possible. Clearly this is not the case according to our results.
In what follows, we try to give an explanation of these counterintuitive. Figure 4 illustrates the performance of two of the learning-to-rank algorithms (SVM and Rank-Boost) for which data sets created according to hedge and LETOR-like methods seem the least effective. The y -axis corresponds to the performance of the ranking functions. The x -axis in the top-row plots corresponds to the percent-age of relevant documents in the learning-to-rank data sets, while at the bottom-row plots, it corresponds to the dis-crepancy among the selected relevant and non-relevant doc-uments. Each dot in these plots corresponds to a training data set at some sampling percentage, regardless of the doc-ument selection algorithm employed. As can be observed, there is a clear negative correlation between the percentage of relevant documents (above a certain threshold) and the performance of both ranking functions. Further, a strong positive correlation is exhibited between the dissimilarity among relevant and non-relevant documents and the perfor-mance of the two algorithms. This is a strong indication that over-representation of relevant documents in the training data sets may harm the learning-to-rank algorithms. Fur-thermore, when relevant and non-relevant documents in the training data set are very similar to each other, performance of the resulting ranking functions decline.

Both hedge and LETOR-like document selection method-ology, by design, select as many relevant documents as pos-sible. As shown in Figure 2, the documents selected by the two methods also exhibit very high similarity to each other.
In contrast to the aforementioned selection methodologies that are designed to select as many relevant documents as possible, infAP, statAP, depth-pooling and MTC tend to construct data sets that are more representative of the com-plete collection. In particular, infAP randomly samples the complete collection, and thus, all documents in the collec-tion have equal probability to be included in the training data set, regardless of their relevance grade. Even though depth-pooling tends to select documents from the top end of the ranked lists of the underlying participating systems, it treats all systems in a fair manner regardless of their qual-ity, giving poor underlying systems an equal opportunity to contribute to the constructed data sets. Thus, many non-relevant documents are included in the resulting training sets. StatAP selects the documents that are commonly re-trieved by multiple systems due to the way the sampling prior is computed. Hence, the quality of the sampled docu-ments depend on the quality of the underlying systems. If a non-relevant document is retrieved by many retrieval sys-tems, then this document is still very likely to be sampled by the statAP sampling. Finally, MTC, by design, selects the most informative documents for the purpose of evaluation, regardless of their relevance.

Therefore, the percentage of relevant documents in the learning-to-rank data sets along with the similarity of rel-evant and non-relevant documents appear to be good ex-planatory parameters for the efficiency and effectiveness of all the aforementioned document selection methodologies.
In order to quantify the explanatory power of these two parameters, we formulate a linear model, with the perfor-mance of RankBoost and SVM as measured by MAP over the testing data sets expressed as a linear function of the per-centage of relevant documents and the dissimilarity among relevant and non-relevant documents in the data sets. Both the adjusted R 2 and the F-statistic of the resulting linear models indicate an excellent goodness of fit, with the former being equal to 0 . 99 and with the p -value of the latter being
Further, to assess the relative importance of the two ex-planatory parameters, with respect to the performance of the learning-to-rank algorithms, we also fit an ANOVA model to the data and performed a variance decomposition analy-sis, according to which the percentage of relevant documents accounts for about 60% of the variance in the MAP scores across all data sets and the discrepancy between relevant and non-relevant documents accounts for about 39%. Therefore, we conclude that both the proportion of relevant documents and the dissimilarity between documents are highly impor-tant for the quality of a learning-to-rank collection, with the former affecting the quality more than the latter. 3
Finally, by revisiting Figure 3, one can observe that some learning-to-rank algorithms are more robust to document
Note that we have tested the effect of other explanatory parameters to the variability of MAP values (e.g. recall, total number of documents, total number of relevant documents, similarity between relevant documents, interactions between these parameters). None of these parameters appeared to be significant. selection methodologies than others. In particular, Lamb-daRank and RankNet seem to be more robust than Regres-sion, RankBoost and Ranking SVM. To assess the relative importance between the learning-to-rank algorithms X  effect on MAP and the selection methodologies X  effect on MAP, we fit a 2-way ANOVA model to the MAP values and again perform a variance decomposition. According to ANOVA 26% of the variance in the MAP scores is due to the selec-tion methodology and 31% due to the learning-to-rank algo-rithm, while 5% is due to the algorithm-selection method-ology interaction. When we perform the same analysis to MAP values over datasets up to 10% of the complete collec-tion, then 44% of the variance in the MAP scores is due to the selection methodology and 23% is due to the learning-to-rank algorithm, while 10% is due to the algorithm-selection methodology interaction.
In this paper, we analyzed the problem of building docu-ment collections for efficient and effective learning-to-rank. In particular, we explored (1) whether the algorithms that are good for reducing the judgment effort for efficient evalu-ation are also good for reducing judgment effort for efficient learning-to-rank and (2) what makes one learning-to-rank collection better than another.

For this purpose we constructed different sized learning collections employing depth pooling, infAP, statAP, MTC, hedge and the LETOR methodology. We then ran a number of state of the art learning-to-rank algorithms over these training collection and compared the quality of the different methods used to create the collections based on the relative performance of the learning algorithms.

We showed that some of these methods (infAP, statAP and depth pooling) are better than others (hedge and the LETOR method) for building efficient and effective learning-to-rank collections. This is a rather surprising result given the wide usage of the LETOR datasets as it suggests that using the same judgment effort, better collections could be created via other methods.

Furthermore, we showed that both (1) the proportion of relevant documents to non-relevant documents and (2) the similarity between relevant and non-relevant documents in the data sets highly affect the quality of the learning-to-rank collections, with the latter having more impact.

Finally, we observed that some learning-to-rank algorithms (RankNet and LambdaRank) are more robust to document selection methodologies than others (Regression, RankBoost and Ranking SVM).
 This material is based upon work supported by the National Science Foundation under Grant No IIS-0533625 and IIS-0534482. Any opinions, findings, and conclusions or rec-ommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Na-tional Science Foundation.
