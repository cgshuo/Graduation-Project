 The paper is focused on blogosphere research based on the TREC blog distillation task, and aims to explore unbiased and significant features automatically and efficiently. Feedback from faceted feeds is introduced to harvest relevant features and information gain is used to select discrimina tive features. The evaluation result shows that the selected feedback features can greatly improve the performance and adapt well to the terabyte data. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval-Search Process Design, Experimentation Blog Distillation, Faceted Distillation, Feedback With the accelerated growth of social network, both organizations and individuals have shown great interest in conveying or exchanging ideas and opinions. The blogosphere provides an ideal platform for communication. Acco rding to the statistics of Blogpulse ( blogpulse.com ) in Jan. 2011, more than 152 million blogs have been published (more than 47,000 blog posts per day). One interesting issue related to the massive blogs is to automatically explore authors X  behaviors from their blog posts. Focusing on this issue, TREC introduces the blog distillation track in 2009 with two subtasks: ba seline distillation and faceted distillation. The purpose of the former is to retrieve all the relevant feeds corresponding to given topics without any consideration of facets. The latter aims to rerank the baseline feeds according to specific facets [1]. At present, most published work deals with the two subtasks in sequence. Retrieving the relevant feeds according to given queries is regarded as a common IR task. For faceted distillation, several methods have been attempted. In [3], SVM and ME classifiers are introduced to predict the faceted inclinations of each feed according to pre-trained models. In [2], feed faceted scores are heuristically given to re-rank feed s. For classification as well as re-ranking, the challenge is to select the features related to each inclination. Most work above focuses on exploring heuristic features like permalink numbers or comment numbers. However, we observe that for some facets these features are far from enough. In view of this, we first discov er more (non-heuristic) feature candidates with faceted feedback information. Then, a feature selection approach is employed to select discriminative features. Furthermore, we take some flex ible processing to adapt to the massive dataset. In a word, we believe this can help us to explore the significant features automatically and efficiently. In the remainder of this paper, more deta ils of our approach are given in Section 2; then the evaluation of baseline and faceted distillations is presented in Section 3; Sec tion 4 concludes the whole paper. Since the faceted blog distillation is based on the baseline distillation, we briefly introduce the latter first. To enhance efficiency in the face of the hug e and noisy raw data (2.3TB), we implemented a distributed system and adopted the Indri tool ( www.lemurproject.org ) for our purpose, with its default language model and related toolkits. With th e help of these tools, the top related feeds can be retrieved according to given topics in the baseline distillation. Based on the ranking of the baseline distillation, we then focus on the faceted blog distillation subtask. The TREC 2010 task specifies three inclination pairs: opinionated vs. factual, personal vs. official, and in-depth vs. shallo w, for operational simplicity [1]. According to those inclinations, the baseline feeds are to be re-discriminative features for each faceted inclination, and determine the weight of each feature. To solve the issue above, our approach explores features from three orientations: heuristic fe atures, available lexicons, and corpora. Heuristic features, which have been used in some existing work, include Average Permalink/Sentence Length, Comment number, Organization Nu mbers, Opinion Rule, etc, which can be helpful for distinguishing some inclinations. In our method, besides these heuristic feat ures we also use the statistics of the presence of Cyber Word s and Cyber Emoticons (like  X  LOL  X  and  X   X   X ) in feeds, which provides clues to personal and official feeds. To discover the lexic on features, the SentiWordNet ( sentiwordnet.isti.cnr.it ) and Wilson Lexicon are used to manually select about 1500 opinion words as our own opinion lexicon. These two lexicons are vital in identifying the opinionated inclination feeds. However, most of these features suit the opinionated inclination and ma y not work well in other inclinations, and may introduce noise for other inclinations, especially for the factual and shallow inclinations. Thus, in order to discover more relevant features , we take the effort to explore some useful features from corpora. Here, we propose a feature expansi on approach by learning more feature candidates through feedback information of faceted feeds. Since TREC has released some faceted feeds for each topic, we can select some faceted-related word unigrams in the top ranking faceted feeds as new feature candidates according to the official release. These features are mainly opinion-independent words and unbiased for particular inclinati ons, resulting in more balanced feature structure. A byproduct of feature expansion is that the unprocessed feature candidates contain too much usel ess information, which not only wastes computing resources bu t also harms the performance. Therefore, we need to select the top discriminative features with feature selection methods. For fast and efficient handling of large and noisy data, the filter approach, rather than wrapper approaches is adopted [4]. With lexicon-based features and feedback features, an unanswered question is how to determine the weight of both features. Though each opinion word has a polarity weight and a feature selection measure is assigned to each feedback term, these weights are not in the same scale. To unitize the weights of selected features, for each inclination we conduct a SVM classifier with a linear kernel and calculate the weight of a support vector from the trained model that corresponds to a featur e. The feeds are re-ranked with the summation of the products of the feature values and their weights. Our experiment is conducted on the blog08 collection crawled over 14 weeks. It contains pe rmalinks, feeds, and related information. The size of the blog08 collection is up to 2.3TB. In order to efficiently handle the terabyte dataset and reduce noise, the raw dataset is first cleaned and filtered by several regular expression rules, which reduce the size to 30%. Then, Indri is used to index the cleaned blog08 collection, and fetch the top 2000 related blog posts according to the 50 topics provided in TREC2009. Since feeds are what the task is concerned with, we rank the feeds by summing the releva nce scores of retrieved blog posts corresponding to the same feed number. The top 100 relevant feeds are obtained and evaluated in table 1, and TREC provides four measures: the mean average precision (MAP), R-Precision (rPrec), binary Preference (bPref) and Precision at 10 documents (P@10), among which MAP is the primary measure for evaluating the retrieval performance [1]. 
Table 1: Evaluation of baseline distillation and comparison As shown in table 1, our Indri-based language model run ranks competitively against official submissions. Based on our baseline feed ranking, we conduct th e faceted distillation. We first collect the lexicon features and the feedback features from the top ranking five feeds as feature candidates for feature selection. There are several co mmonly used feature selection approaches, and according to [4], information gain (IG) selects some negative features, but also has a bias for positive features. Thus, in our experiment IG is used to select features, and the formula is as follows: entropy; , X  X  X  X  X   X  the set of all attributes. Instead of using all instances in the official released answers, we calculate H ( Ex | a ) using the top five feeds in our experiment. The change can greatly reduce the complexity of computing and make our approach more adaptable for the massive data collection. The top five feeds are a good surrogate for the whole feed set as they are statistically found to contain an approximately equal number of faceted and non-faceted feeds. More importantly, this  X  X hortcut approach X  adapts very well to the large dataset. We select the top 2,000 features (about 10% features in all feature candidates). To determine the weights of these features, an SVM classifier is conducted with these features for each inclination. We use the same strategy to randomly divide the top five feeds into training and testing datasets (ratio 4:1). Then, the weights of support vectors are calculated from traine d models as the weights of these features for facet re-ranking. With selected features and their weights, feeds are re-ranked according to each inclination, and for comparison, ranking without feedback features (W/O Feedback) are evaluated as well. From the evaluation (Table 2), co mpared with re-ranking without feedback features (W/O Feedback), re-ranking with feedback features (Feedback+IG) achieves a significant improvement, and outperforms the best of official r uns. From the evaluation against each inclination, we can find that great improvements are observed for factual, personal and offi cial identification. It is thus plausible that those inclinations may be more amenable to the usage of words rather than some heuristic features. One exception is the degradation of the shallow inclination performance, which may suggest that information gain is not suitable for selecting discriminative features and th us introduces noise for shallow inclination. In the last experiment, comparisons are made to investigate the influence of different numbers of features selected. Figure 1 shows that the with-feedback (W/I feedback) and without-feedback (W/O feedback) approaches peak at 500 features and 50 features, respectiv ely. The flat ta il of without-feedback approach can be explai ned by the fact that only about 750 out of the 1500 features (shown by the points in the circle) are present in the features. The bottom line shown in this figure is that re-ranking with feedback featur e outperforms the performance of that without feedback. Thus, this proves that feedback features are obviously efficient in faceted blog distillation. To sum up, feedback feature e xpansion coupled with feature selection is effective and efficient for faceted blog distillation and adapts well to the terabyte dataset. It helps to automatically discover relevant and discriminative features. In the future, besides expanding features with unigrams, a possible extension is to learn some term combinations from unigrams. The work presented in this paper is supported by a Hong Kong RGC project (No. PolyU 5230/08E). [1] http://ir.dcs.gla.ac.uk/wiki/TREC-BLOG. [2] Mostafa Keikha, Mark Carman, et al 2009. University of Lugano at [3] Richard McCreadie, Craig Macdonald, Iadh Ounis, et al 2009. [4] Hua Liu, Hiroshi Motoda 2007. Computational Methods of Feature 
