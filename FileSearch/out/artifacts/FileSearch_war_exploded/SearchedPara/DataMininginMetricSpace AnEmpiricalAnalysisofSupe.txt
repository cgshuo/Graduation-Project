 Man y criteria can be used to evaluate the performance of sup ervised learning. Di eren t criteria are appropriate in di eren t settings, and it is not alw ays clear whic h criteria to use. A further complication is that learning metho ds that perform well on one criterion may not perform well on other criteria. For example, SVMs and boosting are de-signed to optimize accuracy , whereas neural nets typically optimize squared error or cross entrop y. We conducted an empirical study using a variet y of learning metho ds (SVMs, neural nets, k-nearest neigh bor, bagged and boosted trees, and boosted stumps) to compare nine boolean classi ca-tion performance metrics: Accuracy , Lift, F-Score, Area under the ROC Curv e, Average Precision, Precision/Recall Break-Ev en Point, Squared Error, Cross Entrop y, and Prob-abilit y Calibration. Multidimensional scaling (MDS) sho ws that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are in-terpreted as probabilities: squared error, cross entrop y, and calibration, lay in one part of metric space far away from metrics that dep end on the relativ e order of the predicted values: ROC area, average precision, break-ev en point, and lift. In between them fall two metrics that dep end on com-paring predictions to a threshold: accuracy and F-score. As exp ected, maxim um margin metho ds suc h as SVMs and boosted trees have excellen t performance on metrics like ac-curacy , but perform poorly on probabilit y metrics suc h as squared error. What was not exp ected was that the mar-gin metho ds have excellen t performance on ordering metrics suc h as ROC area and average precision. We introduce a new metric, SAR, that com bines squared error, accuracy , and ROC area into one metric. MDS and correlation anal-ysis sho ws that SAR is cen trally located and correlates well with other metrics, suggesting that it is a good general pur-pose metric to use when more speci c criteria are not kno wn. Categories &amp; Subject Descriptors: I.5.2 [Pattern Recog-nition]: Design Metho dology -classi er design &amp; evaluation. General Terms: Algorithms, Measuremen t, Performance, Exp erimen tation.
 Keyw ords: Sup ervised Learning, Performance Evaluation, Metrics, ROC, Precision, Recall, Lift, Cross Entrop y.
In sup ervised learning, nding a mo del that could predict the true underlying probabilit y for eac h test case would be optimal. We refer to suc h an ideal mo del as the One True Model . Any reasonable performance metric should be opti-mized (in exp ectation, at least) by the one true mo del, and no other mo del should yield performance better than it.
Unfortunately , we usually do not kno w how to train mo d-els to predict the true underlying probabilities. The one true mo del is not easy to learn. Either the correct paramet-ric mo del type for the domain is not kno wn, or the training sample is too small for the mo del parameters to be esti-mated accurately , or there is noise in the data. Typically , all of these problems occur together to varying degrees.
Even if magically the one true mo del were given to us, we would have dicult y selecting it from other less true mo dels. We do not have performance metrics that will reliably assign best performance to the probabilistically true mo del given nite validation data.

In practice, we train mo dels to minimize loss measured via a speci c performance metric. Since we don't have metrics that could reliably select the one true mo del, we must ac-cept the fact that the mo del(s) we select will necessarily be sub optimal. There may be only one true mo del, but there are many sub optimal mo dels.

There are di eren t ways that sub optimal mo dels can dif-fer from the one true mo del { tradeo s can be made between di eren t kinds of deviation from the one true mo del. Di er-ent performance metrics re ect these di eren t tradeo s. For example, ordering metrics suc h as area under the ROC curv e and average precision do not care if the predicted values are near the true probabilities, but dep end only on the rela-tive size of the values. Dividing all predictions by ten does not change the ROC curv e, and metrics based on the ROC curv e are insensitiv e to this kind of deviation from truth. Metrics suc h as squared error and cross entrop y, however, are greatly a ected by scaling the predicted values, but are less a ected by small changes in predicted values that migh t alter the relativ e ordering but not signi can tly change the deviation from the target values. Squared error and cross entrop y re ect very di eren t tradeo s than metrics based on the ROC curv e. Similarly , metrics suc h as accuracy de-pend on how the predicted values fall relativ e to a threshold. If predicted values are rescaled, accuracy will be una ected if the threshold also is rescaled. But if small changes to predicted values are made for cases near the threshold, this can have large impact on accuracy . Accuracy re ects yet another tradeo in how deviation from truth is measured.
The one true mo del, if available, would have (in exp ecta-tion) the best accuracy , the best ROC curv e, and the best cross entrop y, and the di eren t tradeo s made by these met-rics would not be imp ortan t. But once we accept that we will not be able to nd the one true mo del, and must there-fore accept sub optimal mo dels, the di eren t tradeo s made by di eren t performance metrics become interesting and im-portan t. Unfortunately , little is kno wn about how di eren t performance metrics compare to eac h other.

In this pap er we presen t results from an empirical anal-ysis of nine widely used performance metrics. We perform this empirical comparison using mo dels trained with sev en learning algorithms: SVMs, neural nets, k-nearest neigh-bor, bagged and boosted trees, and boosted stumps. We use multidimensional scaling (MDS) and correlation analy-sis to interpret the results. We also examine whic h learning metho ds perform best on the di eren t metrics. Finally , we introduce a new metric, SAR, that com bines squared error, accuracy , and ROC area into a single, robust metric.
We exp erimen t with nine performance metrics for boolean classi cation: Accuracy (ACC), Lift (LFT), F-Score (FSC), Area under the ROC Curv e (AUC), Average Precision (APR), the Precision/Recall Break-Ev en Point (BEP), Root Mean Squared Error (RMS), Mean Cross Entrop y (MXE), and Probabilit y Calibration (CAL). De nitions for eac h of the metrics can be found in App endix A.

Figure 1 sho ws level curv es for six of the ten performance metrics for a mo del with only two parameters ( W 1 and W 2) trained on a simple syn thetic binary problem. Peak perfor-mance in the rst two plots occurs along a ridge in weigh t space. In the other four plots peak performance is indicated by solid dots. Peak performance for some metrics nearly coincide: RMS and MXE peak at nearly the same mo del weigh ts. But other metrics peak in di eren t places: CAL has a local optim um near the optima for RMS and MXE, but its global optim um is in a di eren t place. Also, the ridges for optimal ACC and optimal AUC do not align, and the ridges do not cross the optima for the other four metrics. Optimizing to eac h of these metrics yields di eren t mo dels, eac h represen ting di eren t tradeo s in the kinds of errors the mo dels mak e. Whic h of these tradeo s is best dep ends on the problem, the learning algorithm, and how the mo del predictions ultimately will be used.

We originally divided the nine metrics into three groups: threshold metrics, ordering/rank metrics, and probabilit y metrics. The three threshold metrics are accuracy (ACC), F-score (FSC) and lift (LFT). F-score is the harmonic mean of precision and recall at some threshold. Lift measures the true positiv e rate in the fraction of cases that fall above threshold. (See App endix A for a de nition of lift, and [3] for a description of Lift Curv es. Lift is the same as precision at some threshold, but scaled so that it can be larger than 1.) Usually ACC and FSC use a xed threshold. In this pap er we use 0.5. With lift, often the threshold is adjusted so that a xed percen t, p , of cases are predicted as positiv e, the rest falling below threshold. Usually p dep ends on the problem. For example, in mark eting one migh t want to send iers to 10% of customers. Here we somewhat arbitrarily set p = 25% for all problems. Note that for all threshold metrics it is not imp ortan t how close a prediction is too a threshold, only if the predicted value is above or below threshold.
The ordering/rank metrics look at predictions di eren tly from the threshold metrics. If cases are ordered by predicted value, the ordering/rank metrics measure how well the or-dering ranks positiv e cases above negativ e cases. The rank metrics can be view ed as a summary of the performance of a mo del across all possible thresholds. The rank metrics we use are area under the ROC curv e (AUC), average precision (APR), and precision/recall break even point (BEP). See [10] for a discussion of ROC curv es from a mac hine learn-ing persp ectiv e. Rank metrics dep end only on the ordering of the predictions, not the actual predicted values. If the ordering is preserv ed it mak es no di erence if the predicted values range between 0 and 1 or between 0.29 and 0.31. Although we group Lift with the threshold metrics, and BEP with the ordering metrics, BEP and Lift are similar to eac h other in some resp ects. Lift is directly prop ortional to BEP if Lift is calculated at p equal to the prop ortion of pos-itiv es in the data set. This threshold also is the break-ev en point where precision equals recall. BEP and Lift are sim-ilar to the ordering metrics because the threshold dep ends implicitly on the ordering, but also are similar to the thresh-old metrics because neither is sensitiv e to the orderings on either side of the threshold once that threshold has been de ned. Results presen ted later suggest that both Lift and BEP are more similar to the ordering metrics than to the threshold metrics.

The three probabilit y metrics dep end on the predicted val-ues, not on how the values fall relativ e to a threshold or rela-tive to eac h other. The probabilit y metrics are uniquely min-imized (in exp ectation) when the predicted value for eac h case coincides with the true probabilit y of that case being positiv e. The probabilit y metrics we consider are squared error (RMS), cross entrop y (MXE) and calibration (CAL). CAL measures the calibration of a mo del: if a mo del predicts 0.85 for a large num ber of cases, about 85% of those cases should pro ve to be positiv e if the mo del is well calibrated. See App endix A for details of how CAL is calculated.
We also exp erimen t with a new performance metric, SAR, that com bines squared error, accuracy , and ROC area into one measure: SAR = ( AC C + AU C + (1 RM S )) = 3. SAR beha ves somewhat di eren tly from ACC, AUC, and RMS alone, and is a robust metric to use when the correct metric is unkno wn. SAR is discussed further in Section 8.
Performance metrics suc h as accuracy or squared error have range [0 ; 1], while others (lift, cross entrop y) range from 0 to q where q dep ends on the data set. For some metrics lower values indicate better performance. For others higher values are better. Metrics suc h as ROC area have baseline rates that are indep enden t of the data, while others suc h as accuracy have baseline rates that dep end on the data. If baseline accuracy is 0.98, an accuracy of 0.981 probably is not good performance, yet on another problem, if the Bayes optimal rate is 0.60, achieving an accuracy of 0.59 migh t be excellen t performance.

In order to compare performance metrics in a meaningful way, all the metrics need to be placed on a similar scale. One way to do this is to scale the performances for eac h problem and metric from 0 to 1, where 0 is poor performance, and 1 is good performance. For example, we migh t place baseline performance at 0, and the Bayes optimal performance at 1. Unfortunately , we cannot estimate the Bayes optimal rate on real problems. Instead, we can use the performance of the best observ ed mo del as a pro xy for the Bayes optimal performance. We calculate baseline rate as follo ws: predict p for every case, where p is the percen t of positiv es in the test set. We normalize performances to the range [0 ; 1], where 0 is baseline and 1 represen ts best performance. If a mo del performs worse than baseline, its normalized score will be negativ e. See Table 1 for an example of normalized scores. The disadv antage of normalized scores is that reco vering the raw performances requires kno wing the performances that de ne the top and bottom of the scale, and as new best mo dels are found the top of the scale changes.

CAL, the metric we use to measure probabilit y calibra-tion, is unusual in that the baseline mo del that predicts p for all cases, where p is the percen t of positiv es in the test set, has excellen t calibration. (Because of this, measures like CAL typically are not used alone, but are used in conjunc-tion with other measures suc h as AUC to insure that only mo dels with good discrimination and good calibration are selected. See Figure 1 for a picture of how unusual CAL's error surface is compared with other metrics.) This creates a problem when normalizing CAL scores because the baseline mo del and Bayes optimal mo del have similar CAL scores. This does not mean CAL is a poor metric { it is e ectiv e at distinguishing poorly calibrated mo dels from well calibrated mo dels. We address this problem later in the pap er.
The goal of this work is to analyze how the ten metrics compare to eac h other. To do this we train man y di eren t kinds of mo dels on sev en test problems, and calculate for eac h test problem the performance of every mo del on the ten metrics.

We train mo dels using sev en learning algorithms: Neu-ral Nets (ANN), SVMs, Bagged Decision Trees (BA G-DT), Boosted Decision Trees (BST-DT), Boosted Decision Stumps (BST-STMP), single Decision Trees (DT) and Memory Based Learning (KNN). For eac h algorithm we train man y varian ts and man y parameter settings. For example, we train ten styles of decision trees, neural nets of di eren t sizes, SVMs using man y di eren t kernels, etc. A total of 2000 mo dels are trained and tested on eac h problem. See App endix B for a description of the parameter settings we use for eac h learn-ing metho d. While this strategy won't create every possible mo del, and won't create a uniform sample of the space of possible mo dels, we feel that this is an adequate sample of the mo dels that often will be trained in practice.
For eac h problem, the 2000 mo dels are trained on the same train set of 4000 points. The performance of eac h mo del is measured on the same large test set for eac h of the ten performance metrics. In order put the performances on the same scale across di eren t metrics and di eren t problems, we transform the raw performance to normalized scores as explained in Section 3. In total, across the sev en problems, we have 2000 7 = 14 ; 000 mo dels and for eac h mo del we have it's score on eac h of the 10 performances metrics.
We compare the algorithms on sev en binary classi cation problems. ADUL T, CO VER TYPE and LETTER are from UCI Rep ository [1]. ADUL T is the only problem that has nominal attributes. For ANNs, SVMs and KNNs we trans-form nominal attributes to boolean. Eac h DT, BA G-DT, BST-DT and BST-STMP mo del is trained twice, once with the transformed attributes and once with the original at-tributes. CO VER TYPE has been con verted to a binary problem by treating the largest class as the positiv e and the rest as negativ e. We con verted LETTER to boolean in two ways. LETTER.p1 treats the letter "O" as positiv e and the remaining 25 letters as negativ e, yielding a very unbalanced binary problem. LETTER.p2 uses letters A-M as positiv es and the rest as negativ es, yielding a well balanced problem. HYPER SPECT is the IndianPine92 data set [4] where the dicult class Soybean-min till is the positiv e class. SLA C is a problem from collab orators at the Stanford Linear Accel-erator and MEDIS is a medical data set. The characteristics of these data sets are summarized in Table 2.

Training 2000 mo dels on eac h problem using sev en learn-ing algorithms gives us 14,000 mo dels, eac h of whic h is eval-uated on ten performance metrics. This gives us 14,000 sample points to compare for eac h performance metric. We build a 10x14,000 table where lines represen t the perfor-mance metrics, columns represen t the mo dels, and eac h en-try in the table is the score of the mo del on that metric. For MDS, we treat eac h row in the table as the coordinate of a point in a 14,000 dimension space. The distance be-tween two metrics is calculated as the Euclidean distance between the two corresp onding points in this space. Be-cause the coordinates are strongly correlated, there is no curse-of-dimensionalit y problem with Euclidean distance in this 14,000 dimensional space.

We are more interested in how the metrics compare to eac h other when mo dels have good performance than when mo dels have poor performance. Because of this, we delete columns represen ting poorer performing mo dels in order to focus on the \interesting" part of the space where mo dels that have good performance lie. For the analyses rep orted in this pap er we delete mo dels that perform below baseline on any metric (except CAL).
 Ten metrics permits 10 9 = 2 = 45 pairwise comparisons. We calculate Euclidean distance between eac h pair of met-rics in the sample space, and then perform multidimensional scaling on these pairwise distances between metrics. MDS is sensitiv e to how the performance metrics are scaled. The normalized scores describ ed in Section 3 yield well-scaled performances suitable for MDS analysis for most met-rics. Unfortunately , as discussed in Section 3, normalized scores do not work well with CAL. Because of this, we per-form MDS two ways. In the rst, we use normalized scores, but exclude the CAL metric. In the second, we include CAL, but scale performances to mean 0.0 and standard deviation 1.0 instead of using normalized scores. Scaling by standard deviation resolv es the problem with CAL for MDS, but is somewhat less intuitiv e because scores scaled by standard deviation dep end on the full distribution of mo dels instead of just the performances that fall at the top and bottom of eac h scale.

Figure 2 sho ws the MDS stress as a function of the num ber of dimensions in the MDS (when CAL is included). The ten metrics app ear to span an MDS space of about 3 to 5 dimensions. In this section we examine the 2-D MDS plots in some detail.

Figure 3 sho ws two MDS plots for the metrics that result when dimensionalit y is reduced to two dimensions. The plot on the left is MDS using normalized scores when CAL is excluded. The plot on the righ t is MDS using standard deviation scaled scores when CAL is included.

Both MDS plots sho w a similar pattern. The metrics ap-pear to form 4-5 somewhat distinct groups. In the upp er righ t hand corner is a group that includes AUC, APR, BEP , LFT, and SAR. The other groups are RMS and MXE, ACC (by itself, or possibly with FSC), FSC (by itself, or possibly with ACC), and CAL (by itself ). It is not surprising that squared error and cross entrop y form a cluster. Also, pre-sumably because squared error tends to be better beha ved than cross entrop y, RMS is closer to the other measures than MXE. We are somewhat surprised that RMS is so cen trally located in the MDS plots. Perhaps this partially explains why squared error has pro ved so useful in man y applications. Figure 2: MDS stress vs. number of dimensions
It is somewhat surprising that accuracy does not app ear to correlate strongly with any of the other metrics, except possibly with FSC. ACC does not fall very close to other metrics that use thresholds suc h as Lift and F-Score, even though F-Score uses the same 0.5 threshold as accuracy in our exp erimen ts. (The threshold for Lift is adjusted dynam-ically so that 25% of the cases are predicted as positiv e.) Accuracy is surprisingly close to RMS, and closer to RMS than to MXE, again suggesting that part of the reason why RMS has been so useful is because of its close relationship to a metric suc h as ACC that has been so widely used.
The most surprising pattern in the MDS plot that in-cludes CAL is that CAL is distan t from most other met-rics. There app ears to be an axis running from CAL at one end to the ordering metrics suc h as AUC and APR at the other end that forms the largest dimension in the space. This is surprising because one way to achiev e ex-cellen t ordering is to accurately predict true probabilities, whic h is measured by the calibration metric. However, one can achiev e excellen t AUC and APR using predicted values that have extremely poor calibration, yet accurately predict the relativ e ordering of the cases. The MDS plot suggests that man y mo dels whic h achiev e excellen t ordering do so without achieving good probabilistic calibration. Closer ex-amination sho ws that some mo dels suc h as boosted decision trees yield remark ably good ordering, yet have extremely poor calibration. We believ e maxim um margin metho ds suc h as boosting tradeo reduced calibration for better mar-gin. See Section 9 for further discussion of this issue. One also can achiev e good calibration, yet have poor AUC and APR. For example, decision trees with few leaves may be well calibrated, but the coarse set of values they predict do not pro vide a basis for good ordering.

Figure 4 sho ws 2-D MDS plots for six of the sev en test problems. The sev enth plot is similar and is omitted to save space. (The omitted plot is one of the two LETTER problems.) Although there are variations between the plots, the 2-D MDS plots for the sev en problems are remark ably consisten t given that these are di eren t test problems. The consistency between the sev en MDS plots suggests that we have an adequate sample size of mo dels to reliably detect re-lationships between the metrics. Metrics suc h as ACC, FSC, and LFT seem to move around with resp ect to eac h other in these plots. This may be because they have di eren t sensi-tivities to the ratio of positiv es to negativ es in the data sets. For example, BEP is proprtional to LFT (and thus beha ves similarly) when the percen tage of positiv es in the dataset equals the fraction predicted above threshold (25% in this pap er). Other than this, we have not been able to correlate di erences we see in the individual plots with characteris-tics of the problems that migh t explain those di erences, and curren tly believ e that the MDS plots that com bine all sev en problems in Figure 3 represen ts an accurate summary of the relationships between metrics. Note that this does not mean that the performance of the di eren t learning al-gorithms exhibits the same pattern on these test problems (in fact they are very di eren t), only that the relationships between the ten metrics app ear to be similar across the test problems when all the learning algorithms are considered at one time.
As with the MDS analysis in the previous section, we used eac h of the ten performance metrics to measure the performance of the 2000 mo dels trained with the di eren t learning metho ds on eac h of the sev en test problems. In this section we use correlation analysis on these mo dels to compare metrics instead of MDS.

Again, to mak e the correlation analysis easier to inter-pret, we rst scale performances to the range [0 ; 1] so that the best performance we observ ed with that metric on eac h problem with any of the learning metho ds is performance 1, and baseline performance with that metric and data set is performance 0. This eliminates the inverse correlation between measures suc h as accuracy and squared error, and normalizes the scale of eac h metric.
 Ten metrics permits 10 9 = 2 = 45 pairwise correlations. We do these comparisons using both linear correlation (ex-cluding CAL) and rank correlation. The results from the linear and rank correlation analyses are qualitativ ely sim-ilar. We presen t the results for non-parametric rank cor-relation because rank correlation mak es few er assumptions about the relationships between the metrics, and because rank correlation is insensitiv e to how CAL is scaled.
Table 3 sho ws the rank correlation between all pairs of metrics. Eac h entry in the table is the average rank cor-relation across the sev en test problems. The table is sym-metric and con tains only 45 unique pairwise comparisons. We presen t the full matrix because this mak es it easier to scan some comparisons. The nal column is the mean of the rank correlations for eac h metric. This gives a rough idea how correlated eac h metric is on average to all other metrics.

Metrics with pairwise rank correlations near one beha ve more similarly than those with smaller rank correlations. Ig-noring the SAR metric whic h is discussed in the next section, sev en metric pairs have rank correlations above 0.90: 0.96: Lift to ROC Area 0.95: ROC Area to Average Precision 0.93: Accuracy to Break-ev en Point 0.92: RMS to Cross-En trop y 0.92: Break-Ev en Point to ROC Area 0.92: Break-Ev en Point to Average Precision 0.91: Average Precision to Lift
We exp ected AUC and average precision to beha ve very similarly and thus have high rank correlation. But we are surprised to see that Lift has suc h high correlation to AUC. Note that because Lift has high correlation to AUC, and AUC has high correlation to average precision, it is not sur-prising that Lift also has high correlation to average preci-sion. As exp ected, break-ev en point is highly correlated with the other two ordering metrics, AUC and average precision. But the high correlation between accuracy and break-ev en point is somewhat surprising and we curren tly do not kno w how to explain this.

The weak est correlations are all between the calibration metric (CAL) and the other metrics. On average, CAL cor-relates with the other metrics only about 0.63. We are sur-prised how low the correlation is between probabilit y cali-bration and other metrics, and are curren tly looking at other measures of calibration to see if this is true for all of them.
Figure 5 sho ws an MDS plot for the metrics when distance between metrics is calculated as 1 rank cor relation , mak-ing MDS insensitiv e to how the metrics are scaled. (Dis-tances based on 1 rank cor relation do not resp ect the triangle inequalit y so this is not a proper metric space.) The overall pattern is similar to that observ ed in the MDS plots in Figure 3. CAL is at one end of the space far from the other metrics. Cross-en trop y is closest to RMS, though not as close as in the other plots. Cross-en trop y and RMS have high rank correlation, but because cross-en trop y has lower rank-correlation to other most metrics than RMS, it is pushed far from RMS whic h is close to other metrics in the MDS plot. APR and AUC are at the other end of the space farthest from CAL. FSC is in the upp er left side of the space. ACC and RMS are near the cen ter of the space.
When applying sup ervised learning to data, a decision must be made about what metric to train to and what met-ric to use for mo del selection. Often the learning algorithm dictates what metrics can be used for training, e.g. it is dif-cult to train a neural net for metrics other than RMS or MXE. But there usually is much more freedom when select-ing the metric to use for mo del selection, i.e. the metric used to pick the best learning algorithm and the best parameters for that algorithm.

If the correct metric for the problem is kno wn, mo del se-lection probably should be done using that metric even if the learning algorithm cannot be trained to it. What should be done when the correct metric is not kno wn? The MDS plots and correlation analysis suggest that RMS is remark-ably well correlated with the other measures, and thus migh t serv e as a good general purp ose metric to use when a more speci c optimization criterion is not kno wn.

We wondered if we could devise a new metric more cen-trally located than RMS and with better correlation to the other metrics. Rather than devise a completely new met-ric, we tried averaging sev eral of the well beha ved metrics into a new metric that migh t be more robust than eac h one individually . SAR com bines S quared error, A ccuracy , and R OC area into one measure: SAR = ( AC C + AU C + (1 RM S )) = 3. We chose these metrics for SAR for three rea-sons: 1. we wanted to select one metric from eac h metric group: 2. ACC, AUC, and RMS seemed to be the most popular 3. these three metrics are well correlated to the other As can be seen from the MDS plots and in the tables, SAR beha ves di eren tly from ACC, AUC, and RMS alone. In Table 3 SAR has higher mean rank correlation to other metrics than any other metric. In the MDS plots, SAR tends to be more consisten tly cen trally located than other metrics. And in Table 4 it is the metric that best re ects the ordering by mean performance of the sev en learning metho ds.
These results suggest that of the ten metrics we exam-ined, SAR is the metric that on average is most correlated with the other metrics, both separately , and in groups. SAR is even more represen tativ e than RMS (though RMS also is very good). In an exp erimen t where SAR was used for mo del selection, SAR outp erformed eigh t of the nine metrics in se-lecting the mo dels with the best overall, and tied with RMS. We believ e our results suggest that SAR is a robust com bi-nation of three popular metrics that may bey appropriate when the correct metric to use is not kno wn, though the bene t of SAR over RMS is mo dest at best. Attempts to mak e SAR better by optimizing the weigh ts given to ACC, AUC, and RMS in the SAR average did not signi can tly im-pro ve SAR compared to equal weigh ts for the three metrics. We are very impressed at how well beha ved RMS alone is and are curren tly working to devise a better SAR-lik e metric that yields more impro vemen t over RMS alone.
Table 4 sho ws the normalized performance of eac h learn-ing algorithm on the nine metrics. (CAL is scaled so that the minim um observ ed CAL score is 0.0 and the maxim um observ ed CAL score is 1.0) For eac h test problem we nd the best parameter settings for eac h learning algorithm and compute it's normalized score. Eac h entry in the table av-erages these scores across the sev en problems. The last two columns are the mean normalized scores over the nine met-rics, and the SAR performance. Higher scores indicate bet-ter performance. The mo dels in the table are ordered by mean overall performance. We have written a separate pa-per to compare the performance of the learning metho ds to eac h other on these metrics, but there are a few interesting relationships between learning algorithms and metrics that are worth discussing in the con text of this pap er.
Overall, the best performing mo dels are neural nets, SVMs, and bagged trees. Surprisingly , neural nets outp erform all other mo del types if one averages over the nine metrics. ANNs app ear to be excellen t general purp ose learning meth-ods. This is not to say that ANNs are the best learning algorithm { they only win on RMS and CAL, but because they rarely perform poorly on any problem or metric, they have excellen t overall performance.
 The SVMs perform almost as well as ANNs. Note that SVM predictions on [ 1 ; + 1 ] are not suitable for measures like cross entrop y, calibration, and squared error. SVMs do well on these metrics because we use Platt's metho d [8] to transform SVM predictions to calibrated probabilities. Lik e neural nets, SVMs app ear to be a safe, general purp ose, high performance learning metho d once their predictions have been calibrated by a metho d suc h as Platt scaling.
Although single decision trees perform poorly , bagged trees perform nearly as well as neural nets and SVMs. Bagging impro ves decision tree performance on all metrics, and yields particularly large impro vemen ts on the probabilit y metrics. Lik e neural nets and SVMs, bagged trees app ear to be a safe, general purp ose, high performance learning metho d. Boosted trees outp erform all other learning metho ds on ACC, LFT, ROC, APR, and BEP . Boosting wins 2 of 3 threshold metrics and 3 of 3 rank metrics, but performs poorly on the probabilit y metrics: squared error, cross en-trop y, and calibration. Maxim um margin metho ds suc h as boosted trees yield poorly calibrated probabilities. (SVMs perform well on these because Platt scaling \undo es" the maxim um margin.) Overall, boosting wins 5 of the 6 met-rics for whic h it is well suited, and would easily be the top performing learning metho d if we consider only the 6 thresh-old and ordering metrics.

The KNN metho ds were not comp etitiv e with the bet-ter algorithms, but migh t done better with larger train sets. Single decision trees also did not perform as well as most other metho ds, probably because recursiv e partitioning runs out of data quic kly with 4k train sets, and because small trees are not good at predicting probabilities [9]. We tested man y di eren t kinds of decision trees, including smo othed unpruned trees, and then picked the best, so the poor per-formance of trees here is not due to any one tree type being inferior, but because all of the man y tree types we tested did not perform as well as other metho ds.

Interestingly , boosting stump mo dels does not perform as well as boosting full decision trees. Boosted stumps do outp erform single trees on 5 of the 6 threshold and rank metrics. Their last-place ranking below decision trees is due to their extremely poor performance on the three probabilit y measures.
There is not a large literature comparing performance metrics. The closest work to ours is by Flac h [7]. In this work Flac h uses the ROC space to understand and compare di eren t metrics. He analyzes accuracy , precision, weigh ted relativ e accuracy and sev eral decision tree splitting criteria.
The STATLOG pro ject [6] performed a large scale empir-ical evaluation of a num ber of learning algorithms in 1995. STATLOG compared the performance of the di eren t algo-rithms, and also did an analysis of how the predictions made by the algorithms compared to eac h other. STATLOG, how-ever, did not compare performance using di eren t metrics.
Our analysis allo ws us to dra w a variet y of conclusions whic h we summarize here. If the goal is to maximize accu-racy , but the mo del needs a con tinuous performance metric (e.g. using bac kpropagation to train a neural net), it proba-bly is better to train the mo del using squared error instead of cross entrop y because squared error sits closer to accuracy in metric space. This result is surprising since cross entrop y is the theoretically preferred loss function for binary classi -cation. We susp ect cross entrop y is not as robust as squared error on real data sets because real data sometimes con tains class noise that cross entrop y is very sensitiv e to.
Squared error is a remark ably robust performance metric that has higher average correlation to the other metrics than any other metric except SAR. Squared error app ears to be an excellen t general purp ose metric.

Man y mo dels achiev e excellen t performance on the or-dering metrics AUC, APR, and BEP without making pre-dictions that yield good probabilities. For example, the k-nearest neigh bor mo dels with the best ROC performance use values of K that are so large that most of the predic-tions are close to p , the fraction of positiv es in the data. This yields predictions that are poor when view ed as proba-bilities, yet small di erences between these predicted values are sucien t to pro vide for good ordering.

As exp ected, maxim um margin metho ds suc h as boosting and SVMs yield excellen t performance on metrics suc h as ac-curacy for whic h they are designed. Surprisingly , however, the maxim um margin metho ds also yield excellen t perfor-mance on the ordering metrics. We had not exp ected that maximizing distances to decision boundaries would pro vide a good basis for ordering cases that fall far from those bound-aries.

Although boosted trees perform well on accuracy and ROC, they perform poorly on probabilit y metrics suc h as squared error and cross entrop y. This poor performance on prob-abilit y metrics is a consequence of boosting being a max-imum margin metho d. SVMs do not exhibit this problem because we scale SVM predictions with Platt's metho d; Lin-early scaling SVM predictions to [0 ; 1] does not work well.
Neural nets trained with bac kpropagation have excellen t overall performance because, unlik e boosting, they perform well on all metrics including the probabilit y metrics RMS, MXE, and CAL. We believ e part of the reason why the neu-ral nets perform so well is that they were trained with bac k-propagation on squared error, and as we have seen squared error is an excellen t metric.

The three ordering metrics, AUC, APR, and BEP , cluster close in metric space and exhibit strong pairwise correla-tions. These metrics clearly are similar to eac h other and somewhat interc hangeable. We originally group ed LFT with the threshold metrics ACC and FSC, but the results suggest that LFT beha ves more like BEP , an ordering metric. We now would group LFT with BEP in the ordering metrics along with AUC and APR.

The metric space for the ten metrics has three or more signi can t dimensions. The ten metrics do not all measure the same thing. Di eren t performance metrics yield di er-ent tradeo s that are appropriate in di eren t settings. No one metric does it all, and the metric optimized to or used for mo del selection does matter. The SAR metric that com-bines accuracy , ROC area, and squared error app ears to be a good, general purp ose metric, but RMS is so good that SAR may not pro vide much bene t over using RMS alone. We hop e that additional researc h in this area will enable us to design better metrics, and will shed more ligh t on whic h metrics are most appropriate to use in di eren t settings.
Thanks to Geo Crew and Alex Ksik es for help running some of the exp erimen ts. Thanks to the creators of XGVIS and XGOBI for the interactiv e MDS soft ware used to gen-erate the MDS plots. Thanks to collab orators at Stanford Linear Accelerator for the SLA C data, and to Tony Gualtieri at NASA Goddard for help with the Indian Pines data. [1] C. Blak e and C. Merz. UCI rep ository of mac hine [2] M. DeGro ot and S. Fien berg. The comparison and [3] P. Giudici. Applie d Data Mining . John Wiley and [4] A. Gualtieri, S. R. Chettri, R. Cromp, and [5] T. Joac hims. Making large-scale svm learning [6] R. King, C. Feng, and A. Shutherland. Statlog: [7] P.A.Flac h. The geometry of roc space: understanding [8] J. Platt. Probabilistic outputs for supp ort vector [9] F. Pro vost and P. Domingos. Tree induction for [10] F. J. Pro vost and T. Fawcett. Analysis and accuracy: probably the most widely used performance met-root-mean-squared-error (RMSE): widely used in regres-mean cross entrop y (MXE): is used in the probabilistic 1 Root-mean-squared error is applicable to binary classi ca-tion settings where the classi er outputs predictions on [0 ; 1] that are compared with the true target lab els on f 0 ; 1 g . receiv er operating characteristic (ROC): has it's roots in lift: often used in mark eting analysis, Lift measures how precision and recall : These measures are widely used in precision-recall F-score: for a given threshold, the F-score precision at a recall level: as the name suggests, set the precision-recall break-ev en point: is de ned as the precision average precision: usually is computed as the average of
CAL is based on reliabilit y diagrams [2]. It is calculated
We use the follo wing parameter settings and algorithm variations for the sev en learning metho ds: KNN: we use 26 values of K ranging from K = 1 to K = j trainset j . We use KNN with Euclidean distance and Euclidean distance weigh ted by gain ratio. We also use dis-tance weigh ted KNN, and locally weigh ted averaging. The kernel widths for locally weigh ted averaging vary from 2 0 to 2 10 times the minim um distance between any two points in the train set.
 ANN: we train nets with gradien t descen t bac kprop and vary the num ber of hidden units f 1, 2, 4, 8, 32, 128 g and the momen tum f 0, 0.2, 0.5, 0.9 g . We don't use validation sets to do weigh t deca y or early stopping. Instead, for eac h performance metric, we examine the nets at man y di eren t epochs.
 DT: we vary the splitting criterion, pruning options, and smo othing (Laplacian or Bayesian smo othing). We use all of the tree mo dels in Bun tine's IND pac kage: Bayes, ID3, CAR T, CAR T0, C4, MML, and SMML. We also generate trees of type C44 (C4 with no pruning), C44BS (C44 with Bayesian smo othing), and MMLLS (MML with Laplacian smo othing). See [9] for a description of C44.
 BAG-DT: we bag at least 25 trees of eac h type. With BST-DT we boost eac h tree type. Boosting can over t, so we consider boosted DTs after f 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048 g steps of boosting. With BST-STMP we use stumps (single level decision trees) with 5 di eren t splitting criteria, eac h boosted f 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192 g steps.
 SVMs: we use most kernels in SVMLigh t [5] f linear, poly-nomial degree 2 &amp; 3, radial with width f 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2 gg and vary the regularization parameter C by factors of ten from 10 7 to 10 3 . The output range of SVMs is [ 1 ; + 1 ] instead of [0 ; 1]. To mak e the SVM predictions compatible with other mo dels, we use Platt's metho d to con vert SVM outputs to probabilities by tting them to a sigmoid [8]. Without scaling, SVMs would have poor RMS and it would not be possible to calculate MXE and CAL.
