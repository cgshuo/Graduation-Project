 Association rules have received a lot of attention in the data mining community since their introduction. The classical approach to find rules whose items enjoy high support (ap-pear in a lot of the transactions in the data set) is, however, filled with shortcomings. It has been shown that support can be misleading as an indicator of how interesting the rule is. Alternative measures, such as lift, have been proposed. More recently, a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that cannot be explained by pairwise associations between the items involved. This approach, however, has its limita-tions, since it stops short of considering higher order interac-tions (other than pairwise) among the items. In this paper, we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items. Since fitting loglinear models for large data sets can be computationally prohibitive, we apply graph-theoretical results to divide the original set of items into components (sets of items) that are statistically inde-pendent from each other. We then apply loglinear modeling to each of the components and find the interesting associa-tions among items in them. The technique is experimentally evaluated with a real data set (insurance data) and a series of synthetic data sets. The results show that the technique is effective in finding interesting associations among the items involved.
 H.2.8 [ Database Management ]: Database Applications -data mining, statistical database Association Rule, Log-linear Model, Graphical Model
Since their introduction in [1], association rules have re-ceived a lot of attention in the data mining community, hav-ing been used in multiple applications. Association rules are defined by the support of the set of items (itemset) that are involved in the rule (number of transactions in the database that contain the items), and their confidence (number of times that the right hand side appears in records where the left hand side itemset appears). Algorithms to discover as-sociation rules usually prune the choices by considering only itemsets whose support exceeds a threshold. A key property, called Apriori, states that for an itemset to exhibit high sup-port, all its subsets must have high support. This has given way to a popular algorithm (Apriori [2]) that searches for high support itemsets incrementally, beginning from item-sets of size 1, and considering candidates for high support whose size is one unit higher than those considered in the previous iteration. Other efficient algorithms have been in-vestigated ([14]).

In spite of the success of association rules, there are in-herent problems with the concept of finding rules based on their support and confidence. In [19], Silverstein et. al show the pitfalls of using support as the guide for pruning rules. It shows that  X  X nterest X  (or lift), the ratio between the ac-tual probability of the itemset divided by the product of the individual probabilities of each item, is a better guide. Obviously, the denominator in the interest is simply the esti-mated probability using independence. So, this ratio simply compares the actual support with the estimation that results from assuming independence among the items. Contrary to rules based exclusively on support, those that are found by using lift show that there exists some correlation between the itemset on the right hand side of the rule and the one on the left hand side (as long as the lift value is greater than 1). As the authors of [19] show, rules of the type X  X  &gt; Y , that have high support for the itemset XY , may be mis-leading in the sense that the itemset Y overall support may be higher when considered by itself than when considering only transactions that also contain the itemset X .
In [12], DuMouchel and Pregibon go further in showing the limitations of support-based algorithms. Assume you have a three item set ABC with strong support and lift. You really do not know if these are the consequence of a strong show of the triplet ABC or because a combination of two attributes is the strong one (e.g., AB or AC )  X  They present a meaningful example using two drugs ( AB ) and kidney failure ( C ): given that you find association between the three is it due to the combined effect of the two drugs (ABC)? Or is it simply the effect of one ( AC )? The authors propose to select the multi-item associations that can not be explained by the p airwise associations in the item set by using the standard statistical theory of log-linear models.
However, DuMouchel and Pregibon stop short of fully an-alyzing the i nterestingness of multi-item associations. The interpretation of  X  X nteresting X  large item sets can be confus-ing since it is often unclear whether the item set is interest-ing because it contains all the items, or if it is interesting because it consists of interesting subsets of items.
In this paper, we will analyze and interpret the associ-ations among items by using loglinear modeling. Loglinear models describe association patterns among categorical vari-ables. With the loglinear approach, we model cell counts in a contingency table in terms of associations among the vari-ables. There are several problems that need to be addressed in order to apply loglinear models to market basket data. First, loglinear modeling is usually applied to domains with low or medium dimensionality ( &lt; 20). In a typical mar-ket basket application, the number of dimensions may be much larger than that. The number of transactions may be very large as well [19], as opposed to the typical data sets for which loglinear modeling is applied. Also, with loglinear models, we need to have at least 5 times the number of cases as cells in our data, a requirement that is not commonly met by market basket data (as the contingency table is very sparse). Lastly, the complexity of algorithms for comput-ing the maximum likelihood estimates (MLE) in loglinear models is exponential in the dimension of the table thus computationally expensive for large tables. Hence build-ing loglinear models directly over all items is prohibitive. Fortunately for us, not all combinations of items exhibit as-sociations: some itemsets may be independent from other itemsets. We apply graph-theoretical results to divide the problem into smaller components of items and fit each com-ponent using a loglinear model.

Our work is different from DuMouchel X  X  work [12] in the following aspects. First, we aim to get only one optimal log-linear model to describe all the possible associations among the items in the component instead of building many all-two-factor models. For example, in the component com-posed by five variables (item ABCDE ), they need to build 15 all-two-factor models, one for each multi-item set (i.e, ABC, ABD,  X  X  X  , ABCD,  X  X  X  , ABCDE ) and compare with shrinkage estimates. Second, we interpret the associations among the items by using standardized parameters of fit-ted loglinear model instead of the EXCESS2 measure used in [12] 1 . The large EXCESS2 value indicates complex re-lationships involving more than pairwise association among the items of the item set. However, from EXCESS2 we can not always infer what causes the support of the itemset to be a large value. For example, if we know that the EX-CESS2 measure for ABCD is large, is it due to ABC, ABD or ABCD ? By analyzing the parameters of the fitted log-linear model, we can interpret the interestingness of asso-
EXCESS 2 =  X   X  e  X  e All 2 F denotes an estimate of the number of transactions containing the item set over and above those that can be explained by the pairwise associa-tions of the items in the item set,  X   X  e is shrinkage estimates which is a substitute of raw data, e All 2 F is predicted count of all-two-factor model based on all two-way distribution. ciations among items. The  X  -term included in the fitted loglinear model (  X  ABC ,  X  ABCD etc.) precisely describes the interactions of items. Third, by analyzing residuals, we can automatically pick out the multi-item associations that can not be explained by all the (not just pairwise) associations included in our fitted loglinear model in the item set. As our model fits better than the assumed all-two-factor model, the number of residuals generated by our method is far less than that generated by all-two-factor model.

The rest of the paper is organized as follows. In Section 2 we review the loglinear model. Section 3 presents our method. Experimental results are discussed in Section 4. In Section 5 we draw conclusions and describe directions for future work.
Loglinear modeling is a methodology for approximating discrete multidimensional probability distributions. The multi-way table of joint probabilities is approximated by a product of lower-order tables. In the database area, loglinear mod-eling techniques have been successfully applied to high di-mensional data compression [5, 6], histogram synopses [11], query approximation [17], and exploratory data cube anal-ysis [18]. Here we should note that loglinear models use only categorical attributes and continuous attributes must be converted to discrete values first.

For a value y i 1 i 2  X  X  X  in at position i r of the r th dimension d (1  X  r  X  n ), we define the log of anticipated value  X  y i 1 as a linear additive function of contributions from various higher level group-bys as:
We will refer to the  X  terms as the coefficients of the model. The coefficients corresponding to any group-by G are obtained by subtracting from the average l value at group-by G all the coefficients from higher level group-by-s. For instance, in a 4-dimensional table with dimensions A, B , C, D , we use ( i, j, k, l, y ijkl ) to denote the cell in a 4-D cube space, where i = 0 ,  X  X  X  , I  X  1, j = 0 ,  X  X  X  , J  X  1, k = 0 ,  X  X  X  , K  X  1, l = 0 ,  X  X  X  , L  X  1. Equation 2 shows the satu-rated loglinear model which contains all the possible k -factor effects, all the possible k  X  1-factor effects, and so on up to the 1-factor effects and the mean  X  . For example,  X  A i one-factor effect,  X  AB ij is two-factor effect which shows the dependency within the distributions of the associated at-tributes A, B . The singly-subscripted terms are analogous to main effects, and the doubly-subscripted terms are anal-ogous to two-factor interactions.
Equation 3 shows the linear constraints among coefficients, where a dot  X . X  means that the parameter has been summed over the index (For example,  X  AB i. = the constraints specify that the loglinear parameters sum to 0 over all indices.
Equation 4 shows how to compute the coefficients in a 4-dimensional table.  X 
In [18] a fast computation technique called the UpDown method that makes this approach feasible for large sets is described. In the Up-phase, all the l parameters shown be-fore are computed. For each group-by in Equation 4, the corresponding l value from the parameters in the previous group-by-s is computed. For example, in order to compute l ij.. , we could use the values of l ijk. , aggregating for all k . (In general, there is more than one way of computing the parameters, since there is a lattice of group-by aggregations; A benefit analysis approach like the one in [15] can be used to select the best choice.) We need to start from the most detailed group-by: in general this is the one defined by the raw data.

In the Down-phase, for each group-by starting from the least detailed (for instance, l.... in Equation 4), we can com-pute the corresponding effect (i.e.,  X  ) at G by subtracting from the corresponding l value the parameters from all the group-by-s H where H  X  G . (For instance, to compute  X  AB we need to subtract from l ij.. the values of  X  A i ,  X  B
It is obvious that a large number of models can be used to fit a given data set. For an k -dimensional loglinear model, there are a total 2 2 k possible models (determined by which parameters of the saturated model are set to zero). There are several possible strategies of model selection (see [8] for more discussion). One approach consists of fitting the model having only single-factor terms, then the model having only single-factor and two-factor terms, then the model having only three-factor and lower order terms, and so forth. Fit-ting such models often reveals a restricted range of good-fitting models. In our earlier work [6], we apply this strategy to compress data cubes where the objective is to achieve a good compression ratio instead of interpreting the associa-tions.

Brown et. al., in [9, 7], suggested model fitting by using two tests to screen the importance of each possible term. In one test the term is the most complex parameter in a simple model, whereas in the other test all parameters of its level of complexity are included. This strategy works well when our main objective is to test whether a particular interaction is present or significant. However, this strategy involves a large computational cost to build loglinear model as it needs to evaluate the importance of all possible terms.
In this section we describe in detail how we screen and interpret associations by means of building loglinear models and examining their parameters and residuals using market basket data. For market basket data, we define each trans-action, such as list of items purchased, as a subset of all possible items.

Definition 1. Let I 1 ,  X  X  X  , I k be a set of k boolean vari-ables called attributes. Then a set of baskets B = { b 1 , is a collection of n k -tuples from { T RU E, F ALSE } k which represent a collection of value assignments to the k attributes.
Our method involves decomposing the initial set of items into groups that are mutually independent, building loglin-ear models for these components, interpreting associations and examining residuals. The method can be sketched as follows:
As we stated in the introduction, to effectively process a data set with large number of items, we need to decompose the items into components and build a loglinear model for each component separately. All the significant interactions of a loglinear model built over the original data set must remain unchanged in the loglinear models built over compo-nents. In other words, the MLEs for each parameter of the original model should equal to the MLEs of models for com-ponents. We apply graph-theoretical results to decompose the items into components while keeping the MLEs of pa-rameters unchanged. We leave the discussion of this part in Section 3.3 and assume the number of dimensions is low or medium (i.e., k &lt; 20) in Sections 3.1 and 3.2. In Section 3.1 Table 1: COIL 2000 data set with four dimensions denoted by A, B, C, D respectively D True C True 457 1162 175 526 D False C True 944 851 89 307 AB AC AD BC BD CD Figure 1: Lattice for the data set with four dimen-sions denoted by A, B, C, D respectively. The value in () denotes the value of  X  -term of saturated loglinear model we focus on how to fit loglinear model for each component. In Section 3.2 we present how to interpret the interesting patterns of associations and how to screen interesting item-sets by examining the parameters and residuals of the fitted loglinear model.
The first step of loglinear model fitting is to compute the parameters of the saturated model by applying the UpDown method(Step 3.1).

We present our strategy by using one example. Table 1 shows a contingency table with four attributes. This table is derived from COIL real data set [10]. The original data set contains 86 attributes and we present our experiment with full 86 attributes in Section 4.1. Table 2 shows the meaning of the four attributes (A,B,C,D) and the other six attributes (E-J). These ten attributes are the most signifi-cant attributes after applying univariate analysis [22]. We use these ten attributes to illustrate our method (including decomposition and loglinear model fitting).

Figure 1 shows the parameter values from the saturated model computed by using the UpDown method. Each of the  X  -term in the saturated loglinear model describes the interaction of item variables. For example,  X  AB represents the interaction between item A and B. Notice that in market basket data, each item variable can only have two categories: presence, absence. Hence, each of the  X  -term has only one absolute value due to linear constraints of coefficients (See Equation 3) and the positive (negative) value implies posi-tive (negative) associations. For example,  X  AB =  X  0 . 044 in Figure 1 implies  X  AB 00 =  X  0 . 044,  X  AB 01 = 0 . 044,  X  and  X  AB 11 =  X  0 . 044. It can be interpreted that the presence (absence) of A implies the absence (presence) of B with in-teraction effect 0.044. Note in general contingency table cases, the  X  -term for a particular interaction (i.e.,  X  AB more than one absolute value due to variables with more than two categories.

Furthermore, we can compare the interactions according to their magnitude of  X  -terms derived from the saturated models in market basket case. For example, the compari-son of  X  AC (0.681) and  X  CD (0.245) implies the interaction of AC is more significant than that of CD. It is important to point out that, in general, we cannot compare the mag-nitude of  X  -terms directly. This is due to several reasons. First, the degree of freedom (d.f.) for each particular inter-action varies (however, in market basket data, the d.f. for each particular interaction is always 1). Secondly, the vari-ance for each interaction varies (however, in market basket data, the variances for all  X  -terms equal to the same value -see Appendix A for proof details). The values  X  AC 00 = 0 . 681 and  X  CD 00 = 0 . 245 do not necessarily imply that the interac-tion of AC is greater than that of CD, since the variances of  X  00 ,  X  CD 00 can be different. So in the general case, we have to compute the standardized parameter value (  X / X  (  X  )) for each  X  -term in order to compare the significance of each in-teraction. The cost to standardize each  X  -term is very high. Thirdly, in general, there can be more than one absolute value for each  X  -term and we have to combine the estimates in some way to form an overall test statistic (This is usually hard and subjective [13]). However, for market basket data, we do not need to do this since each  X  term exactly has one absolute value.

Our modeling strategy consists in ordering the  X  -terms based on their magnitude and including those  X  -terms ex-ceeding some threshold (we can do this since the  X  -terms are comparable). The idea of fitting the saturated model and noting which estimates of association and interaction parameters are large compared to their estimated standard errors was first proposed by Goodman, in [13]. However, it is not widely used because the high computational cost of standardizing parameters. For market basket data, this idea is very attractive as we can drastically decrease the cost of modeling without computing the variance of each  X  -term.
To determine which interactions should be included in the fitted model, we need a threshold. However, there is no good way to determine the threshold for a given data set. It is unknown what the distribution of all  X  -terms estimates is, although each  X  -term estimate follows an approximate nor-mal distribution with mean  X  and variance  X  (  X  ) [3]. We apply a heuristic strategy here. We order  X  -terms according to their magnitude and divide them into bins (equi-width). We first include in the starting model those terms in the first bin. When that model fits well, it may be possible to simplify it and remove some terms with small absolute values. When it does not fit well, we need to include ad-ditional parameters in the second bin. In other words, we original variable to binary variable. keep comparing models built with parameters up to the j -th and j + 1-th bin until the latter fits well. During step 3.3, we apply the likelihood ratio L 2 (see equation 5) to assess the importance of terms in j +1-th bin. The likelihood ratio is minimized and follows a chi-square distribution with the d.f. equal to the number of  X  -terms set equal to zero. For a given d.f., larger L 2 values give smaller right-tail probabil-ities (P-values), and represent poor fits. Equation 6 shows how the L 2 statistics is used for comparison of two models. The d.f. is calculated by subtracting the d.f. of model2 from the d.f. of model1. In this step, the difference of d.f. is al-ways 1 as the two models we compared are same except the tested  X  -term.

As we stated in the introduction, we are departing from the majority of published approaches to the market basket problem by going beyond the examination of frequent item-sets. The idea of using measures other than itemset fre-quency has been explored a few times. For example, in [19], they propose measuring significance of dependence via the chi-squared test for independence from classical statistics. In [12], they only distinguish between multi-item associa-tions that can be explained by all pairwise associations, and item sets that are significantly more frequent than their pair-wise associations would suggest. In our framework, we inter-pret associations by examining the  X  -terms of fitted loglin-ear models instead of by examining the differences between observed frequencies of itemsets and expected frequencies computed from assumed models.
Now we illustrate the difference of our work with previous approaches using an example. Equation 7 assumes the in-dependence model and includes all-one-factor (main) effects and grand mean. Equation 8 includes all-two-factor effects apart from all-one-factor effects and grand mean. The com-parison between the observed value y with either  X  y lift  X  y pairwise is used to screen interesting itemsets in [19] or [12] respectively. The assumed independence model (shown in Equation 7) or pairwise model (shown as Equation 8) may be inaccurate. By comparing with an inaccurate model, false interpretations may be introduced when we examine itemsets. In our framework, we fit the market basket data to derive the fitted loglinear model (as shown in Equation 9) instead of just assuming some specific model (independence or pairwise model).

As our model really fits the underlying data and includes significant interactions at all possible levels, we can derive the association patterns by examining the  X  -terms of our fitted model directly. For example, from  X  AC = 0 . 681,  X 
BC =  X  0 . 765, and  X  ABC = 0 . 223, we can see the pos-itive two-factor interaction (i.e., the presence of one item implies the presence of the other one) between item A and C, the negative two-factor interaction between item B and C, no significant two-factor interaction between item A and B, and positive three-factor interaction among ABC. From  X 
BD =  X  0 . 296 ,  X  ABD =  X  0 . 185, we can see the negative two-factor interaction between item B and D, the three-factor negative interaction among ABD, however no signifi-cant two-factor interaction for item sets AB or AD.
We would like to point out that we apply a non-hierarchical modeling strategy (step 3.2 and 3.3). Hierarchical models are nested models in which when an interaction of d factors is present, all the interactions of lower order between the variables of that interaction are also present. For example, if a three-way interaction (  X  ABC ) is present, the model must also include all two-way effects (  X  AB ,  X  AC ,  X  BC ) as well as the single variable effects (  X  A ,  X  B ,  X  C ) and the grand mean (  X  ). Non-hierarchical modeling can better interpret the as-sociations for market basket data. Consider one example for the concept of synergism 2 where each item from A, B, C is sold independently and the third item is free if cus-
A response occurs when two factors are present together but not when either occurs alone. This is the perfect illus-tration for the example that two drugs together cause kidney failure. Table 3: Comparison of three models. Residuals is the number of cells by comparing standardized residuals to standard normal percentage points 3.29 independence 2597.4 11 10 all-two-factor 226.7 5 6 tomer buys any other two items. Clearly there exists a three-way interaction effect (  X  ABC ) but no two-way inter-action between any pairs among A, B, C (  X  AB ,  X  AC ,  X  BC Clearly only non-hierarchical model (i.e., log  X  y ijk =  X  +  X   X  j +  X  C k +  X  ABC ijk ) can explain this case correctly. Notice in the non-hierarchical model, the two-way effects are not included in the model therefore violating the hierarchical requirement.

The parameters of the loglinear model provide the inter-actions between item variables. Further analysis of residuals may reveal in cell-by-cell comparisons of observed and fit-ted frequencies. Note here our loglinear model is built at the finest level (containing all variables) and it is easy to compute expected frequencies of itemsets at any upper level by simply summing those cells from the finest level. Equa-tion 10 shows the standardized residual form used in our framework.
When the model holds, e i is asymptotically normal with mean 0. In comparing standardized residuals to standard normal percentage points, we obtain conservative indica-tions of cells having lack of fit. Table 3 shows the compar-ison of independence model, pairwise model, and our fitted model for the COIL data set. The likelihood ratio and the size of residuals from Table 3 clearly show that our fitted model is better than the independence and pairwise mod-els as it includes significant high-factor effects and excludes those non-significant 2-factor effects (even the main effect).
As we stated in the introduction, we cannot build loglin-ear models over the very sparse and large contingency table that results from market basket data. Besides, even if the data set is dense, the complexity of algorithms for computing the MLEs in loglinear models is generally exponential in the dimension of the item variables and thus computationally expensive for large tables. In this section, we discuss how to decompose the problem into subsets and build loglinear models for each subset without losing any significant inter-action. We do this by using graph-theoretical results. The procedure involves two steps: 1) we build one i ndependence graph for all item variables; 2) we apply graph-theoretical results to decompose the graph into non-decomposable irre-ducible components.

The i ndependence graph is defined by making every ver-tex of the graph correspond to a discrete random variable, and the edges denoting the dependency of the two variables linked. A missing edge in the graph represents the condi-tional independence of the two variables associated with the two vertices. Models with the maximal permissible higher-order interactions corresponding to a given independence graph are called g raphical models. (See [16, 21] for compre-hensive treatment of graphical models.) Figure 2(a) shows the independence graph (two disconnected subgraphs FI, ABCDJGEH) for our COIL data set. From this graph, we can infer for instance that variables I and F are independent with respect to the remaining variables. We can also derive that variables E and H are conditionally independent with respect to the set ACDEJ given the variable G. Intuitively, there is no interaction between any variable from set EH and any variable from the set ACDEJ given variable G.
The second step is to decompose the graph into basic, ir-reducible components. Graph-theoretical results show that if a graph corresponding to a graphical model for a con-tingency table is decomposable into subgraphs by a clique separator 3 , the MLEs for the parameters of the model can easily be derived by combining the estimates of the models on the lower dimensional tables represented by the simpler subgraphs. Hence, applying a divide-and-conquer approach based on the decompositions will make the procedure appli-cable to much larger tables.

The theory may be interpreted by the following way: if two disjoint subsets of vertices S a and S b are separated by a subset S c in the sense that all paths from S a to S b go through S , then the variables in S a are conditionally independent of those in S b given the variables in S c . The subgraphs may be further decomposed into subgraphs. The requirement that the subgraph on S c is complete implies that there is no further independence constraints on the elements of S c so that this factorization contains all the information about the joint distribution.
 Figure 2(b) shows the components (ABCD, ACJ, EHG, AG, IF). We can see the interactions among ABCD are in-dependent with respect to other variables. The interactions among ABCD (i.e.,  X  AB ,  X  AC ,  X  ABC etc.) can be derived directly from the condensed 4-dimensional contingency ta-ble (i.e., ABCD) instead from the original 10-dimensional contingency table (i.e., ABCDEFGHIJ). The MLEs of the interactions for each component are the same as those for the original graphs. In our experiments, we apply CoCo [4] within XLISP-STAT with a complexity of O ( nm 3 ), with m the number of generators and n the number of variables, to perform decomposition for large contingency tables. To find the clique separators of a graph or to find the vertex-sets of the irreducible components of the graphs, an algorithm with a complexity of O ( ne + n 2 ) can be used [20], where n is the number of vertices and e is the number of edges.

To build the independence graph, we need to test condi-tional independence for every pair of variables, controlling for the other variables. There are several approaches to test conditional independence (See [3]). In our paper, we build the independence graph by applying the Cochran-Mantel-Hasenzel test. For any pair of two items I i , I j from item set I = { I 1 ,  X  X  X  , I k } , we derive one partial 2  X  2 contingency table (stratum) for each possible value from set I \{ I i Hence we can have L ( L = 2 k  X  2 ) strata. For each stratum l ,
A clique is a subset of vertices which induce a complete subgraph for which the addition of any further vertex ren-ders the induced subgraph incomplete. A graph is complete if all vertices are joined with undirected edges. In other words, the clique is maximally complete. we need to compute the marginal totals { n ( l ) . 0 , n ( Table 5(a) shows the stratum form for item variable A and B while Table 5(b) shows one stratum ( C = 1 , D = 1) derived from Table 1. Equation 11 shows the summary statistics where m ( l ) 11 and V ( n 11 ) is mean and variance respectively.
The summary statistics M 2 has approximately a chi-squared distribution with d.f. = 1 under the null hypothesis of con-ditional independence. Hence, if M 2 &gt; P  X  , we can reject the null hypothesis of conditional independence and include the edge of I i and I j in the interaction graph. In our ex-periments, we choose  X  = 0 . 05 and P  X  = 3 . 84146. However, the Cochran-Mantel-Hasenzel test does not work well for very sparse data sets because the marginal totals for a given partial table usually equal zero. To deal with very sparse market basket data sets, we may test marginal independence for each pair of variables by applying log odds ratio over one m arginal 2  X  2 table (shown in Table 5(c)) which contains summary frequencies and ignores the other controlling vari-ables.
In this section we show the results of experimenting with one real data set and some synthetic data sets. The experi-ments were conducted in a DELL Dimension 8100, with one 1.7G processor, and 640 Mbytes of RAM.
The COIL Challenge 2000 [10] provides data from a real insurance business. The competition consisted of two tasks: 1) Predict which customers are potentially interested in a Caravan insurance policy; 2) Describe the actual or poten-tial customers; and possibly explain why these customers buy a Caravan policy. Information about customers con-sists of 86 attributes and includes product usage data and socio-demographic data derived from zip area codes. The training set consists 5822 descriptions of customers, includ-ing the information of whether or not they have a Cara-van insurance policy. A test data set contains 4000 tuples Table 4: A 2  X  2 contingency table for variable A and B which only the organizers know if they have a Caravan in-surance policy. Here our aim is to identify interaction pat-terns among 86 attributes varying from product usage to socio-demographic. Our data is formed by collapsing non-binary categorical attributes into binary form (the data can be found at www.cs.uncc.edu/ xwu/classify/b86.dat), with n = 5822 baskets and k = 86 binary items.

We successfully decomposed the data set with 86 variables into components with much less variables (the largest one with 20 variables and most components with less than 5 vari-ables). After decomposition, we got 22 components (Figure 5 shows 10 components which contain 3 or more variables) and we then fit each component by using loglinear model.
We also did the experiment over this data set by using the Apriori algorithm. The algorithm generated 6050 large item sets and 13131 rules under support 0.1 and confidence 0.8. We found it was much harder to draw interesting con-clusions about data from support-confidence results. We compared the significant interactions discovered by our al-gorithm with the large item sets discovered by Apriori al-gorithm and found the percentage of overlap is very low. Table 6 shows several significant interactions discovered by our loglinear fitting algorithm and the actual support val-Table 5: Component after decomposition for COIL data set, we omit 12 components which contain less than 3 variables. ues for those subsets. The lower support value for all subsets (except for  X  1 , 5 , 21 ) definitely prevent them to be discovered by traditional support-confidence framework. For instance, the association  X  48 , 74 , 79 reveals that people are inclined to buy delivery van policies (48), agricultural machines policies (74) and disability insurance policies (79) together.
The COIL data set is too sparse to study the performance (running time) of our algorithm. In order evaluate the per-formance our algorithm properly, we turn to synthetic data (the same market basket data generator used in [1]) from IBM X  X  Quest Group.

We generated two data sets (one with 50 items and the other with 100 items). We have not done the experiments over data sets with more than 100 variables as we have used CoCo [4] (an environment for graphical models) which can not deal with more than 128 variables. We are currently im-parameter value meaning Figure 3: Execution time by varying ntrans from (10K, 20K, 50K, 100K, 200K, 500K, 1M) plementing the decomposition algorithm proposed by [20] to be able to handle data sets with larger number of variables. We set the average basket size to be 10, the average of large itemsets to be 4, the correlation between large itemsets to be 0.25, the confidence in a rule to be 0.75, the number of transactions varying from 10k to 1M. We ran some experi-ments with the tlen set to 6 or the correlation level set to 0.75 but did not find significant difference in the nature of our performance results.

Figure 3 shows our execution time. Note that decompo-sition step is determined by the size of independence graph (i.e., the number of variables k , the number of edges e or the number of generators m ). We observe the decomposition time is small compared with the preprocessing time because the size of independence graph in our experiment is usually small (with 100 nodes and several hundreds edges). As the number of items contained in each component is compara-bly small (most less than 10), the time of loglinear model fitting for each component is trivial. In Figure 3, we also include the execution time of Apriori algorithm (with min-inum support 0.1% and minimum confidence 80%). We can see the execution time of our algorithm is comparable to that of Apriori algorithm for medium dimension size (50, 100). Figure 4 shows the number of components generated in our experiment. When we fix the other parameters of market basket generater and increase the number of transactions, the number of components decreases because the number of edges in independence graph increases.
In this paper we presented how to interpret associations among items by fitting loglinear models and examining those magnitude of parameters for market basket data. Our work departed from earlier work that just aims to find large or interesting itemsets and leaves those itemsets to domain ex-pert directly. On the contrary, we build loglinear model and apply the values of  X  -terms as measures of associations among item variables directly. We believe those values pro-vided by our loglinear model are very helpful for domain expert to make judgments about cause and effect relations among items. To deal with large number of variables, we Figure 4: Number of components by varying ntrans from (10K, 20K, 50K, 100K, 200K, 500K, 1M) applied graph-theoretical results to decompose items into subsets without losing any significant interaction.
There are some aspects of this work that merit further re-search. Among them, we are trying to automatically derive rules from the  X  -terms included in fitted loglinear model. For components with more than 10 variables, it is hard for user to grasp all the association patterns. We will be explor-ing how to combine visualization techniques and association graph for this issue.

Another aspect that merits further research is that of in-teractive analysis of associations among items. For example, the user may want to examine a given subset (say ABC). Clearly collapsing into contingency table of ABC directly will lose information as item A, B, or C may have interac-tions with other items. To find the smallest set containing a given set (i.e., ABC) and onto which the model is collapsi-ble was studied in [4]. We will investigate this problem for online association analysis.

Finally, we will study how to better deal with sparse data when either structural zero cells present or it contains many small cell values. It is known that loglinear model can still work for small incomplete table with structural or sam-pling zeros [8]. We will investigate other techniques such as shrinkage estimates [12] for large incomplete market bas-ket data.
The authors would like to thank Christian Borgelt for providing his implementation of the Apriori algorithm. We would like to thank Jens Henrik Badsberg for his CoCo pro-gram which makes our experiments possible. We would also like to thank IBM Quest group for providing the market basket data generator. [1] R. Agrawal, T. Imilienski, and A. Swami. Mining [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] A. Agresti. Categorical data anlysis . Wiley, 1990. [4] J. Badsberg. An environment for graphical models. [5] D. Barbar  X  a, W. DuMouchel, C. Faloutsos, P. Hass, [6] D. Barbar  X  a and X. Wu. Loglinear based quasi cubes. [7] J. Benedetti and M. Brown. Stratigies for the selection [8] Y. M. Bishop, S. E. Fienberg, and P. W. Holland. [9] M. Brown. Screening effects in multidimensional [10] COIL challenge 2000. The insurance company (tic) [11] A. Deshpande, M. Garofalakis, and R. Rastogi. [12] W. DuMouchel and D. Pregibon. Empirical bayes [13] L. Goodman. The analysis of multidimensional [14] J. Han, J. Pei, and Y. Yin. Mining frequent patterns [15] V. Harinarayan, A. Rajaraman, and J. Ullman. [16] S. Lauritzen. Graphical Models . Oxford University [17] D. Pavlov, H. Mannila, and P. Symth. Probabilistic [18] S. Sarawagi, R. Agrawal, and N. Meggido.
 [19] C. Silverstein, S. Brin, and R. Motwani. Beyond [20] R. Tarjan. Decomposition by clique separators. [21] J. Whittaker. Graphical Models in Applied [22] X. Wu and D. Barbar  X  a. Modeling and imputation of
Each parameter (  X  ) in loglinear model (shown in Equation 12) can be rewritten in the form of a linear contrast of the logarithms of the expected cell counts. For example, Equa-tion 13 shows the linear contrast form for each parameter of 2-d loglinear model.  X   X   X  ij = (1 Theorm 1. The asymptotic variance of f i ( X  p ) = P k =1 T c ik , N is the size of samples and T is the size of cells.

Theorem 1 ([8], page 495) shows the variance form for parameters which can be expressed as linear contrasts. For example, Equation 14 shows how to compute variance for each parameter of 2-d loglinear model.

V ar (  X  A i ) = ( 1 IJ ) V ar (  X  B j ) = ( 1 V ar (  X  AB ij ) = ( 1
In general case, the variances of parameters are different from each other. However, the variance of each parameter is the same for market basket data as the domain size for each variable (I,J etc.) is always 2.
