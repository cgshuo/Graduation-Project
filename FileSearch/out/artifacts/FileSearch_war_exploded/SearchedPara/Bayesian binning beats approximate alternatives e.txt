 Plotting a peristimulus time histogram (PSTH), or a spike density function (SDF), from spiketrains evoked by and aligned to a stimulus onset is often one of the first steps in the analysis of neurophys-iological data. It is an easy way of visualizing certain characteristics of the neural response, such as instantaneous firing rates (or firing probabilities), latencies and response offsets. These measures also implicitly represent a model of the neuron X  X  response as a function of time and are important parts of their functional description. Yet PSTHs are frequently constructed in an unsystematic man-ner, e.g. the choice of time bin size is driven by result expectations as much as by the data. Recently, there have been more principled approaches to the problem of determining the appropriate temporal resolution [1, 2].
 We develop an exact Bayesian solution, apply it to real neural data and demonstrate its superiority to competing methods. Note that we do in no way claim that a PSTH is a complete generative description of spiking neurons. We are merely concerned with inferring that part of the generative process which can be described by a PSTH in a Bayes-optimal way. Suppose we wanted to model a PSTH on [ t min , t max ] , which we discretize into T contiguous in-tervals of duration  X  t = ( t max  X  t min ) /T (see fig.1, left). We select a discretization fine enough so that we will not observe more than one spike in a  X  t interval for any given spike train. This can be achieved easily by choosing a  X  t shorter than the absolute refractory period of the neuron under investigation. Spike train i can then be represented by a binary vector ~z i of dimensionality T . We model the PSTH by M +1 contiguous, non-overlapping bins having inclusive upper boundaries k m , is constant. M is the number of bin boundaries inside [ t min , t max ] . The probability of a spike train Figure 1: Left : Top: A spike train, recorded between times t min and t max is represented by a binary vector ~z i . Bottom: The time span between t min and t max is discretized into T intervals of duration is chosen such that at most one spike is observed per  X  t interval for any given spike train. Then, we model the firing probabilities P ( spike | t ) by M +1 = 4 contiguous, non-overlapping bins ( M is the number of bin boundaries inside the time span [ t min , t max ] ), having inclusive upper boundaries k m compute the evidence contribution subE m [ T  X  1] of a model with a bin boundary at T  X  1 and m bin boundaries prior to T  X  1 , we sum over all evidence contributions of models with a bin boundary at k and m  X  1 bin boundaries prior to k , where k  X  m  X  1 , because m bin boundaries must occupy at least time intervals 0; . . . ; m  X  1 . This takes O ( T ) operations. Repeat the procedure to obtain requires O ( T 2 ) operations. For details, see text. ~z of independent spikes/gaps is then train ~z i in bin m , i.e. between intervals k m  X  1 +1 and k m (both inclusive). In other words, we model the spiketrains by an inhomogeneous Bernoulli process with piecewise constant probabilities. We also define k  X  1 =  X  1 and k M = T  X  1 . Note that there is no binomial factor associated with the contribution of each bin, because we do not want to ignore the spike timing information within the bins, but rather, we try to build a simplified generative model of the spike train. Therefore, the where s ( { ~z i } , m ) = P N i =1 s ( ~z i , m ) and g ( { ~z i } , m ) = P N i =1 g ( ~z i , m ) 2.1 The priors We will make a non-informative prior assumption for p ( { f m } , { k m } ) , namely i.e. we have no a priori preferences for the firing rates based on the bin boundary positions. Note that the prior of the f m , being continuous model parameters, is a density. Given the form of eqn.(1) and the constraint f m  X  [0 , 1] , it is natural to choose a conjugate prior The Beta density is defined in the usual way [3]: There are only finitely many configurations of the k m . Assuming we have no preferences for any of them, the prior for the bin boundaries becomes where the denominator is just the number of possibilities in which M ordered bin boundaries can be distributed across T  X  1 places (bin boundary M always occupies position T  X  1 , see fig.1,left , hence there are only T  X  1 positions left). To calculate quantities of interest for a given M , e.g. predicted firing probabilities and their variances or expected bin boundary positions, we need to compute averages over the posterior which requires the evaluation of the evidence, or marginal likelihood of a model with M bins: where the summation boundaries are chosen such that the bins are non-overlapping and contiguous and By virtue of eqn.(2) and eqn.(4), the integrals can be evaluated: Computing the sums in eqn.(8) quickly is a little tricky. A na  X   X ve approach would suggest that a computational effort of O ( T M ) is required. However, because eqn.(10) is a product with one factor per bin, and because each factor depends only on spike/gap counts and prior parameters in that bin, the process can be expedited. We will use an approach very similar to that described in [4, 5] in the context of density estimation and in [6, 7] for Bayesian function approximation: define the function between the start interval k s and the end interval k e (both included). Furthermore, collect all contri-butions to eqn.(8) that do not depend on the data (i.e. { ~z i } ) and store them in the array pr [ M ] : Substituting eqn.(10) into eqn.(8) and using the definitions (11) and (12), we obtain with k M = T  X  1 and the constant of proportionality being pr [ M ] . Since the factors on the r.h.s. depend only on two consecutive bin boundaries each, it is possible to apply dynamic programming [8]: rewrite the r.h.s. by  X  X ushing X  the sums as far to the right as possible: P ( { ~z i }| M )  X  Evaluating the sum over k 0 requires O ( T ) operations (assuming that T M , which is likely to be the case in real-world applications). As the summands depend also on k 1 , we need to repeat this This procedure is then repeated for the remaining M  X  1 sums, yielding a total computational 1 , . . . , M : A close look at eqn.(14) reveals that while we sum over k M  X  1 , we need subE M  X  1 [ k ] for k = M  X  1; . . . ; T  X  2 to compute the evidence of a model with its latest boundary at T  X  1 . We can, however, compute subE M  X  1 [ T  X  1] with little extra effort, which is, up to a factor pr [ M  X  1] , equal to P ( { ~z i }| M  X  1) , i.e. the evidence for a model with M  X  1 bin boundaries. Moreover, having computed subE m [ k ] , we do not need subE m  X  1 [ k  X  1] anymore. Hence, the array subE m  X  1 [ k ] can be reused to store subE m [ k ] , if overwritten in reverse order. In pseudo-code (E [ m ] contains the evidence of a model with m bin boundaries inside [ t min , t max ] after termination): { f m } and { k m } , we can write where the indicator function 1 ( x ) = 1 iff x is true and 0 otherwise. Note that the probability of a spike given { k m } and { f m } does not depend on any observed data. Since the bins are non-evaluates to the corresponding firing rate. To finish we average eqn.(16) over the posterior eqn.(7). The denominator of eqn.(7) is independent of { f m } , { k m } and is obtained by integrating/summing the numerator via the algorithm in table 1. Thus, we only need to multiply the integrand of eqn.(9) (i.e. the numerator of the posterior) with P ( spike |  X  k, { f m } , { k m } , M ) , thereby replacing eqn.(11) with getIEC ( k s , k e , m ) := i.e. we are adding an additional spike to the data at  X  k . Call the array returned by this modified algorithm E  X  k [] . By virtue of eqn.(7) we then find P ( spike |  X  k, { ~z i } , M ) = E  X  k [ M ] E variance, we need the posterior expectation of f 2 m . This can be computed by adding two spikes at  X  k . To choose the best M given { ~z i } , or better, a probable range of M s, we need to determine the model posterior where P ( M ) is the prior over M , which we assume to be uniform. The sum in the denominator runs over all values of m which we choose to include, at most 0  X  m  X  T  X  1 .
 Once P ( M |{ ~z i } ) is evaluated, we could use it to select the most probable M 0 . However, making this decision means  X  X ontriving X  information, namely that all of the posterior probability is concentrated at M 0 . Thus we should rather average any predictions over all possible M , even if evaluating such an average has a computational cost of O ( T 3 ) , since M  X  T  X  1 . If the structure of the data allow, it is possible, and useful given a large enough T , to reduce this cost by finding a range of M , such that the risk of excluding a model even though it provides a good description of the data is low. In analogy to the significance levels of orthodox statistics, we shall call this risk  X  . If the posterior of M is unimodal (which it has been in most observed cases, see fig.3, right, for an example), we can then choose the smallest interval of M s around the maximum of P ( M |{ ~z i } ) such that and carry out the averages over this range of M after renormalizing the model posterior. 6.1 Data acquisition We obtained data through [9], where the experimental protocols have been described. Briefly, extra-cellular single-unit recordings were made using standard techniques from the upper and lower banks of the anterior part of the superior temporal sulcus (STSa) and the inferior temporal cortex (IT) of two monkeys (Macaca mulatta) performing a visual fixation task. Stimuli were presented for 333 ms followed by an 333 ms inter-stimulus interval in random order. The anterior-posterior extent of the recorded cells was from 7mm to 9mm anterior of the interaural plane consistent with previous studies showing visual responses to static images in this region [10, 11, 12, 13]. The recorded cells were located in the upper bank (TAa, TPO), lower bank (TEa, TEm) and fundus (PGa, IPa) of STS and in the anterior areas of TE (AIT of [14]). These areas are rostral to FST and we collectively call them the anterior STS (STSa), see [15] for further discussion. The recorded firing patters were turned into distinct samples, each of which contained the spikes from  X  300 ms before to 600 ms after the stimulus onset with a temporal resolution of 1 ms. 6.2 Inferring PSTHs To see the method in action, we used it to infer a PSTH from 32 spiketrains recorded from one of the available STSa neurons (see fig.2, A). Spikes times are relative to the stimulus onset. We discretized the interval from  X  100 ms pre-stimulus to 500 ms post-stimulus into  X  t = 1 ms time intervals and Figure 2: Predicting a PSTH/SDF with 3 different methods. A : the dataset used in this comparison consisted of 32 spiketrains recorded from a STSa neuron. Each tick mark represents a spike. B : PSTH inferred with our Bayesian binning method. The thick line represents the predictive firing rate (section 4), the thin lines show the predictive firing rate  X  1 standard deviation. Models with 4  X  M  X  13 were included on a risk level of  X  = 0 . 1 (see eqn.(19)). C : bar PSTH (solid lines), optimal binsize  X  26 ms, and line PSTH (dashed lines), optimal binsize  X  78 ms, computed by the methods described in [1, 2]. D : SDF obtained by smoothing the spike trains with a 10ms Gaussian kernel. computed the model posterior (eqn.(18)) (see fig.3, right). The prior parameters were equal for all bins and set to  X  m = 1 and  X  m = 32 . This choice corresponds to a firing probability of  X  0 . 03 in each 1 ms time interval (30 spikes/s), which is typical for the neurons in this study 1 . Models with 4  X  M  X  13 (expected bin sizes between  X  23ms-148ms) were included on an  X  = 0 . 1 risk level (eqn.(19)) in the subsequent calculation of the predictive firing rate (i.e. the expected firing rate, hence the continuous appearance) and standard deviation (fig.2, B). Fig.2, C, shows a bar PSTH and a line PSTH computed with the recently developed methods described in [1, 2]. Roughly speaking, P these methods try to optimize a compromise between minimal within-bin variance and maximal between-bin variance. In this example, the bar PSTH consists of 26 bins. Graph D in fig.2 depicts a SDF obtained by smoothing the spiketrains with a 10ms wide Gaussian kernel, which is a standard way of calculating SDFs in the neurophysiological literature.
 All tested methods produce results which are, upon cursory visual inspection, largely consistent with the spiketrains. However, Bayesian binning is better suited than Gaussian smoothing to model steep changes, such as the transient response starting at  X  100 ms. While the methods from [1, 2] share this advantage, they suffer from two drawbacks: firstly, the bin boundaries are evenly spaced, hence the peak of the transient is later than the scatterplots would suggest. Secondly, because the bin duration is the only parameter of the model, these methods are forced to put many bins even in intervals that are relatively constant, such as the baselines before and after the stimulus-driven response. In contrast, Bayesian binning, being able to put bin boundaries anywhere in the time span of interest, can model the data with less bins  X  the model posterior has its maximum at M = 6 (7 bins), whereas the bar PSTH consists of 26 bins. 6.3 Performance comparison Figure 3: Left : Comparison of Bayesian Binning with competing methods by 5-fold crossvalidation. The CV error is the negative expected log-probability of the test data. The histograms show rela-tive frequencies of CV error differences between 3 competing methods and our Bayesian binning approach. Gaussian: SDFs obtained by Gaussian smoothing of the spiketrains with a 10 ms kernel. Bar PSTH and line PSTH: PSTHs computed by the binning methods described in [1, 2]. Right : Model posterior P ( M |{ ~z i } ) (see eqn.(18)) computed from the data shown in fig.2. The shape is fairly typical for model posteriors computed from the neural data used in this paper: a sharp rise at a moderately low M followed by a maximum (here at M = 6 ) and an approximately exponential decay. Even though a maximum M of 699 would have been possible, P ( M &gt; 23 |{ ~z i } ) &lt; 0 . 001 . Thus, we can accelerate the averaging process for quantities of interest (e.g. the predictive firing rate, section 4) by choosing a moderately small maximum M .
 For a more rigorous method comparison, we split the data into distinct sets, each of which contained the responses of a cell to a different stimulus. This procedure yielded 336 sets from 20 cells with at least 20 spiketrains per set. We then performed 5-fold crossvalidation, the crossvalidation error is given by the negative logarithm of the data (spike or gap) in the test sets: Thus, we measure how well the PSTHs predict the test data. The Gaussian SDFs were discretized into 1 ms time intervals prior to the procedure. We average the CV error over the 5 estimates to obtain a single estimate for each of the 336 neuron/stimulus combinations. On average, the negative log likelihood of our Bayesian approach predicting the test data ( 0 . 04556  X  0 . 00029 , mean  X  SEM) was significantly better than any of the other methods (10ms Gaussian kernel: 0 . 04654  X  0 . 00028 ; Bar PSTH: 0 . 04739  X  0 . 00029 ; Line PSTH: 0 . 04658  X  0 . 00029 ). To directly compare the performance of different methods we calculate the difference in the CV error for each neuron/stimulus combination. Here a positive value indicates that Bayesian binning predicts the test data more accurately than the alternative method. Fig.3, left, shows the relative frequencies of CV error differences between the 3 other methods and our approach. Bayesian binning predicted the data better than the three other methods in at least 295/336 cases, with a minimal difference of  X   X  0 . 0008 , indicating the general utility of this approach. We have introduced an exact Bayesian binning method for the estimation of PSTHs. Besides treating uncertainty  X  a real problem with small neurophysiological datasets  X  in a principled fashion, it also outperforms competing methods on real neural data. It offers automatic complexity control because the model posterior can be evaluated. While its computational cost is significantly higher than that of the methods we compared it to, it is still fast enough to be useful: evaluating the predictive probability takes less than 1s on a modern PC 2 , with a small memory footprint ( &lt; 10MB for 512 spiketrains).
 Moreover, our approach can easily be adapted to extract other characteristics of neural responses in a Bayesian way, e.g. response latencies or expected bin boundary positions. Our method reveals a clear and sharp initial response onset, a distinct transition from the transient to the sustained part of the response and a well-defined offset. An extension towards joint PSTHs from simultaneous multi-cell recordings is currently being implemented.
 [1] H. Shimazaki and S. Shinomoto. A recipe for optimizing a time-histogram. In B. Sch  X  olkopf, [2] H. Shimazaki and S. Shinomoto. A method for selecting the bin size of a time histogram. [3] J.O. Berger. Statistical Decision Theory and Bayesian Analysis . Springer, New York, 1985. [4] D. Endres and P. F  X  oldi  X  ak. Bayesian bin distribution inference and mutual information. IEEE [5] D. Endres. Bayesian and Information-Theoretic Tools for Neuroscience . PhD thesis, School [6] M. Hutter. Bayesian regression of piecewise constant functions. Technical Report [7] M. Hutter. Exact bayesian regression of piecewise constant functions. Journal of Bayesian [8] D. P. Bertsekas. Dynamic Programming and Optimal Control . Athena Scientific, 2000. [9] M. W. Oram, D. Xiao, B. Dritschel, and K.R. Payne. The temporal precision of neural signals: [10] CJ Bruce, R Desimone, and CG Gross. Visual properties of neurons in a polysensory area in [11] DI Perrett, ET Rolls, and W Caan. Visual neurons responsive to faces in the monkey temporal [12] G.C. Baylis, E.T. Rolls, and C.M. Leonard. Functional subdivisions of the temporal lobe [13] M. W. Oram and D. I. Perrett. Time course of neural responses discriminating different views [14] K Tanaka, H Saito, Y Fukada, and M Moriya. Coding visual images of objects in the infer-[15] N.E. Barraclough, D. Xiao, C.I. Baker, M.W. Oram, and D.I. Perrett. Integration of visual and
