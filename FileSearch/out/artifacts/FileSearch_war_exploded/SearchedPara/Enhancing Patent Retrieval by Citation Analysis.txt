 This paper proposes a method to combine text-based and citation-based retrieval methods in the invalidity patent search. Using the NTCIR-6 test collection including eight years of USPTO patents, we show the effectiveness of our method experimentally.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Experimentation, Measurement Patent retrieval, Citation analysis, NTCIR
Processes of patent retrieval differ significantly, depending on the purpose of retrieval. One process is the  X  X echnology survey X , in which patents related to a specific technology, e.g.,  X  X lue light-emitting diode X , are searched. This pro-cess is similar to ad hoc retrieval tasks targeting nonpatent documents.

Another process is the  X  X nvalidity search X , in which prior arts related to a patent application are searched. Away from academic research, invalidity searches are performed by ex-aminers in government patent offices and searchers in the intellectual property divisions of private companies.
The use of patents in information retrieval research dates back at least to the 1970s [1], when 76 US patents were used to evaluate the effectiveness of local feedback techniques. The experiment in this research simulated the technology survey task, in which seven queries, such as  X  X lectrode struc-ture X , were used. For each query, an electronics engineer selected relevant patents from the 76 US patents.
Osborn and Strzalkowski [5] performed experiments on the invalidity search task. They used approximately 6000 US patents as a target document collection. Each search topic was also a patent. The patents cited in the topic patent (i.e., citations) were used as relevant documents, because citations are usually prior arts for a citing patent. Thus, no relevance judgment by human experts was needed.
 In the Sixth NTCIR Workshop (NTCIR-6), the Patent Retrieval Task was organized and three subtasks were per-formed; Japanese Retrieval, English Retrieval, and Classifi-cation [3] 1 . The English Retrieval subtask intended the in-validity search targeting US patents. However, the number of target documents was larger than those used in previous experiments.

In this paper, we propose our retrieval method partici-pated in NTCIR-6. Our method uses both text content and citations to enhance the invalidity search. We also show the effectiveness of our method experimentally.
In the NTCIR-6 English Retrieval subtask, target doc-uments are USPTO patents published in 1993 X 2000. The number of documents is 981,948. In each document, a num-ber of additional SGML-style tags are inserted to specify the fields, such as bibliographic information and text content.
Each search topic is one or more claims extracted from a patent published in 2000 X 2001. The organizers of the Patent Retrieval Task used a number of criteria to select 2221 patents as search topics.

The pooling-based relevance judgement was not performed and relevant documents for a search topic are the citations in the search topic. If a topic patent and its relevant doc-ument are assigned to the same IPC (International Patent Classification) code, the document can usually be retrieved with a high accuracy. Thus, the degree of the relevance of each citation is classified into the following ranks. The primary IPC code for each patent is identified in the bibliography field.
Traditional research in citation analysis can be used in dif-ferent applications for patents [4]. For example, if a patent is cited by a large number of other patents, this cited patent is possibly a foundation of those citing patents and is, there-fore, important. http://if-lab.slis.tsukuba.ac.jp/fujii/ntc6pat/cfp-en.html
This idea is similar to identifying authoritative pages by analyzing hyperlink structures on the World Wide Web. For example, Yang [7] combined text-based and link-based methods in the Web retrieval.

Following the above ideas, we combine text and citation information in the invalidity patent search.

For the text-based retrieval, we use the claim(s) in each document to perform word-based indexing. We use Okapi BM25 [6] to compute the text-based score for each document with respect to a query.

For the citation-based retrieval, we use two alternative methods. In either method, we first perform the text-based retrieval and obtain top N documents. We then compute the citation-based score for each of the N documents. Finally, we combine the text-based and citation-based scores and resort the N documents. We compute the final score for document d , S ( d ), by Equation (1).
 S ( d ) and S C ( d ) denote the text-based and citation-based scores for d , respectively.  X  is a parametric constant to control the effects of S C .

As a citation-based method, we use PageRank [2], which estimates the probability that a user surfing on the Web vis-its a document. We use this probability as the citation-based score for each document. Given a document collection, the value of PageRank for each document is a constant and is independent of the topic.

As an alternative citation-based method, we propose a topic-sensitive method. We use only citations among the top N documents. As in PageRank, the citation-based score of document d is determined by the total votes by other documents. If d is cited by a large number of documents, a high score is given to d . However, if a document cites n documents, the vote for each cited document is 1 n . We compute S C ( d ) by Equation (2).
 D  X  X  X  d and D d  X  X  X  denote a set of documents citing d and a set of documents cited by d , respectively.
Using the NTCIR-6 test collection described in Section 2, we compared the effectiveness of the following methods. For all methods, N = 1000. We determined the optimal value of  X  in Equation (1) through preliminary experiments. The values of  X  were 0.01 and 0.1 for methods (b) and (c), respectively.

Tables 1 and 2 show different evaluation measures for the above three methods. While in Table 1 we used only A documents as correct answers, in Table 2 we used both A and B documents as correct answers.

In Tables 1 and 2,  X  X @X X  denotes the recall at the top X documents. For the value of X , we used 100, 200, and 500, Table 1: Evaluation results for rigid relevance. Table 2: Evaluation results for relaxed relevance. because examiners and searchers usually investigate a couple of hundreds of documents for a single topic. We also used Mean Average Precision (MAP) as an evaluation measure.
Looking at Tables 1 and 2, methods (b) and (c) were more effective than method (a), irrespective of the evalua-tion measure and the relevance degree. However, method (c) was more effective than method (b). We used the paired t-test for statistical testing. Although the MAP values of (a) and (b) in Table 1 were not significantly different, for the other cases the difference was statistically significant at the 1% level.
We used eight years of USPTO patents and demonstrated the effectiveness of the citation analysis in the invalidity patent search. A combination of the text-based and citation-based methods improved the text-based method. The im-provement was even greater when we used the topic-sensitive citation-based method. [1] R. Attar and A. S. Fraenkel. Local feedback in full-text [2] S. Brin and L. Page. The anatomy of a large-scale [3] A. Fujii, M. Iwayama, and N. Kando. Overview of the [4] M. M. S. Karki. Patent citation analysis: A policy [5] M. Osborn and T. Strzalkowski. Evaluating document [6] S. E. Robertson, S. Walker, S. Jones, M. M.
 [7] K. Yang. Combining text-and link-based retrieval
