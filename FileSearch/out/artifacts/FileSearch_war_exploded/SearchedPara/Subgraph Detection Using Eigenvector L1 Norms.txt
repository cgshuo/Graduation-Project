 A graph G = ( V, E ) denotes a collection of entities, represented by vertices V , along with some relationship between pairs, represented by edges E . Due to this ubiquitous structure, graphs are used in a variety of applications, including the natural sciences, social network analysis, and engineering. While this is a useful and popular way to represent data, it is difficult to analyze graphs in the traditional statistical framework of Euclidean vector spaces.
 In this article we investigate the problem of detecting a small, dense subgraph embedded into an unweighted, undirected background. We use L 1 properties of the eigenvectors of the graph X  X  modu-larity matrix to determine the presence of an anomaly, and show empirically that this technique has reasonable power to detect a dense subgraph where lower connectivity would be expected. In Section 2 we briefly review previous work in the area of graph-based anomaly detection. In Section 3 we formalize our notion of graph anomalies, and describe our experimental regime. In Section 4 we give an overview of the modularity matrix and observe how its eigenstructure plays a role in anomaly detection. Sections 5 and 6 respectively detail subgraph detection results on simulated and actual network data, and in Section 7 we summarize and outline future research. The area of anomaly detection has, in recent years, expanded to graph-based data [1, 2]. The work of Noble and Cook [3] focuses on finding a subgraph that is dissimilar to a common substructure in the network. Eberle and Holder [4] extend this work using the minimum description length heuristic to determine a  X  X ormative pattern X  in the graph from which the anomalous subgraph deviates, basing 3 detection algorithms on this property. This work, however, does not address the kind of anomaly we describe in Section 3; our background graphs may not have such a  X  X ormative pattern X  that occurs over a significant amount of the graph. Research into anomaly detection in dynamic graphs by Priebe et al [5] uses the history of a node X  X  neighborhood to detect anomalous behavior, but this is not directly applicable to our detection of anomalies in static graphs.
 There has been research on the use of eigenvectors of matrices derived from the graphs of interest to detect anomalies. In [6] the angle of the principal eigenvector is tracked in a graph representing a computer system, and if the angle changes by more than some threshold, an anomaly is declared present. Network anomalies are also dealt with in [7], but here it is assumed that each node in the network has some highly correlated time-domain input. Since we are dealing with simple graphs, this method is not general enough for our purposes. Also, we want to determine the detectability of small anomalies that may not have a significant impact on one or two principal eigenvectors. There has been a significant amount of work on community detection through spectral properties of graphs [8, 9, 10]. Here we specifically aim to detect small, dense communities by exploiting these same properties. The approach taken here is similar to that of [11], in which graph anomalies are detected by way of eigenspace projections. We here focus on smaller and more subtle subgraph anomalies that are not immediately revealed in a graph X  X  principal components. As in [12, 11], we cast the problem of detecting a subgraph embedded in a background as one of detecting a signal in noise. Let G B = ( V, E ) denote the background graph; a network in which there exists no anomaly. This functions as the  X  X oise X  in our system. We then define the anoma-lous subgraph (the  X  X ignal X ) G S = ( V S , E S ) with V S  X  V . The objective is then to evaluate the following binary hypothesis test; to decide between the null hypothesis H 0 and alternate hypothesis H Here the union of the two graphs G B  X  G S is defined as G B  X  G S = ( V, E  X  E S ) .
 In our simulations, we formulate our noise and signal graphs as follows. The background graph G B is created by a graph generator, such as those outlined in [13], with a certain set of parameters. We then create an anomalous  X  X ignal X  graph G S to embed into the background. We select the vertex subset V S from the set of vertices in the network and embed G S into G B by updating the edge set to be E  X  E S . We apply our detection algorithm to graphs with and without the embedding present to evaluate its performance. Newman X  X  notion of the modularity matrix [8] associated with an unweighted, undirected graph G is given by Here A = { a ij } is the adjacency matrix of G , where a ij is 1 if there is an edge between vertex i and vertex j and is 0 otherwise; and K is the degree vector of G , where the i th component of K is the number of edges adjacent to vertex i . If we assume that edges from one vertex are equally likely to be shared with all other vertices, then the modularity matrix is the difference between the  X  X ctual X  and  X  X xpected X  number of edges between each pair of vertices. This is also very similar to Figure 1: Scatterplots of an R-MAT generated graph projected into spaces spanned by two eigenvec-tors of its modularity matrix, with each point representing a vertex. The graph with no embedding (a) and with an embedded 8-vertex clique (b) look the same in the principal components, but the embedding is visible in the eigenvectors corresponding to the 18th and 21st largest eigenvalues (c). the matrix used as an  X  X bserved-minus-expected X  model in [14] to analyze the spectral properties of random graphs.
 Since B is real and symmetric, it admits the eigendecomposition B = U  X  U T , where U  X  R | V | X | V | is a matrix where each column is an eigenvector of B , and  X  is a diagonal matrix of eigenvalues. We denote by  X  i , 1  X  i  X  | V | , the eigenvalues of B , where  X  i  X   X  i +1 for all i , and by u i the unit-magnitude eigenvector corresponding to  X  i .
 Newman analyzed the eigenvalues of the modularity matrix to determine if the graph can be split into two separate communities. As demonstrated in [11], analysis of the principal eigenvectors of B can also reveal the presence of a small, tightly-connected component embedded in a large graph. This is done by projecting B into the space of its two principal eigenvectors, calculating a Chi-squared test statistic, and comparing this to a threshold. Figure 1(a) demonstrates the projection of an R-MAT Kronecker graph [15] into the principal components of its modularity matrix.
 Small graph anomalies, however, may not reveal themselves in this subspace. Figure 1(b) demon-strates an 8-vertex clique embedded into the same background graph. In the space of the two prin-cipal eigenvectors, the symmetry of the projection looks the same as in Figure 1(a). The foreground vertices are not at all separated from the background vertices, and the symmetry of the projection has not changed (implying no change in the test statistic). Considering only this subspace, the subgraph of interest cannot be detected reliably; its inward connectivity is not strong enough to stand out in the two principal eigenvectors.
 The fact that the subgraph is absorbed into the background in the space of u 1 and u 2 , however, does not imply that it is inseparable in general; only in the subspace with the highest variance. Borrowing language from signal processing, there may be another  X  X hannel X  in which the anomalous signal subgraph can be separated from the background noise. There is in fact a space spanned by two eigenvectors in which the 8-vertex clique stands out: in the space of the u 18 and u 21 , the two eigenvectors with the largest components in the rows corresponding to V S , the subgraph is clearly separable from the background, as shown in Figure 1(c). 4.1 Eigenvector L 1 Norms The subgraph detection technique we propose here is based on L 1 properties of the eigenvectors of the graph X  X  modularity matrix, where the L 1 norm of a vector x = [ x 1  X  X  X  x N ] T is k x k 1 := P a few values of i , then its L 1 norm will be smaller than that of a vector of the same magnitude where this is not the case. For example, if x  X  R 1024 has unit magnitude and only has nonzero components along two of the 1024 axes, then k x k 1  X  axes, then k x k 1 = 32 . This property has been exploited in the past in a graph-theoretic setting, for finding maximal cliques [16, 17].
 This property can also be useful when detecting anomalous clustering behavior. If there is a subgraph G
S that is significantly different from its expectation, this will manifest itself in the modularity Figure 2: L 1 analysis of modularity matrix eigenvectors. Under the null model, k u 18 k has the distribution in (a). With an 8-vertex clique embedded, k u 18 k 1 falls far from its average value, as shown in (b). matrix as follows. The subgraph G S has a set of vertices V S , which is associated with a set of indices corresponding to rows and columns of the adjacency matrix A . Consider the vector x  X  { 0 , 1 } N , where x i is 1 if v i  X  V S and x i = 0 otherwise. For any S  X  V and v  X  V , let d S ( v ) denote the number of edges between the vertex v and the vertex set S . Also, let d S ( S 0 ) := P v  X  S 0 d S ( v ) and d ( v ) := d V ( v ) . We then have and k x k 2 = p | V S | . Note that d ( V ) = 2 | E | . A natural interpretation of (2) is that Bx repre-sents the difference between the actual and expected connectivity to V S across the entire graph, and likewise (3) represents this difference within the subgraph. If x is an eigenvector of B , then of course x T Bx/ ( k Bx k 2 k x k 2 ) = 1 . Letting each subgraph vertex have uniform internal and external degree, this ratio approaches 1 as P v /  X  V P v  X  V S ( d V S ( v )  X  d ( v ) d ( V S ) /d ( V )) subset of background vertices, x is likely to be well-correlated with an eigenvector of B . (This be-comes more complicated when there are several eigenvalues that are approximately d V S ( V S ) / | V S | , but this typically occurs for smaller graphs than are of interest.) Newman made a similar observa-tion: that the magnitude of a vertex X  X  component in an eigenvector is related to the  X  X trength X  with which it is a member of the associated community. Thus if a small set of vertices forms a commu-nity, with few belonging to other communities, there will be an eigenvector well aligned with this set, and this implies that the L 1 norm of this eigenvector would be smaller than that of an eigenvector with a similar eigenvalue when there is no anomalously dense subgraph. 4.2 Null Model Characterization To examine the L 1 behavior of the modularity matrix X  X  eigenvectors, we performed the following experiment. Using the R-MAT generator we created 10,000 graphs with 1024 vertices, an average degree of 6 (the result being an average degree of about 12 since we make the graph undirected), and a probability matrix For each graph, we compute the modularity matrix B and its eigendecomposition. We then compute k u i k 1 for each i and store this value as part of our background statistics. Figure 2(a) demonstrates deviation of 0.35) and no large deviations from the mean under the null ( H 0 ) model. After compiling background data, we computed the mean and standard deviation of the L 1 norms for each u i . Let  X  i be the average of k u i k 1 and  X  i be its standard deviation. Using the R-MAT graph with the embedded 8-vertex clique, we observed eigenvector L 1 norms as shown in Figure 2(b). In have L 1 norms close to the mean for the associated index. There are very few cases with a deviation from the mean of greater than 3  X  . Note also that  X  i decreases with decreasing i . This suggests that the community formation inherent in the R-MAT generator creates components strongly associated with the eigenvectors with larger eigenvalues.
 The one outlier is u 18 , which has an L 1 norm that is over 10 standard deviations away from the mean. Note that u 18 is the horizontal axis in Figure 1(c), which by itself provides significant separation between the subgraph and the background. Simple L 1 analysis would certainly reveal the presence of this particular embedding. With the L 1 properties detailed in Section 4 in mind, we propose the following method to determine the presence of an embedding. Given a graph G , compute the eigendecomposition of its modularity matrix. For each eigenvector, calculate its L 1 norm, subtract its expected value (computed from the background statistics), and normalize by its standard deviation. If any of these modified L 1 norms is less than a certain threshold (since the embedding makes the L 1 norm smaller), H 1 is declared, and H 0 is declared otherwise. Pseudocode for this detection algorithm is provided in Algorithm 1. Input: Graph G = ( V, E ) , Integer k , Numbers ` 1 MIN ,  X  [1 ..k ] ,  X  [1 ..k ] B  X  M OD M AT ( G )
U  X  EIGENVECTORS ( B, k )  X  X  k eigenvectors of B  X  X  for i  X  1 to k do end for return H 0  X  X  no embedding found  X  X  We compute the eigenvectors of B using eigs in MATLAB, which has running time O ( | E | kh + | V | k 2 h + k 3 h ) , where h is the number of iterations required for eigs to converge [10]. While the modularity matrix is not sparse, it is the sum of a sparse matrix and a rank-one matrix, so we can still compute its eigenvalues efficiently, as mentioned in [8]. Computing the modified L 1 norms and comparing them to the threshold takes O ( | V | k ) time, so the complexity is dominated by the eigendecomposition.
 The signal subgraphs are created as follows. In all simulations in this section, | V S | = 8 . For each simulation, a subgraph density of 70%, 80%, 90% or 100% is chosen. For subraphs of this size and density, the method of [11] does not yield detection performance better than chance. The subgraph is created by, uniformly at random, selecting the chosen proportion of the 8 2 possible edges. To determine where to embed the subgraph into the background, we find all vertices with at most 1, 3 or 5 edges and select 8 of these at random. The subgraph is then induced on these vertices. For each density/external degree pair, we performed a 10,000-trial Monte Carlo simulation in which we create an R-MAT background with the same parameters as the null model, embed an anomalous subgraph as described above, and run Algorithm 1 with k = 100 to determine whether the embed-ding is detected. Figure 3 demonstrates detection performance in this experiment. In the receiver operating characteristic (ROC), changing the L 1 threshold ( ` 1 MIN in Algorithm 1) changes the po-sition on the curve. Each curve corresponds to a different subgraph density. In Figure 3(a), each vertex of the subgraph has 1 edge adjacent to the background. In this case the subgraph connectivity is overwhelmingly inward, and the ROC curve reflects this. Also, the more dense subgraphs are more detectable. When the external degree is increased so that a subgraph vertex may have up to 3 edges adjacent to the background, we see a decline in detection performance as shown in Figure 3(b). Figure 3(c) demonstrates the additional decrease in detection performance when the external subgraph connectivity is increased again, to as much as 5 edges per vertex. Figure 3: ROC curves for the detection of 8-vertex subgraphs in a 1024-vertex R-MAT background. Performance is shown for subgraphs of varying density when each foreground vertex is connected to the background by up to 1, 3 and 5 edges in (a), (b) and (c), respectively. To verify that we see similar properties in real graphs that we do in simulated ones, we analyzed five data sets available in the Stanford Network Analysis Package (SNAP) database [18]. Each net-work is made undirected before we perform our analysis. The data sets used here are the Epinions who-trusts-whom graph (Epinions, | V | = 75,879, | E | = 405,740) [19], the arXiv.org collaboration networks on astrophysics (AstroPh, | V | = 18,722, | E | = 198,050) and condensed matter (CondMat, | V | =23,133, | E | =93,439) [20], an autonomous system graph (asOregon, | V | =11,461, | E | =32,730) [21] and the Slashdot social network (Slashdot, | V | =82,168, | E | =504,230) [22]. For each graph, we compute the top 110 eigenvectors of the modularity matrix and the L 1 norm of each. Comparing each L 1 sequence to a  X  X moothed X  (i.e., low-pass filtered) version, we choose the two eigenvec-tors that deviate the most from this trend, except in the case of Slashdot, where there is only one significant deviation.
 Plots of the L 1 norms and scatterplots in the space of the two eigenvectors that deviate most are shown in Figure 4. The eigenvectors declared are highlighted. Note that, with the exception of the asOregon, we see as similar trend in these networks that we did in the R-MAT simulations, with the L 1 norms decreasing as the eigenvalues increase (the L 1 trend in asOregon is fairly flat). Also, with the exception of Slashdot, each dataset has a few eigenvectors with much smaller norms than those with similar eigenvalues (Slashdot decreases gradually, with one sharp drop at the maximum eigenvalue).
 The subgraphs detected by L 1 analysis are presented in Table 1. Two subgraphs are chosen for each dataset, corresponding to the highlighted points in the scatterplots in Figure 4. For each subgraph we list the size (number of vertices), density (internal degree divided by the maximum number of edges), external degree, and the eigenvector that separates it from the background. The subgraphs are quite dense, at least 80% in each case.
 To determine whether a detected subgraph is anomalous with respect to the rest of the graph, we sample the network and compare the sample graphs to the detected subgraphs in terms of density and external degree. For each detected subgraph, we take 1 million samples with the same number of vertices. Our sampling method consists of doing a random walk and adding all neighbors of each vertex in the path. We then count the number of samples with density above a certain threshold and external degree below another threshold. These thresholds are the parenthetical values in the 4th and 5th columns of Table 1. Note that the thresholds are set so that the detected subgraphs comfortably meet them. The 6th column lists the number of samples out of 1 million that satisfy both thresholds. In each case, far less than 1% of the samples meet the criteria. For the Slashdot dataset, no sample was nearly as dense as the two subgraphs we selected by thresholding along the principal eigenvector. After removing samples that are predominantly correlated with the selected eigenvectors, we get the parenthetical values in the same column. In most cases, all of the samples meeting the thresholds are correlated with the detected eigenvectors. Upon further inspection, those remaining are either correlated with another eigenvector that deviates from the overall L 1 trend, or correlated with multiple eigenvectors, as we discuss in the next section. Figure 4: Eigenvector L 1 norms in real-world network data (left column), and scatterplots of the projection into the subspace defined by the indicated eigenvectors (right column). Table 1: Subgraphs detected by L 1 analysis, and a comparison with randomly-sampled subgraphs in the same network.
 Figure 5: An 8-vertex clique that does not create an anomalously small L 1 norm in any eigenvector. The scatterplot looks similar to one in which the subgraph is detectable, but is rotated. In this article we have demonstrated the efficacy of using eigenvector L 1 norms of a graph X  X  mod-ularity matrix to detect small, dense anomalous subgraphs embedded in a background. Casting the problem of subgraph detection in a signal processing context, we have provided the intuition behind the utility of this approach, and empirically demonstrated its effectiveness on a concrete example: detection of a dense subgraph embedded into a graph generated using known parameters. In real network data we see trends similar to those we see in simulation, and examine outliers to see what subgraphs are detected in real-world datasets.
 Future research will include the expansion of this technique to reliably detect subgraphs that can be separated from the background in the space of a small number of eigenvectors, but not necessarily one. While the L 1 norm itself can indicate the presence of an embedding, it requires the subgraph to be highly correlated with a single eigenvector. Figure 5 demonstrates a case where considering mul-tiple eigenvectors at once would likely improve detection performance. The scatterplot in this figure looks similar to the one in Figure 1(c), but is rotated such that the subgraph is equally aligned with the two eigenvectors into which the matrix has been projected. There is not significant separation in any one eigenvector, so it is difficult to detect using the method presented in this paper. Minimizing the L 1 norm with respect to rotation in the plane will likely make the test more powerful, but could prove computationally expensive. Other future work will focus on developing detectability bounds, the application of which would be useful when developing detection methods like the algorithm outlined here.
 Acknowledgments This work is sponsored by the Department of the Air Force under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government. [1] J. Sun, J. Qu, D. Chakrabarti, and C. Faloutsos,  X  X eighborhood formation and anomaly detec-[2] J. Sun, Y. Xie, H. Zhang, and C. Faloutsos,  X  X ess is more: Compact matrix decomposition for [3] C. C. Noble and D. J. Cook,  X  X raph-based anomaly detection, X  in Proc. ACM SIGKDD Int X  X . [4] W. Eberle and L. Holder,  X  X nomaly detection in data represented as graphs, X  Intelligent Data [5] C. E. Priebe, J. M. Conroy, D. J. Marchette, and Y. Park,  X  X can statistics on enron graphs, X  [6] T. Id  X  e and H. Kashima,  X  X igenspace-based anomaly detection in computer systems, X  in Proc. [7] S. Hirose, K. Yamanishi, T. Nakata, and R. Fujimaki,  X  X etwork anomaly detection based on [8] M. E. J. Newman,  X  X inding community structure in networks using the eigenvectors of matri-[9] J. Ruan and W. Zhang,  X  X n efficient spectral algorithm for network community discovery and [10] S. White and P. Smyth,  X  X  spectral clustering approach to finding communities in graphs, X  in [11] B. A. Miller, N. T. Bliss, and P. J. Wolfe,  X  X oward signal processing theory for graphs and other [12] T. Mifflin,  X  X etection theory on random graphs, X  in Proc. Int X  X  Conf. on Information Fusion , [13] D. Chakrabarti and C. Faloutsos,  X  X raph mining: Laws, generators, and algorithms, X  ACM [14] F. Chung, L. Lu, and V. Vu,  X  X he spectra of random graphs with given expected degrees, X  Proc. [15] D. Chakrabarti, Y. Zhan, and C. Faloutsos,  X  X -MAT: A recursive model for graph mining, X  in [16] T. S. Motzkin and E. G. Straus,  X  X axima for graphs and a new proof of a theorem of Tur  X  an, X  [17] C. Ding, T. Li, and M. I. Jordan,  X  X onnegative matrix factorization for combinatorial opti-[18] J. Leskovec,  X  X tanford network analysis package. X  http://snap.stanford.edu. [19] M. Richardson, R. Agrawal, and P. Domingos,  X  X rust management for the semantic web, X  in [20] J. Leskovec, J. Kleinberg, and C. Faloutsos,  X  X raph evolution: Densification and shinking [21] J. Leskovec, J. Kleinberg, and C. Faloutsos,  X  X raphs over time: Densification laws, shinking [22] J. Leskovec, K. Lang, A. Dasgupta, and M. Mahoney,  X  X ommunity structure in large networks:
