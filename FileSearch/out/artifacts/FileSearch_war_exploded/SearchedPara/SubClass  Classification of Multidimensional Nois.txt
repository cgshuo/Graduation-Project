 Data produced in application domains like life sciences, meteorology, telecommuni-cation, and multimedia entertainment is rapidly growing, increasing the demand for data mining techniques which help users generate knowledge from data. Many applica-tions require incoming data to be classifie d according to models derived from labeled historic data. In a current project, we investigate flight delays for airport scheduling purposes. The significance of flight delays can e.g. be studied in reports of the Bureau of Transportation Statistics in the U.S. [7] and the Central Office for Delay Analysis of Eurocontrol [11]. Extensive flight data is recorded by flight information systems at all major airports. Using such databases, we classify flights as on time, delayed or ahead of schedule. This classification is essential in refining robust scheduling methods for airport resources and ground staff (like the one presented in [6]).

For classification, numerous techniques exist. For our noisy database that contains nominal attributes, numerical classifiers are not applicable. Neural networks or support vector machines do not allow users to easily understand the decision model for flight classification [20,17]. Bayes classifiers, decision trees, and nearest neighbor classifiers provide explanatory information, yet assume globally uniform relevance of attributes [20,18,3]. It has been shown that each type of classifier has its merit; there is no inherent superiority of any classifier [10]. However, classification is difficult in the presence of noise. Moreover, patterns may not show across all data attributes for all classes to be learned. In multidimensional data only a s ubgroup of attributes may be relevant for classification. This relevance is not globally uniform, but differs from class to class and from instance to instance.

We have validated the assumption of local relevance of attributes for the flight classi-fication project by training several types of classifiers. When using only attributes which are determined as relevant by standard statistical tests, classification accuracy actually drops. This suggests that globally irrelevant attributes are nonetheless locally relevant for individual patterns. We therefore target at grouping flights with similar characteris-tics and identifying structure on the attribute level. In the flight domain, several aspects support the locality of flight delay structures. As an example, passenger figures may only influence departure delays when the air craft is parked at a remote stand, i.e. when bus transportation is required. At some times of the day, these effects may be super-posed by other factors like runway congesti on. Weather conditions and other influences not recorded in the data cause significant noise.

Recent classification approaches like [9] use local weighting in nearest neighbor classification to overcome this drawback. Com bing relevant attributes hierarchically a subspace is constructed for nearest neighbor classification. However, locally adaptive nearest neighbor methods do not consider the correlation of different attribute sets. Association rules have been extended to classification [16]. Recent approaches adopt subspace clustering methods to identify relev ant subspaces for rule based classification [21].

In this work, we propose a nearest neighbor classifier which directly uses the result of our new subspace clustering method. Note that our approach is different from semi-supervised learning where unlabeled data is used for training [22]. Our approach as-sumes class labels that are directly incorporated into subspace clustering. Clustering is helpful for understanding the overall structure of a data set. Its aim is automatic group-ing of the data in absence of any known class labels in historic data [13]. Since class labels are not known in advance ( X  X nsupervised learning X ), they are not used to classify according to given groupings ( X  X upervised lear ning X ). Hence clustering is not appropri-ate for classification purposes by its very natu re [13]. However, the structures detected by clustering may be helpful for detecting local relevance of attributes. For noisy and high-dimensional data, clustering is often infeasible as clusters are hidden by irrelevant attributes. Different attribute combinations might show different clustering structures, thus the aim of subspace clustering is to detect clusters in arbitrary projections ( X  X ub-spaces X ) of the attributes. As the number of subspaces is exponential in the number of attributes, most approaches try to prune th e subspace search space [1,8,4]. Subspace clustering has been shown to successfully detect locally relevant attribute combinations [15,5].

We propose combining both worlds, supervised learning and unsupervised learning by incorporating class label information into subspace search and clustering. Classifi-cation based on these classifying subspace cl usters exploits both class and local corre-lation information. The flight classification problem is used to evaluate our model. Its applicability, however, goes beyond this scen ario. In fact, there are many more applica-tion areas where classification has to handle noisy multidimensional data with locally relevant attributes.

This paper is structured as follows: we d efine interesting subspaces for subspace classification in Section 2.1. Classifying s ubspace clusters and the overall classifica-tion scheme are discussed in Sections 2.2 and 2.3, respectively. Algorithmic concepts are presented in Section 3. The proposed method is evaluated in the experiments in Section 4 on both synthetic and flight data, before concluding the paper. Subspace clustering is a recent research area wh ich tries to detect local structures in the presence of noise or high-dimensional data where meaningful clusters can no longer be detected in all attributes [1,8,15]. As s earching all possible subspaces is usually in-tractable, subspace clustering algorithms try to focus on promising subspace regions. The challenge is a suitable notion of inter estingness for subspaces to find all relevant clusters. Subspace clustering is a technique well-suited to identify relevant regions of historic data, however, it is not suited for classification  X  X s is X . Our classification ap-proach is capable of exploiting local patterns in the data for classification. This requires detecting subspaces and subspace clusters th at are also based on class structure. Our SubClass model thus comprises three steps:  X  Step 1: interesting subspaces for classifying clusters: Section 2.1  X  Step 2: classifying subspace clusters : Section 2.2  X  Step 3: a classification scheme: Section 2.3. 2.1 Step 1: Interesting Subspaces Interesting subspaces for classifying clusters exhibit a clustering structure in their at-tributes as well as coherent class label in formation. Such a structure is reflected by homogeneity in the attribute values or class labels of that subspace. Homogeneity can be measured using Shannon Entropy [19], or entropy for short. From an information theoretic perspective, Shannon entropy is the minimum number of bits required for en-coding information. More frequently occurring events are encoded with fewer bits than less frequent ones. The sum over logarithmic probabilities weighted by their probability, measures the amount of information, i.e. the heterogeneity of the data.
 Definition 1. Shannon Entropy. Given a random variable X and its possible events v , .., v m the Shannon Entropy H ( X ) is defined as: Transferring the entropy notion to the clustering or classification domain, an attribute can be seen as a random variable whose domain is the set of all possible events. In case of continuous domains, the entropy requires discretization of attributes. Entropy according to a set of attributes with respect to a set of class labels is then: Definition 2. Attribute Entropy. Given a set of attributes X 1 ,...,X m , their possible values v 1 ,...,v m , and class labels C = { c 1 ,...,c n } , attribute entropy is defined as: Attribute entropy is thus the sum over all conditional attribute entropy value combina-tions weighted by the class label probabilities. It is a measure for the clustering tendency for all class labels c i of a subspace in terms of the attributes. To measure the cluster-ing tendency in terms of individual class la bels, we define class entropy according to conditional entropy H ( C | X ) (as e.g. in [18]).
 Definition 3. Class Entropy. Given a set of attributes X 1 ,...,X m , their possible val-ues v 1 ,...,v m , and class label C the conditional entropy of a segmentation along these attribute values is defined as: H ( C | X 1 ,...,X m )=  X  Class entropy is thus the sum over all conditional class entropy value combinations for individual class labels C . It corresponds to investigating the data for individual classes instead of aggregated as for attribute entropy.

We are interested in subspaces that exhibit both a distinct class structure as well as a clear clustering structure. Since entropy measures homogeneity, we are interested in low entropy values that reflect a non-uniform distribution of class or attribute values.
However, comparing subspaces using entro py is clearly biased with respect to the number of attributes. Subspaces with more attributes typically have lower entropy val-ues. This is due to the fact that with increasing attribute number, objects tend to be less similar: each attribute contributes potential dissimilarity [5]. Thus, we have to normalize entropy with respect to the number of attributes. Normalization to a range of [0,1] can be achieved by taking the maximum possible entropy value for a given number of attributes into account. Maximum entropy means all values are equally likely, i.e. a uniform distri-bution. H uniform ( X 1 , .., X m | C ) for d = | X 1  X  X  X  X  X  X m | possible attribute combina-since in uniform distributi on, each attribute value occurs 1 /d times. For larger numbers of attributes, the theoretical upper bound of log 2 d cannot be reached, as the actual num-ber of instances is smaller than the number of possible attribute value combinations d . To account for this, we the number of instances | I | is used in this case: In a similar spirit, we use the overall class distribution to normalize class entropy: Since those subspaces are interesting that c over both aspects, we define interestingness as a convex combination of attribute and clas s entropy, provided that each of the two is within reasonable bounds: Definition 4. Subspace Interestingness. Given attributes X 1 ,...,X m ,aclass attribute C , and a weighting factor 0  X  w  X  1 , a subspace is interesting with respect to thresholds  X ,  X  iff: Thus, a subspace is interesting for subspace classification if it shows low normalized class and attribute entropy as an indi cation of class and cluster structure. w allows as-signing different weights to these two aspects for different applications, while  X  is set to fairly relaxed threshold values to ensure that both aspects fulfill minimum entropy requirements. 2.2 Step 2: Classifying Subspace Clusters Having defined interesting subspaces, the next step is detecting classifying subspace clusters . On discretized data, clusters can be defined as frequent attribute value com-binations. To incorporate class information, these groupings should be homogeneous with respect to class label. We defined the absolute frequency as the number of objects o which exhibit the attribute values ( v 1 ,...,v m ) in subspace S (projection o | S contains those attribute values v i from o where X i  X  S ).
To ensure that non-trivial clusters are mined, we normalize frequency with respect to the expected frequency of uniformly dist ributed subspaces. The expected frequency is the number of cluster objects in comparison to the number of instances | I | per at-tribute combination under uni form distribution. Classifying subspace clusters exceed minimum frequency for both absolute and relative (expected) frequency. Note that min-imum absolute frequency simply ensures th at a cluster exceeds a minimum size even for very small expected frequency values: Definition 5. Classifying Subspace Cluster. Given a subspace S of attributes X 1 ,..., X m , a classifying subspace cluster SC with respect to attribute values v 1 ,...,v m , minimum frequency thresholds  X  1 , X  2 , and maximum entropy  X  is defined as follows:  X  H N ( C | X 1 = v 1 ,...,X m = v m )  X   X   X  AbsF req ( v 1 ,...,v m )  X   X  1  X  ExpFreq ( v 1 ,...,v m )  X   X  2 Classifying subspace clusters have low nor malized class entropy, as well as high fre-quency in terms of attribute values. Thus, they are homogeneous in terms of class and show local attribute correlations. 2.3 Step 3: Classification Classification of a given object o is based on the class label distribution of similar clas-sifying subspace clusters. For nominal values as they occur in our flight data, an object o is typically contained in several subspace clusters and similarity is reduced to con-tainment. Let CSC ( o )= { SC i | v k = o k  X  v k  X  SC i } denote the set of all classifying subspace clusters containing object o . Simply assigning the majority class label from this set CSC ( o ) would be biased with respect to very large and redundant subspace clusters, where redundancy means similar clusters in slightly varying projections [5]. We therefore propose an iterative procedure that takes the information gain into ac-count to build the decision set DS k ( o ) .

Just as in the subspace clustering step we measure class homogeneity using the con-ditional class entropy. Starting with an empty decision set and apriori knowledge about class distribution H ( C ) we select up to k subspace clusters with maximal information gain on the class label as long as more than  X  1 objects are contained in the decision space, i.e. the projection to the union of di mensions of the subspace clusters in the decision set.
 Definition 6. Classification. Given a dataset D , parameter k , an object o =( o 1 ,..., o ) is classified to the majority class label of decision set DS k . DS k is iteratively constructed from DS 0 =  X  by selecting the subspace cluster SC j  X  CSC ( o ) which maximizes the information gain about the class label: DS j = DS j  X  1  X  SC j ,SC j = argmax under the constraints that the decision space contains at least  X  1 objects: and that the information gain is positive Hence, the decision set of an object o is created by choosing those k subspace clusters containing o that provide most information on the class label, as long as more than a minimum number of object s are in the decision space. o is then classified according to the majority in the decision set DS k . The decision set is then the set of locally relevant attributes that were used to classify object o . The attributes in the decision set are helpful for users wishing to understand the information that led to classification. Our algorithmic concept focuses on step 1 that is the computationally most complex. A simple brute-force search would require evaluating all 2 N subspaces which is not acceptable for high dimensionality N . We thus propose lossless pruning of subspaces based on two entropy monotonicities.
 Theorem 1. Upward Monotony of the Class Entropy. Given a set of m attributes, subspace S = { X 1 , .., X m } , e  X  IR + and T  X  S , the class entropy in subspace T is less than or at most equal to the class entropy of its superspace S : Proof. The theorem follows immediately from H ( X | X i ,X j )  X  H ( X | X i ) [12]. This theorem states that the class entropy decreases monotonically with growing num-ber of attributes. Conversely, attribute entropy increases monotonically with the number of attributes.
 Theorem 2. Downward Monotony of the Attribute Entropy. Given a set of m at-tributes, subspace S = { X 1 , .., X m } , e  X  IR + and T  X  S , the attribute entropy in subspace T is greater than or at most equal to the class entropy of its superspace S : Proof. The theorem follows immediately from H ( X i ,X j | C )  X  H ( X i | C ) [12]. We exploit monotonicity by pruning  X  all those subspaces T whose superspaces S  X  T fail the class entropy threshold.  X  Prune all those superspaces T whose subspaces S  X  T fail the attribute entropy
Our proposed algorithm alternately determines lower dimensional and higher dimen-sional one-sided homogeneous subspaces, i.e. subspaces that are homogeneous w.r.t. to class or attribute entropy, respectively. In each step new candidates are created from the set of one-sided homogeneous subspaces mined in the last step.

Figure 1 illustrates pruning in a subspace lattice of four attributes. The solid line is the boundary for pruning according to attribut e entropy and the dashed line according to class entropy. Each subspace below the attribute boundary and above the class bound-ary is homogeneous with respect to the entropy considered. The subspaces between both boundaries are interesting subspace candidates, whose combined entropy has to be computed in the next step.

For the bottom up case, the apriori property, originally from association rule mining, can be used to create new candidates [2,8,15]. Following the apriori approach, we join two attribute homogeneous subspaces of size m with identical prefixes (e.g. in lexi-cographic ordering) to create a candidate subspace of size m +1 . After this, each new candidate is checked for entropy validity, i.e. if all of its possible subspace of cardinality m are contained in the set of attribut e homogeneous candidate subspaces.

We suggest a similar method for top down candidate generation using class mono-tonicity. From the set of class homogeneous subspaces of dimensionality m , we gener-ate all subspace candidates of dimensionality m  X  1 . We develop a method that ensures that each subspace candidate is only generated once. Based on the lexicographic order, our method uniquely generates a subspace of dimensionality m  X  1 from its smallest su-perspace. Note that this guarantees that all candidates but no supe rfluous candidates are generated (see example below). After this, just as with apriori, we check whether all su-perspaces containing the newly generated candidates are class homogeneous subspaces. Otherwise the new generated subspace is removed from the candidate set.
 Example. Assume four attributes X 1 ,...,X 4 from the previous step subspaces X 1 X 2 X 3 , X 1 X 2 X 4 ,and X 2 X 3 X 4 that satisfy the class entropy criterion. In order to generate candidates, we iterate over these subspaces in lexicographic order. The first three-dimensional subspace X 1 X 2 X 3 generates the two-dimensional subspaces X 1 X 2 (drop X 3 ), X 1 X 3 (drop X 2 ), X 2 X 3 (drop X 1 ). Next, X 1 X 2 X 4 generates X 1 X 4 and X 2 X 4 . X 1 X 2 is not generated, because dropping X 4 is not possible, as it is preceded by X 3 which is not contained in this subspace. The last three-dimensional subspace X 2 X 3 X 4 does not generate any two-dimensional subspace since the leading X 1 is not contained; its subsets X 2 X 3 and X 2 X 4 have been generated by other three-dimen-sional subspaces. After candida te generation, we check their respective supersets. For example, for X 1 X 2 , its supersets X 1 X 2 X 3 and X 1 X 2 X 4 exist. For X 1 X 3 , its superset X 1 X 2 X 3 exists, but X 1 X 3 X 4 does not, so it is removed from further consideration following monotony pruning. Likewise, X 1 X 4 is removed as X 1 X 3 X 4 is missing, but X 2 X 3 and X 2 X 4 are kept.

As we use two entropies, one with downward, one with upward pruning, subspaces may need to be considered twice. Minimizing computations is thus a trade-off. Fig-ure 3 illustrates these effects. A missing candidate in S Down (e.g. X 1 X 2 ) means that this candidate has an attribute entropy above  X  . According to the attribute monotony, superspaces (e.g. X 1 X 2 X 3 ) have an attribute entropy above  X  and thus the combined entropy is also greater than  X  . Even though the subspace could be pruned according to combined entropy, it is still required for valid class entropy candidate generation. There is thus a trade off between avoiding computations and reducing the search space by pruning high entropy subspaces. A good heur istic is to evaluate the entropy of those subspaces for which larger subspaces alr eady had a high entropy. Randomly picking subspaces for additional evaluation also performs quite well in practice.

If the bottom up approach has not pruned the investigated subspace, the top down approach computes the entropy of the subs pace. If the weighted normalized entropy is below  X  the subspaces is added to the result set and marked as one-sided homogeneous. The algorithm finally computes the combine d entropy of all subspaces for which both subspaces are marked one-sided homogeneous in the result sets.

Once subspaces have been evaluated for step 1 , the most complex algorithmic task has been solved. Having reduced the poten tially exponential number of subspaces to the interesting ones, the actual clustering ( step 2 ) is performed for each of these subspaces. This is done by computing the frequency and class entropy for all attribute value combi-nations in these subspaces. The resulting classifying subspace clusters then provide the model that is used for the actual classification ( step 3 ). For incoming objects, compute the most similar classifying subspace cluste rs according to relative Hamming distance. If tied, compute reverse class entropy. The decision is then based on their class label distribution.
 Experiments were run on both synthetic and real world data. Synthetic data is used to show the correctness of our approach. Local patterns are hidden in a data set of 7.000 objects and eight attributes . As background noise, each attribute of the synthetic data set is uniformly distributed over ten values. On top of this, 16 different local patterns (subspace clusters) with different dimensionalities and different numbers of objects are hidden in the data set. Each local pattern contains two or three class labels among which one class label is dominating. We randomly picked 7.000 objects for training and 1.000 objects for testing.

The flight data contains historic data from a large European airport. For a three-month period, we trained the classifier on arrivals of two consecutive months and tested on the following month. Outliers with dela ys outside [-60, 120] minutes have been elim-inated. In total, 11.072 flights have been used for training and 5.720 flights for testing. Each flight has a total of 13 attributes, including e.g. the airline, flight number, aircraft type, routing, and the scheduled arrival tim e within the day. The class labels are  X  X head of schedule X ,  X  X n time X  and  X  X elayed X . Finally we use two well-known real world data sets from the UCI KDD archive (Glass and Iris [14]), as a general benchmark.
As mentioned before, preliminary experiments on the flight data indicate that no global relevance of attributes exist. Moreover, the data is inherently noisy, and impor-tant influences like weather conditions are not collected from scheduling. For realistic testing as in practical application, classifiers can only draw from existing attributes. Missing or not collected parameters are not available for training or testing neither in our experiments nor during the actual scheduling process.

We have conducted prior experiments to evaluate the effect of  X  and  X  for minimum frequency and maximum entropy thresholds, respectively. For each data set we used a cross validation to chose  X  1 (absolute frequency),  X  2 (relative frequency) and  X  .For  X  we have chosen 0 . 9 . This value corresponds to a rather relaxed setting as we only want to remove completely inhomogeneous s ubspaces from consideration. To restrict the search space  X  can be set to a low value.

In our first experiments we develop a heuristic to set up reasonable parameters for the threshold  X  of the interestingness and the weight w of the class and attribute entropy, respectively.

Figure 4(a) illustrates varying  X  from 0 . 45 to 0 . 95 on the synthetic data, measuring classification accuracy and the numbe r of classifying subspaces. The weight w for in-terestingness was set to 0 . 5 . As expected, the number of classifying subspaces (CSS) decreases when lowering the threshold  X  . At the same time, the classification accuracy does not change substantially or even increas es slightly when less subspaces are used. This effect may be related to the effect of overfitting. Using too many subspaces pat-terns are not sufficiently generalized, and noise is not removed. To set up the threshold  X  , slowly increasing  X  until the number of classifying subspace clusters shows a rapid rise, allows adjusting  X  to a point between generalization and overfitting. For both our data sets, a value around 0 . 65 obtains produces good results.

The effect of slightly increasing classifi cation accuracy when reducing the number of subspaces can also be observed on the flight de lay data (see Figure 4(c)). This confirms that the flight data contains local patterns for classification. Varying parameter w yields the results depicted in the left part of Figure 4(b) and 4(d). The number of classifying subspaces decreas es when giving more weight to attribute entropy. At the same time, cl assification accuracy does not change significantly. This ro-bustness is due to the ensuing subspace clus tering phase. As classification accuracy does not change this confirms that our classifying subspace cluster definition selects the rele-vant patterns. Setting w =0 . 5 gives equivalent weight to the class and attribute entropy and hence is a good choice for pruning subspaces. We summarize our heuristics used to setup the parameters for our SubClass algorithm in Figure 5.

Next, we evaluate classifica tion accuracy by comparing SubClass with other well-established classifiers that are applicable on nominal attributes: the k -NN classifier with Manhattan distance, the C4.5 decision tree that also uses a class and attribute entropy model [18], and a Naive Bayes classifier, a probabilistic classifier that assumes independence of attributes. Parameter se ttings use the best values from the preceding experiments.

Figure 6 illustrates the classification accuracy using four different data sets. In the noisy synthetic data set, our SubClass approach outperforms other classifiers. The large degree of noise and the varying class label distribution within the subspace clusters make this a challenging task. From the real world experiment on the flight data, depicted in Figure 6, we see that the situation is even more complex. Still, our SubClass method performs better than its competitors. This r esult supports our analysis that locally rele-vant information for classification exists that should be used for model building. Experts from flight scheduling confirm that additional information on further parameters, e.g. weather conditions, is likely to boost classification. This information is inexistent in the current scheduling data that is collected routinely. SubClass exploits all the information available, especially locally relevant attribute and value combinations, for the best clas-sification in this noisy scenario. Fin ally we evaluated the performance of SubClass on Glass and Iris [14]. The results indicate that even in settings containing no or little noise SubClass performs well. Classification in noisy data with locally varying attribute relevance, as for our project in scheduling at airports, requires an approach that detects local patterns. Our SubClass method automatically detects classifying subspace clusters by incorporating class struc-ture into the subspace search and the subspace clustering process. The general concept requires a definition of interesting subspaces for classification, of classifying subspace clusters and a classification scheme. Base d on class and attribute value entropy, our Sub-Class ensures that clusters contain class-relevant information. Working both bottom-up and top-down on the lattice of subspaces, SubClass prunes irrelevant subspaces from the mining process. Our experiments on synthetic and real world data demonstrate that local structures are successfully detected an d employed for classification, even in ex-tremely noisy data.
