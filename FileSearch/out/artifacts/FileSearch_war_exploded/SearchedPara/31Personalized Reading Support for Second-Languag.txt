 English is now so widespread that, in non-English speaking countries, office workers who are not specializing in English need to read English documents during their office work. Furthermore, more and more users are now browsing Web pages, using English word glossing systems which eliminate the need to consult a dictionary every time an unknown word is encountered. A glossing system presents the user with the meaning of an unknown word in a pop-up window when the user clicks on or mouses over it. An example is  X  X op jisyo X , shown in Figure 1. The background sentences that begin with  X  X  group of Han Chinese X  are sentences on a Web page that we assume the user is reading. Suppose that the user encounters the word  X  X aramilitary X  and does not know its meaning. If the user mouses over the word, a pop-up window opens and the meaning of the word  X  X aramilitary X  is displayed in it.
 While useful, existing English glossing systems do not utilize the user X  X  vocabulary. They waste valuable data about the user X  X  vocabulary which could be accumulated. Instead, we may harness collective intelligence by utilizing the accumulated word click logs from many users. We are developing a system that can accumulate this kind of knowledge and make full use of it to help users to read English Web pages by predicting the words that the user does not know and displaying their meanings aim is to combine an English word glossing system with word difficulty prediction in an integrated and intelligent user interface. Existing research on word difficulty relies heavily on corpus frequency. However, corpus frequency may not reflect the difficulty that users actually feel: some words occur frequently in corpora, but actually may not be known to users, while some words are rarely seen in corpora, but actually may be known to users. By accumulating the word click logs from many users and uncovering the  X  X ollective intelligence X  beneath the logs, we can get the collective intelligence to adjust the weight for each word to reflect the difficulty that users actually feel; even if some words occur frequently in corpora, those words are regarded as difficult if many users frequently click them to see their glosses. Even if some words are rarely seen in corpora, those words are regarded as easy if many users click those words to hide their glosses. Thus, we can regard our system as using the collective intelligence embedded in the click log to correct the corpus-frequency-based word difficulty.

The main contribution of this study is a mechanism for predicting words unknown to a user. We use logistic regression to estimate which words on the Web page are unknown to the user because it is widely used to test language ability in Item Response Theory (IRT). This theory lets one estimate both a user X  X  language ability difficulty d at the same time. To train the logistic regression faster, we chose Stochastic Gradient Descent (SGD) from among many parameter optimization methods because it is an online algorithm. An online algorithm can perform training faster than a batch method because it accesses a datum in the dataset only once, whereas a batch method accesses the data many times.
 The example in Figure 2 shows how the system works when a reader, being an English learner as well, tries to read a Web page. The yellow words are predicted to be known to this reader and the red word, namely  X  X orries X  in this example, is the word predicted to be unknown to the user. Thus, this red word is  X  X lossed X , that is, a gloss is attached to it. The system makes this prediction when the reader loads the Web page so that, from the reader X  X  viewpoint, the Web page seems to come with the gloss in the first place. The reader can also get definitions by clicking words: when the reader reader actually does not know the word. When the reader clicks a red (i.e., predicted to be unknown) word, the system is informed that the reader actually  X  X nows X  the word. These corrections are instantly dispatched to the system from the reader X  X  Web browser by an Ajax script, so that the system can learn and reflect the correction in its prediction when the reader goes to another Web page.

Another contribution of this study is that we evaluated our system to see whether it can reduce the number of clicks and the number of unreadable documents. The t-test was performed for both evaluations and the results show that the system worked significantly well for these evaluations.

In summary, our system is characterized as follows. (1) It accumulates knowledge about the vocabulary of each user and fully utilizes it; (2) it has a machine-learning-based mechanism for predicting words unknown to a
This article is organized as follows. First, we introduce related work. Second, we intro-duce the design and implementation issues of our system. We then explain the relation-ships among IRT, logistic regression, and the Rasch model, which is a special case of IRT and also a special case of logistic regression, which our system uses. We then explain the methods for estimating the parameters of logistic regression and the derivation of SGD. We then describe an experimental evaluation of our system. Finally, we conclude this article by briefly summarizing the main points and mentioning future work.
Note that throughout this article we use the term  X  X nfamiliar words X  to denote words unknown to a user. Although we know that word familiarity is, strictly speaking, a different concept from simple word knowledge, since word familiarity is not the focus of this article and the term  X  X nknown words X  has another meaning, we use  X  X nfamiliar words X  for simplicity. This section describes previous work related to this article.

Glossing systems for Web pages have been implemented in two forms: as desktop applications that retrieve Web pages and perform the glossing on the user X  X  local machine and as Web applications in which Web page retrieval and glossing is performed on a Web proxy server instead of on the user X  X  local machine. While desktop glossing systems are fast because they do not require communication between the Web proxy server and the user X  X  local machine, they are not suitable for accumulating the user click logs because the logs are distributed across many local machines in desktop applications. Since we accumulate and utilize the click logs, we implemented our system as a Web application.

Desktop glossing systems are usually implemented as add-ons for Web browsers, such as: FireDictionary [Nori 2005], and popIn [popIn Inc., 2008]. They work as add-ons for the Firefox Web browser.

Notable Web-based glossing systems include popjisyo [Coolest.com Inc., 2002], which we used as an example, and rikai.com [Rudick 2001]. In particular, popjisyo supports glossing between multiple languages 2 .
However, as far as we know, no application estimates users X  language ability or predicts the words unknown to users from the accumulated click logs, though some applications may simply accumulate them for quick access to the words previously clicked. Our application is novel in that it assesses the user X  X  language ability and predicts unknown words.

Our approach also differs from machine translation in that our system glosses only those words predicted to be unknown to the user whereas machine translation does not utilize the user X  X  language ability and translates the entire text, including words known to the user. This section explains the operation of our system and implementation issues. The way our system operates is schematically illustrated in Figure 3. It is a glossing system that uses  X  X GI-proxy X  and predicts words unknown to the user by machine learning. In use, it operates as follows. Web documents are usually have tags like  X  &lt; html &gt; represented by a tree, as shown in Figure 4(a). Let Dom D and Dom T be the set of trees. We define Parse ( D ) as a function that takes a Web page D  X  Dom Unparse ( R ) as a function that takes tree R  X  Dom T and returns the Web page that corresponds to R .

Step (4) in Figure 3 involves a tokenization function  X  X okenize X  and a prediction and glossing function  X  X redictGloss X .  X  X okenize X  takes tree R a tree whose sentences are tokenized into words. An example is shown in Figure 4:  X  X okenize X  takes Figure 4(a) and returns the tree in Figure 4(b) except for the bold parts.  X  X okenize X  also tags every token by  X  &lt; span &gt; that enables the user to consult a dictionary simply by clicking the word.  X  X redictGloss X  takes tokenized tree R  X  Dom T , glosser function g , user identifier u , and the weight of classifier w ( k ) and g takes token t and returns its gloss (meaning) unfamiliar to user u and then glosses only unfamiliar ones with g . The prediction is determined by the sign of the function h ( u , t ,w ( k )
While previous systems do not require any communication with the browser after (4), our system communicates with the browser in (5) and (6) to collect the  X  X lick log X  ( y , t ) (defined shortly) to train w ( k ) . This communication is made possible by the use of  X  X JAX X , asynchronous JavaScript, and XML (extensible markup language). We used  X  X Query X  3 for the library for AJAX.
Here, y codes whether or not user u knows word t . It is defined as follows: y to the system if word t was predicted to be unfamiliar to user u prior to the click and is clicked for the first time after this prediction. y = 0 means that the system regards user u as not knowing word t because user u is now requesting the gloss for word t even though the system first regarded user u as knowing word t in (4). If word t is predicted to be known to user u and is clicked for the first time, the exact opposite happens: y is sent to the system, which means that the system regards user u as knowing word t because user u is now hiding word t even though the system first regarded user u as not knowing word t in (4). The data ( u , t , y ) is used to update the value of as shown in Figure 3. The function Update ( w ( k ) , u , First, note that the system should be multithreaded and every thread processes the requests from one of the users. Simply speaking, our system involves two kinds of data: the weight vector w ( k ) for classification, and the click logs accumulated so far to train the weight vector. These data have very different natures: the weight vector should almost always be stored in memory and accessible from every thread because it is used every classification. Moreover, it is frequently updated by the users X  usage. By contrast, the click logs are, strictly speaking, not updated but rather added. Simply recording new click data suffices. Moreover, they do not need to be always loaded in memory as they are used only when the system retrains the weight vector, say during the night, for example.

Exploiting this different nature of the data, we store the weight vector in  X  X em-cached X  and the clicks logs in a Relational DataBase Management System (RDBMS). When the user clicks a word, the thread processing that user dispatches two requests: one to update the weight vector stored in  X  X emcached X  and one to add a new data record to the RDBMS. This  X  X emcached X  is the usual solution for this kind of envi-ronment; it is, simply speaking, a hash table shared by all the threads processing a user X  X  request. Being a hash table X  X ne update consists of the pair of a key (the id of a feature) and a value (the value of the feature) X  X eans that  X  X emcached X  involves only that key and does not affect any other keys. This means that, for example, the English ability features of two different users can be simultaneously updated because they have different feature ids. Logistic regression is used to model our system because it includes the Rasch model, which is a basic IRT. IRT is widely used to test language ability. To predict the words unknown to a user, it is necessary to model the user X  X  language ability. Thus, logistic regression is suitable for use in this application. This section explains IRT and how it is involved in our system. IRT is a statistical method for analyzing the results of tests of human abilities including language tests. Indeed, IRT is thought to be used in TOEFL (Test of English as a Foreign Language) and TOEIC (Test of English for International Communication). Details of the mathematical aspects of IRT are given in Baker and Kim [2004].

IRT has been used to generate fill-in-the-blank questions automatically from text retrieved from the Web [Sumita et al. 2005].

First, IRT calculates  X  X arameters X  from human ability test results. Once the param-eters have been calculated, IRT can then estimate the following probabilities:  X  X robability that a user will answer a  X  X ifferent X  question correctly;  X  X robability that a  X  X ifferent X  user will answer the original question correctly. Here,  X  X ifferent X  means that no record with the same pair of user and question is found in the previous test results. This covers the user or question being a new one and the user or question existing but the user not answering that question in the previous test. 4.1.1. Definition of IRT. IRT involves three concepts: users, items, and responses. Users are simply the set of users. Items correspond to the questions in a test. The response indicates how correctly the user responded to an item.
 Given user u  X  U and item t  X  T , Eq. (1) models user u  X  X  response y to item t , where U and T are finite sets of users and items, respectively, and y is the response given u and t .

Eq. (1) codes y as a binary random variable (though in some variations of IRT, y can take more than two states); that is, y = 1whenuser u answers item t correctly and y = 0 otherwise. Eq. (1) shows a typical IRT formulation.

Here,  X  is a logistic sigmoid function, which is widely used in probabilistic models because, for a value x  X  R ,0 &lt; X  ( x ) &lt; 1holds.
Eq. (1) has four parameters to estimate, as listed and explained next.
The ability of every user u  X  U is modeled by user parameter the greater is the ability that our system regards user u as having.

The difficulty of each item t  X  T is modeled by d t . The higher d item t and the fewer users can answer correctly.

The latter parameters are optional. Discrimination parameter a steep the item characteristic curve of item t is. The higher a characteristic curve.

Guessing parameter c t determines the probability of test takers guessing the correct answer. This parameter is typically used to model multiple-choice items. For example, if a test taker is supposed to choose one item from five choices, then c 4.1.2. Rasch Model. Our system simply assumes items to be words.
 By comparing Eq. (1) with Eq. (3), one can find that Eq. (3) is a special case of Eq. (1). Indeed, Eq. (3) is derived from Eq. (1) by substituting a where a t = 1 means that all the questions are regarded as equally discriminative and c = 0 means that the model assumes that it is impossible to guess the correct answer. This section introduces logistic regression, which is a kind of  X  X upervised classification X  in machine learning literature [Bishop 2006].

In supervised classification, a classifier is first trained with training data . A training datum can be represented as ( y , x ), which is a pair of input vector x and class y where Y is the domain of the classes and X is the domain of input vectors. A classifier has a parameter vector, w , that stores all the information for the current status of the training, where w determines the classifier. Given another input vector x and parameter vector w , the classifier further predicts the class of the input vector. When it is predicting, the classifier does not use the input vector x as-is: rather, it first constructs a feature vector  X  ( x ) =  X  ( u , t ).

Supervised classification methods can be categorized into two types: probabilistic or nonprobabilistic. Probabilistic methods calculate the probability that y belongs to a certain class given input vector x . They then classify x as the most probable class in domain Y . Logistic regression is one of the most typical of these methods. Non-probabilistic methods such as Support Vector Machines (SVM) do not use probability explicitly but simply map x to some y  X  Y . For our system, probabilistic classification is preferable because, even if the prediction fails, it tells us how significant the failure is in the form of a probability. This is beneficial in this application because it enables users to prioritize the predictions to correct.

Given a training datum ( y , x ), logistic regression directly models the conditional probability of y given x  X  X by a sigmoid function. In this application, input vector x consists of two components: the index of user u  X  X  1 ,..., | t  X  X  1 ,..., | T |} . Thus, x = ( u , t ).

Random variable y represents how well user u knows word t . In this study, we simply assumed that this is represented by two states:  X  X now X  and  X  X on X  X  know X . That is, y when user u knows word t and y = 0 otherwise. Thus, the domain of y is in this application. Limiting y to be a binary random variable has a benefit: it reduces the number of variables to optimize. In our system, we need to optimize vector that it fits the data. If y can take more than two classes, say K classes, then ( K variables need to be optimized. However, if y is binary, then optimizing sufficient; the probability of one class, y = 1 in Eq. (4), also determines the probability of the other class y = 0, as shown next.

First, we explain how the system, given a weight vector w sifier h to be represented as Eq. (5), in which the notation in Eq. (4) is used. Here, k is the index of updates, that is, the number of times the weight vector has been changed.
Second, we explain how the system learns the weight vector lated log which stores all the clicks of all the users indexed by n the notation introduced in Eq. (4) also combines P ( y = 1 one representation P ( y |  X  ; w ).

Thus, the negative log likelihood E ( w ) of Eq. (6) is derived as shown in Eq. (7). The maximum likelihood estimator of Eq. (6) is given by minimizing Eq. (7).
 This section shows that IRT is a special case of logistic regression when parameter vector w and feature vector  X  are modeled as follows. Let Let  X  denote the direct sum of two vectors; that is, given two vectors Eq. (9) shows r  X  s . In addition, this article uses the following notation: a vector representing user language ability and a vector of the difficulty of the words
The Rasch model is a special case of logistic regression where are defined as in Eq. (12). e u and e t are unit vectors: e whose u  X  X h element is 1 while all the other elements are 0. e vector whose t  X  X h element is 1 while all the other elements are 0.
Eq. (12) can be extended by adding additional features, as shown in Eq. (13).
Here,  X  a is an additional feature vector that depends on u and t ,and vector for  X  a ( u , t ). As the additional feature vector provides more information about u and t , when selected appropriately, it improves the system X  X  prediction performance.
Our system uses  X  a to introduce word difficulty. An appropriate choice of accuracy. Remember that our system needs to estimate user language ability word difficulty d . Among these two, not much information about language ability is expected to be available ahead of time because many users are anonymous by the nature of the Web community. In contrast, much is known about word difficulty d . While many measures of word difficulty have been proposed, most of them are based on word frequency in a large corpus because word frequency has been demonstrated to be a good measure of word difficulty for ESL (English as a Second Language) learners [Tamayo 1987]. Thus, we used two kinds of features for word difficulty:  X  X oogle 1-gram X  and  X  X VL X . Google 1-gram is created from the raw word frequency taken from Brants and Franz [2006]. Taken from approximately a trillion Web pages, Brants and Franz [2006] is one of the largest word frequency lists known. Specifically, we used the value of normalized  X  log( f t [2006]. Although also based on word frequency, SVL [SPACE ALC Inc., 1998] is a word difficulty measure manually checked by English native speakers for Japanese ESL learners. SVL12000 covers 12,000 words and has 12 levels.

A Rasch model is not simply applicable to this system because the input data are too sparse. However, by using the notation used for logistic regression, we can tackle this problem by just adding more features to  X  . In particular, we can approximate average word difficulty by using word frequencies; many report that word frequency can be used as a measure of word difficulty [Tamayo 1987]. We used word frequencies from the Google corpus [Brants and Franz 2006], whose n-gram counts are claimed to have been  X  X enerated from approximately 1 trillion word tokens of text from publicly accessible Web pages X .
 This section introduces parameter estimation methods for logistic regression. The main parameter estimation algorithms are shown in Table I. To train the classifier for our system, online learning methods are preferable to batch learning methods because the click logs of our system are accumulated in an online manner. Batch learning may, however, be applicable to our system, for example, for training the classifier periodically.
The  X  X ptimal X  column shows whether the algorithm can find the global optimal solution of the objective function of a logistic regression. The  X  X atch/online X  column shows whether the algorithm is a batch algorithm or an online algorithm.

L-BFGS [Liu and Nocedal 1989] is a widely used method of estimating logistic regres-sion parameters. It is a variant of the quasi-Newton method. It consumes a memory space only linear to the number of features while a na  X   X ve method consumes a memory space squared to the number of features.

Trust region [Lin et al. 2008] is another recently proposed batch method for optimiz-ing binary logistic regression parameters. It can optimize the parameters faster than L-BFGS, but it is limited to logistic regression whereas L-BFGS is more general and more widely applicable. An implementation of Trust region is given in Fan et al. [2008] and this implementation was used for the evaluation later described. The proposed system uses SGD (Stochastic Gradient Descent) to estimate the param-eters because it is an online algorithm. While a batch algorithm requires the whole training data to be given from the beginning to the end, an online algorithm can pro-cess every record of the data one by one. An online algorithm suits a Web application, because users of a Web application do not feed the whole data at a time.

SGD is derived from steepest gradient descent, a batch algorithm, by removing summation over the data points. The steepest gradient descent for logistic regression is shown in Eq. (14) where  X  E n ( w ) = ( y n  X   X  ( w T  X  the n  X  X  1 ,..., N } th click log among all N click logs. Eq. (14) contains the summation of E n over all the data, so it is a batch algorithm. In this case, an online algorithm is easily derived by removing the summation over n from Eq. (14). The resulting online algorithm is SGD, the algorithm used in our system. SGD is given by In Eq. (15),  X  k is defined as  X  k = 1  X  ( k + k is defined in this way so that Eq. (15) converges [Novikoff 1963]. Note that a constant in Eq. (14). Update ( w ( k ) , u , t , y ) in Figure 3 is defined by Eq. (15). This section presents the results of our experiments and discusses them.
 We developed a database of the vocabulary knowledge of 16 human subjects for 12,000 words for training and evaluating our system. This database is a matrix of the numbers representing the degree of vocabulary knowledge, where the rows correspond to the human subjects and the columns correspond to the words. The vocabulary knowledge was obtained by assessing the language ability of humans introduced in Read [2000].
The database was developed by first determining the set of words for assessing the language ability of humans. The word set was selected from the Standard Vocabulary List 12000 (SVL12000) [SPACE ALC Inc., 1998]. SVL12000 lists the most fundamental 12,000 words that an English learner should learn and they have been checked by native English speakers. Then, the questions for assessing the subject X  X  language ability were developed. Each human subject answered the question for every word in SVL12000.

In Read [2000], two methods are introduced to make questions for measuring the degree of human language ability.
 The self-report scale was used for developing the evaluation data because creating and answering multiple-choice items for 12,000 words has a much higher cost burden than the self-report scale.

There have been several studies on designing scales for the self-report scale. The scale shown in Table II was proposed in early research by Dale [1965]. Paribakht and Wesche [1997] proposed the scale shown in Table III. Although the first four ranks on the scales show some correspondence between Dale X  X  scale and Paribakht and Wesche X  X  scale, the latter has one more additional rank, rank V, which tests the subject X  X  writing ability by testing whether the subject can use the word correctly in a sentence. Rank V is additional to the other ranks; those who do rank V must also do rank IV, as described in the note in Table III. We created the scale shown in Table IV. It is based on both Dale X  X  scale and Paribakht and Wesche X  X  scale with two major modifications. The first modification is that we omitted rank V because that exists for testing writing ability rather than testing reading ability, which our system tries to support. The second modification is that knowledge of synonyms and translations is omitted in our ranks. The reason for this is to prevent bias in the results. As it takes time for the subjects to give a synonym or translation, they may hesitate to choose an item that requires one, especially if the test covers many words. Another modification to Dale X  X  scale is that we divided this rank 2 into two ranks: 2.1 and 2.2. This modification was introduced because we wanted to discriminate the case where the subject has tried to learn the word before from the case where the subject simply has seen the word before. Dale X  X  scale does not discriminate these two because it was originally for testing L1 language ability, and words are usually learned incidentally in L1 acquisition. 16 students mostly from graduate schools of the University of Tokyo participated in the test as human subjects. Each student answered 12,000 questions corresponding to the 12,000 words using our scale (Table IV). Note that the ranks in Table IV are numbered to correspond to those of Dale X  X  scale (Table II) and Paribakht and Wesche X  X  scale (Table III). The subjects saw a simple ordinal numbering 1 numbering did not affect their results.

The scale in Table IV needs to be binarized to evaluate our system because our system eventually classifies words into  X  X nown X  and  X  X nknown X  to a user. This scale binarization was performed as follows: Only rank 4 in Table IV was regarded as y that is, the case where user u knows word t . All the other ranks were regarded as y that is, the case where user u does not know word t .

The aim of this binarization is as follows: as the goal of our system is to support reading by automatically showing glosses of words, in our system X  X  evaluation, the criteria when binarizing these scales should be whether or not the user needs the glosses of the words. Those words in ranks 1, 2.1, and 2.2 clearly need to be glossed when they appear in texts because the subject does not know them, so they surely hamper reading. The words in rank 3 also need to be glossed because, although the user may be able to  X  X uess X  their meanings, these guesses could be wrong and, without glosses being shown when the words appear in a text, the user cannot be informed of his/her wrong guesses and might not fully comprehend the text containing the words or might misinterpret the text, either of which would hamper reading. The database was divided into a training set, development set, and test set. Six hundred words were randomly chosen for the training set, 1400 words were randomly chosen for the development set, and 9999 words were randomly chosen for the test set. Accuracy was defined as the percentage of the words that our system correctly answered among the 9999 words in the test set.

The evaluation setting simulated the case where a new user starts using our system with a specified log. Every user among the 16 subjects was assumed to start using the system as a new user and to click at most 600 words. This number 600 was chosen so that everyone could click them within about 5 minutes.

The system was trained with data from smart.fm [Cerego Japan Inc., 2009] instead of our system X  X  log because no log had been accumulated at the start. smart.fm is an implementation of a computer-assisted vocabulary acquisition (CAVOCA) system in the field of Computer-Assisted Language Learning (CALL). The purpose of a CAVOCA system is to support its users, typically ESL learners, so that they can learn as many English words as possible in a fixed period. A CAVOCA system works as follows: First, the CAVOCA system gives the user a word list and the user selects words in the list to learn. If the user finds words that he/she already knows, he/she can check them and skip them. For our system, we used the checked words as training data because they are regarded as known words and the unchecked words in the word list are regarded as unknown words. Note that words absent from the word list were not used. Once the set of words to learn has been determined, the CAVOCA system automatically generates a set of questions about the words and presents these questions to the user. The CAVOCA system continues generating questions and the user continues answering them until he/she is able to answer the whole set of questions correctly. The question types include multiple-choice questions and spelling questions. While we obtained the records for 10,526 smart.fm users, we eventually chose 675 users from amongst these and used their records because many users simply tried smart.fm and did not use it seriously and repeatedly. These 675 users were chosen according to the following criteria: selection of at least 100 words for learning and 15% or more skipped words out of the total number of words in the word lists.

Our system was trained with data created from smart.fm ( N user words ( N 1 ) as training data. Here, N = N 0 + N 1 , where N denotes the same N as in Section 5.1. The hyperparameters were tuned for the development set, and the accuracy of our system was measured on the test set. The accuracy was defined as the average percentage for the case where the classifier predicted correctly over the test sets of the 16 subjects.

The Rasch model is a kind of IRT that can be defined as a logistic regression by provid-ing user-IDs and word-IDs as features in logistic regression. Our model is an extension of the Rasch model with the addition of word difficulty features in logistic regression. This section describes how we evaluated the effectiveness of the word difficulty fea-tures. Fan et al. [2008] was used for both IRT and LR with L2-regularization. The regularization parameter was selected from 1 . 0 , 0 . 5 , highest accuracy in the development set was used in the test. Note that since IRT is a logistic regression that does not use word difficulty features, evaluating the case where no word difficulty features are used corresponds to evaluating the effectiveness of IRT.
The results are shown in Table V and Figure 5. Both show that LR, the case that includes word difficulty features, exhibited considerably higher accuracy than IRT. This means that word difficulty features are effective because the difference between LR and IRT lies only in the word difficulty features. Thus, the use of word difficulty features is confirmed to be reasonable.

Another observation from Figure 5 is that the accuracy of both settings seemed to saturate in the range where the number of training data was over 300. LR achieved about 5% higher accuracy than IRT as a result of using word difficulty features. There are many algorithms for binary classification besides logistic regression. Our system can be trained with them by using the same features. This section compares logistic regression with other binary classification algorithms in terms of accuracy. The following algorithms were compared with logistic regression.
 (1) SVM (Linear) (2) SVM (RBF)
SVM (Linear) is a support vector machine with a linear kernel. This algorithm was chosen for the comparison because a support vector machine is a state-of-the-art algorithm for binary classification. Unlike a support vector machine with a Gaussian kernel, a support vector machine with a linear kernel cannot fully classify nonlinear data. However, this does not cause much problem with high-dimensional data because there are enough dimensions to classify the given data linearly. Explanations of support vector machines are found in Cristianini and Shawe-Taylor [2000]. Again, Fan et al. [2008] was used for the implementation. The performance of a support vector machine is known to be affected by the value of regularization parameter C . In this evaluation, C was chosen from 1 . 0 , 2 . 0 , 0 . 5. The value was tuned for the development set.
SVM (RBF) is a support vector machine with a Gaussian kernel, so it is another state-of-the-art algorithm for binary classification. Fan et al. [2008] was used for the implementation. Unlike all the other algorithms, SVM (RBF) takes hours to optimize. The value of regularization parameter C was set to 1 . 0.

Stochastic Gradient Descent (SGD) is the SGD described so far. Note that no regu-larization is performed in SGD.
 Finally, LR denotes the results for logistic regression. Again, LR was optimized with L2-regularization using Fan et al. [2008]. The regularization parameter was selected from 1 . 0 , 0 . 5 , 2 . 0, and the value that gave the highest accuracy in the development set was used for the test.
 Accuracy values for logistic regression and other algorithms are listed in Table VI. Logistic regression achieved the highest accuracy when the number of the training data was 300 and 600. It also achieved nearly the highest accuracy for 30 that logistic regression is sufficiently accurate compared with the other algorithms. Thus, the use of logistic regression is reasonable for this application.

When the numbers of the training data were 10 and 30, SGD outperformed LR and achieved the highest and the second highest accuracy, respectively. This result is encouraging because it suggests that SGD is quick to adapt to a user. SVM (RBF) had the lowest accuracy when the number of training data was set to 10. We attributed this to the amount of training data being too small and to parameter C of the SVM (RBF) being fixed to 1.

As shown in Figure 6, all the algorithms saturated in the range where the number of training data exceeded 300 and even the highest of accuracy was only about 80%. This shows that about 80% is the accuracy limit owing to the nature of the given data. This section evaluates our system by simulating users X  document-reading processes using the data mentioned in Section 6.1. We simulated the case where each user reads all the documents in the  X  X rown corpus X , which consists of 500 documents taken from various sources for balance. The average number of words in a Brown corpus document is 2029.

Leave-one-out evaluation was performed. First, we selected one user for the devel-opment to tune the parameters of the classifiers. The data from this user were used for development purposes only, and never used for training or testing. In each fold, a test user was selected, and all the data of the other users were used as training data of  X  X raining 1 X , that is, to learn the word difficulty, as explained later. (1) The next document to read was given randomly. (2) Given the document to read and a test user, each classifier classified and glossed (3) Using the vocabulary knowledge collected in Section 6.1, we simulated the test user (4) Each classifier was trained from these corrections. Go back to 1.

The evaluation of our system involved both its ability to estimate difficulty of words and its ability to estimate the users X  language abilities. As the focus of this evaluation was the latter, we wanted to make the training data separate. Therefore, the training data were divided into two parts:  X  X raining 1 X  and  X  X raining 2 X . Here,  X  X raining 1 X  corresponds to the logs accumulated so far and was the part for difficulty of words.
The following three methods were compared. The C hyperparameter for the logistic regression was chosen from 100 . 0 , 10 . 0 , 1 . 0 , 0 . the highest accuracy (in number of types) using the data from the development user was selected. For hyperparameters of SGD, the setting  X  =
First, we evaluated our system in terms of the number of clicks required. Table VII lists the average percentage of words to be clicked, that is, the total number of clicks divided by the total number of words in the 500 documents. As the denominator is a constant, these percentage values are proportional to the required number of clicks. Note that, here, the number of words means the number of occurrences of words, not the number of types of words as used in the previous section. The users were sorted by the percentage of their unfamiliar words in descending order. The asterisks are T-test trial results for each method versus  X  X nfamiliar X , the baseline.  X * X  denotes that the p-value is lower than 0 . 05,  X ** X  denotes that the p-value is lower than 0 denotes that the p-value is lower than 0 . 001.

Investigating Table VII, we can find that, from user0 to user7, LR significantly reduced the number of clicks and from user0 to user6, LR+SGD significantly reduced the number of clicks. Intuitively, the smaller the number of the user X  X  unfamiliar words becomes, the more difficult for the system to make significant reductions in the number of clicks. However, this case is not problematic for the user because, if the user has a large enough vocabulary to read the documents without any support, the user does not need reading support in the first place.

Readers do not always need to be familiar with all the words in a document to read that document. Nation [2006] surveyed and investigated the percentage of words necessary to read documents and reported that one must know from 95% to 98% of the words in a running text to read documents sufficiently without assistance.
We also evaluated our system in terms of the percentage of  X  X emaining unfamiliar words X , as shown in Table VIII. As the words predicted to be unknown to a user were glossed in advance, the users could know the meaning of those glossed words when using our system. Thus, the unfamiliar words remaining were the words predicted to be known to a user, though the user actually did not know them. We call these words  X  X emaining unfamiliar words X . The meaning of asterisks is the same as in the previous table.

From Table VIII, we can see that the numbers of remaining unfamiliar words were significantly decreased for all users. In particular, for user2 X  X o user8, the system sig-nificantly made unreadable documents nearly readable as the average remaining un-familiar words became lower than 5% while originally being above 5%, according to Nation [2006]. Moreover, interestingly, the proposed LR+SGD achieved better results than the costly LR method. This is probably due to the fact that in LR+SGD, SGD up-dates only the language ability parameters while LR tries to adjust all the parameters and causes overtraining.
 Finally, we compared the time required to train the logistic regression in LR and SGD in SGD. While the total time to finish the training for 500 documents was 0 (s) in SGD on average, LR took 1975 . 53 (s) and the t-test was of course significant as the p-value was under 10  X  22 . We can easily see that LR+SGD was the most efficient. We described a new glossing system that can predict words unknown to the user by utilizing logs that contain valuable information about a user X  X  vocabulary. Although existing glossing systems are helpful for English learners because they provide the meaning of a word by displaying it in a pop-up window when the user encounters an unknown word and clicks on or mouses over it, they waste this log information.
We investigated models for our system X  X  prediction. The use of logistic regression was found to be appropriate because the Rasch model, a simple form of Item Response Theory (IRT) widely used to assess human language ability, is also a logistic regression that uses a specific feature vector. We extended IRT by introducing word difficulty features and achieved accuracy higher than that of straightforward IRT.

We also proposed a method of training parameters suitable for our system. IRT and the extended IRT can estimate both the difficulty of words and the users X  language abilities simultaneously from the accumulated logs as one parameter vector. Note that the estimated difficulty of the words is not simple usage of existing word difficulty measures, but is adjusted using the combination of existing word difficulty measures to fit the training data from the system users. For example, if an existing word difficulty measure is not correlated with the system users, the weight word difficulty measure approaches zero and is nearly omitted.

Our system requires two kinds of estimation: one is estimation of the difficulty of the words so that it suits the users of our system; the other is estimation of user language levels including those of new users. The former is more stable because the difficulty of a word does not change suddenly, while the latter is more dynamic because user language levels differ from user to user and our system needs to become accustomed to a new user quickly.

To meet these requirements, the proposed estimation method combines both batch learning and online learning by sharing the parameter vector. The costly batch learning parameter is copied to the Stochastic Gradient Descent (SGD) learner. SGD updates only the users X  language ability parameters quickly in an online manner and is thus appropriate for our system where the logs are accumulated in an online manner.
We evaluated our system in terms of the number of clicks and the percentage of unfamiliar words remaining even after the prediction and glossing. To evaluate our system, we collected knowledge about 12,000 words from 16 participants and used these data to simulate users X  reading of the Brown corpus.

The simulation results show that our system can significantly reduce the number of clicks for users who know up to 94.7% of the running words in a document (a.k.a., coverage). As Nation [2006] shows that a reader should have 95% or higher coverage of a document to read it sufficiently, this shows that our system can reduce the number of clicks for most readers with insufficient word coverage to read documents. The sim-ulation results also show our system can significantly reduce the number of remaining unfamiliar words after the prediction and glossing for all users.

Finally, we compared the time required to train the costly batch model and our combined method. As the latter took 0 . 43 s for the training of 500 documents on average while the former took 1975 . 53 s, the effectiveness of the combined method is shown.
To conclude, the combined use of batch learning for mainly learning word difficulty and online learning for mainly learning the users X  language ability is efficient for increasing the training speed, decreasing the required number of clicks, and decreasing the number of unfamiliar words.

