 Sapienza University of Rome Sapienza University of Rome of manual annotations, not only for training purposes, but also for testing purposes. Word Sense
Disambiguation (WSD) is a case in point, as hand-labeled data sets are particularly hard and time-consuming to create. Consequently, evaluations tend to be performed on a small scale, which does not allow for in-depth analysis of the factors that determine a system X  X  performance. tion for the WSD task. We do this by providing two main contributions: First, we put forward two novel approaches to the wide-coverage generation of semantically aware pseudowords (i.e., suitable type of pseudoword to create large pseudosense-annotated corpora, which enable a large-scale experimental framework for the comparison of state-of-the-art supervised and knowledge-based algorithms. Using this framework, we study the impact of supervision and knowledge on the two major disambiguation paradigms and perform an in-depth analysis of the factors which affect their performance. 1. Introduction
Word Sense Disambiguation (WSD) is a core research field in computational linguis-tics dealing with the automatic assignment of senses to words occurring in a given context (Navigli 2009, 2012). There are two major paradigms in WSD: supervised and knowledge-based. Supervised WSD starts from a training set and learns a computa-instances of the same word. Knowledge-based WSD, instead, performs the disambigua-tion task by using an existing lexical knowledge base X  X hat is, a semantic network to which graph algorithms, for example, can be applied. However, both disambiguation paradigms have to face the so-called knowledge acquisition bottleneck, namely, the difficulty of capturing knowledge in a computer-usable form (Buchanan and Wilkins 1993).
 which has to be carried out separately for each word sense and repeated for each new language of interest. Importantly, the largest manual efforts for providing a wide-coverage semantic network and training corpus for WSD date back to the early 1990s for the WordNet dictionary (Miller et al. 1990; Fellbaum 1998) and to 1993 for the SemCor corpus (Miller et al. 1993). In fact, although cheap and fast annotations could be obtained by means of the Amazon Mechanical Turk (Snow et al. 2008) or voluntary collaborative editing such as in Wikipedia (Mihalcea 2007), producing annotated resources manually is still an arduous and understandably infrequent endeavor. Despite recent efforts in this direction, including OntoNotes (Pradhan et al. 2007b) and MASC (Ide et al. 2010), most work is now aimed either at the automatic acquisition of training data (Zhong and Ng 2009; Moro et al. 2014) and lexical knowledge resources (Navigli 2005; Cuadros and Rigau 2008; Ponzetto and Navigli 2010), or at the large-scale acquisition of an-notations via games (Venhuizen et al. 2013) or even video games with a purpose, as recently proposed by Vannella et al. (2014). As a result, state-of-the-art performance can be achieved with both supervised (Zhong and Ng 2010) and knowledge-based (Navigli and Ponzetto 2012b; Moro, Raganato, and Navigli 2014) paradigms in different settings and conditions. Moreover, existing studies hypothesize that this performance can be further improved when larger amounts of manually crafted sense-tagged data or structured knowledge are made available (Martinez 2004; Cuadros and Rigau 2008;
Martinez, de Lacalle, and Agirre 2008; Navigli and Lapata 2010). All these results, how-ever, are obtained on small-scale data sets with different characteristics, thus making it difficult to draw conclusions on the factors that impact the system X  X  performance. generation of semantically aware pseudowords, called similarity-based pseudowords.
At the core of this approach was the Personalized PageRank algorithm (Haveliwala 838 2002) on the WordNet graph, which was utilized to find the most semantically similar monosemous representative for a given sense of a real ambiguous word. The main strength of the similarity-based approach lies in its flexibility, allowing high minimum frequency constraints to be set on its selection of pseudosenses, while maintaining its overall sense modeling quality.
 approach for generating semantically aware pseudowords which leverages topic signa-tures; 2) we utilize the best type of pseudoword to create a novel framework for large-scale evaluation and comparison of WSD systems; 3) based on this framework, we carry out a large-scale comparison of state-of-the-art supervised and knowledge-based WSD algorithms; and 4) we study the impact of the amount of supervision and knowledge on the two major disambiguation paradigms and perform an in-depth analysis of the factors and conditions that determine their performance.
 work concerning the impact of the knowledge acquisition bottleneck on WSD and pro-vide an explanation of our pseudoword-based approach. In Section 3 we describe pseu-dowords and overview the existing approaches to their generation. We then present two new approaches that address the issues associated with existing pseudowords, hence enabling the wide-coverage generation of semantically aware pseudowords. In Sec-tion 4, we perform various experiments to assess the degree of realism of our proposed pseudowords. We then illustrate how we leverage our pseudowords to generate large sense-tagged data sets in Section 5. The experimental set-up for pseudoword-based
WSD is described in Section 6. Experimental results as well as the findings are presented and discussed in Section 7. Finally, we provide concluding remarks in Section 8. 2. Related Work 2.1 Supervised WSD and the Knowledge Acquisition Bottleneck
Over the last few decades, WSD systems have been suffering from disappointingly low performance, especially in an all-words setting in which one has to cover the entire lexicon of the given language (Snyder and Palmer 2004; Pradhan et al. 2007a).
In fact, one of the major obstacles to high-performance WSD is the so-called knowledge acquisition bottleneck (Gale, Church, and Yarowsky 1992b): In order to learn accurate word experts, supervised systems need training data for each word of interest, a very demanding task as far as wide coverage is concerned (i.e., one which would require the manual annotation of millions of word instances in context).
 sense-tagged corpora have been proposed. Some of these approaches are based on boot-strapping techniques (Yarowsky 1995; Mihalcea 2002; Pham, Ng, and Lee 2005), namely, algorithms which start from a large unlabeled corpus, and a small labeled one, and iter-atively populate the latter with an increasing number of sense-annotated sentences from the former data set. Other approaches search the Web or large corpora to retrieve, for each sense, a large number of sentences containing either a set of sense-specific mono-semous relatives (Leacock, Chodorow, and Miller 1998; Martinez, de Lacalle, and Agirre 2008) or search phrases (Mihalcea and Moldovan 1999). Collaborative knowledge re-sources, such as Wikipedia, have also been exploited for generating sense-tagged data (Mihalcea 2007; Shen, Bunescu, and Mihalcea 2013), giving rise to issues, however, such as the encyclopedic nature of the sense inventory and the lack of training of annotators. resources such as parallel corpora (Chan and Ng 2005a; Wang and Carroll 2005; Chan,
Ng, and Zhong 2007). Most of these techniques, however, require human intervention for mapping the translation of a word in the target language to the correct sense of the corresponding word in the source language. Recently, Zhong and Ng (2009) tackled this problem by using a bilingual dictionary. However, the dictionary has to be aligned to the sense inventory of interest (e.g., WordNet) and a large parallel corpus must be available that covers the full range of meanings in a lexicon. The approach, implemented in a system based on Support Vector Machines and called It Makes Sense (Zhong and Ng 2010, IMS), attains state-of-the-art performance on lexical sample and all-words WSD tasks. However, according to our calculation on the available models, only provide training examples for about one third of ambiguous nouns in WordNet, more than half of which have only one of their senses covered.
 tagged corpora with a small amount of tagged data for the domain of interest (Khapra et al. 2010), or estimate the sense distribution of the new domain data set with the help of parallel corpora (Chan and Ng 2005b, 2007), thus relieving the knowledge acquisi-tion bottleneck. However, domain adaptation approaches typically suffer from lower disambiguation performance and still require annotated data for the domain of interest. 2.2 Knowledge-Based WSD and the Knowledge Acquisition Bottleneck
Knowledge-based WSD systems are equally affected by the knowledge acquisition bottleneck, as they exploit the knowledge and structure of lexical knowledge bases in carrying out the disambiguation task. Therefore, in order to obtain high performance, knowledge-based systems are applied to large, wide-coverage lexical knowledge bases.
However, the largest hand-crafted resource of this kind (i.e., WordNet) dates back to 1990 with subsequent updates, which attests to the high cost of knowledge engineering on a large scale. Moreover, WordNet mostly provides taxonomic knowledge, while ne-glecting much syntagmatic relational information between concepts. As a consequence, over the past few years several automatic techniques have been proposed that enrich
WordNet with new relation edges, such as those obtained from disambiguated glosses (Mihalcea and Moldovan 2001), collocation dictionaries (Navigli 2005), topic signa-tures (Agirre et al. 2001; Cuadros and Rigau 2008), and collaborative semi-structured resources (Hovy, Navigli, and Ponzetto 2013).
 proaches such as Personalized PageRank (PPR; Agirre, de Lacalle, and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014), context-based vertex degree (Navigli and Lapata 2010), or, more recently, a densest-subgraph algorithm that jointly performs
WSD and Entity Linking (Moro, Raganato, and Navigli 2014). Not only do these methods outperform supervised WSD systems when applied within a domain, but, when the knowledge base is enriched with tens of thousands of semantic relations automatically extracted from Wikipedia, performance comparable to that of state-of-the-art supervised systems can be obtained in a general all-words setting, too (Ponzetto and Navigli 2010; Moro, Raganato, and Navigli 2014).
 ages a large multilingual semantic network, called BabelNet (Navigli and Ponzetto 840 2012a), to achieve state-of-the-art results on both general all-words and domain-oriented WSD (Navigli and Ponzetto 2012b). Experimental results show that the joint use of multilingual knowledge enables further improvements over monolingual WSD. However, the power of this disambiguation system lies mainly in its usage of the
BabelNet multilingual semantic network. In fact, Agirre, Lopez de Lacalle, and Soroa (2014) showed that under similar conditions (i.e., when the same lexical knowledge base was used), the PPR algorithm can outperform the graph-based WSD algorithms used by Navigli and Ponzetto (2012b). 2.3 The Supervision vs. Knowledge Dilemma
Unfortunately, as of today we do not have unequivocal insights into which disambigua-tion paradigm is more suitable under which conditions. As a matter of fact, not only does each implemented system come with its own amount and kind of supervision or knowledge, making it hard to determine the contribution of the supervision or knowl-edge vs. that of the WSD algorithm, but test data sets are small, typically comprising one or two thousand sense-tagged word items, which prevents us from drawing solid conclusions. Even the largest annotation effort ever X  X amely, the SemCor sense-tagged data set (Miller et al. 1993), comprising around 235,000 semantic annotations X  X overs only about 15% of word types in WordNet with an average of 10 instances per word, thus precluding large-scale experimental studies.
 to generate sense-annotated data with the help of artificial ambiguous words, called pseudowords. Pseudowords are created by conflating a set of unambiguous words called pseudosenses. The idea of pseudowords was simultaneously introduced by Gale,
Church, and Yarowsky (1992a) and Sch  X  utze (1992) as a means of generating large amounts of artificially sense-tagged evaluation data for WSD algorithms. Pseudowords have also been used in other work aimed at studying the effects of data size on machine learning for confusion set disambiguation (Banko and Brill 2001), evaluation of selec-tional preferences (Erk 2007; Bergsma, Lin, and Goebel 2008; Chambers and Jurafsky 2010), or Word Sense Induction (Di Marco and Navigli 2013; Jurgens and Stevens 2011). biguous words picked out to be in the same range of occurrence frequency (Sch  X  utze 1992), or leveraging homophones and OCR ambiguities (Yarowsky 1993), does not provide a suitable model of a real polysemous word (Gaustad 2001), since in the real world different senses, unless homonymous, share some semantic or pragmatic relation.
For this reason, random pseudowords, when used for WSD evaluation, were found to be easier to disambiguate compared with the human-generated pseudowords (Gaustad 2001), thus leading to an optimistic upper-bound estimate on the performance of WSD classifiers (Nakov and Hearst 2003).
 semantic relationships between senses. To this end Nakov and Hearst (2003) used lexical category membership from a medical term hierarchy (extracted from MeSH
Subject Headings]) to create  X  X ore plausible X  pseudowords. By considering the distri-butions from lexical category co-occurrence, they produced a set of pseudowords that were closer to real ambiguous words in terms of disambiguation difficulty than random pseudowords. However, this approach requires a specific hierarchical lexicon and falls short of creating many pseudowords with high polysemy.
 in the surroundings of a sense, that is, selected among concepts directly related to the given sense. Senses of a real ambiguous word have been modeled by picking out the most similar monosemous morpheme from a Chinese hierarchical lexicon (Lu et al. 2006). Pseudowords are then constructed by conflating these morphemes accordingly.
However, this method leverages a specific Chinese hierarchical lexicon, in which differ-ent levels of the hierarchy correspond to different levels of sense granularity. A more flexible approach is proposed by Otrusina and Smrz (2010), who model ambiguous words in WordNet. For each particular sense, they search its surroundings in the Word-Net graph in order to find an unambiguous representative for that sense.
 can enable a large-scale evaluation framework for WSD, mainly because they suf-fer from coverage issues that prevent the creation of wide-coverage sense-annotated data sets. In this article we propose new pseudoword generation techniques that allow for the creation of thousands of artificial words having sufficient occurrence coverage within a large corpus. We then leverage our semantically aware pseudowords to create an evaluation framework which enables a large-scale comparison of state-of-the-art supervised and knowledge-based WSD. 3. Pseudowords
A pseudoword is an artificially created ambiguous word created by concatenating two or more distinct words. Formally, p = w 1 * w 2 *. . . * w degree n where each w i is called a pseudosense . Each pseudosense is usually identified by an unambiguous word drawn from the set of monosemous words in a given lexicon (e.g., WordNet). For instance, press release*ship*camel is a pseudoword with three distinct meanings explicitly identified by its pseudosenses (i.e., press release , ship , and camel ).
To this end, an untagged corpus C is automatically annotated with a pseudoword p = w 1 * w 2 *. . . * w n by substituting all occurrences of w i  X  X  1, ... , n } . As an example, consider the following three sentences: rences of press release , ship and camel with the pseudoword press release*ship*camel , while noting the replaced term as the corresponding sense: where b1, b2, and b3 are three annotated sentences for our pseudoword press release*ship*camel with three different intended senses. This way, pseudowords can be leveraged to automatically annotate an arbitrarily large number of sentences. As men-842 be unambiguous, so as to avoid the introduction of uncontrolled ambiguity. Another constraint is that the pseudosense w i must appear in a sufficient number of sentences in the corpus C . This constraint on the occurrence frequency guarantees that there exist as many sentences in the corpus as the number of annotated sentences that are requested for the task of interest which will exploit the resulting annotated corpus.
 monosemous words from WordNet. This can be considered as the most immediate approach for generating a pseudoword where constituents are randomly picked from the set of all monosemous words given by a lexicon. This results in a set of pseudowords (hereafter called random pseudowords ) that are highly likely to have semantically unrelated pseudosenses. However, we know that the different senses of a real word are often in a semantic or etymological relationship. Therefore, random pseudowords can only model homonymous distinctions (such as the centimeter vs. curium senses of the noun cm ), and fall short of modeling systematic polysemy (such as the lack vs. insufficiency senses of the noun deficiency ).
 random pseudowords. A possible solution is to create pseudowords that model existing ambiguous words by providing, for each pseudoword, a one-to-one correspondence be-tween each pseudosense and a corresponding sense of the modeled word. For instance, lack*shortfall is a good pseudoword modeling the real word deficiency as its pseudosenses preserve the meanings of their corresponding real word X  X  senses. We call artificial words of this kind semantically aware pseudowords , in that they aim at listing senses that the senses of real words in the lexicon. For example, the lack-insufficiency relation is encoded in the pseudoword for deficiency , which would not be possible if we generated a random pseudoword.
 data sets that have similar properties to their real counterparts and this makes them particularly suitable for the evaluation of WSD and Induction algorithms (Bordag 2006;
Jurgens and Stevens 2011; Di Marco and Navigli 2013). In fact, in a real sense-annotated data set different senses of a word appear in distinct contexts. The extent of this dis-tinction, however, depends on the semantic relatedness of the corresponding senses.
The intuition behind semantically aware pseudowords is that they model each sense of an ambiguous word through a semantically similar monosemous representative that should appear naturally in contexts that are similar to those of its corresponding real sense. For this reason, these pseudowords should be expected to result in data sets sense-annotated data sets.
 in full detail for the first time in this article, for the generation of semantically aware pseudowords that use WordNet as the reference lexicon. In what follows we focus on nominal pseudowords, and leave the extension to other parts of speech to future work. 3.1 Vicinity-Based Pseudowords
As discussed in Section 2, earlier techniques for the generation of semantically aware pseudowords were either inherently restricted to specific hierarchical lexicons utilized in the generation process, or to the number of pseudowords they could generate. An idea put forward by Otrusina and Smrz (2010) was to create pseudowords by combining representatives for each individual sense of a real ambiguous word in WordNet. The representatives were selected among monosemous relatives (i.e., unambiguous words that are structurally related to a given sense). This method for finding monosemous representatives for senses has been in use since 1998, when it was first proposed for the unsupervised acquisition of sense-tagged corpora (Leacock, Chodorow, and Miller 1998).
 synonym sets, called synsets, which encode the different meanings of words. In order to find a monosemous representative for a given synset, the approach (hereafter referred to as the vicinity-based approach) performs a search on the set of words in the same synset and the surrounding ones (i.e., the synsets connected to that synset by means of
WordNet X  X  lexico-semantic relations). These related synsets include siblings and direct hyponyms. In the case where no monosemous candidate could be found among these synsets, the search space is further extended to hypernyms and meronyms.
 drink, and drug) in WordNet 3.0. We show in Table 1, for each of the three senses synsets. Monosemous words are shown in bold in the table. As can be seen, there exist multiple monosemous candidates for each sense ( coca cola , pepsi , and pepsi cola for the second sense; nose candy , cocaine , and cocain for the third sense; and dozens of candidates in the direct siblings X  vicinity of the first sense). Among these candidates
Otrusina and Smrz (2010) select those whose occurrence frequency ratio in a given text corpus is most similar to that of the senses of the corresponding real word as given by a sense-annotated corpus. However, calculating the occurrence frequency of individual senses of a word requires a large-enough sense-tagged corpus. This dependency on sense-annotated data is a disadvantage of the vicinity-based approach that limits its ability in modeling arbitrary words.
 844 flexibility in generating pseudowords that can be leveraged for creating a large-scale pseudosense-tagged corpus, where we need each pseudosense to occur with a relatively large minimum frequency. Due to its small search space, the approach falls short of identifying suitable monosemous representatives for many given senses, which under-mines its ability to cover most of the ambiguous nouns in WordNet. We show in Table 2 the percentage of nouns in WordNet that could be modeled using the vicinity-based approach when Gigaword (Graff and Cieri 2003) was used as our corpus. Coverage statistics are presented for four different values of minimum frequency: 0 (no minimum frequency constraint), 50, 200, and 1,000. Besides the overall coverage (rightmost col-umn), in the table we also present the coverage percentage by degree of polysemy.
Here, an ambiguous noun in WordNet is considered as covered by its corresponding vicinity-based pseudoword if, for each of its senses, a suitable monosemous candidate can be found in its surrounding that also satisfies the specified minimum frequency in the corpus. As can be seen from the table, the approach can only model about 60% of ambiguous nouns in WordNet 3.0 when a small minimum frequency of 50 sentences in the large Gigaword corpus is assumed. The coverage continues to drop with the increase of minimum frequency up to only 25% of the ambiguous nouns covered when a minimum frequency of 1,000 noun occurrences is required (last row of Table 2), with most of the covered words having low polysemy. This shows that the approach is not flexible enough for generating pseudowords that can be leveraged for creating large, wide-coverage pseudosense-annotated data sets.
 proach, in the next two sections we propose two new approaches for the generation of semantically aware pseudowords. 3.2 Similarity-Based Pseudowords
We propose a new approach to the generation of pseudowords that enables the creation of semantically aware pseudowords while tackling the coverage and flexibility issues of the vicinity-based approach. In contrast to the vicinity-based method, which takes as its search space the surroundings of a sense, our technique considers the WordNet semantic network in its entirety, hence enabling us to determine a graded degree of similarity between a given sense and all other synsets in WordNet. The similarity-based approach identifies, for each sense of a given ambiguous word, the most semantically similar monosemous word satisfying the minimum occurrence frequency constraint.
Our method can be considered an extension of the vicinity-based approach as it replaces its pseudosense selection technique with a graph-based similarity measure.
This expands the search space for finding pseudosenses from a small set of surrounding synsets to virtually all synsets in WordNet.
 rithm, a graph-based technique that has been used previously as a core component for semantic similarity (Hughes and Ramage 2007; Pilehvar, Jurgens, and Navigli 2013) and
WSD (Agirre and Soroa 2009; Agirre, Lopez de Lacalle, and Soroa 2014). PPR can be used to estimate a probability distribution denoting the structural importance of all the nodes in a graph for a given target node. When applied on a semantic network, such as the WordNet graph whose nodes are synsets and edges the lexico-semantic relations, the notion of importance can be interpreted as semantic similarity. The reason behind our selecting a graph-based similarity measure was that the alternative context-based methods, such as Lin X  X  (1998) measure, have been shown to require a wide-coverage sense-tagged data set in order to calculate similarities on a sense-by-sense basis for all words in the lexicon (Otrusina and Smrz 2010). Also, among WordNet-based approaches, PPR reports state-of-the-art results on semantic similarity (Agirre et al. 2009) and WSD data sets (Agirre, Lopez de Lacalle, and Soroa 2014), thus representing a suitable graph-based measure for finding the most appropriate pseudosenses. pseudowords. The algorithm takes as input an ambiguous word w , and generates its corresponding similarity-based pseudoword P w whose i th pseudosense models the i sense of w . Additionally, the algorithm provides, for each generated pseudoword, a confidence degree denoting the average ranking of the selected pseudosenses. corresponding to its individual senses (lines 5 X 18) and identifying the most suitable monosemous representative for each. For each sense of w , we run the PPR algorithm Algorithm 1: Generate a similarity-based pseudoword 846 by initializing it from the corresponding synset s (line 6). As a result, PPR outputs a probability distribution over all synsets in WordNet denoting the semantic similarity of each synset to s . 4 The synset distribution is then sorted according to its values (line 7).
We then go through all its nominal synsets ( s 0 ) in the search for a suitable monosemous noun (line 11). This search continues until a suitable candidate is found that satisfies the minimum occurrence frequency minFreq . Upon finding this candidate, the selected monosemous word w 0 is added as the corresponding pseudosense for the i (line 12). These steps are repeated for every sense of w .
 the more confidence we have in the preservation of meaning. Therefore, we calculate a confidence score ( averageRank in the algorithm) as the average of the synset X  X  positions (in the various similarSynsets lists) from which the pseudosenses of P (line 19). We will later use this confidence score for evaluating our pseudowords. The algorithm returns as its output, for a given word w , the corresponding pseudoword P along with its averageRank score (line 20).
 coke . Table 3 shows the list of top-five most similar synsets for each of the three senses of this term, as given by the PPR algorithm. Our algorithm selects the highest ranking monosemous candidates that satisfy the minimum frequency (=1,000 in the example) for each sense (shown in bold in the table). Hence, fuel*coca cola*cocaine is returned as the pseudoword corresponding to the word coke . Note that the top-ranking synsets are those also found by the vicinity-based approach. However, thanks to PPR working on the entire network, our similarity-based approach can back off to more distant, though similar, synsets. We show in Table 4 some examples of ambiguous words along with their generated similarity-based pseudowords (minimum frequency is again set to 1,000).
 based approach is able to select a monosemous candidate for each sense from a relatively-large ranked list of similar synsets. This solves both the coverage and flex-ibility issues of the vicinity-based approach for higher values of minimum frequency.
However, as mentioned earlier, the higher the position of a selected pseudosense in the sorted list of similarSynsets , the more confidence we have in the preservation of meaning.
For this reason, we analyzed the averageRank values output by Algorithm 1 in order to see how often our algorithm needs to resort to lower-ranking items in the similarSynsets minFreq , the mean and mode statistics of the averageRank scores of all the generated pseudowords for all the nouns (up to polysemy degree 12) in WordNet. As can be seen in the table, the higher the value of minFreq , the further the algorithm descends through 848 the list similarSynsets to select a pseudosense. However, the mode statistics in the table suggests that even when minFreq is set to a large value, most of the pseudosenses are picked out from the highest-ranking positions in the similarSynsets list. 3.3 Topic Signature-Based Pseudowords
As an alternative means of finding suitable monosemous representatives for word senses with the PPR algorithm, we propose using automatically generated topic signa-tures. Topic signatures (TS) are weighted topical vectors that are associated with senses or concepts (Lin and Hovy 2000). The dimensions of these vectors are the words in the vocabulary and their weights determine the relatedness of each of these words to the target word sense. These vectors can be obtained automatically from large corpora or the Web with the help of monosemous relatives.
 the monosemous word with highest relatedness (i.e., largest weight) which satisfies the minimum frequency constraint. The generation process of the TS-based pseudowords is very similar to that of similarity-based pseudowords: Whereas the latter performs a search in the sorted PPR vector of a particular sense to obtain a suitable monosemous representative, the former considers the sorted TS vector as its search space. Also note that the PPR vectors are indexed with synsets, whereas topic signatures have lemmas as their indices.
 (2004) for nominal senses of polysemous nouns in WordNet 1.6. relatives for each sense were obtained by taking into account WordNet relations such as synonyms, hypernyms, hyponyms, and siblings that were later used to query the Web and create a large corpus. This corpus was then used to build topic signatures. word coke . The first monosemous candidate for each sense is shown in bold (again, the minimum frequency is assumed to be 1,000 here). The corresponding pseudoword generated using this approach is fuel*pepsi*heroin .
 the vicinity-based approach, the additional step of gathering related instances for these representatives guarantees wider coverage. We calculated the coverage of TS-based pseudowords to be 84% (over ambiguous nouns of WordNet 1.6 and with no minimum frequency constraint), which is comparable to that of vicinity-based pseudowords (i.e., 83%, see Table 2). We observed in Table 2 that the coverage of the vicinity-based pseudowords drops rapidly with the increase in minimum frequency such that for a minimum frequency of 1,000 only 25% of the polysemous nouns could be modeled. The
TS-based approach, instead, provides a better flexibility for higher values of minimum frequency, hence enabling the generation of large-scale annotated data sets. Thanks to its larger search space, the TS-based approach is able to retain the same 84% coverage for a minimum frequency of 1,000.
 tion 3.2), this approach provides a different way of overcoming the coverage issue of vicinity-based pseudowords. However, the former guarantees 100% coverage, whereas the latter suffers from the lack of monosemous relatives for a portion of WordNet senses, leading to non-optimal coverage. 4. Pseudoword Evaluation
In Sections 3.2 and 3.3 we presented two techniques for the generation of semantically aware pseudowords that were able to address the coverage and flexibility issues of the vicinity-based approach. In order to verify the ability of these pseudowords to model various properties of real ambiguous words, we performed three separate evaluations so as to assess them from different perspectives: pseudosense-annotated data sets, we performed evaluations on pseudowords gener-ated with minFreq per pseudosense set to a high value of 1,000 (i.e., we can generate 1,000 annotated sentences for each pseudosense) using the English Gigaword corpus (Graff and Cieri 2003). 4.1 Disambiguation Difficulty of Pseudowords
Our first experiment is an extrinsic evaluation to assess the correlation between the difficulty of the disambiguation task when using pseudowords and real words. The basic idea behind this experiment is to verify, through a disambiguation task, if the semantic similarity among the senses of an ambiguous word is preserved in its cor-responding pseudoword. Semantically similar senses of an ambiguous word will tend to appear in similar contexts, making it relatively difficult to discriminate between them.
Conversely, ambiguous words that have semantically distinct senses (e.g., homonyms) will be relatively easier to disambiguate. Given that our pseudowords directly model real ambiguous words, we ideally expect a pseudoword to preserve the same level 850 of semantic similarity between pseudosenses as that of its corresponding real word, and therefore to exhibit a comparable degree of disambiguation difficulty to that of its corresponding real word.
 first, all the sense-tagged words in a manually annotated lexical sample data set are modeled using the approach. Next, a corresponding pseudosense-annotated data set is automatically constructed by sampling sentences from a corpus while maintaining the same number of training and test sentences for each word as that of the original manually tagged data set. A correlation analysis is then carried out to compare the disambiguation performance of a supervised WSD system on a given ambiguous word against its corresponding pseudoword. In this experiment we evaluate our similarity-based, TS-based, and, as baseline, random pseudowords. Owing to the fact that for the given minimum frequency of 1,000 we could generate only 5 of the 20 nouns using the vicinity-based approach, we had to exclude the approach from this experiment. and Kilgarriff 2004) as our manually sense-tagged corpus. The data set provides for 20 nouns of polysemy 3 to 10 an average number of 180 and 90 sense-tagged sentences in its training and test sets, respectively. We generated the similarity-based and TS-based pseudowords corresponding to these 20 nouns, as well as a set of 20 random pseudo-words. For each set of these pseudowords we generated corresponding pseudosense-annotated training and test data sets by randomly sampling distinct sentences from the English Gigaword corpus (Graff and Cieri 2003). Therefore, we ended up with four data sets, namely: the Senseval-3 data set of real words, and the three artificially sense-tagged data sets for the similarity-based, TS-based, and random pseudowords.
Each of the artificially annotated data sets consisted of training and test portions comprising the same number of instances per sense (i.e., the same sense distribution) as that of the original Senseval-3 training and test data sets. Next, for each of our four data sets, we trained a supervised WSD system on the training set and applied it to the corresponding test set. In order to ensure more reliable results we follow Otrusina and
Smrz (2010) and report, for all experiments in this evaluation, the average results for five runs. To this end, we randomly sampled the training and test data sets from the combination of all items while preserving the original proportions. Also, in the random setting, we provide the results averaged on a set of 25 different pseudowords modeling a given ambiguous noun.
 state-of-the-art supervised WSD system that is based on support vector machines (we will describe IMS in more detail in Section 6.4). Note that we measure disambiguation measures).
 biguation difficulty) for real words vs. those for the similarity-based, TS-based, and random pseudowords. For each set of pseudowords, we also show the line fitted to the corresponding set of points by means of linear regression. Ideally, this line should coincide with the dashed diagonal line in the figure, denoting perfect similarity. In the next three subsections we provide an analysis of the scatter plot in Figure 1 and a discussion. 4.1.1 Overall Disambiguation Difficulty. The closer a line of best fit is to the center of the plot, the closer are its corresponding pseudowords to real words in terms of overall disambiguation difficulty (note that the plot X  X  axes are truncated to the range [40,100] and the center point is shown by the star). As can be seen in Figure 1, the line corre-sponding to our similarity-based pseudowords is the closest to the center, showing that these pseudowords provide a better modeling of real words in terms of disambiguation difficulty. We also show the corresponding values of recall performance in Table 7. We can see from the table that the overall system performance of similarity-based 852 pseudowords (75.43) is closest to that of real words (73.26). This value is 76.42 and 78.80 for TS-based and random pseudowords, respectively.
 pseudowords that are closest to real words in terms of WSD recall performance to 5 and 3 for the TS-based and random pseudowords, respectively. Accordingly, the overall sum of the differences (distance) between the recall values is smallest (129.31) for similarity-based pseudowords among the three kinds of pseudoword (194.51 for
TS-based pseudowords and an average of 196.35 for random pseudowords, ranging from 158.32 to 262.04).
 dowords in terms of overall performance, their distance from real words is much higher than that of similarity-based pseudowords (194.51 vs. 129.31). This suggests that the former tend to have a lower correlation with real words than the latter. In the following, we investigate the correlation between the disambiguation difficulties of real words and our three types of pseudowords. 4.1.2 Correlation Between Disambiguation Difficulties. The smaller the angular deviation of the line of best fit for a set of pseudowords is from the diagonal line, the higher is the cor-relation between the disambiguation difficulties of those pseudowords and real words.
As can be seen in the figure, the line corresponding to the similarity-based pseudowords has the smallest deviation from the diagonal line, showing its higher correlation with real words. The Pearson correlation coefficient between the disambiguation difficulties of similarity-based pseudowords and real words is 0.74. This value drops to 0.43 and 0.54 for TS-based and random pseudowords, respectively. Even worse, the value of 0.54 is the average of 25 highly variable correlation values (in the range of [0.18, 0.67]) over our 25 sets of random pseudowords. The reason why TS-based pseudowords show a lower correlation than random pseudowords can be found in the fact that the reported values for the latter are averaged over 25 runs. More precisely, the correlation value of 0.43 of topic signatures has to be compared with the range [0.18, 0.67] of correlations obtained by different sets of random pseudowords. 4.1.3 Discussion. We leveraged PPR and topic signatures as our sense modeling compo-nents for the generation of semantically aware pseudowords. Both approaches could solve the low coverage problem, although the results presented in this section suggest that the topic signature-based approach is not good at providing suitable monosemous substitutes for senses of real ambiguous words. A closer look at the similarity-based and TS-based pseudowords generated for some of the nouns in the Senseval-3 data set, shown in Table 8, provides a clear explanation for this shortcoming of topic signature-based pseudowords. In fact, topic signatures are based on co-occurrence information from the Web snippets retrieved for each sense of an ambiguous noun. As a result, many of the top-ranking words in each topic signature are syntagmatically related to the given sense. For example, consider the paralysis pseudosense of arm , european pseudosense of plan , and moral or weekly pseudosenses of paper . Despite being semantically related, these pseudosenses cannot be considered as good substitutes for their correspond-ing senses. Our similarity-based approach, instead, tends to favor paradigmatic (i.e., suitable substitutes for senses of real words.
 doword generation approach cannot be considered as a candidate for the generation pseudoword in our further evaluations and focus on similarity-based pseudowords only. 4.2 Representative Power of Pseudosenses
In order to maximize the possibility of preserving the meaning of the original synset, a pseudosense should be selected from the set of words in the same synset, or in the directly related synsets (e.g., hypernym synsets). However, many of the WordNet synsets do not contain monosemous terms and the similarity-based approach often needs to look further into the other indirectly related synsets so as to find a suitable pseudosense. In order to assess how often this happens, we carried out an experiment pseudosenses are selected from the synsets containing the original senses. To this end, we went through all our similarity-based pseudowords and, for each pseudosense w , checked the relationship in WordNet between the synset containing w corresponding real sense.
 WordNet relations, including indirect ones. As can be seen in the table, when minFreq 854 is set to 1, 000, only about 20% of the pseudosenses are picked out from synonyms or generalization/specialization relations (hypernym and hyponyms). This shows that a considerable portion of our pseudosenses are selected from synsets that are indirectly related to the target synset that is being modeled. These indirectly related synsets can potentially result in pseudosenses that do not have very similar meanings to the original synsets, and hence are not good representatives of them.
 power of similarity-based pseudosenses to assess how well each pseudosense models its corresponding real sense. For this purpose, we randomly sampled 10 pseudowords for each degree of polysemy from 2 to 12 from the entire set of pseudowords with minimum frequency of 1,000, totaling 110 pseudowords with 770 pseudosenses.
We then asked two annotators, neither of whom was an author of this paper, to judge the representative power of each pseudosense according to the following scale: 1 (com-pletely unrelated), 2 (somewhat related), 3 (good substitute), 4 (perfect substitute). The annotators were provided with the WordNet definitions of the corresponding synsets. in Table 10. We present in the table the representativeness scores given by each of our annotators to the individual pseudosenses of this word. The overall representativeness score is calculated as the average of the scores given by the two annotators. In the case of our example, the overall score is 3.085. We also calculated the Spearman correlation between the scores given by the two annotators for all the 770 cases to be 0.66. We show in Table 11 (top) the overall representativeness scores averaged for the full set of 770 pseudosenses, classified by polysemy degree. As can be seen from the table, the overall representative score remains around 3.0 for all polysemy degrees from 2 to 12, with the overall score being 3.1. This shows that even though about 64% of the pseudosenses are picked out from indirect relations (when minimum frequency is 1,000, cf. Table 9), they can still be considered as good representatives for their corresponding real senses.
We also present in Table 11 (bottom) the average representativeness scores only for those pseudosenses that are picked from words in the same synset (synonyms) or in the directly related and sibling synsets. As can be seen, the synonymous pseudosenses are always rated with the highest possible score of 4, whereas those obtained from direct relations maintain a relatively higher score compared with the overall repre-sentative score, which includes many pseudosenses picked from indirect relations. In fact, the similarity-based pseudoword generation approach improves the vicinity-based method to full coverage and provides a significantly better level of flexibility for higher values of minimum frequency, while maintaining a good degree of sense modeling ability. 4.3 Distinguishability Between Pseudosenses
A fundamental property of an ambiguous word is that its different senses have distinct meanings. We expect a semantically aware pseudoword to inherit this property of its real counterpart (i.e., to have pseudosenses that are semantically distinguishable from each other while being semantically similar to their corresponding senses). As an exam-ple, consider the similarity-based pseudoword philanthropist*benefactor to the noun donor . 8 The two pseudosenses of the pseudoword can be considered as good representatives for their corresponding senses. However, the distinguishability of the two real senses is not preserved in the corresponding pseudoword: whereas philanthropist only applies to the first sense, benefactor can be equally good for both senses of donor .
 distinguishability of pseudosenses of our pseudowords. For this evaluation, we used the same set of 110 pseudowords as in the previous experiment (Section 4.2). For each of these pseudowords, we presented its pseudosenses in random order to two annotators. In addition, we provided these annotators with the WordNet definitions of the senses of the corresponding noun and asked them to associate each pseudosense with the most appropriate WordNet sense. The annotators were instructed to leave a pseudosense unmapped if they found it to be equally mappable to multiple senses. We then calculated the distinguishability score for each polysemy degree as the ratio of the number of correct mappings to the total number of senses. 856 also saw in the previous experiment, the corresponding similarity-based pseudoword for this noun is fine art*disease*web browser*knowledge*electronic equipment*photograph . As illustrated in Figure 2, we provided the shuffled list of pseudosenses of this pseu-doword (left column) and the WordNet definitions of the senses of its corresponding noun, namely, mosaic (right column), to each annotator and asked them to map each pseudosense to its most suitable real sense. In this case, both annotators mapped all pseudosenses to their correct senses; hence, the distinguishability score given by each annotator for this pseudoword was 6/6 = 1.
 polysemy scores. As can be seen in the table, the distinguishability score is inversely proportional to the polysemy degree (there is a high negative Pearson correlation of 0.9 between the two). However, the score remains above 0.70 even for the pseudowords with higher polysemous degrees. The overall score of 0.79 shows that a large portion of pseudosenses can be associated with their corresponding real senses only. Therefore we can conclude that the similarity-based pseudowords effectively preserve the distin-guishability of senses of their real counterparts. 4.4 Discussion
We performed three experiments to evaluate the reliability of our pseudowords. We showed that the similarity-based pseudowords are fairly close to their real counterparts in terms of disambiguation difficulty. Even though our similarity-based pseudowords were slightly easier to disambiguate in comparison to real words, the high correlation observed in the first evaluation (Section 4.1) serves as a guarantee that our pseudowords can be reliable substitutes for real words in experiments concerning the analysis and comparison of WSD systems.
 individual pseudosenses of our similarity-based pseudowords as well as the distin-guishability of their pseudosenses from one another. In the representativeness exper-iment, we assessed, for each individual sense of each pseudoword in our sample set, if the meaning of the corresponding real sense is preserved and if each pseudosense can be considered as a good representative of its corresponding real sense. Finally, in the distinguishability experiment our aim was to investigate the ability of similarity-based pseudowords at preserving the distinguishability among senses of real words.
Experimental results proved that the similarity-based approach is able to provide a good modeling of individual senses of real words while preserving the distinguishability of their senses. 5. Sampling Pseudosense-Tagged Corpora
As a result of our evaluations we know that the similarity-based pseudowords are reliable substitutes for real ambiguous words in the disambiguation task. As described in Section 3, a pseudosense-tagged corpus can be generated for each pseudoword p = w 1  X  w 2  X  ...  X  w n by substituting individual occurrences of its pseudosenses w with the pseudoword p itself, while marking the pseudosense w obvious question that arises here is how to sample and distribute the sentences for a pseudoword across its pseudosenses. In the following two sections we illustrate two corpus sampling strategies used in our experiments. 5.1 Uniform Sense Distribution
A first, simple sampling strategy for pseudosense-tagged corpora is the uniform sense distribution. In this setting, all senses of a pseudoword are assumed to be observed with equal probability in the tagged corpus (i.e., we extract the same number of sentences from the corpus for each pseudosense of a given pseudoword). 5.2 Natural Sense Distribution
Although the uniform distribution can be useful in specific applications such as dictio-nary disambiguation (Litkowski 2004; Flati and Navigli 2012), or knowledge resource mapping (Navigli and Ponzetto 2012a; Matuschek and Gurevych 2013), in natural text we know that most of the occurrences of an ambiguous word correspond to a usually small subset of predominant senses of that word (Zipf 1949; Sanderson and
Van Rijsbergen 1999). In other words, occurrences of an ambiguous word in a real text are usually distributed across its senses according to a highly skewed distribu-tion. In order to model this natural distribution, we adopt a distribution sampling strategy. To this end we estimate sense distributions from SemCor (Miller et al. 1993), the largest sense-tagged corpus of English. However, as we show in Table 13, Sem-
Cor provides reliable distribution estimates for only some hundred words. The table shows for each degree of polysemy the number of distinct nouns in SemCor that are sense-annotated at least once, 10 times, or 20 times, compared with the correspond-ing total number of ambiguous nouns in WordNet (last column in the table). Be-cause we could obtain from SemCor the sense distribution of only some hundred 858 low-polysemy nouns and a few dozen high-polysemy nouns, we decided to drop the requirement of estimating sense distributions directly (i.e., to model the semantically aware pseudoword p w on the sense distribution of w ). Instead, we first collected all the sense distributions of nouns with at least 10 occurrences in SemCor. Our choice of 10 as the minimum occurrence frequency was to guarantee some hundreds of distributions for lower polysemy degrees and dozens for the higher ones (see Table 13). In addition, given the highly skewed nature of sense distributions in SemCor, 10 samples should usually be enough for a reliable estimation of the corresponding sense distributions to be made, even for higher polysemy degrees. Having at hand a large set of distri-butions estimated for each polysemy degree, every time we needed a new pseudo-word with m senses, we randomly picked out a sense distribution of size m from our collection.
 2 to 12 in Table 14. As can be seen from the table, all average distributions, especially those of low polysemy nouns, are skewed towards predominant senses. 6. Experimental Set-up pseudosense-annotated data sets by proposing a flexible approach for generating se-mantically aware pseudowords that model arbitrary real words. We have also ex-plained different sampling strategies for distributing pseudosense-annotated sentences according to two different distributions. We are now ready to set up our experimental framework for large-scale WSD.
 how we selected a reliable subset of pseudowords for the experiments (Section 6.2); this is followed by a description of the process of generating training and test data sets (Section 6.3). In Section 6.4 we introduce the two WSD systems used as representatives of the two main WSD paradigms (i.e., supervised and knowledge-based) in our experi-ments. We then provide, in Section 6.5, the details of the method through application of which our knowledge-based system is able to benefit from the training data. Finally, in
Section 6.6 we describe the evaluation measures used in our experiments. 6.1 Corpus
We sampled all the sentences for pseudosense tagging from the English Gigaword cor-pus (Graff and Cieri 2003), a comprehensive corpus of English newswire text. The cor-pus comprises about 4.1 million documents, each containing an average of 430 words, totaling approximately 1.76 billion words. In a preprocessing phase, we removed sen-tences whose length was either longer than 50 words or shorter than 10 words. The corpus was then annotated with part-of-speech tags using the C&amp;C tagger (Curran and
Clark 2003) trained on the Penn Treebank (Marcus et al. 1994). The resulting corpus contained around 50 million sentences. 6.2 Pseudoword Selection
As a result of our similarity-based approach, we could generate as many pseudowords as polysemous nouns in WordNet 3.0 (i.e., 15,935 pseudowords). However, for two pseudowords for generating the data sets for our experiments.
 experiments, as it is not possible to perform a reliable analysis on such degrees given that very few pseudowords can be generated for them (about 0.3% of ambiguous nouns in WordNet have polysemy degree 13 or higher). Secondly, we observed that in practice a large enough portion of pseudowords for each polysemy degree can provide a reliable performance estimation on that polysemy degree. Therefore, we selected, for each polysemy degree, the top 300 pseudowords according to the calculated averageRank score (cf. Section 3.2). Given that the score denoted our confidence in the preservation of meaning while modeling pseudosenses, this top-ranking subset of pseudowords pseudowords across different degrees of polysemy. Note that for polysemy degrees 6 to 12, where there exist less than 300 nouns in WordNet, we consider all the corresponding pseudowords. In Appendix A, we show that this subset is large enough for an accurate estimation of the performance of a WSD system. We also sampled a separate set of 199 pseudowords for tuning purposes (cf. Section 6.5.1 for tuning). Table 15 (second 860 row) shows the distribution of this tuning set of pseudowords across different polysemy degrees. 6.3 Generating Data Sets
Data set size. The first question that comes to mind before generating data sets is that of the number of sentences to be pseudosense-tagged for each pseudoword. As we showed in Section 3.2 (Table 5), the minimum occurrence frequency ( minFreq , which corresponds to the number of sentences to be tagged with a particular pseudosense) directly affects the averageRank score, a measure that we interpreted as our confidence in the preservation of meaning of a real sense through its corresponding pseudosense. This suggests a trade-off between the scale of our experiments and their overall accuracy. We show in Table 16 the statistics of the averageRank score for the subset of pseudowords selected for our experiment when six different values of minFreq were assumed while generating pseudowords. Currently, the MASC corpus (Ide et al. 2010), even though covering a small set of 20 nouns, provides the highest number of manually annotated instances per word, namely, 1,000 sentences. In our experiments we followed MASC and generated 1,000 annotated instances for each of our 1,960 pseudowords. As can be seen from Table 16, when minFreq = 1,000, a pseudosense is on average selected from the 8.4 th position in the similarSynsets list (given by mean), with most of them being picked out from the first position (given by mode).

Data set configurations. In addition to being large-scale and accurate, we also wanted our experiments to cover a wide range of possible real-world scenarios. In Section 5, we identified two different sense distributions according to which we could produce pseudosense-tagged corpora, namely, the uniform distribution and the natural one.
In our experiments, we considered all four possible ways of combining the sense distributions of training X  X est data X  X hat is, Natural-Natural (Nat-Nat), Uniform-Uniform (Uni-Uni), Uniform-Natural (Uni-Nat), and Natural-Uniform (Nat-Uni). We provide the following rationale for each of them:
Nat-Nat . This is the traditional open-text WSD scenario (Kilgarriff and Rosenzweig 2000; McCarthy et al. 2004), in which senses are naturally distributed according to the same distribution both in the training and the test data sets.

Nat-Uni . This configuration trains a WSD system with a natural distribution but applies that any choice of a different sense distribution for the test data set would have been arbitrary, we selected the uniform one as the approximate average of sense distribu-tions across different domains. In other words, our assumption was that the uniform distribution could be thought of as the fairest different distribution. To verify this, we studied the variability of sense distribution across texts belonging to different domains.
We started from a data set of sense-annotated documents from 30 different domains provided by Faralli and Navigli (2012). We then estimated the average sense distribu-tion of all nouns across documents, shown in Figure 3 (light columns) for polysemy degrees 2 to 12 as sorted according to WordNet sense order. As can be seen, the average sense distribution across domains is not skewed, in contrast to the natural sense dis-tribution (dark columns in the figure). In addition, this configuration models a setting in which the system is not effectively provided with knowledge of all senses in the test set.

Uni-Uni . This configuration assumes a system with the same amount of knowledge for all senses, tested on a task in which all senses are equally important. As is also the case for the Nat-Uni configuration, the uniformly distributed test set in Uni-Uni also models tasks such as dictionary disambiguation, in which sense-wise precision matters (Flati and Navigli 2012).

Uni-Nat . Similarly to Uni-Uni, this configuration takes no stand on the training sense distribution, but tests it on naturally distributed data. 862 pseudoword from the Gigaword corpus for each of the two sense distributions (i.e., the impact of the amount of knowledge on the disambiguation performance, the 800 10 steps) while at the same time preserving the original sense distribution for all these sets. Overall, the data sets comprised about 2 million 10 for each of the four configurations. 6.4 Systems
We chose state-of-the-art off-the-shelf representatives for the two mainstream WSD paradigms, that is, supervised and knowledge-based WSD. 6.4.1 Supervised: It Makes Sense (IMS). In our experiments we used IMS (Zhong and Ng 2010) as the representative supervised WSD system. IMS is a publicly available
English all-words WSD system achieving state-of-the-art results on several Senseval and SemEval tasks. 11 The system classifies words in context using linear support vector machines. The context (a sentence in our case) is represented as a standard vector of features including parts of speech, surrounding words, and local collocations (Lee and Ng 2002).
 was trained with the corresponding training set and the learned word expert model was then applied to the test set. In our experiments, we used the default configuration of IMS where the system adopts a linear SVM classifier with L2-loss function. 6.4.2 Knowledge-Based: UKB. As the state-of-the-art knowledge-based WSD system, we used UKB. 12 UKB is a publicly available graph-based WSD system that exploits a pre-existing lexical knowledge base (Agirre, Lopez de Lacalle, and Soroa 2014). UKB pro-vides an implementation of the PPR algorithm (Haveliwala 2002), adapted to the task of
WSD, as proposed by Agirre and Soroa (2009). PPR is applied to a graph representation of a Lexical Knowledge Base (LKB), which is typically WordNet or an extension of it with additional semantic edges. We used the w2w variant, which has been shown to perform best (Agirre and Soroa 2009), where PPR is initialized by concentrating the probability mass on the context words other than the target word to be disambiguated. vertex (i.e., sense) of the word.
 test configuration. However, a typical knowledge-based WSD system (such as UKB) cannot directly learn from the training data (which, instead, is naturally suited to supervised WSD systems). In the following section we describe the method used in our experiments to transfer these data into readily available knowledge for UKB. systems, respectively, since we consider these two systems as state-of-the-art represen-tatives of their corresponding paradigms. 6.5 Enriching the LKB Using Training Data
Whereas supervised WSD exploits a training set to perform sense classification, knowledge-based approaches use lexical knowledge bases instead. Therefore, a simi-lar operation to that of providing an increasingly large training set is to enrich basic knowledge bases such as WordNet with additional semantic edges, as has previously been done, among others, by Navigli (2005), Cuadros and Rigau (2008), and Navigli and
Lapata (2010). The automatic knowledge injection step, however, is less immediate and natural than the supervised one. In fact, pseudowords cannot be directly used to obtain a ready-to-use set of relation edges. To cope with this issue, in each configuration we used the corresponding training set (on which IMS was trained) to extract knowledge that could be used to enrich the WordNet LKB. 13 To this end, given a pseudoword p and for each pseudosense w i  X  p , we identified the most semantically related words w using the Dice coefficient: where c ( w i , w 0 ) is the number of sentences in which w c ( w 0 ) are the total number of sentences containing individual occurrences of w respectively. We then connect, in the WordNet graph, w i to all the senses of each of the top-K related words. Ideally, the corpus used for calculating these statistics should be fully sense-tagged, namely, each usage of an ambiguous co-occurring word tagged with the intended sense. However, because our training data (as is customary for WSD lexical sample data sets) do not provide sense annotations for context words, these edges are semi-noisy in that we connect an unambiguous endpoint w i 40 other nodes (synsets) in WordNet: 15 hyponyms, 10 meronyms, 14 domain-related synsets, and a hypernym. We show 10 of these connections in Figure 4 (dashed lines). By exploiting the sentences tagged with pseudosense airplane in the training set, we obtain the list of K top-ranking semantically related words using the above-mentioned pro-cedure. We then connect in the WordNet graph airplane 1 n (we show 10 such new linkings in the figure). The highlighted nodes in the figure are the new direct neighbors of airplane 1 n in the enriched graph. As can be seen in the example, our enrichment approach provides many additional syntagmatic relations to the initial mostly paradigmatic relations in WordNet. As a result of this enrichment procedure, we are able to generate a LKB consisting of WordNet plus semi-noisy semantic edges obtained from the co-occurrence statistics of each pseudosense of our pseudowords. 6.5.1 Tuning. Although we described the method used in our experiments to obtain new semantic edges with the help of co-occurrence statistics, we did not show how we set the value of K  X  X hat is, the number of top-ranking related words we obtain from a given set of n pseudosense-tagged sentences to be used for LKB enrichment. 864
To calculate the optimal value of K , we carried out a tuning experiment on a data set built for our subset of 199 pseudowords dedicated for tuning (cf. Section 6.2). In order to consider the data set size factor in our tuning, we experimented on three different sizes of training data: 80, 400, and 800 sentences (first, middle, and last size steps). For each of these training data set sizes, we generated LKBs for different values of K and carried out disambiguation on the tuning test set.
 tions measure see Section 6.6) varies when the value of K is varied from 25 to 800 (in increasing steps of 25). We show in the figure the average performance value for the three training sizes (i.e., 80, 400, and 800). As can be observed from the figure, recall a certain point recall starts to decay as the number of additional edges increases. We present in Table 17 the corresponding values for a part of Figure 5 (i.e., K in the range [25, 300]), where the optimal recall value seems to occur for all four data set configurations. As can be seen in the table, the best performance occurs at K = 125 for Uni-Uni and
Nat-Nat configurations. However, the best performance is seen at K = 75 and K = 150 for Uni-Nat and Nat-Uni configurations, respectively.
 sense distribution of the test data set, in order to enable a fair comparison we chose the same optimal K value irrespective of the test data. The last two rows in the table show the average performance for the two distributions of training data (for instance,  X  X ni-* X  stands for the average performance over Uni-Uni and Uni-Nat configurations). It can be seen that the maximum average performance occurs at K = 125 for the Uni training data and at K = 150 for the Nat training data. Therefore, depending on the sense distribution of training data, we used two different cutting thresholds ( K ) on the number of related words considered for enriching the corresponding LKB. 6.6 Evaluation Measures
It is customary in the WSD literature to evaluate the performance of a disambiguation system based on precision, recall, and F1 measure (Navigli 2009). Precision calculates the portion of items that are correctly disambiguated from among the total output by the system, and recall measures the portion of the total items in the data set that are correctly disambiguated by the system. F1 is the harmonic mean of precision and recall. Because in our setting all the pseudowords to be disambiguated in the test set are covered in the training data and also included as a node in the LKB, IMS and UKB always provide an answer for each item in the test set. For such a full-coverage case, the values of precision, recall, and F1 will be equal. Hence, in our experiments, we report the recall performance of the systems only. In addition, throughout this article, we present the results in terms of recall percentage (i.e., the value of recall multiplied by 100). 7. Experiments and Results
As discussed in the experimental set-up, WSD experiments were carried out with IMS and UKB while injecting an increasingly higher amount of supervision and knowledge, respectively, that is, from 0 to 800 training sentences (cf. Section 6.3). We show the overall 866 recall performance of both systems on natural and uniform test sets in Tables 18 and 19, respectively. 14 Note that in each table the training set can also be either uniformly or naturally distributed, resulting in an overall four training X  X est configurations for each system in the two tables. For each configuration, we show the recall performance values as we vary the size of the corresponding training set from 0 to 800 sentences per pseudoword (whereas the size of the test set, which comprises 200 sentences per pseudoword, is the same across different training sizes, cf. Section 6.3). For 0 training size, we only show the results of UKB, which is merely based on the vanilla WordNet LKB.
 distributed test sets are 70.5% and 25.0%, respectively. The best recall performance (among the two systems) in each training X  X est configuration and for each size of the training data set is shown in bold. 7.1 General Overview of the Results
We observe that IMS has a considerably larger performance variation across different configurations (ranging from 35.7%  X  to 81.9%  X  with 80 training sentences, and from 47.1% ? to 90.3% ? with 800), whereas UKB is less sensitive to training and test distri-butions (52.3%  X 56.5% with 80 training sentences, and 58.6% performance of IMS (Nat-Nat) with 160 training sentences is in line with competitive results on the Senseval-3 lexical sample data set (Mihalcea, Chklovski, and Kilgarriff 2004) in which there exist around 180 training sentences for each noun on average. In fact the latter are in the 73% ballpark against an MFS recall of 55.2% (Zhong and Ng 2010), whereas IMS obtains 84.4% against 70.5% MFS in our setting. The 15% shift in
MFS is due to the sense distribution of our Nat data set, which is more skewed towards frequent senses compared to that of the Senseval-3 lexical sample data set. In addition, the average polysemy degree of our pseudowords is slightly lower than that of the
Senseval-3 nouns (average polysemy 5.1 of pseudowords vs. 5.8 of Senseval-3 nouns), which also contributes to higher recall in our experiments. 7.2 Corroboration of Previous Findings on a Large Scale
Before moving to a detailed analysis and discussion of our results, we briefly report here on the results of the experiments that were conducted in order to confirm some of the previous findings in the literature on a large scale. We provide the details of these experiments in Appendix B. In summary, we were interested in verifying: data (e.g., in Navigli and Lapata 2010), whereas we show for the first time that these hold in large-scale experiments with several orders of magnitudes more annotated data. 7.3 UKB Largely Benefits from Semi-Noisy Edges
Thanks to our framework, we can go into considerably greater detail on the second point that we verified in Section 7.2. As can be seen in Tables 18 and 19, the enrichment of the WordNet LKB proves to be highly beneficial. The performance of UKB increases significantly, even when the edges are harvested from a low number of training sen-tences, which is particularly impressive because the added edges are semi-noisy. In fact, recall grows, with Nat test, from 38.8% to 56.5% (Nat-Nat) and 53.6% (Uni-Nat), and, with Uni test, from 38.8% to 52.3% (Nat-Uni) and 53.9% (Uni-Uni), when using just 80 training sentences per pseudoword. The impact of semi-noisy edges is even higher with more training data, ranging between +19.8% and +25.8% improvement when the full set of 800 training sentences is used. 868 7.4 Performance of the Systems in Different Configurations: IMS Leads
Except in Nat-Uni, IMS outperforms UKB in all other configurations. The gap is particu-larly evident in the Nat-Nat configuration, which is the typical setting for lexical sample
WSD tasks. The performance of UKB is closer to that of IMS for smaller sizes of training data irrespective of the configuration. The gap, however, expands with the growth in the number of sentences in the training set. This shows that the learning rate of IMS is faster than that for UKB.
 formance of IMS shows that providing enough training instances for all senses is always beneficial for IMS, and this happens in the Nat-Nat, Uni-Uni, and Uni-Nat configurations.
 the sense distribution in the test set and (2) Due to the skewed sense distribution in this configuration, often some of the senses of a word are not covered in the training data even though making the disambiguation an easier task in the Nat-Nat configuration, is responsible for the very low performance of IMS on the uniformly distributed test set (i.e., Nat-Uni configuration). UKB, on the other hand, is not as sensitive as its supervised counterpart to the sense distribution, making it robust across different configurations X  with generally lower performance, however. 7.5 UKB is More Robust with Respect to Sense Skewness
To investigate if our WSD systems are biased in favor of more frequent senses, we calculated the recall performance by sense predominance in the Nat test setting (in the Uni test setting we assume no sense predominance). In other words, we separately calculated the recall performance of IMS and UKB on items tagged in the test set with the first (i.e., most frequent) sense of each pseudoword, the second (i.e., second most frequent) sense, and so on. We show in Figure 6 the overall recall by sense predominance, averaged over the 10 training sizes for each of the two possible configurations with the Nat test set. As can be seen UKB tends to be more robust across sense ranking, irrespective of the distribution of the training data. In contrast,
IMS is not equally robust across configurations: Although its recall is relatively stable (Nat-Nat). In fact, this shows that IMS, when trained on naturally distributed data, is biased towards classifying most of the instances as more frequent senses. This lack of robustness is shown in the figure from the rapid performance drop of IMS in the Nat-
Nat configuration (from 97.1% recall on the most predominant sense to about 11.3% on the tenth pseudosense of a pseudoword), indicating that the system tends to perform considerably better on more frequent senses when a natural distribution is assumed. 7.6 Impact of the Knowledge of Sense Distribution
Previous work (Escudero, M ` arquez, and Rigau 2000; Agirre and Mart  X   X nez 2004; Chan and Ng 2005b) has highlighted the impact of the underlying sense distribution for a supervised disambiguation task. However, we do not know much, especially on a large scale, about the effect of integrating sense distribution information into knowledge-based systems. In order to gain more insight into this, we carried out a pilot study to see how much improvement UKB can gain if explicitly provided with domain knowledge in the form of sense distribution, so as to give the same advantage to both knowledge-based and supervised systems. UKB provides, for each disambiguation instance, a probability distribution over the senses of the target word where each probability value can be regarded as the chance of the corresponding sense to be selected in the given context. A possible way to inject the sense distribution knowledge into UKB is to scale the scores assigned to each sense by the corresponding probability values in the sense distribution. According to this procedure, the scaled score P word w (with | S | senses) is obtained by: where d i is the probability of the i th sense according to the corresponding sense dis-tribution and p i is the probability score assigned to the i the given context. In this way we provide UKB with additional information that can increase the selection chance of more frequent senses in situations wherein the system is not confident in its choice of the winning sense (i.e., instances where UKB considers comparable chances for multiple senses to be selected). We used two different ways of calculating sense probability values d i : when injected into the pseudoword-specific and average sense probabilities. We can see from the table that, when using only the WordNet LKB (training size = 0), the provided pseudoword-specific information can boost the performance of UKB by over 26 percentage points (from 38.9 percentage points to 65.6 percentage points). For other 870 sizes of the training data an improvement of 15 percentage points to 21 percentage points is achieved. When provided with this additional sense distribution information,
UKB yields performance comparable to that of IMS (especially for smaller sizes of the training data, e.g., 78.0  X  of UKB vs. 81.9  X  of IMS for training size 80). Note that, being a supervised system, IMS already benefits from this pseudoword-specific sense distribution information. Similarly, when provided with the average sense distribution information, UKB exhibits a considerable improvement ranging from 20 percentage points for WordNet LKB only (training size = 0) to 12 percentage points for the training size of 800 sentences. 7.7 Performance Upperbound of the Systems
A possible way to examine how well a system fits the training data is to carry out train-ing and test on the same data set. This is precisely the setting we explore in this experi-ment. Our aim was to have an estimate of the performance upperbound of each of our two WSD systems. We observed that IMS attains an optimal recall value of 100.0 for all data set sizes and for both sense distributions (i.e., uniform and natural distributions), showing that its models perfectly fit the training data. However, as mentioned earlier in Section 6.5, in our setting the automatic enrichment of the LKB is less immediate and natural than the training of the supervised system. In fact, the annotated data cannot be directly utilized by UKB. Instead, co-occurrence statistics obtained from this data were used to enrich the LKB of UKB. We estimated the performance upperbound of
UKB (and therefore the capability of our LKB enrichment approach) by performing an experiment where additional edges were obtained by exploiting the sentences in the test data set. The enrichment procedure was, however, the same as the one used in our main experiments (see Section 6.5). The experiment was carried out on both our test data sets, namely, naturally and uniformly distributed. In addition, we did not use the values of K (i.e., maximum number of related words per pseudosense used for enriching the LKB) tuned for our main experiments (cf. Section 6.5.1) as we expected the test sentences to be able to provide more beneficial additional edges; instead we used five different values of K , from 200 to 1,000.
 data sets. We present the overall as well as polysemy-specific performance values for five different values of K . As expected, a higher performance was shown by UKB in this setting in comparison with the normal setting where sentences in the training data were exploited for enriching the LKB. An interesting finding here is that even when the test data set is used for obtaining the additional edges, UKB can hardly cross into 90%. In other words, there is a gap of about 10% resulting from the semi-noisy enrichment of
UKB. This shows that our LKB enrichment approach is not optimal. We defer the task of improving the current enrichment technique to future work. In fact, our framework enables other knowledge enrichment approaches to be effectively tested and compared. 7.8 Summary of the Results
Here, we summarize our experiments aimed at analyzing the behavior of state-of-the-art supervised and knowledge-based systems in different settings, and also to verify their dependence on various factors: 872 findings in the literature at a large scale. We briefly reported these results in Section 7.2.
See Appendix B for the details. 8. Conclusion
In this article we proposed a novel framework for the experimental comparison of state-of-the-art supervised and knowledge-based WSD systems on a large scale. At the core of our approach lies the usage of a new type of realistic pseudowords, which makes it possible to model virtually all ambiguous nouns in a lexicon. As a result, we could generate pseudowords modeling each polysemous noun in WordNet, whose high quality we assessed from different perspectives.
 tagged instances from a large corpus, resulting in a 2 million pseudosense-tagged corpus. This corpus was then used for training a state-of-the-art supervised WSD system (i.e., IMS) as well as for automatically injecting large quantities of (semi-noisy) semantic relations into the WordNet graph for use by an off-the-shelf knowledge-based
WSD system (i.e., UKB). Our pseudoword-based framework enabled the analysis of the conditions and factors that impact the performance of these state-of-the-art WSD systems on a large scale, a study which has never heretofore been possible.
 exploitation of large-scale sense-annotated corpora. Furthermore, our new type of pseu-doword might also be used for a realistic, wide-coverage evaluation of other difficult tasks such as Word Sense Induction (Bordag 2006; Di Marco and Navigli 2013; Navigli and Vannella 2013), Entity Linking (Moro, Raganato, and Navigli 2014) and selectional preference acquisition (Chambers and Jurafsky 2010; Erk, Pad  X  o, and Pad  X  o 2010), among others.
 of WordNet 3.0 polysemous nouns, including those selected for our WSD experiments ( http://lcl.uniroma1.it/pseudowords/ ). Together with the pseudosense-annotated corpus, this will allow for future experimental comparisons and studies with other WSD systems, also in other languages. In fact, our pseudowords and our WSD framework are not language-dependent and can readily be applied to other languages with the help of multilingual semantic networks such as BabelNet (Navigli and Ponzetto 2012a) and the use of multilingual WSD algorithms (Moro, Raganato, and Navigli 2014). Finally, along the lines of Cuadros and Rigau (2007), our framework could be used in the future to test and compare various LKB enrichment techniques.
 Appendix A: Reliability of Our Findings enough to provide a reliable estimation of per-polysemy performance, we calculated, for both our systems, confidence intervals of performance values. Table A.1 shows, for the training set consisting of 400 sentences, the 95% confidence interval values for the obtained per-polysemy performance (Table B.1) as well as for the overall performance (Tables 18 and 19). As could have been expected, the confidence interval is smaller for lower polysemy degrees where there exist more pseudowords. We can see from the table that the confidence interval always remains below 2.0 and 3.0, respectively, for IMS and UKB for polysemy degrees up to five for which we set an upperbound of 300 pseudowords. This shows that the subset of 300 pseudowords we picked for those polysemy degrees is large enough to provide an accurate polysemy-specific performance.
 figurations. This shows that the overall recall performance values we reported in
Tables 18 and 19 are quite accurate. These results also hold for other sizes of the training data.
 Appendix B: Corroboration of Previous Findings
In the next two sections, we provide the details of the experiments carried out in order to verify some existing findings in the literature on a large scale.
 874 B.1 Performance by Polysemy
Previous work (Palmer, Dang, and Fellbaum 2007) has shown that both manual and automatic disambiguation can be affected by polysemy. In this section, we verify with our large-scale framework the relation between disambiguation performance and pol-ysemy degree. Table B.1 presents the performance values (averaged over all 10 sizes of training data) as classified by polysemy degree for each system and for all the four configurations. As a general trend, irrespective of the configuration and system, the performance is inversely proportional to polysemy degree. The type of inverse proportionality is approximately logarithmic in all system configurations except for IMS in the Nat-Nat configuration, where it is approximately linear.
 mance difference between the two systems across different configurations. On average, the absolute polysemy-wise difference between the two systems is 14.1 in the Uni-Uni,
Uni-Nat, and Nat-Uni configurations with the minimum difference being 8.7 (Uni-Nat, polysemy 2) and the maximum being 17.4 (Uni-Nat again, polysemy 6). However, in the Nat-Nat configuration the difference between the two systems increases rapidly with polysemy. Starting with a value of 13 at polysemy 2, the difference value rapidly increases with polysemy to a maximum of 35.9 at polysemy 10 (the absolute difference is on average 28.2 in this configuration). This divergence in the polysemy-wise perfor-mance of our two systems in the Nat-Nat configuration shows that IMS, in addition to being particularly good at this configuration, is able to further extend its lead over UKB at higher polysemy degrees.
 B.2 Performance by Pseudosense Node Degree
As discussed in Section 6.4, UKB adopts the PPR algorithm, a variant of eigenvector centrality, whose behavior highly depends on the structure of the graph it is applied to. Previous research (Cuadros and Rigau 2006; Navigli 2008; Navigli and Lapata 2010) has shown that a denser graph with a large number of semantic relations benefits the eigenvector centrality-based approaches, enabling them to provide more accurate dis-ambiguation judgments. These evaluations, however, were carried out on the WordNet graph leveraged for the disambiguation of instances from the SemCor data set. In this section, we perform a similar analysis but on a much larger scale, that is, in a setting with hundreds of thousands of disambiguation instances and using a much denser graph. Essentially, the graphs used in our experiments consist of the same nodes (i.e., synsets) as the WordNet graph but enriched with thousands of additional semantic edges obtained from co-occurrence statistics (cf. Section 6.5).
 the degree centrality which is calculated based on the number of edges incident to a particular node in a graph. We show in Figure B.1 how the nodes are distributed in the graph according to their degree. We present the distributions for the two LKBs enriched with full naturally and uniformly distributed training data (i.e., 800 sentences) as well as for the original WordNet graph. The slightly higher degree of the nodes in the LKB enriched using the naturally distributed data set is due to the availability of a higher number of additional edges per pseudosense in this setting (obtained from 150 related words per pseudosense for the naturally distributed data set vs. 125 for the uniformly distributed data set, cf. Section 6.5.1).
 performance (20 intervals from 0.0 to 1.0). Each point in the graph shows the average degree of the set of nodes (i.e., pseudosenses) on which UKB obtains a recall that falls within the corresponding range. As can be seen in the figure, irrespective of the configuration and training size, the higher the connectivity of a node, the more accurate will be its disambiguation. This is in line with earlier research (Navigli and Lapata 2010), in which it is hypothesized that WSD performance increases when the target sense in the graph tends to have a higher number of incident edges. On the other hand, this 876 trend is almost identical for the two training data sizes, namely, 80 and 800. This shows that the direct proportionality of node degree and disambiguation performance holds for different sizes of training data. Recall that the value of K  X  X he maximum number of top-ranking related words used for LKB enrichment X  X as fixed (cf. Section 6.5.1).
This explains why the average node degree values belonging to the two highly different sizes of the training data (i.e., 80 and 800 sentences) are comparable in Figure B.2. In fact, as the number of training sentences increases, more reliable sets of related words get selected that are likely to provide semantic edges that are more beneficial. However, the value of K , and hence the number of additional edges, remains almost constant across different sizes of the training data.
 Acknowledgments References 878 880
