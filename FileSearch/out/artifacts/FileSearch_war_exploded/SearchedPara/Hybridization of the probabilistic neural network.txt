 1. Introduction
Feed-forward neural networks (FFNNs) are flexible computing frameworks and universal approximators that can be applied to a wide range of forecasting problems with a high degree of accuracy. Several distinguishing features of feed-forward neural networks make them valuable and attractive for a forecasting task. First, feed-forward neural networks are data-driven self-adaptive methods in that there are few a priori assumptions about the models for problems under study. Second, feed-forward neural networks can generalize. Third, feed-forward neural net-works are universal functional approximators that can approx-imate a large class of functions with a high degree of accuracy, and finally, feed-forward neural networks are nonlinear. Given the advantages of feed-forward neural networks, it is not surpris-ing that this methodology has attracted overwhelming attention in many areas of prediction, especially financial markets predic-tion ( Khashei 2005 ).

Improving forecasting especially time series forecasting accu-racy is an important yet often difficult task facing forecasters.
Both theoretical and empirical findings have indicated that integration of different models can be an effective way of improving upon their predictive performance, especially when the models in the ensemble are quite different. The basic idea of the model combination in forecasting is to use the unique feature of each model to capture different patterns in the data. In addition, a single model may not be sufficient to identify all the characteristics of the time series or may not identify the true data generating process. In combined models, the aim is to reduce the risk of using an inappropriate model by combining several models to reduce the risk of failure and obtain results that are more accurate ( Khashei and Bijari, 2011 ).
 hybrid models of artificial neural networks (ANNs) and especially feed-forward neural networks for time series forecasting, since the early work of Reid (1968) and Bates and Granger (1969) .In pioneering work on combined forecasts, Bates and Granger showed that a linear combination of forecasts would give a smaller error variance than any of the individual methods. Since then, the studies on this topic have expanded dramatically. In recent years, more hybrid forecasting models have been proposed using feed-forward neural networks and applied in many areas with good prediction performance. Yu et al. (2005) proposed a novel nonlinear ensemble forecasting model integrating general-ized linear auto regression (GLAR) with back-propagation neural networks (BPNNs) in order to obtain accurate prediction in foreign exchange market. ( Faruk (2010 ) proposed a hybrid approach for water quality time series prediction which consists of an ARIMA methodology and feed-forward, back propagation network struc-ture with an optimized conjugated training algorithm.
Khashei et al. (2009) proposed a new hybrid model using feed-forward neural networks (FFNNs) and fuzzy logic in order to overcome the data and linear limitations of autoregressive inte-grated moving average. Cadenas and Rivera (2010 ) developed a hybrid model consisting of feed-forward neural networks (FFNN) and autoregressive integrated moving average (ARIMA) models for wind speed forecasting. Tseng et al. (2002) proposed a hybrid model called SARIMABP that combines the back-propagation neural network and the seasonal autoregressive integrated mov-ing average (SARIMA) in order to predict seasonal time series data. Amin-Naseri and Soroush (2008 ) presented a hybrid model of feed-forward neural networks for daily electrical peak load forecasting using self-organizing maps (SOMs).

Aladag et al. (2009) proposed a new hybrid approach for time series forecasting that combines the Elman X  X  recurrent neural networks (ERNN) and autoregressive integrated moving average (ARIMA) models. Khashei et al. (2008) proposed a hybrid model called FANN in order to use the advantages and to fulfill the limitations of the fuzzy regression and back-propagation neural network for time series forecasting. Lin and Wu (2009 ), in similar work, proposed a hybrid neural network model to forecast the typhoon rainfall using the self-organizing maps and the feed-forward neural networks. Shafie-khah et al. (2011) proposed a novel hybrid model to forecast day-ahead electricity price, based on the wavelet transform, autoregressive integrated moving average (ARIMA) models and radial basis function neural net-works (RBFN).

Khashei and Bijari (2010 ) proposed a novel hybrid model of feed-forward neural networks, based on the basic concepts of autoregressive integrated moving average models, called
ANN( p,d,q ), in order to overcome the linear limitation of tradi-tional feed-forward neural networks and yield more accurate results. Cheng et al. (2010) developed an evolutionary fuzzy hybrid neural network (EFHNN) to enhance project cash flow management, which incorporates four artificial intelligence approaches, namely the neural network (NN), high order neural network (HONN), fuzzy logic (FL), and genetic algorithm (GA).
Tsaih et al. (1998) proposed a hybrid artificial intelligence model combining the feed-forward neural networks and the rule-based systems technique, by highlighting the advantages and over-coming the limitations of both mentioned techniques in order to accurately predict the direction of daily price changes in S&amp;P 500 stock index futures. Leigh et al. (2002) introduced a method for combining template matching, from pattern recognition, and the feed-forward neural network, from artificial intelligence, to forecast stock market activity. Wu et al. (2010) proposed a new version of support vector machines (SVM), named g-SVM, in order to handle white noises from inputting data by the integration of Gaussian loss function and v-SVM.

In this paper, in contrast of the traditional hybrid models, the classifier methods  X  probabilistic neural networks (PNNs)  X  are applied to construct a new hybrid model of the feed-forward neural networks in order to yield more accurate results. In our proposed model, a classifier analyzes the residuals of the FFNN model in order to distinguish their trend. In the next stage, a mathematical programming model calculates the optimum step length using obtained results from the first stage. Then, the estimated values of the FFNN model are modified according to the optimum step length and the distinguished trend. In the proposed model, the chance to capture different patterns in the data can be efficiently increased by using unique advantages of the probabilistic neural networks in detecting specific patterns and break points, which may not be completely modeled by the feed-forward neural networks. Moreover, by combining FFNN and PNN models, complex nonlinear autocorrelation structures can be modeled more completely; and hence the achieved results will be improved.

The rest of the paper is organized as follows. In the next section, the basic concepts of the feed-forward neural networks (FFNNs) are briefly reviewed. In Section 3 , the probabilistic neural networks (PNNs), which are selected as classifier model, are briefly reviewed. In Section 4 , the formulation of the proposed model is introduced. In Section 5 , the proposed model is applied to three well-known real data sets  X  the Wolf X  X  sunspot data, the Canadian lynx data, and the British pound against the U S dollar exchange rate data  X  forecasting and its performance is compared with those of other forecasting models in order to show the appropriateness and effectiveness of the proposed model. Section 6 contains the concluding remarks. 2. The feed-forward neural networks (FFNNs)
Artificial neural networks (ANNs), based on the connection pattern (architecture), can be generally categorized into two categories: feed-forward neural networks (FFNNs) and recurrent neural networks (RNNs). Feed-forward neural networks (FFNNs) are among the most important and widely used forms of neural networks for time series modeling and forecasting. One significant advantage of the feed-forward neural networks over other classes of nonlinear models is that they are universal approximators that can approximate a large class of functions with a high degree of accuracy. Their power comes from the parallel processing of the information from the data. No prior assumption of the model form is required in the model building process. Instead, the network model is largely determined by the characteristics of the data. The model is characterized by a network of three layers of simple processing units connected by acyclic links (see Fig. 1 ). The relationship between the output ( y t ) and the inputs ( y has the following mathematical representation: y  X  w 0  X  model parameters often called connection weights; P is the number of input nodes; Q is the number of hidden nodes; e the residual of model at time t ;and g is the transfer function. The that is, Sig  X  x  X  X  1 1  X  exp  X  x  X  :  X  2  X 
Hence, the feed-forward neural network model of (1), in fact, performs a nonlinear functional mapping from the past observa-tions to the future value y t , i.e., y  X  f  X  y t 1 , ::: , y t P , W  X  X  e t ,  X  3  X  where W is a vector of all parameters and f(.) is a function determined by the network structure and connection weights.
Thus, a feed-forward neural network is equivalent to a nonlinear autoregressive model. The expression (1) implies one output node in the output layer, which is typically used for one-step-ahead forecasting. The simple network given by (1) is surprisingly powerful in that it is able to approximate arbitrary function as the number of hidden nodes Q is sufficiently large ( Zhang et al., 1998 ). In practice, simple network structure that has a small number of hidden nodes often works well in out-of-sample forecasting. This may be due to the over-fitting effect typically found in feed-forward neural networks modeling process. It occurs when the feed-forward neural network has too many free parameters, which allow the network to fit the training data well, but typically lead to poor generalization. In addition, it has been experimentally shown that the generalization ability begins to deteriorate when the network has been trained more than necessary, that is when it begins to fit the noise of the training data. These are main reasons that researcher believe designing the architecture of the feed-forward neural networks (determin-ing the P , Q , number of epochs, etc.) is a problematic task.
The choice of Q is data dependent and there is no systematic rule in deciding this parameter. In addition to choosing an appropriate number of hidden nodes, another important task of feed-forward neural network modeling of a time series is the selection of the number of lagged observations, P , and the dimension of the input vector. This is perhaps the most important parameter to be estimated in a feed-forward neural network because it plays a major role in determining the (nonlinear) autocorrelation structure of the time series. However, there is no theory that can be used to guide the selection of P . Hence, experiments are often conducted to select an appropriate P , Q , and number of training epochs.

There exist a number of different approaches such as the pruning algorithm, the polynomial time algorithm, the canonical decomposition technique, and the network information criterion for finding the optimal architecture of a feed-forward neural network ( Khashei 2005 ). These approaches can be generally categorized as follows: (i) empirical or statistical methods that are used to study the effect of a feed-forward neural network X  X  parameters and to choose appropriate values for them based on the model performance ( Ma and Khorasani, 2003 ). The most systematic and general of these methods utilizes the principles from Taguchi X  X  design of experiments ( Ross, 1996 ). (ii) Hybrid methods such as fuzzy inference ( Leski and Czogala, 1999 ) where the feed-forward neural network can be interpreted as an adap-tive fuzzy system or in cases where it can operate on fuzzy instead of real numbers. (iii) Constructive and/or pruning algo-rithms that, respectively, add and/or remove neurons from an initial architecture using a previously specified criterion to indicate how feed-forward neural network performance is affected by the changes ( Jiang and Wah, 2003 ). The basic rules are that neurons are added when training is slow or when the mean squared error is larger than a specified value, and that neurons are removed when a change in a neuron X  X  value does not correspond to a change in the network X  X  response or when the weight values that are associated with this neuron remain constant for a large number of training epochs ( Marin et al., 2007 ). (iv) Evolutionary strategies that search over topology space by varying the number of hidden layers and hidden neurons through application of genetic operators ( Lee and Kang, 2007 ) and evaluation of the different architectures according to an objective function ( Benardos and Vosniakos, 2007 ).
 optimal architecture of a feed-forward neural network, these methods are usually quite complex in nature and are difficult to implement ( Zhang et al., 1998 ). Furthermore, none of these methods can guarantee the optimal solution for all real forecast-ing problems. To date, there is no structured and simple clear-cut method for determination of these parameters. Hence, the tedious experiments and trial-and-error procedures are often used. In this procedure, the numerous networks with varying numbers of input and hidden units ( P , Q ) are tested, and generalization error for each is estimated, then the network with the lowest general-ization error is selected ( Hosseini et al., 2006 ). ready for training, a process of parameter estimation. The para-meters are estimated such that the cost function of neural network is minimized. Cost function is an overall accuracy criterion such as the following mean squared error: E  X  1 N where N is the number of error terms. This minimization is done with some efficient nonlinear optimization algorithms other than the basic back propagation training algorithm ( Rumelhart and
McClelland, 1986 ), in which the parameters of the neural net-work, w i , j , are changed by an amount D w i , j , according to the following formula: where the parameter Z is the learning rate and q E / q w i , j partial derivative of the function E with respect to the weight w forward pass, an input vector from the training set is applied to the input units of the network and is propagated through the network, layer by layer, producing the final output. During the backward pass, the output of the network is compared with the desired output and the resulting error is then propagated backward through the network, adjusting the weights accord-ingly. To speed up the learning process, while avoiding the instability of the algorithm, Rumelhart and McClelland (1986 ) introduced a momentum term d in Eq. (5), thus obtaining the following learning rule:
D w i , j  X  t  X  1  X  X  Z @ E learning process from being trapped into poor local minima, and is usually chosen in the interval [0;1]. Finally, the estimated model is evaluated using a separate hold-out sample that is not exposed to the training process. 3. Probabilistic neural networks (PNNs) classifier ( Masters, 1995 ) that is often an excellent pattern classifier in practice. The foundation of the approach is well known decades ago (1960s); however, the method was not of a widespread use because of the lack of sufficient computation power until recently ( Hajmeer and Basheer, 2003 ). Specht (1990 ) first introduced the probabilistic neural networks in 1990, who demonstrated how the Bayes X  X arzen classifier could be broken up into a large number of simple processes implemented in a multilayer neural network each of which could be run indepen-dently in parallel.
 Because the probabilistic neural network is primarily based on
Bayes theorem for conditional probability and Parzen X  X  method for estimating probability density function of random variables.
In order to understand Bayes X  theorem, consider a sample x  X  [ x 1 , x 2 ,..., x n ] taken from a collection of samples belonging to a (prior) probability that a sample belongs to the k th population l , and that the true probability density function of all populations f ( x ), f unknown sample into the i th population ( Tsai, 2006 )if h l f  X  x  X  4 h j l j f j  X  x  X 8 j a i , j  X  1 , 2 , ::: K :  X  7  X 
The density function f k ( x ) corresponds to the concentration of class k examples around the unknown example. As seen from Eq. (7), Bayes X  theorem favors a class that has high density approxi-mately the unknown sample, or if the cost of misclassification or prior probability is high.

The biggest problem with the Bayes X  classification approach lies in the fact that the probability density function f k usually known. In nearly all standard statistical classification algorithms, some knowledge regarding the underlying distribu-tion of the population of all random variables used in classifica-tion should be known or reasonably assumed. Most often, normal (Gaussian) distribution is assumed; however, the assumption of normality cannot always be safely justified ( Hajmeer and Basheer, 2002 ). When the distribution is not known (which is often the case) and the true distribution deviates considerably from the assumed one, the traditional statistical methods normally run into major classification problems resulting in high misclassifica-training set composed of the training example, rather than just assume normal distribution. The resulting distribution will be a multivariate probability density function (PDF) that combines all the explanatory random variables.

In order to derive such distribution estimator from a set of training examples, the Parzen X  X  method ( Parzen, 1962 ) is usually used. The univariate case of PDF was proposed by Parzen (1962 ) and then was extended to the multivariate case by Cacoullos g  X  x , x 2 , ::: , x n  X  X  where s 1 , s 2 ,... s n are the smoothing parameters representing standard deviation (also called window or kernel width) around the mean of n random variables x 1 , x 2 ,..., x n , W is a weighting function to be selected with specific characteristics ( Masters, 1995; Specht, 1990 ), and N is the total number of training examples. If all smoothing parameters are assumed equal (i.e.,  X  s 2  X  ....  X  s n  X  s ) and a bell-shaped Gaussian function is used for W , a reduced form of Eq. (8) is as follows ( Ge et al., 2008 ): g  X  x  X  X  1 where x is the vector of random variables (explanatory variables), and x i is the i th training vector. Eq. (9) represents the average of the multivariate distributions where each distribution is centered at one distinct training example. It is wo rth mentioning that the assump-tion of a Gaussian weighting function does not imply that the overall
PDF will be Gaussian (normal), however, other weighting functions such as the reciprocal function ( w ( r )  X  1/1  X  r 2 ) may be used ( Masters, 1995 ).Asthesamplesize, N , increases, the Parzen X  X  PDF estimator asymptotically approaches the true underlying density function.

Regarding the network X  X  operation based on the aforemen-tioned mathematics, consider the simple network architecture in Fig. 2 with n input nodes in the input layer, two population classes (classes 1 and 2), N 1 training examples belonging to class 1, and N 2 examples in class 2. The pattern layer is designed to contain one neuron for each training case available and the neurons are split into the two classes. The summation layer contains one neuron for each class. The output layer contains one neuron that operates a trivial threshold discrimination; it simply retains the maximum of the two summation neurons ( Chen et al., 2003 ).

The probabilistic neural network executes a training case by first presenting it to all pattern layer neurons. Each neuron in the pattern layer computes a distance measure between the pre-sented input vector and the training example represented by that pattern neuron. The probabilistic neural network then subjects this distance measure to the Parzen window (weighting function, W ) and yields the activation of each neuron in the pattern layer. Subsequently, the activation from each class is fed to the corresponding summation layer neuron, which adds all the results in a particular class together. The activation of each summation neuron is executed by applying the remaining part of the Parzen X  X  estimator equation (e.g., the constant multiplier in Eq. (9)) to obtain the estimated probability density function value of population of a particular class ( Hajmeer and Basheer, 2003 ).
If the misclassification cost and prior probabilities are equal between the two classes, and the classes are mutually exclusive (i.e., no case can be classified into more than one class) and exhaustive (i.e., the training set covers all classes fairly), the activation of the summation neurons will be equal to the poster-ior probability of each class. The results from the two summation neurons are then compared and the largest is fed forward to the output neuron to yield the computed class and the probability that this example will belong to that class.

The most important parameter that needs to be determined to obtain an optimal probabilistic neural network is the smoothing A straightforward procedure involves selecting an arbitrary value of s  X  X , training the network, and testing it on a test (validation) set of examples. This procedure is repeated for other s  X  X  and the set of s  X  X  that produces the least misclassification rate (percen-tage of examples that were misclassified) is chosen. A better and more efficient procedure for searching for the optimal smoothing parameter of random variables and classes is proposed by Masters (1995 ). This procedure prevents any bias in the network to the correctly classified examples, and thus will be followed in this study. Other details on the mathematics as well as advanced variations of probabilistic neural networks are given in Specht (1990 ) and Masters (1995 ). 4. Formulation of the proposed model
Despite the numerous time series models available, the accu-racy of time series forecasting currently is fundamental to many decision processes, and hence, never research into ways of improving the effectiveness of forecasting models been given up. Many researches in time series forecasting have been argued that predictive performance improves in combined models ( Khashei, 2005 ). In the literature, different combination techni-ques have been proposed in order to overcome the deficiencies of single models and yield more accurate hybrid models. In this paper, in contrast of the traditional hybrid techniques, which combine different time series models together, a time series model  X  feed-forward neural network (FFNN)  X  are combined with a classifier  X  probabilistic neural network (PNN)  X  model.
The aim of our proposed model is to use the unique advan-tages of the probabilistic neural networks as classifier models in order to classify and determine the existing trend in the residuals of the feed-forward neural networks. The procedure of the proposed model can be summarized in the five stages (see
Fig. 3 ). In the first stage, the under-study time series { y initially modeled by a feed-forward neural network, as follows. y  X  Fit FFNN  X  t  X  X  e t  X  ^ y t  X  e t  X  10  X  of the feed-forward neural network at time period t , respectively.
In the second stage, according to the obtained results of the first stage  X  the estimated values and residuals of the feed-forward neural network model  X  and desired level of error (DLE), the residuals of feed-forward neural network are classified in three categories as follows. The desired level of error is a non-negative value that determines the sensitivity of the proposed model against the residuals of the feed-forward neural network.
The DLE value is often the ideal level of accuracy for under-study problem, which is chosen by forecaster or decision maker. distinguish the existing trend in the residuals. In this paper, probabilistic neural network (PNN) is used as classifier. Techni-cally, probabilistic neural network is able to deduce the class/ group of a given input vector after the training process is completed. There are a number of appealing features, which justify our adoption of this type of neural networks in this study.
First, training of probabilistic neural networks is rapid, enabling us to develop a frequently updated training scheme. Essentially, the network is re-trained each time the data set is updated and thus the most current information can be reflected in estimation.
Second, the logic of probabilistic neural network is able to extenuate the effects of outliers and questionable data points and thereby reduces extra effort on scrutinizing training data.
Third and the most important, probabilistic neural networks are conceptually built on the Bayesian method of classification which given enough data, is capable of classifying a sample with the maximum probability of success.
 considering the assigned numbers of each category (trend) and subset of effective variables as output and input values, respec-tively. The effective variables on the target value of the mentioned probabilistic neural network at time period t are as follows: (iii) Estimated value of the feed-forward neural network at time (iv) Lags 1 until r th of the estimated values of the feed-forward obtained results of the previous stages  X  the target values obtained from the designed probabilistic neural network ({ 1,0,  X  1}) and estimated values of the feed-forward neural network  X  optimum step length (OSL) is calculated using a mathematical programming model as follows: where tar( t ) is the target value obtained from probabilistic neural network at time period t and n is the training sample size.
In the fifth stage, according to the obtained results of the previous stages  X  the estimated values of feed-forward neural network, target values of probabilistic neural network, and the optimum step length (OSL)  X  the fitted values of the proposed model is calculated as follows:
Fit  X  t  X  X  Fit FFNN  X  t  X  X  X  tar  X  t  X  OSL  X  12  X  model and FFNN model at time period t , respectively. According to Eq. (12) the performance of the proposed model in mean absolute error (MAE) and mean squared error (MSE) is respec-tively calculated as follows:
MAE
MSE where MAE p , MAE MLP , MSE p , and MSE MLP are the mean absolute error and mean squared error of the proposed model and feed-forward neural network model, respectively, and D ( x , y )isa function as follows:
D  X  x , y  X  X   X  1 if x  X  y 1 if x a y 5. Application of the proposed model to time series forecasting
In this section, the proposed model is applied to time series forecasting using the three well-known real data sets in order to demonstrate the appropriateness and effectiveness of the pro-posed model and its performance is compared with those of other forecasting models. 5.1. Data sets In this section, three well-known real data sets including the
Wolf X  X  sunspot data, the Canadian lynx data, and the British pound against the U S dollar exchange rate data are used in order to demonstrate the appropriateness and effectiveness of the proposed model. These time series come from different areas and have different statistical characteristics. They have been widely studied in the statistical as well as the neural network literature ( Khashei and Bijari 2011 ). Both linear and nonlinear models have been applied to these data sets, although more or less nonlinearities have been found in these series. 5.1.1. The Wolf X  X  sunspot data set
The sunspot series is record of the annual activity of spots visible on the face of the sun and the number of groups into which they cluster. The sunspot data, which is considered in this investigation, contains the annual number of sunspots from 1700 to 1987, giving a total of 288 observations. The study of sunspot activity has practical importance to geophysicists, envir-onment scientists, and climatologists. The data series is regarded as nonlinear and non-Gaussian and is often used to evaluate the effectiveness of nonlinear models. The plot of this time series, which is shown in Fig. 4 , also suggests that there is a cyclical pattern with a mean cycle of about 11 years ( Khashei and Bijari, 2011 ). The sunspot data has been extensively studied with a vast variety of linear and nonlinear time series models including autoregressive integrated moving average and artificial neural networks. To assess the forecasting performance of proposed model, the sunspot data set is divided into two samples of training and testing. The training data set, 221 observations (1700 X 1920), is exclusively used in order to formulate the model and then the test sample, the last 67 observations (1921 X 1987), is used in order to evaluate the performance of the established model with two forecast horizons of 35 and 67 periods. 5.1.2. The Canadian lynx series data set
The lynx series, which is considered in this investigation, contains the number of lynx trapped per year in the Mackenzie River district of Northern Canada. The data set are plotted in Fig. 5 , which shows a periodicity of approximately 10 years. The data set has 114 observations, corresponding to the period of 1821 X 1934. It has also been extensively analyzed in the time series literature with a focus on the nonlinear modeling ( Khashei and Bijari, 2011 ), see Wong and Li (2000 ) for a survey. Following other studies, the logarithms (to the base 10) of the data are used in the analysis. As in the previous section, the lynx data set is divided into two samples of training and testing. The training data set, 100 observations (1821 X 1920), is exclusively used in order to formulate the model and then the test sample, the last 14 observations (1921 X 1934), is used in order to evaluate the performance of the established model. 5.1.3. The exchange rate (British pound/US dollar) data set
The last data set that is considered in this investigation, is the exchange rate between British pound and US dollar. Predicting exchange rate is an important yet difficult task in international finance. Various linear and nonlinear theoretical models have been developed but few are more successful in out-of-sample forecasting than a simple random walk model. Recent applica-tions of neural networks in this area have yielded mixed results. The data used in this paper contain the weekly observations from 1980 to 1993, giving 731 data points in the time series. The time series plot is given in Fig. 6 , which shows numerous changing turning points in the series. Following ( Meese and Rogoff, 1983 ) and ( Zhang, 2003 ), we use the natural logarithmic transformed data in the modeling and forecasting analysis. As in the previous sections, the exchange rate data set is divided into two samples.
The training data set, 679 observations (1800 X 1992), is exclu-sively used in order to formulate the model and then the test sample, the last 52 observations (1993), is used in order to evaluate the performance of the established model with three time horizons of 1, 6 and 12 months. 5.2. Results
In this section, the procedure of the proposed model is illustrated by forecasting three aforementioned data sets. In this paper, all FFNN and PNN modeling is implemented via the
MATLAB7 package software. All DLE values in the constructing procedure of the proposed model are considered equal to 5% MAE FFNN . In addition, all OSL values are calculated using the first form of Eq. (11). The MAE (Mean Absolute Error) and MSE (Mean Squared Error) are employed as performance indicators to measure forecasting performance of proposed model. 5.2.1. The Wolf X  X  sunspot data forecasts
Stage 1: In order to obtain the optimum FFNN architecture, different network architectures are evaluated to compare the performance. The best fitted network which is selected, and therefore, the architecture which presented the best forecast-ing accuracy with the test data, is composed of four inputs, four hidden and one output neurons (in abbreviated form, N 4-1) ), which has also been used by other researchers such as
Zhang (2003 ), De Groot and Wurtz (1991 ) and Cottrell et al. (1995) . The architecture of the designed network is shown in Fig. 7 .

Stage 2: Based on the DLE value, which is considered as 5% MAE FFNN  X  0.677218, and residual values of FFNN model, which are obtained from first stage, the target values are calculated.

Stage 3: A probabilistic neural network is designed and trained using target values, which are obtained from second stage, and effective variables. The final architecture of probabilistic
Stage 5: Finally, the fitted values of the proposed model are calculated based on the obtained results of the previous stages using Eq. (12). The estimated values of the hybrid FFNN/PNN model for Wolf X  X  sunspot data set are plotted in Fig. 9 . The estimated values of feed-forward neural network and hybrid proposed model for test data are also plotted in Figs. 10 and 11 , respectively. 5.2.2. The Canadian lynx series data forecasts
Stage 1: Similar to the previous section, different architectures of the feed-forward neural network s are evaluated in order to obtain the optimum architecture. The best fitted network which is selected, and therefore, the architecture which presented the best forecasting accuracy with the test data, is composed of seven inputs, five hidden and one output neurons ( N (7-5-1) )whichhas also been used by other researchers ( Zhang, 2003 ). The architec-ture of the designed network is shown in Fig. 12 .

Stage 2: Based on the DLE value, which is considered as 5% MAE FFNN  X  0.005605, and residual values of FFNN model, which are obtained from first stage, the target values are calculated.

Stage 3: A probabilistic neural network is designed and trained using target values, which are obtained from second stage, and effective variables. The final architecture of probabilistic neural network, designed for the Canadian lynx series data consists of seven input and two output neurons which is shown in Fig. 13 . The list of input and output variables of the designed PNN is summarized in Table 2 .

Stage 4: The optimum step length (OSL) is calculated using a mathematical programming model as Eq. (11), according to 0 50 100 150 200 250 0 50 100 150 200 250 the target and estimated values, which are respectively obtained from the probabilistic neural network and FFNN model. The calculated value of OSL for exchange rate data set is 0.059, which are used in the next stage.

Stage 5: Finally, the fitted values of the proposed model are calculated based on the obtained results of the previous stages using Eq. (12). The estimated values of the hybrid FFNN/PNN model for the Canadian lynx series data are plotted in Fig. 14 .
The estimated values of feed-forward neural network and hybrid proposed model for test data are also plotted in
Figs. 15 and 16 , respectively. 5.2.3. The exchange rate (British pound/US dollar) data forecasts
Stage 1: Similar to the previous sections, in order to obtain the optimum FFNN architecture, different network architectures are evaluated to compare the performance. The best fitted network which is selected, and therefore, the architecture which presented the best forecasting accuracy with the test data, is composed of seven inputs, six hidden and one output neurons (in abbreviated form, N (7-6-1) ), which has also been used by other researchers ( Zhang, 2003 ). The architecture of the designed network is shown in Fig. 17 .
 1.5 2.0 2.5 3.0 3.5 4.0 4.5 0.0 1.0 2.0 3.0 4.0 0.0 1.0 2.0 3.0 4.0 5.3. Comparison with other forecasting models
In this section, the predictive capabilities of the proposed model based feed-forward neural network (FFNN/PNN) is com-pared with the feed-forward neural network and also autoregres-sive integrated moving average, and Zhang X  X  hybrid (FFNN/
ARIMA) ( Zhang 2003 ) models, using three aforementioned well-known real data sets. According to the previous works in the literature, the MAE (Mean Absolute Error) and MSE (Mean
Squared Error) are employed as performance indicators in order to measure forecasting performance of proposed model in com-parison with those other forecasting models. These performance indicators are respectively computed from the following equa-tions: MAE  X  1 N MSE  X  1 N where y i and ^ y i are actual and estimated values, respectively. 5.3.1. The Wolf X  X  sunspot data
In the Wolf X  X  sunspot forecasting case, two forecast horizons of 35 and 67 periods are used in order to assess the forecasting performance of models. The forecasting results of the hybrid FFNN/PNN is given in Table 4 . In addition, the improvement percentage of the proposed model in comparison with those models for the sunspot data is summarized in Table 5 .
The obtained results indicate that our hybrid model not only has significantly yielded more accurate results than FFNN model, but also outperforms than autoregressive integrated moving average (ARIMA) and Zhang X  X  hybrid model across two different time horizons and with both error measures. For example, in terms of MSE, the percentage improvements of the hybrid (FFNN/ PNN) model over than FFNN model for 35 and 67-period forecasts are 37.17% and 22.3%, respectively. 5.3.2. The Canadian lynx data set
In the Canadian lynx series forecasting case, one forecast horizons is used in order to assess the forecasting performance of models. In a similar fashion, the forecasting results of the hybrid FFNN/PNN model for the last 14 years are given in Table 6 . In addition, the improvement percentage of the proposed model in comparison with those models for the sunspot data is sum-marized in Table 7 .

The numerical results show that our hybrid model gives significantly better forecasts than feed-forward neural network and other forecasting models. For example in terms of MAE and MSE, our hybrid (FFNN/PNN) model indicates 28.97% and 27.33% decrease over FFNN model, respectively. 5.3.3. The exchange rate (British pound/US dollar) forecasts
In the exchange rate series forecasting case, three time horizons of 1, 6 and 12 months are used in order to assess the forecasting performance of models. As in the previous sections, the forecasting results of the hybrid FFNN/PNN model are given in Table 8 . In addition, the improvement percentage of the proposed model in comparison with those models for the exchange rate data is summarized in Table 9 .

Results of the exchange rate forecasting, same the previous cases, indicate that our hybrid model significantly outperforms FFNN, ARIMA, and Zhang X  X  hybrid (FFNN/ARIMA) models with both error measures for all time horizons (short-term forecasting (1 month), and long-term horizons (6 and 12 month)). 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.1 0.12 0.14 0.16 0.18 0.2 0.22 6. Conclusions
Improving forecasting especially time series forecasting accu-racy is an important yet often difficult task facing forecasters.
Both theoretical and empirical findings have indicated that integration of different models can be an effective way of improving upon their predictive performance, especially when the models in the ensemble are quite different. In the literature, several hybrid techniques have been proposed by combining different time series models together, in order to yield results that are more accurate. In this paper, a methodology is proposed in order to construct a new class of hybrid models of feed-forward neural networks (FFNNs) using a probabilistic neural networks as classifier. In our proposed method, the estimated values of the feed-forward neural network model are modified using the unique advantages of the PNNs in order to classify the existing trend in the residuals of the FFNN model and optimum step length. Empirical results with three well-known real data sets indicate that the proposed method can be an effective way in order to construct a more accurate hybrid model than feed-forward neural networks. Therefore, it can be used as an appro-priate alternative model for forecasting task, especially when higher forecasting accuracy is needed.
 Acknowledgments The authors wish to express their gratitude to the referees and
Dr. Gholam Ali Raissi Ardali, Assistant Professor of industrial engineering, Isfahan University of Technology, for their insightful and constructive comments, which helped to improve the paper greatly.
 References
