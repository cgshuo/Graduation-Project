 Mining dense subgraphs such as cliques or quasi-cliques is an important graph mining problem and closely related to the notion of graph clustering. In various applications, graphs are enriched by additional information. For example, we can observe graphs representing different types of relations between the vertices. These multiple edge types can also be viewed as different  X  X ayers X  of the same graph, which is denoted as a  X  X ulti-layer graph X  in this work. Additionally, each edge might be annotated by a label characterizing the given relation in more detail. By exploiting all these differ-ent kinds of information, the detection of more interesting clusters in the graph can be supported.

In this work, we introduce the multi-layer coherent sub-graph (MLCS) model, which defines clusters of vertices that are densely connected by edges with similar labels in a subset of the graph layers. We avoid redundancy in the result by selecting only the most interesting, non-redundant clusters for the output. Based on this model, we introduce the best-first search algorithm MiMAG. In thorough experiments we demonstrate the strengths of MiMAG in comparison with re-lated approaches on synthetic as well as real-world datasets. Categories and Subject Descriptors: H.2.8 Database management: Database applications [Data mining] Keywords: dense subgraphs, graph clustering, networks
Mining graph and network data has gained much atten-tion in recent years. One important mining task is graph clustering that aims at grouping the vertices of a graph into so-called clusters such that many edges between vertices of the same cluster exist, i.e. the vertices are densely connected . This task is often also referred to as X  X ense subgraph mining X  including the detection of cliques or quasi-cliques [12].
Besides the mere graph data, real-world data often con-tains additional information, which can be exploited by clus-tering approaches. Several approaches have been proposed considering additional information about the vertices of a graph (e.g. [19, 14, 7, 6]). In this work, however, we con-sider additional information about the edges of a graph.
For example, a graph can contain different types of edges , which represent different types of relations between vertices. Such different types can occur, for example, when we com-bine information from several information networks: e.g., combining a co-author network with a citation network. In the first graph, authors are connected if they have common papers; in the second graph, if a paper of one author cited a paper of the other author. Thus, we will get two edge types:  X  X o-authorship X  and  X  X itation X . Furthermore, each edge might also be associated with a label, e.g. the number of co-authored (or cited) papers. We denote such a graph with multiple edge types as a  X  X ulti-layer graph X . It is de-fined as a set of graphs (called  X  X ayers X ) where each graph is based on the same set of vertices and represents the edges of one certain type. Accordingly, in each layer a different edge set is given. These layers can also be viewed as  X  X i-mensions X  of the graph. (In the following, we use the terms  X  X ayers X  and  X  X imensions X  interchangeably.) An exemplary multi-layer graph is depicted in Fig. 1; several real-world examples are described in the experimental section.
For graphs with edge labels, we can distinguish between two possible interpretations of the labels: First, labels can be regarded as edge weights that denote the strength of the relation between the incident vertices. In this paper, how-ever, we consider a second interpretation: the edge labels represent characteristics of the relations. For example, a co-author network might contain information about the col-laboration between two authors, as the begin or end time of the collaboration, research topics, conferences/journals where the joint papers were published etc.

Overall, in this work, we aim at finding clusters of vertices that are densely connected by edges with similar labels in a subset of the graph layers. These clusters are denoted as (multi-layer) coherent subgraphs .

We want to highlight that the coherent subgraphs need not to appear across all layers, but we detect them in sub-sets of the layers. This is important as some of the edge types might not be relevant for finding interesting coherent subgraphs at all; other types might be relevant only for cer-tain subgraphs. Thus, for each coherent subgraph we find an individual set of relevant layers. This principle is moti-vated by the field of subspace clustering [10] that aims at analyzing subsets of the dimensions in a vector space and that stems from the fact that in higher-dimensional vector data it is unlikely to find objects that are similar w.r.t. all of their characteristics; each cluster is associated with an in-dividual subspace projection. Exploiting these observations in our model, the detected coherent subgraphs are useful in several scenarios: We might find a dense cluster of au-thors who started their collaboration at a similar time and co-authored papers at the same conferences. The authors in another dense cluster might have cited each others X  pa-pers on similar research topics, but not have published joint papers. Similar, in a co-actor graph representing the joint work of actors (cf. Sec. 5), a cluster might be a group of actors who worked together on movies with similar success.
Additionally, we consider the fact that a vertex can natu-rally belong to more than one cluster, e.g. an author might of course participate in several working groups. Thus, we allow our clusters to overlap. However, similar to subspace clustering, allowing overlap can lead to a huge number of valid clusters that mostly represent redundant information [13, 7]. Thus, we propose a clustering model that allows clus-ters to overlap to a certain extend, but avoids redundancy in the resulting set of clusters. This final set of clusters ( X  X lus-tering X ) should contain the most interesting clusters w.r.t. a quality function which can be specified by the user.
Finally, since determining the overall clustering according to our model is NP-hard (as also with most dense subgraph mining models on a single graph), we introduce the algo-rithm MiMAG using a best-first search [16] to find an ap-proximate solution. Best-first search is an established search principle to explore a graph, which in our case is a search tree for enumerating subgraphs, in an informed fashion. Start-ing in an initial node (in our case: the root node of the search tree), best-first search algorithms iteratively expand the  X  X ost promising X  node based on a given heuristic. In MiMAG, the most promising subgraphs are expanded to de-tect the most interesting clusters first. This concept is re-lated to the well-known A* algorithm for finding minimum-cost paths in a graph [9].

The main contributions of this paper are the following: 1. We propose the new paradigm of clustering multi-layer 2. We introduce the clustering model MLCS, which avoids 3. We propose the best-first search algorithm MiMAG to
For mining graph data there exist various mining tasks [1]. Some of the most active areas are graph clustering, graph classification, and frequent subgraph mining. In this work, we concentrate on clustering graph/network data .An overview of the various existing models and techniques is given in [1] and [4]. The term  X  X raph clustering X  is some-what ambiguous as it is (a) used for the clustering of graph databases, where a cluster represents a set of graphs ,and(b) for clustering in one large graph, where the clusters repre-sent sets of vertices from the graph. The latter is often also referred to as  X  X ense subgraph mining X , and is the meaning that is used in this paper. For the definition of dense sub-graphs, several models exist. Two of the most widely-used models are cliques and  X  -quasi-cliques [17].

The concept of subspace clustering was developed for the task of clustering vector data. Traditionally, clustering is done by using all dimensions of the feature space. However, full-space clustering does not scale to high-dimensional data since locally irrelevant dimensions may obfuscate the clus-tering structure [2, 10]. As a solution, subspace clustering methods detect an individual set of relevant dimensions for each cluster [10]. For subspace clustering, several models and algorithms have been proposed. E.g., cell-based sub-space clustering methods obtain high-quality results and are efficiently computable [15].

Some clustering approaches have been proposed that con-sider graphs with labeled vertices (here, the vertex labels are vectors). These approaches can be seen as a combination of graph clustering with clustering approaches for vector data. However, they mostly rely on fullspace-clustering on the ver-tex attributes (e.g. [19]). Recently, the approaches [14, 7, 6] were introduced to deal with the combination of subspace clustering and dense subgraph mining. In these approaches, clusters are ensured to be densely connected and vertices in a cluster are similar in subsets of their attribute values. Although these cluster models are related to the model in-troduced in this paper, adapting these methods to handle edge labels does not lead to the desired results. Two ap-proaches for such an adaption are discussed and evaluated in the experimental section.

Graphs with a single edge type and labels in terms of edge weights can be considered by some graph clustering approaches like minimum cut [1] and spectral clustering [19].
To the best of our knowledge, there are no previous ap-proaches for clustering in multi-layer graphs with edge labels . Also in the survey by Fortunato [4] it is mentioned that such graphs have not been dealt with by any algorithm. Even though the authors of [11] mention the existence of differ-ent types of relations in a network (which they call  X  X evels of relation X ), they just summarize all the different relations between two vertices into a single edge weight.

A concept which is related to our approach is the detec-tion of cross-graph quasi-cliques [17]. Given a database of graphs each having the same vertices, a cross-graph quasi-clique is defined as a set of vertices that forms a quasi-clique in all of the graphs. Only maximal sets having this prop-erty are output. The approaches [20] and [21] also work on a graph database and mine sets of vertices that form a clique [20] or quasi-clique [21] in at least a certain percentage of the graphs in the database (which is called the  X  X upport X  of the (quasi-)clique). Both approaches aim at mining closed (quasi-)cliques, i.e. a (quasi-)clique O is not contained in the output if one of O  X  X  supersets also forms a (quasi-)clique having the same support. In [3], cross-graph cliques are de-tected in a dynamic graph represented as a 3-dimensional boolean cube. To adapt these approaches to our problem setting, the graphs in the graph database could be seen as the different layers of our input graph. However, edge labels are not considered by these approaches. Furthermore, the existing methods do not avoid redundancy in the result set apart from simply excluding subsets of (quasi-)cliques. Thus their output can often contain a large set of highly overlap-ping vertex sets. In our experimental section, we compare our approach to an adaption of the closed quasi-clique min-ing algorithm Cocain [21].
In this section, we introduce the MLCS ( X  X ulti-layer co-herent subgraph X ) model for the clustering of graphs with different types of edges. We start by providing a formal definition of the input graph. For ease of presentation, we represent the input graph as a set of graphs (called  X  X ulti-layer graph X ) each having the same vertex set V ;eachgraph contains the edges of one type with their corresponding la-bels. Formally, the layer graph is defined as:
Definition 1 (Multi-Layer Graph). Amulti-layer graph G for a set of dimensions Dim = { 1 ,...,d } is a set G = { G i | i  X  Dim } of graphs where each graph layer G i ,i  X  Dim is an undirected graph without self-loops and with an edge labeling function l i
We can easily handle graphs with different vertex sets V i by simply considering the union V = V i . The remainder of this section is structured as follows: In Section 3.1, we introduce our definition for a single cluster. We define our redundancy model and selection criteria for the final clus-tering in Section 3.2. In Section 3.3 we introduce the cluster quality function that is used in our experiments.
As discussed in the introduction, an MLCS cluster is a set of vertices which are connected with a high density by edges with similar labels in a subspace of the multi-layer graph (i.e. in a subset of the graph layers).
 Cluster property for a single graph layer. First, we con-sider a single graph layer G i . For the density of a subgraph, we use the established quasi-clique model [17, 21, 12]. The quasi-clique model defines dense subgraphs based on their intra-cluster connectivity. Formally,
Definition 2 (  X  -quasi-clique). A vertex set O  X  V in a graph G =( V,E ) is a  X  -quasi-clique for  X   X  [0 , 1] if where deg O G ( v )= |{ u  X  O | ( u, v )  X  E }| . The density of a quasi-clique O in graph layer G i is defined by
For our cluster model, we consider a vertex set as dense if it is a 0.5-quasi-clique, i.e. its quasi-clique density is at least 0.5. As shown in [21], for  X   X  0 . 5 the vertices in a  X  -quasi-clique are connected  X  X ightly and relatively evenly X . This also ensures that the subgraph is connected in the graph [21]. For the similarity of the edge labels, we use a cell-based cluster model [15]. To be considered similar, the labels of the edges in a cluster may vary at most by a threshold w . Formally, we define a cluster in a graph layer G i as follows: Definition 3 (One-dimensional MLCS cluster).
 A vertex set O  X  V is a one-dimensional MLCS cluster in a graph layer G i =( V,E i ,l i ) (w.r.t. threshold w and distance function dist ) if it forms a 0.5-quasi-clique in the graph G and where the edge set E i ( O ) is defined as E i ( O )= { ( u, v ) E i | u, v  X  O } Figure 1: Exemplary MLCS cluster in layer 1 &amp; 4
Please note that the threshold w is only needed if the edge labels are continuous valued. If we have categorical labels or the layer graphs are unlabeled, we can simply set w =0. Additionally, since two nodes connected by a single edge trivially fulfill the definition, we only consider subgraphs with at least 3 vertices as clusters.
 Cluster property in subspaces of the multi-layer graph. Next, we consider clusters located in subspaces of the layer graph. Naturally, the vertex set of a multi-dimensional clus-ter should fulfill the one-dimensional cluster property for all of the dimensions in the subspace. This idea leads to some important observations: If an edge ( u, v ) exists in one graph layer, it does not automatically exist in another layer. Thus, when we consider the same vertex set in different graph lay-ers, the corresponding edge sets can differ from each other. Thus, in our model, we ensure the density of the subgraph for each layer individually. Formally, Definition 4 (MLCS cluster). An MLCS cluster C =( O,S ) in a multi-layer graph G = { G i | i  X  Dim } con-sists of a vertex set O  X  V and a non-empty set of relevant layers S  X  Dim such that  X  i  X  Dim : i  X  S  X  O is an MLCS cluster in the graph layer G i .

The density of the cluster C =( O,S ) is defined as
Since the edge sets per layer may differ, also the cluster X  X  density in each layer may vary. Thus, we define the density of the cluster as the average density over all layers in the sub-space. Note that in the case of unlabeled layer graphs, our cluster model resembles the definition of cross-graph quasi-cliques [17] or closed quasi-cliques [21].

In Fig. 1, the vertex set O = { b, c, d, e, f } forms an MLCS cluster for w = 1 in the layers 1 and 4. In layer 2, O is not a quasi-clique and in layer 3, the edge labels of E 3 ( O )are not similar, thus O does not form a cluster in these layers.
In the previous section we introduced the properties that a vertex set has to fulfill to form an MLCS cluster. As mo-tivated in the introduction, the clusters are allowed to over-lap. However, just outputting all valid clusters can lead to a large amount of valid clusters that are possibly very sim-ilar to each other and thus contain redundant information. An example (for simplicity without edge labels) is shown in Fig. 2, where the clusters C 2 and C 3 highly overlap in layer 1. Thus, the final clustering result should be a non-redundant set of the  X  X ost interesting X  clusters. This result setiscalledthe MLCS clustering .

As the  X  X nterestingness X  of a cluster can be highly appli-cation dependent, it is defined by a quality function Q ( C ), Figure 2: Overlapping clusters with given qualities Q ( C 1 )= 2 . 5, Q ( C 2 )= 5andQ ( C 3 )= 5 . 3 which can be specified by the user. The default quality function that was used in our experiments is introduced in Section 3.3.

For the avoidance of redundancy, we first introduce a re-dundancy relation. We define a cluster C to be redundant w.r.t. a cluster C if a significant fraction of C  X  X  edges is also covered by C (thus, they represent similar information) and the quality of C is not smaller than that of C . Formally, Definition 5 (Redundancy relation).
 Acluster C =( O,S ) is redundant w.r.t. a cluster C = ( O ,S ) (short: C  X  red C )if C = C  X  Q ( C )  X  Q ( C )  X  1 | for the redundancy parameter r  X  (0 , 1] .

In our experiments, r =0 . 25 proved to be a good choice, thus this is used as the default redundancy parameter. In Fig. 2, the cluster C 2 is redundant w.r.t. the cluster C In contrast, C 1 is not redundant w.r.t. C 2 . Although it X  X  quality is lower, the edge overlap between the clusters is below the threshold. Please note that two clusters with equal quality might be pairwise redundant w.r.t. each other.
Based on this redundancy relation we now select the maxi-mum-quality clustering Result from the set A of all valid clusters. This clustering should not contain clusters that are redundant w.r.t. each other and at the same time it should maximize the sum of the qualities of the selected clusters: Definition 6 (MLCS clustering).
 Given a multi-layer graph G and the set A of all valid MLCS clusters, the maximum-quality clustering Result  X  X  fulfills: In Fig. 2, the clustering solution would be { C 1 ,C 3 } . Complexity results. We briefly describe the main result of our complexity analysis:
Theorem 1. Given a layer graph G over the vertices V , determining the MLCS clustering is NP-hard w.r.t. | V | .
Proof. We prove this theorem by a polynomial reduction of the NP-hard k -clique problem ( X  X s there a clique with at least k vertices X ?) to the determination of an MLCS clustering. Given a graph G =( V,E ) and an integer k ,we can solve the k -clique problem as follows: Take G = { G 1 ( V,E,l 1 ) } with l 1 ( e )=0  X  e  X  E as the input of the MLCS clustering. As quality function for a cluster C =( O,S ) we choose Q ( C )= MLCS clustering has maximum quality sum it contains only the cliques (  X  G 1 ( O ) = 1) of the graph G with at least k vertices. The answer to the k -clique problem is  X  X o X , if the MLCS clustering is empty, and  X  X es X , else. This proof can even be extended to show the #P-hardness of MLCS.
In this section we introduce a default cluster quality func-tion for MLCS clusters. In most settings, clusters containing many vertices are considered more interesting than smaller ones. Therefore, approaches for mining quasi-cliques mostly aim at finding maximal quasi-cliques w.r.t. the number of vertices. However, just maximizing the number of vertices in a cluster can lead to the detection of low-dimensional clusters with low density. Thus, our quality function real-izes a trade-off between the contradicting objective functions size, dimensionality and density. Furthermore, we are not interested in clusters that are too small (here: less than 8 vertices) or that are only one-dimensional. Thus, the quality of a cluster C =( O,S ) in our instantiation is defined as
Clusters that are not considered interesting are assigned a quality of -1 and will thus never be included in an MLCS clustering, as they would lower the overall quality sum of the clustering. As distance function for the edge labels we use the Manhattan distance. In all experiments in Section 5, these instantiations are used. Though, our model and the algorithm can easily be used with other instantiations, which might be more applicable for some applications. In this section we give an overview of the MiMAG ( Mi ning M ulti-layered, A ttributed G raphs) algorithm. Due to The-orem 1, we cannot expect to find an efficient algorithm computing an exact MLCS clustering. Thus, MiMAG com-putes an approximate solution: Instead of determining a redundancy-free clustering with maximum quality, we com-pute a maximal , redundancy-free clustering with high qual-ity . That is, we determine a clustering to which no further cluster C with Q ( C ) &gt; 0 can be added without violating the redundancy-freeness property.

MiMAG is partly based on the Quick algorithm [12] for finding quasi-cliques. In this algorithm, vertex sets O  X  are enumerated by a depth-first traversal in the set enumer-ation tree [18]. 1 Each set visited by the depth-first traversal is tested for the quasi-clique property. An exemplary tree for a graph with three vertices is shown in Fig. 3 (top left). Each node O is associated with a candidate set cand O ,which contains all vertices that are ordered behind the vertices in O in a given order  X  . A child node O extends its parent node O by adding one of the vertices from cand O .Basi-cally, the set enumeration tree contains all possible vertex sets O  X  V . However, the search space can be reduced: If O  X  X  candidate set contains a vertex v that can never be part of a quasi-clique O  X  O ,wecandelete v from the candi-date set. For example, in Fig. 3 (top left) if the vertex v deleted from the candidate set of O , the subtree rooted at
Figure 3: Synchronizing set enumeration trees { v 1 ,v 2 } is pruned from the tree. Techniques how to detect such vertices were introduced in [12].

Synchronized Tree Traversal. A naive approach to de-termine the MLCS clustering would be: (1) use the quick algorithm on each of the graph layers individually to find all one-dimensional MLCS clusters 2 , (2) compose the resulting patterns to multidimensional clusters, and (3) remove re-dundant clusters. This naive, sequential approach, however, is not suitable for the detection of the MLCS clustering: too many (intermediate) patterns are generated which anyway would not be included in the final result due to their redun-dancy. Thus, we interweave all steps.

We first combine the steps (1)+(2) by proposing a  X  X yn-chronized traversal X  of all set enumeration trees simultane-ously , i.e. all instances of the tree perform the same order of traversal. Trees in which a node O was pruned temporar-ily pause their traversal. Another view on this synchronized traversal is that we use an extended set enumeration tree (cf. Fig. 3, bottom). In this tree, each node O has a set of ac-tive dimensions S O (which represent the set of dimensions in which the node O has not been pruned from the set enum. tree) and candidate sets cand O,i for each dimension i  X  S For each set O we visit during the traversal, we check if O forms an MLCS cluster in a subset of its active dimensions. Please do not confuse the active dimensions S O of a node O and the subspace S of the potential cluster C =( O,S ); it holds S  X  S O but the sets are not necessarily equal. We show in Sec. 4.1/4.2 how the set of active dimensions can be used to prune the tree.

Informed Best-First Traversal. We now combine the steps (1)-(3): Instead of first generating all clusters, we let the final (redundancy-free) clustering grow incrementally. Since we want to maximize the quality of the overall clus-tering, we aim at generating the clusters in decreasing order of their quality and adding the non-redundant clusters with highest quality to the result first. In this case, it is crucial to use a good traversal strategy for the extended set enum. tree. 3 Therefore, we propose an informed best-first traver-sal: For each node O , we compute a quality estimation that provides an upper bound for the maximal quality of any clus-Algorithm 1 MiMAG: Best-first search for MLCS clusters 1: Result :=  X  3: while queue =  X  do 4: Obj := queue. pop() 5: if Obj is cluster C =( O,S ) then 11: return Result ter that can be found in the subtree rooted at O . We start the traversal at the root node and in each search step we ex-pand the node O having the highest estimated quality (i.e. MiMAG descends one step into the subtree rooted at O ).
One important aspect has to be considered: Even if a clus-ter C is found at the currently expanded node, it can not be added to the result directly. Since the quality estimation upper bounds the quality of the subtree , C itself might have a lower quality. Thus, there might exist other subtrees (and potential clusters) with higher (estimated) qualities. There-fore, MiMAG maintains a priority queue which contains the set of subtrees that are still to process (similar to the list OPEN in best-first search) as well as the set of already de-tected clusters that could not be added to the result so far. This queue is sorted by the (estimated) quality values of the subtrees and clusters. If the first element of the queue is a cluster, no better clusters exist; in this case (and if the cluster is non-redundant to previously selected clusters), we can finally add it to the result set.

In the queue, a subtree (short: ST ) is represented by a 3-tuple ST =( O,S O , { cand O,i | i  X  S O } )where O is the vertex set in the root node of ST , S O is the set of active di-mensions for O and cand O,i are the candidate sets. Q est denotes the upper bound for the quality of clusters of this subtree. We discuss these upper bounds in Sec. 4.1.
Overall Processing Scheme. The processing of MiMAG is shown in Algorithm 1. Given the input multi-layer graph G , MiMAG computes a redundancy-free, maximal clustering Result . Initially, the set Result is empty (line 1); it will be iteratively filled during the processing. At the beginning, the queue contains one element which represents the root node of the extended set enum. tree (line 2). As long as the queue contains elements, the object with the highest (estimated) quality is taken from the queue. If the object is a cluster, no cluster with higher quality can be found anymore, thus we add it to the result set if it is not redundant w.r.t. an
ST ... ... ... already selected cluster (line 6). If the object is a subtree, we expand the represented set O by one neighboring vertex u that is contained in the candidate sets; we use the vertex having the highest degree w.r.t. O since it most probably leads to dense subgraphs.

The node expansion is illustrated in Fig. 4, where u = v . MiMAG calls the EXPAND procedure for the sub-tree rooted at O . In this procedure, at first the sets O S next and the candidate sets cand O next ,i are determined. S next can only contain dimensions i for which vertex u was contained in the candidate set cand O,i (line 13). The candi-date sets are reduced using pruning techniques (cf. Section 4.2). These sets represent the new subtree ST next rooted at O next , and it is added to the queue if the estimated qual-ity is non-negative (lines 16,17). Similar steps are done for the remaining subtree ST remain rooted at O (lines 20,21), which contains the sets O  X  O with u/  X  O (cf. Fig. 4). Note that we get a new quality estimate, since u is removed from the candidate sets cand O,i . Finally, if O next is a valid (non-redundant) cluster it is also added to the queue.
In the following, we present upper bounds for the quality of subtrees (all proofs are available on our website 4 ). Even by allowing arbitrary quality functions, we can derive some generally applicable bounds. First, we exploit the fact that in some cases the subtree does not contain any interesting cluster at all; the quality can be upper bounded by -1.
The first case uses the active dimensions: If no active di-mensions are left in the node O , we know that there cannot exist any valid cluster in the subtree rooted at O .This result holds since a cluster X  X  subspace is a subset of the ac-tive dimensions and the active dimensions fulfill the anti-monotonicity property 5 : If a dimension i is not active for the set O , then there cannot exist a superset O  X  O such that i is active for O .

In the second case, we exploit our redundancy model to determine the bound: If all clusters C contained in sub-tree ST (i.e. clusters C =( X,S X )with S X  X  S O and O  X  X  X  O  X  a cluster C  X  Result we cannot add them to the final clus-tering. Thus, even if their quality is larger than 0, we can safely estimate the subtree X  X  quality with -1. To check the redundancy w.r.t. a cluster C =( O ,S )  X  Result ,wehave to check the properties from Def. 5. The properties C = C and Q ( C )  X  Q ( C ) are trivially fulfilled for every possible C due to the ordering of the queue; just the edge overlap prop-Therefore, we determine a lower bound ovl min for the edge overlapsuchthat ovl min  X  1 | S all possible clusters C from the subtree . Then, if ovl min we get Q est ( ST )=  X  1. For every subtree ST and every clus-ter C =( O ,S )  X  Result with S  X  S O we get: For example, in the case O  X  O  X  i  X  S ovl min =1andthus Q est ( ST )=  X  1.

Upper bounding cluster properties. Useful properties to incorporate in quality functions are the density and car-dinality of clusters. Thus, we develop upper bounds for these cluster properties that can be used for specific in-stantiations of the quality function. Given a subtree ST = ( O,S O , { cand O,i | i  X  S O } ), for each one-dimensional MLCS cluster X in dimension i  X  S O with O  X  X  X  O  X  cand O,i the following bounds apply:  X   X | E i ( X ) | X | E i ( O ) | +( n max Furthermore, we have for each multi-dimensional MLCS clus-ter ( X, S X ): | S X | X | S O | due to the anti-monotonicity of the active dimensions.

Specific instantiation. We can use the above bounds for our default instantiation of the quality function: the quality of the subtree ST is upper bounded by Q where max x ( y i ) denotes the x -th highest value of all i  X  S O } . Furthermore, if we have max i  X  S | S
O | &lt; 2, the subtree can not contain any cluster with pos-itive quality; in this case, the estimation is Q est ( ST )=
MiMAG exploits the introduced quality bounds twofold: first, to realize the best-first traversal using a priority queue; and second, to prune the search space if the estimate is nega-tive (lines 17, 21). To further enhance the efficiency, MiMAG exploits pruning techniques for the set of active dimensions and the candidate sets (lines 15, 19).
 One example is the pruning by edge similarity :Dueto Def. 3, a one-dim. MLCS cluster ( O,S ) must only contain edges with similar labels. Thus, if the set E i ( O )contains any two edges with label distance greater than w , O (and also all supersets O  X  O ) cannot be a valid cluster. We use this property to prune the candidate sets cand O,i as follows: If for a vertex v  X  cand O,i it holds that E = E i ( O )  X  X  ( v,o ) E i | o  X  O } does not fulfill the similarity property, we can remove v from cand O,i as no set O  X  O  X  X  v } could form an MLCS cluster in dimension i .

Deleting a vertex from a candidate set can change prop-erties (e.g. the degree) of other vertices from the set, thus we prune the sets iteratively until no more vertices can be deleted. If after the pruning we have cand O,i =  X  for a di-mension i , i becomes inactive and can thus be removed from S . More pruning techniques are left out here due to space limitations. Besides the pruning techniques developed espe-cially for the MLCS model, MiMAG also uses the pruning techniques from the Quick algorithm [12].
We evaluate the clustering quality and runtime of MiMAG experimentally on synthetic and real-world datasets. All ex-periments were conducted on Opteron 2.3 GHz CPU X  X  using Java6 64bit. For the synthetic data, the clustering qual-ity is determined by comparing the clustering results to the ground truth using the E4SC measure, which was developed for the evaluation of subspace clustering results [5]. For the real-world datasets, there is no ground truth available, which hinders an evaluation of the clustering quality. Thus, for those datasets we provide some key characteristics of the clustering results as well as exemplary clusters to illustrate the results of MiMAG. If not specified otherwise, the redun-dancy parameter r is set to r =0 . 25 for all experiments.
Baseline approaches. We compare MiMAG with 3 base-line approaches: The closed quasi-clique mining algorithm Cocain [21] (cf. Section 2) is used on our input graph by considering each of the graph layers as a graph in a graph database. To best match our cluster model, the minimum support parameter of Cocain is set to min sup = 1 | Dim | the minimum quasi-clique density to  X  min =0 . 5. Furthermore, we present two different ideas to adapt the GAMer algorithm [7], developed for clustering graphs with vertex labels, to our problem. Both ideas transform the multi-layer graph covering Dim layers to a graph with Dim -dimensional attribute vectors at the vertices. The resulting graph can then be clustered by GAMer. In the first idea (GAMer-avg), the transformed graph is obtained as follows: the vertices of the original graph are kept; the edges are determined by the union of the edge sets from all graph layers (i.e. E = E i ; the edge labels are deleted). The i -th entry of a vertex v  X  X  attribute vector is the average label value of v  X  X  incident edges from layer i . In the second idea (GAMer-lg) we use the well-known concept of the line graph [8]. Each vertex of a line graph represents an edge of the original graph and vice versa. In our case, a line graph vertex v lg =( v 1 ,v 2 ) represents all the edges ( v 1 ,v the different layers. The i -th entry of v lg  X  X  attribute vector corresponds to the label value l i ( v 1 ,v 2 ), if ( v 1  X  ,else(where  X  is considered not similar to any value).
For the evaluation of MiMAG, we generated various syn-thetic multi-layer graphs with edge labels containing over-lapping MLCS clusters as well as X  X oise X  X ertices and X  X oise X  edges that do not belong to any cluster. The generated edge labels lie in the range [0 , 1]. In our experiments, the param-eter w for MiMAG (and also the corresponding parameter forGAMer)issetto w =0 . 1.

Results for varying graph sizes. First, we analyze the behavior of the approaches for varying graph sizes. The gen-erated graphs consist of 10 layers, the number of generated ( X  X idden X ) clusters increases linearly from 10 to 300. Each cluster contains 10 vertices and 3 relevant layers, with quasi-clique densities of 0.6. 10% of the vertices in the graph are noise vertices and in each layer we have 60 noise edges.
Although the runtimes of all approaches (cf. Fig. 5(a)) in-crease with increasing graph sizes, MiMAG constantly shows the lowest runtimes. Considering the clustering quality (cf. Fig. 5(b)), MiMAG reaches perfect or nearly perfect E4SC values on all datasets. The number of detected clusters (Fig. 5(c)) matches the number of hidden clusters. Cocain also achieves quite good quality values (ca. 0.8 to 0.9), as the closed quasi-clique model is closely related to our MLCS model. However, Cocain outputs a huge amount of quasi-cliques (e.g. nearly 2000 instead of the hidden 300 clusters) because it does not avoid redundancy in the result. This also explains the high runtimes of Cocain. For GAMer-avg, the number of detected clusters approximately matches the number of hidden clusters. However, the clustering quality is significantly lower as MiMAG X  X  as the averaged label val-ues distort the cluster structure. For GAMer-lg the number of found clusters varies very much and the clustering quality is low. This is caused by an important problem with the line graph approach: from the density of a subgraph in the line graph, it is not possible to draw conclusions about the den-sity of the corresponding original subgraph, which hinders the detection of dense subgraphs in the original graph.
Results for varying dimensionality. Next, we analyze the behavior of the different approaches for varying dimen-sionalities (i.e. varying numbers of graph layers) of the input graph. The number of graph layers varies between 5 and 50. The generated multi-layer graphs each contain 30 clusters, each having 10 vertices and 3 relevant layers, with quasi-clique densities of 0.6. Again, we have 10% noise vertices and 60 noise edges per layer.

Comparing the runtimes and clustering qualities of the ap-proaches (Fig. 6(a) and Fig. 6(b)), we observe that MiMAG again achieves the lowest runtime and highest quality. For most approaches, the runtime and the clustering quality re-main relatively stable for increasing dimensionality. Just for GAMer-avg the runtime significantly increases, while the E4SC values dramatically drop. This is caused by the graph transformation: As the edges of the transformed graph are the union of the edge sets from all graph layers, by combining an increasing number of graph layers the transformed graph gets very dense, such for high dimensionalities GAMer-avg detects many clusters that do not exist in the original graph.
Results for varying redundancy parameter. In Fig. 6(c), we analyze how the redundancy parameter r of our clustering model affects the results of MiMAG, using a graph with 10 layers and 100 hidden clusters; the cluster size varies between 10 and 15. We observe that for r&lt; 0 . 5, the correct number of clusters is found with a high clustering quality. For r  X  0 . 5, the number of found clusters increases dramati-cally and the clustering quality drops. For high values for r , less clusters are considered redundant w.r.t. other clusters, which leads to a clustering that contains many low-quality clusters that would be considered redundant for lower r val-ues. Thus, we propose using r =0 . 25 as a reasonable default setting for r .
Besides synthetic graphs, we also evaluate our approach on three real-world datasets: The first one is a multi-layer graph with edge labels extracted from the IMDB movie Database 6 . In this graph, the vertices represent actors; the labeled edges represent information about movies in which the actors worked together. The four layers of the graph are: 1.  X  X irst year of collaboration X , 2.  X  X ast year of collab-oration X , 3.  X  X ental fees X  (the average earnings of all joint movies between two actors), 4.  X  X old tickets X  (the average number of sold tickets of all joint movies between two ac-tors). All label values were normalized to the range [0 , 1], and we used w =0 . 03 for this experiment. In this special case, the same edge sets exist in all layers; though, the edge labels differ. Overall, the IMDB graph contains 300 vertices (the most prolific actors) and 18368 edges. An exemplary cluster from MiMAG X  X  clustering result is shown in Fig. 7. Please note that the cluster does not form a clique (its quasi-clique density is 0 . 625), thus not all actors worked together in the same movie. Actually, all actors connected by an edge worked together (among other movies) in the movie  X  X on Air X  or  X  X he Rock X  (or both).

In our next experiment, we evaluate the potential of our approach to handle also multi-layer graphs without edge la-bels. The second dataset was constructed from an extract of the Arxiv publication database 7 . Here, each vertex repre-sents a publication. From the abstracts of the publications, we extracted the 300 most common keywords. Each layer of the graph represents a certain keyword, and an edge of layer i represents a citation between two publications with the common topic i . Overall, the Arxiv graph contains 13396 vertices and 673800 edges. For example, the largest clus-ter found by MiMAG consists of 19 papers from the field of string theory. The 7 relevant layers of this cluster corre-spond to the keywords given in Fig 8:
Our third real-world dataset is a co-author graph extract-ed from the DBLP database 8 . In this graph, the vertices represent authors and the layers represent the 50 conferences in computer science having the most publications. Two au-thors are connected by an edge in layer i if they co-authored at least two papers that were published at the correspond-ing conference. Overall, the DBLP graph contains 17291 vertices and 22896 edges. As we expect co-author groups to be rather small, for this experiment we adapted our quality function to consider clusters with at least 4 vertices as in-teresting. Fig. 7 shows three exemplary clusters detected by MiMAG and their corresponding conferences. Please note that each of the clusters has different relevant layers. While two of the clusters form cliques in both of their layers, in the top right cluster the edge sets of the layers differ.
Clustering results on real-world datasets. In Table 1, we summarize key characteristics of the clustering results of the different approaches on the real-world datasets. Exper-iments that did not finish within 2 days were aborted. For each approach and dataset we provide the runtime as well as the average number of vertices, density and number of layers of the found clusters. Note that for the adaptions of GAMer, the density and subspace are determined on the correspond-ing transformed graphs (whose densities are generally higher than in the original graph); the clusters do not correspond to MLCS clusters in the original multi-layer graphs. Cocain did not finish on any of the datasets within 2 days; GAMer-lg finished only on the DBLP graph, however with a much higher runtime than the other approaches due to the size of the constructed line graph. MiMAG and GAMer-avg have similar runtimes for all datasets. On the IMDB graph, MiMAG detects clusters with a significantly higher average density and dimensionality than GAMer-avg. On Arxiv, the density of GAMer-avg X  X  clusters is slightly higher, which is caused by the fact that GAMer-avg unions the edge sets from all 300 layers and thus obtains a very dense graph. On DBLP, the clusters detected by MiMAG again show the highest average density, while having similar average size and dimensionality to the other approaches.
 Table 1: Key characteristics of the clustering results (* density in the transformed graph)
We proposed the new paradigm of clustering multi-layer graphs with edge labels. Besides the mere graph data, addi-tional information about the edges is considered for finding coherent subgraphs. We introduced the clustering model MLCS, which defines clusters of vertices that are densely connected by edges with similar edge labels in a subset of the graph layers. Redundancy in the result set is avoided by selecting only the most interesting clusters. Based on this model, we introduced the efficient best-first search al-gorithm MiMAG. The performance and clustering quality of MiMAG were demonstrated in our experimental analysis.
Acknowledgments. This work has been supported by the B-IT Research School of the Bonn-Aachen International Center for Information Technology and the UMIC Research Centre, RWTH Aachen University, Germany. [1] C. Aggarwal and H. Wang. Managing and Mining [2] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and [3] L. Cerf, T. B. N. Nguyen, and J.-F. Boulicaut. [4] S. Fortunato. Community detection in graphs. Physics [5] S. G  X  unnemann, I. F  X  arber, E. M  X  uller, I. Assent, and [6] S. G  X  unnemann, B. Boden, and T. Seidl. DB-CSC: A [7] S. G  X  unnemann, I. F  X  arber, B. Boden, and T. Seidl. [8] F. Harary and R. Norman. Some properties of line [9] P. Hart, N. Nilsson, and B. Raphael. A formal basis [10] H.-P. Kriegel, P. Kr  X  oger, and A. Zimek. Clustering [11] M. Li, Y. Fan, J. Chen, L. Gao, Z. Di, and J. Wu. [12] G. Liu and L. Wong. Effective pruning techniques for [13] E. M  X  uller, I. Assent, S. G  X  unnemann, R. Krieger, and [14] F. Moser, R. Colak, A. Rafiey, and M. Ester. Mining [15] E. M  X  uller, S. G  X  unnemann, I. Assent, and T. Seidl. [16] J. Pearl. Heuristics: Intelligent Search Strategies for [17] J. Pei, D. Jiang, and A. Zhang. On mining cross-graph [18] R. Rymon. Search through systematic set [19] M. Shiga, I. Takigawa, and H. Mamitsuka. A spectral [20] J. Wang, Z. Zeng, and L. Zhou. Clan: An algorithm [21] Z. Zeng, J. Wang, L. Zhou, and G. Karypis. Coherent
