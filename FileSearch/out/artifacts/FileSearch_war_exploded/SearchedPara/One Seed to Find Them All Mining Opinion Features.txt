 Feature-based opinion analysis has attracted extensive at-tention recently. Identifying features associated with opin-ions expressed in reviews is essential for fine-grained opin-ion mining. One approach is to exploit the dependency re-lations that occur naturally between features and opinion words, and among features (or opinion words) themselves. In this paper, we propose a generalized approach to opinion feature extraction by incorporating robust statistical associ-ation analysis in a bootstrapping framework. The new ap-proach starts with a small set of feature seeds, on which it iteratively enlarges by mining feature-opinion, feature-feature, and opinion-opinion dependency relations. Two as-sociation model types, namely likelihood ratio tests (LRT) and latent semantic analysis (LSA), are proposed for com-puting the pair-wise associations between terms (features or opinions). We accordingly propose two robust bootstrap-ping approaches, LRTBOOT and LSABOOT, both of which need just a handful of initial feature seeds to bootstrap opin-ion feature extraction. We benchmarked LRTBOOT and LSABOOT against existing approaches on a large number of real-life reviews crawled from the cellphone and hotel do-mains. Experimental results using varying number of feature seeds show that the proposed association-based bootstrap-ping approach significantly outperforms the competitors. In fact, one seed feature is all that is needed for LRTBOOT to significantly outperform the other methods. This seed feature can simply be the domain feature, e.g.,  X  X ellphone X  or  X  X otel X . The consequence of our discovery is far reaching: starting with just one feature seed, typically just the domain concept word, LRTBOOT can automatically extract a large set of high-quality opinion features from the corpus without any supervision or labeled features. This means that the automatic creation of a set of domain features is no longer apipedream! H.3.3 [ Information Search and Retrieval ]: Information Filtering; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text Analysis Algorithms, Experimentation opinion mining, sentiment analysis, aspect, feature, seed, bootstrapping, association
Opinion mining, also known as sentiment analysis, is the computational study of subjectivity, i.e., the sentiments and opinions expressed in online review texts. Opinions ex-pressedinreviewscanbeanalyzedatdifferentresolutions [6, 17, 24, 8, 25]. Document-level opinion mining identifies the overall opinion expressed in a review document, but of-ten fails to associate specific opinions with different aspects of the commented subject (or entity). This problem also happens, though to a lesser extent, in sentence-level opinion mining, as shown in Example 1.1:
Example 1.1.  X  X he exterior is very beautiful, also not ex-pensive, though the battery is not durable, I still unequivo-cally recommend this cellphone! X 
Example 1.1 on a whole expresses a positive opinion on the cellphone, but contains conflicting opinions associated with different aspects of the cellphone. The opinion orienta-tions for the  X  X ellphone X  itself and its  X  X xterior X  are positive, but the opinion polarity for the aspect of  X  X attery X  is neg-ative. Generally such fine-grained opinions may very well tip the balance in purchase decisions. Consumers are usu-ally not satisfied with just the overall opinion rating of a product, but are eager to find out why it receives the rating, that is, which positive or negative features contribute to the overall rating of the product. It is thus important to extract the specific opinionated aspects and associate them with the corresponding opinions.

An opinion feature ,or feature in short, is defined as the object or aspect on which users have expressed their opin-ions. A feature is called an explicit feature , if it is explicitly mentioned in a sentence, typically as a noun or noun phrase. If it is not expressed explicitly but is implied, the feature is called an implicit feature . Example 1.1 contains three ex-plicit opinion features and one implicit feature. The explicit features, i.e.,  X  X xterior X ,  X  X attery X , and  X  X ellphone X  are asso-ciated with opinion words,  X  X eautiful X ,  X  X urable X , and  X  X ec-ommend X , respectively. The implicit feature X  X rice X  X oes not appear explicitly, but is implied in the opinion word  X  X xpen-sive X . In this paper, we focus on identifying explicit opinion features from reviews.

Existing approaches to feature extraction can be classi-fied roughly into supervised and unsupervised learning cat-egories. Supervised learning approaches tend to yield rela-tively accurate results in a given domain, e.g., cellphone re-views, provided that a large set of annotated training data is available. However, the models must be retrained when applied to different domains such as hotel reviews [11, 14, 10]. Moreover, high-quality annotated training review data are hard to come by, which can be tedious and expensive to collect.

In contrast, unsupervised learning approaches have no such limitations, given the ready availability of unlabeled re-view data online. Natural language processing (NLP) meth-ods attempt to recognize features by manually defining ex-traction rules or templates based on syntactic parsing [21, 19]. Though giving good domain independence, NLP meth-ods do not work well with the informal writing styles used in online reviews, which often contain overly concise or gram-matically incorrect sentences. One other problem associated with NLP methods is the limited coverage of the manually compiled syntactic rules.

Unsupervised corpus statistical approaches aim to extract opinion features by capturing their distributional character-istics from the corpus [26, 8, 18]. As a result, they are some-what resistant to the colloquial nature of online reviews, pro-vided that a suitably large corpus is used. However, the lack of ground-truth knowledge, such as annotated feature terms (i.e., seeds), frequently leads to incoherent and imprecise re-sults. The results quickly deteriorate with every additional iteration, as the error accumulates, as in the case of blind leading the blind. In addition, unsupervised topic model-ing approaches tend to discover coarse-grained and generic opinion topics or aspects, which are just clusters of related opinion features instead of the specific opinionated features themselves [7, 1, 22].

It is observed that co-occurrence dependency relations exist naturally between opinion words and features, even among features (opinion words) themselves. For example, an opinion word  X  X xpensive X  is often used to modify the fea-ture X  X rice X  X n cellphone reviews. Such statistical associative relations between pair-wise terms (features or opinions) can be exploited to bootstrap opinion features based on a labeled set of feature seeds.

In this paper, we therefore propose a generalized boot-strapping framework to identify opinion features by employ-ing term-to-term corpus statistics association analysis. The bootstrapping approach starts from a small manually pre-specified list of feature seeds, which it then iteratively en-larges by mining pair-wise feature-opinion, feature-feature, and opinion-opinion associative relations between terms (fea-tures or opinion words) in the corpus. Two association model types are introduced in the bootstrapping framework, namely likelihood ratio tests (LRT) [4] and latent semantic analysis (LSA) [3], which we termed LRTBOOT and LSA-BOOT, respectively.

The proposed association-based bootstrapping framework has two advantages: (1) The statistical association analy-sis is more robust in mining dependency relations between pair-wise terms (features and opinion words), compared to syntactic parsing, and works especially well on real-life on-line text reviews; (2) The bootstrapping strategy achieves surprisingly good performance with only one word in the seed set. This single seed could simply be the domain con-cept term, e.g.,  X  X ellphone X  for cellphone reviews and X  X otel X  for hotel reviews. Experiments conducted on real-life review domains demonstrate the effectiveness of our approach. The remainder of this paper is structured as follows: In Section 2, we discuss related work. In Section 3, we describe the proposed corpus statistics association based bootstrap-ping approach to opinion feature extraction. We evaluate our association-based bootstrapping approach in Section 4 and conclude the paper with a brief discussion in Section 5.
In feature-based opinion mining, identifying a feature as-sociated with an opinion is essential for fine-grained analysis of online reviews. Feature extraction is an ongoing research problem, with the vast majority of existing work done in the product review domain.

By formulating opinion mining as a joint structural tag-ging problem, supervised learning models including Hidden Markov Models and Conditional Random Fields have been used to tag features or aspects of commented entities [11, 14]. Supervised models may be carefully tuned to perform well on a given domain, but need extensive retraining when applied to a different domain, unless transfer learning is adopted [16].

Syntactic relationships between features and opinions can be naturally used to locate opinion features in an unsuper-vised manner. A syntactic parsing based double propagation (DP) approach was proposed to address the extraction of features as well as opinion words [19, 20]. DP recognizes the dependency relations between pair-wise terms (features and opinions) by manually defining eight syntactic rules. It then extracts features and opinion words iteratively using the ex-tracted known features and opinions via the recognized syn-tactic relations. Independently, a similar method was pro-posed to extract opinionated features by defining a set of dependent syntactic rules between features and opinions in [21]. In addition, a semantic role labeling based approach was introduced to recognize opinion features by analyzing their semantic roles and local context in sentences of online news media texts [13].

Topic modeling approaches, such as latent Dirichlet allo-cation (LDA), which is a generative three-way (term-topic-document) probabilistic model [1], have been used to solve aspect-based opinion mining (or sentiment analysis) prob-lems. The approaches actually tend to identify coarse-grained topics or aspects that correspond to distinguishing proper-ties of the commented entities, which may not necessarily be opinionated product features, but rather user-defined clus-ters of specific opinion features [7, 1, 22, 15, 28]. For exam-ple, X  X here X  X ould be a valid LDA topic-word associated with cellphone reviews, since users like to discuss about phone vendors, but it is not a product feature per se. In addition, the approach usually assumes that each review sentence con-tains exactly one topic [12], which is often violated in real-life online reviews.
An unsupervised corpus statistics approach was proposed to extract product features by using association rule min-ing [9]. The approach mines frequent itemsets as potential features, i.e., frequent nouns with high sentence frequencies (i.e., support). Pruning is necessary to remove many irrel-evant features. The limitations of this approach lie in: (1) Frequent but invalid features are extracted incorrectly, and (2) rare but valid features may be missed out.

Our work aims to extract opinion features, which is re-lated to the aforementioned existing work, but differs in three aspects. First, we use corpus statistics term-to-term associations to bootstrap featu re terms, which performs bet-ter than simple syntactic parsing, especially on colloquial or informal review data. Second, in contrast with typical un-supervised statistics models, a manually pre-defined list of feature seeds is used. Third, the seed features are used not to train a model but as references for the iterative extraction process. In a way, our proposed statistical association based bootstrapping approach can be viewed as a semi-supervised approach. In the extreme case of using a single domain con-cept word (e.g.,  X  X ellphone X  for cellphone review domain) as the sole seed feature, our approach even borders on the unsupervised.
When users express their comments on a feature, a cer-tain cluster of opinion words will be frequently used. For example, the feature term  X  X rice X  is often associated with a cluster of opinion words like  X  X xpensive X , X  X heap X , etc. Quite similarly, an opinion word usually covers a certain group of feature terms that are semantically related to each other. Semantic dependency relations thus exist naturally between opinion words and features in real-world reviews. In some instances, co-occurrence associations also exist among fea-tures (opinion words) themselves, since a user could express his or her opinions on several different product features in a single review. e.g.,  X  X creen X  and  X  X attery X .

Such co-occurrence associative patterns between features and opinion words, as well as among features (or opinion words) themselves, can be measured and quantified. We therefore propose to exploit the distributional characteris-tics of opinion features in review corpus, with the goal of identifying them.

We define three types of term-to-term associations, i.e., feature-opinion (FO or OF), feature-feature (FF), and opinion-opinion (OO) to capture the aforementioned co-occurrence dependency relations in our opinion feature extraction task. Moreover, if opinion feature extraction uses only FO asso-ciation, it could miss out a lot of related feature as well as opinion words. For example, given a known opinion word  X  X xpensive X , we can extract a set of feature terms seman-tically related to  X  X rice X  via the FO association analysis. However, we would miss out some valid co-occurring opin-ion words like  X  X igh X  if we do not consider OO associations, which in return limits the feature extraction performance. Likewise, from the extracted known features like  X  X rice X , we may only recognize semantically related opinion terms like  X  X xpensive X  X r X  X heap X  X sing the FO association. In this case, we would miss out valid features like  X  X creen X  that tend to co-occur with the features unless we consider FF associa-tions. Thus in practice we need to consider FF and OO associations in addition to the FO association. This is a fact that is frequently overlooked in features identification analysis, since most existing research tends to focus on only the FO relationship.

We observed that candidate features strongly associated with invalid features (or opinions) tend to be non-features, while candidates strongly associated with valid features (opin-ions) are most likely true opinion features. Paraphrasing the age-old adage,  X  X eatures of a feather flock together X . In other words, without any ground truth in the form of known seed features, pair-wise term association based approaches may lead to too many frivolous features. Therefore, to extract opinion features reliably, we must start out with a manually crafted set of seed features, which is also known as the initial ground truth seed set. We then iteratively enlarge this fea-ture set by adding newly identified candidate features that are statistically and strongly associated with a known mem-ber in the set.

The feature set is thus like an exclusive invitation-only finals club, where a new member (feature) is inducted into the club if and only if he or she is well-known (strongly associated) to any existing me mber (feature in the feature set). The initial club founders (seed features) thus play an essential role; They must  X  X now X  the most important future members (features) of the club, who would bring in yet more members (features).

Given a review corpus, our proposed approach initially extracts a set of candidate features and a set of candidate opinion words. It identifies two new sets of features and opinions that have strong FF and FO associations with the known set of features (a pre-defined list of feature seeds for the first step, and extracted known features thereafter), re-spectively. Based on the extracted known opinion set, the approach then identifies two sets of opinions and features via OO and FO association analysis, respectively. The feature identification process is performed iteratively until a suit-able stopping criterion is met. Upon termination, we can finally bootstrap a validated set of opinion features (as well as an opinion word set).

We will describe our term-to-term association based boot-strapping algorithm in detail below.
Given a review corpus C , we first need to generate two candidate sets of features and opinions, from which valid fea-tures as well as opinions are bootstrapped via corpus statis-tics association.

Opinion features typically occur as nouns (noun phrases), and tend to be the subject or object of a sentence. Simply selecting nouns or nouns phrases as feature candidates gives good coverage (recall), but comes at the expense of letting in too many noisy candidates, which may negatively impact the subsequent feature extraction process.

Using dependency parsing, we attempt to accurately gen-erate a candidate feature set CF ( CF = { cf 1 , ... , cf cf
M } , M : set size), which comprises only nouns (noun phrases) with  X  X BV X  (subject-verb),  X  X OB X  (verb-object), or  X  X OB X  (preposition-object) dependency relations in the corpus C According to experimental results, our candidate feature ex-traction yields respectable recall rates of 83.8% and 75.22% on the cellphone and hotel domains, respectively. Next we use all adjectives and verbs in the review corpus C to form a candidate opinion word set CO ( CO = { co 1 , ... , co j co N } , N :setsize).
 Table 1: Association matrices for bootstrapping opinion feature extraction.

We then compute a pair-wise term association matrix for each of the three aforementioned types of associative re-lations based on the set of candidate features as well as the candidate set of opinion words. As shown in Table 1, M
FO , M FF ,and M OO represent feature-opinion (FO or OF), feature-feature (FF), and opinion-opinion (OO) association matrices, respectively. Let A denote the generalized corpus statistics association model used to compute the pair-wise term association. Accordingly, the A ( cf i ,co j )( cf i co j  X  X O ), A ( cf i 1 ,cf i 2 )( cf i 1 , cf i 2  X  X F ), and A ( co ( co j 1 , co j 2  X  X O ) are FO, FF, and OO association scores es-timated using the association model A on the given review data C .

Taking the feature-opinion association matrix M FO for instance, to calculate the pair-wise term association entry A ( cf i ,co j ) of the matrix, we first need to obtain the corpus occurrence statistics related to candidate feature cf i and candidate opinion co j . We then estimate the association score between the candidates cf i and co j basedontheir corpus statistics by applying the association model A .We compute two other association matrices M FF and M OO in a similar manner.

Our generalized corpus statistics association based boot-strapping approach, ABOOT in short, is summarized in Al-gorithm 1. Some variables in the algorithm are defined as follows: 1. S : a manually annotated feature seed set that is used 2. F : a feature set that keeps track of the extracted fea-3. O : opinion word set that tracks the extracted opinion 4. A ( t 1 ,t 2 ): an association score estimated via an associ-5. foth , ffth ,and ooth : three thresholds for the FO (or
In Algorithm 1, We first extract two candidate sets of features and opinions in line 1 and line 2, respectively. The known feature set F is initialized with the annotated set of feature seeds in line 3. From line 6 to line 19, we bootstrap newfeaturesaswellasopinionwords,whichhavestrong associations with the known or extracted features in the set F . We then bootstrap more opinion words and features based on the extracted known opinion set O from line 20 to line 33. In line 34 of the algorithm, we update both known feature and opinion sets F and S with the extracted features and opinion words. The bootstrapping process is performed repeatedly from line 5, and will be terminated until no new opinion features are identified, as shown in line 35. Finally, a validated set of opinion features can be bootstrapped from the given review corpus C .
 Algorithm 1 Corpus statistics association based bootstrap-ping algorithm.
 Require: Review corpus C , a labeled feature seed set S Ensure: A validated set of opinion features 1: CF  X  Extract a candidate feature set from corpus C ; 2: CO  X  Extract a candidate opinion set from corpus C ; 3: F X  X  ; 4: O X  X  X  ; 5: repeat 6: for each known feature f in F do 7: for each candidate feature cf in CF do 8: if (A ( f,cf )  X  ffth) AND ( cf /  X  X  ) then 9: Identify candidate cf as a feature; 10: Remove candidate cf from set CF ; 11: end if 12: end for 13: for each candidate opinion word co in CO do 14: if (A ( f,co )  X  foth) AND ( co /  X  X  ) then 15: Identify candidate co as an opinion word; 16: Remove candidate co from set CO ; 17: end if 18: end for 19: end for 20: for each known opinion word o in O do 21: for e ach candidate opinion word co in CO do 22: if (A ( o, co )  X  ooth) AND ( co /  X  X  ) then 23: Identify candidate co as an opinion word; 24: Remove candidate co from set CO ; 25: end if 26: end for 27: for each candidate feature cf in CF do 28: if (A ( o, cf )  X  foth) AND ( cf /  X  X  ) then 29: Identify candidate cf as a feature; 30: Remove candidate cf from set CF ; 31: end if 32: end for 33: end for 34: Update F and O with identified features and opinions; 35: until No new opinion features are identified 36: return An identified opinion feature set F
We illustrate Algorithm 1 using a straightforward example in Figure 1. Given a sample review corpus C (which contains 4 reviews only), We first extract two candidate feature and opinion sets CF and CO from the corpus C .Wethencom-pute a table of pair-wise term association scores by employ-ing an associative model A based on the two candidate sets, as shown at the lower portion of the figure. Note that the association matrix (matrices) shown in the shaded grey area of the table corresponds to FO associative relation, and the matrices shown at the top left and bottom right of the table correspond to FF and OO association relations, respectively. Given a pre-specified threshold thd =2.0 (which is used for FO, FF, and OO associations), by applying the ABOOT al-gorithm, we finally bootstrapped an extended feature set F as well as an opinion set O , based on a manually annotated feature seed set S (which contains a noun feature  X  X creen X  only), as shown at the upper right portion of the figure. Figure 1: A working example for the ABOOT algo-rithm.
AsshowninAlgorithm1,theproposedgeneralizedstrat-egy for opinion feature extraction is called ABOOT (Association-based Bootstrapping). Different pair-wise term association measures can lead to different instance approaches.
There are two schools of thoughts on estimating pair-wise dependency relations: one is the tests for statistical signifi-cance, the other is the association measure. On the tests for statistical significance front, we choose the likelihood ratio tests model to estimate the pair-wise association for feature bootstrapping, which we call LRTBOOT (Likelihood Ratio Tests based Bootstrapping). For the association measure, we use latent semantic analysis as well as cosine similarity, which we call LSABOOT (Latent Semantic Analysis based Bootstrapping). Other types of association models can be evaluated as part of future work.
We first describe the likelihood ratio tests (LRT) [4] as-sociation model. The LRT is well known for not relying critically on the assumption of normality, instead, it uses the asymptotic assumption of the generalized likelihood ra-tio. In practice, the use of likelihood ratios tends to result in significant improvements in text-analysis performance, even with relatively small amount of data [4].

LRT computes a contingency table of two term T i and T j , derived from corpus statistics, as given in Table 2, where k ( T i ,T j ) is the number of documents (reviews) containing containing term T i but not T j ; k 3 (  X  T i ,T j )isthenumberof documents containing term T j but not T i ; k 4 (  X  T i , number of documents containing neither T i nor T j .Note that our purpose here is to measure how greatly pair-wise terms are associated with each other given the corpus statis-tics, rather than performing an actual statistics test. Table 2: Contingency table derived from corpus statistics.

Based on the corpus statistics shown in Table 2, the like-lihood ratio tests (LRT) [4] model captures the statistical association between terms T i and T j by employing the fol-lowing function: where, L ( p, k, n )= p k (1  X  p ) n  X  k ; n 1 = k 1 + k 3 ; n 2 p = k 1 /n 1 ; p 2 = k 2 /n 2 ; p =( k 1 + k 2 ) / ( n 1 + n
The higher the quantity  X  2 log X  , the greater the statistical association between term T i and term T j . We abbreviate this LRT based bootstrapping as LRTBOOT.
Generally, given a term-by-document matrix representing a collection of documents, latent semantic analysis (LSA) [3] applies singular value decomposition (SVD) to the matrix to statistically estimate the latent dimensions (or factors) and term-term associations of the collection.

In particular, we first build a term by document matrix X , given a corpus of review documents. By applying SVD, the term-by-document matrix is then decomposed into a prod-uct of three matrices: where L and R are the left and right singular matrices, and V is a diagonal matrix of singular values .

Let r be the rank of the raw matrix X .Weselectavalue k r .Let V k denote the diagonal matrix generated by choosing the top k singular values from the matrix V ,and let L k and R k be matrices generated by selecting the corre-sponding columns from the matrices L and R ,respectively. We thus obtain a reduced matrix X k by multiplying the three new matrices: The matrix X k is the best low rank ( k ) approximation to the raw matrix X , which minimizes the Frobenius norm [5] or reconstruction error in the form: where E = X -X k , e td : element of matrix E , T :termset size, and D : corpus size.

In the new latent space, we measure pair-wise term associ-ations via cosine similarity of the corresponding row vectors of the  X  X moothed X  matrix X k .TheLSAmodelisonetype of the generalized association model A used to compute the FF, FO, and OO term associations in Algorithm 1. We ab-breviate this approach as LSABOOT.
We evaluate our new corpus statistics association based bootstrapping approach using a large number of real-life Chinese reviews on two different domains of cellphone and hotel. Note that the proposed approach is language inde-pendent and can be easily extended to different language based feature extraction applications.
The cellphone review corpus comprises 7800 real-world reviews (or 12,564 sentences) collected from a major Chinese product website 1 . The hotel review corpus contains 4900 reviews (or 18,239 sentences) crawled from a famous Chinese travel portal 2 . All review documents in the two corpora were parsed using a Chinese language analysis tool named Language Technology Platform (LTP) [2].

To create a golden standard for the evaluation of fea-ture extraction, 508 reviews were randomly selected from the cellphone review corpus and two annotators indepen-dently labeled opinion features. An annotated opinion fea-ture is confirmed valid if it is marked by both annotators. If only one annotator marked an opinion feature, a third person made the final judgement. We obtained a total of 995 opinion features for the cellphone reviews. Similarly, we annotated 1013 opinion features from 206 randomly se-lected hotel reviews. The Kappa coefficients, a quantitative measure of inter-annotator agreement, are 0.66 and 0.62 on the cellphone and hotel reviews, respectively. Generally, a Kappa value in the range of 0.6-0.8 denotes substantial agreement [23].
We first evaluate the feature extraction performance of the proposed LRTBOOT and LSABOOT against two state-of-art competitors, DPHITS and DP, and a BASELINE method.

The BASELINE approach simply evaluates precision and recall for the features in the seed feature set S .DP( double propagation ) extracts opinion features by using syntactic re-lations identified via manually defined dependency rules [19, 20]. Though giving good domain-independency, DP can-not effectively parse online real-world reviews, which are mostly informal and frequently contain grammatically incor-rect sentences. As a result the defined syntactic rules have very limited coverage in real-world application. Notwith-standing, it is difficult to come up with a comprehensive set of dependency relations between features and opinions to cover all real-life cases. DPHITS uses hyperlink-induced topic search algorithm (HITS) to validate potential features recognized by DP plus two additional syntactic patterns of  X  X art-whole X  and  X  X o X  [27].

To be fair, the evaluation results for all methods used the same set of 10 seed features and the same review data partition. The choice of 10 seed features seems reasonable, as most people can easily come up with 10 features for a review domain.
In Table 3, we first list the best F-measure performance for the proposed association-based bootstrapping methods, LRTBOOT and LSABOOT, as well as three comparison methods, DPHITS, DP, and BASELINE, using 10 feature seeds on cellphone reviews. LRTBOOT achieved the best F-measure of 73.25%, which is 9.96%, 13.32%, and 22.66% better than that of DPHITS, DP, and BASELINE, respec-tively. LSABOOT had F-measure of 71.07%, which is 7.78%, 11.14%, and 20.48% better than DPHITS, DP, and BASE-LINE. The maximum recall for LSABOOT is 81.11%. product.tech.163.com www.lvping.com/hotels Table 3: Best feature extraction F-measure on cell-phone reviews.

BASELINE has the largest precision of 95.80% but gives the lowest recall of 34.37%. This is expected since it selects only the top 10 seeds as the identified features. Dependency parsing rule based methods like DP tend to perform badly on real-life review corpus that contains large amounts of in-formal language or grammatically incorrect content. By em-ploying the hyperlink-induced topic search (HITS) algorithm to filter potential features extracted by DP, in addition to engaging two new syntactic dependency patterns, DPHITS improves the feature extraction performance compared to DP. However, it still performed worse than LRTBOOT and LSABOOT.

We then plot precision versus recall curves at various pa-rameter settings for LRTBOOT and LSABOOT, as well as the three competing methods, as shown in Figure 2. Note that the precision-recall curve for DPHITS terminated early at recall levels less than 60%. For DP and BASELINE, only one precision-recall point is obtained each, as indicated by the  X  X iamond X  and  X  X ircle X , respectively. Figure 2: Cellphone Feature extraction perfor-mance.

From Figure 2, we see that the LRTBOOT curve lies well above those of DPHITS, DP, and BASELINE. Though starting out at a similar precision as LRTBOOT (as well as BASELINE) at the recall rate of approximately 35%, LSA-BOOT performed worse than LRTBOOT at increasing recall levels. At low recall levels, nearly all the methods perform well, achieving high precision.

Note that BASELINE achieved very high precision of 95.80% at its only precision-recall pair indicated by the  X  X ircle X , which also happens to coincide with that of LRTBOOT and LSABOOT. DP attained a 70.42% precision at its unique recall of about 50%, which is 7.39% worse than LRTBOOT, but 3.25% better than LSABOOT. Across the recall levels from 40% to 60%, the mean precision of DPHITS is 76.72% which is 3.75% lower than LRTBOOT, but 6.16% better than LSABOOT. This is not a serious problem consider-ing that both the proposed LSABOOT and LRTBOOT can achieve much better coverage (higher recall) compared to other methods which stops at recall levels of less than 60%. In practice, a high recall and precision is more desirable.
Though giving good precision, BASELINE leads to bad coverage in the form of the low 35% recall. DP (the point in-dicated by a X  X iamond X ) has relatively better coverage/recall due to its eight manually crafted syntactic rules compared to BASELINE, but still performed worse than LRTBOOT and LSABOOT. This is expected due to the mismatch be-tween DP rules and informal language used in online re-views. DPHITS is only marginally better than DP due to its use of HITS algorithm as well as two additional syntactic patterns. However, its coverage is still limited to at most 58.29% recall, with a corresponding 63.88% precision.
Based on the experiment results, LSABOOT did not achieve respectable precision values, though it outperformed the 3 other competitors in terms of having the highest recall rates of up to 81.11%. Thanks to the high recall rates, LSABOOT was able to achieve the overall second best F-measure of 71.07%, which is 7.78% better than the next competitor, DPHITS. LRTBOOT is the overall winner in terms of ro-bustness and absolute F-measure. It was the de facto preci-sion leader at all levels of recalls, and was able to achieve a recall of about 80%, at which its best F-measure of 73.25% was also achieved.
The best results in F-measure for the two proposed ap-proaches, LRTBOOT and LSABOOT, as well as the 3 bench-mark methods of DPHITS, DP, and BASELINE, on hotel reviews are shown in Table 4.
 Table 4: Best feature extraction F-measure on hotel reviews.
 Even though having the largest precision of 92.67%, BASE-LINE obtained the worst recall of 31.19%. LRTBOOT again obtained the best F-measure of 61.90%, which is 5.08%, 8.80%, and 15.22% better than that of DPHITS, DP, and BASELINE, respectively, LSABOOT achieved its best F-measure of 54.91% (at 69.60% recall level), which is 1.81% and 8.23% better than that of DP and BASELINE respec-tively, but 1.91% worse than DPHITS. However, LSABOOT still achieved the overall best recall of 69.60%.
Figure 3 plots the precision-recall performance curves for all methods, LRTBOOT, LSABOOT, DPHITS, DP, and BASELINE, on hotel reviews. Similarly to the cellphone results, LRTBOOT lies far above that of competitors for all recall levels, while LSABOOT is way inferior to LRT-BOOT. At about 30% recall, the BASELINE precision is 92.67%, which is the same as that of both LRTBOOT and LSABOOT. DP achieved a 55.69% precision at its unique precision-recall point of approximately 50% recall. This is 15.13% lower than LRTBOOT, but 8.51% better than Figure 3: Hotel Feature extraction performance.
 LSABOOT. Spanning the recall levels from 35% to 55%, the mean precision of DPHITS is around 67.87%, which is 10.08% worse than that of LRTBOOT, but 8.52% better than the mean LSABOOT precision.

Different from cellphone reviews, hotel reviews contain a large number of complex and noisy sentences, including a large number of irrelevant personal anecdotes or stories. This basically makes the exploration of latent (high-level) factors or structures of review data by LSA more challeng-ing, and thus leads to relatively poor feature extraction per-formance of LSABOOT.

Based on the experimental results obtained on the cell-phone and hotel review domains, we therefore conclude that the proposed statistical association based bootstrapping ap-proach, especially LRTBOOT, gives significant performance improvement for opinion feature extraction, compared to the dependency parsing rule based methods of DP and DPHITS, as well as a dictionary (seed set) based BASELINE. Given a suitably large review corpus, statistical association analysis based bootstrapping method like LRTBOOT can more ro-bustly discover the co-occurrence dependency relationships between pair-wise terms (features and opinions), and thus better discriminate valid features from the invalid ones, es-pecially on real-life raw reviews that typically contain gram-matically incorrect sentences or informal language.
We then study the effect of seed set size on feature ex-traction for both LRTBOOT and LSABOOT, as well as DPHITS,DP,andBASELINEoncellphoneandhotelre-views, respectively. To collect seed features, we simply rank all feature candidates by descending document frequency for each review domain. We manually selected up to the top 50 truthful features as seeds.
Figure 4 plots the feature extraction performance in F-measure versus top K seeds (50 seeds max) for LRTBOOT, LSABOOT, and the three benchmark methods on cellphone reviews. Clearly, both LRTBOOT and LSABOOT outper-formed the competitors for all seed sizes from 1 to 50. In particular, the mean F-measure of LRTBOOT across all ob-servations is 73.62%, which is 10.35%, 14.07%, and 21.13% better than that of DPHITS, DP, and BASELINE, respec-tively. The mean F-measure of LSABOOT is 71.02%, which is 7.75%, 11.47%, and 18.53% better than that of DPHITS, DP, and BASELINE, respectively. Figure 4: F-measure versus seed size for cellphone reviews.

Furthermore, when increasing seeds from 1 to 50, the F-measure for LRTBOOT remained almost constant, i.e., it does not depend critically on the seed set size. Remark-ably, LRTBOOT achieved excellent performance with only one seed word, i.e., the domain concept term  X  X ellphone X ! A similar situation can also be observed for LSABOOT for cellphone reviews. This is because both LRTBOOT and LSABOOT bootstrap opinion features by relying on corpus statistics association analysis. In other words, given suitable association thresholds, the candidates that have relatively strong associations with the domain concept word (top 1 seed) or extracted known features (opinions) can be iden-tified after several bootstrapping iterations. Therefore, we can always bootstrap a great many features via robust sta-tistical association analysis based on a properly-sized seed set of 20 seeds or less. Moreover, a seed set size of one will also work as well as a seed size of 20!
The performance for DPHITS, DP, and BASELINE in-creases significantly over the lower spectrum of seed sizes, and begin to level-off at around 15 seeds. Clearly, seed set size has a big effect on the performance of DPHITS, DP, and BASELINE for feature extraction. This agrees well with the observation that dependency parsing based methods tend to suffer from the feature extraction coverage problem for their manually defined syntactic rules. On the other hand, their performance curves are in line with expectation, since the number of product features is finite, that is, when users com-ment on products, the feature vocabulary they use will con-verge [8]. Additionally, our feature seeds were hand-picked from the topmost frequent nouns (noun phrases). Thus we see a miniature version of Zipf X  X  and Heap X  X  Law at work here. Therefore, in order to achieve a decent performance for DPHITS, DP, and BASELINE, a relatively large seed set is needed in practice. In contrast, just one domain word suffices for our proposed LRTBOOT or LSABOOT. We evaluate the performance of LRTBOOT and LSA-BOOT versus seed size on the domain of hotel reviews. Fig-ure 5 plots the F-measure performance for LRTBOOT and LSABOOT, as well as DPHITS, DP, and BASELINE. Figure 5: F-measure versus seed size for hotel re-views.

LRTBOOT still gives superior performance, and this time round LRTBOOT enjoys a slight visible improvement from 15 to 50 seed words. Again the LRTBOOT curve lies well above that of DPHITS, DP, and BASELINE for all seed sizes. The average F-measure of LRTBOOT across all ob-servations is 63.15%, which is 6.14%, 9.81%, and 15.28% better than that of DPHITS, DP, and BASELINE, respec-tively. Compared to the BASELINE, LSABOOT showed better performance for all seed sizes, however it still under-performed LRTBOOT. LSABOOT outperformed DPHITS for 5 or less seeds, but did worse when seed size increased from 5 to 50. Similarly, compared to DP, though LSABOOT achieved better performance with less than 20 seeds, its curve lied under that of DP for seed sizes 20 to 50. The aver-age F-measure of LSABOOT across all seed sizes is 55.27%, which is 1.93% and 7.40% better than DP and BASELINE, but 1.74% lower than that of DPHITS.
 Moreover, the performance curves of both LRTBOOT and LSABOOT still exhibit little variation across all seed num-bers on the hotel reviews. The results again validated our observation on the cellphone reviews. We hence conclude that seed set size has no large performance influence on our proposed bootstrapping approach. In fact we can again se-lect just the domain concept word to apply LRTBOOT as well as LSABOOT to real-life reviews and achieve great fea-ture extraction results with no supervision at all.
The choice of thresholds for FO, FF, and OO associations is very important for our proposed method to work in prac-tice. In this section, we give evaluations on the performance (in F-measure) versus the thresholds, in an attempt to shine some light on this issue.

Figure 6 plots the LRTBOOT feature extraction perfor-mance versus FO association threshold, while keeping both FF and OO thresholds at 21.0 and 12.0 on the cellphone re-views, and 21.0 and 43.0 on the hotel reviews, respectively (note that FF and OO association thresholds are obtained experimentally). Similarly, we can also plot the performance versus FF or OO association threshold on the two domains.
In Figure 6, The performance of LRTBOOT initially in-Figure 6: LRTBOOT F-measure vs FO association threshold on cellphone and hotel domains. creased with the FO threshold, achieving the best F-measure of 73.25% and 61.90% at the FO thresholds of 18.0 and 21.0 on the cellphone and hotel reviews, respectively, and declined slightly thereafter. This is expected as a higher FO threshold will weed out the insignificant or noisy asso-ciations, which gives higher-quality features, However, in-creasing it beyond a certain point will kill some legitimate associations as well.

As shown in Figure 7, similar trends are observed in the performance curves of F-measure versus FO thresholds for LSABOOT, with both FF and OO thresholds at 0.54 and 0.60 on the cellphone, and 0.51 and 0.55 on the hotel do-mains. The LSABOOT curve grows gradually with the FO threshold, and subsequently achieved the best F-measure of 71.07% and 54.91% at the thresholds of 0.53 and 0.52 on the cellphone and hotel reviews, respectively. If we raise the threshold further, the curves will be further degraded. Figure 7: LSABOOT feature extraction F-measure vs. FO association threshold on cellphone and hotel reviews.

Association thresholds for our proposed statistical associ-ation based bootstrapping approach must be set. We cur-rently use a standard grid-search procedure to find suitable threshold values that produce good opinion feature extrac-tion performance. For instance, the FO threshold scores for both LRTBOOT and LSABOOT, in our case, can be selected near 20.0 and 0.5, respectively. In practice, suit-able thresholds can be determined experimentally via cross-validation on a labeled dataset, which renders our approach a semi-supervised one.
In this paper, we propose a generalized corpus statistics association based bootstrapping approach for opinion fea-ture extraction. Starting from a small manually labeled set of feature seeds, LRTBOOT as well as LSABOOT can bootstrap a large number of valid features by mining pair-wise term associations via an effective statistical association model. Experimental results using varying numbers of seeds on the cellphone and hotel review domains demonstrate im-provements of the proposed association-based approaches of LRTBOOT and LSABOOT over two state-of-art methods and one baseline method. In fact, LRTBOOT, as well as LSABOOT, achieved quite good performance with only one seed word, which is simply the domain concept word. This makes our proposed statistical association based bootstrap-ping approach, especially LRTBOOT, powerful and effective for practical opinion feature extraction in opinion mining and sentiment analysis.

Several weaknesses still exist: (1) As a corpus statistics approach, the proposed approach is less successful in dealing with infrequent feature extrac-tion. For instance, the infrequent feature  X  X M X  (a slang for radio in cellphone reviews) is missed out. Basically, it is not a serious problem, since the number of infrequent features, compared to the number of frequent features, is minor. Moreover, frequent features are basically more im-portant than infrequent ones [8]. (2) Our new approach does not currently extract non-noun opinion features, e.g., design, decorate. (3) Our approach is at the mercy of errors in the POS-tagging and syntactic parsing for candidate feature extrac-tion.

For future work, we will employ more advanced techniques like fine-grained topical modeling to jointly identify opinion features, including non-noun features, infrequent features, and implicit features as well. We also plan to further test the extracted opinion features in a real opinion mining system by giving every product a feature-specific sentiment summary. This research was supported in part by Singapore Ministry of Education X  X  Academic Research Fund Tier 2 grant ARC 9/12 (MOE2011-T2-2-056). [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] W. Che, Z. Li, and T. Liu. Ltp: a chinese language [3] S. Deerwester, S. T. Dumais, and R. Harshman. [4] T. Dunning. Accurate methods for the statistics of [5] G. Golub and C. Van Loan. Matrix computations. The [6] V. Hatzivassiloglou and J. M. Wiebe. Effects of [7] T. Hofmann. Unsupervised learning by probabilistic [8] M. Hu and B. Liu. Mining and summarizing customer [9] M. Hu and B. Liu. Mining opinion features in [10] N. Jakob and I. Gurevych. Extracting opinion targets [11] W. Jin and H. H. Ho. A novel lexicalized hmm-based [12] Y. Jo and A. H. Oh. Aspect and sentiment unification [13] S.-M. Kim and E. Hovy. Extracting opinions, opinion [14] F. Li, C. Han, M. Huang, X. Zhu, Y.-J. Xia, S. Zhang, [15] C. Lin and Y. He. Joint sentiment/topic model for [16] S. J. Pan and Q. Yang. A survey on transfer learning. [17] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [18] A. Popescu and O. Etzioni. Extracting product [19] G. Qiu, B. Liu, J. Bu, and C. Chen. Expanding [20] G. Qiu, B. Liu, J. Bu, and C. Chen. Opinion word [21] G. Qiu, C. Wang, J. Bu, K. Liu, and C. Chen. [22] I. Titov and R. McDonald. Modeling online reviews [23] A. J. Viera and J. M. Garrett. Understanding [24] J. Wiebe, T. Wilson, R. Bruce, M. Bell, and [25] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing [26] J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. [27] L. Zhang, B. Liu, S. H. Lim, and E. O X  X rien-Strain. [28] W. X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly
