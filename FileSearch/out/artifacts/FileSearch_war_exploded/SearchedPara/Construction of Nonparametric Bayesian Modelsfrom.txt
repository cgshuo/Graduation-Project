 Nonparametric Bayesian models are now widely used in machine learning. Common models, in particular the Gaussian process (GP) and the Dirichlet process (DP), were originally imported from statistics, but the nonparametric Bayesian idea has since been adapted to the needs of machine learning. As a result, the scope of Bayesian nonparametrics has expanded significantly: Whereas traditional nonparametric Bayesian statistics mostly focuses on models on probability distributions, machine learning researchers are interested in a variety of infinite-dimensional objects, such as func-tions, kernels, or infinite graphs. Initially, existing DP and GP approaches were modified and com-bined to derive new models, including the Infinite Hidden Markov Model [2] or the Hierarchical Dirichlet Process [15]. More recently, novel stochastic process models have been defined from scratch, such as the Indian Buffet Process (IBP) [8] and the Mondrian Process [13]. This paper studies the construction of new nonparametric Bayesian models from finite-dimensional distribu-tions: To construct a model on a given type of infinite-dimensional object (for example, an infinite graph), we start out from available probability models on the finite-dimensional counterparts (prob-ability models on finite graphs), and translate them into a model on infinite-dimensional objects using methods of stochastic process theory. We then ask whether interesting statistical properties of the finite-dimensional models used in the constructions, such as conjugacy of priors and posteriors, carry over to the stochastic process model. In general, the term nonparametric Bayesian model refers to a Bayesian model on an infinite-dimensional parameter space. Unlike parametric models, for which the number of parameters is constantly bounded w.r.t. sample size, nonparametric models allow the number of parameters to grow with the number of observations. To accommodate a variable and asymptotically unbounded number of parameters within a single parameter space, the dimension of the space has to be infinite, and nonparametric models can be defined as statistical models with infinite-dimensional parameter spaces [17]. For a given sample of finite size, the model will typically select a finite subset of the available parameters to explain the observations. A Bayesian nonparametric model places a prior distribution on the infinite-dimensional parameter space.
 Many nonparametric Bayesian models are defined in terms of their finite-dimensional marginals: For example, the Gaussian process and Dirichlet process are characterized by the fact that their finite-dimensional marginals are, respectively, Gaussian and Dirichlet distributions [11, 5]. The probability-theoretic construction result underlying such definitions is the Kolmogorov extension theorem [1], described in Sec. 2 below. In stochastic process theory, the theorem is used to study the properties of a process in terms of its marginals, and hence by studying the properties of finite-dimensional distributions. Can the statistical properties of a nonparametric Bayesian model, i.e. of a parameterized family of distributions, be treated in a similar manner, by considering the model X  X  marginals? For example, can a nonparametric Bayesian model be guaranteed to be conjugate if the marginals used in its construction are conjugate? Techniques such as the Kolmogorov theo-rem construct individual distributions, whereas statistical properties are properties of parameterized families of distributions. In Bayesian estimation, such families take the form of conditional prob-abilities. The treatment of the statistical properties of nonparametric Bayesian models in terms of finite-dimensional Bayes equations therefore requires an extension result similar to the Kolmogorov theorem that is applicable to conditional distributions. The main contribution of this paper is to provide such a result.
 We present an analogue of the Kolmogorov theorem for conditional probabilities, which permits the direct construction of conditional stochastic process models on countable-dimensional spaces from finite-dimensional conditional probabilities. Application to conjugate models shows how a conju-gate nonparametric Bayesian model can be constructed from conjugate finite-dimensional Bayes equations  X  including the mapping to the posterior parameters. The converse is also true: To con-struct a conjugate nonparametric Bayesian model, the finite-dimensional models used in the con-struction all have to be conjugate. The construction of stochastic process models from exponential family marginals is almost generic: The model is completely described by the mapping to the poste-rior parameters, which has a generic form as a function of the infinite-dimensional counterpart of the model X  X  sufficient statistic. We discuss how existing models fit into the framework, and derive the nonparametric Bayesian version of a model on infinite permutations suggested by [9]. By essentially providing a construction recipe for conjugate models of countable dimension, our theoretical results have clear practical implications for the derivation of novel nonparametric Bayesian models. Infinite-dimensional probability models cannot generally be described with densities and therefore require some basic notions of measure-theoretic probability. In this paper, required concepts will be measures on product spaces and abstract conditional probabilities (see e.g. [3] or [1] for general introductions). Randomness is described by means of an abstract probability space ( X  , A , P ) . Here,  X  is a space of points  X  , which represent atomic random events, A is a  X  -algebra of events on  X  , and P a probability measure defined on the  X  -algebra. A random variable is a measurable mapping from  X  into some space of observed values, such as X :  X   X   X  x . The distribution of X is the image measure P X := X ( P ) = P  X  X  X  1 . Roughly speaking, the events  X   X   X  represent abstract states of nature, i.e. knowing the value of  X  completely describes all probabilistic aspects of the model universe, and all random aspects are described by the probability measure P . However,  X  , A and P are never known explicitly, but rather constitute the modeling assumption that any explicitly known distribution P X is derived from one and the same probability measure P through some random variable X .
 Multiple dimensions of random variables are formalized by product spaces. We will generally deal fold product of  X  x with itself. The set of finite subsets of E will be denoted F ( E ) , such that x with I  X  F ( E ) is a finite-dimensional subspace of  X  E x . Each product space  X  I x is equipped with the product Borel  X  -algebra B I x . Random variables with values on these spaces have product
X := X I ( P ) is a product measure; the individual components of X I may be dependent. The elements of the infinite-dimensional product space  X  E x can be thought of as functions of the form Product spaces  X  I x  X   X  J x of different dimensions are linked by a projection operator  X  JI , which of a measure is just its marginal , that is, [  X  JI P J X ] is the marginal of the measure P J X on the lower-corresponding measures and spaces are indexed accordingly, as P X , P  X  ,  X   X  etc. The likelihoods and posteriors that occur in Bayesian estimation are conditional probability distributions. Since densities are not generally applicable in infinite-dimensional spaces, the formulation of Bayesian models on such spaces draws on the abstract conditional probabilities of measure-theoretic probability, which are derived from Kolmogorov X  X  implicit formulation of conditional expectations [3]. We will write e.g. P X ( X |  X ) for the conditional probability of X given  X  . For the reader familiar with the theory, we note that all spaces considered here are Borel spaces, such that regular versions of conditionals always exist, and we hence assume all conditionals to be regular conditional probabilities (Markov kernels). Introducing abstract conditional probabilities here is far beyond the possible scope of this paper. A reader not familiar with the theory should simply read P X ( X |  X ) as a conditional distribu-tion, but take into account that these abstract objects are only uniquely defined almost everywhere. That is, the probability P X ( X |  X  =  X  ) can be changed arbitrarily for those values of  X  within some set of exceptions, provided that this set has measure zero. While not essential for understanding most of our results, this fact is the principal reason that limits the results to countable dimensions. function. Then X E is function-valued, and if for example E := R + and  X  x := R , the product space space is a point on the real line, and a finite index set I  X  F ( E ) is a finite collection of points I = ( i of function values at the points in I . The parameter variable  X  E represents the mean function of the probability measures over a given domain, such as R . A probability measure on R (with its Borel algebra B ( R ) ) is in particular a set function B ( R )  X  [0 , 1] , so we could choose E = B ( R ) and  X  x = [0 , 1] . The parameters of a Dirichlet process DP (  X ,G 0 ) are a scalar concentration parameter  X   X  R + , and a probability measure G 0 with the same domain as the randomly drawn measure x E . The parameter space would therefore be chosen as R +  X  [0 , 1] B ( R ) . 2.1 Construction of Stochastic Processes from their Marginals Suppose that a family P I X of probability measures are the finite-dimensional marginals of an infinite-subspace  X  I x of  X  E x . As marginals of one and the same measure, the measures must be marginals of each other as well: Any family of probability measures satisfying (1) is called a projective family . The marginals of a stochastic process measure are always projective. A famous theorem by Kolmogorov states that the converse is also true: Any projective family on the finite-dimensional subspaces of an infinite-assumption required is that the  X  X xes X   X  x of the product space are so-called Polish spaces , i.e. topological spaces that are complete, separable and metrizable. Examples include Euclidean spaces, separable Banach or Hilbert spaces, countable discrete spaces, and countable products of spaces that are themselves Polish.
 Theorem 1 (Kolmogorov Extension Theorem) . Let E be an arbitrary infinite set. Let  X  x be a Polish X as its marginals.
 The infinite-dimensional measure P E X constructed in Theorem 1 is called the projective limit of the family P I X . Intuitively, the theorem is a regularity result: The marginals determine the values of
X on a subset of events (namely on those events involving only a finite subset of the random variables, which are just the cylinder sets with finite-dimensional base). The theorem then states that a probability measure is such a regular object that knowledge of these values determines the measure completely, in a similar manner as continuous functions on the line are completely determined by their values on a countable dense subset. The statement of the Kolmogorov theorem is deceptive in its generality: It holds for any index set E , but if E is not countable, the constructed measure P E X is essentially useless  X  even though the theorem still holds, and the measure is still uniquely defined.  X  -algebra B E x (the product  X  -algebra on  X  E x ). If E is uncountable, this  X  -algebra is too coarse to resolve events of interest 1 . In particular, it does not contain the singletons (one-point sets), such that According to the Kolmogorov extension theorem, the properties of a stochastic process can be an-alyzed by studying its marginals. Can we, analogously, use a set of finite-dimensional Bayes equa-tions to represent a nonparametric Bayesian model? The components of a Bayesian model are condi-tional distributions. Even though these conditionals are probability measures for (almost) each value of the condition variable, the Kolmogorov theorem cannot simply be applied to extend conditional models: Conditional probabilities are functions of two arguments, and have to satisfy a measurabil-ity requirement in the second argument (the condition). Application of the extension theorem to each value of the condition need not yield a proper conditional distribution on the infinite-dimensional space, as it disregards the properties of the second argument. But since the second argument takes the role of a parameter in statistical estimation, these properties determine the statistical properties of the model, such as sufficiency, identifiability, or conjugacy. In order to analyze the properties of an infinite-dimensional Bayesian model in terms of finite-dimensional marginals, we need a theorem that establishes a correspondence between the finite-dimensional and infinite-dimensional condi-tional distributions. Though a number of extension theorems based on conditional distributions is available in the literature, these results focus on the construction of sequential stochastic processes from a sequence of conditionals (see [10] for an overview). Theorem 2 below provides a result that, like the Kolmogorov theorem, is applicable on product spaces.
 To formulate the result, the projector used to define the marginals has to be generalized from mea-probability on the product space  X  J , and I  X  J , define This definition is consistent with that of the projector above, in the sense that it coincides with the with projective families of measures, we then define projective families of conditional probabilities. Definition 1 (Conditionally Projective Probability Models) . Let P I X ( X I |  X  I ) be a family of regu-lar conditional probabilities on product spaces  X  I x , for all I  X  F ( E ) . The family will be called conditionally projective if [  X  JI P J X ]( . |  X  J ) = a.e. P I X ( . |  X  I ) whenever I  X  J . As conditional probabilities are unique almost everywhere, the equality is only required to hold al-most everywhere as well. In the jargon of abstract conditional probabilities, the definition requires ability on a countably-dimensional product space is uniquely defined (up to a.e.-equivalence) by a conditionally projective family of marginals. In particular, if we can define a parametric model on each finite-dimensional space  X  I x for I  X  X  ( E ) such that these models are conditionally projective, the models determine an infinite-dimensional parametric model (a  X  X onparametric X  model) on the Theorem 2 (Extension of Conditional Probabilities) . Let E be a countable index set. Let P I X ( X I |  X  I ) with respect to the  X  -algebra C E :=  X  (  X  I  X  X  ( E )  X  ( X  I )) . In particular, if the parameter variables Proof Sketch 2 . We first apply the Kolmogorov theorem separately for each setting of the parameters space), projectiveness holds if  X  I =  X  I (  X  ) for all I  X  F ( E ) . However, for any conditionally projective family, there is a set N  X   X  of possible exceptions (for which projectiveness need not hold), due to the fact that conditional probabilities and conditional projections are only unique almost everywhere. Using the countability of the dimension set E , we can argue that N is always a null set; the resulting set of constructed infinite-dimensional measures is still a valid candidate for a regular conditional probability. We then show that if this set of measures is assembled into a function of the parameter, it satisfies the measurability conditions of a regular conditional probability: We first use the properties of the marginals to show measurability on the subset of events which are preimages under projection of finite-dimensional events (the cylinder sets), and then use the  X  - X  theorem [3] to extend measurability to all events. The posterior of a Dirichlet process is again a Dirichlet process, and the posterior parameters can be computed as a function of the data and the prior parameters. This property is known as conjugacy , in analogy to conjugacy in parametric Bayesian models, and makes Dirichlet process inference tractable. Virtually all known nonparametric Bayesian models, including Gaussian processes, P  X  olya trees, and neutral-to-the-right processes are conjugate [16]. In the Bayesian and exponential family literature, conjugacy is often defined as  X  X losure under sampling X , i.e. for a given likelihood and a given class of priors, the posterior is again an element of the prior class [12]. This definition does not imply tractability of the posterior: In particular, the set of all probability measures (used as priors) is conjugate for any possible likelihood, but obviously this does not facilitate computation of the posterior. In the following, we call a prior and a likelihood of a Bayesian model conjugate if the posterior (i) is parameterized and (ii) there is a measurable mapping T from the data x and the prior parameter  X  to the parameter  X  0 = T ( x, X  ) which specifies the corresponding posterior. In the definition below, the conditional probability k represents the parametric form of the posterior. The definition is applicable to  X  X onparametric X  models, in which case the parameter simply becomes infinite-dimensional.
 Definition 2 (Conjugacy and Posterior Index) . Let P X ( X |  X ) and P  X  ( X  |  X ) be regular conditional probabilities. Let P  X  ( X  | X,  X ) be the posterior of the model P X ( X |  X ) under prior P  X  ( X  |  X ) . Model and prior are called conjugate if there exists a regular conditional probability k : B  X   X   X  t  X  [0 , 1] , parameterized on a measurable Polish space ( X  t , B t ) , and a measurable map T :  X  x  X   X   X   X   X  t , such that The mapping T is called the posterior index of the model.
 The definition becomes trivial for  X  t =  X  x  X   X   X  and T chosen as the identity mapping; it is mean-ingful if T is reasonably simple to evaluate, and its complexity does not increase with sample size. Theorem 3 below shows that, under suitable conditions, the structure of the posterior index carries over to the projective limit model: If the finite-dimensional marginals admit a tractable posterior index, then so does the projective limit model.
 Example. (Posterior Indices in Exponential Families) Suppose that P X ( X |  X ) is an exponential P
 X  ( X  |  X ) as the  X  X atural conjugate prior X  with parameters  X  = (  X ,y ) . Its density, w.r.t. a suitable measure  X   X  on parameter space, is of the form q (  X  |  X ,y ) = K (  X ,y )  X  1 exp(  X   X ,y  X  X  X   X  X  (  X  )) . The posterior P  X  ( X  | X,  X ) is conjugate in the sense of Def. 2, and its density is q (  X  |  X  + 1 ,y + S ( x )) . is T ( x, (  X ,y )) := (  X  + 1 ,y + S ( x )) .
 The main result of this section is Theorem 3, which explains how conjugacy carries over from the finite-dimensional to the infinite-dimensional case, and vice versa. Both extension theorems discussed so far require a projection condition on the measures and models involved. A similar commute with the preimage under projection, The posterior indices of all well-known exponential family models, such as Gaussians and Dirich-lets, satisfy this condition. The following theorem states that (i) stochastic process Bayesian models that are constructed from conjugate marginals are conjugate if the projection equation (4) is satisfied, and that (ii) such conjugate models can only be constructed from conjugate marginals.
 Theorem 3 (Functional Conjugacy of Projective Limit Models) . Let E be a countable index set and  X  E x and  X  E  X  be Polish product spaces. Assume that there is a Bayesian model on each finite-dimensional subspace  X  I x , such that the families of all priors, all observation models and all poste-The theorem is not stated here in full generality, but under two simplifying assumptions: We have omitted the use of hyperparameters, such that the posterior indices depend only on the data, and all involved spaces (observation space, parameter space etc) are assumed to have the same dimension for each Bayesian model. Generalizing the theorem beyond both assumptions is technically not dif-ficult, but the additional parameters and notation for book-keeping on dimensions reduce readability. Proof Sketch 2 . Part (i): We define a candidate for the probability kernel k E representing the projec-tive limit posterior, and then verify that it makes the model conjugate when combined with the map-ping T given by assumption. To do so, we first construct the conditional probabilities P I  X  ( X  I | T I ) , show that they form a conditionally projective family, and take their conditional projective limit sents the posterior, we show that the two coincide on the cylinder sets (events which are preimages under projection of finite-dimensional events). From this, equality for all events follows by the Caratheodory theorem [1].
 definition of conjugacy, which is a straightforward computation. Theorem 3(ii) states that conjugate models have conjugate marginals. Since, in the finite-dimensional case, conjugate Bayesian models are essentially limited to exponential families and their natural conjugate priors 3 , a consequence of the theorem is that we can only expect a non-parametric Bayesian model to be conjugate if it is constructed from exponential family marginals  X  assuming that the construction is based on a product space approach.
 When an exponential family model and its conjugate prior are used in the construction, the form of the resulting model becomes generic: The posterior index T of a conjugate exponential fam-ily Bayesian model is always given by the sufficient statistic S in the form T ( x, (  X ,y )) := (  X  + 1 ,y + S ( x )) . Addition commutes with projection, and hence the posterior indices T I of a family of such models over all dimensions I  X  F ( E ) satisfy the projection condition (4) if and limit model. In the case of countable dimensions, Theorem 3 therefore implies a construction recipe for nonparametric Bayesian models from exponential family marginals; constructing the model boils down to checking whether the models selected as finite-dimensional marginals are conditionally projective, and whether the sufficient statistics satisfy the projection condition. An example con-struction, for a model on infinite permutations, is given in below. The following table summarizes some stochastic process models from the conjugate extension point of view: A Construction Example. The analysis of preference data, in which preferences are represented as permutations, has motivated the definition of distributions on permutations of an infinite number of items [9]. A finite permutation on r items always implies a question such as  X  X ank your favorite movies out of r movies X . A nonparametric approach can generalize the question to  X  X ank your favorite movies X . Meila and Bao [9] derived a model on infinite permutations, that is, on bijections of the set N . We construct a nonparametric Bayesian model on bijections, with a likelihood component Choice of marginals. The finite-dimensional marginals are probability models of rankings of a finite number of items, introduced by Fligner and Verducci [6]. For permutations  X   X  S r of length r , the model is defined by the exponential family density p (  X  |  X , X  ) := Z (  X  )  X  1 exp( S (  X  X   X  1 ) , X  ) , P permutation  X  defines the distribution X  X  mean. If all entries of  X  are chosen identical as some con-stant, this constant acts as a concentration parameter, and the scalar product is equivalent to the Kendall metric on permutations. This metric measures distance between permutations as the min-imum number of adjacent transpositions (i.e. swaps of neighboring entries) required to transform one permutation into the other. If the entries of  X  differ, they can be regarded as weights specifying the relevance of each position in the ranking [6].
 Definition of marginals. In the product space context, each finite set I  X  F ( E ) of axis labels is a I  X  J of indices by deleting all items indexed by J \ I , producing the restriction  X  J | I . We overload notation and write  X  JI for both the restriction in the group S I and axes-parallel projection in the will define a nonparametric Bayesian model that puts a prior on the infinite-dimensional analogue of  X  , i.e. on the weight function  X  E . For I  X  F ( N ) , the marginal of the likelihood component is define two families P I ( X I |  X  I ) and P I ( X  I |  X ) of measures over all finite dimensions I  X  F ( E ) . It is reasonably straightforward to show that both families are conditionally projective, and so is the family of the corresponding posteriors. Each therefore has a projective limit, and the projective limit Posterior index. The posterior index of the infinite-dimensional model can be derived by means infinite-dimensional hyperparameter, and only consider the corresponding infinite-dimensional prior bijections of N as follows. For each bijection  X  : N  X  N , and each j  X  N , set S E j (  X  ) := P is non-zero only for a finite number of indices l , such that the entries of S E are always finite. Then satisfies the projection condition (4) for any I  X  F ( E ) . By Theorem 3, this makes T E a posterior index for the projective limit model. We have shown how nonparametric Bayesian models can be constructed from finite-dimensional Bayes equations, and how conjugacy properties of the finite-dimensional models carry over to the infinite-dimensional, nonparametric case. We also have argued that conjugate nonparametric Bayesian models arise from exponential families.
 A number of interesting questions could not be addressed within the scope of this paper, including (1) the extension to model properties other than conjugacy and (2) the generalization to uncountable dimensions. For example, a model property which is closely related to conjugacy is sufficiency [14]. In this case, we would ask whether the existence of sufficient statistics for the finite-dimensional marginals implies the existence of a sufficient statistic for the nonparametric Bayesian model, and whether the infinite-dimensional sufficient statistic can be explicitly constructed. Second, the results presented here are restricted to the case of countable dimensions. This restriction is inconvenient, since the natural product space representations of, for example, Gaussian and Dirichlet processes on the real line have uncountable dimensions. The GP (on continuous functions) and the DP are within the scope of our results, as both can be derived by means of countable-dimensional surrogate constructions: Since continuous functions on R are completely determined by their values on Q , a GP can be constructed on the countable-dimensional product space R Q . Analogous constructions have been proposed for the DP [7]. The drawback of this approach is that the actual random draw is just a partial version of the object of interest, and formally has to be completed e.g. into a continuous function or a probability measure after it is sampled. On the other hand, uncountable product space constructions are subject to all the subtleties of stochastic process theory, many of which do not occur in countable dimensions. The application of construction methods to conditional probabilities also becomes more complicated (roughly speaking, the point-wise application of the Kolmogorov theorem in the proof of Theorem 2 is not possible if the dimension is uncountable).
 Product space constructions are by far not the only way to define nonparametric Bayesian models. A P  X  olya tree model [7], for example, is much more intuitive to construct by means of a binary partition argument than from marginals in product space. As far as characterization results, such as which models can be conjugate, are concerned, our results are still applicable, since the set of Poly  X  a trees can be embedded into a product space. However, the marginals may then not be the marginals in terms of which we  X  X aturally X  think about the model. Nonetheless, we have hopefully demonstrated that the theoretical results are applicable for the construction of an interesting and practical range of nonparametric Bayesian models.
 Acknowledgments. I am grateful to Joachim M. Buhmann, Zoubin Ghaharamani, Finale Doshi-Velez and the reviewers for helpful comments. This work was in part supported by EPSRC grant EP/F028628/1. References [1] H. Bauer. Probability Theory . W. de Gruyter, 1996. [2] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The infinite hidden Markov model. In [3] P. Billingsley. Probability and measure, 1995. [4] S. R. Dalal and W. J. Hall. Approximating priors by mixtures of natural conjugate priors. [5] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics , [6] M. A. Fligner and J. S. Verducci. Distance based ranking models. Journal of the Royal Statis-[7] J. K. Ghosh and R. V. Ramamoorthi. Bayesian Nonparametrics . Springer, 2002. [8] T. L. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process. [9] M. Meil  X  a and L. Bao. Estimation and clustering with infinite rankings. In Uncertainty in [10] M. M. Rao. Conditional Measures and Applications . Chapman &amp; Hall, second edition, 2005. [11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning . MIT Press, [12] C. P. Robert. The Bayesian Choice . Springer, 1994. [13] D. M. Roy and Y. W. Teh. The Mondrian process. In Advances in Neural Information Pro-[14] M. J. Schervish. Theory of Statistics . Springer, 1995. [15] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal [16] S. G. Walker, P. Damien, P. W. Laud, and A. F. M. Smith. Bayesian nonparametric inference [17] L. Wasserman. All of Nonparametric Statistics . Springer, 2006.
