 Evaluation plays a critical role in IR research as objective functions for system effectiveness optimization. Traditional evaluation paradigm focused on assessing system performance on serving  X  X est X  results for single queries. The Cranfield method proposed by Cleverdon [ 4 ] evaluates performance with a fixed document collection, a query set, and relevance judgments. The relevance judgments of the documents are used to calculate various metrics which are proposed based on different understanding of users X  behavior. We refer this type of evaluation paradigm as offline evaluation, which is still predominant form of evaluation. To line up the evaluation and the real user experience, online evaluation tries to infer users X  preference from implicit feedback (A/B test [ 17 ], interleaving [ 14 ]), or explicit feedback (satisfaction [ 11 ]). Online methods naturally take user-based factors into account, but the evaluation results can hardly be reused. Offline and Online methods have already achieved great success in promot-ing the development of search engine. However, offline evaluation metrics do not always reflect real users X  experience [ 26 ]. The fixed user behavior assumptions (e.g. Cascade assumption) behind offline metrics may lead to failures on indi-vidual users. Consider an example in our experiment (depicted in Fig. 1 ), user A and B worked on the same task in one search engine and behaved in simi-lar ways, the offline measurements should also be similar. However, the actual normalized scores given by external assessors showed that there was a relatively great difference in their success degrees.
 In a typical search, according to the Interpretive Theory of Translation (ITT) [ 15 ], the search process can be modelled as three interrelated phases: (1) reading the content, (2) knowledge construction and (3) answer presentation. Inspired by this idea, we formalize the search success evaluation as a machine translation evaluation problem and propose a Search Success Evaluation frame-work based on Translation model (SSET). The ideal search outcome, which can be constructed manually, is considered as the  X  X eference X  . Meanwhile, the indi-vidual search outcome collected from a user is regarded as a  X  X ranslation X  . In this way, we can evaluate  X  X hat degree of success the user has achieved X  by evaluat-ing the correspondence between the ideal search outcome and individual search outcome. We investigate a number of machine translation (MT) evaluation metrics and choose BLEU (Bilingual Evaluation Understudy) for its simpleness and robustness.
 also propose an automatic extraction method with various users X  behavior data. Experiments indicate that evaluation with automated extracted outcome per-forms comparatively as well as with manually organized outcomes. Thus, it is possible to perform automated online evaluation including relatively large scale of users. In summary, our contribution includes: (1) A search evaluation frame-work based on machine translation model. To the best of our knowledge, our study is among the first to evaluate success with machine translation models. (2) An extraction method for the automatic generation of references with the help of multiple users X  search interaction behavior (e.g. eye-tracking) is proposed and enables quick or frequent evaluations. (3) Experiment framework and data shared with the research community. Online/Offline Search Evaluation. Cranfield-like approaches [ 4 ] introduced a way to evaluate ranking systems with a document collection, a fixed set of queries, and relevance assessments from professional assessors. Ranking systems are evaluated with metrics, such as Precision, Recall, nDCG etc. The Cranfield framework has the advantage that relevance annotations on query-document pairs can be reused.
 centred on real users X  experience. The online evaluation methods, observing user behavior in their natural task procedures offer great promise in this regard. The satisfaction [ 11 ] method will ask the users to feedback their satisfaction during the search process explicitly, while the interleaving [ 14 ], A/B testing [ 17 ] methods try to infer user preference depending on implicit feedbacks, such as click-through etc. The evaluation results can hardly be reused for other systems which are not involved in the online test.
 Session Search Evaluation. Beyond serving  X  X est X  results for single queries, for search sessions with multiple queries, several metrics are proposed by extend-ing the single query metrics, i.e. the nsDCG based on nDCG [ 12 ]and instance recall based on recall [ 23 ]. Yang and Lad [ 31 ] proposed a measure of expected utility for all possible browsing paths that end in the k th reformulation. Kanoulas et al. [ 16 ] proposed two families of measures: one model-free family (for example, session Average Precision ) that makes no assumption about the user X  X  behavior and the other family with a simple model of user interactions over the session ( expected session Measures ).
 Search Success Prediction. Previous researchers intuitively defined search success as the information need fulfilled during interactions with search engines. Hassan et al. [ 10 ] argued that relevance of Web pages for individual queries only represented a piece of the user X  X  information need, users may have different information needs underlying the same queries. Ageev et al. [ 1 ] proposed a prin-cipled formalization of different types of  X  X uccess X  for informational tasks. The success model consists of four stages: query formulation , result identification , answer extraction and verification of the answer . They also presented a scalable game-like prediction framework. However, only binary classification labels are generated in their approach.
 What sets our work apart from previous approaches is the emphasis on the outcomes the users gained through multiple queries. Our framework evaluates the success based on the information gained by users rather than implicit behavior signals. Ageev et al. X  X  definition about  X  X uccess X  was designed to analyze the whole process of their designed informational tasks. In our work, we simplify this definition and mainly focus on in what degree the user has gained enough information for certain search tasks.
 Machine Translation Evaluation. Machine Translation models have been explored in Information Retrieval research for a long time [ 3 , 7 ]. However, machine translation evaluation methods have not been explored in search success evaluation problem. Several automatic metrics were accomplished by comparing the translations to references, which were expected to be efficient and correlate with human judgments. BLEU was proposed by Papineni et al. [ 24 ] to evalu-ate the effectiveness of machine translation systems. The scores are calculated for individual language segments (e.g. sentences) combining modified n -gram precision and brevity penalty. Several metrics were proposed later by extending BLEU [ 5 , 28 ].
 Based on our definition of success, we mainly focus on whether a user has found the key information to solve the task. BLEU offers a simple but robust way to evaluate how good the users X  outcome are comparing to pre-organized ideal search outcome on n -gram level. Other MT evaluation metrics could be adopted in this framework in a similar way as BLEU and we would like to leave them to our future work. 3.1 Search Success Evaluation with Translation Model (SSET) During a typical Web search, the user X  X  information gathering actions can be regarded as  X  X istilling X  information gained into an organized answer to fulfill his/her information need [ 3 ]. We take the view that this distillation is a form of translation from one language to another: from documents, generated by Web page authors, to search outcome, with which the user seeks to complete the search task. Different from the standard three step translation process (understanding, deverbalization and re-expression [ 19 , 27 ]),  X  X e-expression X  is not always nec-essary in search tasks. In our framework, we retain the re-expression step by asking the participants to summarize their outcomes with the help of a prede-fined question so that we can measure the success of search process. The details of the framework will be represented in Sect. 4 .
 emphasis on user perceived information corresponding to the search task. We at first define some terminologies to introduce our framework: Individual Search Outcome: for a specific user engaged in a search task, search outcome is the information gained by the user from interactions to fulfill the task X  X  information need.
 Ideal Search Outcome: for a certain topic, ideal search outcome refers to all possible information that can be found (by oracle) through reading the relevant documents provided by the search engine to fulfill the task X  X  information need. Search Success: Search Success is the situation that the user has collected enough information to satisfy his/her information need.
 outcome of the user can be described by another sequence of words as S the ideal search outcome can be represented by a sequence of words as T We assume users X  search outcomes and the ideal search outcomes have identical vocabularies due to both of them come from the retrieved documents.
 lation model (SSET), which is presented in Fig. 2 . Suppose the user X  X  individ-ual search outcome is a  X  X ranslation X  from examined documents in the search session, we can treat the ideal search outcome as a  X  X eference X  , which can be constructed manually by human assessors, or automatically based on group of users X  interaction behaviors.
 that  X  X he closer a machine translation is to a professional human translation, the better it is X  [ 24 ]. We assume that  X  X he closer a user X  X  individual search outcome is to an ideal search outcome, the more successful the search is X . The SSET model is proposed to evaluate the search success by estimating the closeness between the individual search outcome and the ideal search outcome.
 search outcome S J , according to the ideal search outcome T where c ( n -gram, S ) indicates the times of appearances of the n -gram in S .In other words, one truncates each n -gram  X  X  count, if necessary, to not exceed the largest count observed in the ideal search outcome. Then the brevity penalty BP is calculated by considering the length of the individual search outcome ( c ), and the length of the ideal search outcome ( r ): The SSET score combine both the modified precision of n -gram in different lengths and the brevity penalty, where w n is the weight of the modified precision of n -gram , we use the typical value N = 4 in our experiment. In this way, we can measure how close the individual search outcome is to ideal search outcome. The ideal search outcome organized by human assessors could be reused to evaluate other retrieval systems.
 However, the generation process of ideal search outcome is still expensive and time-consuming. The individual search outcome generated by explicit feedback would also bring unnecessary effort to the users. We further explore the automa-tion generation of ideal search outcome and individual search outcome based on multiple search interaction behaviors. 3.2 Automated Search Outcome Generation We employ a bag-of-words approach, which has proven to be effective in many retrieval settings [ 9 , 18 ], to generate pseudo documents as the users X  individual search outcome and ideal search outcome.
 Consider a user u involving in certain task t , we can calculate a modified TF-IDF score for each n -gram in the snippets and titles read by the user, the IDFs of terms are calculated on the titles and snippets of all the tasks X  SERPs: where w r denotes the weight of documents. We can estimate w clicks or eye-fixations. For the score based on user clicks, w the score based on fixations, w r = f  X  fixations on r log ( duration Thus, we can construct the user X  X  individual search outcome indiv so by joining the top-k n -grams with greatest scores in different lengths. Note that all the n -grams appearing in the task description are removed because we want to capture what extended information the user have learned from the search process. We could calculate s n -gram for the n -gram with group of users X  clicks or eye-fixations in a similar way. The ideal search outcome could be organized by utilizing group of users X  interactions, which is assumed to be a kind of  X  X isdom of crowds X .
 ing to help the generation of search outcome extraction. Due to the limit of experimental settings in our system, we would leave them to our future work. get several SSET models which are shown in Table 1 . The performance of these models will be discussed in Sect. 5 . In the experiments we find that the SSET models with eye-tracking data always outperform the models with clickthrough information, thus, we only report the performance of models with eye-tracking in the remainder of this paper. We conducted an experiment to collect user behaviors and search outcomes for completing complex tasks. During the whole process, users X  queries, eye fixation behaviors on Search Engine Result Pages (SERPs), clicks and mouse movements are collected.
 Search Task. We selected 12 informational search tasks for the experiment. 9 of them were picked out from recent years X  TREC Session Track topics. According to the TREC Session Track style, we organized 3 tasks based on the participants X  culture background and the environment they live in. The criteria is that the tasks should be clearly stated and the solutions of them cannot be retrieved simply by submitting one query and clicking the top results. Each task contains three parts: an information need description, an initial query and a question for search outcome extraction. The description briefly explains the background and the information need. To compare user behavior on query level, the first query in each task was fixed. We summarized the information needs and extracted key words as relatively broad queries. People may argue that the fixed initial queries might be useless for searcher. Statistics shows that there are average 2.33 results clicked on the SERPs of initial queries. At the end of the task, the question is showed to the participants which requires them summarize the information gained in the searching process and the answers were recorded by voice. Experimental System. We built an experimental search system to provide modified search results from a famous commercial search engine in China. First, all ads and sponsors X  links were removed. Second, we removed vertical results to reduce possible behavior biases during searching process [ 30 ]. Third, we remove all the query suggestions because we suppose that query reformula-tion might reflect potential interests of users. Besides these changes, the search system looks like a traditional commercial search engine. The users could issue a query, click results, switch to the landing pages and modify their queries in a usual way. All the interactions were logged by our background database, includ-ing clicks, mouse movement and eye-tracking data.
 Eye-tracking. In the experiment, we recorded all participants X  eye movements with a Tobii X2-30 eye-tracker. With this tracking device, we are able to record various ocular behaviors: fixations, saccades and scan paths. We focus on eye fixations since fixations and durations indicate the users X  attention and reading behavior [ 25 ].
 Participants. We recruited 29 undergraduate students (15 females and 14 males) from a University located in China via email, online forums, and social networks. 17 of 29 participants have perfect eyesight. For the others, we cali-brated the eye-tracker carefully to make sure the tracking error was acceptable. All the participants were aged between 18 and 26. Ten students are major in human sciences, fifteen are major in engineering while the others X  majors range from arts and science. All of them reported that they are familiar with basic usage of search engines.
 Procedure. The experiment proceeded in following steps, as shown in Fig. 3 . First the participants were instructed to read the task description carefully and they were asked to retell the information need to make sure that they had under-stood the purpose of the search tasks. Then the participants could perform searches in our experimental system as if they were using an ordinary search engine. We did not limit their time of searching. The participants could finish searching when they felt satisfied or desperate. After searching for information, the participants were asked to judge and rate queries/results regarding their contribution to the search task. More specifically, they were instructed to make the following three kinds of judgments in a 5-points Likert scale, from strong disagreement to strong agreement:  X  X oreach clicked result , how useful it is to solve the task?  X  X oreach query , how useful it is to solve the task?  X  Through the search session , how satisfied did the participant feel? At last, the system would present a question about the description, which usually encourage the participant to summarize their searches and extract search outcome. The answers from users would be recorded by voice. We notice that answering the question by voice-recording could not only reduce participants X  effort but also give them a hint that they should be more serious about the search tasks.
 This section will lead to answers to 3 research questions:
RQ1: How well do the result of SSET correlate with human assessments? Can RQ2: What X  X  the relationship between SSET, and offline/online metrics?
RQ3: Does automatic methods work for SSET? Can we extract the 5.1 Data and Assessments In our experiment, we collected search behavior and success behavior from 29 participants on 12 unique tasks. To evaluate search success, we recruited 3 anno-tators to assess the degree of success based on the users X  answer after each task. The assessors were instructed to make judgments with magnitude estimation (ME) [ 29 ] methods, rather than ordinal Likert scale. ME could be more pre-cise than traditional multi-level categorical judgments and ME results were less influenced by ordering effects than multi-points scale [ 6 ]. For each task, before assessments, the assessors were represented with the task description and the question. The records of 29 participants are randomly listed on a webpage, each assessor make judgments sequentially. For each record, the assessor can listen to the record one or more times and then assign a score between 0 and 100 to the record in such a way that the score represents how successful the record is. The score was normalized according to McGee et al. X  X  method [ 22 ]. In this paper, we use the mean of normalized scores from three assessors as the Ground Truth of search success evaluation.
 ( X  X lease tell the support conditions of the Hong Kong version iphone to domestic network operators. X ) fails to help the participants to summarize their search outcome depending on the task description ( X  X ou want to buy a iphone6 in Hong Kong. Please find the domestic and Hong Kong price of iphone6, how to purchase iphone in Hong Kong, whether it is necessary to pay customs on bringing iphone home, whether the Hong Kong version of iphone would support domestic network operators, etc. X ), because the question just focuses on a detailed fact about the task. Thus, in the reminder analysis of this paper, Task 10 and corresponding data is removed and we have 319 sessions (11 tasks with 29 participants) in total. After assessments, we asked the assessors to organized standard answers for the 12 tasks. More specifically, the three assessors were instructed to search information about the tasks with the retrieval system that were used by the par-ticipants. Note that the assessors did not perform any search before assessments for individual search outcome to avoid potential biases, e.g., they may prefer the individual outcomes similar to the documents examined by them. Then, the assessors organized their own answers and summarized the ideal search outcomes based on both their own answers and the 29 participants X . In addition, all the recorded voices were converted to text, with discourse markers removed, which were regarded as users X  individual search outcomes. 5.2 SSET vs. Human Assessment With our proposed evaluation framework, Search Success Evaluation with Trans-lation model (SSET), we attempt to evaluate what degree of success a searcher has achieved in a certain task. The normalized scores for three assessors are regarded as Ground Truth of the performance evaluation of search success eval-uation model.
 For each session (a user in a certain task), the input of SSET includes a  X  X eference X  , the ideal search outcome, and a  X  X ranslation X  , the individual search outcomes from each participants and the SSET outputs the degree of success in a value range.
 We calculate the correlation of SSET1MM model and the Ground Truth. The SSET1MM model uses the ideal search outcomes organized by external assessors as  X  X eferences X  and use the answers of questions (individual search outcomes) as  X  X ranslations X  . The correlation on each task is shown in Table 2 . The results show that SSET1MM correlates with the human judgments on most of tasks. The Pearson X  X  r is significant at 0 . 01for10of11tasks,which makes this method as an automated understudy for search success evaluation when there is need for quick or frequent evaluations.
 We notice that the performance of SSET1MM varies with the tasks. It may suggest that the SSET is task-sensitive, in other words, SSET1MM is not appro-priate for all kinds of tasks. From the facet of search goal identified by Li et al. [ 20 ], we can classify the tasks into 2 categories: specific (well-defined and fully developed) and amorphous (ill-defined or unclear goals that may evolve along with the user X  X  exploration). Thus, we find SSET performs better on the spe-2 , 1 , 12 , 11 , 7). For an amorphous task (e.g. find a ice breaker game), it is very difficult to construct a  X  X erfect X  search outcome including all possible answers. Therefore, SSET is more appropriate to evaluate the tasks which are well-defined and have restrained answers.
 5.3 SSET vs. Offline/Online Metrics SSET attempt to combine the advantages of offline and online evaluation meth-ods. The tasks and ideal search outcomes organized by human experts offline can be reused easily and the individuals X  search outcomes can be collected online effi-ciently and effectively. In this section, we investigate the relationship between SSET and offline/online metrics.
 well with user satisfaction. We use sCG as a offline measure of the search out-come, which is the sum of each query X  X  information gain. For each query, its gain is originally calculated by summing the gains across its results. In this work, we use the participants X  subjective annotation ( X  X ow useful it is to solve the task? X ) as a proxy of the query X  X  gain, e.g. SearchOutcome = sCG = weak correlation between SSET1MM and sCG. It is partly due to the difference in cognitive abilities between users. Consider the example in Sect. 1 , two users search for  X  X he side effects of red bulls X , they issued similar queries, viewed similar SERPs and got quite close sCG scores. However, the information they gained for completing the task differed at quality and quantity. In other words, it means that the offline metric may lead to failure to evaluate in what degree the user has achieved success in complex tasks.
 experiment, we asked the users to rate their satisfaction for each task. The correlation of SSET1MM and user satisfaction is shown in Table 3 .
 faction. Jiang et al. [ 13 ] reported that the satisfaction was mainly affected by two factors, search outcome and effort. However, the search success evaluation mainly focuses on the search outcome of users. No matter the degree of success is assessed by external assessors or the SSET systems, they are not aware of the effort that the user has made to achieve the search outcome. This could be a plausible explanation for the closeness to uncorrelated between SSET and satisfaction. Jiang et al. proposed an assumption that the satisfaction is the value of search outcome compared with search effort. As our proposed SSET is also a measurement of search outcome, we investigate the correlation between SSET/Search Effort.
 Search effort is the cost of collecting information with the search engine, e.g., formulating queries, examining snippets on SERPs, reading results, etc. We follow the economic model of search interaction proposed in [ 2 ]. For a particular search session, we can use Q (number of queries) as a proxy of search effort. Table 4 shows that there is strong correlation between SSET1MM/#queries and user X  X  satisfaction for most of the tasks and our proposed SSET is able to act as an indicator of search outcome. 5.4 Performance of Automated Outcome Extraction Development of search engine is based on ongoing updates. In order to vali-date the effect of a change to prevent its negative consequences, the developers compare various versions of the search engines frequently. This motivate us to improve SSET with automated methods for the organization of ideal/individual search outcomes.
 In Table 4 , we compared the correlation between 4 different SSET models and the Ground Truth (external assessments). SSET1MM is the model which use manually organized as ideal search outcome and users X  answer for questions as individual search outcome. We use SSET1MM as a baseline to evaluate other SSET models.
 collect individual search outcomes (e.g. summarized by users) but constructs the ideal search outcomes automatically based on users X  eye fixations on snippets. Thus, in practical environment, we can generate ideal search outcome based on group of users X  behavior.
 two models, we adopt the individual search outcome extraction method based on the user X  X  eye fixations on SERPs. The individual search outcomes generated automatically differs a lot from their answers. The potential two reasons are: (1) the sparsity of user behavior makes it difficult to extract search outcome. (2) what the user has read is not equal to what he/she has perceived. Similar phenomenon has been observed by previous researches [ 21 ].
 users X  behaviors. We randomly split the all the participants into five groups, four groups has six participants while the remaining one has five. Then we construct multiple ideal search outcomes by sequentially adding group of users X  fixations into the SSET2MA model. Then we compare the correlations between SSET2MA models and the Ground Truth.
 model based on the first k groups of users. As the size of users grows, the correlation between SSET2MA and the Ground Truth becomes stronger. The SSET2MA 3 almost performs as well as SSET3AM 5 . In other words, in practice, we need about behavior data from about 15 people to construct a reliable ideal search outcome for SSET. Although previous offline/online evaluation frameworks have achieved significant success in the development of search engines, they are not necessarily effective in evaluate in what degree of success the search users have achieved. In this work, we put emphasis on the outcomes the users gained through multiple queries. We propose a Search Success Evaluation framework with Translation model (SSET). The search success evaluation is formalized as a machine translation evaluation problem. A MT evaluation algorithm called BLEU is adopted to evaluate the success of searchers. Experiments shows that evaluation methods based on our proposed framework correlates highly with human assessments for complex search tasks. We also propose a method for automatic generation of ideal search outcomes with the help of multiple users X  search interaction behaviors. It proves effective compared with manually constructed ideal search outcomes. Our work can help to evaluate search success as an understudy of human assessments when there is need for quick or frequent evaluation. In the future work, we plan to adopt more MT evaluation methods in this framework and compare the performance in evaluate different types of tasks. Experiments with a relatively large scale of participants will be conducted based on crowdsourcing platforms.
