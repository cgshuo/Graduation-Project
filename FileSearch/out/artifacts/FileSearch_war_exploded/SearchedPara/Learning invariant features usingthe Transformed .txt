 One way the human brain manages the massive amount of sensory information it receives is by learning invariants  X  regularities in its input that do not change across many sti muli sharing some property of interest. Learning and using invariants is esse ntial to many aspects of cognition and that the features of an object can occur differently across p resentations, but will be transformed in a few predictable ways. Representing objects in terms of i nvariant features poses a challenge for models of feature learning. From a computational perspe ctive, unsupervised feature learning involves recognizing regularities that can be used to compa ctly encode the observed stimuli [2]. When features have the same appearance and location, techniq ues such as factorial learning [3] or various extensions of the Indian Buffet Process (IBP) [4] ha ve been successful at learning features, and show some correspondence to human performance [5]. Unfo rtunately, invariant features do not always have the same appearance or location, by definition. D espite this, people are able to identify invariant features (e.g., [6]), meaning that new machine le arning methods need to be explored to fully understand human behavior.
 We propose an extension to the IBP called the Transformed Ind ian Buffet Process (tIBP), which infers features that vary across objects. Analogous to how t he Transformed Dirichlet Process extends the Dirichlet Process [7], the tIBP associates a parameter w ith each instantiation of a feature that determines how the feature is transformed in the given image . This allows for unsupervised learning the tIBP and presenting a Gibbs sampling inference algorith m, we show that this model can learn visual features that are location invariant by modeling pre vious behavioral results (from [6]). (a) Figure 1: Ambiguous representations. (a) Does this object h ave one feature that contains two vertical bars or two features that each contain one vertical bar? (b) A re these two shapes the same? The shape on the left is typically perceived as a square and the shape on the right is typically perceived as a diamond despite being objectively equivalent after a trans formation (a 45 degree rotation). One new issue that arises from inferring invariant features is that it can be ambiguous whether parts of an image are the same feature with different transformati ons or different features. For example, two vertical bars a fixed distance apart, or two features each of which is a vertical bar with its own translational transformation (see Figure 1 (a)). The tIBP s uggests an answer to this question: pick the smallest feature representation that can encode all obs erved objects. By presenting objects that independently in location, we confirm that people use sets of objects to infer invariant features in a behavioral experiment and that the different feature repre sentations lead to different decisions. Introducing transformational invariance also raises the q uestion of what kinds of transformations a feature can undergo. A classic demonstration of the difficu lty of defining a set of permissable transformations is the Mach square/diamond [8]. Are the two shapes in Figure 1 (b) the same? The shape on the right is typically perceived as a diamond whi le the shape on the left is seen as a square, despite being identical except for a rotational tra nsformation. We extend the tIBP to include of whether people can infer the permissable transformation s of a feature. We demonstrate that this is the case by showing that people vary in their generalization s from a square to a diamond depending on whether the square is shown in the context of other squares that vary in rotation. This provides an interesting new explanation of the Mach square/diamond: Pe ople learn the allowed transformations of features for a given shape, not what transformations of fe atures are allowed over all shapes. One common approach to unsupervised learning is to explicit ly define the generative process that created the observed data. Latent structure can then be iden tified by inverting this process using Bayesian inference. Nonparametric Bayesian models can be u sed in this way to infer latent struc-ture of potentially unbounded dimensionality [9]. The Indi an Buffet Process (IBP) [4] is a stochastic process that can be used as a prior in nonparametric Bayesian models where each object is repre-sented using an unknown but potentially infinite set of laten t features. 2.1 Learning features using the Indian Buffet Process The standard treatment of feature learning using nonparame tric Bayesian models factors the obser-and (2) a matrix Y that represents how the features instantiated. If there are N objects and K fea-tures, then Z is a N  X  K binary matrix (where object n has feature k if z matrix (where D is the dimensionality of the observed properties of each obj ect, e.g., the number of pixels in an image). The IBP defines a probability distributi on over Z when K  X  X  X  such that only a finite number of the columns are non-zero (with prob. 1 for fin ite N ). This distribution is where  X  is a parameter affecting the number of non-zero entries in th e matrix, K of features with history h (the history is the corresponding column of each feature, in terpreted as a binary number), K number, and m used for Y (Gaussian for generating real-valued observations, or Ber noulli for binary observations). The observed properties of objects can be summarized in a N  X  D matrix X . The vector x representing the properties of object n is generated based on its features z can be done using a linear-Gaussian likelihood for real-val ued properties [4], or a noisy-OR for binary properties [10]. All of the modeling results in this p aper use the noisy-OR, with where x 2.2 The Transformed Indian Buffet Process (tIBP) Following Sudderth et al. X  X  [7] extension of the Dirichlet P rocess, the Transformed Indian Buffet a sense, when an object takes a feature, the feature is transf ormed with respect to the object. Let of transformations parameterized by  X  , r for object n , and f ( x the data distribution. The following generative process de fines the tIBP: In this paper, we focus on binary images where the transforma tions are drawn uniformly at random from a finite set (though Section 5.1 uses a slightly more comp licated distribution). The reason for invariances in translation, size, or rotation and to model i mages where a feature occurs in a novel starting place of its feature in each dimension by r prior on shifts: r new interpretation of the feature, r Equation 2, substituting the vector of transformed feature interpretations r 2.3 Inference by Gibbs sampling We sample from the posterior distribution on feature assign ments Z , feature interpretations Y , and transformations R given observed properties X using Gibbs sampling [11]. The algorithm consists of iteratively drawing each variable conditioned on the cur rent values of all other variables. For features with m over transformations. This avoids a bottleneck in sampling , as otherwise we would have to get lucky in drawing the right feature and transformation. The margin alization can be done directly, with where the first term on the right hand side is proportional to p ( x vided by the likelihood and the IBP prior respectively, with Z the second term is uniform over all r where the relevant probabilities are also used in computing Equation 3, and can thus be cached. We follow Wood et al. X  X  [10] method for drawing new features ( ie. features for which currently m k = 0 where Z new is Z augmented with K new know that K new to marginalize over the possible new feature images and thei r transformations ( Y and R in an image are not identifiable, this assumption is valid and necessary to aid in inference. With no transformations, drawing the new features in the noisy-O R tIBP model is equivalent to drawing the new features in the normal noisy-OR IBP model. Thus, we ca n use the same sampling step for K where r Finally, to complete each Gibbs sweep we resample the featur e interpretations ( Y ) given the state of the other variables. We sample each y where p ( X | Y , Z , R ) is the likelihood, given by the noisy-OR function. 2.4 Prediction To compare the feature representations our model infers to b ehavioral results, we need to have judge-ments of the model for new test objects. This is a prediction p roblem: computing the probability of a new object x P ( x N +1 | X ) = X The Gibbs sampling algorithm gives us samples from P ( Z , Y , R | X ) that can be used to approxi-mate this sum. However, a further approximation is required to compute P ( x each sweep of Gibbs sampling, we sample a vector of features z mations r R in that sweep, under the constraint that no new features are g enerated. We use these samples to approximate the calculation of P ( x do not reoccur in the exact same location. A common strategy f or dealing with this problem is to to discover that features are translation invariant, and to infer them directly from the data. Fiser and colleagues [6, 12] showed that when two parts of an i mage always occur together (forming a  X  X ase pair X ), people expect the two parts to occur together as if they had one feature representing the pair. In Experiments 1 and 2 of [6], participants viewed 1 44 scenes, where each scene contained three of the six base pairs in varied spatial location. Each b ase pair was two of twelve parts in once (but were not a base pair). Participants strongly prefe rred the base pair. To demonstrate the ability of tIBP to infer translation invariant features tha t are made up of complex parts, we trained the model on the scenes with the same structure as those shown to participants. The only difference was to lower the dimensionality of the images by recoding eac h part to be a 3 by 3 pixel image (the images from [6] were 1200 by 900 pixels). Figure 2 (a) shows th e basic parts (grouped into their base pairs), while 2 (b) shows one scene given to the model. Fi gure 2 (c) shows the features inferred base pairs grouped in rectangles. (b) One example scene. (c) Features inferred by the tIBP model (one sample from the Gibbs sampler). The tIBP infers the base pairs as features. by the tIBP model (one sample from the Gibbs sampler after 100 0 iterations with a 50 iteration pairs can occur in any location. To compare the model people X  s familiarity judgments, we calculated the tIBP model gave higher probability to the image containi ng the base pair. A new problem arises out of learning features that can transf orm. Is an image composed of the same or may not be transformed? One way to decide between two possi ble feature representations for the with. For example, the object from Figure 1 (a) is the first obj ect (from the top left) in the two set can be represented as translations of one feature that is two vertical bars. Although this object set can also be described in terms of two features (each of whi ch are vertical bars that can each represented in terms of two features, where each is a vertica l bar.
 be expected to be in the set. Representing the objects with a s ingle feature containing two vertical are each vertical bars; however, any object with two vertica l bars is expected ( New Separate )  X  not sentations has consequences for how to generalize set membe rship. In the following experiment, we test these predictions by asking people after viewing eit her the unitized or separate object sets to judge how likely the New Unitized or New Separate objects are to be part of the object set they viewed. We then compare the behavioral results to the featur es inferred by the tIBP model and the predictive probability of each of the test objects given eac h of the object sets. (a) (b) Figure 3: Training sets for Experiment 1. (a) Objects made fr om spatial translations of the unitized feature. (b) Objects made from spatial translations of two s eparate features. The number of times each vertical bar is present is the same in the two object sets . Figure 4: Results of Experiment 1. (a) Human judgments. The unitized group only rated those images with two vertical bars close together highly. The separate group rate any image with two vertical bars highly. (b) The predictions by the tIBP model. 4.1 Methods A total of 40 participants were recruited online and compens ated a small amount. Three participants were removed for failing to complete the task leaving 19 and 1 8 participants in the separate and unitized conditions respectively. There were two phases to the exper iment: training and test. In the training phase, participants read this cover story (adapte d from [13]):  X  X ecently a Mars rover found a cave with a collection of different images on its walls. A te am of scientists believes the images could have been left by an alien civilization. The scientist s are hoping to understand the images so they can find out about the civilization. X  They then looked th rough the eight images (which were either the unitized or separate object set in a random order) and scrolled down to the next sec tion once they were ready for the test phase. Once they scrolled do wn to the next section, they were informed that there were many more images on the cave wall tha t the rover had not yet had a chance to record. Their task for the test phase was to rate how likely on a scale from 0 to 6 they believed the rover would see each image as it explored further through the cave. There were nine test images presented in a random order: Seen Both (an image in both training sets), Seen Unit (an image that only the unitized group saw), Seen Sep (an image only the separate group saw), New Unit (an image valid under the unitized feature set), New Sep (a image valid under separate feature set), and four other images that acted as controls (the images are under the horizontal axes of Figure 4). 4.2 Results Figure 4 (a) shows the average ratings made by participants i n each group for the nine test images. believed the Mars rover was likely to encounter the two image s it observed and the New Unit image (the unitized feature in a new horizontal position), but did not think it wo uld encounter the other objects. The separate group rated any image with two vertical bars highly. This ind icates that they represent the images using two features each containing a si ngle vertical bar varying in horizontal objects (taking into account the different horizontal posi tion of the features in each object). Figure 4 (b) shows the predictions made by the tIBP model when given each object set. The pre-dictive probabilities for the test objects were calculated using the procedure outlined above (with the parameter values from Section 3), using 1000 iterations of Gibbs sampling and a 50 iteration burn-in. A non-linear monotonic transformation of these pr obabilities was used for visualization, inferred type of invariance a New Rotation and a New Size object. raising the unnormalized probabilities to the power of 0.05 and renormalizing. The Spearman X  X  rank order correlation between the model X  X  predictions and huma n judgments is 0.85. Qualitatively, the the 1 Bar test image highly. Unlike the participants in the separate condition, the model does not infer that each object has two features and so having only one feature is not a good object. This sug-of features each object typically has. Investigating how pe ople infer expectations about the number of features objects have is an interesting phenomenon that d emands further study. A natural next step for improving the tIBP would be to make the set of transformations  X  larger and thus extend the number of possible invariants that can be lea rned. Although this may be appropriate from a machine learning perspective, it is inappropriate fo r understanding human cognition. Re-call the Mach square/diamond example in Figure 1 (b). Many sh apes are equivalent when rotated; however, rotational invariance does not hold for all shapes . This example teaches a counterintuitive moral: The best approach is not to include as many transforma tions as possible into the model. Though rotations are not valid transformations for what peo ple commonly consider to be squares, they are appropriate for many objects. This suggests that pe ople infer the set of allowable transfor-seems clear that the New Rotation object in Figure 5 (c) belongs in the set, but not the New Size this phenomenon, we first extend the tIBP to infer the appropr iate set of transformations by intro-ducing latent variables for each feature that indicate whic h transformations it is allowed to use. We demonstrate this extension to the tIBP predicts the New Rotation object when given the rotation set and predicts the New Size object when given the size set  X  effectively learning the appropriate type of invariance for a given object class. Finally, we confirm ou r introspective argument that people infer the type of invariance appropriate to the observed cla ss of objects. 5.1 Learning invariance type using the tIBP It is straightforward to modify the tIBP such that the type of transformations allowed on a feature is of transformation allowed for that feature. Then, the featu re transformation is generated conditioned on this hidden variable from a probability distribution spe cific to the transformation type. The experiment in this section is learning whether or not the feature defining a set of objects is same as the tIBP, but introduces the latent variable t allowed by feature k . If t discrete uniform distribution distribution ranging in mul tiples of fifteen degrees from zero to 45). If t k = 0 , then size transformations are drawn from  X   X  (which is the discrete uniform distribution The inference algorithm for this extension is the same as for the tIBP except we need to infer the values of t Figure 6: Results of Experiment 2. (a) Responses of human par ticipants. (b) Model predictions. Prediction is as above except t 5.2 Methods A total of 40 participants were recruited online and compens ated a small amount, with 20 partici-pants in both training conditions ( rotation and size ). The cover story from Experiment 1 was used. the rotation set ), Same Size (the last object of the size set ), New Rot and New Size . 5.3 Results Figure 6 (a) shows the average human judgments. As expected, participants in the rotation condition are allowed to use for a class of objects. Figure 6 (b) shows th e model predictions with parameters as Experiment 1 (with T = 0 . 005 ), run for 1000 iterations (with a burn-in of 50 iterations) o n the sets of images (downsampled to 38 by 38 pixels). Qualitative ly, the extended tIBP model has nearly high probability to the Same Size when given the rotation set , an artifact from downsampling. The Spearman X  X  rank order correlation between the model X  X  pred ictions and human judgments is 0.68. Importantly, the model predicts that only when given the rotation set should participants generalize to the New Rot object and only when given the size set should they generalize to the New Size object. In this paper, we presented a solution to how people infer fea ture representations that are invariant over transformations and in two behavioral experiments con firmed two predictions of a new model sketch of a new computational theory of shape representatio n  X  the features representing an object are transformed relative to the object and the set of transfo rmations a feature is allowed to undergo depends on the object X  X  context. In the future, we would like to pursue this theory further, expanding the account of learning the types of transformations and exp loring how the transformations between features in an object interact (we should expect some intera ction due to real world constraints on the transformations, e.g., prospective geometry). Finall y, we hope to include other facets of visual perception into our model, like a perceptually realistic pr ior on feature instantiations and features relations (e.g., the horizontal bar is always ON TOP OF the ve rtical bar).
 [1] S. E. Palmer. Vision Science . MIT Press, Cambridge, MA, 1999. [2] H. Barlow. Unsupervised learning. Neural Computation , 1:295 X 311, 1989. [3] Z. Ghahramani. Factorial learning and the EM algorithm. In Advances in Neural Information [4] T. L. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process. [6] J. Fiser and R. N. Aslin. Unsupervised statistical learn ing of higher-order spatial structures [7] E. Sudderth, A. Torralba, W. Freeman, and A. Willsky. Des cribing visual scenes using trans-[8] E. Mach. The analysis of sensations . Open Court, Chicago, 1914/1959. [9] M. I. Jordan. Bayesian nonparametric learning: Express ive priors for intelligent systems. In [10] F. Wood, T. L. Griffiths, and Z. Ghahramani. A non-parame tric Bayesian method for inferring [11] S. Geman and D. Geman. Stochastic relaxation, Gibbs dis tributions, and the Bayesian restora-[12] G. Orban, J. Fiser, R. N. Aslin, and M. Lengyel. Bayesian learning of visual chunks by human
