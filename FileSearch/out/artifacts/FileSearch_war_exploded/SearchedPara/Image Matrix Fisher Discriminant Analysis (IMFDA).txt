 matrix, at last find the subspace such that in that space the ratio of the between-class less than that of data dimension. Moreover, the conversion from image matrix into 1D classification. representation model: 2D matrix based model was proposed, in which the data is represented by two-dimensional matrix. Matrix based algorithms had been widely IMPCA (Image Matrix Principle Component Analysis), whose aim was to find a projection matrix X such that in the projected space the variance of total projection is maximal. Then Ye J.P. [7,9] adopted the same concept and presented GLRAM (Generalized Low Rank Approximations Matrix), which found two projection matrices L and R by iterative way. From different point of view Shashua A. [8] in principal, optimization problem in GLRAM and TRP is the same. Although these based model. Although TDLDA (Two-Dimensional Linear Discriminant Analysis) uses discriminant information, but it is solved by iterative steps, suffering from heavy computation load. only based on the 2D matrix based model, but also let the projection matrix maximize the ratio of the between-class distance to the within-class distance. We conclude with extensive empirical evaluations of IMFDA, showing its advantage over alternative algorithms in lower test error and shorter running time. proposed algorithm IMFDA. In section 3 we give experimental results. Conclusion can be seen in the last section. IMFDA algorithm, and in the second part we present the retrieval step. 2.1 IMFDA Feature Extraction Step matrix the ratio of the between-class distance to the within-class distance is maximal. To begin with the description of IMFDA, let X  X  fix some notations. Suppose C classes space derived by projection matrix X , the distance of between-class b TS , the distance of within-class w TS and the sum distance t TS are defined as follow: 
The optimal projection matrix X can be found in the following maximization step: 
According to Fukunaga K. [1], eigenvectors calculated from above two equations function in our experiment. As pointed out also in [1], the solutions of the following equations are identical: Total Scatter matrix t G and Image Between-class Scatter matrix b G : 
So IMFDA is also equal to calculate the following equation: 
This is a generalized eigenvalue problem. The optimal projection matrix X can be obtained by calculating the first k largest eigenvalues and corresponding eigenvectors compared with the conventional total scatter covariance matrix. 2.2 IMFDA Retrieval Step Each image ij A is mapped into projection matrix X , and coefficient matrix ij Y is NN classifier: 
That is to say we only need to calculate the distance between different coefficient belongs to. In this section we conducted experiments to test the performance of our algorithm by comparing it with other algorithms, including matrix based algorithms: IMPCA [5] GLRAM [7] and TDLDA [9], and vector based algorithms: PCA [2] and FDA [3]. A NN classifier is employed to classify in the projected feature space. 
The experiments were based on a widely used pattern recognition benchmark subject has ten images with resolution of 112 92  X  . All the images were taken against a conduct experiment ten times and each time randomly select five images of each person for training and the remaining five for testing (training set and testing set both Bernoulli random test and give the average test error number, which is defined as the number of wrong classification. 4.1 Test Error First we give the test error number of matrix based algorithms in Table 1. Because the test error number depends highly on the number of discriminant vector k in projection dimension theory the optimal k is about 1 C  X  (equals to 39 in ORL dataset). 
Algorithm 
From table 1, we can say:  X 
Even when k is small the test error number is low, and when k is above 6, the error largest eigenvalues already take the majority energy of t G [6].  X 
According to test error number IMFDA is the best among 2D matrix based algorithms. One reason is that IMFDA uses discriminant information between different classes, whereas IMPCA and GLRAM are just trying to minimize the reconstruction error. Although 2DLDA algorithm takes advantage of the classification information as IMFDA used, it is difficult for the user to set parameters in TDLDA. The optimal performance of vector based algorithms PCA and FDA is given in Table2. 
Compare Table 1 with Table 2, it is obviously to say, matrix based algorithms can Image Between-class Scatter matrix b G are small, we can calculate corresponding eigenvectors more accurately. Furthermore the defined matrix t G and b G is mainly based on the column of original matrix, so this kind of algorithms are more powerful stable recognition ratio in different face datasets. 4.2 Computation Time Feature extraction and classification time of di fferent algorithms are given in Table 3. 
From Table 3, it can be seen that the matrix based algorithms can always have shorter running time compared with vector based algorithms. There are two reasons: Firstly vector based algorithms have a time-consuming step which converts image to projection matrix in order to get lower test error number. 
According to section 4.1, we know that IMFDA has the lowest test error number, and almost the second shortest running time, only a little longer than IMPCA. 4.3 Storage From Table 1, we can see that we need small k to get satisfactory result for 2D matrix need 92 6  X  times more space for FDA than IMFDA. On the other hand, more storage time. In this paper a novel feature extraction 2D matrix based algorithm IMFDA is two advantages, first it adopt a new 2D matrix model to represent image data, avoiding the time consuming step of co nverting the image matrix into vector, encountered in small sample problem. Second it uses the information between total variance matrix, improves the discriminant performance, so we can conclude that IMFDA is more suitable to image classifica tion compared with other 2D matrix based algorithms, and it will plays a very important role in image retrieval system. 
