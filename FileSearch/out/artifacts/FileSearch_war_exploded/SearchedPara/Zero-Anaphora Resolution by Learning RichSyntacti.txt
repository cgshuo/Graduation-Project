 RYU IIDA, KENTARO INUI, AND YUJI MATSUMOTO Nara Institute of Science and Technology 1. INTRODUCTION Zero-anaphora is a gap in a sentence that has an anaphoric fun ction similar to a pro-form (e.g., pronoun) and is often described as  X  X eferr ing back X  to an ex-pression that supplies the information necessary for inter preting the sentence. For example, in the sentence  X  There are two roads to eternity, a straight and narrow, and a broad and crooked , X  the gaps in  X  a straight and narrow (gap)  X  and  X  a broad and crooked (gap)  X  have a zero-anaphoric relationship to  X  two roads to eternity . X  anaphora resolution , is essential in a wide range of natural language pro-cessing (NLP) applications. This is the case particularly i n a language such as Japanese, where even obligatory arguments of a predicate are often omitted when they are inferable from the context. In fact, in our Japanese newspaper corpus [Iida et al. 2007], for example, 45.5% of th e nominative ar-guments of verbs are omitted. Since such gaps cannot be inter preted only by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be built on top of both syntactic and semantic models .
 research contexts. First, zero-anaphora resolution is stu died in the con-text of anaphora resolution (AR), in which zero-anaphora is regarded as a subclass of anaphora. In AR, the research trend has been shi fting from rule-based approaches [Baldwin 1995; Lappin and Leass 1994 ; Mitkov 1997] to empirical, or corpus-based, approaches [McCarthy and Le hnert 1995; Ge et al. 1998; Soon et al. 2001; Ng and Cardie 2002; Strube and M  X  uller 2003; Yang et al. 2003; Ng 2004; Yang et al. 2005] because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best-performing rule-based systems (see the Coreference t ask in Message Understanding Conference (MUC) 1 and the Entity Detection and Tracking task in the Automatic Content Extraction (ACE) program 2 ). The same trend is observed also in Japanese zero-anaphora resolution, whe re the findings made in rule-based or theory-oriented work [Kameyama 1986; Walker et al. 1994; Nakaiwa and Shirai 1996; Okumura and Tamura 1996; Mura ta and Nagao 1997] have been successfully incorporated in machine -learning-based frameworks [Seki et al. 2002; Iida et al. 2003; Isozaki and Hi rao 2003]. Propbank 2 -style semantic role labeling (SRL), which has been intensi vely studied, for example, in the context of the CoNLL SRL task. 3 In this task, given a sentence  X  X o attract younger listeners, Radio Free E urope intersperses the latest in Western rock groups, X  an SRL model is asked to id entify the Noun Phrase (NP) Radio Free Europe as the A0 (Agent) argument of the verb attract . This can be seen as the task of finding the zero-anaphoric rel ationship between a nominal gap (the A0 argument of attract ) and its antecedent ( Radio Free Europe ) under the condition that the gap and its antecedent appear i n the same sentence.
 findings that have yet to be exchanged between them, partly be cause the two fields have been evolving somewhat independently. The AR com munity has recently made two important findings:  X  X  model that identifies the antecedent of an anaphor by a seri es of com-parisons between candidate antecedents has a remarkable ad vantage over a model that estimates the absolute likelihood of each candi date indepen-dently of other candidates [Iida et al. 2003; Yang et al. 2003 ].  X  X n AR model that carries out antecedent identification before anaphoricity determination, the decision whether a given NP is anaphoric or not (i.e., discourse-new), significantly outperforms a model that exe cutes those sub-tasks in the reverse order or simultaneously [Poesio et al. 2 004; Iida et al. 2005].
 To our best knowledge, however, existing SRL models do not ex ploit these ad-vantages. In SRL, on the other hand, it is common to use syntac tic features derived from the parse tree of a given input sentence for argu ment identi-fication. A typical syntactic feature is the path on a parse tr ee from a tar-get predicate to a noun phrase in question [Gildea and Jurafs ky 2002; Car-reras and Marquez 2005]. However, existing AR models deal wi th intra-and intersentential anaphoric relations in a uniform manner; t hat is, they do not use as rich syntactic features as state-of-the-art SRL mode ls do, even in finding intrasentential anaphoric relations. We believe that the A R and SRL commu-nities can learn more from each other.
 mentioned techniques derived from each research trend make s significant im-pact on zero-anaphora resolution, taking Japanese as a targ et language. More specifically, we demonstrate the following:  X  X ncorporating rich syntactic features in a state-of-the-art AR model dramat-ically improves the accuracy of intrasentential zero-anap hora resolution, which consequently improves the overall performance of zer o-anaphora res-olution. This is to be considered as a contribution to AR rese arch.  X  X nalogously to intersentential anaphora, decomposing th e antecedent iden-tification task into a series of comparisons between candida te antecedents works remarkably well also in intrasentential zero-anapho ra resolution. We hope this finding will be adopted in SRL.
 task definition of zero-anaphora resolution in Japanese. In Section 3, we re-view previous approaches to AR. Section 4 described how the p roposed model incorporates effectively syntactic features into the mach ine-learning-based approach. We then report the results of our experiments on Ja panese zero-anaphora resolution in Section 5 and conclude in Section 6. 2. ZERO-ANAPHORA RESOLUTION In this article, we consider only zero-pronouns that functi on as an obligatory argument of a predicate for two reasons:  X  X roviding a clear definition of zero-pronouns appearing in adjunctive argu-ment positions involves awkward problems, which we believe should be post-poned until obligatory zero-anaphora is well studied.  X  X esolving obligatory zero-anaphora tends to be more impor tant than adjunc-tive zero-pronouns in actual applications.
 A zero-pronoun may have its antecedent in the discourse; in t his case, we say the zero-pronoun is anaphoric . On the other hand, a zero-pronoun whose referent does not explicitly appear in the discourse is call ed a nonanaphoric zero-pronoun. A zero-pronoun may be nonanaphoric typicall y when it refers to an extralinguistic entity (e.g., the first or second perso n) or its referent is unspecified in the context.
 anaphoric as its antecedent,  X  shusho (prime minister), X  appears in the same sentence. In sentence (2), on the other hand,  X  j is considered nonanaphoric if its referent (i.e., the first person) does not appear in the di scourse. (1) shusho i -wa houbeisi-te , ryoukoku-no gaikou-o (2) (  X  j -ga ) ie-ni kaeri-tai . the combination of two subproblems, antecedent identificat ion and anaphoric-ity determination, which is analogous to NP-anaphora resol ution: 3. PREVIOUS WORK Anaphora resolution can be decomposed into two subtasks; an tecedent identi-fication and anaphoricity determination. In this section, w e briefly show the previous work of each task respectively. In addition, we sum marize related work that incorporates syntactic information into anaphor a resolution. 3.1 Antecedent Identifi cation Previous machine-learning-based approaches to anteceden t identification can be classified as either the candidate-wise classification approach or the preference-based approach. In the former approach [Soon et al. 2001; Ng and Cardie 2002], given a target anaphor, TA , the model estimates the absolute likelihood of each of the candidate antecedents (i.e., the N Ps preceding TA ), and selects the best-scored candidate. If all the candidate s are classified nega-tive, TA is judged nonanaphoric.
 2003] decomposes the task into comparisons of the preferenc e between can-didates and selects the most preferred one as the antecedent . For example, Iida et al. [2003] proposed a method called the tournament model . This model conducts a tournament consisting of a series of matches in wh ich candidate antecedents compete with each other for a given anaphor.
 single candidate independently of others, the tournament m odel learns the relative preference between candidates, which is empirica lly proved to be a significant advantage over candidate-wise classification [ Iida et al. 2003]. 3.2 Anaphoricity Determination There are two alternative ways for anaphoricity determinat ion: the single-step model and the two-step model . The single-step model [Soon et al. 2001; Ng and Cardie 2002] determines the anaphoricity of a given anap hor indirectly as a by-product of the search for its antecedent. If an approp riate candidate antecedent is found, the anaphor is classified as anaphoric; otherwise, it is classified as nonanaphoric. One disadvantage of this model i s that it cannot employ the preference-based model because the preference-based model is not capable of identifying nonanaphoric cases.
 other hand, carries out anaphoricity determination in a sep arate step from antecedent identification. Poesio et al. [2004] and Iida et a l. [2005] claimed that the latter subtask should be done before the former. For example, given a target anaphor ( TA ), Iida et al. X  X  selection-then-classification (SCM) model: (1) selects the most likely candidate antecedent ( CA ) of TA using the tourna-(2) classifies TA paired with CA as either anaphoric or nonanaphoric using The anaphoricity determination model learns the nonanapho ric class directly from nonanaphoric training instances whereas the single-s tep model cannot use nonanaphoric cases in training. 4. PROPOSAL In this section, we decompose the zero-anaphora resolution the problem into two subtasks, intrasentential and intersentential zero-anaphora resolution and then we propose a method that incorporates syntactic patter ns as features into an intrasentential problem. 4.1 Task Decomposition We approach the zero-anaphora resolution problem by decomp osing it into two subtasks: intrasentential and intersentential zero-a naphora resolution. For the former problem, syntactic patterns in which zero-pr onouns and their antecedents appear may well be useful clues, which, however , does not ap-ply to the latter problem. We therefore build a separate comp onent for each subtask, adopting Iida et al. [2005] X  X  selection-then-cla ssification model for each component: (1) Intrasentential antecedent identification . For a given zero-pronoun Z P in (2) Intrasentential anaphoricity determination . Estimate plausibility p 1 that (3) Intersentential antecedent identification . Select the most-likely candidate (4) Intersentential anaphoricity determination . Estimate plausibility p 2 that 4.2 Representation of Syntactic Patterns In the first two of the above four steps, we use syntactic patte rn features. Anal-ogously to SRL, we extract the parse path between a zero-pron oun to its an-tecedent to capture the syntactic pattern of their occurren ce. Among many alternative ways of representing a path, in the experiments reported in the next section, we adopted a method as we will describe, leavin g the exploration of other alternatives as future work.
 [Kudo and Matsumoto 2002], to obtain the dependency parse tr ee, in which words are structured according to the dependency relation d efined in the Kyoto Corpus 4 [Kurohashi and Nagao 1997]. Figure 1(a), for example, shows the dependency tree of sentence (1) in Section 2. In the figure , a content word node depends on the other content word node, sometimes invol ving functional word nodes. The node labeled  X  X dnom X  denotes the adnominal r elation between suishinsuru (promote) and houshin (plan) and the  X   X   X  node denotes a zero-pronoun. We then extract the path between a zero-pron oun and its antecedent as in Figure 1(b). Finally, to encode the order of siblings and reduce data sparseness, we further transform the extracted path as in Figure 1(c):  X  X  path is represented by a subtree consisting of backbone no des:  X  (zero-pronoun), Ant (antecedent), Node (the lowest common ancestor), LeftNode (node corresponding to a left-branch content word), and RightNode .  X  X ach backbone node has daughter nodes, each corresponding to a function word associated with it.  X  X ontent words are deleted.

This way of encoding syntactic patterns is used in intrasent ential anaphoric-ity determination. Our intrasentential anaphoricity dete rmination model is trained by providing a set of labeled trees as a training set, where a label is either anaphoric or nonanaphoric . Each labeled tree consists of the path tree shown in Figure 1(c) and the node corresponding to the bi nary features in Table I, each of which is linked to the root node as illustra ted in Figure 2. This way of organizing a labeled tree allows the model to lear n, for example, the combination of a subtree of T C and some of the binary features.
Analogously, in antecedent identification, the tournament model allows us to incorporate three paths, a path for each pair of a zero-pro noun and the left and right candidate antecedents. The model is trained by pro viding a set of labeled trees as a training set. A label is either left or right , indicating which candidate is more likely to be an antecedent than the other co mpeting candi-date. Each labeled tree has (1) path trees T L , T R , and T I (as given in Figure 3) and (2) a set of nodes corresponding to the binary features su mmarized in Table I, each of which is linked to the root node as illustrate d in Figure 4. ister) and ryoukoku (both countries), for the zero-pronoun  X  in Figure 1(a). For example, in order to represent the relation between the left candidate and the zero-pronoun, a path linking shusho to  X  is extracted and then it is trans-formed into a syntactic pattern as in T L in Figure 3. The patterns between the right candidate and the zero-pronoun and ones between two ca ndidates are also created by the same way ( T R and T I in Figure 3). Note that the label of each node is prefixed either with L , R or I to indicate which node belongs to which subtree.
 effectively works for this kind of problem and has been paid a ttention by sev-eral research groups [Hobbs 1978; Luo and Zitouni 2005; Yang et al. 2006]. If we take English as a target language, it is reasonable to de al with syntac-tic trees. In contrast, languages like Japanese involve gra mmatical properties such as word scrambling (e.g., objects followed by subjects ), posing an obstacle to Japanese phrase structure analysis, so it is still difficu lt to construct phrase structure parsers. We, therefore, use a dependency tree in o rder to represent the structure of a sentence. 4.3 Learning Algorithm As noted in Section 1, the use of zero-pronouns in Japanese is relatively less constrained by syntax compared, for example, with English. This forces the above way of encoding path information to produce an explosi ve number of different paths, which inevitably leads to serious data spa rseness. vised a range of variants of the standard path representatio n to reduce the complexity [Carreras and Marquez 2005]. Applying kernel me thods such as tree kernels [Collins and Duffy 2001] and hierarchical DAG k ernels [Suzuki et al. 2003] is another strong option. The boosting-based al gorithm pro-posed by Kudo and Matsumoto [2004] is designed to learn subtr ees useful for classification.
 iments, we have so far examined Kudo and Matsumoto [2004] X  X  a lgorithm, which is implemented as the BACT system. 5 Given a set of training instances, each of which is represented as a tree labeled either positiv e or negative, the BACT system learns a list of weighted decision stumps with a b oosting algo-rithm. Each decision stamp classifier is represented as a lab eled ordered tree appearing in the training instances.
 x i  X  X is a labeled ordered tree and y i  X  { X  1 } is a class label associated with each training data item. In each iteration of boosting, the d ecision stumps are data {h x i , y i i} L i =1 : weight of each instance, and h h t , y i ( x ) is a decision stump classifier given by At the classification step, we use the following mapping func tion: rithm,  X  k is calculated based on a variant of boosting algorithm Arc-GV (see Breiman [1999]).
 (anaphoric) training trees and a set of negative (nonanapho ric) training trees, BACT induces a set of subtrees (decision stumps) that are use ful for our bi-nary classification. Each subtree is associated with weight w  X  [  X  1 , 1] as shown in Table II and Table III. A subtree with a positive weig ht corresponds to a decision stump that votes for anaphoric . In the test phase, given an input tree involving a zero-pronoun and the most-likely candidat e antecedent, BACT computes the score by summing up the weights of all the decisi on stumps that match with the input tree and concludes anaphoric if the score is positive, or nonanaphoric otherwise. Each choice between left or right in the tourname nt model for antecedent identification is carried out analogou sly.
 learning trees are more human readable than those learned fr om algorithms such as Support Vector Machines, because the result of each i teration is given as a pair of decision stumps h h t , y i and weight  X  k as shown in Table II and Table III. So, we can easily interpret what kinds of subtrees or features are useful for classification by viewing the weights of induced d ecision stumps. 5. EXPERIMENTS We conducted an evaluation of our method using Japanese news paper articles. The following four models were compared: (1) BM : We employ Ng and Cardie X  X  [2002] model, as a baseline model. It (2) BM SYN : BM with the syntactic features such as those in Figure 1(c). (3) SCM : The selection-then-classification model explained in Sec tion 3. (4) SCM SYN : SCM with all types of syntactic features shown in Figure 3. 5.1 Setting We are now developing the corpus annotated with predicate-a rgument and coreference relations, which is named the NAIST Text Corpus [Iida et al. 2007]. 6 Unfortunately, the NAIST Text Corpus still contains many un reli-able tags. Therefore, we limit experiments to include only a reliable data set: 287 articles annotated by two annotators and 60 by one. The ag reement ra-tio between two annotators on the 287 articles was 84.0%, whi ch indicated that the annotation was sufficiently reliable. We discarded from this data set the zero-pronouns for which the two annotators disagreed. C onsequently, the data set contained 1,384 intrasentential anaphoric zero-p ronouns, 1,128 in-tersentential anaphoric zero-pronouns, and 784 nonanapho ric zero-pronouns (3,306 zero-pronouns in total), with each anaphoric zero-p ronoun annotated to be linked to its antecedent. For each experiment, we used 1 37 articles for training, 60 articles for optimizing  X  intra (threshold parameter of intrasenten-tial zero-anaphora resolution), and 150 articles for testi ng.
 All the features were automatically acquired with the help o f the following NLP tools: the Japanese morphological analyzer ChaSen 7 and the Japanese dependency structure analyzer CaboCha , 8 which also carried out named-entity chunking. 5.2 Results on Intrasentential Zero-Anaphora Resolution In both intra-anaphoricity determination and antecedent i dentification, we investigated the effect of introducing the syntactic featu res for improving the performance. First, the results of antecedent identific ation are shown in Table IV. The comparison between BM (SCM) with BM SYN (SCM SYN) in-dicates that introducing the structural information effec tively contributes to this task. In addition, the large improvement from BM SYN to SCM SYN indicates that the use of the preference-based model has sig nificant impact on intrasentential antecedent identification. This finding may well contribute to semantic role labeling because these two tasks have a larg e overlap as dis-cussed in Section 1.

Second, to evaluate the performance of intrasentential zer o-anaphora res-olution, we plotted recall-precision curves altering thre shold parameter and  X  inter for intra-anaphoricity determination as shown in Figure 5, where recall R and precision P were calculated by: The curves indicate the upper bound of the performance of the se models; in practical settings, the parameters have to be trained befor ehand. indicates that incorporating syntactic pattern features w orks remarkably well for intrasentential zero-anaphora resolution. Futhermor e, SCM SYN is sig-nificantly better than BM SYN. This is because the former has an advan-tage of learning nonanaphoric zero-pronouns as negative tr aining instances in intrasentential anaphoricity determination, which ena bles it to reject nonanaphoric zero-pronouns more accurately than the other s.
 automatically estimated with the development data set. The result of each model is shown in Table V. Note that these thresholds are chos en when the F-measure of each model is maximized. Table V shows that the S CM SYN achieved the best performance among them, indicating that o ur model has reasonable performance in practical situations.
 manually analyzing the decision stumps (syntactic pattern s) induced by BACT. The extracted patterns for antecedent identification and an aphoricity determi-nation are shown in Table II and Table III respectively. Tabl e II shows some patterns used in antecedent identification; the patterns wi th positive weight supports the left candidate of the two competitors. Table II I shows some pat-terns used in anaphoricity determination; the patterns wit h positive weight supports that a candidate anaphor is anaphoric.
 a predicate that has a zero-pronoun, and ANT node denotes a candidate an-tecedent. For example, Pattern No. 874 from Table II is depic ted in Figure 6. In this figure the node prefix L indicates a pattern taken from the left an-tecedent path, likewise, R indicates a pattern from the right antecedent path. The current left candidate (L ANT) and right candidate (R ANT) are not ex-plicitly present in this decision stump. However, as L ANT must be in the left candidate path, it should be a descendant of the L B node. R ANT must be to the right of L ANT, so it should be a descendant of either the L B or L P nodes as the two graphs in Figure 6 show. Since it is given a neg ative weight, this pattern favors the left candidate. Interestingly enou gh, this rule can be interpreted linguistically as follows.

Zero-anaphoric phenomena have been extensively researche d in the last two decades in the area of Japanese linguistics. Minami [1974], for example, man-ually examined what patterns between two predicates are hel pful for inter-preting zero-anaphoric relations. Minami argued that  X  X f t wo predicates are connected with the conjunctive particle te , the nominative arguments of the two predicates tend to be shared in the same sentence X  as the u seful patterns. Also, Nariyama [2002] introduced such patterns into her cen tering-based al-gorithm to zero-anaphora resolution and showed that the pat terns argued by Minami contributes to improving performance quantitative ly in her manual evaluation. Note that the pattern described by Minami loose ly corresponds to Pattern No. 874 described above. It is indirect evidence tha t learning syn-tactic patterns by BACT contributes to improving the perfor mance of both an-tecedent identification and anaphoricity determination. 5.3 Discussion Our error analysis reveals that a majority of errors can be at tributed to the cur-rent way of handling quoted phrases and sentences. Figure 7 s hows the differ-ence in resolution accuracy between zero-pronouns appeari ng in a quotation (185 zero-pronouns) and the rest (871 zero-pronouns), wher e  X  X N Q X  denotes the former (in-quote zero-pronouns) and  X  X UT Q X  the latter. The accuracy on the IN Q problems is considerably lower than that on the OUT Q cases, which indicates that we should deal with in-quote cases with a sepa rate model so that it can take into account the nested structure of discourse se gments introduced by quotations.

To investigate the effect by separating in-quote zero-pron ouns from out-quote ones, we trained two cases independently for the SCM mo del using syn-tactic pattern features. At the test phase, the system utili zes either a model that was trained from in-quote instances or one from out-quo te cases depend-ing on the context. Figure 8 shows recall-precision curves o f such a separate model and the original SCM model. Figure 8 shows that the sepa rate model does not work compared with the original one. It is because th e relationship between a zero-pronoun in a quoted sentence and its antecede nt is too compli-cated to automatically extract effective syntactic patter ns. As quote sentences, speakers are able to utter anything as long as the utterance i s coherent to the preceding context. Thus, quotes can contain various typ es of expressions; word, clause, sentence, or text, causing severe data sparse ness problems. To avoid them, we need to adapt the current model to each situati on appearing in an in-quote zero-pronoun and its antecedent. 5.4 Impact on Overall Zero-Anaphora Resolution We next evaluated the effects of introducing the proposed mo del on overall zero-anaphora resolution including intersentential case s.
 intrasentential zero-anaphora and intersentential zero-anaphora simultane-ously with no syntactic pattern features. Here, we adopted S upport Vector Ma-chines [Vapnik 1998] to train the classifier on the baseline m odel and the inter-sentential zero-anaphora resolution in the SCM using struc tural information. lecting different value for threshold parameters  X  intra and  X  inter . The results are shown in Figure 9, which indicates that the proposed mode l significantly outperforms the original SCM if  X  intra is appropriately chosen.
 ting the area under the curve (AUC) values for different  X  intra values. Here, each AUC value is the area under a recall-precision curve. Th e results are shown in Figure 10. Since the original SCM does not use  X  intra , the AUC value of it is constant, depicted by the SCM. As shown in Figure 10, t he AUC-value curve of the proposed model does not have a well-defined peak, which indicates the selection of parameter  X  intra is not difficult. 6. CONCLUSION In intrasentential zero-anaphora resolution, syntactic p atterns of the ap-pearance of zero-pronouns and their antecedents are useful clues. Taking Japanese as a target language, we have empirically demonstr ated that incor-porating rich syntactic pattern features in a state-of-the -art learning-based anaphora resolution model dramatically improves the accur acy of intrasenten-tial zero-anaphora, which consequently improves the overa ll performance of zero-anaphora resolution. We approach the zero-anaphora r esolution problem by decomposing it into two subtasks: intrasentential and in tersentential zero-anaphora resolution. We built a separate component for each subtask, adopt-ing Iida et al. [2005] X  X  selection-then-classification mod el. In intrasentential zero-anaphora resolution, the model treats syntactic patt erns as features for both antecedent identification and anaphoricity determina tion.
 pronouns, which requires us to design a broader framework th at allows zero-anaphora resolution to interact with predicate-argument s tructure analysis. With regard to this problem, we need a verb dictionary such as VerbNet [Kip-per 2005] or FrameNet [Baker et al. 1998] for verb sense disam biguation. In the past decade, automatic verb frame construction methods have received in-creasing attention [Resnik 1993; Utsuro and Matsumoto 1997 ; Kawahara et al. 2000; Gildea 2002, etc.], which cluster verbs and arguments based on the sim-ilarity between instances in corpora. By adopting such a str ategy, we obtain a scalable frame dictionary constructed automatically fro m large amounts of text data such as that available on the Web. Of course, dictio naries that are automatically constructed are noisy and not always fine grai ned; however, we are convinced that it is beneficial to introduce dictionarie s into the process of anaphora resolution in light of these issues.
 set of zero-anaphora resolution problems in a given discour se. In the problem of zero-anaphora resolution, there are often more than one z ero-pronoun for a given predicate. Thus, we have to consider the interdepende ncy between zero-pronouns. For example, the model must resolve zero-anaphor ic relations with the constraints that an NP which is the nominative case for a g iven predicate does not become the dative case. This issue leads us to explor e methods as discussed by McCallum and Wellner [2003].
 portance. Although using syntactic patterns improves the a ccuracy of both anaphoricity determination and antecedent identification , it involves a higher cost in matching between an input tree and each of its decisio n stumps. Ef-ficient computation is beyond the scope of this article, but r educing computa-tional cost without decreasing performance is an important issue in this area.
