 1. Introduction
Cluster analysis is a data analysis tool used to group data with similar characteristics. It has been used in data mining tasks such as unsupervised classification and data summation, as well as segmentation of large heterogeneous data sets into smaller homogeneous subsets that can be easily managed, separately modeled ter analysis techniques have been used in many areas such as manufacturing, medicine, nuclear science, radar scanning and research and development planning. For example, Jiang et al. [13] analyze a variety of cluster techniques for complex gene expression data. Wu et al. [40] develop a clustering algorithm specifically designed to handle the complexities of gene data that can estimate the correct number of clusters and find them. Wong et al. [39] present an approach used to segment tissues in a nuclear medical imaging method known as positron emission tomography (PET). Mathieu and Gibson [26] use cluster analysis as a part of a decision support tool for large-scale research and development planning to identify programs to participate in and to determine resource allocation. Finally, Haimov et al. [8] use cluster analysis to segment radar signals in scanning land and marine objects.
A problem with many of the clustering methods and applications mentioned above is that they are appli-cable for clustering data having numerical values for attributes. Most of the work in clustering is focused on position of the numerical data. Unlike numerical data, categorical data have multi-valued attributes. Thus, similarity can be defined as common objects, common values for the attributes, and the association between co-occurrences (common values for the attributes) can be examined [40] .
 A number of algorithms for clustering categorical data have been proposed including work by Huang [12] , contributions to the issue of clustering categorical data, they are not designed to handle uncertainty in the clustering process. This is an important issue in many real world applications where there is often no sharp boundary between clusters. Recently, there has been work in the area of applying fuzzy sets in clustering cat-egorical data including work by Huang [12] and Kim et al. [16] . However, these algorithms require multiple runs to establish the stability needed to obtain a satisfactory value for one parameter used to control the mem-bership fuzziness.

Therefore, there is a need for a robust clustering algorithm that can handle uncertainty in the process of clustering categorical data. This research proposes a clustering algorithm based on Rough Set Theory (RST). The proposed algorithm, named Min X  X in-Roughness (MMR), is designed to deal with uncertainty in the process of clustering categorical data. In addition, the algorithm is implemented and tested with three real world data sets. To compare the algorithm performance in handling uncertainty, Soybean and Zoo data sets are used and the results are compared with fuzzy set theory based algorithms (including K -modes, fuzzy
K -modes and fuzzy centroids). To test the applicability to large scale date sets, the Mushroom data set is used and the results are compared with Squeezer, K -modes and LCBCDC, as well as ROCK and a traditional hier-archical algorithm. The contributions of our proposed approach include: (1) Unlike previous methods, MMR gives the user the ability to handle uncertainty in the clustering process. (2) Using MMR, the user is able to obtain stable results given only one input: the number of clusters. (3) MMR has the capability of handling large data sets.

This paper is structured as follows: Section 2 presents an overview of standard clustering methods existing in the literature. In Section 3 , the basics of the rough set theory are introduced followed by the proposed
MMR algorithm. A synthetic data set is used to illustrate the MMR algorithm. Section 4 discusses the imple-mentation of the algorithm and the results from the application of the algorithm on Soybean, Zoo and Mush-room data sets (from the UCI Machine Learning Repository 1 analyzed. Section 5 presents conclusions and identifies future research directions. 2. Literature review In this section, an overview of methods available in the literature to cluster categorical data is presented.
Ralambondrainy [33] proposes a method to convert multiple category attributes into binary attributes using 0 and 1 to represent either a category absence or presence, and to treat the binary attributes as numeric in the
K -means algorithm. Dempster et al. [2] presents a partitional clustering method, called the Expectation-Max-imization (EM) algorithm. EM first randomly assigns different probabilities to each class or category, for each cluster. These probabilities are then successively adjusted to maximize the likelihood of the data given the specified number of clusters. Since the EM algorithm computes the classification probabilities, each observa-tion belongs to each cluster with a certain probability. The actual assignment of observations to a cluster is determined based on the largest classification probability. After a large number of iterations, EM terminates database based on an association rule hypergraph. A hypergraph is used as a model for relatedness. The overlap amongst them. However, this assumption may not hold in practice as transactions in different clusters may have a few common items. K -modes [12] extends K -means and introduces a new dissimilarity measure for categorical data. The dissimilarity measure between two objects is calculated as the number of attributes whose values do not match. The K -modes algorithm then replaces the means of clusters with modes, using a frequency based method to update the modes in the clustering process to minimize the clustering cost func-tion. One advantage of K -modes is it is useful in interpreting the results [12] . However, K -modes generates local optimal solutions based on the initial modes and the order of objects in the data set. K -modes must be run multiple times with different starting values of modes to test the stability of the clustering solution.
Huang [12] also proposes the K -prototypes algorithm, which allows clustering of objects described by a com-bination of numeric and categorical data. CACTUS (Clustering Categorical Data Using Summaries) [4] is a summarization based algorithm. In CACTUS, the authors cluster for categorical data by generalizing the def-inition of a cluster for numerical attributes. Summary information constructed from the data set is assumed to be sufficient for discovering well-defined clusters. CACTUS finds clusters in subsets of all attributes and thus performs a subspace clustering of the data. Guha et al. [6] propose a hierarchical clustering method termed
ROCK (Robust Clustering using Links), which can measure the similarity or proximity between a pair of objects. Using ROCK, the number of  X  X  X inks X  X  are computed as the number of common neighbors between two objects. An agglomerative hierarchical clustering algorithm is then applied: first, the algorithm assigns each object to a separate cluster, clusters are then merged repeatedly according to the closeness between clus-ters, where the closeness is defined as the sum of the number of  X  X  X inks X  X  between all pairs of objects. Gibson et al. [5] propose an algorithm called STIRR (Sieving Through Iterated Relational Reinforcement), a general-ized spectral graph partitioning method for categorical data. STIRR is an iterative approach, which maps cat-egorical data to non-linear dynamic systems. If the dynamic system converges, the categorical data can be clustered. Clustering naturally lends itself to combinatorial formulation. However, STIRR requires a non-triv-clusters are not discovered by STIRR [4] . Moreover, Zhang et al. [41] argue that STIRR cannot guarantee convergence and therefore propose a revised dynamic system algorithm that assures convergence. He et al. [11] propose an algorithm called Squeezer, which is a one-pass algorithm. Squeezer puts the first-tuple in a cluster and then the subsequent-tuples are either put into an existing cluster or rejected to form a new cluster based on a given similarity function. He et al. [10] explore categorical data clustering (CDC) and link cluster-ing (LC) problems and propose a LCBCDC (Link Clustering Based Categorical Data Clustering), and com-pare the results with Squeezer and K -mode. In reviewing these algorithms, some of the methods such as
STIRR and EM algorithms cannot guarantee the convergence while others have scalability issues. In addition, all of the algorithms have one common assumption: each object can be classified into only one cluster and all objects have the same degree of confidence when grouped into a cluster [7] . However, in real world applica-tions, it is difficult to draw clear boundaries between the clusters. Therefore, the uncertainty of the objects belonging to the cluster needs to be considered.

One of the first attempts to handle uncertainty is fuzzy K -means [34] . In this algorithm, each pattern or object is allowed to have membership functions to all clusters rather than having a distinct membership to exactly one cluster. Krishnapuram and Keller [18] propose a probabilistic approach to clustering in which the membership of a feature vector in a class has nothing to do with its membership in other classes and modified clustering methods are used to generate membership distributions. Krishnapuram et al. [17] pres-ent several fuzzy and probabilistic algorithms to detect linear and quadratic shell clusters. Note the initial work in handling uncertainty was based on numerical data. Huang [12] proposes a fuzzy K -modes algorithm with a new procedure to generate the fuzzy partition matrix from categorical data within the framework of the fuzzy K -means algorithm. The method finds fuzzy cluster modes when a simple matching dissimilarity measure is used for categorical objects. By assigning confidence to objects in different clusters, the core and boundary objects of the clusters can be decided. This helps in providing more useful information for dealing with boundary objects. More recently, Kim et al. [16] have extended the fuzzy K -modes algorithm by using fuzzy centroids to represent the clusters of categorical data instead of the hard-type centroids used in the fuzzy K -modes algorithm. The use of fuzzy centroids makes it possible to fully exploit the power of fuzzy sets in representing the uncertainty in the classification of categorical data. However, fuzzy K -modes and fuzzy centroids algorithms suffer from the same problem as K -modes, that is they require multiple runs with have to adjust one control parameter for membership fuzziness to obtain better solutions. This necessitates the effort for multiple runs of these algorithms to determine an acceptable value of this parameter. There-fore, there is a need for a categorical data clustering method, having the ability to handle uncertainty in the clustering process while providing stable results. One methodology with potential for handling uncertainty is
Rough Set Theory (RST) which has received considerable attention in the computational intelligence liter-ature since its development by Pawlak in the 1980s. Unlike fuzzy set based approaches, rough sets have no requirement on domain expertise to assign the fuzzy membership. Still, it may provide satisfactory results for rough clustering. The objective of this research is to develop a rough set based approach for categorical data clustering. The approach, termed Min X  X in-Roughness (MMR), is presented and its performance is evaluated on large scale data sets. 3. Min X  X in-Roughness (MMR) algorithm 3.1. Nomenclature U universe or the set of all objects ( x 1 , x 2 , ... )
X subset of the set of all objects, ( X U ) x i object belonging to the subset of the set of all objects, x
A the set of all attributes (features or variables) a i attribute belonging to the set of all attributes, a i 2 A
V ( a i ) set of values of attribute a i (or called domain of a B non-empty subset of A ( B A ) X B lower approximation of X with respect to B X B upper approximation of X with respect to B
R
Rough a MR ( a i ) minimum roughness of attribute a i MMR minimum of MR of all attributes
Ind( B ) indiscernibility relation [ x ] Ind( B ) equivalence class of x i in relation Ind( B ), also known as elementary set in B . 3.2. Rough Set Theory (RST)
RST is an approach to aid decision making in the presence of uncertainty [30,31] . It classifies imprecise, uncertain or incomplete information expressed in terms of data acquired from experience. In RST, a set of all similar objects is called an elementary set, which makes a fundamental atom of knowledge [29] . Any union inition, each rough set has boundary-line elements. For example, some elements cannot be definitively classi-fied as members of the set or its complement. In other words, when the available knowledge is employed, boundary-line cases cannot be properly classified. Therefore, rough sets can be considered as uncertain or imprecise. Upper and lower approximations are used to identify and utilize the context of each specific object and reveal relationships between objects. The upper approximation includes all objects that possibly belong to the concept while the lower approximation contains all objects that surely belong to the concept. attribute values and U  X  A ! V be an information function.

Definition 1 ( Indiscernibility relation (Ind(B ))). Ind( B )is a relation on U . Given two objects, x indiscernible by the set of attributes B in A , if and only if a ( x if and only if " a 2 B where B A , a ( x i )= a ( x j ).
Definition 2 ( Equivalence class  X  X  x i Ind  X  B  X   X  ). Given Ind( B ), the set of objects x of attributes in B consists of an equivalence classes, [ x B .
 imation of X is defined as the union of all the elementary sets which are contained in X . That is, imation of X is defined as the union of the elementary sets which have a non-empty intersection with X . That is,
Definition 5 ( Roughness ). The ratio of the cardinality of the lower approximation and the cardinality of the upper approximation is defined as the accuracy of estimation, which is a measure of roughness. It is presented as If R B ( X )=0, X is crisp with respect to B, in other words, X is precise with respect to B .If R rough with respect to B , that is, B is vague with respect to X . It is this measure, roughness, which allows an object to belong to a cluster with different degrees of belonging using the MMR algorithm. In other words,
MMR has the ability to deal with uncertainty with the calculation of lower bound and upper bound that gives a degree of belonging rather than having the same degree of belonging to all objects.

Recently, RST has found many different applications. Examples include semiconductor manufacturing few. RST has been used extensively for supervised learning with a focus on classification problems, where prior group membership is known. Results generated usually are rules for group membership [32] . Clustering within the context of RST is attracting increasing interest. Lingras [22] explores how to use a rough set genome to represent a rough set theoretic classification scheme. Later, the modified K -means algorithm [24] and Koho-nen Neural Network [23] are proposed to create intervals of clusters based on RST. Furthermore, considering the possibility that one object may belong to more than one cluster, Lingras et al. [25] investigate three meth-odologies (genetic algorithms, K -means and Kohonen Self-Organizing Maps) for clustering based on the prop-erties of rough sets for developing the lower-upper bound representation of the clusters. Voges et al. [37] propose a technique called rough clustering, which is a simple extension of RST, and apply it to the problem of market segmentation. However, the majority of research in exploring RST for clustering aims to handle numerical data sets where distance can be easily derived from the data set. Instead of defining distance between objects, MMR is developed for categorical data based on the concept of roughness. 3.3. Min X  X in-Roughness (MMR)
Mazlack et al. [27] attempt to use RST for choosing partitioning attributes for clustering. They use a mea-sure called total roughness to determine the crispness of the partition. However, for partitioning, the method starts with binary valued attributes and uses the total roughness criterion only for multi-valued attributes.
This creates from a handicap due to the fact that the partitioning is done on a binary attribute even though the total roughness for a multi-valued attribute is lower. MMR overcomes this drawback by clustering the objects on all attributes. In addition, MMR proposes a new way to measure data similarities based on the roughness concept. MMR utilizes a measure termed mean roughness comparable to that proposed by Maz-lack et al. [27] based on RST. This is reproduced below:
Definition 6 ( Mean roughness [27] ). Given a i 2 A , V ( a of objects having one specific value, a , of attribute a i approximation, and X a as the roughness of X with respect to { a j }, that is
Let j V ( a i ) j be the number of values of attributes a defined as where a i , a j 2 A and a i 5 a j .
 Next, Min-Roughness (MR) and Min X  X in-Roughness (MMR) developed in this research are introduced.
Definition 7 ( Min-Roughness (MR) ). Given n attributes, MR, min-roughness of attribute a the minimum of the mean roughness, that is, Definition 8 ( Min X  X in-Roughness (MMR) ). Given n attributes, the MMR is defined as the minimum of the Min-Roughness of the n attributes. That is,
The lower the mean roughness is, the higher the crispness of the clustering. Min-Roughness (MR) ( Defini-on the attributes. The MMR algorithm (as shown in Table 1 ) iteratively divides the group of objects with the goal of achieving better clustering crispness. The algorithm takes the number of clusters, k , as one input and will terminate when this pre-defined number k , is reached.

Next, we present an illustrative example of the MMR algorithm. [ Illustrative Example] : Table 2 introduces a data set (I) used to illustrate the application of the MMR algorithm. There are ten objects ( m = 10) and six attributes ( n = 6). The maximum number of values is 4 ( l = 4). Our interest is to create clusters of similar objects. As seen from the data set in Table 2 , vari-ables can be multi-valued. That is, the domain of an attribute can contain more than two distinct values.

First, the mean roughness on each attributes a i ( i =1, ... , 6) is calculated. Let us take attribute a sets for a 1 : X ( a 1 = Small) = {3,5,7,8}, X ( a 1 = Medium) = {2,4,10} and X ( a four elementary sets for a 2 : X ( a 2 = Blue) = {1,4}, X ( a
X ( a 2 = Green) = {6,9,10}. According to Definition 3 and 4 , the lower approximation of X ( a {3,5,7,8} and the upper approximation is the same, the lower approximation of X ( a and the upper approximation is {1,2,4,6,9,10}, the lower approximation of X ( a is no need of calculating the upper approximation. According to Definition 6 , the mean roughness on a respect to { a 2 } is 0.6111. Following the same procedure, the mean roughness on a { a 5 }, { a 6 } is computed. These calculations are summarized in Table 3 . Similar calculations are performed for all the attributes.

Second, the partitioning attribute with the MMR is found. Table 4 shows the calculations and illustrates that attribute a 1 and a 3 have the same MMR. In using the algorithm, it is recommended to look at the next lowest MMR inside the attributes that are tied and so on until the tie is broken. In the case where all the num-bers are tied, selecting any attribute randomly can break the tie. In the example, the second MMR correspond-ing to attribute a 1 is lower than that of a 3 . Therefore, attribute a binary splitting is conducted.
Third, the splitting point on attributes a 1 is determined. Note the binary optimal partition problem is NP-hard [28] . MMR determines the splitting point using the heuristic based on the roughness calculation that sim-plifies the computational complexity. The splitting set should include the attribute value which has minimum roughness. Taking a look at Table 3 , X ( a 1 = Small) has overall minimum roughness with respect to { a ( i = 2,3,4,5,6) comparing to X ( a 1 = Medium) and X ( a ( a 1 = Medium) and X ( a 1 = Big) is chosen. The partition at this stage can be represented as a tree and is shown in Fig. 1 .
 The numbers in the parenthesis at each of the child nodes correspond to the objects in the original data set.
Set (1,2,4,6,9,10) corresponds to all objects having either Big or Medium a s the value for attribute a (3,5,7,8) corresponds to all objects having Small as value for attribute a splitting. The algorithm terminates when it reaches a pre-defined number of clusters. This is subjective and is pre-decided based either on user requirement or domain knowledge.

Next we present a comparison with the MMR algorithm with the approach by Mazlack et al. [27] . [ Comparison example ]: As an extension of the approach proposed by Mazlack et al. [27] , MMR provides in Table 5 .
 Following the procedures calculating MMR, the results are summarized in Table 6 .

Clearly, the approach proposed by Mazlack will partition on attribute a has only two distinct attribute values). However, MMR will partition on attribute a the MMR algorithm will result in a crisper solution.

Thus, given a data set, assume n is the number of attributes, m is the number of objects, k is the chosen number of clusters and l is the maximum number of values in the attribute domains. k 1 iterations are required to achieve k clusters from the data set. In each iteration, the time to find all the elementary sets of each attribute is n * m , the time to calculate the mean roughness is approximately n
MR and MMR is 2n. Thus, the computation complexity is polynomial, that is O( knm + kn time increases by m * n . Given the minimum roughness over all other attributes exists on one particular value (e.g., p ) of the attribute (e.g., a i ), the splitting point can be set as ( a
RST in clustering is relatively new, our focus has been on evaluating the performance of MMR. Looking in the future, with the ever-increasing computing capabilities, computation complexity may not be an issue.
However, future plans will include efforts to reduce the complexity of the MMR algorithm. For example, we will explore the use of special data structure to reduce the computation effort.
 The following section describes the implementation and the results obtained from the application of the
MMR algorithm on three real world data sets. It also includes the results of comparison of the MMR algo-rithm with fuzzy set theory based algorithms and a traditional hierarchical algorithm. 4. Experimental analysis
In order to test MMR, a prototype implementation system is developed using VB.Net and tested on several data sets obtained from the UCI Machine Learning Repository. Validating clustering results is a non-trivial defined as The overall purity is defined as
According to this measure, a higher value of overall purity indicates a better clustering result, with perfect clustering yielding a value of 1. Note that similar measures have been used in Kim et al. [16] and Guha et al. [6] . 4.1. Comparison of MMR with algorithms based on fuzzy set theory
Currently, there are only a few algorithms which aim to handle uncertainty in the clustering process. These algorithms are fuzzy set based algorithms and include K -modes, fuzzy K -modes and fuzzy centroids. K -modes uses a dissimilarity measure between two objects which is calculated as the number of attributes whose values do not match. The K -modes algorithm then replaces the means of the clusters with modes and uses a frequency based method to update the modes in the clustering process to minimize the clustering cost function. Fuzzy K -modes generates a fuzzy partition matrix from categorical data. By assigning a confidence to objects in differ-ent clusters, the core and boundary objects of the clusters are determined for clustering purposes. The fuzzy centroids algorithm uses the concept of fuzzy set theory to derive fuzzy centroids to create clusters of objects which have categorical attributes. In this section, MMR is compared with these three algorithms based on Soybean and Zoo data sets in three experiments.

Experiment 1. The Soybean data set contains 47 objects on diseases in soybeans. Each object can be classified as one of the four diseases namely, Diaporthe Stem Canker, Charcoal Rot, Rhizoctonia Root Rot, and Phytophthora Rot and is described by 35 categorical attributes. The data set is comprised 17 objects for
Phytophthora Rot disease and 10 objects for each of the remaining diseases. Since there are four possible class values (four diseases), the algorithms based on fuzzy set theory generate four clusters. For comparison purposes the number of clusters is set to 4 for MMR. The results are summarized in Table 7 . Out of 47 objects, clusters is 83%.

Experiment 2. The Zoo data set is comprised of 101 objects, where each data point represents information of an animal in terms of 18 categorical attributes. Each animal data point is classified into seven classes. There-fore, stopping criterion for MMR is set at seven clusters. Table 8 summarizes the results of running the MMR algorithm on the Zoo data set. Out of 101 objects, 92 belong to the majority class label of the cluster in which they are classified. Thus, the overall purity of the clusters is 91%.

Kim et al. [16] have applied the fuzzy centroid method to the Soybean and Zoo data sets and compared the results with K -modes and fuzzy K -modes. In this research, MMR is applied to the same data sets and therefore can be used to compare with all three algorithms. Similar to Kim et al. [16] , purity is used as the evaluation criteria for comparison study. Table 9 summarizes the results of comparison study.

As shown in Table 9 , MMR out performs K -modes and fuzzy K -modes on both the data sets as well as performing better than the fuzzy centroid method on the Zoo data set. The performance of MMR is compa-rable to the performance of algorithms based on the fuzzy set theory. As discussed above, fuzzy set based algo-rithms all face a challenging issue, namely stability. These algorithms require great effort to adjust the parameter, which is used to control the fuzziness of membership of each data point. Therefore, the algorithms need to be run at different values of the parameter. At each value of this parameter, the algorithms need to be run multiple times to achieve a stable solution. In these regards, MMR has no such issues and offers the fol-lowing advantages: 1. MMR needs just one input parameter namely the number of clusters. 2. MMR does not require multiple iterations with different starting values. 4.2. MMR tested on a large data set
Both Soybean and Zoo data sets are relatively small (47, 101 objects, respectively). To test the applicability of MMR on a larger data set, additional tests on the Mushroom data set (8124 objects) are performed com-paring MMR to first the Squeezer, K -modes and LCBCDC methods and second to a traditional hierarchical algorithm in two experiments. Squeezer is a one-pass algorithm which places the first-tuple in a cluster and then the subsequent-tuples are either put into an existing cluster or rejected to form a new cluster based on a given similarity function. K -modes, as previously discussed, uses a dissimilarity measure between two objects which is calculated as the number of attributes whose values do not match. LCBCDC uses Link Clustering (LC) and Categorical Data Clustering (CDC). Finally, the traditional hierarchical algorithm converts the cat-egorical into boolean attributes with 0/1 values. Euclidean distance is used as the distance measure between the centroids of clusters. Pairs of clusters whose centroids or means are the closest are then successively merged until the desired number of clusters remain.

The Mushroom data set contains 8124 objects where each object contains information of a single mush-room. There are 22 categorical attributes; each attribute corresponds to a physical characteristic of a mush-room. An object also contains a poisonous or edible class label for a mushroom. The data set has 4208 edible mushrooms and 3916 poisonous mushrooms.

Experiment 3. According to He et al. [10] , Squeezer and K -modes algorithms produce better clustering output than other algorithms in categorical data sets with respect to clustering purity. LCBCDC was compared with
Squeezer and K -modes in He et al. [10] based on the Mushroom data set with two clusters created by each algorithm. For comparison purposes, MMR uses two clusters as the stopping criterion. Results are shown in
Table 10 . Clearly, MMR outperforms the Squeezer and K -modes algorithms, which do not handle uncertainty. It is interesting to note that LCBCDC provides better purity (86%). However, the results shown indicate that only partial data set (5478 records out of 8124) is used for LCBCDC. He et al. [10] explain that LCBCDC reasonably discards the malignant records as outliers and presents better cluster results.
Experiment 4. Guha et al. [6] compare ROCK with a traditional hierarchical algorithm based on the Mush-room data set. MMR is applied on the same data set and the results are used to compare MMR to the tra-ditional hierarchical algorithm. As the traditional hierarchical algorithm generates 20 clusters, for comparison purpose, MMR also uses 20 clusters as the stopping criterion. Table 11 summarizes the results of running the
MMR algorithm on the Mushroom data set. Out of 8124 mushrooms, 6785 belong to the majority class label of the cluster in which they are classified. Thus, the overall purity of the clusters is 84%.
Results were then compared with the application of the Mushroom data set on the traditional hierarchical algorithm. One noticeable discrepancy in the results obtained from the traditional hierarchical algorithm is in the total number of mushrooms. The total number of mushrooms turns out to be 7795 instead of 8124 in the original data set. With the assumption that the remaining 329 mushrooms are classified correctly, 4692 out of 8124 mushrooms belong to the majority class label of the cluster in which they are classified. Thus, the overall purity of the clusters is 58%, whereas the purity from MMR is 84%. Clearly, MMR outperforms the tradi-tional hierarchical algorithm. However, results from MMR indicate that there are significant differences in to split with at the beginning of each iteration was selected, which leaves leaf nodes with next largest number ferences in the size of the clusters might be due to the fact that MMR does not remove any outliers. MMR uses will be removed in future work.

Results from Guha et al. [6] indicate that ROCK performs very well on the Mushroom data set after the parameters are fine-tuned. With 21 clusters being created, the purity of ROCK is 97%. However, as discussed by Andritsos et al. [1] , ROCK has some limitations. ROCK is very sensitive to the threshold value. In many ondly, ROCK tends to produce one giant cluster that includes objects from most classes. Thirdly, ROCK can-not guarantee the number of generated clusters is the same as what is specified when ROCK is initially launched. For example, the application of the ROCK algorithm on the mushroom data set resulted in 21 clus-ters even though the input number of clusters was 20. Therefore, a detailed analysis on comparing MMR with
ROCK on the Mushroom data set is not provided. 5. Conclusions
Most algorithms designed to cluster categorical data sets are not designed to handle uncertainty in the data set. The majority of the clustering techniques implicitly assume that an object can be classified into, at most, one cluster and that all objects classified into a cluster belong to it with the same degree of confidence. How-one cluster. Thus, there is a need for a clustering algorithm that can handle this uncertainty in the clustering process. The MMR algorithm proposed in this paper has potential for clustering categorical attributes in data mining. It is capable of handling the uncertainty in the clustering process. Unlike other algorithms, MMR requires only one input, the number of clusters, and has been tested to be stable.

Additionally, we propose a number of future research activities related to MMR. First, we propose to explore a new stopping criterion. Instead of picking the subset of the data set with the maximum number of objects, we can first determine the distance between the objects falling under each leaf node. The leaf node with the maximum distance between the objects can be picked for splitting at the subsequent iteration. Sec-ondly, we will extend MMR to handle both numerical and categorical data. We will explore a discretization algorithm (such as 4cDiscretizzed which is an unsupervised discretization algorithm to divide the attribute range into a constant number of intervals containing an equal number of the attribute values). Thirdly, to achieve lower computation complexity, we will study the roughness measure based on relationship between a and the set defined as A -{ a i } instead of calculating the maximum with respect to all { a
Fourthly, we propose to study the approach proposed by Voges and Pope [38] and explore the potential appli-cation of evolutionary algorithms with rough sets to help determine the number of clusters in advance. Finally, more experiments need to be done on even larger data sets with more objects and more attributes.
References
