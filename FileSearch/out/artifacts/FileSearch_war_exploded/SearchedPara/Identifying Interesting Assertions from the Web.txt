 How can we cull the facts we need from the overwhelming mass of information and misinformation that is the Web? The TextRunner extraction engine represents one approach , in which people pose keyword queries or simple questions and T extRunner returns concise answers based on tuples extracted from Web text. Unfortunately, the results returned by engines such as TextRunner include both informative facts (e.g.,  X  the FDA banned ephedra  X  ) and less useful statements (e.g.,  X  the FDA banned p roducts  X ). This paper therefore investigates filtering TextRunner results to enable people to better focus on interesting assertions. We first develop three distinct models of what assertions are likely to be interesting in response to a query. We then ful ly operationalize each of these models as a filter over TextRunner results. Finally, we develop a more sophisticated filter that combines the different models using relevance feedback.
 In a study of human ratings of the interestingness of TextRunner asser tions, we show that our approach substantially enhances the quality of TextRunner results. Our best filter raises the fraction of interesting results in the top thirty from 41.6% to 64.1%.
 H.0 [ Information Systems ]: General Management, Design Information Extraction, Data Pre -and Post -Processing Information extraction (IE) is a subfield of natural language processing that seeks to obtain structured information from unstructured text. IE can be used to automate the tedious and error prone process of collecting facts from the Web. Open IE is a relation -independent form of IE that scales well to large corpuses. Figure 1 presents the output of the TextRunner Open IE system [3] in response to the question  X  What has the FDA banned?  X . TextRunner homes in on such answers as  X  ephedra  X  and  X  most silicone implants  X  and frees people from sifting through many Web pages to find the desired answers.
 TextRunner crawls the Web and maps sentences on Web pages into triples of strings of the form (subject , relation, object ). The relation string is meant to denote the relationship between the two entities . For example, if the sentence  X  X ranz Kafka was born in Prague, now in the Czech Republic but then part of Austria X  were found on a webpage, then one extraction would be (  X  X ranz Kafka X ,  X  X as born in X ,  X  X rague  X  ). TextRunner has been run on 500 million high -quality webpages yielding over 800 million extractions. These can be queried by entity or relation, or can be used to answer simple questions through pattern matching . Results a re returned ranked by frequency because , all other things being equal, extractions that appear frequently on high -quality Web pages are more likely to be correct (the KnowItAll hypothesis ) [5] . Yet , this method alone will not filter out many assertions that are not interesting to people. A key aspect of our study is that , in order to better scale to the full Web, we are studying models that can impr ove the interestingness of Web extractions in a domain independent and relation independent way. This is important because lexical rules (e.g.  X  X ll assertions about what companies Microsoft has bought are interesting X ) might work well for particular domain s or relations but not apply more generally.
 In traditional IE, system developers pre -specify a set of relations of interest. For example, the NAGA system has considered methods for evaluating quality of web extractions [7] , but their work is grounded in a graph representation based on the specific set of relationships they chose to extract. This limited set of relationships meant they could only evaluate 12 of 50 q ueries for one of their benchmarks. The concept of interestingness as a metric has been applied and studied in other related domains. For instance, Flickr recently launched a new feature for identifying  X  X nterestingness X  in photos on its site 1 . The Flickr notion is based on social feedback such as click data and comments, supporting the idea that people care about what is interesting and leave indirect clues to where interesting content can be found. We use a similar conc ept later in learning from how people populate Wikipedia infoboxes.
 Similarly, automated mathematical discovery programs require a notion of interestingness in order to identify which potential conjectures and concepts will be of interest to people. Colton and Bundy X  X  survey identifie s several key concepts these programs tend to use in deciding what is interesting, including plausibility, novelty, surprisingness, comprehensibility , and complexity [4] . Varying concepts like these have also been occasionally proposed by psychologists to help explain what is interesting [11] . In the area of databases and data mining, Liu et al. found the notion of interestingness helpful for deciding which of a huge number of discovered association rules to present to users [8] . Among other things , they studied the effectiveness of various forms of unexpectedness and successfully applied the ir ideas to a number of applications. http://www.flickr.com/explore/interesting/ Another type of interesting assertion is basic facts . These are definitional assertions that, for example, might be interesting to a person learning about an object. A person learning about Einstein, might look up such facts as  X  Einstein was a physicist  X  or  X  Einstein was born in Ulm, Germany  X . Interest in such basic assertions is evident in the emphasis on this type of information in dictionaries and encyclopedias. We thus operationalize basic assertions by learning a classifier to identify assertions most similar to the basic facts that human editors include in Wikipedia infoboxes.
 Train ing such a classifier requires examples of TextRunner assertions likely to reflect infobox knowledge (positive training examples) and assertions unlikely to reflect infobox knowledge (negative training examples). Starting with the DBPedia Wikipedia infobox dat abase [1] , w e applied a series of filters and isolate d a set of 872 not able people with good infobox coverage . Text matching on infobox values (allowing for small edit distance) produced a set of 1,584 TextRunner assertions that reflected knowledge expressed in those infoboxes. This is comparable to how Kylin matches infobox data to statements [13] , but our matching is stricter and thus achieves higher precision at lower recall. We then sampled 3,000 TextRunner assertions about the same people that did not match infobox values.
 Table 1 lists the features used to train the basic classifier. Because we are interested in a generally applicable classifier, we picked simple low -level features likely to generalize across domains. We test ed features that were more lexical (e.g. presence of certain keywords or relations), but found th ey did not generalize. For our classifier, we use Weka X  X  J48 Decision Tree [9] [12] . To evaluate our specific , distinguishing , and basic models, we used them as the basis of filters that discard TextRunner results that fail to satisfy each model. To assess the ir quality, we conducted a st udy to collect human ratings of assertion interestingness. We first selected a set of ten study query terms , including famous people ( Albert Einstein, Bill Gates, Thomas Edison ), other proper nouns ( Beijing, Brazil, Microsoft, Diet Coke ), improper nouns ( sea lions ), and relationship queries ( invented, destroyed ). This query set is meant to provide a varied sample of the sorts of queries for which TextRunner can provide interesting results. Our analyses are based on the top thirty ass ertions resulting from each of these queries , approximately the number of results that can be seen at a glance on a single results page . As a baseline for comparison, we use AssertionFrequency , which examines the thirty most frequently occurring assertions . We next obtain assertions for our specific , distinguishing , and basic conditions by applying each of our filters in order of assertion frequency, discarding results that fail the filter until we obt ain thirty results that satisfy the filter. This section therefore focuses on 1200 assertions (10 queries * 4 conditions * 30 assertions).
 # character s , # words, # capital letter s , # num eric digits , presence of year s , assertion ends on stop word, proper nouns in arguments , argument frequencies in corpus Figure 2 plots precision at k for our specific , distinguishing , ba sic , and relevance feedback filters against AssertionFrequency . To test for difference between these curves, we conduc t an analysis of variance for the precision at each plotted point, treating condition and k as fixed effects. The omnibus test reveals a significant main effect of condition ( F( 4, 4) = 285, p &lt; .0001), leading us to investigate pairwise differences. We use Tukey X  X  HSD procedure to account for increased Type I error in unplanned comparisons. This shows relevance feedback yields significantly more interesting assertions than specific ( F( 1,144) = 95.4, p &lt; .0001), distinguishing ( F( 1,144) = 78.6, p &lt; .0001), basic ( F( 1,144) = 8, p  X  .005) and AssertionFrequency ( F( 1,144)=926, p &lt;.0001). The largest differences in Figure 2 are between our filter -based ap proaches and TextRunner X  X  original use of AssertionFrequency , indicating the advantage of filtering. The cla ssifier filters trained with user labels and user -contributed knowledge ( relevance feedback and basic ) performed significantly better than all other approaches, indicating the utility of such data for this task. AssertionFrequency achieves a precision at thirty of 41.6%, while relevance feedback achieves a precision at thirty of 64.1% (almost comparable to human inter -annotator agreement levels ). Additionally, our trained filters achieved higher overall precision at thirty than AssertionFrequency in all query term categories we teste d (famous people, other proper nouns, improper nouns, and relationship queries) . We primarily examine precision within the top results because Open IE on the Web can generally return many more results than can be read and so the challenge is in precision more than recall. Even if good assertions are filte red out, there are often other assertions that express similar information that pass through. For queries without many results or applications where it is important to not filt er out good result s, models of interestingness could also be used to rank rather than filter. Extraction engines such as TextRunner are a promising avenue towards improving Web search and generating large knowledge bases. However, such systems are currently hamstrung by the fact they often return uninformative results that are vague or uninteresting. Web extraction systems are particularly prone to this problem because of the general methods they use to extract entities and relationships [2] . This paper has developed a filter system to enhance interaction with TextRunner by better focusing on interesting assertions. As a part of this task, we have presented three models of interesting assertions. These have the virtue of being readily operationalized into filters over TextRunner results. In addition to presenting a study of human ratings of the interestingness of assertions, we combined the filters using a relevance feedback technique that raised the average perc entage of interesting results on a sample of queries from 41.6% to 64.1%.
 There are several exciting avenues of future work here. First, to exceed inter -annotator agreement levels, we could study how different people may find different assertions interesting, and address how a system might learn, represent, and apply personal preferences. Second, carefully examining interesting assertions that did not pass any filters would help to reveal whe ther there are additional important aspects to interestingness . Leveraging resources such as WordNet [6] could provide us with more 
