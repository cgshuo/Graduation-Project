 The growing interest in adaptive learni ng has been received from various chal-lenging learning tasks in non-stationary environments such as non-invasive brain-computer interface (BCI), intrusion det ection and fault detection. For example, online adaptation of the classifier is an important requirement for BCI as brain activity changes naturally over time [1]. In particular, qualitative and quantita-tive evidence indicating non-stationary in the BCI classification problem has been provided by [2]. In these real-worl d learning tasks, online learning ap-proaches should be temporally adaptive when underlying concept of observable data changes over time. However, traditional static learning approaches do not care about the data generating process and ignore the dynamical characteristics of a set of observations.

Over the past ten years, researchers have explored several approaches to the adaptive learning. One approach uses particle filters [3], whose performance is demonstrated on the problem of fault detection of dynamical operated marine diesel engines. A non-stationary logistic regression model presented in [4], in which weights of the model evolve dynamically, uses the usual logistic function as the classifier. The extended Kalman filter is employed to update the weights in an on-line manner and the time-varying state noise variance parameters are estimated by a line search method. Successive rese arch work build on the model. Sykacek et al. use the variational Kalman filtering as an inference method for sequential classification in BCI [5]. A windowed Kalman filter procedure is implied to com-pute a bound of the log evidence. Recently, two different approaches, i.e., dynamic logistic regression using nonlinear Kalman filters and a dynamic generalized lin-ear model, are adopted to the sequential classification [6]. In the more closely related work, sequential Monte Carlo sampling and the extended Kalman filter have been utilized for solving the same problem [7][8][9]. Research also developed almost parallel in several different communities, such as data mining and statis-tics [10][11]. An efficient and effective approach to handle the problem of concept change in data streams is called concept-adapting very fast decision tree (CVFDT) [12]. CVFDT builds the decision trees in an incremental manner and detects the changing data concept from a sliding window of a fixed size.

However, much of previous research efforts only focused on two-class learning problems and they did not provide explicit representations for multi-class learn-ing problems. Traditional approaches for the multi-class problems by combining a set of two-class problems such as 1-vs.-all and 1-vs.-1, have some drawbacks. The approaches ignore the relations between the classes and have computational disadvantages. Although extensions to multi-class had been suggested by intro-ducing multinomial distributions with time-varying parameters, corresponding inference algorithms can not be derived straightforwardly from that of two-class learning problems. Furthermore, the exte nsions substantially complicate the in-ference procedures, leading to different algorithms for statistical inference. This motivates us to investigate the multi-class learning problems. In this study, one of major contributions is that we provide a sequential dynamic multi-class model (SDMM) in conjunction with the polychotomous response model and dynamic logistic regression. Latent variables of the model follow a multivariate random walk. In a novel contribution, we employ the variational Bayesian methods in a recursive manner for estimating the dynamic latent variables from data feature vectors and labels. Approximate inferen ce in the learning procedure is formed on each time step separately. Our proposed inference method is inspired by the recursive noise adaptive Kalman filtering where on each step the state of discrete-time linear state space model is estimated with Kalman filter [13].
The rest of this article is structured as follows. In Section 2, SDMM model is given. In Section 3, the variational Bayesian approach is introduced to approximate the posterior distributions of the latent variables in a recursive filtering manner. A learning algorithm is proposed to estimate parameters. Sec-tion 4 provides a calculation approach for one-step-ahead predictive distribution. After presenting some experimental resul ts in Section 5, conclusions and future research are given in Section 6. beled in K classes. In this research we are con cerned with developing a sequential classifier for multi-clas s learning problems where K is greater than two. The data points arrive one x t at a time t . A correct label Z t  X  X  1 , ..., K } is revealed after the one-step-ahead prediction is done by the classifier, but before the next data point x t +1 is observed. The classifier is updated in an online manner by using the data observed at time t . Considering a given input feature vector h t at time t , a sequential polychotomous response model to the dynamic multi-class learning has the following form: where v t,k  X  N (0 ,q t,k I ) is the Gaussian process noise whose covariance is con-strained to a scaled identity matrix q t,k I . Compared to the polychotomous model proposed by [14], the d -dimensional state variable w t,k using a random-walk model is time-evolving to capture the non-stationary. Moreover, adopting the form of the random-walk model for state variables means that our model is best suitable for sequential classification in the presence of steady changes. Con-tinuous auxiliary variables y t,k are introduced to augment multinomial probit regression model. As detailed in [15], the multinomial probit takes the following form by explicitly marginalizing the auxiliary variables: where the random variable u  X  N (0 , 1) is standardized normal,  X  ()is the stan-dardized normal Cumulative Distribution Function (CDF).
 Fig.1 shows a directed acyclic graph for the hierarchical generating process. The next task is to estimate the state variable w t,k according to the observable labels and feature vectors by B ayesian filtering methods. 3.1 Variational Bayes Approximation To infer the state variables of the nonlinear state space model above, Bayes X  X  rule However, owing to the non-linear and non-Gaussian of observation function, ap-proximation methods are required to simplify the estimation procedure. We use Variational Bayes (VB) approximation method to infer posterior distributions and variational EM to fit the hyperparameters [16].

Assume that the VB approximation has been applied at the previous time the feature vectors h 1: t  X  1 takes the form as follows: Since the state process is linear, the prediction distribution is:
Next, a mean-field approximation for the updated posterior distribution is adopted as follows: Following the VB methodology, a lower bound F ( Q ( w t , y t ) , X  t ) of the logarithmic marginal likelihood log p ( Z t ,h t |  X  t )is: F ( Q ( w t , y t ) , X  t )= Maximizing the lower bound (8) and applying the Euler equation and constraints of Lagrange multiplier type, the solution for each Q ( w t,k )and Q ( y t,k )can be derived as follows: where: with assuming that Z t = i the i th dimension of y t is always the largest [15].
The parameters of the distributions are the following: for Z t = i : for all k = i : with determination solutions, that of Q ( y t,k ) are obtained by importance sam-pling method.

Finally, we provide the expression of the variational lower bound by using (8,11-15): The value of the bound can be employe d to set a convergence criterion for iteratively updating the parameters of Q ( w t,k )and Q ( y t,k ) using (11) to (15).
The calculation as described above is called E-step of variational Bayesian expectational maximizing algorithm (VB-EM), whose purpose is to estimate the hidden states. The M-step of VB-EM is to estimate the state evolution noise variance q t,k using the states estimated in the previous E-step. Ideally, all q t,k are obtained by maximizing the variational lower bound (16) at the end of each E-step, which is guaranteed that the lik elihood will not decrease. However, it is difficult to evaluate q t,k by setting the derivative of (16) with respect to the parameters q t,k to zero. Inspired by the setting methods for the covariance matrix where trace () denotes the trace of matrix, UC as the update coefficients with 0 &lt;UC&lt; 1. Small UC means the state variables change slowly with time and only a small non-stationary takes place. 3.2 Summary Here, we give a step-by-step overview of the proposed recursive filtering method: Initialization: Given a set of measurements z = { Z t ,t =1 ,...,L } and feature vectors h = { h t ,t =1 ,...L } , initialize the parameters  X  m 0 ,k using logistic are set by hand.
 Iterate the VB Approximate Algorithm: Given the approximate posterior 1 ,...,K . Then iterate the following a few steps: Step 1 Calculate Q ( y t,k ) using (10); Step 2 Calculate Q ( w t,k ) using (9);
Step 3 Calculate F ( Q ( w t , y t ) , X  t ) using (16) to monitor the convergence of the above iterating procedure.

We close the summary by analyzing the time and space complexity of the method.

Given S samples for the required importance sampling method, the most expensive operation in Step 1 is to compute (15), which takes O ( K ( K  X  1) Sd ) time. The complexity of Step 2 is O ( Kd 3 ) for computing matrix multiplications in (12). Step 3 has to compute the matrix inversion, hence, takes O ( Kd 3 ).The space complexity of the recursive filtering method is O ( max ( S, Kd 2 )). For a new feature vector h t +1 atthetimestep t + 1 , one-step-ahead predic-tive distribution can be obtained by firstly marginalizing the state variables as follows: then: Constrained by the definition of probabilistic distribution, a proper normaliza-tion for the posterior distribution over classes { 1 ,...,K } should be adopted: When labels received are sparse, the cl assification problem can be reformu-lated as an incomplete learning problem with missing labels. The one-step-ahead method can also be applied to infer the missing labels. In turn, the predictive la-bels with the largest posterior probabilities are set to  X  X uasi-targets X  for further recursive filtering. The approach can also be viewed as a semi-supervised learn-ing problem of time series. As illustrated in [6], moreover, active label requesting can be adopted when observing labels m ay be expensive. In practice, successive applying of one-step-ahead for missing labels can bring high cumulative errors by the recursive filtering method proposed as a bove. Therefore, requesting actively a label is important to the further research. Despite increasing interest and importance of this topic, there have few publiclly available benchmark data sets for dynamic multi-class learning tasks involving non-stationary environments. We present results over three data sets: A synthetic data set,a real data set of electroence phalogram(EEG) and the Waveform data set from the UCI Machine Learning Repository. Some static classifiers, as well as CVFDT and an online linear discriminant classifier (O-LDC) presented in [10], are compared with our proposed approach on the two data sets. CVFDT only reports the one-step-ahead result s on the three data sets because it fails to tackle the problems with missing input labels. Although O-LDC is one of the few dynamic multi-class classifiers, it is linear. 5.1 Synthetic Problem For demonstration purposes, we firstly consider a synthetic four-class problem, where four Gaussian distributions rotate in a circular fashion around a central point [0 , 0]. Initialization mean points of the four Gaussian distributions locate at the left, upper, right and lower points of a circle whose radian is 2 respectively. Covariance matrices of the Gaussian distributions are equally set to 0 . 04 I 2 .Tar-Figure 2 gives three snapshots of the moving-circle class configurations.
The input feature vector h t at time t in the experiments is represented as h t =[ x T t ;1] T and x t is a two-dimensional input data. The initialization values of parameters are given as follows:  X  m 0 ,k  X  N (0 , 1); UC =0 . 55. Results: (1) We performed experiments using first 20 labeled inputs. The re-maining 620 inputs were used for testing. One-step-ahead predicted labels were computed by the recursive filter ing, the test error was only 1 . 61%. As a com-parison, CVFDT and O-LDA gave the test error rate of 55 . 8% and 70 . 8% re-spectively. We attribute the improvements of performance to the fact that our model takes advantages of non-linear classifier. (2) Another experiment was performed for missing input labels. Besides the first 40 inputs which must be labeled, 10% to 90% of the remaining 600 in-puts were labeled randomly. The predict ive results by our proposed method (SDMM) were compared with other three classifiers. Figure 3 illustrated the performances of each classifiers evaluated over 10 independent runs. The pa-rameters of variational Bayesian multinomial probit regression with Gaussian process priors (VBGP) are set to be default [15]. In support vector machines (SVM) with Gaussian kernels, gamma co efficient was set to be 3 according to the best predictive results by observing various numerical experiments under different gamma values (  X  =0 . 1 , 0 . 5 , 1 , 2 , 3 , 5). We combined the binary SVM classifiers by using 1-vs.-1 scheme [18] . In k-nearest neighbors classifiers, k was set to be 7 according to the best predictive results among k =1 , 3 , 5 , 7 , 9.
The left panel of Fig.3 shows that our method (SDMM) takes advantages of sequential dynamic learning. The error rate clearly shows a slow trend to the probabilities of missing labels. Even there are 90% unlabeled test data, the average error rate is only 13%. The other three non-sequential classifiers fail to discrimination. Their error rates have little improvements than that of random labeled procedure.

The results provide the similar conclusion, presented in [6] for the sequential dynamic learning of two-class problems, that the classification performances of the model do not worsen in proportion to the number of missing labels. Although the complexity of the SDMM and corresponding inference algorithms increases, the classification performances with sparse observed labels remain almost un-changeable. Therefore, the results indicate the extension of two-class adaptive classifiers to multi-class environment is feasible. 5.2 Four-Class Motor Imagery EEG Data for the BCI-Competition BCI aims to establishing a direct conn ection between the human and the com-puter, which enables the person to control the computer with the imagination or other mental tasks. Extensive training is required for the adaptation of the user to the designed BCI system. Non-stationary of brain signals during mo-tor imagery poses a challengeable task for the accuracy of BCI control. For investigating the significant challenge, we tried to apply the sequential dynamic multi-class model for a four-class motor imagery task.

The data we use is available at http://www.bbci.de/competition/iii/, whose detailed description of the acquisition procedure is presented in [19]. A four-class classification task including the motor imagery of the left/right hand, one foot, or tongue was provided for the 2005 BCI competition from Graz. For demon-stration purposes, we only considered su bject k3b, where 90 trials per condition were recorded. According the experimental results in [20], we adopted following feature extraction procedure. Firstly, two monopolar channels 3and 34 were selected to perform single-channel analy sis. Data within the interval of to 6.8s after down sampling from 250 to 125 samples per second were remained. Then, three-order AAR procedure was calculated for the difference of two monopo-lar channels of each trail. Finally, 75-dimensional input data for each trail were obtained to construct the input feature vector h t =[ x T t ;1] T . Similarly, first 40 inputs and randomly sam-pled inputs from 10% to 90% of the remaining 320 inputs were labeled. UC =0 . 0055 ; Weights of the logistic regression classifier on first 40 inputs were taken as the initialization values of  X  m 0 ,k .
Thepredictiveperform ance of our proposed method (SDMM) was also com-pared with another four classifiers (VBGP, SVM, k-NN and LR). The reported labeling errors are averages over 10 randomly drawn observable and missing data sets. Figure 4 shows that the comparison of the error rate via five different clas-sifiers on the four-class motor imagery EEG data set. Here, gamma for SVM was set to be 0.01 in order to achieve the best results. PCA (principal component analysis) is used to extract 8 principal component features before k-NN classi-fiers applying to these features. LR stands for Logistic Regression of multi-class learning.

In the right panel of Fig.3, our proposed method (SDMM) has an accuracy improvement of 9% averagely. It is able t o achieve higher classification perfor-mances than the other four non-sequential models. We use the paired t-test and the p-values for the nine groups under varying fraction of missing labels. All p-values of pairs between our SDMM method and other methods are less than 0.01 excluding that of pair between the SDMM and SVM on the seventh group (p=0.02053). According to the quantitative results of statistical analysis, the error rate of SDMM is significantly smaller than that of four static classifiers.
It might be argued that the performance of both classifiers almost does not change when the number of missing labels changes. There are two possible rea-sons: 1) A simple feature extraction approach was presented in our experiments and would lead lower absolute classification accuracies; 2) In BCI, the labels of subject training are not always accurate [8]. In addition, unlike the other classifiers improving performances by ex tensive cross validation procedures, our method is prone to online computation style. 5.3 Waveform Data Set from the UCI Machine Learning Repository The original Waveform data set consists of 5000 instances in three classes with 21 attributes. To form a large data stream, the data set is repeatedly stacked according the order of attributes. For ev ery segment partitio ned by the selected attribute, all instances in the segment are ascendingly sort ed according to the value of attribute [21]. Therefore, the Waveform data set has 105000 instances.
For the large scale stream learning problem, SDMM is compared against the popular multi-class data stream classifier CVFDT. The CVFDT algorithm has some parameters to be set. Typical parameter setting used by the corresponding paper and free software can not achieve the low error rate along time for the data set. In our study, we observed that the setting  X -tc 0.25 -sc 0.1 -chunk 20 X  can give the best results. In our SDMM algorithm, we set UC =0 . 0055.
In Fig.4 the incremental one-step-ahead error rate is illustrated for the two ri-val algorithms. It is observed that SDMM outperforms CVFDT when the number of instances is less than 50000. However, accuracy of CVFDT improves steadily as more instances are increased. This is explained by the fact that SDMM, as a model-based classifier learned by the Bayesian methods, is prone to tackle the small scale learning problems. Another possible reason is that our SDMM algo-rithm keeps a sliding window with fixed size 1 and reduces the significance of the most recent instances arrived. We have presented the sequential dynamic multi-class model for adaptive learn-ing of data streams. We have adopted the variational Bayesian approach for recursive filtering of the model. Our e xperiments have shown that this model outperforms the static approach in the presence of time-varying classification decisions and sparse labeled data. In comparison to the well-known multi-class data stream algorithms, we report promising results.

Sequential adaptive learning is important for exploring the nature of non-stationary and non-linear of online learning problems. Future work will focus on developing an advanced filtering method along an adaptive sliding window with varying size. Active label request ing also deserves further study.
 Acknowledgments. This study has been supported by the Nation Nature Science Foundation of Ch ina 61074113, Shanghai Leading Ac ademic Discipline Project B504, and Fundamental Research Funds for the Central Universities WH0914028.

