 1. Introduction
Numerous applications, like Geographical Information Systems, CAD and VLSI design have emerged dur-ing the last years that demand the efficient manipulation of massive sets of geometric objects like points, lines, areas or volumes in one or more dimensions. The databases that accommodate this specific kind of objects are called spatial and they employ indexes that must be able to answer a very diverse set of geometric queries, like range queries, which ask for all objects lying within a given region, and nearest-neighbor queries, that seek for the object closest to a given object.

The diversity of the query repertoire combined with the massive nature of the involved data sets explains why practical, general purpose indices like R-trees attract so much research interest [17,18] . An R-tree is a height-balanced tree which can be considered as an extension of the B +-tree for multi-dimensional data.
The minimum bounding rectangle (MBR) of each geometric object, along with a pointer to the disk address where the object actually resides, are stored into the leaves. Each internal node entry consists of a pair (pointer to a subtree T , MBR of T ), with the MBR of a tree T defined as the MBR enclosing all the MBRs stored in
T . Like in B +-trees, each node contains at least m and at most M entries, where m P M = 2. On the other hand, unlike B +-trees, a search query may activate several search paths from the root to the R-tree leaves, resulting, in the worst case, in a linear to the size of data set performance just to retrieve a few objects. Fig. 1 illustrates an R-tree instance on a set S of rectangles.

Since its introduction in 1984 [12] , several variants of the R-tree have been proposed, each one aiming at improving the performance by tuning some parameters. Among the members of the  X  X  X -tree family X  X , the most forced re-insertions during insertions (as in the case of deletions), buffering and optimization criteria for split-best performance. However, the construction of any R-tree version by using repeated insertions does not guar-antee the efficiency of the query performance; actually, the linear worst-case query time complexity cannot even be avoided.

On the other hand, bulk-loading techniques for R-trees were designed which capitalize on the priori knowl-edge of static data sets to build the structure from scratch, in a bottom-up or a top-down fashion. In this way, better utilization and search performance are achieved in the average case. Towards this end, [14] use the Hil-bert sorting technique to impose a total order on the data and then build the R-tree in a bottom-up fashion, according to the resultant sequence. Ref. [16] extended this approach, applying successive sorting and division of data into slabs for each of the dimensions. Refs. [7,2] proposed the building of indices with repeated block-wise insertions, attaching auxiliary buffers to index pages. In [6] a recursive top-down algorithm is employed, which, operating in a manner similar to quick sort, determines the tree topology (height, fan-out, etc.) and uses a split strategy to bisect the data in secondary storage and construct the index directory in a depth-first, post-order way. Finally, [1] introduced the priority R-tree which exhibit a bulk-loading algorithm utilizing priority rectangles, in a way that an O  X  X  n = M  X  1 1 = d  X  T = M  X  worst case performance is achieved, where M is the node capacity, T the output size, and d the data dimensionality.

In this paper, we try to incorporate the excellent organization virtues of the (inherently) static bulkloading methods into the updating operations of a dynamic, general purpose spatial index, like the R-tree. Since bulk-loading consists an expensive operation, its invocation should be carefully applied. For this reason, we use for the first time the technique of partial rebuildings in order to generate the Weighted R-tree (WeR-tree), a very practical and well-behaved full dynamic single-tree member of the R-tree family.

To show the applicability of our proposal, a twofold investigation is conducted: Firstly, we prove amortized theoretical bounds for the update operations, with respect to the time complexity BL  X  n  X  of the deployed bulk-loading method. And, secondly, we report detailed experimental results, concerning both synthetic and real data sets, which verify the scalability and the efficiency of WeR-trees. Actually, to clearly depict this fact, we employ two bulkloading methods, one inspired by the algorithms described in [14] , and one based on the approach of [6] . Summarizing our findings, utilization reaches up to 98.1%, range query savings vary 20 X  35% on real data sets and 15 X 85% on synthetic data, k-nn queries are 27 X 49% faster, during mixed operations performance savings are between 18.5% and 20.8%, while the scheme scales up linearly with respect to the number of inserted points achieving 24 X 38% fewer page accesses.

The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 presents the partial rebuilding paradigm and Section 4 introduces the WeR-trees. In Section 5 we give theoretical bounds while
Section 6 provides experimental evidence on the superiority of our scheme over R * -trees. Finally, Section 7 concludes our work. 2. Related work
Since it is easier to design data structures for static sets with good query time, a great number of efforts introduced general methods for converting static or semi-dynamic data structures into dynamic ones [4,20,19,21] .

This kind of research has been initially conducted in the area of main memory data structures. The methods data; (iii) block-based segmentation for query problems satisfying certain decomposability criteria; and (iv) local balancing , which maintain query efficiency by local changes to the tree structure. The applicability of (i) X (iv) depends on the specific characteristics of the transformed data structures, for instance, the rate of performance degradation or the ability to absorb insertion-or deletion-only operations. On the other hand, the partial rebuild-ing method appears more appropriate to manipulate multi-dimensional data, since it can maintain single-tree indexes which smoothly integrate the accumulated global knowledge about the data set.

Examining disk-based indexing solutions, various mobile indexes [9] in a loose way may be classified as members of the global rebuilding family, because of the limited horizon validity restriction they exhibit. R-trees can be considered of the last category, since their maintenance is based on local node adjustments along the path leading to the insertion or deletion location. On the other hand, the LR-tree [10] is an example of a block-based segmented index; it consists of a logarithmic number of component substructures, called blocks, prises a forest of indexes which may be difficult to be handled when one wants to combine it with single R-tree-like structures, say, in a merging [24] or join [8] operation.

Since partial rebuilding can be applied to a single-tree structure and leave it in  X  X ne piece X , we choose it for our reconstruction method. Actually, to make it applicable to multi-way tree indexes, we bound the number of leaf items a subtree accommodates by an exponential function of its height. Next, we elaborate on this design paradigm. 3. The partial rebuilding paradigm
Let us assume a tree structure index T already built for a set of items. Whenever one wants to insert or delete a point into/from T , the balance of the nodes along the involved search path changes. As a matter of fact several of them may become out of balance. The criteria of balance are specific to each data structure.
For example, in B-trees, one node cannot accommodate fewer than B /2 or more than B children, B being the node capacity. Once violations are detected, balance must be restored.

The treatment of imbalance characterizes each data structure. B-trees, for instance, remedy violations applying local adjustments; repeated splitting/merging of nodes along the search path is employed until invar-iants are restored. In the partial rebuilding paradigm, the actions are more  X  X  X lementary X  X : one locates the high-est problematic node v , and rebuilds the subtree T v rooted at v in a perfectly balanced shape.  X  X  X erfect balance X  X  is determined exclusively by the underlying data structure definition and the aiming properties.
In the above coarse description there is a missing point; the higher the unbalanced node v the bigger the subtree T v and therefore the more expensive its rebuilding. However, enforcing fewer expensive rebuildings and more cheaper ones, good amortized update times can be achieved [19,21] . Usually this involves proving: (i) a lower bound on the number of updates performed on node v before becoming unbalanced since the last rebuilding of T v ; and (ii) an upper bound on the rebuilding cost in terms of the size of T determined, the cost of rebuilding or bulk loading is charged to the updates that caused the reconstruction so that the total accumulated cost is bounded. 4. WeR-trees 4.1. Motivation designed heuristics, it addresses the deficiencies of the original R-tree algorithms about query performance, into account: overlap between nodes, node perimeters and storage utilization. Also, it uses the plane-sweep technique to separate the node entries. However, deletion and searching are identical to the respective R-tree algorithms. The following theorem [10] summarizes the R * -tree performance:
Theorem 1. A set S of n geometric items can be accommodated in a R * -tree using O (n/M) space so that a search for an item has linear worst-case time complexity, an insertion of an item can be completed in O (M log case I/O time, while, given the location of an item, it can be deleted in O (M log denotes the node capacity or block size and I/O time refers to the number of block retrievals).
Additionally, we must note that several analytical works for the query operations do exist (e.g., [13,15,22,23] ) and are characterized by limited generality since they simply derive approximate estimates based on a number of assumptions, like uniformity of the underlying distribution, known aspect ratio of MBRs, etc. Only in [1] a static R-tree variant with worst case performance guarantee is presented.
 tion on internal nodes v , i.e., a process that performs a kind of local rearrangement of the respective subtree.
This fact motivated investigating of the replacement of the original update algorithms by carefully triggered perfect rebuilding of unbalanced subtrees. Bulk-loading techniques, that build the structure from scratch and achieve better utilization and search performance in the average case, exist for R-trees and are adopted to achieve the desired perfect balance.
 4.2. Definition The WeR-trees are defined as follows:
Definition 1. A WeR-tree is an R-tree in which every internal node v is in perfect balance. An internal node v is in perfect balance if the respective subtree T v is equivalent to a bulk-loaded R-tree on the same data set S
The invariant of the definition of the WeR-tree is enforced by carefully triggered subtrees X  rebuilding, as specified by the update operations. Contrary to B +-trees, which can be reconstructed in a single way, R-tree-like structures permit a number of solutions, as we saw in Section 1 . In the following paragraphs, we describe the procedures for insertion and deletion.
 Insertion
Fig. 2 illustrates the insertion algorithm. Given the item p to be inserted, we firstly locate the leaf l into which p should reside. The search process depends on the kind of R-tree the adopted auxiliary bulk-loading method constructs X  X his will be exemplified in Section 6 , where a concrete experimental set up is described.
Subsequently, we locate the deepest ancestor v of l such that the rebuilding of the subtree T on the path towards the tree root r would have bigger height from the other children of v , whereas the rebuild-ing of T v would yield a subtree of the same height as before the insertion of p . In this way, we ensure that all leaves are at equal distance from r , while node capacities are within limits. Finally, the data items of T passed to the bulk-loading procedure so that a new space and query efficient subtree is built.
Fig. 3 illustrates a rebuilding example after inserting K. Since rebuilding at node w would increase the height of T w , node v is chosen and T v is bulkloaded from scratch.
 Alternatively, one can use the insertion procedure described in Fig. 4 , that enforces the following invariant:
Invariant 1. The subtree T v of every node v of height h v with M being the page capacity and c &lt; 1 a properly chosen constant.
 Similarly to the previous algorithm, in the beginning we find the leaf l that should accommodate new item p .
Next, we locate the highest ancestor v of l meeting the requirements of the invariant while having a child w with more descendants than the number the invariant suggests. After that, we substitute T new bulk-loaded subtree.
 Deletion
As usual, deletion ( Fig. 5 ) is symmetrical to the insertion operation: After locating item p to be deleted, we locate the deepest ancestor v of accommodating leaf l such that the rebuilding of the subtree T on the path towards root r would have smaller height from the other children of v , while the rebuilding of T would yield a subtree of unaltered height, and therefore, all leaves are at the same level while node capacities are within limits. Finally, T v is rebuilt by the adopted bulk-loading procedure.

Alternatively, one can use the deletion procedure of Fig. 6 , that enforces invariant 1 . Here, the goal is to find the highest ancestor v of involved leaf l such that obeys invariant 1 , while having a child w with less descendants than the number the invariant suggests. 5. Theoretical bounds
Since R-trees have linear worst case performance, we cannot prove better theoretical bounds. However, in subtrees are regularly rearranged by the adopted bulk-loading procedure; Section 6 provides evidence on this claim.
 On the other hand, the reorganization is not prohibitively costly, as the following theorem describes: Theorem 2. The insertion and deletion procedures have O  X  log number of stored items, M the page capacity and BL  X  n  X  the time complexity of bulk-loading reconstruction.
Proof. We will focus on the insertion procedure X  X he arguments about the deletion case are symmetrical. Let v and w be the father-target of the rebuilding and the child whose height must be increased, respectively. Since the height h v of v remains constant before and after bulk-loading, the subtree T that:
On the other hand, let n w ; n 0 w be the number of leaf pages T and immediately before the new rebuilding of v , respectively. Then, the following equations hold:
Hence, we have: with k a properly chosen constant. Therefore, between the last and the new rebuilding, X ( n place. This means O  X  BL  X  n v  X  = n v  X  amortized complexity per insertion due to node v . Summing up the amortized costs along the leaf-to-root path, the claimed insertion complexity follows. h
Usually, bulk-loading algorithms cost O( n log M n ) time, which gives an O  X  log plexity. The previous proof was based on the first version of insertion/deletion algorithms, which are height-balanced ones, while the alternative versions can be characterized as weight-balanced. In [11] the equiv-alence of height-balanced and weight-balanced binary trees was proved. While R-trees are multi-way trees, they can easily be converted into binary ones. By extending the arguments of [11] , one can show that the same bounds also hold for the alternative versions of the update operations. 6. Performance results 6.1. Experimental set up and heuristics
We have implemented all examined methods in C, using the same components for the common tasks among the methods. All experiments were performed on a machine with a Pentium IV processor at 2.3 GHz, with 1 GB main memory, running SUSE Linux 10.0. We used both real and synthetic data sets which contain two-, three-and four-dimensional points, since the R-tree family indexes are efficient for data of low dimensionality. Specifically, we present results for the LB data set, which contains 53,000 two-dimen-sional points representing postal address at Long Beach, and the NE data set, which contains 123,593 two-dimensional points representing postal addresses of three metropolitan areas (New York, Philadelphia and
Boston). Both the aforementioned data sets have been used as benchmarks in prior work (e.g., [14] ). For syn-thetic data sets we present results for two-, three-and four-dimensional points following uniform and zipfian distribution. We used a default page size equal to 8 KB (other page sizes gave analogous results). We also employed the k -nearest neighbor algorithm that is described in [5] .

We tested our structure using two bulk-loading techniques. The first one is inspired by the algorithms described in [14] , that is, the data points at every time are sorted according to their Hilbert value. The second technique is based on the approach that is described in [6] where the data points that are about to be bulk loaded are arranged continuously in each dimension according to their values. The first and the second imple-mentation of our WeR-tree will be termed in the sequel as WeR-A and WeR-B, respectively. Notice that in contrast with WeR-A, WeR-B arranges the data points of a subtree that is going to be rebuilt, invoking a
WeR-A trees. Moreover, we have to mention that the bulk load technique that is adopted by the WeR-B tree is not efficient for low dimensional points as the authors of [6] imply; the superiority of their technique is appar-ent in data sets that contain points of dimensionality larger than 8. In our case, where the dimensionality is
On the contrary, we will show that, although the bulk loading technique of WeR-B is not suggested for low dimensional points, the well tuned WeR structure balances the bad performance of this technique.
The rebuilding in both implementations follows a top-down fashion. During its way down, the algorithm decides the appropriate number of entries (subtrees) at every node. If the current node v (with weight of N data points) has height h v then the fanout of this node is defined as and the weight of each of the x subtrees is therefore defined as same number of accommodated data points and most importantly they are filled with the average allowed number. In this way, we limit the bulk loading operations in the presence of mixed update operations because the leaves are neither fully nor least filled. During its way up, the top-down algorithm adjusts the correspond-ing MBRs (and the largest Hilbert values in the case of WeR-A implementation) as a classic depth-first algo-rithm would do.

For convenience, we adopt the approach of prior work and consider square-shaped range queries, charac-terized by the size of the square. We are interested in the relative performance of the examined methods; there-fore, we use a path-buffer (containing the current path from root to leaf) but no other buffer space, to clearly examine the behavior of the methods regardless the effect of buffering. 6.1.1. Experimental results In the paragraphs that follow, we present our experimental findings when we compared WeR-trees against age overhead, and cache performance. 6.1.1.1. Range and nearest-neighbor queries. Our first experiment considers range search queries on both real and synthetic data sets that contain two-dimensional points. As performance measure we use the number of page accesses. We assume six query files (Q1) X (Q6) of 100 intersection window queries each, all of them uni-formly distributed over the data space. The area of the window queries of each query file (Q1) X (Q6) varies from 0.001%, 0.01%, 1%, 1.5%, 1.8% to 2% relatively to the area of the whole data space. Figs. 7 and 8 illus-trate the results with respect to the range query size; the latter is expressed in terms of the percentage (%) of the work space.

Considering the real data sets, we observe in Fig. 7 that WeR-A and WeR-B trees clearly outperform R * -tree: the savings vary between 20% and 35%. Analogous conclusions can be drawn from the synthetic data set with uniformly distributed points ( Fig. 8 a), where gains reach 35%. For the synthetic data set with points fol-lowing a Zipf distribution ( Fig. 8 b), the WeR trees compares favorably to the R * -tree; the WeR-tree family presents an improvement between 68% and 85%. This can be justified by the demanding nature of the zipfian data set. On the other hand, the WeR-trees demonstrate better organization X  X he activated paths are fewer X  which evidently deals with this case. Please observe that the behavior in the case of Q5 and Q6 is different from the other query files. This can be explained as follows: The input distribution generates data sets with centers located close to zero with high probability. Since the sizes of Q5 and Q6 are quite big, an adequate number of path nodes leading to output data is kept inside the cache. The latter fact is modulated by the uniform distri-bution of the query centers in the whole space.

Then, we studied the k -nearest neighbor queries. We present results only for the real data sets since the find-ings for the synthetic ones are analogous. For every different value of parameter k , we performed 100 queries with point queries uniformly distributed over the whole data space. The average page accesses are given in
Fig. 9 . It is apparent that in all cases, the WeR trees have better performance than the R * -tree; savings vary from 27% to 49%. Even though the WeR-B invokes an inefficient to the particular data sets bulk loading 6.1.1.2. Update operations. Our next experiments evaluate the behavior of the update operations. As perfor-mance main measure we use the wall-clock time, returned by the instruction void clock(void), which reports ing WeR-A and WeR-B tree construction phase) and other operators which require non-negligible CPU cost, additional to the I/O cost (which is not the case for search queries, where I/O cost is the dominant one). There-fore, wall-clock time depicts the inherent complexity of insertions and deletions.

Insertion is the first operation we deal with. We give results only for the NE data set; the other data sets led to the same conclusions. We tested all methods by initially inserting a number of points from the data set, and measuring the insertion time with respect to the number of remaining points. The number of remaining points is expressed as percentage of the total number of points in the data set.

Fig. 10 a illustrates the findings. As depicted, the WeR trees outperform the R * -tree. Although WeR trees spend much time to reconstruct the subtrees, they consume much less time to find the appropriate leaf in where the new point will reside. This is because the ChooseSubtree() method of the R * -tree costs O( pM log n ) worst case while the relative method of the WeR-A tree only O( M log n ). Additionally, the good MBR packing of WeR structures leads to fewer intersections and thus fewer activated paths. As a result, despite the fact that
For this experiment we also give in Fig. 11 a, the average number of page accesses per operation, which fol-lows the theoretical analysis for a multiplicative factor of less than 2. Furthermore, in Fig. 11 b we demonstrate the average number of index nodes that undergo rebuilding as we build the index from scratch. Additionally, we investigated the execution time of the deletion operation. Fig. 10 b illustrates the results with respect to the 6.1.1.3. Mixed workloads. Next, we simulated a typical workload with a sequence of mixed deletion, insertion and range query operations in order to examine the impact of interleaved updates on query performance. Ini-tially, all the data set points were inserted and no deletion is performed. For the remaining points, the ratio, C , of the number of inserted points to the number of deleted points defines the experimental parameter. Here we must note that deletions were carried out by removing already accommodated points. Interleaved with inser-tions and deletions, range queries were performed and the average number of disk accesses needed by the range queries was determined. Fig. 12 a illustrates the results with respect to C for the LB data set. As shown, performance gains range from 18.5% to 20.3%. 6.1.1.4. Scalability. We also studied the scale-up properties of each method with respect to the data set size, using synthetic data sets which follow the uniform distribution. Fig. 12 b depicts the results for the range search query (with uniformly distributed windows queries from the Q6 query file) for increasing number of 38% fewer page accesses. Moreover, we observe that the performance of the WeR-A tree grows linearly with respect to the number of inserted data points in a slow way. This can be explained as follows: the Hilbert val-ues of the data points are distributed uniformly over the whole data space because the Hilbert value is a linear function of uniform variables (the coordinates of the data points follow uniform distribution). This means that after consecutive reconstructions, where the data points are sorted according to their Hilbert value, the data points in the WeR-A tree are uniformly distributed in the leaves. Hence, queries with uniformly distributed guarantee that the data points are stored in the leaves uniformly. As a result, the more the inserted points are, the more paths are activated due to bad packing. 6.1.1.5. Storage overhead. In addition, the WeR-trees owe their good performance to the storage utilization 70.4% of their resources at most, WeR-trees reach up to 98.1%. Fig. 13 shows the storage utilization for all the data sets used in our experiments, after the insertion of all the data points in each case. The superiority of improvement in the uniform data file but still is much worst than the WeR-trees. 6.1.1.6. Cache performance. In Fig. 14 , the range query performance with the six query files (Q1) X (Q6) with respect to the size of cache is given. This is our last experiment involving two-dimensional points with which we want to show that WeR family trees are better independently of any cache size. Here, we use the LB real data set.

Fig. 15 presents the k -nearest neighbor query performance (using the NE real data set) with respect to the size of cache as well to enhance our claim that the performance of WeR trees is better in any cache size. Similar results appear for all data files in both cases. 6.1.1.7. Higher dimensionality data sets. In the previous paragraphs, we presented experimental results on two-dimensional data sets. In the following, we consider three-and four-dimensional points. We used a data set of 123,000 three-dimensional points that follow uniform distribution. We measured the range query and k -near-est neighbor performance with respect to the cache size. For the range queries, we used nine query files (Q1) X  (Q9) that consist of 100 window queries each. Every window query in each file is distributed uniformly over the whole data space and has volume equal to 0.001%, 0.01%, 1%, 1.5%, 1.8%, 2%, 5%, 10% and 20% of the whole data volume respectively.
 Fig. 16 depicts the results. Obviously, the WeR family trees have better performance compared to this of
R * -tree: saving gains vary from 23% to 32%. Analogous conclusions are derived from Fig. 17 , where the k -nearest neighbor performance is displayed with respect to cache size: gains reach up to 35%.
Finally, we examined the performance of our structures regarding four-dimensional points. For the follow-ing experiments we used a data set that contains 123,000 4D points. These points are distributed uniformly over the whole data volume. We measured the range query performance with the nine query files (Q1) X  (Q9) that we have already defined in the previous paragraph, varying the size of cache. The results are shown in Fig. 18 . We observe that WeR-A is much better than R * -tree. The performance of WeR-B is similar to this Saving gains reaches up to 35%.

Our last experiment invokes k -nearest neighbor queries. The results are depicted in Fig. 19 with 15% to 35%.
 7. Conclusion
We presented a new scheme, called WeR-tree, for the dynamic manipulation of large data sets. WeR-trees are the first members of the R-tree family which employ the technique of partial rebuildings, i.e., a gradual reconstruction of subtrees in a carefully triggered way. The update operations are theoretically investigated and proven to be of O  X  log M nBL  X  n  X  = n  X  amortized complexity. Additionally, the basic geometric queries are thoroughly tested with carefully designed experiments which confirm the applicability of the proposed method 35% on real data sets and 15 X 85% on synthetic data, k-nn queries are 27 X 49% faster, during mixed operations performance savings are between 18.5% and 20.8%, while the scheme scales up linearly with respect to the number of inserted points achieving 24 X 38% fewer page accesses. These gains resulted from a careful employ-ment of a top-down reconstruction method. For future work, we would like to investigate on how repeated reconstruction can be tuned in conjunction with designing specially tailored bulk-loading methods. Towards this end, merging subtree methods [24] may be proven helpful.
 Acknowledgements
The authors would like to thank D. Katsaros and A. Nanopoulos for their useful comments and construc-tive discussions.

References
