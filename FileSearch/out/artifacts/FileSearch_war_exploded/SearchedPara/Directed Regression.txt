 When used to guide decision-making, linear regression analysis typically treats estimation of re-coefficients are then used to optimize decisions.
 it is beneficial to account for the decision objective when computing regression coefficients. Im-number of features is typically restricted in order to avoid over-fitting.
 Empirical optimization (EO) is an alternative to OLS which selects coefficients that minimize em-insufficient.
 In this paper, we propose a new algorithm  X  directed regression (DR)  X  which is a hybrid between lected by OLS and those by EO. The weights of OLS and EO coefficients are optimized via cross-validation.
 We study DR for the case of decision problems with quadratic objective functions. The algorithm together with a quadratic loss function that depends on decision variables and response variables. Regression coefficients are computed for subsequent use in decision-making. Each future decision depends on newly sampled feature vectors and is made prior to observing response variables with the goal of minimizing expected loss.
 We present computational results demonstrating that DR can substantially outperform both OLS and features. In some cases, OLS and EO deliver comparable performance while DR reduces expected loss by about 20% . In none of the cases considered does either OLS or EO outperform DR. We also develop a theory that motivates DR. This theory is based on a model in which selected features do not perfectly capture relationships among response variables. We prove that, for this model, the optimal vector of coefficients is a convex combination of those that would be generated by OLS and EO. the linear combination We restrict attention to cases where M &gt; 1 , with special interest in problems where M is large, because it is in such situations that DR offers the largest performance gains.
 We consider a setting where the regression model is used to guide future decisions. In particular, symmetric. We aim to minimize expected loss, assuming that the conditional expectation of y given x is The question is how best to compute the regression coefficients r for this purpose. To motivate the setting we have described, we offer a hypothetical application.
 Example 1. Consider an Internet banner ad campaign that targets M classes of customers. An average revenue of y m is received per customer of class m that the campaign reaches. This quantity across customers classes; for example, they could capture customer preferences as they relate to ad content or how current economic conditions affect customers. For each m th class, the cost of  X  m is a known constant.
 The application we have described fits our general problem context. It is natural to predict the response vector y using a linear combination r in u and y : One might ask why not construct M separate linear regression models, one for each response vari-where regression coefficients are shared across multiple response variables, are sometimes referred to as general linear models and have seen a wide range of applications [7, 8]. It is well-known involving a single response variable [7]. Ordinary least squares (OLS) is a conventional approach to computing regression coefficients. This would produce a coefficient vector Note that OLS does not take the decision objective into account when computing regression coeffi-so. This approach minimizes empirical loss on the training data: Instead it focusses on decision loss that would be incurred with the training data. Both r OLS and As we will see in our computational and theoretical analyses, OLS and EO can be viewed as two extremes, each offering room for improvement. In this paper, we propose an alternative algorithm  X  directed regression (DR)  X  which produces a convex combination r DR = (1  X   X  ) r OLS +  X r EO of coefficients computed by OLS and EO. The term directed is chosen to indicate that DR is influ-enced by the decision objective though, unlike EO, it does not simply minimize empirical loss. The on validation data. Average loss is a convex quadratic function of  X  , and therefore can be easily minimized over  X   X  [0 , 1] .
 OLS. As such, DR addresses issues similar to those that have motivated work in data-driven robust optimization, as surveyed in [3]. Our focus on making good decisions despite modeling inaccuracies also complements recent work that studies how models deployed in practice can generate effective decisions despite their failure to pass basic statistical tests [4]. In this section, we present results from applying OLS, EO, and DR to synthetic data. To generate a data set, we first sample parameters of a generative model as follows: as follows: C feature vectors, only K  X  P are used in the regression model.
 Figure 1: (a) Excess losses delivered by OLS, EO, and DR, for different numbers N of training samples. (b) Excess losses delivered by OLS, EO, and DR, using different numbers K of the 60 features. us to make comparisons in percentage terms.
 We carried out two sets of experiments to compare the performance of OLS, EO, and DR. In the first set, we let M = 15 , L = 15 , P = 60 , Q = 20 ,  X  w = 5 , and K = 50 . For each N  X  and loss function. With DR,  X  is selected via leave-one-out cross-validation when N  X  20 , and via the excess loss incurred by DR is never larger than that of OLS or EO. Further, when N = 20 , the excess loss of OLS and EO are both around 20% larger than that of DR. For small N , OLS is as effective as DR, while, EO becomes as effective as DR as N grows large.
 losses averaged over trials. Note that when K = 55 , DR delivers excess loss around 20% less than EO and OLS. When K = P = 60 , there are no missing features and OLS matches the performance of DR.
 a function of N and K . As the number of training samples N grows, so does  X  , indicating that DR is weighted more heavily toward EO. As the number of feature vectors K grows,  X  diminishes, indicating that DR is weighted more heavily toward OLS. this model, optimal coefficients are convex combinations of r OLS and r EO . As such, our model and analysis motivate the use of DR. 5.1 Model features, X  and a representative future observation. We then formulate an optimization problem where unavailable when computing regression coefficients. However, we will later establish that optimal average values of selected  X  , using different numbers K of the 60 features. coefficients are convex combinations of r OLS and r EO , each of which can be computed without observing missing features. Since directed regression searches over these convex combinations, it should approximate what would be generated by a hypothetical algorithm that observes missing features.
 We will assume that each feature, whether observed or missing, is a linear function of an  X  X nfor-mation vector X  drawn from &lt; Q . Specifically, the N training data samples depend on information an inner product for such matrices. In particular, we define the inner product between matrices A and B by Our generative model takes several parameters as input. First, there are the number of samples N , the number of response variables M , and the number of feature vectors K . Second, a parameter  X   X  this were not the case, observed features would provide information about missing features. Second, J -dimensional subspace from which they are drawn. In other words, all directions in that space are equally likely.
 selecting regression coefficients  X  r  X &lt; K that solve  X  r may depend on N , M , K ,  X  Q ,  X  r ,  X   X  ,  X  w , and O . 5.2 Optimal Solutions Our primary interest is in cases where prior knowledge about the coefficients r  X  is weak and does asymptotically large. Hence,  X  r will no longer depend on  X  r .
 establishes that these extremes are delivered by OLS and EO.
 Theorem 1. For all N , M , K ,  X  Q ,  X  w , and O , and Note that  X   X  represents the degree of bias in a regression model that assumes there are no missing features. Hence, the above theorem indicates that OLS is optimal when there is no bias while EO require knowledge of Q or  X  w .
  X  r Theorem 2. For all N , M , K ,  X  Q ,  X  w ,  X   X  , and O , where  X  = 1 Our two theorems together imply that, with an appropriately selected  X   X  [0 , 1] , (1  X   X  ) r OLS + missing features or requiring knowledge of Q ,  X   X  , or  X  w . 5.3 Interpretation It can be shown that r OLS is a biased estimator of r O , while r EO is an unbiased one. However, value of  X  provided in Theorem 2. In particular, as the number of training samples N increases, variance diminishes and  X  approaches 1 , placing increasing weight on EO. On the other hand, as the number of observed features K increases, model bias decreases and  X  approaches 0 , placing increasing weight on OLS. Our experimental results demonstrate that the value of  X  selected by cross-validation exhibits the same behavior. Though we only treated linear models and quadratic objective functions, our work suggests that there can be significant gains in broader problem settings from a tighter coupling between machine learning and decision-making. In particular, machine learning algorithms should factor decision classes of models and objectives.
 of model bias, one might select an enormous set of features and apply a method like the lasso [10] might be ameliorated by generalizations of DR. There is also the concern that data requirements grow with the size of the large feature set, albeit slowly. Understanding how to synthesize DR with subset selection methods is an interesting direction for future research.
 by Theorem 2. More general work on the selection of convex combinations of models (e.g., [1, 5]) may lend insights to our setting.
 Let us close by mentioning that the ideas behind DR ought to play a role in reinforcement learning (RL) as presented in [9]. RL algorithms learn from experience to predict a sum of future rewards called approximate value function is then used to guide sequential decision-making. The problem we addressed in this paper can be viewed as a single-period version of RL, in the sense that each decision incurs an immediate cost but bears no further consequences. It would be interesting to extend our idea to the multi-period case.
 We thank James Robins for helpful comments and suggestions. The first author is supported by a Stanford Graduate Fellowship. This research was supported in part by the National Science Foun-dation through grant CMMI-0653876.
 Let X =  X  each column of Z . Because r  X  , r  X  X  X  , O are jointly Gaussian, as  X  r  X  X  X  , we have  X  Taking  X   X   X  0 and  X   X   X  X  X  yields The first part of the theorem then follows because We now prove the second part. Note that Since the residual Y 0 = Y  X  XX  X  Y  X  ZZ  X  Y upon projecting Y onto span { col X, col Z } is have and comparing the resultant expression with ( 7 ) and ( 8 ) yield the desired result. [1] J.-Y. Audibert. Aggregated estimators and empirical complexity for least square regression. [2] P. L. Bartlett and S. Mendelson. Empirical minimization. Probability Theory and Related [3] D. Bertsimas and A. Thiele. Robust and data-driven optimization: Modern decision-making [4] O. Besbes, R. Philips, and A. Zeevi. Testing the validity of a demand model: An operations [5] F. Bunea, A. B. Tsybakov, and M. H. Wegkamp. Aggregation for Gaussian regression. The [6] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other [7] K. Kim and N. Timm. Univariate and Multivariate General Linear Models: Theory and [8] K. E. Muller and P. W. Stewart. Linear Model Theory: Univariate, Multivariate, and Mixed [9] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction . MIT Press, Cam-[10] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of Royal Statistical
