 Discovering significant patterns that exist implicitly in huge spatial databases is an important computational task. A common approach to this problem is to use cluster analy-sis. We propose a novel approach to clustering, based on the deterministic analysis of random walks on a weighted graph generated from the data. Our approach can decom-pose the data into arbitrarily shaped clusters of different sizes and densities, overcoming noise and outliers that may blur the natural decomposition of the data. The method requires only O(n log n) time, and one of its variants needs only constant space. 
Spatial data are the data related to objects that occupy space [7]. Advances in database technologies have resulted in huge amounts of spatial data, and knowledge discovery techniques become essential tools for successful analysis of spatial data bases. This paper deals with clustering, which is one of the central techniques in spatial data mining. 
Clustering methods are used to discover natural groups in data sets, and to identify abstract structures that might reside there, without having any background knowledge of the characteristics of the data. Prior literature on the clus-tering problem is huge, see e.g., [5]. However, to a large extent the problem remains elusive, and there is still a dire need for a clustering method that is natural and robust, yet very efficient in dealing with large data sets. 
The characteristics of spatial data pose several difficulties for clustering algorithms. Since we are seeking a natural decomposition, the clusters may have arbitrary shapes and non-uniform sizes. Moreover, different clusters may have different densities. Another issue is the existence of noise, which may interfere the clustering process and should be identified. Regarding complexity, the huge sizes of spatial databases imply the need for very efficient clustering algo-*For a full version of this paper see [4] requires prior specific permission and/or a t~e. KDD 01 San Francisco CA USA Copyright ACM 200t 1-58113-391-x/01/08...$5.00 rithms. Furthermore, it is impractical to assume that the entire database can reside in the main memory all at once. 
In this paper, we present a new approach to clustering spatial data, based on deterministic exploration of random walks on a weighted graph associated with the data. The heart of the method is in what we shall be calling separating operators, which are applied to the graph iteratively. Their effect is to 'sharpen' the distinction between the weights of inter-cluster edges (those that ought to separate clusters) and intra-cluster edges (those that ought to remain inside a single cluster), by decreasing the former and increasing the latter. The operators can be used on their own or can be embedded in a classical agglomerative clustering framework. 
The resulting algorithms are simple, fast and general, and seem to cope successfully with the unique difficulties com-mon to spatial data. We exhibit encouraging results of ap-plying these algorithms to several recently published data sets. 
We will be using standard graph-theoretic notions. Specif-ically, let G(V, E, w) be a weighted graph. The w is a weight-ing function w : E ~ R, that measures the similarity be-tween pairs of items (a higher value means more similar). Let S C_ V. The set of nodes that are connected to some node of S by a path with at most k edges is denoted by Vk(S). The degree of G, denoted by deg(G), is the max-imal number of edges incident to some single node of G. The subgraph of G induced by S is denoted by G(S). The edge between i and j is denoted by (i,j). Sometimes, when the context is clear, we will write simply (i,j I instead of (i, j) E E. A random walk is a natural stochastic process on graphs. Given a graph and a start node, we select a neighbor of the node at random, and 'go there', after which we continue the random walk from the newly chosen node. The probability of a transition from node i to node j, is p~j = ~'J) where di = ~(i,k) w(i, k) is the weighted degree of node i. 
Now, denote by P~isit(i) E R n the vector whose j-th com-ponent is the probability that a random walk originating at i will visit node j in its k-th step. 
The escape probability from a source node s to a target node t, denoted by Pe~p~(s, t), is defined as the probabil-ity that a random walk originating at s will reach t before returning to s. This probability can be computed as follows. 
The values of pi are calculated by solving these equa-
Our method, as many other hierarchical clustering algo-
The Delaunay triangulation (DT) of a point set is the dual 
The k-mutual neighborhood graph contains all edges (a, b) 
Regarding edge weights, we adopt a commonly used ap-
Our approach to identifying natural clusters in a graph is 
Any set of edges F C E gives rise to an induced cluster-
We have decided to concentrate on discovering a set of 
The strategy we propose for identifying separators is to 
Wc now offer two methods for performing the edge sepaxa-
Now, in order to estimate the closeness of two nodes v and &lt;k and P~sit(u). The smaller the difference the greater the same time. However, if we are willing to sum some steps of the two walks, we may find that they visit roughly the same nodes. We now define the separating operator itself: 
DEFINITION 4.2. Let G(V, E, w) be a weighted graph and k be some small constant. The separation of G by neigh-borhood similarity, denoted by NS(G), is defined to be: NS(G) ~'~ G,(V, E, w~), Here, slink(Z, y~ is some similarity measure of the vectors and if, whose value increases as ~ and ff become more similar. A suitable choice is: The norm L1 is defined in the standard way: For ~, b E R n, 
Another suitable choice is the cosine, or the correlation, of  X  and if, defined as: where (., .) denotes the inner-product. 
The key component in computing NS(G) is the calcula-in time and space O(deg(G)a), which is independent of the size of G and can be treated as a constant. Hence, NS(G) can be computed in space O(1) and time O(IEI) , which in this case is just @(n). CE: Separation by circular escape An alternative method for capturing the extent of intimacy between nodes u and v, is by the probability that a random walk that starts at v visits u exactly once before returning to v for the first time. (This notion is symmetric, since the event obtained by exchanging the roles of v and u has the same probability.) Ifv and u are in different natural clusters, the probability of such an event will be low, since a random walk that visits v will likely return to v before reaching u (and the same with u and v exchanged). The probability of this event is given by: 
Seeking efficient computation, and on the reasonable as-sumption that data relevant to the intimacy of v and u lies in a relatively small neighborhood around v and u, we can constrain our attention to a limited neighborhood, by the following: 
DEFINITION 4.3. Let G(V, E,w) be a graph, and let k be some constant. Denote by P(k~2ape(V,U) the probability Pescape(v, u), but computed using random walks on the sub-graph G(Vk( {v, u})) instead of on the original graph G. The circular escape probability of v and u is defined to be: We can now define separation by circular escape: 
DEFINITION 4.4. Let G(V, E, w) be a weighted graph, and let k be some small constant. The separation of G by circular escape, denoted by CE(G), is defined to be: 
For graphs with bounded degree, the size of G(Vk(v, u)) is independent of the size of G, so that CEk(v, u) can be com-puted essentially in constant time and space. Hence, as with NS(G), the separating operator CE(G) can be computed in time O(IEI) --O(n) and space O(1). 
Our experiments show that in general the CE operator yields better results than the NS operator. However, the computation of the CE operator is clearly more complicated in terms of numerical precision, as well as running time. 
The idea of separating operators is to uncover and bring to the surface a closeness between nodes that exists implicitly in the structure of the graph. Separating operators increase the weights of intra-cluster edges and decrease those of inter-cluster ones. Iterating the separating operators sharpens the distinction further. After a small number of iterations we expect the difference between the weights of the two kinds of edges to differ sufficiently to be readily apparent, be-cause the weights of separators are expected to diminish significantly. Moreover, Iterating the separating operators causes information from distant parts of the graph to 'flow in', reaching the areas where separating decisions are to be made. A detailed demonstration of activating the separating operators on two graphs is given in [4]. 
We now illustrate the ability of our method to cluster "correctly" 2D points, in a number of typical cases, some of which have been shown to be problematic for agglomerative methods [6]. (More extensive examples are given in Section 6.) We show only examples in 2D, although the method works well in higher dimensions too, because two dimensions are easier to visualize and evaluate. 
We have used 10-mutual neighborhood graphs for model-ing the points ( intersection with the Delaunay triangulation gives similar results). The results are achieved using 3 it-erations of either CE or NS, with k = 3. For NS, we took the function sire(., .) to be f(., .). In general, other choices work equally well. 
The partition of the edges into separators and non-sepa-rators is based on a threshold value, such that all the edges whose weight is below this value are declared as separators. Without loss of generality, we may restrict ourselves to the O(n) edge weights as candidates for being thresholds. The actual threshold value (or several, if a hierarchy of decom-positions is called for), is found by some statistical test, e.g., inspecting the edge-weight frequency histogram, where the frequency of the separators' weights is usually smaller, since most of the edges are inside the clusters, and have higher weights than those of the separators. 
Figure 2 shows the clustering decomposition of three data sets using our algorithm. 
The data set DS1 shows the inherent capability of our algorithms to cluster at different resolutions at once, i.e., to detect several groups with different intra-group densities. This ability is beyond the capabilities of many clustering al--gorithms that can show the denser clusters only after break-ing up the sparser clusters. Data set DS2 demonstrates the ability of our algorithm to separate the two left hand side clusters, despite the fact that the distance between these clusters is smaller than the distance between points inside the right hand side cluster. 
The data set DS3 exhibits the capability of our algorithm to take into account the structural properties of the data set, which is the only clue for separating these evenly spaced points. , :~3 .............  X  3' ........................ ,  X  ,, 
When there is a hierarchy of suitable decompositions, our method can reveal it by using a different threshold for each level of the hierarchy. For example, consider the two data sets in Figure 3. For each of these we have used two different thresholds, to achieve two decompositions. 
It is noteworthy that the general methodology of revealing the graph structure by exploration of random walks is fun-damental and robust enough to be applied to the clustering of spatial data, even though our use of spatial properties of the data is minimum. 
One should observe that the memory requirements of our clustering method are low, which is very important for huge spatial data bases that cannot fit in main memory. The only "global" operation in the process is that of computing connected components after removing separators. Since this operation does not require too many accesses to each single node, it can be performed quite efficiently without copying the entire data base into main memory. 
Agglomerative clustering is a well-known hierarchical clus-tering method that starts from the trivial partition of n points into n clusters of size 1 and continues by repeatedly merging pairs of clusters. At each step the two clusters that are most similar are merged, until the clustering is satisfac-tory. Different similarity measures between clusters result in different agglomerative algorithms. 
The separation operators can be used as a preprocess-Figure 3: Clustering at multiple resolutions using different thresholds. When values of CE are differ-ent from values of NS, the CE values are given in parentheses. CE values are multiplied by 100. ing stage before activating agglomerative clustering on th~ graph. Such a preprocessing sharpens the edge weights, adding structural knowledge to them, and greatly enhances the agglomerative algorithms, as it can effectively prevent bad local merging opposing the graph structure. 
Implementation of the agglomerative algorithm can be done using a dynamic graph structure. At each step we take the edge of the highest weight, merge ("contract") its two endpoints, and update all the adjacent edges. When contracting nodes u and v having a common neighbor t, the way we determine the weight of the edge between t and the contracted node uniquely distinguishes between differ-ent variants of the agglomerative procedure. For example, when using maximal similarity -"single link", we take this weight as max{w(v, t), w(u, t)}, while when using total sim-ilarity we fix the weight as w(v, t) + w(u, t). For a bounded degree graph, which is our case, each such step can be car-ried out in time O(logn), using a binary heap. 
It is interesting that the clustering method we have de-scribed in the previous section is in fact completely equiv-alent to a "single link" algorithm preceded by a separation operation. Hence we can view the integration of the sepa-ration operation with the agglomerative algorithm as a gen-eralization of the method we have discussed in the previous section, that enables us to use any variant of the agglomer-ative algorithm. 
We have found particularly effective the normalized to-tal similarity variant, in which we measure the similarity between two clusters as the total sum of the weights of the original edges connecting these clusters. We would like to eliminate the tendency of such a procedure to contract pairs of nodes representing large clusters whose connectiv-ity is high due to their sizes. Accordingly, we normalize the weights by dividing them by some power of the sizes of the relevant clusters. More precisely, we measure the similarity of two clusters C1 and C2 by: where w(C~, Cz) is the sum of original edge weights between 
C~ and C2, and d is the dimension of the space in which the points lie. We took ~ and ~ as an approxima-tion of the size of the boundaries of the clusters C~ and Cz, respectively. 
The overall time complexity of our algorithm is O (n log n), which includes the time needed for constructing the graph and the time needed for performing n contractions using a binary heap. This equals the time complexity of the method described in the previous section (because of the graph con-struction stage). However, the space complexity is now worse. We need ~(n) memo~T for efficiently handling the binary heap. 
An agglomerative clustering algorithm provides us with a dendrogram, which is a pyramid of nested clustering de-compositions. The question of which are the meaningful decompositions inside the dendrogram, still remains. 
Each level in the dendrogram is constructed from the level below, by merging two clusters. We associate with each level a grade that measures the importance of that level. Inspired by the work of [3], a rather effective way of measuring the im-portance of a level is by evaluating how sharp is the change that this level introduces to the clustering decomposition. 
Since changes that are involved with small clusters do not have a large influence, we define the prominency rank level in the dendrogram, in which the clusters C~ and Cj of the level below were merged, as: 
We demonstrate the effectiveness of this measure in the next section. 
In this section we show the results of running our algo-rithm on several data sets from the literature. We modeled these data sets using the intersection of the Delaunay trian-gulation and the 15-mutual neighborhood graph. For all the results we have used total similarity agglomerative cluster-ing, preceded by 2 iterations of the NS separation operator with k = 3 and similarity function defined as cos(., .). ing the CE operator, changing the value of k, or increasing the number of iterations, do not have a significant effect on the results. Using the method described in Section 4 may change the results in few cases. 
We implemented the algorithm in C++, running on a Pen-tium III 800MHz processor. The code for constructing the Delaunay triangulation is of Triangle, which is available from reader is encouraged to see the full electronic version of this and clearer format, and in color X  
Figure 4 shows the results of the algorithm on data sets taken from [6]. These data sets contain clusters of different shapes, sizes and densities and also random noise X  A nice property of our algorithm is that random noise gets to stay inside small clusters. Alter clustering the data, the algo-rithm treats all the relatively small clusters, whose sizes are below half of the average cluster size, as noise, and simply omits them, showing only the larger clusters. 
Figure 5 shows the result of the algorithm applied to a data set from [2 I. We show two levels in the hierarchy, rep-resenting two possible decompositions. We are particularly happy with the algorithm's ability to break the cross shaped cluster into 4 highly connected clusters, as shown in Figure 
In Figure 6, which was produced by adding points to a data set given in [2], we show the noteworthy capability of the algorithm to identify clust.ers of different densities at the same level of the hierarchy. Notice that the intra-dis-tance between the points inside the right hand side cluster, is larger than the inter-distance between several other clus-ters. 
Throughout all the examples given in this section we have used the prominency rank introduced in Section 5 to reveal the most meaningful levels in the dendrogram. Figure 7 demonstrates its capability with respect to the data set DS4 (shown in Figure 4). We have chosen the five levels with the highest prominency ranks, and for each level we show the level that precedes it. It can be seen that these five levels are exactly the five places where the six large natural clusters are merged. In this figure we have chosen not to hide the noise, so the reader can see the results of the algorithm before removing the noise. 
Table 6 gives the actual running times of the algorithm on the data sets given here. We should mention that our code is not optimized, and the running time can certainly be improved. 
Figure 4: Data sets taken from [6] (see [4] for larger, clearer color versions of this figure and of Figs. 5-7). Figure 5: Two different clusterings of a data set taken from [2] Figure 6: A data set with clusters of different den-sities Figure 7: A hierarchy containing five decomposi-tions of DS4 corresponding to the five levels with the highest prominency rank. (Level index indicates the number of clusters.) 
We have introduced a novel algorithm for achieving a hier-archical clustering of spatial data, using random-walk-based separating operators. This approach seems to have several advantages. 
First, it is robust in the presence of noise and outliers, and is flexible in handling data of different densities. It can reveal clusters of any shape without a special tendency to-wards spherically shaped clusters or ones of similar sizes. At the same time, the decisions the algorithm makes are based on the relevant structure of the associated graph, making it essentially immune to outliers and noise. 
The second advantage is the running time and space re-quirements. The time complexity of our algorithm applied to n data points is O(n log n), and its practical running time, in general, is very fast. We have been able to cluster 10,000 points in less than two seconds. Regarding space complex-ity, one of the variants of the algorithm can be applied with a constant amount of space. 
Since the algorithm does not rely on spatial knowledge, we plan to try it on other types of data. We have already used a variant of it for image segmentation with very encouraging results, and will report on it separately. [1] M. T. Dickerson and D. Eppstein, "Algorithms for [2] V. Estivill-Castro and I. Lee,"AUTOCLUST: [3] Y. Gdalyahu, D. Weinshall and M. Werman, [4] D. Harel and Y. Koren, "Clustering Spatial Data Using [5] A. K. Jain and R. C. Dubes, Algorithms for Clustering [6] G. Karypis, E. Han, and V. Kumar, "CHAMELEON: [7] X. Xu , M. Ester, H.P. Kriegel and J. Sander, 
