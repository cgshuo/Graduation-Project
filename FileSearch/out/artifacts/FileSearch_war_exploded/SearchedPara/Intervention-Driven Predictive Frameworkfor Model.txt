 Medical interventions cure us, and keep us alive. They form the cornerstone of modern medical practice. Doctors carefully study the clinical observations related to our illness, and perform interventions. To formulate the most effective post-discharge care plan, they assess the prognosis. For example, what is the risk of readmission? How long will this person live? Answering these questions requires risk prediction models.

A patient X  X  condition captures usual risk factors that can then be used in prognostic models. But medical interventions performed on patients are con-founding, changing the outcome and thus prediction rules. For example, different cancer treatments (such as r adiotherapy, chemotherapy or their combinations) have different prognosis profiles for the same tumor type [1]. Similarly, prognosis of cardiac patients for different procedur es are different [2]. Thus interventions should be taken into account when developing prediction models.

Traditionally, in the healthcare community both the patient conditions and interventions are augmented together and a single prediction rule is learnt [3]. A single rule, however, may not be expressi ve enough to capture differential rules due to different interventions. Current predictive methods, such as logistic re-gression (LR), Support vector machine (SVM), Na X ve Bayes (NB), and Random Forest (RF) require amalgamation of interventions with the patient condition variables, and suffer from the same limitation. At the other extreme, learning prediction rules for each intervention separately is not useful either -out of hun-dreds of unique interventions, all are not equally important and many of them are performed together (as groups of interventions) -for a variety of reasons in-cluding current treatment policies, hosp ital capacity and cost. This opens up the need to learn a set of intervention groups and group-specific prediction models.
Following this, we propose a nonparametric, supervised framework that uses a mixture distribution over interventions, learning a prediction model for each mixture component. A Dirichlet Process (DP) prior over interventions mixture is used allowing extraction of latent intervention groups, for which the number of groups is not known a priori . The outcome is then modeled as conditional on this latent grouping and patient condition data through a Bayesian logistic regression (B-LR). The use of DP also allows formation of new intervention groups when necessary, thus coping with cha nges in medical pract ice. In addition, the intervention based clustering inferred by the model is made predictive. This encourages formation of intervention groups that lead to a low prediction error. We refer to this model as DPM-LR. Efficient inference is derived for this model.
To evaluate our model, prediction of 30-day readmission on two retrospective cohorts of patients from an Australian hospital is considered: 2652 admissions related to Acute Myocardial Infarction (AMI) between 2007-2011 and 1497 ad-missions related to Pneumonia between 2009-2011. On both the cohorts, DPM-LR outperforms several baselines -dpMNL [4], Bayesian Logistic Regression, SVM, Na X ve Bayes and Random Forest. We show that the intervention groups discovered using DPM-LR are clinically meaningful. We also illustrate that the highest risk factors identified by DPM-LR for different intervention groups are different, validating the necessity of intervention-driven predictive modeling.
In summary, our main contributions are:  X  A nonparametric Bayesian, supervised prediction framework (DPM-LR) that  X  Efficient inference for DPM-LR is derived and implemented.  X  Validation on both synthetic and two real-world patient cohorts, demon-Hospital readmissions are common and costly. The 30-day readmission rate among the Medicare beneficiaries in the USA is estimated at 18%, costing $17 billion [5]. Some hospital readmissions are considered avoidable and thus 30-day readmission rates are used for benchmarking across hospitals, with financial penalties for hospitals with high risk-adjusted rates [5]. Avoidable readmissions can be avoided by appropriately planning post-discharge care [6]. This requires accurate risk prediction.

Few models exist in the healthcare community to predict 30-day readmis-sion risk in general medical patients [7,8,3]. All these methods employ Logistic Regression to derive a score based system f or risk stratification using retrospec-tive clinical and administrative data collected mainly from Electronic Health Records. Readmission prediction using o ther machine learning techniques such as SVM, Na X ve Bayes and Random Fores t have been studied respectively for heart-failure patients in [9] and for ten different diseases [10]. In all the meth-ods, both the patients condition and interventions are augmented together to learn a single prediction rule.

A single rule, however, may not be sufficient to model the effect of different interventions. On the contrary, learning prediction rules for each intervention is not necessary -out of all the unique interventions, many of them are performed together and only a few latent groups exist. This gives rise to the need to learn the set of intervention groups and group-specific prediction models. The intervention grouping can be learnt using a mixture distribution with a Dirichlet Process prior to account for the unknown number of groups.

The use of Dirichlet process (DP) has been previously studied for modeling a set of classifiers under mixture model settings. In an attempt to develop a nonlinear classifier, Shahbaba and Neal [4] use DP as a prior for dividing data in clusters learning a separate linear classifier for each cluster. This model (dpMNL) learns nonlinear boundaries through a piecewise linear approximation. The idea from this model can be adapted for dividing patients for different intervention groups. Instead of using a single feature for both clustering and classification, we can use interventions to cluster the patie nts, and learn separate classifiers using patient condition features for each of the intervention groups. We describe a prediction framework that learns a set of latent, predictive in-tervention groups and builds a prediction rule for each intervention group. In developing such a framework, our intention is to develop a predictive model that is flexible in modeling the effect of medical interventions on patient condition variables and outcome.

Typically, healthcare data has the following form: for each patient, we have a list of patient conditions (denoted by x ), a list of medical interventions (denoted by i ) and an outcome variable (denoted by y ). We denote the data as D = { ( x n , i n ,y n ) | n =1 ,...,N } where x n  X  R M x  X  1 , i n  X  R M i  X  1 .
To model the effect of interventions, we cluster the interventions into a set of predictive groups. A Dirichlet process mixture (DPM) over interventions is used to extract a set of latent intervention groups. The use of DPM allows us to form new intervention groups when necessary and thus copes with changes in hospital practices and policies. Further, the intervention-based clustering is made predictive so that it encourages formation of intervention groups that lead to a low predictive error. Given such clustering, we learn a separate classifier for each intervention group. We refer to this model as DPM-LR.

The generative process of DPM-LR can be described as follows: A random probability measure G is drawn from a Dirichlet process DP ( ,H ) where is a positive concentration parameter and H is a fixed base measure. Since we are using a DP prior, the random measure G is discrete with probability one [11]. In stochastic process notation, we can write: Stick-breaking construction of Dirichlet process [12] often provides more intuitive and clearer understanding of DP-based models. Using stick-breaking notation, the above generative process can be written as: where  X  k are independent random variables (also called  X  X toms X ) distributed according to H .Further,  X   X  k denotes an atomic measure at  X  k and  X  k are the  X  X tick-breaking weights X  such that k  X  k =1 . For our model, the variable  X  k takes values in a product space of two independent variables  X  k and w k .Thus, we can explicitly write  X  k  X {  X  k , w k } .ForDPM-LRmodel,the  X  k can be inter-preted as k -th  X  X ntervention topic X  while the w k is the classifier weight vector for k -th intervention topic. We model  X  k  X  Dir (  X  ) , i.e. a Dirichlet distribution with parameter  X  and w k  X  X  0 , X  2 w I , i.e. a multivariate normal distribution with zero mean and single standard deviation parameter  X  w . The two representations (the stochastic and the stick-breaking) can be tied by introducing an indicator variable z n such that n  X   X  z n . We summarize the generative process as:  X   X  GEM ( ) , (  X  k , w k ) iid  X  H (  X , X  w ) ,H (  X , X  w )= Dir (  X  )  X N 0 , X  2 w I (3)
For n =1 ,...,N where GEM distribution is named after the first letters of Griffiths, Engen and McCloskey [13]. Ber ( . ) and Dir ( . ) denote the Bernoulli and Dirichlet distribu-tions, respectively and f ( . ) denotes the logistic function. Graphical representa-tions of DPM-LR is shown in Figure 1. The inference of parameters in a fully Ba yesian model is performed by sampling them from their joint posterior distribut ion, conditioned on the observations. For DPM-LR model, this distribution does not take a closed form. A popular way to circumvent this problem is to approximate this distribution using Markov chain Monte Carlo (MCMC) sampling. Asymptotically, the samples obtained using MCMC are guaranteed to come from the true posterior distribution [14]. We use Gibbs sampling (a MCMC variant) -an algorithm that iteratively samples a set of variables conditioned upon the remaining set of variables and the obser-vations. The MCMC parameter state space consists of the variables {  X ,z, X , w } and the hyperparameters  X  , and  X  w . To improve the sampler mixing, we inte-grate out  X  ,  X  and only sample variables { z , w } and the hyperparameter .The hyperparameters  X  and  X  w are fixed to one. After the sampler convergence, we finally estimate  X  as it provides useful insights into different intervention groups.
Since our model uses a Dirichlet process (DP) prior, Gibbs sampling of variable  X  conditioned on other variables remains identical to the standard DP mixture model. However, due to the changes in the g enerative process caused by altering the model into a supervised setting, the Gibbs sampling updates for the variables z and w need to be derived. 4.1 Sampling z We sample the variable z n from Gibbs conditional posterior integrating out  X  and  X  from the model. For the assignment of z n ,thereare two possibilities: (1) the intervention i n is assigned to an existing intervention cluster, i.e. given K clusters, z n takes a value between 1 and K (2) the intervention i n is assigned to a new intervention cluster, i.e. z n is set to K +1 .Forthe former case, the Gibbs sampling updates can be obtained from the following posterior distribution: In the above posterior, three terms interact: intervention likelihood (how likely is the cluster k for intervention i n given other interventions), class likelihood (if i n is assigned to cluster k , how small would be the classification error for the k -th cluster) and the predictive prior (the prior probability of an intervention being assigned to the cluster k given other assignments). For the case when z n is assigned to a new cluster, the Gibbs sampling updates can be obtained from the following posterior distribution: The class likelihood term in the above expression requires integrating out a Bernoulli likelihood with respect to w K +1 . We approximate this integral numer-ically using Monte Carlo samples of w K +1 . 4.2 Sampling w k Using the generative process of (3-5), the Gibbs conditional posterior of w k can be written as: wherewedefine X k { x n | z n = k } , which contains the patient condition data from the k -th intervention group and x k i is the i -th data column of X k .Further, we have N k # { n | z n = k } and s k i f w T k x k i . The direct sampling from the above posterior is not possible as this does not reduce to any standard distri-bution. However, we can approximate the density using Laplace approximation [15,16]. The idea is to find the mode of the posterior distribution through an op-timization procedure and then fitting a Gaussian with its mean at the computed mode. Instead of optimizing the posterior directly, we optimize the logarithm of the posterior (results are unaltered due to monotonicity of logarithm), for which it is possible to compute the first and the second derivatives in closed form. The first and the second derivatives of the log posterior are given as: where D s ( w k ) diag with entries between 0 and 1 . For the above optimization, we use quasi-Newton (L-BFGS) method as it converges faster c ompared to steepest-descent given good initializations. The optimization solution (denoted as w  X  k )isusedasmeanofthe approximating Gaussian. The covariance matrix of the Gaussian is computed (in closed form) by taking the negative of the inverse of the Hessian of the log posterior, i.e.  X   X  w samples of w k are drawn from N w  X  k , X   X  w 4.3 Sampling  X  k , X  Sampling  X  k is not necessary for the prediction. H owever, since it provides useful insights into different intervention groups, we finally estimate (after the sampler of the m -th intervention in the k -th group. Sampling of the hyperparameter remains same as in standard DPM model. Further details can be found in [17]. 4.4 Prediction for New Observations After training the model with data D = { ( x n , i n ,y n ) | n =1 ,...,N } ,wehave samples sampled from the following distribution: The posterior p  X  z |  X  i , z ( l ) , can be computed similar to the corresponding terms in (6) as the model is not updated during the test phase. We perform experiments with a synthetic dataset and two hospital datasets. Baseline methods used for comparison are first presented followed by results on synthetic data. Finally, evaluation is performed on two patient cohorts. 5.1 Baselines We compare the predictive performance of DPM-LR with the following methods: (a) Standard DP-Multinomial Logit model, with Gaussian observation model (dpMNL)[4]. The method learns a nonlinear classifier with data constructed by augmenting patients condition and intervention features (b) An adaptation of dpMNL with Multinomial observation model (referred to as dpMNL(MM)) (c) Bayesian Logistic Regression (B-LR) (d) SVM with linear kernel (Linear-SVM) (e) SVM with 3rd order polynomial kernel (Poly3-SVM) (f) Na X ve Bayes (g) Random Forest. Weka implementation [18] is used for the SVM, Naive Bayes and the Random Forest. For all the base lines, the feature vector is created by merging the patient condition and intervention features. 5.2 Experiments with Synthetic Data The synthetic dataset spans 5 years with 100 unique patients per year. Nine different interventions are considered. S ix intervention topics are created from horizontal and vertical bar patterns of a 3x3 matrix (Fig 2a). Per patient  X  X nter-vention X  feature is synthesized by sampling an intervention topic from a uniform mixture distribution and then sampling 4 interventions from the selected inter-vention topic. Each intervention topic is considered as an intervention group. The classification weight vector of each group is sampled from a 50-variate Nor-mal distribution. The  X  X atient condition X  feature is randomly sampled from a set of 10 distinct random binary vectors. The label (or outcome) is computed by combining the group-specific classifier with patient data following (5). The pre-diction task is to predict labels for the patients in the 5th year. Default settings from Weka is used for SVM, Na X ve Bayes and the Random Forest.

DPM-LR outperforms all the baselines (Table 1) in terms of AUC (Area un-der the ROC curve). DPM-LR outperforms (AUC 0.942) the closest contender, Random Forest (AUC 0.873). The performance of standard dpMNL (AUC 0.630) with Gaussian observation model was poor, however, the adapted version with multinomial observation model did reasonably well (AUC 0.836). All the other methods performed poorly (AUC&lt;0.750). Figure 2b shows the number of inter-vention topics sampled over 1000 Gibbs iterations (including 500 burnins). It can be seen that the convergence to the true number of topics is achieved quickly ( i.e. the mode of the number of groups ( K m ) remains unchanged after about 50 iterations), implying stable estimate of the posterior. Intervention topics inferred by the DPM-LR closely match true intervention topics (Figures 2a). 5.3 Experiments with Hospital Data The data is collected from a large public hospital 1 in Australia. The hospi-tal patient database provides a single point of acces s for information on patient hospitalizations, emergency department visits, in-hospital medications and treat-ments. Detailed records of these patient interactions with the hospital system are available through the EMR. This includes International Classification of Disease 10 (ICD-10) codes 2 , Diagnosis-related Group (DRG) codes of each admission, ICD-10 codes for each emergency visit, details of procedures, and departments that have been involved in the patient X  X  care. Other information includes demo-graphic data (age, gender, and occupation ) and details of the patient X  X  access to primary care facilities.
 Cohort 1: Acute Myocardial Infarction (AMI). The patient cohort con-sists of 2652 consecutive admissions with confirmed diagnosis of Acute Myocar-dial Infarction (AMI) admitted between 1st January 2007 and 31st December 2011. For each patient, we have a sequence of interactions with the hospital sys-tem. Of these, the discharge corresponding to an admission with primary reason for admission as AMI is treated as assessment points (APs) from which predic-tion is made. Patient records prior to an AP are used to construct features. The  X  X atient condition X  feature contains demographic (age, gender and occupation) and disease information (ICD-10 codes) for each admission, accumulated at four different time scales -past 1 month, past 3 months, past 6 months and past 1 year. The  X  X ntervention X  feature cons ists of procedure codes associated with only the current admission. The label is set to one if there are any readmissions in 30-day period following an AP with a cardiac related diagnosis. Readmission rate in this cohort varied from 11.7% (2007) to 4.8% (2011).
 Experimental Results. Patient data from 2007-2010 are used for training and patient data from 2011 for testing. The comparative results with the baselines (Table 2) shows that DPM-LR outperforms all other methods.

DPM-LR is better (AUC 0.677) than the the closest contender dpMNL(MM) (AUC 0.641) by a significant margin. This is followed by dpMNL (AUC 0.635) and B-LR (AUC 0.607). All other methods have AUC less than 0.6. Surprisingly, more complex models such as SVM with polynomial kernel and the Random Forest perform the worst.
 Table 3 lists the 5 strongest risk factors for the three intervention groups. These risk factors are the patient condition features that correspond to the largest positive weights in the linear regression model. We can see from the table that the strongest risk factors for different intervention groups are different. This vindicates the need of modeling intervention-specific prediction rules. Cohort 2 -Pneumonia. This cohort consists of 1497 admissions with con-firmed diagnosis of Pneumonia, admitted between 1st January 2009 and 31st December 2011. Similar to AMI, the discharges corresponding to an admission with primary reason for admission as Pneumonia is treated as the assessment points (APs) from which prediction is made. Patient records prior to an AP are used to construct the features, in a similar fashion as in the AMI cohort described in the previous section. The label is set t o one if there are any readmissions in 30-day period following an AP with respiratory related diagnosis. Readmission rate in this cohort varied between 5-6% over the study years (2009-2011). Experimental Results. The model is trained using patient data from 2009-2010 and then tested on patient data from 2011. The comparative results with the baselines are presented in Table 4. Once again, DPM-LR outperforms (AUC 0.667) the closest contender dpMNL(MM) (AUC 0.664).

DPM-LR learns two intervention groups. The risk factors corresponding to these two intervention groups are different (Table 5) -a point that was also observed for AMI cohort.
 We present a novel predictive framework for modeling healthcare data in the presence of medical interventions. Thi s framework automatically discovers the latent intervention groups and builds group-specific prediction rules. A Dirichlet process mixture used over the intervent ion groups ensures that new groups are created when a new intervention is int roduced. The prediction rule is learnt using patients condition data through a Bayesian logistic regression. Efficient inference is derived for this model. Experiments demonstrate that this method outperforms state-of-the-art baselines in predicting 30-day hospital readmission on two cohorts -Acute Myocardial Infarction and Pneumonia. As a future work, it would be interesting to explore the performance improvement through sharing across various intervention groups using Bayesian shared subspace learning [19].
