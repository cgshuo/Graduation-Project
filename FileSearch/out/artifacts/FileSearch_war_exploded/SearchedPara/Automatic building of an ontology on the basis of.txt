 Aurawan Imsombut  X  Asanee Kawtrakul Abstract This paper presents a methodology for automatic learning of ontologies from Thai text corpora, by extraction of terms and relations. A shallow parser is used to chunk texts on which we identify taxonomic relations with the help of cues: lexico-syntactic patterns and item lists. The main advantage of the approach is that it simplify the task of concept and relation labeling since cues help for identifying the ontological concept and hinting their relation. However, these techniques pose certain problems, i.e. cue word ambiguity, item list identification, and numerous candidate terms. We also propose the methodology to solve these problems by using lexicon and co-occurrence features and weighting them with information gain. The precision, recall and F-measure of the system are 0.74, 0.78 and 0.76, respectively. Keywords Thai ontology learning Lexico-syntactic patterns Taxonomic list 1 Introduction Ontology has a crucial role to play in information retrieval, however, its building by experts is an expensive task, and also a never-ending process which relies on users X  requirements and evolution of knowledge especially in science. Hence we suggest learning ontologies automatically in order to spare experts the bulk of the job.
Texts are a valuable resource for extracting ontologies as they contain a lot of information concerning concepts and their relationships. In this work, we propose a methodology for extracting ontological concepts and taxonomic relations by using explicit cue expressions, i.e. lexico-syntactic patterns and an item list (bullet list and numbered list). There are several difficulties when using lexico-syntactic patterns as cue words are ambiguous and candidate terms numerous. Also, item lists may be difficult to identify as well as detection of hypernym from candidate term list. We propose a method by using the lexicon and co-occurrence features and information gain for weighting the features. Afterward, we select the appropriate related terms from a list of candidates, the one with the most likely hypernym value. We tested the system by using Thai corpora in the domain of agriculture.

Section 2 of this paper presents the related works of ontology learning from unstructured text. Section 3 describes difficulties with extraction of an ontology in Thai. In Sect. 4 , we propose methods for automatically building such an ontology from corpora. The experimental results are discussed in Sect. 5 . 2 Related works There are a number of proposals to build ontologies from unstructured text. The first one to propose the extraction of semantic relations by using patterns was Hearst ( 1992 ). She proposed a method for acquiring automatically hyponym relations by identifying a set of frequently used unambiguous lexico-syntactic patterns in the form of regular expressions. Moreover, Pantel and Pennacchiotti ( 2006 ) proposed a bootstrapping algorithm to detect new patterns in each iteration. Secondly, clustering techniques have often been used for the same task (Agirre et al. 2000 ; Bisson et al. 2000 ; Nedellec 2000 ). For example, Nedellec ( 2000 ) learned semantic knowledge by using clustering features in the form of sub-categorization frames of verbs. This approach allows to process a huge set of data and a lot of features, but it needs an expert to label each cluster node and each relationship name. Another approach combines many techniques, and seems to be the most promising ones in this area. Maedche and Staab ( 2001 ) proposed an algorithm based on statistical techniques and association rules of data mining technology for detecting relevant relationships between ontological concepts. Navigli et al. ( 2003 ) applied statistical and machine learning techniques for disambiguating word senses to build an ontology in the domain of tourism. Girju et al. ( 2003 ) used patterns and machine learning techniques to learn semantic constraints for the automatic discovery of part-whole relation (meronymy). The advantage of the combined techniques approach is that it can extract taxonomic and non taxonomic relations, but it also needs a lot of examples for the system to learn and a knowledge base of the size of WordNet. Shinzato and Torisawa ( 2004 ) presented an automatic method for acquiring hypernymy relations from item list of HTML documents. They used statistical measures and some heuristic rules. For this paper, the proposed methods are very close to the pattern-based approach, but since there are many problems in Thai language due to the ambiguity of cue words of patterns and the candidate terms selection, we present an additional method for solving these problems. In order to extract more complete information concerning concepts and relations, we suggest extracting the ontology from item lists, especially in technical documents. Since we work on plain text corpora, i.e. a document does not contain any HTML markup, like in most of the previous works, it causes the problem of item list identification that need to be solved. 3 Crucial problems for the extraction of a Thai ontology There are three non-trivial problems in identification of related ontological terms and relations: cue word ambiguity, item list identification and candidate term selection. 3.1 Cue word ambiguity in lexico-syntactic pattern Using cue words, such as  X  X /dai-kae/(i.e.) X  X ,  X  X /chen/(for example) X  X  and  X  X /pen/(is) X  X , for hinting relationships of terms is a technique for ontology learning, but a word might have several functions and several meanings. For example, a cue word like  X  X / pen/(is) X  X  might signal a  X  X  X ypernym X  X , a  X  X  X ymptom X  X  or a semantic  X  X  X roperty X  X : (1) / kalam-pli pen phuet phak chanit nueng /( Cabbage is a kind of vegetable :  X  : *(2) / kalam-pli pen rok-nao-le / (Cabbage has symptom as Soft-rot.) . *(3) / kap-bai pen si-namtan /( Leaf is brown color .)
In example (1), the cue word  X  X /pen/(is) X  X  signals a hypernym relation, while in the others it does not. We solve this problem by utilizing Name Entity and property list as features for pruning inappropriate relations. 3.2 Problems of item list identification Since the input of our system is plain text, we do not have any markup symbols to show the position and the boundaries of the list. Then we used bullet symbols and numbers to indicate the list, which is not without posing certain problems (see Fig. 1 ).
 3.2.1 Long description in each list item focused item is meant to continue from the previous list or start a new list. 3.2.2 Embedded lists problems. We solve this issue by detecting each list following the same bullet symbol or numbering order. Still, there are cases where an embedded list may have a following number. In this case, we assume that different lists mention about different topics; hence we need to identify the meaning of each item of each list. 3.2.3 Ambiguity between non-ontological/ontological list item Authors frequently express procedures and descriptions in list form. But the procedure list items are not the domain X  X  ontological terms, and some description list items may not be ontology terms at all, hence the system needs to detect whether the ontological list or the non-ontological list. 3.3 Candidate term selection When both cues (lexico-syntactic patterns and item lists) are used to identify the related terms, they also pose a problem which is that there are many candidate terms for being an ontological term. In our texts, we have often found that the term which we are interested in can be very far from the related terms. In addition, the ontological term can be in any position of the sentence. For example, (4) /pi thilaeo mi kan namkhao kulap chak tangprathet pen chamnuan mak daikae (5) /pi thilaeo mi kan namkhao kulap chak tangprathet pen chamnuan mak
Both sentences, (4) and (5), have two candidate terms: rose and abroad , but while the correct ontological term of (4) is rose , the correct ontological term of (5) is abroad . The problem here is the attachment of the noun clause conjunction. Theoretically, this could be solved by any good parser, yet we do not have such resource, and building one is a very difficult task. This being so we propose solving this problem by using lexical and contextual features which will be described in Sect. 4 . Moreover, concerning item lists, it also has a problem of candidate term selection. Since all the terms of the previous paragraph of the item list are candidates as hypernym term. As shown in Fig. 2 , there are 16 candidate terms. The system also uses the lexical and contextual features to select the appropriate hypernym term. 4 Building an ontology by learning from Thai corpora Figure 3 gives an overview of the underlying architecture of our ontology construction and the maintenance system for Thai language. The system performs three tasks: ontology learning, ontology tree organizing, and verification. In this paper we focus only on the enhancement of ontology learning by adding item list identification and using lexicon and co-occurrence features instead of heuristic rules (Kawtrakul et al. 2004 ) for candidate term selection. Details concerning the ontology tree organizing and verification are published in (Kawtrakul et al. 2004 ). As far as the ontology learning is concerned, there are three main processes involved: ontological-element (concept and relation) identification by using cues: lexico-syntactic patterns and item list, candidate term generation, and candidate term selection.

Morphological analysis and NPs chunking . Similarly to many other Asian languages, in Thai there are no delimiters (blank space) to show word boundaries. Texts are a single stream of characters. Hence, word segmentation and part-of-speech (POS) tagging (Sudprasert and Kawtrakul 2003 ) are necessary for identifying a term unit and its syntactic category. Once this is done documents are chunked into phrases (Pengphon et al. 2002 ) to identify shallow noun phrase boundaries within a sentence. In this paper, the parser relies on Noun Phrase (NP) rules, word formation rules, and lexical data. The accuracy of compound noun grouping is 92% and the accuracy of NP analysis with word formation is 90%. Before experimenting in the next process, the experts verified and corrected all the NPs in the documents in order to test the actual performance of the ontological learning system. 4.1 Ontological-element identification We identify the ontological-element (concepts and relations) hinted by cues, which are lexico-syntactic patterns and item lists. 4.1.1 Pattern-based identification In order to collect hypernym relation patterns, we use IS-A relations with words from the AGROVOC 1 thesaurus. Then, we extract from agricultural documents all sentences with words occurring in previously selected word pairs. Finally, we manually select lexico-syntactic patterns from sentences by considering only the ones which structure occurs often.

The most frequent ones are focused (the top-5 patterns) in this article. By using the patterns above (Table 1 ), the sentence anchoring process could identify plausible sentences whose content bare its ontological relation. 4.1.2 Ontological-list identification In this process, we propose a methodology for identifying an ontology element from item lists that we focus on bullet list and numbering list. Since item lists could be used to describe objects, procedures, and the like, this might lead to non-taxonomic lists. In order to identify object lists which contains ontological terms, the items of the list should be the Named Entity (NE), recognized by NER system (Chanlekha and Kawtrakul 2004 ). Applying NER works well in technical domain such as agriculture and bio-informatics since the growth of ontological terms almost come from NE. As shown in Fig. 1 , it still has two problems: long boundary description and embedded list which cause the ambiguity of item list members. In order to solve these problems, the items which use the same bullet symbol and have same NE class number and having the same NE class will be considered as the same list. 4.2 Candidate term generation In this work, we uses linguistic information in the form of a grammar that mainly allows NPs to be extracted as candidate terms. Thus this process checks whether some NPs occurred before the cue as candidate term in order to generate the corresponding ontological terms. Thus all NPs on the left hand side of the cue word in the pattern are generated as the candidate terms. And the terms that occur in the preceding paragraphs of the item list are candidate hypernym terms. To do so it will consider only NPs corresponding to the NP X  X  grammatical rules as shown in Table 2 . NPs can be generated with many grammatical rules, but according to some rules entire NPs could not be an ontological term such as [ncn + conj + ncn], [ncn + det], where conj is conjunction and det is determiner. For example, [/phak/ (vegetable):ncn /lae/(and):conj/phonlamai/(fruit):ncn] (vegetable and fruit). The selected ontological terms should be separated into two terms, i.e. /phak/(vegetable) and /phonlamai/(fruit).
 4.3 Ontological term selection Having generated the ontological candidate terms, the system will discover the ontological term from a set of candidates. The most likely hypernym value ( MLH ) of term in the candidates list will be computed on the basis of an estimated function taking lexical and co-occurrence features into account. Let h i [ H , H is the set of candidates of possible hypernym terms, while t j is the related term j which is the term on the right hand side of lexico-syntactic pattern or the term in the item list. The features of the learning system are lexical and co-occurrence features. The estimate function for computing the most likely hypernymy term is defined as follows: total number of features (here, we use 5 features). f 1 -f 4 are lexical features and f 5 is co-occurrence feature. The system will select the candidate term that has the positive and maximum MLH value in each candidate set to be the ontological term of the related terms.

We use the information gain for weighting each feature proposed by Ayan ( 1999 ). Information gain is used to decide which of the features are the most relevant. It is defined in terms of Entropy as the following: Information gain Gain(S,k) of feature k is the reduction in entropy caused by partitioning the examples S according to this feature. Where Values(k) is the set of all possible values for feature k and S v is the subset of S for which feature k has value v . p i is proportion of examples in class i i.e. positive and negative class.

However, calculating information gain needs discrete value but the co-continuous value to discrete value that is described in the detail of feature 5. f 1 : Head word compatible. This feature evaluates whether head word of candidate term is compatible with head word of related term or not. If the head word of a constituent is identical to the head word of another constituent, then these terms are related to each other. For more details, consider the following example. (6) / po-kra-chao thi niyom pluk kan nai prathed-thai mi 2 chanit dai kae po-In this example, the candidate terms are Jute and Thailand . The head word of Tossa Jute and White Jute is Jute , then Jute has more possibilities to be an ontological term than Thailand . f 2 : NE class. This feature evaluates whether a candidate term of a hypernym belongs to the same NE class as related term or not. We consider the NE class as the feature because the cue word /pen/ might occasionally have the meaning  X  X  X as symptom as X  X . For example, *(7) / kalampli pen rok-naole / (Cabbage has symptom as Soft-Rot.) .

Here, Cabbage and Soft-Rot are NEs which have different classes, i.e. plant and disease, respectively. Accordingly, Soft-Rot is not a hypernym of Cabbage .In addition, we classify this feature X  X  value to three values, i.e., 1, -1 and 0, where 0 is assigned for the terms being at a high level of the taxonomy, e.g. /phuet trakun thua/ (pulse crops) which are not NE. properties of an object. For example, *(8) / kap-bai pen si-namtan /(The leaf is brown-color .)
Brown-color is not the hypernym of leaf , but a property of the object leaf. This concepts and which are properties. In the domain of agriculture, there are 3 types of property lists: colors, shapes, and appearances; e.g. powder . f 4 : Topic term. This feature evaluates whether candidate term is the topic term of the document (short document) or a topic term of the paragraph (long document) or not. Here, topic term will be computed by using tf * idf where tf is the term frequency and idf is inverse document frequency (Salton 1989 ). f 4  X  h ; t  X  X  f : Co-occurrence feature. Some statistical methods are used to analyze the co-occurrence of the candidate and the related terms. We explore three alternatives, Mutual Information (MI) (Church and Hanks 1989 ), log-likelihood ratio (LL) (Dunning 1994 ), and Chi-square testing ( v 2 ). After experimenting with Thai agriculture document, we found that Chi-square has the highest precision. Chi-square is based on hypothesis testing. It measures the divergence of the observed and expected data. Chi-square can be defined as follows: Where a represents the frequency of the term h occurring in the same sentence with the term t . The value b (resp. c ) is the number of occurrences of term h (resp. t) in the corpus for sentences not containing term t (resp. h). The value d indicates the number of sentences that do not contain neither h nor t . The total number of sentences in the corpus is represented by N .

Since feature value for calculating information gain must be discrete value but the result of chi-square is continuous value. Then the method for partitioning the continuous value to a discrete value is needed. We partition this feature value into two intervals at value x . However chi-square value is very sparse. From observation, the minimal value of chi-square of our corpus is 0.001 and the maximum value is 206.667. Partitioning the data into two interval by using this minimal and maximum value is not appropriate because in some candidate sets the chi-square values of all data are very low and this cut point can not separate data between positive and negative class. Then we define x depending on each candidate term set as x =( Max + Min )/2 where Max and Min are the maximum and minimum chi-square value in the candidate term set. This partition value will separate data into two groups ( B x and &gt; x ) for calculating information gain. 5 Experimental results and discussion The measurement of the system X  X  performance was based on test cases in the domain of agriculture and divided according to the cue types: lexico-syntactic pattern and item list. At each phase, we computed the precision, recall, and F-score, by comparing the outputs of the system with the results produced by the agreement of two experts of agriculture from Thai National AGRIS Center. 2 Precision (p) gives the number of extracted correct results divided by the number of total extracted results, while recall (r) shows the number of extracted correct results divided by the number of total correct results, and F-measure (F) is the harmonic mean of precision (p) and recall (r), i.e., 2pr/(p + r).

Our training corpus for calculating the feature weight consists of 2,000 examples with positive and negative class. The information gain values for each feature are shown in Table 3 . We can conclude that the NE feature ( f 2 ) is the most important feature for selecting the candidate term of the lexico-syntactic pattern-based ontology learning with the highest value of information gain (0.150). The reasons are NE usually occurs in the agricultural document and if the candidate terms had the same NE class as the related term they should be selected. In addition, the co-occurrence feature ( f 5 ) has the crucial role for selecting hypernym term of the list item terms with the highest value of information gain (0.267). It caused by the hypernym and hyponym terms have more co-occurrence value than the other terms in the candidate term set. Conversely, the head word compatible feature ( f 1 ) occurs rarely with the lexico-syntactic pattern then this feature is not significant for using to select the candidate term, as well as, the property term feature ( f 3 ) on the item lists.
By testing with a corpus of about 100,000 words, the system is able to extract about 966 concepts and 821 taxonomic relations by using 5 lexico-syntactic patterns. And with the item list approach, the system can extract 334 concepts and 264 relations, most of which are different from the previous ones. Table 4 shows the evaluation results of each cue. The precision, recall and F-measure of the system are 0.74, 0.78 and 0.76, respectively. The F-measure of the system using only lexico-syntactic pattern and item list as cue are 0.66 and 0.85, respectively. The important cause of error of pattern approach is that there are many sentences contained anaphora terms, then in this case the system can not extract the correct ontological terms. The anaphora causing this problem is the direct reference. They are definite NPs and zero anaphora. From observation, some of these anaphora terms can be solved by using heuristic rule by getting the subject of previous sentence. This method can increase the precision value by 7%, i.e. raising the level from 0.64 to 0.71. Moreover, the errors of item-list cue technique occur because some item lists are composed of two classes, for example, disease and pest. This is why the system can not detect this item list. 6 Conclusion and future works In this article, we presented and evaluated the learning methodologies for the automatic building of ontology that is composed of term-and relation extraction. A shallow parser is used for candidate terms extraction, and cues-words: lexico-syntactic patterns and item list (numbering and bullet list) are used for relation extraction. Concerning the lexico-syntactic patterns, there are some problems of many candidate terms and cue word sense ambiguity, then the lexicon and the co-occurrence features of each candidate term are used to solve these problems. We also applied information gain for weighting each feature to measure its relevance. This technique can be used to extract the hypernym term of the item lists from the set of candidate terms. One of the most important advantages of using cues is that it reduces the problems of concept and relation labeling which are the crucial problems of the research of ontological engineering.

We consider our results to be quite good, given that the experiment is preliminary, but the vital limitation of our approach is that it works well only for documents that contain a lot of cue-words. Based on our error analysis the performance of the system can be improved and the methodologies can be extended to other sets of semantic relations. Another research direction is to extract the semantic relation embedded in the sentence without the cues.
 References
