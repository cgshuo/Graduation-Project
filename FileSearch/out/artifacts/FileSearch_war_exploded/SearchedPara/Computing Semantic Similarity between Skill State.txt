 Knowledge-intensive industries need to become more efficient at deploying the right expertise as quickly and smoothly as possible, thus it is desired to have systems that can quickly match and deploy skilled individuals to meet customer needs. The searches in most of the current matching systems are based on exact matches between skill state-ments. However, exact matching is very likely to miss individuals who are very good matches to the job but didn X  X  select the exact skills that appeared in the open job description. 
It is always hard for individuals to find the per-fect skills to describe their skill sets. For example, an individual might not know whether to choose a skill stating that refers to  X  X aintaining X  a given product or  X  X upporting X  it or whether to choose a skill about maintaining a  X  X atabase X  or about maintaining  X  X B2 X . Thus, it is desirable for the job search system to be able to find approximate matches , instead of only exact matches, between available individuals and open job positions. More specifically, a skill similarity computation is needed to allow searches to be expanded to related skills, and return more potential matches. 
In this paper, we present our work on develop-ing a skill similarity computation based upon se-mantic commonalities between skill statements. Although there has been much work on text simi-larity metrics (Lin, 1998a; Corley and Mihalcea, 2005), most approaches treat texts as a bag of words and try to find shared words with certain statistical properties based on corpus frequencies. As a result, the structural information in the text is ignored in these approaches. We will describe a new semantic approach that takes the structural information of the text into consideration and matches skill statements on corresponding seman-tic roles . We will demonstrate that it can outper-form standard statistical text similarity techniques, and reach the level of human agreement. 
In Section 2, we first describe the skill state-ments we extracted from an enterprise-wide exper-tise taxonomy. In Section 3, we describe the performance of a standard statistical approach on this task. This motivates our semantic approach of matching skill statements on corresponding seman-tic roles. We also compare and evaluate the per-formance of four natural language parsers (the Charniak parser, the Stanford parser, the ESG parser, and MINIPAR) for the purpose of our task. An inter-rater agreement study and evaluation of our approach will be presented in Section 4. We end with a discussion and conclusion. An expertise taxonomy is a standardized, enter-prise-wide language and structure to describe job role requirements and people capabilities (skill sets) across a corporation. In the taxonomy we util-ize for this study, skills are associated with job roles. The taxonomy has 10667 skills. Each skill has a title, for example,  X  X dvise BAAN eBusiness ASP. X  We refer to this title as the skill statement . 
The official taxonomy update policies require that skill statements be verb phrases using one of 18 valid skill verbs (e.g., Advise, Architect, Code, Design, Implement, Sell, and Support ). In this section, we first explain a statistical infor-mation-theoretic approach we used as a baseline, and show examples of how it performs for our task. The error analysis of this approach motivates our semantic approach that takes the structural in-formation of the text into consideration. In the re-mainder of this section, we describe how we extract semantic role information from the syntac-tic parse trees of the skill statements. Four natural language parsers are compared and evaluated for the purpose of our task. 3.1 Statistical Approach In order to compute semantic similarities between skill statements, we first adopted one of the stan-dard statistical approaches to the problem of com-puting text similarities based on Lin X  X  information-theoretic similarity measure (Lin 1998a). Lin de-fined the commonality between A and B as where common ( A, B ) is a proportion that states the commonalities between A and B and where the amount of information in proposition s is The similarity between A and B is then defined as the ratio between the amount of information needed to state the co mmonality of A and B and the information needed to fully describe A and B: In order to compute common ( A , B ) and descrip-tion ( A , B ), we use standard bag-of-words features, i.e., unigram features --the frequency of words computed from the entire corpus of the skill state-ments. Thus common ( A , B ) is the unigrams that both skill statements share, and description ( A , B ) is the union of the unigrams from both skill state-ments. 
The words are stemmed first so that the words with the same root (e.g., managing &amp; manage-ment ) can be found as commonalities between two skill statements. A stop-word list is also used so that the commonly used words in most of the docu-ments (e.g., the , a ) are not used as features. A for-mal evaluation of this approach will be presented in Section 4 where the similarity between 75 pairs of skill statements will be evaluated against human judgments, but we discuss some examples here. 
In order to see how to improve Lin X  X  statistical similarity measure, we examine sample skill state-ment pairs which achieve high similarity scores from Lin X  X  measure but were rated consistently as dissimilar by human subjects in our evaluation. Here are two examples: 1. Advise Business Knowledge of CAD function-2. Advise on Money Market In these two examples, although many words are shared between the two pairs of skill statements ( Advise Business Knowledge of ... for FEM for the first pair; Advise on Money for the second pair), they are not similar to human judges. We conjec-ture that this judgment of dissimilarity is due to the differences between the key components of the skill statements ( CAD functionality vs. Process in the first pair; Money Market vs. Money Center Banking in the second pair). 
This kind of error is common for most statistical approaches to the problem, where common infor-mation is computed without considering the struc-tural information in the text. From the above examples, we can see that the similarity computa-tion would be more accurate if the verb phrases match on corresponding semantic roles , instead of matching words from any location in the skill statements. By identifying semantic roles, we can provide more weights to those semantic roles criti-cal for our task, i.e., the key components of the skill statements. 3.2 Identifying and Assigning Semantic The following example shows the kind of semantic roles we want to be able to identify and assign. business Middleware]] to [ purpose PLM Solu-tions] In this example,  X  X pply X  is the  X  X ction X  of the skill;  X  X nowledge of IBM E-business Middle-ware X  is the  X  X heme X  of the skill, where the  X  X on-cept X  semantic role ( IBM E-business Middleware ) specifies the key component of the skill require-ment and is the most important role for skill matching;  X  X LM Solutions X  is the  X  X urpose X  of the skill. 
Our goal was to extract all such semantic role patterns for all the skill statements, and match on corresponding semantic roles. Although there ex-ists some automatic semantic role taggers (Gildea and Jurafsky, 2002; Giuglea and Moschitti, 2006), most of them were trained on PropBank (Palmer et. al., 2005) and/or FrameNet (Johnson et. al., 2003), and perform much worse in other corpora (Pradhan et. al., 2004). Our corpus is from a very different domain (information technology) and there are many domain-specific terms in the skill statements, such as product names, company names, and company-specific nomenclature for product offerings. Given this, we would expect poor performance from these automatic semantic role taggers. Moreover, the semantic role informa-tion we need to extract is more detailed and deeper than most of the automati c semantic role taggers can identify and extract (e .g., the  X  X oncept X  role embedded within the  X  X heme X  role). 
We developed a specialized parser that extracts semantic role patterns from each of the 18 skill verbs. This semantic role parser can achieve a much higher performance than the general-purpose semantic role taggers. The inputs needed for the semantic role parser are syntactic parse trees gen-erated by a natural language parse of the original skill statements. 3.3 Preprocessing for Parsing We first used the Charniak parser (2000) to parse the original skill statements. However, among all the 10667 skill statements, 1217 were not parsed as verb phrases, leading to very poor performance. After examining the error cases, we found that ab-breviations are used widely in the skill statements. For example, These abbreviations made the system unable to determine the part of speech of some words, result-ing in incorrect parses. Thus, the first step of the preprocessing was to expand abbreviations. 
There were 225 valid abbreviations already identified by the expertise taxonomy team. How-ever, we found many abbreviations that appeared in the skill statements but were not listed there. Since most abbreviations are not words found in a dictionary, in order to find the abbreviations that appear frequently in the skill statements, we first found all the words in the skill statements that were not in WordNet (Miller, 1990). We then ranked them based on their frequencies, and manu-ally identified high freque ncy abbreviations. Using this approach, we added another 187 abbreviations to the list (a total of 412). 
From the error cases, we also found that many words were mistagged as proper nouns, For exam-ple,  X  X echnically X  in 
Advise Technically for Simulation was parsed as a proper noun. We realized the rea-son for this error was that all the words, except for prepositions, are capitalized in the original state-ments and the parser tends to tag them as proper nouns. To solve this problem, we changed all the capitalized words to lower case, except for the first word and the acronyms (words that have all letters capitalized, e.g., IBM). After applying these two steps of preprocessing, we parsed the skill state-ments again. This time, more than 200 additional skill statements were parsed as verb phrases after the preprocessing. 
When we examined the error cases more closely, we found the errors occur mostly when the skill verbs can be both a noun and a verb (e.g., de-sign , plan ). In those cases, the parser may parse the entire statement as one noun phrase, instead of a verb phrase. In order to disambiguate such cases, we added a subject ( X  X mployees X ) to all the skill statements to convert them into full sentences. Af-ter applying this additional step of preprocessing, we parsed the skill statements again. This time, only 28 skill statements were not parsed as sen-tences containing verb phrases, a significant im-provement. The remaining errors were due to the use of some words as skill verbs, e.g.,  X  X rchitect X  1 , not recognized as verbs by the parser. 3.4 Parser Evaluation and Comparison While the Charniak parser performed well in our initial verb phrase (VP) test, we decided to com-pare the Charniak parser X  X  performance with other parsers. For this evaluation, we compared it with the Stanford parser, the ESG parser, and MINIPAR. 
The Stanford parser (Klein and Manning, 2003) is an unlexicalized statistical syntactic parser that was trained on the same corpus as the Charniak parser (the Penn TreeBank). Its parse tree has the same structure as the Charniak parser. 
The ESG (English Slot Grammar) parser (McCord, 1980) is a rule-based parser based on the slot grammar where each phrase has a head and dependent elements, and is also marked with a syn-tactic role. 
MINIPAR (Lin, 1998b), as a dependency parser, is very similar to the ESG parser in terms of its output. It represents sentence structures as a set of dependency relationships between head words. 
Since our purpose is to use the syntactic parses as inputs to extract semantic role patterns, the cor-rectness of the bracketing of the parses and the syntactic labels of the phrases (e.g., NP, VP, and PP) are the most important information for our pur-poses, whereas the POS (Part-Of-Speech) labels of individual words (e.g., nouns vs. proper nouns) are not that important (also, there are too many do-main-specific terms in our data). Thus, our evalua-tion of the parses is only on the correctness of the bracketing and the syntactic labels of the phrases, not the correctness of the entire parse. For our task, the correctness of the prepositional phrase attach-ment is especially important for extracting accurate semantic role patterns (Gildea and Jurafsky, 2002). For example, for the sentence 
Apply Knowledge of IBM E-business Middle-ware to PLM Solutions. the correct bracketing should be 
Apply [Knowledge [of [IBM E-business Mid-dleware]]] [to [PLM Solutions]]. Thus the parser needs to correctly attach  X  X f IBM E-business Middleware X  to  X  X nowledge X  and at-tach  X  X o PLM Solutions X  to  X  X pply X , not  X  X nowl-edge X . 
To evaluate the performance of the parsers, we randomly picked 100 skill statements from our cor-pus, preprocessed them, and then parsed them us-ing the four different parsers. We then evaluated the parses using the above evaluation measures. The parses were rated as correct or incorrect. No partial score was given. Figure 1 shows the evalua-tion results. The error analysis reveals four major sources of error for all the parsers, most of which are specific to the domain we are working on: (1) Many domain specific terms and acronyms. (2) Many long noun phrases. For example,  X  X m-(3) Some specialized use of punctuation. For ex-(4) Prepositional phrase attachment can be diffi-Figure 1. An Evaluation of Four Parsers on the Task of Parsing Human Skill-related Verb Phrases worse compared with the other parsers. The main reason is that it always parses the phrase  X  X ERB knowledge of Y X  (e.g.,  X  X mployees apply knowl-edge of web technologies . X ) incorrectly --the parse result always mistakenly attaches  X  X f Y X  (e.g., of web technologies ) to the VERB (e.g., apply ), not  X  X nowledge X . Since there were so many of phrases in the test set and in the corpus, this kind of error significantly reduced the pe rformance for our task. These kinds of errors on prepositional phrase at-tachment in MINIPAR were also mentioned in (Pantel and Lin, 2000). can see that the Charniak parser performs the best for our task among all the four parsers. This result is consistent with a more thorough evaluation (Swanson and Gordon, 2006) on a different corpus with a set of different target verbs, which showed the Charniak parser performed the best among three parsers (including the Stanford parser and MINPAR) for labeling semantic roles. We note that although the ESG parser performed a little worse than the Charniak parser, its parses contain much richer syntactic (e.g., subject, object) and semantic (e.g., word senses) slot-filling informa-tion, which can be very useful to many natural lan-guage applications. 3.5 Extracted Semantic Role Patterns From the parse trees generated by the Charniak parser, we first automatically extracted patterns for each of the 18 skill verbs (e.g.,  X  X dvise on NP for NP X ), and then we manually identified the seman-tic roles. For example, the semantic role patterns identified for the skill verb  X  X dvise X  are:  X  Advise [Theme] (for [Purpose])  X  Advise (technically) on/about [Theme] (for  X  Advise clients/customers/employees/users The corpus also contains embedded sub-semantic-role patterns, for example, for the  X  X heme X  role we extracted the following sub-patterns:  X  (application) knowledge of/for [Concept]  X  sales of [Concept]  X  (technical) implementation of [Concept] We have extracted and identified a total of 74 such semantic role patterns from the skill statements. In order to evaluate the two approaches (semantic role parsing and statistical) to computing semantic similarity of skill statements in our domain, we first conducted an experime nt to evaluate how hu-mans agree on this task, which also provides us with an upper bound accuracy for the task. 4.1 Inter-Rater Agreement and Upper To assess inter-rater agreement, we randomly se-lected 75 skill pairs from the expertise taxonomy. Since random pairs of verb s would have little or no similarity, we selected skill pairs that share the same job role, or same secondary or primary job category, or from across the entire expertise taxon-omy. 
These 75 skill pairs are then given to three raters to independently judge their similarities on a 5 point scale from 1 as very similar to 5 as very dis-similar. Since this 5 point scale is very fine-grained, we also converted the judgments to a more coarse-grained measure --binary judgment: 1 and 2 count as similar; 3-5 as not similar. 
The metric we used is the kappa statistic (Car-letta, 1996), which factors out the agreement that is expected by chance: where P ( A ) is the observed agreement among the raters, and P ( E ) is the expected agreement, i.e., the probability that the raters agree by chance. 
Since the judgment on the 5 point scale is ordi-nal data, the weighted kappa statistic is used to take the distance of disagreement into considera-tion (e.g., the disagreement between 1 and 2 is smaller than that between 1 and 5). 
The inter-rater agreement results for both the fine-grained and coarse -grained judgments are shown in Table 1. In gene ral, a kappa value above 0.80 represents perfect agreement, 0.60-0.80 repre-sents significant agreement, 0.40-0.60 represents moderate agreement, and 0.20-0.40 is fair agree-ment (Chklovski and Mihalcea, 2003). We can see that the agreement on the fine-grained judgment is moderate, whereas the agreement on the coarse-grained (binary) judgment is significant. 
From the inter-rater agreement evaluation, we can also get an upper bound accuracy for our task, i.e., human agreement without factoring out the agreement expected by chance (i.e., P ( A ) in the kappa statistic). The average P ( A ) for the coarse-grained (binary) judgment is 0.81, and that consti-tutes the upper bound accuracy for our task. 4.2 Evaluation of the Statistical Approach We use the 75 skill pairs as test data to evaluate our semantic similarity approach against human judgments. Considering the reliability of the data, only the coarse-grained (binary) judgments are used. The gold standard is obtained by majority voting from the three raters, i.e., for a given skill pair, if two or more raters judge it as similar, then the gold standard answer is  X  X imilar X , otherwise it is  X  X ot similar X . 
We first evaluated Lin X  X  statistical approach de-scribed in Section 3.1. Among 75 skill pairs, 53 of them were rated correctly according to the human judgments, that is, 70.67% accuracy. The error analysis shows that many of the errors can be cor-rected if the skills are matched on their correspond-ing semantic roles. We then evaluated the utility of the extracted semantic role information to see whether it can outperform the statistical approach. 4.3 Evaluation of Semantic Role Matching For simplicity, we will only report on evaluating semantic role matching on the "concept" role that specifies the key component of the skills, as intro-duced in Section 3.2. performing semantic role matching for the skill similarity computation: 1) match on the entire se-mantic role; 2) match on the head nouns only. But both have their drawbacks: the first approach is too strict and will miss many similar skill statements; the second approach may not only miss the similar skill statements, e.g., but also misclassify dissimilar ones as similar, e.g., In order to solve these problems, we used a simple matching criterion from Tversky (1977). The simi-larity of two texts t 1 and t 2 is determined by: Similarity(t 1 , t 2 ) = This equation states that two texts are similar if shared features are a large percentage of the total features. We set a threshold of 0.5, requiring that at least 50% of the features be shared. We apply this criterion to the text contained in the  X  X oncept X  role. 
The words in the calculation are preprocessed first: abbreviations are expanded, stop-words are excluded (e.g., the and of don't count as shared words), and the remaining words are stemmed (e.g., manager and management are counted as shared words), as was done in our previous infor-mation-theoretic approach. Words connected by punctuation (e.g., e-business, software/hardware ) are treated as separate words. For example, The shared words between the two  X  X oncept X  roles (bracketed) are  X  X ield X  and  X  X orce X , and their shared percentage is (2*2)/7 = 57.14% &gt; 50%, so they are similar. 
We have also evaluated this approach on our test set with the 75 skill pairs. Among 75 skill pairs, 60 of them were rated correctly (i.e., 80% accuracy), which significantly outperforms the statistical ap-proach, and is very close to the upper bound accu-racy, i.e., human agreement (81%), as shown in Figure 2. 
Figure 2. Evaluation on Semantic Similarity be-
The difference between this approach and Lin X  X  information content approach is that this computa-tion is local --no corpus statistics is used. Also, using this approach, it is easier to set an intuitive threshold (e.g., 50%) for a classification problem (e.g., similar or not for our task). With this ap-proach, however, there are also cases that are mistagged as similar, for example, 
Apply Knowledge of [Basic Field Force Auto-mation ] Advise on [Sales Force Automation ] Although  X  X ield Force Automation X  and  X  X ales Force Automation X  seem similar on their surface form, they are two quite different concepts. Deeper domain knowledge (such as an ontology) is needed to distinguish such cases. We have also investigated several approaches to improving the semantic role text similarity meas-ure we described. One appro ach is to also consider similarities between skill verbs. In this example: although the key components of the skill state-ments ( Domino Mail Manager ) are the same, their skill verbs are different ( implement vs. develop for ). The skills required for  X  X mplementing X  a sys-tem or software product are usually different from those required for  X  X eveloping for X  the same sys-tem or software product. This example shows that a semantic similarity computation between skill verbs is required to distinguishing such cases. 
Many approaches to the problem of word/concept similarities are based on taxonomies, e.g., WordNet. The simplest approach is to count the number of nodes on the shortest path between two concepts in the taxonomy (Quillian, 1972). The fewer nodes on the path, the more similar the two concepts are. The assumption for this shortest path approach is that th e links in the taxonomy rep-resent uniform distances. However, in most tax-onomies, sibling concepts deep in the taxonomy are usually more closely related than those higher up. Different approaches have been proposed to discount the depth of the concepts to overcome the problem. Budanitsky and Hirst (2006) thoroughly evaluated six of the approaches (Hirst and St-Onge, Leacock and Chodor ow, Jiang and Conrath, Lin, Resnik, Wu and Palmer), and found that Jiang and Conrath (1997) was superior to the other ap-proaches based on their evaluation experiments. 
For our task, we compared two approaches to computing skill verb similarities: shortest path vs. Jiang and Conrath. Since the words are compared based on their specific senses, we first manually assigned one most appropriate sense for each of the 18 skill verbs from WordNet. We then used the library developed by Pedersen et al. (2004) to compute their similarity scores. 
Table 2 shows the top nine pairs of skill verbs with the highest similarity scores from the two ap-proaches. We can see that the two approaches agree on the top four pairs, but disagree on the rest in the list. One intuitive example is the pair  X  X ead X  and  X  X anage X  which is ranked the 5 th by the Jiang and Conrath approach but ranked the 46 th by the shortest path approach. It seems that the Jiang and Conrath approach matches better with our human intuition for this example. While we didn X  X  com-pare these results with human performance, in gen-eral most of the similar skill verb pairs listed in the table don X  X  look very similar for our domain. This may be due to the fact that WordNet is a general-purpose taxonomy --although we have already selected the most appropriate sense for each verb, their relationship represented in the taxonomy may still be quite different from the relationship in our domain. A domain-specific taxonomy for skill verbs may improve the performance. The other reason may be due to the structure of WordNet X  X  verb taxonomy, as mentioned in (Resnik and Diab, 2000), which is considerably wider and shallower than WordNet X  X  noun taxonomy. A different verb lexicon, e.g., VerbNet (Kipper et al., 2000), can be explored. In this paper, we have presented our work on a se-mantic similarity computation for skill statements in natural language. We compared and evaluated four different natural language parsers for our task, and matched skills on their corresponding semantic roles extracted from the parse trees generated by one of these parsers. The evaluation results showed that the skill similarity computation based on se-mantic role matching can outperform a standard statistical approach and reach the level of human agreement. 
The extracted semantic role information can also be incorporated into the standard statistical ap-proaches as additional features. One way is to give higher weights to those semantic role features deemed most important. This approach has achieved a high performance for a text categoriza-tion task when combining extracted keywords with the full text (Hulth and Megyesi, 2006). 
We have shown that good results can be achieved for a domain-specific text matching task by performing a simple word-based feature com-parison on corresponding structural elements of texts. We have shown that the structural elements of importance can be identified by domain-specific pattern analysis of corresponding parse trees. We believe this approach can generalize to other do-mains where phrases, sentences, or other short texts need to be compared. The majority of this work was performed while the first author was a summer intern at IBM T. J. Wat-son Research Center in Hawthorne, NY. Thanks to Yael Ravin and Jennifer Lai for supporting this work, Brian White for his help on the software, Michael McCord for assistance with the IBM ESG parser, and the IBM Expertise Taxonomy team for letting us use their data. 
