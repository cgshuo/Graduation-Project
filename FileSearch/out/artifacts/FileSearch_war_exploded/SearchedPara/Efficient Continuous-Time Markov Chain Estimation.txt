 Continuous-time Markov chains (CTMCs) play a cen-tral role in applications as diverse as queueing theory, phylogenetics, genetics, and models of chemical interac-tions (Huelsenbeck &amp; Ronquist, 2001; Munsky &amp; Kham-mash, 2006). The process can be thought of as a timed random walk on a directed graph where the countable, but potentially infinite, set of graph nodes are the values that the process can take on. There are probabilities of transi-tion associated with the edges of the graph, and the holding time, or length of time between two transitions, is expo-nentially distributed with a rate depending on the current node. A path simulated from this random process is an or-dered list of the nodes visited and the times at which they are reached.
 In leveraging the modelling capabilities of CTMCs, the bottleneck is typically the computation of the transition probabilities : the conditional probability that a trajectory ends in a given end state, given a start state and a time inter-val. This computation involves the marginalization over the uncountable set of end-point conditioned paths. Although we focus on the Bayesian framework in this work, where the transition probabilities appear in Metropolis-Hastings ratios, the same bottleneck is present in the frequentist framework, where transition probabilities are required for likelihood evaluation. When the state space is small, exact marginalization can be done analytically via the matrix ex-ponential. Unfortunately, this approach is not directly ap-plicable to infinite state spaces, and is not computationally feasible in large state spaces because of the cubic running time of matrix exponentiation.
 We propose an efficient Monte Carlo method to approach inference in CTMCs with weak assumptions on the state space. Our method can approximate transition probabilities as well as estimate CTMC parameters for this general class of processes. More precisely, we are interested in count-ably infinite state space CTMCs that satisfy the following two criteria. First, we require the construction of a cer-tain type of potential on the state space. We describe this potential in more detail in Section 2, and show in Section 3 that such potentials can be easily constructed even for com-plex models. Second, the CTMC should be explosion-free to avoid pathologies (i.e., we require that there is a finite number of transitions with probability one in any bounded time interval).
 In contrast, classical uniformization methods assume that there is a fixed bound on all the rates (Grassmann, 1977), a much stronger condition than our explosion-free assump-tion. For example, in the first of the two application domains that we investigated, inference in string-valued CTMCs for phylogenetics, the models are explosion-free but do not have a fixed bound on the rates. Other approaches, based on performing Markov chain Monte Carlo (MCMC) with auxiliary variables, relax the bounded rate assumption (Rao &amp; Teh, 2011; 2012), but they have a running time that depends linearly on the size of the state space in the sparse case and quadratically in the dense case. Particle-based methods offer an interesting complementary approach, as they have a time complexity per particle that depends on the imputed number of transitions between the two end points instead of on the size of the state space. In the simplest case, one can implement this idea using a proposal distribution equal to the generative process over paths initialized at the start point. The weight of a parti-cle is then equal to one if the end point of the generated path coincides with observed end point, and zero other-wise. We call this proposal the forward sampling proposal. This idea can be turned into a consistent estimator of pos-terior distributions over parameters using pseudo-marginal methods (Beaumont, 2003; Andrieu &amp; Roberts, 2009) (or in more complicated setups, particle MCMC methods (An-drieu et al., 2010)).
 Unfortunately, the forward sampling method has two seri-ous limitations. First, the requirement of imputing wait-ing times between each transition means that the proposal distribution is defined over a potentially high-dimensional continuous space. This implies that large numbers of par-ticles are required in practice. Second, in problems where each state has a large number of successors, the probabil-ity of reaching the end state can become extremely small, which for example further inflates the number of particles required to obtain non degenerate Metropolis-Hastings ra-tios in particle MCMC (Andrieu et al., 2010) algorithms. End point informed proposals over transitions and waiting times have been developed in previous work (Fan &amp; Shel-ton, 2008), but this previous work is tailored to dynamic Bayesian models rather than to the combinatorial problems studied here. Our method greatly simplifies the develop-ment of end point informed proposals by marginalizing all continuous variables. There has also been work on related end-point conditioning problems in the rare event simula-tion literature (Juneja &amp; Shahabuddin, 2006), but this pre-vious work has focused on the discrete-time setting. For expositional purposes, we start by describing the sim-plest setup in which our method can be applied: computing the probability that a CTMC with known rate parameters occupies state y  X  X at time T given that it occupies state x  X  X at time 0, where X is a countable set of states. The main contributions of this paper can be understood in this simple setup. We then show that our method can be ex-tended to certain types of partial or noisy observations, to more than two observations organized as a time series or a tree (branching process), and to situations where some or all the parameters of the CTMC are unknown.
 Notation. Let  X  ( x,y ) denote the transition probability from state x  X  X  to state y  X  X  given that a state jump oc-note the rate of the exponentially-distributed holding time at state x (  X  : X  X  [0 ,  X  ) ). 1 We only require efficient point-wise evaluation of  X  (  X  ) , X  (  X  ,  X  ) and efficient simula-tion from  X  ( x,  X  ) for all x  X  X . We start by assuming that  X  and  X  are fixed, and discuss their estimation afterward. We define some notation for paths sampled from this pro-cess. Let X 1 ,X 2 ,... denote the list of visited states with X i 6 = X i +1 , called the jump chain , and H 1 ,H 2 ,... , the list of corresponding holding times . The model is character-ized by the following distributions: X i +1 | X i  X   X  ( X H | X i  X  F (  X  ( X i )) , where F (  X  ) is the exponential dis-tribution CDF with rate  X  . Given a start state X we denote by P x the probability distribution induced by this model. Finally, we denote by N the number of states visited, counting multiplicities, in the interval [0 ,T ] , i.e. Overview of the inference method. Using the simple setup introduced above, the problem we try to solve is to approximate P x ( X N = y ) , which we approach using an importance sampling method. Each proposed particle con-sists of a sequence (a list of variable finite length) of states, x  X  = ( x 1 ,...,x n )  X  X  X  , starting at x and ending at y . In other words, we marginalize the holding times, hence avoiding the difficulties involved with sequentially propos-ing times constrained to sum to the time T between the end points.
 Concretely, our method is based on the following elemen-tary property, proved in the Supplement: Proposition 1. If we let  X  ( x  X  ) =  X  ( x  X  ) / P x ( X where, where the H i  X  X  are sampled according to F (  X  ( X i )) inde-pendently given X  X  = ( X 1 ,  X  X  X  ,X N ) and where n = | x then  X  is a normalized probability mass function.
 As our notation for  X , X  suggests, we use this result as fol-lows (see Algorithm 1 in the Supplement for details). First, we define an importance sampling algorithm that targets the unnormalized density  X  ( x  X  ) via a proposal  X  P x ) . Let us denote the k -th particle produced by this algo-rithm by x  X  ( k )  X  X  X  , k  X  { 1 ,...,K } , where the num-ber of particles K is an approximation accuracy parameter. Each of the K particles is sampled independently accord-ing to the proposal  X  P . Second, we exploit the fact that the sample average of the unnormalized importance weights algorithm provide a consistent estimator for the normalizer of  X  . Finally, by Proposition 1, this normalizer coincides with the quantity of interest here, P x ( X N = y ) . The only formal requirement on the proposal is that P x ( X  X  = x  X  0 should imply  X  P ( X  X  = x  X  ) &gt; 0 . However, to render this algorithm practical, we need to show that it is possible to define efficient proposals, in particular proposals such that ( X  X  = x ) &gt; 0 if and only if  X  P ( X  X  = x  X  ) &gt; 0 (in order to avoid particles of zero weight). We also need to show that  X  can be evaluated point-wise efficiently, which we establish in Proposition 2.
 Proposal distributions. Our proposal distribution is based on the idea of simulating from the jump chain, i.e. of se-quentially sampling from  X  until y is reached. However this idea needs to be modified for two reasons. First, (1) since the state is countably infinite in the general case, there is a potentially positive probability that the jump chain sam-pling procedure will never hit y . Even when the state is finite, it may take an unreasonably large number of steps to reach y . Second, (2) forward jump chain sampling, assigns zero probability to paths visiting y more than once. We address (1) by using a user-specified potential  X  X  X  N centred at the target state y (see Supplement for the conditions we impose on  X  y ). For example we used the Levenshtein (i.e., minimum number of insertion, deletion, and substitution required to change one string into another) and Hamming distances for the string evolution and RNA kinetics applications respectively. Informally, the fact that this distance favors states which are closer to y is all that we need to bias the sampling of our new jump process towards visiting y .
 How do we bias the proposal sampling of the next state? Let D ( x )  X  X be the set of states that decrease the po-tential from x . The proposed jump-chain transitions are chosen with probability
We show in the Supplement that under weak conditions, we will hit target y in finite time with probability one if we pick  X  y x = max {  X , P x 0 1 / 2 is a tuning parameter. We discuss the sensitivity of this parameter, as well as strategies for setting it in Section 3.2. Point (2) can be easily addressed by simulating a geometrically-distributed number of excursions where the first excursion starts at x , and the others at y , and each ex-cursion ends at y . We let  X  denote the parameter of this geometric distribution, a tuning parameter, which we also discuss at the end of Section 3.2.
 Analytic jump integration. In this section, we describe how the unnormalized density  X  ( x  X  ) defined in Equa-tion (1) can be evaluated efficiently for any given path x It is enough to show that we can compute the following in-tegral for H i | X  X   X  F (  X  ( X i )) independently conditionally on X  X  : and where f is the exponential density function. Unfor-tunately, there is no efficient closed form for this high-dimensional integral, except for special cases (for example, if all rates are equal) (Akkouchi, 2008). This integral is related to those needed for computing convolutions of non-identical independent exponential random variables. While there exists a rich literature on numerical approximations to these convolutions, these methods either add assumptions on the rate multiplicities (e.g. |{  X  ( x 1 ) ,..., X  ( x | (  X  ( x 1 ) ,..., X  ( x N )) | ), or are computationally intractable (Amari &amp; Misra, 1997).
 We propose to do this integration using the construction of an auxiliary, finite state CTMC with a n + 1 by n + 1 rate matrix  X  Q (to be defined shortly). The states of  X  Q corre-spond to the states visited in the path ( x 1 ,x 2 ,...,x multiplicities plus an extra state s n +1 . All off-diagonal en-tries of  X  Q are set to zero with the exception of transitions going from x i to x i +1 , for i  X  { 1 ,...,n } . More specifi-cally,  X  Q is This construction is motivated by the following property which is proven in the Supplement: Proposition 2. For any finite proposed path ( x 1 ,x 2 ,...,x n ) , if then where exp( A ) denotes the matrix exponential of A . 2 Trees and sequences of observation. We have assumed so far that the observations take the form of a single branch with the state fully observed at each end point. To approach more general types of observations, for example a series of partially observed states, or a phylogenetic tree with ob-served leaves, our method can be generalized by replacing the importance sampling algorithm by a sequential Monte Carlo (SMC) algorithm. We focus on the tree case in this work which we describe in detail in Section 3, but we out-line here how certain partially observed sequences can also be approached to start with something simpler.
 Consider a setup where the observation at time T i a set A i  X  X (i.e. we condition on ( X ( T i )  X  A ; i  X  X  1 ,...,m } ) which arises for example in (Saeedi &amp; Bouchard-C  X  ot  X  e, 2011)). In this case, the importance sam-pling algorithm described in the previous section can be used at each iteration, with the main difference being that the potential  X  is modified to compute a distance to a set A rather than a distance to a single point y . See Algorithm 5 in the Supplement for details.
 For other setups, the construction of the potential is more problem-specific. One limitation of our method arises when the observations are only weakly informative of the hidden state. We leave these difficult instances for future work and reiterate that many interesting and challenging problems fall within the reach of our method (for example, the computational biology problems presented in the next section). Parameter estimation. So far, we have assumed that the parameters  X  and  X  governing the dynamics of the process are known. We now consider the case where we have a parametric family with unknown parameter  X   X   X  for the jump transition probabilities  X   X  and for the holding time mean function  X   X  . We denote by P x, X  the induced distribu-tion on paths and by p a prior density on  X  . To approximate the posterior distribution on  X  , we use pseudo-marginal methods (Beaumont, 2003; Andrieu &amp; Roberts, 2009) in the fixed end-point setup and particle MCMC methods (Andrieu et al., 2010) in the sequences and trees setup. While our algorithm can be combined with many variants of these pseudo-marginal and particle MCMC methods, in this section, for simplicity we describe the grouped inde-pendence Metropolis-Hastings (GIMH) approach.
 At each MCMC iteration t , the algorithm keeps in memory  X  approximation is obtained from the algorithm described in the previous subsections. Even though this approximation is inexact for a finite number of particles, the GIMH sam-pler is still guaranteed to converge to the correct stationary distribution (Andrieu et al., 2010).
 The algorithm requires the specification of a proposal den-sity on parameter q (  X  0 |  X  ) . At the beginning of each MCMC iteration, we start by proposing a parameter  X   X  from this proposal q . We then use the estimate  X  Z  X   X  of P  X   X  ( Y ) given by the average of the weights w ( x  X  ( t ) ( k )) to form the ra-We accept (  X   X  ,  X  Z  X   X  ) , or remain as before, according to a Bernoulli distribution with probability min { 1 ,r (  X  ( t ) where See Algorithm 4 in the Supplement for details. 3.1. String-valued evolutionary models Molecular evolutionary models, central ingredients of modern phylogenetics, describe how biomolecular se-quences (RNA, DNA, or proteins) evolve over time via a CTMC where jumps are character substitutions, inser-tions and deletions (indel), and states are biomolecular sequences. Previous work focused on the relatively re-stricted range of evolutionary phenomena for which com-puting marginal probabilities of the form P x ( X N = y ) can be done exactly.
 In particular, we are not aware of existing methods for do-ing Bayesian inference over context-dependent indel mod-els, i.e. models where insertions and deletions can depend on flanking characters. Modelling the context of indels is important because of a phenomenon called slipped strand mispairing (SSM), a well known explanation for the evo-lution of repeated sequences (Morrison, 2009; Hickey &amp; Blanchette, 2011; Arribas-Gil &amp; Matias, 2012). For exam-ple, if a DNA string contains a substring of  X  X ATATA X  , the non-uniform error distribution in DNA replication is likely to lead to a long insertion of extra  X  X A X  repeats. Model. In order to describe our SSM-aware model, it is enough to describe its behavior on a single branch of a tree, say of length T . Each marginal variable X t is assumed to have the countably infinite domain of all possible molecular sequences. We define  X  ( x ) , as a function of the mutation rate per base  X  sub , the global point insertion (i.e. insertion of a single nucleotide) rate  X  pt , the point deletion rate per base  X  , the global SSM insertion rate  X  SSM (which copies a substring of length up to three to the right of that substring), and the SSM deletion rate per valid SSM deletion location  X 
SSM (deletion of a substring of length up to three at the right of an identical substring):  X  ( x ) = m ( x )  X  sub +  X  pt + m ( x )  X  pt +  X  SSM + k ( x )  X  where m ( x ) is the length of the string x and k ( x ) is the number of valid SSM deletion locations in x . We denote these evolutionary parameters by  X  = (  X  sub , X  pt , X  pt , X  SSM , X  SSM ) . The jump transition probabil-ities from x to x 0 are obtained by normalizing each of the above rates. For example the probability of deleting the first character given that there is a change from sequence x is  X  pt / X  ( x ) . Note that since the total insertion rate does not depend on the length of the string, the process is explosion-free for all  X  . At the same time, there is no fixed bound on the deletion rate, ruling out classical methods such as uni-formization or matrix exponentiation.
 Validation on a special case. Before moving on to more complex experiments, we started with a special case of our model where the true posterior can be computed nu-merically. This is possible by picking a single branch, and setting  X  SSM =  X  SSM = 0 , in which case the pro-cess reduces to a process for which analytic calculation of
P x, X  ( X N = y ) is tractable (Bouchard-C  X  ot  X  e &amp; Jordan, 2012). We fixed the substitution parameter  X  sub , and com-puted as a reference the posterior by numerical integration on  X  pt ,  X  pt truncated to [0 , 3] 2 and using 100 2 bins. We generated 200 pairs of sequences along with their se-quence alignments 4 , with T = 3 / 10 , X  =  X  pt = 2 , X  =  X  pt = 1 / 2 and held out the mutations and the true value of parameters  X  and  X  . We put an exponential prior with rate 1.0 on each parameter. We approximate the posterior using our method, initializing the parameters to  X  =  X  = 1 , using  X  = 2 / 3 , X  = 19 / 20 , 64 particles, and a proposal q over parameters given by the multiplicative proposal of Lakner et al. (Lakner et al., 2008). We show the results of  X  in Figure 1a and in the Supplement Figure: results of param-eter  X  . In both cases the posterior approximation is shown to closely mirror the numerical approximation. The evolu-tion of the Monte Carlo quartiles computed on the prefixes of Monte Carlo samples also shows that the convergence is rapid (Figure 1b).
 Next, we compared the performance of a GIMH algorithm computing  X  Z  X  ( t ) using our method, with a GIMH algorithm computing  X  Z ( t )  X  ( t ) using forward sampling. We performed this comparison by computing the Effective Sample Size (ESS) after a fixed computational budget (3 days). For the parameter  X  , our method achieves an ESS of 1782.7 versus 44.6 for the forward sampling GIMH method; for the pa-rameter  X  , our method achieves an ESS of 6761.2 versus 90.2 for the forward sampling GIMH method. In those ex-periments, we used 100 particles per MCMC step, but we also tried different values and observed the same large gap favoring our method (see Supplement Figure: Varying the number of particles per MCMC step).
 We also generated three datasets based on branch lengths quences ( x,y ) along with their sequence alignments and estimated the transition probability P x ( Y N = y ) using our method (denoted Time-Integrated Path Sampling, TIPS), and using forward simulation (denoted forward sampling, FS). We compared the two methods in Figure 1c by look-ing at the absolute log error of the estimate  X  p , error(  X  p ) = | log  X  p  X  log P x ( X N = y ) | . We performed this experiment with a range of numbers of particles, { 2 1 , 2 2 ,..., 2 plotted the relative errors as a function of the wall clock time needed for each approximation method. We also com-puted the variances of the importance weights for specific alignments and compared these variances for FS and TIPS (see Figure 1d). We observed that the variances were con-sistently two orders of magnitudes lower with our method compared to FS.
 Tree inference via SMC. We now consider the general case, where inference is on a phylogenetic tree, and the SSM parameters are non-zero. To do this, we use existing SMC algorithms for phylogenetic trees (Teh et al., 2008; Bouchard-C  X  ot  X  e et al., 2012; Wang, 2012), calling our algo-rithm at each proposal step. We review phylogenetic infer-ence in the Supplement where we also give in Algorithm 6 the details of how we combined our method with phyloge-
Variance Experiments on trees via SMC.
 weights, FS, IS, etc this experiment with a range of numbers of particles, { 2 of the wall clock time needed for each approximation method. (5) [[need to explain CPU time]] We also computed the variances of the importance weights for specific alignments and compared these variances for FS and IS for a range of branch lengths (see (6) [[figure, more in SI]] ). We observed that the vari-ances were consistently two orders of magnitudes lower with our method compared to FS.

Tree inference via SMC. We now consider the gen-eral case, where inference is on a phylogenetic tree, and the SSM parameters are non-zero. To do this, we use existing SMC algorithms for phylogenetic trees ( ?? Wang , 2012 ), calling our algorithm at each pro-posal step. We review phylogenetic inference in the
Supplement, where we also give in Algorithm (7) [[TODO]] the details of how we combined our method with phylogenetic SMC.

To evaluate our method, we sampled 10 random trees from the coalescent on 10 leaves, along each of which we simulated 5 sets of molecular sequences according to our evolutionary model. We used the following pa-netic SMC.
 To evaluate our method, we sampled 10 random trees from the coalescent on 10 leaves, along each of which we simulated 5 sets of molecular sequences according to our evolutionary model. We used the following parameters: SSM length=3,  X  sub = 0 . 03 ,  X  pt = 0 . 05 ,  X  pt = 0 . 2 ,  X 
SSM = 2 . 0 , and  X  SSM = 2 . 0 . One subset of simulated data is shown in the Supplement Figure: Sequence Sim-ulation. The unaligned sequences on leaves are used for tree reconstruction using our method. We summarized the posterior over trees using a consensus tree optimizing the posterior expected pairwise distances (Felsenstein, 1981). Figure 1e shows tree distances using the partition metric (Felsenstein, 2003) between generated trees and consen-sus trees reconstructed using our evolutionary model. The tree distance decreases as the number of particles increases, and a reasonable accuracy is obtained with only 100 parti-cles, suggesting that it is possible to reconstruct phyloge-nies from noisy data generated by complex evolutionary mechanisms. 3.2. RNA folding pathways Nucleic acid folding pathways predict how RNA and DNA molecules fold in on themselves via intra-molecular inter-actions. The state space of our stochastic process that de-scribes folding is the set of all folds, or secondary struc-tures, of the nucleic acid molecule which is a combinato-rial object. For RNA molecules, the secondary structure is the primary determiner of RNA function. For DNA its fold can help determine gene transcription rates. Under-standing the folding pathways can be useful for designing nano-scale machines that have potential health applications (Venkataraman et al., 2010). For these reasons, it is of-ten useful in applications to get an accurate estimate of the probability that a nucleic acid molecule beginning in one secondary structure, x , will transition in the given time, T , to a target structure, y . This is called the transition proba-bility, and it is typically computed by either solving a sys-tem of linear differential equations or by computing a ma-trix exponential of a large matrix. Here, we will use our method (denoted as TIPS) to approximate these transition probabilities.
 Model. An RNA fold can be characterized by a set of base pairs, either C-G, A-U, or G-U, each of which specifies the sequence positions of the two bases involved in the pairing. We will default the discussion to RNA sequences where we are interested in pseudo-knot-free RNA structures. These secondary structures can be represented as a planar circle graph with the sequence arrayed along a circle and non-crossing arcs between positions of the sequence which are base paired. Here, we will use structure to mean secondary structure. The folding of a molecule into secondary struc-tures happens in a dynamic fashion.
 In the pathway model we consider, successive structures X and X i +1 must differ by exactly one base pair. Let X 1 = x and X N = y where x is the given start structure and y is the given final structure. See for example Figure 5 of the Supplement, where a folding path is given for a short RNA (holding times not shown) with x being the unfolded state and y being the Minimum Free Energy (MFE) structure. To formalize the folding pathway, we need to introduce the generator matrix, Q . This matrix contains an en-try for every possible pair of secondary structures. The Kawasaki rule gives the rate of the probabilistic process moving from structure x to structure x 0 as  X  ( x )  X  ( x,x exp ( E ( x )  X  E ( x 0 )) / ( kT ) if x 0  X  R ( x ) , and zero other-wise where E ( x ) is the energy of structure x , R ( x ) is the set of secondary structures within one base pair of structure x and k is the Boltzmann constant. When given a nucleic acid sequence of m bases, there are at most O (3 m ) sec-ondary structures that can be created from it, making the size of the generator matrix exponential in the sequence length. This model was described by Flamm et al. (Flamm et al., 2000).
 Results. In this section, we compare the accuracy of the transition probability estimates given by our method (TIPS) to those obtained by forward sampling method (FS) which is still widely used in the field of RNA folding path-ways (Flamm et al., 2000; Schaeffer, 2012). We used the RNA molecules shown in Supplement Table: Biological RNA Sequences.
 For each method (TIPS and FS) and molecule, we first ap-proximated the probability P x ( X N = y ) that beginning in its unfolded structure x , the molecule would end, after fold-ing time T , in its MFE structure y . We then computed, as a reference, the probability of this transition using an expen-sive matrix exponential. Computing the matrix exponential on the full state space was only possible for the RNAs of no more than 12 nucleotides. For the longer RNAs, we re-stricted the state space to a connected subset S of secondary structures (Kirkpatrick et al., 2013). While our method scales to longer RNAs, we wanted to be able to compare against forward sampling and to the true value obtained by matrix exponentiation.
 We ran the experiments with a range of number of parti-from { 0 . 125 , 0 . 25 ,  X  X  X  , 8 } . Here, similarly to the previous example, we compare the performance of the two methods by looking at the absolute log error of the estimate  X  p (i.e., error(  X  p ) = | log  X  p  X  log P x ( X N = y ) | ) over all replicates. The parameters used for the TIPS method are as follows:  X  = 2 3 and  X  = max(0 . 25 , 1  X  T 16 ) where T is the speci-fied folding time interval.
 Figures 2a, 2d show the performance of the FS and TIPS methods on selective folding times, { 0 . 25 , 1 , 4 } . Figures 2b, 2e show the CPU times (in milliseconds) corresponding to the minimum number of particles required to satisfy the folding times. Supplement Figure: Performance vs. fold-ing time shows similar plots for two other RNA molecules. The variances of FS and TIPS weights, for 5 6 = 15625 par-ticles, are also computed and compared on different folding times (see Figures 2c, 2f).
 The graphs show that our novel method (TIPS) outperforms FS in estimating the probability of transition from x to y in shorter folding times, since it needs many fewer particles (and correspondingly faster CPU times) than FS to be able to precisely estimate the probability. For instance, for the RNA21 molecule with folding time 0.25, FS cannot satisfy the accuracy level I , given above, even with 15625 parti-cles, however TIPS only needs 5 particles with 16 ms of CPU time to satisfy the same accuracy level. Similarly, the variance of our method is smaller by a larger margin (note that the variance is shown in log scale in Figures 2c, 2f). For longer folding times in Figure 2, the performance of the TIPS and FS methods would be comparable (in terms of the obtained errors and CUP times) slightly in favour of forward sampling. For example, for the HIV23 molecule with folding time 4.0, TIPS and FS require 5 and 25 parti-cles, and CPU times, 12 ms and 5 ms, respectively to satisfy I .
 One caveat of these results is that in contrast to the phylo-genetic setup, where TIPS was not sensitive to a range of values of the tuning parameters  X , X  , it was more sensitive to these tuning parameters in the RNA setup. See Supple-ment Figure: Tuning parameter  X  . We believe that the be-havior of our method is more sensitive to  X , X  in the RNA case because the sampled jump chains are typically longer. Intuitively, for longer folding times, the transition proba-bilities are more influenced by the low probability paths, as these low probability paths comprise a greater percent of all possible paths. This means that any setting of  X  that heavily biases the sampled paths to be from the region just around x and y will need to sample a large number of paths in order to approximate the contribution of paths with a low proba-bility. This situation is analogous to the well-known prob-lems in importance sampling of mismatches between the proposal and actual distributions. Similar sampling consid-erations apply to parameter  X  which controls the number of excursions from y . If  X  is too restrictive, again, paths will be sampled that do not well reflect the actual probability of excursions. Parameter tuning is therefore an important area of future work. It might be possible to use some auto-mated tuners (Hutter et al., 2009; Wang et al., 2013) or to approach the problem by essentially creating mixtures of proposals each with its own tuning parameters.
 At the same time, note that the reason why FS can still per-form reasonably well for longer folding times is that we picked the final end point to be the MFE, which has high probability under the stationary distribution. For low prob-ability targets, FS will often fail to produce even a single hitting trajectory, whereas each trajectory sampled by our method will hit the target by construction. We have presented an efficient method for approximating transition probabilities and posterior distributions over pa-rameters in countably infinite CTMCs. We have demon-strated on real RNA molecules that our method is com-petitive with existing methods for estimating the transition probabilities which marginalize over folding pathways and provide a model for the kinetics of a single strand of RNA interacting chemically with itself. We have also shown, us-ing a realistic, context-dependent indel evolutionary pro-cess, that the posterior distributions approximated by our method were accurate in this setting.
 What makes our method particularly attractive in large or countably infinite state space CTMCs is that our method X  X  running time per particle is independent of the size of the state space. The running time does depend cubically on the number of imputed jumps, so we expect that our method will be most effective when the typical number of transi-tions between two observations or imputed latent state is moderate (no more than approximately a thousand with current architectures). The distribution of the jump chain should also be reasonably concentrated to ensure that the sampler can proceed with a moderate number of particles. We have shown two realistic examples where these condi-tions are empirically met.
 Acknowledgment This work was partially funded by an NSERC Discovery Grant and a Google Faculty Award. Computing was sup-ported by WestGrid.

