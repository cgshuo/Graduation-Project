 Modern data science relies on data analytic pipelines to organize interdependent computational steps. Such analytic pipelines often involve different algorithms across multiple steps, each with its own hyperparameters. To achieve the best performance, it is often critical to select optimal algorithms and to set appropriate hyperparameters, which requires large computational efforts. Bayesian optimization provides a principled way for searching optimal hyperparameters for a single algorithm. However, many challenges remain in solving pipeline optimization problems with high-dimensional and highly conditional search space. In this work, we propose Fast LineAr SearcH (FLASH), an efficient method for tuning analytic pipelines. FLASH is a two-layer Bayesian optimization framework, which firstly uses a parametric model to select promising algorithms, then computes a nonparametric model to fine-tune hyperparameters of the promising algorithms. FLASH also includes an effective caching algorithm which can further accelerate the search process. Extensive experiments on a number of benchmark datasets have demonstrated that FLASH significantly outperforms previous state-of-the-art methods in both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20% improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art performance on a real-world application for healthcare predictive modeling.
 Bayesian optimization; Automated hyperparameter tuning; Data analytic pipeline; Health analytics
Modern data science often requires many computational steps such as data preprocessing, feature extraction, model building, and model evaluation, all connected in a data analytic pipeline . Pipelines provide a natural way to represent, organize and standardize data analytic tasks, which are considered to be an essential element in the data science field [ 11] due to their key role in large-scale data science projects. Many machine learning toolboxes such as scikit-learn [ 36 ], RapidMiner [31 ], SPSS [10 ], Apache Spark [30] provide mechanisms for configuring analytic pipelines. An analytic pipeline skeleton is shown in Figure 1. Each step, such as feature preprocessing and classification, includes many algorithms to choose from. These algorithms usually require users to set hyperparameters, ranging from optimization hyperparameters such as learning rate and regularization coefficients, to model design hyperparameters such as the number of trees in random forest and the number of hidden layers in neural networks. There are an exponential number of choices for the combination of algorithms and hyperparameters in a given analytic pipeline skeleton. Because of the interdependency between all the algorithms and their hyperparameters, the choices can have huge impact on the performance of the best model.

Tuning hyperparameters of a single algorithm can be viewed as an optimization problem of a black-box objective function, which is noisy and often expensive to evaluate. Here the input of black-box are the hyperparameters, and the objective function is the output performance such as accuracy, precision and recall. To tackle this problem, simple methods have been applied such as grid or random search [5 , 3]. While on difficult problems where these simple approaches are not efficient, a more promising model-based approach is Bayesian optimization [ 32, 27, 7, 39 ]. The high-level idea of Bayesian optimization is to define a relatively cheap surrogate function and use that to search the hyperparameter space. Indeed, there exist other global optimization methods, such as evolutionary algorithms [2 ] and optimistic optimization [ 33]. We choose Bayesian optimization framework due to its great performance in practice. Recently, Bayesian optimization methods have been shown to outperform other methods on various tasks, and in some cases even beat human domain experts to achieve better performance via tuning hyperparameters [42, 4].

Despite its success, applying Bayesian optimization for tuning analytic pipelines faces several significant challenges: Existing Bayesian optimization methods are usually based on nonparametric models, such as Gaussian process and random forest. A major drawback of these methods is that they require a large number of observations to find reasonable solutions in high-dimensional space. When tuning a single algorithm with several hyperparameters, Bayesian optimization works well with just a few observations. However, when it comes to pipeline tuning, thousands of possible combinations of algorithms plus their hyperparameters jointly create a large hierarchical high-dimensional space to search over, whereas existing methods tend to become inefficient. Wang et al. [46] tackled the high-dimensional problem by making a low effective dimensional assumption. However, it is still a flat Bayesian optimization method and not able to handle the exploding dimensionality problem caused by hierarchically structured hyperparameters in analytic pipeline tuning.
 Motivating example: We build an analytic pipeline for classification task (details in Section 4). If we give 10 trials for each hyperparameter over 1,456 unique pipeline paths and 102 hyperparameters, we have more than 2 million configurations, which can take years to complete with a brute-force search. Even with the state-of-the-art Bayesian optimization algorithm such as Sequential Model-based Algorithm Configuration (SMAC) [22 ], the process can still be slow as shown in Figure 2. If we know the optimal algorithms ahead time (Oracle) with just hyperparameter tuning of the optimal algorithms, we can obtain significant time saving, which is however not possible. Finally, our proposed method FLASH can converge towards the oracle performance much more quickly than SMAC.

In this paper, we propose a two-layer Bayesian optimiza-tion algorithm called Fast LineAr SearcH (FLASH): the first layer for selecting algorithms, and the second layer for tun-ing the hyperparameters of selected algorithms. FLASH is able to outperform the state-of-the-art Bayesian optimiza-tion algorithms by a large margin, as shown in Figure 2. By designing FLASH, we make three main contributions:  X  We propose a linear model for propagation of error (or other quantitative metrics) in analytic pipelines.
We also propose a Bayesian optimization algorithm for minimizing the aggregated error using our linear error model. Our proposed mechanism can be considered as a hybrid model: a parametric linear model for fast exploration and pruning of the algorithm space, followed by a nonparametric hyperparameter fine-tuning algorithm.  X  We propose to initialize the hyperparameter tuning algorithm using the optimal design strategy [1 , 15 , 38 ] which is more robust than the random initialization. We also propose a fast greedy algorithm to efficiently solve the optimal design problem for any given analytic pipeline.  X  Finally, we introduce a caching algorithm that can significantly accelerate the tuning process. In particular, we model the (time) cost of each algorithm, and incorporate that in the optimization process. This ensures the efficiency of fast search.

We demonstrate the effectiveness of FLASH with exten-sive experiments on a number of difficult problems. On the benchmark datasets for pipeline configurations tuning, FLASH substantially improves the previous state of the art Figure 2: Performance comparison of a data analytic pipeline on MRBI dataset, including the previous state of the art SMAC, proposed method FLASH, and Oracle (i.e., pretending the optimal algorithm configuration is given, and only performing hyperparameter tuning with SMAC on those algorithms). We show the median percent error rate on the test set along with standard error bars (generated by 10 independent runs) over time.
 FLASH outperforms SMAC by a big margin and converges toward Oracle performance quickly. by 7% to 25% in test error rate within the same time budget. We also experiment with large-scale real-world datasets on healthcare data analytic tasks where FLASH also exhibits superior results.
The data analytic pipeline refers to a framework consisting of a sequence of computational transformations on the data to produce the final predictions (or outputs) [26 ]. Pipelines help users better understand and organize the analysis task, as well as increase the reusability of algorithm implementations in each step. Several existing widely adopted machine learning toolboxes provide the functionality to run analytic pipelines. Scikit-learn [ 36 ] and Spark ML [30 ] provide programmatic ways to instantiate a pipeline. SPSS [10 ] and RapidMiner [31 ] provide a visual way to assemble an analytic pipeline instance together and run. Microsoft Azure Machine Learning 1 provides a similar capability in a cloud setting. There are also specialized pipelines, such as PARAMO [35 ] in healthcare data analysis.
However, a major difficulty in using these systems is that none of the above described tools is able to efficiently help users decide which algorithms to use in each step. Some of the tools such as scikit-learn, Spark ML, and PARAMO https://studio.azureml.net allow searching all possible pipeline paths and tuning the hyperparameters of each step using an expensive grid search approach. While the search process can be sped up by running in parallel, the search space is still too large for the exhaustive search algorithms.
Bayesian optimization is a well-established technique for global and black-box optimization problems. In a nutshell, it comprises two main components: a probabilistic model and an acquisition function. For the probabilistic model, there are several popular choices: Gaussian process [41 , 42], random forest such as Sequential Model-based Algorithm Configuration (SMAC) [ 22 ], and density estimation models such as Tree-structured Parzen Estimator (TPE) [ 5]. Given any of these models, the posterior mean and variance of a new input can be computed, and used for computation of the acquisition function. The acquisition function defines the criterion to determine future input candidates for evaluation. Compared to the objective function, the acquisition function is chosen to be relatively cheap to evaluate, so that the most promising next input for querying can be found quickly. Various forms of acquisition functions have been proposed [43 , 20 , 45 , 21 ]. One of the most prominent acquisition function is the Expected Improvement (EI) function [32 ], which has been widely used in Bayesian optimization. In this work, we use EI as our acquisition function, which is formally described in Section 3.

Bayesian optimization is known to be successful in tuning hyperparameters for various learning algorithms on different types of tasks [ 42 , 14, 4, 41 , 46 ]. Recently, for the problem of pipeline configurations tuning, several Bayesian optimization based systems have been proposed: Auto-WEKA [44 ] which applies SMAC [ 22 ] to WEKA [ 17 ], auto-sklearn [ 13 ] which applies SMAC to scikit-learn [ 36 ], and hyperopt-sklearn [ 24 ] which applies TPE [ 5] to scikit-learn. The basic idea of applying Bayesian optimization to pipeline tuning is to expand the hyperparameters of all algorithms and create large search space to perform optimization as we will show in the experiments. However, for practical pipelines the space becomes too large which hinders convergence of the optimization process. Auto-sklearn [13 ] uses a meta-learning algorithm that leverages performance history of algorithms on existing datasets to reduce the search space. However, in real-world applications, we often have unique datasets and tasks such that finding similar datasets and problems for the meta-learning algorithm will be difficult.
A data analytic pipeline G = ( V,E ) can be represented as a multi-step Directed Acyclic Graph (DAG), where V is the set of algorithms, and E is the set of directed edges indicating dependency between algorithms. Algorithms are distributed among multiple steps. Let V ( k ) i denote the i th algorithm in the k th step. Each directed edge ( V V i to V algorithms in the same step. We also have an input data vertex V in which points to all algorithms in the first step, and an output vertex V out which is pointed by all algorithms in the last step. Figure 3: A toy example of data analytic pipeline.
 One possible pipeline path, flowing from the input V in to the output V out , is highlighted in shaded area.
A pipeline path is any path from the input V in to the output V out in pipeline graph G . To denote a pipeline path of K steps, we use K one-hot vectors p ( k ) (1  X  k  X  K ), each denoting the algorithm selected in the k -th step. Thus, the concatenation of one-hot vectors p = p (1) ,..., p ( K ) { 0 , 1 } N denotes a pipeline path, where N is the total number of algorithms in the pipeline G . Figure 3 shows a small data analytic pipeline with two steps. The first step contains two algorithms, and the second step contains three. One possible pipeline path is highlighted in the shaded area. On this pipeline path, V (1) 2 and V (2) 3 are selected in the first and second step, so that we have p (1) = [0 , 1] and p (2) = [0 , 0 , 1]. Thereby, the highlighted pipeline path is given by p = p (1) , p (2) = [0 , 1 , 0 , 0 , 1]. For any pipeline path p , we concatenate all of its hyperparameters in a vector  X  p . The pair of path and hyperparameters, i.e. ( p ,  X  p ), forms a pipeline configuration to be run. For ease of reference, we list the notations in Table 1.

The problem of tuning data analytic pipelines can be formalized as an optimization problem:
Problem 1. Given a data analytic pipeline G with input data D , resource budget T , evaluation metric function m ( G, D ; p ,  X  p ) , resource cost of running pipeline  X  ( G, D ; p ,  X  p ) , how to find the pipeline path p and its hyperparameters  X  p with best performance m ? ? The performance of the best pipeline path is denoted by m objective is to approach the optimal performance within the budget T via optimizing over p ,  X  p ; i.e., we would like our solution b p , b  X  values of .
 To efficiently tackle this problem, we propose a two-layer Bayesian optimization approach named Fast LineAr SearcH (FLASH). We generally introduce the idea of linear model and describe the algorithm in Section 3.1 . An immediate advantage of using linear model is that we can use more principled initialization instead of random initialization, as discussed in Section 3.2 . We use cost-sensitive modeling to prune the pipeline, as described in Section 3.3 . Finally, we accelerate the entire optimization procedure via pipeline caching, which we describe in Section 3.4 .
Inspired by the performance of linear regression under Table 1: Mathematical notations used in this paper. model misspecification [48, 16 , 28 ] and superior sample com-plexity compared to more flexible nonparametric techniques [47 ], we seek parametric models for propagation of error (or other quantitative metrics) in analytic pipelines. The high level idea of FLASH is as follows: we propose a linear model for estimating the propagation of error (or any other met-ric) in a given analytic pipeline. The linear model assumes that the performance of algorithms in different steps are in-dependent, and the final performance is additive from all algorithms. That is, we can imagine that each algorithm is associated with a performance metric, and the total perfor-mance of a pipeline path is the sum of the metrics for all algorithms in the path. This linear model will replace the Gaussian process or random forest in the initial stages of the pipeline tuning process. In the rest of this section, we provide the details of Bayesian optimization with our linear model.

We apply the linear model only to the pipeline selection vector p and assume that the variations due to hyperparam-eters of the algorithms are captured in the noise term. That is, we assume that the error of any pipeline path p can be written as where  X   X  R N denotes the parameters of the linear model. Given a set of observations of the algorithm selection and the corresponding evaluation metric for the selected pipeline path in the form of ( p i ,m i ), i = 1 ,...,n , we can fit this model and infer its mean  X  ( p ) and variance  X  of the performance estimation for any new pipeline path represented by p . In particular, let the design matrix P  X  R n  X  N denote the stacked version of the pipeline paths, i.e., P = [ p 1 ,..., p n ] &gt; , and m  X  R corresponding response values of the evaluation metrics, m = [ m 1 ,...,m n ]. We use the following L 2 regularized linear regression to obtain the robust estimate for  X  from history observations:
Algorithm 1: Fast Linear Search (FLASH) input : Data analytic pipeline G ; input data D ; total output : Optimized pipeline configuration b p and b  X  /* Phase 1: Initialization (Section 3.2 ) */ while budget T init not exhausted do 2 p  X  new pipeline path from Algorithm 2 3  X  p  X  random hyperparameters for p 4 m, X   X  RunPipeline( G, D ; p ,  X  p ) with Algorithm 3 5 P  X  [ P ; p &gt; ], m  X  [ m ,m ],  X   X  [  X  , X  ] 6  X   X  b  X  ( P , m ),  X   X   X  c  X   X  ( P ,  X  ) using Eq. ( 1) /* Phase 2: Pipeline pruning (Section 3.3 ) */ while budget T prune not exhausted do 8 p  X  argmax p EIPS ( p , P ,  X  ,  X   X  ) using Eq. ( 4) 9  X  p  X  random hyperparameters for p 10 m, X   X  RunPipeline( G, D ; p ,  X  p ) with Algorithm 3 11 P  X  [ P ; p &gt; ], m  X  [ m ,m ],  X   X  [  X  , X  ] 12  X   X  b  X  ( P , m ),  X   X   X  b  X   X  ( P ,  X  ) using Eq. ( 1)
G 0  X  construct subgraph of G with top r pipeline paths with largest EIPS ( p , P ,  X  ,  X   X  ) using Eq. ( 4) /* Phase 3: Pipeline tuning */
S  X  history observations within G 0
Initialize model M given S while budget T total not exhausted do 17 p ,  X  p  X  next candidate from M 18 m  X  RunPipeline( G 0 , D ; p ,  X  p ) with Algorithm 3 19 S  X  S  X  X  ( p ,  X  p ,m ) } 20 Update M given S b p , b  X  where for any vector x  X  R n the L 2 norm is defined as k x k 2 = q P n i =1 x 2 i . The predictive distribution for the linear model is Gaussian with mean b  X  p = b  X  &gt; p and variance b  X  noise in the model. We estimate  X   X  as follows: the residual in the i th observation is computed as b i = b  X  i  X  m i where b  X  i = b  X  &gt; p i is the estimate of m i by our model. Thus, the variance of the residual can be found as b  X  2 = var ( b  X  where var (  X  ) denotes the variance operator.

To perform Bayesian optimization with linear model, we use the popular Expected Improvement (EI) criteria, which recommends to select the next sample p t +1 such that the following acquisition function is maximized. The acquisition function represents the expected improvement over the best observed result m + at a new pipeline path p [44 ]: the trade-off between exploitation and exploration. EI function is maximized for paths with small values of m and large values of  X  p , reflecting the exploitation and exploration trade-offs, respectively. To be more specific, larger  X  encourages more exploration in selecting the next sample. The functions  X (  X  ) and  X  (  X  ) represent CDF and PDF of standard normal distribution, respectively. The idea of Bayesian optimization with EI is that at each step, we compute the EI with the predictive distribution of the existing linear model and find the pipeline path that maximizes EI. We choose that path and run the pipeline with it to obtain a new ( p i ,m i ) pair. We use this pair to refit and update our linear model and repeat the process. Later on we also present an enhanced version of EI via normalizing it by cost called Expected Improvement Per Second (EIPS). We provide the full details of FLASH in Algorithm 1. While the main idea of FLASH is performing Bayesian optimization using linear model and EI, it has several additional ideas to make it practical. Specifically, FLASH has three phases: In Phase 3, we use state-of-the-art Bayesian optimization algorithm, either SMAC or TPE. These algorithms are iterative: they use a model M such as Gaussian process or random forest and use EI to pick up a promising pipeline path with hyperparameters for running, and then update the model with the new observation just obtained, and again pick up the next one for running. The budget is bounded by T total . Note that our algorithm is currently described for a sequential setting but can be easily extended to support parallel runs of multiple pipeline paths as well.
Most Bayesian optimization algorithms rely on random initialization which can be inefficient; for example, it may select duplicate pipeline paths for initialization. Intuitively, the pipeline paths used for initialization should cover the pipeline graph well, such that all algorithms are included enough times in the initialization phase. The ultimate goal is to select a set of pipeline paths for initialization such that the error in estimation of  X  is minimized. Given our proposed linear model, we can find the optimal strategy for initialization to make sure the pipeline graph is well covered and the tuning process is robust. In this section, we describe different optimality criteria studied in statistical experiment design [1 , 15 ] and active learning [38 ], and design an algorithm for initialization step of FLASH.

Given a set of pipeline paths with size n , there are several different optimality criteria in terms of the eigenvalues of the Gram matrix H = P n i =1 p i p &gt; i as follows [38 ]: A-optimality: maximize P n ` =1  X  ` ( H ).
 D-optimality: maximize Q n ` =1  X  ` ( H ).
 E-optimality: maximize  X  n ( H ), the n th largest eigen-
In practice, it is better to use the time normalized EI (that is EIPS) during Phase 2; this idea is described in Section 3.3 .
Algorithm 2: Initialization with Optimal Design /* Batch version */ input : B initial candidates { p i } B i =1 ; number of output : Optimal set of pipeline paths Q p 1  X  random pipeline path for initialization
Q  X  X  p 1 } for ` = 2 ,...,n init do 5 j ?  X  argmax j D ` ( H + p j p &gt; j ) for j = 1 ,...,B . 7 Q  X  Q  X  X  p j ? } /* Online version */ input : B candidates { p i } B i =1 ; current Gram matrix H output : Next pipeline path p j ? , j ?  X  X  1 ,...,B } j ?  X  argmax j D ` ( H + p j p &gt; j ) for j = 1 ,...,B It is easy to see that any arbitrary set of pipeline path designs satisfies the A-optimality criterion.

Proposition 1. Any arbitrary set of pipeline paths with size n is a size-n A-optimal design.

Proof. For any arbitrary set of pipeline paths with size n , we have: X The last step is due to particular pattern of p in our problem. Thus, we show that P n ` =1  X  ` ( H ) is constant, independent of the design of pipeline paths.

Proposition 1 rules out use of A-optimality in pipeline initialization. Given the computational complexity of E-optimality and the fact that it intends for optimality in the extreme cases, we choose D-optimality criterion. The D-optimality criterion for design of optimal linear regression can be stated as follows: suppose we are allowed to evaluate n init samples p i , i = 1 ,...,n init , these samples should be designed such that the determinant of the Gram matrix H is maximized. While we can formulate an optimization problem that directly finds p i values, we found that an alternative approach can be computationally more efficient. In this approach, we first generate B candidate pipeline paths for an integer B larger than the number of algorithms in the pipeline N . This set may include all possible pipeline paths if the total number of paths is small. Then, our goal becomes selecting a subset of size n init from them. We can formulate the optimal design as follows The last constraint 1 &gt; a = n init indicates that only n pipeline paths should be selected. The objective function is concave in terms of continuous valued a [6, Chapter 3.1.5]. Thus, a traditional approach is to solve it by convex programming after relaxation of the integrality constraint on a . The matrix in the argument of the determinant is only N -dimensional which means calculation of the determinant should be fast. Nestrov X  X  accelerated gradient descent [34 ] or Frank-Wolfe X  X  [23 ] algorithms can be used for efficiently solving such problems.

An even faster solution can be found by using greedy forward selection ideas which are fast and popular for optimal experiment design, for example see [37 , 18 , 25 ] and the references therein. To apply greedy technique to our problem, we initialize the solution by picking one of the pipeline path H = p i p &gt; i . Then, at ` th step, we add the path that maximizes j ? = argmax j D ` ( H + p j top min( `,p ) eigenvalues of its argument. The algorithm is described in Algorithm 2. The optimization problem in Eq. (3 ) appears in other fields such as optimal facility location and sensor planning where greedy algorithm is known to have a 1  X  1 e approximation guarantee [8 , 40 ].
One further desirable property of the greedy algorithm is that it is easy to run it under a time budget constraint. We call this version the online version in Algorithm 2, where instead of a fixed number of iteration n init , we run it until the exhaustion of our time budget. See Line 2 in Algorithm 1 and the online version of Algorithm 2.
The Expected Improvement aims at approaching the true optimal (doing well) within a small number of function evaluations (doing fast). However, the time cost of each function evaluation may vary a lot due to different settings of hyperparameters. This problem is particularly highlighted in pipeline configurations tuning, since the choice of algorithms can make a huge difference in running time. Therefore, fewer pipeline runs are not always  X  X aster X  in terms of wall-clock time. Also, in practice, what we care about is the performance we can get within limited resource budget, rather than within certain evaluation times. That is why we need cost-sensitive modeling for the pipeline tuning problem.

Expected Improvement Per Second (EIPS) [ 41] proposes another acquisition function for tuning of a single learning algorithm by dividing the EI of each hyperparameter by its runtime. To apply EIPS in pipeline tuning problem, we use a separate linear model to model the total runtime of pipeline paths. Similar to the linear model for error propagation, the linear model for time assumes that on a pipeline path each algorithm partly contributes to the total time cost and the runtimes are additive. To apply the linear model, we replace the performance metric m with the cost metric  X  . The linear cost model parametrized by  X   X  can be efficiently updated using Eq. ( 1). As described in Algorithm 1,  X   X  be updated together with  X  at the end of Phase 2. We note that, in practice, the budget T and the cost  X  (  X  ) can be any quantitative costs of budgeted resources (e.g., money, CPU time), which is a natural generalization of our idea.
With the cost model above, we get the cost-sensitive acquisition function over the best observed result m + at a new pipeline path p : EIPS ( p , P ,  X  ,  X   X  ) = E [ I m + ( p )] where u = m
Algorithm 3: Pipeline Caching input : Data analytic pipeline G ; input data D ; /* Run pipeline with cache pool */ for k  X  1 ,...,K do 4 if h  X  C then 5 D ( k +1)  X  cached result from C 6 else /* Clean up cache pool when necessary */ if T cache exhausted then 10 Discard least recently used (LRU) items in C Here the dependency in  X  and  X   X  is captured during computation of  X  p ,  X  p , and  X  ( p ). We take logarithm of cost  X  (  X  ) to compensate the large variations in the runtime of different algorithms. This acquisition function balances  X  X oing well X  and  X  X oing fast X  in selecting the next candidate path to run. During the optimization, it will help avoid those costly paths with poor expected improvement. More importantly, at the end of Phase 2 in Algorithm 1, EIPS is responsible to determine the most promising paths, which perform better but cost less, to construct a subgraph for the last phase fine-tuning. For this purpose, we set the exploration parameter  X  to 0 to only select (Line 13 in Algorithm 1).
During the experiments, we note that many pipeline runs have overlapped algorithms in their paths. Sometimes these algorithms have exactly the same pipeline path and the same hyperparameter settings along the path. This means that we are wasting time on generating the same intermediate output again and again. For example, consider the min-max normalization algorithm in the first pipeline step: this algorithm will be executed many times, especially when it performs well so that Bayesian optimization methods prefer to choose it.

To reduce this overhead, we propose a pipeline caching algorithm, as described in Algorithm 3. When running a pipeline, we check the cache before we run each algorithm. If it turns out to be a cache hit, the result will be immediately returned from cache. Otherwise, we run the algorithm and cache the result. There is a caching pool (e.g., disk space, memory usage) for this algorithm. We use the Least Recently Used (LRU) strategy to clean up the caching pool when budget becomes exhausted.

Caching can significantly reduce the cost of pipeline runs, and accelerates all three phases of FLASH. Algorithms closer to the pipeline input vertex, usually the data preprocessing steps, have higher chance to hit the cache. In fact, when we deal with large datasets on real-world problems, the preprocessing step can be quite time-consuming such that caching can be very efficient.
In this section, we perform extensive experiments on a number of benchmark datasets to evaluate our algorithm compared to the existing approaches. Then we study the impact of different algorithm choices in each component of FLASH.

We conduct experiments on a group of public benchmark datasets on classification task, including Madelon , MNIST, MRBI and Convex 3 . These prominent datasets have been widely used to evaluate the effectiveness of Bayesian optimization methods [44 , 13 , 5]. We follow the original train/test split of all the datasets. Test data will never be used during the optimization: the once and only usage of test data is for offline evaluations to determine the performance of optimized pipelines on unseen test set. In all benchmark experiments, we use percent error rate as the evaluation metric.

As discussed in Section 2, SMAC [ 22] and TPE [5 ] are the state-of-the-art algorithms for Bayesian optimization [44 , 13 , 24 ], which are used as baselines. Note that Spearmint [ 41 ], a Bayesian optimization algorithm based on Gaussian process is not applicable since it does not provide a mechanism to handle the hierarchical space [12 ]. Besides SMAC and TPE, we also choose random search as a simple baseline for sanity check. Thus, we compare both versions of our method FLASH (with SMAC in Phase 3) and FLASH ? (with TPE in Phase 3) against three baselines in the experiments.
 Implementation: To avoid possible mistakes in imple-menting other methods, we choose a general platform for hyperparameter optimization called HPOlib [ 12 ], which pro-vides the original implementations of SMAC, TPE, and ran-dom search. In order to fairly compare our method with others, we also implement our algorithm on top of HPOlib, and evaluate all the compared methods on this platform. We make the source code of FLASH publicly available at https://github.com/yuyuz/FLASH .

We build a general data analytic pipeline based on scikit-learn [36 ], a popular used machine learning toolbox in Python. We follow the pipeline design of auto-sklearn [13 ]. There are four computational steps in our pipeline: 1) feature rescaling, 2) sample balancing, 3) feature preprocessing, and 4) classification model. Each step has various algorithms, and each algorithm has its own hyperparameters. Adjacent steps are fully connected. In total, our data analytic pipeline contains 33 algorithms distributed in four steps, creating 1,456 possible pipeline paths with 102 hyperparameters (30 categorical and 72 continuous), which creates complex high-dimensional and highly conditional search space. More details and statistics of this pipeline are available on our Github project page.
In all experiments, we set a wall-clock time limit of 10 hours for the entire optimization, 15 minutes time limit and 10GB RAM limit for each pipeline run. We perform 10
The benchmark datasets are publicly available at http: //www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets. independent optimization runs with each baseline on each benchmark dataset. All experiments were run on Linux machines with Intel Xeon E5-2630 v3 eight-core processors at 2.40GHz with 256GB RAM. Since we ran experiments in parallel, to prevent potential competence in CPU resource, we use the numactl utility to bound each independent run in single CPU core.

For our algorithm FLASH, we set both T init and T prune as 30 iterations (equal to the number of algorithms in the pipeline), which can be naturally generalized to other budgeted resources such as wall-clock time or money. We set  X  to 100 in the EIPS function. Note that the performance are not sensitive to the choices of those parameters. Finally, we set the number of pipeline paths r to 10 , which works well in generating a reasonable-size pruned pipeline G In benchmark experiments, we compare the performance of FLASH without caching to other methods because the pipelines do not have complex data preprocessing data like many real-world datasets have. We will use caching for real-world experiments later in Section 5.
Table 2 reports the experimental results on benchmark datasets. For each dataset, we report the performance achieved within three different time budgets. As shown in the table, our methods FLASH and FLASH ? perform significantly better than other baselines consistently in all settings, in terms of both lower error rate and faster convergence. For example, on the Madelon dataset, our methods reach around 12% test error in only 3 hours, while other baselines are still far from that even after 10 hours.
Performing statistical significance test via bootstrapping, we find that often FLASH and FLASH ? tie with each other on these benchmark datasets. For all the methods, the test error is quite consistent with the validation error, showing that the potential overfitting problem is well prevented by using cross validation.

Figure 4 plots the convergence curves of median test error rate along with time for all baseline methods. As shown in the figure, after running about 4 hours, FLASH and FLASH ? start to lead others with steep drop of error rate, and then quickly converge on a superior performance.
FLASH has three main components: optimal design for initialization, cost-sensitive model for pipeline pruning, and pipeline caching. To study their individual contributions to the performance gain, we drop out each of the component and compare the performance with original FLASH. Since caching will be used for real-world experiments on large dataset, we describe the analysis of caching component in Section 5. Here we use MRBI dataset for these experiments.
Figure 5(a) shows the difference between using random initialization and optimal design by plotting the perfor-mance on initial 30 pipeline runs. The desirable property of optimal design ensures to run reasonable pipeline paths, giving FLASH a head start at the beginning of optimization. While random initialization is not robust enough, especially when the number of pipeline runs is very limited and some algorithms will have no chance to run due to the randomness. Figure 5(b) shows the impact of pipeline pruning in the sec-ond phase of FLASH. Dropping out the pruning phase with EIPS, and using SMAC immediately after Phase 1, we see best according to a 10,000 times bootstrap test with p = 0 . 05 .
Madelon
MNIST
MRBI
Convex Figure 4: Performance of our methods (FLASH and FLASH ? ) and other compared methods on MRBI dataset. We show the median percent error rate on test set along with standard error bars (generated by 10 independent runs) over time. a major degradation of the performance. The figure clearly shows that in Phase 2 of FLASH, the linear model with EIPS acquisition function is able to efficiently shrink the search space significantly such that SMAC can focus on those al-gorithms which perform well with little cost. This figure confirms the main idea of this paper that a simple linear model can be more effective in searching high-dimensional and highly conditional hyperparameter space.
In this section, to demonstrate a real-world use case, we apply FLASH on a large de-identified medical dataset for classifying drug non-responders. We show how our method can quickly find good classifier for differentiating non-responders vs. responders.

With a collaboration with a pharmaceutical company, we created a balanced cohort of 46,455 patients from a large claim dataset. Patients who have at least 4 times of treatment failure are regarded as drug non-responders (case group). Other patients are responders of the drug (control group). The prediction target is whether a patient belongs to case group or control group. Each patient is associated with a sequence of events, where each event is a tuple of format (patient-id, event-id, timestamp, value) . Table 3 summarizes the statistics of this clinical dataset, including the count of patient, event, medication, and medication class.
 train case 18,581 982,025 434,171 547,854 train control 18,582 622,777 286,198 336,579 test case 4,646 245,776 108,702 137,074 test control 4,646 153,303 70,395 82,908 total 46,455 2,003,881 899,466 1,104,415
Unlike benchmark experiments, the input to the real-world pipeline is not directly as feature vectors. Given a cohort of patients with their event sequences, like [9 ] the pipeline for non-responder classification has two more additional steps than the pipeline described in previous benchmark experiments: 1) Feature construction to convert patient event sequence data into numerical feature vectors. This step can be quite time-consuming as advanced feature construction techniques like sequential mining [ 29] and (a) The impact of optimal design on MRBI dataset Table 4: Performance of real-world dataset. Results are reported using the same settings as Table 2.

Budget (hours) tensor factorization [19 ] can be expensive to compute. On this medical dataset, we consider two kinds of parameters in this step: i) frequency threshold to remove rare events (frequency ranging from 2 to 5) ii) various aggregation functions (including binary, count, sum and average) to aggregate multiple occurrence of events into features. The output of this step will be a feature matrix and corresponding classification targets; 2) Densify the feature matrix from above feature construction step if necessary. Features of this real-world dataset can be quite sparse. We by default use sparse representation to save space and accelerate computation in some algorithms. Unfortunately, not all algorithm implementations in scikit-learn accept sparse features. A decision has to be made here: either sparse matrix for faster result or dense matrix for broader algorithm choices in later steps.

We run the experiments on same machine, use same pa-rameter setting and same budget as benchmark experiments. We compare our method with the same baselines as bench-mark experiments and we continue using error rate as met-ric. Our algorithm has built-in caching mechanism and we will use that. For this real-world dataset, we first compare with baselines with cache enabled. Then we analyze the contribution of caching.
Table 4 shows the performance of our methods compared to baselines when caching is enabled. Due to lack of space we only report the test performance. All cases FLASH and FLASH ? significantly outperform all the baselines.

Figure 5(c) shows the performance of FLASH without caching and original FLASH with caching on the real-world medical dataset. With caching, more pipeline paths can be evaluated within given period of time and our EIPS-based path selection leverages caching to select paths with high performance that run fast. As a result, we can see FLASH with caching converges much faster. For example, with caching we can get low test error within 6 hours.
In this work, we propose a two-layer Bayesian optimiza-tion algorithm named FLASH, which enables highly efficient optimization of complex data analytic pipelines. We showed that all components of FLASH complement each other: 1) our optimal design strategy ensures better initialization, giv-ing a head start to the optimization procedure; 2) the cost-sensitive model takes advantage of this head start, and sig-nificantly improves the performance by pruning inefficient pipeline paths; 3) the pipeline caching reduces the cost dur-ing the entire optimization, which provides a global accelera-tion of our algorithm. We demonstrate that our method sig-nificantly outperforms previous state-of-the-art approaches in both benchmark and real-world experiments. This work was supported by the National Science Foundation, award IIS-#1418511 and CCF-#1533768, research partnership between Children X  X  Healthcare of Atlanta and the Georgia Institute of Technology, CDC I-SMILE project, Google Faculty Award, Sutter health and UCB.

