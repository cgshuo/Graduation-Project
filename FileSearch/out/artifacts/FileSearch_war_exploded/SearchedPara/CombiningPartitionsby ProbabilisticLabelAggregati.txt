 Data clustering represen ts an imp ortan t tool in exploratory data analysis. The lack of objectiv e criteria render mo del selection as well as the iden ti cation of robust solutions par-ticularly dicult. The use of a stabilit y assessmen t and the com bination of multiple clustering solutions represen ts an imp ortan t ingredien t to achiev e the goal of nding useful partitions. In this work, we prop ose a novel way of com-bining multiple clustering solutions for both, hard and soft partitions: the approac h is based on mo deling the proba-bilit y that two objects are group ed together. An ecien t EM optimization strategy is emplo yed in order to estimate the mo del parameters. Our prop osal can also be extended in order to emphasize the signal more strongly by weigh ting individual base clustering solutions according to their con-sistency with the prediction for previously unseen objects. In addition to that, the probabilistic mo del supp orts an out-of-sample extension that (i) mak es it possible to assign pre-viously unseen objects to classes of the com bined solution and (ii) renders the ecien t aggregation of solutions possi-ble. In this work, we also shed some ligh t on the usefulness of suc h com bination approac hes. In the exp erimen tal result section, we demonstrate the comp etitiv e performance of our prop osal in comparison with other recen tly prop osed meth-ods for com bining multiple classi cations of a nite data set.
 I.2.6 [ Computing Metho dologies ]: Arti cial Intelligence| Learning Algorithms Clustering, Consensus Partition, Re-sampling
Data clustering represen ts an imp ortan t tool in mo dern exploratory data analysis. The goal of clustering usually consists of nding a suitable (i.e. meaningful) partition of a given data set. Recen tly, sev eral authors have considered the possibilit y to com bine multiple partitions of the same data set into a single individual clustering solution. Ensem bles of partitionings are believ ed to impro ve robustness and scala-bilit y of the clustering pro cess (as stated e.g. in [24]). This is also motiv ated by exp eriences made in sup ervised settings where ensem ble metho ds have been successfully emplo yed to increase the robustness of the nal classi cation rule. One prominen t represen tativ e of this type of approac hes is Bag-ging as introduced in [3]. Furthermore, the certain ty about an assignmen t can be estimated. And, last but not least, the com bination of man y clusterings migh t lead to a solu-tion that is sup erior to the majority of individual solution. A di eren t application consists of multi-ob jectiv e optimiza-tion, as discussed in [16], where di eren t clustering strategies have been used to create an ensem ble of base hypotheses.
This work addresses the problem of obtaining a consen-sus solution from an ensem ble of clustering solutions. In sup ervised learning, it is relativ ely easy to com bine multi-ple classi ers: in the simplest case, the aggregated classi er will consist of a (poten tially weigh ted) sum of the resp onses of the individual base classi ers. In the con text of unsu-pervised learning, a ma jor dicult y arises since there is no xed cluster lab el order that applies globally . Hence, even if two clustering solutions represen t the same partitioning of the underlying data set, the lab els in the rst solution can be a perm utation of those in the second one. This prob-lem becomes even more serious when the ensem ble consists of clusterings with di eren t num bers of classes. Sometimes com bining multiple clusterings is form ulated as the searc h for a so-called median partition , i.e. a partition that mini-mizes the distance to all solutions in the ensem ble. In this work, we circum vent the so-called label correspondenc e prob-lem just men tioned by switc hing to a di eren t represen tation of a clustering solution, whic h is also kno wn as co-asso ciation matrix in the literature [13]. This transformation yields two bene ts: (i) the aggregation of individual base solutions can be written as a simple weigh ted sum and (ii) it supp orts an interpretation of the individual solutions in the ensem ble as basis functions that are used to appro ximate the nal clus-tering. It furthermore allo ws us to introduce weigh ts for the individual solutions suc h that the most self-consisten t group structure in the ensem ble is extracted.

Our metho d is related to sev eral other approac hes dis-cussed in the literature: In Fred et al. [9, 10], the co-asso ciation matrix has also been emplo yed in order to de-rive similarities between di eren t clustering solutions. The single-link age algorithm is then emplo yed in order to arriv e at a nal lab eling for the data. Strehl and Gosh prop osed in [23] three di eren t approac hes to generating consensus func-tions most of them based on hyper-graph-partitioning. As in our approac h, an average co-asso ciation matrix is used in the rst of the three approac hes discussed there. The second metho d relies on re-phrasing the consensus prob-lem as a hyper-graph cutting problem. Their third metho d directly addresses the cluster corresp ondence problem by \clustering clusters" so that similar clusters can be merged. Direct re-lab eling approac hes to obtain a consensus parti-tion from clusterings of multiple bootstrap samples have been prop osed in [7, 8]. Leisc h [18] discussed an approac h where the cen ters of multiple k-means clustering solutions are \bagged". Topchy et al. [24] prop osed a mixture mo del in order to obtain a consensus function. The basic idea is to consider the lab els of the individual partitions as features characterizing the objects. The consensus partition is ob-tained by grouping this data set.

A di eren t issue represen ts the dictionary creation pro cess in that one needs to nd means of constructing a reasonable ensem ble that allo ws us to nd good groupings. That is, one has to ensure, that the desired result is actually an el-emen t of the set of solutions that can be obtained by mere aggregation. In the literature, the dictionary creation has been performed by using di eren t standard algorithms [16], the randomness induced by stochastic searc h [10], the use of random pro jections prior to applying a xed clustering algorithm [25] and, nally , data re-sampling (e.g. in [7]).
In this work, we emplo y sub-sampling com bined with a prediction strategy in order to obtain multiple solutions for the same data set. This choice is motiv ated by the con-cept of the stabilit y analysis for mo del selection as prop osed in [15] as well as by the theoretical analysis of Ben-Da vid in [2]. Furthermore, an empirical study , presen ted in [19], has demonstrated that the use of sub-sampling { instead of e.g. bootstrapping { leads to comparable and even su-perior results in the con text of com bining multiple parti-tionings while the computational workload is signi can tly reduced by the use of small subsets of the original data sets. Our approac h to obtaining a (soft) assignmen t is essen tially based on a non-negativ e matrix factorization (see e.g. [17]) of the aggregated co-asso ciation matrix suc h that one ob-tains estimates for class-p osteriors and class-lik eliho ods. In con trast to previous approac hes that also emplo y the aggre-gated co-asso ciation matrix, it is based on a solid mo del for the consensus solution. Optimizing the mo del aims at min-imizing the Kullbac k-Leibler div ergence between the join t distribution of objects { as deriv ed from the co-asso ciation matrix { and the factorial appro ximation and, thus, repre-sen ts a clear cut objectiv e function. Furthermore, the mo del is related to probabilistic laten t seman tic analysis (PLSA) as introduced by Hofmann in [12].

We emplo y the Exp ectation-Maximization algorithm [5] in order to optimize the log-lik eliho od of the mo del. We further extend the mo del so that individual solutions can be weigh ted according to their imp ortance. This issue has not been addressed before. Additionally , we introduce an out-of-sample extension strategy that allo ws us (i) to further reduce the computational and memory requiremen ts of the pro cedure and (ii) to nd an assignmen t of previously unseen data points to clusters from the solution ensem ble, thereb y, allo wing the ensem ble to generalize to new data points.
The pap er is organized as follo ws: In section 2, we discuss clustering algorithms and the use of sub-sampling com bined with a prediction strategy in order to obtain a lab eling on the full data set. This strategy is borro wed from ideas in [15] and [2] and is emplo yed in order to generate the solution en-sem ble in the exp erimen ts. However, other approac hes to generating suc h ensem bles can also be used in conjunction with our prop osal. In section 3, we prop ose a probabilistic mo del for obtaining a consensus partition as a weigh ted com-bination of individual solutions. The exp erimen tal results in section 4 demonstrate the at least comp etitiv e performance of the prop osed com bination strategy on arti cial as well as real world data sets.
In the follo wing, we assume that we have given a data set X 2 n d for the sak e of simplicit y, where n is used to denote the num ber of data points and d the dimension of the data. We would like to stress, however, that the approac h presen ted here can also be applied to problems where other types of data, e.g. histogram data, as in text classi cation, or just general (dis-) similarit y data are emplo yed.
By dra wing m samples without replac ement , where m n , from the original data set, one obtains two sub-samples of that set, one with m and another with n m elemen ts. An initial clustering is then performed on the sub-sample of size m . We assume now that the mo del underlying the clus-tering principle emplo yed is able to generalize to new data points, i.e. one implicitly learns function : d ! [0 ; 1] where k denotes the num ber of clusters, that partitions the whole space. The k -means mo del, for example, has a natu-ral generalization in terms of the nearest cen troid predictor (see e.g. [11]): given the learned cluster cen troids 1 ; : : : whic h induce a Voronoi tessellation of d , a lab el for a new data point x can be predicte d by and = 0 otherwise. This routine is easily generalized to problems where the squared Euclidean distance k x k 2 is replaced by a distance measure d ( x ; ), e.g. the Kullbac k-Leibler div ergence for documen t classi cation. Note, that this approac h is also applicable to pairwise dissimilarit y data thanks to the equiv alence between k -means and Pairwise Data Clustering sho wn in [21]. Recen t work by Dhillon et al. ([6]) has sho wn, furthermore, that man y cut-based criteria suc h as the Normalize d Cut [22] can be re-cast as weigh ted (kernel) k-means problems; thereb y, the application of the same prediction routine becomes possible. Similarly , in a mixture mo del, one can predict class posteriors for new, pre-viously unseen data points. For Single Link age clustering, the natural generalization rule consists of the nearest neigh-bor predictor. In general, we can use a least cost incr ease strategy in all approac hes relying on cost criterion: assign a novel point to a cluster, suc h that the cost di erence between original clustering on the sub-sample and the clustering of the new data point join t with the original sample becomes minimal. In all other cases, where one cannot easily devise a classi er that ts the clustering mo del, we can resort to a K -nearest neigh bor classi er (with K appropriately chosen) whose risk is kno wn to con verge to the Bayes risk almost surely (c.f. [20]).

By means of suc h generalization mec hanisms, a solution deriv ed from a subsample can be extended to the full data set X without the need to actually perform clustering on the full data set. The strategy of sub-sampling and prediction is also worth-while, because it yields, for example, a constan t time appro ximation to the k -median clustering problem, if m is assumed to be a constan t. This imp ortan t result was sho wn by Ben-Da vid in [2] and further motiv ates the use of suc h a step. In the follo wing, we enco de a clustering solution Y in terms of a n k matrix, where ( i; ) th entry y i 2 [0 ; 1] quan ti es the degree of mem bership of object i to cluster , and y i = 1, 1 i n , 1 k . For hard clustering solutions, we can strengthen the requiremen t y i 2 [0 ; 1] to y i 2f 0 ; 1 g .
 If one rep eats the sub-sampling and prediction pro cess for L times, one, therefore, obtains L (soft) partitions Y l of the original data set. The use of a small subset of X in or-der to deriv e a clustering and a predictor allo ws on the one hand, the computationally ecien t (and also highly paral-lel) generation of solutions. On the other hand it comes at the exp ense of high variabilit y in an ensem ble of solutions. This is not surprising since the estimated parameters of the clustering and the predictor are likely to su er from small-sample-size e ects. Figure 2(c) in the exp erimen tal section gives an impression of the variabilit y of the solutions ob-tained on the subsamples (generated with Single Link age and the NN classi er). In a recen t work [26], it has been established that aggregating clustering solutions will lead to an impro ved nal solution if the base solutions in the en-sem ble have been generated using random lab el ipping in the desired partition. This serv es as a strong motiv ation for considering lab el aggregation strategies. In the follo wing, we switc h to a di eren t represen tation: Consider Z = YY t whic h is a n n matrix, that is also kno wn as co-asso ciation matrix. Here, the entry indicates how strongly the data x i and x j belong to the same cluster. In the hard clustering case, we have z ij = 1 i ob-jects i and j have been assigned to the same cluster. In the case of hard assignmen ts, we can obtain the original parti-tion (up to a lab el perm utation) from this represen tation by iden tifying the connected comp onen ts in the graph with n nodes and adjacency matrix Z . The matrix Z is symmetric, positiv e semi-de nite and, therefore, alw ays a kernel. This holds true for both, hard and soft assignmen ts. Adopting this kernel persp ectiv e, we can view the entries in the ma-trix Z as similarities between objects i and j . Note that Z will be rather sparse whic h reduces the memory consump-tion. Considering the entries in Z as (iid) realizations of a random variable Z with exp ectation E Z and variance V[ Z ], we exp ect the aggregation of iid Z l to lead to a variance reduction (since 1 L 2 l V[ Z l ] 1 L l V[ Z l ]). However, we cannot exp ect a bias reduction without additional assump-tions about the base solutions. Thus, we exp ect the solution com bination to yield more stable, i.e. less variable solutions. This is similar to Breiman's reasoning, why Bagging clas-si ers yields impro ved estimates whenev er the underlying base classi er is highly instable.
Supp ose now, that we have given a set of clustering solu-tions as matrices Y 1 ; : : : ; Y L . Using these matrices, we can deriv e the matrix This aggr egate d matrix describ es the num ber of times we have seen objects i and j in the same cluster. Note that when re-normalized Z describ es an un-lab eled (soft) parti-tion of the data set. Here, we try to turn the Z into a lab eled partition again. In order to arriv e there, we seek a factoriza-tion of the join t probabilit y p ( i; j ) of seeing two objects in the same class into a \marginal probabilit y" p ( i ), a \lik eli-hood" term p ( j j ) and a \posterior" term p ( j i ) (i.e. we are looking for a non-negativ e matrix factorization). The mo del that we adopt here is similar to the one emplo yed in Prob-abilistic Laten t Seman tic Analysis (PLSA) as discussed in [12]. We call it Probabilistic Label Aggregation (PLA) . Be-low, this mo del will be further mo di ed. As in PLSA, we assume indep endence conditioned on the class variable, i.e. Note that this indep endence assumption is reasonable as the input data ideally decouples as well given the cluster mem bership variable. Thus, the log-lik eliho od of our obser-vations { summarized in Z -becomes
L = log p ( Z j ) = where the parameters consists of ( p ( i )) i , the ( p ( j i )) and the ( p ( j j )) j; . Note that maximizing this log-lik eliho od is equiv alen t to minimizing the Kullbac k-Leibler div ergence (see e.g. [4]) between the re-normalized observ ations in Z (considered as join t distribution p ( i; j ) = z ij = a;b the factorial form p ( j i ) p ( j j ) p ( i ). To mak e the rela-tionship to non-negativ e matrix factorization more explicit, we note that we can summarize the p ( j i ) in a k n matrix H and the p ( j j ) in a n k matrix W . Hence, we essen-tially seek a non-negativ e matrix factorization of the matrix with entries p ( j j i ) = z ij = b z ib into the pro duct WH . For the numerical minimization, we apply the Exp ectation Maximization (EM) [5] strategy as alternating minimization for the local optimization of this mo del. Note that a sym-metric factorization (i.e. a decomp osition in H t H ) can be found follo wing a similar line of though t.
At rst, we note that the maxim um likeliho od estimate of p ( i ) is whic h is indep enden t of the estimates of the other param-eters and, therefore, has to be computed only once. Thus, we can concen trate on the more interesting term
The p ( i ) can be determined indep enden tly of the factoriza-tion. At rst, we obtain via Bayes' rule for the E-step
Hence, the exp ectation w.r.t. the assignmen t variable be-comes Furthermore, the constrain ts have to be obeyed in order to obtain a feasible solution for the maximization problem in the M-step , i.e.

E[ L ] + Taking partial deriv ativ es w.r.t p ( j j ) and p ( j i ) yields the estimates and Eac h EM iteration requires O ( n 2 ) steps. In practice, we have observ ed con vergence after 20-50 EM iterations.
The aggregation used in equation (3) assigns equal im-portance to all solutions. The set of solutions, however, migh t con tain spurious partitions that disturb the pro cess of obtaining the desired consensus solution. Therefore, we introduce additional weigh ts l , 1 l L with l l = 1. For xed l , the aggregated solution becomes the convex com bination Hence, the maxim um likeliho od estimate of p ( i; j ) is ap-paren tly a mixtur e of individual probabilities p l ( i; j ), i.e. a mixture of di eren t explanations.
 Follo wing the reasoning above, the log-lik eliho od becomes Our goal is to maximize this criterion: it will put most em-phasis on the solutions that agree with the curren t t, since this will lead to the estimates with the largest log-lik eliho od. Hence, maximizing this criterion aims at extracting the most self-consisten t (i.e. best factorisable) group structure presen t in the solution ensem ble. At rst, however, assume that the coecien ts l are xed. Then, the EM update equations be-come (by taking the deriv ativ e again w.r.t. p ( j j ) and p ( j i )) and We note that the likeliho od p ( j j ) and the posterior p ( j i ) become weigh ted averages according to the imp ortance of the l -th hypothesis enco ded in the l -th co-asso ciation matrix ( z
Again, direct minimization of the functional in equation (15) is infeasible. We, therefore, resort to a two-step pro-cedure: Assume, that we have an initial guess for the l , e.g. l = 1 L for all 1 l L . Then, we can apply EM with the update rules given above in order to arriv e at es-timates for the \marginals" p ( i ), the \lik eliho ods" p ( j j ) and the \posteriors" p ( j i ). Supp ose now the parameters p ( i ), p ( j i ) and p ( j j ) have been estimated. Then, for eac h 1 l L , we de ne whic h represen ts a constan t. We summarize the c l in the vector c = ( c l ) l . Maximizing the expression in equation (15) (assuming xed parameter estimates from the EM it-erations) sub ject to the constrain ts l l = 1 and l 0, therefore, becomes a linear program, namely where 1 L is the L -dimensional vector of ones and denotes the point-wise -relation. The LP solution is, however, likely to be very sparse as the optimal solution for the linear program lies on the corners of the simplex in the positiv e or-than t spanned by the constrain ts l l = 1 and l 0. In fact, only one or two solutions from the ensem ble are usually selected by suc h a weigh ted PLA, and it has, thus, the ten-dency to under-t (as in overly regularized). We, therefore, use an (maxim um) entrop y regularization ([14]) to con trol the sparsit y of the coecien ts l leading to the optimization problem where H denotes the (discrete) entrop y and 2 + is a positiv e Lagrange parameter. By adding the entrop y con-strain t, we mo dify the objectiv e stated in equation (15). The minimization with resp ect to the coecien ts ( l ) has an analytical solution (note that the problem is con vex): For ! 1 one gets uniformly weigh ted base hypotheses while for ! 0, the estimates becomes the sparser the more the individual c l di er. Selection of the parameter will be discussed in the next sub-section.
 With the newly estimated coecien ts l , we can re-run EM in order to get new parameter estimates. Iterating this pro cedure will yield a locally optimal solution to the prob-lem of maximizing the expression in equation (15) (sub ject to the entrop y constrain t), since (i) EM is kno wn to yield Figure 1: Demonstration of re-w eigh ting for the News-Di er ent-3 data set (c.f. section 4) with 50 subsamples. local maxim um of the log-lik eliho od function and (ii) solv-ing the outer optimization problem can only incr ease the constrained likeliho od function (within the bounds of the feasible region). We call the mo di ed mo del weighte d PLA (wPLA) in the follo wing. Figure 1 sho ws the resulting l the News-Di er ent-20 data set, where wPLA is the best per-forming metho d (see the exp erimen tal section for details). The example underscores that re-w eigh ting can be bene -cial.
A signi can t adv antage of our prop osal is that we can rea-sonably de ne an out-of-sample extension for previously un-seen data after the initial mo del tting has been performed. The idea emplo yed here is similar to the one used in [21].
In order to perform the out-of-sample extension, we need access to estimates of p ( i j x ) and p ( x j i ) for a new datum x . For two objects in the original data, we have the estimate We can extend this mec hanism by virtually adding an addi-tional row and column to eac h matrix Z l = Y l Y t l . This is trivial if x represen ts an object that has been left out only for the pro cess of lab el aggregation. If x has not been con-sidered in adv ance, one can resort to the abilit y to generalize an individual clustering solution to new data points: In sec-tion 2, we have assumed that we have access to a predictor : d ! [0 ; 1] k ( l ( x ) is a column vector) for eac h of the L base hypotheses. Hence, we can estimate z ( l ) i; where y ( l ) i denotes the l -th row of the matrix Y l . Hence, one obtains Similarly , one arriv es at an expression for p ( x j i ). Note, that the p ( i j x ) and p ( x j i ) are probabilities w.r.t. the data set used for the original (w)PLA mo del tting.

Our goal is to \predict" the posterior p ( j x ) based on the given data and the previously tted mo del. Now, we appro ximate Hence, the likeliho od of the new datum x is expanded in or interp olated using the \basis" of learned likeliho ods p ( i j ) eac h con tributing with weigh t p ( x j i ). We emplo y this ap-pro ximation and obtain by applying Bayes rule Hence, we interp olate p ( j x ) by the p ( j i ) according to the similarit y { enco ded in p ( i j x ) { of i and x .
We would like to emphasize that the out-of-sample exten-sion can be emplo yed in order to speed up the aggregation pro cess: Instead of using all n objects for the initial tting of mo del parameters, we only tak e r n man y into accoun t and apply the out-of-sample extension to the n r remain-ing objects afterw ards. Follo wing this pro cedure, computing the out-of-sample extension involves matrix multiplications whic h can be carried out in O ( k r ( n r ))) steps. The amoun t of necessary computation as well as the memory re-quiremen ts are, therefore, signi can tly reduced. The use of sparse matrices can further reduce the amoun t of memory required.

An additional bene t of the out-sample-extension is that we can use it to determine the Lagrange parameter by means of a cross-v alidation strategy: For a hold-out set, we can consider the corresp onding average negativ e constr aine d test log-lik eliho od (since we have altered the original objec-tive function by adding the entrop y constrain t) whic h can be computed using the estimated p ( i j x ), p ( j x ) and p ( x j ) ( p ( x ) can be estimated by i p ( i ) p ( x j i )). For mo del selection, we can, hence, perform cross-v alidation in order to select by choosing the that minimizes this pe-nalized average negativ e test likeliho od.
In this section, we shed ligh t on the performance of our prop osal for (weigh ted) probabilistic lab el aggregation by emplo ying arti cial as well as real world data sets. For the purp ose of generating multiple clustering solutions, we have emplo yed the standard k -means algorithm together with the nearest-cen troid predictor as well as Single Link age cluster-ing (see e.g. [13] for an accoun t on both metho ds) together with the nearest neigh bor predictor as classi cation routine. In order to quan titativ ely assess the performance of our ap-proac h, we have made comparisons with the ground-truth solutions. These comparisons have been performed by using the permute d empiric al risk w.r.t. the 0-1 classi cation loss as discussed in [15]: For two clustering solutions Y and Y 0 , we de ne their disagreemen t as where k denotes the set of all perm utation on sets of size k . The measure quan ti es the 0-1 loss after the lab els have been perm uted so that the two partitionings are in the best pos-sible agreemen t. Perfect agreemen t up to a perm utation of the lab els, therefore, implies d ( Y ; Y 0 ) = 0. Note that iden-tifying the optimal perm utation is feasible, since it can be determined in O ( k 3 ) by phrasing the problem as a weigh ted bipartite matc hing problem (see e.g. [15] for details).
We compare our metho d with four di eren t approac hes where three of them are discussed in [23]: the Cluster-based Similarit y Partitioning Algorithm (CSP A), the Hyp erGraph Partitioning Algorithm (HGP A) and the Meta-CLustering Algorithm (MCLA). The CSP A algorithm is also based on the similarities induced by the co-asso ciation matrix. The HGP A metho d emplo ys a constrained minim um cut objec-tive function in order to appro ximate a consensus solution maximizing the (normalized) mutual information between the distinct lab ellings. The third metho d, MCLA, poses the consensus problem as a \cluster corresp ondence prob-lem. Essen tially , groups of clusters (meta-clusters) have to be iden ti ed and consolidated." as it is stated in [23]. The fourth metho d under consideration has been recen tly pro-posed by Topchy et al. [26] (abbreviated here by T-EM ): it implemen ts a mixture mo del for clustering whic h essen-tially uses the lab els from the di eren t clustering solutions as features.

In all exp erimen ts, we have randomly dra wn L 2f 10 ; 20 ; 50 ; 100 ; 150 g di eren t sub-samples, whic h are used in a sub-sequen t step to generate di eren t partitions of the (full) data set. For the weigh ted PLA varian t, we have varied 2 f 0 : 1 ; 0 : 5 ; 1 ; 10 ; 20 ; 50 ; 100 ; 500 g and selected by cross-validation for the resp ectiv e problems. In the tables, we have also pro vided the smallest error rates seen for the optimal (rep orted in paren theses).

Exp erimen ts using Toy Data: We have used the data set depicted in gure 2(a) consisting of two banana-shap ed clusters with 100 points in the rst and 200 points in the sec-ond cluster. In order to generate an ensem ble of solutions, we have emplo yed Single Link age Clustering (with k = 2) applied to sub-samples of size n = d n 10 e and the nearest-neigh bor classi er for generalizing the solutions to the com-plete data set. At rst we would like to stress that the Single Link age solution separates one data point from the rest of the data set, i.e. creates a singleton, when applied to the full data set. This is a result of the high noise sensitivit y of the Single Link age clustering algorithm. The example, therefore, represen ts a hard case. Figure 2(c) sho ws the dis-tribution of dissimilarities between ground-truth and the so-lutions generalized from the sub-samples in box-plots. The upp er and lower end of the box in this plot mark the up-per/lo wer quartile of the resp ectiv e dissimilarities. We note the median of the dissimilarities is around 0 : 25. Figure 2(d) summarizes the results for the toy data for di eren t num-bers of re-samples and for the metho ds under consideration (consult table 1 for the num bers): Our prop osals PLA and wPLA rather clearly outp erform three of the four comp eting metho ds w.r.t. the similarit y to the ground-truth solution. Both, HGP A and CSP A, pro duce consensus solutions whic h are close to the median dissimilarit y of the individual solu-tions. It is, however, notew orth y, that the aggregation of almost every metho d leads to a consensus solution whic h is at least sligh tly better than the solution pro duced on an average sub-sample. Only MCLA performs similarly to our prop osal whic h has an error rate between 0 : 13 and 0 : 16. We note here, that the solutions generated by PLA, MCLA and Table 1: Disagreemen t measure (rounded) of the toy data set. MED denotes the median di erence to the ground-truth in the ensem ble.
 Table 2: Disagreemen t measure (rounded) of the News-Similar-3 data set: PLA/wPLA demonstrate comp etitiv e performance. wPLA almost alw ays have a similarit y to the ground truth that is sup erior to at least 3 = 4 of the solutions in the ensem-ble. The poor performance of T-EM for L 50 re-samples can be attributed to the curse of dimensionalit y (since the metho d uses 300 data points in 150 dimensions). For small num ber of re-samples it is comp etitiv e. Except for L = 50, wPLA also leads to comparable or impro ved results in com-parison with PLA.

Exp erimen ts using Newsgroup Data Sets: Our sec-ond exp erimen t is about the categorization of newsgroup data sets whic h we have deriv ed from the data set used in [1] 2 . We also follo w the nomenclature of that work. We have used two datasets: News-Similar-3 con tains roughly 300 documen ts from three similar topics. Due to this, the clusters are not well-separated (see [1] for details) in this data set. News-Di er ent-3 consists of three clusters corre-sponding to highly di eren t topics. One can, thus, exp ect the groups to be well-separated. Laten t seman tic indexing is used to transform the term frequency and inverse docu-men t frequency normalized documen t vector to a 20D fea-ture vector. We have used 3-means clustering and the near-est cen troid predictor in order to generate solutions. The sub-sample size m has been set to d n= 5 e .

Tables 2 and 3 as well as the gures 3(a) and 3(b) sum-marize the results on these two data sets. On the News-Similar-3 data set, one can conclude from the dissimilar-ity measuremen ts, that this represen ts a dicult data set. We observ e here that PLA and wPLA pro duce, except for L = 10, where HGP A is better, sup erior results. For L 50 HGP A becomes, however, the worst routine in the compar-ison. Furthermore, T-EM and CSP A, both do not generate very good solutions. MCLA is again the strongest com-petitor. On the News-Di er ent-3 data, PLA and wPLA are again among the best performing. It is notew orth y the MCLA results on this data set are among the worst { in con trast to the previous exp erimen ts. see http://www.cs.utexas.edu/users /ml/r isc/ . signi can tly better results in comparison to an average solution. Table 3: Disagreemen t measure (rounded) of the News-Di er ent-3 data set. wPLA sho ws the best agreemen t to the ground-truth with L = 150 sub-samples.

Application to Image Segmen tation: We now em-ploy a Mondrian image (depicted in Figure 4(a)) that con-sists of ve di eren t segmen ts: three di eren t texture and two rather noisy gra y segmen ts. This 512 by 512 image is, at rst, divided into a 51-b y-51 grid, so that 24-dimensional feature vector can be extracted for eac h site suc h that 12 features stem from a 12-bin histogram of gra y-lev el values and the remaining 12 features represen t the average of Ga-bor lter resp onses for four orien tations at three di eren t scales at the resp ectiv e site. The grouping problem, there-fore, con tains more than 2500 objects. The segmen t lab els of di eren t sites are obtained from the a-priori kno wn ground-truth image. In the data set, the texture features are much more dominan t than the gra y value information, and, hence, clustering this data set does not reco ver the ground-truth information. We have excluded CSP A from this nal exp er-imen t whic h is the default option for a data set of that size in the source code pro vided by Strehl and Gosh (since its run-ning time and memory consumption becomes prohibitiv e). We have again used k -means clustering (with k = 5) and the nearest cen troid predictor. The sub-sample size was set to m = d n= 5 e . Table 4 as well as gure 5 summarize the results of this exp erimen t. At rst, we note, that HGP A does not pro vide a reasonable lab eling of the data. Due to the size Table 4: Disagreemen t measure (rounded) of the Mondrian data set. PLA, MCLA and T-EM sho w similar beha viour. of the data set, we have used the out-of-sample extension for PLA to predict the lab els of the ma jorit y of data points. Still, for L 100, it is the best aggregation metho d un-der comparison. However, MCLA and T-EM demonstrate similar performance in comparison with our approac h. Fig-ure 4(b) sho ws the resulting segmen tation for L = 100 sub-samples (note, that this is a predicted solution).
In this work, we have prop osed a novel approac h to com-bining multiple partitions into a single solution. To this end, a probabilistic mo del for the aggregation of partitionings has been introduced that has been additionally extended in or-der to weigh solutions according to their compatibilit y with the curren t consensus solution. An entrop y-regularization is emplo yed in order to avoid under-tting problems, where we have investigated the use of cross-v alidation for param-eter selection. In the exp erimen tal results section, we have demonstrated the performance of our prop osals on four data sets. The comparison with the metho ds discussed in [23] and the mixture mo del approac h prop osed by Topchy et al. in [24] has demonstrated the at least comp etitiv e, and often su-perior performance of our prop osal, the probabilistic lab el aggregation, on four di eren t data sets.

Future work will focus on directly con trolling the sparsit y of the posterior estimate by imp osing additional constrain ts on the set of feasible solutions. Furthermore, the proba-Figure 4: Segmen tation results with L = 100 sub-samples 4(b) for the image sho wn in gure 4(a). Figure 5: Results of the comparison on the Mon-drian image in gure 4(a). bilistic framew ork emplo yed in this work renders the de-sign of reasonable re-w eigh ting schemes for individual data points possible, e.g. in order to obtain more data adaptiv e re-sampling approac hes (similar to boosting). Adopting a learning-theoretic persp ectiv e, the bene ts of using aggre-gation schemes for data clustering are still not well under-stood. Future work will additionally put particular emphasis on this imp ortan t asp ect.
The authors would like to thank M. H. Law and P. Or-banz for fruitful discussions and A. Strehl and A. Topchy for pro viding the source code of their metho ds. [1] S. Basu, M. Bilenk o, and R. J. Mo oney . A [2] S. Ben-Da vid. A framew ork for statistical clustering [3] L. Breiman. Bagging predictors. Machine Learning , [4] T. M. Cover and J. A. Thomas. Elements of [5] A. P. Dempster, N. M. Laird, and D. B. Rubin.
 [6] I. Dhillon, Y. Guan, and B. Kulis. A uni ed view of [7] S. Dudoit and J. Fridly and. Bagging to impro ve the [8] B. Fisc her and J. M. Buhmann. Bagging for [9] A. Fred. Finding consisten t clusters in data partitions. [10] A. Fred and A. K. Jain. Data clustering using [11] T. Hastie, R. Tibshirani, and J. Friedman. The [12] T. Hofmann. Unsup ervised learning by probabilistic [13] A. K. Jain and R. C. Dub es. Algorithms for Clustering [14] E. T. Jaynes. Information theory and statistical [15] T. Lange, M. Braun, V. Roth, and J. Buhmann.
 [16] M. H. C. Law, A. P. Topchy, and A. K. Jain.
 [17] D. D. Lee and H. S. Seung. Algorithms for [18] F. Leisc h. Bagged clustering. Technical rep ort, TU [19] B. Minaei-Bidgoli, A. P. Topchy, and W. F. Punc h. A [20] B. D. Ripley . Pattern Recognition and Neur al [21] V. Roth, J. Laub, M. Kawanab e, and J. M. Buhmann. [22] J. Shi and J. Malik. Normalized cuts and image [23] A. Strehl and J. Ghosh. Cluster ensem bles { a [24] A. Topchy, A. Jain, and W. Punc h. A mixture mo del [25] A. Topchy, A. K. Jain, and W. Punc h. Com bining [26] A. P. Topchy, M. H. C. Law, A. K. Jain, and A. L. N.
