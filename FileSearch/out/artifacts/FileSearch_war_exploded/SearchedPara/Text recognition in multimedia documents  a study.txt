 ORIGINAL PAPER Khaoula Elagouni  X  Christophe Garcia  X  Franck Mamalet  X  Pascale S X billot Abstract Text embedded in multimedia documents represents an important semantic information that helps to automatically access the content. This paper proposes two neural-based optical character recognition (OCR) systems that handle the text recognition problem in different ways. The first approach segments a text image into individual char-acters before recognizing them, while the second one avoids the segmentation step by integrating a multi-scale scanning scheme that allows to jointly localize and recognize charac-ters at each position and scale. Some linguistic knowledge is also incorporated into the proposed schemes to remove errors due to recognition confusions. Both OCR systems are applied to caption texts embedded in videos and in nat-ural scene images and provide outstanding results showing that the proposed approaches outperform the state-of-the-art methods.
 Keywords OCR  X  Character segmentation  X  Convolutional neural network  X  Language model 1 Introduction Textual patterns embedded or captured in images, and videos provide high-level semantic clues often interesting for appli-cations and services such as multimedia document indexing and retrieval, teaching videos, and robotic vision systems. In these contexts, the design of efficient optical character recognition (OCR) systems specifically adapted to multi-media documents is an important issue. However, the huge diversity of texts, especially in natural scene images, and the difficult acquisition conditions (low resolution, complex background, non uniform lighting, occlusions, and blurring effects) make the task of text recognition a challenging prob-lem that has raised a growing interest in recent research activ-ities [ 3 , 11 , 34 , 42 ].

In this paper, we propose to study and compare two dif-ferent approaches for text recognition in images. The first one [ 11 ] consists in segmenting the text image into indi-vidual characters before recognizing them. In contrast with existing methods, this OCR system performs a nonlinear character segmentation taking into account the local mor-phology of text images, in order to improve character recog-nition performance. The second OCR system [ 10 ] avoids explicitly segmenting characters and addresses the problem in a way different from prior approaches using a multi-scale scanning process that allows to recognize characters at their appropriate scale and position within the whole text image. In both methods, in order to tackle the high variabil-ity of the input images, we propose to rely on ConvNets (Convolutional Neural Networks [ 19 ]) that are particularly a robust pattern recognition technique. Another contribu-tion of our work consists in introducing, for both meth-ods, a supervision scheme based on language models in order to remove some errors due to recognition confu-sions.
In the community of text recognition, texts in multime-dia documents have been classified into two categories [ 16 ]:  X  X aption texts X  (cf. Fig. 1 c), which are artificially overlaid on images or videos and  X  X cene texts X  (cf. Fig. 1 a, b), which exist naturally in images or videos. Here, in order to eval-uate and compare the proposed OCR methods, which is the focus of this paper, new experiments are carried out on two difficult datasets: one containing  X  X aption texts X  embedded in digital videos and the other containing natural  X  X cene text X  images. Performed experiments show that both designed OCRs obtain outstanding results and outperform the other state-of-the-art methods. The results are analyzed and discussed, highlighting the benefits and limits of both approaches.

The remainder of this paper is organized as follows. After a review of state-of-the-art methods dedicated to text recog-nition techniques in multimedia documents (Sect. 2 ), the two proposed OCRs are detailed in Sects. 3 and 4 . The integration of a language model is described in Sect. 5 . The evaluation of the proposed methods as well as comparisons with state-of-the-art approaches is presented and discussed in Sect. 6 . Finally, conclusions are drawn in Sect. 7 . 2 Related work Since years, OCR systems have been an important appli-cation of pattern recognition and computer vision. In these research domains, prior works have mainly focused on sys-tems operating on scanned documents and on handwritten texts. Recently, a considerable progress has been made in the specific field of text recognition in images and videos. A review of the new advances achieved in text recogni-tion in multimedia documents is presented in [ 36 ]. Different issues related to this recognition problem have been identi-fied, including text detection [ 6 , 21 , 23 , 46 ], text enhancement and binarization [ 3 , 15 , 24 , 47 , 48 ] (a pre-processing step that aims to improve recognition performance), character seg-the integration of linguistic knowledge [ 11 , 45 , 49 ].
In this paper, we do not focus on the detection of text, but on the steps involved in the text recognition task, i.e., text image pre-processing, character segmentation, charac-ter recognition, and text recognition. Related works are pre-sented in this section. 2.1 Text image pre-processing Most OCR methods rely on pre-processing treatments and specifically on binarization in order to ease the recognition step. Sa X dane et al. [ 33 ] introduced an automatic binarization step based on a ConvNet particularly robust to complex back-ground and low resolution. The main idea is to automatically learn the parameters of the ConvNet to transform a text image into a binarized version, using a large training set. Mishra et al. [ 28 ] presented a Markov random field (MRF)-based technique of binarization adapted to scene text images. Li et al. also proposed a method dedicated to scene text images where local visual information and contextual label infor-mation are integrated in a conditional random field (CRF) [ 22 ]. Another binarization approach using text contours and a local thresholding method was proposed by Zhou et al. [ 50 ]. Recently, Wakahara et al. [ 41 ] defined a binarization method that relies on a K-means clustering and a support vector machine (SVM) model to binarize color scene text images with complex background, while Ntirogiannis [ 30 ] used the stroke width and convex hull analysis to binarize texts embedded in videos. For video data, Hua et al. [ 15 ] and Yi et al. [ 47 ] were interested in solving problems related to complex backgrounds by using multiple frame integration. The main idea consisted in taking advantage of the tempo-ral redundancy of a text appearing in successive frames of a video. 2.2 Character segmentation Among state-of-the-art methods, two major approaches can bedistinguished: segmentation-based approacheswhichseg-ment the text into individual characters before the recogni-tion step, and segmentation-free approaches which recognize a succession of characters directly from the whole text image without any segmentation.

Casey et al. provide a complete survey of character seg-mentation methods in [ 2 ]. In [ 27 ] and [ 35 ], segmentation methods that rely on a classical projection profile technique are presented and applied to caption texts extracted from digital videos. Shivakumara et al. propose a gradient-based character segmentation scheme [ 37 ], while Phan et al. use a gradient vector flow-based method to segment characters [ 31 ]. To improve performance, other authors [ 26 , 34 ] propose hybridapproachesthatcombineimageprocessingtechniques and recognition results. The key point is to build concurrent segmentations and to rely on a character recognizer to iden-tify the correct ones.

In contrast to these methods, other authors propose OCR systems that do not rely on conventional segmentation tech-niques. Kusachi et al. [ 18 ] use a coarse-to-fine scanning technique and classify clipped regions to recognize charac-ters. Wang et al. [ 42 ] present a word recognition approach that relies on a generic object recognition method, in which words are considered as object categories. Words are con-sidered as sequences of characters identified and localized in text images based on some extracted features. However, the major issue of these approaches is the difficulty to choose the discriminant features to represent extremely variable charac-ters. 2.3 Character recognition Other works have focused on the problem of single character recognition in images and videos. Among these single char-acter recognizers, two main approaches can be distinguished as follows: pattern matching methods and machine learning methods.

In the first category, characters are usually identified by a set of features. First, a database of models of features is generated. Then, for eachimagecorrespondingtoacharacter, features are extracted and matched against the database in order to recognize the character class. In [ 17 ], edges and contours are considered as features characterizing characters, while in [ 4 ], for each binarized image character, four side profiles are extracted and matched to recognize characters. Side profiles are obtained by counting the white pixels in each direction (left, right, up, and down) until encountering black pixels. Halima et al. also used a projection profile technique to recognize Arabic character images extracted from digital videos [ 13 ]. Negishi et al. proposed instead to use corners and curves that are matched relying on a voting algorithm [ 29 ]. Recently, inspired by speech recognition, Som et al. [ 39 ] designed an OCR system that uses a hidden Markov model (HMM) to identify characters as a sequence of states. However, as in any pattern recognition problem, the major issue is to define the robust features that represent characters independently of the image resolution and the background complexity. Therefore, performance of these methods may be very variable depending on the chosen features and the image conditions.

Inthesecondcategory,methodsaredesignedtolearnauto-matically how to classify characters either directly from their images or after extracting features. In [ 32 ], Sa X dane and Gar-cia have presented an automatic method for scene charac-ter recognition based on a convolutional neural classification approach. The system is able to deal directly with the raw pixels of extremely variable characters and appears to be par-ticularly robust to different image distortions. Another work presented in [ 8 ] relies on an SVM classifier which learns how to recognize characters from image pixels and which also obtains good results. Aiming at recognizing Kanji characters captured in natural scene images, a voting method was cho-sen by Kusachi et al. [ 18 ] to identify characters with recog-nition dictionaries obtained by patterns learning. Recently, a method based on unsupervised features learning was pro-posed to detect and recognize characters in natural scene images [ 5 ]. 2.4 Text recognition Methods that rely on a character segmentation-based recog-nition often use a graph able to handle different concurrent segmentations [ 34 ]. Free-segmentation approaches propose a peak detection system to recognize texts [ 18 ]. The idea is to analyze extracted features (segments of character can-didates) in order to detect peak points present in these fea-tures and eliminate the others. Texts are thus recognized as the sequences of characters identified by means of the peak detection. Further information, such as language properties and lexicons, can also be integrated to improve the perfor-mance of OCR systems [ 43 ].

In this paper, we present two approaches for text recog-nition where no binarization step is required: One relies on a segmentation step well adapted to the local morphol-ogy of images, while the other uses a multi-scale scanning scheme which avoids character segmentation. We also pro-pose a robust character recognizer based on a neural model able to learn how to extract relevant features and identify characters without any pre-processing step. Some linguistic knowledge is integrated in both OCRs to avoid the drawbacks of local character-by-character recognition and improve per-formance. 3 The segmentation-based OCR This section presents our first OCR approach [ 11 ] designed to recognize texts captured in natural scene images and embed-ded in videos. Figure 2 depicts the outline of this approach whose first step consists in a character segmentation. 3.1 Character segmentation In order to find reliable separations between characters, we start by analyzing the text image to distinguish the text from the background. Assuming that pixels of a text image are of two classes,  X  X ext X  and  X  X ackground X , and that their respec-tive intensities are governed by Gaussian distributions, a Gaussian mixture is fit to the image intensity histogram. The estimation of both distributions can thus be performed by maximizing the likelihood between a set of observations X  the image intensity histogram X  X nd a Gaussian mixture model. Using an expectation X  X aximization (EM) algorithm [ 7 ], parameters of both distributions are obtained, and then used to generate a fuzzy map indicating, for every pixel, its membership degree to the class  X  X ext. X  To do so, a model is applied such as the membership value is 0 if i  X   X  1 ,1 if i  X   X  2 and varies linearly between these bounds, with i being the intensity in the fuzzy map and  X  1 and  X  2 the means of the distributions.

In the case of video data, the temporal redundancy of each text is also taken into consideration to generate another fuzzy map. Since intensity distributions and temporal variation rep-resent two independent sources of information, the two fuzzy maps are then combined to obtain a more accurate one. To do so, a fusion system, with an adaptive behavior depending on the values to combine, is required. According to [ 44 ], the chosen operator should be conjunctive (with a severe behav-ior) if both values are low, disjunctive (with an indulgent behavior) if both values are high and depends only on the intensity distribution analysis if the temporal variation is low. The operator expressed by Eq. 1 satisfies these conditions: f ( x , y ) = x if y where x refers to the intensity analysis result, y refers to the temporal variation result, and th is a threshold determined empirically.  X ( x , y ) is the associative symmetric sum, and g ( x , y ) is a positive increasing function.

In contrast with other state-of-the-art methods that search for linear segmentations to separate characters, we propose to segment characters by nonlinear borders which are well suited to different morphologies. This is done by using the obtained fuzzy map in order to enhance recognition perfor-mances. Each segmentation border is computed as the short-est vertical path containing pixels of low probabilities (i.e., membership degrees) to belong to the class  X  X ext. X  Consider-ing the fuzzy map as a grid of vertices and using the shortest path algorithm, segmentations are determined as paths con-necting pixels from the top to the bottom of the image without crossing any pixel of class  X  X ext X  (typically, pixels having a membership degree over a fixed threshold). Three directions are allowed in our algorithm: 45  X  , 90  X  , and 135  X  with respect to the horizontal axis. Three weights  X  45  X  , X  90  X  , and fixed empirically, are, respectively, assigned to each direc-tion, with  X  45  X  =  X  135  X  (cf. Fig. 3 ). Typically, assuming that each pixel in the fuzzy map is identified by its coordinates ( x , y ) and characterized by its intensity I, the shortest path Path ( S ) which starts from one pixel S of the first line of the image is computed with the following formula: Path ( S ) = A i , A i + 1 = argmax where A 0 = S is the first pixel in the path, B is a pixel of the image that satisfies the two conditions x B  X  x A i &lt; 2 and y to  X 
The resulting segmentation borders are characterized by the value of their highest pixel probability. In the rest of the paper, this value is called the score of the path. Two cat-egories of segmentation borders are distinguished depend-ing on their cost:  X  X ccurate X  ones with low costs (under a threshold set empirically) and  X  X isky X  ones with higher costs.  X  X ccurate X  paths are considered as corresponding to correct separations between two characters while  X  X isky X  ones will be questioned later relying on further information: character recognition results (cf. Sect. 3.3 ) and linguistic knowledge (cf. Sect. 5 ). Figure 4 illustrates an example of the obtained nonlinear segmentations.

According to the survey of character segmentation tech-niques presented by Casey et al. [ 2 ], our method can be considered as a hybrid method which takes advantages of both of  X  X issection X  and  X  X ecognition-based X  techniques. Indeed,  X  X ccurate X  segmentations are obtained by an intel-ligent process including an analysis of the image and without any symbol classification. Thus, they can be consid-ered as deriving from  X  X issection X  techniques. In contrast,  X  X isky X  segmentations that will be discussed in accordance with recognition results can be considered as derived from  X  X ecognition-based X  techniques. 3.2 Character recognition Once segmented, characters have to be recognized. Among state-of-the-art approaches, the dominant methodology con-sists first in binarizing the images and then extracting visual features to recognize characters. The main drawback of this kind of methods is that binarization may fail when the background is complex, leading to poor recognition rates. Unlike these techniques, we propose to train a neural network able to learn to recognize characters directly from the input image. Convolutional Neural Networks (ConvNets) are bio-inspiredhierarchical multi-layeredneural networks proposed by LeCun et al. [ 19 ] to learn visual patterns directly from the image pixels without any pre-processing step. Relying on specific properties (namely local receptive fields, weight sharing, and sub-sampling), this neural model is particularly robust to noise, geometric transformations, and distortions and has shown a great ability to deal with a large number of extremely variable patterns. This neural model has been used in many classification tasks [ 38 ] ranging from handwritten character recognition [ 20 ] to face detection [ 12 ], and it gen-erally outperforms other classification models such as SVMs [ 11 ].

For our character recognition problem, several configura-tions were tested on our datasets. The ConvNet, hereafter CRConvNet for Character Recognizer ConvNet, takes as input a color image of a character mapped into three T  X  T maps, one map for each color channel, and containing val-ues normalized between  X  1 and 1, and returns a vector of N values (with N the number of classes of characters) where each value (between  X  1 and 1) encodes a score of belong-ing to a given class of characters. The ConvNet architecture contains four convolutional layers and two neural layers (cf. Fig. 5 ). The first two layers (a convolution followed by a sub-sampling layer) can be interpreted as a feature extractor where the sub-sampling layer permits to reduce sensitivity to affine transformations and to reduce computational com-plexity. The two next layers (a convolution followed by a sub-sampling layer) combine extracted features considering their spatial relationships. The two last layers correspond to a classical multi-layer perceptron and provide output scores that can be interpreted as the probabilities of the input image to belong to each class of character. CRConvNet is trained with the classical back-propagation algorithm with momen-tum.
 3.3 Text recognition Individualcharacterrecognitionresultscannowbecombined in order to determine the whole text present in images or in videos. Since text images are segmented into separated characters, we intuitively recognize texts as the sequences of recognized characters. In Sect. 3.1 , two categories of seg-mentation borders were distinguished as follows:  X  X ccurate X  and  X  X isky X  ones. Characters located between two  X  X ccurate X  segmentations (green separations in Fig. 6 ) are recognized and directly considered as letters of the text. However, when, between two successive  X  X ccurate X  segmentations,  X  X isky X  ones are observed (red separations in Fig. 6 ), the CRCon-vNet is applied on each possible segmentation ( X  X ccurate X  and  X  X isky X ) and the configuration that obtains the highest score is selected. Figure 6 illustrates all the candidate char-acters on which CRConvNet is applied. At this stage of the processing, for each possible segmentation, only the best response (i.e., the class obtaining the highest probability) of the CRConvNet is considered, while the rest of the responses is ignored.

As shown in Fig. 6 , even though errors related to  X  X isky segmentations X  are reduced, confusions between similar characters are still present (such as the  X  X  X  recognized as a  X  X  X ). In Sect. 5 , we show how to introduce linguistic knowl-edge able to drive the recognition scheme and to tackle these character confusions. 4 The segmentation-free OCR Our first OCR system relies on the segmentation of text imagesintoindividualcharacters.However,inthecaseoftext images with strong distortions, this step can lead to potential under-or over-segmentation. Inspired by works dedicated to handwritten recognition [ 9 , 14 ], which use a sliding win-dow technique, we propose a second OCR approach able to avoid this segmentation step by incorporating a multi-scale scanning process [ 10 ]. The different steps of this OCR are presented in Fig. 7 and described in this section. 4.1 Multi-scale image scanning The first step of our segmentation-free OCR consists in scan-ning the text image. This is done by moving a sliding window, from the left to the right, centered at regular and close posi-tions. In our experiments, best results were obtained with a moving step of one-eighth of the image height h .Wealso consider windows at various scales (i.e., windows with differ-ent widths) in order to cover different character sizes. Four scales (namely S 1 , S 2 , S 3 , and S 4 ) are used in our experi-ments, corresponding to window widths equal to h 4 , h 2 , 3 h and h . Figure 8 illustrates an example of a text image scanned at different scales and shows characters framed at their cor-responding scales (e.g.,  X  X  X  and  X  X  X  are framed with win-dows equal to h and h 2 respectively) and an example of a misaligned window. A classification is applied to every window to identify non-valid characters and recognize valid ones.

In order to improve recognition performance of charac-ters framed with sliding windows, we propose to adapt the window borders to the local morphology of the images. The purpose is to remove parts of other characters that can be extracted with the central character when using vertical bor-ders of windows (e.g., the character  X  X  X  extracted with the character  X  X  X  in Fig. 8 ). At each window position and scale, nonlinear borders are defined as shortest vertical paths within the text image as described in Sect. 3.1 . In case of important image distortions, non-separated characters, or misaligned windows, the shortest path algorithm induces straight ver-tical borders since pixels in the local area have the same probability. Figure 9 shows some examples of the obtained windows with nonlinear borders and gives an example of straight vertical borders due to the non-separation between two characters ( X  X  X  and  X  X  X  in the word  X  X TAR X  in Fig. 9 ). 4.2 Window classification Before recognizing characters, a step of pre-sorting is required to identify windows containing  X  X alid X  characters and  X  X on-valid X  ones. Hence, we propose to use a Convo-lutional Neural Network whose task is to classify windows as  X  X alid character X  or  X  X arbage X  (i.e., window misaligned with a character, part of a character, or interstice between characters).

In our application, several network architectures were tested. The best configuration, hereafter WCConvNet for Window Classifier ConvNet, takes as input a color window image mapped into three T  X  T input maps, containing values normalized between  X  1 and 1. The architecture of WCConvNet is similar to that of CRConvNet presented in Sect. 3.2 except that it has a single output neuron trained to respond  X  1 for  X  X arbage X  windows, and + 1 for  X  X alid X  char-acters. After training, windows obtaining a negative output are labeled as  X  X arbage, X  while the others are presented to the CRConvNet (see Sect. 3.2 ). Figure 10 illustrates the whole windowclassificationscheme:Intersticesbetweencharacters are labeled as  X  X arbage X  and well-framed characters (such as  X  X  X ) are recognized. Nevertheless, some parts of characters produce recognition confusions (e.g., the part of  X  X  X  on the right in Fig. 10 is recognized as an  X  X  X ). In the next subsec-tion, we show how we deal with window classification results and handle recognition errors. 4.3 Text recognition using a graph model Multi-scale window classification results can now be com-bined to recognize texts extracted from images or videos. To that end, we choose to use a directed acyclic graph model able to represent the spatial constraints between different overlap-ping windows (cf. Fig. 11 ). The borders of the windows are represented by vertices. The first (resp. last) vertex in the graph corresponds to the left (resp. right) border of the first (resp. last) window position in the text image. Vertices are connected by directed edges, called arcs, each representing oneextractedwindow.Sincethemulti-scalescanningscheme includes four different window sizes, each vertex v is thus connected to four successor vertices (i.e., the right borders of the four different windows starting from v ).

In Sect. 4.1 , we have explained how nonlinear borders of windows are computed and characterized by scores encod-ing their probabilities to correspond to borders between characters. These segmentation scores are assigned to their corresponding vertices in the graph. In the same way, the classification results of each window (i.e., the output of WCConvNet for non-valid characters and the best output of CRConvNet for the valid ones) are assigned to each arc. Figure 11 shows one part of the graph built on a sample image representing each possible window. A best path search algo-rithm, namely the classical Viterbi algorithm, is then applied to determine the best sequence of characters corresponding to the text image. All possible paths within the built graph can then be tested and evaluated taking into account the spatial constraints between sliding windows, the scores of windows borders, and the classification results. The recognized text is thus obtained as the sequence of characters corresponding to the path having the best score and avoiding arcs which represent windows that do not contain valid characters. 5 Improving recognition results by integrating linguistic information Both proposed OCR systems still generate some errors related to character confusions and incorrect segmentations or misaligned windows. To tackle these ambiguities and improve the recognition rate, we propose to incorporate into both OCRs, some information provided by the lexical con-text. The idea is to take advantage of the language properties and to introduce some linguistic knowledge in order to super-vise the recognition process.

In this context, n-gram models (widely used in speech recognition) haveshowntobewell adaptedtoour recognition problem [ 1 ]. By learning the joint probabilities of sequences of items (which can be phonemes, words, or characters), these models allow to predict the next item to be recog-nized given items that have just been recognized. For our recognition problem, since single words or short sentences are considered, a character n-gram model is trained over a corpus of words to estimate the sequences of letters proba-bilities in a given language. These probabilities are then inte-grated into both recognition frameworks to adjust transitions between characters. Hence, when evaluating word scores in the segmentation-based OCR or path scores within the graph of the segmentation-free OCR, these joint probabilities of character sequences are introduced to weight the different word propositions and paths. Furthermore for each recogni-tion result, not only the best class of character X  X amely the class obtaining the best output value X  X s considered, but also the next four best classes. Thus, new word propositions can be tested and evaluated aiming to reduce character confusion (e.g., the  X  X  X  recognized as a  X  X  X  in Fig. 6 ).

Typically, for a given text image, if we assume that  X  C is the sequence of characters present in the image,  X  C can be char-acterized as the sequence which maximizes the probability p (
C | sig ) , where C is a sequence of characters and sig is the given text image. A maximum a posteriori (MAP) approach is applied as follows:  X  C = argmax where p ( sig | C ) is the a posteriori probability of sig given the character sequence C , and p ( C ) is the a priori probability of C . p ( sig | C ) is computed from the character (or window) recognition outputs as follows: p ( sig | C ) = where c i and s i are, respectively, the i th character of C and its image (or window). p ( C ) is obtained from the language model. Using the n-gram model, we assume that a character only depends on its n  X  1 predecessors; thus: p (
C ) = where  X ( h i ) is the context of a character c i and corresponds to the sequence c i  X  n + 1 ... c i  X  1 . In our previous work [ 11 ], the best recognition results were obtained using a character tri-gram model ( n = 3). Hence, we chose to use this model in our experiments.

Because probabilities are low (between 0 and 1), their decimal logarithms (between  X  X  X  and 0) are preferred, and two coefficients  X  and  X  are introduced as follows:  X  C = argmax  X  , called the grammar scale factor, encodes the weight of the language model and serves to balance the influence of the lin-guisticknowledgeintherecognitionprocess,while  X  isincor-porated to compensate the over-and under-segmentations by controlling lengths of word candidates.

In our experiments, using the SRILM toolkit [ 40 ], two lan-guage models, one for the English language and another for the French language, were trained on two corpora of about, respectively,11,000Englishwordsand10,000Frenchwords. 6 Experimental setup and results Thissectionreportstwomainexperiments,comparesthepro-posed OCRs, and discusses their results. After a presentation of the datasets used in the experiments, the proposed OCR systems are evaluated and compared to other state-of-the-art approaches. Results are also analyzed highlighting the benefits and the limits of the character segmentation step. The contribution of the language model incorporated in both recognition schemes is also evaluated. 6.1 Text image datasets Our experiments have been carried out on two types of multi-media documents:  X  X aption texts X  in videos, and  X  X cene text X  images. 6.1.1 The  X  X aption text X  video dataset (Dataset I) This dataset consists of 12 videos of French news broadcast programs. Each video, encoded in MPEG-4 (H. 264) format at 720  X  576 resolution, is about 30min long. In this paper, we focus on text recognition; however, since the first task for video text recognition consists in detecting and extracting texts from videos, this step is performed using the text detec-tor proposed by Delakis et al. [ 6 ] as described in [ 11 ]. Each video contains about 400 words, roughly corresponding to 2,200 characters (i.e., small and capital letters, numbers, and punctuation marks). As shown in Fig. 12 , embedded texts can vary a lot in terms of size (a height of 8 X 24pixels), color, style, and background (uniform and complex backgrounds).
Four videos of this dataset are used to generate a database of 15,168 images of single character perfectly segmented and 1,001 images of non-valid characters (i.e.,  X  X arbage X ). The obtained database, called CharDataset I, is used to train WCConvNet and CRConvNet. The other eight videos, called TextDataset I, are annotated to evaluate the OCRs X  recogni-tion performance. In this dataset, 41 character classes are considered as follows: 26 Latin letters, 10 Arabic numbers, 4 special characters ( X . X ,  X   X   X ,  X ( X , and  X ) X ), and a class for spaces between words. 6.1.2 The  X  X cene text X  dataset (Dataset II) This dataset is the public database ICDAR 2003 1 created for a competition on scene word recognition [ 25 ]. It con-tains English scene text images of different sizes (a height of 12 X 504pixels), presents several kinds of distortions (non uni-form illumination, occlusions, blur, etc.), and contains char-acters printed, written, and painted in various fonts and colors (cf. Fig. 13 ).

The ICDAR 2003 database consists of two distinct data-bases: an isolated character database of 5,689 images of characters and a scene text database of 2,266 images of text in which 1,156 images are provided for training and 1,110 images for test. The training set of text images is used to generate 4,056 images of non-valid characters that we add to the single character database to obtain a set of 9,745 images, called CharDataset II, used to train WCConvNet and CRCon-vNet.Thetestset,calledTextDatasetII,isusedtoevaluatethe two proposed OCRs. For the  X  X cene text X  dataset, 36 classes of characters are considered as follows: 26 Latin letters and 10 Arabic numbers. 6.2 ConvNets training and results In both experiments, called  X  X aption text X  and  X  X cene text X  recognition, CharDataset I and CharDataset II are divided into two subsets: one containing 90 % of the images and used to train WCConvNet and CRConvNet, and another set containing the remaining 10 % used to evaluate classification performance and generalization. Table 1 shows the classifica-tion results on both subsets. A 10 % difference for both clas-sifiers (WRConvNet and CRConvNet) between the results obtained on CharDataset I and CharDataset II can be noticed. This difference can be explained by the high variability of  X  X cene text X  characters compared with  X  X aption text X  char-acters. CRConvNet usually obtains better performance than WCConvNet for which the classification task seems to be harder because of important confusions between some mis-segmented characters (considered as non-valid ones) and other valid characters, such as a part of a  X  X  X  that can be recognized as an  X  X . X  6.3 Performance of the proposed OCRs Using the trained ConvNets, the segmentation-based and segmentation-free OCRs are tested and evaluated both on TextDataset I and TextDataset II. Figure 14 presents exam-ples of texts recognized using these OCRs and illustrates the character segmentation step of the first OCR and the result-ing best path within the graph model of the second OCR. Recognition results are reported in Table 2 .
 Experiments carried out on TextDataset I show that both OCRs perform well on embedded texts with more than 93 % of good character recognition rate and that they obtain similar results ( &lt; 2% of difference in term of character recognition rate). However, the segmentation-based OCR is slightly bet-ter than the segmentation-free OCR on this dataset. This proves that when the character segmentation step works well (on text with small distortion like  X  X aption text X ), it enhances the following steps and leads to better recognition perfor-mance. Particularly, the segmentation-based OCR system obtains higher recognition rate on spaces between words than the second OCR. On the contrary, results obtained on nat-ural scene texts (i.e., TextDataset II) show a larger difference between OCRs X  performance. While the segmentation-free OCR achieves a character recognition rate above 70 %, cor-responding to a word recognition rate of about 47 %, the segmentation-based OCR achieves 65 % of character recog-nition rate corresponding to a word recognition rate of 41 %. These results demonstrate the drawbacks of the segmentation step where any error directly induces a drop in the recog-nition accuracy. Particularly, in the case of natural scene text, images are usually affected by various distortions which make the segmentation very hard and thus lead to errors such as false segmentations considered as  X  X ccurate X  ones that over-segment characters or to confusing  X  X isky X  segmenta-tions.
 The difference between the performances achieved on TextDataset II and those obtained on TextDataset I can be explained by the fact that: (i) the character recognizer per-forms better for CharDataset I, and (ii) TextDataset I is less complex than TextDataset II. In addition, the few remain-ing errors on TextDataset I can be justified by some charac-ter confusions between visually similar characters and some charactersegmentationerrorsinthecaseofthesegmentation-based OCR. Regarding the errors produced on TextDataset II, the strong distortions of an important number of images and the small sizes of some of them are the major causes of the remaining errors (30 % of characters in the case of the segmentation-free OCR and 35 % in the case of the segmentation-based OCR).

Concerning the computational time, the segmentation-based OCR is in average 7 times faster than the second OCR. For instance, for a text image with a size of 418 pix-els, while the segmentation-based OCR takes 700 ms, the segmentation-free OCR takes 5,000ms.

Table 3 presents a comparison of the two proposed OCRs with state-of-the-art methods [ 34 , 42 ] and commercial OCR engines (ABBYY FineReader OCR and Tesseract OCR). Notice that ABBYY FineReader OCR and Tesseract OCR were not trained on the same datasets as our OCRs and other state-of-the-art methods. This fact is due to practical issues: Actually, the training of ABBYY OCR requires to purchase the SDK that we do not own and the last version of Tesseract OCR (namely Tesseract 3.0x) is not adapted to deal with real text images.

Since Sa X dane et al. [ 34 ], and Wang et al. [ 42 ]have designed their methods to recognize single words in nat-ural scene images, comparisons with previously published state-of-the-art methods is done only on the public data-base ICDAR 2003 (the  X  X aption text X  video dataset contains mainly images with sentences). In these comparisons, three experiments were performed on TextDataset II, evaluating the word recognition rate as in [ 34 ] and [ 42 ]. Besides the experimentation on the full TextDataset II (Exp1), the OCRs are evaluated on the 901 images selected in [ 34 ] (Exp2) and on the 1 , 065 images selected in [ 42 ] using the same lexi-con, created from all the words that appear in the test set, as the one used in [ 42 ] (Exp3). These different tests show that our segmentation-free OCR yields the best word accu-racy. It also outperforms Wang et al. X  X  system [ 42 ] by about +7% even though their method uses hand-designed features to recognize characters.
 Concerning commercial OCR systems, namely ABBYY FineReader and Tesseract, since they were not trained on the same datasets as those used for our systems, results reported in Table 3 are basically provided for illustrative purposes. Experiments carried out on  X  X aption text X  in TextDataset I show that Tesseract OCR obtains a word recognition rate of 70 % while ABBYY FineReader obtains a word recognition rate of 87 %. Even though ABBYY FineReader is not trained, it still outperforms our systems. In our opinion, this is due to the use of a dictionary, absent in our OCRs (experiments with a small dictionary enabled us to obtain equivalent per-formances); we also notice some extra errors in our systems due to a confusion between quotes (which are not consid-ered in our CRConvNet) and the letter  X  X , X  inducing word errors. In the case of  X  X cene text X  in TextDataset II, ABBYY FineReader OCR and Tesseract OCR evaluated by Wang et al. [ 42 ] obtain poor results with less than 45 % of word recognition rate, while our segmentation-free OCR achieves 66 %. Hence, our OCR systems prove their great ability to handle both  X  X aption X  and  X  X cene texts. X  6.4 Contribution of the language model This subsection focuses on the contribution of the language model incorporated in our OCR systems. Recognition perfor-mance of both OCRs integrating language models (presented in Sect. 6.3 ) is compared to their performance when no lin-guistic knowledge is provided.

Table 4 highlights the contribution of the language model (LM) integrated into the designed OCRs and evaluated on TextDataset I and TextDataset II. All performed experiments demonstrate that the integration of the character tri-gram lan-guage model results in an important improvement on the character recognition rate, which reaches 16 % in the case of  X  X cene texts X  recognized by the segmentation-free OCR. The language model also increases the word recognition rate considerably, by +13% in average.

Figure 15 illustrates some confusions corrected using the language model, especially for similar characters such as  X  X  X  and  X  X , X  and shows an example of discarded over-segmentation, namely  X  X  X  previously over-segmented into  X  X  X  and  X  X . X  7 Conclusion and future work In this paper, we have presented two different approaches for text recognition in multimedia documents (images and videos). The first OCR relies on a step of character seg-mentation that aims at separating characters before recog-nizing them. One of the contributions of this system is the nonlinear segmentation performed in order to obtain bor-ders well adapted to the local morphology of text images and thus improve recognition rates. In contrast, the second OCR avoids character segmentation by using a multi-scale scanning scheme and a graph model. Unlike other state-of-the-art methods, this system allows to recognize characters at their appropriate positions and scales directly from the whole textimagewithoutanypre-processing.Arobustneural-based classification method is designed to recognize characters and is used in both OCR systems. Linguistic knowledge is also integrated in both systems to remove errors.

Both systems were tested and evaluated on texts embed-ded in videos and on natural scene text images. Our exper-iments showed that both OCRs perform very well (over 93 % of correctly recognized characters) in the case of  X  X aption text X  images. In the case of images with strong distortions, like natural scene texts, the segmentation-free OCR performs well, achieving good results (of about 70 % of character recognition rate) better than those provided with the segmentation-based OCR. The proposed OCRs were also compared to other state-of-the-art methods and obtained the best results. In this paper, we have also con-firmed that the incorporation of linguistic knowledge, namely a character n-gram model, improves performance of both OCRs.

Their high efficiency allows to use our OCRs in automatic indexing and retrieval systems, like TV news broadcast con-tent analysis. Moreover, the genericity of our systems per-mits to use them in many applications. For instance, they can serve to enhance a video teaching service by recognizing texts embedded in filmed slides, or help visually impaired people by reading using audio devices.

As a future extension of this work, we plan to use an unsu-pervised learning technique (namely auto-encoders) to pro-duce relevant representations of text images. A connectionist neural model can then be trained to recognize the encoded sequences of characters.
 References
