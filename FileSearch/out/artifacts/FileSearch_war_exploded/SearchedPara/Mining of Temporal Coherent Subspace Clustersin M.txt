 Mining patterns from multivariate temporal data is important in many appli-cations, as for example analysis of human action patterns [12], gene expression data [8], or chemical reactions [17]. Temporal data in general reflect the possi-bly changing state of an observed system over time and are obtained by sensor readings or by complex simulations. Examples include financial ratios, engine readings in the automotive industry, patient monitoring, gene expression data, sensors for forest fire detection, and scientific simulation data, as e.g. climate models. The observed objects in these examples are individual stocks, engines, patients, genes, spatial locations or grid cells in the simulations. The obtained data are usually represented by multivariate time series, where each attribute represents a distinct aspect of observed objects; e.g., in the health care exam-ple, each patient has a heart rate, a bo dy temperature, and a blood pressure. The attributes are often correlated; e.g. for the forest fire, the attributes tem-perature and degree of smoke are both signs of fire. Unknown patterns in such databases can be mined by clustering approaches, where time series are grouped together by their similarity . Accordingly, clusters of time series correspond to groups of objects having a similar evolu tion over time, and clusters represent these evolutions.

In many applications, however, existing approaches for clustering univariate or multivariate time series are ineffect ive due to a specific aspect of the analyzed data: patterns of interest that are neither bound to the whole dimensional nor temporal extent of the time series. Since our method is designed for effective mining under this scenario, we elaborate on this aspect in the next paragraph.
Temporal patterns of interest often only exist over a partial temporal extent of analyzed time series, i.e. they are constrained to an interval, and a single multivariate temporal pattern can have different intervals for each of its dimen-sions, as it is illustrated in Fig. 1. More concretely, time series belonging to one cluster only have similar values in these intervals, and values in the remaining intervals are noisy. Also, for some clusters there are dimensions in which there is no similarity between the time series. In the following, the intervals and di-mensions belonging to a cluster are called relevant, while the remaining intervals and dimensions are called non-relevant. If non-relevant intervals are considered in distance measures that are used to decide which time series are grouped to-gether, clusters can result that do not reflect the true patterns in the data.
Our novel, subspace clustering related approach handles this aspect by using an effective cluster model that distinguishes explicitly between relevant and non-relevant intervals for each cluster, and only relevant intervals are incorporated into the similarity function used for deciding which time series belong to a specific cluster. Our approach prevents incoherent t ime series clusters, i.e. clusters that have points in time that do not belong to any of the cluster X  X  individual intervals. This ensures that there are no single (incoherent) cluster which would better be represented by several sing le (coherent) clusters.

Summarized, we propose a novel, subspace clustering related approach for effective clustering of multivari ate time series databases that  X  uses a cluster definition that distinguishes explicitly between relevant and  X  is efficient due a approximate computation of our model that delivers high This paper is structured as follows: in Section 2 we discuss related work. In Section 3 we introduce our approach for effective clustering of multivariate time series, for which an efficient algorithm is presented in Section 4. In Section 5 we evaluate our approach and Section 6 contains concluding remarks. Clustering of temporal data can roughly be divided into clustering of incoming data stream, called stream clustering [9], and clustering of static databases, the topic of this paper. Our method is related to two static clustering research areas, namely time series clustering and subspace clustering, and we discuss these areas in the following. In the experiments, we compare to methods from both areas. Time Series Clustering. There is much research on clustering univariate time series data, and we suggest the comprehen sive surveys on this topic [4,11]. Clus-tering of multivariate time series is recently gaini ng importance: Early work in [14] uses clustering to identify outliers in multivariate time series databases. Multivariate time series clustering approaches based on statistical features were introduced in [2,19,20]. There are no concepts in these approaches to discover patterns hidden in parts of the dimensional or temporal extents of time series.
Most clustering approaches are based on an underlying similarity measure between time series. There is work on noise-robust similarity measures based on partial comparison, called Longest Common Subsequences (LCSS) [18]. We combined k-Medoid with LCSS as a competing solution.

There is another type of time series clustering methods, designed for ap-plications in which single, long time series are mined for frequently appearing subsequences. These methods perform subsequence clustering , with subsequences being generated by a sliding window. Since we are interested in patterns that occur in several time series at similar p oints in time and not in patterns that occur in a single time series at arbitrary positions, those approaches cannot be applied in our application scenario.
 Subspace Clustering (2D) an d TriClustering (3D). Subspace cluster-ing [1,10,15,21] was introduced for high-dimensional (non-temporal) vector data, where clusters are hidden in individual dimensional subsets of the data. Since subspace clustering is achieved by simultaneous clustering of the objects and dimensions of dataset, it is also known as 2D clustering. When subspace clus-tering is applied to 3D data (objects, dimensions, time points), the time series for the individual dimensions are conc atenated to obtain a 2D space (objects, concatenated dimensions). While subspace clustering is good for excluding irrel-evant points in time, there are problems when it is applied to temporal data: First, by the transformation described ab ove, the correlation between the dimen-sions is lost. Second, subspace clustering in general cannot exploit the natural correlation between subsequent points in time, i.e. temporal coherence is lost.
Accordingly, for 3D data, Triclustering approaches were introduced [7,8,16,22], which simultaneously cluster objects, dimensions, and points in time. Special Triclustering approaches are for clustering two related datasets together [6], which is a fundamentally different concept than the one in this paper. Gen-erally, Triclustering approaches can only find block-shaped clusters: A cluster is defined by a set of objects, dimensions, and points in time [22] or intervals [7,16]. The points in time or intervals hold for all objects and dimensions of a cluster. In contrast, our approach can mine cluste rs where each dimension has different, independent relevant intervals. In the following we introduce our model for subspace clustering of multivariate time series data. In Section 3.1, we introduce our definition for subspace clus-ters of complex multivariate time series data, and in Section 3.2 we formalize an optimal clustering that is redundancy-free and contains clusters of maximal interestingness. 3.1 Time Series Subspace Cluster Definition As input for our model we assume a database DB of multivariate time se-ries where Dim = { 1 ,...,Dim max } denotes the set of dimensions and T = { 1 ,...,T max } the set of points in time for each time series. We use o [ d, t ]  X  R to refer to the attribute value of time series o  X  DB in dimension d  X  Dim at time t  X  T . As an abbreviation, o [ d, t 1 ...t 2 ] denotes the univariate subsequence obtained from object o by just considering dimension d between points in time t and t 2 . Our aim is to detect temporal coherent patterns, i.e. similar behaving objects, in this kind of multivariate data.

Since we cannot expect to find temporal patterns over the whole extent of the time series or within all dimensions, we have to restrict our considerations to subsets of the overall domain. Naively, a cluster could be defined by a tuple ( O, S, I ) where the objects O show similar behavior in subspace S  X  Dim and time points I  X  T . This is straightforward extension of subspace clustering to the temporal domain and is used in triclustering approaches like [7,16,22].
A model based on this extension is limit ed because each selected dimension d  X  S has to be relevant for each selected point in time t  X  I . For example, if the t , we cannot get a single cluster for the objects O . We either have to exclude dimension d 1 or time point t 2 from the cluster. Thus, important information is lost and clustering eff ectiveness degrades.

Our novel model avoids this problem by selecting per dimension an individual set of intervals in which the time series are similar in (cf. Fig. 1). Such an interval, which contains a specific temporal pattern, is denoted as interval pattern. Definition 1. An interval pattern IP =( O, d, Int ) is defined by:  X  an object set O  X  DB  X  one selected dimension d  X  Dim  X  an interval Int = { start,...,end } X  T with length ( Int ) &gt; 1 for  X  X specific cluster property the corresponding subsequences o [ d,start...end ] To avoid isolated points in time, i.e. where time series are rather similar by chance, we require that length ( Int ) &gt; 1. The cluster property defines how sim-ilarity between subsequences is meas ured and how similar they need to be in order to be included in the same interval pattern. This property can be chosen by specific application needs. Besides the cluster compactness, which is also used by other subspace clustering methods [13,15], other distance measures applicable for time series including DTW [3] can be used.
Based on the introduced interval patte rns, clusters are g enerated: for each dimension, zero, a single, or even several i nterval patterns can exist in the cluster. This allows our method to systematically exclude non-relevant intervals from the cluster to better reflect the existing pa tterns in the analyzed data. However, not all combinations of interval patterns correspond to reasonable temporal clusters. The temporal coherence of the pattern is crucial. For example, let us consider a set of objects O forming three interval patterns in the time periods 1-10, 25-40, and 35-40, as illustrated in Fig. 2. Since for the remaining points in time no pattern is detected, there is no temporal coherence of the patterns. In this case, two individual clusters would reflect the data correctly.

To ensure temporal coherence, each point in time t  X  T that is located between the beginning a  X  T and ending b  X  T of a cluster, i.e. a  X  t  X  b ,hasto be contained in at least one interval pattern of an arbitrary dimension. Thus, by considering all dimensions simultaneously, the cluster has to form a single connected interval, and each point in tim e can be included in several dimensions. Definition 2. TimeSC . A coherent time series subspace cluster (TimeSC) dimensions, is defined by:  X  for each interval i  X  X  1 ,...,m } it holds that ( O, d i ,Int i ) is a valid interval  X  the intervals per dimension are disjoint, i.e.  X  the cluster is temporal coherent, i.e. combined, we have a single connected We require disjoint intervals per dimension because for overlapping intervals single points in time could be included multiple times in the cluster, which is obviously not beneficial for describing the cluster.

Overall, our novel cluster model avoids the drawbacks of previous methods and flexibly identifies the coherent temporal patterns in complex multivariate time series data. 3.2 Clustering Model: Redundancy Avoidance Accounting for the properties of tempor al data, an object can naturally occur in several clusters. Definition 2 allows for grouping various objects within different dimensions and time intervals. By generating the set Clusters = { C 1 ,...,C k } of all TimeSC C i , we a priori permit overlapping cl usters. Overlap, however, poses a novel challenge: the set Clusters potentially is very large and the contained clusters differ only marginally. In the worst case, two clusters differ only by few objects and hence one of theses clusters provides no novel information. Thus, some clusters may be highly redundant and are not beneficial for the user. As a solution, we aim to extract a subset Result  X  Clusters that contains no redundant information.

In a set of clusters M  X  Clusters redundancy can be observed, if at least one cluster C  X  M exists whose structural properties can be described by the other clusters. More precisely: if we are able to find a set of clusters M  X  M which together group almost the same objects as C and that are located in similar intervals, then C  X  X  grouping does not represent novel knowledge.
 Definition 3. Structural Similarity. A single time series subspace cluster abbreviated C  X  M ,iff with redundancy parameters  X  obj , X  int  X  [0 , 1] , Obj ( M )= Int ( C ) representing C  X  X  intervals via 2-tuples of dimension and point in time: Int ( C )= Int (( O, K )) = { ( d, t ) | X  ( d, Int )  X  K : t  X  Int } . The higher the redundancy parameter values  X  obj and  X  int are set, the more time series (  X  obj ) or intervals (  X  int )of C have to be covered by M so that M is considered structural similar to C . In the extreme case of r obj = r dim = 1, C  X  X  time series and intervals have to be completely covered by M ;inthis setting, only few clusters are categorized as redundant. By choosing smaller values, redundancy occurs more often.

The final clustering Result must not contain structural similar clusters to be redundancy-free. Since, however, several clusterings fulfill this property, we intro-duce a second structural property that allows us to choose the most-interesting redundancy-free clustering. On the one hand, a cluster is interesting if it contains many objects, i.e. we get a strong gener alization. On the other hand, a cluster can represent a long temporal pattern but with less objects, corresponding to a high similarity within the cluster. Since simultaneously maximizing both crite-ria is contradictory, we introduce a combined objective function that realizes a trade-off between the number of objects and the pattern length: Definition 4. The Interestingness of a TimeSC C =( O, { ( d i ,Int i ) { 1 ...m } } ) is defined by By adding up the lengths of all intervals, overlap of intervals between different dimensions is rewarded. The optimal clustering result is defined by demanding the two introduced properties: Definition 5. The Optimal Clustering of the set of all valid clusters Clusters , i.e. Result  X  Clusters ,fulfills (1.) redundancy-free property: (2.) maximal interestingness: With this definition of an optimal clustering, the formalization of our novel cluster model for subspace clustering mult ivariate time series data is complete. In the next section, we will present a n efficient algorithm for this model. In this section we present an efficient algorithm for the proposed model. Due to space limitations we just present a short overview. Since calculating the op-timal clustering according to Def. 5 is NP-hard, our algorithm determines an approximative solution. The general processing scheme is shown in Fig. 3 and basically consists of two cyclically processed phases to determine the clusters. Thus, instead of generating the whole set of clusters Clusters and selecting the subset Result afterwards, we iteratively generate promising clusters, which are added to the result.
 Phase 1: In the first phase of each cycle a set o f cluster candidates is generated based on the following procedure: A time series p acting as a prototype for these candidates is rando mly selected, and this prototype is contained in each cluster candidate of this cycle. The cluster candidates, i.e. groups of time series O , are obtained by successively adding objects x i to the previous group, i.e. O on its size, which is constant for O i , and the length of the intervals, the choice of x i is completely determined based on the latter. Accordingly, the best object x 0 is the one which would induce the longest interval interval patterns w.r.t. p (summed over all dimension).

An interval pattern for the prototype p and x 0 at the beginning can poten-tially include each point in time. Interval patterns for the subsequent objects x , however, have to be restricted to the relevant intervals of O i . Overall, we generate a chain of groups O i containing objects with a high similarity to p . Based on these candidates we select the set O + with the highest interestingness, i.e. according to Def. 4 we combine the size with the interval lengths. Phase 2: In the second phase, a cluster C for the object set O + should be added to the current result Res j . In the first cycle of the algorithm the result is empty ( Res 0 =  X  ), whereas in later cycles it is not. Thus, adding new clusters could induce redundancy. Accordingly, for cluster C we determine those clusters C  X  Res j with similar relevant intervals (cf. Def. 3). For this set we check if a subset of clusters M covering similar objects as C exists. If not, we can directly add C to the result. In case such a set M exists, we test whether the (summed) interestingness of M is lower than the one of C . In this case, selecting C and removing M is beneficial. As a further optimization we determine the union of C  X  X  and M  X  X  objects, resulting in a larger cluster U with potentially smaller intervals. If U  X  X  interestingness exceeds the previous values, we select this cluster. This procedure is especially useful if clusters of previous iterations are not completely detected, i.e. some o bjects of the clusters were missed. This step improves the quality of these clusters by adding further objects. Overall, we generate a redundancy-free clustering solution and simultaneously maximize the interestingness as required in Def. 5.
 By completing the second phase we initiate the next cycle. In our algorithm the number of cycles is not a priori fixed but it is adapted to the number of detected clusters. We perform c  X | Res j | cycles. The more clusters are detected, the more prototypes should be drawn an d the more cycles are performed. Thus, our algorithm automatically adapts to the given data.

In the experimental evaluation, we w ill demonstrate the efficiency and effec-tiveness of this algorithm w.r.t. large scale data. We evaluate our TimeSC in comparison to six competing solutions, namely kMeans, a kMeans using statistical features for multivariate time series [19], Proclus [1], MineClus [21], and MIC [16]. We also included kMedoid, where we used the Longest Common Subsequences (LCSS) [18] as a distance measure to allow for partial comparison in the distance computation. We also compared to TriCluster [22], which was provided by the authors on their webpage, but it either delivered no result or the obtained accuracy was very low (  X  3%); there-fore we no longer included it in the experiments. For TimeSC, we used w =30 for the compactness parameter. For the redundancy model, we used  X  obj =0 . 5 and  X  int =0 . 9. Some of the competing algorithms are not suitable for large datasets; thus, in some experiments, we could not obtain all values. If not stated otherwise, we use the following settings for our synthetic data generator: The dataspace has an extend of [-100,+100], time series length is 200, clusters length is 100, the dataset dimensionality is 10, the number of relevant dimensions per cluster is 5, the number of clusters is 10, the average number of time series per cluster is 25, and there is 10% noise (outliers) in the data. The experiments were performed on AMD Opteron servers with 2.2GHz per core and 256GB RAM. In the experiments, the F1 measure is use d to measure accuracy of the obtained clusterings [5,13], and the values are averages of three runs.
 5.1 Evaluation w.r.t. Effectiveness Performance on different variations of our standard dataset is analyzed in Fig. 4 and Fig. 5. In Fig. 4(a) and 4(b) we change the dataset size by enlarging either the length of the included time series or t he number of attributes per time series. In both experiments, our TimeSC outperforms all competing solutions by a sig-nificant margin. Runner ups are the 2D subspace clustering algorithms MineClus and Proclus, which achieve about 80% accuracy. The standard fullspace cluster-ing approach kMeans also performs surprisingly well with about 60% accuracy. The statistical kMeans approach, which was specifically introduced for clustering multivariate time series, however, performs worse than the original kMeans ap-proach. The Triclustering (3D) approach MIC is outperformed by both kMeans variants. This was not expected, as MIC is designed for temporal data. And finally, the LCSS-based kMedoid only achieves about 20% accuracy in both ex-periments.

Next, we enlarge the data by increasing the number of clusters (Fig. 4(c)) and by increasing the number of time series p er cluster (Fig. 4(d)). With an increas-ing number of clusters, TimeSC and Proclus achieve stable results, while the accuracies of the other competing approac hes continuously sink. For an increas-ing number of time series per cluster, all the algorithms achieve stable results. Overall, TimeSC outperforms the competing solutions in all settings.
In Fig. 5(a) and 5(b) we change the number of relevant dimensions per cluster and the number of relevant points in time per cluster, i.e. the minimal cluster length. Overall, the obtained accuracies are similar to the preced ing experiments. TimeSC outperforms the other methods, and from these methods only the 2D subspace clustering algorithms can achieve stable results of about 80% accuracy. Also, as expected, with increasing relevant dimensions and relevant points in time, finding clusters in the data becomes simpler which is expressed by the strong increase in accuracy for the standard kMeans algorithm. This, however, does not hold for the statistical kMeans, whose accuracy sinks with increasing relevant dimensions and points in time. 5.2 Evaluation w.r.t. Efficiency Our algorithm is designed for larger datasets. To show this, we scaled the exper-iments from Fig. 4 to much higher values, as shown in Fig. 6. For example, the database size for the last step (70 , 000 time series) in Fig. 6(d) is 12 . 32 GB. The experiments illustrate that only the kMeans algorithms and TimeSC are suitable for the larger datasets. Due to the high runtimes, many values could not be ob-tained for the other approaches. From the subspace and triclustering algorithms, only Proclus shows acceptable runtime s, while the results of MineClus and MIC indicate that these algorithms are not applicable for larger datasets. We introduced a novel model for subspace clustering of multivariate time se-ries data. The clusters in our model are formed by individual sets of relevant intervals per dimension, which together fulfill temporal coherence. We develop a redundancy model to avoid structurally similar clusters and introduce an ap-proximate algorithm for generating clusterings according to our novel model. In the experimental comparison, we showed that our approach is efficient and generates clusterings of higher quality than the competing methods.
 Acknowledgments. This work has been supported by the UMIC Research Centre, RWTH Aachen University.

