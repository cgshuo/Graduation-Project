 Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hier-archical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006).
Among different approaches, GIZA++ 1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner 2 , Nile 3 (Riesa et al., 2011), and pialign 4 (Neubig et al., 2011). fast align 5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2 , which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2 , fast align introduces a  X  tension  X  parameter to model the overall accordance of word orders and an effi-cient parameter re-estimation algorithm is devised. It has been reported that the fast align ap-proach is more than 10 times faster than baseline GIZA++ , with comparable results in end-to-end French-, Chinese-, and Arabic-to-English transla-tion experiments.

However, the simplicity of the IBM model 2 also leads to a limitation. As demonstrated in this study, fast align does not perform well when applied to language pairs with drastically different word orders, e.g., Japanese and English. The problem is because of the IBM model 2  X  X  in-trinsic inability to handle complex distortions. In this study, we propose a simple and efficient re-ordering approach to improve the fast align  X  X  performance in such situations, referred to as segmenting-reversing ( seg rev ). Our motivation is to apply a rough but robust reordering to make the source and target sentences have more simi-lar word orders, where fast align can show its power. Specifically, seg rev first segments a source-target sentence pair into a sequence of minimal monotone chunk pairs 6 based on the au-tomatically generated word alignment. Within the chunk pairs, source word sequences are exam-ined to determine whether they should be com-pletely reversed or the original order should be retained. The objective of this step is to con-vert the source sentence to a roughly target-like word order. The seg rev process is applied re-cursively but not deeply (only twice in our ex-periments) for each source sentence in the train-ing data. Consequently, the seg rev process is lightweight and shallow. Local word sequences, except those at chunk boundaries, are not scram-bled, while global word orders are re-arranged if there are large chunks.
 Our primary experimental results for Japanese-English translation show that applying seg rev significantly improves fast align  X  X  perfor-mance to a level comparable to GIZA++ . The training time becomes 2  X  4 times that of a baseline fast align , which is still at least 2  X  4 times faster than the training time required by base-line GIZA++ . Results for German-, French-, and Chinese-English translations are also reported. The seg rev is inspired by the  X  REV preorder  X  (Katz-Brown and Collins, 2008), which is a sim-ple pre-reordering approach originally designed for the Japanese-to-English translation task. More efficient pre-reordering approaches usually require trained parsers and sophisticated machine learning frameworks (de Gispert et al., 2015; Hoshino et al., 2015). We adopt the REV method in Katz-Brown and Collins (2008) considering it is the simplest and lightest pre-reordering approach (to our knowledge), which may bring a minimal ef-fect on the efficiency of fast align .

An example seg rev process, where the word alignment is generated by fast align , is illus-trated in Fig. 1. The example we selected has rel-atively correct word alignment and seg rev per-forms well. In general cases, the alignment has significant noise and the reordering is rougher .
Algorithm 1 describes the repeated (  X  times) application of the seg rev process, and Algo-rithm 2 describes a single application. Specifi-cally, Algorithm 1 applies Algorithm 2  X  times. For each application of Algorithm 2, source sen-tence S and source indices in alignment A are reordered, and the overall permutation R I is up-dated and recorded. In Fig. 1, the original En-glish sentence had 10 words (including the pe-After the first application of seg rev , R I was [ 8,7,6,5,4,3,2,1,0 , 9] , and after the second appli-cation, R I was [ 7,8 , 6 , 1,2,3,4,5 , 0 , 9] (reversed parts are boxed).

In Algorithm 2, the main for loop (line 3) scans the source sentence from the beginning to the end to obtain monotone segmentation. The foreach (line 5) and if (line 11) are general phrase pair ex-traction process. The if (line 13) guarantees that the chunk is monotone on the target side. The rev function (line 16), which is described in Al-gorithm 3, determines whether the sub-sequence from s start to s end should be reversed by exam-ining the related alignment A sub . For example, in the first application shown in Fig.1, two sub-sequences [0 : 8] and [9 : 9] are processed by rev and [0 : 8] is reversed. Four sub-sequences [0 : 1] , [2 : 2] , [3 : 7] , and [9 : 9] are processed in the sec-ond application and [0:1] and [3:7] are reversed. 7 Finally, source sentence S and source indices in alignment A are reordered (lines 19  X  20). 8
Algorithm 3 performs the reversal. We count
Algorithm 1: seg rev  X 
Algorithm 2: seg rev the sub-sequence if and only if there are more dis-cordant pairs than the concordant pairs. In Fig.1, the sub-sequence [0 : 8] in the first application has 8 = 28 pairs of aligned word pair (i.e., 28 gray block pairs for eight gray blocks); however, only 11 pairs are concordant ( C 2 5 =10 pairs in [1:5] and one pair in [7:8] ), Consequently, the sub-sequence [0 : 8] is reversed because there are more discor-dant pairs ( 17 = 28  X  11 ). The two reversed sub-sequences in the second application are obvious.
Algorithm 4 describes the training frame-work, where fast align and seg rev are ap-plied alternately. To generate word alignment, fast align is run bi-directionally and sym-metrization heuristics are applied to reduce noise (line 11). In each iteration, the source sen-tences for seg rev are the original sentences,
Algorithm 3: rev
Algorithm 4: fast align with seg rev and fast align uses the reordered sentences with the exception of the first iteration. The word alignment generated is thus based on the reordered source sentences; consequently, the recorded per-mutation (line 14) is used to recover word align-ment before the next iteration. The permutation is a one-to-one mapping; therefore, recovering is re-alized by the inverse mapping of the permutation, which transfers the source-side word alignment in-dices to match the original source sentences.
The time complexity of Algorithm 3 is O ( l 2 ) , where l is the size of A sub that is related to the chunk size. If the average chunk size is a constant C depending on languages pairs or data sets, then the time complexity of Algorithm 2 is O ( C  X  I 2 ) assuming J and the size of A are both linear against I . The average chunk size will be reduced when seg rev is applied successively; there-fore, the time required for subsequent seg rev processes will decrease. In practice, compared with the training time required by fast align , seg rev processing time is negligible. Note that seg rev processes are accelerated easily by par-allel processing. We applied the proposed approach to Japanese-English translation, a language pair with dramat-ically different word orders. In addition, we ap-plied the approach to German-English translation, a language pair with relatively different word or-ders among European languages.
 For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Eu-roparl v7 corpus 10 (Koehn, 2005) for training, the / testing, respectively. Default settings for the PB SMT in MOSES 13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set pa-rameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit 14 .

We compared GIZA++ and fast align with default settings. GIZA++ was used as a mod-ule of MOSES . The bi-directional outputs of fast align were symmetrized by atools in steps were conducted using MOSES . grow-diag-final-and symmetrization was used consistently in the experiments. For the the proposed approach, we set  X  =2 and M =4 in Algorithm 4. Note that  X  can be set to a larger value and seg rev could be applied repeatedly until no additional reorder-ing is possible. As mentioned, the word alignment is noisy and our intention is a robust and rough process; therefore, we restricted seg rev to two applications and did not consider the difference in sentence lengths or different languages during training. Within each iteration, fast align was run with default settings, except initial diagonal-Table 1: Test set BLEU scores for Japanese-face, no significance; all compared with GIZA++ ) tension (  X  ini ) was set to 0 . 1 in the first iteration, to avoid overly strong monotone preference at the beginning of training.
 Experimental results for Japanese-English and German-English translations in both directions are listed in Table 1. The first two rows show the baseline performance. fast align (using a default  X  ini = 4 . 0 ) performance was statisti-cally significantly lower than GIZA++ , particu-larly for Japanese-English translation. The fol-lowing four rows show the results of the pro-posed approach. For the first iteration,  X  ini was set to 0 . 1 , and the performance did not change significantly. The translations from English im-proved (equal to GIZA++ ) at the second itera-tion. However, translations to English improved more slowly. We attribute the difference in im-provement rates between translation to and from English to the relatively fixed word order of En-glish, whereby the reordering process is easier and more consistent. Note that once transla-tions from English improved in the second iter-ation, performance decreased in the following it-erations. The results in Table 1 were obtained us-ing predictable-seed for tuning, which generated determinate results. Another attempt using ran-dom seeds to tune returned test set BLEU scores of 30 . 5 , 30 . 4 on en-ja and 12 . 8 , 12 . 8 on en-de , for iterations 3 and 4 , respectively. These four scores had no statistical significance against GIZA++ . The instability is largely due to the alignment of function words, which affects trans-lation performance (Riesa et al., 2011). The align-ment does not change significantly after the sec-ond iteration; however, it is unstable around func-tion words, 16 because seg rev does not process Table 2: Test set BLEU scores for French-to-English and Chinese-to-English translations. For fr-en , the data sets were the same as for de-en . For zh-en , NIST 2006 OpenMT data were used for training and test; test data from 2002 to 2005 OpenMT were used for tuning. unaligned function words between chunks. Our approach is too rough to handle function words precisely. We plan to address this in future. We also tested our approach on French-and Chinese-to-English translations. The results are listed in Table 2. GIZA++ and fast align showed no statistically significant difference in performance, which is consistent with Dyer et al. (2013). The proposed approach did not affect performance for French-and Chinese-to-English translations. These results are expected as these language pairs have similar word orders.

With regard to processing time, a na  X   X ve, single-thread implementation of seg rev in C++ took approximately 60 s / 40 s in the first / second ap-The recover process took less than 30 s in each it-eration. In contrast, fast align , although very fast, took approximately one hour for one round of training (using five iterations for its log-linear model) on the same corpus. Therefore, the addi-tional time required in our approach is quite small and can be ignored compared with the training time of fast align . 18 We have proposed a simple and efficient approach to improve the performance of fast align on language pairs with drastically different word or-ders. With the proposed approach, fast align obtained results comparable with GIZA++ , and its efficiency is retained. We are investigating further properties of seg rev and plan to extend it to We thank Dr. Atsushi Fujita for his helpful discus-sions on this work.

