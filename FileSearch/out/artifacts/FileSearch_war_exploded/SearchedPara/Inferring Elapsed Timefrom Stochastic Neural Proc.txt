 The experience of the passage of time, as well as the timing of events and intervals, has long been information is crucial for the correct functioning of a larg e number of processes, such as accurate limb movement, speech and the perception of speech (for exam ple, the difference between  X  X a X  and  X  X a X  lies only in the relative timing of voice onsets), and ca usal learning.
 ticularly when compared to the divergent set of specific mech anisms which have been theorized. One of the most influential proposals, the scalar expectancy theory (SET) of timing [1], suggests that interval timing is based on the accumulation of activit y from an internal oscillatory process. Other proposals have included banks of oscillators which, w hen fine-tuned, produce an alignment of phases at a specified point in time that can be used to genera te a neuronal spike [2]; models in which timing occurs via the characteristic and monotonic de cay of memory traces [3] or reverberant activity [4]; and randomly-connected deterministic netwo rks, which, given neuronal processes of appropriate timescales, can be shown to encode elapsed time implicitly [5].
 Although this multitude of theories shows that there is litt le consensus on the mechanisms respon-of different processes, from oscillations to decaying memo ries and the dynamics of randomly con-nected neural networks. All of the theories above choose one specific such process, and suggest that time estimation as a statistical problem, in which the elaps ed time  X  t is extracted from a collec-appeared in the psychological literature in the form of numb er-of-events models [6], which suggest been related to recent psychological findings the show that t he nature of the stimulus being timed affects judgments of duration [7].
 Here, by contrast, we consider the properties of duration es timators that are based on more general stochastic processes. The particular stochastic processe s we analyze are abstract. However, they may be seen as models both for internally-generated neural p rocesses, such as (spontaneous) net-by the existing mechanistic models. In addition, there is ev idence that timing mechanisms are dis-time-varying processes which are already present in the bra in is implementationally efficient, and may be exploited for different timing purposes. Here, we sho w primarily that interval estimates based on such processes obey a Weber-like scaling law for acc uracy under a wide range of assump-tions, as well as scaling with process number that is consist ent with experimental observation; and Neuronal spike trains exhibit internal dependencies on man y time scales, ranging from milliseconds to tens of seconds [11, 12], so these  X  or, more likely, proces ses derived from spike trains, such as average network activity  X  are plausible candidates for t he types of processes assumed in this paper. Likewise, sensory information too varies over a larg e range of temporal scales [13]. The particular stochastic processes we use here are Gaussian Pr ocesses, whose power spectra are chosen to be broad and roughly similar to those seen in natural stimu li. To illustrate how random processes contain timing informat ion, consider a random walk starting at the origin, and suppose that we see a snapshot of the random wa lk at another, unknown, point in time. If the walk were to end up very far from the origin, and if some statistics of the random walk were known, we would expect that the time difference between the two observations,  X  t , must be reasonably long in comparison to the diffusion time of the pr ocess. If, however, the second point were still very close to the origin, we might assign a high pro bability to  X  t  X  0 , but also some random walk would lead to more accurate estimates (e.g. if tw o random walks had both moved very little between the two instances in time, our confidence that  X  t  X  0 would be greater). From such probabilistic model for  X  t .
 To formalize these ideas, we model the random processes as a f amily of independent stationary observations is completely specified by a mean value (here se t to zero) and a covariance structure (here assumed to remain constant in time). We denote the set o f processes by { y this is not a necessity, we let each process evolve independe ntly according to the same stochastic dynamics; thus the process values differ only due to the rand om effects. Mimicking the tempo-f = frequency = 1 / ( time scale ) . Some instances of such processes are shown in Figure 1. Stationary Gaussian processes are fully described by the co variance function K ( X  t ) : so that the probability of observing a sequence of values [ y tributed, with zero mean and covariance matrix  X  Figure 1: Left: Two examples of the GPs used for inference of  X  t . Right: Their power spectrum. This is approximately a 1 /f 2 spectrum, similar to the temporal power spectrum of visual s cenes. To generate processes with multiple time scales, we approxi mate a 1 /f 2 spectrum with a sum over Q squared exponential covariance functions: Here  X  2 the indicator function, which equals 1 when its argument is zero), and l component squared exponential functions. We take these to b e linearly spaced, so that l mimic a 1 /f 2 spectrum, we choose the power of each component to be constan t:  X  2 1 shows that this choice does indeed quite accurately reprod uce a 1 /f 2 power spectrum. tion of an interval [ t, t +  X  t ] from two instantaneous observations of the processes, name ly { y and { y matrix  X ( X  t ) of y i , which is of size 2x2, gives rise to a likelihood of these obse rvations, This distribution gives a probabilistic description of the time difference between two snapshots of the true value of  X  t , showing that such random processes may indeed be exploited to obtain timing on  X  , and show that they correspond to several experimental findi ngs. Figure 2: Statistics of the inference of  X  t from snapshots of a group of GPs. The GPs have time times ( dashed ). Right: The Weber law of timing,  X   X   X  t , approximately holds true for this model. line fit is shown with a dashed line. The Cramer-Rao bound ( blue ), which will be derived later in the text, predicts the empirical data well. 3.1 Empirical demonstration of Weber X  X  law we show that GP-based estimates share this property under br oad conditions.
 To compare the behaviour of the model to experimental data, w e must choose a mapping from the function  X  to a single scalar value, which will model the observer X  X  rep ort. A simple choice is to assume that the reported  X  t is the maximum a-posteriori (MAP) estimator based on  X  , that is, c  X  t
MAP = argmax  X  t  X ( X  t ) . To compare the statistics of this estimator to the experime ntal obser-vation, we took samples { y containing time scales from 1 to 40 time units. 100 samples we re generated for each  X  t (ranging  X  t  X  t . Thus, time estimation is possible using the stochastic pro cess framework, and the Weber law of timing holds fairly accurately. 3.2 Fisher Information and Weber X  X  law the power spectrum of the processes? What if one changes the sc ale of the instantaneous noise? We increased the noise scale  X  2 changing the power spectrum of the processes from a 1 /f 2 -type spectrum to a 1 /f 3 -type spectrum (by letting  X  2 3). This result may appear somewhat counter-intuitive, as o ne might expect that the accuracy of the the power spectrum to 1 /f 3 might be expected to result in more accurate estimates of lar ge  X  t (lower frequencies) as compared to estimates of small  X  t , but this was not the case. simpler analytical approximation to this relation can be co nstructed through the Cramer-Rao bound. This is a lower bound on the asymptotic variance of an unbiase d Maximum Likelihood estimator of  X  t and is given by the inverse Fisher Information: Figure 3: Left: Two examples of GPs with a different power spectrum (  X  2 approximates a 1 /f 3 power spectrum, resulting in much smoother dynamics). Right: Inference of i.e., the new 1 /f 3 statistics. The Weber law still approximately holds, even t hough the dynamics analytical Cramer-Rao bound ( blue ).
 The Fisher Information, assuming that the elapsed time is es timated on the basis of N processes, each evolving according to covariance matrix  X ( X  t ) , is given by the expression approximation to the empirical behaviour of the model.
 but related model, in which there are N Gaussian processes, again labeled i , but each now evolv-ing according to different covariance matrix C many timescales. In this new model, each process evolves wit h a single squared-exponential covari-to the accuracy of the estimator.
 Thus, in this model, [ C is then shaped as exp(  X  f 2 l 2 This model shows very similar behaviour to the original mode l, but is somewhat less natural. Its time scales, Using the Fisher Information to plot Cramer-Rao bounds for d ifferent types of processes { y of time scales ( l Figure 4: Fisher Information and Cramer-Rao bounds for the m odel of equation 2. The Cramer-Rao bound is the square root of the inverse of the sum of all the Fis her Information curves (note that only a few Fisher Information curves are shown). The noise sc ale  X  2 either l process is either  X  2 to a robust Weber-like behaviour of the estimator of elapsed time. (  X  i = 1 these manipulations caused the Cramer-Rao bound to deviate much from linearity.
 Next, we can evaluate the contribution of each time scale to t he accuracy of estimates of  X  t , by inspecting the Fisher Information I a certain time scale l than l of power does not drastically affect the Cramer-Rao bound. F rom the graphs of I that the Weber law arises from an accumulation of high values of Fisher Information at low values of  X  t .
 However, it may be assumed that the shortest times that neura l systems can evaluate are no shorter than the scale of the fastest process within the system, maki ng these small  X  t  X  X  irrelevant. 3.3 Dependence of timing variability on the number of proces ses Increasing the number of processes, say N processes , will add more terms to the likelihood and make empirically (data not shown). Psychologically and neurally, increasing the number of pro cesses would correspond to adding more perceptual processes, or expanding the size of the network t hat is being monitored for timing es-rhythm tapping results in a higher variability of tapping ti mes than bimanual rhythm tapping, and that tapping with two hands and a foot results in even lower va riability.
 similar scaling law is obtained from the Multiple Timer Mode l [16]. This is not a model for timing itself, but for the combination of timing estimates of multi ple timers; the Multiple Timer Model combines these estimates by averaging, which is the ML estim ate arising from independent draws of equal variance Gaussian random variables, also resultin g in a 1 /  X  N scaling law. Experimentally, a slower decrease in variability than a 1 /  X  N law was observed. This can be ac-the number of effectively independent processes grows more slowly than the number of effectors. We have shown that timing information is present in random pr ocesses, and can be extracted prob-external stimuli to drive its processes.
 considering simpler estimators: First, simpler estimator s might be more easily implemented in neu-ral systems. Second, to calculate  X ( X  t ) , one needs all of { y may be based on the posterior distribution over  X  t conditioned on a reduced set of parameters, with cesses, which have more compact sufficient statistics (e.g. Brownian motion, being translationally invariant, would require only { y processes because they are unbounded and therefore hard to a ssociate with sensory or neural pro-cesses). We have not addressed how a memory mechanism might b e combined with the stochastic process framework; this will be explored in the future.
 that take place in the brain or in the sensory world. It was dem onstrated that estimators based on such processes replicate several important behaviors of hu mans and animals. Full models might be based on the same substrate, thereby naturally incorporati ng the same behaviors, but contain more completely specified relations to external input, memory me chanisms, adaptive mechanisms, neural implementation, and importantly, (supervised) learning o f the estimator.
 different? This is unknown; however, the Multivariate Cent ral Limit Theorem implies that sums of i.i.d. stochastic processes tend to Gaussian Processes  X  so that, when e.g. monitoring average neuronal activity, the correct estimator may well be based o n a GP likelihood.
 An issue that deserves consideration is the mixing of intern al (neural) and external (sensory) pro-cesses. Since timing information is present in both sensory processes (such as sound and movement of the natural world, and the motion of one X  X  body) and intern al processes (such as fluctuations in network activity), and because stimulus statistics influen ce timing estimates, we propose that psy-chological and neural timing may make use of both types of pro cesses. However, fluctuations in the code for temporal frequency in V2 [17]), so that neural and st imulus fluctuations cannot always be treated on the same footing. We will address this issue in the future.
 The framework presented here has some similarities with the very interesting and more explicitly physiological model proposed by Buonomano and colleagues [ 5, 18], in which time is implicitly encoded in deterministic 2 neural networks through slow neuronal time constants. Howe ver, temporal information in the network model is lost when there are stimu lus-independent fluctuations in the network activity, and the network can only be used as a reliab le timer when it starts from a fixed robust to noise, internal fluctuations, and stimulus change s. The stochastic process framework is, however, more abstract and farther removed from physiology , and a neural implementation may well share some features of the network model of timing.
 Acknowledgements: We thank Jeff Beck for useful suggestions, and Peter Dayan an d Carlos Brody for interesting discussions.

