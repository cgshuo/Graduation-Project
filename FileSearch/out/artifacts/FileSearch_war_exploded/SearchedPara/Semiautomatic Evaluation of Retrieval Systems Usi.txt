 Taking advantage of the well-known cluster hypothesis that  X  X losely associated documents tend to be relevant to the same request X , we can use inter-document similarity to pro-vide more accurate and robust evaluation of retrieval sys-tems. Using our method, we are able to accurately rank retrieval systems with up to 99% fewer relevance judgments than collected for the TREC conferences, and significantly more accurately than other algorithms given the same num-ber of judgments.
 Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Perfor-mance Evaluation General Terms: Experimentation, Measurement Keywords: information retrieval, evaluation, test collec-tions, clustering
Test collection construction is an important part of in-formation retrieval experimentation. But it is expensive X  although documents and queries are relatively easy to come by, relevance judgments are much harder. Assessors must be hired to read and judge documents, a process that is very resource-intensive. As a result, there has been a great deal of work on test collection construction and evaluation in the presence of incomplete or imperfect relevance judg-ments. Some of the work on the latter includes Buckley &amp; Voorhees X  bpref evaluation measure [2], Yilmaz &amp; Aslam X  X  inferred average precision [12], and Carterette et al. X  X  ex-pected average precision [4]. For the former, there are two sub-fields: intelligent selection of documents for human as-sessors to judge (e.g. Cormack et al. X  X  Move-to-Front pool-ing [6], Carterette et al. X  X  algorithm [4], and Aslam et al. X  X  unbiased sampling method [1]) and automatic evaluation without human assessors (e.g. Joachims X  method based on Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. clicks [9] and Jensen X  X  method based on assigning relevance from web taxonomies [8]). In this work we unite manual and automatic assessments of relevance with estimation methods for incomplete and imperfect judgments.

To do so we take advantage of van Rijsbergen X  X  cluster hypothesis , which says that  X  X losely associated documents tend to be relevant to the same request X  [11]. In other words, a document that is similar to other relevant docu-ments is likely to be relevant as well. The cluster hypothesis says nothing about documents that are nonrelevant or dis-similar, but these provide evidence as well: although there are many more ways a document can be nonrelevant than relevant, it still may be the case that a document similar to other nonrelevant documents is itself likely to be non-relevant. Using this hypothesis to create probabilities that unjudged documents are relevant, we can estimate the dif-ferences between retrieval systems.

Our main contribution in this work is a model for eval-uation based on document similarities. The most closely related previous work is that of Jensen [8], who used man-ual web taxonomies to assign relevance to documents for web evaluation. Our work is complementary to his, showing how uncertainty due to these types of automatic relevance judgments can be incorporated formally.
In previous work, we showed that average precision could be estimated by treating it as a random variable with a dis-tribution over possible judgments of relevance [4]. One of the advantages of our approach is that we can model many different sources of evidence for the relevance of documents. There has been other work on estimating evaluation metrics, particularly by Aslam et al. [1], but to the best of our knowl-edge it cannot incorporate multiple sources of evidence for relevance.

Specifically, we showed that average precision can be writ-ten as a quadratic equation over X i , Bernoulli random vari-ables for the relevance of documents i : where a ij is a constant derived from the ranks of documents i and j 1 . The distribution of AP converges to normal, so it can be described by its expectation and variance alone.
See Carterette et al. [4] for details
Calculating expectation and variance involves summing over all possible assignments of relevance. Since there are 2 n possible relevance assignments, this is intractable in prac-tice. However, it can be approximated as follows:
E [ AP ]  X  1 Var [ AP ]  X  1 The error in these approximations is a negligible O ( n 2
Assuming topics are independent, we can easily extend this to mean average precision (MAP), the mean of average precisions over a set of topics T . MAP is also normally distributed with expectation and variance: Instead of assuming that all unjudged documents are non-relevant (the conventional assumption), systems can then be ranked by E MAP, taking into account any information we have about the relevance of documents.

In addition to ranking documents, we would also like to be able to estimate our confidence in our ranking. A measure of confidence allows us to quantify how good we believe a ranking of systems to be. Define  X  MAP to be the difference in mean average precisions for two different systems over the same set of topics. We will then define the confidence in the sign of the difference of mean average precision to be confidence = P ( X  MAP &lt; 0) =  X  where  X ( X ) is the normal cumulative density function with zero mean and unit variance.

Note that the expressions for expectation and variance of average precision depend upon knowing the probability of relevance p i of each document. In our earlier work, we used a uniform 1 2 probability for each document [4]. More recently, we have shown that better estimates of probability can provide much more accurate evaluation [3]. In the next section, we turn to the task of estimating the probability of relevance of documents.
How well we can estimate average precision depends on how well we can estimate relevance. The advantage of our model is that it can incorporate any type of evidence for relevance that can be modeled.

The cluster hypothesis gives us an idea for a type of evi-dence: the similarity of documents to other relevant docu-ments. The cluster hypothesis says  X  X losely associated doc-uments tend to be relevant to the same request X  [11]. As we acquire judgments and learn about which documents are relevant, we may be able to take advantage of the cluster hy-pothesis to estimate the relevance of unjudged documents, which we can use in our evaluation.

We will model the probability of relevance of a document conditional on its similarity to other documents. This is similar to the approach Diaz [7] takes in regularizing re-trieval scores to be re-ranked, substituting  X  X robabilities of relevance X  for Diaz X  X   X  X etrieval scores X . In addition, since we require our outputs to be probabilities, we will use logis-tic regression to fit the model rather than the least-squares approach Diaz used.
In our logistic regression model, the log-odds of relevance is modeled as a weighted sum of similarities: where p i is P ( X i = 1), the probability of relevance of docu-ment i .

The weights are found by maximizing the likelihood of the data. The log-likelihood is log L (  X  )= ( y i log p i +(1  X  y i )log(1  X  p i )) +  X  X  To help avoid overfitting,  X  is a penalization parameter that can also be seen as a prior; the greater  X  is, the stronger the prior, and the closer to 0 the trained coefficients will be.
The response y i is the prior probability of relevance that we wish to regularize. Given some set of judgments J = R  X  N (relevant and nonrelevant judgments), let If no judgments have been made, then y i = 1 2 , the uni-form probability of relevance. The vector y =( y 1 ,y 2 , ... judgments and plus-one-smoothed probabilities is the set of scores to be regularized.
A well-known and commonly-used measure of similarity is the cosine of the angle between document vectors: where V is the vocabulary and w i,t is the term weight of term t in document i . Term weights are generally a combination of term frequency in the document and document frequency in the collection.
We obtained the retrieval runs submitted to TREC ad hoc tracks in 1994 and 1996 X 1999 (TRECs 3 and 5 X 8). Each run ranks at most 1000 documents to 50 topics. The number of runs in each set varies from 40 in TREC-3 to 129 in TREC-8. We also obtained the NIST qrels files to use as the  X  X rue X  relevance judgments. These contain relevance judgments for the top 50 or 100 documents retrieved by nearly every system.

Because of the computational cost of calculating the vari-ance of MAP, we truncated all ranked lists after 100 doc-uments. We are therefore only computing both true MAP and E MAP over the top 100.
We compare two algorithms for selecting documents to judge: minimal test collections (MTC), presented in our earlier work [4], and a simple pooling method we call incre-mental pooling (IP). MTC takes as input a minimum confi-dence level  X  ; it select documents for judging by adaptively reweighting based on previous judgments until that level of confidence is reached. We will target a minimum confidence of  X  = 95% in our experiments. IP simply pools docu-ments, orders them by the highest rank at which they were retrieved, then asks for judgments down the list.
We indexed TREC disks 1 X 5 with Indri, using the Krovetz stemmer and the default list of stopwords included in the Indri package. Cosine similarities between all documents in the pool of depth 100 were pre-computed and stored on disk. To solve the logistic regression problem, we use our own R implementation of TR-IRLS [10], a conjugate gradient de-scent algorithm for iteratively reweighted least squares. We used the TREC-4 collection to train the regularization pa-rameter  X  ;  X  = 1 was selected for providing the best combi-nation of few judgments and accurate evaluation.

The baseline we compare to is using the plus-one-smoothed estimates alone, without any similarity information. MTC+sim refers to the MTC algorithm plus similarity-based relevance probabilities; MTC+one refers to MTC with plus-one-smoothed relevance probabilities.
We wish to compare MTC to IP, and for MTC compare the probability estimation methods MTC+sim and MTC+one. To evaluate, we will look at the following statistics: 1. the rank correlation between a ranking with an incom-2. the number of judgments needed by MTC+sim and 3. the accuracy at predicting the sign of  X  MAP . 4. the accuracy at predicting the sign of  X  MAP for the The last statistic has been proposed by Cormack &amp; Ly-man [5] as an alternative to Kendall X  X   X  rank correlation, as the cost of missing the significant differences is much greater than the cost of missing non-significant differences.
For each experiment we run multiple trials with random-ized orderings of systems and topics. Using the same ran-domization for two algorithms allows us to evaluate the sig-nificance of our results using paired hypothesis tests.
The  X  correlation, number of judgments needed by MTC+sim to reach 95% confidence, the accuracy at predicting the sign of  X  MAP , and the accuracy on pairs with a significant dif-ference by a paired one-sided t-test are shown in Table 1. With 99% fewer judgments than in the qrels ,weareableto achieve about 90% accuracy at identifying the sign of the difference in MAP between two systems. Although the  X  correlations appear low, this method is doing an excellent job at identifying the significant differences. For example, on the TREC-3 set, we made only 951 judgments total (19 per topic, 23 per system, or less than one judgment per topic per system) and correctly identified 95% of the significant differences between systems.

Table 1 also shows the pairwise accuracy and  X  corre-lation when using IP to judge the same number of docu-ments judged by MTC+sim. The correlations and accu-racies are close to MTC+sim, reinforcing that the pooling method works well, but the differences are statistically sig-nificant ( p&lt; 0 . 001). MTC+one (not shown) requires more judgments to reach 95% confidence and has slightly lower  X  correlations and accuracy than MTC+sim. Its results are significantly better than IP, but significantly worse than MTC+sim.

Figure 1 plots the true ranking and the estimated ranking by E MAP for each TREC. Manual runs are highlighted with boxes around their points.
Figure 2 shows how  X  correlation changes for all three algorithms as documents are judged for one of the TREC-5 trials.  X  correlation increases steadily, jumping fairly fast during the first hundred judgments. MTC+sim shows the greatest rate of increase, followed by MTC+one (which is much more variable), followed by incremental pooling.
A concern about using document similarity to estimate relevance is the problem of the  X  X yranny of the majority X : most of the submitted runs are automatic, using variants of bag-of-words approaches to rank documents. Our similarity measure also relies on a bag-of-words approach, and thus it is reasonable to wonder whether we are actually doing a good job, or if we have managed to do well simply by doing well on those documents that were retrieved by the bulk of the submissions.

To answer this, we point to Figure 1. The manual runs (the runs that have retrieved the most different documents) are generally the best systems submitted, and therefore are on the right-hand side of the plot. For example, the right-most ten points in Figure 1(a) are manual runs. Manual runs are ranked well by our approach, suggesting that it is not dominated by such an effect.
We have shown how document similarities can be used to evaluate retrieval systems with greatly reduced effort. The resulting similarity-based test collections provide more ac-curate evaluation results than the same number of pooled judgments and better confidence estimates than a simpler method of probability estimation.
 This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by the Defense Ad-between the true ranking and the estimated ranking by E
MAP for the full set of systems and the subset of Figure 1: True and estimated rankings for the five TREC collections. Manual runs are highlighted with boxed points. vanced Research Projects Agency (DARPA) under contract number HR001-06-C-0023. Any opi nions, findings, and con-clusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. Figure 2: Kendall X  X   X  correlation increases with the number of judgments.
