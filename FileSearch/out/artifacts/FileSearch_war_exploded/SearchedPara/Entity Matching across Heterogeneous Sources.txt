 Given an entity in a source domain, finding its matched entities from another (target) domain is an important task in many appli-cations. Traditionally, the problem was usually addressed by first extracting major keywords corresponding to the source entity and then query relevant entities from the target domain using those key-words. However, the method would inevitably fails if the two do-mains have less or no overlapping in the content. An extreme case is that the source domain is in English and the target domain is in Chinese.

In this paper, we formalize the problem as entity matching across heterogeneous sources and propose a probabilistic topic model to solve the problem. The model integrates the topic extraction and entity matching, two core subtasks for dealing with the problem, into a unified model. Specifically, for handling the text disjointing problem, we use a cross-sampling process in our model to extract topics with terms coming from all the sources, and leverage ex-isting matching relations through latent topic layers instead of at text layers. Benefit from the proposed model, we can not only find the matched documents for a query entity, but also explain why these documents are related by showing the common topics they share. Our experiments in two real-world applications show that the proposed model can extensively improve the matching perfor-mance (+19.8% and +7.1% in two applications respectively) com-pared with several alternative methods.
 H.2.8 [ Database Applications ]: Data Mining; H.3.3 [ Information Search and Retrieval ]: Retrieval Models Heterogeneous sources; Cross-lingual matching; Topic model
With the rapid growth of the Web, including online digital li-braries, online social and information networks, and E-commerce c  X  systems, the Web provides abundant information to describe enti-ties from different sources. Given an entity in a source domain, finding its matched entities from another (target) domain is an im-portant task in many applications. For example, a patent expert may be interested in finding related patents in a patent database for a product; a user may be interested in finding all the related Chi-nese Wiki pages for a particular English Wiki page; and a doctor may be interested in finding all related drugs for a specific disease. Similar search problems can be found in many other applications.
The problem can be generalized as an entity matching problem across corpora from heterogeneous sources. In other words, given an entity (e.g., product) in one source, the goal is to find related entities (e.g., patents) from a different source. Despite many stud-ies on entity matching tasks [23, 22, 3, 13, 23]. Different from traditional search tasks, one key challenge of such problem is that different sources of corpora may use rather different languages or terminologies even when describing the same topic. For example, the terms used to express the same topic about Siri, are quite differ-ent in Wikipedia and patents. As Figure 1 (a) shows, the Siri Wiki article uses more daily expressions (e.g.,  X  X oice control, X   X  X ersonal assistant, X   X  X Phone, X  etc.) to describe Siri, in order to make it easier to understand by everyone. However, more professional and techni-cal terms are used in patents (e.g.,  X  X nformation retrieval, X   X  X euris-tic modules, X   X  X omputer-readable medium, X  etc.). The descriptions of two related entities from different sources can be very dissimilar in terms of their text similarity, and thus the traditional text-based search can no longer solve the problem. In addition, for each rel-evant entity, it would be interesting to know on which topic the target entity is relevant to the source entity. For example, as shown in Figure 1 (a), the patent  X  X ethod for improving voice recogni-tion X  is talking about  X  X oice control X  and its relevance probability to the source Wiki article on this topic is 0.83, while the relevance probability of the second patent is 0.54 but on topic  X  X anking X .
One possible solution is to map two entities into the same latent topic space. Intuitively, two entities are relevant to each other if they refer to the same topic, e.g., a Wiki article and a patent ar-ticle should be relevant if they are both talking about the topic of Siri. A topic in such case should contain terms from heterogeneous sources. For example, the topic of Siri should contain both the gen-eral terms in Wiki and the special terms in the related patents. If we can extract hidden topics from heterogeneous sources, we will be able to infer the relevance score between two entities. However, for most topic modeling methods, such as PLSA [11] and LDA [4], they do not deal with the issue of heterogeneous sources and are not able to generate topics with terms from different sources, since these terms seldom appear in the same entities.
 In this paper, we propose a novel probabilistic model, Cross-Source Topic (CST) model, to solve the entity matching problem method, and a CST based method. for a two-source case, which integrates the topic extraction and en-tity matching into a unified model. We first ask the users to give a small portion of labels indicating the matching between entities from heterogeneous sources. Then we model both the hidden top-ics and the entity matching in a unified framework, where a topic contains terms from heterogeneous sources and the entity matching is determined by the topic distributions of the two entities. By using this model, we can not only find the matched entities for a query entity, but also explain why these entities are related by showing the common topics they share. It turns out that our model can suc-cessfully overcome the little-text-overlap problem across hetero-geneous corpus sources, by modeling a topic with terms coming from all the sources and utilizing the matching labels for entities across different sources. A mean-field variational inference [28, 12] method is used to learn the model, which can be used to infer the matching relation between entities with no labels.
 We evaluate the CST model in two real scenarios: 1) given a Wiki article describing a specific product, searching patents in the online patent database USPTO 1 that are related to the same product; 2) given an English Wikipedia article, searching the corresponding article from the Chinese Wiki knowledge base. Figure 1 (b)-(c) show the experimental results in each scenario respectively, from which we can see that the proposed model extensively improve the performance (averagely +19.8% and +7.1% in two real scenarios respectively).

In all, our contributions of this paper are summarized in the fol-lowing. http://www.uspto.gov/ Organization Section 2 formulates the problem. Section 3 ex-plains our proposed model, describes the algorithm for model learning, and introduces the applications of the model. Section 4 introduces our experiment that validates the effectiveness of our methodology, including its setup, baseline methods and results. Section 5 reviews some related work, and finally, Section 6 con-cludes this work.
In this section, we present related definitions and formulate the problem. We first give the formal definition of heterogeneous source corpus. Generally, a heterogeneous source corpus contains the descriptions of entities from multiple sources. However, to make the definition and the description of the proposed model clear, we use a dual source corpus as an instance in all related definitions. We leave the source extension as future work.
 D EFINITION 1. Dual Source Corpus. A dual source corpus C is a set of text collections { C 1 ,C 2 } from two sources with vo-cabulary V t = { w t 1 ,w t 2 ,...,w t N t } ( t  X  { 1 , 2 } ), where C { d sented by a document describing it) from source t , D t is the num-ber of entities in C t , and N t is the total number of words in V . Following the common assumption of bag-of-words represen-tation, each entity d t i in C t can be represented as a bag of words { w tity d t i .

Given a dual source corpus, we can extract cross-source topics, which contain terms from different sources:
D EFINITION 2. Cross-Source Topic. A cross-source topic  X  contains multiple multinomial distributions over words from dif-ferent sources. For example, a 2-source topic contains two word distributions P 1 ( w |  X  ) and P 2 ( w |  X  ) , where P t probability of a word w from source t ( t  X  { 1 , 2 } ) appearing in this topic. Thus words with highest probabilities associated with each topic would suggest the semantics represented by the topic. Notice that we have P w  X  V cross-source topic  X  .

Next, we use a matching relation matrix to represent the correla-tions between entities from different sources.

D EFINITION 3. Matching Relation Matrix. A matching rela-tion matrix L represents the matching status between entities in a dual source corpus C . If d 1 i and d 2 j is matched, l i,j l i,j =  X  1 . l i,j =? denotes that the value is missing and needs to be inferred.

Since entities from different sources may share few terms, the known values in the matching relation matrix are important guid-ance to extract the cross-source topics and infer the missing values in the matrix. We can finally define the main problem addressed in this paper: P ROBLEM 1. Entity Matching across Heterogeneous Sources. Given a heterogeneous source corpus C , and a matching relation matrix L . The goal of cross-source entity matching is to determine the missing values in L .
 For example, we have a dual source corpus from Wikipedia and USPTO, the cross-source entity matching problem is: given a Wiki article describing a specific product, finding patents from USPTO which report the technologies related with the product. As an ex-ample, given a Wiki article describing Siri, one of the matched patent could be the one claims on the technology about  X  X niver-sal interface for retrieval of information, X  which is highly relevant to Siri.

Another example is cross-lingual Wiki article matching. Given an English Wiki article, the task aims to find a Chinese Wiki article that reports the same content. Compared with cross-lingual infor-mation retrieval problems, which mostly incorporate bilingual dic-tionaries, however, our problem is more general. Instead of using dictionaries, we focus on utilizing known relations to help extract cross-source topics and infer unknown relations. Modeling cross-source matching entities is a challenging task. Intuitively, two entities are relevant if they refer to the same topic, and topic extraction will help us infer the connection between enti-ties. However, due to the different terminologies used in different domains, word distributions of corpora from two sources may be quite different. In this situation, traditional topic modeling tech-nologies would fail to identify the same topic from two sources but separate the topic into two or more, as shown in our Siri example Figure 2: Plate representation of the Cross-Source Topic model. Modeling part for entities in source 2 has a symmet-rical structure as source 1 . For simplicity, the modeling part for the entities in source 2 is omitted.
 in Figure 1. In this paper, we propose a new semi-supervised prob-abilistic model called Cross-Source Topic (CST) model to capture the cross-source topics and perform entity matching from different sources simultaneously. Framework. The basic assumption of the proposed model is that, for entities from different sources, their matching relations and hidden topics are influenced by each other . Matching entities are similar in hidden space of topics, though the topics have differ-ent representations (e.g., word distributions) in different sources, and vice versa, entities that are similar in hidden space of topics tend to be matched. Thus the basic idea here is to leverage the known matching relations to help the extraction of hidden topics, and use the extracted topics to infer the unknown relations .
Figure 2 shows the plate representation of the proposed semi-supervised model. For simplicity, we omit the modeling part for the words in source 2 as it is the same as source 1 . Table 1 summarizes the notations used in the CST model.

In order to avoid pairwise relation modeling, before we use CST to model the generation of given entities and the generation of matching relations, we first process a candidate filtering. For the entities that have no chance to be matched with each other, CST will not model the relation generation for them. For example, given
Input : a dual source corpus C , a matching relation matrix L, foreach entity d do end % cross-sampling-based entity generation foreach d in each source t do end % matching relation generation foreach ( d,d 0 ) with possible links do end a Wiki article describing a product (e.g., iPhone), we only consider patents belonging to the company which creates this product (e.g., Apple) in relation generation part. More general method to filter candidates is left as future work.
 Cross-Sampling. We then introduce an important concept in the CST model: cross-sampling, which allows CST to leverage known relations and extract cross-source topics. The idea of cross-sampling is: when generating topics for an entity d , the sampling process is not only based on the topic distribution of d , but also the topic distributions of all the matching entities of d . The intuition behind the idea is that the matched entities are similar in hidden space of topics. For example, a user would like to edit a Chinese Wikipedia article about  X  X arack Obama. X  Before he starts, he may take a look at what topics the corresponding English Wikipedia ar-ticle contains, and finds out that the article contains Obama X  X  early career as a Chicago community organizer. Thus he will edit the Chinese Wikipedia article to present Obama X  X  experience as a com-munity organizer but in different words. This process of cross-sampling allows us to bridge the topics in entities from different sources and model the cross-source topics.

By cross-sampling, the CST model utilizes the known match-ing relations and makes the matching entities to have similar topic distributions. Similar ideas are proposed in some other models [8, 10, 17]. However, these unsupervised methods can hardly infer un-known relations in a unified model. As we will introduce later, CST employs a semi-supervised learning algorithm to infer unknown re-lations. Another kind of linked topic models [7, 20, 21] are able to infer missing links between entities. However, they do not con-sider the direct effect of known links on hidden topics, and CST employs cross-sampling to model a more explicit and high-order dependency between matching entities. The more sufficient uti-lization of known relations makes the CST model more suitable for heterogeneous source corpses than traditional topic models (exper-iments show that the CST model outperforms RTM, a traditional linked topic model, by 40.9% on average).
 Figure 3: An intermediate step of cross-sampling. There is a matching relation between d 1 from source 1 and d 2 from source 2. Latent topics for a word w 1 from d 1 is sampled based on both d and d 2 .
Formally, the generative process is described in Algorithm 1. It consists of two parts: (1) cross-sampling-based entity generation and (2) matching relation generation.
 Cross-Sampling-Based Entity Generation. Here, we introduce the entity generation in detail. First, for each entity d in source 1, we sample its topic distribution  X  d :  X  d  X  Dir (  X  ) . Next, for each word w in d , we choose a topic z : z  X  Mult (  X  c could be d itself or one of d  X  X  matching entities. We sample c according to c  X  Mult (  X  d ) , where  X  d indicates how likely an entity matched with d (including d itself) will be sampled.  X  d is sampled according to  X  d  X  Dir (  X  d ) ,  X  d is a | D | -dimensional vector, where | D | is the total number of entities, and we define  X  d as follows: we set  X  d,d = e 1 , where e 1 is a constant value denotes the weight of the prior to sample d  X  X  topics from its own topic distribution  X  for an entity d 0 matched with d , we set  X  d,d 0 = e 2 , where e another constant value represents the weight of the prior to sample topics from one of d  X  X  matching entities; for other entities we set the corresponding values in  X  to 0.

Figure 3 gives an example, in which we have three entities d d , and d 3 ; d 1 is from source 1; d 2 and d 3 are from source 2; the only matching relation exists between d 1 and d 2 . Thus we set  X  1 = ( e 1 ,e 2 , 0) . From Figure 3, we can see that d 1 signed with topics z 1 and z 2 in last step. However, in this step, as there is a matching relation between d 1 and d 2 , the word w d 1 can still be assigned with topics from d 2 ( z 3 and z bridges the latent topic space between linked entities.

With above definition, there is no chance to sample an entity d  X  X  topics from entities not matching with d . If d has no match-ing relations, each z is sampled according to its own entity X  X  topic distribution  X  d . Thus the generation of d is the same with LDA [4].
Finally the word w is sampled according to the word distribution of topic z in source 1: w  X  Mult (  X  1 ,z ) . As different terminologies are used to represent the same topic in different sources, we sepa-rate the word distribution of a topic z into  X  1 ,z and  X  source 1 as an example above and the documents in source 2 are generated in the same way.
 Matching Relation Generation. In this step, each matching rela-tion l d , d 0 is modeled as a binary variable. As entities with similar topic distributions tend to be matched with a higher probability, it is natural to model the probability of a matching relation as a func-tion  X  of topic distributions. There are many possibilities for the function  X  . In this paper, we consider the following form where the  X  notation denotes the Hadamard product ((  X  z  X  z d,k  X   X  z d 0 ,k ),  X  z d is a K -dimension vector indicating the appearance parameterized by coefficients  X  . We define the function as an ex-ponential one thus when z d and z d 0 are close, with large weighted Hadamard product, the probability increases exponentially.
A similar regression method is used in Relational Topic Model (RTM) [7]. The difference between RTM and CST is, RTM can hardly deal with the entities from multiple sources while CST bridges multiple sourced entities by learning how likely they will be influenced by each other (  X  ). Also, by cross-sampling, CST mod-els a high-order dependency between matching entities and utilize the known relations more sufficiently.
 As a conclusion, cross-sampling-based entity generation allows CST to leverage the known relations to help extract hidden cross-source topics. The matching relation generation uses extracted top-ics to infer the relations between entities in a latent space.
According to the model description above, the likelihood of the observed data in the CST model is given as where w is a set of observed words in given corpus, L is the match-ing relation matrix, d 1 and d 2 are two entities with a labeled l ( l 1 ,d 2 6 =? ), and N d is the number of words in entity d .
We employ MAP estimation to learn the parameters of the CST model. However, the exact posterior inference is intractable and we appeal to approximate inference methods. In this work, we em-ploy the mean-field variational inference [28, 12]. Generally, we define four variational parameters and aim to maximize the evi-dence lower bound (ELBO) [30]. Specifically, We define  X  and as variational multinomial parameters. We also define  X  and  X  as variational Dirichlet parameters. The approximate posterior is then defined as
We aim to minimize the Kullback-Leibler (KL) divergence be-tween the variational distribution and the true posterior, which is equivalent to maximizing the evidence lower bound (ELBO) [30]. The complete equation of ELBO is shown below.
 where d 1 and d 2 stratify l d 1 ,d 2 6 =? . We then need to compute each item in Eq. 4. We focus on the first item as others, which are ex-pected values of the log of a single probability component under the Dirichlet or the multinomial, can be expanded similar with LDA model. The first term is:
We then take the derivatives with respect to each variational pa-rameter. We use  X  as an example. We first collect all of the terms associated with  X  and get:
We then take the derivative with respect to  X 
The derivations of other variational parameters could be obtained similarly. We then set the derivations to zero, and find: where t is the source of entity d , v is the n -th word of d , and R ( d ) is a set of entities matched with d . Intuitively, Eq. 9 utilizes the known relations to update  X  . The first summation in this equation is related with cross-sampling and the second one is based on the regression part of CST. These updates above are performed itera-tively until convergence, since they depend on each other.
We then fit the model by maximizing the resulting ELBO with respect to the model parameters  X  and  X  . In source t , given a topic k and a term v , the update for  X  t,k,v is:
The derivate with respect to  X  takes a convenient form. To solve this problem, we add a 2-norm regularizer, which penalizes the ob-jective function with the term  X  ||  X  || 2 , where  X  is a free parameter. We then have: where d and d 0 are two entities with a labeled l d,d 0 ( l  X  known relations.

With all update equations above, we employ the variational expectation-maximization algorithm to learn the model, which yields the following iterations: (See Algorithm 2 for details.)
E-step: optimize the ELBO with respect to the variational pa-rameters {  X  ,  X  ,  X  , } . Update these variational parameters accord-ing to Eqs. 6-9.

M-step: maximize the resulting ELBO with respect to the model parameters {  X  ,  X  } . Update the model parameters according to Eqs. 10-11.
 Inferring Matching Relations. We finally detect the matching entities from different sources. Given a dual source corpus and a matching relation matrix with missing values, we use the learning algorithm from Section 3.3 to estimate the model X  X  parameters by optimizing the ELBO for the observed data: words from the corpus and known relations in the matching relation matrix. After that, given two entities d and d 0 with an unknown relation ( l we use the fitted model X  X  variational parameters to approximate the predictive probability:
The right hand of Eq. (12) is an expectation of  X  (defined in Eq. 1) with respect to the approximation posterior (Eq. 3). Intuitively, the approximated predictive probability indicates that CST consid-ers the content information and infers the matching relations be-tween entities in hidden space of topics. Also, CST can be plugged into other detection frameworks (e.g., random walk [15] or factor graphs [14]) easily, to further leverage structural information. De-tails and two examples will be described in the next section.
We evaluate our proposed model with two experiments. All datasets and codes used in this work are publicly available http://arnetminer.org/document-match/
Input : a dual source corpus C , a matching relation matrix L,
Initialize {  X  ,  X  ,  X  , ,  X  ,  X  } randomly; repeat until Convergence ;
We validate the proposed model in two real scenarios: product-patent matching and cross-lingual matching. We describe the de-tails of each task below.
 Product-patent matching. In this task, given a Wiki article de-scribing a specific product, we aim to find relevant patents, e.g., a Wiki article and a patent should be relevant if they are both talk-ing about the topic of Siri. We collect 13,085 Wiki articles and 15,000 patents from Wikipedia and USPTO respectively. For some Wiki article that describes a product, we use it as a query to find patents related with the same product. One Wiki article may be matched with more than one patent, e.g., a Wiki article describing iPhone corresponds to patents that claim on touch screen, camera, soft keyboard, etc.. We sample 233 Wiki articles as queries and find 1,060 matching relations in total. We randomly choose 30 % of the matching relations as known. The remaining relations are regarded as unknown and need to be inferred.

The ground truth data, which consists of 1,060 matching rela-tions, is labeled by four human annotators. For each of 233 Wiki articles as queries, each annotator reads all patents belonging to the same company with the corresponding product in the query. Some online systems and materials are referred when filtering the candi-dates and labeling the data (e.g., PatentMiner [25] 3 , news related with companies X  lawsuit, official documents of the products, etc.). To see more details of how we label the data, please refer to our public web page 2 . We say a Wiki article is matched with a patent when four annotators all agree. Based on this work, we have de-ployed a product-patent matching function to PatentMiner. We are
A public patent search and analysis system: http://pminer.org collecting user feedbacks to create a bigger evaluation data set for future work.
 Cross-lingual matching. In this task, given an English Wiki arti-cle, we aim to find a Chinese article, which reports the same con-tent, from a Chinese Wiki knowledge base. We use the same data set with [29]. The data set is collected as follows: we first randomly select an English article A with a cross-lingual link to a Chinese ar-ticle B from Wikipedia. We then use the B  X  X  title to find another Chinese article C with the same title in Baidu Baike cross-lingually linked with B in Wikipedia, and B has the same main idea with C (normally a Wiki article uses its main idea as the title). It is reasonable to say there is a cross-lingual matching relation between A and C .
 The data set consists of totally 2,000 English articles from Wikipedia, and 2,000 Chinese articles from Baidu Baike. Each En-glish article corresponds to one Chinese article. We conduct 3-fold cross validation on the evaluation data set. Evaluation metrics. In the first experiment, for each Wiki arti-cle, we rank all patents according to the probability predicted by the proposed model and alternative methods. We evaluate all the methods in terms of P@3 (Precision for the top 3 ranking results), P@20, MAP (Mean Average Precision), R@3 (Recall for the top 3 results), R@20, and MRR (Mean Reciprocal Rank).

In the second experiment, to keep consistence with [29], we con-sider cross-lingual matching as a two-class classification problem: given an English Wiki article and a Chinese Wiki article, we la-bel this pair of two entities as  X  X atched X  or  X  X ot matched X . We compare all baselines in terms of Precision (Prec.), Recall (Rec.), F -Measure (F 1 ), and F 2 -Measure (F 2 ).
 Comparison methods. For the first experiment, we compare the following methods for product-patent matching:  X  Content Similarity based on LDA (CS + LDA): It calculates the similarity between a Wiki article and a patent based on their topic distributions calculated by LDA. Specifically, we use p p 2 to represent the topic distribution of a Wiki article and a patent respectively. The similarity score is defined based on the Cosine similarity between p d 1 and p d 2
A Chinese Wiki knoledge base: http://baike.baidu.com/  X  Random Walk based on LDA (RW + LDA): It ranks candi-dates by combining the extracted topics into a random walk with restart algorithm [27]. Specifically, it creates a graph containing Wiki articles and patents as nodes. And it links a Wiki article u to a patent v with a weight where  X  is a threshold value defined manually, and Sim ( u,v ) is the Cosine similarity between u and v . Thus there is a bigger chance for a Wiki article node to reach a more similar patent node. It em-ploys LDA to calculate the topic distributions. Besides the textural contents of entities, this framework also considers the structural in-formation. We create a link from one patent node to another if the former one cites the latter one. We also create a link from one Wiki article nodes to another if they have a hyperlink in Wikipedia. The weights of these links are defined as a constant value (in practice, we define all of them as 1). Finally, the transition probability from u to v can be defined as where s is the start node, a is the restart probability.  X  Relational Topic Model (RTM): It employs the RTM, which is generally used to model the links between entities, proposed by Blei et al. [7]. In our problem, this method regards there is a link between two matching entities. We use Blei X  X  implementation of RTM 5 .  X  Random Walk based on CST (RW + CST): The difference between this method and RW + LDA is, instead of using Sim ( u,v ) to define the weight of links from a Wiki node to a patent node, it uses P ( l u,v ) (see Section 3.3 for details) calculated by CST.  X  CST: It is our proposed model. We first use the training set to learn the model. Then we use the fitted model to detect unknown relations. We set K = 50 ,  X  = 50 /K , e 1 = 4 , and e 2 = 1 in both this method and RW + CST.

All methods use entities in the training set to fit the model. Meth-ods related to RTM or CST utilize known matching relations as guidance, while LDA is unable to leverage this information. Ran-dom walk based methods further consider structural information (citations in the patent database and hyperlinks in Wikipedia).
For the second experiment, we compare the following methods for cross-lingual matching:  X  Title Only: This method first translates the title of Chinese articles into English by Google Translation API 6 , then matches the translated titles with English articles. Two articles are considered as equivalent ones if they have strictly the same English titles.  X  SVM-S: It is a classifier proposed by Sorg et al. [24] to find cross-lingual links between English Wikipedia and German Wikipedia. The authors define several graph-based and text-based features. Here we train a SVM with their features on evaluation data set. For SVM, we choose LIBSVM [6].  X  LFG: It is the method proposed by Wang et al. [29], which is based on a factor graph model and mainly considers the structural information to solve the problem of cross-lingual matching. http://www.cs.princeton.edu/~blei/topicmodeling.html https://developers.google.com/translate/?hl=zhcn and the F1 score of the CST model in cross-lingual matching experiment.  X  LFG + LDA: It adds a feature, which captures the content similarity between articles, to the feature function of LFG. It uses Sim ( u,v ) (see Eq. 14) as the feature value.  X  LFG + CST: LFG mainly considers structural information. We enhance it by bringing in content information (hidden topics ex-tracted by CST). The difference between this method and LFG + LDA is that, instead of using Sim ( u,v ) to define the newly added feature, it uses P ( l u,v ) calculated by CST. We compare this method with LFG to see if content information can help in this problem. We compare it with Title Only and SVM-S to show the power of utiliz-ing corss-lingual topics extracted by CST. We also compare it with LFG + LDA to show the effectiveness of the CST model compared with a traditional topic model. Here we keep values of K ,  X  , and e the same with the first task, and set e 1 = 2 . We will give the intuitive explanation why we change e 1 latter. Product-patent matching. Table 2 lists the performance of product-patent matching problem using different methods. We first compare CST with two unsupervised methods, CS + LDA and RW + LDA. With the help of known relations as guidance, we can see CST clearly outperforms these two methods (+72.4%-75.5% in terms of MAP). We then compare CST with RTM, which also utilizes the known relations as guidance. With the help of the cross-sampling, CST can better extract cross-source topics. Thus it can better detect the matching relations (+74.9% in terms of MRR). To our surprise, when employing the CST model, combining content and structural information hurts the performance (RW + CST drops 23.4% in terms of MAP). By a careful investigation, we find that a Wiki article normally has lots of hyperlinks to other articles (56.4 out-links in average). Much noise is contained in these links and hurts the performance. However, the structural information does help for top results (+14.5% in terms of R@3).
 Cross-lingual matching. Table 3 shows the performance of cross-lingual matching problem. Title Only and SVM-S employ the translated terminologies and perform well in terms of Prec. How-ever, without capturing the hidden topics of entities, the translation can not be performed precisely. Thus these methods miss a number of matching relations between entities, which hurts the Recall.
LFG focuses on utilizing structural information. We enhance this method by bringing in hidden topics extracted by LDA and CST respectively. From the table, we see that LFG + CST improves the performance. It outperforms all baselines in terms of Recall, F , and F 2 (e.g., averagely +15.2% in terms of F 2 ). In fact, cross-lingual topics can hardly be extracted due to the low co-occurrence of English and Chinese terminologies. Without a precise cross-lingual topic extraction, LFG + LDA performs worse than LFG, which indicates the incorrect topics will hurt the performance. By studying some cross-lingual topics found by the CST model, we find that the top Chinese and English terminologies in the same topic are very relevant. Some Chinese terminologies are translated results of English ones.
 Topics analysis. How many topics are enough for the product-patent matching problem and cross-lingual matching problem? We perform an analysis by varying the number of topics in the CST model. Figure 4(a) shows its performance with number of topics K varied. We can see that, the performance improves by increasing K when K is small ( &lt; 50 ). After that, the trend becomes stable. Ratio analysis. We study how the ratio of e 1 to e 2 influence the performance. We fix e 2 as 1 and vary e 1 . Figure 4(b) shows the trend of the performance following the changes of the ratio in both two problems. In the product-patent matching problem, the value of MAP reaches largest when e 1 : e 2 = 4 . And in the cross-lingual problem, F1 reaches the maximum value when e 1 : e corresponding to a larger prior probability of cross-sampling.
Intuitively, compared with cross-lingual matched articles, patents and Wiki articles with matching relations are more dissim-ilar in hidden space of topics: patents focus on specific technolo-gies, while Wiki articles describe general descriptions of products (e.g., histories, sales, etc.). And cross-lingual matched articles re-port the same objects. Thus the prior of cross-sampling in cross-lingual matching problem should be larger (smaller e 1 , larger e It indicates that the hyper-parameters of CST can be determined in-tuitively: if the matching entities in a specific problem assumed to be more similar in topics, we can give a smaller value to e otherwise we should set e 1 : e 2 a larger value.
 Precision analysis. We further investigate how the precision [19] of  X  , which indicates the confidence in the prior, influence the per-formance. We vary the precision from 1 to 450. As Figure 4(c) shows, the CST model X  X  matching performance is not sensitive to the precision of  X  .
 Convergence analysis. We finally investigate the convergence of the CST model. Figure 4(d) shows the convergence analysis of the CST model on product-patent matching problem and cross-lingual matching problem. We see the CST model converges within 100 iterations ion both two tasks.
In this section, we demonstrate some examples generated from our experiments to show the effectiveness of the CST model. Figure 5: Examples of the correlations between topics, patents, and Wiki articles in the CST model.  X  , the probability of a topic give an entity, is represented on each black-solid edge. And the weight on each red-dotted edge denotes the likelihood of a matching relation. The titles of topics are hand-labeled. And for each topic, we separate the terminologies used in patents (the upper part of each topic box) and the terminologies used in Wiki articles (the lower part of each topic box). We remove some edges whose probabilities are negligible.
 Table 4: Examples of topics highly relevant to both Apple and Samsung found by the CST model. Top terminologies from each source are showed. The titles of topics are hand-labeled. Product-patent matching. Figure 5 shows a part of the matching results of  X  X acbook Pro X  Wiki article. We select 3 topics extracted by the CST model and display them with top words in both two sources. We also represent the probability of a specific topic z given an entity d (  X  z,d ), and the matching probability of two en-tities in the form of edges. As we can see from the figure, a patent mostly focus on one topic, a specific technology. And a Wiki arti-cle generally describe a number of features of a product. Thus Wiki articles have more diverse topic distributions.

When predicting a matching relation for two entities, the regres-sion part of the CST mode is able to distinguish relevant topics from others. As the figure shows, the CST mode successfully detects the Macbook Pro is matched with  X  X ide touchpad on a portable com-puter X  and  X  X isplay that emits circularly-polarized light X  respec-tively. Each of the two patents is associated with a topic relevant to Macbook Pro.
 Apple vs. Samsung. The CST model will be helpful to find the patents that a company uses to protect her products, by detecting the matching relations between products and patents. CST is also able to infer the inner connections between companies. Given a set of companies, we train the CST model by these companies X  patents and Wiki articles describing the companies X  products. And for a company g , we define its topic distribution as P ( z | g ) = where D g is the set of entities relevant to g . Here we use Apple and Samsung as an example. Table 4 lists the top three topics related with both Apple and Samsung. We also represent each topic X  X  top words from both Wiki articles and patents. We see that terminolo-gies related with technologies are more likely to appear in patents (e.g., recognition, range, etc.). And most terms closer to our lives and applications are from Wikipedia (e.g., video, iPad, etc.).  X  X ravity Sensing X  and  X  X ouchscreen X  are both highly related with the products of Apple and Samsung (e.g., smart phones, iPad, etc.), which indicates through the label information between patents and products, the CST model can identify the topics bridg-ing products and related technologies. Moreover,  X  X pplication Icons X  is also discovered by CST. As we know, one of the Ap-ple patents been violated by Samsung 7 , is the design patent 305: Rounded square icons on interface, which is related to this topic. It indicates that the results of CST may be helpful to infer the com-petitive relationships between companies. Cross-source matching. We first review some related work on cross-source matching problem. Wang et al. [29] study the cross-lingual knowledge linking problem. They aim to link Chinese and English Wiki articles which report on the same content. However, the model they proposed, called LFG, only considers the structural information. In this paper, we utilize our proposed model to bring in content information to LFG. We conduct a similar experiment with Wang et al. and the result shows that the performance is signif-icantly improved. Mimno et al. [18] has studied a similar problem. They propose a polylingual topic model to discover topics aligned across multiple languages. Tang et al. [26] propose a method called Cross-domain Topic Learning. Their goal, which is to recommend cross-domain collaborations, is different from ours. More impor-tantly, their method separates topic extraction and link prediction into two models. Our model integrates topic modeling and entity matching into a unified model.

Besides, Barnard et al. [2] propose an approach for modeling segmented images with associated text simultaneously. However, their approach do not integrate entity matching and topic modeling into a uniform framework.
 Topic modeling. It is natural to apply topic modeling (e.g., LDA [4] and PLSA [11]) on a collection of documents, and use the derived topic distribution to represent each document. The basic mechanism behind these models is to exploit co-occurrence pat-terns of words in documents to find K semantically meaningful topics and best describe the given corpus. However, both PLSA and LDA treat documents in a given corpus independently. To deal with the pairwise information of documents, Cohn and Hoffman [10] build an extension to the PLSA model, which is called PHITS. A similar model called mixed membership model is developed by Erosheva et al. [9]. Mei et al. [17] add a regulariza-tion constraint on a prior knowledge that some pairs of documents should be similar, to the traditional topic models. Dietz et al. [8, 16] also propose similar methods. These approaches regard links as input data, whereas in our work, the proposed model is able to infer the unknown relations between documents from different sources. http://www.businessinsider.com/apple-versus-samsung-2012-8
To integrate supervised information, Blei et al. propose a method called Relational Topic Model (RTM) [7], which models the links of each pair of documents as a binary random variable that is con-ditioned on their contents. Nallapati et al. [20] propose a model combines the ideas of LDA and Mixed Membership Block Stochas-tic Models [1] and allows modeling arbitrary link structure. They also propose another model [21], which assumes the link struc-ture is a bipartite graph and combines the LDA and PLSA into a single graphical model. Blei et al. [5] introduce supervised LDA and use it to predict ratings for movies. All these models intro-duced above depend on the co-occurrence of terms, whereas mul-tiple source documents (entities) share few terms. Thus they can hardly deal with corpus from different sources. By cross-sampling, our proposed model is able to extract cross-source topics and infer matching documents (entities) from different sources.
In this paper, we propose an approach to solve the problem of entity matching across heterogeneous sources. The model we proposed is named as the Cross-Source Topic model, which inte-grates the topic extraction and entity matching into a unified frame-work. A semi-supervised learning algorithm is proposed to learn the model. We validate the model on two real scenarios. The ex-perimental results demonstrate that the proposed model can exten-sively improve the performance compared with baseline methods (+19.8% and +7.1% in two scenarios respectively). The proposed model mainly considers the text content information of entities. Meanwhile, the model is easy to be plugged into other frameworks which can leverage both the content and structural information to-gether. We give two examples to show how the proposed model can be plugged into a random walk framework and a factor graph respectively.
 Acknowledgements. The work is supported by the National High-tech R&amp;D Program (No. 2014AA015103), National Ba-sic Research Program of China (No. 2014CB340506, No. 2012CB316006), Natural Science Foundation of China (No. 61222212), National Social Science Foundation of China (No. 13&amp;ZD190), NSF CAREER Award (No. 1453800), NSFC-ANR (No. 61261130588), and a research fund supported by Huawei Inc. [1] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing. [2] K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D. Blei, [3] K. Bellare, S. Iyengar, A. G. Parameswaran, and V. Rastogi. [4] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. [5] D. M. Blei and J. D. McAuliffe. Supervised topic models. [6] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support [7] J. Chang and D. Blei. Relational topic models for document [8] L. Dietz, S. Bickel, and T. Scheffer. Unsupervised prediction [9] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed-membership [10] D. C. T. Hofmann. The missing link-a probabilistic model of [11] T. Hofmann. Probabilistic latent semantic indexing. In [12] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An [13] J. Li, J. Tang, Y. Li, and Q. Luo. Rimom: A dynamic [14] H.-A. Loeliger. An introduction to factor graphs. Signal [15] L. Lov X sz. Random walks on graphs: A survey.
 [16] A. McCallum, A. Corrada-Emmanuel, and X. Wang. Topic [17] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic modeling with [18] D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and [19] T. Minka. Estimating a dirichlet distribution. Technical [20] R. Nallapati, A. Ahmed, E. Xing, and W. Cohen. Joint latent [21] R. Nallapati and W. Cohen. Link-plsa-lda: A new [22] D. Rinser, D. Lange, and F. Naumann. Cross-lingual entity [23] W. Shen, J. Wang, P. Luo, and M. Wang. Linking named [24] P. Sorg and P. Cimiano. Enriching the crosslingual link [25] J. Tang, B. Wang, Y. Yang, P. Hu, Y. Zhao, X. Yan, B. Gao, [26] J. Tang, S. Wu, J. Sun, and H. Su. Cross-domain [27] H. Tong, C. Faloutsos, and J. Pan. Fast random walk with [28] M. Wainwright and M. Jordan. Graphical models, [29] Z. Wang, J. Li, Z. Wang, and J. Tang. Cross-lingual [30] J. Winn. Variational message passing and its applications.
