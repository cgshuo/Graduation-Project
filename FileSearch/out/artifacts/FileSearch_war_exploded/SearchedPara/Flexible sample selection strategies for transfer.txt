 1. Introduction to use its abundant data for its English-speaking market to bootstrap a ranker for its Chinese market. datasets both with relevance judgment labels are available: (A) Small dataset for target domain of interest (e.g. Chinese market). (B) Large dataset for another domain, which we term  X  X  X ource domain X  X  (e.g. English market). contains labels; here we use the general term  X  X  X ransfer learning. X  X  tance, but also practical ramifications in terms of building better systems with lower cost.  X  cycles.
 ratio estimation ( Sugiyama et al., 2008 ).
 we conclude with discussions about related work (Section 3 ) and future directions (Section 6 ). 2. Sample selection for ranking 2.1. Notation judgment labels. Specifically, let q i be a query and X i
An annotator provides a relevance label to each query-document pair, forming the vector y ferred to as a  X  X  X ample X  X  and indicates the desired ranking of a list of documents, given a particular query.
We have two datasets in transfer learning. The target domain training data consists of N
The source domain training data consists of N s samples S as index for the samples and the superscripts t / s to indicate target and source domain. tween w t and query-document pair feature vector. In the transfer setting, the dataset S rated. Our proposed transfer learning strategy amounts to selecting a subset of samples in S ranker w t + s based on the combined dataset. Generally, S 2.2. Intuition
The issue with a small target dataset S t is that the observed q t samples, but the accuracy suffers if the dataset is simply too small.
The idea of our sample selection strategy is to increase the coverage by adding samples from source domain S different from the true target domain function (initially approximated by w  X  X  X unctional change X  X  assumption in transfer learning ( Jiang, 2008 ).  X  X  X elatedness X  X  is to note that a sample-specific ranker w the functional relationship between X i and y i . We assume that two samples are more related if their w 2.3. Algorithm training on single queries is extremely fast.
 , we estimate the ratio: where p t ( ) is the probability density function for target domain weights, and p source domain weights. In the example of Fig. 1 , we can thus imagine two distributions in R
Rank &lt; 2, p t ( ) is likely to be smaller than p s ( ), leading to a small ratio r s domain.
 main data. This is done by using a threshold a and selecting sample i if r s 17 X 20). 2 Finally we output the ranker trained on this target and best subset of source data.
In practice, the ratio r s i is not computed by estimating p rectly computes r s i ( Sugiyama et al., 2008 ). First, we define a function r combination of kernels r s  X  w  X  X  P N t j  X  1 c j k w ; w t and a weighted source domain distribution r s ( w ) p s ( w ): (1) . We use Gaussian kernels k w ; w t j  X  exp w w t j = 2 efficiently optimized by a convex program, with additional constraints c 1  X 
R are used to compute the ratio r s i for each source domain sample.
 on whether the sample-specific ranker occurs in a region where
The reader may wonder why we chose samples with high ratios Algorithm 1. Transfer Learning in Ranking by Sample Selection Input: Target training data: S t  X  q t i ; X t i ; y t i i  X  1 ; ... ; N t
Input: Target development data: S v  X  q v i ; X v ; y v i  X  1 ; ... ; N
Input: source domain training data: S s  X  q s i ; X s i ; y
Output: Ranker w t + s 1: for i =1, ... , N t do 2: w t i = RANK-LEARN q t i ; X t i ; y t i # compute query-specific rankers 3: end for 4: for i =1, ... , N s do 5: w s i = RANK-LEARN q s i ; X s i ; y s i # compute query-specific rankers 6: end for 8: b =0 9: for a =0, ... , max( r s ) do 11: for i =1, ... , N s do 12: if r s i P a then 13: S t  X  s S t  X  s ; q s i ; X s i ; y s i # select source subset 14: end if 15: end for 16: ^ w = RANK-LEARN( S t + s ) # train overall ranker 17: ^ b = ACCURACY  X  ^ w ; S v  X  18: if ^ b P b then 19: b  X  ^ b ; w t  X  s  X  ^ w # choose overall ranker based on development set 20: end if 21: end for 22: Return w t + s 3. Related work tree structure).
 (and feature id f is the same value as feature id f + 700, for 1 active (and feature id f + 700 equals feature id f + 1400, for 1 the samples (this work).
 tween labeled and unlabeled features X , following the covariate shift assumption. ing samples based on query ( q i ) similarity and Banerjee et al. (2009) does so based on document ( X the functional relationship, w i , which is derived from X ratio estimation to explicitly model the functional change assumption in transfer learning. that these are two different approaches to address the same functional change assumption. 4. Experiment setup 4.1. Datasets where target training data is artificially reduced while source domain data is held fixed. erogeneous source domain data. Source domain data is 3.3 times the size of target data. that LETOR has fewer target queries (45) compared to the Yahoo dataset (1012), but more documents per query. 4.2. Evaluation metric This is a popular evaluation metric for information retrieval: y various positions. We treat NDCG@10 as the main evaluation criterion. 5. Results 5.1. Main results ues that correspond to the 50, 60, 70, 80, 90 percentiles of r {0.01,0.1,1,10,100}. All results show averaged 5-fold results on the test set.
We compare the following domain adaptation methods: source samples by comparing densities in function space. amount of samples to select is determined by cross-validation, similar to the proposed method. (Chen et al., 2008; Daume, 2007).
 erence, we also have the following simple baselines: 1. Target-only : Training on only target data (non-transfer) 2. Source-only : Training on only source domain (mismatch condition) 3. Combined Data : Training on combined target and source data 60, respectively.) according to the t -test, with p &lt; 0.05.
 The proposed method also outperforms the competing domain adaptation methods SampleSelect (Label) and Feature tions and ERR metric shows similar trends.
 and (2) the target training data is sufficiently small so that any extra data may be beneficial. Duplication . It is therefore a robust approach to transfer learning. 5.2. Detailed analysis
We performed additional experiments to analyze our method in more detail, focusing on the Yahoo dataset: 5.2.1. How does the method work when amount of target training data varies? cially good for larger training datasets. 5.2.2. How important is the density ratio estimation step? gives good results, or if density ratio estimation is important.
 identify outliers in the source domain data. 5.2.3. How does the KL method compare with other density ratio estimation methods? allow for model selection, out-of-sample extension, and relatively fast computation. logistic regression classifier using the set of target weights, e.g. { w { w for inclusion into the ranker training set.
 skewed to 0 for the source samples. 5.2.4. What happens when the number of documents per query is reduced? the subset of samples selected by our method.
 which is relatively close to the original Sample Select (Function) result in Table 2 . 6. Discussions and conclusions 6.1. Summary
The contributions of this work are twofold: pared to state-of-the-art baselines. 2. A specific method based on density ratio estimation on sample-specific rankers w ranking.
 learning process. 6.2. Limitations and extensions kernel k ( w i , w j ) = exp ( k w i w j k /2 r ) to measure the similarity between rankers. between rankers. The criteria for such a kernel is: PRED( w of queries, and use the L 2 distance between these prediction vectors to measure similarity. References
