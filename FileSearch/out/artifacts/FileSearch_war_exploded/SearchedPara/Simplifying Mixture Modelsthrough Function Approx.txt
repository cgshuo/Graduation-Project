 by a mixture of simple parametric functions  X  ( )  X  X  as f ( x ) = P n parameter for the j th component, and the mixing parameters  X  digits.
 plified mixture models. By adopting the L the sample size and dimensionality.
 Given a mixture model we assume that the j th component  X  with weight  X  the kernel function, K where r = xH  X  1 x . Our task is to approximate f with a simpler mixture model with m  X  n , where each component g with weight w an upper bound of E . Consider the L the mixture components {  X  see that the approximation error E is bounded by E = Denote this upper bound by E = m P m Note that E is the sum of the  X  X ocal X  approximation errors E representative w basic algorithm proceeds as follows: 1. (Section 2.1.1) Partition the set of mixture components (  X 
Let S 2. (Section 2.1.2) For each cluster, approximate the local mix ture model P component w 3. The simplified model g is obtained by g ( x ) = P m These steps will be discussed in more detail in the following sections. 2.1 Procedure 2.1.1 Partitioning of Components simple but highly efficient partitioning method called sequential sampling (SS): 1. Randomly select a  X  2. For all the components ( j = 1 , 2 , . . . , n ), do the following 3. Terminate when all the components have been processed. This procedure partitions the components by choosing those  X  refine the partition, i.e., find the best representative R to the closest representative R 2.1.2 Local Approximation In this part, we consider how to obtain a good representative , w The task is to determine the unknown variables w the upper bound (6) of the local approximation error can be wr itten as Here, C dependent constant (irrelevant to the unknown variables), and r To minimize E the relations among these three parameters. First, observe that E Therefore, given  X  H The remaining task is to minimize E min where This is an iterative contraction mapping. If  X  H t  X  H where In summary, we first initialize and then iterate (8) and (9) until convergence. The converge d values of t into  X  2.2 Complexity the complexity is l P m practice, we can enforce a diagonal structure on the covaria nce matrix  X  H l )) 2.3 Remarks estimator [11], where all  X  Equation (9) then reduces to where It shows that the bandwidth  X  H original kernel density estimator, and the covariance V matrix  X  Then  X  Moreover, h the same position (i.e., V the larger is h In comparison, our covariance term V that our choice of  X  H this coincidence naturally indicates the robustness of the L over, note that the adjusting matrix  X  Second, in determining the center of g p ( x ) = | H +  X  H i |  X  1 2 P j approximate originally is the local density f (with H = h 2 , and  X  H It appears intriguing that on fitting a kernel density f one needs to locate the maximum of another density function p f ( x ) itself or simply, the mean of the sample set { x j } j  X  S choices coincide when the distribution of S Intuitively, when the data is asymmetric, the center t the data distribution. The maximum of f the mean of S p heavier side of the distribution compared with that of f controlled by the mean shift iterations in (12).
 shows the histogram of a local cluster S ure 1(b) plots the corresponding approximation error E parameter, w much lower. R algorithm on the simplified density model. 3.1 Simplifying Nonparametric Density Models mixtures, f = P n where  X  ( j ) is the function that maps each component  X  g by [5]. We can see that under the L about 99 . 35% of that by [5]. 3.2 Image Segmentation identify arbitrarily-shaped clusters in the feature space . then apply the iterative mean shift procedure on the simplifi ed model g ( x ) . of an image is subjective, so only a visual comparison is inte nded here. computations associated with the large number of kernels.
