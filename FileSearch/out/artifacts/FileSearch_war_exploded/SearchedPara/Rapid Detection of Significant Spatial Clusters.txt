 Given an N N grid of squares, where each square has a count c and an underlying population p i j , our goal is to find the rectangular region with the highest density , and to calculate its significance by randomization. An arbitrary density function D , dependent on a re-gion X  X  total count C and total population P , can be used. For exam-ple, if each count represents the number of disease cases occurring in that square, we can use Kulldorff X  X  spatial scan statistic D find the most significant spatial disease cluster. A naive approach to finding the maximum density region requires O ( N 4 ) time, and is generally computationally infeasible. We present a multiresolu-tion algorithm which partitions the grid into overlapping regions using a novel overlap-kd tree data structure, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufficiently dense regions, this method finds the maximum density region in O (( N log N ) 2 ) time, in practice resulting in significant (20-2000x) speedups on both real and simulated datasets.
 H.2.8 [ Database Management ]: Database Apps-Data Mining Algorithms Cluster detection, spatial scan statistics, biosurveillance
One of the core goals of data mining is to discover patterns and relationships in data. In many applications, however, it is impor-tant not only to discover patterns, but to distinguish those patterns that are significant from those that are likely to have occurred by chance. This is particularly important in epidemiological appli-cations, where a rise in the number of disease cases in a region may or may not be indicative of an emerging epidemic. In order to Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. decide whether further investigation is necessary, epidemiologists must know not only the location of a possible outbreak, but also some measure of the likelihood that an outbreak is occurring in that region. More generally, we are interested in spatial data min-ing problems where the goal is detection of overdensities : spatial regions with high scores according to some density measure. The density measure can be as simple as the count (e.g. number of dis-ease cases, or units of cough medication sold) in a given area, or can adjust for quantities such as the underlying population. In addition to discovering these high-density regions, we must perform statisti-cal testing in order to determine whether the regions are significant. As discussed above, a major application is in detecting clusters of disease cases, for purposes ranging from detection of bioterrorism (e.g. anthrax attacks) to identifying environmental risk factors for diseases such as childhood leukemia [8, 11, 6]. [5] discusses many other applications, including mining astronomical data (e.g. identi-fying star clusters), military reconnaissance, and medical imaging.
We consider the case in which data has been aggregated to a uni-form, two-dimensional grid. Let G be an N N grid of squares, where each square s i j 2 G is associated with a count c underlying population p i j . For example, a square X  X  count may be the number of disease cases in that geographical location in a given time period, while its population may be the total number of people  X  X t-risk X  for the disease. Our goal is to search over all rectangular regions S G , and find the region S with the highest density ac-cording to a density measure D : S = arg max S D ( S ) . We use the abbreviations mdr for the maximum density region S , and mrd for the maximum region density D ( S ) , throughout. We will also find the statistical significance ( p -value) of this region by randomization testing, as described below.

The density D ( S ) of a region S can be an arbitrary function of the total count of the region, C ( S ) =  X  S c i j , and the total popula-tion of the region, P ( S ) =  X  S p i j . Thus we will often write D where C and P are the count and population of the region under consideration. It is important to note that, while the term  X  X en-sity X  is typically understood to mean the ratio of count to popula-tion, we use the term in a much broader sense, to denote a class of density functions D which includes the  X  X tandard X  density func-tion D 1 ( C ; P ) = C P . For our purposes, we assume that the density function D satisfies the following three properties: 1. For a fixed population, density increases monotonically with 2. For a fixed count, density decreases monotonically with pop-3. For a fixed ratio C P , density increases monotonically with The first two properties state that an overdensity is present when a lar ge count occurs in a small population. In the case of a uniform population distrib ution, the population of a region is proportional to its area, and thus an overdensity is present when a lar ge count occurs in a small area. The third property states, in essence, that an overdensity is more significant when the underlying population is lar ge. This is true because smaller populations will have higher variance in densities. Ho we ver, we also allo w D to remain constant as population increases for a fix ed ratio C P , thus including the stan-dard density function; we do not, howe ver, allo w functions where D decreases in this case. In our discussion belo w, we will also mak e one more assumption involving the second partials of D ; this fourth property is not strictly necessary but mak es our computation eas-ier (eliminating the need to check for local maxima of the density function). A lar ge class of functions satisfy all four properties, in-cluding Kulldorf f X  X  spatial scan statistic, discussed in detail belo w.
This work builds on our pre vious work on detection of spatial overdensities [7]. The primary dif ference is in the statement of the problem: the goal of the present work is to detect the most signifi-cant rectangular region, as opposed to the most significant squar e region. This extension is extremely important in epidemiological applications because disease clusters are often elong ated: airborne pathogens may be blo wn by wind, creating an ellipsoid  X  X lume,  X  and waterborne pathogens may be carried along the path of a river. In each of these cases, the resulting clusters have high aspect ra-tios, and a test for squares (or circles, as in Kulldorf f X  X  original scan statistic) will have low power for detecting the overdensity . While our discussion belo w focuses on finding  X  X xis-aligned X  rectangular regions, it can be easily extended to find rectangular regions which are not aligned with the coordinate axes. One simple method of doing this is to examine multiple  X  X otations X  of the data, mapping each to a separate grid and computing the maximum density region for each grid. In this case, we must also perform the same rotations on each replica grid, and thus the comple xity of the algorithm is multiplied by the number of rotations. Ho we ver, if we have infor -mation about rele vant conditions such as wind direction or the flo w of a river, we can use this information to align the coordinate axes, reducing or avoiding the need to examine multiple rotations.
While this change from square to rectangular regions has little effect on the underlying statistics, it creates a much more dif ficult computational problem. While the maximum density square region can be found nai vely in O ( N 3 ) time for an N N grid, finding the maximum density rectangular region requires O ( N 4 ) , and thus is computationally infeasible for even moderately sized grids. Our solution is similar to our pre vious work in that we propose a mul-tiresolution partitioning algorithm: we divide the grid into overlap-ping regions, bound the maximum score of subre gions contained in each region, and prune regions which cannot contain the maximum density region. Within that general frame work, howe ver, there are major dif ferences in our multiresolution data structure and the re-sulting algorithm. Our current  X  X enter -based X  approach (using a novel  X  X verlap-kd tree X  data structure) allo ws us to achie ve tighter upper bounds on the score of a region, allo wing much more prun-ing to tak e place. As a result, our algorithm gives huge speedups (as compared to the nai ve approach) without relying on approxima-tion; the non-approximate version of our pre vious algorithm only resulted in speedups when the maximum density region was suf-ficiently dense, and actually performed slo wer than nai ve in some cases. Our current algorithm is also more rob ust: while the pre-vious algorithm was severely slo wed by non-uniform populations, these are sho wn to have little or no effect on the current method.
Various other methods for finding  X  X ense clusters X  have been proposed in the data mining literature, including grid-based hierar -chical methods such as CLIQ UE [1], MAFIA [3], and STING [12]. Our work dif fers from these in three main ways: most importantly , as discussed abo ve, our goal is not only to find the highest scor -ing cluster , but to determine the statistical significance of that clus-ter (whether it is a true overdensity , or if it is lik ely to have oc-curred by chance). Second, our method deals with non-uniform underlying populations: this is particularly essential for real-w orld epidemiological applications, in which an overdensity of disease cases is more significant if the underlying population is lar ge, and is also important in man y other applications where an  X  X verdensity X  is defined relati ve to some other statistic (e.g. population, baseline score, or covariate). Finally , our method is applicable to a wide class of density measures D , where the other algorithms are specific to the  X  X tandard X  density measure D 1 ( S ) = C ( S ) P is the ratio of count per unit population; for example, in epidemi-ology , maximizing D 1 corresponds to finding the region with the highest observed disease rate. Ho we ver, this is not generally the region we are interested in finding, since a region with a high dis-ease rate and very small population (e.g. one person, who happens to be sick) is not lik ely to be significant. In fact, the  X  X e gion X  with the highest D 1 will be the single square with highest c i j because D 1 density is monotonic : if a region S with D 1 partitioned into any set of disjoint subre gions, at least one subre-gion S 0 will have D 1 ( S 0 ) d . Thus the other algorithms, rather than maximizing D 1 , search for maximally sized regions with D greater than some threshold. There are two disadv antages to this approach: one is the dif ficulty of measuring statistical significance within this frame work. The other is that the algorithms rely hea v-ily on the monotonicity of the D 1 measure by first finding  X  X ense X  1 1 squares, then mer ging adjacent squares in bottom-up fash-ion. For a non-monotonic density measure such as Kulldorf f X  X , it is possible to have a lar ge dense region where none of its subre gions are themselv es dense, so bottom-up methods are not guaranteed to find the correct region. Here, we will optimize with respect to arbi-trary non-monotonic density measures, and thus require a dif ferent approach from CLIQ UE, MAFIA, or STING.
A non-monotonic density measure which is of great interest to epidemiologists is Kulldorf f X  X  spatial scan statistic [4], which we denote by D K . This statistic is in common use for finding signif-icant spatial clusters of disease cases, which are often indicati ve of an emer ging outbreak. Kulldorf f X  X  statistic assumes that counts c are generated by an inhomogeneous Poisson process with mean qp i j , where q is the underlying  X  X isease rate X  (or expected value of ). We then calculate the log of the lik elihood ratio of two possi-bilities: that the disease rate q is higher in the region than outside the region, and that the disease rate is identical inside and outside the region. For a region with count C and population P , in a grid with total count C tot and population P tot , we can calculate: if P &gt; statistic is individually most powerful for finding a single signifi-cant region of ele vated disease rate: for a fix ed false positi ve rate, and for a given set of regions tested, it is more lik ely to detect the overdensity than any other test statistic. Ag ain, we note that our al-gorithm is general enough to use any density measure, and in some cases we may wish to use measures other than Kulldorf f X  X . For instance, if we have some idea of the size of the maximum den-sity region, we can use the D r measure, D r ( S ) = C ( S with lar ger r corresponding to tests for smaller clusters. We have also deri ved a variant of the D r measure for normally distrib uted counts, where the cumulati ve statistics of a region are not its raw count and population, but a weighted average of squares X  z -scores. We are in the process of using this statistic to look for emer ging epidemics based on national sales of over-the-counter medications. Once we have found the maximum density region ( mdr ) of grid G according to our density measure, we must still determine the statistical significance of this region. Since the exact distrib ution of the test statistic is only kno wn in special cases (such as D sity with a uniform underlying population), in general we must find the region X  s p -value by randomization. To do so, we run a lar ge number R of random replications, where a replica has the same un-derlying populations p i j as G , but assumes a uniform disease rate q all counts c i j randomly from an inhomogeneous Poisson distrib u-tion with mean q rep p i j , then compute the maximum region density ( mr d ) of G 0 and compare this to mrd ( G ) . The number of replicas G 0 with mrd ( G 0 ) mrd ( G ) , divided by the total number of repli-cations R , gives us the p -value for our maximum density region. If this p -value is less than .05, we can conclude that the disco v-ered region is statistically significant (unlik ely to have occurred by chance) and is thus a  X  X patial overdensity . X  If the test fails, we have still disco vered the maximum density region of G , but there is not suf ficient evidence that this is an overdensity .
The simplest method of finding the maximum density region is to compute the density of all rectangular regions of sizes k where k min k 1 ; k 2 k max . 1 Since there are a total of 1 )( N k 2 + 1 ) regions of each size k 1 k 2 , there are a total of O (
N 4 ) regions to examine. We can compute the density of any region S in O ( 1 ) , by first finding the count C ( S ) P (
S ) , then applying our density measure D ( C ; P ) . 2 This allo ws us to compute the mdr of an N N grid G in O ( N 4 ) time. Ho we ver, significance testing by randomization also requires us to find the mrd for each replica G 0 , and compare this to mrd ( G ) . Since calcu-lation of the mrd tak es O ( N 4 ) time for each replica, the total com-ple xity is O ( RN 4 ) , and R is typically lar ge (we assume R As discussed in [7], several tricks may be used to speed up this procedure for cases where there is no significant spatial overden-sity , but these do not help in cases when an overdensity is found. In general, the O ( N 4 ) comple xity of the nai ve approach mak es it infeasible for even moderately sized grids: we estimate a runtime of 45 days for a 256 256 grid on our test system, which is clearly far too slo w for real-time detection of disease outbreaks.
While one alternati ve would be to search for an approximate so-lution using one of the variety of cluster detection algorithms in the literature, we present an algorithm which is exact (al ways finds the maximum density region) and yet is much faster than nai ve search. The key intuition is that, since we only care about finding the max-imum density region, we do not need to search over every single
We use k min = 3 and k max = N throughout.
An old trick mak es it possible to compute the count and popu-lation of any rectangular region in O ( 1 ) : we first form a matrix of the cumulati ve counts, then compute each region X  s count by adding/subtracting at most four cumulati ve counts, and similarly for populations. rectangular region: in particular , we do not need to search a set of regions if we can pro ve (based on other regions we have searched) that none of them can be the mdr . As a simple example, if a given region has a very low count, we may be able to conclude that no subre gion contained in that region can have a score higher than the mrd, and thus we do not need to actually compute the score of each subre gion. These observ ations suggest a top-do wn, branc h-and-bound approach: we maintain the current maximum score of the regions we have searched so far, calculate upper bounds on the scores of subre gions contained in a given region, and prune regions which cannot contain the mdr . Similarly , when we are searching a replica grid, we only care about whether the mrd of the replica is higher than the mrd of the original grid. Thus we can use the mrd of the original grid for pruning on the replicas, and can stop searching a replica if we find a region with score higher than this mrd.
Our top-do wn approach to cluster detection can be thought of as a multiresolution search of the space under consideration: we search first at coarse resolutions (lar ge regions), then at succes-sively finer resolutions (smaller regions) as necessary . This sug-gests that a hierarchical, space-partitioning data structure such as kd-trees [9], mrkd-trees [2], or quadtrees [10] may be useful in speeding up our search. Ho we ver, our desire for an exact solu-tion mak es it dif ficult to apply these data structures to our problem. In a kd-tree, each spatial region is recursi vely partitioned into two disjoint  X  X hild X  regions, each of which can then be further subdi-vided. The dif ficulty , howe ver, is that man y subre gions of the par -ent are not contained entirely in either child, but overlap partially with each. Thus, in addition to recursi vely searching each child for the mdr , we must also search over all of these  X  X hared X  regions at each level of the tree. 3 Since there are O ( N 4 ) shared regions even at the top level of the tree (i.e. regions partially overlapping both halv es of grid G ), an exhausti ve search over all such regions is too computationally expensi ve, and thus a dif ferent partitioning approach is necessary .

An initial step toward our partitioning can be seen by consider -ing two divisions of a rectangular spatial region S : first, into its left and right halv es (which we denote by S 1 and S 2 ), and second, into its top and bottom halv es (which we denote by S 3 and S 4 suming that S has size k 1 k 2 , this means that S 1 and S k 1 k 2 , and S 3 and S 4 have size k 1 1 2 k 2 . Considering these four (overlapping) halv es, we can sho w that any subre gion of S either a) is contained entirely in (at least) one of S 1 ::: S 4 , or b) contains the centroid of S . Thus one possibility would be to search S by exhaus-tively searching all regions containing its centroid, then recursing the search on its four  X  X hildren X  S 1 ::: S 4 . Ag ain, there are O  X  X hared X  regions at the top level of the tree (i.e. regions containing the centroid of grid G ), so an exhausti ve search is infeasible.
Our solution, as in our pre vious work [7], is a partitioning ap-proach in which adjacent regions partially overlap, a technique we call  X  X verlap-multiresolution partitioning,  X  or  X  X verlap-multires X  for short. Ag ain we consider the division of S into its left, right, top, and bottom  X  X hildren.  X  Ho we ver, while in the discussion abo ve each child contained exactly half the area of S , now we let each child contain mor e than half the area. We again assume that region S has size k 1 k 2 , and we choose fractions f 1 ; f 2 &gt; and S 2 have size f 1 k 1 k 2 , and S 3 and S 4 have size k
Note that an attempt to find the two  X  X ieces X  of the mdr , one in each child, and then mer ge the two, fails because of the non-monotonicity of the density measure: the mdr may have a higher score than either of its two pieces! partitioning (for f 1 = f 2 = 3 4 ) is illustrated in Figure 1. Note that there is a region S C common to all four children; we call this region the center of S . The size of S C is (( 2 f 1 1 ) k 1 ( 2 f thus the center has non-zero area. When we partition region S in this manner , it can be pro ved that any subre gion of S either a) is contained entirely in (at least) one of S 1 ::: S 4 , or b) contains the center region S C . Figure 1 illustrates each of these possibilities.
No w we can search S by recursi vely searching S 1 ::: S 4 searching all of the regions contained in S which contain the center S . Unfortunately , at the top level there are still O ( N 4 tained in grid G which contain its center G C . Ho we ver, since we kno w that each such region contains the lar ge region G C place very tight bounds on the score of these regions, often allo w-ing us to prune most or all of them. (W e discuss how these bounds are calculated in the follo wing subsection.) Thus the basic outline of our search procedure (ignoring pruning, for the moment) is:
No w we consider how to select the fractions f 1 and f 2 for each call of overlap-search, and characterize the resulting set  X  of re-gions S on which overlap-search( S ) is called. Re gions S called gridded regions , and regions S = 2  X  are called outer regions . For simplicity , we assume that the grid G is square, and that its size N is a power of two. We begin the search by calling overlap-search( G ). Then for each recursi ve call to overlap-search( S ), where the size of S is k 1 k 2 , we set f 1 = 3 4 if k 1 = 2 r for some inte ger r , and f 1 = 2 3 if k 1 = 3 2 r for some inte ger r . We define f tically in terms of k 2 , and then the child regions S 1 ::: center region S C are defined in terms of f 1 and f 2 as abo ve. This choice of f 1 and f 2 has the useful property that all gridded regions have sizes 2 r or 3 2 r for some inte ger r . For instance, if the orig-inal grid G has size 64 64, then the children of G will be of sizes 64 48 and 48 64, and the grandchildren of G will be of sizes 64 32, 48 48, and 32 64. This process can be repeated recur -Figur e 2: The first tw o levels of the overlap-kd tree. Each node repr esents a gridded region (denoted by a thick rectangle) of the entir e dataset (thin squar e and dots). sively down to regions of size k min k min , forming a structure that we call an overlap-kd tree . The first two levels of the overlap-kd tree are sho wn in Figure 2. Note that even though grid G has four child regions, and each of its child regions has four children, G has only ten (not 16) distinct grandchildren, several of which are the child of multiple regions.

Our overlap-kd tree has several nice properties, which we present here without proof. First, for every rectangular region S G , either S is a gridded region (contained in the overlap-kd tree), or there exists a unique gridded region S 0 such that S is an outer region of S 0 (i.e. S is contained in S 0 , and contains the center region of S 0 ). This means that, if overlap-search is called exactly once for each gridded region, and no pruning is done, then base-case-search will be called exactly once for every rectangular region S G . In practice, we will prune man y regions, so base-case-search will be called at most once for every rectangular region, and every region will be either searched or pruned. The second nice property of our overlap-kd tree is that the total number of gridded regions O (( N log N ) 2 ) rather than O ( N 4 ) . This implies that, if we are able to prune (almost) all outer regions, we can find the mdr of an N N grid in O (( N log N ) 2 ) time. In fact, we may not even need to search all gridded regions, so in man y cases the search will be even faster .
Before we consider how to calculate score bounds and use them for pruning, we must first deal with an essential issue in searching overlap-kd trees. Since a child region may have multiple parents, how do we ensure that each gridded region is examined only once, rather than being called recursi vely by each parent? One simple answer is to keep a hash table of the regions we have examined, and only call overlap-search( S ) if region S has not already been ex-amined. The disadv antage of this approach is that it requires space proportional to the number of gridded regions, O (( N log N spends a substantial amount of time doing hash queries and up-dates. A more ele gant solution is what we call lazy expansion : rather than calling overlap-search( S i ) on all four children of a re-gion S , we selecti vely expand only certain children at each stage, in such a way that there is exactly one path from the root of the overlap-kd tree to any node of the tree. One such scheme is sho wn in Figure 2: if the path between a parent and child is mark ed with an X , lazy expansion does not mak e that recursi ve call. No extra space is needed by this method; instead, a simple set of rules is used to decide which children of a node to expand. A child is expanded if it has no other parents, or if the parent node has the highest priority of all the child X  s parents. We give parents with lower aspect ratios pri-ority over parents with higher aspect ratios: for example, a 48 48 parent would have priority over a 64 32 parent if the two share a 48 32 child. This rule allo ws us to perform variants of the search Figur e 3: Maximizing count c in for a given population p Count must be less than C 1 ( p in ) , C 2 ( p in ) , and C where regions with very high aspect ratios are not included; an ex-treme case would be to only search for squares, as in our pre vious work. Within an aspect ratio, we fix an arbitrary priority ordering. Since we maintain the property that every node is accessible from the root, the correctness of our algorithm is maintained: every grid-ded region will be examined (if no pruning is done), and thus every region S G will be either searched or pruned.
We now consider which regions can be pruned (discarded with-out searching) during our multiresolution search procedure. First, given some region S , we must calculate an upper bound on the scores D ( S 0 ) for regions S 0 S . More precisely , we are interested in two upper bounds: a bound on the score of all subre gions S 0 S , and a bound on the score of the outer subre gions of S (those regions contained in S and containing its center S C ). If the first bound is less than or equal to the mrd, we can prune region S completely; we do not need to search any (gridded or outer) subre gion of S . If only the second bound is less than or equal to the mrd, we do not need to search the outer subre gions of S , but we must recursi vely call overlap-search on the gridded children of S . If both bounds are greater than the mrd, we must both recursi vely call overlap-search and search the outer regions.

No w we will explain the calculation of the second bound (on subre gions containing the center); the calculation of the first bound (on all subre gions) can be treated as a special case where the pop-ulation, count, and area of the center are zero. We begin by assum-ing as kno wn various pieces of information about the subre gions of S ; we discuss belo w how these are obtained. This information includes: upper and lower bounds p max , p min on the population of subre gions S 0 ; an upper bound d max on the D 1 density of S 0 ; an up-per bound d inc on the D 1 density of S 0 S C ; and a lower bound d on the D 1 density of S S 0 . We also kno w the count C and popu-lation P of region S , and the count c cen ter and population p region S C . Let c in and p in be the count and population of S 0 ; these are presently unkno wn. To find an upper bound on D ( S 0 calculate the values of c in and p in which maximize D ( c ject to the given constraints: This potentially dif ficult maximization problem could be solv ed by con vex programming, but is made much easier by the properties of Figur e 4: Division of region S into lay ers of differing density . In the typical case, subr egion S 0 includes all but the outer lay er. the density function D . Since  X  D  X  C 0, we kno w that the maximum value of D for a given p in occurs when c in is maximized subject to the constraints. We solv e the first three constraints for c us c in = min ( C 1 ; C 2 ; C 3 ) , where: In the typical case, 4 we have d min d max d inc , B 1 &gt; 0: this means that c in = C 1 for small p in , c in = C 2 and c in = C 3 for lar ge p in , as illustrated in Figure 3. Thus we can solv e for the intersection points P 12 , P 13 , and P 23 for p in P i j , and we use these quantities to find the maximum allo wable count c in for a given p in . Solving the equations, we find that P 12 = B 1 d typical case, 5 we have 0 &lt; P 12 P 13 P 23 &lt;  X  . In this case, we use the values of P 12 and P 23 , and the value P 13 is not needed. Then the count c in = c cen ter + d inc ( p in p cen ter ) for p c P 23 ) for p in P 23 . This is illustrated by Figure 4: the region S is separated into four  X  X ayers X  of dif fering densities. Starting from the inside, we have the center (with a kno wn population p count c cen ter ), a layer of high D 1 density d inc , a layer of moderate D 1 density d max , and a layer of low D 1 density d min .

No w we can write c in as a function of p in , and thus the score D becomes a function of the single variable p in . Where does the max-imum of this function occur? Ag ain we rely on properties of the function D ( C ; P ) , and a case-by-case analysis is necessary . In the with population in the  X  X igh density X  and  X  X oderate density X  lay-ers. This follo ws from two properties of our density function: 0 and  X  D  X  P + C P  X  D  X  C 0. In the high density layer , the D tion, so the score D is monotonically increasing with population. In the moderate density layer , the D 1 density of S 0 stays constant (at d max ) as population increases, so again D is monotonically increas-ing. In the low density layer , D 1 density of S 0 decr eases as popu-lation increases: in this case, since count and population are both
We must also handle a variety of special cases where one or more of these inequalities are violated, and some constraints may not be rele vant. We omit the details of this case-by-case analysis.
See pre vious note. increasing, the score may increase or decrease. We assume that the score function D has no local maxima in the interv al ( P thus that the maximum occurs either at ( c in ; p in ) = ( or at ( c in ; p in ) = ( C ; P ) . 6 We are only interested in finding sub-regions with scores higher than the parent, so we can ignore the latter case. Thus our upper bound on D ( S 0 ) is D ( c in ; p in = P 23 and c in = d max p in . The various special cases, where one or more of the inequalities abo ve are violated, are handled simi-larly using the intersection points P 12 , P 13 , and P 23 We also must adjust our value of p in if it violates the inequality p min p in p max , adjusting c in accordingly given the density of the layers being added or subtracted.

We now consider how the bounds on populations and D 1 densi-ties are obtained. The simplest method of doing so is to use global values: first, we precompute the minimum and maximum popu-lation and D 1 density of all single squares s i j in the grid. This gives us usable (though very conserv ative) values for d min and d inc . We can also use the minimum and maximum square pop-ulations, together with the minimum and maximum area of a re-gion, to obtain bounds p min and p max . Slightly less conserv ative bounds can be obtained using our assumption of a minimum region size k min = 3. An y k 1 k 2 region, where k 1 ; k 2 3, can be tiled with rectangles of sizes 3 k 1 ; k 2 5. Thus we can precompute the minimum and maximum D 1 density and population per square of all such rectangles in O ( N 2 ) time, and use these rather than the single square bounds when allo wable. For example, when bound-ing maximum score of the outer regions of S , we can use the less conserv ative bound for d max ; when bounding maximum score of all subre gions of S , we can also use the less conserv ative bound for d
These global bounds on populations and D 1 densities are ine x-pensi ve to compute (we need only compute them once per grid), but are very conserv ative estimates of the densities of squares in a given region. We use these bounds in our algorithm as a sort of  X  X irst pass X  which prunes man y regions but also lea ves man y un-pruned. If a region survi ves this round of pruning, we compute much tighter bounds on subre gion scores in a  X  X econd pass,  X  which is also more computationally expensi ve. To do so, we obtain tighter bounds on the population and D 1 density of S 0 using a novel tech-nique we term quartering , then use these constraints to bound D as abo ve.
 Given a region S of size k 1 k 2 , with a (non-zero) center region S , the quartering procedure calculates bounds on subre gion popu-lation and D 1 density in O ( k 1 k 2 ) time. The first step of quartering
Formally , we assume the follo wing constraint on the first and sec-ond partials of D : D 2 P D CC + D 2 C D PP 2 D C D P D CP for a lar ge class of functions, including Kulldorf f X  X  statistic. If this constraint is violated, we must also calculate D ( C ; P ) maximum, which is not dif ficult if the number of maxima is small and each maximum is easy to calculate. is to divide S into its four (non-o verlapping) quadrants S in Figure 5. We now consider each S i separately , together with the quarter of the center ( S Ci ) which overlaps that quadrant. For each quadrant, we consider all rectangles S 0 i with one corner at the cen-troid of S , and one corner outside S Ci (i.e. on one of the dots in Figure 5). Note that there are O ( k 1 k 2 ) such rectangles, and thus we can search over all of these regions S 0 i in quadratic time. Our search procedure is very simple: given a region S 0 i , let p and A in denote its population, count, and area; let p out A out denote the population, count, and area of S i S 0 i ; and let p c di f , and A di f denote the population, count, and area of S 0 We then calculate the D 1 density d and the average population per square p s for each of S 0 i , S i S 0 i , and S 0 i S Ci and the other quantities ( d out , d di f , p s ; out , p larly . We then set d max equal to the maximum of all d in to the maximum of all d di f , and d min equal to the minimum of all d out . Similarly , we tak e the minimum and maximum values of p in , p s ; out , and p s ; di f ; we can use these to calculate bounds p and p max once we are given the minimum and maximum area of S 0 . In essence, what we have done is bounded the D 1 densities and populations for the piece of region S 0 contained in each quadrant. Then since D 1 density is monotonic, we kno w that the D 1 of the entire region S 0 is bounded by the maximum of the max-densities and the minimum of the min-densities computed for all regions S 0 i . Population per square is also monotonic, so an identical argument applies. Another way to think of this is that we are cal-culating bounds on population and D 1 density for all the irre gular (but rectangle-lik e) regions containing the center C S and consist-ing of one rectangle in each quadrant, as dra wn in Figure 5; then these quantities are also bounds on the population and density of all rectangles which contain C S .

We do not pro vide a formal proof here, but we note that the bounds on population and density deri ved by quartering are ex-act (i.e. no rectangle S 0 S , such that S C S 0 , can have density or population outside these bounds) and that the y are much tighter than the global population and density bounds, allo wing man y more regions to be pruned. Ho we ver, as noted abo ve, quartering is sig-nificantly more computationally expensi ve than using the global bounds, taking time quadratic in the size of region S , and thus O (
N 2 ) for lar ge regions. This is wh y we first use the global bounds for pruning outer regions, and only use quartering on regions that this initial pruning does not eliminate. We also note that quarter -ing can be done in linear time in the special case where the parent region S and its center S C have the same row size k 1 or the same column size k 2 ; in this case we need only divide the region into two halv es, and each half can be searched linearly and the bounds com-bined. We apply this linear -time  X  X alving,  X  as well as the standard quadratic-time quartering, in our algorithm presented belo w.
We now possess all of the algorithmic and statistical tools needed to present our algorithm in full. The basic structure is similar to the top-do wn  X  X verlap-search X  routine presented abo ve, with sev-eral important dif ferences. First, we use a best-first search (im-plemented using a pair of priority queues q 1 and q 2 ) rather than a recursi ve depth-first search. Our algorithm has two stages: in the first stage we examine only gridded regions, and in the second stage we search outer regions if necessary . In both stages, we prune re-gions whene ver possible, calculating increasingly tight bounds on subre gions X  population and D 1 density , and using these to calcu-late upper bounds D max on D ( S 0 ) as abo ve. The first stage of our algorithm proceeds as follo ws, using the (loose) global bounds on population and D 1 density to calculate D max :
Thus, after the first stage of our algorithm, we have searched or pruned all gridded regions (requiring at most O (( N log N and the current mdr is the gridded region with highest D ( now contains the subset of gridded regions whose outer regions have not yet been pruned, prioritized by their upper bounds D The second stage of our algorithm proceeds as follo ws:
No w the only question left is how to perform the search-outer -regions procedure. We first note that a rectangular region requires four coordinates for specification: we use the row size k umn size k 2 , the minimum row x 1 , and the minimum column x Then a nai ve search of the outer regions of S could be done using four nested loops, stepping over each legal combination of these four coordinates (i.e. such that the resulting region S 0 is in S and contains S C ). We also use four nested loops (in the order k k , x 2 ), but tak e several more opportunities for pruning. Once we have fix ed k 1 ( S 0 ) and x 1 ( S 0 ) , we can obtain a very tight bound on D max ( S 0 ) by expanding the center region S C and contr acting the parent region S such that k 1 ( S ) = k 1 ( S C ) = k 1 ( x (
S C ) = x 1 ( S 0 ) . We then recalculate bounds on the D and population using the new S and S C (this can be done in linear time using  X  X alving,  X  the special case of quartering, since the par -ent and center have the same k 1 ), and finally recompute D the new parent and center . Only if D max is greater than the mrd do we need to loop over k 2 and x 2 for that combination of k x . Finally , once we have fix ed k 2 , we can recompute D since we now precisely kno w the area of S 0 , giving us much tighter population bounds. Only if D max is greater than the mrd must we search all x 2 for that combination of k 1 , x 1 , and k 2
Thus the second stage of our algorithm can be seen as a series of  X  X creens X  that an outer region must pass through if it is to be searched. The first screen is whether the parent region is tak en off q and examined, the second screen is whether the parent region passes the  X  X uartering X  test, the third screen is whether the new parent region (formed after k 1 and x 1 are fix ed) passes the  X  X alving X  test, and the fourth screen is whether the new parent region passes the  X  X alving X  test once the area of S 0 is fix ed. We now examine the comple xity of this procedure, given a lar ge parent region (i.e. one containing O ( N 4 ) outer regions S 0 ). If the parent region does not pass the first screen, we have spent only O ( 1 ) to search these O regions; if the parent does not pass the second screen, we have spent only the O ( N 2 ) time required by quartering. If the parent passes the second screen, but none of the new parent regions pass through the third and fourth screens, we have spent only O ( N 2 ) O ( halving, given each k 1 and x 1 ) + O ( N 3 ) (for bounding, given each k , x 1 , and k 2 ) = O ( N 3 ) time. Thus only if all four screens fail will the algorithm have O ( N 4 ) comple xity; typically well over 90% of regions are eliminated at each screen, and thus we search only a small fraction of possible regions.
As opposed to our pre vious results [7], the algorithm presented abo ve gives lar ge speedups as compared to the nai ve approach with-out approximation: the algorithm is guaranteed to find the max-imum density rectangular region. In some cases, howe ver, very rapid detection may be more important than guaranteed accurac y; thus we present an approximate version of the algorithm which finds the mdr 5-20x faster while maintaining over 90% accurac y.
As noted abo ve, the  X  X irst pass X  of our algorithm uses very con-serv ative bounds on the D 1 densities of S 0 , S 0 S C , and S S 0 , de-rived from the global minimum and maximum density values. Thus one way to increase the speed of the algorithm is to use a closer approximation of these densities as a bound. The disadv antage is that if we use an estimate which is not guaranteed to bound the densities, we may underestimate the score of a region, and hence possibly prune away the mdr and find an incorrect region. Here we consider an approximate lower bound on the D 1 density of S S 0 : using this bound instead of a guaranteed but much more conserv a-tive bound typically results in lar ge speedups with minimal loss of accurac y.

To deri ve tighter bounds on the maximum density of a subre-gion S 0 contained in a given region S , we consider the assumptions being made by our statistical test. Kulldorf f X  X  statistic assumes, both under the null hypothesis and the alternati ve hypothesis, that at most one disease cluster S dc exists, and that the disease rate q is expected to be uniform outside S dc (or uniform everywhere, if no disease cluster exists). Thus, if S dc is contained entirely in the region under consideration S , we would expect that the maximum density subre gion S 0 of S is S dc , and that the disease rate of S S 0 is equal to the disease rate outside S : E suming that the D 1 density of S S 0 is equal to its expected value d out , we can use the deri vation abo ve (using d out in place of d to find the maximum subre gion score.

The problem with this approach is that we have not compensated for the variance in densities. Our calculated value of d out a lower bound for the D 1 density of S S 0 in the most approxi-mate probabilistic sense, in that we expect D 1 ( S S 0 ) &gt; the time. We can impro ve the accurac y of our probabilistic bound by also considering the variance of C c in P p all counts outside S dc are generated by a inhomogeneous Poisson distrib ution with parameter qp i j , we obtain:  X  2  X  h Po Since the actual value of the parameter q is not kno wn, we use a conserv ative empirical estimate: q = C tot P  X  h maximum subre gion density by using d out b  X  , for some constant b , in place of d min in the deri vation abo ve. One minor complication is that, since  X  is dependent on p in , we must solv e equations for P and P 23 which are quadratic rather than linear; we omit the details of this calculation.

By adjusting our approximation of d min in this manner , we com-pute a higher score D , reducing the lik elihood that we will un-derestimate the maximum subre gion density and prune a region that should not necessarily be pruned. 7 Given a constant b , the D 1 density of S S 0 will be greater than d out b  X  with probabil-ity Pr ( Z &lt; b ) , where Z is chosen randomly from the unit normal. For b = 2, there is an 98% chance that we will correctly bound
This also increases the number of regions searched, and thus we have a tradeof f between speed and accurac y. D ( S S 0 ) , giving a guaranteed correct upper bound for the max-imum subre gion score. In practice, the maximum score will be lower than our approximate bound more often than this, since our estimates for the other parameters are conserv ative. Thus, though our algorithm is approximate, it is very lik ely to con verge to the globally optimal mdr .

One interesting feature of this approximation is that we expect to under estimate the maximum subre gion score if the disease clus-ter S dc is not contained entirely in S , since we are calculating d based on a region which includes this region of higher density . In cases where there is only a single disease cluster , this is acceptable (and desirable) since a region not containing S dc does not need to be expanded. In applications where multiple disease clusters are present, howe ver, there is a risk that the presence of one significant disease cluster will cause the approximate algorithm to miss an-other more significant cluster . This phenomenon is visible in our re-sults belo w: in two of the three real-w orld datasets, the approximate algorithms did not find the maximum density region, though the y did find another cluster that was also significant. In the cases where only one disease cluster was present, as in our simulated trials, the approximate algorithms achie ved high accurac y. We present results for the exact and approximate versions of the algorithm belo w.
We first describe results with artificially generated grids and then real-w orld case data. An artificial grid is generated from a set of an N N grid, and randomly selects a k 1 k 2  X  X est region.  X  Then the population of each square is chosen randomly from a normal distrib ution with mean  X  and standard deviation  X  (populations less than zero are set to zero). Finally , the count of each square is chosen randomly from a Poisson distrib ution with parameter qp i j q = q 0 inside the test region and q = q 00 outside the test region.
For all our simulated tests, we used grid size N = 256, and a background disease rate of q 00 = : 001. We tested for four dif ferent combinations of test region parameters ( k 1 k 2 , q 0 ): (7 9, (11 5, : 002), (4 3, : 002), and (0 0, : 001). These represent the cases of an extremely dense disease cluster , lar ge and small disease clusters which are significant but not extremely dense, and no dis-ease cluster respecti vely . We used three dif ferent population distri-butions for testing: the  X  X tandard X  distrib ution (  X  = 10 and two types of  X  X ighly varying X  populations. For the  X  X ity X  distri-bution, we randomly selected a 10 10  X  X ity region X : square pop-ulations were generated with  X  = 5 10 4 and  X  = 5 10 3 inside the city , and  X  = 10 4 and  X  = 10 3 outside the city . For the  X  X igh- X   X  distrib ution, we generated all square populations with  X   X  = 5 10 3 . For each combination of test region parameters and population distrib ution, run times were averaged over 20 random trials, and an additional 90 trials (for a total of 110) were used to test accurac y. A trial was counted  X  X orrect X  if the algorithm either found the test region S dc , or another region S with D ( we emphasize that this is always the case for the exact version of our algorithm, which always finds the maximum density region. We also recorded the average number of regions examined; for our algorithm, this includes calculation of score bounds as well as scores of indi vidual regions. Separate results are presented for the original grid and for each replica; for a lar ge number of random replications ( R = 1000) the results per replica dominate, since total
In additional to our theoretical argument for correctness, we con-firmed this empirically by running a lar ge number of tests on smaller grids, for all possible test region sizes. These results are not given here, but we note that the algorithm found the correct region in all cases.
 Figur e 6: Emer gency Department dataset. The left pic-tur e sho ws the  X  X opulation X  distrib ution and the right pictur e sho ws the  X  X ounts.  X  The winning region is sho wn as a rectangle. run time is t orig + R ( t rep ) to search the original grid and perform randomization testing. See Table 1 for results.

Our first observ ation was that the run time and number of re-gions searched were not significantly affected by the underlying population distrib ution; typically the three results dif fered by only 5-10%, and in man y cases test regions were found faster for the highly varying distrib utions than the standard distrib ution. Thus Table 1, rather than presenting separate results for each population distrib ution, presents the average performance over all three popu-lation distrib utions for each test. This result demonstrates the ro-bustness of the algorithm to highly non-uniform populations; this is very dif ferent than our pre vious work [7], where the algorithm was severely slo wed by highly varying populations. The exact al-gorithm achie ved average speedups ranging from 35x (for no test region), to 2300x (for an extremely dense test region) as compared to the nai ve approach. We note that, for the case of no test re-gion, it is typically not necessary to run more than 10-20 random-izations before vconcluding that the disco vered region is not sig-nificant; thus our true average  X  X  orst-case X  results will be closer to the 95x speedup on small, significant (but not extremely dense) test regions. Since the nai ve approach requires approximately 45 days for a 256 256 grid with R = 1000, this suggests that our exact algorithm can complete the same task in less than 12 hours. We also tested two approximate variants of the algorithm,  X  X pprox-2 X  and  X  X pprox-3,  X  with adjustments for density variance b = b = 3 respecti vely . These variants achie ved up to 5000x speedups, with over 1000x speedups whene ver a test region was present, and 84-173x speedups in the  X  X o test region X  case, enabling us to find the maximum density region and its significance in less than 1 hour . While all variants of the algorithm achie ved 100% accurac y on the very dense test regions, the approximate versions missed some of the less dense test regions. For the lar ger (11 5) test regions, approx-2 and approx-3 achie ved 98.5% and 99.7% accurac y re-specti vely (averaged over the 330 trials); for smaller (4 3) test regions, these accuracies were reduced to 93.0% and 98.2% respec-tively . In some cases, the guaranteed accurac y of the exact algo-rithm may be more necessary than the additional speedups gained by the approximate algorithm; in other cases, extremely fast results are needed, and an approximation may be suf ficient.

We now discuss the performance of the algorithm on various real-w orld datasets. Our first test set was a database of anon ymized Emer genc y Department data collected from Western Pennsylv ania hospitals in the period 1999-2002. This dataset contained a total of 630,000 records, each representing a single ED visit and giv-ing the latitude and longitude of the patient X  s home location to the nearest .005 degrees ( 1 3 mile, a suf ficiently low resolution to en-sure anon ymity). These locations were mapped to three grid sizes: N = 128, 256, and 512. For each grid, we tested for spatial clus-tering of  X  X ecent X  disease cases: the  X  X ount X  of a square was the number of ED visits in that square in the last two months, and the  X  X opulation X  of a square was the total number of ED visits in that square. See Figure 6 for a picture of this dataset, including the highest scoring region. For each of these grids, the exact and ap-proximate versions of our algorithm found the same, statistically significant region ( p -value 0/1000) as the nai ve approach. The ma-jor dif ference, of course, was in runtime and number of regions searched (see Table 2). Our algorithms found the mdr of the orig-inal grids 22-31x faster than the nai ve approach; howe ver, much faster performance was achie ved when searching the replica grids. The exact algorithm achie ved speedups increasing from 450x to 4700x as grid size increased from 128 to 512; the approximate ver-sions did even better , achie ving 2300-24000x speedups.

Our second test set was a nationwide database of retail sales of over-the-counter cough and cold medication. Sales figures were re-ported by zip code; the data covered 5000 zip codes across the U.S., with highest coverage in the Northeast. In this case, our goal was to see if the spatial distrib ution of sales on a given day (2/14/2004) was significantly dif ferent than the spatial distrib ution of sales a week before (2/7/2004), and to identify a significant cluster of in-creased sales if one exists. Thus we used the sales on 2/7 as our un-derlying population distrib ution, and the sales on 2/14 as our count distrib ution. Slight modifications to Kulldorf f X  X  statistic were nec-essary to deal with regions with zero population and nonzero count (i.e. sales on 2/14 but not 2/7). We created four grids from this data, two using all of the national data, and two using only data from the Northeast (where a greater proportion of zip codes report sales data). For both  X  X ational X  and  X  X e gional X  over-the-counter data, we created grids of sizes N = 128 and N = 256, con verting each zip code X  s centroid to a latitude and longitude. For each of these four grids, our exact algorithm found the same statistically signif-icant region ( p -value 0/1000) as the nai ve approach, and achie ved speedups of 96-132x on the 128 128 grids and 440-739x on the 256 256 grids. The approximate versions of the algorithm did not find the correct region on these four grids, and thus we do not include these in Table 2. We note, howe ver, that the y did find an-other statistically significant region, though with a lower score than the mdr; it is possible that the presence of this region caused the algorithms to miss the most significant region, as discussed abo ve.
Thus the exact version of our algorithm found the maximum density region in all of our simulated and real-w orld trials, while achie ving speedups of at least 20x (and typically much lar ger) as compared to the nai ve spatial scan. The approximate versions of the algorithm achie ved much lar ger speedups, though at the cost of occasionally failing to find the correct region. This speedup is ex-tremely important for the real-time detection of disease outbreaks: if a system is able to detect an outbreak in minutes rather than days, pre venti ve measures or treatments can be administered earlier , pos-sibly saving man y lives. We belie ve that our algorithm will be use-ful for rapid detection of significant spatial clusters in a variety of other applications as well.
Thus we have presented a fast multiresolution partitioning algo-rithm for detection of significant spatial overdensities, and demon-strated that this method results in significant (20-2000x) speedups on real and artificially generated datasets. We are currently ap-plying this algorithm to national-le vel hospital and pharmac y data, attempting to detect disease outbreaks based on statistically signifi-cant changes in the spatial clustering of disease cases. Our eventual goal is the automatic real-time detection of outbreaks, and applica-tion of a fast partitioning method using the techniques presented here may allo w us to achie ve this dif ficult goal.

Additionally , we are extending the algorithm (and the underlying overlap-kd tree data structure) in various ways, making it usable for a broader range of application domains. Most importantly , overlap-kd trees can be extended to higher dimensions, as can the other techniques (e.g. quartering) used in our multiresolution search. We note, howe ver, that various quantities (for example, number of chil-dren of a node) gro w exponentially with dimension, so overlap-kd trees are probably not appropriate for very high dimensional data. Ne vertheless, we hope that our techniques will be useful for various 3-D (and higher dimensional) applications, including the disco very of regions of significantly increased brain acti vity (corresponding to given cogniti ve tasks) using fMRI data. As discussed abo ve, we also are acti vely eng aged in deri ving more powerful statistical tests for overdensities (and the corresponding density functions) under a variety of application-specific models (for example, normally dis-trib uted counts inferred from the time series of pre vious counts, applied to the over-the-counter retail data). As long as the den-sity function satisfies the simple conditions described abo ve, our algorithm can be used to rapidly find the maximum density region according to this function. Finally , we are interested in extending our search for overdensities to more general classes of multi variate density functions, thus allo wing the disco very of clusters which are significant even after adjusting for multiple covariates.
This work was supported in part by NSF Grant IIS-0325581. [1] R. Agra wal, J. Gehrk e, D. Gunopulus, and P. Ragha van. [2] K. Deng and A. W. Moore. Multiresolution instance-based [3] S. Goil, H. Nagesh, and A. Choudhary . MAFIA: efficient and [4] M. Kulldorf f. A spatial scan statistic. Communications in [5] M. Kulldorf f. Spatial scan statistics: models, calculations, [6] M. Kulldorf f and N. Nag arw alla. Spatial disease clusters: [7] D. B. Neill and A. W. Moore. A fast multi-resolution method [8] S. Opensha w, M. Charlton, A. Craft, and J. Birch.
 [9] F. Preparata and M. Shamos. Computational Geometry: An [10] H. Samet. The Design and Analysis of Spatial Data [11] L. Waller , B. Turnb ull, L. Clark, and P. Nasca. Spatial [12] W. Wang, J. Yang, and R. Muntz. STING: a statistical
