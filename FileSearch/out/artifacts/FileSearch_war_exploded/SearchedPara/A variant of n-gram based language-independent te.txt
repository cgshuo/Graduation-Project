 Faculty of Mathematics, University of Belgrade, Studentski Trg 16, 11000 Belgrade, Serbia Tel.: +38 164 865 0063; E-mail: jgraovac@matf.bg.ac.rs 1. Introduction
Text categorization is the task of classifying unlabelled natural language documents into a predefined set of categories. This may be done manually, which is time-consuming and expensive, or automatically, which has become the key approach to efficient organizing and processing large amounts of information and to knowledge discovery. Automated text classification is important because it frees from the need of manually organizing document bases, which can be too expensive, or simply not feasible given the time constraints of the application or the number of documents involved. The accuracy of modern text classification systems rivals that of trained human professionals, thanks to a combination of information retrieval technology and machine learning technology [23].

There is a number of challenges in solving the problem of text categorization. One difficulty in han-dling some classes of documents is the presence of different kinds of textual errors, such as typing, spelling and grammatical errors. Some of documents have been derived from existing paper documents by means of an error-prone scanning and character recognition process. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. Although most of the research activity has concentrated on English text, categorization in some other languages is also an important area of research. This introduces a number of additional difficulties such as a problem of word segmentation in Chinese or any other Asian language. Chinese text dose not have explicit whitespace between words so word segmentation itself is a difficult problem in this language. There is a number of techniques for solving the problem of text categorization with very good results. However, many of them show some disadvantages such as using some language-specific knowledge or requiring some non-trivial text preprocessing steps such as feature extraction and feature selection.
The main goal of this paper is to present a technique which is very simple and language indepen-dent, that overcomes the above shortcomings. The approach uses byte-level n-gram frequency statistics technique for documents representation and K nearest neighbors machine learning algorithm for classi-fication process. This approach derives from Ke X elj X  X  [26] method to solving the authorship attribution problem by using an n-gram technique and some Tomovi  X  c X  X  ideas from [1], where the problem of auto-mated classification of genome isolates was examined. Since it is based on byte level n-grams, technique presented in this paper do not require word or character segmentation, do not need any text preprocessing or higher level processing, such as tagging, parsing, feature selection, or other language-dependent and non-trivial natural language processing tasks. The approach is also tolerant to typing errors: since every string is decomposed into small parts, any errors that are present tend to affect only a limited number of those parts, leaving the remainder intact. Using n-gram techniques, word stemming is got essentially for free. The n-grams for related forms of a word (e.g.,  X  X dvance X ,  X  X dvanced X ,  X  X dvancing X ,  X  X dvance-ment X , etc.) automatically have a lot in common when viewed as sets of n-grams. Using such n-gram profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.

The rest of the paper is organized as follows. Section 2 shows some background information and basic notions and Section 3 gives a brief discussion of related work. Section 4 describes the methodology presented in this paper for categorization of text documents. Several dissimilarity measures are presented such as a new, very simple but sufficiently powerful dissimilarity measure. This section also presents the approach. Section 5 reports on experimental results and shows comparison of new dissimilarity measure with other, perviously published measures. Sections 6 and 7 present comparison results of the approach proposed in this paper with other n-gram based and state-of-art methods. Section 8 concludes the paper. 2. Background
In this section a brief overview of text categorization task, document representations and n-grams notions are given. 2.1. Text categorization
Text categorization is the task of assigning a Boolean value to each pair ( d j ,c i )  X  D  X  C ,where D is a domain of documents and C = c 1 ,...,c | C | is a set of predefined categories. A value of T (true) is to approximate the unknown target function ought to be classified) by means of a function  X : D  X  C  X  X  T,F } called the classifier, such that and  X   X  X oincide as much as possible X  [23]. How to precisely define and measure this coincidence will be discussed in Section 4.4.

Text categorization can be single-label where categories are mutually exclusive i.e. each document has to belong to exactly one of the categories and multi-label where categories are not mutually exclu-sive and document may belong to several categories simultaneously, or to a single category, or to none of the categories. Also, classification can be supervised, where some external mechanism (such as hu-man feedback) provides information on the correct classification for documents and unsupervised (also known as document clustering), where the classification is to be done without any reference to exter-nal information. Document classification may also be semi-supervised, where parts of the documents are labelled by the external mechanism. Algorithms for text categorization can be roughly divided into knowledge based, consisting in manually defining a set of rules encoding expert knowledge on how to classify documents into the given categories and machine learning algorithms, where a general in-cal text classification if the learning method is statistical [3]. Some of the most commonly used machine learning techniques that have been applied to automated text classification are: K nearest neighbors (kNN) [33], Support vector machines (SVM) [23], Decision trees [23] and Rule-based classifiers [7] (e.g. RIPPER [31]), Bayesian classifier [23], Rocchio X  X  algorithm [23], Neural networks [23], Genetic algorithms [28], Latent semantic analysis [21], Centroid based classifier [5], Conditional random fields (CRF) [10], Hidden Markov Models (HMM) [34] etc. Among these algorithms, SVM and kNN algo-rithms are most commonly used in text classification. Also, there are many hybrid techniques created as a combination of individual classifiers in order to improve their performance. 2.2. Document representation
Before being processed by machine learning algorithms, text documents should be transformed from the full text version into document vectors which describe the contents of the documents. This process is called  X  X ocuments representation X  and it is one of the preprocessing steps essential for text catego-rization. Document representation techniques can be classified into  X  X eature extraction X , that is used to transform text documents into a reduced representation set of features and  X  X eature selection X  used to select a subset of relevant features for building robust learning models. In order to evaluate a feature, metrics have been explored and one of the most popular is  X  X f-idf X  (term frequency  X  inverse document frequency) metric used to weight each word in the text document according to how unique it is [2]. Other frequently used metrics are  X  X nformation gain X ,  X  X hi-square X ,  X  X xpected cross entropy X ,  X  X dds ratio X  etc. However, effective document representation model is needed to build an efficient classification system. The traditional document representation is  X  X ag-of-words X  (BOW) that is used to represent a document as a vector of unordered words in which every element indicates the presence or absence of a word in the document. The major limitation of this technique is that the information about the sequence get lost. Un-like  X  X ag-of-words X  approach, n-gram techniques are based on n-grams that are n-contiguous sequences and can be defined on the word, character or byte level. Less popular but more effective is  X  X emantic and ontology based documents representation X . Ontology is a data model that represents a document as a set of concepts within a domain and relationships between those concepts. Some methods of this type are based on Wordnet [18], a semantic network encoding a large number of concepts and semantic relations. This approach requires more research but it provides more effective process of classification. However, the syntax-based methods are more frequently used. 2.3. N-grams
Given a sequence of tokens S =( s 1 ,s 2 ,...,s N +( n  X  1) ) over the token alphabet A ,where N and n are positive integers, an n-gram of the sequence S is any n-long subsequence of consecutive tokens. The i th n-gram of S is the sequence ( s i ,s i +1 ,...,s i + n  X  1 ) [1].
Note that there are N such n-grams in S .Thereare ( | A | ) n different n-grams over the alphabet A ( |A| is the size of A ).

For example, if A is the English alphabet, and l string on alphabet A , l =  X  X ife is a miracle X  then fe_,e_i,...;4-gramsare:life,ife_,fe_i,... andsoon.The underscore character ( X  X  X ) is used here to represent blanks.

The use of n-gram models and techniques based on n-gram probability distribution in natural language identification [25], authorship attribution [26]. It also proved useful in domains not related to language processing such as music representation [4], computational immunology [16], protein classification [24] etc.

As already mentioned, the term n-gram could be defined on word, character or byte level. Although word level analysis seems to be intuitive, it ignores the fact that many Asian languages such as Chinese do not have word boundaries e xplicitly identified in a text so word segmentation itself is a difficult problem. In the case of Latin-alphabet languages, character-level and byte-level n-gram models are quite similar according to the fact that one character is usually represented by one byte. The only difference is that character-level n-grams use letters only and typically ignore digits, punctuation, and whitespace while byte-level n-grams use all printing and non-printing characters. In the case of Asian languages, one character is usually represented by two bytes, depending on the coding scheme that is used, so 75% byte-level n-grams include half-characters (i.e., all odd-length n-grams, and half of the even-length n-grams) because the text is simply treated as a sequence of bytes instead of characters.
When used in processing natural-language documents, n-grams exhibit some good features [1]:  X  Robustness: Relatively insensitive to spelling variations/errors;  X  Completeness: Token alphabet known in advance;  X  Domain independence: Language and topic independent;  X  Efficiency: One pass processing; and  X  Simplicity: No linguistic knowledge is required.

The problem with using n-grams is exponential combinatorial explosion. If A is the Latin alphabet with the space delimiter, then |A| = 27. If one distinguishes between upper and lower case letters, and also uses decimal digits, then |A| = 63. It is clear that many of the algorithms with n-grams are computationally too expensive even for n =5 or n =6 (for instance, 63 5  X  10 9 ). 3. Related work
Many papers have been written on the subject of text categorization and many different techniques have been used for this purpose. Some of them rely on n-gram based models. They are applied on different corpora written in different languages. Popular corpora that have been used in numerous studies are Reuters-21578 and 20-Newsgroups in English and Tancorp in Chinese.

Text categorization based on character level n-gram models has been attempted by Cavnar [29]. The technique is based on using an ad hoc rank order statistic to compare the prevalence of character-level n-grams. Comparison between testing and training profiles is based on comparing ranks of the most frequent n-grams. Cavnar presented the system that worked very well for language categorization, and also worked reasonably well for classifying articles from a number of different computer-oriented news-groups according to subject.

Rahmoun and Elberrichi [20] also used character n-grams to represent documents. The effects of this method are examined in several experiments using the multivariate chi-square to reduce the dimen-sionality, the cosine and Kullback&amp;Liebler dissimilarity measures, and two benchmark corpora  X  the Reuters-21578 newswire articles and the 20-Newsgroups data for evaluation. The evaluation was done by using the macro-averaged F 1 function. The results show the effectiveness of this approach compared to the  X  X ag-of-words X  and stem representations.

In [15] Makoto Suzuki el al. have proposed a new mathematical model of automatic text categorization or a classification method based on Vector Space Model. They have also used character level n-grams to generate index terms. Moreover, they have shown that the proposed method had good classification accuracy by several experiments. These experiments used the English Reuters-21578 dataset and the Taiwanese China Times Newspaper 2005 dataset.

Wei el al. [35] and Luo el al. [32] performed Chinese text categorization on TanCorp corpus. In [35] they used different, character level n-gram feature (1-, 2-grams or 1-, 2-, 3-grams) to represent docu-ments and different feature weights (absolute text frequency, relative text frequency, absolute n-gram frequency and relative n-gram frequency) were compared. The SVM algorithm was used. They found out that the feature selection methods based on n-gram frequency (absolute or relative) always give bet-ter results. Luo et al. [32] propose to use the combination of character 1-gram and 2-gram after feature transformation which proved to be the most efficient method to represent Chinese documents. They fur-ther propose a serial approach based on feature transformation and dimension reduction techniques to improve the performance of Chinese text classification. SVM algorithm was used, too. The experimental results show that normalizing absolute frequency to relative frequency followed by power transformation significantly improved the performance. Principal Component Analysis effectively reduced the dimen-sionality without deterioration of the performance. Furthermore, the roles of the different part-of-speech were explored in feature selection and nouns were found to be the most important features to repre-sent Chinese documents while suitable combination of part-of-speech may lead to better classification performance.

In addition to methods based on n-grams, there are other methods that give extremely good results when they are applied to the Reuters, 20-Newsgroups and Tancorp corpora.

Significant results of the text classification on English corpora Reuters-21578 and 20-Newsgroups are achieved by Lan el al. [14]. They represented text documents as  X  X ag-of-words X  and they have inves-tigated several widely-used unsupervised and supervised term weighting methods in combination with SVM and kNN algorithms. They introduced new techniques called  X  X f.rf X  (term frequency  X  relevance frequency) which have proved to be more effective than others.
 Songbo Tan and his colleagues [22] reported the best performance of several learning algorithms on Tancorp corpus in Chinese and three corpora in English (20-Newsgroups, WebKB and Sector-48). They presented a novel strategy, called DragPushing, for improving the performance of text classifiers. They examined the effect of this strategy for Centroid Classifier and Naive Bayes algorithms. 4. Methodology and data
This approach derives from Ke X elj X  X  [26] method to solving the authorship attribution problem. Ke X elj byte n-grams x i and their normalized frequencies f i . The authorship is determined based on the dissim-ilarity between two profiles, comparing the most frequent n-grams. Identical texts will obviously have an identical set of the L most frequent n-grams, and thus have zero dissimilarity. Different texts will be more or less similar to each other, based on the am ount of the most-fre quent n-grams whi ch they share. the dissimilarity measure, is changed. 4.1. Dissimilarity measures
Dissimilarity measure d is a function that maps the Cartesian product of two sets of sequences P 1 between these two profiles and it should meet the following conditions [1]:  X  d ( P , P )=0 ;  X  d ( P 1 , P 2 )= d ( P 2 , P 1 ) ;  X  The value d ( P 1 , P 2 ) should be small if P 1 and P 2 are similar ;  X  The value d ( P 1 , P 2 ) should be large if P 1 and P 2 are not similar .

The last two conditions are informal as the notion of similarity (and thus the dissimilarity) is not strictly defined.

The original dissimilarity measure used by Ke X elj [26] in text authorship attribution is a form of relative distance: P Based on this measure, a wide range of dissimilarity measures were introduced [1].

Taking into consideration all these 19 measures (from [1,26]), experimental results that we obtained best on automated topic-based categorization of text documents, with respect to different text corpora:
In this work, inspired by Dice X  X  measure [19], a new, very simple but effective dissimilarity measure In mathematics, this is known as symmetric difference and it is commonly denoted by . This measure will be named dSymmDif . As can be seen, this function does not require the calculation of normalized frequencies. Unlike Ke X elj X  X  method, the normalized frequencies of the n-grams are used only for ranking the n-grams and are not included in the profiles. In this approach, profile is the set of the L most frequent byte n-grams x 4.2. Text classification procedure
The text classification procedure presented in this paper is as follows: 1. Collect the text documents in corpora and divide them into training and test data; 2. Concatenate to each other all the training documents that belong to the same category so that each 3. For each category and testing document, construct their profiles: 4. For each testing document, compare its profile with each category profile using the dSymmDif
In order to maximize the accuracy, this technique will be tested for different values of n-gram size n and the profile length L . 4.3. Text corpora In this study, two widely-used text corpora in English are used as the benchmark datasets: 20-Newsgroups corpus and Reuters-21578 corpus. In order to show language independence of this method, experiments were also conducted over the Tancorp corpus in Chinese and Ebart corpus in Serbian lan-guage (see Table 1). 4.3.1. 20-Newsgroups
The 20 Newsgroups 1 dataset is a collection of approximately 20000 newsgroup documents, evenly divided into 20 different newsgroups, each corresponding to a different topic. It was first collected by Lang [12].
Three versions of this dataset are public available. The most popular is  X  X ydate X  version. It is sorted by date into training (60%) and testing (40%) sets, does not include cross-posts (duplicates) and does not include newsgroup-identifying headers (Xref, Newsgroups, Path, Followup-To, Date). This is the corpus edition that is used for testing categorization technique presented in this work. 20-Newsgroups corpus is a single label dataset so each document is guaranteed to have only one category label. There are 18846 documents in this corpus with about 60% documents for training and 40% documents for testing. Some of the newsgroups are very closely related to each other (e.g. comp. sys.ibm.pc.hardware/comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale/soc. religion.christian). 4.3.2. Reuters-21578
Reuters-21578, 2 corpus of Reuters News stories, is currently the most widely used testing collection for text categorization research [13]. One of the most popular type of training and testing divisions is  X  X odeApte X  split in which 75% of the stories are used to build classifiers and the remaining 25% to test the accuracy of the resulting models. Unlike the 20-Newsgroups, Reuters-21578 is a multi-label corpus and there may be more than one category labels for a document. One noticeable issue regarding the Reuters corpus is the skewed category distribution problem. The number of stories in each category varied widely.

In this work, only the top 10 largest categories are taken into consideration. Figure 1 shows distribution of these ten most frequent categories. Among the top ten categories, the largest category  X  X arn X  has a training set of 40% documents, but 8 out of those 10 categories have less than 7.5% documents. 4.3.3. Tancorp
The Tancorp corpus 3 is a Chinese corpus for text categorization collected by Tan [22]. It is a single-label corpus. There are 14150 documents in this corpus categorized in two hierarchies. The first hierarchy contains 12 large categories and the second hierarchy consists of 60 small categories. This corpus may be used in a form of three categorization datasets: one hierarchical dataset (TanCorpHier) and two flat datasets (TanCorp-12 and TanCorp-60) contains 12 and 60 categories respectivelly. In this paper the first level categorization Tancorp-12 is used. Testing and training data are randomly distributed in the ratio 2:1. Like Reuters-21578, Tancorp is characterized by uneven distribution of categories (see Fig. 1).
As said earlier, a large difference between Chinese and Latin-alphabet languages text classification lies in the text representation. Unlike most of the western languages, the Chinese words do not have a re-markable boundary. This means that the word segmentation is necessary before any other preprocessing. Use of a dictionary is necessary, too. This makes Chinese representation of text using words, phrases, meanings, and concepts more difficult [35]. 4.3.4. Ebart
Ebart is the largest digital media corpus in Serbia with almost one million news articles from daily and weekly newspapers archived by early 2003 onwards. 4 Within it, complete editions of fifteen daily and weekly newspapers published in Serbia are stored, as well as selected articles from the biggest local weekly. The current archive is classified into thematic sections following the model of regular newspaper columns (e.g.  X  X port X ,  X  X olitics X ,  X  X conomics X ,  X  X hronicle X ,  X  X ulture X ,  X  X orld X ,  X  X ociety X , etc.).
In this paper a subset of the Ebart corpus is taken into consideration  X  articles from the Serbian daily newspaper  X  X o litika X  that belong to column s Sport, Economics and Politics, published from 2003 to Figure 1 show the distribution of this corpus. 5 4.4. Performance evaluation
To assess the performance of the approach presented in this paper, a standard set of evaluation metrics is used, the one being commonly used in many text categorization tasks such as Man X  X  [14] and Tan X  X  research [22]. 4.4.1. Recall, precision, accuracy and error
For a given category, the  X  X ositives X  (respectively,  X  X egatives X ) can be defined as the set of documents assigned to that category (respectively, not assigned to that category).  X  X rue positives X  (TP) are defined as the documents that were correctly assigned to the considered category. The  X  X alse positives X  (FP) are the documents that were wrongly assigned to that category. Similarly, the  X  X rue negatives X  (TN) were correctly not assigned to the considered category, while the  X  X alse negatives X  (FN) were not assigned to the considered category but should have been assigned to it (since they belong to it). Precision and Recall are defined in terms of these sets, as follows: Accuracy and Error are also commonly used performance measures. They are defined as follows: 4.4.2. F 1 function
Neither precision nor recall makes sense in isolation from each other as it is well known from the practice that higher levels of precision may be obtained at the price of low values of recall. One of the most widely-used measures that combine precision and recall and give both of them equal importance is the F 1 function introduced by van Rijsbergen [27]: 4.4.3. Micro-and macro-averaged F 1 function
The presented classification accuracy measures are applicable to individual classes. In order to obtain a single measure for the evaluation of a classification as a whole, the F 1 function needs to be averaged considering all the documents as a single dataset, regardless of classes and macro-average F 1 is the average on F 1 scores of all the classes. Macro-average F 1 gives equal weight to each class, while micro-average F 1 is a per document function, so it is heavy influenced by larger classes.
 5. Results
This section presents the experimental results obtained for each corpus individually. No preprocessing is done on texts, and a simple byte n-grams representation is used, treating texts simply as byte se-quences. The newly introduced measure dSymmDif is taken into consideration as well as selected mea-For producing n-grams and their normalized frequencies the software package called Ngrams written by Ke X elj [11] is used. In the process of separating testing from training documents and in the process of classification, the software package called NgramsClassification is used, developed specifically for this purpose (source code can be obtained on request from the author). One of the most important question in the byte n-gram classification is what are the values of n and L that produce the best results. To give an answer to this question, the accuracy (micro-and macro-sense to do.

Figures 2 and 3 show graphical representation of this extensive set of experiments taking into ac-count only a few values of n for which the highest accuracy is achieved and only dissimilarity measure dSymmDif (all other measures achieve the maximum accuracy for the same value of n as dSymmDif ). The choice of an optimal value of n varies with text corpora. It can be seen that in the case of 20-Newsgroups and Reuters-21578 corpora, maximum values for micro-and macro-averaged F 1 function are reached for n =7 , in the case of Tancorp, maximum accuracy is reached for n =6 and in the case of Ebart corpus maximum is reached for n =5 . Marked in green are the results for smaller n (and in blue for greater n )than n for which the maximum is reached.

It is interesting that there is a sudden drop of performance after L exceeds a certain value. This is particularly noticeable in the case of 20-Newsgroups. As presented in Section 4, for each category and testing document, and for each considered value of n , the maximum possible profile length is calculated (as the number of different n-grams occurring in it). The category and documents profiles have varying maximum lengths depending on the length of the input data and the n-gram size n . For example, in the case of 20-Newsgroups when 6-grams are extracted, the category  X  X alk.politics.mideast X  allows the maximum length of the profile (the number of different 6-grams in that category) to be 361369, while the category  X  X omp.os.ms-windows.misc X  allows the maximum profile length to be 202047. The average profile length of all categories in that case is 241758 while the average profile length of all testing documents is 1353. This large difference in size comes from the fact that category profile is derived from the document obtained by concatenating all training documents of that category into one. A sudden drop of performance occurs when the profile length L exceeds the maximum possible profile length for the considered n-gram size for at least one category. This is because the dissimilarity measure dSymmDif (as length, the greater the difference measure). When the maximum possible profile length for some category is lower than L , some documents are wrongly classified to that category. For example, when 6-grams are extracted (maximum profile length for smallest category in that case is 202047), for L = 202000 micro-averaged F 1 is 81.09% while macro-averaged F 1 is 80.6%, for L = 202500 micro-averaged F while macro-averaged F 1 is 15.4%. For L bigger then 204000 micro-averaged F 1 is less then 10% and macro-averaged F 1 is undefined. That happens because there is at least one category where  X  X rue positives X  and  X  X alse positives X  are equal to zero (all documents of that category are wrongly classified to zero. Therefore, F 1 measure and macro-averaged F 1 measure are undefined too (see Section 4.4). In summary, it can be concluded that the dissimilarity measure dSymmDif (aswellasmeasures d , d 1 , d 2 and d 3 ) is not as accurate for those n , L combinations where L exceeds the length of at least one category profile in the corpus.

For the n-gram size n for which micro-and macro-averaged F 1 measures achieve their maximum values, comparison between measure dSymmDif and other measures d , d 1 , d 2 and d 3 is performed. The results of these experiments are shown in Figs 4 and 5.

Additionally, Figs 6 and 7 present only the best results for each corpus and each measure. All these results show that the newly introduced measure dSymmDif , even being very simple, achieves comparable mately the same maximum value of micro-and macro-averaged F 1 function in all considered corpora (see Fig. 8).
 Figure 8 illustrates that in the cas e of Reuters-21578 corpus, value s for micro-and macro-averaged F distribution of categories. Moreover, for the largest categories in this corpus, experimental results show that F 1 measure reaches very high values. These are the reasons why the values for the micro-averaged F for macro-averaged F 1 function. 6. Comparison with other n-gram based methods
In order to evaluate performance of byte-level n-gram based classification techniques presented in this work, it has been compared with other n-gram based techniques. In the case of 20-Newsgroups and Reuters-21578, comparison is done with n-gram based classification technique presented by Rahmoun and Elberrichi [20]. For Reuters-21578, results in this paper are also compared with results of Suzuki el al. [15]. In the case of Tancorp corpus comparison is performed with works of Wei el al. [35] and Luo el al. [32]. All these comparison results are presented in Fig. 9 (approach presented in this paper is marked as  X  X NN byte-n-grams X ).

It can be seen that the technique presented in this paper, in the case of 20-Newsgroups and Tancorp, shows better performance than other n-gram based techniques.

In the case of the Ebart corpus, there are no published results with which to make comparison. 7. Comparison with other state-of-the-art methods
A good way of evaluating performances of the introduced technique is also to compare it with kNN and SVM applied to  X  X ag-of-words X  representation. For comparison purposes, reported state-of-the-art results on Reuters-21578 and 20-Newsgroups are cited in Man X  X  research [14], and the results on Tancorp-12, are cited in Tan X  X  research [22].

To make the comparison more convincing and fair, the same datasets and testing/training split are used here. There will be only one result for each learning algorithm, for each approach on each corpus. Therefore, only the best reported results will be cited, with no concern about how many features are selected or kind of derivation used.
 From Fig. 10 it can be seen that the  X  X NN byte-n-grams X  method reaches better accuracy than  X  X NN BOW X  (with exception of macro-averaged F 1 for Reuters-21578 corpus) and worse than  X  X VM BOW X  (with exception of 20-Newsgroups corpus). 8. Conclusion and further work
In this paper a new variant of a document categorization approach is presented based on byte-level n-grams, including all printing and non-printing characters. The approach relies on a profile document representation of restricted size and a very simple algorithm for comparing profiles. It provides an inex-pensive and effective way of classifying documents. A new, simple but effective dissimilarity measure is defined and compared to the dissimilarity measures introduced elsewhere.

One of the inherent advantages of this approach is that it is language independent since it is based on low-level information. This is demonstrated in experiments performed not only on English (Reuters-21578 and 20-Newsgroups), but also on Chinese (Tancorp) and Serbian data (Ebart corpus). Experi-mental results show that this new approach, although very simple, gives better results than other n-gram based methods in the case of all corpora except Reuters-21578. Moreover, it gave comparable results with other state-of-art methods. It performs better than  X  X ag-of-words X  kNN classifier and in the case of 20-Newsgroups works even better than  X  X ag-of-words X  SVM classifier. This categorization mechanism is robust, and does not require any knowledge of different languages.

Dissimilarity measures are subject to further investigation and improvement, as well as categorization methods themselves. Further tests are to be applied to other corpora such as English corpus web pages  X  X ebKB X , Spanish movie reviews corpus  X  X uchocine X  and corpora in different languages such as Per-sian corpus Hamshahri and Arabic dataset introduced by Mesleh [17]. Some preliminary experiments are already done on these corpora and very encouraging results were obtained. The method presented, being based on a sequence of bytes, is applicable to different domains and problems and will be further tested on specific corpora such as bioinformatics and multi-lingual corpora and on tuning Internet search engines. Acknowledgements The work presented has been financially supported by the Ministry of Science and Technological Development, Republic of Serb ia, through Projects No. 174021 and N o. III47003. The author is grateful to prof. Gordana Pavlovi  X  c-La X eti  X  c for her unfailing support and supervision of my research. References
