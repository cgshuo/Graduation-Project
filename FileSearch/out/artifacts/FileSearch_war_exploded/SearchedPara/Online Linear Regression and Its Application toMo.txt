 learning from past experience in different but similar situ ations. ization in RL.
 ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x m , y m ) , that the data satisfies a linear relationship, that is y n  X  whose i th row consists of the i th input x T output y input vector x x may be chosen in any way that depends on the previous inputs an d outputs ( x The output y and  X   X  R is a known constant. After observing x must produce an output  X  y able to provide an output  X  y ( x ) for any input vector x  X  { 0 , 1 } n .  X  KLRP.
 the following conditions: 1.1 Solution row of X . Since X T X is symmetric, we can write it as where U = [ v Let the corresponding eigenvalues be  X  R algorithm at time t ), define Algorithm 1 KWIK Linear Regression 0: Inputs:  X  1 ,  X  2 1: Initialize X = [ ] and y = [ ] . 2: for t = 1 , 2 , 3 ,  X   X   X  do 3: Let x t denote the input at time t . 4: Compute  X  q and  X  v using Equations 2 and 3. 5: if ||  X  q ||  X   X  1 and ||  X  v ||  X   X  2 then 6: Choose  X   X   X  R n that minimizes P 7: Output valid prediction x T  X   X  . 8: else 9: Output  X  . 10: Receive output y t . 11: Append x T 12: Append y t as a new element to the vector y . 13: end if 14: end for KWIK Linear Regression Problem with a sample complexity bou nd of  X  O ( n 3 / 4 ) . interpretation. Given a new input x is large, the estimate is not trusted and the algorithm produ ces an output of  X  . X eigenvalues separately and this consideration is dealt wit h by  X  v . 1.2 Analysis The analysis hinges on two key lemmas that we now present. bound and is omitted. 1 . Suppose that ( x || x i ||  X  1 positive constant z , if then with probability at least 1  X  2  X  0 .
 that | (  X   X   X   X  ) T x | is bounded by a quantity proportional to  X  1 . Suppose that ( x || x i ||  X  1 denote the error term q P m Proof sketch: (of Theorem 1) of times the algorithm makes a prediction of  X  ), in terms of the input parameters  X  second is to choose the parameters  X  valid prediction made by the algorithm is accurate.
 Step 1 Step 2 We choose  X  quantity  X  was chosen to minimize the term P m (finitely many) applications of Lemma 1. 2 1.3 Notes outputs y could instead allow a larger (but still finite) bound.
 outputs  X  for the current input x least-squares problems (see Chapter 12 of the book by Golub a nd Van Loan (1996)). 1.4 Related Work vectors x Brafman &amp; Tennenholtz, 2002).
 should be applicable to many other classes of MDPs as describ ed in the conclusion. 2.1 Problem Formulation space, T : S  X  A  X  P state and u where x and M is an n component of the noise term w is linearly parameterized , because the next-state x (which describes the current state and action) plus a noise t erm. We assume that the learner (also called the agent ) receives n in the interval [0 , 1] . For any policy  X  , let V  X  Specifically, let s from execution of policy  X  in some MDP M from state s The optimal policy is denoted  X   X  and has value functions V  X  cannot have a value greater than v 2.2 Algorithm and output next states. For each state component i  X  { 1 , . . . , n problem that can be solved by any instance A prediction of A state (state-action) and we ensure that the value function o f our model assigns v function for action a from state s a self-loop with reward 1 (yielding a value of v for the resulting KWIK-RMAX algorithm is provided in Algori thm 2. Theorem 2 For any and  X  , the KWIK-RMAX algorithm executes an -optimal policy on at most a polynomial (in n , n Algorithm 2 KWIK-RMAX Algorithm 1: for all state components i  X  { 1 , . . . , n S } do 3: end for 5: for t = 1 , 2 , 3 ,  X   X   X  do 6: Let s denote the state at time t . 7: Choose action a :=  X   X   X  ( s ) where  X   X   X  is the optimal policy of the MDP M odel . 8: Let s 0 be the next state after executing action a . 9: for all factors i  X  { 1 , . . . , n } do 10: Present input-output pair (  X  ( s, a ) , s 0 ( i )) to A i,a . 11: end for 12: Update MDP M odel . 13: end for 2.3 Analysis Proof sketch: (of Theorem 2) event can occur is bounded by the maximum number of times the i nstances A is polynomial in the relevant parameters. 2 2.4 The Planning Assumption 2.5 Related Work on a reset assumption, while ours does not.

