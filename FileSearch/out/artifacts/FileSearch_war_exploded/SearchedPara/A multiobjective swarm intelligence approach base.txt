 fi t traditional optimization methods. In this paper, DNA sequence design has fi cantly surpass other results previously published. 1. Introduction
Molecular computing, or more speci fi cally DNA computing, refers to a computational model which uses DNA molecules as information storage and their biological reactions as processing operators ( Adleman, 1994 ). In DNA computing, the hybridization between a speci fi c DNA sequence and its base-pairing comple-ment (also known as Watson  X  Crick pairing) is considered a crucial task, because this process makes possible to retrieve the informa-tion stored in DNA sequences and to perform operations with that information ( Garzon and Deaton, 1999 ). However, undesirable hybridizations usually lead to incorrect computations. For this reason, DNA sequences generated for molecular computing have to be carefully designed. Thus, although DNA computing is a promising paradigm which is supposed to replace silicon-based computers in future decades, it presents some technical draw-backs that must be overcome. In fact, when these technical problems are solved, DNA computing will become an important area within Computer Engineering. Therefore, much work has focused on improving the reliability and ef fi ciency of DNA com-puting, and particularly, on the design of error-minimized DNA sequences that are able to reduce the possibility of illegal reactions ( Brenneman and Condon, 2002 ). The design of reliable DNA sequences which generate speci fi c duplexes during hybridization, while simultaneously avoiding other undesirable cross-hybridiza-tion, involves several heterogeneous and con fl icting design criteria which cannot be tackled by traditional optimization methods described in the related literature. Typical existing approaches for DNA sequence design problem include a wide range of non-exact approaches, such as evolutionary algorithms, dynamic pro-gramming, and heuristic methods ( Brenneman and Condon, 2002 ). However, a design based on multi-objective evolutionary algorithms (MOEAs) represents the most appropriate design alternative ( Coello et al., 2002 ; Deb, 2001 ) because MOEAs take into account several con fl icting objectives simultaneously without the arti fi cial adjustments which are included in classical mono-objective optimization methods. These approaches usually include the use of different weighting factors assigned to each objective.
This paper proposes a new DNA sequence design evolutionary approach based on multiobjective swarm intelligence to automa-tically generate reliable DNA strands that can be applied to molecular computing (a promising area in Computer Engineering).
Speci fi cally, the arti fi cial bee colony algorithm ( Karaboga and Akay, 2009 ) was adapted with the inclusion of several multiobjective features, such as the idea of non-dominated solutions or the concept of non-dominated sorting ( Deb et al., 2001 ). Furthermore, our approach is compared against the well-known multiobjective standard NSGA-II (fast non-dominated sorting genetic algorithm ( Deb et al., 2001 )). Finally, our results are validated by using other works published in the literature. As will be discussed, our approach generates more reliable DNA sequences for DNA com-puting than other relevant approaches previously published.
The rest of this work is organized as follows: Section 2 discusses related work. Section 3 describes the basic background on the problem and the multiobjective formulation followed. The metaheuristics which have been developed are explained in Section 4 . Section 5 is devoted to present and analyze the results.
In Section 6 , our approach is compared with other methods published in the literature. Finally, Section 7 summarizes the conclusions of the paper. 2. Related work
DNA computing has been a very active research fi eld in the last decade. There are a great number of works in which different approaches have been proposed to generate reliable DNA sequences that are suitable for bio-molecular computing. How-ever, most of related publications manage the problem in terms of threshold-based constraints by considering different biological criteria as requirements which are combined into a fi nal mono-objective function. Table 1 summarizes a review of recent DNA sequence generators classi fi ed according to the kind of methodol-ogy used to generate sequences.
 Exhaustive and random searches ( Hartemink et al., 1998 ;
Penchovsky and Ackermann, 2003 ) represent the simplest meth-ods, but they are not effective because they use a great amount of computational resources. Template-map strategies ( Frutos et al., 1997 ; Arita and Kobayashi, 2002 ; Liu et al., 2003 ) are designed to choose dissimilar sequences among a huge set of sequences automatically generated. In Feldkamp et al. (2001) , the design of a limited number of sequences is performed by using a representa-tion based on directed graphs. In this approach, graph nodes represent base strands in which each node has four strands that can appear as successors in a longer sequence. Tanaka et al. (2001) generated sequences by using simulated annealing. They tried to fi nd promising solutions by combining different criteria into a fi tness function. Dynamic programming was proposed in Marathe et al. (1999) to design DNA sequences based on Hamming distance and free energy. Contrary to the system referred previously, biologically inspired and evolutionary methods have been recently used. Deaton et al. (2002) proposed a method based on in-vitro evolution to fi nd non-cross-hybridizing DNA libraries. However, biologically inspired methods have some inherent problems, such as they are not able to distinguish between DNA sequences in the library. Other biological approaches consider thermodynamic properties presented in DNA structures or free energy of DNA sequences ( Deaton et al., 2002 ; Heitsch et al., 2002 ).
However, according to the vast number of works published in the last years ( Deaton et al., 1998 ; Ibrahim et al., 2012 ), evolu-tionary algorithms (EAs) can be considered the most widely applied techniques for designing reliable DNA sequences. EAs use one or more design criteria to make evolve the particular evolutionary scheme adopted. For example, genetic algorithms (GAs) have been frequently used due to their simplicity ( Deaton et al., 1998 ; Ruben et al., 2001 ). In Deaton et al. (1998) , the function was designed by using the Hamming distance between sequences. An iterative genetic search was developed in ( Zhang and Shin, 1998 ) to design sequences in the context of DNA computing. Several fi tness criteria (similarity, H-measure, ham-ming distance and GC ratio) were applied in a constrained genetic algorithm in Arita et al. (2000) . Shin et al. (2002 , 2005 ) proposed one of the few multiobjective evolutionary algorithms published in the literature. For this reason, our work is compared against these studies in Section 6 . A multiobjective approach based on six DNA design criteria was proposed in Shin et al. (2002) and improved in Shin et al. (2005) . Two criteria (melting temperature and GC ratio) were considered as constraints meanwhile similarity, H-measure, continuity and hairpin were the four objectives of the system. Other studies also consider several design criteria, but they eventually manage them into a weighted mono-objective function. Thus, in Xu et al. (2008) and Cui and Li (2010) ,DNA sequence design problem is formulated as a multi-criteria optimi-zation problem tackled with Genetic Algorithm (GA)/Particle Swarm Optimization (PSO). In Khalid et al. (2008) , PSO is applied again with the aim of minimizing the H-measure design criterion. Different ordering methods were proposed for an Ant Colony
System (ACS) in Kurniawan et al. (2008) . Similarly, a mono-objective population-based ant colony optimization strategy was proposed in Kurniawan et al. (2009) , and single-stranded equal-length DNA sequences were designed in Hongyan and Xiyu (2009) by using a genetic algorithm. On the other hand, Wang et al. proposed the standard multiobjective NSGA-II algorithm by con-sidering several constraints, but results in Shin et al. (2005) are signi fi cantly better, so our study uses them as reference multi-objectiveresults.Anyway,thesuita bility of evolutionary approaches is con fi rmed by very recent publications ( Zhang et al., 2010 ; Ibrahim et al., 2012 )inwhichdifferentEAsareproposedtosolvetheproblem by including different design criteria. However, the algorithms proposed in those works cannot be considered as multi-objective
EAs, because the problem is simpli fi ed and converted into a single-criterion optimization problem by using a constrained weighted summation method.

In conclusion, as can be observed in Table 1 , evolutionary algorithms (EAs) are the most widely used methods to design reliable DNA sequences. Indeed, EAs are the only method used to design sequences for molecular computing in the last decade.
Moreover, most of works published take in consideration more than one biological design criterion to create more reliable sequences. However, only Shin et al. (2005) and Wang et al. (2009) combine several DNA design criteria in a multi-objective approach. In this paper, a novel multiobjective evolutionary algorithm (MOEA) to tackle the problem is proposed. The proposal was designed by considering different con fl icting design criteria. It is proved that in those cases MOEAs are very suitable alternatives ( Coello et al., 2002 ; Deb, 2001 ). In fact, Shin et al. (2005) proposed a constrained multiobjective evolutionary algorithm (NACST/Seq) which obtained the best results published so far. Therefore, the results obtained in our work will be compared with results obtained in that study in Section 6 . 3. DNA sequence design for reliable computing
The design of reliable DNA sequences is crucial in many bio-molecular technologies, such as nanotechnology, DNA sequencing or DNA computing ( Brenneman and Condon, 2002 ). One of the most important processes in all those technologies is the
Watson  X  Crick pairing ( Garzon and Deaton, 1999 ), or the hybridi-zation between a sequence and its basepairing complement ( Fig. 1 ). The problem here is to control undesirable hybridizations, because they can produce errors in the biological reactions, so they have to be avoided when sequences are designed.
 reliable sequences which form stable duplexes (double stranded
DNAs, Fig. 1 ) while avoiding interactions of non-complementary sequences. Every sequence design criteria should contribute to improving reliability, because this property is a very important requirement for any system based on DNA sequences. There are several biological criteria that can be considered to achieve this purpose. According to their biological meaning, design criteria can be classi fi ed into four groups ( Shin et al., 2005 ). First, properties that avoid inconvenient reactions; second, criteria that control the generation of secondary structures; third, properties that control the biochemical characteristics of DNA sequences; and fi nally, criteria that restrict the sequences composition. The fi rst group was described previously, and it consists of generating sequences that only form duplexes with their complements. This feature is crucial for DNA computing, so our system takes in consideration several criteria from this category. Some of the most relevant objectives are similarity , which calculates the inverse Hamming distance between two sequences, and H-measure , which is also based on the Hamming distance and tests the possibility of unintended DNA basepairing. Both criteria are checked by con-sidering shifts in sequences under study.
 formed by the interaction of single stranded DNA, and they include hairpin , internal loop and bulge loop ( Fig. 2 ). Several criteria to avoid the formation of secondary structures are considered in this work. One of the most extended restrictions is the continuity , which counts the repetitions of identical bases. This is important because if one base is repeated several times, an unusual second-ary structure could be formed. The third category refers to the biochemical characteristics of the sequences. It is important to control that every sequence have similar chemical features.
Restrictions in this group include: free energy , which is de as the energy needed for breaking a duplex; melting temperature , which is the temperature at which half of the DNA strands are in the double-helical state ( Fig. 1 ) and half are in a random coil state (dissociated) ( Lucia, 1998 ); GC ratio , which indicates the percen-tage of cytosine (C) and guanine (G) in a sequence. Both, the melting temperature and the free energy are very reliable mea-sures to control the relative stability of the DNA duplex. The GC content, though less precise, is very easy to compute. Finally, in case of necessity, the composition of DNA sequences can be restricted, for example, to create a concrete DNA subsequence which controls a speci fi c biological reaction. 3.1. Multiobjective formulation
Sequence design problem involves simultaneously several con fl icting criteria (or objectives), so it can be formulated as a multiobjective optimization problem ( Wang et al., 2007 ). The goal of any DNA sequence design system consists of obtaining a set of stable and reliable DNA sequences that satis fi es certain biochem-ical constraints. As a general rule, the more biochemical criteria considered, the more reliable the system is, but it is not compu-tationally practical to include a very high number of constraints, and some of them even overlap with others, so it is necessary to study carefully which objectives will be part of the system. After revising related publications, six biochemical criteria were chosen.
These criteria (or objectives) have to be simultaneously optimized by using a multiobjective evolutionary algorithm (MOEA). The objectives are: continuity, hairpin, similarity, H-measure, GC ratio and melting temperature, and all of them have to be minimized.
As is known, a multiobjective optimization problem is well formed if there is not a single solution that simultaneously minimizes each objective, but a set of alternatives solutions, known as Pareto-optimal solutions ( Wang et al., 2007 ) which provide a multi-objective solution to the problem. The design of reliable DNA sequences can be formulated as a multiobjective problem as follows.

Minimize y  X  F  X  x  X  X  X  f 1  X  x  X  ; f 2  X  x  X  ; ... ; f n  X  x  X  X  X  1  X  where f 1 ( x ) are the biochemical criteria (or objectives) previously mentioned. A formal de fi nition of each objective is given bellow. (1) H-measure : This objective is very important, because it prevents cross hybridization between sequences. H-measure is calculated by computing how many nucleotides are com-plementary between the sequences in a generated set.

Sequences in this set are supposed to not interact with each other, so H-measure is an objective to be minimized. To provide a more reliable measure, elongated sequences with gaps are considered. The mathematical de fi nition for this objective is described in Eq. (2) . where  X  i and  X  j are anti-parallel sequences and n is the number of sequences which are in the set of DNA sequences (2) Similarity : This objective computes, including shift positions, ) (3) Continuity : Indicates the degree of successive occurrences of 3.2. Multiobjective function multiobjective optimization problem and solved by using a multi-objective evolutionary algorithm (MOEA). The problem has been formulated by considering four objectives (similarity, H-measure, continuity and hairpin) and two restrictions (melting temperature and GC content) as expressed in Eq. (11) .

Minimize f i  X  x  X  ; 4. Description of the algorithms
In this section, the representation of the individuals along with the explanation of the algorithms developed in this work, are detailed. 4.1. Representation of the individuals
Each individual includes the necessary information to generate valid set of sequences that can be applied to DNA computing. The representation adopted is very important because the behavior of the algorithms developed depends on the design of the individual. More-over, as our study is based on different instances, the length and the number of sequences included in the individual are not constant. In
The individual is formed by a set of n sequences. This parameter depends on the problem. Each sequence includes the DNA strand and the values for each constraint taken in consideration (simi-larity, H-measure, continuity, hairpin, GC content and melting temperature). DNA strands are composed of m nucleotides each.
These nucleotides can be: adenine (A), cytosine (C), guanine (G) and thymine (T). Finally, the global quality of each individual is evaluated through the average values for the four objectives considered (Eq. (11) ) of the n sequences. 4.2. Multiobjective arti fi cial bee colony
In this paper, a new MOEA to tackle DNA sequence design problem is proposed. Our proposal is an adapted multiobjective version of the Arti fi cial Bee Colony (ABC) algorithm. ABC is a population based algorithm proposed by Karaboga and Akay (2009) motivated by the intelligent behavior of honey bee swarms.
In ABC, the colony of arti fi cial bees contains three groups of bees: employed, onlookers and scouts bees. Employed bees go to their food source and come back to hive and dance on this area.
Onlookers watch the dances of employed bees and choose food sources depending on them. Finally, scout bees explore new areas with the aim of fi nding new sources of food.

As DNA sequence design was form ulated as a multiobjective optimization problem, a new multiobjective algorithm (MO-ABC) was de fi ned. This new algorithm incl udes to the original design of
ABC some multiobjective techniq ues from well-known multiobjec-tive algorithms. For example, from NSGA-II (Fast Non-Dominated
Sorting Genetic Algorithm ( Deb et al., 2000 )), it was taken the idea of non-dominated sorting . This kind of sorting organizes the population into different categories according to their relation of dominance ( Deb et al., 2000 ). Other multiobjective concept was taken from PAES (Pareto Archived Evolution Strategy ( Knowles and Corne, 1999 )): the non-dominated solution archive (NDS archive). This archive keeps updated the best solutions found, according to an acceptance function ( Knowles and Corne, 1999 ). In Algorithm 1, the pseudocode for the multiobjective version of the Arti fi cial Bee Colony (MO-ABC) is presented.

In the fi rst place, the fi rst half of the colony, C ,is random employed bees (line 2). DNA sequences are randomly generated with A, C, G and T nucleotides. After that, the value of each objective and constraint for each sequence is obtained by using the formulation explained in the previous section. Then, employed bees in C are classi fi ed according to different Pareto fronts by ranking and afterwards each bee is sorted according to non-dominated ranks (line 3). A particular bee is of more quality than another if it is dominated by fewer bees. A particular bee dominates another if it is better, at least, in one of the objectives and it is not worse in any of the others. After the dominance ranking, the crowding distance ( Deb et al., 2000 ) of each indivi-dual is also calculated (line 4). An individual is better than another if it has the same or fewer dominance ranking and it presents higher crowding distance ( Deb et al., 2000 ), because solutions which are located in lesser crowded regions are preferred.
Previous to the main loop of the algorithm, the set of non-dominated solutions ( NDS_archive ) is also initialized, as is indi-cated in line 5 of Algorithm 1.

Algorithm 1 Pseudocode for MO-ABC 1: / n Generate the fi rst half of the colony C (number of 2: C  X  generateEmployedBees ( nEB ) 3: C  X  fastNonDominantedSort ( C, nEB ) 4: C  X  crowdingDistanceAssignment ( C, nEB ) 5: NDS_archive  X   X  6: while not stop condition satis fi ed do 7: / n Employed bees search for solutions in their 8: for i  X  1to nEB do 9: indMut  X  mutateBee ( C i , MutProb ) 10: C i  X  updateEmployedBee ( C i , indMut ) 11: end for 12: / n Generate a probability vector using the employed 13: probVector  X  generateProbabilityVector ( C , nEB ) 14: / n Generate the second half of the colony C (number of 15: for i  X  nEB to nOB do 16: C selected  X  selectEmployedBee ( probVector , C, nEB ) 17: indMut  X  mutateBee (C selected , MutProb) 18: C selected  X  updateOnlookerBee ( C selected , indMut ) 19: end for 20: / n Generate scout bees n / 21: for i  X  1 to nEB do 22: if C i .iterations 4 limit then 23: C i  X  replaceWithScoutBee () 24: end if 25: end for 26: / n Sort the colony by quality n /
Algorithm 1 Pseudocode for MO-ABC 27: C  X  fastNonDominantedSort ( C ) 28: C  X  crowdingDistanceAssignment ( C ) 29: / n Update Pareto solutions archive n / 30: NDS_archive  X  updateNDS_archive( C ) 31: end while
Every generation of the algorithm (lines 6  X  31) can be divided into four steps. Firstly, the fi rst half of the colony, C , which contains the employed bees, is improved (lines 8  X  11). This kind of bees are improved by applying a random mutation to each individual (the amount of mutation is de fi ned by the mutation probability
MutProb ). After that, the algorithm checks whether the mutated bee ( indMut ) is better in terms of dominance and crowding distance (multiobjective indicators). In that case, C i is replaced by indMut ; otherwise, C i stays in the colony and the mutated individual is discarded. Once the employed bees have been processed, a probability vector is generated (line 13). This vector contains the probability of an employed bee to be selected in the next step.

Secondly, the second half of the colony (lines 15  X  19), which contains the onlooker bees, is produced. To generate this kind of bee, an employed bee, C selected , must be selected according to the probability vector previously generated. After applying a mutation to the selected bee (line 17), it is checked whether this mutated bee, indMut , obtains better or equal values of dominance and crowding distance. In that case, indMut is stored in the colony; otherwise, the selected bee, C selected , is saved and the mutated individual is discarded. Allowing equal quality values helps to promote population diversity.

The third step consists of generating scout bees (lines 21
The limit parameter (line 22) plays a key role in this step. If the solution associated to a bee is not improved in limit iterations, it must be discarded because it is considered a local optimum. Then, this individual becomes a scout bee which will be randomly generated and improved by using a local search that tries to improve the four objectives included in our problem (Eq. (11) ).
Each scout bee will explore different undiscovered regions of the search space looking for new high-quality solutions.

Once the fi rst three stages have been completed, the whole colony is sorted by quality (lines 27  X  28) and the best half of the colony will be the new employed bees for the next generation.
Finally, in line 30, the NDS (non-dominated solutions) is updated by saving the best ranked solutions in the population. When the algorithm fi nishes, this record will be processed to obtain the best ranked solutions in the global process. 4.3. Fast non-dominated sorting genetic algorithm
The Fast Non-Dominated Sorting Genetic Algorithm (NSGA-II) is a classical multiobjective metaheuristics created by Deb et al. (2000) that tries to improve the current population, P , with an offspring, Q , by applying classical genetic operators. These opera-tors are binary tournament selection (line 5, Algorithm 2), uniform crossover with a probability factor ( Cr , line 6) and random mutation which is also controlled by a probability ( f , line 7). NSGA-II is a widely used algorithm in multiobjective optimization.
In fact, it has become very popular in the last few years; such that it has become a reference algorithm. For that reason NSGA-II was included in our study, to obtain reference results that could be used to compare with the results provided by our proposal (MO-
ABC). NSGA-II applies a Pareto-compliant ranking method that is on the side of non-dominated solutions, and it is also used in our multiobjective version of ABC. Algorithm 2 shows the pseudocode of our version of NSGA-II.
 of valid solutions (line 1). The algorithm tries to improve that population in each iteration by creating a descendant population,
Q (with PSize individuals) , with the previously mentioned genetic operators (lines 4  X  8). After that, the algorithm combines the two populations to create R of size 2 PSize (line 9). Next, a non-dominated sorting function is used to classify the population R in different Pareto fronts (line 10). The new population is generated from these fronts ( fi rst with F 1 , then with F 2 , and so on, lines 12 15). As R contains 2 PSize individuals and there must be only
PSize in the descendant population, not all the elements in R will be part of the new population. Those fronts that cannot be accommodated will be removed. If there are individuals of the last front that cannot be added to the new population, the remaining will be chosen by using the crowding distance (lines 16  X  17). 5. Experimental methodology and results work is detailed. Moreover, results obtained with the developed algorithms (MO-ABC and NSGA-II) are presented and discussed. 5.1. Experimental methodology with 1 GB of RAM. In order to reproduce our work under the same conditions, it is necessary to clarify that all source codes of the developed MOEAs were compiled by using gcc 4.4.5 compiler with no optimization options.
 ( Shin et al., 2005 ), all the developed algorithms use the same stop condition and population size as con fi gured by them. The popula-tion size was established to 3000 individuals, and the maximum number of generations was 200 ( Shin et al., 2005 ). Three sets of
DNA sequences with different sizes have been tackled. These datasets were proposed by different authors ( Deaton et al., 2002 , 1996 ; Tanaka et al., 2002 ; Faulhammer et al., 2000 ) and they have been successfully applied to reliable DNA computing. Details of these sequence sets will be given in the following sections, but problem instances with different number of sequences and differ-ent sizes (nucleotides per sequence) ensure that our algorithms are able to work with different types of instances that have been tested for bio-molecular computing.

Since MOEAs are stochastic methods, a statistical analysis was performed in this work with the aim of ensuring a certain level of con fi dence. First, a test to check whether the obtained values followed a Gaussian distribution was performed. Kolmogorov
Smirnov test ( Sheskin, 2007 ) was used for this purpose. If the result of this test is not satisfactory, in other words, values follow a non-Gaussian distribution, a non-parametric test, such as Kruskal
Wallis analysis, is mandatory. In case of being satisfactory, the homogeneity of the variances was veri fi ed by using the Levene test.
If the result of this test is af fi rmative, an ANOVA analysis (parametric homogeneous, the non-parametric Kruskal  X  Wallis analysis is used.
The con fi dence level adopted in this paper is always 95% in every statistical test (signi fi cance level of 5% or p -value under 0.05). This means that differences are unlikely to have occurred by chance with a probability of 95%. For each MOEA, 30 independent runs were performed in order to ensure statistical signi fi cance. As our study is immersed in a multiobjective environment, the value for each para-meter was established according to the quality of the Pareto front produced for each parameter. In this work, two well-known multi-objective quality indicators have been used: Hypervolume ( HV )and Set
Coverage ( SC )( Zitzler et al., 2000 ). On the one hand, the Hypervolume metrics evaluates the volume (in the objective function space) covered by members of a non-dominated set of solutions (Pareto front solutions). In order to calculate this metrics, two reference points are required, r min ( obj1 min , obj2 min , obj3 min and obj4 ( obj1 max , obj2 max , obj3 max and obj4 max ), where each element of those points are the four objectives considered in our problem (similarity,
H-measure, hairpin and continuity). Note that HV is not free from arbitrary scaling of objectives, that is to say, the value of this metrics maybedistortediftherangeofeachobjectivefunctionisdifferent.
Thus, before calculating this indica tor, the objective function values have to be normalized. Normalization points for each DNA sequence set generated in our study are presented in Table 2 (Min and Max values). Therefore, after normaliza tion, the ideal reference points are r min  X  (0,0,0,0)and r max  X  (1,1,1,1)foralldatasets.Notethat normalization points will be different for each set of DNA sequences and they were calculated from the experience.

On the other hand, if two sets of non-dominated solutions obtained by two different algorithms, A and B , are considered, the Set Coverage ( SC ) measures the fraction of non-dominated solu-tions obtained by B which are covered by the non-dominated solutions obtained by A .If SC ( A , B )  X  1, all points in B are dominated by or equal points in A , whereas SC ( A , B )  X  0 means that no point in B is covered by the set of A . As the dominance operator is not symmetric, it is necessary to calculate both SC ( A , B ) and SC ( B , A ). 5.2. MO-ABC and NSGA-II results
In this subsection, a comparison between our swarm proposal (MO-ABC) and the well-known Fast Non-Dominated Sorting Genetic Algorithm (NSGA-II) is discussed. The parameters of each algorithm were adjusted separately. Moreover, the work followed the statistical study described in the previous subsection. The parameter con fi guration for each proposal, along with the infor-mation for the three sets of sequences used in our study is summarized in Table 2 . It is important to point here that parametrical setting indicated in Table 2 is the result of a complete experimental process, in which the value for each single para-meter has been established following the experimental methodol-ogy described in the previous subsection.

Table 3 summarizes the multiobjective results in terms of the hypervolume indicator obtained with our two proposals. Results show average hypervolumes of 30 independent runs for the three sets of sequences under study. Furthermore, Table 4 presents a summary of the statistical analysis performed to the fi nal results provided by our MOEAs. Conditions for ANOVA test were passed (residual normality and variance homogeneity). The null hypoth-esis of the test is that the average values provided by our algorithms for each data set are not signi fi cantly different (con-sidering a con fi dence level of 95%). This hypothesis must be rejected, because signi fi cance values are lower than 0.05 in every case (as shown in Table 4 ). In this way, it can be said that the results obtained by using our two proposals (and in every data set) present differences which are statistically signi fi cant.
As can be seen in Table 3 , MO-ABC approach obtains better results than NSGA-II for all data sets. In addition to this, it can be observed that differences are more signi fi cant for the bigger instances, with 14 and 20 sequences. Moreover, standard devia-tions in all cases are minimal and, as shown in Table 4 , results present differences statistically signi fi cant. Therefore, it can be said that our swarm proposal (MO-ABC) is able to explore the search space better than the standard NSGA-II, and consequently, the sets of sequences generated will be of more quality attending to the four objectives that are considered in this work (similarity, H-measure, hairpin and continuity). This means that sequences generated with MO-ABC will be more reliable for DNA computing than sequences generated with NSGA-II.

On the other hand, a comparison between the MOEAs was performed by using the Set Coverage ( SC ) indicator. SC allows comparison of two multiobjective metaheuristics in terms of
Pareto dominance. Table 5 shows how the non-dominated solu-tions of MO-ABC cover more than 57% of the non-dominated solutions of NSGA-II, while NSGA-II covers less than 15% in the best case. For the instance with 7 sequences and 20 nucleotides per sequence, MO-ABC covers 92.479% of the non-dominated solutions provided by NSGA-II, but this algorithm only covers 6.227% of non-dominated solutions provided by MO-ABC. If the reader focuses on the data set with 20 sequences (and 15 nucleotides per sequence), MO-ABC covers all the non-dominated solutions produced by NSGA-II, and on the contrary, NSGA-II cannot cover any non-dominated solutions generated by
MO-ABC. Even in the instance with 14 sequences, MO-ABC is able to cover 57.25% of non-dominated solutions generated by NSGA-II, versus 14.35% of NSGA-II. This means that many of the sets of DNA sequences generated by MO-ABC dominate the sets of DNA sequences found by NSGA-II, because Pareto fronts provided by MO-ABC are, in general, of better quality.
 known multiobjective metrics ( HV and SC ), MO-ABC obtains better results than the provided by the classical NSGA-II algorithm. In a global view, the MO-ABC algorithm seems to be a very promising approach to generate sets of reliable DNA sequences that can be applied to molecular computing. 6. Comparison with other authors the generation of reliable DNA sequences. In this section, the sets of DNA sequences obtained by our best algorithm, MO-ABC, are carefully analyzed by comparing our results with the results provided in other relevant studies published in the literature. Our study is supported by reliable instances previously used for
DNA computing. As described in Table 2 , reference sequences were taken from Deaton et al. (2002 , 1996 ), Tanaka et al. (2002) , and
Faulhammer et al. (2000) . The best multiobjective study for the problem tackled is the work by Shin et al. (2005) , so our results are compared with that study (which uses the same data sets).
However, the comparison is made by considering each objective separately, because unfortunately, neither in that study nor in other studies, multiobjective indicators, such as hypervolume or set coverage , have been taken into account so far. Therefore, for each data set the quality of each design criterion is examined for a set of sequences taken from the optimal Pareto front.
 parametric adjustment for the biochemical constraints was estab-lished. Therefore, for similarity and H-measure , it was established lower limits for the continuous case equal to six nucleotides and 17% for the discontinuous case. For continuity , the threshold value was 2. Hairpin formation requires at least six basepairings and a six base loop. The melting temperature ( T m ) was obtained by using the nearest neighbour model with 10 nM DNA concentration and 1 M salt concentration. Finally, it is important to note that every value was decided empirically with biochemical background ( Shin et al., 2005 ).

GC ratio and melting temperature were considered constraints for the problem with the same limits as the established in the literature. Thus, in Deaton et al. (1996) , Table 6 and Tanaka et al. (2002) , Table 7 , sequences have the GC ratio restricted to 50% and the melting temperature restricted between 46 1 and 53 1 s. On the other hand, in ( Faulhammer et al., 2000 , Table 8 ), the range of the
GC ratio was established between 40% and 50% and the melting temperature between 31 1 and 39 1 . Shin et al. (2005) uses the same restrictions. The comparison between our results and those studies are given in Tables 6  X  8 and in Figs. 6  X  8 .
Deaton et al. (1996) propose a genetic algorithm to design good sequences for Adleman ' s graph. Shin et al. (2005) propose a new algorithm based on NSGA-II named NACST/Seq. Results given in
Table 6 and Fig. 6 show that our approach obtains sequences with lower similarity and H-measure values, while obtaining minimal values for hairpin and continuity objectives. This means that sequences obtained with MO-ABC have higher probability to hybri-dize with its correct complementary sequences, while secondary structures are virtually prohibited, due to the minimal values obtained for continuity and hairpin constraints. In addition to this, it can be observed that ranges for melting temperature ( T better, while keeping the same values for GC ratio.

On the other hand, Table 7 shows the sequences obtained in the study by Tanaka et al. (2002) by using simulated annealing and in the study by Shin et al. (2005) by using NACST/Seq. The data set used in this case included 14 DNA sequences of 20-mer nucleo-tides. Our proposal, MO-ABC, is able to obtain better sequences by considering all measures ( Fig. 7 ). Continuity and hairpin values are again minimal, avoiding the occurrence of secondary structures, while H-measure and similarity also present lower values than in Shin et al. (2005) and Tanaka et al. (2002) . Even melting tempera-tures are more regular and lower than in the other two studies. Therefore, sequences generated by MO-ABC are more reliable for bio-molecular computing.

Finally, Table 8 presents the sequences generated in Faulhammer et al. (2000) for the chess knight movement problem. These sequences were veri fi ed by real-laboratory experiments. Faulhammer et al. (2000) designed sets of 20 sequences with 15 nucleotides which were improved by sequences generated by Shin et al. (2005) by using the
NACST/Seq algorithm. As occurred with the previous instances ana-lyzed, sequences generated by our approach seem to be more reliable for DNA computing because they ar e more dissimilar between them ( similarity is lower), they avoid secondary structures ( hairpin and continuity values are minimal, Fig. 8 )andmeltingtemperatureandGC ratio fl uctuate in smaller ranges.
 different data sets and a variety of approaches published in the literature, it can be stated that our multiobjective proposal (MO-ABC) is able to obtain better DNA sequences, by considering four different objectives (continuity, similarity, H-measure and hairpin) and two constraints (GC ratio and melting temperature), than the results provided in other relevant studies. 7. Conclusions and future work
In this work, it has been proposed the use of a new multi-objective approach based on Arti fi cial Bee Colony (ABC) metaheur-istics (MO-ABC) for the design of reliable DNA sequences that can be applied to molecular computing. MO-ABC is able to obtain sets of sequences which simultaneously minimize similarity, H-mea-sure, continuity and hairpin of each generated sequence in the set. Solutions are also restricted for a speci fi c melting temperature and GC ratio which provide even more stable sequences. Furthermore, a well-known MOEA was also developed to solve the problem: the Fast Non-Dominated Sorting Genetic Algorithm (NSGA-II). Results provided by this approach were compared with results from our proposed metaheuristics by using multiobjective metrics to ensure the accuracy of MO-ABC. Hypervolume indicator and the coverage relation were applied to present our multiobjective results. More-over, each experiment is supported by a statistical analysis that demonstrates its statistical relevance. Three different real-world datasets proposed by different authors were used to ensure the effectiveness of our algorithms. These data sets include different number of sequences, number of nucleotides per sequence and restrictions, and all of them have been successfully used for DNA computing. MO-ABC can generate better sequences taking in consideration all the objectives and restrictions than other approaches previously published in the literature.

Due to the good results obtained with MO-ABC, as future work, it could be interesting to work with other multiobjective approaches based on swarm intelligence that can be applied to the problem. It is possible that a hybrid version of MO-ABC with other multi-objective approach could provide very interesting results. In addition to this, it would be interesting to generate even larger sets of sequences by applying parallel computing together with evolutionary computation. MOEAs generally takes signi fi cant CPU time to fi nd optimal sequence sets, because the high dif fi culty of the design problem addressed is combined with the explorative behavior of evolutionary algorithms.
 References
