 Classical probabilistic information retrieval (IR) models, e.g. BM25, deal with document length based on a trade-off be-tween the Verbosity hypothesis, which assumes the indepen-dence of a document X  X  relevance of its length, and the Scope hypothesis, which assumes the opposite. Despite the effec-tiveness of the classical probabilistic models, the potential relationship between document length and relevance is not fully explored to improve retrieval performance. In this pa-per, we conduct an in-depth study of this relationship based on the Scope hypothesis that document length does have its impact on relevance. We study a list of probability density functions and examine which of the density functions fits the best to the actual distribution of the document length. Based on the studied probability density functions, we pro-pose a length-based BM25 relevance weighting model, called BM25L, which incorporates document length as a substan-tial weighting factor. Extensive experiments conducted on standard TREC collections show that our proposed BM25L markedly outperforms the original BM25 model, even if the latter is optimized.
 H.4 [ Information Search and Retrieval ]: Retrieval mod-els Algorithms, Experimentation, Theory Probabilistic IR, BM25, Document length, Normalization
An information retrieval system receives a query from user and returns the supposedly relevant documents. A crucial issue underlying an IR system is to rank the returned docu-ments by decreasing order of relevance. Generally, ranking is based on a weighting model . The basic probabilistic model proposed in [23] is one of the most popular weighting models in modern IR systems, which is developed from the Prob-ability Ranking Principle [22]. This probabilistic approach is refined based on the Verbosity hypothesis [24] which as-sumes independence of document length from relevance. In other words, long documents simply use more words than short documents to cover similar scope [24].

An opposite assumption about document length is the so-called Scope hypothesis, which states that some docu-ments may contain more material than others if longer [24]. That is, long documents are more likely to be retrieved. In practice, a document may be considered as a trade-off be-tween the Verbosity hypothesis and the Scope hypothesis. How to balance between these two hypotheses by model-ing document length within the basic probabilistic weighting paradigm remains a challenging research issue. The impact of document length on relevance is particularly important for ad-hoc retrieval, where relevance is defined in a binary or graded manner. Compared to a short document, a long document is likely to be relevant if it contains paragraphs that meet the information need of the query, even if a large part of the document is in fact non-relevant.

To address the effect of document length on relevance, the basic probabilistic weighting function takes into account the document length d as follows [24]: where w ( x, d ) is the relevance weight of a given document. d is the document evidence for relevance, which is given by document length.  X  denotes the average document length of the reference vector 0 ,and x represents all other information about the document. R and R stand for the non-relevance and relevance, respectively. This function measures the dif-ference between the probabilities of document length and all other information we have for the document when it is rel-evant and when it is not relevant, respectively, in log scale. The above equation also implies a relevant document should receive a higher weight than a non-relevant document in or-der to achieve a satisfying retrieval performance.
Equation 1 can be further decomposed into the three com-ponents as follows [24]: where and
Under the Verbosity hypothesis, document length has been considered as independent evidence of relevance. This hy-pothesis nullifies the component w ( d,  X ) 22 in Equation 2, which as a consequence is set to zero [24]. Thus, the weight-ing function becomes The classical BM25 weighting model [10] is derived from Equation 3, more specifically, w BM 25 = w ( x ,d ), where w is the relevance score of BM25, given by the following weight-ing function [10]: where N is the number of indexed documents in the collection, n is the number of documents containing the query term, R is the number of known relevant documents to a specific topic, r is the number of relevant documents containing the term, tf is within-document term frequency, qtf is within-query term frequency, dl is the document length (i.e. the docu-ment evidence d in Equation 2), avdl is the average docu-ment length, nq is the number of query terms, k i sand b are tuning constants (whose setting depends on the dataset used and is usually empirically determined), and  X  indicates that its following component is added only once per document. Particularly, b functions as a justification factor that adjusts the relative importance between the two hypotheses [25].
The main focus of this research is to study the relation-ship between document length and relevance in the context of the Scope hypothesis by exploring a list of probability density distribution for document length. The Scope hy-pothesis suggests the existence of a relationship between document length and relevance. It implies that the com-ponent w ( d,  X ) 22 in Equation 2 may not be zero. In this paper, we consider document length itself as a direct pre-dictor of relevance. Our study of document length is based on the intuition that long documents tend to have high re-trieval probabilities, since long documents usually have a large number of unique terms, which are likely to be picked up by the query term matching [28]. Our experiments show that long documents tend to have high frequencies of query terms, which leads to high relevance scores. This provides evidence supporting the Scope hypothesis that there exists a dependency of relevance on document length. Particularly, there are two extreme cases that need to be considered. One is that the relevance of each document in a collection is in-dependent of its length. This is the case of BM11 or b =1 in BM25 [25]. The other one is BM15 or b = 0 in BM25 [25], which implies that long documents are likely to be retrieved. To balance between these two extreme cases, Robertson et al. introduce a parameter b in BM25 to control the effect of tf normalization [25]. In our study, we first use statistical tools to learn the pattern of the relationship between the document length and its relevance, investigate the behavior of w ( d,  X ) 22 in Equation (2), and propose a new weight-ing function incorporating this relationship, which is the re-sult of a mixture of the two hypotheses. Our results show that the retrieval performance of BM25 can be markedly improved over different settings of the parameter b by ex-ploiting the document length evidence.

The rest of the paper is organized as follows. Section 2 gives a brief survey in previous work. Then Section 3 in-troduces the datasets and presents the idea of the length-based weighting function, and proposes seven models based on the density analysis. The proposed models are evaluated through extensive experiments in Section 4. Finally, we con-clude on the work and suggest future research directions in Section 5.
The classical probabilistic models for IR rank documents according to their relevance scores, assigned by matching the query terms with adjustment for the relationship be-tween document length and term frequency. This approach is developed based on the Verbosity hypothesis which as-sumes the document X  X  relevance is independent of its length [24]. However, in practice, the impact of document length on relevance may be a mixture of both the Scope hypothesis and the Verbosity hypothesis [24].

Many previous studies have been conducted to investi-gate the impact of document length on relevance. Singhal et al. [28] suggest that long documents tend to have more unique terms, and consequently, long documents have a bet-ter chance to be retrieved than short documents. As the document length increases, the number of times the query terms occur in the documents also increases, which in turn increases the matching score. For instance, Singhal et al. illustrate that the probability of a document X  X  relevance in-creases proportionally with document length in the early TREC test collections [28, 29]. Similar results have also been reported on the later  X  X d-hoc X  test collections [15]. More-over, a number of empirical studies have provided statistical evidence supporting that the probability of a document X  X  rel-evance to an information need is considered to be correlated with the length of the document. Kraaij et al. show that the probability of relevance is positively correlated with docu-ment length on a number of TREC ad-hoc and Web collec-tions [16]. Singhal et al. state that the documents retrieved by a model produce a retrieval pattern by the distribution of the document length [28]. Huang et al. use functional curve to approximate the distribution of document length on the TREC data sets and conclude that the retrieval system can be improved through an appropriate document length func-tion [12, 13]. Furthermore, proper term weighting strate-gies based on document length can also improve retrieval performance [31]. For example, normalization techniques have been applied for each term in the query through the length adjustment to avoid the bias introduced by document length. Normalizing the document length within a retrieval system could improve the performance [7]. By applying sta-tistical regression of the similarity scores within the normal-izing document length and query size, Lamprier et al. show that a significant improvement can be made to IR systems [17]. Blanco et al. device a probabilistic document length prior for language modeling [4]. Losada et al. apply smooth-ing techniques for document length in language modeling to show the significant impact of document length on the in-formation retrieval performance [19]. They also argue in [20] that the causal link between document length and its relevance may not exist when incompleteness is taken into account, although the evidence is not concrete enough to nullify the effect of document length on relevance. For the issue addressed in [20], we set up the experiments in this paper using Expectation-maximization (EM) algorithm and bootstrapping method [8] to avoid this problem.
Under the Scope hypothesis, w ( d,  X ) 22 in Equation 2 is no longer zero since a dependence of relevance on document length is assumed. To add the length information into the weighting function w ( x, d ), we decompose the w ( d,  X ) ther into
The second component of Equation 5 is constant over a given document collection. This is because the average doc-ument length  X  for the reference vector 0 in a document collection is known and fixed. Therefore, for each docu-ment in a collection, the second component of Equation 5 above is the same across the whole document collection and does not affect the document ranking. For simplicity, we refer w ( d,  X ) 22 to as the first component in the Equation 5. Thus, the relevance weight w ( d,  X ) 22 is given by the log-odd of the relevance and non-relevance probabilities P ( d | R )and P ( d | R ). In other words, w ( d,  X ) 22 measures the difference between the probabilities of given document length condi-tion on relevance and non-relevance in log scale. We name Equation 5 as length relevance weighting . Our ultimate goal is to calculate the w ( d,  X ) 22 in Equation 5, this needs a way to estimate the probabilities P ( d | R )and P ( d | R ). By adding the measurement of document length itself into the basic weighting function, the re trieval system is expected to achieve high accuracy since the length information brings more evidence of relevance. The estimation of probability distribution function 1 of document length will be discussed in the next subsections.

The density estimation has three parts. First, we study the distributional pattern of document length on standard TREC test collections using kernel density estimation method [3, 27], which gives us a guidance in density estimation. In the second part, we apply data transformation techniques on the document length in order to get a better fitting of the document length distribution. Finally, Maximum Likeli-hood Estimation (MLE) is applied to obtain the distribution parameters estimates of document length. Length relevance weighting function is then derived based on the above den-sity estimation.

In the rest of this paper, we use d to denote the document length. As a general rule, we usually make an assumption about observed d  X  X , i.e. d 1 ,d 2 , ..., d N are independent and identically distributed, N is the number of documents in the collection. We first introduce the test collections we used and then give the details of density estimation in the following subsections.
We use probability distribution function and probability density function interchangeable in the rest of this paper.
We examine the impact of document length on relevance using 4 standard TREC test collections. These four test collections are the most recent TREC datasets, and provide a good coverage on the a variety of commonly used datasets in IR evaluation, and are used for different test purposes andvaryinsizeintermofthedocumentlength. Basic information about the test collections and topics are given in Table 1.
 Table 1: Information about the test collections.

The disk1&amp;2 collection contains newswire articles from various sources, such as Association Press (AP), Wall Street Journal (WSJ), Financial Times (FT), etc., which are usu-ally considered as high-quality text data with little noise. It usually used for ad hoc test. The WT10G collection is a medium size crawl of Web documents, which was used in the TREC 9 and 10 Web tracks. It contains 10 Gigabytes of uncompressed data. The .GOV2 collection, which has 426 Gigabytes of uncompressed data, is a crawl from the .gov domain. This collection has been employed in the TREC 14 (2004), 15 (2005) and 16 (2006) Terabyte tracks. The ClueWeb collection is a very large crawl of the Web, and is currently the largest TREC test collection. We use the category B of ClueWeb, which contains about 50 million En-glish Web pages, and its associated topics used in the TREC 2009 Relevance Feedback track. We index all documents in the above four collections. For all four test collections used, each term is stemmed using Porter X  X  English stemmer, and standard English stopwords are removed.
Kernel density estimation (or Parzen window method) is a non-parametric way of estimating the probability density function of a random variable. It can be used to extrapolate the data to the entire population as follows [27]: where d i ,i =1 ,...,n is the independent and identically-distributed sample from some unknown distribution, n is the number of samples we draw from the population, K is the kernel function and h is the bandwidth (also called smoothing parameter). We can obtain the smoothing curve by adjusting the parameter h . Usually K is instantiated by a standard Gaussian function with a mean of zero and a variance of 1: Kernel density estimation gives us a global picture of the given dataset.

Figure 1 shows the distributional pattern of relevant and non-relevant document length for the test collections disk1&amp;2, .GOV2, WT10G and ClueWeb B respectively using kernel density estimation with the standard normal kernel func-tion. In Figure 1, the length curves have been cut when the document length is large than 1500 in x  X  axis in order to vi-sualize the difference between the relevant and non-relevant document length because the long documents have low fre-quencies to be emerged in test collections. The length of for document length non-relevant and relevant documents from four test collec-tions are both positively skewed and have a long tail with different tail shapes. Non-relevant document has higher fre-quency than relevant documents when the document length is relatively short. The curve on disk1&amp;2 appears to have two distinct peaks called bimodality. In contrast, there is only one mode that arises on WT10G, .GOV2 and ClueWeb B.

We also list the basic statistics in Table 2 for the length of relevant and non-relevant documents on each of the four test collections. At first glance, we may not see much dif-ference between relevant and non-relevant documents from these two tables. However, further looking into the Table 2, we observe that disk1&amp;2 has the longest document length in term of maximum and the shortest document length in term of mean and minimum value, but relative high variance, which indicate that document length of disk1&amp;2 is spread out over a large range of values. The .GOV2 has the high-est variability and mean value among the four test collec-tions. The length of relevant and the length of non-relevant documents are very close to each other on the ClueWeb B. However, the length of relevant and the length non-relevant documents of WT10G differ most among the four test col-lections.

We apply the data transformation techniques to visualize the difference between the relevant and non-relevant docu-ment length on each test collection used. By applying the data transform technique, we can also obtain higher likeli-hood distribution function and achieve more accurate esti-mates of distribution parameters.
The data transformation technique is widely used in data processing or pre-processing for stabilizing the variance and make the data more normal distribution-like. In our case, all document lengths are positive, whose distribution is skewed to the right as described in Figure 1, and document length cannot be described by standard statistical methods because of the skewness. Therefore, data transformation is required to extract a better characteristic of the data. We initiate it by applying central limit theory, i.e. transformation I, it is also called standardization. By standardizing the data, it forces the data to locate on the common scales to be com-pared. Secondly, the Power transformation is from the fam-ily of functions that are applie d to create a rank-preserving transformation of data which improves the correlation be-tween variables and for other data stabilization procedures [5]. Box-Cox power transformation is commonly used to al-leviate heteroscedasticity when the distribution of the vari-able of interest is not known, i.e transformations II and III. Transformation II is the special case of transformation III when  X  = 0. We also transform the document length to be within the scale of 0  X  1 using transformation IV. The four types of transformation is described as follows: where z is the document length after the transformation,  X  is the average document length, s d is the standard deviation of document length, d min denotes the minimum document length and d max is the maximum document length.

Figure 2 plots the distributional pattern of transformed relevant and non-relevant document length on four test col-lections. In Figure 2, we examine length distribution pat-terns of relevant and non-relevant documents on the four test (c) Transformation III collections used. A major observation is that the the curves of the transformed document length distribution have simi-lar shapes before and after the transformation. That is, the curves on disk1&amp;2 remain bimodal, while the curves on the other three test collections are still left-skewed, but not as much as those of the original document length distribution, thanks to the data transformat ion. Moreover, the curves on .GOV2 and ClueWeb B become more symmetric after the transformation. On disk1&amp;2, .GOV2 and WT10g, the cen-ter of the non-relevant document length distribution shifts far away to the right of the relevant document length distri-bution. From Figure 2(d), we can clearly see that relevant and non-relevant document length on Clueweb B can be dis-tinguished from their distributional frequencies. Note that similar observations can also be drawn from other three col-lections used, but the difference between relevant and non-relevance document length distribution is not as obvious as on ClueWeb B. This is an encouraging finding as it gives us clue of differentiating between relevance and non-relevant documents based on their length distribution. In the next section, we propose to fit the document length distribution with a list of statistical distributions, in order to find the distributions that can match the characteristics of relevance and non-relevant documents.
The criterion of selecting distributions is that the distri-bution must be positive skewed with shape and rate param-eters. With different shape and rate parameters, the proba-bility distribution can describe as many different shapes as possible that document length may have. It is impossible to use one single distribution to capture all kinds features of all different document collections. The commonly used distri-butions we applied to fit the transformed document length are as follows:
Figure 3 illustrates the six distribution fittings for the rel-evant document length of four test collections using Trans-formation I. Similar plots can be obtained for the relevant and non-relevant document length of all four test collections using Transformation I, Trans formation III and Transfor-mation IV respectively. All six distributions fit the .GOV2, WT10G and ClueWebB well, not disk1&amp;2 since the bimodal-ity. Inverse Gaussian ans GEV distribution fit the data best on all test collections, Weibull distribution can preserve the skewness better than the Lognormal, Gamma distribution, but normal distribution performs very badly in this case since skewness of the data. After the density functions are fit to the actual length distribution, it is necessary to use goodness of fit test to determine how well the distributions fit to the actual data.
We adopt two methods in distribution parameter estima-tion for P ( d | R )and P ( d |  X  R ) in the Equation 2: bootstrap-ping and expectation-maximization(EM) algorithm. These are very simple but powerful statistical methods in param-eter estimation.

Bootstrapping is a Monte Carlo method to learn about the sample characteristics to infer the population by resam-pling. It has been proved effective in reducing the bias of samples [8]. Ad` er recommend to use bootstrapping when the sample size is insufficient for straightforward statistical inference [1]. The bootstrapping procedure is described as follows: 1. Construct an empirical probability distribution  X  from 2. From the empirical distribution function,  X , draw a 3. Calculate the statistic of interest,  X  , for this resample, 4. Repeat steps 2 and 3 for B times, where B is a large 5. Compute  X   X   X   X  = 1 B
The Expectation-Maximization (EM) algorithm is a gen-eral algorithm for maximum-likelihood estimation where the data are  X  X ncomplete X  [8]. The EM algorithm is an iterative method which seeks to find the MLE of the marginal likeli-hood by iteratively applying the following two steps: where L (  X  ; z ) is the likelihood function,  X  is the parameter vector, z is the transformed document length and y repre-sents the unobserved data.

The estimates from two methods are very close. For sim-plicity reason, we average the estimates from two methods in the experiments.
After we obtain the distribution function for transformed document length Z , we apply the change variable technique [26] to obtain the distribution function for the original docu-ment length D , i.e. the document length before transforma-tion. The theorem of change variable we used is as follows
Theorem 1. Let Z be a random variable with probability density function (pdf) f Z ( z ) and support S Z .Let D = g ( Z ) , where g ( z ) is one to one differentiable function, on the sup-port of Z , S Z . Denote the inverse of g by z = g  X  1 ( d ) and with the support of D which is the set S D = { d = g ( z ): z S } .
 Where z  X  1 ( d ) is equivalent to Equations 8, 9, 10 and 11 when transforming the document length, and |  X  X   X  X  | is the de-terminant of Jacobian of the transformation [11].
Based on the discussion above, we initiate the pattern of the document length by kernel density estimation. Based on the findings in step one, second, we apply data transforma-tion and the change variable techniques to find the distribu-tion functions of relevant and non-relevant document length and use MLE to obtain the parameter estimators. Two sta-tistical methods, EM and bootstrapping are exploited to prevent potential bias during parameter estimation, such as incomplete test collection, randomness of sampling. Hy-pothesis test employees to eliminate the distributions at 95% significant level. Finally Equation 5 is used to construct the following seven models: 1. In this model, transformed relevant and non-relevant 2. In this model, transformed relevant and non-relevant 3. In this model, transformed relevant and non-relevant 4. In this model, transformed relevant and non-relevant 5. In this model, transformed relevant and non-relevant 6. In this model, transformed relevant and non-relevant 7. In this model, transformed relevant and non-relevant
By adding w ( d,  X ) 22 into the weighting function w ( x, d ) or BM25, we propose a new length-based weighting function BM25L as follows: where w BM 25 is the relevance score of BM25,  X  indicate that the term w ( d,  X ) 22 is added only once for each document,  X  is not only the interpolation factor which is empirically de-termined and highly depends on the dataset used, but also an adjust factor of the mixture of two hypotheses: Verbosity and Scope hypothesis. A document could be either extreme or of mixture of these two hypotheses as discussed in[24]. More over, the reason of adding  X  here is that we ignore the constant term in the calculation of log P ( X  | R ) P ( X  | adjust the scale for the weights between w BM 25 and w ( x ,d ), and the weights between two hypotheses because BM25L consider the situation when both Verbosity and Scope hy-pothesis are presented in the same document. For a given query, each of the w BM 25 or w ( x ,d ) scores is normalized by the maximum w BM 25 or w ( x ,d ) score. The parameter  X  is obtained by Simulated Annealing [14] over a set of training topics.
We introduce our methodology for evaluating the BM25L model in Section 4.1, and present the related evaluation re-sults in Sections 4.2 and 4.3.
We evaluate our proposed BM25L model over the 4 test collections used, namely disk1&amp;2, WT10G, .GOV2, and ClueWeb B. Each topic contains three topic fields, namely title, de-scription and narrative. We only use the title topic field that contains very few keywords related to the topic. The title-only queries are usually short which is a realistic snapshot of real user queries in practice.

On each collection, the associated topics are divided into the odd-numbered and even-numbered topics. Over those two topic subsets, our proposed model is evaluated by a 2-fold cross-validation. In each fold, one of the topic subsets is used for training, and the other subset is used for testing purposes. More specifically, the half of the training topics with lower topic numbers are used to train the length dis-tribution estimation parameters, and the other half of the training topics are used to train the score combination pa-rameter  X  in Equation 26. Finally, our proposed BM25L model is evaluated by its retrieval performance on average over the two subsets of test topics. We use the TREC of-ficial evaluation measures in our experiments, namely the statMAP on ClueWeb B [30], and the Mean Average Preci-sion (MAP) on the other three collections [31].

Our evaluation baseline is the classical BM25 model with different settings of its parameter b .Byvaryingthe b value, we investigate to which extent BM25L improves the retrieval performance. In particular, we c ompare the retrieval perfor-mance of BM25L to BM25 with b = 0, that is, BM25 without tf normalization, and BM25 with its parameter b optimized. All statistical tests are based on Wilcoxon Matched-pairs Signed-rank test.
Tables 3 and 4 compare the retrieval performance of BM25L to the original BM25 without tf normalization (i.e. when b =0),andwiththe tf normalization with its parameter b optimized, respectively.

From Tables 3 and 4, we see that modeling document length distribution using GEV distribution leads to the most stable retrieval performance of our proposed length-based BM25L model. This is not of a surprise as we have shown that the GEV density fits the best to the actual document length distribution. Using the GEV density fitting of the document length, BM25L appears to outperform the BM25 baseline, and the improvement is statistically significant in most cases on all four test collections except WT10G.
The use of other distribution functions, in particular Gamma distribution, also leads to retrieval performance over the BM25 baseline on some of the test collections. However, their retrieval performance does not appear to be as robust as that obtained by GEV distribution. An extreme case is Normal distribution, which does not improve the BM25 baseline on disk1&amp;2, WT10G, and .GOV2 when the parame-ter b is optimized. In contrast, on ClueWeb B, it provides an MAP that is as high as 0.6340, which is 173% higher than the BM25 baseline, even if its parameter b is optimized. Similar observation can also be made with Gamma distri-bution, which leads to a 130% improvement. One possible explanation is that it has lightest skewness among other 3 test collections that could be observed in Figure 3. Another possible reason for BM25L X  X  extremely high retrieval perfor-mance with Gamma and Normal distribution on ClueWeb B is the shallow pool depth of this collection. Out of the four test collections used, ClueWeb B has the most incomplete relevance assessments, for which only the top-10 documents returned by the TREC participating runs are judged by hu-man assessors [6]. As the top ranked documents are mostly overlong, the biase towards long documents in the document ranking could be so evident that the length distribution of relevant and non-relevant documents fits very well with the distribution functions on both training and testing topics. As a consequence, BM25L leads to extremely high retrieval performance on ClueWeb B.
 Figure 4: Performance of BM25L over BM25 with b=0
To visualize the improvement brought the proposed length-based BM25L model, we plot the results in Figures 4 and 5forthecomparisontoBM25with b =0andwith b opti-mized, respectively. As we can see on the WT10G collection, although the improvement is not as much as that obtained on other three test collections using all six distributions, the increase in retrieval performance is the evidence of length ef-Figure 5: Performance of BM25L over BM25 with optimal b value fect in information retrieval. Using Weibull distribution and normalization transformation has the best results, this may due to that Weibull distribution does retain the skewness of data between zero and one scale on all collections very well as we can see from Figure 3.

By comparing the performance improvement by BM25L over BM25 with b = 0 with the improvement over BM25 with the optimized b , we can see that the improvement over the optimized b is overall of a less scale than that over BM25 without tf normalization. This is because optimizing the parameter b in BM25 has exaggerated the length impact on the relevance weighting of term frequency tf , and in return, it reduces the impact of length relevance weighting itself on improving the document ranking.

When comparing BM25L with the best known results, for the WT10G, BM25L X  X  best MAP is 0.2143, and the best published MAP is 0.2085. A possible explanation of the rel-atively minor improvement is as follows: the data transfor-mation on WT10G does not show much difference between the length distribution of relevance and non-relevance doc-uments. Compared to large-scale collections such as GOV2 and ClueWeb B, it leaves little room for the BM25L model to further improve the retrieval performance by utilizing such difference (in document length distribution). In other words, the TREC pools are biased by the length distribu-tion. Such bias is minor on WT10G, and becomes evident on heterogeneous collections like GOV2 and ClueWeb B, which is captured by BM25L to boost the ranking effectiveness. For ClueWeb B, we believe the best statMAP in the TREC 2009 Relevance Feedback track, i.e. 0.2638, is achieved by combining BM25 with relevance feedback [32], although the overview paper is not available. Our model BM25L gives an MAP of 0.3963. For GOV2, on top of the retrieval baselines, e.g. BM25 and language model, the best run in TREC 2006 further improved the effectiveness by using pseudo relevance feedback and term dependency [18, 21]. Since our model only considers document length, the best MAP presented in this paper, i.e. 0.3321, is not directly comparable to the best known MAP of 0.3737. For disk1&amp;2, there hasn X  X  been known best result for all 150 topics used in the TREC 1-3 ad-hoc tasks. According to evaluatir.org, the best known MAP on each task is 0.2062, 0.2475 and 0.3231, respectively, with an average of 0.2589. Note that the above best known re-sults are achieved by stacking additional techniques such as Table 3: Evaluation results over the BM25 baseline with b=0 improvement over the baseline.
 significant improvement over this baseline.
 relevance feedback over the retrieval baseline. Therefore, the results are not directly comparable. Even though, BM25L provides an MAP of 0.2579.
Experimental results in the previous section shows that, on one hand, BM25L leads to more improvement over BM25 when tf normalization is disabled. This is expected since there is no length information added to BM25 with b =0 when compare to BM25L. On the other hand, BM25L pro-vides higher MAP/statMAP values when the tf normal-ization parameter b is optimized. From this observation, a question arises: what is the impact of the setting of b on BM25L X  X  effectiveness? To answer this question, Fig-ure 6 plots the MAP/statMAP obtained by BM25L using the 6 different statistics of the document length distribution against different b values, from 0 to 1. BM25L and the orig-inal BM25 X  X  retrieval performance is seen to be correlated. A better setting of BM25 X  X  b leads to a better retrieval per-formance of BM25L. The document length itself do have a power as a stand-alone factor on the document relevance weighting other than normalization adjustment. The results for the full range of b are illustrated in Figure 6 for all 4 test collections.

Another important factor that could heavily affect BM25L X  X  retrieval performance is the parameter  X  in Equation 26. Figure 7 plots the MAP/statMAP obtained by BM25L against  X  on the four collections used. As we can see that length im-pact on the relevance weighting increase first as  X  increase, then either decrease or remain flat as  X  increase. Thisisno coincidence because with only one factor, i.e. length, among other many important factors that can affect document rel-evance weighting, the improvement would be limited.
Another parameter in Equation 4 is k 1 . BM25L outper-forms BM25 with different settings of k 1 , although the lat-ter X  X  retrieval performance is fairly sensitive to k 1  X  X  setting. The related experiments are not included in this paper for brevity, since changing k 1  X  X  setting does not affect the con-clusions.

In summary, we have shown that the length information can be used for leveraging the bias towards long documents in the document ranking. The retrieval performance of the classical well-established BM25 model can be marked im-proved by incorporating a length-based weighting compo-nent with different settings of BM25 X  X  tf normalization pa-rameter, including the optimal setting. Finally, we recom-mend applying GEV distribution for modeling the document length distribution as it has demonstrated effective and ro-bust retrieval performance in our experiments. In our exper-iments, all parameters are lea rned from the tra ining data in the two-fold cross-validation. It is therefore of note that our proposed model is trained and tested with different queries.
Ourresearchinthispaperisbasedontheassumptionthat a document may exhibit both Verbosity and Scope hypothe-ses. We derive the relationship between document length and its relevance through a list of probability density func-tions, and propose a BM25L model that incorporates this relationship into the classical BM25 model. The proposed BM25L model is evaluated on standard large-scale TREC collections. Our experiments demonstrate that BM25L is able to markedly outperform the BM25 baseline even with the optimized tf normalization. The results empirically con-firm our assumption that the actual relationship between document length and relevance is a mixture and compro-mise between the Verbosity and Scope hypotheses.

In this paper, we have proposed a general method of utiliz-ing the relationship between document length and relevance for improving retrieval performance. The proposed method is shown to be effective in its application to the classical BM25 weighting model. In the future, we plan to apply our proposed method to other state-of-the-art IR models, such as language modeling, or the PL2 model [2]. In addition, we also plan to investigate possible ways to balance between the effectiveness and efficiency of our proposed method.
This research is supported by NSERC of Canada and the Early Researcher Award/Premier X  X  Research Excellence Award. We thank four anonymous reviewers for their excel-lent comments on this paper.
