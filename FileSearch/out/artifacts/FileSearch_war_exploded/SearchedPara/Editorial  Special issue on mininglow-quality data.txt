 EDITORIAL Xingquan Zhu  X  Taghi M. Khoshgoftaar  X  Ian Davidson  X  Shichao Zhang 1 Introduction Data mining is dedicated to searching for novel and actionable patterns and re-lationships that exist in a large volume of data. The mining process typically in-volves four major steps: (1) data collection, for example, transferring data col-lected from the production systems into data warehouses; (2) data preprocess-ing, transforming/cleansing the data to remove errors, filling missing values, and checking for inconsistency or duplicates; (3) finding patterns and models from pre-processed data; and (4) developing and monitoring the knowledge model [ 5 , 7 ]. In data-driven application domains, many potential reasons, such as unreliable data acquisition sources, faulty sensors, data collection errors, and the lack of data representation standards, will make data vulnerable to errors and therefore lead to poor quality data. Although these factors and constraints are widely accepted by general data mining practitioners, most applications have traditionally ignored the need for developing appropriate approaches for representing and reasoning with such data imperfections. As data mining is increasingly recognized as a key technology to analyzing and understanding the data, the need for knowledge dis-covery from real-world low-quality data becomes not just overwhelming, but also compelling. As a result, issues related to data quality have become more and more crucial and have consumed a majority of the time and budget of data mining projects [ 14 ]. 2 Mining from low-quality data There are many issues related to mining from low-quality data. Determining the quality of the underlying data provides a mean for users to wisely select tools in the succeeding knowledge discovery process. Data quality evaluation, or the pro-cess of assessing the data reliability and effectiveness, has been studied for more than a decade, especially from the data warehousing and information systems per-spective [ 3 , 15 , 16 ]. Although data quality was meant to cover broad principles, such as data completeness, believability, being error free, understandability, and relevance [ 16 ], in reality it is often regarded as a measure of outliers, missing data, and misalignments [ 14 ], and the quality of the data is usually revealed by the fol-lowing three data properties: incomplete information, imprecise information, and uncertain information [ 12 ].
 gested three types of measures: (1) simple ratio X  X easuring the ratio of desired outcomes to total outcomes; (2) Min or Max operation X  X andling dimensions that require the aggregation of multiple data quality indicators; and (3) weighted average X  X nifying different data quality indicators with respect to their degree of importance. Unfortunately, the estimation of data quality has proven to be a sig-nificant challenge, simply because the genuine data values are hard, if not impos-sible, to determine due to the contamination of the data. Research efforts in the area therefore have been focused primarily on finding various data items with data quality concerns, such as inconsistent or noisy instances [ 6 , 8 , 10 , 19 ], duplicate [ 4 , 9 ], and incomplete or missing data values [ 11 ]. The essential goal is to rec-tify the contaminated data, such that large-scale data manipulation, including data warehousing, data auditing, and data profiling, becomes possible.
 been observed by many researchers [ 1 , 13 , 17 ]. Many algorithms have a special mechanism to handle certain types of data imperfections, such as noise or missing values. For example, pruning on a decision tree is designed to reduce the chance that the tree is overfitting data inconsistency [ 17 ], and the prototype selection is used to immunize an instance-based learner from noise [ 1 ]. In a narrow sense, the existing data mining framework heavily relies on a so-called data cleansing pro-cess to eliminate suspicious data before the hypothesis formation, such that noisy examples do not influence hypothesis construction. Although the theme of data quality is far beyond the problem of data cleansing, the majority research efforts on data quality related data mining issues have been concentrated on developing effective noise identification or data imputation tools to improve the quality of the underlying data, from which a better knowledge model is expected to be dis-covered. Although effective, when dealing with low-quality information sources, the existing data cleansing based data mining framework suffers from the fol-lowing two disadvantages: (1) information loss incurred by data cleansing, where falsely identified errors may further deteriorate the system performances; and (2) isolating the data from the succeeding mining process, where data quality enhancement and data mining are two disjoint processes with no awareness of the existence of each other.
 merous solutions have been proposed, are also closely related to the problem of mining low quality data. While a real-world data mining project usually requires a systematic framework from data collection to model monitoring, it is unfortunate that data mining research has not paid enough attention to bridging data quality and data mining but rather views them separately. As a result, users often have to rely on the robustness of the adopted data mining algorithms to ensure the success of the project. It is the guest editors X  view that our existing data mining framework is far from perfect in handing real-world data imperfections. We need to explore data quality and knowledge quality benchmark which are customized for data min-ing and knowledge discovery process, rather than relying on existing data integrity and completeness-based measures. More intelligent data mining algorithms which are aware of the underlying data quality are needed, rather than simply relying on the data cleansing. More effective data mining frameworks will allow practition-ers seamlessly bridge data quality and data mining for mutual benefits, instead of sequentially conducting data preprocessing and knowledge discovery. 3 Papers in this special issue Our editorial goal for this special issue was to report and introduce the recent studies on data quality related data mining activities, and more particularly, bring together some of the latest research results in mining from low quality data. We received 12 submissions, which deal with a wide range of problems, from which we select five papers after a thorough review.
 problem of outlier detection from time series sensor data, with consideration that data closer in time are more correlated to each other than those farther apart. The objective was to investigate efficient data cleansing methods that will facilitate the analysis and modeling of signals obtained from multiple sensors. The authors propose two methods which use data point neighborhood median and a threshold value to find data abnormality. The techniques in this paper are valid on general time series but suffer from the following two disadvantages: (1) ineffective in de-tecting outlier burst; and (2) sensitive to the selection of the threshold. tive clustering based ensembling (CCE) method for a particular learning problem X  multi-instance learning, where the training set is composed of labeled bags each consisting of many unlabeled instances. A bag is positively labeled if it contains at least one positive instance and negatively labeled otherwise, and the goal is to learn some concept from the training set for correctly labeling unseen bags. The proposed CCE method employs a clustering process to help construct new fea-tures, on which common supervised learning algorithms can work. Meanwhile, CCE also utilizes the power of ensemble learning paradigms to achieve strong generalization ability.
 detection algorithm, which aims at identifying instance containing erroneous attribute values. The output consists of a list of instances ordered from most noisy to least noisy. The proposed noise detection algorithm, PANDA, seeks to identify instances with a large deviation from normal given the value of a pair of attributes. The idea is to find a set of instances with similar values for one particular attribute, then large deviations from normal for the second attribute may be considered sus-picious. The larger the deviation is, the stronger the evidence is that the instance is noisy. To evaluate the method and ideas proposed in this paper, the authors have performed experimental comparisons with a nearest neighbor and a distance-based outlier detection method on a real-world software measurement dataset.
 ity management and awareness for the KDD process, with a case study on cost optimal association rule mining. The author first presents an overview on data quality characterization and management for improving the quality awareness of the knowledge discovery process, followed by a generic framework for integrat-ing data quality measure into the KDD process. The whole process is instantiated by quality-aware association rule mining with a probabilistic cost model, which is used to estimate the cost of  X  X egitimately interesting X  association rules based on correct-quality data. Experimental results conclude that variations on data quality have a great impact on the cost and quality of the discovered association rules. theories for handing incompleteness in first-order learning. The wide existence of data imperfections, such as noise and incomplete instances, often raises the need of expressive representations, such as abductive reasoning, rather than replying on a single inference mechanism. As traditional abductive reasoning strongly reply on the availability of an in-depth background knowledge about the specific ap-plication domain, the proposed effort aims at automatically discovering the meta-knowledge needed to abduction inference strategy to complete the incoming infor-mation in order to handle the cases of missing knowledge. To this end, the authors develop a procedure that starts from the training data and generates a set of special rules to be exploited in the abductive proof procedure supporting the standard in-ductive reasoning. Experiments confirm that the automatically inferred abductive theories are capable of improving the learning process in different domains. 4 Summary In summary, the five papers present some of the recent research results in the field of mining low-quality data. The guest editors wish to thank all the authors and reviewers for their contributions. We would also like to express our gratitude to Dr. Xindong Wu for the cheerful help he provided with this special issue. Finally, we hope the reader will enjoy this special issue and find it useful.
 References Author Biographies
