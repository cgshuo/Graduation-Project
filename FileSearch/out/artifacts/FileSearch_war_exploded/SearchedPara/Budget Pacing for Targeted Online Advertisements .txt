 Targeted online advertising is a prime source of revenue for many Internet companies. It is a common industry practice to use a generalized second price auction mechanism to rank advertisements at every opportunity of an impression. This greedy algorithm is suboptimal for both advertisers and pub-lishers when advertisers have a finite budget. In a greedy mechanism high performing advertisers tend to drop out of the auction marketplace fast and that adversely affects both the advertiser experience and the publisher revenue. We describe a method for improving such ad serving systems by including a budget pacing component that serves ads by being aware of global supply patterns. Such a system is ben-eficial for both advertisers and publishers. We demonstrate the benefits of this component using experiments we con-ducted on advertising at LinkedIn.
 H.1.0 [ Information Systems ]: Models and Principles X  General budget pacing; targeted online advertising; generalized sec-ond price auction
Targeted advertising is a significant source of revenue for many Internet companies. Members visiting a website are served ad impressions. Members are described by their pro-file which may include their demographic attributes like their geographic location, age, gender etc. Advertisers may target a segment of users specified by a combination of attributes. Advertisers are required to set up campaigns by specifying creatives, a target member segment, a bid value and a daily budget. As described in [7] and [18], many internet com-panies use a generalized second-price auction (GSP) mech-anism to determine which ads to show. A GSP ad auction is one where at each opportunity of an impression, eligible campaigns are ranked based on bids and the winner pays the bid of the second highest bidder.

A second price auction mechanism used for individual auc-tions is a greedy algorithm. Ad campaigns participate in ev-ery eligible auction until their daily budget is used up. Here by eligible auction for a campaign, we mean an opportunity of an impression to a member who belongs to the targeted member segment of the campaign. When a campaign X  X  bud-get is used up it drops out of the auction marketplace for the rest of the day. The second price auction mechanism is greedy as it tries to maximize revenue at each opportunity independent of other auctions. This greedy mechanism is not optimal and has several practical drawbacks.
In this paper we describe a budget pacing algorithm that improves the ad serving mechanism on each of the above mentioned issues. The budget pacing algorithm aims to spread out impressions to every campaign evenly over a bud-get day. The main idea is that we serve impressions to cam-paigns by being aware of global traffic patterns. For each campaign we obtain a forecast for the traffic pattern of eli-gible impressions during the day. Based on the forecasts we determine an allocation plan in which we allocate spending budget of the campaign proportional to the forecasted eli-gible traffic. At runtime, we monitor the spend of each ad campaign closely and for campaigns that spend faster than the allocation plan, we throttle them and do not allow them to participate in some auctions.
Since this algorithm slows down the spending rate of cam-p aigns that spend faster than the plan, it increases the life-time of campaigns and helps to mitigate the problems de-scribed above. Most importantly, this is a desired feature for many advertisers since they get a more diverse audience. This improves revenue metrics as campaigns stay alive for longer and provide higher support for other campaigns in the GSP auctions.
 We describe the system we implemented and deployed at LinkedIn and the experiments we conducted that proved our claim. Targeted advertising is a significant source of revenue for LinkedIn, see [2]. Members visiting LinkedIn.com are served ad impressions and advertisers pay LinkedIn on a pay-per-view (CPM) or a pay-per-click (CPC) mode. LinkedIn has a very rich data about members and allows advertis-ers to target member segments by attributes like their geo-graphic location, industry, company, education, professional skills etc., or their activity attributes on LinkedIn.com like following companies, group memberships etc. Advertisers may target a segment of users specified by any combination of such attributes. Advertisements are ranked using GSP on expected revenue from the ads where the winner pays an amount so that the expected revenue from the impression equals that of the second highest ad.

Our implementation has been deployed in two different advertising products at LinkedIn.
In section 2 we describe the algorithm in details. In section 3 we discuss the engineering design of our implementation at LinkedIn and In section 4 we discuss the design and the results of the experiments we ran at LinkedIn. The Direct Ads and the SSU are very different in terms of their age and status. We observe beneficial but very different effects of our pacing algorithm on these two products Targeted advertising bears a lot of similarity to the Ad-Words problem, also known as search advertising, where ad-vertisers target online search keywords. There is a rich lit-erature studying various aspects of the AdWords problem, e.g., the study of game theoretic properties of such auctions in [7], [18] and [4]; advertiser optimizations like budget op-timization in [9] and selecting profitable keywords in [16]; revenue maximization for the search engine provider in [13], [10], [8] and [12].

The AdWords problem is a generalization of the online bi-partite matching problem. As explained in [13], the greedy algorithm to serve ads for AdWords has a worst-case com-petitive ratio of 1/2. Goel and Mehta [10] though showed that the greedy algorithm has a competitive ratio of 1-1/e in the random permutation input model and in the i.i.d. input model. Mehta et.al. described a simple determinis-tic algorithm in [13] that achieves an optimal competitive ratio of 1-1/e. There are many other articles that study practical aspects and methods for revenue and/or relevance maximization for search ads, see e.g., [15] and [11].
Member demographics based targeted advertising is sig-nificantly different from the AdWords problem in many ways. The main difference is that advertisers in AdWords target at the very atomic target level which are search keywords-they are able to set different bids for each different keyword. In targeted advertising, such as the one in use at LinkedIn, advertisers are not able to target at the most atomic level. Advertisers set the same bid value for all members in the target audience. This adds significant complexity to the problem and makes it difficult to obtain theoretical guaran-tees.

Mehta et.al. [13], [6], and [1] suggest using a modified bid of campaigns in every ad auction. In an auction mar-ketplace where the competition is low, the modified bids of campaigns exhausting their budget is likely to reduce to very low amounts. Often times though a reserve price as in [14] is used for auctions that sets a lower limit on what an ad-vertiser can bid and pay. Having a reserve price restricts how low the bid amount can get and hence this technique might not help in increasing the life of campaigns. Instead of using a modified bid for auctions we use probabilistic throt-tling to filter out the high performing campaigns from some randomly chosen auctions.

Our main contribution is that we developed and deployed a simple novel algorithm that improves advertiser experience and publisher revenue for targeted advertising. We set up appropriate real-life experiments and show the benefits of our deployed algorithm.
Suppose S is the set of all members. An ad campaign i targets a subset S i of S , has a bid b i and a daily budget d The day is discretized into T equal length time windows and s i,t , for 0  X  t &lt; T , denotes the cumulative spend of cam-paign i from the beginning of the day till the start of window t . We obtain a forecast for volume of eligible impressions for campaign i during each time window-f i,t denotes the cu-mulative number of eligible impressions for campaign i till the start of window t . We allocate the amount of budget we would like to spend for a campaign during each time win-dow proportional to the forecasted volume of eligible traffic for the campaign during that window. Hence, the amount allocated to be spend from the beginning of the day till the start of window t is where f i ,T is the forecasted total volume of eligible impres-sions for campaign i for the entire day. We apply a filter during each ad auction. For each eligible auction a cam-paign is allowed to participate with probability p i,t , the pass through rate(PTR). The value of p it is updated at the start of each time window by Here 0 &lt; r t &lt; 1 is an adjustment rate.

For a campaign i , the allocation curve a i,t is fixed at the start of the day and we control the spend curve s i,t by mod-ifying the PTR p i,t . We aim to keep the spending curve s close to the allocation curve a i,t for every campaign i .
Mehta et.al. [13] suggest using a modified bid of cam-is an optimal choice. We have already discussed in section 1.1 why we use probabilistic throttling instead of bid modifi-cation. For probabilistic throttling, it would have been nat-ural to use p  X  i,t =  X  ( s i,t /d i ) as PTR. In a linear system this would result in same expected payoff as in [13]. The GSP auction mechanism is clearly not a linear system though. We still tried this choice of PTR. We observed an improvement in revenue but not a significant increase in campaign life. This may seem counterintuitive since  X  ( x )  X  0 as x  X  1. This happens due to a combination of two reasons. Firstly, this form of PTR does not depend on the rate of budget spend and can therefore be slow to react. Secondly, system inefficiencies also reduce the effectiveness of this method. Based on this experience we developed the dynamic method of choosing PTR as described in (2) where the value at the current time window is set as a small perturbation of the value at the previous time window. We observed remark-ably good results with this approach.
We update the PTR for each campaign at 1 minute in-tervals. This frequent updating makes the system agile and reach a steady state fast.
Forecasting the number of eligible impressions for cam-paigns plays an important role in our algorithm. It is used to determine the allocation curve for each campaign. Inac-curate forecasts may cause unnecessary throttling and may result in budget not being used up for some good campaigns.
We use a tool that we built at LinkedIn using the method described in [3]. Note that, in order to forecast the number of eligible impressions for campaign i , we need to forecast the number of impressions from the member segment S i , which is the target audience for campaign i . We maintain a large list of important member segments which we call base profiles . For each base profile we update the historical time series data of number of ad impression opportunities. We use an ensemble statistical time series model to fit the up-to-date time series data for each of these base profiles daily. We obtain forecasts for each targeted segment S i using correlation models described in [3].
We set the adjustment rate r t at a constant 10%. This naive choice of the adjustment rate is not only the easiest to implement, but is also a very robust one. A more theoreti-cally sound algorithm should involve the derivative  X  X  i,t the rate of budget spend while computing the PTR; a cam-paign that spends budget very fast needs to throttled more whereas a campaign spending budget just a little bit faster than the allocation curve needs to be throttled less. We chose the naive form for two main reasons. First, an esti-mate for  X  X  i,t / X  X  is likely to be noisy. The budget spend curve is not smooth, especially for cost-per-click campaigns for whom it changes only when there is a click which is signif-icantly rare compared to the impressions. Second, we adjust the PTR frequently and even if we are not at the ideal PTR at any time point we can reach there fast. We always start with a PTR of 10% for all campaigns. We call this a slow start . This may be a low starting value for some campaigns and may be a high value for some other campaigns. The advantage of a low starting value is that it gives the system the time to learn about individual cam-paigns. On one hand, if a campaign is spending its budget too slow then it can open up fully with in 25 minutes. On the other hand if a campaign is spending too fast then hav-ing a 10% PTR to start with gives the system more time to reach the correct PTR. For such campaigns starting from a high PTR may result in using up the budget before the system learns that it needs to set a low PTR.

It is sensible to customize the starting value of the PTR for individual campaigns. We aim to do that in future devel-opments where we plan to make the initial value of the PTR for a campaign, a function of the demand and the forecasted supply for the campaign. The demand for a campaign is the number of impressions required to spend the budget of the campaign and the supply is the total number of impressions available for the campaign. It is natural to set a low initial value of PTR for a campaign with a high demand-supply ratio.
We use statistical models to forecast future supply of im-pressions for each campaign. The models typically have high accuracy but there are always statistical errors. As a result there may be situations where the algorithm continuously sets lower values of PTR for a campaign than what should have been set. We try to mitigate this issue by fast finish -we modify the allocation curve slightly so that the algorithm as-sumes there will not be any traffic during the last two hours of the day. This way the pacing algorithm attempts to ex-haust budget within 22 hours. This in turn gives campaigns that have been wrongly throttled earlier in the day, more opportunities to use up their budget later.
In LinkedIn X  X  ad serving system, data on advertiser events and member events continuously flows through a tracking pipeline. Advertiser events may include creation of a new campaign or a modification to an existing one, like change in bid or daily budget etc. User events include member page views that resulted in ad request or opportunity, ad i mpressions served and user actions like click or share etc. This data is stored into a database.

The ad server maintains an index of all campaigns along with their current status. A pacing module that implements of our algorithm is deployed in the ad server. The pacing module receives this data from the database using a technol-ogy called databus, see [5]. Periodically the pacing module recalculates the PTR for each campaign using the latest bud-get/spend data and the allocation curve, and pushes it into the campaign index. The campaign index is scanned at ad matching and auction time, during which the PTR is used in a simple probability test to determine whether a given campaign should participate in the auction or not.
The forecast for eligible supply of impressions for cam-paigns is computed in an offline Hadoop system [17]. The forecast information is pushed to the pacing module on a daily basis.

As mentioned before, we update the PTR for each cam-paign at 1 minute intervals on average. The updates are randomly distributed across campaigns within each minute. That is, instead of updating all campaigns at once every minute, we update roughly 12% of all campaigns every 7 sec-onds. This updating frequency allows the system to reach a steady state fast and evenly spreads out the server load.
We designed and ran online experiments on the advertis-ing products at LinkedIn to ascertain the benefits of deploy-ing the budget pacing algorithm.

Experimental design, although an age-old topic of research, has been revolutionized by the Internet industry. Internet companies use experiments, often called A/B tests, to vali-date practically every step they take. In a typical A/B test, in order to compare control A with a treatment B, members are randomly split into two groups with one group receiv-ing the control and the other group receiving the treatment. This approach is not directly applicable in what we wanted to test.

Randomly splitting traffic into two groups is not enough since the pacing algorithm would affect the budget spending for a campaign for an entire day and hence we would need to apply the same treatment to a campaign for the entire day. We could randomly split all campaigns into two groups and run different algorithms on two groups; this would help us determine the benefits of pacing to advertisers (like life-time of campaigns etc.) but not the revenue benefits to the publisher since campaigns from both the groups would participate in the same auctions.

In order to observe the full benefits of the algorithm it needs to be turned on for all campaigns for an entire day. We designed the experiment to run pacing on alternate days. By running the experiment for at least 2 weeks we are able to estimate the benefits of the pacing algorithm and control for any day specific bias and weekly seasonality that may be present in the system. Table 1 describes the experimental design.

In our experiments we also controlled for any bias that m ay result due to trend. A weekly trend is of much smaller magnitude compared to the effect of our algorithm but by controlling for it we expect to get more accurate estimates.
When running the experiment for two weeks we consid-ered 15 days of data for computing the metrics of interest. Starting the experiment from Sunday in Week 0 we contin-ued till Sunday in week 2. The effect of pacing is measured by averaging over the 7 days when the algorithm was turned on during this period. The metrics corresponding to the con-trol was measured using data from 8 days where we take an average of Sunday in week 0 and Sunday in week 2 to es-timate the Sunday effect. Every other day gets an equal weight of 1/7. This mechanism helps to eliminate any linear trend present in the system.

Another way to run an experiment to determine the effects of pacing would be one where we create two independent ad serving systems and put in all the ad campaigns in both the systems with half the daily budget. We can then split incoming traffic and randomly allocate to the two different systems. The advantage of this design would be that we would not have to be concerned about weekly seasonality or trend or other daily variations. We chose not to use this design for two reasons. First, It would be difficult to apply the algorithm to campaigns with small daily budget and second, it would take a lot of time and effort to implement such a two-fold ad serving system which is likely not cost effective.
The following are the main metrics we measure in our ex-periments. For most metrics we take the case when pacing algorithm is turned off as baseline and report the percent-age change when the algorithm is turned on. The metrics are computed using all the data from LinkedIn without any sampling. LinkedIn gets tens of millions of page views regu-larly and has tens of thousands of advertisers advertising at LinkedIn on any day. Given the volume of the data we are confident about the statistical significance of the metrics.
Direct Ads is the advertising product that serve adver-tising content on various LinkedIn webpages. This product is more than 4 years old and the demand from advertisers exceed the supply. There is a high coverage in most member segments.

We ran the experiment in Direct Ads for 2 weeks during 2013 Q4. The key results are tabulated in table 2. As we mentioned before, due to privacy issues we report most met-rics in relative form with the baseline being the case when the pacing algorithm in not running.

The metrics reveal that the pacing algorithm provides b enefits to both advertisers and LinkedIn. The algorithm increases campaign life time by 44%. Advertisers are able to reach to a broader audience with the same budget and more advertisers are served impressions.

There is a benefit to LinkedIn as the cost per ad request increases. The pacing algorithm keeps campaigns alive for a longer time in a budget day. As discussed before, this helps increase competition later in the budget day. This is re-flected in figure 2 which plots the variation in cost per click during a budget day. Cost per click (CPC) denotes what advertisers pay on average per click and in a second price auction it is a measure of support price for auctions. It is primarily used for campaigns who pay per click and they are the dominant kind in Direct Ads. In figure 2, the solid red line denotes the average CPC curve during each hour within a budget day on the days when pacing was turned on. The blue dotted line denotes the same for the days when pacing was turned off. We see that with pacing turned on, the CPC is lower during the initial part of the day but higher for a much longer period during the second half of the day. The CPC is lower at the start of the day because we throttle all the high performing campaigns during that time who oth-erwise all compete together and exhaust their budget fast. Again, the plot was created from all the data collected by LinkedIn during the two weeks of experimentation. Figure 3 shows the variation in cost per request during a budget day.

The loss due to over delivery of impressions to campaigns is reduced by 10%. There is a little time delay in the avail-ability of the latest budget spent information in LinkedIn X  X  ad serving system. In absence of pacing, the good campaigns spend at a fast rate and because of the delay in latest bud-get information, the system often serves impressions to such campaigns even after the budget for the day has been spent. The pacing algorithm slows down the rate of spend of such campaigns and reduces the chances of over delivery.
We do not see a statistically significant effect of pacing algorithm on the diversity of ad content seen by members. That is likely because of the long tail of the distribution of the number of unique campaigns seen by a member. Figure 2: Variation in average cost per click during a day in Direct Ads (scaled so that the baseline at time 0 is set to 1)
Sponsored Status Updates is a new advertising product from LinkedIn that was launched in mid 2013. Advertisers can sponsor a  X  X tatus update X  and promote it to a specified segment of LinkedIn members. The sponsored content from an advertiser is blended with the organic content in the up-date stream and delivered to the members in the targeted audience.

The Sponsored Status Update marketplace was very dif-ferent from Direct Ads at the time we ran the experiment. SSU is a new product, the demand to supply ratio is much lower but is growing rapidly. The SSUs appear on a prime Figure 3: Variation in average revenue per request d uring a day in Direct Ads (scaled so that the base-line at time 0 is set to 1) space in the LinkedIn webpage and smart phone/tablet apps. It has a higher visibility because of which campaigns can po-tentially exhaust the daily budget very fast. Pacing is thus very important for SSU. We ran a two week experiment on this product and the key results are tabulated in table 3. Table 3: Results for Sponsored Status Updates
It is expected that the pacing algorithm will have a much d ifferent effect in SSU and that is what we observe. First, we see that the increase in cost per request is of much lower magnitude compared to that in Direct Ads. The reason for that is the total revenue for SSU (at the time of the experi-ment) is largely determined by the advertiser demand which is lower than the supply. Second, the increase of campaign median life time is much more drastic (149%) compared to what we observed in Direct Ads.

We observe an improvement in the audience reach for ad-vertisers as the average number of unique impressions per spend increased by 10.52%. There is a small increase in the average number of campaigns that got served per day, but this change is not statistically significant. We also observe a marked decrease in loss due to over delivery (42%). The effect on the average number of unique campaigns seen by members was slightly lower but this change is statistically insignificant.
Here we show the behavior of two specific campaigns dur-ing our experiments. Albeit two examples, in conjunction with the results described in sections 4.3 and 4.4, they fur-ther illustrate how well the pacing algorithm performs. Figure 4 shows the performance of a specific campaign in SSU in two consecutive week days during the experiment. Pacing was turned off on day 1 (blue dotted line) and was running on day 2 (red solid line). On day 1 the budget was spent with in the first 30 mins of the day and there was more than 20% loss due to over delivery. On day 2, the campaign was active till the 20th hour of the day and had a significantly lower over delivery loss.
 Figure 4: Cumulative budget spent for a campaign in SSU (scaled so that the total budget is 1)
Figure 5 demonstrates for a specific campaign that we are able to align the actual spending curve with the allocation curve well. The blue line is the observed spending curve and the red dotted line is the allocation curve for the campaign. This behavior is observed consistently for most campaigns with a reasonably high daily budget. Figure 5: Actual spend curve vs. the allocation curve for a campaign in SSU(scaled so that the total budget is 1)
We develop a budget pacing algorithm that we use to evenly distribute the spend of advertising campaigns in a day. We implemented the algorithm and it is deployed in two advertising products at LinkedIn. The algorithm helps improve advertiser experience and provides revenue benefits to LinkedIn at the same time. We ran real life experiments at LinkedIn to demonstrate the benefit of using this algo-rithm. Based on the results of the experiments this algo-rithm was been fully deployed in both the advertising prod-u cts at LinkedIn. [1] Z. Abrams, O. Mendelevitch, and J. Tomlin. Optimal [2] D. Agarwal. Computational advertising: The linkedin [3] D. Agarwal, D. Chen, L.-j. Lin, [4] G. Aggarwal, A. Goel, and R. Motwani. Truthful [5] A. Auradkar, C. Botev, S. Das, D. De Maagd, et al. [6] C. Borgs, J. Chayes, N. Immorlica, K. Jain, [7] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [8] J. Feldman, A. Mehta, V. Mirrokni, and [9] J. Feldman, S. Muthukrishnan, M. Pal, and C. Stein. [10] G. Goel and A. Mehta. Online budgeted matching in [11] S. Lahaie and D. M. Pennock. Revenue analysis of a [12] S. Lahaie, D. M. Pennock, A. Saberi, and R. V. [13] A. Mehta, A. Saberi, U. Vazirani, and V. Vazirani. [14] M. Ostrovsky and M. Schwarz. Reserve prices in [15] F. Radlinski, A. Broder, P. Ciccolo, E. Gabrilovich, [16] P. Rusmevichientong and D. P. Williamson. An [17] R. Sumbaly, J. Kreps, and S. Shah. The big data [18] H. R. Varian. Position auctions. International Journal
