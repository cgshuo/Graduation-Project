
Center for Imaging Science, Department of Biomedical Engin eering, Johns Hopkins University of unknown dimension d &lt; D to N sample points X = { x subspaces { S obtained from the derivatives of these polynomials at the da ta points. n moving hyperplanes in R D , S At time t , we seek an estimate  X  b ( t ) of b that minimizes f ( b ) = P t shown in Theorem 2.8, page 77 of [5], the following normalized gradient recursive identifier persistently exciting , i.e. if there is an S  X  N and  X  where A  X  B means that ( B  X  A ) is positive definite and I for stability purposes, as it imposes a uniform upper bound o n the covariance of the data. and the sequence { b ( t +1)  X  b ( t ) } is L the error f ( b ( t )) = P t in the tangent vector v  X  T S D  X  1 is b cos( k v k )+ v normalized gradient recursive identifier on the sphere is where the negative normalized gradient is computed as needs to be projected onto the subspace orthogonal to  X  b ( t ) by the matrix I the persistence of excitation condition (3) needs to be modi fied to where the projection matrix P (4) is such that  X  b ( t )  X  b exponentially, while if { b ( t + 1)  X  b ( t ) } is L estimate all the hyperplanes, without first clustering the p oint trajectories. in one of the n hyperplanes. Then there is a vector b n where c known as the Veronese map of degree n , which is defined as [8]: where I is chosen in the degree-lexicographic order and M independent monomials. Notice that since the normal vector s { b are defined up to scale, we will assume that k b efficients c ( t ) , rather than on the normal vectors { b follows. At each time t , we seek to find an estimate  X  c ( t ) of c ( t ) that minimizes where the negative normalized gradient is computed as Notice that (11) reduces to (4) and (12) reduces to (5) if n = 1 and N = 1 . the polynomial  X  p where D X  vector b derivative of the true polynomial p in the embedded space {  X  Theorem 1 Let P {  X  Then the sequence c ( t )  X   X  c ( t ) is L hyperplane, then the corresponding  X  b ( x ( t )) in (13) is such that b addition the hyperplanes are static, then c ( t )  X   X  c ( t )  X  0 and b c have k D X  showing that  X  b ( x ( t ))  X  b is a constant matrix of exponents E Therefore, k D X  and  X  is compact, the sequences { b Segmentation of the point trajectories. Theorem 1 provides us with a method for computing an estimate  X  b ( x { x  X  b The main difference with K-means is that we maximize the dot p roduct of each data point with Algorithm 1 (Recursive hyperplane segmentation) the number of hyperplanes may be unknown and time varying. Fo r example, the number of moving We denote by n ( t )  X  N the number of hyperplanes at time t and assume we are given an upper condition on the right hand side of (15) holds trivially when the regressors x M n ( D )  X  1 regressors must be  X  X ich enough X  either in space or in time.
 The case in which there is a  X  time t independently. Notice also that (17) is equivalent to (15) w ith S = 1 . The case in which n ( t ) = 1 and there are  X  hyperplanes whose motion is rich enough so that (18) holds.
 the number of hyperplanes, the recursive identifier (11)-(1 3) will still provide L Experiments on synthetic data. We randomly draw N = 200 3D points lying in n = 2 planes { x Segmentation of dynamic textures. We now apply our algorithm to the problem of segmenting of all intensity trajectories lie in multiple subspaces, on e per dynamic texture. Given  X  consecutive frames of a video sequence { I ( f ) } t channels. We obtain a data point x w i ( t ) R One can notice that the orientation of the bird is related to t he value of the coefficient c facing to the right showing her right side, the value of c if the bird is oriented to the left, the value of c estimated polynomial give useful information about the sce ne motion. Middle: segmentation obtained using our method. Bottom: te mporal evolution of c GPCA and our method initialized with GPCA to this video seque nce. For GPCA we used a moving window of  X  = 5 frames. For our method we chose D = 5 principal components of the  X  = 5 recursive estimation of the polynomial coefficients make ou r method much faster than GPCA. Sequence GPCA Our method 24, 65, 77, and 101. Middle: segmentation with GPCA. Bottom: segmentation with our method. The author acknowledges the support of grants NSF CAREER IIS -04-47739, NSF EHS-05-09101 and ONR N00014-05-10836.

