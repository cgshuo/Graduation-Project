 Manually inspecting text to assess whether an event occurs in a doc-ument collection is an onerous and time consuming task. Although a manual inspection to discard the false events would increase the precision of automatically detected sets of events, it is not a scalable approach. In this paper, we automatize event validation , defined as the task of determining whether a given event occurs in a given doc-ument or corpus. The introduction of automatic event validation as a post-processing step of event detection can boost the precision of the detected event set, discarding false events and preserving the true ones. We propose a novel automatic method for event valida-tion, which relies on a supervised model to predict the occurrence of events in a non-annotated corpus. The data for training the model is gathered via crowdsourcing. Experiments on real-world events and documents show that our method (i) outperforms the state-of-the-art event validation approach and (ii) increases the precision of event detection while preserving recall.

Event descriptions are of paramount importance and provide con-cise summaries of news articles, making the search for information more enjoyable for general readers and more effective for profes-sionals like historians and journalists.

In our work, we do not focus on how events are detected or ex-tracted from text, but we are interested in assessing their occurrence within a given corpus. For this purpose, we model events as a set of participants related within a given time period. This is in line with both, the basic definition of event given by the Topic Detection and Tracking (TDT) project ( X  X omething that happens at some specific time and place X ) [1] and with more recent works on event detec-tion, e.g. [4, 6, 10]. For instance, the event {( Kim Clijsters , Li Na , Australian Open ), 2011-01-18 to 2011-02-07 } would represent the participation of two tennis players in the 2011 Australian Open.
Manually assessing whether an event occurs in a document col-lection becomes infeasible in scenarios where events are contin-uously and automatically detected from news streams on a large-scale. In this paper, we propose to automate event validation by learning to assess the occurrence of events in a given unannotated corpus. Given a set of events, our approach can (i) reduce the num-ber of false events with respect to a given corpus, and (ii) find doc-uments to corroborate the occurrence of events and enrich avail-able knowledge bases (e.g. [7]) with such event descriptions. We particularly tackle the former application, introducing event valida-tion as a post processing step of event detection to boost the preci-sion within the detected set of events. This has to be accomplished without affecting recall, i.e. preserving accurately detected events. Since the verity of occurrence of an event depends on the consid-ered documents in a corpus, we distinguish two levels of validation: document-level validation validates the occurrence of an event in a given document, while corpus-level validation considers the occur-rence within the entire corpus. This distinction is compliant with the two aforementioned use cases: a user or application would ben-efit by either handling a cleaner event set (corpus-level) or having more available information related to the event (document-level).
To validate events, we develop a supervised learning model by extracting different features from events and documents. Given the lack of assumptions on the nature of events and documents, our model for event validation can be applied to a wide range of events and related corpora. Validity annotations for training our model are acquired by exploiting the crowdsourcing paradigm.

Our main contributions are: (i) the introduction of automatic event validation in cascade to event detection; (ii) a novel and effec-tive automatic event validation approach, which outperforms base-lines and previous state-of-the-art approaches; (iii) significant boost of precision with a low decrease in recall of event detection, when experimenting with a state-of-the-art event detection method.
The automatic detection of events from natural language content has been widely studied, e.g. in [1, 4, 5, 6]. However, event de-tection is different from event validation, since the output of event detection methods (i.e., a set of events ) constitute the known input for event validation methods. Although the task of event valida-tion with respect to a corpus can be posed as being analogous to an Information Retrieval task, we find that checking the mere appear-ance of event participants in text (e.g. via keyword matching) is insufficient to validate the occurrence of events in documents while establishing mutual relationships and temporal conformation.
A first attempt at automatic event validation has been proposed in [3], where the occurrence of events in documents was evaluated based on hand-crafted rules. On the contrary, our proposed method relies on combining a wide set of features extracted from events and documents and developing a supervised model. Araki et al. [2] performed historical fact validation by casting the problem as Pas-sage Retrieval . Differing from our approach, they assess event va-lidity in terms of the textual similarity between facts and passages of fixed length, thus assuming the evidence of facts in documents to be restricted to the pieces of adjacent content. Moreover, their approach is focused and evaluated only on historical facts. In [8] events are used as input, albeit with the different goal of attaching temporal references in documents to the events.
Event. An event e is a tuple e : = ( K e ; t 0 e ; t f e ) , where K f k e ; : : : ; k n e e the event, t 0 e and t f e indicate the timespan within which the event occurred. Although this is in line with event definitions used in previous works [4, 6, 10], other representations like named events, temporal subject-object predicates [7], or sentences in natural lan-guage can be adapted to our model by aligning the keyword sets.
Document. A document d is defined by its textual content that is subject to scrutiny in order to assess the validity of an event.
Evidence. An event e is said to have evidence of its occurrence in a document d with a threshold ; 0 &lt; 1 , iff at least % of the keywords K e participate together in an event reported in d , strictly within the timespan f t 0 e ; t f e g of the event.
Pair. Given an event e and a document d , we logically couple them within an (event, document) pair p : = ( e; d ) .
We distinguish between two levels of event validation: document-level and corpus-level. An ( e; d ) pair may be judged as invalid if the document contains no evidence of the event, although the event might be true with respect to other documents in the corpus. Conversely however, a pair that is judged as being valid may suffi-ciently indicate the validity of an event at the corpus-level. We do not aim at stating whether an event is true or false in general, but whether it occurs in a document or a collection.

Document-level Validation. Given a pair p and an evidence threshold , we define the document X  X evel validation as a function which determines the evidence d of e in d with threshold . The codomain of V d ( ) can vary based on the application requirements. In case a binary classification between valid and invalid pairs based on the threshold is required, then d 2f valid ; invalid g wise, one can measure evidence as the percentage of conforming event keywords without considering any threshold, and d 2 Corpus-level Validation. Given an event e , a set of documents D , and an evidence threshold , we define the corpus X  X evel valida-tion as a function which determines the evidence c of e in the corpus D . Just like at the document-level, the codomain of V c ( ) depends on the applica-tion requirements. The function depends on which corpus is used as reference to validate occurrence of the given event.
We introduce automatic event validation as a post-processing step of event detection to boost its precision. Given the set of events originally detected, only the ones judged as valid by our validation at corpus-level are retained in the set. In addition to advances intro-duced within the event detection itself, we believe that automatic event validation can boost the overall performances by process-ing the detected events in a dedicated way, exploiting information that is unavailable as input to event detection (i.e. participants and timespan of events). Event detection usually identifies events in a document collection based on a vague input, represented by a wide set of entities [4] or terms with high occurrence frequency [5, 6]. In contrast, we do not intend event validation to discover new events, since entity relationships and their durations are already suggested by event detection in form of candidate events. We instead focus on verifying their occurrence in documents.

We call this effect precision boosting , because the goal is to in-crease the precision within the set of detected events by discarding invalid events. However, this has to be accomplished without af-fecting recall, i.e. the valid events originally detected by the event detection phase should not be discarded during the validation. The benefits of this approach will be shown in Section 5.2.2.
We build a supervised learning model using features from pairs to predict their validity. Features are extracted from plain text, with-out requiring any information about structure, publication date, or markup, making our approach potentially applicable to any kind of corpora. We use the term statistics to indicate average, standard deviation, minimum, and maximum values of a given feature.
Event Features. Event features describe the event without cou-pling it with any document. They consist of the number of event keywords, their length statistics, as well as the percentage of key-words representing people, locations, organizations, and artifacts.
Document Features. Document features are extracted from each document, independent of the event to be validated. For each doc-ument, we compute the percentage of words representing different parts of speech (nouns, verbs, adjectives) and named entity types (people, locations, organizations, artifacts). For each of the pos and named entity types mentioned above, we also compute statis-tics about their positions and mutual distances. We derive features from time information in each document, such as number of tempo-ral expressions, statistics of time (in days), statistics of temporal ex-pressions positions and mutual distances in the document. We also consider the length of the document, number of sentences, length statistics of words, and length statistics of sentences.
Pair Features. Pair features are extracted from ( e; d ) pairs to give information about the extent to which a document contains evidence of the event. First, we compute the percentage of event keywords that have at least one match in the document, statistics about the number of matches of event keywords, and the percent-age of event keywords that fully appear in the document. Since matched event keywords that are far apart might have a higher prob-ability of being unrelated, we compute statistics regarding the po-sitions and mutual distances of matching keywords. Despite event keywords being mentioned in a document, they might refer to a different timespan than that of the event. Therefore, we derive fea-tures considering the temporal expressions within the event times-pan (called matching dates hereafter). We compute statistics about positions and mutual distances of both matching and not match-ing dates (the proximity of not matching dates to matching dates might be a hindrance to assessing the validity of the timespan). We also consider the position of the first date and whether it belongs to the event timespan, supposing that it might have more impact than other dates. Finally, to estimate the likelihood of event key-w ords being mutually related while referring to a matching date, we compute features representing distances between them. For each matching date and distinct keyword, we compute statistics about the distances between the date and the nearest matching word.
Document-level Validation. Once pairs have been described in terms of the features, a Support Vector Machine (SVM) is trained to predict the validity of new unseen pairs. The meaning of the pair labels l p depends on which of the scenarios discussed in Section 3.2 is considered: in case of binary classification based on a validity threshold, then l p 2f valid ; invalid g ; in case of regression of the percentage of event keywords conforming to the same event in the document and within the event timespan, l p 2 [0 ; 1] .

Given a set of pairs p i , their corresponding feature vectors f and their validity labels l p i , an SVM is trained and the learned model V d is used to predict the validity of unseen pairs p The model was trained via 10-fold cross validation over the pairs.
Corpus-level Validation. We performed corpus-level validation by combining document-level decisions. Given an event e , a set of candidate documents D , a validity threshold , and a document-level validation function V d ( ) , our corpus-level validation func-tion V c ( e; D; ) is designed as follows. In case of classification, it returns valid iff 9 d 2 D : V d ( e; d; ) = valid , while in case of regression V c ( e; D; ) = max d 2 D V d ( e; d; ) . This means that the best ( e; d ) pair evaluated at document-level drives the decisions at corpus-level. We chose this policy, as opposed to others such as averaging the validity values computed at document-level, be-cause we believe that the evidence of event occurrence in a single document is sufficient to indicate event validity in many scenarios.
Dataset. The dataset that we considered consists of a set of events and candidate documents associated with each one of them. This information is used to construct input pairs.

We used the publicly available event set considered in [3], made of 258 events detected through the algorithm presented in [11]. In this set, event keywords are titles of Wikipedia pages and the con-sidered timespan is from 18 th January 2011 to 7 th February 2011.
We chose the Web as a source for documents due to its easy ac-cessibility and wide event coverage. However, any document col-lection could be used in principle to build the ground truth. For each event, queries have been constructed by concatenating the event keywords along with month and year of the event timespan (one distinct query for each month). We used the Bing Search API to perform queries and to retrieve the top-20 webpages for each query in terms of URLs. After removing duplicates and discarding non-crawlable webpages we got 7257 documents. Plain text has been extracted from each html document by using BoilerPipe 1 , while the Stanford CoreNLP parser 2 has been used for POS tagging, named entity recognition, and temporal expression extraction.

Ground Truth. We exploit crowdsourcing, deploying the tasks on CrowdFlower 3 , in order to manually evaluate the validity of the 7257 ( e; d ) pairs in our dataset. For each pair, workers were pre-sented with a description of the event keywords and timespan as well as the document URL. The event timespan, specified by a start and end day, was strictly considered during the tasks. The work-ers were then asked to report the number of event keywords con-forming to the same event in the document and within the event timespan. We followed task design guidelines and employed gold standard questions to detect untrustworthy workers.

For each pair, we gathered 5 independent judgments resulting in 36 ; 285 responses in total (pairwise percent agreement of 0.7). We accumulated these judgments to arrive at the notions of validity of each pair by considering the judgment with the highest pairwise percent agreement between workers to indicate the validity label (in case of a tie, we considered the label that is closest to the av-erage of all judgments). From these real-valued pair labels l derived binary validity labels ( l p; ) for pairs (i.e. valid or invalid ) by applying a validity threshold as discussed in Section 3.2, such that l p; = valid iff l p &gt; and l p; = f alse otherwise. These binary labels are used when considering event validation as a classi-fication task (Section 3.2) and allow to have a more intuitive notion of document-level validity. In our experiments, we consider three different values for : 0.5, 0.65, and 1.0 (depending on the fraction of the event keywords required to conform to the event within the given timespan). At the corpus-level, following the merging pro-cess described in Section 4.2, 70.9% of the events depict a valid-ity greater than  X 0 X . After performing the same binarization as for document-level, the percentage of valid events at the corpus-level is 63.2% ( = 0 : 5 ), 62.7% ( = 0 : 65 ), and 60.4% ( = 1 : 0 ).
Evaluation Metrics. We center the evaluation around the Co-hen X  X  Kappa between validity labels and the output of our automatic validation, both at document and corpus-level. The Cohen X  X  Kappa ( K ) determines the level of agreement between two judges by con-sidering the probability that they agree by chance. We also report the accuracy ( ACC ) of the methods for completeness. We compute ACC and K for each validity threshold previously discussed. For instance, K with = 0 : 5 represents the Cohen X  X  Kappa observed after binarizing the validity labels with a threshold of 0.5. Since the original validity labels are real-valued, we also report the Pearson Correlation Coefficient ( r ) when predicting them via regression.
Parameter Settings. The classifiers employed to predict valid-ity of pairs, built using the SVM implementation of LibSVM Gaussian Kernels and were trained via 10 X  X old cross validation. The parameters were tuned to C = 6 : 2 ; = 1 : 0 in case of classifi-cation, and to = 0 : 5 ; = 0 : 3 for regression. The evaluation was done using the predictions generated during the cross validation.
Baselines. The first baseline, Keyword Matching (KM), vali-dates pairs by counting the percentage of event keywords present in documents. In case of multi-term keywords, the matching of one term is considered a match for the entire keyword. The second one is the approach described in [3], denoted CF Validation (CF). The occurrence of events in documents is assessed by considering the presence of dates within the event timestamps, estimating the re-gions of text associated to these dates, and returning the percentage of event keywords present in these regions. The validation is based on hand-crafted rules, a big limitation of the work. The thresholds imposed on validity labels are also applied to the baselines.
Document-level. The results of event validation at the document-level, considering different validity thresholds ( = 0 : 5 ; 0 : 65 ; 1 : 0 ), are reported in Table 1. We evaluated our method, called Eventful , when considering two different models, namely pairs and all : the former only considers pair features in the learning, while the lat-T able 1: Performances of the automatic document-level validation.
T able 2: Performances of the automatic corpus-level validation. ter exploits all the available features described in Section 4.1. Our approach outperforms the baselines under all the criteria, showing that combining information from events and documents via ma-chine learning is more effective than (i) considering mere keyword matching ( KM ), or (ii) designing validation rules that cater for re-lationships between event keywords and temporal conformity to the event timespan ( CF ). In particular, according to [9], the values achieved by our models indicate a substantial level of agreement. The improvement of the all model over CF is of 81% for = 0 : 5 and of 138% for = 1 . Pairs alone sufficiently outperforms the baselines. However, exploiting support information from events and documents ( all model), we achieve higher agreement.
Considering different validity thresholds, we observe a decreas-ing trend of performances for increasing validity thresholds (except KM , characterized by low and noisy results). We allude this to the fact that the methods face hindrance when validating those pairs where all the event keywords appear in a document, but only a sub-set of them conform to the same event within the given timespan.
Corpus-level. Table 2 shows the results for corpus-level valida-tion. Our method outperforms the baselines under all the criteria, with all achieving the highest performances. Comparing the results of document-level and corpus-level evaluation (Tables 1 and 2), it can be observed that the absolute values in the latter are relatively higher. This is because at the corpus-level a single valid ( e; d ) pair is sufficient to validate the event, despite the presence of errors in the other pairs corresponding to the event.

Another point of difference is that the values of K at corpus-level increase for increasing values of validity threshold , while they decrease at document-level. As mentioned before, increasing the validity threshold at document-level introduces difficulties in correctly evaluating valid pairs. Inspecting the confusion matrices, we found that the learning process adapted by increasing the per-centage of negative detections (either true or false) to reduce clas-sification errors. This ensured a lower number of false positives at corpus-level despite being less accurate at document-level. On the contrary, when the validity threshold is low ( = 0 : 5 ), the learning process was more confident in classifying valid pairs and the re-sulting output rate of positive detections was higher, leading to an increase of false positive pairs as well. This generated more false positive events at corpus-level, since a single false positive pair is sufficient to validate an invalid event as valid.
The effects on the performances of event detection after apply-ing event validation as post-processing phase are reported in Table T able 3: Effects of applying event validation after event detection. 3 for different values of . P det refers to the precision values (one for each ) within the set of detected events, which are coherent with the performances reported in [4]. The precision and recall (with respect to the true events discovered through event detection) in the event set after applying event validation are indicated as P and R val , respectively. Our method boosts the precision achieved by event detection up to 0.894 for = 0 : 5 and 0.919 for = 1 : 0 . Moreover, the high values of recall (0.899 or higher) indicate that most of the true events present in the original set are preserved by the validation. Our method beats CF both in precision and espe-cially in recall, showing a better capability of not discarding true events. Regarding KM , the low increase in precision and the high recall show that performing validation via keyword matching has no effect on the performances of event detection: most of the events are judged as valid and retained, since the appearance of their key-words in documents is a sufficient condition of event validity.
In this paper, we investigated the problem of event validation with respect to a given corpus, proposing an effective method for automatically validating the occurrence of events in documents. We also introduced automatic event validation as a post-processing step of event detection to boost precision. The proposed method outper-forms baseline and state-of-the-art methods for event validation and allows to increase the precision of event detection. We plan to ex-tend our work by considering semantic features to predict validity, as well as by incorporating user feedback via active learning. Acknowledgments This work was partially funded by the Euro-pean Commission in the context of the FP7 ICT projects ForgetIT (grant no: 600826) and DURAARK (grant No. 600908).
