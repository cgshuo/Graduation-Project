 Modeling and predicting user attention is crucial for inter-preting search behavior. Current applications include quan-tifying web search satisfaction, estimating search quality, and measuring and predicting online user engagement. The most direct way to measure attention is through eye gaze tracking, which is not yet widely available. While prior re-search has demonstrated the value of mouse cursor data and other interactions as a rough proxy of user attention, accu-rately predicting where a user is looking on a page remains a challenge. This problem is exacerbated when moving be-yond the traditional search result pages to other domains, where high diversity of content and visual presentation often affect how users examine a page. We posit that in order to accurately model user attention online, interaction signals should be grounded to the underlying content. To this end, we introduce a principled model to connect interaction sig-nals with page content features, which we call Mixture of Interactions and Content Salience (MICS). To our knowl-edge, our model is the first to effectively combine user in-teraction data with visual prominence, or salience, of the page content elements. Extensive experiments on multiple popular types of Web content demonstrate that our model significantly outperforms previous approaches to searcher gaze prediction, which use only the interaction information. Grounding the observed interactions to the underlying page content provides a general and robust approach to user at-tention modeling, which could enable more powerful tools for search behavior interpretation, and ultimately search qual-ity improvements.
 searcher attention; search behavior; content salience
Inferring searcher attention in Web search, and more gen-eral online settings, has been recognized as a key aspect of  X 
Work done as student at Emory University.  X  Work done at Emory University.
 Figure 1: User attention on a Web search result page (a) vs. on a social network Web site (b). The color indicates the time spent viewing a region of the page, ranging from red (high concentration of attention) to blue (low attention). The marginal distributions are projected onto the horizontal and vertical axes. relevance evaluation, search quality, and user interface de-sign. More generally, inferring and measuring user attention is key for diverse areas such as online advertising, education, and crowdsourcing. The availability of accurate user atten-tion data at scale could potentially enable vast opportunities for search quality evaluation and richer models of user inter-action with generic Web page content. The challenge is how to infer attention robustly and for diverse kinds of content -the problem we aim to address in this paper.

Inferred attention has already been used in a variety of ap-plications ranging from improvements of web site usability [24, 28], to search relevance estimation[15, 16], and auto-matic generation of attention-biased summaries [1]. User attention data gains even greater importance as Web search shifts towards addressing user information needs directly on the search result page, which does not require a click on the result, making click-based evaluation of search quality more challenging (e.g., [13, 19, 32, 26]).

Previous work on attention modeling from cursor inter-actions on the Web has mostly focused on Web search and E-learning settings, where a user X  X  eye gaze, and mouse cur-sor positions, are somewhat coordinated[34, 14, 18]. When extending the attention prediction task to other Web page types, prediction becomes more challenging. For example, as search over social media gains popularity, atten-tion models must be adapted to take into account more com-plicated page layouts, with some content static (not query-specific), whereas other, related content in fact might answer the search intent directly. Figure 1 (b), based on the data reported by [25], illustrates searcher attention heatmap for a popular social media site. Note that attention is shifted to the right and towards the bottom of the page  X  contrasting it with the more well known  X  X olden triangle X  examination pat-tern for a traditional Web search result page, illustrated in Figure 1 (a). As search spans increasingly diverse domains, such differences abound. Furthermore, as search engines in-creasingly incorporate images and other visually attractive elements into the search result, models of searcher attention have to be revisited accordingly.

Our aim is to develop a robust, yet principled model of searcher attention that combines both content and inter-action signals. For this, we adapt techniques and ideas from computational neuroscience of visual saliency to de-velop a Mixture of Interaction and Content Salience ( MICS ) model, that is able to integrate content-based static signals, together with the user X  X  interaction data, in order to pre-dict where on a page a searcher is paying attention. We show that our model achieves significantly lower error com-pared to previously reported state-of-the-art techniques us-ing interaction-only signals. Specifically, our contributions are:
Next, we review previous work on predicting searcher at-tention. Then, in Section 3 we present out general MICS model for inferring user attention on Web pages, and describe the specific reference implementation for popular search domains (Section 4). In Sections 5 and 6 we present empirical results on using our MICS model to predict user attention. Finally, we outline the implications and directions for future work in Section 7, which concludes the paper.
Our work bridges three main areas of research: computa-tional modeling of visual attention, primarily developed in the fields of computational neuroscience and computer vi-sion, user engagement and attention modeling on the Web, and searcher interaction modeling, primarily focused on the web search domain, from the fields of human computer in-teraction and information retrieval.
There has been extensive research on automatically iden-tifying the most important, or salient regions in a given im-age, where a person examining the image is likely to attend. As we build on the ideas explored in computational visual salience research for Web search tasks, we briefly introduce the underlying ideas and techniques.

Different formulations of salience have been proposed, e.g., focusing on identifying the image regions which initially at-tracts attention, or the aggregate attention distribution after a longer period of examination. In order to model salience computationally, three major factors were identified that af-fect human attention during visual examination of an im-age (or a Web page): (i) the visual importance, or salience of areas in the scene, (ii) memory and expectations about where to find the information, and (iii) the task and infor-mation need at hand. Depending on the modeling choices, the resulting models can be either task-agnostic (i.e., only consider the salience of the image regions based on content alone) or task-driven (i.e., that consider salient regions for a given task). These models are typically categorized as either bottom-up models or top-down models.

In bottom-up models the salience of image areas is typi-cally computed based on low-level image characteristics, par-ticularly contrast, color, intensity, edge density, and edge orientation (see [20]). One well known ( bottom-up ) salience model was introduced by Itti [20]. As do other bottom-up approaches, this framework attempts to simulate human at-tention as a feed-forward neural network. That is, it takes various features of the stimulus, such as the color contrast, gradient and motion maps, as input, and produces a single salience map that highlights the locations where the human gaze is mostly likely to attend. According to at least some neuroscience theories[20], a representation similar to the de-scribed salience map may be used by the human brain to control the human oculomotor system and to direct eye gaze to explore the stimulus, such as an image or a web page.
While it is thought that the human eye is initially at-tracted by the most salient regions of the image, theories about subsequent examination differ. For example, some have argued that the subsequent examination points are planned in order to maximize the resulting information gain [20, 21, 39]. While low level visual salience may direct the first gaze position, or fixation, it is believed that ultimately memory and expectations (e.g., about what is shown in the image and where to find specific objects) begin to also play important roles in subsequent examination positions. To take advantage of this insight, top-down (task specific) mod-els which account for these effects were introduced [20, 30, 31].
Modeling searcher attention on the Web introduces addi-tional challenges as content can be more varied semantically (e.g., combining both images and text), dynamic, and inter-active. Stone and Dennis [38] used latent semantic analy-sis of topics to predict eye movement fixations on a generic Web page given position of text elements and hyperlinks. Our work is different in that we incorporate much richer in-formation about elements semantics. In addition, we aim to combine Web page content salience with user interac-tions on the page. More recently, reference[35] predicted aggregated salience of Web pages based on visual content alone. As in most visual salience approaches, the authors used pixel based information to construct variety of salience maps (based on pixel level contrast, color, orientation) that are combined to produce a single map that approximates dis-tribution of eye gaze fixations on a Web page. Our method builds on some of the ideas in these papers, but takes a different approach by modeling user attention with a para-metric distribution derived on a relatively small number of prominent Web page elements. It allows us to avoid dealing with pixel level salience (and high computational complex-ity associated with it) and develop a unified model of user attention on Web pages that takes advantage of Web page element importance and user interaction with the page.
More closely related to our work, Buscher el al. [4] ana-lyzed the factors determining fixation time, i.e., the time a user spends carefully examining, or fixating on, individual elements of a Web page. They found that the size of an HTML element and its proximity to the top left corner of the Web page play a major role in amount of attention this element receives. Instead of implicitly modeling scrolling ac-tivity like in [4], our work focuses on modeling user attention on a visible portion of the page (i.e., viewport ).
In the Web search domain, studies of user attention using eye tracking provided numerous insights about typical con-tent examination strategies, such as top to bottom scanning of Web search results [11, 29]. Other studies have investi-gated the effect of caption length and quality on searcher attention, identifying relevant page regions, and for many other tasks. Perhaps the most well known outcome of this line of work is the  X  X olden triangle X  of attention on Web search results, often present in the traditional search result layouts. A more comprehensive overview of classical models of searcher attention is available in [17]. More recently, a line of research on user engagement with online content has emerged[27]. For example, the users X  engagement with news examination is often influenced by affect, sentiment, and vi-sual clues[2]  X  which in turn can be used to better predict attention.
Perhaps the most practical application of attention model-ing is to predict searcher attention  X  which could be applied for tasks such as search evaluation, improving search quality, and search advertising. Due to the high cost and the nec-essarily small scale of eye tracking studies (due to requiring specialized eye tracking equipment), a considerable amount of research has been devoted to finding more scalable meth-ods of attention measurement. In particular, mouse cursor tracking was proposed as a cheap alternative to eye tracking by inferring searcher gaze position from mouse cursor po-sition. The relationship between cursor and gaze has been studied empirically[8, 34, 14, 18, 32]. Chen et al. [8] was one of the first to study coordination patterns between mouse cursor and gaze. They classified mouse cursor movement into five classes:  X  X tay Nowhere X ,  X  X o Nowhere X ,  X  X tay the Same Region X  and  X  X o to New Region X . They found that the distance between mouse cursor and gaze position was smallest when the user moved mouse cursor in order to per-form an action, e.g., pointing or clicking. However, when the cursor remained inactive, the reported accuracy of attention measurement degraded.

In the context of Web search, the coordination between mouse cursor and eye movements was first reported by Rod-den et al. [34]. They reported the alignment between the user X  X  eye movements and mouse movements when scanning a web search results page, and identified three patterns of active mouse usage: following the eye position vertically , fol-lowing the eye position horizontally , and using the mouse to mark a relevant result . Guo and Agichtein [14] proposed a natural extension Rodden X  X  work -to predict eye-mouse co-ordination (i.e., whether the mouse cursor is in close proxim-ity to eye gaze at any given point in time). These works were further extended by Huang et al. [18] to directly predict the gaze position from mouse cursor movement, showing that the cursor and eye gaze are best aligned when the user is per-forming click action, and have the largest average distance in periods of cursor inactivity. Navalpakkam et al. [32] stud-ied the coordination of cursor and gaze on non linear search result page layouts, e.g., in the presence of rich information panel on the right side. They showed that a non-linear re-gression model offers more accurate predictions of gaze posi-tion, and outperforms previous approaches. Also related to our work is the research of Diaz et al.[9], which proposed a two-dimensional attention transition model based on mouse cursor movement over grid layouts.

In this work, we build upon these ideas[18, 32, 9] and instead of predicting gaze position in isolation (without spa-tial constraints) we develop the first model that combines the evidence provided by interaction data with the informa-tion about Web page elements visible to the user through the browser X  X  visible part of the page, or viewport. Further-more, unlike in previous work in gaze prediction, we extend our model to predict attention on pages in other domains, moving beyond web search result pages.
In this section we present our MICS model that allows us to more effectively infer user attention on Web pages by combining content and interaction signals. This is an even more challenging problem than predicting attention in im-ages, as is done in computational visual salience research: Web pages contain extensive layout structure and multiple layers of meaning encoded in the text, layout, and metadata about a page.

To address this challenge, we exploit the observation that web pages, more so than image-only stimuli, can be effec-tively annotated with areas of interests (i.e., potential tar-gets in the top-down models terminology), that can enable more accurate modeling of user gaze during web browsing or information seeking activities. Such annotations can be based on set of rules or rely on an automatic classifier to segment page elements that take part in the model. While the ultimate accuracy of the model is likely to depend on the quality of the page segmentation, for now let us assume that for popular types of Web pages (e.g., Search Engine Result pages or social media news feeds) such segmentation is available. The details of the particular page segmenta-tion algorithm used in this work are provided in Section 4.1. Assuming a page segmentation is given to us, we can now define our Mixture of Interactions and Content Salience (MICS) model.
Our approach to modeling the allocation of user attention on a page is derived from the general idea of the mixture of experts model in machine learning [22]. Our goal is esti-mating the task-specific (top-down) element importance on the page, and then refine the prediction based on how long each element on a page was displayed to the user, and where it was in the viewport 1 and what interactions the user per-formed.

MICS operates by sub-dividing the visual space into re-gions -each corresponding to a particular Web page ele-ment. While the distribution of gaze positions within each element is determined only by the features of the element, the probability of attending a page element depends on rel-ative attractiveness of all the elements displayed in the visi-ble portion of the Web page. Intuitively, in our model each element  X  X ompetes X  for user attention against other visible elements on the page. Unlike previous approaches, which mainly use the visual stimuli information on pixel level (i.e., visual salience) to predict attention, our model takes advan-tage of the information about page element rendering (how elements are displayed by an Web browser to a user) and constructs compact, yet expressive, distribution of user at-tention in the browser viewport.

More formally, our model defines a probability distribu-tion of gaze position over the visual space (browser viewport)
We use the term viewport to denote the portion of a Web page visible to the user at given point of time. Figure 2: The MICS model for search attention modeling. Variable Description
N number of gaze data points n i number of Web page elements at i -th view-x i  X  R 2 i -th gaze position z i  X  X  1 ,..,n i } index of the Web page element being  X  ij  X  R 2 mean of the j -th element Normal distribu- X  ij  X  R 2 variance of the j -th element Normal distri-d ij  X  R 2 position of the j -th element p dimensionality of element X  X  feature space  X   X  R p feature weights for the element importance  X   X  R ( p  X  2) feature weights for the element means  X  ij
 X   X  R ( p  X  2) feature weights for the element variances f j  X  R p feature vector of j -th page element.
 Table 1: Summary of the notation used in the MICS model. which is represented as a mixture of distributions -each cor-responding to a particular web page element. This can be viewed as a particular type of a mixture of experts model (MoE, [22]), where each expert corresponds to a distribu-tion representing the individual Web page elements. The reason MoE formulation is particularly well-suited to this setting is that it naturally manages uncertainty about the  X  X ttractiveness X  of each element, which can be refined using additional features of the content element itself, or, later on, with interaction data.

MICS can also be viewed as a generative model. Fig-ure 2 presents the MICS model diagram in plate notation. Table 1 defines the notation used in Figure 2. In the di-agram, i stands for each data point, which consists of the set of elements and their locations on a page, visible at that time on the page, and the corresponding gaze position co-ordinates x i . MICS states that the i  X  th gaze position is generated from the observed element positions d ij with their corresponding features f ij . The element X  X   X  ij parameters are defined as: where  X  ( x ) ij is the horizontal component of element X  X  Normal distribution mean parameter, d ( x ) ij is the element X  X  top left coordinate x , width ij is the width of the element,  X  is a free parameter estimated during training, and f ij is vector of element X  X  features. The element X  X  variance parameter  X  is computed as: where  X  is free parameter estimated during training. The probabilities of viewing an element are parametrized using the softmax function with the free parameter  X  :
To make the model training more tractable, we make a simplifying assumption that all gaze positions are generated independently from each other. This allows us to derive an efficient inference and learning algorithm. Our algorithm learns the element importance weights  X  for the MICS model as follows. Let the dataset D = { x i } N k i =1 collection of N gaze positions for k -th page view. Note that depending on the scroll position of the browser, there could be a different number of elements visible in the viewport, we denote this number as n i . We assume that information about position of page elements ( d ij ) and their features f ij is available.
In order to find plausible values for model parameters  X  = {  X ,  X  ,  X  } we perform maximum likelihood estimation That is we optimize log-likelihood of gaze observations given the model parameters: L ( x i |  X  ,  X  , X  ) = In order to optimize the log-likelihood we use Stochastic Gradient Ascent (SGA) method with learning rate anneal-ing. The model is implemented using symbolic differenti-ation tool Theano[3] that automatically generates code for gradient computation.
Once the MICS model is trained, gaze prediction distri-bution is computed as: Note that P ( z ij = j ) gives us an importance weight (from 0 to 1) for each of page element d ij . Thus, we could view it as a mixture distribution of n i Normal distributions as-sociated with the attractiveness and uncertainty predicted for each element, respectively. Computing the density of this distribution over a fixed grid of 2-dimensional points is tractable, as we demonstrate in the experiments section. Given the predicted density, the expected gaze position can be obtained by computing the maximum likelihood estimate of the x and y values under the predicted density distribu-tion.

Since MICS is a generative model, for completeness we describe the generative process of how gaze positions could be generated by our trained model. The following generative process can be used to generate an i -th sample from our model: 1. Generate z i with probability P ( z i = j |  X , f ij ) 2. Generate gaze position x i  X  N ( d ij +  X  ij , X  2 ij ), where
In practice, in order to avoid computationally costly sam-pling procedure, we could obtain estimates of gaze positions by numerically computing expectation over the predictive density: where L and M are number if integration points in x lm are nodes in two dimensional grid used for computing the expectation.

Having defined the general MICS model, we now turn to the specific implementations of MICS to be validated on two increasingly difficult tasks, as described next.
We now describe the specific implementation of the MICS model including Web page segmentation and content fea-tures that were used in this work.
Identifying most prominent Web page elements is not al-ways a trivial task. Often, Web pages contain thousands of HTML elements, many of which are not even displayed to the user. As our goal is to model attention in presence of significant (visible and important) page elements, it is desirable to eliminate page elements that are unlikely to at-tract user attention, thus, considerably simplifying model-ing complexity. To this end, our web page content analysis consists of first segmenting a web page into HTML DOM el-ements, then selecting a subset of the elements to consider, and finally extracting content features just from that sub-set. To take advantage of all Web pages in our dataset we employ both rule based segmentation, applied for frequent page types, and classifier based segmentation, applied for less frequent page types in our dataset. We would like to emphasize that this is just one of many ways to implement content element segmentation and other variations could be explored in future work to further improve performance.
For web pages that occur relatively frequently in our data, such Google search result pages or Twitter pages, we imple-ment manually engineered segmentation. This is a common approach taken in previous work, and is applicable to a large and important subset of web pages which tend to share the same layout and page template.

For less frequent pages, we apply a supervised automatic classifier that for each web page layout element outputs a binary decision -whether this needs to be segmented or not. This makes our approach potentially applicable to a wider range of web pages. To perform this classification we use Gradient Boosting Decision Tree classifier (GBDT) [10]. The classifier uses page element X  X  features to determine if element needs to be included or not. In order to train the this classifier we manually annotated page segmentation for 20 pages. Table 2 shows features used by our classifier. We utilized several types of information including the element X  X  DOM Tree features (e.g. amount of links), the element X  X  position information and size (e.g., width and height), as rendered by the browser at the time of page visit, and the element X  X  style (e.g. visibility and text font size).
Figure 3 shows example of the page segmentation output for a Google search result page. While the granularity of the segmented elements varies for different page types, we see that the elements carrying most important content in-formation are captured. The fact that such segmentation Figure 3: Example of page segmentation for a search result page (bottom of screen shot is cropped to fit). only eliminates page elements that are not displayed in the browser or used only for layout or formatting, simplifies the salience modeling in a sense that we do not need to account for thousands of elements in our model.
We re-use content features employed by page segmenta-tion algorithm (shown in Table 2, Content feature group). Our features encode information about element size, posi-tion on the page, style and font size, and simple information content measures such as number of words normalized by area. As discussed, additional more sophisticated content representation features could be invented, but in this refer-ence implementation we opted for simplicity and generality. Despite the simplistic representation, the MICS model is able to use these features effectively, as shown in the exper-iments below.

MICS naturally allows to enrich the previously proposed regression models by allowing the features to be element-specific . For example, how MICS can exploit the information on how close is the mouse cursor to the particular element or whether a mouse cursor will hover over the element in the next few seconds. Such features allow the MICS model to learn cursor gaze coordination patterns not only on the overall behavior level, but on the element level as well. For example, if the mouse cursor hovers over the search box el-ement, it is very likely that the user is going to reformulate the query terms, which implies the attention is focused on the search box. In contrast, if the cursor hovers over the elements located on the right side the search result page, it is less likely that the user X  X  attention is following the cursor. Thus, to capture user interaction with the given element we include features that encode relative position of the cur-sor the element, cursor velocity, binary features indicating whether cursor is currently hovering the element or user is clicking on the element. Table 2 lists both Content and Interaction features used in our model. To account for a po-tential lag between interaction and eye gaze movement we concatenate features in the Interaction group at adjacent time d steps. The offsets for the adjacent time steps are { X  1 ,  X  2 ,  X  4 ,  X  8 ,  X  16 } .

We train the MICS model using Stochastic Gradient As-cent algorithm with minibatch size = 100 and learning rate 0 . 001. To improve convergence speed we randomly shuffle training examples before start of the training.

To obtain the predicted gaze position using the MICS model the we use expected  X  x under the predicted attention distri-bution as described in Section 3.3 with number of integration steps L = M = 100.
In order to compare the effectiveness of different approaches for attention modeling, we performed a realistic user study, with eye tracking to collect the eye gaze data as ground truth. In the rest of the section we provide the details on data collection, the baseline models that were proposed in prior work, and the evaluation metrics used for the experi-ments in the next section.
In order to investigate the effects of domain and task on searcher attention, we systematically varied the scope of the search task, and the search domain . The tasks were mod-eled on the studies in [12, 33] to be representative of the common search tasks in common search domains. Specifi-cally, the task ( scope ) was designated as either Focused or Broad . The Focused information need required the users to find specific information, e.g.,  X  X ow many megapixels does Nexus 5 camera have? X  in Web Search domain, while Broad information tasks had no specific answer, but rather asked the users to learn about a particular topic, e.g.,  X  X earn what people on Twitter are saying about gay marriage X  in the So-cial Network domain. Two focused and two broad tasks were performed by each user in each of the five common search domains: Web Search (Google), Shopping (Amazon), Social Network (Twitter), News (CNN) and Wikipedia , for a total of 20 tasks per user. We randomized the presentation order of the tasks to eliminate possible learning effects. To reduce the biases in the training data, we balanced the study design by ensuring that the same amount of data was collected for each ( domain , scope ) pair.

For the user study, we recruited 20 undergraduate and graduate students (11 of them males) from a major univer-sity. Each user was asked to perform four  X  X arm-up X  prac-tice tasks to become familiar with the study flow, followed by the 20 tasks that we use in our analysis. All user actions, including query input, page navigation, clicks and mouse cursor movements were recorded using a custom extension to the Firefox internet browser. To capture the user X  X  eye movements we used the Tobii T60 eye tracker system built into a 17 X  monitor with 1280  X  1024 screen resolution, record-ing eye gaze positions with frequency of 60 Hz. The To-bii system is head-free, where the participant could sit and interact with the computer naturally without being locked into a specific head position or body posture, making the collected interaction data more realistic. The eye movement data was pre-processed using Tobii Studio software to seg-ment the data points into eye fixations (i.e., times of slow and detailed examination) and saccades (i.e., times of fast movement when the eye jumps to examine a new position).
Overall, the data includes eye movement and interactions data for 2,890 page views, with 673 page views correspond-ing to the search pages. The eye gaze data contains 93,290 fixations. As in prior work [18, 32] we interpolated the gaze and cursor data every 100ms using nearest neighbor inter-polation method, which resulted in 233,225 aligned eye gaze and cursor data points. Discretizing gaze data with fixed sample rate greatly simplifies prediction task by eliminating need to infer fixation duration [18, 32]. The dataset size is comparable to previously reported studies, and was primar-ily limited by the effort required to recruit and supervise the user study participants.
Several works have attempted to infer the user X  X  gaze po-sition from cursor interactions. Most of the prior approaches trained a regression model to estimate the gaze position from cursor interaction features. We describe the two recent well known models that we use as state-of-the-art baselines for the subsequent experiments.
 Linear Regression (LR). Huang et al. [18] proposed to directly predict a searcher X  X  eye gaze position from mouse cursor movements on search result pages. They used a lin-ear regression model to learn the relationship between eye gaze and cursor movement features. Their model can be formulated as: where w is the vector of feature weights, v i is vector of fea-tures for the i -th data point. During training the model computes an optimal vector of weights w , such that the discrepancy between model predictions and the actual gaze positions is minimized. Specifically, the optimization min-imizes the squared error between the actual eye gaze posi-tions and the predicted positions.
 Non-Linear Regression with Kernels (KR). Recently, a more sophisticated model of attention prediction from was introduced by Navalpakkam et al. [32]. Unlike the LR model, which assumes a linear relationship between cursor features and gaze position, the KR model is able to capture non-linearities of the data, but adding an additional transforma-tion  X  over the feature vector. The KR model is defined as: where w is the vector of feature weights, v i is vector of fea-tures for the i -th datapoint, and  X  ( v i ) is the Nystrom ap-proximation of the Gaussian Radial Basis Function kernel matrix [36] with n = 500 basis vectors. This transformation is known as a  X  X ernel trick X  that helps capture non-linear dependencies between cursor features and gaze, while main-taining relatively easy training procedure. During model training we find the optimal set of vectors w so that it min-imizes the discrepancy between the model predictions and gaze positions in the training data.
 Baseline Interaction Features In our experiments we use the same basic cursor movement features for all models for a fair comparison. This set of features include and extend the published features used in prior work [18, 32]. These feature vectors are computed to represent each data point (mouse cursor position): To account for longer range dependencies between the gaze and cursor movement for each time step we include features from previous time steps, logarithmically spaced, following the approach of [32]. The time step offsets were chosen as { X  1 ,  X  2 ,  X  4 ,  X  8 ,  X  16 } , capturing the 100ms to 1.6 second  X  X istory X  of the mouse movements. While this is a minor extension compared to previously proposed approaches, all the compared methods benefited from this additional con-textual information.
For comparing the performance of the MICS model against the baseline LR and KR models, we use the root mean squared error (RMSE) and mean absolute error (MAE) met-rics, used in prior work for this task.

More formally, given a sequence of true and predicted gaze where N is the number of gaze data points, x ( i ) gaze actual gaze position at step i , and x ( i ) pred is the predicted position, and the difference is the square of the Eucledian distance between the two. While RMSE is convenient from the optimization perspective (both LR and KR minimize the mean squared error, or MSE, on the training data), it dis-proportionally weights large errors. Therefore, we also consider mean absolute error, also used in prior work, which does not introduce this bias. The MAE is computed as: where the sum is over the Euclidean distance between the actual and the predicted gaze positions.

To achieve more robust estimates of models X  performance, all the experiments were performed with 3-fold cross valida-tion (CV). Each of the metrics is computed as the average across the hold-out (test) folds.
Table 3 summarizes prediction performance for the base-line models LR and KR and our MICS model, averaged across the hold-out samples, in the cross validation setting. MICS performs significantly better than LR and KR in all of the domains ( p &lt; 0 . 001, two tailed t-test). Reduction in error varies from 7% in Shopping domain to 35% in So-cial Network domain RMSE =237.8 px. The lowest pre-diction error was obtained in the Social Network domain ( RMSE =237.8px, MAE =206.3px), while the Shopping do-main appeared to be the most difficult to predict resulting in the highest error ( RMSE =335.7 px, MAE =298.6px). We believe that the reason for the large performance improve-ments lie in the additional power available to the MICS model. Both LR and KR models make strong assumptions about the relationship between gaze and cursor interactions, relying on a constant bias term independent of the actual content shown to the user. Since user attention distribution heav-ily depends on what is shown the screen (e.g, see Figure 1), a constant bias that works for different types of pages may not exist. In contrast, MICS , by design, follows the content, and is able to supply a multi-modal predictive distribution dictated by the Web page elements visible to the user.
Interestingly, on the Web search domain, MICS also ex-hibits substantial reduction in error on the horizontal dimen-sion (RMSE x and MAE x ), making it even more appealing for evaluation  X  when search results may be shown to the right of the organic search results [32]. Such results attempt to provide users with direct answers to their information needs without requiring users to click. Previously, it has been proposed [32, 26] to utilize user attention for evalua-tion, providing a natural application of MICS for this task.
Our results demonstrate that it is possible to learn Web page element salience or attractiveness that is generalizable across different page types. This is even more encouraging since the Web search engines are constantly experimenting with various ways to improve user interface of search results and maintaining an attention model that can only work for a certain page configuration would severely impact its use cases. While MICS outperforms prior approaches in the gaze prediction task, it provides a general and principled way to integrate page content information into the attention model. The behavioral features allow MICS to make more sensible, time dependent predictions and capturing cursor-gaze coordination patterns.
We have shown that MICS is able MICS to learn salience of Web page elements and to combine it with information about user interaction. In this section we highlight implica-tions of this work, provide more intuition on why MICS is able to outperform previous models and discuss potential limitations of our approach.

We first analyze the contributions of the content-based vs. interaction-based signals to better understand the per-formance improvements of the MICS model. To this end, we performed a feature ablation experiment, where we com-pare the model variants while removing the corresponding feature groups (Table 4). We find that certain page types benefit from Content and Interaction to different extent. In Web Search and Wikipedia domains we find that both Content and Interaction feature groups are not particularly helpful independently of each other. Interaction features ap-pear to be more important for News , Shopping and Social Network domains, where model performance drops substan-tially, compared to the model with ablated Content features. In all cases, the combination of both content and interaction features performs better than either signal alone.

To further understand which Web page elements MICS finds important we examined element importance weights given by P ( z i ). Figure 4a shows an example prediction for a part of a Social Network Web page. Red boxes boxes indicate the segmented Web page elements. Line width of the bounding boxes reflects the element X  X  relative importance weight given by the MICS model. Hence, only a few elements stand out as important in Figure 4a. We see that the Twitter mes-sage displayed in the center of the page draws most of the attention. The next most prominent element on the page is another message, displayed towards the bottom of the page. In contrast, the  X  X ecommended users X  feature elements re-Table 5: Features with largest weights learned by the MICS model. ceive much smaller weights -an indication that MICS is able to find elements which are likely to attract user atten-tion. Another example is shown on Figure 4b. It visualizes the relative importance weights for a Wikipedia page. We see that text paragraphs near the center of the screen (on the right column) are the most prominent. Menu items and contents navigation blocks receive smaller weights. This is in agreement with prior work of Buscher et al. [5] that an-alyzed aggregated pattern of user attention on the screen. These examples illustrate that MICS identifies task-specific, salient page elements across different page layouts. These el-ements (as described in Section 3) are then used to construct a mixture distribution that helps predict user attention on any given viewport.

To further understand which element and interaction fea-tures MICS considers important with respect to  X  X ttract-ing X  attention, we report the weights  X  assigned to the ele-ment features (Table 5). For interaction features we include the time step offset modifier. The DistEuclidean feature has largest negative weight, which reduces element impor-tance when cursor moves further away (DistEuclidean in-creases) from the element in next 16 time steps which in our data translates to 1.6 seconds. Height is positively related with element importance and has weight of 0.763. Interest-ingly, CursorOn(dt+=8) has almost two times higher weight than 0.402 CursorOn(dt=0)  X  which capture element hover interaction. This is consistent with findings of Huang et al. [18] who reported a positive lag of 700ms between gaze and cursor positions. MICS assigns relatively high weights to features related to the element X  X  tag (IsH3 X  X eader and IsP X  X aragraph). Regardless of the element importance, the weights decrease when the cursor starts moving (SpeedAbs has negative weight -0.414), indicating the attention is no longer on the element. Finally, MICS finds CursorSameVert to be positively related with the element importance.
This work can be potentially improved or extended in an number of ways. In particular, web page segmentation ap-proaches (rule based and classifier based) used in the cur-rent implementation can be improved. That is, rule-based segmentation may only capture a limited set of pages with pre-defined HTML template or layout. Alternatives include other popular Web page segmentation approaches, such as [7], or using semantic relationship between Web page blocks [37], or a combination of visual, text and link information [6]. These more powerful segmentation mechanisms can be naturally incorporated into the MICS implementation with operates over whatever page elements are provided by the segmentation step. Another area of improvement is ef-ficiency of the implementation. More accurate prediction performance comes at a price of higher computational com-plexity during training, compared to the baseline models. While LR and KR models enjoy the benefits of convex opti-mization and even allow closed form solutions (in a matrix form), optimization problem that comes with MICS model is inherently non-convex and requires application of iterative methods. In our experiments, MICS converged in about 10-20 iterations, which translate to 1-2 hours of run time using Theano [3] generated code on Tesla K20 GPU. At in-ference time, MICS incurs the additional cost of computing the expected gaze position through a numerical integration described in Section 3.3. Our results demonstrate that our optimization method (Stochastic Gradient Ascent) is able to successfully train a model that performs well on the evalua-tion metrics. As another promising future direction, Naval-pakkam et al.[32] showed that gaze prediction error may be further reduced by personalization  X  i.e., by additionally tuning the set of vectors w for each user. While person-alization is orthogonal to the ideas proposed in this paper, further personalizing the MICS model can be explored in future work.
We have introduced MICS , a robust and principled method for connecting interaction data with the underlying page content for predicting user attention. Results validated against eye gaze tracking data show that MICS is more accurate than previous state of the art models that consider inter-actions alone. We have shown that the cursor interaction features allow MICS to make more sensible predictions that capture cursor-gaze coordination patterns on specific Web page content. Importantly, MICS forces the most likely gaze position to be within, or in close proximity to, the prominent web page elements. This feature could poten-tially offer better gaze prediction performance for users who do not use the mouse pointer actively, or only to perform necessary actions. This could be particularly important for attention prediction in mobile phones and tablets [16, 26]. The MICS implementation as well as the user study data, including the eye gaze ground truth, will be made available to the research community 2 .

Our work can be expanded in multiple directions. First, other complementary content features could be designed, capturing visual attractiveness, semantic concepts embed-ded in the text, or readability. Similarly, more complex behavior or mouse cursor movement features could be in-corporated into the model, including cursor patterns discov-ered automatically, e.g., as in [23]. These richer features can be naturally incorporated into the MICS model to fur-ther improve prediction performance. In turn, accurately inferring search attention prediction from observable user behavior data, obtained at Web scale, could enable numer-ous improvements to the user experience in search and other online settings.
 ACKNOWLEDGMENTS : This work was supported by the National Science Foundation grant IIS-1018321, the Na-tional Institutes of Health grant R01EB014266, the DARPA grant D11AP00269, and the Yahoo FREP program.
See http://ir.mathcs.emory.edu/software-data/ relative importance compared to all other elements within the viewport.
