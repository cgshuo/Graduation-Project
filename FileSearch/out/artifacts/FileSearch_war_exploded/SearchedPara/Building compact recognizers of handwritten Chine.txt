 ORIGINAL PAPER Yongqiang Wang  X  Qiang Huo Abstract In our previous work, a so-called precision constrained Gaussian model (PCGM) was proposed for character modeling to design compact recognizers of hand-written Chinese characters. A maximum likelihood training procedure was developed to estimate model parameters from training data. In this paper, we extend the above-mentioned work by using minimum classification error (MCE) training to improve recognition accuracy and using both split vec-tor quantization and scalar quantization techniques to further compress model parameters. Experimental results on a hand-written character recognition task with a vocabulary of 2,965 Kanji characters demonstrate that MCE-trained and com-pressed PCGM-based classifiers can achieve much higher recognition accuracies than their counterparts based on tra-ditional modified quadratic discriminant function (MQDF) when the footprint of the classifiers has to be made very small, e.g., less than 2MB.
 Keywords Handwriting recognition  X  MQDF  X  Covariance modeling  X  MCE  X  Model compression 1 Introduction A state-of-the-art handwriting recognition system for East Asian (EA) languages such as Chinese, Japanese, and Korean typicallyincludesacharacterclassifierconstructedbyusinga so-called modified quadratic discriminant function (MQDF) [ 13 ]. Researchers have been exploring different ways of reducing the memory requirement of MQDF-based classi-fiers, hoping to deploy it in mobile devices with limited memory (e.g., less than 2MB). A recent interesting exper-imental study was reported in [ 17 ], which was a straight-forward application of the relevant model compression techniques originally described in [ 8 , 9 , 11 ]. It was observed and confirmed by our own study that a trade-off on recogni-tion accuracy has to be made in order to construct a compact MQDF-based classifier.

In the past several years, we have being exploring other possible solutions for designing high-performance compact handwriting recognizers based on the concept of struc-tured covariance modeling , which was proposed originally to improve covariance modeling for Gaussian mixture con-tinuous density hidden Markov model (CDHMM) X  X ased we have studied two modeling techniques, namely semi-tied covariance (STC) Gaussian model [ 24 ] and precision constrained Gaussian model (PCGM) [ 25 , 26 ], respectively. In [ 24 ], detailed procedures are described for estimating the STC model parameters under both maximum likelihood (ML) and minimum classification error (MCE) criteria. In [ 25 , 26 ], only ML training is studied to estimate PCGM parameters from large amount of training data. Without using model parameter compression techniques, both STC-based and PCGM-based classifiers can achieve a better memory-accuracy trade-off than MQDF-based classifiers, yet PCGM-based approach seems more promising. In this study, we extend the above PCGM work by using MCE training to improve recognition accuracy and using both split vector quantization (VQ) and scalar quantization techniques [ 10 ] to compress model parameters and compare its performance with that of the MCE-trained and compressed classifiers based on MQDF. The preliminary results of this study have been published in [ 27 ]. The current paper is an extended version of the above report by including more detailed descriptions of relevant procedures, reporting additional experimental results and findings, and adding new figures and references to make the presentation more readable and accessible.

The rest of this paper is organized as follows. After a brief description of MQDF-and PCGM-based approaches in Sect. 2 , we present our MCE training and model compression procedures in Sects. 3 and 4 , respectively. This is followed by experimental results in Sect. 5 . Finally, we conclude the paper in Sect. 6 . 2 MQDF-and PCGM-based approaches Given a handwriting sample, we first extract a D 0 -dimen-sional raw feature vector z using the procedures described in [ 3  X  5 ]. z will then be transformed into a lower dimensional feature space using a D 0  X  D transformation matrix W , i.e., x = W T z , where W is obtained by linear discriminant anal-ysis (LDA) (e.g., [ 11 ]).

Let { C j | j = 1 ,..., M } be the set of M character classes, vectors, where x r is the r -th training sample and i r denotes the index of its true class label. Assume that feature vectors from the same character class C j can be modeled by a Gauss-ian distribution with a mean vector  X  j and a full covariance matrix  X  j . By setting the ( K + 1 ) -th to D -th eigenvalues of  X  j as a class-dependent constant be defined as follows [ 13 ]: g ( x ;  X  where  X   X  sponding eigenvector of  X  j , p jk = ( x  X   X  j ) T v jk ,  X  are two control parameters. In practice, setting  X  j as the average of ( K + 1 ) -th to D -th eigenvalues works well.
Under the same Gaussian assumption, PCGM [ 26 ] imposes a different constraint on each covariance matrix  X  Morespecifically,eachinversecovariancematrix P j =  X   X  1 j (a.k.a., precision matrix) is constrained to lie in a subspace spanned by a set of L basis symmetric matrices (referred to as  X  X rototypes X  hereinafter), i.e.,  X  ={ S which are shared by all the character classes. Consequently, the precision matrix P j can be written as follows: P where  X  jl  X  X  are class-dependent basis coefficients (referred to as  X  X oefficients X  hereinafter) and L is a control parame-ter. The discriminant function of PCGM can be derived from log-likelihood function as g ( x ;  X  ) = log det where  X  ={  X  In practice, the above discriminant function can be evaluated more efficiently as follows: g ( x ;  X  ) = 1 where c m
In recognition stage, an unknown feature vector x will be classified as the class with the maximum discriminant func-tion value as follows: x  X  C where g w ( x ) is the discriminant function of MQDF or PCGM, depending on which type of recognizer is used. 3 MCE training of PCGMs using Quickprop In [ 26 ], we have described ML training procedure for PCGM. In this section, we present an MCE training procedure to fur-ther improve the recognition accuracy of PCGM-based clas-sifiers, which is a straightforward application of the general MCE formulation described in e.g., [ 12 ].
Given the discriminant function of PCGM in Eq. ( 3 ) and the decision rule in Eq. ( 5 ), a misclassification measure for each training sample x r is defined first: d ( x where G with  X  being a control parameter. The PCGM parameters  X  can then be estimated by minimizing the following empirical loss function (  X  ; X ) (  X  ; X ) = 1 where  X  and  X  are two control parameters.

Among several options, we use Quickprop algorithm [ 6 ] to minimize the objective function in Eq. ( 8 ) because of its effectiveness for MCE training as demonstrated in other pattern recognition applications (e.g., [ 18 ]). Starting from ML-trained seed PCGM parameters, the following iterative Quickprop procedure adapted from [ 18 ] is used to fine-tune the mean vectors, {  X  j } , only. Updating other model param-eters such as {  X  jl } is also possible but nontrivial because the positive definite constraint P j 0 must be maintained. Fur-ther research is needed to find an effective updating method for these parameters.

Step 1: Let t = 1. Calculate the derivative of (  X  ; X ) w.r.t.
Step 2: Let t  X  t + 1. Calculate the approximate second
Step 3: Calculate update step differently depending on the
Step 4: If |  X  t  X  jd | &gt; limit  X |  X  t  X  1  X  jd | ,set
Step 5: Update  X  jd by  X  ( t + 1 ) jd  X   X  ( t ) jd +  X  t  X 
Step 6: Repeat Step 2 to Step 5 T  X  1 times.
Because the above procedure works in batch mode, it can be easily parallelized, for example, by using multiple com-puters to calculate the derivative in Step 1. 4 Model compression for PCGMs According to the discriminant function in Eq. ( 4 ), we need to store the following three sets of parameters to implement a PCGM-based recognizer:  X  the set of transformed mean vectors and constants  X  the set of coefficients {  X  jl } , in total L  X  M raw parame- X  the set of prototypes  X  , in total D ( D + 1 )  X  L / 2raw If a 4-byte floating point number is used to represent each raw parameters, the total memory requirement is about 4  X  (
D + L + 1 )  X  M + 4  X  D ( D + 1 )  X  L / 2 bytes, which translates into about 2.83MB for a typical system setup of D = 128 , L = 32 , M  X  3 , 000.

To compress the above PCGM parameters, we use the split-VQ technique [ 10 ], which is well known in speech rec-ognition and coding areas and was also used previously in For each transformed mean vector m j  X  R D ,wefirstuni-formlysplititinto Q 1 D Q 1 -dimensionalsubvectors(i.e., D Q rithm [ 14 ] is used to group the set of subvectors { m q 1 ,..., M } into 256 clusters, with Euclidean distance as the distortion measure, where m q j is the q -th subvector of m After clustering, each m q j is represented by the index of its nearest centroid. Each centroid (or codeword), a D Q 1 -dimen-sional vector, is represented by D Q 1 4-byte floating point numbers, while each index is represented as a single-byte unsigned integer. In total, we require Q 1  X  M bytes to store the indices and 4  X  D  X  256 bytes to store the codebook. Figure 1 illustrates this compression scheme.

Likewise, for the set of coefficients {  X  jl } , we first uni-formly split each vector  X  j = ( X  j 1 ,..., X  jL ) T into Q dimensional subvectors (i.e., L = Q 2  X  D Q 2 ) . Then, for each q  X  X  1 ,..., Q 2 } , group the set of subvectors {  X  q 1 ,..., M } into 256 clusters, where  X  q of
 X  store the coefficients.

For the compression of prototypes  X  ={ S 1 ,..., S L } , since each prototype S l is symmetric, it is only necessary to store the diagonal and upper-diagonal items. It is noted that the diagonal items reflect the auto-correlations of elements in the feature vector and have a dynamic range significantly different from that of upper-diagonal items. Therefore, we quantize all the upper-diagonal items using 8-bit scalar quan-tization, leaving the diagonal items intact. The compression scheme for the prototype is illustrated in Fig. 2 . This gives rise to a memory requirement of D ( D  X  1 )  X  L / 2 + 256 4 + D  X  L  X  4 bytes to store the prototypes.

It is also possible to quantize c j . However, since storing c costs only several kilobytes, we just skip the compression of c j in this study.

If all the model parameters are updated by MCE training, model compression should be made after the completion of MCE training. However, because we only update mean vec-tors in MCE training, we can combine MCE training and model compression as follows:
Step 1: Quantize coefficients {  X  jl } and prototypes  X  into
Step 2: Invoke MCE training to fine-tune the mean vectors
Step 3: Transform mean vectors, i.e., m j =  X  P j  X   X  j , and
Step 4: Quantize the transformed mean vectors { m j } by A similar model compression strategy is also applied to MQDF-based recognizers. We use an approach slightly dif-ferent from the one in [ 17 ]. Mean vectors are quantized as above by split-VQ algorithm with each subvector in D R 1 -dimensional space. Each eigenvector v jk is also uniformly split into R 2 D R 2 -dimensional subvectors. Then for each r , the set of subvectors { v r jk | j = 1 ,..., M ; k = 1 ,..., grouped into 256 clusters, where v r jk is the r -th subvector of the eigenvector v jk . We also compress eigenvalues  X  jk and constants  X  j using 8-bit scalar quantization.

Similarly, because we only update mean vectors in MCE training of MQDF-based classifiers, we can also combine MCE training and model compression as follows:
Step 1: Quantize class-dependent constant {  X  j } , eigen-
Step 2: Invoke MCE training to fine-tune the mean vectors
Step 3: Quantize the mean vectors {  X   X  j } by using the above-5 Experiments and results 5.1 Experimental setup In order to evaluate and compare the capability and limita-tion of different modeling approaches, we conduct a series of experiments on the task of the recognition of isolated online handwritten characters with a vocabulary of 2,965 level-1 Kanji characters in JIS standard. The popular Nakayosi and Kuchibue Japanese character databases [ 19 ] are used. The Nakayosi database consists of about 1.7 million character samples from 163 writers and the Kuchibue database con-tains about 1.4 million character samples from 120 writers. Samples written by 150 writers in Nakayosi database are selected to form the training set, while samples from 90 writ-ers in Kuchibue database are selected to form the testing set. All the remaining samples from both databases are used to form a development set for tuning control parameters. By this partition, there are 704,650 samples in the training set, 229,398 in the development set and 506,848 in the testing set, respectively. It is noted that due to the introduction of the development set, our partition is different from the one used in other X  X  work (e.g., [ 16 ]); therefore, the recognition accuracies reported in this paper are not directly compara-ble with those in e.g., [ 16 ]. However, the experimental results reported here serve the purpose well to compare our proposed PCGM approach with the conventional MQDF approach in a consistent manner.

As for feature extraction, a 512-dimensional raw feature vector z is first extracted from each handwriting sample by using the procedure described in [ 4 ]. Then, we use the LDA transformation matrix W estimated from the training data to transform z into a new feature vector x of dimension 128 (e.g., [ 11 ]). All of our experiments are conducted on these 128-dimensional feature vectors. To speed up recognition, a multiple-prototype-based pre-classifier (e.g., [ 11 ]) is used first to identify a short list of 50 candidates for each testing sample. After that, relatively more  X  X xpensive X  MQDF-or PCGM-based classifiers are called to choose the top candi-date from the short list. 5.2 Experimental results 5.2.1 Effects of parameter compression In the first set of experiments, we study the effectiveness of the proposed parameter compression scheme for PCGM-based classifiers. Using the ML training algorithm developed in [ 26 ], three classifiers, namely PCGM(32), PCGM(64), and PCGM(128) are constructed first, where PCGM( L ) means L prototypes are used. 1 After that, we do compression for either mean vectors or precision matrices only. Although it is pos-sible to use larger D Q 1 and D Q 2 to generate more compact models, we set D Q 1 = 1 and D Q 2 = 1 here, because the resultant models have been very compact already. Recogni-tion accuracies (in %) on testing set and the corresponding footprints (in MB) of three ML-trained PCGMs and their different compressed versions are summarized in Table 1 . In calculating the footprint, we assume that each uncom-pressed model parameter is represented by a 4-byte floating point number. It is observed that compressing mean vectors or precision matrices only incurs a slight degradation of rec-ognition accuracies.

As a comparison, we also carried out parameter compres-sion experiments for an ML-trained MQDF-based classifier with 20 eigenvectors per character class, which gives a rec-ognition accuracy very close to the best accuracy achiev-able by the ML-trained MQDF classifiers on the recognition task concerned [ 26 ]. The corresponding method described in Sect. 4 is used to compress either mean vectors or covari-ance matrices only. For mean vector compression, various settings of D R 1 are tried. For covariance matrix compres-sion, to compress the eigenvectors { v jk } , split-VQ algorithm with various settings of D R 2 is used. The eigenvalues {  X  and constants {  X  j } are compressed by 8-bit scalar quantiza-tion. Recognition accuracies (in %) and the corresponding footprints (in MB) of the ML-trained MQDF-based classi-fier and its compressed versions are summarized in Table 2 . From the results, it is clear that representing covariance matrices appropriately are critical for achieving a good memory-accuracy trade-off. To construct MQDF-based classifiers with similar footprints as our PCGM-based clas-sifiers, a setting of D R 2  X  4 has to be used. This leads to a significant degradation in recognition accuracy. For mean vector compression, only a minor drop in recognition accu-racy is observed for relatively aggressive compression, e.g., D 5.2.2 Effects of combined MCE training and model In the second set of experiments, we study the effectiveness of combined MCE training and model compression. The fol-lowing two options are compared for building PCGM-based classifiers:  X  Option 1 : Using ML-trained PCGMs as seed models, do  X  Option 2 : Using ML-trained and precision-matrix-com-In the first experiment, we take ML-trained PCGM(128) as seed models. Then do MCE training to fine-tune the mean vectors, followed by mean vector and precision matrix compression using the setting of D Q 1 = 1 and D Q 2 = 1. In the second experiment, the precision matrices of ML-trained PCGM(128) model are first compressed using the setting of D Q 2 = 1. Then the mean vectors of this preci-sion compressed PCGM are further fine-tuned by MCE train-ing, followed by mean vector compression using the setting of D Q 1 = 1. To save computation during the MCE train-ing process, we actually used 100 most competing classes, instead of M  X  1 classes, for each training sample to define G is as follows:  X  = 0 . 1 , X  = 0 , X  = 0 . 2 , X  0 = 0 . 1 , Table 3 summarizes a comparison of recognition accuracies (in %) of different systems constructed by using the above two options of combined MCE training and model compres-sion. It is observed that the second option achieves slightly better recognition accuracy than that of the first one (98.59% vs. 98.52%). Therefore, the second approach is used in sub-sequent experiments.
Next, we repeat experiments by using the second approach for other two PCGM-based classifiers. The settings for model compression and MCE training are the same as what we used in the previous experiments. No significant drops in recog-nition accuracies are observed after mean vector/precision matrix compression, but the footprints are reduced dramati-cally. It is also observed that MCE training brings relative error reductions of 17.9, 13.5, and 8.0% for PCGM(32), PCGM(64), and PCGM(128), respectively, compared with ML-trained and precision-matrix-compressed PCGMs. The power of MCE training is clearly demonstrated. These results are summarized in the second, third, and fourth rows of Table 4 .

For comparison, we also use a similar procedure to build an MQDF-based classifier. Again, 20 eigenvectors are retained per character class. To construct an MQDF-based classifier with a footprint similar to our PCGM-based clas-sifiers, a setting of D R 2 = 4 has to be used in compressing the covariance matrices. This leads to a significant degrada-tion in recognition accuracy (error rate increases about 30%). Starting from this ML-trained and covariance compressed MQDF-based classifier, MCE training [ 15 ] is conducted to fine-tune the mean vectors. MCE training achieves a rela-tive error reduction of 10.7%. Finally, the mean vectors are compressed using split-VQ with the setting of D R 1 = 2. A minor drop in recognition accuracy (from 98.25 to 98.20%) is observed in this step. The corresponding experimental results are summarized in the last row of Table 4 . 5.2.3 Discussions It is clear from the above experimental results that our PCGM-based classifiers can be made very compact yet with-outincurringsignificantdegradationofrecognitionaccuracy. Our PCGM-based approach can achieve a much better accuracy-memory trade-off than MQDF-based approach. As pointed out in [ 26 ], the main disadvantage of PCGM approach is that PCGM-based classifiers require more com-putations than that of MQDF. For example, running on a PC with a 3GHz Pentium-4 CPU and using a short list of 50 candidates, the average recognition time (in terms of user CPU time) of a compact PCGM-based recognizer (PCGM(32) with 0.88MB memory footprint and 98.35% recognition accuracy) is 2.45ms per handwriting sample, while that of an MQDF-based recognizer (MQDF(20) with 2.33MB memory footprint and 98.20% recognition accu-racy)isabout1.50mspersample.AlthoughthePCGM-based recognizer is much slower than the MQDF-based recognizer, it is fast enough for the purpose of designing practical hand-writing recognizers on most of today X  X  computational plat-forms.

As for training, most computation is consumed in cal-culating the derivatives of the empirical loss function. Fortunately, for batch-mode discriminative training algo-rithm like Quickprop, the calculation of the derivatives can be highly parallelized since the empirical loss func-tion is the sum of loss functions for each individual sam-ple. Our experience is that with the access to the cluster computing infrastructure, the discriminative training using parallelized Quickprop can be speeded up many times. For example, the training time can be reduced from one day run-ning on a single CPU to less than one hour by using 40 CPUs in our experiments.
 6 Conclusion Given the above results, we conclude that PCGM-based approach offers greater flexibility than MQDF-based approach in striking for a better memory-accuracy trade-off and therefore could be a good solution to designing practi-cal compact handwriting recognition systems for East Asian languages such as Chinese, Japanese, and Korean.
 References
