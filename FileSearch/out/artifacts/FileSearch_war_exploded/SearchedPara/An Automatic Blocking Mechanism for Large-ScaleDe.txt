 De-duplication X  X dentification of distinct records referring to the same real-world entity X  X s a well-known challenge in data integra-tion. Since very large datasets prohibit the comparison of every pair of records, blocking has been identified as a technique of divid-ing the dataset for pairwise comparisons, thereby trading off recall of identified duplicates for efficiency . Traditional de-duplication tasks, while challenging, typically involved a fixed schema such as Census data or medical records. However, with the presence of large, diverse sets of structured data on the web and the need to organize it effectively on content portals, de-duplication sys-tems need to scale in a new dimension to handle a large number of schemas, tasks and data sets, while handling ever larger problem sizes. In addition, when working in a map-reduce framework it is important that canopy formation be implemented as a hash func-tion , making the canopy design problem more challenging. We present CBLOCK, a system that addresses these challenges.
CBLOCK learns hash functions automatically from attribute do-mains and a labeled dataset consisting of duplicates. Subsequently, CBLOCK expresses blocking functions using a hierarchical tree structure composed of atomic hash functions. The application may guide the automated blocking process based on architectural con-straints, such as by specifying a maximum size of each block (based on memory requirements), impose disjointness of blocks (in a grid environment), or specify a particular objective function trading off recall for efficiency. As a post-processing step to automatically generated blocks, CBLOCK rolls-up smaller blocks to increase re-call. We present experimental results on two large-scale de-duplication datasets from Yahoo! X  X onsisting of over 140K movies and 40K restaurants respectively X  X nd demonstrate the utility of CBLOCK. H.0 [ Information Systems ]: General Algorithms, Performance de-duplication, blocking, canopy formation
Integrating data from multiple sources containing overlapping information invariably leads to duplicates in the data, arising due to different sources representing the same entities (or facts) in slightly different ways; e.g., one source says  X  X eorge Timothy Clooney X  and another says  X  X . Clooney X . The problem of identifying dif-ferent records referring to the same real-world entities is known as de-duplication 1 . De-duplication has been identified as an important problem in data integration, and has enjoyed significant research interest, e.g. [11, 12, 24, 29].

Conceptually, de-duplication may be performed by considering each pair of records, and applying some matching function [19, 27, 12] to compute a similarity score, then determining duplicate sets of records based on clustering similar pairs. However, com-paring all pairs of records to be de-duplicated is prohibitively ex-pensive in commercial or web applications that require matching data sets with millions of records (e.g., persons, business listings, etc). Blocking or canopy-formation (e.g., [4, 8, 14, 18, 16, 28, 20, 7, 21]) has been identified as a standard technique for scaling de-duplication: The basic idea is to find a set of (possibly overlapping) subsets of the entire dataset (called blocks ), and then compute sim-ilarity scores only for pairs of entities appearing in the same block. We use the term  X  X locking function X  to refer to any function that maps entities to block numbers, usually based on the value of one or more attributes. One example of a blocking function would be the value of the  X  X hone number X  attribute, or the first seven digits of the same, etc. In an ideal situation, all (or most) of the duplicates would appear together in at least one block.

As a result, a good blocking function must be designed for each large-scale matching task . We are seeking to build a scalable sys-tem for de-duplication of web data. The system will be used for a wide variety of de-duplication tasks, and must support agility , the ability to rapidly develop new de-duplication applications. Ac-cordingly, an important part of developing this system is effec-tive, automatic construction of blocking functions. Like [26], de-duplication tasks in our system execute in a map-reduce framework like Hadoop. In this setting, computation is broken into rounds con-sisting of a map phase in which a set of keys is generated by which work is split over a potentially large number of compute nodes and a reduce phase in which partial results from each compute node are combined. A natural approach for de-duplication is to use the map
De-duplication is also known by many other names such as refer-ence reconciliation, record linkage, and entity resolution. phase to execute the blocking function, allowing match scores to be computed in parallel on each mapper.

In order to design appropriate blocking functions for our setting, we face four important challenges. First, a premium is placed on minimizing the number of rounds of computation in a map-reduce setting, since each round involves significant scheduling and co-ordination overheads. Second, data in our system comes from a variety of feeds, and is often noisy. In particular, attributes may be only partially populated, leading to asymmetric block sizes if these attributes are used for blocking. Third, matching is executed in par-allel, meaning that a premium is placed on minimizing the size of the largest block without exceeding the maximum number of com-pute nodes available. Fourth, the complexity of the de-duplication process can be significantly reduced if every object is given only a single hash value for mapping; which we refer to as the disjoint blocking condition.

We present CBLOCK, a system that automatically creates canopies based on the information specified by the application. We now de-scribe the approach taken in CBLOCK to address the above chal-lenges. We introduce a conditional tree of blocking functions, the BlkTree . In this tree, blocks with large expected size are explicitly mapped to a child blocking function, making each path in the tree equivalent to a conjunctive blocking function applied to a subset of the data. The introduction of the BlkTree allows for an expres-sive blocking function, which allows us to effectively block even skewed data, such as attributes with many null values.

Second, to handle the situation in which the number of blocks exceeds the number of compute nodes, we introduce a roll-up step for the BlkTree to efficiently reduce the number of compute nodes without excessively increasing complexity of the hash function. Third, we optimize for the best blocking function while keeping the size of the largest block within a constrained size. Since the over-all latency of the parallel computation corresponds to the slowest node, this is a natural optimization goal, but is not addressed by existing techniques. As an aside, we note that our system can also be used for other applications that require a similar capability as blocking: (a) In a binary classifier with many features, CBLOCK may be used to pick a small set of features that most effectively captures the classification; (b) We can use CBLOCK to determine which sets of values from two relations may contribute most to join results in a distributed join solution such as [22].

The flow of data through CBLOCK is illustrated in Figure 1. As input to the system, shown on the left, is a set of training exam-ples consisting of true-positive match pairs shown at the top, and a set of configuration parameters shown a the bottom including size constraints, disjointness conditions and any tuning of the cost ob-jective. These inputs feed into the CBLOCK system, shown in the middle block, that designs a blocking configuration. This config-uration is then passed to the runtime system for execution of the blocking as the first phase of the de-duplication algorithm.
Our paper makes the following contributions, addressing the re-quirements of automatic blocking configuration for web-scale de-duplication:  X  In order to decrease the number of rounds (disjuncts), we ar- X  Section 5 introduces the roll-up problem of merging small canopies  X  Section 6 studies  X  X rill-down X  problem, i.e., given a domain of  X  For most of the paper we focus our attention on disjoint block- X  CBLOCK is fully implemented along with all the functional-Related work is described in Section 2. Due to space constraints, formal proofs for all technical results are omitted from the paper.
To the best of our knowledge, ours is the first work to: (1) Present techniques on finding blocking functions by explicitly trading-off recall for efficiency, and in a more expressive tree-based structure than flat conjunctive structures of past work; (2) Formally introduce and study the problem of rollup as an important post-processing step to assemble small canopies and increase recall; (3) Provide automatic solutions to the drill-down problem as a way of boot-strapping blocking with no manual effort, or augmenting manually-generated hash functions; (4) Present an automatic blocking sys-tem for de-duplication in a distributed setting that is applied to two large commercial datasets from a search engine. Very few pieces of previous work consider blocking based on labeled training data, while there is a much larger body of work on hand-tuned blocking techniques using similarity functions. We start by describing the relationship of our work with blocking based on labeled data (Sec-tion 2.1), followed by blocking without labeled data (Section 2.2).
Two recent papers [7, 21] presented approaches to constructing a blocking function using a labeled dataset of positive and nega-tive examples. Roughly speaking, both papers learn conjunctive rules (and disjunctions of conjunctive rules) to maximize recall. [7] attempts to maximize the number of positive minus negative ex-amples covered, effectively using negative examples as a proxy for minimizing the size. [21] uses only positive examples, but does not explicitly incorporate any size restriction. Below we give a detailed comparison with these past approaches:  X  We present BlkTrees, a more expressive language for express- X  Minimizing negative training examples covered by a blocking  X  Minimizing negative training examples does not match the cost  X  We are the first to introduce and solve the rollup and drill down [14] introduced the notion of blocking (called  X  X erge-purge X ) by constructing a key for each record, sorting based on the key, and then performing matching and merging in a sliding window. [14] (and other variants [18, 16]) do not consider automatic generation of optimal blocking functions in a distributed environment, based on training data.

SWOOSH [5] is a recently developed generic entity resolution system from Stanford. Their specific paper on blocking [28] fo-cuses on  X  X nter-block communication X , by propagating matched records to other blocks. Once again, automatic generation of block-ing functions is not the subject of [28]. Further, D-Swoosh [6] (and other similar work [25, 22]), their distributed framework for entity resolution focus on distributing pairwise comparisons across mul-tiple processors, as opposed to our focus of partitioning the data to reduce the number of total pairwise comparisons.

Reference [20] presented techniques for generating non-disjoint canopies based on distance measures such as jaccard similarity of tokens. After choosing a distance function, they pick records as canopy centers, and add to each canopy all records that are within some distance based on the distance measure. The algo-rithms from [20] cannot be directly scaled to a distributed envi-ronment. A similar approach of generating (non-disjoint) canopies by clustering based on any distance measure was also proposed in [23]. Some other work [9] considers blocking based on bi-grams of string attributes, followed by creation of inverted lists for each bigram. Another recent piece of work [17] considered transforming the data into a euclidean space. While the above ap-proaches weren X  X  designed specifically for a distributed environ-ment, recently [26] studied the problem of performing approxi-mate set similarity joins using a map-reduce framework. Their work can be used for blocking when records are compared for du-plicates based on set similarity functions. Also, a recent system, MAHOUT [3], described an implementation of canopy clustering in a map-reduce framework. Finally, [4] performed a comparative study of blocking strategies from [14, 20, 9, 16].

In general, the approaches described above rely on the knowl-edge of specific similarity/distance functions. Furthermore, they necessarily generate non-disjoint canopies, whereas one primary goal of our work was to consider disjoint canopies as an important choice for distributed de-duplication and obtain non-disjointness as multiple rounds of disjoint sets of canopies. Finally, none of this past work considers the rollup and drill-down problems.
We use U to denote the set of entities (i.e., records) to be de-duplicated. Dividing U for pairwise comparisons is known as block-ing (or canopy formation ). The divided pieces are called blocks (or canopies ). We use C to denote the set of canopies, and C note the individual canopies. Formally, given a universe U , a set of canopies is given by a finite collect C = { C 1 ,...,C k } , C and S i C i = U . A specific method to construct C from U is called a blocking function . We start by restricting our attention to block-ing functions that create a disjoint set of canopies (i.e., if i 6 = j , then ( C i  X  C j ) =  X  ) and then extend our results for non-disjoint sets of canopies (Section 7). Intuitively, a good blocking function must satisfy two desirable properties. First, canopy formation in-creases the efficiency of de-duplication by eliminating the need for performing pairwise comparisons between all pairs of entities in U . Second, the quality of de-duplication (i.e., recall of identified duplicates) must not be significantly reduced by performing fewer comparisons. Therefore, our goal is to find a set of canopies such that most duplicates in U fall within some canopy. We shall use T +  X  U  X  U to denote a training dataset consisting of labeled du-plicates in U , over which recall of blocking functions is measured. We shall construct blocking functions using a space H of hash func-tions that partition U based on attributes of the entities in U ; each hash function assigns one hash value for each entity. For exam-ple, one hash function partitions U based on the first character of the titles of movies. A conjunction of hash functions h 1 equivalent to creating a single hash value by concatenating the hash values obtained by each h i , effectively creating partitions (equiva-lence classes) where values of each of the hash functions matches. Typically H is generated manually based on domain knowledge, and we shall present techniques to construct blocking functions us-ing any H . In addition, we shall also present techniques to automat-ically identify optimal hash functions for each attribute (Section 6).
While CBLOCK can be configured with any cost model for opti-mizing canopy formation, we use latency as the default cost model in our discussion. 2 The latency of any canopy formation is given by the total time it takes to perform all pairwise comparisons in each canopy.

In a grid environment (such as our de-duplication system im-plemented using map-reduce), pairwise comparisons in the set of canopies are performed in parallel. Given a canopy formation C = { C 1 ,...,C k } , with number of entities in canopy C i denoted by s , the total number of pairwise comparisons being performed is P use the cost model cost ( C ) = max i s i : Clearly, in a truly elas-tic grid with a potentially infinite supply of machines, pairwise comparisons for each canopy are performed on a separate machine. Therefore, the latency is given by the largest canopy, justifying our cost model of using max i s i .

When the number of machines on the grid are limited (and specif-ically, when there are fewer machines than canopies), we are faced with the problem of assigning canopies to machines. The following theorem shows that this assignment is NP-hard in general, based on a direct reduction from a scheduling problem. However, we
All our algorithms and complexity results carry over for any  X  X onotonic cost function X , i.e., cost ( C )  X  cost ( C  X  C  X  X  ,  X  C 0  X  X  0 such that C  X  C 0 . also show that the latency using the largest canopy gives an upper bound on the best possible assignment.

T HEOREM 3.1. Given a set M = { M 1 ,...,M m } of m ma-chines, a canopy formation C = { C 1 ,...,C k } over N entities, m &lt; k , any assignment A : C  X  M of canopies to machines has a cost given by cost A ( C ) = max m j =1 ( P C have: 1. It is NP-hard to find an assignment that minimizes cost 2.  X  A : max k i =1 | C i | 2  X  cost A ( C )  X  (1 + k m ) max Based on the theorem above, henceforth, we focus on the problem of finding best canopies that satisfy the constraint of max for some given S .
This section addresses the problem of constructing disjoint block-ing functions using a labeled dataset of positive examples. Af-ter formally defining the problem (Section 4.1), we introduce a tree-structured language for expressing blocking functions (Sec-tion 4.2). We then show that the general problem of finding an optimal blocking function is NP-hard (Section 4.3), and finally we present a greedy heuristic algorithm (Section 4.4) to find an approx-imate blocking function.
We formally define the problem of creating canopies given la-beled data consisting of examples of duplicates (positive pairs). Recall the two conflicting goals of canopy formation: The more divisive a set of canopies is, the more likely it is to miss out on true duplicates. We formulate an optimization problem that trades off the two objectives of canopy formation, by associating a hard con-straint on the maximum size of each canopy and maximizing the number of covered positive examples (recall) subject to this size constraint.
 D EFINITION 4.1 (B LOCKING P ROBLEM ). Given a labeled set T + of positive examples, a space H of hash functions, a size bound S on every canopy, and a size function size () that returns the size of a canopy obtained by applying any conjunction of hash functions in H on any input dataset I , construct a disjoint blocking function B that partitions any input I into a set C of disjoint canopies of size at most S , while maximizing the number of pairs from T + within canopies, i.e., maximizing: recall = |{ ( r 1 ,r 2 .
 We make a few important observations about our problem defini-tion. (1) As a reminder, we start by considering only disjoint block-ing, and extend to non-disjoint blocking in Section 7. The next sec-tion describes a language to represent disjoint blocking functions ( B ), and subsequently we give algorithms for finding B . (2) We assume that there is a known size estimation function. In practice, some previous work on blocking [7] has used negative examples as an indirect way of incorporating size restrictions. Alternatively, previous work on estimating the cardinality of selection queries us-ing histograms (refer [15]) can be used to estimate canopy sizes, as we shall see each canopy is obtained as a conjunction of hash functions. Of course, if the entire dataset were available during the construction of blocking predicates, it could be used for size computation. (In particular, exact size computation for the block-ing technique we propose can be done in a few scans. Also, we
Figure 2: Example of a tree-structured disjoint blocking function. shall see that our technique can be adaptively applied even in case of inaccurate size estimates.) (3) For this section we assume the existence of a space H of hash functions. Most previous work has assumed the manual creation of such atomic hash functions. We also present in Section 6 an automated method of enumerating hash functions for each attribute. (4) Finally, we assume the positive ex-amples T + are known; we describe the construction of this dataset in the experiments section (Section 8).
This section presents a generic language for expressing disjoint blocking functions. We introduce a hierarchical blocking tree (called cal fashion by successively applying atomic hash functions from a known class H . Formally:
D EFINITION 4.2. A BlkTree B = ( N,E,h ) is composed of a tree with nodes N and edges E , and h : N  X  H maps each node in the tree to a particular hash partitioning function from H . Intuitively, each leaf node of the tree corresponds to a canopy. The BlkTree is built using the inputs described in Definition 4.1, namely the training data, a known space of atomic hash functions H , and canopy-size estimates. Each node n  X  N in the tree corresponds to a set of entities from the entire set obtained by applying the hash functions from the root down to n . Each node n (with a size es-timate exceeding the allowed maximum) then applies a particular partitioning hash function to create disjoint partitions of the set of entities corresponding to n .

At run-time, each entity is run through the BlkTree, and directed to the machine in the cluster based on the leaf node. (Note that in a distributed environment, the entire data itself is initially partitioned across multiple machines; therefore, the BlkTree is stored on every machine in order to redistribute the data based on the canopies.) Note that in practice the total number of large canopies created by any hash function on any node is a constant, for instance due to NULL values in the data, or a common default value for an at-tribute. Therefore, the size of the constructed BlkTree in terms of the number of nodes is small, so that the BlkTree fits in memory, and applying the BlkTree to an entity is efficient.

E XAMPLE 4.3. Figure 2 shows an example BlkTree for movie data with the root partitioning the movies lexicographically based on the title. This partition results in two large canopies X  X he node corresponding to NULL titles, and the node corresponding to ti-tles that start with  X  X  X  (assume all titles have been capitalized in advance). In the NULL canopy a partition based on the release-year of the movie is performed, while the movies starting with  X  X  X  are partitioned by the name of the movie X  X  director. All leaf nodes in the resulting tree satisfy the maximum canopy-size requirement, and hence no further partitioning is performed. 1: 2:
We note that BlkTrees are a very expressive language to describe disjoint blocking functions. In particular, the following natural lan-guages are obtained as restrictions of BlkTrees: 1. Single hashes: Clearly single hash functions are equivalent to 2. Conjunctive functions (chains): Conjunctions of hash func-3. Chain-tree: Chain-trees are an extension of conjunctions where In our experiments, we implement algorithms for BlkTrees and all the restricted languages above and compare them in terms of recall, to observe a significantly higher recall using BlkTrees.
Next we demonstrate that the general problem of finding the op-timal BlkTree is NP-hard, which is proced by a reduction from the Set Cover problem [13], and subsequently present a heuristic greedy algorithm.

L EMMA 4.4 (B LK T REE INTRACTABILITY ). Given a training set T + with positive examples, a space H of hash functions, and a bound S on the maximum size of any canopy, assuming P 6 = NP , there does not exist any polynomial-time (in T + , H ) algorithm to find the optimal BlkTree.
We propose a simple heuristic for constructing the BlkTree de-scribed in Algorithm 1. The general scheme of the algorithm is to locally pick the best hash function at every node in the tree, if the size (estimate) of the number of entities in this node is over the allowed maximum S . (If a particular hash function generates many large canopies, it is ignored, in order to maintain a small Blk-Tree. However, as described before, the number of large canopies is typically small; in our experiments over 140K movie entities, no hash function created more than a few large canopies.) The best hash function for a node is picked greedily by counting for all hash functions h  X  H , the number of duplicates that get elimi-nated on choosing the hash function h . The hash function that min-imizes the number of eliminated duplicates is chosen. We describe three ways of counting the number of examples eliminated (func-tion elim-count in Algorithm 1). Suppose a node n has P n pairs, and application of h eliminates P h duplicates and creates canopies C 1 ,...,C k exceeding size S (among other canopies that are smaller than S ). If the number of positive pairs in C P ( C i ) , then the three ways of counting the drop in the number of positive pairs are as follows: Finally we note that an important feature of constructing the Blk-Tree is that it can be naturally adapted at runtime based on the ac-tual canopy sizes, such as when the canopy-size estimates turned out to be inaccurate, or when available memory has reduced. Sup-pose while construction of the BlkTree a canopy-size bound of S (say, 5000 entities) was imposed, we may choose to construct the BlkTree based on a maximum canopy-size of a fraction of S (say, 1000 entities). Effectively, we will create a longer tree than neces-sary, and this  X  X xtra X  portion of the tree may be used if any canopy needs to be split further based on the reasons described above. Con-versely, if actual canopy sizes turn out to be smaller than expected, we may choose to run through only a smaller part of the tree.
In this section, we introduce the problem of rolling up small canopies. The primary motivation for studying this problem is that a blocking function may unnecessarily have to create many small canopies, in order to make some of the larger canopies fit the required size bound. Therefore, as a post processing step, we can take the result of any blocking function, and combine multiple small canopies maintaining the size requirement yet increasing the overall recall.

We are given a set of canopies C = { C 1 ,C 2 ,...,C m } , where each canopy C i has size (much) less than our canopy size limit S . We are also given a set of pairs of matching records T D = { D 1 ,D 2 ,...,D ` } such that  X  Disjointness Constraint :  X  i,j , i 6 = j , D i  X  D j =  X   X  Roll Up Constraint :  X  i ,  X  i 1 ,i 2 ,... , such that, D Figure 3: Rollup applied on canopies (leaf-nodes) generated in Fig. 2. 1: 2: 3:  X  Maximum Size Constraint :  X  i, | D i | X  S  X  Maximize Recall : minimize the number of pairs of matching Note that the rollup problem can be applied on any set of canopies generated using any previous blocking function. In particular, it can be applied on the BlkTree blocking function generated in Section 4. Each leaf of the BlkTree corresponds to a canopy, and by applying rollup, some leaves of the BlkTree get merged so as to maintain the size requirement but increase recall. Figure 3 shows an example blocking function obtained by performing rollup on the BlkTree in Figure 2. Note that although the resulting blocking function isn X  X  a tree, the resulting DAG can still be used for distributed canopy formation: Each entity starts at the root and traverses all the way down through the directed edges to a (possibly rolled-up) leaf node, which corresponds to a canopy.

We start by observing that the roll-up problem is intractable, which is proved by a reduction from the 0 / 1 -Knapsack problem:
L EMMA 5.1 (NP-COMPLETENESS ). The rollup problem de-scribed above is NP-complete.

Next we propose a greedy heuristic for the rollup problem that is inspired by Dantzig X  X  2-approximation algorithm [10] for the knap-sack problem. Conceptually, our algorithm (Algorithm 2) starts with the initial set of canopies, and progresses in steps. In each step, the algorithm finds the pair of sets D 1 ,D 2 that together have less than S records, and maximize the following quantity: benefit ( D 1 ,D 2 ) is the number of matching pairs ( r i such that r i 1  X  D 1 and r i 2  X  D 2 . Intuitively, in each step we pick the canopy that has the smallest size but also puts a large number of matching pairs in the same canopy.

Algorithm 2 can be efficiently implemented in time linear in the number of matching pairs ( |T + | ) and quadratic in the number of input canopies ( |C| ). Initially, we compute for each canopy D  X  X  , one merge candidate . This is a canopy D 0 such that | D and | D | + | D 0 | X  S such that benefit ( D,D 0 ) is maximum. This step takes O ( |T + | X |C| 2 ) time. In each step, we find the canopy D whose merge candidate has the maximum benefit to size ratio; we then merge D with its merge candidate. The new merge candidate for a canopy other than D and D 0 is either D  X  D 0 or its old merge candidate  X  this step takes O (1) time for each canopy. The new merge candidate for D  X  D 0 can be computed in O ( |T + | X |C| ) time by considering all the other canopies and the positive examples. Since the algorithm terminates in at most |C| steps, our algorithm has O ( |T + | X |C| 2 ) time complexity.
In Section 4 we assumed a pre-existing and manually-generated space of hash functions (as is done in most previous work). Next we propose automatic (only using an attribute X  X  domain and labeled dataset) techniques for generating hash functions. Automatically constructed hash functions may be used to bootstrap the blocking methods, eliminating the need for a significant upfront manual ef-fort. Moreover, even in the presence of an existing space of man-ually constructed hash functions, we can augment the space with (better) automatically generated hash functions.

We introduce the  X  X rill-down X  problem for a single attribute. Our goal is to optimally divide a single-attribute X  X  domain into disjoint sets so as to cover as many duplicate pairs as possible, but ensuring that the cost associated with any set is below a required threshold. First we formally define the partitioning of an attribute X  X  domain into disjoint, covering, contiguous subsets (called a DCC partition ), then define the problem of finding an optimal DCC partition.
D EFINITION 6.1 (DCC P ARTITION ). Given a domain D with total ordering  X  , least element  X  start  X  and greatest element  X  end  X  we say that a set I is a DCC partition of D if  X  I  X  X  : I  X  D and all of the following hold:  X  Disjoint: I 1 ,I 2  X  X  ,I 1 6 = I 2  X  I 1  X  I 2 =  X   X  Contiguous subset: Every I  X  X  is of the form [ I 1 ,I 2  X  Covering: D A = S I  X  X  I 2 Intuitively, a DCC partition completely divides D by  X  X iling X  the entire domain. Also, note that the total ordering doesn X  X  need to cor-respond to the  X  X atural ordering X  such as lexicographic for strings or  X  &lt;  X  ordering for numeric. For instance, may choose to order di-rector names by their last name and find a hash function, then also order them by first name and find another hash function. Next we formally define the drill down problem.

D EFINITION 6.2 (D RILL D OWN P ROBLEM ). Consider a sin-gle attribute A with an ordered domain D A ,  X  ,start,end , a set of n duplication pairs T + = { ( a 1 1 ,a 2 1 ) ,..., ( a 1 n D
A ,a 1 i  X  a 2 i , and any monotonic black-box cost function I  X  D  X  R , and a maximum cost bound S on any partition. Our goal is to find a DCC partition I of D A such that: 1.  X  I  X  X  : cost ( I )  X  S
The least and greatest element may be part of D in some cases (e.g., all 10-digit phone numbers) and not part of D in others (e.g.,  X  X  X  and +  X  for real numbers).
I,I 0  X  D A : I  X  I 0  X  cost ( I )  X  cost ( I 0 ) . Note that, in prac-tice, for uniformly distributed data the cost function may simply bound the total size of the interval. But for skewed data, the size of the interval depends on the density of the data; therefore, we allow any arbitrary cost function. Figure 4: Example of drilling-down on release-year of movies. 2. Let cov ( I , T + ) be the number of duplicates covered: cov ( I , T
E XAMPLE 6.3. Figure 4 gives an example of a hash function that may be obtained using the drill down problem. This hash func-tion may be added to the existing space of hash functions in con-sideration by a blocking function construction algorithm such as Algorithm 1.

Next we provide an optimal polynomial-time algorithm for the drill down problem based on dynamic programming. We use two core ideas in the algorithm described next. First, suppose we are finding the first partition in the given domain, the only  X  X nteresting endpoints X  of a partition must be either a value at which a duplicate entity lies, or must be due to the boundary caused by the cost bound. Intuitively, we discretize the domain, and now only need to look at a finite number of endpoints in constructing the optimal partition; the space of possible DCC partitions still remains exponential. This observation is formalized below.

L EMMA 6.4 (I NTERESTING E NDPOINTS ). Given a domain D ,  X  , with least and greatest elements start , end , with duplicate pairs T S , consider finding the first partition [ start,X ) or [ start,X ] (or open interval on start if start 6 X  D ) for the drill down problem. Let Y end be the greatest value such that cost ([ start,Y ])  X  S , then there is an optimal drill down solution with X  X  ( { Y }  X  { a i | a The second observation is the optimal substructure property ex-ploited by our dynamic programming algorithm. Given a domain D,  X  ,start,end over which we want to solve the drill down prob-lem, the optimal solution for a sub-domain D s ,  X  ,start start 0 start , with the same cost function and cost bound is iden-tical irrespective of the partitions chosen for D  X  D s , i.e., from start to start 0 . This property allows us to memoize the solutions for all sub-domains of known interesting end-points, namely from a to end , for every a j i . We can then find an optimal solution to the entire domain by recursively considering sub-domains, as formal-ized below.
 L EMMA 6.5 (O PTIMAL S UBSTRUCTURE ). Given a domain D,  X  ,start,end with duplicate pairs T + = { ( a 1 1 ,a 2 cost function I , cost bound S , let Y be greatest value satisfying cost bound (as defined in Lemma 6.4). Let V ( I ) be the total number of violations in the optimal solution for the subset of T + with each endpoint in I . Then, V ( D ) can be recursively computed as: V ([ a,end ]) = min 1: 2: 4: 5: 7: 8: 9: Algorithm 3: Sketch of the dynamic programming algorithm with mem-where B ([ a,P ]) is the number of duplicate pairs broken due to the interval B ([ a,P ]) ; i.e., B ([ a,P ]) = |{ i | a a The above lemma provides a natural dynamic programming algo-rithm (described in Algorithm 3), where we recursively solve the drill down problem for sub-domains, and memoize these solutions for future recursive calls in M ; initially, no solution is memoized. Algorithm 3 returns the total number of violated duplicate pairs but also tracks the specific endpoints. It can be seen easily that this algorithm runs in near-linear time and space based on the obser-vation that the total number of different recursive calls is at most O ( n ) : O ( n ) corresponding to all possible endpoints of duplicate pairs, and another O ( n ) corresponding to each maximum Y -value from Lemma 6.5 for each endpoint.

So far we have considered the drill down problem under the dis-jointness condition (recall Definition 6.1). We finally note that the drill down problem is trivial if we were allowed a non-disjoint set of intervals: We simply look at each duplicate pair ( a 1 i ,a ally and create an interval I i = [ a 1 i ,a 2 i ] if and only if cost ( I
In this section we consider the construction of a set of canopies that don X  X  need to be disjoint. The first thing to note is that we need to revise our cost model from Section 3.2. We note that a cost func-tion that only penalizes the size of the largest canopy doesn X  X  suf-fice any longer: Given a set U of entities, we can create canopies, with one canopy for each pair of entities in U . Note that this set of canopies has a maximum canopy size of 2 , and a perfect recall of 1 . However, constructing a canopy for each pair is clearly prohibitive, as it incurs a large communication cost, i.e., each en-tity needs to be transferred to machines corresponding to O ( | U | ) canopies. Therefore, we introduce a cost metric that minimizes the combination of communication and computation cost. The cost of a set C = { C 1 ,...,C m } is given by:
A similar expression for B ([ a,P )) + V ([ P,end ]) , which is omit-ted. We have a similar formula for every combination of open and closed interval, i.e., [ a,end ) , ( a,end ) , ( a,end ] . 1: 2: 3: Algorithm 4: Generic algorithm for performing non-disjoint canopy formation as multiple rounds of disjoint canopy formation. The computation cost, as before, is approximated by the computa-tion for the largest canopy, where a complete pairwise comparison is performed. The communication is given by the total size of all canopies put together, which is roughly the number of entities that need to be transferred to different machines.

We address the problem of finding non-disjoint canopies as find-ing sets of canopies C 1 , C 2 ,... , where each C i is a disjoint set of canopies. In a distributed environment, each C can be performed in one map-reduce round. (Alternatively, if non-disjoint canopies are inherently supported, we may simply construct a single set of canopies as  X  C = S i C i .) When treating non-disjoint canopies as multiple rounds of disjoint canopies, once we bound the compu-tation cost (i.e., the size of the largest canopy) in each round, our goal reduces to minimizing the number of rounds to obtain maxi-mum recall with respect to a training dataset.

We present a generic algorithm (Algorithm 4) that extends any algorithm for disjoint canopy formation to an algorithm for the non-disjoint case. We assume a bound on the maximum computation in any round, and use the disjoint algorithm to maximize recall in a round. The duplicate pairs that are covered are then removed from the labeled dataset, and the next round is performed. We may truncate the algorithm when all pairs are covered, or no more pairs can be covered, or a pre-specified maximum number of rounds has reached.
This section presents a detailed experimental study using two large commercial datasets at Yahoo: (1) a movie dataset consisting of 140K entities, and (2) a restaurants dataset consisting of 40K en-tities. We present a summary of results based on both the datasets, but focus on movies for a more detailed evaluation. (We focus only on one dataset for a detailed evaluation due to space constraints; the movies dataset being larger makes for a more interesting study although trends are similar in the restaurants dataset.)
The primary goal of our study is to measure the effectiveness (in-creased recall) due to the more expressive BlkTree-based blocking, as compared to restrictions of BlkTrees. We measure recall for dis-joint and non-disjoint versions of all our algorithms. In addition to the primary objectives described above, our experiments also un-derstand the effects of increasing the size of canopies on recall, variation of recall with the number of disjuncts, effects of specific greedy strategies used, and understanding some basic properties of BlkTree-based blocking. Our experimental setup is described in Section 8.1 and results are presented in Section 8.2. We have applied CBLOCK on two commercial datasets: movies and restaurants . The primary movies dataset used in our ex-periments is a large database D movie of 140K movies. In addition, we use a sample of movies from DBPedia [1] to obtain new dupli-cates, in addition to the duplicate already existing in D constructed a labeled dataset T + consisting of 1054 pairs of dupli-cates: Around 350 pairs of duplicates were obtained using manual labeling by paid editors. The remaining 704 pairs were obtained automatically by finding common references to IMDb [2] movies; a small sample of 100 automatically generated pairs were checked manually to confirm that these were all duplicates. The schema of movies consisted of attributes title , director , release year , runtime on which hash functions were created, and also other attributes (such as crew members ) that weren X  X  used for blocking. A sample of the space of hash functions used in our ex-periments is shown in Table 1.

The restaurants dataset used in our experiments consists of 40 , 000 restaurant records with attributes name , street , city , state , and zip . After de-duplication, there are 13 , 000 unique restaurant records. We use a labeled dataset of 4 , 674 duplicate pairs, and we used a similar set of hash functions as in Table 1.
 We evaluate our canopy generation algorithms using two metrics  X  recall and computation cost . Recall is measured as the fraction of matching pairs in T + that appear within some canopy (Defini-tion 4.1). Our algorithms are used to learn blocking hash functions, which are in turn applied to new data. We measure the computation cost in terms of the time taken to apply the hash function learnt by our algorithms on the dataset. Note that this is not the time taken to learn blocking functions. For non-disjoint canopy formation (Al-gorithm 4 in Section 7) we measure the increase in recall as the number of disjuncts (or map-reduce steps) is increased.
 We describe our algorithms next. If any of our algorithms result in canopies C with size larger than our maximum size limit S , we further split it randomly into d | C | S e smaller parts. The algorithms we compare are  X  Random (R): Each entity in U is assigned uniformly at random  X  Single-Hash (SH): Canopies are formed by picking a single  X  Chain (C): Canopies are formed by picking the best conjunction  X  Chain-Tree (CT): A restriction to our tree based hash function  X  Hierarchical Blocking Tree (HBT): Our BlkTree-based canopy We also consider non-disjoint variants of all our algorithms; if A  X  { SH, C, CT, HBT } denotes one of the above algorithms, we use A -ND to denote its non-disjoint variant (i.e., using A in Algo-rithm 4). We perform 5-fold cross-validation for all runs of algorithms: We split T + into 5 equal pieces randomly, then average over five runs with each run using 4 pieces of T + as a training set to obtain the blocking function, then use the 5 th piece as a test set. Since we don X  X  make any novel contribution on size estimation, our oracle size () computes the exact sizes of canopies based on the entire dataset. Our experiments were performed by varying the allowed maximum canopy size with 1 K, 5 K, 10 K, 20 K, 100 K entities per canopy.
We start by presenting detailed results on the movie dataset (Sec-tion 8.2.1 X 8.2.3). Finally, we present a brief summary of results on the restaurants dataset in Section 8.2.4.
Our first experiment was to compare the overall recall obtained by each of the algorithms X  X , SH, C, CT, and HBT. Figure 5(a) shows the recall obtained by each of the algorithms on the movie dataset, varying the maximum allowed canopy size. (For each of the algorithms, we picked the best of the optimistic, pessimistic, and expected greedy picking strategies.) The most important ob-servation is that HBT achieves a significantly higher recall than C and SH, particularly when the maximum canopy size is lower. The reason for HBT X  X  higher recall is the greater expressive power of BlkTrees as a construct for describing disjoint blocking functions; BlkTree X  X  are able to apply a hash function at the first level that creates many good small canopies and a few large canopies, which are further split at subsequent levels of the tree. Another interest-ing observation from Figure 5(a) is that CT performs roughly as well as HBT, despite the slightly lower expressive power: Intu-itively, the added power of HBT is effective when different nodes in the same level need different hash functions. Such a case would arise when different sections of the data have differing properties (e.g., US movies versus German movies); our dataset, however, only contained US movies. Finally, as expected R gives the lowest recall among all algorithms; henceforth, we omit R from the rest of our experiments.

To further understand the effects of the three greedy picking strategies X  X ptimistic, pessimistic, and expected X  X escribed in Sec-tion 4.4, in Figure 5(b) we plot the recall for each of the algo-rithms by varying the greedy picking strategy. We note that in most cases all three algorithms perform very similarly, with the optimistic picking strategy slightly outperforming the others. The intuition for optimistic greedy strategy performing slightly better is that an optimistic estimate is better than an expected estimate since future levels of blocking are significantly better than a random split of each large canopy. Since optimistic is never worse than the other strategies, for the rest of our experiments we choose the optimistic strategy for each algorithm.
Next we consider the non-disjoint variants of each of the algo-rithms. Figure 5(c) shows the overall recall for each non-disjoint algorithm as the number of canopies is varied. We notice, once again, that HBT-ND achieves a significantly higher recall than C-ND and SH-ND. In particular, C-ND is the size-aware analogues of previous state of the art [7, 21]. The reason for a much higher recall in HBT-ND is again the larger space of blocking functions BlkTrees can represent. Specifically, any conjunction that contains even one canopy larger than the maximum allowed is not permitted (or the conjunction needs to be further restricted losing more du-plicate pairs). Note however that overall recall of CT-ND is very similar to that of HBT-ND; however, we shall see shortly that in the initial rounds of disjunction, HBT-ND increases recall slightly more rapidly than CT-ND.

A second observation on non-disjoint canopy formation is that the non-disjoint versions of each algorithm obtain higher recall than the corresponding disjoint versions. In Figure 5(d) we show the increase in recall obtained by HBT-ND as compared to HBT, for each maximum canopy size. Note that the additional benefit of non-disjointness diminishes as the maximum canopy size is increased.
Next let us take a closer look at how the recall changes as the number of iterations is increased. To examine the difference be-tween CT-ND and HBT-ND (as well as other non-disjoint algo-rithms), we plot the recall obtained after each round of disjoint canopy formation. Figure 5(e) plots the overall recall for the case of maximum canopy size 5000 ; we picked one fold of our cross-validation in which CT-ND ends with a slightly higher recall than HBT (therefore the apparent discrepancy with Figure 5(a)). First, note that for every iteration, HBT-ND is better than C-ND and SH-ND, which means that the number of positive examples covered increases more steeply for HBT-ND. Second, we see that HBT-ND obtains a higher recall than CT-ND initially, but CT-ND eventually ends with a slightly higher recall; in other words, with a limited number of map-reduce rounds, HBT-ND performs better than CT-ND. An optimal strategy of choosing a non-disjoint canopy forma-tion by combining HBT-ND and CT-ND is left as future work. Computational Cost: We compared the computation cost (i.e., running) of applying a BlkTree against the cost of applying other blocking functions. The primary objective of investigating the run-ning time is to establish the fact that BlkTrees do not add signif-icant burden on the time required to apply the blocking function on an entire dataset. Table 2 shows the running time of applying the best blocking function (for maximum canopy size 10 , 000 ) for each of the algorithms (conjunctions being similar to applying a single hash function are omitted); these numbers are averaged over the  X  140 K movie entities and over 5 repeated applications of the blocking function on the entire dataset. We note that apply-ing each of the blocking functions requires a negligible amount of time (always under 20  X s per record), and BlkTrees don X  X  add any discernible computational cost.
 Tree Size: Table 2 also shows the height of the tree for CT and HBT (averaged over the 5 folds of cross-validation). It is noteworthy that HBT obtains similar recall with a shorter BlkTree than CT. This is because the BlkTree constructed using HBT is able to selectively create longer branches only when necessary. The longer tree for CT explains the higher blocking time per record. Table 2: (1) Average running time (in  X s .) of applying the best blocking function for each record. (2) Average length of the tree. All the numbers are for a max canopy size of 10 , 000 .
We present a very brief summary of our results on the restaurants dataset; restaurants displayed a similar general trend as movies, and a detailed study of restaurants is omitted due to space con-straints. Table 5(f) presents the overall recall for HBT and HBT-ND compared against C and C-ND, varying the sizes of the maximum canopy: (1) We note that both the disjoint and non-disjoint ver-sions of HBT significantly outperform the disjoint and non-disjoint versions of conjunctive blocking. (2) Further, as with movies, the recall achieved by HBT is very high on restaurants, and very close to 1 with non-disjoint blocking even for small canopy sizes.
