 Extended Collaborative Less-is-More Filtering xCLiMF is a learning to rank model for collaborative filtering that is specifically designed for use with data where information on the level of relevance of the recommendations exists, e.g. through ratings. xCLiMF can be seen as a generalization of the Collaborative Less-is-More Filtering ( CLiMF ) method that was proposed for top-N recommendations using binary relevance (implicit feedback) data. The key contribution of the xCLiMF algorithm is that it builds a recommendation model by optimizing Expected Reciprocal Rank, an eval-uation metric that generalizes reciprocal rank in order to incorporate user feedback with multiple levels of relevance. Experimental results on real-world datasets show the effec-tiveness of xCLiMF , and also demonstrate its advantage over CLiMF when more than two levels of relevance exist in the data.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information Filtering Collaborative filtering, expected reciprocal rank, graded rel-evance, top-N recommendation, ranking
The purpose of a recommender system is, typically, to gen-erate a ranked item list (or top-N item list) that is tailored to the preferences of each individual user [1, 8]. Collabora-tive Filtering (CF) is one of the most successful techniques for recommender systems [6], the core concept of which is to provide recommendations to a user based on the common interest between this user and other users.

Learning to rank techniques originated in the Informa-tion Retrieval (IR) and Machine Learning communities [3]. One main branch of learning to rank techniques is to di-rectly optimize a IR measure such as Mean Average Pre-cision (MAP), Area Under the ROC Curve (AUC), Mean Reciprocal Rank (MRR) , or Normalized Discounted Cu-mulative Gain (NDCG), e.g., [2, 15]. Learning to rank tech-niques have been recently used to optimize CF models. CF models for ranking binary relevance data are based on op-timizing a smooth version of binary relevance measures for IR such as AUC in the case of Bayesian Personalized Rank-ing (BPR) [10], MAP for the TFMAP algorithm [12] and MRR for CLiMF [13]. Note that in this paper we use the term binary relevance to refer to data where users provide implicit feedback about their preferences (e.g., clicks, pur-chases, views etc.) not the case where there is explicit feed-back on a binary scale (e.g., like/dislike). When multiple levels of relevance exist in the data, e.g., rating levels as-signed by users can be productively treated as grades of relevance, these methods cannot be easily used since that would require an arbitrary threshold to binarize the rele-vance. In this way though fine-grained rating information is thrown out that actually may be helpful.

Ranking methods for CF have been also developed for multiple level relevance data which typically come in the form of ratings, for example, CoFiRank [14] optimized an upper bound of a NDCG loss function. One of the main disadvantage of NDCG is the underlying assumption that the usefulness of a document at rank i is independent of the usefulness of the documents at rank less than i . Thus a single perfectly relevant item in the top of a recommendation list might get less NDCG score than a list with several lower scored relevant items in the top positions.

The Expected Reciprocal Rank (ERR) [4] measure was re-cently introduced to deal with these shortcomings. ERR is a generalized version of RR designed to be used with multiple relevance level data (e.g., ratings). It has similar properties to RR in that it strongly emphasizes the relevance of results returned at the top of the list. Importantly, ERR, like RR, imposes the assumption that the user scans the results list from top to bottom, and stops when a result is found that fully satisfies the user X  X  information need. This model of user behavior provides a particularly good fit with top-N recom-mender use scenarios. In such scenarios, the user is often looking for a single item/product and is completely satis-fied after having found it in the list. This work builds upon our previous contribution of CLiMF [13], which aimed at optimizing Reciprocal Rank (RR), for top-N recommenda-tion in domains with binary relevance data. It extends our initial experimentation with exploiting graded relevance for top-N recommendation [11]. Its critical contribution is the focus on the benefits of ERR within top-N recommendation use scenarios. We propose a new CF approach, Extended Collaborative Less-is-More Filtering (xCLiMF) , which aims at directly optimizing ERR, for top-N recommendation in domains with graded relevance data. We derive an opti-mization procedure that is linear to the number of relevance data (i.e., ratings), and show that xCLiMF outperforms a few baseline methods on two datasets.

The remainder of the paper is structured as follows. In the next section, we present the technical detail of xCLiMF , after which we demonstrate the experimental evaluation in Section 3. Finally, Section 4 summarizes the key aspects of this work and briefly addresses the directions for future work
Using the definition of ERR in [4], we can express ERR for a ranked item list of user m as follows: I (  X  ) is an indicator function, which is equal to 1 if the condi-tion is true, and otherwise 0. R mi denotes the rank position of item i for user m , when all items are ranked in descend-ing order of the predicted relevance values. r mi denotes the probability that user m finds the item i is relevant. We use a mapping function similar to the one used in [4], to convert ratings (or levels of relevance in general) to probabilities of relevance, as shown below: where y mi denotes the rating given by user m to item i , and y max is the highest rating. Note that I mi &gt; 0 ( I indicates that user m  X  X  preference to item i is known (un-known). We employ factor models and in particular matrix factorization where the relevance between user m and item i is modeled by the inner product of U m and V i (i.e., the latent factors of user m and item i ), as follows: Note that the value of rank R mi depends on the value of f For example, if the predicted relevance value f mi of item i is the second highest among all the items for user m ,then we will have R mi = 2.

Similar to other ranking measure such as RR, ERR is also non-smooth with respect to the latent factors of users, items, i.e., U and V , it is thus impossible to optimize ERR directly using conventional optimization techniques. We thus employ smoothing techniques that where also used in CLiMF [13], to attain a smoothed version of ERR. In particular we ap-proximate the rank-based terms 1 /R mi and I ( R mj &lt; R in Eq. (1) by smooth functions with respect to the model parameters U and V , as shown below where g ( x ) is a logistic function, i.e., g ( x ) = 1 / (1 + e
Substituting Eq. (4) and (5) into Eq. (1), we obtain a smoothed approximation of ERR m : Note that for notation convenience, we make use of the sub-
Using Jensen X  X  inequality and exploiting the concavity of the log function in a similar manner to [13], we can derive a lower bound of ERR m in Eq.(6) : L ( U m ,V ) = Using the lower bound we can optimize a factor model with respect to ERR. Taking into account all M users and using the Frobenius norm of the latent factors for regularization, we obtain the objective function of xCLiMF : We can now maximize the objective function (8) with re-spect to the latent factors argmax U,V F ( U,V ). Note that F ( U,V ) represents an approximation of the mean value of ERR across all the users, we can thus remove the constant coefficient 1 /M , since it has no influence on the optimiza-tion of F ( U,V ). Since the objective function is smooth we can use gradient ascent for the optimization. The gradients can be derived in a similar manner to CLiMF [13], as shown below:  X  X 
Note that xCLiMF can be seen as a generalized version of the CLiMF algorithm. We can derive CLiMF from the xCLiMF algorithm as a special case when r mi is binary, i.e., 1 for relevant items and 0 otherwise. The overall complexity of the optimization procedure is in the order of O ( d  X  n where  X  n the average number of items per user. Note that we have  X  nM = S , in which S denotes the number of non-zeros in the user-item matrix. The complexity of the learning Figure 1: The distributions of ratings in ML1m and TVid datasets. the complexity is O ( dS ) even in the case that  X  n is large, i.e., being linear to the number of non-zeros (i.e. relevant observations in the data).
In this section we present a series of experiments for the evaluation of xCLiMF . We are trying to address the follow-ing two research questions: 1. Does xCLiMF outperform state-of-the-art CF approaches 2. How does xCLiMF perform compared to CLiMF ? We use two datasets for the experiments: The first is the MovieLens 1 million dataset 1 (ML1m) [7], which contains ca. 1M ratings (1-5 scale) from ca. 6K users and 3.7K movies. The sparseness of the ML1m dataset is 95.53%. The second dataset was collected from the video watching data in the Tuenti social network 2 . In the experiments, we convert the count of video-watching into four levels: 1 if a user did not watched a video completely; 2 if a user watching a video only once; 3 if a user watching a video 2-5 times; 4 if a user watched a video more than 5 times. Those levels (similar to ratings) are defined to indicate different levels of user preference to videos, and used as explicit feedback data in the experiments. In the following, we denote this dataset as TVid, it contains ca. 3.2 million rating levels from ca. 55.5K users and 51.1K videos with 99.89% data sparseness. The distributions of ratings in the two datasets are presented in Figure 1.
For each dataset, we randomly selected 5 rated items (movies or videos) for each user to form a test set. We then randomly selected a varying number of rated items from the rest to form a training set. For example, under the condition of  X  X iven 5 X , we randomly select 5 rated items (disjoint to the items in the test set) for each user in order to generate a training set. We investigated a variety of  X  X iven X  conditions for the training sets, i.e., 5, 10 and 15 for the ML1m dataset and 5, 15, 25 for the TVid dataset. Generated recommenda-tion lists for each user are compared to the ground truth in the test set in order to measure the performance. Apart from ERR, we also measure the recommendation performance by http://www.grouplens.org/node/73 https://www.tuenti.com/ NDCG, a well-known evaluation metric for measuring the performance of ranked results with graded judgments. Since in recommender systems the user X  X  satisfaction is dominated by only a few items on the top of the recommendation list, our evaluation in the following experiments focuses on the performance of top-5 recommended items, i.e., ERR@5 and NDCG@5. Note that in the evaluation we cannot treat all the unrated items as irrelevant to a given user. For this rea-son, we apply a widely-used practical strategy, as suggested in literature [5, 9], which is to first randomly select 1000 un-rated items (which are assumed to be irrelevant to the user) in addition to the ground truth (i.e., rated items) for each user in the test set, and then evaluate the performance of the recommendation list that consists of only the selected unrated items and the ground truth items. In addition, con-sidering the domination effect of top popular items [13], we exclude the top-3 most popular items in each dataset from recommendations for the evaluation in the test set. The parameters in xCLiMF are tuned so as to yield the best bal-ance between performance and computational cost based on a validation set of each dataset, i.e., D =10,  X  =0.001 and learning rate 0.001.
We compare the performance of xCLiMF with that of two baseline algorithms. The approaches we compare against are listed below:
The results of the experiments are shown in Table 1 and 2, from which we obtain two observations.

First, xCLiMF outperforms the baseline approaches in terms of both ERR and NDCG in most of the cases. The im-provements against baselines are all statistically significant according to the Wilcoxon signed rank test with p&lt; 0.05. The results also show that the improvement of ERR aligns consistently with the improvement of NDCG, indicating that optimizing ERR would not degrade the utility of recommen-dations that are captured by the NDCG measure.

Second, the relative performance of xCLiMF improves as the number of observed ratings from the users increases. This result indicates that xCLiMF can learn better top-N recommendation models if more observations of the graded relevance data from users can be used. The results also reveal that it is difficult to model user preferences encoded in multiple levels of relevance with limited observations, in particular, when the number of observations is lower than the number of relevance levels. We compare the performance between xCLiMF and CLiMF . We use CLiMF on the explicit feedback datasets by bina-rizing the rating values with different thresholds. On the ML1m dataset, we take rating 4 and 5 (the highest two relevance levels), respectively, as the relevance threshold for http://www.cofirank.org/ Table 1: Performance comparison of xCLiMF and the Table 2: Performance comparison of xCLiMF and the training the CLiMF model and measure the performance us-ing Mean Reciprocal Rank (MRR). On the TVid dataset, we take rating 3 and 4 (the highest two relevance levels), respec-tively, as the relevance threshold. The results are shown in Table 3 and 4. We observe that, although CLiMF is trained to optimize RR, it suffers from the additional data sparse-ness as a result of the binarization of the multi-level rele-vance data. CLiMF is outperformed by xCLiMF in terms of both MRR and ERR, across all the settings of relevance thresholds and datasets. The results indicate that the infor-mation loss from binarizing multi-level relevance data would inevitably make recommendation models based on binary relevance data, such as CLiMF , suboptimal for the use cases with explicit feedback data. We can thus conclude that, xCLiMF is addressing the top-N recommendation problem for multiple-level relevance data.
We have presented xCLiMF , an extended version of CLiMF that can be used with multi-level relevance data for the purpose of top-N recommendation. Compared to xCLiMF , which optimizes RR, xCLiMF learns latent factors of users and items by optimizing ERR, a measure of quality of ranked results with multiple relevance levels. The experiments demon-strate the competitive performance of xCLiMF for top-N recommendation in the domains with multi-level relevance data. We also demonstrate the specific advantage of xCLiMF compared to CLiMF when used on multi-level relevance data.

There are several possibilities for future work. One is to investigate the relationship between ERR optimization and diversification. Another interesting topic is to compare the recommendation models that are designed for optimizing different ranking measures, and to investigate the possibility of optimizing recommendation results for multiple criteria.
This work is funded as part of a Marie Curie Intra Euro-pean Fellowship for Career Development (IEF) award (CARS, PIEF-GA-2010-273739) held by Alexandros Karatzoglou. Table 3: xCLiMF vs. CLiMF on the ML1m dataset Table 4: xCLiMF vs. CLiMF on the TVid dataset
