
Text classification is widely used in applications rang-ing from e-mail filtering to review classification. Many of these applications demand that the classification method be efficient and robust, yet produce accurate categoriza-tions by using the terms in the documents only. We present a supervised text classification method based on discrimi-native term weighting, discrimination information poolin g, and linear discrimination. Terms in the documents are as-signed weights according to the discrimination informatio n they provide for one category over the others. These weights also serve to partition the terms into two sets. A linear opinion pool is adopted for combining the discrimination information provided by each set of terms yielding a two-dimensional feature space. Subsequently, a linear discrim i-nant function is learned to categorize the documents in the feature space. We provide intuitive and empirical evidence of the robustness of our method with three term weighting strategies. Experimental results are presented for data se ts from three different application areas. The results show th at our method X  X  accuracy is higher than other popular meth-ods, especially when there is a distribution shift from trai n-ing to testing sets. Moreover, our method is simple yet ro-bust to different application domains and small training se t sizes.
Automatic content based text classification into prede-fined categories is becoming extremely useful with the in-creasing availability of text documents in digital formats such as Web pages, e-mails, Web blogs, digital libraries, and corporate text databases. A common application of text classification is information filtering where a stream of doc -uments (e.g. e-mails) is classified before or after reaching its destination. Other applications of text classification in-clude document organization, web page categorization, and query classification. More recently, text classification ha s been used for semantic analysis of documents such as re-view documents X  categorization as positive or negative and word sense disambiguation. The scope and scale of text classification applications is bound to increase in the futu re as more text documents in digital formats become available.
The prototypical text classification problem can be de-fined as follows. Given a set of labeled text documents the category of document x i and | C | and | L | are the to-tal number of predefined categories and labeled documents; learn a classifier that assigns a category label from 1 to | C | to each document in the set U = {h x i i} | U | i =1 . This is a su-pervised learning setting in which it is assumed that the joint probability distribution of documents and categorie s is identical in sets U and L (although this is not guaran-teed in practice for some applications). In other words, the task is to learn to approximate the unknown target function  X   X  : U  X  { 1 , 2 , . . . , | C |} by the classifier function  X  : U  X  { 1 , 2 , . . . , | C |} such that the number of documents in U for which  X ( x j ) 6 =  X   X  ( x j ) is a minimum. A document is represented as a 0/1 vector x i = h x i 1 , x i 2 , . . . , x x ij  X  { 0 , 1 } indicates whether the term (typically a word) j exists in document i or not. The integer | T | is the num-ber of terms in the dictionary of L and U (after standard preprocessing of stop word removal and stemming). The terms and categories are assumed to be just symbolic labels without semantics and that no additional knowledge of a procedural or declarative nature is available.

Text classification, as defined above, is challenging for two reasons. First, the dimensionality of the term space ( | T | ) is large and the set of documents L is sparsely rep-resented in it. Second, selecting the relevant terms and the ir weights (relative importance) for the classification. Diff er-ent methods for text classification address these challenge s in different ways. Besides these challenges, text classifi-cation methods must be efficient in order to handle large volumes of data in various applications. Furthermore, text classification methods must be robust to small sizes of la-beled sets and differing distributions of labeled and unla-beled sets.

In this paper, we present a robust and efficient text clas-sification method based on discriminative term weighting, discrimination information pooling, and linear discrimin a-tion in a two-dimensional feature space. Three term weight-ing strategies are investigated for discriminating and par ti-tioning the terms: odds, log-odds, and Kullback-Leibler di -vergence. These strategies weigh the terms based on the discrimination information they provide for one category over the others. A two-dimensional one-category-versus-others feature space is constructed as the weighted sum of terms. This transformation is based on a technique for com-bining experts X  opinions known as linear opinion pool. The classification is then learned in the feature space by a sim-ple linear discriminant function. Our method is a hybrid generative-discriminative method where the term weights represent a generative model and the linear discriminant represents a discriminative model of the classification pro b-lem. We evaluate our method on three data sets belonging to three different application areas -spam filtering, movie re -view, and SRAA. The results are compared with four com-mon text classification methods, demonstrating the overall effectiveness of our method with improved classification ac -curacies.

The rest of the paper is organized as follows. We present the motivation and related work in Section 2. Our text clas-sification method, DTWC, is described in Section 3, includ-ing a comparison with the naive Bayes classifier. We de-scribe the data sets and evaluation setup in Section 4. Sec-tion 5 presents the results of our evaluations and compar-isons with other methods. We conclude in Section 6.
Text classification has been studied extensively in the literature. A comprehensive review of text classification methods is given in [28]. Here we focus on document representation, feature selection, supervised methods, a nd generative-discriminative methods. Many text classifica-tion methods use the  X  X ag-of-words X  representation that describes a document by a term vector where each term (typically a word) is given a weight (term position infor-mation is not preserved). The common weighting tech-niques include term occurrence (binary), term frequency, and term-frequency-inverse-document frequency [27, 23]. Our method can work with any weighting technique as long as a term vector representation is used. In this paper, how-ever, we restrict ourselves to the binary term vector rep-resentation that has been shown to produce more accurate classifiers in some settings [16].

Regardless of the document representation approach, the dimensionality of the term space is very large for text clas-sification problems. Feature selection and dimensionality reduction for text classification has been studied extensiv ely [5, 11, 7]. Techniques for feature selection and dimen-sionality reduction can be supervised or unsupervised de-pending on whether they require class information. How-ever, for the text classification problem setting discussed in this paper, supervised techniques are more commonly used [2, 8, 4, 9]. These techniques rely on class information and information theoretic measures, such as entropy, to iden-tify high relevance terms. Our term weighting and selection technique belongs to this latter category of techniques. In particular, we weigh each term by the discrimination infor-mation it provides for discriminating between one category and the rest. The weights also serve to partition the terms into two sets, and they can be thresholded for term selection and dimensionality reduction. A novel information pooling technique is adopted to aggregate the discrimination infor -mation of each set to form a two-dimensional feature space in which a linear discriminant function is learned. The ap-proach of learning terms X  weights from training data based on their distributions in the two categories appears to have been first proposed by [8]. They present information theo-retic functions to replace the IDF component of the TFIDF term weighting strategy and use these weights in the clas-sification model. In this work, we focus on discrimination information measures of weighting the terms for both selec-tion and classification.

Supervised text classification methods can be based on a generative or discriminative model of the problem. The most common generative methods are naive Bayes and maximum entropy [29, 17, 24]. The naive Bayes classifier results from the application of the Bayes rule with the as-sumption that each term is independent of the others given the category label, while the maximum entropy method es-timates the class conditional distributions by maximizing the entropy among them. The most popular discrimina-tive method for text classification is support vector machin e (SVM) [14]. SVM, which is based on statistical learning theory and structural risk minimization, learns a maximum margin linear discriminant in a high dimensional feature space. The balanced winnow method is another example of a discriminative method that learns a linear discriminan t in the term space by minimizing the mistakes made by the classifier [6].

There has been continuing interest in hybrid generative-discriminative methods [12, 26, 19, 21]. These methods try to exploit the strengths of generative and discrimina-tive methods by first learning the data distribution and then building a discriminative classifier using the learned dist ri-bution. Several variants of this general concept have been explored with promising results. Our method can also be categorized as a hybrid generative-discriminative method . However, our method is simpler and efficient requiring fewer parameters and learns faster.
In this section, we describe our text classification method based on discriminative term weighting and linear discrimi -nation. Our method, subsequently referred to as DTWC, ad-dresses the key issues of high dimensionality, term weight-ing and selection, and feature enhancement faced by su-pervised text classification methods. DTWC uses statis-tical and probabilistic techniques in a hybrid generative-discriminative model of the classification problem. DTWC is efficient and robust  X  characteristics much desired for today X  X  text classification applications. And, as demon-strated by our evaluations, DTWC X  X  classification accuracy is higher than other well-known methods for text classifi-cation. In the remaining subsections, we present our dis-criminative term weighting strategies, term space partiti on-ing and term selection strategy, discrimination informati on pooling, linear discriminant learning in the feature space , and relation of DTWC with the naive Bayes classifier.
In the literature, term weighting has often been em-ployed for effective document representations whereby the frequency of the term or a derived measure like term-frequency-inverse-document-frequency (TFIDF) is used to weigh the term. In this paper, we represent a document as a term occurrence binary vector x = h x 1 , . . . , x | T | term weighting as a measure of the relevance of the term for the classification problem. As such, term weights are prop-erties of the individual terms and are derived from the train -ing data L and not from an individual document only. This view will also help us in term space partitioning and term selection, as described in the next subsection. Each term is weighed by the discrimination information it provides for a specific category over the others. We present three discrim-inative term weighting strategies: odds, log-odds, and KL divergence.

If a document x contains a term j (i.e. x j = 1 ) then it is more likely to belong to category k if p ( x j = 1 | c = k, L ) is greater than p ( x j = 1 | c = C \ k, L ) , where notation C \ k denotes all categories but k . Equivalently, a document x is likely to belong to category k if the odds for category k are greater than one: p ( c = k | x j = 1) p ( C \ k | x j = 1) In the above and subsequent equations, the conditioning on the labeled set L has been omitted for brevity. We would like to quantify the discriminative information that a term j provides regarding category k over categories C \ k . One way of doing this is to weigh the term by its odds of occur-ring in documents belonging to category k over documents of categories C \ k : where a j = p ( x j = 1 | c = k ) and b j = p ( x j = 1 | c = C \ k ) . Notice that the discrimination information that term j provides for categories C \ k over category k is b j /a Thus, the smallest weight assigned by Eq. 2 is one.
A second strategy for discriminative term weighting is to use the log-odds for the term. Using this strategy, the weight for term j is defined as Following this strategy, the smallest weight is zero which is consistent with no discrimination information. Nonethe -less, Eqs. 2 and 3 are monotonically related with Eq. 2 always giving a larger value than Eq. 3. This difference in values becomes greater with increasing difference between a and b j .

A third strategy for discriminative term weighting is to use the information theoretic measure known as Kullback-Leibler (KL) divergence. The KL divergence of probability distribution p ( x ) from q ( x ) is defined as The KL divergence can also be interpreted as the expected discrimination information for p ( x ) over q ( x ) . In our con-text, the two probability distributions are p ( x j | c = k ) and p ( x j | c = C \ k ) where x j can take on values of zero and one. Then, the expected discrimination information pro-vided by knowledge of term j for category k over other cat-egories is given by the KL divergence as Unlike the previous two strategies, this term weighting strategy considers both the occurrence and the absence of a term. Eq. 4 is also monotonically related with Eqs. 2 and 3. However, unlike Eqs. 2 and 3, Eq. 4 is not symmetric. In any case, all three equations quantify the discriminatio n information provided by term j for discriminating between category k and categories C \ k with larger weights signify-ing larger discrimination information.
The probabilities a j and b j are estimated from the train-ing data L by maximum likelihood estimation. A Laplacian prior is used for each event for smoothing (add-one smooth-ing).
The discriminative term weighting strategies described in the previous section can be used for term space parti-tioning and discriminating term selection. Our weighting strategies naturally partitions the terms into two sets: on e set, identified by the index set Z k , contains terms for which a j &gt; b j and the other set, identified by the index set Z contains the remaining terms. All terms j  X  Z k provide evidence for category k over the rest, and this evidence is quantified in their weights in the form of discrimination in-formation. In the next subsection, we describe how we use this partitioning to create a discriminative model of the cl as-sification problem.

Our weighting strategies also provide a natural way of selecting highly discriminating and relevant terms. A term j is selected as relevant for the k versus C \ k classification problem if where t is a positive valued threshold. All terms that do not satisfy this condition are discarded from the classificatio n model. By increasing the value of t , the number of relevant terms can be reduced by eliminating terms that provide littl e discrimination information.

DTWC does not require term selection and dimension-ality reduction as it transforms the input terms to a two-dimensional feature space (described in the next subsec-tion). However, term selection may be necessary for large scale applications like personalized spam filtering by e-ma il service providers [16]. For such applications, DTWC X  X  ac-curacy can be traded off with its space complexity by vary-ing the value of t .
We use the two set partitioning of the term space, which is based on discrimination information, to form a two-dimensional feature space. Consider a document x . Each term j  X  Z k in the document expresses an opinion regard-ing the document X  X  categorization. This opinion is capture d by the discriminative term weight w k j . The aggregated opin-ion of all these terms is obtained as the linear combination of individuals X  opinions: This equation follows from a linear opinion pool or an en-semble average, which is a statistical technique for com-bining experts X  opinions [13, 1]. Each opinion ( w k j ) is weighted by the normalized term occurrence ( x j / P x j ) and all weighted opinions are summed yielding an aggre-gated discrimination score for category k ( Score k ( x ) ) of the document. If a term i does not occur in the document (i.e. x i = 0 ) then it does not contribute to the pool. Also, terms that do not belong to set Z k do not contribute to the pool. Similarly, an aggregated discrimination score can be computed for all terms j  X  Z C \ k as The two-dimensional feature space is defined by the two scores Score k ( x ) and Score C \ k ( x ) . In this space, docu-ments are well separated and discriminated, as illustrated for a spam classification data (Figure 1). We learn the cate-gorization in this space by a linear discriminant function: where  X  k and  X  0 are the slope and bias parameters, respec-tively. The discriminating line is defined by f k ( ) = 0 f k ( ) &gt; 0 then the document x is likely to belong to cate-gory k (Figure 1). For a | C | category classification problem, we learn | C |  X  1 discriminant functions each with two pa-rameters. In practice, however, the bias parameter set to zero often yields better results, leaving only the slope pa-rameter to be learned. The discriminative model parameters are learned by minimizing the classification error over the labeled training set L . This represents a straightforward op-timization problem that can be solved by any iterative op-timization technique [20]. DTWC X  X  overall classifier func-tion is defined as DTWC derives its strength from the discrimination infor-mation based term weighting, discrimination information pooling to form a two-dimensional feature space, and a sim-ple linear discriminative model for classification. These characteristics make DTWC efficient, in terms of both time and space, and robust to noise and changing data distri-butions. DTWC contains three key steps: (1) discrim-inative term weight computation, which can be done in one pass over the labeled data set, (2) forming the two-dimensional feature space, and (3) learning the parameters of the discriminating line which can be done efficiently us-ing straightforward optimization algorithms. The DTWC algorithm is given in Algorithm 1.
Figure 1. The two-dimensional feature space and the linear discriminant function for a spam classification problem
In this section, we develop the naive Bayes classifier and show its relation to DTWC. The odds that a document x belongs to category k rather than categories C \ k can be written as Assuming that the occurrence of each term is independent of others given the category, the document odds on the right-hand side becomes a product of terms X  odds. The naive Bayes classification of document x is category k when
Equivalently, taking the log of both sides, the above ex-pression can be written as log This equation computes a non-negativescore, and when this score is greater than zero the naive Bayes classification for the document x is k . Notice that only those terms are in-cluded in the summation for which x j = 1 .
 Comparing the naive Bayes classifier, as expressed by Eq. 10, with DTWC yields some interesting observations. The discriminative model of DTWC is similar to Eq. 10 in Algorithm 1 DTWC
Input: set of labeled documents L , set of unlabeled doc-uments U Output: labels for documents in U
On training data L for k = 1 to | C |  X  1 do end for
On test data U for k = 1 to | C |  X  1 do end for output k = argmax k f k ( x ) (Eq. 9) that the structure of the discrimination score computation (Eqs. 6 and 7) is similar to the summation in Eq. 10 and the bias parameter  X  0 corresponds to the first term in Eq. 10. The log-odds for term j in Eq. 10 corresponds to the log-odds discriminative weighting strategy in DTWC. However, there are also significant differences between DTWC and naive Bayes. (1) The discrimination scores in DTWC are normalized (for each document) using the L 1 norm. Document length normalization is typically not done in naive Bayes classification, and when it is, the L 2 norm is used. Recently, it has been shown that performing L 1 doc-ument length normalization improves the precision of naive Bayes for text classification [17]. (2) DTWC partitions the summation into two, based on discrimination information, and then learns a linear discriminative model of the classi-fication. Naive Bayes, on the other hand, is a purely gener-ative model with no discriminative learning of parameters. (3) DTWC allows the use of different discriminative term weighting strategies as long as they quantify the discrim-ination information that a term provides for one category over the others. (4) DTWC does not require the naive Bayes assumption of conditional independence of the terms given the category.

DTWC will be identical to naive Bayes when the log-odds term weighting strategy is used, discrimination score s are not normalized, and the slope parameter  X  k is equal to one.
We evaluate DTWC on three commonly-used text clas-sification data sets  X  personalized spam filtering, movie review, and SRAA  X  and compare its performance with four other classifiers  X  Naive Bayes (NB), Maximum En-tropy (ME), Balanced Winnow (BW), and Support Vector Machine (SVM). The performance of DTWC with odds (DTWC-O), log-odds (DTWC-LO), and KL divergence (DTWC-KL) discriminative term weighting strategies is re-ported. For naive Bayes, Maximum Entropy, and Balanced Winnow we use the implementation provided by the Mallet toolkit [22]. For SVM, we use the implementation provided by SV M Light [15]. We report the classification accuracy for spam data set, and the mean and standard deviation of classification accuracy for movie and SRAA data sets cal-culated over 5 runs of the algorithms.
In all three data sets, documents are represented as bag-of-words/terms. We convert them to formats in which doc-uments are represented by term frequency vectors and term occurrence vectors. Where applicable stop words, HTML tags, and message headers are removed from the data sets.
The personalized spam filtering data set, henceforth identified as the Spam data set, captures the e-mail clas-sification problem in which individual user X  X  e-mails are la -beled as either spam or non-spam (2 categories) after learn-ing from a general labeled training set. This data set cor-responds to data set A provided by the 2006 ECML/PKDD Discovery Challenge [3]. It contains a labeled training set of 4000 e-mails and three unlabeled users inboxes of 2500 e-mails each. The composition of the training set is: 50% spam e-mails sent by blacklisted servers of the Spamhaus project (http://www.spamhaus.org), 40% non-spam e-mails from the SpamAssassin corpus, and 10% non-spam e-mails from about 100 different subscribed English and German newsletters. The composition of e-mails in users inboxes is more varied with 50% non-spam e-mails of distinct Enron employees from the Enron corpus and 50% spam e-mails from various sources. Low frequency terms have already been removed. A key characteristic of this data set is that th e distribution of e-mails in the training set is different fro m those in the users X  inboxes (test sets).

The movie review data set, henceforth identified as the Movie data set, captures the sentiment classifica-tion problem in which movie reviews from IMDB (In-ternet Movie Database) are labeled as either positive or negative (2 categories). This data set is obtained from http://www.cs.cornell.edu/people/pabo/movie-rev iew-data. It consist of 2000 positive and 2000 negative reviews. We remove the stop words/terms using the Mallet toolkit [22]. We holdout 400 examples of each class for testing and randomly select different numbers of examples for training .
The SRAA (Simulated/Real/Aviation/Auto) data set 1 is a collection of 73,218 documents from four newsgroups (simulated-aviation, simulated-auto, real-aviation, an d real-auto), representing a 4 category classification problem. We remove the HTML header and the stop words using the Mal-let toolkit [22]. We holdout 1000 examples of each class for testing and randomly select different numbers of examples for training.
Documents are represented by term frequency vectors for the NB, ME, BW, and SVM classifiers. For DTWC, however, we use term occurrence vectors for document rep-resentation. An extensive evaluation of DTWC with dif-ferent document vector representations is beyond the scope of this paper, although we do find that the term occurrence representation outperforms the term frequency representa -tion on the Spam data set. The default algorithm settings provided by Mallet are adopted for NB, ME, and BW.
The SVM (using SV M Light ) is tuned for each data set by evaluating its performance on a validation set that is a 30% holdout of the training set. The SV M Light parame-ter C that controls the trade-off between classification error and margin width is tuned for each data set. Similarly, we evaluate the performance of SVM with both linear and non-linear kernels and find the linear kernel to be superior. This observation is consistent with that reported in the literat ure [18, 10, 30]. We perform document length normalization using L 2 (Euclidean) norm. This improves performance slightly from the non-normalized case, as observed by oth-ers as well [25, 10, 30]. We keep the remaining parameters of SV M Light at default values. There are no tunable pa-rameters in DTWC (we keep the threshold t = 0 , unless mentioned otherwise).
Tables 1, 2, and 3 show the classification accuracies of DTWC, naive Bayes (NB), maximum entropy (ME), balanced winnow (BW), and SVM on Spam, Movie, and SRAA data sets, respectively. The results for DTWC with odds, log-odds, and KL divergence discriminative term weighting strategies are identified by DTWC-O, DTWC-LO, and DTWC-KL, respectively. For Movie and SRAA data sets, we give the mean and standard deviation of the classification accuracies over five runs of the classifiers wi th 2500 e-mails, respectively.
 selected test sets of size 800.
 each run using randomly chosen examples for training and testing. For Spam data set, we give classification accuracie s for each user inbox.

The results show that for all the runs, DTWC outper-forms all the other classification algorithms. This is also true for averaged results. The average performance of DTWC on the Spam data set is impressive (90.56% by DTWC-LO) with the second best performance being over 6% lower. The personalized spam filtering problem is chal-lenging because the distributions of e-mails in the trainin g set and the users X  inboxes are quite different. As such, al-though ME, BW, and SVM can achieve a high accuracy on the training set, their generalization onto the unseen data is very poor. Usually for such differing distribution classifi ca-tion problem settings, techniques that make use of the users  X  inboxes (unlabeled data) during learning, i.e., transduct ive or semi-supervised learning, will achieve better results [ 16]. Nonetheless, DTWC, which uses supervised learning, ap-pears to be little affected by this change in distribution of the two sets. The naive Bayes classifier appears to be second least affected by this change. The SVM performs poorly on this data set. The superior performance of DTWC can be attributed to the discriminative term-based model of spam and non-spam and the simple and generalized discrimina-tive model. An interesting observation for the Spam data set is that DTWC-KL X  X  performance is significantly lower than that of DTWC-O and DTWC-LO. DTWC-KL uses the KL divergence as the discriminative term weighting strat-egy as opposed to the odds and log-odds strategies used by the other two. This observation needs further investigatio n. Here we only conjecture that this may be related to the con-sideration of both presence and absence of terms in the con-text of personalized spam classification.

The distribution of training and testing sets are similar for the Movie and the SRAA data sets. For these data sets also, DTWC outperforms the other algorithms. The im-provement, however, is less significant as compared to that for the Spam data set. DTWC-KL is the best performer for the Movie data set, while DTWC-O is the best per-former for the SRAA data set. The results obtained by NB, ME, and SVM are comparable to those reported in [10, 21]. DTWC X  X  performance appears slightly lesser than that of multi-conditional learning reported in [21]; however, the ir exact evaluation and data set up is not known so a direct comparison is not possible. Notice that the performance of DTWC degrades gracefully as the number of examples in the training set is reduced.
DTWC uses a set of generative model parameters  X  the discriminative term weights  X  and | C |  X  1 discriminative model parameters  X  the slope  X  k and bias  X  0 . The weights are computed from the labeled training set by maximum likelihood estimation. This is a straightforward computa-tion requiring a single pass over the training set. The dis-criminative model parameters are learned by minimizing the classification error over the labeled training set. This selected test sets of size 4000.

Figure 2. Number of terms selected versus threshold t for Spam data set (DTWC-O) is a convex optimization problem, as empirically verified from the error versus slope parameter graph (Figure 4). The bias parameter, which is usually close to zero in our evalua-tions, can be determined after learning the slope parameter . The optimization problems can be solved efficiently by an iterative optimization technique or by grid search.
The threshold t can be used to trade-off DTWC X  X  space requirement and accuracy performance. This is evident from Figure 2 which shows the variation of the number of selected terms with threshold t for the Spam data set (us-ing DTWC-O). The number of selected terms drops signif-icantly with only a small increase in t . Remarkably, how-ever, the classification accuracy does not decrease drasti-cally (Figure 3). Table 4 shows the number of terms and the
Figure 3. Average accuracy versus threshold t for Spam data set (DTWC-O) average accuracy (averaged over the 3 inboxes) of DTWC-O for Spam data set. It is seen that even when the number of terms is reduced by one-eighth (from 40516 to 4913 terms) the average accuracy value for DTWC is still higher than the second best performer, i.e., naive Bayes. This result demon -strates the robustness and scalability of our algorithm, an d its suitability for application like personalized spam filt er-ing by e-mail service providers.
In this paper, we describe a new text classification method, named DTWC, based on discriminative term weighting, discrimination information aggregation, and l in-ear discrimination in a two-dimensional feature space. Eac h term in the classification problem is assigned a weight that
Figure 4. Classification error versus slope pa-rameter curve
Table 4. Selected terms and accuracy at dif-ferent values of threshold t for Spam data set (DTWC-O) quantifies the discrimination information it provides for c at-egory k over the others. These discriminative term weights are then used to transform the input term space into a two-dimensional feature space. The transformation is based on a statistical model of opinion pooling. Category k and the rest are discriminated in the feature space by a straight lin e. As such, the discriminative model has only two parameters, the slope and the bias of the line, which can be computed efficiently by an iterative optimization algorithm. DTWC is simple, efficient, effective, and robust. All these charact er-istics make it suitable for many application areas, includi ng personalized spam filtering where scalability and robustne ss are essential.

DTWC is evaluated on spam filtering, movie review, and simulaed/real/aviation/auto data sets. Its classificatio n ac-curacy is compared with that of four other classifiers  X  naive Bayes, maximum entropy, balanced winnow, and SVM. DTWC outperforms all classifiers in all settings. Its perfor -mance is substantially better in situations where the train ing and testing sets follow different distributions. We also di s-cuss the efficiency and robustness characteristics of DTWC by evaluating its performance against term selection. The first author gratefully acknowledges support from Lahore University of Management Sciences (LUMS) and Higher Education Commission (HEC) of Pakistan for this research.
 [1] E. Alpaydin. Introduction to Machine Learning . MIT [2] L.D. Baker and A. McCallum. Distributional cluster-[3] Steffen Bickel. Ecml/pkdd: Discovery challenge. [4] B. Bigi. Using kullback-leibler distance for text cate-[5] A.L. Blum and P. Langley. Selection of relevant fea-[6] I. Dagan, Y. Karov, and D. Roth. Mistake driven learn-[7] A. Dasgupta, P. Drineas, B. Harb, V. Josifovski, [8] Franca Debole and Fabrizio Sebastiani. Supervised [9] I.S. Dhillon, S. Mallela, and R. Kumar. A divisive [10] G. Druck, C. Pal, A. McCallum, and X. Zhu. [11] G. Forman. An extensive empirical study of feature [12] T.S. Jaakkola and D. Haussler. Exploiting generative [13] R.A. Jacobs. Methods for combining experts X  proba-[14] T. Joachims. Text categorization with support vector [15] T. Joachims. Making large-Scale SVM Learning Prac-[16] K.N. Junejo and A. Karim. Pssf: A novel statistical [17] A. Kolcz and W. Yih. Raising the baseline for high-[18] D.D. Lewis, Y. Yang, T.G. Rose, and F. Li. Rcv1: A [19] J. Liu, J.-Q. Song, and Y.-L. Huang. A genera-[20] D.G. Luenberger. Linear and Nonlinear Program-[21] A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-[22] A.K. McCallum. A machine learning for language [23] E. Montanes, I. Diaz, J. Ranilla, E.F. Combarro, and [24] K. Nigam, J. Lafferty, and A. McCallum. Using max-[25] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs [26] R. Raina, Y. Shen, and A.Y. Ng. Classification with [27] G. Salton and C. Buckley. Term weighting approaches [28] F. Sebastiani. Machine learning in automated text cat-[29] A.K. Seewald. An evaluation of naive bayes variants [30] L. Zhang, J. Zhu, and T. Yao. An evaluation of sta-
