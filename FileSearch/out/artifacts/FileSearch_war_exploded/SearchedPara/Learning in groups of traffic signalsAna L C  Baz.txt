 1. Introduction
Given the increasing demand for mobility in our society and the fact that it is not always possible to provide additional capacity, a more efficient use of the available transportation infrastructure is necessary. This issue relates closely to artificial intelligence (AI) and multiagent systems. AI and multiagent techniques have been used in many stages of the process of traffic management. In fact many actors in a transportation system fit very well the concept of an autonomous agent: the driver, the pedestrian, and the traffic light.

This paper focus on control strategies as defined in the control loop described by Papageorgiou (2003) (the physical network, its model, the model of demand and disturbances; control devices; surveillance devices; and the control strategy). Specifically, control strategies based on learning are emphasized. In order to discuss this relationship, the next section briefly reviews basic concepts on reinforcement learning and summarizes the current discussion on approaches for multiagent reinforcement learning (MARL).

Techniques and methods from control theory are applied in traffic control in a fine-grained way. This leads to the problem that those techniques can be applied only to single intersections or a small number of them. For any arbitrarily big network, the real-time solution of the control loop faces a number of apparently insurmountable difficulties ( Papageorgiou, 2003 ).
Similarly, the problems posed by many actors in a multiagent reinforcement learning scenario are inherently more complex than in single-agent reinforcement learning (SARL). These pro-blems arise mainly due to the fact that no agent lives in a vacuum (Littman, 1994). While one agent is trying to model the environment (other agents included), other agents are doing the same (or at least reacting). This results in an environment which is inherently non-stationary. Thus, the notions of convergence as previously known cannot any longer be guaranteed.

One popular formalism for MARL is the one based on stochastic games (SG), which is investigated by game theory and is an extension of the basic Markov decision processes (MDP). However, the aforementioned increase in complexity has some consequences for this formalization. First, the approaches proposed for the case of general sum SG require several assumptions regarding the game structure (agents X  knowledge, self-play, etc.). Also, it is rarely stated what agents must know in order to use a particular approach. Those assumptions restrain the convergence results to common payoff (team) games and other special cases such as zero-sum games. Moreover, the focus is normally put on two-agent games, and not infrequently, two-action stage games. Otherwise, an oracle is needed if one wants to deal with the problem of equilibrium selection when two or more equilibria exist. This can be implemented in several ways (e.g. using the concept of focal point) but it would probably require extra communication. Second, despite recent results on formalizing multiagent reinforcement learning using SG, these cannot be used for systems of many agents, if any flavor of joint-action is explicitly considered , unless the obligation of visiting all pairs of state-action is relaxed, which has impacts on the convergence. The problem with using a high number of agents happens mainly due to the exponential increase in the space of joint actions.
 Up to now, these issues have prevented the use of SG-based
MARL in real-world problems, unless simplifications are made, such as letting each agent learn individually through single-agent based approaches. It is well known that this approach is not effective since agents converge to sub-optimal states. Therefore, partitioning the problem into several, smaller multiagent systems may be a good compromise between complete distribution and complete centralization. This research line has also been tried by others: Boutilier et al. (1999) propose decomposition of actions, rewards and other components of an MDP. In Kok and Vlassis (2006) a sparse cooperative reinforcement learning algorithm is used in which local reward functions only depend on a subset of all possible states and variables.

In traffic networks, where each agent represents a traffic signal controlled junction, the problems mentioned above may prevent the use of MARL in control optimization since this is a typical many-agent system in which joint actions do matter. It is not clear whether one must address all agents of the network at once, e.g. in a single-agent learning effort. This seems not to be the case and practice in traffic engineering, even because the control using standard measures (see e.g. Hunt et al., 1981; Diakaki et al., 2003 ) is computationally demanding as well. Rather, there is a trend in traffic engineering towards decentralization of control, with the network being divided into smaller groups.
As shown later in this paper, even small groups pose a challenge for MARL as the number of joint actions and states grows exponentially.

Thus alternative approaches to cope with this increase are necessary. Here we propose the use of the standard individual agent-based reinforcement learning approach, with the addition of a supervisor that observes these individuals for a time period, collects joint actions and records their rewards in a kind of case base, where a case is the set of states agents find themselves in, plus the rewards they get; the solution of the case is the joint action. After that period is over, the supervisor observes the state in which the agents it controls are in, looks for the best reward it has observed so far when agents were in that same joint state, and recommends a joint action. This way, the present paper addresses a many-agent system in which these are divided into groups that are then supervised by further agents that have an overview of the group X  X  performance.

The paper is organized as follows. In Section 2 some approaches to distributed traffic signal control, which are based on AI and multiagent systems are discussed. Section 3 focuses on multiagent learning. In Sections 4 and 5 our approach is introduced; experiments and results are discussed. A conclusion and future directions are presented in Section 6. 2. AI and MAS based approaches to distributed traffic signal control
It is beyond the scope of this paper to review the extensive literature on traffic signals control. The reader is referred to Bazzan (2009) for a survey on opportunities for learning and multiagent systems in traffic control. We notice however that classical approaches, e.g. based on optimization via hill climbing (e.g. Robertson, 1969 ), signal synchronization and actuated control (e.g. Hunt et al., 1981 ) can be combined within the hierarchical approach proposed here in Section 4 in the sense that those forms of control can be encapsulated in agents which could then be supervised by another agent. The latter has a broader view and can collect instances of joint control in order to make future recommendations for the agents it supervises.

In what follows we review some traffic signal control approaches proposed within the AI community in order to situate the reader and allow some comparisons at a qualitative level. Several AI and multiagent system based approaches to traffic modeling have been proposed. However, the focus of these approaches has usually been on the management level. On the other hand, our work focuses on a fine-grained level or traffic flow control via traffic signals. For an overview on other management and control measures, as well as open challenges, see Bazzan (2009) . There is again an expressive number of publications reporting applications of various AI techniques to traffic control, such as genetic algorithms and fuzzy inference. We do not include all of them here because our focus is on distributed and/or decentralized control of traffic signals. However, for complete-ness, some references for readers that are interested in those forms of AI based control are included next.
 A reservation-based intersection control is proposed in Dresner and Stone (2004) for a simplified version of intersections without conventional traffic signals designed for dealing with autonomous guided vehicles. Balan and Luke (2006) propose history-based controllers intended to provide global fairness, reducing the variance in waiting time. Both are expected to work only with future technologies related to autonomous vehicles and auctions among intersections. Next we review some distributed approaches to decentralized control of traffic signals in more detail.

A simple stage game is discussed in Bazzan (2005) for synchronization of traffic signals. Interactions are modeled as coordination games where the highest reward is given when neighbor traffic signals coordinate their actions so that they synchronize their green phases. Different signal plans can be selected in order to coordinate in a given traffic direction or during a pre-defined period of the day. This approach uses techniques of evolutionary game theory: self-interested agents receive a reward or a penalty given by the environment. Moreover, each agent possesses only information about its local traffic state. However, payoff matrices (or at least the utilities and preferences of the agents) are required, i.e. these figures have to be explicitly formalized by the designer of the system.
In Oliveira et al. (2005) the formation of groups of traffic signals was considered in an application of a technique of distributed constraint optimization called cooperative mediation. However, in this case the mediation itself is not decentralized: group mediators communicated their decisions to the mediated agents in their groups and these agents just carried out the tasks. Also, the mediation process could require a long time in highly constrained scenarios, imposing a negative impact in the coordination mechanism. For these reasons Oliveira and Bazzan (2006) proposes a decentralized and swarm-based model of task (signal plan) allocation in which the dynamic group formation, without mediation, combines the advantages of decentralization via swarm intelligence and dynamic group formation.

Camponogara and Kraus (2003) have studied a simple scenario with only two intersections, using stochastic game theory and reinforcement learning. With this approach, their results were better than a best-effort (greedy) technique, better than a random policy, and also better than Q -learning. Also, in
Nunes and Oliveira (2004) a set of different techniques were applied in order to try to improve the learning ability of agents in a simple scenario.

A hierarchical multiagent system with three levels is proposed in France and Ghorbani (2003) . On the first level, local traffic agents (LTAs) represent intersections. Because the local optimum may not be a good one when observed from another perspective, there is a second level in the system in which a coordinator traffic agent (CTA) supervises a few LTAs. The paper does not address the important issue of how to resolve conflicts among the LTAs though.

To address the issue of the highly dynamic and non-stationary nature of flow patterns, one solution would be to keep multiple models of the environment (and their respective policies). In Silva et al. (2006a) , a technique is proposed that allows for automatic partitioning of the environment dynamics into relevant partial models, thus using model-based reinforcement learning. However the approach is single-agent and an extension is necessary to deal with joint states and joint actions.

Wiering (2000) describes the use of reinforcement learning by traffic signal agents in order to minimize the overall waiting time of vehicles in a small grid. Those agents learn a value function that estimates the expected waiting times of vehicles given different settings of traffic signals. One interesting issue tackled in this research is that a kind of co-learning is considered: the value functions are learned not only by the traffic signals, but also by the vehicles that can thus calculate policies to select optimal routes to the respective destinations. The ideas and some of the results presented in this paper are important. However, the necessary communication for knowledge formation has a high cost. Also, there is no account of experience made by the drivers based on their local experiences only.

In summary, the state-of-the-art regarding traffic light con-troller is as follows. Classical approaches from traffic engineering are fine-grained but in general cannot deal with a high number of intersections in real-time, especially in non-trivial, non-arterial-based topologies. AI based approaches normally address the coarse-grained, management level. Moreover, some are not yet ready for immediate deployment as the technology required is either expensive or non-existing. Few works do deal with control in fine-grained level but either these cannot handle many intersections or require a lot of communication, or do not solve conflicts. 3. Multiagent learning 3.1. Single agent reinforcement learning Usually, reinforcement learning (RL) problems are modeled as
Markov decision processes (MDPs). These are described by a set of states, S , a set of actions, A , a reward function R  X  s ; a  X  probabilistic state transition function T  X  s ; a ; s 0  X  s , performed action a and ended up in s 0 with reward r . The goal of an MDP is to calculate the optimal policy p , which is a mapping from states to actions such that the discounted future reward is maximized.

Q -learning is a model-free approach to reinforcement learning that does not require the agent to have access to information about how the environment works. Q -learning works by estimat-ing state-action values, the Q -values, which are numerical estimators of quality for a given pair of state and action.
More precisely, a Q -value Q  X  s ; a  X  represents the maximum discounted sum of future rewards an agent can expect to receive if it starts in state s , chooses action a and then continues to follow an optimal policy. Q -learning algorithm approximates Q  X  s ; a  X  as the agent acts in a given environment. The update rule for each experience tuple / s ; a ; s 0 ; r S is given by Eq. (1) where a is the learning rate and g is the discount for future rewards.
If all pairs state-action are visited during the learning process, then Q -learning is guaranteed to converge to the correct Q -values with probability one ( Watkins and Dayan, 1992 ). When the
Q -values have nearly converged to their optimal values, the action with the highest Q -value for the current state can be selected. During the learning process, the trade-off between exploitation versus exploration has to be considered ( Kaelbling et al., 1996)
Q  X  s ; a  X   X  Q  X  s ; a  X  X  a r  X  g max 3.2. Multiagent reinforcement learning: stochastic games
Learning in systems with two or more players has a long history in game-theory. The connection between multiagent systems and game-theory as to what regards learning has been explored as well at least from the 1990s. Thus, it seems natural to the reinforcement learning community to explore the existing formalisms behind stochastic (Markov) games (SG) as an exten-sion for MDPs.

Most of the research on SG-based MARL so far has been based on a static, two-agent stage game (i.e. a repeated game) with common payoff (payoff is the same for both agents), and with few actions available as in Claus and Boutilier (1998) . The zero-sum case was discussed in Littman (1994) and attempts of general-izations to general-sum SG appeared in Hu and Wellman (1998), among many others (as a comprehensive description is not possible here, the reader is referred to Shoham et al. (2007) and references therein).

The multiagent reinforcement learning problem can also be approached using a formalism that considers not individual states and actions as above, but joint states and/or actions, for any number of agents. In this case the formalism to model this is a stochastic game or multiagent Markov decision process (SG or MMDP), a generalization of a MDP for n agents. An n -agent SG is a tuple  X  N ; S ; A ; R ; T  X  where: N  X  1 ; ... ; i ; ... n is the set of agents.
 S is the discrete state space (set of n -agent stage games). A  X  A i is the discrete action space (set of joint actions).
R i is the reward function ( R determines the payoff for agent i as r i : S A 1 A k -R ).

T is the transition probability map (set of probability distributions over the state space S ).
 If all agents keep mappings of their joint states and actions, this implies that each agent needs to maintain tables whose sizes are exponential in the number of agents: j S 1 jj S n j j A jj A n j . This is hard even in the case of single state game where j S j X  1. For example, assuming that agents playing the repeated game have only two actions, the size of the tables is . Therefore one possible approach is to partition the agents in order to decrease j N j . Even after this partition, it is necessary to redefine Bellman X  X  equations for the multiagent environment. For instance, using Q -learning the problem is how to update the Q values that now also depend on other agents: Q i  X  s ; ~ a  X   X  1-agent i and V i  X  s 0  X   X  max ~ a A A Q i  X  s 0 ; ~ a  X  ).
Thus, the alternative approach proposed here is to not only partition agents in groups but also let them use joint actions only when this has proven to be efficient. In order to do this we introduce a supervisor that is in charge of suggesting joint actions that it has recorded as good ones in the past. 4. Supervised learning based approach 4.1. Overview
The supervised learning control strategy that is proposed here, when applied to traffic controllers, is composed of two kinds of agents. Low-level, local agents control, each, a junction. Besides, hierarchically superior agents (supervisors or tutors) are in charge of controlling groups containing a small number of low-level agents. Control here is used in a relaxed way. Supervisors in fact must be seen as facilitators or tutors that will observe the traffic situation from a broader perspective and recommend actions to low-level agents in their groups. This recommendation will be made based on a group perspective, in opposition to the purely local perspective low-level agents have. Details of this process are given in Section 4.3.
 Three traffic signal agents per group are used in this paper.
Notice that the number of agents in each group determines the size of search space for the learning algorithm. Therefore small groups are preferred as the combinatorial space j S 1 jj S n j j A jj A n j can be probed within a relatively small time frame. The higher this space, the longer the supervisor agent must observe in order to collect information and be able to do useful recommendations. 4.2. Individual agents at low level
Low-level agents communicate with the traffic simulator infrastructure (described in Section 5.1) in order to get informa-tion about the state of the road portions they are controlling, as well as to send control actions back to the simulator. This communication can be as frequent as desired but here we let agents decide about an action with intervals of d a  X  60s. Thus the simulator pools the signal agents each d a seconds and requests an action to be performed. In order to return this information to the simulator, agents decide which action to perform next using a reinforcement learning scheme.

To do the necessary calculations, agents must have informa-tion about the state of the their links in terms of number of stopped vehicles in each link (an average over the last d
The agent then compares the load in the approaching links. It is assumed that links are one way and that each junction is a cross-type junction so that there are only two approaching links namely vertical and horizontal. However this can be easily generalized to a more complex geometry of junctions. That comparison is then discretized in j S j states.

In our case, three states are generated: state 1 if the vertical load is higher than the horizontal; state 2 if the vertical load is lower than the horizontal one; and state 0 if vertical load is nearly equal to the horizontal one. Nearly equal here means e -equal where e  X  20 % was used. Notice that this coarse discretization of states aims at preventing a combinatorial explosion in the number of pairs / state ; action S , which would demand a long experimentation stage, especially when it comes to the supervisor which has to consider not only those three states but actually the size of its group to the number of states, i.e. 3 3 here. In traffic domains that are highly dynamic, the issue of how fast agents react is an important one.

Regarding the number of actions, each agent has three possible actions: to run signal plan 0, signal plan 1 and signal plan 2. All three plans have cycle time equal to 60s but different splits. Plan 0 allows both traffic directions the same green time (30-30). Plan 2 allows 70% of the cycle (42) to the vertical direction and 30% (18s) to the horizontal one. Plan 1 does the opposite.
Such plans are designed following traffic engineering manuals that not only specify the green time for a given topology and load at the junction, but also comply with safety issues. For instance, if plan 0 is active in the controller of a given junction, lights at one direction of this junction (say, horizontal) are red from time step 1 to 30 while lights in the other direction are green. From time step 31 to 60 the opposite happens i.e. lights at the vertical direction are red. Because an action may change the plan that is active in the controller only at given time steps (see discussion on d before), a change to plan 2 for instance ensures that at time step 61 lights in the vertical direction will receive green (in the case of plan 2 this happens for further 42s) while lights in the horizontal direction are red.

Given the states and actions described before, a greedy strategy would be just to associate each state to one action in the obvious way, i.e. state 0 with plan 0, state 1 with plan 2, and state 2 with plan 1. However because agents are not alone, this strategy is likely to fail. Other agents act in this environment and their actions are highly coupled. One obvious coupling is the interaction between upstream and downstream traffic signals which is, again, not trivial because each agent has more than one neighbor (i.e. two in the vertical and two in the horizontal direction).

The reward agents got is inversely proportional to the average number of stopped vehicles they observe over their approaching links, normalized to remain between 0 and 1.

The learning strategy used here is the basic Q -learning equation presented in Section 3 (Eq. (1)). We refer to this as individual learning and it is described in Algorithm 1. Simulations were run where agents only use this algorithm in order to compare the performance of the system to the one it has when individual agents are supervised (described in the next section).
 Algorithm 1. Plain individual learning. 1: for all j A N do 2: initialize Q values, list of neighbors 3: while not end of simulation do 4: when in state s j , select action a j with probability 5: observe reward r j 6: Q  X  s j ; a j  X   X  Q  X  s j ; a j  X  X  a  X  r j  X  g max a 0 7: end while 8: end for 4.3. Supervising individual agents Supervised learning works as formalized in Algorithms 2 X 5. Algorithm 2 describes the input of parameters. The main ones are the set of low level agents L ; the set S of supervisor agents; D (time period during which each L j A L learns and acts indepen-dently, updating the Q table Q ind j ); D tut (time period during which each S i A S recommends an action to each L j based on cases observed so far); D crit (time period during which each L independently or follow rule of supervisor); a the learning rate; and the discount rate g . As mentioned in the three stages the low-level individual agents (those described in the previous section) are tutored by supervisor agents; each supervisor is associated with a group of low-level agents. The task of the supervisor is, initially, to observe states, actions, and rewards of the low-level agents and record this information in a case base. Later, in stages 2 and 3 this information is used to guide actions of the low-level agents.

Algorithm 2. Supervised learning: input parameters. 1: input set S of supervisor agents (set of S i  X  X ) 2: input set L of local agents (set of L j  X  X ), one for each S 3: input D ind : time period during which each L j learns and acts independently, updating the Q table  X  Q ind j  X  ; each S i and records state and action of each of its L j  X  X , plus the average reward among the L j 4: input D tut : time period during which each S i recommends an action to each L j based on cases observed so far 5: input D crit : time period during which each L j can act independently or follow rule of supervisor; S i records new training instances if at least one L j does not follow S i prescription 6: input a : learning rate 7: input g : discount rate 8: t  X  0: current time period 9:
Algorithm 3. Supervised learning (cont.): individual learning stage (stage 1). 1: while t r D ind do 2: for all L j A L do 3: when in state s j , select action a j with probability 5: for all S i A S do 6: observe state, action, and reward for each L j 7: compute the average reward r (among L j  X  X ) 8: if tuple / ~ a t 9: add tuple / ~ a t 10: else 11: r  X  a r  X  X  1-a  X  r old 12: add tuple / ~ a t 13: end if 14: end for 15: end for 16: end while
Stage 1 is described in Algorithm 3. Each low-level agent j uses basic Q -learning to select an action (line 3) and update the Q table (line 4). Each supervisor i just observes the low-level agents and collects information to a base of cases (line 6). This information consists of joint states, joint actions, and rewards. Thus the base of supervised agents (line 7). Besides, if one tuple already exists in the base, the corresponding reward is calculated by considering both the old value ( r old ) as well as the newest observed value as shown in line 11. This stage takes D ind time steps.

Algorithm 4. Supervised learning (cont.): tutoring stage (stage 2). 1: while t r  X  D ind  X  D tut  X  do 2: for all S i A S do 3: given ~ s t 4: end for 5: for all L j A L do 6: perform action a j communicated by supervisor or 8: end for 9: for all S i A S do 10: observe state, action, and reward for each L j 11: compute the average reward r (among L j  X  X ) 12: if tuple / ~ a t 13: add tuple / ~ a t 14: else 15: r  X  a r  X  X  1-a  X  r old 16: add tuple / ~ a t 17: end if 18: end for 19: end while At the second stage (see Algorithm 4), which takes further D time steps, low-level agents stop learning individually and follow the joint action the supervisor finds in its base of cases.
Supervisors do as in stage 1. It is important to note that in any case the local Q tables continue to be updated (line 7). In order to find an appropriated case, the supervisor observes the states the low-level agents are in and retrieves a set of actions that yielded the best reward when agents were in those states in the past (line 3). This reward is also communicated to the agents so that they can compare this reward, which is the one the supervisor expects, with the expected Q values and with the actual reward they get when performing the recommended action. However, at this stage, even if the expected reward is not as good as the expected Q values, low-level agent cannot refuse to do the action recommended by the supervisor. If the supervisor does not have a case that relates to that particular joint state, then the low-level agents receive no recommendation of action and select one independently (locally) using their local policies. In this case, the supervisor is able again to observe and record this new case (lines 12 and 13).

At the third stage (that takes D crit , see Algorithm 5) the low-level agents must not follow the recommended action.
Rather, after comparing the expected reward that was commu-nicated by the supervisor with the expected Q value (lines 7 and 8), the agent can decide to carry out the action associated with its local Q value (line 11). This means that the low-level agent will only select the recommended action if this is at least as good as the expected Q value plus a tolerance t as indicated in line 7 (see details when we explain the experiments). No matter whether the low-level agents do follow the prescription or not, the supervisor is able to observe the states, actions, and rewards and thus form a new case (or update an existing one) as indicated in lines 16 X 22. During D crit , each supervisor S i records new training instances if at least one L j does not follow S i prescription).

Algorithm 5. Supervised learning (cont.): critique stage (stage 3). 1: while t r D ind  X  D tut  X  D crit do 2: for all S i A S do 3: given ~ s t 4: end for 5: for all L j A L do 6: { //compare Q ind j and r e : } 7: if r e  X  1  X  t  X  4 Q ind j then 8: perform a p j { //where a p j is action recommended by 9: update Q ind j 10: else 11: perform a ind { //where a ind is selected locally; in this 12: update Q ind j 13: end if 14: end for 15: for all S i A S do 16: observe state, action, and reward for each L j 17: compute the average reward r (among L j  X  X ) 18: if tuple / ~ a t 19: add tuple / ~ a t 20: else 21: r  X  a r  X  X  1-a  X  r old 22: add tuple / ~ a t 23: end if 24: end for 25: end while 5. Experimental validation 5.1. Simulation infrastructure: ITSUMO
The simulations described in this section were performed using ITSUMO ( Silva et al., 2006b) which is a microscopic traffic simulator based on the Nagel X  X chreckenberg cellular automaton (CA) model ( Nagel and Schreckenberg, 1992 ). In short, each road is divided into cells with fixed length. This allows the representa-tion of a road as an array where vehicles occupy discrete positions. Each vehicle travels with a speed based on the number of cells it may advance without hitting another vehicle. The vehicle behavior is expressed by rules that represent a special form of car-following behavior. In this model there is a randomization factor (vehicles decelerate with probability p )in order to simulate the non-deterministic dynamics of vehicle movement. Obviously, the higher p , the higher the number of vehicles in the network at each given instant. Thus, we have used that parameter p in our simulations as a way to increase the number of vehicles in the network (over the basic input flow) and hence test the efficiency of our approach.

ITSUMO is composed of several modules: data module, simulation kernel, driver definition module, traffic signals agent module, and the visualization module. The more relevant to the present paper is the traffic signal agent module that controls the lights. Agents can control one or more intersections. For the approach discussed in the present paper we have incorporated the supervisor agent. 5.2. Experiments and results
In order to run experiments to evaluate the performance of the supervised learning, ITSUMO was also used to create the topology of the network as depicted in Fig. 1 . In this figure one sees a grid network composed of 64 nodes (denoted A1 to H8). Between each two nodes there is a one-way link. 2 Border nodes are either sinks (lighter or red node at borders) or sources (darker or blue nodes at borders). Roads are defined as a set of links, connecting one source with one sink in one of two possible directions. Hence we have road A, road B, etc. in vertical directions, and road 1, road 2, etc. in the horizontal directions.

ITSUMO was run with the following setting: vehicles are inserted at the beginning of each road with a given probability. In the vertical roads, this probability is 1/3 meaning that one vehicle is inserted, on average, at each 3 time steps; this probability is 0.1 in the horizontal roads. We keep these probabilities constant during the simulation because we are primarily interested in the comparison of supervised versus individual learning and not in the performance of the individual learners.

Keeping constant insertion rates does not mean that the environment is deterministic. There are at least three sources of stochasticity. First (and mainly) individual traffic signal agents are learning independently (at least in stage 1 of the algorithm) which is the main problem in multiagent learning as local agents are trying to maximize their rewards in a uncoordinated way (see Section 3). Second, vehicles can decelerate with probability p ,a feature of the cellular automata model (see Section 5.1) which may cause unexpected jams. Finally, drivers are able to turn in each junction, although this is likely to play a minor role here as we keep this probability artificially low (1%) in order to focus on the performance of individual learning versus supervised learn-ing. We remark however that when this probability is higher the main trends do not change although the time for convergence of the plots increases. Therefore, this setting is far from well behaved and greedy strategies by the traffic signals (e.g. give priority to the more congested direction) are not likely to work. For the same reason the fixed strategy of always running signal plan 1 (which gives priority to the vertical direction which has more traffic) is also not a good decision. The simulator was run for 8000 time steps. If the values of D ind , D tut and D crit change then the simulation time must change too.

In all cases discussed below the total number of stopped vehicles was measured, i.e. the sum of vehicles stopped in all lanes over time. All simulations were repeated 10 times. Because the plots are running averages of the 8000 steps we do not show the deviations in the plots but notice that they are inferior to 10%. Table 2 shows averages and standard deviations for learning stages 1, 2, and 3 for each curve of the graphs depicted in Figs. 2 and 3 .

Values for the main parameters used in the simulation are given in Table 1 . A series of experiments was performed to evaluate the approach. First there was no supervision and low-level agents just run the standard Q -learning algorithm with learning rate a  X  0 : 5 and discount rate g  X  0 : 0 in an individual way. No discounting is used here as reward values for both local and supervisor agents must be comparable; supervisors do not use discount values in the algorithm, only learning values. The value of a  X  0 : 5 was selected to put equal probability on new and past Q values. This is necessary in dynamic environments such as the one described here.
The performance of the individual learning scheme can be seen in Figs. 2 and 3 (dashed line). As one notes, the performance is good in the beginning (time necessary for the network to saturate) with a lower number of stopped vehicles. However this changes with time and there are peaks of more than 1000 stopped vehicles.

We then run simulations with supervised learning. The 12 supervisors are in charge of three low-level agents each. We let low-level agents at the borders of the grid to run greedy strategies because these are more efficient when agents are close to the sources (where the vehicles are inserted or removed). This means that 36 low-level agents are supervised. This is depicted in Fig. 1 .
As mentioned before, in stage 3, which starts at time 5000, low-level agents may refuse to do the action recommended by the supervisor. In order to decide whether or not to refuse, a low-level agent compares the reward the supervisor is expecting with the value of the Q table for the current state. The comparison between the local expected Q value and the expected reward commu-nicated by the supervisor has a tolerance factor t . This tolerance means that when a supervisor says that the expected reward is r e and the low-level agent expects a reward Q j , it will accept the action proposed by the supervisor if it is nearly as good as Q Algorithm 5, line 7, where the tolerance t is considered).
Experiments with different values of t were performed in order to see whether this makes a difference. When t  X  0 the agent has no tolerance: it will only carry out the action recommended by the supervisor if this is strictly higher than the one it expects from its local learning process. Here the results for t  X  0 : 01 and 0 : 1 are shown. Further two values of p were tested: p  X  0 : 05 and 0 : 1. Given our experience, higher values of p are irrealistic: when p  X  0 : 1 this means that there is a probability of 10% of each driver decelerating at each time step.
The results of these experiments are depicted in Fig. 2 (for p  X  0 : 05) and in Fig. 3 (for p  X  0 : 1). One can see that up to time step 5000 the behavior of the curves that refer to the supervised learning is nearly the same because the tolerance t is only used in stage 3. In all cases the number of stopped vehicles is lower than when the agents use only individual learning. Increasing p has the effect of yielding more jams. This is reflected in Fig. 3 : the number of stopped vehicles is higher.
 In any case, one may conclude that supervision does pay off.
While the agents are being strictly supervised (they must carry out the joint action recommended, which is the case of stage 2), the number of stopped vehicles is lower than in the case they learn independently. In stage 3 (after time step 5000) the best performance (i.e. lesser number of stopped vehicles) is achieved when t  X  0 : 01 i.e. low-level agents only carry out the action proposed by the supervisor is almost as good as the values they expect if they use their Q tables. This has a consequence that they act in a coordinated way and carry out a joint action that was the one intended by the supervisor after searching its case base.
When low-level agents are more tolerant there is a tendency to perform the joint action, even when it is not completely appropriate.

A final remark here is that this problem of multiagent learning in a group of three traffic signal agents already poses computa-tional challenges if tackled as a standard joint learning problem. To see this we return to the discussion at the end of Section 3.
Considering the size of the learning problems we address within the groups, if there were no supervisor and the three agents were to keep mappings of their joint states and actions, this would imply that each agent needs to maintain tables whose sizes are: j S jj S n jj A 1 jj A n j . Given that n  X  3, j S j X  3 and j A j X  3, the size of the tables is 3 3 3 3 for each agent. In the supervised learning it is not always the case that the supervisor will necessarily see all these combinations; however for those seen, it is able to keep records of previous rewards and it is then able to make recommendations. 6. Conclusion
Multi-agent reinforcement learning is inherently more com-plex than single agent reinforcement learning because while one agent is trying to model the environment, other agents are doing the same. This results in an environment that is non-stationary.
MDP-based formalisms extended to multiagent environments (MMDP) is an efficient solution only in case of few agents, few states, and few actions. For a large number of agents, alternative solutions are necessary. In this paper the problem is partitioned into several, smaller multiagent systems.

Our approach is to have agents divided into groups that are then supervised by further agents; these have a broader view, even it is not detailed or up-to-date. A supervised learning with three stages was proposed: in the first the supervisor only collects information about states, actions, and rewards received by the agents it supervises, storing this in a case base. In a second stage, the supervisor retrieves the best case for a given state and recommends actions for the low-level agents. These must carry out these recommended joint actions. In a third stage low-level agents still receive suggestions but must not carry them out if they have a better action to select. During all the times each supervisor updates its case base, and the low-level agents update their Q tables.
 This approach was applied to a scenario of traffic lights control. As mentioned at the end of Section 2 existing classical or AI based approaches all have drawbacks. In particular, learning-based approaches are promising (see e.g. Bazzan, 2009; Camponogara and Kraus, 2003; Silva et al., 2006a; Wiering, 2000) but so far cannot deal with the exponential growth in the space of pairs state-action, especially if joint actions are explicitly considered. Therefore our partition and supervision based approach is a good compromise between complete distribution and complete cen-tralization.

Our results show that supervision pays off in the traffic control scenario regarding the number of stopped vehicles. When there is no supervision, low-level agents just learn using individual Q -learning, which does not explicitly take joint actions into account. In this case the number of stopped vehicles is higher meaning that the signal plans selected are not the best.
In the future an investigation is planned about the relation between the number of times that a suggestion was or was not followed and the reward obtained along time. We also want to implement further levels of supervision (a kind of hierarchical learning). Finally we are currently working in a different way to compare rewards at stage 3 in order to be able to use discounting in Q -learning.
 Acknowledgements Ana Bazzan is partially supported by CNPq and by the Alexander v. Humboldt Foundation; Denise de Oliveira was partially supported by CAPES and CNPq.
 References
