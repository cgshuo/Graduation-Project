 Context-aware recommender systems (CARS) take contextual con-ditions into account when providing item recommendations. In recent years, context-aware matrix factorization (CAMF) has e-merged as an extension of the matrix factorization technique that also incorporates contextual conditions. In this paper, we intro-duce another matrix factorization approach for contextual recom-mendations, the contextual SLIM (CSLIM) recommendation ap-proach. It is derived from the sparse linear method (SLIM) which was designed for Top-N recommendations in traditional recom-mender systems. Based on the experimental evaluations over sev-eral context-aware data sets, we demonstrate that CLSIM can be an effective approach for context-aware recommendations, in many cases outperforming state-of-the-art CARS algorithms in the Top-N recommendation task.
 H.3.3 [ Information Search and Retrieval ]: Information fi ltering Recommendation; Context; Context-aware recommendation; SLIM; Matrix Factorization Context-aware recommender systems (CARS) are an extension of traditional recommender systems (RS) that adapt their recommen-dations to users X  speci fi c situations. The fundamental assumption of CARS is that a rating for an item is a function not just of the user and the item but also of the context in which the item is evaluated or used. A user X  X  preferences on a given item may vary from con-text to context. For example, companion is an in fl uential contextual variable in the movie domain; you may make a different decision in selecting movies if you intend to see the movie on a date, than if you plan on watching a movie with children.

Recent work has demonstrated that matrix factorization (MF) is an effective approach in recommendation. Extensions of stan-dard matrix factorization approach have incorporated contextual variables, such as time-aware MF. Karatzoglou et al. [3] proposed to use tensor factorization (TF) to integrate context in recommen-dation. Their approach assumed that contexts are independent of other dimensions. But the computation cost in TF is prohibitive, increasing exponentially as the number of contextual variables in-creases. Another approach, context-aware matrix factorization (CAM-F) [2] was developed to adapt to contextual recommendations by modeling contextual dependencies with the user or item dimen-sions. CAMF is based on traditional MF [4], where both users and items are represented by factor weightings in a matrix. Those CARS algorithms based on MF have been demonstrated to outper-form the ones based on neighborhood collaborative fi ltering (e.g. differential context modeling [9, 10]).

Most recently, a new matrix factorization for traditional RS, called sparse linear method (SLIM) [5] was proposed and shown to be more accurate than the state-of-the-art MF algorithms in the top-recommendation task. In this paper, we extend this work to create a contextual SLIM (CSLIM) model incorporating contextual con-ditions. We compare the performance of our proposed approach-es to the state-of-the-art context-aware recommendation algorithms through extensive empirical evaluations on multiple data sets. Assume that there are M users, N items, and let us denote the associated 2-dimensional rating matrix by R .Weuse u i to denote user i and t j to denote the item j .Anentry, R i,j ,inmatrix represents u i  X  X  rating on t j . A row vector in R representing ratings over all items is denoted by R i, : ,and R : ,j denotes a column vector in R representing ratings on item t j from all users. Sparse linear method (SLIM) is an approach designed for top-N recommendations. It improves upon the traditional item-based K -nearest neighbor (ItemKNN) collaborative fi ltering by learning, directly from the data, a sparse matrix of aggregation coef fi cients that are analogous to the traditi onal item-item similarities [5].
In SLIM, the ranking score for user u i on item t j is represented by
S i,j , which is calculated by a sparse aggregation of the ratings on the other items that have been rated by u i . We denote this ap-proach as SLIM-I, since it assigns a N by N non-negative matrix W to represent the aggregation coef fi cients between each two item-s. Accordingly, S i,j can be estimated by Equation 1.
 An example of how the SLIM-I approach works is provided in Figure 1. To estimate the ranking score of u 2 on item t 1 extracts u 2  X  X  ratings on the other items, and aggregates the ranking score by multiplying of those ra tings by the corresponding coef fi -cients between the item t 1 and t h (i.e. t h belongs to the set of other items u i has rated) in matrix W . The estimated ranking score will be used to rank the items to fi nally provide top-N recommenda-tions. Similarly, a SLIM-U approach can be easily derived, where S i,j is calculated by a sparse aggregation of ratings on this specif-ic item t j but were given by other users, where W is a M non-negative matrix representing the coef fi cients between every t-wo users. A similar example can be viewed in Figure 2.

Traditional matrix factorization techniques, such as those pro-posed by Koren et al. [4], usually represent users and items by a set of weights on individual latent factors, and both the number of latent factors and training iterations are required to tune the mod-el and fi nd the optimal solution. In contrast to those approaches, the sizes of matrices R and W are fi xed and there is only a single matrix, W , required to be learned in SLIM-I and SLIM-U. Analogously to context-aware matrix factorization techniques [2], we propose to incorporate context into the SLIM approach for top-N recommendations. We refer to this as the contextual SLIM (C-SLIM) approach. In the following, we use contextual factor to refer to the contextual variables such as "Time", "Location", and "Com-panion". The term contextual condition refers to a speci fi cvalue in a contextual factor. For example, "weekend" and "weekday" are two contextual conditions for the "Time" contextual factor.
Assume there are F contextual factors and L contextual condi-tions in total in a context-aware data set, in addition to the and N items. The primary task in the CSLIM approach is to es-timate the ranking score S i,j,c for user u i on item t j We use a binary vector, c =&lt; c 1 ,c 2 , ..., c L &gt;, to denote the contex-tual situation. For example, assume all contextual conditions with L =4 can be represent by { Time= weekend ,Time= weekday , Loca-tion= school , Location= home } . Then, the vector c = &lt;1, 0, 1, 0&gt; indicates that the current contextual situation is { Time= weekend , Location= school } .

In CARS, users X  preferences may vary from context to context for same item. Therefore, it is necessary to make predictions based on rating pro fi les in the same context c . Recall that the estimated ranking score matrix S in SLIM-I is derived based upon the idea in ItemKNN  X  aggregation of users X  ratings on other items. As a result, it is possible to estimate the ranking score S i,j,c aggregation of u i  X  ratings on other items in the same contexts However, a context-aware data set, with multiple ratings for each item, is usually very sparse  X  it is not guaranteed that a user has rated other items in the same context c . In this case, we estimate u  X  X  rating on an item t j in context c (i.e. R i,j,c ) based on the user X  X  non-contextual rating on this item (i.e. R i,j , ratings without con-sidering contexts) and the aggregated contextual rating deviations (CRD, i.e., the rating deviations in different contextual conditions). Accordingly, we can build different CSLIM models based on how to estimate the CRD. In this section, we present three CSLIM-I models based on how to estimate the CRD. First, we estimate contextual ratings (i.e. based on the non contextual rating R i,j and aggregated CRD. Specif-ically, we model CRD as the contextual rating deviations on item-s. In other words, we assume there is a rating deviation for each &lt;item, context condition&gt; pair. Thus, the predicted contextual rat-ing R i,j,c can be computed as follows: where R is the 2D non-contextual rating matrix. Typically, users may provide non-contextual rati ngs in addition to the ratings for an item in given contexts. But it is not necessary that both non-contextual ratings and contextual ratings are assigned to a same &lt;user, item&gt; entry. The average rating of each user on each item rated within multiple contexts can be added to the rating matrix if there is no knowledge about the non-contextual ratings.
We build a N  X  L CRD matrix D where each row represents an item, and each column represents an individual contextual condi-tion as shown in Figure 3. Thus, D j, : is a row in D representing CRD for the item t j in L different contextual conditions.
We can then use the SLIM-I approach to estimate the ranking score S i,j,c for user u i on item t j in contexts c . This is described in Equation 3, where t h belongs to the set of items for which user u has provided non-contextual ratings (i.e. R i,h &gt; 0). The ranking score is estimated by an aggregation of user X  X  ratings on other items in the same context c . We call this approach CSLIM-I-CI because the CRD are assumed for each &lt;item, context condition&gt; pair, and D is built as a CI matrix (i.e. columns and rows represent contexts and items respectively).

It is important to note that we set h = j to avoid the model learn-ing from u i  X  X  non-contextual rating on t j . Generally, the CSLIM-I-CI model will learn the parameters in D and W based on each entry of known contextual ratings R i,j,c in the training set, and make predictions only relying on three matrices: non-contextual rating matrix R , CRD matrix D , and the aggregation coef fi cien-tmatrix W . Using squared error as the optimization criteria, the loss function with regularization terms can be described in Equa-tion 4, which can be solved by stochastic gradient descent (SGD) approach.  X  and  X  parameters are the learning rates. Both (e.g. W 2 F )and 1 terms (e.g. W 1 ) are included, where the regularization term is usually applied for sparse models.
Minimize D,W
In CSLIM-I-CI, D is modeled as a CI matrix, where the devi-ation is computed for each &lt;item, context condition&gt; pair. The deviations can also be considered for users  X  we assume there is a rating deviation for each &lt;user, context condition&gt; pair, where D results in a CU matrix (i.e. columns denote contexts and rows represent users). The latter mode l is called CSLIM-I-CU. In addi-tion to these two approaches, the rating deviations can also simply be viewed as the deviations only associated with each contextual condition rather than as being paired with users or items. In this case, the CRD can be represented by a L -length vector d instead of a matrix D . We call this approach CSLIM-I-C. Note that the only difference among those three CSLIM-I models is the matrix D , while the matrix W is still the same  X  a N  X  N non-negative matrix of item-item coef fi cients. Contexts can also be incorporated into the SLIM-U models. In this case, W is a M  X  M matrix representing user-user coef fi cients. The matrix D can be built using a similar approach as introduced in the CSLIM-I models. In other words, we can build three CSLIM-U models: CSLIM-U-CI, CSLIM-U-CU, CSLIM-U-C. The CSLIM-U-CU model, for example, utilizes user-based K -nearest-neighbor (UserKNN) collaborative fi ltering to estimate S i,j,c using a sparse aggregation of the ratings on item t j in contexts c by other users who have rated it. Meanwhile, the CRD matrix D is modeled by a CU matrix which is of size M  X  L ,and W is a M  X  M non-negative matrix, with zero values in the diagonal, which is used to estimate coef fi cients between each pair of users. The loss function is the same as Equation 4 and the estimation for ranking score can be computed by Equation 5, where u h belongs to the set of users who have rated item t j except u i . The parameters in matrices and D can be updated in a similar way using SGD as the optimizer.
In a summary, CSLIM-I and CSLIM-U models utilize the intu-itions behind ItemKNN and UserKNN, respectively, with the CRD matrix computed in one of three ways: CRD associated to each individual contextual condition, CRD associated with each &lt;item, contextual condition&gt; pair, or CRD associated with each &lt;user, contextual condition&gt; pair, which results in six CSLIM models. In addition to the CRD matrix D , context-aware matrix factoriza-tion (CAMF) [2] has to also learn user and item matrices, as well as other parameters, such as user or item biases. In CSLIM, however, there is only one matrix W must be learned. The complexity of CSLIM models is associated with the number of users or items and the number of contextual conditions. For large-scale recommenda-tions, contexts can be selected in advance based on contextual rel-evance [6], and feature selection [5] can also be applied to reduce the number of users or items, which was suggested in the original SLIM approach in order to deal with large W matrix.

Moreover, CSLIM helps alleviate the sparsity problem in con-textual ratings by making use of the non-contextual ratings. In C-SLIM, the contextual ratings are used to train the model, and the ranking score is estimated by the non-contextual ratings and the and W matrices. The model can be trained and built incrementally with contextual ratings. It is a good solution for an application that has just started collecting context information.

CSLIM are based on co-ratings among items or users. Thus, this approach may be affected by the cold-start problem. For example, consider a situation where a user has no ratings or an item has not been rated before. This, of course, is a long-standing problem in recommender systems research. One possible solution is to use the item X  X  or the user X  X  average rating or even the global average rating in the data to fi ll the non-contextual rating matrix and then make predictions based on CSLIM models. Once a user has at least one contextual rating, CSLIM is able to learn the corresponding CRD to make recommendations in the future. We have evaluated our CSLIM models in fi ve data sets 1 .Forthe reason of limited space, we just present the experimental results for three: Food [7], Restaurant [8] and Music [1] data sets. A summary of those data sets can be viewed in Table 1. For more information, please refer to the papers cited above.

We use two density metrics (see Equation 6) to describe the lev-el of sparsity in the data. Assume there are F contextual factors, D cr measures the density of contextual ratings, where |  X  |isusedto measure the size of the user, item or context dimension. D sures the density of multiple ratings in contexts based on unique &lt;user, item&gt; entries. D mr = 100% indicates each unique &lt;user, item&gt; entry was given multiple ratings (i.e. more than one rating) in different contexts. Thus the Food and Music data are dense in multiple contextual ratings. Food data has no non-contextual rat-ings, so we use user X  X  average rating on each item to fi ll the non-contextual rating matrix R . It is reliable, because D mr this data. Even if D mr is small, we can still use average ratings to fi ll R . Because the diagonal values in W are 0s, the model will never make a prediction based on user X  X  existing ratings on the item we X  X  like to predict.
 We compare the CSLIM algorithms with state-of-the-art context-aware recommendation algorithms, including TF [3], CAMF (i.e. CAMF-CI, CAMF-CU, CAMF-C) [2] and context-aware splitting approaches (CASA) using biased MF as the recommender (i.e. item splitting, user splitting and UI splitting) [11]. To better present the results based on CAMF and CASA, we just present the results by the best performing CASA and CAMF approaches. In addition, we include the original non-contextual SLIM algorithm.

Since SLIM was built for top-N recommendations, we evalu-ate CSLIM by comparing with baseline algorithms in the top-recommendation task. We split the contextual ratings to 5 fold-s and measure the quality of recommendations using 5-fold cross validation across three metrics: precision, recall and mean average precision (MAP). Those metrics are measured using context as a parameter. Traditionally, precision is measured based on the hit ra-tio of relevant items in the top-N recommendation list. In CARS, we prefer to add contexts as additional constraints. Speci fi cally, we provide a list of top-N recommended items for each &lt;user, con-texts&gt; pair, instead of a single user. Then, we measure the hit ratio averaged by each context for each user. This type of evaluation has been used in prior research [11]. Recall and MAP can be mea-sured in a similar manner. The resulting metric values are typically much smaller than the ones in traditional RS, because it is not very common for users to rate multiple items within a same context. The results in precision and MAP are shown in Figure 4. We did not show results for recall, since they are consistent with the ones in precision. As the results indicate, CSLIM approaches are able to signi fi cantly outperform the three baseline approaches (i.e. TF, CAMF, CASA), especially in the Food and Music data sets. Over-all, the best performing CSLIM provides 102%, 23.7% and 320% improvement on precision@10 for the Food, Restaurant and Music data respectively, compared with the best performing baselines.
The original SLIM algorithms are able to outperform other base-line algorithms (i.e. CASA, CAMF, TF) in some cases, e.g. SLIM-I outperforms the other baseline algorithms in the music data. How-ever, the best performing CSLIM always outperform the original SLIM algorithms in those context-aware data sets.

The ranking of the recommended items is another way of evalu-ating top-N recommendations, because a high-precision is not nec-essary to guarantee a good ranking. In terms of MAP, the results show a more signi fi cant gap between CSLIM and the baseline ap-proaches. Some baseline approaches are able to outperform some CSLIM approaches in MAP. For example, CASA outperforms the CSLIM-I models in the Food data, and TF works better than some CSLIM-U models in the Restaurant data. But, we observe that the best performing CSLIM models can generally outperform the state-of-the-art context aware recommendation algorithms. Similar observation can be made in the other two data sets we evaluated, however, we did not present the results here due to limited space.
In our experiments, we also fi nd that the performance of CSLIM is correlated with the D mr value. A large D mr value indicates users have multiple ratings on items in different contexts, where the parameters in both the CRD matrix D and coef fi cient matrix W will be learned in multiple times based on those dense entries and it is able to fi nally result in a more accurate ranking model for top-N context-aware recommendations. In this paper, we successfully incorporate contexts into SLIM and develop several CSLIM algorithms, where they estimate the rank-ing score by the the intuition behind ItemKNN or UserKNN via the aggregation of users X  estimated contextual ratings on items. C-SLIM models are demonstrated to outperform the state-of-the-art CARS algorithms for top-N recommendations in our experiments. We also fi nd that the performance of CSLIM is correlated with the density of multiple ratings in contexts. In our future work, we plan to incorporate the contexts into the coef fi cient matrix helps estimate the coef fi cients between each contextual conditions and discover more insights about the contextual effects.
