 Lihong Li lihong@cs.rutgers.edu Reinforcement learning (RL) is a learning paradigm for optimal sequential decision making (Bertsekas &amp; Tsitsiklis, 1996; Sutton &amp; Barto, 1998) and has been successfully applied to a number of challenging prob-lems. In the RL framework, the agent interacts with the environment in discrete timesteps by repeatedly observing its current state, taking an action, receiving a real-valued reward, and transitioning to a next state. A policy is a function that maps states to actions; se-mantically, it specifies what action to take given the current state. The goal of an agent is to optimize its policy in order to maximize the expected long-term re-turn, namely, the discounted sum of rewards it receives by following the policy.
 An important step in this optimization process is pol-icy evaluation  X  X he problem of evaluating expected returns of a fixed policy. This problem is often the most challenging step in approximate policy-iteration algorithms (Bertsekas &amp; Tsitsiklis, 1996; Lagoudakis &amp; Parr, 2003). Temporal difference (TD) is a fam-ily of algorithms for policy evaluation (Sutton, 1988) and has received a lot of attention from the commu-nity. Unfortunately, it is observed ( e.g. , Baird (1995)) that TD methods may diverge when they are combined with function approximation . An alternative algo-rithm known as residual gradient (RG) was proposed by Baird (1995) and enjoys guaranteed convergence to a local optimum. Since RG is similar to TD(0), a par-ticular instance of the TD family, we will focus on RG, TD(0), and a variant of TD(0) in this paper.
 Despite convergence issues, little is known that com-pares RG and TD(0). Building on previous work on online learning of linear functions (Cesa-Bianchi et al., 1996) and a similar analysis by Schapire and Warmuth (1996), we provide a worst-case (non-probabilistic) analysis of these algorithms and focus on two evaluation metrics: (i) total squared prediction error, and (ii) total squared temporal difference. The former measures accuracy of the predictions, while the latter measures consistency and is closely related to the Bellman error (Sutton &amp; Barto, 1998). Either metric may be preferred over the other in different situations. For instance, Lagoudakis and Parr (2003) argue that TD solutions tend to preserve the shape of the value function and is more suitable for approximate policy iteration, while there is evidence that minimizing squared Bellman errors is more robust in general (Munos, 2003). Our analysis suggests that TD can make more accurate predictions, while RG can result in smaller temporal differences. All terms will be made precise in the next section. Although our theory focuses on worst-case upper bounds, we also provide numerical evidence and expect the resulting insights to give useful guidance to RL practitioners in deciding which algorithm best suits their purposes.
 Fully observable environments in RL are often mod-elled as Markov decision processes (Puterman, 1994), which are equivalent to induced Markov chains when controlled by a fixed policy. Here, however, we con-sider a different model that is suitable for worst-case analysis, as introduced in the next subsection. This model makes no statistical assumption about the ob-servations, and thus our results apply to much more general situations including partially observable or ad-versarial environments that subsume Markov chains. Some notation is in order. We use bold-face, lower-case letters to denote real-valued column vectors such as v . Their components are denoted by the corresponding letter with subscripts such as v t .Weuse  X  to denote the Euclidean, or 2 -norm: v = the transpose of v . For a square matrix M ,theset of eigenvalues of M , known as the spectrum of M ,is denoted  X  ( M ). If M is symmetric, its eigenvalues must be real, and its largest eigenvalue is denoted  X  ( M ). 2.1. The Sequential Online Learning Model Our learning model is adopted from Schapire and War-muth (1996) and is an extension of the online-learning modeltosequentialpredictionproblems. Let k be the dimension of input vectors. The agent maintains a weight vector of the same dimension and uses it to make predictions. In RL, input vectors are often fea-ture vectors of states or state X  X ction pairs, and are used to approximate value functions (Sutton &amp; Barto, 1998). Learning proceeds in discrete timesteps and terminates after T steps. The agent starts with an ini-tial input vector x 1  X  R k and an initial weight vector w 1  X  R k . At timestep t  X  X  1 , 2 , 3 ,  X  X  X  ,T } :  X  The agent makes a prediction  X  y t = w t x t  X  R ,  X  The agent then observes an immediate reward By convention, r t = 0 and x t = 0 for t&gt;T .Define the return at time t by y t =  X   X  = t  X   X   X  t r  X  ,where  X  [0 , 1) is the discount factor .Since  X &lt; 1, it effectively diminishes future rewards exponentially fast. A quick observation is that y t = r t +  X y t +1 , which is analogous to the Bellman equation for Markov chains (Sutton &amp; Barto, 1998). The agent attempts to mimic y t by its prediction  X  y t , and the prediction error is e t = y t  X  y . Our first evaluation metric is the total squared prediction error : P = T t =1 e 2 t = e 2 .
 Another useful metric in RL is the temporal differences (also known as TD errors ), which measures how con-sistent the predictions are. In particular, the temporal difference at time t is d t = r t +  X  w t x t +1  X  w t x t and the total squared temporal difference is TD = 2.2. Previous Work Previous convergence results of TD and RG often rely heavily on certain stochastic assumptions of the en-vironment such as the assumption that the sequence of observations, [( x t ,r t )] t  X  N , are generated by an ir-reducible and aperiodic Markov chain. Tsitsiklis and Van Roy (1997) first proved convergence of TD with linear function approximation, while they also pointed out the potential divergence risk when nonlinear ap-proximation is used.
 To resolve the instability issue of TD(0), Baird (1995) proposed the RG algorithm, but also noted that RG may converge more slowly than TD(0) in some problems. Such an observation was later proved by Schoknecht and Merke (2003), who used spectral analysis to compare the asymptotic convergence rates of the two algorithms. Although their results are in-teresting, they only apply to quite limited cases where, for example, a certain matrix associated with TD up-dates has real eigenvalues only (which does not hold in general). More importantly, they study synchronous updates while TD and RG are often applied asynchro-nously in practice. Furthermore, their results assume that the value function is represented by a lookup ta-ble, but the initial motivation of studying RG was to develop a provably convergent algorithm when func-tion approximation is used.
 Schapire and Warmuth (1996) were also concerned with similar worst-case behavior of TD-like algorithms within the model described in Subsection 2.1. They defined a new class of algorithms called TD  X  (  X  ), which is very similar to the TD(  X  ) algorithms of Sut-ton (1988). They developed worst-case bounds for the total squared prediction error of TD  X  (  X  ), but not the total squared temporal difference. 2.3. Algorithms The algorithms we consider all update the weight vec-tor incrementally and differ only in the update rules. TD(0) uses the following rule: where  X   X  (0 , 1) is the step-size parameter control-ling aggressiveness of the update. Although TD(0) is widely used in practice, analysis turns out to be easier with a close relative of it, TD  X  (0). This algorithm dif-fers from TD(0) in that it adapts the step-size based on the input vectors (Schapire and Warmuth (1996) defined TD  X  (0) in a different, but equivalent, form): Due to space limitation, we only provide results for TD  X  (0), but similar results hold for TD(0). It is ex-pected, and also supported by the numerical evidence in Section 4, that TD(0) and TD  X  (0) have similar be-havior and performance in practice. For this reason, we refer to both algorithms as TD in the rest of the paper if there is no risk of confusion. In contrast, RG uses the following update rule: This section contains the main theoretical results. We will first describe how to evaluate an algorithm in the worst-case scenario. For completeness, we also sum-marize the squared prediction error bounds for TD  X  (0) due to Schapire and Warmuth (1996). Then, we ana-lyze total squared temporal difference bounds and RG. Our analysis makes a few uses of matrix theory (see, e.g. , Horn and Johnson (1986)), and several tech-nical lemmas are found in the appendix. Two ba-sic facts about  X  ( M ) will be used repeatedly: (i) if M is negative-definite, then  X  ( M ) &lt; 0; and (ii) the Rayleigh-Ritz theorem (Horn &amp; Johnson, 1986, Theo-rem 4.2.2) states that  X  ( M )=max v = 0 v M v v v . 3.1. Evaluation Criterion Analogous to other online-learning analysis, we treat
P and TD as total losses , and compare the total loss of an algorithm to that of an arbitrary weight vector, u . We wish to prove that this difference is small for all u , including the optimal (in any well-defined sense) but unknown vector u  X  .
 The prediction using vector u at time t is y u t = u x t . Accordingly, the prediction error and tem-poral difference at time t are e u t = y t  X  y u t and d t = r t +  X  u x t +1  X  u x t , respectively. The total squared prediction error and total squared temporal difference of u are u P = e u 2 = T t =1 y t  X  u x t 2 respectively. 3.2. Squared Prediction Errors of TD  X  (0) Using step-size  X  = 1 X 2 +1 ,SchapireandWar-muth (1996) showed a worst-case upper bound: Furthermore, if E and W are known beforehand such that u P  X  E and w 1  X  u  X  W , then the step-size  X  can be optimized by  X  = W X  X  E + X 2 W to yield an asymptotically better bound: 3.3. Squared Temporal Differences of TD  X  (0) We will extend the analysis of Schapire and War-muth (1996) to the new loss function TD by exam-ining how the potential function, w t  X  u 2 ,evolves when a single update is made at time t .Itcan be shown (Schapire &amp; Warmuth, 1996, Eqn 8) that  X  w 1  X  u 2  X   X  2 X 2 e D D e +2  X  e D ( e u  X  e ), where Define f = D e . According to Lemma A.1(1), d u = D e u , and hence the inequality above is rewritten as:  X  w 1  X  u 2  X   X  2 X 2 f f  X  2  X  f D  X  1 f +2  X  f D  X  1 d u Using the fact that 2 p q  X  p 2 + q 2 for p = ity becomes  X  w 1  X  u 2  X  f M 1 f + b u TD ,where
M 1 =  X  2 X 2 I + is a symmetric matrix. Since  X  ( M 1 )isthelargest eigenvalue of M 1 ,wehave f M 1 f  X   X  ( M 1 ) f 2 ,and hence,  X  w 1  X  u 2  X  f 2  X  ( M 1 )+ b u TD . Combining this with Lemma A.4, we have that f 2 is at most (1 +  X  ) 2 X 2 + when the step-size is Due to Lemma A.1 (2), we have Therefore, TD is at most (1 + 2  X  ) 2 X 2 + Using b =1,wehavethusprovedthefirstmainresult. Theorem 3.1. Let  X  be given by Eqn 7 using b =1 , then the following holds for TD  X  (0):
TD  X  (1+2  X  ) 2 X 2 + Theorem 3.2. If E and W are known beforehand such that u TD  X  E and w 1  X  u  X  W , then  X  can be optimized in TD  X  (0) so that Proof. Previous analysis for Theorem 3.1 yields for any b&gt; 0. We may simply choose b = W X (1  X   X  )  X  and the step-size in Eqn 7 becomes 3.4. Squared Prediction Errors of RG By the update rule in Eqn 3 and simple algebra,  X  w t ( w t  X  u )=  X d t ( x t  X   X  x t +1 ) ( w t  X  u ) Similar to the previous section, we use the potential function w t  X  u 2 to measure progress of learning: According to Lemma A.1 (1) and using the fact that 2 p q  X  p 2 + q 2 for p =  X   X  b D d , q = arbitrary b&gt; 0, the inequality above is written as: Due to Lemma A.1 (3), d = X  D e ,where Then, the inequality above becomes: where Since e M 2 e  X   X  ( M 2 ) e 2 , Lemma A.5 implies the following theorems when the step-size is Theorem 3.3. Let  X  be given by Eqn 11 using b =1 , then the following holds for RG: Theorem 3.4. If E and W are known beforehand such that u P  X  E and w 1  X  u  X  W ,then  X  can be optimized in RG so that Proof. Previous analysis in this subsection yields We simply choose b = W X  X  E and accordingly the step-size in Eqn 11 becomes 3.5. Squared Temporal Differences of RG It is most convenient to turn this problem into one of analyzing the total squared prediction error in the original online-learning-of-linear-function frame-work (Cesa-Bianchi et al., 1996). In particular, define z t = x t  X   X  x t +1 and thus z t  X  (1 +  X  ) X .Now, RG can be viewed as a gradient descent algorithm op-Due to Theorem IV.1 of Cesa-Bianchi et al. (1996), we immediately have W are known beforehand so that u TD  X  E and u  X  W ,then  X  can be optimized (Theorem IV.3 of Cesa-obtain the following improved bound: 3.6. Discussions Based on Eqns 4, 8, 12, and 13, Table 1 summarizes the asymptotic upper bounds (when T  X  X  X  )assum-ing E and W are known beforehand to optimize  X  . 1 Although our bounds are all upper bounds, results in the table suggest that, in worst cases, TD  X  (0) (and also TD(0)) tend to make smaller prediction errors, while RG tends to make smaller temporal differences. The gaps between corresponding bounds increase as  X   X  1. On the other extreme where  X  =0,allthese asymptotic bounds coincide, which is not surprising as TD(0), TD  X  (0), and RG are all identical when  X  =0. Since it is unknown whether the leading constants in Table 1 are optimal, the next section will provide nu-merical evidence to support our claims about the rel-ative strengths of these algorithms.
 It is worth mentioning that in sequential prediction or decision problems, the factor 1 1  X   X  often plays a role similar to the decision horizon (Puterman, 1994). Therefore, in some sense, our bounds also character-ize how prediction errors and temporal differences may scale with decision horizon, in the worst-case sense. When P or TD are relatively small, the asymptotic bounds in Table 1 are less useful as the w 1  X  u 2 in the bounds dominate P or TD . However, we still get similar qualitative results by comparing the constant factors of the term w 1  X  u 2 in the bounds.
 Since our setting is quite different from that of Schoknecht and Merke (2003), our results are not com-parable to theirs. This section presents empirical evidence in two Markov chains that supports our claims in Section 3.6. Thefirstisthe Ring Markov chain (Figure 1 (a)), a variant of the Hall problem introduced by Baird (1995) in which RG was observed to converge to the optimal weights more slowly than TD(0). The state space is a ring consisting of 10 states numbered from 0 through 9. Each state is associated with a randomly selected feature vector of dimension k =5: x (0) ,  X  X  X  , x (9)  X  R k . Transitions are deterministic and are indicated by arrows. The reward in every state is stochastic and is distributed uniformly in [  X  0 . 1 , 0 . 1]. As in Hall , the value of every state is exactly 0. The second problem is a benchmark problem known as PuddleWorld (Boyan &amp; Moore, 1995). The state space is a unit square (Figure 1 (d)), and a start state The agent adopts a fixed policy that goes north or east with probability 0 . 5 each. Every episode takes about 40 steps to terminate. The reward is  X  1un-less the agent steps into the puddles and receives penalty for that; the smallest possible reward is  X  41. We used 16 RBF features of width 0 . 3, whose cen-ters were evenly distributed in the state space. We also tried a degree-two polynomial feature: for a state s =( s 1 ,s 2 ) , the feature vector had six components: x ilar to those for RBF features, they are not included. We ran three algorithms in the experiments: TD(0), TD  X  (0), and RG. For a fair comparison, all algorithms started with the all-one weight vector and were given the same sequence of ( x t ,r t ) for learning. The pro-cedure was repeated 500 times. For Ring ,eachrun used a different realization of feature x ( s ) and T = 500; for PuddleWorld ,eachrunconsistedof50episodes (yielding slightly less than 2000 steps in total). A wide range of step-sizes were tried, and the best choices for each discount-factor X  X lgorithm combination were used to evaluate P and TD , respectively. Figure 1 (b,c,e,f) gives the average per-step squared prediction errors and squared temporal differences for these two prob-lems, with 99% confidence intervals plotted.
 These results are consistent with our analysis: TD(0) and TD  X  (0) tended to make more accurate predictions, while RG did a better job at minimizing temporal differences; the differences between these algorithms were even larger as the discount factor  X  approached 1. 2 Finally, as a side effect, it is verified that TD(0) and TD  X  (0) had essentially identical performance, al-though their best learning rates might differ. We have carried out a worst-case analysis to compare two policy-evaluation algorithms, TD and RG, when linear function approximation is used. Together with previously known results due to Schapire and War-muth (1996) and Cesa-Bianchi et al. (1996), our re-sults suggest that, although the TD algorithms may make more accurate predictions, RG may be a bet-ter choice when small temporal differences are desired. This claim is supported by empirical evidence in two simple Markov chains. Although the analysis is purely mathematical, we expect the implications to deepen the understanding of these two types of algorithms and can provide useful insights to RL practitioners. There has been relatively little attention to this sort of online-learning analysis within the RL community. Our analysis shows that this kind of analysis may be helpful and provide useful insights. A few direc-tions are worth pursuing. First, we have focused on worst-case upper bounds, but it remains open whether matching lower bounds can be found. More exten-sive empirical studies are also necessary to see if such worst-case behavior can be observed in realistic prob-lems. Second, we wish to generalize the analysis of total squared temporal difference from TD(0) and TD  X  (0) to TD(  X  )andTD  X  (  X  ), respectively. Finally, we would like to mention that, in their original forms, both TD and RG use additive updates. Another class of updates known as multiplicative updates (Kivinen &amp; Warmuth, 1997) has been useful when the number of features ( i.e. ,the k in Subsection 2.1) is large but only a few of them are relevant for making predictions. Such learning rules have potential uses in RL (Precup &amp; Sutton, 1997), but it remains open whether these al-gorithms converge or whether worst-case error bounds similar to the ones given in this paper can be obtained. Lemma A.1. This lemma collects a few basic facts useful in our analysis ( D is given in Eqn 5): 1. In all three algorithms, d u = D e u . 2. In TD  X  (0), d t =(1  X   X  X  x t x t +1 )( e t  X   X e t +1 ) . Proof. 1. Since y t = r t +  X y t +1 ,wehave 2. Since w t = w t +1  X   X  w t and y t = r t +  X y t +1 , 3. Similar to the proof for part (2) except that  X  w t Two technical lemmas are useful to prove Lemma A.4. It should be noted that the bounds they give are tight. Lemma A.2. For D given in Eqn 5, let A be D D or DD ,and B be D  X  1 D  X  or D  X  D  X  1 .
 Then,  X  ( A )  X  (1  X   X  ) 2 , (1 +  X  ) 2 and  X  ( B )  X  (1 +  X  )  X  2 , (1  X   X  )  X  2 .
 Proof. It can be verified that D D equals Since D D is symmetric,  X  D D  X  R . It fol-lows from Ger X  sgorin X  X  theorem (Horn &amp; Johnson, 1986, Theorem 6.1.1) that  X  D D  X  (1  X   X  ) 2 , (1 +  X  ) 2 . The same holds for  X  DD . The second part follows immediately by observing that D  X  1 D  X  = D D  X  1 Lemma A.3. Let D be given by Eqn 5, then Proof. It can be verified that D  X  1 + D  X  equals and that ( G  X  I )  X  1 equals Clearly, ( G  X  I )  X  1 is symmetric, and it follows from Ger X  sgorin X  X  theorem that Therefore, Consequently,  X  ( G )  X  1+ We are now ready to prove the following lemma. where M 1 is given in Eqn 6.
 Proof. By Weyl X  X  theorem (Horn &amp; Johnson, 1986, Theorem 4.3.1), The lemma then follows immediately from Lem-mas A.2 and A.3.
 Lemma A.5. Let M 2 be defined by Eqn 10 and sup-pose the step-size is given by Eqn 11, then Proof. Let  X  =  X  2 b and  X  =  X  2 X 2 (1 +  X  ) 2  X  2  X  ,then M 2 = D  X   X DD +  X I  X  D .Itisknownthat Define v 2 = D v 1 and we have:  X  ( M 2 )=max where the last step is due to Lemma A.2 and the fact that M 2 is negative-definite for  X  1. Similarly, we define v 3 = X  v 2 and use the fact that 0  X  v 2 v 2 = v 3  X   X  2 v 3  X  1+  X  (1 +  X  )  X X 2 2 v 3 2 to obtain: If we choose  X  as in Eqn 11, then the lemma follows immediately from the fact that 1+ We thank Michael Littman, Hengshuai Yao, and the anonymous reviewers for helpful comments that im-proved the presentation of the paper. The author is supported by NSF under grant IIS-0325281.

