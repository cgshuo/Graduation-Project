 Many applications in text processing require significant human ef-fort for either labeling large document collections (when learning statistical models) or extrapolating rules from them (when using knowledge engineering). In this work, we describe a way to reduce this effort, while retaining the methods X  accuracy, by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text patterns to complement machine learning. Using a standard sentiment-classification dataset and real customer feed-back data, we demonstrate that the resulting technique results in significant reduction of the human effort required to obtain a given classification accuracy. Moreover, the hybrid text classifier also re-sults in a significant boost in accuracy over machine-learning based classifiers when a comparable amount of labeled data is used. H.1.2 [ Models and Principles ]: User/Machine Systems X  Human information processing ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing Methods ; I.5.4 [ Pattern Recognition ]: Applications X  Text processing Algorithms, Experimentation active learning, classification, machine learning, supervised learn-ing, support vector machines, text classification, text mining
The automated categorization/classification of natural-language text into a number of predefined categories is the basis for many tasks in empirical text processing. The predominant approaches to this problem have been two-fold: (a) knowledge-engineering tech-niques, which involve manually building a set of rules encoding expert knowledge on how to classify documents and (b) machine learning techniques that use statistical models (such as Support-Vector Machines [19, 6] or Bayesian Networks [16]) which are Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. trained using previously classified text corpora. The specific choice of knowledge-engineering framework or statistical model has been studied extensively in the research literature; however, from the perspective of an end-user looking to deploy a system, the true cost of setting up a specific NLP task does not lie in the overhead of developing the right classification algorithm, but rather in the over-head of having a human manually examine significant amounts of text to either infer classification rules or assign training labels.
This paper studies ways to reduce the amount of human effort required to build a text classifier. In our approach, we seek to leverage human input beyond individual labels for text documents, by exploiting two salient characteristics of text classification tasks: (1) A key property of text classification is that often knowledge of only a small subset of a documents X  text is sufficient to make a cor-rect classification decision. In fact, in some classification scenarios only a small subset of text is relevant  X  e.g. consider the example of sentiment detection in reviews where typically only a small fraction of a document conveys sentiment information, while the remainder can be ignored [12]. (2) The other property we leverage is the fact that humans are typically proficient at reasoning over classification rules specified in terms of text fragments, i.e. when presented with a set of possible rules that hold for a small training set of docu-ments, humans can pick rules that generalize well without knowing the text corpus itself, by relying on knowledge of the relevant do-main only. Variations of this property have been leveraged in a number of scenarios, e.g. [8, 15].

In a classification scenario we now leverage these properties as follows: initially, we run a pattern discovery algorithm over a small set of labeled training data to compute text patterns that are highly correlated with the occurrence of a specific label (i.e. if the pattern occurs, then  X  with high probability  X  so does the label). These pat-terns are then presented to a human user, who now selects among them text patterns that carry sufficient information to assign a doc-ument X  X  label based on the pattern itself. We refer to such patterns as discriminating patterns. Here, special care must be taken to en-sure that the patterns in question are of such type that a human can reason well over them.

Now, having obtained a set of discriminating patterns, we con-struct a 2-stage classifier as follows: An unclassified document is first checked if it matches any of the discriminating patterns. In this case, the document is assigned a label based on the pattern; if not, then the label is assigned by a machine-learning based classifier, that was trained on the same training documents used to discover the discriminating patterns (Figure 1).

To give an example of discriminating and non-discriminating patterns, consider the task of classifying user-feedback emails sent to a large company into mail expressing positive (+) and negative (-) sentiment. In this context, a frequent text pattern that has high correlation to the label (-) might be  X  Iwill...switchto...XYZCorp  X , where XYZCorp is the name of a competitor and the symbol  X  ... X  accounts for gaps between the matching words in the sentence. A human annotator may now conclude that this pattern is discrim-inating and sufficient to identify negative customer feedback. In contrast, the text pattern  X  XYZCorp  X  may be as highly correlated and even more frequent in the training corpus, however, it does not convey sufficient information to make a classification decision based on the pattern itself, and hence is not discriminating.
This framework has two advantages over pure machine-learning: (1) Using a pattern-based classifier enables us to capture more com-plex and specific patterns than we would typically be able to learn (due to sparsity of the training data) using machine-learning based approaches alone. (2) We do get some of the benefits associated with knowledge-engineering approaches (transparency of the dis-criminating text patterns, ability to leverage domain knowledge) without having a human read through and abstract from a large col-lection of documents manually.

The main advantage over pure knowledge engineering is effi-ciency: coming up with discriminating rules manually requires sig-nificant human effort for examining the document collection for variations of and counterexamples to each rule. Moreover, the rules a human designer intuitively comes up with may not correspond to the most frequent discriminating patterns in the text collection; thus additional human cycles are required to search the document col-lection for additional patterns.

Our work makes the following contributions: (1) A framework that combines text mining with manual annotation of text patterns to construct a hybrid classifier that significantly increases the clas-sifier accuracy relative to the amount of human effort. (2) Scalable algorithms to discover the most frequent text patterns satisfying a threshold on their correlation to a specific label. (3) A pattern matching framework that is sensitive to the contexts in which the meaning of a pattern may be changed (e.g. if the pattern is in the scope of a negation); as we will show later, this component is cru-cial in enabling our approach of having humans reason over text patterns without knowing all contextual information, which is the basis for our scheme X  X  efficiency.

The remainder of this paper is organized as follows. In Sec-tion 2, we will briefly review related work. In Section 3, we define the class of text patterns used in our approach and describe scalable algorithms to extract such patterns from text corpora. In Section 4 we describe scenarios in which human reasoning is unreliable and augment our pattern-matching scheme to address this. In Section 5 we evaluate the efficiency of the proposed framework using multi-ple real-life datasets. We conclude in Section 6.
The value of human feedback for feature selection was recently studied in [15]. Here, the authors show that feature selection solely based on the labeled training corpus only has little effect, whereas human feedback on feature relevance can identify a large fraction of the most relevant features. Similar to our findings, this work shows that by leveraging the fact that features can be annotated much more rapidly than training documents, one can significantly reduce the overall overhead in classifier construction. Our work differs from this paper in that we use significantly more complex patterns and consider the patterns X  context when making match-ing decisions. Also, we use text patterns in isolation of the actual machine-learned text classifier itself, allowing us to be agnostic to the specific classification technique and feature set.

On the surface, our method has some similarity with active learn-ing techniques (e.g., [17, 18]). While both methods can be used to ultimately reduce the human effort in categorization, their approach is different: active learning is concerned with using humans to max-imize the efficiency of labeling additional documents, whereas our approach leverages text mining to come up with what amounts to complementary rules among which humans are then asked to se-lect. Recent results [15] indicate that these approaches may be combined effectively and can complement each other.

In the context of machine learning, a large number of differ-ent approaches exist that leverage bootstrapping (or co-learning, transduction...) schemes to construct a machine-learning classi-fier using little initial training data combined with human feedback and/or large amounts of unlabeled data (e.g. [4, 8, 11]). Because our approach isolates the text patterns from the machine learning classifier, all of these approaches can be combined with our work, potentially reducing the human overhead even further.

Regarding the data structures used to obtain the text patterns from the labeled documents themselves, our work is closely related to other approaches in text mining via suffix arrays [2, 3]. The key difference in our approach is that we seek to maximize a very dif-ferent objective function, enabling pruning of the search space ([2, 3] require exhaustive traversal).
Given a vocabulary V over words, we define a text pattern as an ordered sequence of elements from V X  X  ... } , where the sym-bol  X ... X  denotes gaps in the text. We say that a sentence matches a pattern if it contains the symbols of the pattern in identical order while gaps may be filled with arbitrary sequences of words within a sentence (including the empty sequence) i.e. the pattern  X  a . . . good product  X  would match both  X  This is a good product.  X  X nd X  a cer-tainly very good product.  X . Patterns do not match across sentence boundaries. A document d matches a pattern p , if a sentence in d matches p . For a set of documents D , we define the frequency of a pattern p as the number of distinct documents in D that match p .
We refer to a pattern p as maximal for a document collection if no pattern p exists for which it holds that (a) p is a subsequence of p and (b) every sentence in D that matches p also matches p .We refer to a pattern as simple if it doesn X  X  contain the gap symbol. Notation: In the remainder of this paper, we will use the following notation: given a set of documents D = { d 1 ,...d n } ,andaset of labels L = { l 1 ,...,l h } , we will denote the label of a document d  X  X  by Label ( d )  X  X  . For a given document set D and a pattern p , we use the term freq ( p ) to denote the number of documents from D that match p . Similarly, we use the term freq ( l, p ) to denote the the number of documents from D that match p and are labeled with the label l  X  X  . We use the symbol  X  to denote the concatination of strings, i.e.  X  X   X   X   X  X tring X  =  X  X  string X .
Given this pattern definition, we want to discover patterns that are highly indicative of one label; to measure this, we simply use the probability of a label l given a pattern p in the training data: We also can use any other type of impurity function here without changing the resulting algorithm. For a pattern to be displayed to the human annotator, we require a value of Pr ( l, p ) greater than a threshold  X  , chosen to be larger than the desired classification accu-racy. Among all patterns satisfying this condition, we now want to display the ones which occur in the largest number of documents, as they are assumed to occur more frequently in the unlabeled data as well. Thus, the search problem becomes: Problem Statement : Given a set of documents D = { d 1 ,...d and a threshold  X  on the required minimum probability induced by a pattern, compute the k most frequent patterns p 1 ,...,p that for each p i it holds that
For very unbalanced corpora, we may want to extract the most frequent patterns indicative of each label separately; our overall algorithm remains the same in either case.

This text mining problem differs from many data-mining scenar-ios studied over in the context of itemset data in two respects: first, the text data itself (when using a dictionary-based encoding of each word as an integer) typically fits into main memory  X  given that vo-cabulary sizes for natural language text typically are in the range of 100K-200K, one billion words can be represented in 3GB of memory. Second, mining approaches that explore the search-space bottom-up by computing the number of occurrences and Pr ( l, p ) -values for short word-combinations (e.g. bi-or trigrams) do not scale due to the large vocabulary size and the fact that the patterns allow gaps; e.g. a vocabulary of |V| =25 K words can yield up to |V| 2 = 625  X  10 6 different bigrams. While the actual num-bers encountered are smaller, they still grow fast enough to rule out bottom-up pruning. Therefore, we use an algorithm that is based on multiple suffix arrays [9] to compute these patterns.
Given a vocabulary V and a text corpus T (of words from V made up of D documents, we represent T by encoding each word as an integer in { 1 ,..., |V|} , marking the end of each sentence by a special symbol  X # X  . We denote the string resulting from concati-nating all symbols in the corpus by T , using the notation T [ j ] to denote the j -th symbol in T ,and N to denote the length of T .
Now, a suffix array is a compact representation of a suffix tree [10] over T . The key advantage of suffix arrays over suffix trees is that their space requirements do not grow proportional to the product of the vocabulary size and N , but only proportional to N . Concep-tually, a suffix array SA is an array of all N suffixes in T , sorted lexicographically. Each suffix is represented as a pointer into the corpus only, i.e. if SA [ j ]= i then the j -th (in lexicographical or-der) suffix contained in T is the word sequence T [ i ] ...T [ N ] .We will refer to this number j as the rank of a the suffix starting at T [ i ] .
In addition to the suffix-array, we maintain an auxiliary array for storing LCPs (longest common prefixes), where each element, LCP [ j ] , indicates the length of the common prefix between the suffixes denotes by SA [ j ] and SA [ j  X  1] . Finally, we also main-tain an inverted suffix array SA  X  1 defined via SA  X  1 [ i ]= j SA [ j ]= i . Given that all of these arrays have N fields, the total structure requires O ( N ) space, which in practice means less than 3 times the space required to store T , as all pointers and members of
V are stored using 4 bytes, and the LCP array typically only requires single-byte counters (for natural text, patterns longer than 256 words are typically irrelevant). Construction of Suffix Arrays and the LCP array requires overhead linear in N (e.g. [7]).
Once a suffix array over T has been constructed, we can compute the Pr ( l, p ) for all maximal patterns that do not contain the gap symbol  X  ... X  usinga modification of the algorithm proposed in [21] to compute the term frequency (= total number of occurrences) and document frequency (= freq ( p ) ) of text patterns. This algorithm computes the document frequency for any maximal n -gram occur-ring in the text (and therefore any maximal pattern without gaps), with n  X  X  1 ,...N } in O ( N log N ) time; the modification we make is that we keep track of the |L| different document frequen-cies  X  corresponding to the sets of documents of each label. This modification is trivial, so we omit a detailed description.
The algorithm traverses all maximal patterns contained in T and also outputs for each pattern p the lowest and largest rank of suf-fixes starting with p (which means that all suffixes with ranks be-tween these boundaries must also start with p ), and the length of each pattern. In the following, we will denote the set of ranks for a pattern p as rank ( p ) and its length by length ( p ) . Note that for our purposes it suffices to consider only the maximal patterns, as any non-maximal patterns have identical correlation, but contain less information for the human annotator.
In the following, we will describe how to compute  X  for a given pattern p  X  all patterns of the form p  X   X ... X   X  p 1 T , with each p i being a maximal simple pattern, as well as the corresponding suffix-ranks and Pr ( l, p  X   X ... X   X  ...  X  p refer to this as expanding the pattern.

First, consider the case of p being a simple pattern. Now, us-ing the algorithm described above, we compute rank ( p ) ; conse-quently, we can compute the starting positions of all suffixes after occurrences of p as { SA [ r ]+ length ( p ) | r  X  rank ( p ) given that patterns cannot cross sentence-boundaries, we can com-pute the set of all positions pos ( p ) in T that can potentially contain patterns of the form p  X   X ... X   X  p using the set of suffixes of the occur-rences of p in T . If we define the next sentence boundary after a po-sition k in T as bound ( k ):=min { l  X  N | l&gt;k and T [ l ]=  X # X  then we can compute the set of positions as pos ( p ):=
Now define the subset T p as the subset of the text corpus T con-sisting of the items T [ i ] where i  X  pos ( p ) . We compute a suffix array SA p on T p by sorting the ranks corresponding to members of pos ( p ) (which is typically much more efficient than building it by comparing the corresponding suffixes), including the correspond-ing LCP information and inverse suffix array SA  X  1 p . Now, we can run the algorithm of [21] on this suffix array. Any patterns p found in T p correspond to occurrences of p  X   X ... X   X  p in T , with the value of Pr ( l, p ) with respect to T p being the value of Pr ( l, p with respect to T .

The construction for cases where p is not a simple pattern is al-most identical. Let p be of the form p 1  X  ...  X  p m , where each p a simple pattern. Then we compute the positions in pos ( p ) as pos ( p ):= By repeating this construction, we can find patterns with arbitrary numbers of gaps. Overall, this construction results in a search tree over suffix arrays containing smaller and smaller subsets of the cor-pus, corresponding to the suffixes of the patterns found along the paths in the tree. We illustrate this in Figure 2.

One crucial aspect of this search scheme is that  X  when memory is sparse  X  we can traverse the search space in a depth-first man-ner, which in turn requires very little memory. This is due to the large vocabulary size (and the short length of the patterns we use), which results in the suffix-arrays further down the search tree only requiring a small fraction of the size of the initial suffix array.
The approach we describe above is very similar to the one taken in [3], with the key difference being the objective function used to evaluate patterns. While we search for the most frequent patterns satisfying a minimum level of correlation, the work of [3] tries to find the pattern p that (in the case of two labels { 0 , 1 an evaluation function of the form: with  X  denoting any convex impurity function ,suchas information entropy or the gini index . The key difference between these formu-lations is that the latter function does not lend itself easily to prun-ing of the search space when exploring the search tree over suffix arrays described above. Instead, the algorithm of [3] traverses the entire search space.

In contrast, we are only interested in the k most frequent pat-terns, allowing us to abort any search path that cannot produce pat-terns of sufficiently high frequency. Due to this, the issue of the order in which we explore the search tree becomes important, since the faster we obtain high-frequency patterns, the smaller the sub-sequent search space. To implement this pruning functionality, we maintain a list P found of the k most frequent patterns found thus far that satisfy the condition (1) given in the problem statement. Now, we only need to expand a pattern p if for otherwise the expanded pattern cannot satisfy condition (1) and be among the top k most frequent patterns. Due to space-constraints, we omit a further description of the search algorithm.
When presenting the resulting candidate patterns to a human an-notator, one key to the efficiency of this approach is that the anno-tator is presented with patterns only, but not their sentential context (which would take orders of magnitude longer to review). How-ever, this can be problematic in cases in which the context of a pattern invalidates the way the pattern is interpreted in isolation: consider the the text pattern  X  I like your product  X . In our sentiment-classifier example this phrase may induce the user to assign a (+) label to the feedback mail; however, if the phrase is in the scope of a negation, such as  X  It X  X  not true that I like your product . X , this con-clusion would be invalid, even though the negation does not show up as part of the pattern itself. Natural language contains a number of constructs that have this type of effect: Negation  X  these may occur either as external negations that occur outside a pattern and invalidate an entire pattern (such as in the ex-ample above), or as internal negations where the word triggering the negation is  X  X idden X  in gap within the pattern itself (e.g.  X  this is not great at all  X  matching the pattern  X  this is ...great  X ). Conditionals  X  If a pattern occurs as part of a conditional, then the statement implied by the pattern typically does not hold. For example, consider the pattern  X  this is ...a good product  X  and the sentence  X  If this is such a good product, then why does it crash ? X . Subjunctive  X  These affect patterns similar to conditionals. Exam-ple: the phrase  X  were it the case that  X  preceding a pattern. Factives and Speech  X  These affect patterns similar to condition-als. Example: the phrase  X  He claims that  X  preceding a pattern.
We refer to the four types of constructs above as invalidating constructs . Fortunately, in English the words associated with these types of constructs are known and are independent of a specific training corpus. Thus by using a simple keyword (or key-phrase) matching algorithm, we can identify most sentences containing such constructs, if we are willing to allow false positives.

So, in order to deal with invalidating constructs, we constructed sets of words that can trigger such a construct (e.g. not, never, can-not , etc. in case of negation) and use a simple keyword-matching algorithm to identify sentences that potentially may contain an in-validating construct. Now, we extend the definition of matching a pattern as follows: a sentence containing an invalidating construct matches a text pattern only if the invalidating construct is part of the pattern itself. For example, the sentence  X  don X  X  buy this product . X  matches the pattern  X  don X  X  buy this  X , but not  X  buy this product  X .
This may result in false positives (i.e. we might flag an invalidat-ing construct either where none exists, or when it does not affect the pattern in question), but since the pattern-based classifier is backed by a machine-learning one, we can afford to be overly conservative.
In this section we describe experiments which evaluate both the overall efficiency of the hybrid classifiers as well as the reduction of human effort required to obtain a target accuracy.
 Datasets: We used two different datasets in our experiments: the first is the polarity dataset V2.0 available from [1], which consists of 2000 movie-reviews from rec.arts.movies.reviews , with 1000 reviews being positive and 1000 negative. This dataset has be-come the de facto standard dataset for sentiment-classification and has been used in over 15 research papers. The second dataset we used is real customer feedback data that is internal to Microsoft. The data set contains more than 1.8 million pieces of individual customer feedback and 118MB of raw text data. The individual messages contain comments on web help documents and are on average significantly shorter and vary more in content than the re-views in the first dataset. Each feedback item is tagged with one of the labels ( X  useful  X ,  X  not useful  X ,  X  maybe  X ), reflecting the customer assessment of the help document in question.
In these experiments we compare the accuracy of the hybrid clas-sifier to a machine-learning based one given the required amount of human effort (i.e. the number of labeled training documents and text patterns examined by a human). The machine learning method we use is a Support Vector Machine (SVM) [19]. SVMs have con-sistently been shown to outperform other classification algorithms for text classification in general [6, 5], and for sentiment classi-fication in particular [13, 12]. The training algorithm we used is Sequential Minimal Optimization [14]. When training the SVMs, each document is represented as a feature vector, where we varied the feature sets to include (a) all unigrams, or (b) all unigrams, bi-grams and trigrams found in the training document set. All features were binary, i.e. only the absence or presence of a feature in a doc-ument is indicated, but not its frequency, which is consistent with results in research literature in which binary features outperform frequency features for text classification (e.g. [6, 12]). Setup of the Text-Pattern based Classifier: To construct the text-pattern based classifier we first used the algorithm of Section 3.2 to discover the most frequent text patterns satisfying condition (1) (for  X  =0 . 99 ) in the full training data. The 300 most frequent pat-terns 1 were then presented to 15 different human annotators. Each annotator was given only the patterns, but no information about their frequency or examples of sentences containing the patterns, as we were interested in validating our hypothesis that humans can reason sufficiently well about the patterns themselves. Depending on the annotator, identifying discriminating patterns required 20-40 minutes, with the average being about 30.

Obviously, the quality of the resulting pattern-based classifier varies with the annotator  X  consequently, all experiments on classi-fication accuracy using such classifiers are plotted with error-bars, with the high/low bar corresponding to the best/worst annotator and the plotted point corresponding to the average over all annotators. Classification Results: The classification accuracy of the result-ing machine-learning based and hybrid classifiers is shown in Fig-ure 3 for varying sizes of the training data. Note that the accura-cies of the SVM-baseline compares nicely with other results pub-lished on this dataset. For example, [12] reports 87.15% accu-racy for a SVM trained on unigrams when using the full reviews as training data, which matches our results almost exactly. How-ever, in addition to the unigram-experiments we also use bi-and trigrams, which increases the classification accuracy significantly, so our baseline without text patterns already outperforms the results given in other papers. All experimental results are based on 5-fold cross-validation.

All experiments show clearly that the hybrid classifier results in a significant boost of the overall classification accuracy when compared to the machine-learning one. This holds true for all 15 annotators, indicating that humans can reason accurately over the patterns we provide.

Moreover, in nearly all cases the hybrid classifier also outper-formed the machine-learning one using an additional 10% (or  X  in many cases  X  even more) training data. Given that 10% of the train-ing data corresponds to 120 documents (of 600-700 words, as op-posed to patterns which are about two orders of magnitude shorter), whose annotation almost certainly requires more time than the 30 minutes used for discriminating pattern selection on average, these results also show that using our framework can yield significant reduction in the amount of human effort required.

In addition, we ran some initial experiments where we used the discriminating patterns themselves as binary features in the SVM classifier. Unfortunately, when simply adding these to the feature set, they are  X  X rowned out X  by the other features, since the number of such patterns is extremely small in comparison to the number of n -grams. The alternative of using only the discriminating patterns as features is also not practical, as these only cover a fraction of the data set, i.e. many documents do not contain a single discrim-inating pattern. None of these findings rule out the use of discrim-inating patterns as classifier features, or as input to the classifier-construction. For example, techniques such as [20] which incorpo-rate prior knowledge into weighted margin SVMs could potentially leverage discriminating patterns to come up with a better classifier, and we will investigate their use in the future.
Given that our experiments require human interaction beyond labeling, a natural comparison was to active learning techniques. Here, we use an active learning approach specifically aimed at SVM-based classification of text documents, using the Simple Margin method described in [18] to chose additional documents to label. In order to achieve an apples-to-apples comparison we need to com-pare techniques that require the same amount of human effort; how-ever, we can only guess at the time required at labeling a movie review. Hence, we conducted multiple experiments, one assuming that a human annotator would require 1 minute to label a review (the reviews average 600-700 words) and another one under the optimistic assumption that the annotator only requires 30 seconds per document. Since labeling the discriminating patterns required 30 minutes of time on average, we thus compare our approach to a machine-learning classifier which uses active learning to add 30 or 60 documents to its training data. The results are shown in Figure 4  X  here the x-axis corresponds to the amount of training data used before the active learning sets in. Active learning performs com-parably to the hybrid classifier for small training data sizes (out-performing SVM-classifiers without active learning using signifi-cantly more training data), but does not achieve the performance of the hybrid classifier for larger training sets.

To study the effects of text patterns in scenarios where training data is more abundant, we ran similar experiments on the user feed-back dataset, with the classification task being to assign the labels  X  useful  X  X nd X  not useful  X  to the comments. Here, we used between 23K and 32K comments as training data and a test-set of 10K com-ments. Note that this data is much more heterogeneous than the movie data, and individual items also much shorter, making this classification task somewhat more difficult.
 To construct the machine-learning classifier, we again used an SVM model, trained on all uni-, bi-and trigrams occurring in the training data. To construct the hybrid classifier, we presented a hu-man annotator with the 300 most frequent patterns, using a thresh-old of  X  =0 . 95 ; we only used a single annotator in this experi-ment. Finally, we also ran active-learning experiments similar to the above, allowing active learning to select 300 comments, to ac-count for these being much shorter than the movie reviews.
The experimental results are shown in Figure 5. Again, invest-ing a short amount of time into pattern annotation gives a boost to classification accuracy, which would have required a large number of additional labeled training data otherwise. Here, the overall im-provement (and the classifier accuracy) is lower, as the comments vary more (many of them express specific issues relevant to only a subset of the help pages). As before, the hybrid classifier performs better in relation to active learning as the size of the initial training data becomes larger.

In this paper we describe a novel framework to extracts text pat-terns that are highly correlated to a specific label from an annotated text corpus, among which discriminating patterns are then selected by a human annotator, resulting in text patterns that are highly pre-dictive and hence very valuable in classifier construction.
Our experiments suggest that these text patterns result in sig-nificant improvements in accuracy over pure machine-learning ap-proaches. This is in part due to the fact that the text patterns we use are often more complex than the individual features used in text classification (due to the sparseness of text data) and in part due to our combination of text mining and human domain knowledge being able to pick up highly predictive patterns/features, which are hard to properly weigh by machine-learning approaches (if the pat-terns are part of the feature space), as they are statistically indistin-guishable from other, less predictive features in the training data.
