 It is a normal situation in Natural Language Processing (NLP) that two tasks interact with each other. For example, Chinese word segmentation and POS-tagging, POS-tagging and chunking, intent identi cation and slot lling in goal-driven spoken language dialogue systems, and so on.
 rst task is thought to be more fundamental or lower than the second one. It is so called pipeline method, i.e. low level tasks are followed by high level tasks. For example, chunking in character-based languages such as Chinese, Japanese and Thai requires word segmentation and POS-tagging as pre-processing steps [ 1 , 2 , 3 ]. In Spoken Language Understanding (SLU), intent is rstly identi ed as a classi cation problem using Support Vector Machines (SVMs) [ 4 ], and then sequence labeling methods such as Conditional Random Field (CRF) [ 5 ] are employed for slot lling task. However, pipeline method suffers from error prop-agation. Lots of methods for jointly modeling the rst and the second tasks simultaneously have been proposed to tackle this problem.
 [ 6 ] introduced a transition-based framework for joint segmentation, POS-tagging and chunking, and it achieved better results compared with a pipelined baseline. Zhu et al. (2010) [ 7 ] proposed a joint segmentation and POS-tagging system based on undirected graphical models which could make full use of the depen-dencies between the two stages. Shi et al. (2015) [ 8 ] proposed a hybrid model of Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) which could exploit possible correlations among intent classi cation and slot ll-ing. Lee et al. (2015) [ 9 ] introduced a new tag addition method turning utterance classi cation task into sequence labeling task, then classi cation and slot lling could be analyzed by one sequence tagger. Duh (2005) [ 10 ] proposed Factorial Hidden Markov Models (FHMMs) with additional cross-sequence dependencies, enabling information sharing between POS-tagging/chunking subtasks. Li (2010) [ 11 ] studied four joint learning approaches on various sequence labeling tasks. tions between sub-tasks by jointly modeling two tasks. Most of the joint models aim to model two speci c tasks, i.e. the joint model is task speci c. For example, a joint model for word segmentation and POS-tagging is not suit for intent iden-ti cation and slot lling. We argue that there are some common things behind different joint tasks. High level tasks receive information from low level tasks, while raise some constraints on low level side by hierarchical structures. Both of the interactions between two levels and the hierarchical structures for the joint tasks have important in uences on the performances of both tasks. This paper therefore considers to build a joint model that can deal with different types of combinations of two tasks by balancing the interactions and hierarchical con-straints between tasks in different levels. For example, a single model (or its slight variants) can deal with segmentation and POS-tagging, intent identi ca-tion and slot lling, and others.
 They are used to deal with a wide types of joint tasks without signi cant mod-i cations. The model has two-layer LSTM, each layer deals with one task. The two LSTMs takes both interactive and hierarchical information into considera-tion. Hierarchical relations are found to be very important in our experiments for most of joint tasks. It is not seriously considered and discussed in most previous work. All parameters in two layers are estimated together to minimize a joint ob-jective function. Experimental results show that the proposed hierarchical model outperforms non-hierarchical methods in diverse tasks.
 posed model in detail; Section 3 presents the tasks and experimental results; Finally, Section 4 draws conclusions.
 LSTM is the basic unit of models. We give a brief introduction to LSTM, and then propose our models one-by-one. 2.1 LSTM LSTM model [ 12 ] is widely used in NLP since it can deal with arbitrary-length sequences of input. It alleviates the problem of gradients exploding or vanishing in Recurrent Neural Networks (RNNs) by introducing a memory cell. Commonly, a memory cell is composed of four components: an input gate, a forget gate, an output gate and a memory cell. The input gate controls how much information will be updated in memory cell, the forget gate controls how much information from previous time step to be remembered, and the output gate controls how much information will be outputted to next memory cell. At time step t , the hidden state vector h t is calculated as following: where i t , f t , o t are the gating vectors representing the input gate, the forget gate and the output gate respectively. x t is the input at the current time step. denotes the sigmoid function and  X  denotes elementwise multiplication. W ( i ) , U of input data. They can be created by stacking multiple LSTM hidden layers on the top of each other [ 13 ]. At time step t , the hidden state vector of layer n -1 is the input of layer n : where W ( m;m ) denotes weight matrices between two layers. 2.2 Hierarchical LSTM and Training We rstly derive our basic joint model from two-layer LSTM for one basic type of joint tasks, and then propose its variants which t to different types of joint tasks.
 tion task and a word-level sequence labeling task for a sentence. Examples for this type of joint tasks include intent identi cation and slot lling, sentiment classi cation and sentimental elements extraction, and so on.
 sentence, Z be the label set for each word. A joint model assigns a y 2 Y to the sentence and z 2 Z for each word. A hierarchical model called HLSTM (Hier-archical LSTM) including a sentence-level classi cation at bottom and a token-level sequence labeling on top is therefore proposed. The structure of HLSTM is shown in Fig. 1(a) .
 of HLSTM. Follow the Eq.( 5 ) we get the last time step of hidden state vector h n of lower LSTM, and the hidden state vector is fed to a softmax classi er. Then probabilities are estimated for sentence label where W (1) is a weight matrix for softmax classi er with respect to lower LSTM, and b (1) is a bias term. For each time step t, h (1) t is still the input of upper LSTM unit. Then we estimate probabilities for sequence output tags where W (2) is a weight matrix for softmax classi er with respect to upper LSTM, b (2) is a bias term.
 U ( u ) , W (1) , W (2) g (bias items are omitted). Given a set of training set D , the regularized objective function is the loss function J ( ) including a l 2 -norm term: error. They are de ned as: = 0, the model only cares about token-level labeling. The network architecture degenerates into standard two-layer LSTM only for token-level labeling task. At the other extreme, if is set to 1, only sentence-level classi cation is considered. the token-level sequence labeling is on the top of the model. The inverse situation is another choice, i.e. the sentence-level classi cation is on the top and the token-level sequence labeling is at the bottom of the model. We call it Inverse HLSTM (I-HLSTM). It has same optimal objective and training algorithm.
 cludes two sequence labeling tasks interacting with each other. For example, POS-tagging and word segmentation, chunking and POS-tagging and so on. We use same hierarchical frame to deal with this type of joint tasks by extending basic HLSTM to HLSTM-L (HLSTM for two Labeling tasks). The structure of HLSTM-L is shown in Fig. 1(b) .
 for substituting class error in HLSTM with sequence label error. Like that in HLSTM, HLSTM-L can also be reversed by exchanging the LSTMs in two layer. We call the Inverse HLSM-L (I-HLSM-L).
 not modeled explicitly. For natural language tasks like word segmentation and POS-tagging, dependencies between sequence labels are important.
 by extending HLSTM-L. HLSTM-DL keeps the same frame with HLSTM and HLSTM-L. The structure of HLSTM-DL is shown in Fig. 1(c) , where dependen-cies between labels in lower level LSTM are taken into consideration explicitly. tag transition matrix A . Every output tag sequence is given a score by summing tag transition score and tagging score: possible tag sequences. Then the predicted tag sequence y ( i ) can be computed as: where  X  (^ y ( i ) ;y )) = parameter, the loss is proportional to the number of words with incorrect tags in the proposed tag sequence.
 a little bit different since the correct tag sequence is unknown, thus Eq.( 13 ) replaced by Moreover, L S in Eq.( 10 ) is modi ed to Eq.( 15 ): We can also have I-HLSTM-DL which is the reversed version of HLSTM-DL. Three different joint tasks are used for evaluating our models experimentally. They are Intent Classi cation and Slot Filling in SLU, POS-tagging and Chunk-ing, Chinese Word Segmentation and POS-tagging.
 tion that hyper-parameters of all our models are tuned only by trying a few different settings in all experiments. We choose the smaller networks to achieve \reasonable" performances rather than picking the best hyper-parameters care-fully to achieve their top performances. We employ standard experimental setups for models: for each group of tasks, we use AdaGrad [ 15 ] with mini-batches [ 16 ] to minimize the objective function. Derivatives are calculated from standard back-propagation [ 17 ]. The model achieving the best performance on the devel-opment set is used as the nal model to be evaluated. The overall performance of joint model is simply evaluated by averaging performances of two tasks. All models are implemented in Theano [ 18 , 19 ].
 3.1 Intent Classi cation and Slot Filling Joint intent classi cation and slot lling is typically rst type of joint tasks, where the former is a classi cation task and latter is treated as a sequence labeling problem.
 Basic information about this corpus is listed in Table 1 . The number of intent classes is 13, and the number of different slots is 9 including the O label. layer of HLSTM deals with intent classi cation and upper layer aims at slot lling task. For I-HLSTM, the lower layer deals with slot lling and upper layer is for intent classi cation. Lee's model [ 9 ], which is a CRF-based joint model for intent classi cation and slot lling simultaneously, is compared. The open source toolkits is used 3 .
 1) Both HLSTM and I-HLSTM perform better than CRF model on two tasks. It is similar with the conclusion in Mesnil et al. (2013) [ 21 ] and Mesnil et al. (2015) [ 22 ]. They showed RNN model is better than CRF in slot lling only task. 2) Although the overall performances of two HLSTMs are almost same, HLSTM achieves a slightly higher performance and they show different strengths. HLSTM prefers to slot lling task, while I-HLSTM prefers intent classi cation. Reminded that slot lling is on the top of HLSTM, and intent classi cation is also on the top of I-HLSTM, we think there is a preference for top-layer LSTM in our two-layer LSTM models. It is a helpful hint on how to make choice between HLSTM and I-HLSTM. of model change with . We can nd several points from Fig. 2 . 1) It is clear that both HLSTM and I-HLSTM achieve best performance when  X  = 0 and  X  = 1. It means two tasks can help each other if is properly given. = 0 : 3 seems to be a good choice. On the contrary, improper combinations might hurt both of them. 2) Again, we nd different strengths of two HLSTMs. Hierarchical structure help tasks on top-layer receive better scores. 3.2 POS-tagging and Chunking Both POS-tagging and chunking can be regarded as sequence labeling problems. HLSTM-L and its reverse version are therefore used for joint modeling of the two tasks.
 chunking. Basic information about this corpus is listed in Table 3 . Their training data consists of sections 02-21 and test data consists of section 00. 10% of the training set is split into a development set. The corpus contains 45 different types of POS tags, and 23 types of chunking tags respectively. A transformation-based learning model [ 23 ] was used for comparison since it used same dataset. 1) Both HLSTM-L and I-HLSTM-L are comparable with Florian et al. (2001). They achieve improvements on chunking, but a little lower in POS-tagging task. 2) HLSTM-L who models the hierarchical structure of joint tasks performs better overall. Similar to that in previous model, an appropriate value of is crucial to the performance of both tasks. The impact of is shown in Fig. 3 . be bene t by joint training. POS-tagging gets a performance boost and achieves best performance by introducing moderate chunking information, the same situ-ation applies to chunking. = 0 : 6 seems to be a good choice. 2) Two HLSTM-Ls show different strengths on different tasks. HLSTM-L prefers to chunking, while I-HLSTM-L prefers POS-tagging. 3.3 Chinese Word Segmentation and POS-tagging Both Chinese word segmentation (CWS) and POS-tagging can be regarded as sequence labeling problems. Instead of using HLSTM-L, the in uence of depen-dency relation is taken into consideration, and HLSTM-DL is employed to model the joint tasks. Since the result of POS-tagging includes the CWS, it is not pos-sible to put CWS on the top layer with POS-tagging at the bottom, the reversed version will not be included in this experiment. Inspired by Pei et al. (2014) [ 24 ], bigram embeddings are used as models' inputs as well.
 lar used newswire dataset, the NLPCC 2015 dataset collects informal texts from Weibo. The information of the dataset is shown in Table 5 . 10% of the train-ing set is split into a development set and keep the remaining 90% as the real training set. Each character is labeled as one of f B, M, E, S g to indicate the seg-mentation. For POS-tagging labels, each is the cross-product of a segmentation label and a POS tag, e.g. f B-NN, M-NN, E-NN, S-VP, ... g .
 NLPCC 2015 shared task, is used as baseline. The templates are unigram fea-ture, bigram feature and trigram feature. Word segmentation can be inferred from the output.
 compared to HLSTM-L. Even with fewer features, HSLTM-DL gets a much better result on CWS and performs slightly better on POS-tagging compared to CRF model.
 signi cant boost to CWS. The same analysis of also applies to this experiment like previous ones.
 on different kinds of combinations of NLP tasks. In most situation, hierarchical models achieve better results than strong baselines. Top-layer in hierarchical model is more powerful. Hierarchical model re ecting task hierarchy is likely to achieve higher overall performance. The detailed analysis of experimental results shown that the hyper-parameter of hierarchical joint model can leverage information in one task to another, thus bring signi cant performance boosts to both tasks. We have presented a hierarchical LSTM model and several its variants that can handle different kinds of combinations of NLP tasks. Experimental results on three different combinations of NLP tasks show promising results. In most sit-uation, they outperform strong baselines. Hierarchical relations and correlation information between different layers are also discussed experimentally. Tasks get better performances if they are on top-layer. We believe they provide a series of potential solutions for joint learning of two NLP tasks.
 structure has been shown to be a good choice for arranging two NLP tasks, it is still not so clear how the information of two tasks interacts each other, especially how tasks at the bottom transmit supervision to the tasks on the top layer, and vice versa. Another problem is how to character dependency relation between labels in sequence labeling tasks.
 Acknowledgments. This paper is partially supported by National Natural Science Foundation of China (No 61273365), discipline building plan in 111 base (No.B08004) and Engineering Research Center of Information Networks of MOE, and the Co-construction Program with the Beijing Municipal Commission of Education.
