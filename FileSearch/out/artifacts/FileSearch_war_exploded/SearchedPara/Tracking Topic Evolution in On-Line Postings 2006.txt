 The past decade has witnessed the proliferation of computers and an explosive growth in electronically stored data. Contributions by the data mining commu-nity have been helpful for managing data archives, however, some traditional approaches are inadequate, because the volume of output from analyzing mas-sive archives is too large for human understanding. Temporal analysis of dynamic databases has added another dimension to an already difficult technical prob-lem. Nevertheless, impressive progress has made in understanding dynamic text documents sets. Examples of work in this area include analysis of news articles 1 , Web search logs, e-mails, blogs 2 , customer call center data, and US patent data. Recently, researchers have successfully identified and tracked the evolution of topics in massive archives of text documents  X  over 100 years of journal archives [1]; 9 months of personal e-mail, 17 years of Neural and Information Processing Systems , and over 200 years of presidential state-of-the-union addresses [7]. The works are detailed and accurate, but require extensive computations.
The focus of our work is to develop computationally fast methods to perform real-time analysis of the content of very large and dynamically evolving sets of unformatted text data, such as on-line discussion records, internet postings, and e-mail. Detailed analysis is forsaken for speed since prospective participants need a quick means for browsing topics and catching up on on-going discussions.
Two works that are very closely related to ours analyzed on-line discussion data from a different persp ective. Spangler et al. [6] developed a system to help human analysts in interactive text mining using vector space models (VSMs) and a k-means algorithm to generate the classes for postings from three on-line discussion sessions, each of which produced thousands of postings by thousands of participants around the world over a 72-hour period: ValuesJam (2003), IBM WorldJam (2004), and HabitatJam (2005). The goal of our work is not just to aid, but replace human labor as much as possible.
 Murakami et al. [4] developed a system to analyze a dataset from Innovation Jam (2006) by creating network graphs of relationships between postings and the importance values of individual postings. Graphs of postings (nodes) and their relationships (edges, lines) are based on data from the  X  reply to  X  X eature. Phrases that express opinions about previous postings (e.g.,  X  I like this idea  X  or,  X  Idisagree  X ) are extracted as a first step for evaluating the importance of a posting. Five types of opinions are identified using pattern dictionaries: applause, agreement, disagreement, suggestion ,and fact indication . Opinions that are a direct reply to a posting are given greater importance weight than opinions that are a reply to a reply to a posting. Users can quickly spot a posting that is central to discussions since the size of a node is proportionate to its importance value.
We developed a prototype to quickly and automatically identify topics and track their flow during on-line discussions, i.e., unedited text postings by thou-sands of different people. The system identifies topics through computationally inexpensive and fast approximation of cluster centroids. To track the evolution of topics, documents are assigned to overla pping slices of manageable size, based on time stamp information. Documents on a topic of interest that lie in an overlap are used to find similar documents in the subsequent slice (Section 2). We tested our method through implementation studies using our prototype and real-world data from I-Jam, a world-wide on-line discussion session (Section 3). The final section proposes some directions for future research (Section 4). This section introduces algorithms for identifying and tracking topics. We present an overview of steps performed by our prototype before details of the algorithms.
To enable fast processing, our system divides a large set of documents into overlapping time slices, each of which contains several thousand documents (Fig. 1). Next, it applies a text parser [5] and automatically constructs a VSM with term frequency-inverse document frequency (tf-idf) weighting for each time slice since topics and word usage change over time. We use the cosine of the angle defined by vectors to determine do cument similarity. When the collec-tion of documents is large, principal component analysis (PCA) will be used to address the curse of dimensiona lity problem. The VSM will be projected onto a subspace spanned by the first few hundred principal components of the document-keyword matrix. PCA-based dimensional reduction also resolves some difficulties caused by noise (e.g., misspellings, acronyms, slang), synonymy and polysemy and reduces the need for constru cting domain-specific dictionaries for unedited, unstructured text.

Our system applies a topic identification algorithm (Algorithm 1) to the first time slice to find sets of documents on similar topics. Documents in the overlap between the first and second time slices are identified by their document num-bers, which are assigned in chronologica l order with respect to posting time. Doc-uments that lie in the overlap help track a topic. Their corresponding document vectors in the second time slice are input as seeds (queries) to find documents on similar topics that were posted during the latter half of the second time slice (Fig. 1). The system carries out an analogous procedure to identify and track topics in subsequent time slices.
 Topic Identification and Tracking 1. Assign documents in collection to overlapping time slices. 2. Construct a vector space model for each time slice. 3. Apply Algorithm 1 to the first time slice to identify topics. 4. Use Algorithm 2 and documents in the overlap between the first 5. Follow an analogous procedure for subsequent time slices.
Our prototype uses principal components as queries because the vectors point in directions with the most information about the contents of a database. Ini-tial steps of a clustering algorithm identify sets of documents on a topic [2]. The algorithm must be a soft clustering algorithm (i.e., it permits cluster over-laps), because topics that appear in postings may be inter-related. Since our goal is to find topics (not clusters), our system navigates around the difficulty of determining borders of clusters and the cohesiveness of clusters by using only documents that lie in the core of cluster s to estimate the centroid. The centroid vector gives a keyword description of topi cs covered in the cluster. Empirical ob-servations from implementations indicate that using 20 to 30 documents in the cluster core is sufficient for computing a good approximation of the centroid for topic identification. Our system does not automatically use 30 top ranking doc-uments to approximate a centroid, beca use some small clusters may have fewer than the default number. To check the clu ster size, our prototype computes the plot of document relevancy (vertical axis) as a function of document relevancy ranking (horizontal axis). If the slope undergoes a large change before the 30th ranked document, the system only considers documents left of the change point to approximate the centroid (Fig. 2).
 Algorithm 1: Topic Identification 1. Make vector space model of documents. 2. Compute principal components of document vectors. 3. Perform dimensional reduction (if necessary). 4. Find documents on the same topic. 5. Find set of documents to approximate centroid of topic. 6. Compute weights of keywords for centroid
To track the evolution of topics, divide the document set into overlapping time slices, and construct a new VSM for each time slice since keywords may become obsolete, and important new keywords may be introduced over time. Identify topics and relevant documents for the first time slice using Algorithm 1. To track a topic to the second time slice, use Algorithm 2. Identify documents that lie in the slice overlap using document identification numbers that have been assigned in chronological order, a ccording to the posting time. Documents that lie in the latter half of a time slice are used to find similar documents and topics in next slice (Fig. 1). An analogous process for identifying and tracking topics is carried out for subsequent time slices.
 Algorithm 2: Topic Tracking 1. Assign docs to overlapping time slices (Fig. 1). 2. Perform VSM of docs for each time slice. 3. Identify topics and relevant docs in first time slice (Alg. 1). 4. Track topic from the first to second time slice by identifying 5. Find new topics in second time slice (as in Step 3) 6. Track topic in subsequent time slices using analogous procedure We conducted implementation studies with real-world data to identify and track topics in a collection of opinions (text) that were posted during the first phase of 2006 Innovation Jam (I-Jam), an on-line brainstorming session organized by IBM Corporation 3 [6]. Participants could post new trains of thought or re-ply to existing postings in any one of several categories, known as forums .Al-though postings were more formal than chat rooms, they still contained mate-rial not commonly found in edited text (e.g., typos, grammatical errors, slang, emoticons). Participants were permi tted but not required to read posts by others.

During the first Jam, participants were asked to rate ideas in postings, help analyze events, and communicate findings to other participants. This arrange-ment led to an unintentional bias favori ng ideas introduced early on. Over suc-cessive Jams, text mining technologies we re introduced to help analyze postings. However, the number and rate of postings has increased so much that a new generation of more powerful tools are needed to:  X  find and identify topics under discussion;  X  determine the discussion set to which each untagged posting 4 belongs;  X  analyze and format results in near real-time during a Jam so people can  X  analyze postings in-depth after conclusion of a Jam for deeper understanding Data analysis software reduces labor and is relatively free of unintentional human bias. We believe that systems, such as ours, that can analyze large collections of text documents in (almost) real-time, w ith low associated hardware costs, will interest Jam organizers and decision-makers for large organizations. 3.1 2006 Innovation Jam Data Set The 2006 I-Jam consisted of two distinct phases. The aim of the first was to create new ideas, then concrete proposals for design and implementation of prototypes. The ideas were reviewed off-line to deter mine a set of top proposals. The focus of the second phase was testing and refining the top proposals. We analyzed over 37,000 opinions posted during the first phase over a 78 hour period [4] on four pre-selected subjects:  X  Going Places -transforming travel, transportation, recreation &amp; entertainment;  X  Finance &amp; Commerce -the changing nature of global business and commerce;  X  Staying Healthy -the science and business of well-being; and  X  A Better Planet -balancing economic and environmental priorities. Omission of 200-300 postings in languages other than English, left a final set of 37,037 documents. We divided the data set into overlapping time slices consisting of 8000 documents each. Each slice had 50% overlap with those directly preceding and following it so that all documents belong to one overlap, with the exception of those in the first half of the first time slice and the latter half of the last time slice. For larger data sets, more docume nts may be assigned to each slice with a different degree of slice overlap, depending on the intended use, properties of the data, hardware requirements, and financial constraints.

A shallow parser extracted keywords (nouns, verbs, adjectives) and conducted parts-of-speech tagging [5]. Standard and domain specific stopwords were deleted, as were words that appeared less than 10 times in a slice. If a larger number of documents are assigned to each slice, then the cutoff point for removing rare terms should be set higher. Principal components of the document vectors in each slice were computed using SVDLIBC, version 1.34 6 . Each slice contained 8000 documents and approximately 3000 keywords. Given extracted keywords and their frequency counts, the computations to determine the tf-idf VSM and all 8000 principal components took about 10 minutes with no special tuning for each time slice on an IBM ThinkPad T42, model 2373-J8J with 1.0 GB of main memory. We can expect much faster performance in a working system since only the first several hundred (not all) components will not be computed. Computations will be easily under a minute with tuning and increase in the main memory.
 3.2 Experimental Results To find topics in the first time slice, principal components were input as queries, and the most relevant documents were used as a queries to identify important topics. Typical results from experiments are given in Table 1 for the second prin-cipal component, which consists of major keywords: energy, solar, power, panel, fuel, electricity, water, electric, car, and heat in the positive direction and pa-tient, doctor, medical, and health in the negative direction. The relevancy ranking curve for document distances to the seco nd principal component experiences a sharp drop-off just after the tenth ranked document (Fig. 2), so we use it as the cut-off point for approximating the centroid (note the 30 document default). A number of interesting topics were seen to evolve over ten time slices, as shown by two representatives examples below: food, health, exercise and methods for payment .
 Topic 1: food, health &amp; exercise. The weights of twelve predominant key-words had relatively large changes during discussions: junk, vegetable, cancer, diet, eat, calorie, healthy, cook, exercise, fat, nutritional ,and price .Tracking three keywords ( eat, healthy, exercise ) with relatively large changes over 10 time slices shows the flow of topics in the postings (Fig. 3). During the sixth time slice, curves for healthy and exercise have local maxima, while eat has a local minimum. However, by the eighth time slice, eat has its global maximum, while healthy and exercise are in decline. Samples of unedited quotations from the sixth and eighth time slices (marked with 6 or 8), with typos, acronyms, and facial icons, are consistent with these findings: (6) Title: Healthy Living! (7/25/06 11:41 PM) ... Finally someone who nows and takes care about health c We should enjoy our lives and work ... (6) Title: Healthy Living Day Every Month (7/25/06 10:28 PM) ... we need to step up on providing and urging IBMers into a better and healthy lifestyle ... This might include exercises, games, h ealthy foods, etc. A healthy IBMer will definitely contributes better than an unhealthy one!!=) (6) Title: Need for Gym! (7/26/06 4:22 AM) Yes, we needs a Gym at IBM. This will keep us healthy and fresh ... (8) Title: How about checking before eating (7/27/06 12:25 AM) Why check after eating? Why not before eating? (8) Title: Education on Nutrition (7/27/06 2:16 AM) Yes i agree, being active is just as important as eating the right food when it comes to staying healthy .but what really are the best foods for us to eat. Topic 2: Methods for payment &amp; security. The weights of four keywords had relatively large changes over 10 time slices: cash, payment, security and signature (Fig. 4). During the third time slice, payment and security dominate the discussion. By the fifth time slice, the discussion shifts to signature and methods for authentication. There is another shift in the discussion by the eighth time slice to security issues. Some samples of unedited postings from the third, fifth, and eighth time slices that support the findings using our system are: (3) Title: Share credit card trasaction data (7/25/06 1:19 AM) ... many people owns multi credit cards of different banks, if the data of these transaction data can be shared between banks, bank and credit card owner will both benefit. of cause, there may be some privacy and security issues ... (5) Title: Change signature to finger print/eye identity (7/25/06 8:58 PM) I think we need alternative signature wh en we use our credit card. If your arm broken and you canft sign, how can we use our credit card? How about using our finger print or eye identitiy? ... (8) Title: Some issues (7/26/06 10:07 PM) ... There are however issues that have to be considered specifically: 1. Who will administer the card? 2. Addi-tional equipment may be required in order to use the card properly (or rather, will available equipment be able to read it?) 3. If the card is lost/stolen, the replacement process may not be so trivial ... We developed and implemented a prototype to automatically identify and track topics in a text data set consisting of ti me-stamped postings from on-line dis-cussions. The prototype divides the dataset into overlapping time slices and au-tomatically computes a vector space mod el and principal components for each time slice. Principal components are used as queries to find postings on impor-tant topics. Postings in the overlap of time slices are used to track topics from the earlier time slice to the subsequent time slice. Our prototype shows the evolution of topics in discussions by displaying changes in the contributions of keywords in relevant documents in each time slice. The results can help users understand the overall flow of a discussion without having to read individual postings.
In future work, our first task is to develop methods to speed up computa-tions and automate tuning for arbitrary data sets, for example, determining the minimum number of principal components to find topics and the  X  X ight size X  for time slices. Small time slices enable fast computation during PCA. However, they may not reliably find slowly evolving topics or minor topics. Large time slices are better for finding major and minor topics, but incur a higher computational cost, especially if PCA-based dim ensional reduction is needed.

A second area to explore is use of supplementary information such as the  X  X eply-to X  feature input by users in on-line postings and e-mail. The reliability of reply-to information depends on the reliability of participants who post and may be inaccurate. Merging information output by our prototype graph-based analysis systems is likel y to be highly beneficial.

A third area of interest is improvement of the GUI. We conducted some pre-liminary studies with a real-time binary k-means-based method for displaying an overview of related topics in the database [3]. The bird X  X  eye, macroscopic view is useful for a quick overview of results for users who do not have a clear idea of what to search and need to browse before zooming in on a topic of interest.
A fourth area to investigate is refinement of the mathematical model for doc-uments. Currently, a very good shallow p arser extracts ter ms for vector space modeling. We would like to incorporate advanced natural language processing technologies, such as key phrase extraction and identification of patterns that express sentiments (e.g., positiv e, negative, neutral opinions).
 Acknowledgments. The authors thank the IBM Natural Language Processing team at the TJ Watson Lab for use of their English parser and A. Murakami at IBM Research, Tokyo for providing data and helpful technical discussions.
