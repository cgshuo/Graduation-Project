 Duplicates in a dataset are multiple representations of the same real-world entity and constitute a major data qual-ity problem. This paper investigates the problem of esti-mating the number and sizes of duplicate record clusters in advance and describes a sampling-based method for solv-ing this problem. In extensive experiments, on multiple datasets, we show that the proposed method reliably es-timates the number of duplicate clusters, while being highly efficient.

Our method can be used a) to measure the dirtiness of a dataset, b) to assess the quality of duplicate detection configurations, such as similarity measures, and c) to gather approximate statistics about the true number of entities rep-resented in the dataset.
 H.2.0 [ Database Management ]: General Algorithms, Theory Data integration, estimation, duplicate, cluster, pair
Duplicates in a dataset are multiple representations of the same real-world entity and constitute a major data quality problem. The consequences range from unsatisfied employ-ees and customers, to incorrect analyses that lead to poor business decisions.

Duplicate detection (aka. record linkage, entity resolution, etc.) is the task of finding duplicate records, so that they can be subsequently eliminated or merged. Various tech-niques have been proposed to solve the two main challenges: When to declare a pair or cluster of records as a duplicate and how to efficiently find them all without na  X   X vely search-ing the complete Cartesian product of possible record pairs. While the first challenge is usually solved by employing spe-cific similarity measures and thresholds, the latter is solved by employing smart candidate selection techniques, such as the Sorted Neighborhood (SNM) [7] or (non-)overlapping Blocking [11].

However, all methods have in common that different pa-rameters (e.g., thresholds, window sizes, and blocking keys) influence the result in different, often non-obvious, ways. Usually, these parameters are iteratively tweaked in succes-sive runs and the respective outcome evaluated against a validation set or by close examination. This process con-sumes much time and resources and can be frustrating.
In practical scenarios, prior to the full and costly detec-tion of all duplicates, the user might be rather interested to quickly gain a vague idea on the approximate sizes of dupli-cate clusters in the dataset, based on a user-defined similar-ity measure. If done in a simplistic way, the estimation of the cluster sizes may be as expensive as the retrieval of the du-plicates itself. Hence, to be practically relevant, any method for estimating the numbers and sizes of duplicate clusters in a dataset would have to run much faster than any dupli-cate detection algorithm on the same dataset. In addition to being a means for estimating the degree of dirtiness of a dataset, such a method could be used for fine-grained analy-ses of the distribution of duplicates and thus help estimating the costs of various duplicate detection strategies.
In this paper, we propose a reliable and highly efficient technique to estimate the sizes of duplicate clusters in a dataset. The overarching vision of this work is to provide useful tools for interactive data cleansing systems, and thus provide users with a means for quickly calculating approxi-mate statistics, planning resources accordingly (e.g., for the query optimizer), and analyzing data cleansing strategies.
In the following, we informally describe the challenges and formally define the key concepts.

Definition 1 (Duplicate Clusters). The output of a duplicate detection run on a dataset R is the set of dupli-cate clusters C R , where each C  X  C R represents the set of all representations of the same real-world entity according to the used duplicate detection algorithm. Formally, C R partition over the equivalence relation  X  , i.e., the quotient set R /  X  := { C  X  X  |  X  r i ,r j  X  C : r i  X  r j } = C R .
We are especially interested in the size distribution of the duplicate clusters.
Definition 2 (Cluster Size Histogram). The clus-ter size histogram H R is the absolute histogram over the number of clusters of a certain size from the set of all clus-ters C R . We denote the i-th entry in the histogram with H
R = |{ C  X  C R : | C | = i }| corresponding to the number of clusters of size i .

The duplicate clusters and our estimations highly depend on the parameters of a duplicate detection run . In this paper, we assume the traditional, three-phase duplicate de-tection approach, which starts with a candidate selec-tion method, such as SNM or Blocking, to retrieve a list of record pairs that have a high probability to be dupli-cates. It then performs the candidate comparison , which may apply similarity measures and fixed thresholds on the candidates. Finally, a clustering algorithm (e.g., transi-tive closure) generates duplicate clusters from the given set of pairs. Most other duplicate detection approaches can be simulated with this approach, so that our estimation method is transferable to other approaches as well.

All algorithms and parameters influence the result of a duplicate detection run. Therefore, our estimations need to be considered in the context of the actual parameters and we accordingly define the problem we aim to solve: Given a configured duplicate detection run on a dataset R , Duplicity Estimation determines the estimated cluster size histogram H R 0  X  H R with much less effort than the run itself would take.
We envision four types of applications of our presented techniques.
 Data Quality Estimation. In data integration processes, experts have to decide which data sources to integrate. Usu-ally, they need to trade effort with the expected data quality gain. Our methods help with both: We can estimate the du-plicity of one data source as an indicator for its quality and we can estimate the overlap between two data sources. Interactive Systems. Tweaking the parameters of a du-plicate detection task can be cumbersome, especially if it takes a long time to calculate the results. With our tech-niques, users can quickly see the approximate numbers of duplicates and some examples.
 Approximate Statistics. For complex statistical analyses to reach business decisions, additional data sources are often integrated in an ad-hoc fashion. Since these statistics usu-ally aggregate data, a full duplicate detection unnecessarily delays the results. With our techniques, we can efficiently answer questions, such as  X  X ow many distinct customers do two companies have after a potential fusion? X .
 Query Optimization. Recent development in (scientific) data workflows aims to optimize a large variety of operators. A complete integration of these operators into the optimizer requires an estimation of the output cardinality and reorder-ing rules. In this paper, we address the former.

We address the duplicity estimation problem with the fol-lowing contributions: (1) We analyze the effects of sampling onto the cluster size histogram in Section 2. (2) In Section 3, we develop a random walk model based on these insights to iteratively estimate the histogram. (3) Section 4 introduces a bootstrapping method to speed up the convergence. (4) We extensively evaluate our method on three datasets with different characteristics in Section 5. (5) Section 6 extends the basic model to incorporate can-didate selection and clustering.
 Sections 7 and 8 close with a discussion of related work and a conclusion.
To extrapolate findings on a sample S  X  R to the origi-nal dataset R , it is important to understand how sampling changes the number of duplicates and the cluster size his-togram. In this section, we develop a probabilistic model of the sampling process.

We denote with n = |S| the number of samples, N = |R| the size of the original dataset, and with r or r i record in R , and consider random sampling with a sample rate p ( r  X  S ) = n N . In the following, we first examine how the number of duplicate pairs changes for a random sample and then quantify the expected cluster size histogram.
In some cases, it is already enough to know the num-ber of duplicate pairs instead of the complete cluster size histogram. For example, when integrating two clean data sources (i.e., without duplicates within each source), we would match only records from different sources and would split clusters having a size larger than two.

We define duplicates as a subset of the pair set , such that each pair satisfies the equivalence relation  X  .

Definition 3 (Pair set). A pair set of R contains all pairs of different records of R once: R
Definition 4 (Duplicate pairs). The set of the du-plicate pairs D R contains all pairs of duplicates in R : D R = { ( r i ,r j ) | ( r i ,r j )  X  X  &lt; 2  X  r i  X  r j } . We now inspect the ratio of duplicate pairs in the sample S with respect to the original dataset R . First, we estimate how many duplicate pairs D S can be found in the sample. The expected number of duplicate pairs E [ |D S | ] depends on the probability p ( d  X  D S ) that we sample a duplicate pair d  X  X  R in the sample S .

Because random sampling is independent of the probabil-ity that a pair is a duplicate, the second probability is equal to the probability that we draw a specific pair during sam-pling p ( d  X  S &lt; 2 ). We can further factor in that we have 2 possibilities to draw a pair from X
Hence, the number of duplicate pairs increases quadrati-cally with the sample size.
 Example. Consider a dataset with 1000 tuples and 500 du-plicate pairs. If we sampled 100 records, we would expect |D a 200 records sample, the expected value would be  X  19 . 92.
Apparently, extrapolating from such a sample to estimate the original (i.e., actual) number of duplicates is highly sen-sitive to the sample variance. In the above example, we should receive five duplicates in most samples to obtain a a slight variation of one sampled duplicate changes the result by approximately 100 duplicate pairs. We now generalize the model to arbitrary cluster sizes. Similar to estimating the number of duplicate pairs, we use the number of permutations that can occur during sampling.
Our problem is related to the multivariate hypergeometric distribution; that is, drawing a sample without replacement from a multi-type population. We can map our problem to drawing a number of colored balls without replacement from an urn: Each different duplicate cluster of size i is represented by i balls of the same color. However, we are not interested in how many balls of a specific color are drawn but only in how many balls of the same color would be drawn on average if the random sampling experiment was repeated infinitely many times.

We can thus model our expected value by using the indi-cator function J | C | = i K that is 1 iff the size of a duplicate cluster C is i and 0 otherwise. We iterate over all possible events that can yield a duplicate cluster of size i in a sample of size n . Each outcome is weighted with N n  X  1 , i.e., the re-ciprocal to the number of possibilities of randomly sampling n out of N elements without replacement:
The first sum iterates over all clusters in R and the sec-ond sum enumerates all possible sample outcomes w.r.t. a specific cluster. Now, we can replace the second sum with the probability that we draw i elements from a cluster in the original dataset, which can be expressed as: Intuitively, there are N n possible sample permutations. For a cluster of size i in the sample, we draw i records from the cluster C , add n  X  i records from the remaining dataset R\ C to fill up the sample, and normalize the result over all sample permutations.

Because the probabilities of drawing i elements from two different clusters with the same sizes are the same, we can simplify the calculation by collapsing the cases. For each cluster size k in the original dataset, we calculate the prob-ability once and multiply it by the number of clusters of the given size H k R .
 Table 1: Four different cluster size histograms for a dataset of size 1000 (left) and the expected his-tograms from sampling 100 records (right).

For ease of presentation, we assume a b = 0 for a &lt; b in this paper. Alternatively, ranges in sum formulas must be adjusted to guarantee a  X  b . Now we are interested in understanding which clusters the sampled elements come from. Hence, we define a random variable X i S,n representing whether an element from a cluster of size i occurs in a sample S of size n . The probability of this event happening is:
As discussed earlier, the sum in the above formula rep-resents all possibilities to generate a cluster of size i in a sample of size n . Given that we have such a sample of size n at hand (in which there occurs a cluster of size i ), the probability of picking an element from an i -cluster is i/n .
Table 1 shows four different cluster size histograms of a dataset with 1000 records and corresponding expected his-tograms for a sample of 100 records calculated with Equa-tion 5. The first row constitutes a sanity check: If all 1000 records are duplicate-free, all 100 samples also must be duplicate-free. The second row corresponds to the previous example of 500 duplicate pairs in the previous section and indeed returns the same result. Interestingly, in the third row, even though the dataset consists of 250 clusters with size four, sampling one complete cluster of size four is highly unlikely with a sample size of 100 (one in 43 sample runs). Lastly, the expected sampling histogram of a somewhat re-alistically distributed dataset in the fourth row shows that sampling clusters of size 3 and 4 is even more unlikely.
In the last section, we deduced Equation 5 to calculate the expected histogram from a given histogram. The calcu-lation may be interpreted as multiplication of a matrix with transition probabilities T S and the cluster size histogram interpreted as a vector ~ H R .
We could now treat the matrix multiplication as a sys-tem of linear equations, which can be exactly solved if the sample size is equal to or greater than the maximum cluster in R , i.e., |S|  X  max i ( H i R &gt; 0), and we would sample the expected numbers E [ H S ].

However, as seen in the last examples of Table 1, it be-comes increasingly improbable to sample a larger cluster with random sample resulting in two drawbacks of the exact approach. First, we could sample a large cluster despite the low odds and would thus heavily overestimate the number of clusters in the original dataset. In fact, we would even receive an impossible high number of clusters with the exact method that needs to be compensated with negative his-togram counts of smaller clusters. Second and more prob-able, we would not sample a large cluster at all and thus underestimate the number of clusters.
We present a solution based on a random walk approach to probabilistically estimate from which original cluster a sample cluster has been drawn. The main intuition is to maximize the knowledge that we can obtain from a certain random sample and then successively enlarge the sample until we are confident with the estimation.
 To extrapolate the sample histogram H S to the estimation H
R , we use the matrix T R , whose components are the con-ditional probabilities p ( X k R = 1 | X i S,n = 1), where X is a random variable representing whether a cluster of size k is present in R .
The last equation shows the two pieces of information that we need to calculate T R . The first term is the conditional probability p ( X k R = 1 | X i S,n = 1) that correspond to the mirrored entry of T S , which in turn depend only on | R | and | S | . The second term p ( X k R = 1) corresponds to an entry in ~ H
R , which we actually want to estimate. In the denomina-tor, we normalize over all cluster sizes in the sample.
Because we need prior knowledge of ~ H R to calculate T R the iterative calculation of ~ H R is an obvious choice. We combine the transition matrices from Equations 7 and 11 to a matrix M R : We clearly see that ~ H R is the principal eigenvector of M Therefore, we propose a random walk approach to start with a rough initial estimation H R (0) and iteratively refine the estimation until the convergence criterion is met.

Algorithm 1 consists of two nested convergence loops. In the outer loop (Lines 2 X 12), the algorithm first enlarges the sample and calculates the relative histogram H S by perform-ing na  X   X ve duplicate detection on the sample. In Section 6,
Input : Initial estimate ~ H R (0) ,
Output : Estimated original histogram ~ H R 1 i,j  X  0; 2 repeat 3 j  X  i ; 5 calculate ~ H S ( j ) from S ; 6 calculate T S for current sample size ; 7 repeat 10 i  X  i + 1 ; 11 until || ~ H R ( i )  X  ~ H R ( i  X  1) || 1 &lt;  X  or i &gt; j + 100; 12 until || ~ H R ( j )  X  ~ H R ( i ) || 1 &lt;  X  and | S | X  minS ; we show how advanced candidate selection can be used in-stead. It also calculates the sampling transition matrix T which depends on the current size of the sample and | R | .
The inner loop (Lines 7 X 11) performs the random walk to find a suitable H R for the current H S . In each iteration, the algorithm first calculates the current transition matrix T with the previous estimate ~ H R ( i ) . This transition matrix is then used to calculate a better estimate ~ H R ( i +1) . The inner loop terminates when the estimate converges; that is, the L1-distance between two successive estimations is below a given threshold  X  (i.e., less than  X  clusters are changed), or 100 Power iterations [8] are reached, which avoids overfitting to early samples and speeds up the overall computation.
Having an estimate for a certain, possibly small sample yields a high risk of overfitting to a poor sample. We thus rerun the random walk several times with the outer loop and enlarge the sample until 1) the estimates also converge across samples and 2) we sampled a given minimum num-ber of records. The first criterion causes the algorithm to eventually converge to a well fitting estimate for the current sample and the second criterion avoids the algorithm being stuck in local maximum for poor initial samples.
Algorithm 1 iteratively computes the principal eigenvector of the matrix M R (see Equation 14). To show that this com-putation is well-defined, we have to show that real principal eigenvectors of the matrix exist.

We know that every quasi-positive, and thus every posi-tive, matrix is ergodic and therefore has a principal eigenvec-tor. For M R , it is straight-forward to show that indeed all components have to be positive. All components of T S and T
R are non-negative, so that no component in the product can be negative. Further, the first row of T S and the first column of T R are filled with positive numbers. Because the
For n &gt; k , we calculate the more efficient s k,i = ( n which is derived through symmetry of binomial coefficients matrix multiplication each component in M R involves the multiplication of the first row of T S and the first column of T R , we can conclude that each component must be positive. Intuitively, from any estimated cluster of size k , we could have sampled exactly one element, which could have been drawn from any other cluster size. Thus, we can change the estimation for one record in each iteration to a completely different cluster size.

Moreover for such matrices, the principal eigenvectors can be effectively approximated through Power iterations [8], which is represented also in the inner loop of the algorithm. These facts lay the mathematical foundation for the correct-ness of Algorithm 1.

Theorem 1. Algorithm 1 effectively approximates the principal eigenvector of the matrices M R from Equation 14.
Another interesting fact about Algorithm 1 follows from the Law of Large Numbers.

Theorem 2. The estimated variance of H R as computed by Algorithm 1 converges to the true variance of the distri-bution of cluster sizes in R .

The estimation model reliably estimates the actual du-plicate histogram even for poor initial estimates ~ we experimentally show in Section 5. Nevertheless, we also show that better initial estimates lead to faster convergence. Thus, in the next section we provide a quick method for pro-viding a good ~ H R (0) .
In this section, we propose a bootstrapping method to quickly sample larger clusters and retrieve an initial estimate H
R (0) that covers a broad spectrum of cluster sizes. Ran-dom sampling is not adequate for quickly generating viable estimates of the proportions of larger clusters ( | C | &gt; 2, see Section 2). Therefore, our focused sampling method specifi-cally oversamples larger clusters and then counter-balances it.

Up to this point, we did not assume anything about the similarity measure used in the candidate comparison. For the following focused sample approach we have to assume monotonicity.

Definition 5 (Monotonicity). A similarity measure is monotonic if the overall similarity does not decrease if the similarity of an attribute value increases.

The assumption is quite intuitive and should hold for most similarity measures. Counterexamples occur mostly for sim-ilarity measures that are based on non-linear weighted com-binations of features or mixtures of negatively and positively weighted features. We use this assumption to sample dupli-cates in a focused way: We specifically draw from groups of samples with the same value in at least one attribute . For any similarity measure sim , the similarity between equal values is maximal and thus by monotonicity we know that for a given attribute A  X  X  :
The equations holds for attributes A that are used in the similarity measure. Nevertheless, intuitively the same Input : Sample size s , randomness ratio  X 
Output : Initial estimate H R (0) 1 S  X  random sample of s  X   X  records; 2 Choose suitable attributes A in dataset (Equation 16); 3 Initialize focused sample F  X  S ; 4 repeat 5 foreach A  X  top 3 attributes in A do 6 Randomly select non-distinct value v  X   X  A ( F ); 7 Randomly select record r  X  X \F :  X  A ( r ) = v ; 8 F  X  X   X  X  r } ; 9 end 10 until |F| X  s ; 11 Determine H F and H S from F and S ; 12 Calculate counter-balanced H SF (Equation 17); should be true for any attribute. Algorithm 2 shows the focused sampling algorithm.

The algorithm starts by choosing the three attributes that are best suited for generating duplicates with equal values in the sample. Intuitively, we want attributes where some values appear more than once, but also not too often, so that an equal value may be a good indicator for a duplicate. For example, for CD records the same title or artist make a duplicate more probable.

We empirically developed the following, straight-forward scoring function, which works well on our evaluation dataset. The score maximizes for uniform distributions with an av-erage value count of two and middle-sized string lengths: score = distinct count
For other datasets, users are free to define their own scor-ing functions, or directly select the most suitable attributes. If none of the above are feasible, we remind the reader that focused sampling is only a runtime boost, and our estimation method works also well without it.

After choosing the top three attributes, the algorithm ran-domly samples a fraction of the initial sample S (we use  X  = . 5). It then repeatedly picks records with equal values in these attributes and adds them to the focused sample. In our implementation, we use indexes on the duplicate at-tribute values to efficiently pick the records in near-linear time in relation to the sample size s .

When the focused sample is complete, the algorithm cal-culates the duplicate size histogram, which is skewed to-wards larger duplicate clusters. We counter-balance the skewed histogram H F by calculating the weighted average with the histogram over the random sample H S .

Here, p ( v 1 = v 2 ) is the probability to randomly choose a record pair with equal values from one of the three at-tributes. This probability is usually quite low ( &lt; . 01), so that the weighted average indeed resembles the expected sampling histograms (Section 2). We now use H SF to tweak any initial histogram towards the gold histogram in a few Power iterations (10 iterations of the inner loop of Algo-rithm 1).
Focused sampling results in a better initial estimate H R (0) that approaches the gold histogram up to five iterations faster than the original initial estimate and therefore speeds up the overall estimation for duplicate detection runs with compute-expensive similarity measures.
We evaluated our algorithm on three datasets with dif-ferent characteristics to demonstrate the generality of our approach. We first describe the datasets and the test setup and then examine how fast our estimates converge to the gold standard with different qualities of initial histograms. The main design goal of the estimation method is to save as many candidate comparisons as possible. Hence, we are mostly interested in the number of iterations of the outer loop in Algorithm 1, since this loop enlarges the sample and causes additional comparisons. Further, we observe the variance and discuss the accuracy. Lastly, we measure the number of iterations needed to achieve certain error bounds and measure the runtime of our method. The CD dataset consists of 750,000 CD records from FreeDB with a semi-automatic gold standard [14] created by combining automatic labeling for easy-to-classify pairs and manual labeling for hard pairs. Because of its many contrib-utors, the dataset contains 55,323 duplicates clusters, which are approximately power law distributed with cluster sizes up to 50. We use it as the main dataset for our evaluation.
A much cleaner dataset is the Customer dataset con-taining 1,039,776 person records including 89,782 artificially polluted duplicate pairs. The dataset was generated by a large industry partner to test (their) duplicate detection al-gorithms and simulates the integration result of three rela-tively clean, duplicate-free datasets with a small overlap.
On the other side of the spectrum is the Cora dataset with only 182 clusters in 1878 bibliographic records and a max-imum cluster size of 238 records. It is a real-world dataset with a gold standard [4]. Due to the relatively huge clusters, it tests the boundaries of our method.
For each dataset we used the corresponding gold standard to create a gold histogram. We use an oracle as the candi-date comparison, which simulates a perfect similarity mea-sure by looking up the result in the gold standard. Note, that we could have created other gold histograms with a na  X   X ve duplicate detection run with other candidate compar-isons; the gold histogram would be generated with the result of that particular run. With the oracle, however, we receive the most realistic gold histogram. Further, through prelim-inary experiments we discovered that  X  = 10 and minS = 5 show a good trade-off between accuracy and efficiency.
We repeat the estimation 100 times for each dataset with three different initial histograms and measure the estimated cluster size distribution after each iteration. In each itera-tion, we add random sample. We choose the following three initial his-tograms with a maximum cluster size m . Later, we use focused sample to receive better initial histograms. Uniform : Each record has the same probability to be in a cluster of size k . Because the cluster of size k in H contains k records, we normalize by 1 k : H k R (0) = 1 k Figure 1: Convergence for 10 iterations on the CD dataset with the three different initial histograms (y-axis is square-scaled).
 Power law : Half of the records are duplicate-free, a quarter is in pairs, an eights in triples and so on: H k R (0) = 1 This is a realistic distribution for most datasets. Inverse power law : We mirror the power law: H k R (0) = ployed to test the robustness of our approach.
First, we measure the convergence of the estimation on the CD dataset in Figure 1 with the three different initial vectors. Note that we use square scales on the y-axis to better observe the relatively rare, larger cluster sizes. For further improved readability, we plotted in this and follow-ing figures only a few data points; the corresponding line is generated from data points for each cluster size.

If we use the power law distribution (top), the estimate converges quickly to the gold histogram within ten itera-tions. Early iterations already result in good approxima-tions, because the initial vector corresponds well to the gold histogram. Figure 2: Standard deviation of the estimates for du-plicates and non-duplicates in the Customer dataset.
The uniform distribution (middle) needs 12 iterations to converge. After two iterations, we can already see a power law distribution, which then quickly approaches the gold histogram.

The estimation algorithm needs five iterations to change the inverse power law (bottom) into a power law and a total of 15 iterations to converge to the gold histogram. Neverthe-less, we want to emphasize that the algorithm still converges with a sample of only about 17,000 records (2.3%) to the correct result despite the completely wrong initial estimate.
Altogether, we can see that the algorithm reliably con-verges. The better the initial estimate, the faster the con-vergence occurs. For the Cora and Customer dataset, we observe similar convergence after 15 iterations. We also con-ducted a sanity check and used the gold histogram as the initial histogram and observed that the estimates do not di-verge from the gold histogram.
In the next experiment, we examine how the estimates vary throughout the 100 aggregated runs on the Customer dataset. This dataset contains only two cluster sizes (namely sizes 1 and 2), which allows a clear visualization of variances.
Figure 2 shows the standard deviation of the estimates at a specific iteration for the non-duplicates and the duplicate pairs and the three initial histograms. The average estimate quickly converges to the gold histogram within 15 iterations. From that point, the standard deviation decreases with ad-ditional iterations and is only  X  1% after 50 iterations.
The first two experiments indicate that our algorithm it-eratively fits the estimation to the gold histogram. We now want to closely examine the estimates of the irregular his-togram of the Cora dataset. The histogram exhibits a com-parably flat power law-like shape with clear peaks between cluster sizes 30 to 60 as well as peaks (individual clusters) at 127, 148, and 238.

We see that after five iterations, the curve already ap-proaches to the overall form of the distribution in Figure 3. Figure 3: Convergence for 20 iterations on the Cora dataset. (y-axis square-scaled to highlight larger cluster sizes) In successive iterations the estimate is further refined un-til also the individual peaks at cluster sizes 127, 148, and 238 are approximated. Please note that the y-axis is again square-scaled, so that we can examine the low frequencies of 1-5 appropriately.

Obviously, our algorithm fails to exactly determine the cluster sizes even after 20 iterations on this dataset. Nev-ertheless, for most use cases, the estimate after five itera-tions may be enough: The power-law distribution is esti-mated well enough and there is at least one cluster with a size over 100, which may either indicate poor data quality or an ineffective duplicate detection configuration. Further, according to our experience, larger real-world datasets are distributed more evenly, so that the hard-to-estimate peaks are less probable.
Depending on the use case, the number of iterations for an estimation may vary. In this experiment, we determine a good number of iterations to meet different error bounds.
We use two measures to assess the difference between the current estimation and the gold histogram. The root mean square error (RMSE) calculates the normalized squared er-ror between the estimate of each cluster size and the actual value. Because of the power law distributions, this measures heavily favors smaller, more frequent cluster sizes.
The earth mover X  X  distance (EMD) calculates the number of records that is needed to transform one histogram to an-other by moving records to adjacent fields. Intuitively, the distance does not penalize slight misestimations as strongly as the RMSE. For example, if a cluster of size 237 is esti-mated instead of 238, the distance is at most 2  X  237, while RMSE would quadratically penalize both the present cluster at size 237 and the missing cluster at 238. We define both measures over a gold histogram H G and the current H R . Figure 4: Root mean square error and earth mover X  X  distance for up to 100 iterations on the CD dataset.
Figure 4 shows the mean errors for up to 100 iterations on the CD dataset. The root mean square error steadily ap-proaches zero after an initial phase that adjusts the general form of the distribution. For a good initial estimate, the error immediately decreases, while the estimate needs up to ten iterations to correct worse initial estimate.

Similarly, the earth mover X  X  distance reflects the correc-tion of the general form for the inverse power law distribu-tion. For the uniform distribution the distance decreases earlier, because the shifting of the elements towards smaller clusters immediately begins. The power law distribution ex-hibits a low EMD from the start, since only few records need to be adjusted.

If users are not confident in their initial estimate, ten it-erations are needed to guarantee a good fit of the distribu-tion  X  as seen in the EMD plots. The RMSE reveals that fine-grained estimates need up to 50 iterations on the CD dataset. For better initial estimates (e.g., uniform distribu-tion), the general form is found already after five iterations. Fine-grained estimates, however, need almost the same num-ber of iterations for good and poor initial histograms.
The RMSE and EMD curves of the difficult Cora dataset in Figure 5 show a step decrease of error similar to the EMD curve of the CD dataset. For this dataset, the uniform distri-bution shows the smallest error and converges fastest. The power law surprisingly exhibits the greatest RMSE, because the number of non-duplicates is heavily overestimated (900 assumed but only 64 present). Nevertheless, the power law still represents the general form well, so that it approaches the error of the uniform distribution within three iterations. The algorithm corrects the general form of the inverse power law distribution in the seventh iteration, but keeps peaks for Figure 5: Root mean square error and earth mover X  X  distance for 100 iterations on the Cora dataset. the large clusters and thus achieves even smaller errors than the uniform distribution.

For the Customer dataset, only two kinds of errors are pos-sible and, thus, both RMSE and EMD are much smoother and start to decrease to zero at the first iteration. Both curves (not shown) exhibit the same characteristics, because of the close relation of EMD and RMSE in this case.
In the last experiment, we measure the runtime of the estimation. As before, we simulate the candidate compar-ison with a hash lookup in the gold standard. Hence, we measure the overhead of our random walk without perform-ing any actual candidate comparisons. Further, we measure the reduction ratio [1] that is defined as the ratio of saved comparisons during estimation and the total number of all pairs:
The single-threaded experiment ran on an i5 desktop PC with 16 GB RAM and Java 1.7. The times do not include the initial data loading phase, where all data is loaded and held in-memory. For all configurations, the runtime of the random walk with fewer than two seconds is negligible even for high number of iterations. Table 2 lists the runtime in milliseconds and the reduction ratio for the three datasets.
In general and as expected, the runtime increases with the maximum cluster size and the number of records. For the CD dataset, the three slowest parts of the algorithm took 90% of the runtime, where 40% of the time is spent on the random sampling and calculation of H S , 10% on the calcu-lation of T S , and 40% on the Power iterations (inner loop of Algorithm 1). For later iterations, the calculation of H and T S take relatively more time, because the sample be-comes larger. In contrast, the number of Power iterations linearly decreases from 100 to 25. Nevertheless, the over-all time per iteration increases linearly. Hence, performing rough estimations on small samples is relatively cheap. Table 2: Runtime and relative number of compar-isons for a given number of iterations per dataset.
The reduction of comparisons for the two larger datasets is enormous and solely depends on the number of iterations. As shown in the last section, coarse-grained estimation re-quires fewer than ten iterations and needs to perform only every 10,000th comparison. Of course, candidate selection techniques also save comparisons significantly, but as we show in Section 6, we can combine both techniques to fur-ther improve the efficiency and accuracy.
 Finally, the performance can be easily improved. For the Cora dataset, the calculation of T S dominates the overall time, but can be sped up with precalculated tables. For a product, H S should be incrementally calculated. Nonethe-less, the potential of our method already becomes apparent: If one assumes that one comparison takes 20 microseconds (measured in previous experiments [14]), a full na  X   X ve run needs half a year and a single-pass SNM with w = 100 1 hour, while a rough estimate in ten iterations take only 45 seconds, and a fine-grained estimate with near-perfect results in 50 iterations takes less than 4 minutes. Focused sampling. For the CD dataset, focused sampling took 10 seconds and sped up the convergence between one iteration for power law and five iterations for the inverse power law distribution. Saving one iteration reduces the estimation by 9 seconds. Therefore, focused sampling amor-tizes for fine-grained estimation with a higher number of iterations or more expensive candidate comparisons. Fur-ther, it quickly pays off for repeated estimation with differ-ent duplicate detection settings, since the focused sampling is performed only once and could be run in the background.
The random walk algorithm performs na  X   X ve duplicate de-tection over the Cartesian product of the sample. If the actual duplicate detection run encompasses candidate selec-tion techniques, we can see two effects. First, some dupli-cate pairs are not found and therefore cluster sizes become smaller and the number of clusters increases (potentially as non-duplicate clusters with single elements). Second, the runtime of the duplicate detection run decreases. Therefore, our estimate technique must also be more efficient.
Both effects can be used to improve the algorithm. The obvious solution is to calculate the sample histogram using the same candidate selection techniques, so that the estima-tions become more accurate for the configuration and faster at the same time. However, small adjustments to some can-didate selection techniques are necessary. Further, to avoid pruning too many duplicate pairs with the candidate selec-tion techniques, usually multiple passes are run. We discuss the most common candidate selection techniques as follows. (Overlapping) blocking [11] groups records by some blocking key and na  X   X vely compares the records within the groups. Duplicates can be found only within these groups. Therefore, the estimation for blocking uses the same block-ing keys and analogously looks for the duplicates within the blocks. Index structures speed up the insertion of new sam-ple records over iterations.

Sorted neighborhood method (SNM) [7] sorts the records and compares the records only within a certain win-dow size. The estimation for SNM uses sort-merge to ex-tend the sample using the same sorting keys. The window size w should be linearly adjusted to the sample rate with  X  w = max (2 , d w n N e ), so that a comparable amount of neigh-bors are selected.

Multiple passes should be analogously applied to the sample as well. The estimation algorithm maintains multi-ple index structures or sorted lists. Further, duplicate com-parisons can be avoided with a cache data structure.
Results from SNM or multiple passes need to be post-processed with a clustering algorithm to produce transitively closed, sane results. Our estimation implicitly assumes the transitive closure to be performed to cluster the results be-cause of the random sample model.

Advanced clustering algorithms , such as CENTER [6] and MERGE-CENTER [5], split large, inhomogeneous clus-ters into smaller homogeneous clusters. Hierarchical algo-rithms may produce much smaller clusters on a sample if root records are missing in the sample. Coherent cluster-ing algorithms may create relative large clusters on a sam-ple, because coherence conditions depending on the size of clusters are more likely to be met in a sample. A proper adjustment of these clustering algorithm for our duplicity estimation problem is interesting future work.
While  X  to the best of our knowledge  X  our problem is new, similar problems have been addressed in related work.
Our problem is related to estimating the transitive clo-sure of graph data [3, 10]. In contrast to our problem, the (directed) edges between nodes are explicitly given.
Lipton and Naughton estimate the size of a generalized transitive closure with a basic urn model similar to our sampling model [10]. Their algorithm estimates the total number of resulting edges in near-linear time with linearly decreasing error. Similarly to our approach, the algorithm adaptively samples more records to improve the estimate un-til a given time limit is met. However, for our use cases the estimate of edges(=duplicate pairs) is too coarse-grained.
Cohen proposes a more stable, linear algorithm to esti-mate the total numbers of edges [3]. The Monte Carlo al-gorithm requires reversible edges, but can also be extended to estimate the individual neighborhoods of specific nodes, which can be used to estimate a histogram as we do.
However, both algorithms require the edges to be present, which would correspond to a full duplicate detection run in our case. While the lookup for one edge is negligible in their settings, it is an expensive candidate comparison in our case. Therefore, these techniques are not applicable as we actively try to reduce the number of candidate comparisons.
 Duplicate detection can be viewed as a generalized similarity join if duplicate pairs are of main interest or a generalized similarity group-by if the result should be clustered.
Silva et al. define a similarity group-by operator for data-bases and sketch a small change to the cardinality estimation of a relational group-by operator to correctly estimate the number of groups in a similarity group-by [13]. However, the technique is not applicable to duplicate detection that uses several attributes, because a lower bound for one specific attribute often leads to low thresholds and a high overhead, while multi-attribute similarity statistics are too costly to be built and maintained.

Lee et al. use locality sensitive hashing to estimate the result size of a similarity join with cosine similarity [9]. The approach would be directly applicable to our problem if the candidate comparison consists only of one cosine similarity and threshold. However, we want to treat the candidate comparison as a black box to support a large range of du-plicate detection algorithms and configurations.
 Candidate selection techniques reduce the runtime and de-liver near-complete results [7, 11]. Our estimation may in-corporate these techniques as shown in Section 6 and thus run more accurately and efficiently.

One goal of this paper is to provide users of duplicate detection systems a feedback of their parameter settings to help improve them. Our system can be easily combined with other interactive systems to improve the user experience. Active learning incrementally tweaks a learned candidate comparison function to increase the quality [12]. Chaud-huri et al. propose an example-driven automatic assembly of a candidate selection query [2]. Both systems may be extended with our method to show users the global impli-cations of changed settings. Further, with our focused sam-pling method, we can early return potentially large clusters that may indicate suboptimal settings.
In this paper, we defined and solved the problem of duplic-ity estimation , which estimates the cluster size distribution of a complete duplicate detection run while performing only a fraction of the candidate comparisons.

First, we developed a general sampling model that calcu-lates the expected histogram given the histogram of a com-plete dataset and a sample size. Second, we devised a ran-dom walk approach that reliably and efficiently estimates the original histogram from an increasing sample. Because the original histogram is the principal eigenvector of our transition matrix, our algorithm is guaranteed to converge. Third, we proposed a focused sampling approach that finds a good initial histogram, so that the random walk algorithm converges faster.

We extensively evaluated our approach and could verify the convergence even with poor initial estimates. Better initial estimates indeed lead to faster convergence. The run-time of our approach is very low and the comparison reduc-tion is significant. Further, we discussed how established candidate selection techniques can be incorporated to fur-ther increase the efficiency.

In future research, we plan to combine our method with more sophisticated clustering algorithms, so that the qual-ity of the estimates in these settings is comparable to the evaluated settings. [1] M. Bilenko, B. Kamath, and R. J. Mooney. Adaptive [2] S. Chaudhuri, B.-C. Chen, V. Ganti, and R. Kaushik. [3] E. Cohen. Size-estimation framework with applications [4] U. Draisbach and F. Naumann. DuDe: The Duplicate [5] O. Hassanzadeh and R. J. Miller. Creating [6] T. H. Haveliwala, A. Gionis, and P. Indyk. Scalable [7] M. A. Hern  X andez and S. J. Stolfo. The merge/purge [8] A. N. Langville and C. D. Meyer. Survey: Deeper [9] H. Lee, R. T. Ng, and K. Shim. Similarity join size [10] R. J. Lipton and J. F. Naughton. Estimating the size [11] A. McCallum, K. Nigam, and L. H. Ungar. Efficient [12] S. Sarawagi and A. Bhamidipaty. Interactive [13] Y. N. Silva, W. G. Aref, and M. H. Ali. Similarity [14] T. Vogel, A. Heise, U. Draisbach, D. Lange, and
