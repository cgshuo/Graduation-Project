 This study investigated query formulations by users with Cogni-tive Search Intents (CSIs), which are users X  needs for the cognitive characteristics of documents to be retrieved, e.g. comprehensibil-ity, subjectivity, and concreteness. Our four main contributions are summarized as follows: (i) we proposed an example-based method of specifying search intents to observe query formulations by users without biasing them by presenting a verbalized task description; (ii) we conducted a questionnaire-based user study and found that about half our subjects did not input any keywords representing CSIs, even though they were conscious of CSIs; (iii) our user study also revealed that over 50% of subjects occasionally had experi-ences with searches with CSIs, while our evaluations demonstrated that the performance of a current Web search engine was much lower when we not only considered users X  topical search intents but also CSIs; and (iv) we demonstrated that a machine-learning-based query expansion could improve the performances for some types of CSIs. Our findings suggest users over-adapt to current Web search engines, and create opportunitie s to estimate CSIs with non-verbal user input.
 H.3.3. [ Information Search and Retrieval ] cognitive search intent; query formulation; Web search
While search engines have become an essential tool for a wide range of purposes such as learning and investigations, users usually input a few keywords into search engines despite having diverse search intents [6, 40]. For example, users may input two keywords  X  X arry potter X  with different search intents such as  X  X  want to go to the Harry Potter official site, X   X  X uy Harry Potter books, X  or  X  X ind a page that subjectively introduces the story of Harry Potter. X  Rele-vant search results would be different for those users. Therefore, the estimation of search intents has been intensively tackled in re-cent years [11, 13, 18, 19], and a better understanding of search in-tents has been considered as one of the most demanding challenges.
Although search intents on the topic of documents (or topical search intents ) have been dominantly addressed in the literature, not only the topic but also the cognitive characteristics of docu-ments can be specified by users X  search intents. The cognitive char-acteristics of documents are define d as the document characteristics perceived by readers, and these include comprehensibility, subjec-tivity, and concreteness. Cognitive Search Intents (CSIs), which are users X  needs for the cognitive characteristics of documents, can be seen in a wide variety of searches, e.g.  X  X  want to find com-prehensible documents on black holes, X   X  X  concrete explanation of monopolies, X  or  X  X ocuments subjectively written about Black Berry. X  There have been several information retrieval (IR) research projects that have estimated the cognitive characteristics of docu-ments, e.g. those estimating the readability or comprehensibility of documents [3, 17, 28], and the concreteness of contents in doc-uments [38]. However, existing studies have not focused much at-tention on users, i.e. the need for CSIs or ways of expressing CSIs. We especially need to understand users X  query formulations so that we can correctly interpret their CSIs from user-input queries.
We investigated users X  query formulations for CSIs as an exten-sion of our previous work [22]. The research questions we asked were: (a) how do users express their CSIs in the form of a query? (b) how different are verbalized search intents and queries for CSIs? (c) how much demand for CSIs is there, and has been met by Web search engines given queries input with CSIs? and (d) to what ex-tent can a query expansion improve the search performance for each type of CSIs? We administered a questionnaire-based user study to answer these questions, where 1,800 subjects were re-cruited online and were asked to input queries for given CSIs. We also evaluated how well a current Web search engine could satisfy their CSIs in response to users X  and expanded queries.

Our four main contributions are summarized as follows: (i) we proposed an example-based method of specifying search intents to observe query formulations by users without biasing them by pre-senting a verbalized task description; (ii) we conducted a questionnaire-based user study and found that about half our subjects did not input any keywords representing CSIs, even though they were conscious of CSIs; (iii) our user study also revealed that over 50% of subjects occasionally had experiences with searches with CSIs, while our evaluations demonstrated that the performance of a current Web search engine was much lower when we not only considered users X  topical search intents but also CSIs; and (iv) we demonstrated that a machine-learning-based query expansion could improve the per-formances for some types of CSIs.
Our findings suggest users over-adapt to current Web search en-gines, and create opportunities t o estimate CSIs with non-verbal user input. Moreover, we discuss problems of topic-dependent query expansion for CSIs as well as estimating user-dependent cog-nitive characteristics toward developing a CSI-aware search engine.
The rest of this paper is organized as follows. Section 2 summa-rizes the previous work on search intents and query formulations. Section 3 classifies search intents to clarify the definition of CSI. Section 4 describes a questionnaire-based user study to investigate users X  query formulations for CSIs, and Section 5 reports our find-ings from this user study. Section 6 discusses the findings from, and implications of, this study, and Section 7 concludes the paper.
We review work on search intents as well as query formulations in the following subsections. Search intents have been defined in two different ways: 1) a goal a user wants to achieve through searches, and 2) information a user wants to find. We will discuss these two types of search intents separately, and then describe some work on query formulations.
An early study on goal-directed search intents was conducted by Broder [10], where he proposed a taxonomy of Web search queries in terms of their potential goals: navigational ( e.g. look-ing for a particular home page), informational ( e.g. finding infor-mation on a certain topic), and transactional ( e.g. trying to buy a ticket). Rose and Levinson later proposed a finer-grained taxon-omy, in which the type of informational query was further classi-fied into five categories, and the type of transactional query was replaced with a broader type called resource [33]. IR evaluation campaigns such as TREC and NTCIR have recently paid attention to the diversity of search intents in the evaluation of ad-hoc retrieval systems [16, 35]. They assume that queries have different interpre-tations or subtopics , and these subtopics can be classified into either navigational or informational in terms of their goals. Several eval-uation metrics have been proposed to evaluate the performance of IR systems for queries with different subtopics and goals [15, 34].
Marchionini proposed a boarder classification schema for search intents, and introduced a concept of exploratory search [26]. While Broder treated search intents as relatively short-term activities [10], Marchionini X  X  cla ssification included long-term search activities such as learn and investigate , and he argued that exploratory searches were searches pertinent to the learn and investigate search activi-ties.

Searches are often motivated by i nformation-seeking tasks, and accomplishing tasks is considered as a search intent. Several stud-ies investigated users X  behavior in different kinds of information-seeking tasks [42]. Liu et al. reported an investigation of users X  behaviors associated with four information-seeking task types [25]. The four types, which were characterized based on a classification schema proposed by Li and Belkin [24], are background informa-tion collection, interview preparation, advance obituary, and copy editing. They mainly focused on analysis of quantitative features of search behaviors such as task completion time, query length, and eye movements, while the detailed differences of queries in differ-ent task types were not reported in their work.

Another example along these lines was the classification of com-mercial and non-commercial intents that Dai et al. conducted [18]. They detected queries and Web pages that users were likely to in-put/visit with commercial intents. Guo and Agichtein addressed a similar problem, which was the classification of research and pur-chase intents, by using mouse behavior and scrolling [19].
New types of search intents have been recently proposed as the use of search engines and devices for searches have become more diverse. Teevan et al. demonstrated that around 40% of queries in Yahoo X  X  logs were input for re-finding information [39]. Church and Smyth proposed a search intent taxonomy for mobile searches [14]. Based on a four-week diary study of mobile information needs, they proposed personal information management search intents, which they defined as the goal of finding something private that related to an individual. Moshfeghi and Jose studied two types of entertainment-based search intents: entertainment by adjusting the arousal level, and entertainment by adjusting the mood [27]. They investigated the cognition, emotion, and action aspects of different video search tasks, and found a significant difference in the charac-teristics of the search tasks they studied.

The search intents discussed here can be regarded as the goals of searchers, and are distinct from what users want to find, as users can have a clear goal but for him/her not to know what is required to achieve that goal.
Search intents can be used for the information a user wants to find as opposed to work focusing on the goal of searchers. Search intents within this context are used almost interchangeably with information needs ,or topics , that users expect in retrieved docu-ments.

Cheng, Gao, Liu proposed a method of predicting search in-tents based on a page read by a user [13]. They utilized the users X  search queries triggered by a page to learn a model for estimat-ing the search intents. Cao et al. mined clickthrough and session logs to deduce the users X  search intents [11]. Umemoto et al. tried to predict users X  search intents based on their eye movements[41]. Agrawal et al. addressed the problem of diversifying search re-sults, in which search intents were modeled at the topical level, and were diversified on a search engine result page [2]. As mentioned earlier, IR evaluation campaigns have also modeled search intents as (sub)topics, and conducted topic-diversity-aware evaluation in recent years [16, 35].

As we mentioned in the previous subsection, a user X  X  goal and topics s/he wants to read are not always the same. Furthermore, topics and cognitive characteristics of documents are also different in many cases. There are documents that are topically relevant but cognitively irrelevant ( e.g. documents on a desired topic that are too difficult for the user).
Jansen, Spink, and Saracevic analyzed query logs of Excite and showed general trends of query formulations in Web search [20]. Aula investigated factors that affected the way of query formu-lations by conducting a questionnaire-based study with 32 sub-jects [4]. The experimental results revealed that experience with us-ing computers, the Web, and Web search engines affected the query formulation process including the query length, and frequency of broad/narrow queries. Subjects were asked to formulate a query for twenty search tasks in the questionnaire. White and Morris studied the query formulation process of advanced search users, and found that advanced search engine users queried less frequently in a ses-sion and composed longer queries than non-advanced users [44]. Aula, Khan, and Guan demonstrated how users change their search behaviors when they faced difficulty with searches [5]. Accord-ing to their large-scale study with 179 participants, users started to formulate more diverse queries, used advanced operators more, and spent longer on the search result page when they had difficulty with finding information compared to the successful tasks. White, Dumais, and Teevan conducted a large-scale, log-based study of the effect of domain expertise on Web search behaviors [43]. They investigated logs of experts and non-experts in four domains, and demonstrated that queries generated by experts were longer and contained more of the vocabulary from the domain-specific lexi-con.

There have been much work that investigates factors affecting users X  query formulations, while few studies have been conducted on query formulations for CSIs.
In this section, we propose classifying search intents based on the types of relevance proposed by Saracevic [37], and introduce six types of Cognitive Search Intents (CSIs) used in our user study.
To avoid confusion by the multiple senses of search intent ,we alternatively use topical , cognitive , situational ,and motivational search intents in this paper. Each search intent type corresponds to relevance types proposed by Saracevic [37]. Saracevic defined relevance as the degree of changes caused by information transmis-sion from file 1 ( e.g. topics, cognitive information needs, situations, and motivations) to another ( e.g. texts retrieved in IR). We sim-ply define the types of search intents based on the types of files, i.e. topical, cognitive, situational, and motivational search intents are equivalent to topics, cognitive information needs, situations (or tasks), and motivations (or goals), respectively.

We distinguish search intents modeled by topics ( i.e. topical search intents) [2, 11, 13, 41], and ones modeled by users X  goals ( i.e. motivational search intents) [10, 14, 18, 19, 26, 27, 33] and sit-uations or tasks ( i.e. situational search intents) [24, 25, 42]. Al-though goals (or motivations) and tasks (or situations) were sepa-rately classified in Saracevic X  X  relevance classification, it seems dif-ficult to clearly distinguish them since task is defined as an activity in order to achieve a goal ( e.g. [42]) and a task is characterized by the goal that can be achieved by conducting the task [24]. On the other hand, we emphasize the difference between CSIs (or needs for cognitive characteristics of documents) and the other types of search intents. First, CSIs are similar to topical search intents but different from motivational and situational search intents in that the former types are needs for the contents and characteristics of doc-uments to be retrieved, while the latter types are needs for goals and tasks to be accomplished by search. Second, CSIs are different from topical search intents in terms of the level of needs for doc-uments. Topics are what are actually written in documents, while cognitive characteristics of documents are ones perceived by read-ers, which can be affected by the writing style, diction, structure, appearance, and meta data such as the author, domain, creation date, etc.

According to Saracevic X  X  work, cognitive correspondence, infor-mativeness, novelty, information quality, and the like are charac-teristics by which cognitive relevance is inferred. We further ex-tended the cognitive characteristics to exhaustiveness, comprehen-sibility, subjectivity, objectivity, concreteness, and abstractness of document content. These types were used mainly because some work has proposed methods of estimating the degree of each crite-rion, which can be applied to a wide variety of applications such as IR, Q&amp;A, and opinion mining.

Below, we briefly discuss the six types of CSIs as well as topics used in our user study.
The term file was used in a broader sense in the original paper: a storage of subject knowledge and/or its representation in an orga-nized manner [36].
 Exhaustiveness. The exhaustiveness (or coverage) of a docu-ment for a query is the fraction of query topics covered by the document, where the query topics are information generally ex-pected from the query. Exhaustiveness has been considered to be an important factor by information seekers, and has been used as a document ranking criterion [47] as well as a summarization cri-terion [23]. We selected ten topics from the queries of NTCIR-10 INTENT-2 Japanese Subtask to investigate CSIs regarding exhaus-tiveness [35], which is an IR evaluation campaign for search result diversification and its queries were designed to have multiple top-ics, e.g.  X  X ahoo! X  and  X  X outh Africa X .
 Comprehensibility. The comprehensibility of a document means how easily users can read and understand the document. Even though a retrieved document is perfectly relevant in terms of the topic, it is useless unless the user can comprehend it. Some re-searchers have tried to estimate comprehensibility by using a Wikipedia link structure [28], and a PageRank-based method that propagates comprehensibility scores in a semi-supervised manner [3]. We se-lected ten topics that were used in the research by Nakatani, Jatowt, and Tanaka, which came from difficult domains such as medicine and economics [28].
 is represented by the fraction of personal opinions in the document, while objectivity is the opposite concept of subjectivity. There has been some work on estimating the subjectivity of documents, mainly within the context of opinion mining. For example, Yu and Hatzivassiloglou proposed a method of separating opinions from fact at the document and sentence levels [46]. The ten topics we used were extracted from queries used in TREC 2008 Blog Track, where opinion-finding and polarity opinion-finding retrieval tasks were carried out, and they used topics that could be described sub-jectively such as  X  X ozart X  and  X  X lackberry X  [29].
 creteness by combining perceivability and imaginability in the first trial to computationally estimate the concreteness of documents [38]. Perceivability is a measure of the extent to which people can sense an object, while imaginability is a measure of how quickly and eas-ily people can imagine a given referent. Since it is difficult to con-vey to subjects notions of their own concreteness, we defined the concreteness of a document in a simpler way: to what extent the content of the document describes a topic with a specific scope and actual examples. In contrast, abstractness is defined as to what ex-tent the content of the document describes a topic in a general and definitional way. We selected ten topics from the ones that Tanaka et al. used [38] for concreteness and abstractness, which were ei-ther queries used in TREC Million Query Tracks [12], or the titles of Wikipedia articles. Examples of these topics are  X  X rithmetic X  and  X  X onopoly X .
We investigated users X  query formulations for CSIs based on a questionnaire-based user study. A significant challenge was to ob-serve unbiased users X  queries while controlling their search intents. To achieve this end, we propose an example-based method specify-ing search intents, in which we present positive and negative doc-ument sets to the subject, and ask him/her to search for documents not similar to the negative set but to the positive set. We then ex-plain the content of the questionnaire and labeling process regard-ing resulting data. We discuss t he limitations with our methodology at the end of this section.
When we try to observe users X  search behaviors, the most pop-ular method is to present a task description and to ask subjects to conduct the task [9]. However, this methodology has a drawback in that users X  query formulations can be highly biased by the task description. For example, if we explain a task as  X  X lease find com-prehensible documents on black holes, X  the user would be likely to input the query  X  X omprehensible black holes X  despite unfamiliarity with the term  X  X omprehensible. X  Moreover, the search task is not usually verbalized. Thus, we may not be able to observe natural, common query formulations when the task is explained through text.

Therefore, we came up with an implicit, non-verbalized way of specifying search intents that presents two types of examples to the subject and enables her/him to understand search intents. The first type of example is called a positive document set that con-sists of documents relevant to the search intents we want to convey to the user, while the second type is called a negative document set that is composed of documents irrelevant to the search intents. When we want the subject to search with the intent of  X  X  want to find comprehensible documents on black holes, X  for example, we use comprehensible documents on black holes as the positive docu-ment set and incomprehensible ones on black holes as the negative document set. Having presented the two types of examples to the subject, we can ask him/her to search for documents that are not similar to the negative set but to the positive set. This example-based specification of search intents does not verbalize the search intents, and is especially effective when we want to study the query formulation process that can be highly biased by task descriptions.
Although the method described here can prevent the subject from inputting a biased query, another c oncern is that s/he cannot clearly understand the CSI that we intended. Our preliminary trial sug-gested that people tended to select some words from the positive document set and were unaware of the CSIs. Thus, we slightly changed the way the subject was informed of the task. The revised six step procedure is summarized below: 1. Prepare topic x and CSI c with which the subject is supposed 2. Prepare multiple topics other than x and let T denote them, 3. Find documents on each topic in T that satisfy CSI c ,and 4. Find documents on each topic in T that do not satisfy CSI 5. Show both document sets to the subject, and 6. Ask him/her to search for  X  X ocuments on topic x that are
In the case of x =  X  X lack hole X  and c =  X  X omprehensible X , for example, the positive and negative document sets can be composed of documents on  X  X arkinson X  X  disease X  and  X  X  X ark matter X . An ac-tual example is shown in Figure 1. Having presented these docu-ment sets to the subject, we can ask him/her to input a query to find documents on x =  X  X lack hole X  that are not similar to the negative (incomprehensible) document set but to the positive (comprehen-sible) document set. The two main differences from the original method of specifications described earlier are (1) using documents Figure 1: Negative document set (Group A) and positive docu-ment set (Group B). on multiple topics other than topic x as positive and negative doc-ument sets, and (2) instructing the subject to search for documents on topic x . Those two changes were expected to help the subject understand common properties across documents in the positive document set ( i.e. the CSI that was intended), and to prevent the simple use of keywords in the positive document set.
We asked the following questions in our questionnaire: 1. How many times per day do you input search keywords on 2. How familiar are you with search engines ( e.g. Google, Ya-3. Please suppose that you are looking for information on 4. What kinds of keywords would you input to find documents 5. How would you modify the keyword below if you want to 6. Have you ever searched to find exhaustive, comprehensible,
We asked Questions 1 and 2 to estimate the subject X  X  search ex-pertise from their search experience. We then used the example-based method of specifying search intents to inform the subject of a search intent, saying  X  X lease compare document groups A and B. Although both of the groups contains documents on t 1 and there is a different point X  by showing positive and negative docu-ment groups on the two topics (see Figure 1). Note that we showed only the contents of documents w ithout their titles and URLs. Hav-ing presented the positive and negative document sets, we asked him/her Question 3 to obtain a verbalized search intent so that we would know how the subject perceived the search intent conveyed by examples, which was used to validate to what extent the subject could clearly understand the search intent. Question 4 presented a mimic search box and asked the subject to input an appropriate query into the search box to find documents relevant to the search intent presented in Question 3. The order in which of Questions 3 and 4 were presented was randomly switched to prevent order bias: writing down the search intent before a query was input might have influenced the way a query was formulated and vice versa .Ques-tion 5 asked subjects to investigate the difference between query in-put and query reformulation, where we presented a query on topic y other than topic x used in Questions 3 and 4, as well as a search male 50.8 10s 16.8 company employee 24.8 female 49.2 20s 17.6 business manager 1.3 result for the presented query. The search result consisted of doc-uments that did not satisfy the CSIs we intended. The subject was then asked to reformulate the presented query so that s/he could obtain documents that met the CSIs asked for Questions 3 and 4. Question 6 was asked for us to estimate the popularity of a CSI. At this point, we explicitly verbalized the CSIs used in Questions 3, 4, and 5, and let the subject select from the four options.
As we explained in Section 3, we used the six types of CSIs and 40 topics (10 for exhaustiveness, 10 for comprehensibility, 10 for subjectivity and objectivity, and 10 for concreteness and abstract-ness) in our user study. The search intents we used were combi-nations of CSIs and topics; thus, there were 60 search intents (10 topics per CSI).
We recruited subjects through an online-questionnaire company in Japan. The entire process for the questionnaire was completed on the Web, and all the instructions, questions, and topics were written in Japanese. We first presented the subjects with three easy ques-tions to filter out bots and cheaters that could be easily answered by those who had an average search literacy.
 A total number of 1,800 subjects finished the main questionnaire. As there were 60 search intents, we randomly assigned 30 subjects to each search intent. Table 1 lists the demographics of the subjects we recruited, where we controlled the sex and age distributions so that subjects were sampled from each demographic as equally as possible. However, the number of students and part-time employ-ees seemed high compared to the actual job type distribution in Japan.

Table 2 summarizes the statistics on the subjects X  search expe-rience, where the first column  X  X earch frequency X  indicates the number of times subjects searched per day on average (Question 1 in the questionnaire), and the second column  X  X earch familiar-ity X  indicates the search familiarity subjects reported on a five-point scale (Question 2). The average search frequency per day was 15.7, which far surpassed 3.2, which is the average search fre-quency per day in Japan 2 . In addition, about 50 percent of subjects answered that they were familiar or very familiar with searches. It follows from these findings that the subjects we recruited had above-average search expertise. Releases/2009/3/Japan_Search_Engine_Rankings [October 1, 2013].

We labeled responses from subjects to filter out those who could not understand a search intent that we tried to inform them of, and to classify queries from several aspects to enable further investiga-tions. Three assessors were hired for this labeling, one of whom was an author of this paper.

We first excluded responses that were generated within five min-utes, as short completion times might have indi cated poor-quality responses. There were 1,452 (80.7% of the original data) responses left at this stage.

We then asked the assessors to label search intents written by the subjects for Question 3 in the questionnaire, and removed responses by which we could estimate that the subject could not fully under-stand the search intents. To this end, the assessors were asked to evaluate two points for each verbalized search intent: whether any CSI was verbalized by the subject, and whether the CSI given to the subject was correctly verbalized by the subject. These two points were slightly different in that the former did not take into consider-ation the correctness of the written search intent. These two criteria were separately and sequentially assessed, i.e. we first labeled all the responses for the former criterion, excluded responses labeled as false by two or three assessors, and then started working on the latter criterion. After the two criteria had been labeled, we excluded responses in the same way as those for the former criterion. Eight hundred forty responses (46.7% of the original data) passed after the former criterion and 422 (23.4%) passed after the latter. The inter-rater agreement was measured by Fleiss X  Kappa, and this was considered to be substantial being 0.76 for the former criterion and 0.73 for the latter.

We further labeled each query input for Question 4 in the ques-tionnaire. The purpose of this labeling was to exclude queries that contained an incorrect topic word, e.g.  X  X lack hole easy X  for a given search intent  X  X  want to find comprehensible documents on stagflation X . Responses labeled false ( i.e. incorrect topic) by two or three assessors were filtered out . The inter-rater agreement was 0.87 (substantial), and 375 (20.8% of the original responses) re-sponses remained. We used these 375 responses in our analysis if not otherwise specified.

We also labeled each query to answer the research question (a) how do users express their CSIs in the form of a query? The as-sessors were instructed to classify queries into the following types: direct (a term representing the CSI directly used in the query, e.g.  X  concrete monopoly X ), transformed (a term representing a CSI some-how transformed in the query, e.g.  X  example monopoly X  and  X  re-view blackberry X ), and none (no term in the query related to the CSI, e.g.  X  X onopoly X ). The queries input for Questions 4 and 5 in the questionnaire were labeled in this way. The inter-rater agree-ments were 0.71 for Question 4 and 0.61 for Question 5 in terms of Fleiss X  Kappa. The query type of each query was decided by votes, i.e. we used the query type labeled by two or more assessors. A few queries (0.4%) were labeled as other , since the three assessors labeled them differently.
We clarified the limitations with the methodology we used for our user study. Even though these limitations may have reduced the value of our reported findings, readers can still interpret and use the results by duly taking into consideration the possible biases discussed here.

First, the example-based method of specifying search intents was appropriate to achieve our goal, whereas subjects could not always understand the search intent we intended to convey with examples. One reason is that online workers are usually poorly skilled due to low pay and unsupervised environments. Another reason is that the example-based method was too complex for subjects to understand our instructions. Note, however, that the quality of data left was ensured by using multiple assessors.

Second, our samples were biased in terms of their demographic and search expertise. As summarized in Tables 1 and 2, the re-cruited subjects included more students and part-time employees, as well as more search experts than those in an ideal sampling re-sult. Since it is particularly well known that search expertise affects users X  search behaviors [4], it might be inappropriate to generalize our findings to all users of search engines including novices.
Third, our user study was conducted in Japanese, and queries in the questionnaire were written in Japanese. However, we do not think that this is a serious limitation because there is little difference from the other languages at the keyword level.
This section attempts to answer the four research questions by re-porting findings from our user study. First, we explain how queries were formulated for different types of CSIs. Second, the transfor-mation from a verbalized CSI to a query is analyzed to gain a better understanding of the query formulation process. Third, we esti-mate the demand for CSIs, and evaluate the supply for this demand, i.e. the performance of a Web search engine for CSIs. Finally, a machine-learning-based query expansion is applied to testing its effectiveness for searches with CSIs.
Table 3 lists the percentages for query types for CSIs. Note that we merged queries that were input for Questions 4 and 5 in the questionnaire, as there were only small differences between queries that were input from scratch (Question 4) and those that were re-formulated (Question 5). Overall, the fraction of direct queries was quite small, while that of transformed and none queries were dom-inant in this categorization. When we compared these two domi-nant query types, there were more none queries than transformed queries. It follows that users were not likely to input a term that directly represented their CSIs, but were more likely to transform such a term into another term that they thought would be effective, or to only use keywords related to a topic that they wanted to read, even though they were conscious of given CSIs (see Section 4.4). Many none queries posed huge challenges in estimates of search intents regarding how silent CSIs could be detected. Possible solu-tions to this problem are discussed in Section 6.

Table 4 summarizes the most frequent topic-independent and topic-dependent terms used in transformed queries, with the per-centage per CSI. We simply used a dictionary to translate Japanese words into English in this table and we can see users X  transforma-tion techniques in these examples. For instance, many users might have tried to restrict sites by adding keywords such as  X  X ikipedia X  and  X  X log X . Users frequently used  X  X ikipedia X  for CSIs regard-ing exhaustiveness, objectivity, and abstract, while they frequently used  X  X log X  to find subjective documents. Those keywords specify-ing domains are topic-independent, and can be used for any topics. We can also see that users selected keywords that were effective for different types of topics, namely, topic-dependent keywords for CSIs. Users tended to use the term  X  X sability X  f or product-related topics ( e.g. Blackberry) for CSIs on subjectivity, while they tended to use  X  X mpression X  for person-related topics ( e.g. Barry Bonds). This topic-dependency of query formulations leads to a discussion on automatic query expansion for CSIs in Section 6.
Figure 2 compares the differences between verbalized search in-tents (Question 3 in the questionnaire) and input queries (Question 4) in terms of their component words.  X  X ouns (overlapping) X  in-dicates terms that are included in both the verbalized search intent and query, while  X  X ouns (unique) X  indicates terms that appear only in a verbalized search intent but not in its query, or those that only appear in a query but not in its intent. It is obvious that queries contain few verbs and adjectives. As the cognitive characteristics of documents are often verbalized in the form of adjectives ( e.g. exhaustive, objective, and concrete), this trend might prevent users from explicitly inpu tting their CSIs as discussed in the previous subsection.

Moreover, it can be seen that about two nouns in the verbalized search intents were not used in subjects X  queries, while on average 1.3 nouns in the queries were not used in their verbalized search intents. This finding might imply that users input some of the nouns from their verbalized search intents; on another front, they generate nouns suitable for keyword queries as alternatives to unused nouns, Figure 2: Difference between verbalized search intents and queries in terms of their component words. verbs, and adjectives. Although not conclusive, this hypothesis plus a large portion of transformed queries together sketch the shapes of query formulations for CSIs: users usually avoid inputting CSIs as adjectives, and translate their CSIs into nouns.
We first briefly discuss the demand for searches with CSIs by using responses to Question 6 in the questionnaire, and then inves-tigate the performance of a current Web search engine for CSIs.
Figure 3 outlines answers to the question  X  X ave you ever searched with a CSI? X  Note that we explicitly informed the subjects of the CSI we intended when they started to answer Question 6. There-fore, we assumed that all the subjects could understand the CSI used in the user study, and we used 1,452 responses obtained just after filtering by using the completion time. The figure indicates that half the subjects had CSIs on exhaustiveness, comprehensibil-ity, objectivity, and concreteness, of which about 10% answered that they often searched with CSIs. Thus, this result indicates rea-sonable demand for executing searches with CSIs. Subjects, on the other hand, did not very frequently experience searches with CSIs regarding subjectivity and abstractness.

Searches with CSIs were at a certain level of demand. Then, how well can Web search engines meet demand given queries input with CSIs? To answer this question, we downloaded documents that were returned in response to subjects X  queries, assessed their topical and cognitive relevance, and evaluated a Web search en-gine X  X  performance in terms of normalized Discounted Cumulative Gain (nDCG) [21].

Yahoo! Japan and Google are two major search engines that ac-cept Japanese queries, and they occupied  X  90% of the Japanese Web search engine market according to a survey in 2009 2 . We only investigated the performance of Google X  X  search engine because the search results of Yahoo! Japan were provided by Google as of March 27, 2013. We crawled the ten top-ranked documents re-turned in response to subjects X  queries on March 27, 2013, and did query-based pooling for each search intent. We then created crowd-sourcing tasks that asked workers to evaluate the topical relevance as well as cognitive relevance on a two-point scale (irrelevant and relevant). Six workers were assigned to each document. We paid 5 JPY per document for both topical and cognitive relevance judg-ments. The inter-rater agreement was measured with Randolph et al.  X  X  Free-Marginal Multirater Kappa, since the number of tasks completed varied across worke rs [32]. The agreement on 5,213 documents for the evaluations of topical relevance was substantial (0.67), whereas that for the evaluations of cognitive relevance was moderate (0.43). As we expected, the agreement in the cognitive relevance judgements was lower than the topical relevance judge-ments, since cognitive relevance was usually subjective by design.
Figure 4 shows the mean nDCG@10 with the standard error of the mean (SEM) for all CSIs and query types in terms of topical rel-evance as well as topical and cognitive relevance. The weight for nDCG was computed in two ways: the number of workers who la-beled the document as topically relevant (topical), and that of work-ers who labeled the document as both topically and cognitively rel-evant (topical+cognitive). The weight was normalized so that it ranges from 0 to 2. The figure only presents the mean nDCG@10 for transformed and none query types, due to a small number of samples for direct and other types.

There is a gap between topical and cognitive relevance in terms of nDCG@10. Although it is obviously harder for a Web search engine to satisfy CSIs than to meet topical search intents, this dif-ference suggests that the current search engine lacks adequate ca-pabilities to pro cess queries input with CSIs, even with transformed queries in which users transformed a term that directly represented a CSI into terms that they thought were effective for finding cognitively-relevant documents. Table 5 lists queries that achieved the highest nDCG@10 for each CSI. Note that Japanese queries were again translated into English. It seems that a few top queries were as effective as those for topical relevance, whereas the third most ef-fective queries for CSIs regarding exhaustiveness, comprehensibil-ity, objectivity, and concreteness could only achieve nDCG lower than the average nDCG in terms of topical relevance. Furthermore, transformed queries were not very effective but were worse than none queries in some cases in which no keywords were related to CSIs. Thus, users X  skills with query formulations did not always solve the problem with the search engine X  X  inability to deal with CSIs.
We further investigate the performance of the Web search engine by automatic query expansions based on judged documents. We compared the performances of users X  and expanded queries to clar-ify whether a machine-learning-based query expansion is effective enough to develop CSI-aware search engines.

We used a query expansion method proposed by Oyama, Kokubo, and Ishida [30], which was designed for developing domain-specific Web search engines. Their proposed method first trains a decision tree to classify training data that consists of relevant and irrelevant documents, where the presences of terms in a document are used as features. The trained decision tree is composed of nodes represent-ing terms T ,leaves L , and branches E  X  V  X  V ( V = T  X  L Each leaf l is labeled as irrelevant ( r ( l )=0 )orrelevant( while each branch e =( v i ,v j ) is labeled as absent ( p ( e )=0 Figure 4: Mean nDCG@10 for each query type in terms of top-ical relevance as well as topical and cognitive relevance. Table 5: Most effective transformed queries. Bold font indicates terms representing given topics.
 or present ( p ( e )=1 ) indicating the presence of the term document. Each path from the root t 0 to a leaf labeled as rele-vant, i.e. q =( t 0 ,t 1 ,...,t i ,l ) ( r ( l )=1 ), represents a Boolean query -t 0  X  t 1  X  X  X  X  X  t i when p (( t 0 ,t 1 )) = 0 , p (( t and p (( t i ,l )) = 1 . From each Boolean query q generated from the induced tree, their method removes literals that improve the F-measure on validation data when it is removed. After the literal pruning, the query expansion method makes a disjunction of the pruned Boolean queries Q = { q 1 ,q 2 ,... } , i.e. q 1  X  q method further simplifies the disjunction by removing conjunctive components that improve the F-measure on the validation data. The resulting query is used as an expansion to a given query.
We utilized the method above to generate queries by feeding judged documents as training and validation data, where relevant documents were ones judged as topically and cognitively relevant by at least four assessors. Recall that ten topics X c were selected for each CSI c .ForCSI c and topic x  X  X c , we used docu-ments retrieved for six topics as training data D training { x } , and ones retrieved for the other three topics as validation data Figure 5: Mean nDCG@10 for users X  transformed queries (User), users X  transformed queries that achieved the highest nDCG for each topic (User Max), and expanded queries (Ex-panded).
 Table 6: Examples of expanded queries. Bold font indicates terms representing given topics.
 D query for each pair of a topic and a CSI. Some of the examples are shown in Table 6. Note that we did not use documents retrieved by a query to be expanded, as we wanted to develop a query expansion method applicable for any queries. Thus, our query expansion was topic-independent.

Figure 5 shows the mean nDCG@10 in terms of topical and cog-nitive relevances, with SEM. The mean nDCG@10 was computed for (1) users X  transformed queries (User), (2) users X  transformed queries that achieved the highest nDCG for each topic (User Max) (see examples in Table 5), which approximates the upper-bound performance achieved by keyword queries, and (3) queries gener-ated by the query expansion method (Expanded).

The query expansion improved the performances for CSIs on comprehensibility, objectivity, and concreteness, while it failed to improve the performances for the other types of CSIs. Table 6 shows examples of the expanded queries. We can see some terms thatmaybeeffectivefortheCSIs, e.g.  X -biology X  for comprehen-sibility,  X -think X  for objectivity, and  X  X ant X  for subjectivity. Such terms might work to filter out scientific articles ( X -biology X ) or in-clude/exclude subjective documents ( X -think X  and  X  X ant X ). Espe-cially, it is interesting that the query expansion method found verbs effective for searches with CSIs, which were not frequently used by our subjects. On the other hand, the other terms in the table are difficult to discuss their effectiveness, e.g.  X -help X ,  X -flat X , and  X -landing X , which were possibly selected as a result of over-fitting due to the small training data. Another possible explanation for the low performances is that there are few terms that improve some types of cognitive relevances. It is indeed hard to select terms indi-cating high exhaustiveness or abstractness even manually.
Our user study uncovered users X  query formulations in which they were likely not to use terms related to CSIs, and revealed that the current Web search engine is not able to satisfy the CSIs as adequately as topical ones. This section discusses some questions raised in our analysis, and indicates approaches to the problem with silent CSIs and the development of a CSI-aware search engine.
We will first discuss the reason why half our subjects did not input any keywords related to CSIs, and then suggest some possible solutions to the problem with silent CSIs.

Our results demonstrated that half the subjects did not include terms representing CSIs in their queries, even though their ver-balized search intent included such terms (this was ascertained by three assessors). A possible explanation for this phenomenon is that subjects over-adapted to the Web search engine. An early study on Web searches conducted by Pollock and Hockley found that some novices tried to enter natural la nguage queries [31] . In addition, Bi-lal reported that 35 % of 22 seventh-grade children tried to search with a natural language question [8]. On the other hand, expe-rienced users do not usually input natural language queries into search engines. Thus, the way users formulate queries might be ac-quired through experience with Web searches. Putting these find-ings all together, our hypothesis is that few experienced subjects input keywords related to CSIs because they knew Web search en-gines could not effectively process such words through their expe-rience with search engines (as is also empirically shown in Figure 4). To support this hypothesis, we conducted an independent test on query types and search expertise measured by Question 2 in the questionnaire, though a null hypothesis was not rejected probably due to the small number of novices. Thus, the problem with silent CSIs bears further investigation.

Although much work has utilized query logs to investigate users X  search activities, the problem with silent CSIs suggests that only query-log-based analysis is not adequate for detecting users with CSIs. Therefore, clickthrough and interaction data are necessary to precisely estimate CSIs as can be seen in some previous work [11, 13, 19, 41].

Another type of approach is to encourage users to explicitly in-put their CSIs. Belkin et al. reported that users input longer queries when they are shown a message near the query box  X  X nformation problem description (the more you say, the better the results are likely to be) X  [7]. Agapie, Golovchinsky, and Qvarfordt proposed an interface designed to encourage users to type longer queries, which produced a halo around the query box that changed in color and size as the length of a query changed [1]. These approaches can be applied to eliciting users X  CSIs by telling users they can in-put adjectives that describe documents they want, or just encourag-ing them to input longer queries to increase the chance of obtaining terms regarding their CSIs. In addition, techniques of interactive IR such as query suggestion and relevance feedback would also help users better express their CSIs. In particular, Yamamoto and Tanaka proposed a method that predicted users X  criteria to assess the credibility of search results by asking them to click on credible search results by visualizing the estimated cognitive characteristics of results such as exhaustiveness and objectivity [45]. Their tech-niques can be easily applied to predicting CSIs.
Our results uncovered a situation where there was reasonable de-mand (frequent experience) but limited supply (poor performance by Web search engines) for CSI-aware searches. According to the experimental results, simple modifications to queries do not neces-sarily help users find cognitively relevant documents. In particular, a topic-independent query expansion was not effective for all of the CSIs. Thus, Web search engines are required either to auto-matically expand the query in a topic-dependent manner or to rank documents by taking into account their cognitive characteristics in response to queries input with a CSIs.

Query expansion for CSIs would be an easier approach to de-veloping CSI-aware search engines, since query expansion can be installed to on search engines without having to modify their docu-ment rankers. Effective query expansion might depend on the top-ics of the queries as observed in Table 4. This is also supported by the result that a topic-independent query expansion failed to im-prove search performances for some of the CSIs. For example, adding a keyword  X  X ome X  would effectively find comprehensible documents on a disease, while the same query expansion would not be helpful for economic topics.

Estimating the cognitive characteristics of documents in advance and utilizing their priority in ranking is another approach to achiev-ing CSI-aware searches. Methods of predicting the cognitive char-acteristics of documents have been proposed in the literature as was discussed in Section 4.2. Although certain types of CSIs can be handled even with existing techniques, a demanding challenge is to identify documents that are cognitively relevant for specific users . As cognitive relevance is that recognized by users, it is in-herently subjective and depends on individuals. For example, doc-uments may differ that are comprehensible by different types of users. Other examples include the likes, usability, and visibility of documents. Therefore, estimating the cognitive characteristics of documents for certain types of users can be one of the greatest challenges in this line of work.
This study investigated users X  formulations of queries for CSIs including exhaustiveness, comprehensibility, subjectivity, objectiv-ity, concreteness, and abstractness. We proposed an exampled-based method of specifying search intents to observe users X  query formulations without bias by presenting a verbalized task descrip-tion. Moreover, we were able to find plausible answers to the four research questions by means of a user study that was questionnaire-based. Our findings are summarized as follows: (a) we found that about half our subjects did not input any keywords representing CSIs, even though they were conscious of CSIs; (b) our user study also revealed that over 50% of subjects occasionally had experi-ences with searches with CSIs; (c) our evaluations demonstrated that the performance of a current Web search engine was much lower when we not only considered users X  topical search intents but also CSIs; and (d) we demonstrated that a machine-learning-based query expansion could improve the performances for some types of CSIs.

Future work includes further studies on silent CSIs, and the de-velopment of a CSI-aware search engine. We also have a plan to investigate the most frequent CSIs per query based on clickthrough data analysis, and the relationship between users X  queries and de-scriptions of situational and motivational search intents.
This work was supported in part by the following projects: Grants-in-Aid for Scientific Research (Nos. 24240013, 24680008, and 26700009) from MEXT of Japan, and Microsoft Research CORE Project.
