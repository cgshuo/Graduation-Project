 1. Introduction without need of document co-occurrence information. This is useful for mapping out research topics at the micro-level. Because we do not consider the within document co-occurrence, our approach can be conceived terms can be said to share a similar context. Terms are clustered depending on the presence and number of shared linguistic relations. For instance, a link will be established between the two terms humoral immune clustering which rely on high frequency information. The resulting system, called TermWatch ( Ibekwe-domain topic mapping, text mining, query refinement or question X  X nswering (Q X  X ).

Some attempts have been made to cluster document contents in the bibliometrics, scientometrics and infor-
Bassecoulard, 1994 ). Although these information units depict the thematic contents of documents, they are external to the documents themselves and do not allow for a fine-grained analysis of the current topics addressed in the full texts. In studies where the document contents were considered, only lone words were &amp; Kumar, 1994 ) are also based on the vector-space representation model of documents (bag-of-words on term weighting indices like the Inverse Document Frequency (IDF), Mutual Information (MI) or the cosinus like science and technology watch where the focus is on novel information often characterised by low fre-quency units (weak signals). Price and Thelwall (2005) have demonstrated the usefulness of low frequency words for scientific Web intelligence (SWI). They showed that removing low frequency words reduced cluster coherence and separation, i.e., clusters were less dissimilar.

Glenisson, Gla  X  nzel, Janssens, and Moor (2005) proposed combining full text analysis with bibliometric tors of lone words. Stemming was performed on the words and bigrams were detected, i.e. sequences of two bigrams may not always correspond to valid domain terms. The authors weighted the bigrams using the
Dunning likelihood ratio test ( Dunning, 1993 ). This led to selecting the 500 topmost bigrams for analysis rather than keywords or terms from the reference section leads to a more fine-grained and accurate mapping of research topics. This finding is in line with our text mining approach.

Polanco, Grivel, and Royaute  X  (1995) developed the Stanalyst informetrics platform. Stanalyst comprises a linguistic component which identifies variants of MWTs used to augment their occurrences. The MWTs are then clustered based on document co-occurrence information. To the best of our knowledge, no informetric method has considered clustering phrases based on linguistic relations. The TermWatch approach is based on the hypothesis that clustering multiword terms (MWTs) through lexico-syntactic and semantic relations can 31,398, none which is eliminated prior to the matrix reduction phase.
 The clustering algorithm implemented in TermWatch is named CPCL (Classification by Preferential differences with existing approaches, setting up an adequate comparison framework with other methods has partitioning and hierarchical algorithms). Evaluation is carried out on a test corpus (the GENIA project) which comes with an answer key (gold standard). This will ensure that the results being presented are grounded in the real world.
 our text mining methodology; Section 4 presents the evaluation method; Section 5 describes the experimental and conclusions. 2. Test corpus In order to carry out an evaluation, we chose a dataset with an existing ideal partition (gold standard).
The GENIA project 2 consists of 2000 abstracts downloaded from the MEDLINE database using the search keywords: Human , Blood Cells , and Transcription Factors . Biologists manually annotated the valid domain the same input. The GENIA project also furnished a hand-built ontology, i.e. a hierarchy of these domain terms arranged into semantic categories. There are 36 such categories at the leaf nodes. Each term in the
GENIA corpus was assigned a semantic category at the leaf node of the ontology. We shall refer to the leaf node categories as classes henceforth. Of course, the GENIA ontology X  X  hierarchy, the number of classes and
GENIA ontology is a result of a human semantic and pragmatic analysis, we do not expect automatic clus-tering methods to reproduce it exactly without prior and adequate semantic knowledge. The goal of the eval-to qualify this hierarchy, it is more of a small taxonomy. Indeed, the GENIA ontology is still embryonic other name has 10,505 terms followed by the protein molecule class with 3899 terms and the dna domain or class can be further refined. Also some relations normally found in a full-fledged ontology are absent (synonymy in particular). This tends to suggest that this hierarchy is a weaker semantic structure than an the GENIA taxonomy henceforth.
 Table 1 gives some examples of terms in the GENIA corpus.
 other name which concentrated 33% of the terms because it was difficult to fit in. A few number of classes the terms (almost 75%). The bars show the proportion of terms according to their length. As a consequence terms. In an OTC task, the intrinsic properties of MWTs (like term length) obviously play an important role since they are the only available context. 3. Overview of our text mining methodology
Our methodology consists of three major components: MWT extraction; relation identifier and clustering 3.1. Term extraction module
Note that in the current experiment, our term extraction module was not used as the terms were already manually annotated in the corpus. We however describe summarily its principle. TermWatch performs term extraction based on shallow natural language processing (NLP) techniques. Extraction is implemented via the
NLP package developed by the University of Edinburgh. LTPOS is a probabilistic part-of-speech tagger based with many other systems. LTCHUNK identifies simplex noun phrases (NPs), i.e., NPs without prepositional attachments. In order to extract more complex terms, we wrote contextual rules to identify complex termino-logical NPs. An example is provided in Appendix A . About 10 such contextual rules were sufficient to take care of the different syntactic structures in which nominal terms appear in English. Given that some domain concepts can appear as long sequences like in parental granulogyte-macrophage colony-stimulating factor
Hence, the difficulty of clustering them with methods based on co-occurrence criteria. 3.2. Relation identifier
Different linguistic operations can occur within NPs. These operations either modify the structure or the length of an existing term. They have come to be known as variations and have been well studied in the com-existing term), syntactic (expansion or structural transformation of a term), semantic (synonyms, generic/ 3.2.1. Morphological variants spelling variants. They enable us to recognise different appearances of the same term. For instance, using WordNet ( Fellbaum, 1998 ). 3.2.2. Lexical variants 3.2.3. Syntactic variants case, we talk of right expansion .
 term. Then these variants are used to recognise the more complex variants. For instance, B cell development clustering depending on their interpretation. This will be further detailed in Section 5 . 3.2.4. Semantic variants that the related terms share some common words. This leaves out terms which can be semantically-linked but which organises English words into synsets. A synset is a particular sense of a given word. SinceWordNet organises only words and not multi-word terms, we had to devise rules in order to map word -word semantic synsets and then apply the same variation relations to sequence of synsets. However, like all external
Second, being a general purpose semantic database, WordNet establishes links which can be incorrect in a specialised domain.

We thus restricted the use of WordNet to filtering out lexical substitutions, and consequently to pairs of terms that share at least one word in order to reduce the number of wrong semantic links. Only a very few the semantic ones using WordNet hierarchy: given two terms related by a lexical substitution, check if the two words substituted are linked by an ascending or descending path in the hierarchy. Observe that, by def-or modifier).
 In this way, we acquired the following synonymy relations:
Only 365 WordNet modifier substitutions and 208 WordNet head substitutions were found whereas lexico-syntactic variants were much more abundant (see Table 2 ).

Table 2 gives the number of variants identified for each type among the GENIA terms. As a term can be related to many others, the number of relations is always higher than the number of terms.
Details of the variation identification rules are given in Appendix B . 3.3. Clustering module
The TermWatch system implements a graph-based approach of the hierarchical clustering called CPCL (Classification by Preferential Clustered Link) originally introduced by Ibekwe-SanJuan (1998b) . The main features of this approach are: (2) an ultrametric model that ensures the existence of a unique and robust solution,
We show here that this algorithm can be applied to other types of inputs. For that, we need to cast the description of the algorithm in the more general context of data analysis.
 objects such that S ij &gt; 0 and s is the valuation of edges defined for all ( i , j ) 2 X of sparse data, the size of E is much smaller than j X j 2
Let Val S be the set of values in S .If j Val S jj S j then, the usual hierarchical algorithms will produce nected components of G . A way to correct this drawback of hierarchical clustering without losing its intu-itiveness and computer tractability is not to consider edge values in an absolute way but in the context of iteration only if S ij is greater than the maximum in the line S in Berry, Kaba, Nadif, SanJuan, and Sigayret (2004) that this variant of hierarchical clustering preserves its main ultrametric properties.
 or to t .

In this approach, the clustering phase can be easily implemented using the following straightforward pro-cedure which we call SLME ( Select Local Maximum Edge). This procedure runs in linear time on the number adjacent edges.
 SLME procedure Input : a valued graph (V,E,s) Output : a relation R on V L :  X  {}
D :  X  {} for every x in V, m[x] :  X  1 while V-L is not empty Once done, the clustering phase consists in computing the reduced graph G / nected components of the subgraph ( V , R )of G and in inducing a new valuation according to a hierarchical criteria chosen among the following: single-link : the value of an edge in G / R between two components C complete-edge : the minimal value in E C 1 C 2 , average-edge : the average value in E C 1 C 2 , vertex-weight : the sum of values in E C
Observe that the above complete-edge and average-edge criteria differ from the usual complete and average because the chain effect has already been reduced by the SLME procedure. In fact, this approach appears drawback can be corrected by the use of a threshold which clarifies the borderline between null values and significant similarities.
 The CPCL algorithm then becomes:
Algorithm CPCL input : a valued graph G =( V,E,s ) parameters : a threshold t and a number of iterations I output : a partition of V for i = 1toi = Ido return V
It involves I calls to the SLME procedure on the current reduced valued graph ( V , E with a reduced range of distinct values.

Until now, this algorithm has been applied to the following similarity matrix defined on groups of objects and generated in two steps:
Step 2 : We select a second subset of variations denoted by CLAS to group components. Next, given two com-the CPCL algorithm. 3.4. Implementation issues Fig. 2 gives an overall view of the system. It is currently run on-line on a Linux Apache MySQL Php PERL implemented as PERL5 OO programs while all the data are stored in a MySQL database. Clustering outputs can be accessed either via an integrated visualisation package (aiSee based on Graph Description Language) for domain topic mapping or through an interactive hypertext interface based on PERL DBI and CGI pack-cuted from this interface. 4. Evaluation metrics
Pantel and Lin (2002) for cluster evaluation. 4.1. Out-of-context Term Clustering (OTC) tion, without any training set and in a completely unsupervised way. We refer to this task as OTC (Out-of-context Term Clustering).
 Let us emphasise that OTC is different from Entity Name Recognition (ENR). ENR task as described in
Kim, Ohta, Tsuruoka, Tateisi, and Collier (2004) is based on massive learning techniques and new terms are forced to enter known categories. Whereas in unsupervised clustering, a new cluster can be formed of terms also be noted that MWTs cannot be reduced to single words. Unlike single words, a MWT can occur only representation to find enough frequency information to form clusters. Therefore methods based on term -document representation cannot be directly applied to OTC without adaptation. This adaptation is described in further details in Section 5 . 4.2. Existing measures for cluster evaluation
Cluster evaluation generally falls under one of these two frameworks: (1) Intrinsic evaluation: evaluation of the quality of the partitions vis-a ` -vis some criteria. (2) Extrinsic evaluation: task-embedded evaluation or evaluation against a gold standard. the absence of an external ideal partition. Internal criteria concern measures like cluster homogeneity and Alternatively, the measure can also seek to determine the optimal number of clusters ( Hur et al., 2002 ). iant ( Hubert &amp; Arabie, 1985 ) that measures the degree of agreement between two partitions. index. According to them, computing the degree of agreements and disagreements between proposed parti-95%. We observe also that the Rand index and the adjusted Rand Index ( Hubert &amp; Arabie, 1985 ) have the following flaws: they are computationally expensive since they require j X j large, they are too sensitive to the number of clusters when comparing clustering outputs of different size ( Wehrens, Buydens, Fraley, &amp; Raftery, 2003 ), the adjusted Rand Index supposes a hyper-geometric model which is obviously not fitted to the distribution of terms in the current experiment (GENIA categories).
 between two partitions. The Jaccard measure appeared as the best in this task since it does not have the drawbacks of the (adjusted) Rand Index. It computes the number of pair of items clustered together by two algorithms divided by the total number of pairs clustered by one of the algorithms. However, it cannot experimental condition. They tested their method on gene expression (microarray) data. This approach, aside information.

In the task-embedded evaluation framework, what is evaluated is not the quality of the entire partition but
Following the extrinsic evaluation approach, Pantel and Lin (2002) proposed the use of the editing distance three elementary actions: copy, merge, move. Considering the OTC task, we needed a measure that focused editing distance appeared as the most suitable for this task. It is adapted to the comparison of methods producing a great number of clusters (hundreds or thousands) and of greatly differing sizes. On a more the-culate the effort or the cost required to attain an existing partition from the ones proposed by automatic clustering methods. 4.3. Metrics for evaluation of clusters the two elementary operations: merges which is the union of disjoint sets and moves that apply to singular elements. In this restricted context, Pantel and Lin X  X  (2002) measure has a more deterministic behaviour and shows some inherent bias which we will correct.

To measure the distance between a clustering output and an ideal partition, these authors considered the minimal number of merges and moves that have to be applied to a clustering output in order to obtain the target partition. In fact, this number can be easily computed since the number of merges corresponds to the number of extra-classes and the number of moves to the number of elements that are not in the dominant The elements of a cluster which are not in the intersection will then have to be moved. Thus, let X be a set of objects for which we know a crisp classification C 2 X representing the output of a clustering algorithm. For each cluster F 2 F , we denote by C such that j C \ F j is maximal. Pantel and Lin X  X  measure can be re-formulated thus: In the numerator of formula (1) , the term j F jj C j gives the number Mg of necessary merges, and the sum P this turns out not to be the case.
 this happens to be the case with the GENIA classes. Following these observations, we propose the corrected version (2) where the weight of each move is no more 1 but j X j /( j X j g ) and the weight of a merge is j X j =  X j X jj C j X  : tition. It is equal to 0 in the case that F is a trivial partition (discrete or complete).
However, l ED can also take negative values. Indeed consider the extreme case where C is of the form { A , B 1 , ... , B n } with one class A ={ a 1 , ... , a
Now take as F the whole family of n pairs { a i , b i } for 1 6 i 6 n augmented with the singletons { w and lim n !1 l ED  X  C ; F  X  X  1.
 of classes fits an exponential model, we have experimentally checked that l Based on the corrected l ED index, we propose a complementary index, Cluster homogeneity ( l the number of savings (product of l ED per j X j ) over the number Mv of movings: l takes its maximal value j X j if F  X  C and, like the l ED partitions.

We will use l H to distinguish between algorithms having similar editing distances but not producing clusters of the same quality (homogeneity). However, since the cluster homogeneity measure relies on the corrected editing distance ( l ED ), for a method to obtain a good cluster homogeneity measure ( l has to show a good savings value (good l ED ). 5. Experimental setup resentations adopted for the methods evaluated (Section 5.2 ) and the clustering parameters for each method (Section 5.3 ). 5.1. The relations used for clustering
Given the OTC task, our experiment consisted in searching for the principle and the method that can best perform this task. Three principles were tested:
CLS : Clustering by coarse lexical similarity: grouping terms simply by identical head word. We call this  X  X  X aseline X  X  clustering as it is technically the most straightforward to implement and is also a more basic relation than the ones used by TermWatch (see Section 3.2 ). However, it should be noted that this head relation is not so trivial for the GENIA corpus. Indeed, Weeds, Dowdall, Keller, and Weir (2005) showed that grouping terms by identical head words enables to form rather homogeneous clusters with regard to the GENIA taxonomy. In their experiment, out of 4797 clusters, 4104 (85%) contained terms with the same
GENIA category while 558 (12%) clusters contained terms with 2 or 3 semantic categories. A further 135 (3%) clusters contained terms with more than p semantic categories.

LSS : Clustering by fine-grained Lexico-Syntactic Similarity as implemented in the TermWatch system using the CPCL clustering algorithm described in Section 3.3 . Terms are represented as a graph of variations.

LC : Clustering by Lexical Cohesion. This principle required a spatial representation based on a vector representation of terms in the space of words they contain. It was suggested by the characteristics of the baseline and graph (LSS) representations. The LC representation offers a numerical encoding of term similarity that allows us to subject statistical clustering approaches (hierarchical and partitioning algorithms) to the OTC task. We describe this representation in more details below. 5.2. Vector representation for statistical clustering methods and built a term  X  word matrix where the rows were the terms and the columns the unique constituent words.
To ensure that the statistical methods will be clustering on a principle as close as possible to the LSS relations used by TermWatch and to the head relation used by the baseline, we further adapted this matrix as follows: words were assigned a weight according to their grammatical role in the term and their position with regard to the head word. Since a head word is the noun focus (the subject), it receives a weight of 1. Modifier words are assigned a weight which is the inverse of their position with regard to the head word. will be weighted 1/2 and coronary 1/3.

More formally, let W =( w 1 , ... , w N ) be the ordered list of words occurring in the terms. A term t =( t 1 , ... , t q ) can be simply viewed (modulo permutations) as a list of words where the t
V t such that
Let M be the matrix whose rows are the V t vectors. We derive two other matrices from M : (2) A core matrix C by removing all rows of M corresponding to terms with less than three words and all the weaker the semantic link with the concept represented by the head. This idea shares some fundamental cation of lexically-cohesive terms (i.e., terms that often share the same words). This idea was explored by
Dobrynin, Patterson, and Rooney (2004) although in a different way. Their contextual document clustering words which appeared with only a few other words. Lexical cohesion is not a new notion in itself. It has &amp; Hanks, 1990; Smadja, 1993 ). 5.3. Clustering parameters
MWTs were clustered following the three types of relations described in Section 5.1 . The following methods chical (CPCL on similarity matrix S ).

Baseline on CLS : No particular parameter is necessary. All terms sharing the same head word are put in the same cluster.

CPCL on LSS : Parameter setting consists in assigning a role to each relation ( COMP or CLAS ). Among all the variations extracted by TermWatch, we selected a subset that optimised the number of terms over the maximal size of a class. Hence this selection was done without prior knowledge of the GENIA taxonomy.
The variations selected for the COMP phase are those where terms share the same head word or WordNet semantic variants. In the current experiment, by order of ascending cardinality, COMP relations were:  X  spelling variants,  X  substitutions of modifiers filtered out using WordNet (sub_wn_modifier),  X  insertion of one modifier word (strong_ins),  X  addition of one modifier word to the left (strong_exp_l),  X  substitutions of the first modifier in terms of length P 3 (strong_sub_modifier_3).

The CLAS variations were:  X  WordNet head substitutions (sub_head_wn),  X  insertions of more than one modifier (weak_ins),  X  addition of more than one modifier word to the left (weak_exp_l),  X  substitution of modifiers in terms of length P 3 (weak_sub_modifier_3).

No threshold was set so as not to exclude terms and relations. Since the objective of this experiment is to a few part of relations induced by the variations were really used in the clustering. More precisely, only relations induced by rare variations which are assigned a higher weight or relations between near-isolated terms were considered. Hence, the exact technique used in agglomerative clustering (single, average or com-plete link) did not come into play here. We also tested the performance of the 1st step grouping, i.e., the level forming connected components ( COMP ) with a subset of the relations. This level is akin to baseline clustering although the relations are more fine-grained.
 values under a threshold th . We used the following values for th :  X  0.8: this weight imposes the same head on related terms.

Because the dissimilarity matrix was too large, we had to use our own PERL programs to handle such sparse matrices. Based on a graph representation of the data, only non zero values were stored as edge val-ues enabling each iteration to be done in a single search. We were thus able to run the usual variants of single, average and complete link hierarchical clustering on this system but they did not produce any rel-requirements to be an input to the CPCL algorithm, we subjected it to the CPCL algorithm. After some
Thus the results shown for hierarchical clustering were obtained using the CPCL algorithm on the term  X  -word matrix.

Partitioning on LC : This method is based on the computation of k -means centers and medoids on the core matrix C . We used the standard functions of k -means and CLARA (Clustering LARge Applications) fully described in Kaufman and Rousseeuw (1990) . CLARA considers samples of datasets of fixed size on which it finds k medoids using PAM algorithm (Partitioning Around Medoids) and selects the results that induce the best partition on the whole dataset. PAM is supposed to be a more robust version of k -means because it directly applied since it requires a lot of computation time. CLARA and PAM are available on the standard
R cluster package. 6 To initialize CLARA, we used the same procedure as CLARANS ( Ng &amp; Han, 2002 )to draw random samples using PERL programs and a graph data structure. We ran these two variants ( k -means and CLARA) for the following values of k : 36, 100, 300, 600 and 900. Then, given these centers and medoids, we again used our PERL programs for storing large sparse matrix, to assign each term to its nearest center or medoid and to obtain a partition on the whole set of terms.
 tition (the GENIA taxonomy) using the measures described in Section 4.3 . Combining R and PERL 5 has sample extraction was done with PERL, leaving to R the massive numerical computations based on C and FORTRAN subroutines. All the tests were performed on a PENTIUM IV PC server running LINUX
DEBIAN stable with 1Go of RAM, SCSI disk and no X11 server for memory saving. 6. Results 6.1. Possible impact of the variations on TermWatch X  X  performance
Before comparing the clustering results obtained by the different methods, we investigated the possible impact of the variations used by TermWatch on its performance. The idea was to determine if our variation relations alone could reproduce these categories, i.e., if they grouped together terms from one only GENIA class. In this case, then there would be no need to perform clustering since the variation relations alone can discover the ideal partition. However, our study showed that this was not the case.
The chart Fig. 3 shows for each of our variation relation, the number of links acquired, the proportion of to heterogeneous clusters, they link terms from different GENIA classes. Surprisingly, weak _ exp _ l and 6.2. Evaluation of clustering results Using the relations chosen in Section 5.3 , CPCL on LSS generated 1897 non trivial components (at the
COMP phase) involving only 6555 terms. Adding CLAS relations in the second phase led to 3738 clusters involving 19,887 terms.
 ing 25,129 terms for a threshold th = 0.5 and 1217 clusters involving 19,867 terms for th = 0.8. The plots in Figs. 4 and 5 show the results of the evaluation measures l words are considered. Thus, the further we move down the x -axis, the fewer the input terms for clustering.
Fig. 4 shows the % of savings obtained by the nine algorithms tested using the corrected ED measure. We partitioning methods outperform CPCL and hierarchical algorithms but still remain below the baseline. This is because, at length P 3, CPCL has fewer terms, thus fewer relations with which to perform the clustering.
Statistical methods on the other hand, with longer terms have a better context, thus more relations in the matrix. From terms of length P 4 words, partitioning methods outperform the baseline.

However, the ED measure masks important features of the clustering outputs since it is a compromise between the number of necessary moves and merges needed to reach the target partition. More important measured by the l H which calculates the ratio between the value of ED and the number of movings. The l performance of the algorithms is shown in the plot of Fig. 5 .

It appears clearly that on cluster quality, CPCL is the only algorithm that significantly outperforms the baseline irrespective of term length. Hierarchical algorithm with th = 0.8 and the COMP phase of CPCL follow closely but only on all terms (length P 1). Their performance drops when terms of length &gt;= 4 are considered. Partitioning algorithms show poor cluster homogeneity. K -means with k = 100 performs worse score.

To gain a better insight on the cluster homogeneity property, we generated for every algorithm a chart showing the proportion of terms which share the same GENIA class with the majority of terms in the same cluster (and thus that do not require any move). The nine charts are shown in Fig. 6 .

It appears that the COMP variant of CPCL produced the most homogeneous clusters which is not altogether surprising because the relations used in COMP phase are the most semantically tight. COMP and CPCL sig-nificantly outperform the baseline. This good performance is a bit unexpected for CPCL because the CLAS relations induce a change of head word which could lead to a semantic gap (change of semantic class).
Closely following is the hierarchical algorithm at th = 0.8. The baseline comes fourth which shows that grouping terms simply by identical head words as done by baseline is good but not good enough to form semantically homogeneous clusters.

Partitioning methods clearly produced less homogeneous clusters. These algorithms showed low error rates roughly on categories with a low proportion of one word terms. 7. Concluding remarks
We have developed an efficient text mining system based on meaningful linguistic relations which works well on MWTs and thus on very large and sparse matrices. This method is suitable for highlighting rare phe-nomena which may correspond to weak signals.

The specific evaluation framework set up here led us to redefine a matrix representation in order to enable measure and defined a complementary one focused on cluster homogeneity.

The choice of the evaluation metric made it possible to compare algorithms outputting very high number of done without any assumption of equal cluster size. We believe these differences did not handicap any algo-especially our own method. As we cannot define a priori the number of optimal clusters, CPCL X  X  performance tive to term length.

The results however show that CPCL performs well in terms of cluster quality (homogeneity). Since this separate study.

Overall, this experiment has shown that even without adequate context (document co-occurrence), clustering algorithms can be adapted to partially reflect a human semantic categorisation of scientific terms.
 texts. These can be further refined using more sophisticated approaches: fine-grained linguistic relations, machine learning techniques with manually tagged learning sets.
 Appendix A. Example of rule used in term extraction mation of terms. Thus prepositional phrases introduced by this preposition are attached to their governing NP.

From the tagged sentence: [[The _ DT inability _ NN ]] of _ IN [[ E1A _ CD gene _ NN products _ NNS]] (to _ TO induce _ VB ) [[ cytolytic _ JJ susceptibility _ NN ]] .

Our term extraction module would extract two multi-word terms (MWTs): [[ The _ DT inability _ NN ]] of _ IN [[ E1A _ CD gene _ NN products _ NNS]] [[ cytolytic n _ JJ susceptibility n _ NN ]] This rule can be formulated as the following regular expression:
If : then return: where: &lt; mod &gt; = a determiner (DT) and/or an adjective (JJ) Appendix B. Variation rules For the sake of clarity, all the variation rules will be given for the compound structure only. B.1. Lexical variants
Modifier substitutions ( sub _ modifier ) can be identified with this simple rule: t2 is a substitution of t1 if and only if: with m 0 &lt;&gt; m where
A chain of modifier substitutions can highlight properties around the same concept. For instance, the fol-lowing variants all specify a type of human cell line : human leukemia cell line human lymphoblastoid cell line human monoblastic cell line human monocytic cell line
Head substitutions ( sub head ) are identified via the following rule: t2 is a substitution of t1 if and only if: with h 0 &lt;&gt; h
Head substitutions highlight on the other hand families of concepts sharing the same property: tumor cell killing tumor cell line tumor cell nuclei tumor cell proliferation tumor cell type B.2. Syntactic variants These rules identify the three types of expansion variants.

Left expansion (exp_l) For example, Ad2 infection has as left expansion adenovirus 2 (Ad2) infection .

Insertion (ins)
For instance, CD3-stimulated T lymphocyte has as insertion variant, the term CD3-stimulated human periph-topical shifts as in human disease and human disease syndrome . Equivalent terms which undergo either a identified.

Right expansion (exp_r)
An example of right expansion would be B cell development and B-cell development and differentiation . Left and right expansions (exp-2) can be combined in the same term to yield left-right expansion . An example would be the link between AIDS and second AIDS retrovirus . References
