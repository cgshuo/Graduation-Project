 Although many variants of language models have been pro-posed for information retrieval, there are two related re-trieval heuristics remaining  X  X xternal X  to the language mod-eling approach: (1) proximity heuristic which rewards a doc-ument where the matched query terms occur close to each other; (2) passage retrieval which scores a document mainly based on the best matching passage. Existing studies have only attempted to use a standard language model as a X  X lack box X  to implement these heuristics, making it hard to opti-mize the combination parameters.

In this paper, we propose a novel positional language model (PLM) which implements both heuristics in a unified lan-guage model. The key idea is to define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of  X  X oft X  pas-sage retrieval. We propose and study several representa-tive density functions and several different PLM-based doc-ument ranking strategies. Experiment results on standard TREC test collections show that the PLM is effective for passage retrieval and performs better than a state-of-the-art proximity-based retrieval model.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms Positional language models, proximity, passage retrieval
As a new generation of probabilistic retrieval models, lan-guage modeling approaches [23] to information retrieval (IR) have recently enjoyed much success for many different tasks [30]. In the past decade, many variants of language mod-els have been proposed, mostly focusing on improving the estimation of query language models (e.g., [31, 15]) and doc-ument language models (e.g., [17]). Although these language models are motivated in a different way than a traditional model such as the vector-space model, they tend to boil down to retrieval functions that implement retrieval heuris-tics similar to those implemented in a traditional model, such as TF-IDF weighting and document length normaliza-tion [32]. With sound statistical foundation, these language models make it easier to set and optimize retrieval parame-ters and often outperform traditional retrieval models.
Although much work has been done in language mod-els, there are two related retrieval heuristics remaining  X  X x-ternal X  to the language modeling approach: (1) proxim-ity heuristic which rewards a document where the matched query terms occur close to each other; (2) passage retrieval which scores a document mainly based on the best match-ing passage. Existing studies have only attempted to use a standard language model as a black box to implement these heuristics, which means that these heuristics have not really been incorporated into a language model as a component and make it hard to leverage advantages of language mod-els to optimize the combination parameters. For example, proximity heuristic has been studied in [28] where the au-thors proposed heuristic proximity measures and combined them with the scores of documents computed using standard language models. Also, passage retrieval has been studied in [16] where the authors explored different ways of segmenting text to create passages and then applied standard language models on top of the passages as if they were regular doc-uments. A common deficiency of these studies is that the proximity and passage retrieval heuristics are not modeled from language modeling perspective, making it difficult to optimize the way of combining them with language models.
In this paper, we propose a novel positional language model (PLM) which implements both heuristics in a unified lan-guage model. The key idea is to define a language model for each position of a document (thus the name positional language model), and score a document based on the scores of its PLMs. This is in contrast with virtually all the exist-ing work in which a document language model is generally defined for the entire document. An important advantage of introducing a language model for each position is that it can allow us to model the  X  X est-matching position X  in a document with probabilistic models, thus supporting  X  X oft X  passage retrieval naturally.
The PLM at a position of a document would be estimated based on the propagated word counts from the words at all other positions in the document. Specifically, we let each word at each position of a document to propagate the ev-idence of its occurrence to all other positions in the doc-ument so that positions close to the word would get more share of the evidence than those far away. This way, each position would receive propagated counts of words from all the words in the document with most propagated counts coming from words near the position. We can then estimate a language model for the position based on the propagated counts reaching the position.

A main technical challenge in implementing this idea is how to define the propagation function and estimate the PLM accordingly. We propose and evaluate several differ-ent proximity-based density functions for propagation. With some specific choices, we show that the PLM can cover the standard whole document language model and the fixed-window passage language model, as special cases. Since in all these density functions, close-by positions would receive more propagated counts than positions far away from the current word, the PLM also captures the proximity heuris-tics.

Once we have a language model estimated for each po-sition, we can use one or multiple PLMs of a document as regular document language models to generate a score for the document. We propose and study three general doc-ument ranking strategies for combining different PLMs to score documents, including scoring based on the best PLM, combining scores from PLMs at multiple positions, and com-bining PLMs with different propagation ranges.

Experiment results on several standard test collections show that among all the proximity-based density functions, the Gaussian density kernel performs the best, and that combining PLMs with different propagation ranges is the best document ranking strategy. It is also observed that the proposed PLM not only outperforms the general doc-ument language model, but also outperforms the regular sliding-window passage retrieval method and a state-of-the-art proximity-based retrieval model. Overall, the PLM is shown to be able to achieve  X  X oft X  passage retrieval and cap-ture proximity heuristic effectively in a unified probabilistic framework.
Term proximity in information retrieval has been previ-ously studied in [11, 12, 7, 5, 24, 20, 2, 3, 28, 27]. Keen X  X  work [11, 12] is among the earliest efforts, in which, a X  X EAR X  operator was introduced to address proximity in Boolean re-trieval model. The shortest interval containing a match set was first used as a measure of proximity in [5, 7]. Recent work has attempted to heuristically incorporate proximity into an existing retrieval model (often through score combi-nations) [21, 24, 2, 3, 28]. A variety of proximity measures were proposed, e.g., minimum term span, minimum pair-wise distance, etc.; in [28], the authors systematically exam-ined different measures and concluded that the minimum pair-wise distance is most effective.

An indirect way to capture proximity in the language modeling framework is to use high-order n-grams as units to represent text. For example, in [26], bigram and trigram language models were shown to outperform simple unigram language models. However, n-gram language models cannot capture dependency of non-adjacent terms (we may attempt to capture such proximity by increasing the length of an n-gram, but it is impractical to enumerate all lengths). A more general way to capture proximity through using ap-propriate  X  X atching units X  is Metzler and Croft X  X  work on term dependency [20]. In that work, term structures with different levels of proximity can be defined in a general prob-abilistic model. Unfortunately, they only attempted to use a standard language model as a black box to implement the proximity heuristic.

Our work differs from the previous studies in two impor-tant aspects. First, we propose a new type of language model that incorporates the term proximity evidence in a model-based approach; thus, the existing language modeling techniques (e.g., mixture-model based feedback [31]) can be applied to our model naturally. Second, we capture term proximity directly based on proximity-based term propaga-tion functions.

In passage retrieval [25, 4, 9, 16, 29], documents are of-ten pre-segmented into small passages, which are then taken as units for retrieval. Also documents can be segmented in a more dynamic way defined at query time, referred to as arbitrary passages [9] ( X  X rbitrary X  means that a passage can start at any position in a document). Two subclasses are further defined: fixed-length arbitrary passages resem-ble overlapped windows but with an arbitrary starting point; variable-length arbitrary passages can be of any length. Fixed-length arbitrary passage retrieval was shown to be as effec-tive as, but more efficient than variable-length arbitrary pas-sages [9]. The proposed PLM covers the fixed-length arbi-trary passage as a special case, and can be viewed as a  X  X oft X  fixed-length passage. However, different from general pas-sage retrieval, which only models term position evidence in-directly using a standard language model as a black box, our model can incorporate term position information directly into the estimation of language models using a proximity-based propagation function.

Proximity-based density functions have been used to prop-agate term influence in [6, 13, 19, 22]. Kretser X  X  work [6] proposed to propagate the tf  X  idf score of each query term to other positions, based on several proximity-based kernel functions. The document is scored using the position with the highest accumulative tf  X  idf score finally. But their methods have not been able to achieve effective retrieval performance. The studies [13, 19] are very similar to [6] and based on the vector space model and Boolean model respectively. In our work, we also evaluate the kernel func-tions proposed in these studies. In addition, we also pro-pose several other density functions that are more effective than theirs. Compared with this previous work, our work also differs in that we use the language modeling framework, and incorporate such density functions into the estimation of language models.

Similar to our work, Petkova and Croft X  X  work [22] pro-posed a proximity-based document representation for name entities. Their work emphasizes terms of proximity to en-tities by using a proximity-based density function, which is then used to build description for entities. Our work, how-ever, proposes a positional language model for general doc-ument retrieval, and we evaluate the empirical performance of a number of proximity-based density functions systemat-ically.
In this section, we propose a positional language model (PLM) to incorporate term position information into the language model so that we can naturally implement retrieval heuristics such as proximity and passage retrieval.
In most existing work on language models, a document language model is estimated based on only the counts of words in a document, but not the position of words. The main idea of the PLM is to break this limitation and estimate language models based on the position-dependent counts of words. At a high-level, our idea is to define a language model for each word position in a document. This language model is intended to capture the content of the document at the position, which is roughly like a  X  X uzzy passage X  centered at this position but can potentially cover all the words in the document with less weight on words far away from the position.

Specifically, we assume that a term at each position can propagate its occurrence at that position to other positions within the same document through a proximity-based den-sity function, as shown in Figure 1. The idea is that if a word w occurs at position i , we would like to pretend that the same word has also occurred at all other positions with a discounted count such that if the position is closer to i , the propagated count for word w at that position would be larger than the propagated count at a position farther away, even though both propagated counts would be less than one, which is the count of w at position i .

The PLM at each position can then be estimated based on all the propagated counts of all the words that to the position as if all the words had appeared actually at the position with discounted counts. Such a PLM intuitively gives a position-specific view of the content of the document, and thus can naturally support passage retrieval. It can also implement the proximity heuristic because of the use of a proximity-based propagation function.

Once we obtain a PLM for each position in a document, we can use each PLM as a regular document language model for matching with a query. We can then score the document by using one or combining multiple PLMs as we will explain later.

We now present PLM more formally. We first introduce the following notations. Let D = ( w 1 , ..., w i , ..., w be a document, where 1, i , j , and N are absolute positions of the corresponding terms in the document, and obviously N is the length of the document. c(w, i): the count of term w at position i in document D . If w occurs at position i , it is 1, otherwise 0. k(i, j): the propagated count to position i from a term at position j (i.e., w j ). Intuitively, given w j , k ( i, j ) serves as a discounting factor and can be any non-increasing function plays an important role in PLMs, and we will analyze and explore a number of proximity-based density functions. c (w, i): the total propagated count of term w at posi-tion i from the occurrences of w in all the positions. That is, c 0 ( w, i ) = c ( w, i ) may be greater than 0. As shown in Figure 1, after propagation, position 0  X  0 has a non-zero  X  X ount X  of terms Q and Q 1 .

Based on term propagation, we have a term frequency vec-document D i . We can see that term position information has been translated to term frequency information stored in this vector. Thus the language model of this virtual docu-ment can be estimated as: where V is the vocabulary set. We call p ( w | D, i ) a Posi-tional Language Model (PLM) at position i .

Intuitively, we can imagine that the PLMs give us multiple representations of D . Thus given a query Q , we can adopt the KL-divergence retrieval model [14] to score each PLM as follows: where p ( w | Q ) is an estimated query language model. We can estimate p ( w | Q ) with the maximum likelihood estimate or through some pseudo relevance feedback algorithms (e.g., relevance model [15] or mixture model [31]).

Similar to a regular document language model, the PLM also needs to be smoothed to solve the zero probability prob-lem and to penalize common terms [32]. We consider two popular smoothing methods: Dirichlet prior and Jelinek-Mercer. Dirichlet prior smoothing has proven an effective smoothing method for document language models and cap-tures the document length normalization heuristic [32]. For a PLM, the length of the virtual document at position i is Z = model p ( w |C ) as our background model. Thus the smoothed model is given by: where  X  is a smoothing parameter. Although previous work has shown that Jelinek-Mercer does not work as well as Dirichlet prior [32], it is unclear whether the same conclu-sion holds for PLMs because the virtual document length Z at different positions are similar to each other [18]. We thus also consider it as an alternative smoothing method, which is given by: where  X  is a smoothing parameter.
Clearly, a major technical challenge in PLMs is how to define the propagation function k ( i, j ). Following some pre-vious work [6, 13, 22], we present here four representative Figure 2: Proximity-based kernel functions. We set  X  = 12 . 5 for all kernels. kernel functions: Gaussian, Triangle, Cosine, and Circle, as shown in Figure 2. Different kernels lead to different PLMs. 1. Gaussian kernel 2. Triangle kernel 3. Cosine (Hamming) kernel 4. Circle kernel
All these four kernels have one parameter  X  to tune, which controls the spread of kernel curves, i.e., it restricts the prop-agation scope of each term. In general, the optimal setting of  X  for a term may vary according to the term and may also depend on the query because some general terms presum-ably would have wider semantic scope in a document, thus requiring a higher value of  X  , and similarly, some general query might match a longer relevant passage than a more specific query. Our definition of PLMs would in principle allow us to explore such options. However, as a first study of PLMs, in this paper, we simply assume that  X  is set to the constant across all the terms and all the queries, leaving further optimization of  X  as a future work.

As a baseline, we also present the following non-proximity-based Passage kernel: 5. Passage kernel: With the passage kernel, the PLM can recover the fixed-length arbitrary passage retrieval method [9] in the language modeling framework. We would use this kernel as a base-line to examine whether the proximity-based kernel func-tions perform better than this non-proximity-based kernel.
According to the proximity-based propagation property, p ( w | D, i ) is mainly influenced by terms around the position i , all of which form a  X  X oft X  passage together. Hence, the PLM captures a  X  X oft passage X  naturally in the language modeling framework. Moreover, different from general pas-sage retrieval, which only captures term position evidence indirectly, our model can measure term position informa-tion and incorporate it into a language model directly.
Furthermore, if we set  X  to a very large or infinite value for any of the proposed kernels, we would have k ( i, j ) = 1 for all i and j . Thus, we have c 0 ( w, i ) = c ( w, D ), which means that p ( w | D, i ) degenerates to the basic whole document language model p ( w | D ). This shows that the PLM can cover the basic language model as a special case. In general, we can balance the local term proximity evidence and the document level term statistics by tuning the parameter  X  (a small  X  would emphasize more on local term proximity). Thus, PLM captures term proximity in-formation in the language modeling framework in a natural way.
As discussed earlier, with PLMs, we can compute a position-specific score S ( Q, D, i ) for each position i using the KL-divergence of the PLM at the position and the query lan-guage model. Such position-specific scores serve as the basis for computing an overall score for document D . We now discuss several different ways of doing this.
Our first strategy is to simply score a document based on the score of its best matching position, formally, This strategy resembles most existing studies on passage retrieval, which generally considered evidences from the best matching passage [4, 9, 16].
A more flexible alternative strategy is to first compute the scores of top-k positions separately, and then combine these scores together to take advantage of the evidence from several top ranked positions. Particularly, we can take the average of the top-k scores to score a document: where TopK is the set of positions corresponding to the top-k highest scores of S ( Q, D, i ).
In this strategy, we compute the best position scores for several different  X  values, and then combine these scores to-gether as the final score for a document. The idea is to use different  X  values to capture proximity at different propaga-tion ranges.
 where R is a predefined set of  X  values, S  X  (  X  ) is the score function for PLMs with parameter  X  ,  X   X  is the weight on different  X  ( this strategy equals to an interpolation of the PLM (with a parameter  X  0 ) and the regular document language model. Considering the efficiency issue, we only evaluate this special case of multi- X  strategy, defined formally as follows:
If the PLM is naively implemented, the cost of estimating and ranking PLMs can be extremely high, since the number of positions is much larger than the number of documents or predefined passages. Fortunately, with some mathematical transformation, we may significantly reduce the computa-tional complexity. Below we will show that under reason-able assumptions, PLMs can be implemented similarly to the fixed-length arbitrary passage retrieval.

Given a query, suppose all terms in a document have the same propagation function with the same  X  , and the curve of the kernel density function is symmetric. Then we have k ( i, j ) = k ( j, i ). Since the most time-consuming part is to compute the normalized length Z i = rewrite it as:
This means the sum of propagated count to a position is equal to that propagated from the position. We show the computation of Z i for the Gaussian kernel as an example: where  X  (  X  ) is the cumulative normal distribution and N is the document length. To calculate  X  (  X  ), we can adopt some existing algorithms, such as the algorithm 26.2.17 [1]. For other kernels considered by us, it is also easy to obtain their cumulative distribution functions through integration.
From this analysis, we can see that our PLM can be imple-mented similarly to fixed-length arbitrary passage retrieval model. Thus, we can use the techniques proposed in [10] for passage retrieval to implement PLMs; with such an imple-mentation, ranking documents based on PLMs has a com-plexity of the same order as regular document ranking. We used several standard TREC data sets in our study: AP88-89, FR, TREC8, and WT2G. They represent differ-ent sizes and genre of text collections. AP88-89 is chosen as a homogeneous collection. FR is selected as a collection of long documents, with a large variance in the document length. TREC8 is a relatively large heterogeneous collection, while WT2G is Web data. Queries are taken from the title field of the TREC topics 1 . Table 1 shows some basic statis-tics about these data sets. The preprocessing of documents and queries is minimum, involving only stemming with the Porter stemmer. No stop words have been removed.
In each experiment, we first use the baseline model (KL-divergence) to retrieve 2 , 000 documents for each query, and then use the PLM (or a baseline method) to re-rank them.
Topic 110 in AP88-89 was left out accidently due to a for-mat problem in preprocessing.
 The top-ranked 1 , 000 documents for all runs are compared using the mean average precisions (MAP) as the main met-ric. We first examine the effectiveness of the Best Position Strategy for scoring documents based on PLM. Since the performance of this strategy is directly determined by the ef-fectiveness of the kernel function used to estimate the PLM, we first compare the proposed four different proximity-based kernel functions to see which one performs the best. For this comparison, the initial retrieval results were obtained us-ing the KL-divergence retrieval model with Dirichlet prior smoothing; since the relative performance of different kernel functions would presumably not be affected by the setting of the smoothing parameter in the initial retrieval, we did not tune the smoothing parameter and simply set it to 1 , 000. To compare different kernel functions, we follow [9] and sys-tematically test a set of fixed  X  values from 25 to 300 in increments of 25. For the sake of efficiency, positions start at 25-word intervals, which was shown by [8] to be an effec-tive way for passage retrieval.

Since we also smooth an estimated PLM when computing retrieval scores, we test both Dirichlet prior smoothing (with parameter 1 , 000) and Jelinek-Mercer (with parameter 0.5) (see Equations 3 and 4). The results of comparing differ-ent kernel functions when using each smoothing method are shown in Table 2 and Table 3, respectively. The best result for each  X  value is highlighted. Overall, we see that for all kernels, a relatively large  X  value, e.g., 125, 175, and 275, often brings the best performance. It seems that the perfor-mance of all runs stabilizes after  X  reaches 125. Considering the length of the soft passage is approximately 2  X  (as shown in Figure 2), this result confirms the observation in recent studies of passage retrieval [9, 16] that setting passage length to a value around 350 often achieves the best performance. Among all the kernel functions, the Gaussian kernel clearly has the best performance; it contributed 15 best MAP scores out of 20 for Dirichlet prior smoothing and 13 out of 20 for Jelinek-Mercer. To see whether the setting of the smooth-ing parameter may have affected the relative performance of these kernel functions, we further compare them for a wide range of values of the Dirichlet prior parameter on TREC8 in Figure 3, where we fix  X  = 175 for all kernels. The re-sults clearly show that the Gaussian kernel is the winner among all the four functions. One way to explain why the Gaussian kernel performs the best is that it is the only one of all the functions that exhibits the following property: the propagated count would drop slowly when the distance value | i  X  j | is small, but drop quickly as the distance value is in a middle range, and then drop slowly again when distance value becomes very large. Such an  X  X -shape X  trend is rea-sonable for the following reason. Dependent terms are not always adjacent in documents, but can be a little far from each other, thus we would not like to make the propaga-Figure 3: Sensitivity to Dirichlet smoothing param-eter of different kernels over TREC8 tion so sensitive to the distance when the distance is small. However, when the distance is just around the boundary of strong semantic associations (semantic scope of a term), the propagated count should be more sensitive to the distance change. Then as the distance increases further, all terms are presumably only loosely associated, and thus the prop-agated term count again should not be so sensitive to the difference of distances.

Since the Gaussian kernel performs the best, in all the following experiments, we use this kernel function. In or-der to see whether PLM can effectively capture proximity and passage retrieval, we compare the performance of Gaus-sian kernel (  X  = 175 ) with the baseline whole document language model using both Dirichlet prior smoothing and Jelinek-Mercer smoothing (both the PLM and the baseline would use the same smoothing method). The results are shown in Figure 4, where we vary the smoothing parameters for both smoothing methods on all the four data sets. We can observe that the PLM improves performance on WT2G and FR clearly and consistently, which shows that, similar to general passage retrieval, the PLM can bring added ben-efits to document retrieval when documents are relatively long. Some improvements are also found on TREC8, pos-sibly because it is a heterogeneous data set as compared to AP88-89, which is homogeneous; a heterogeneous collection is relatively nosier, thus term dependence information may be more helpful. Unfortunately, the PLM does not seem to show its advantages on AP88-89 for Dirichlet prior smooth-ing even though it is so for Jelinek-Mercer smoothing.
By comparing the results of the two smoothing methods in Figure 4, we see that in general, Dirichlet prior performs better than Jelinek-Mercer for PLM, and the Dirichlet prior smoothing method seems to perform stably for a range of  X  values around 500.
 Figure 4 shows that PLM outperforms whole document LM baseline likely due to the use of proximity and passage-retrieval heuristics. We would like to further understand whether PLM, which captures  X  X oft X  passages, is also bet-ter than the fixed-length arbitrary passage retrieval method. Thus, we compare the PLM (using the Gaussian kernel,  X  = 175 , Dirichlet prior smoothing,  X  = 500 ) with the fixed-length arbitrary passage retrieval method [9] (i.e., the Passage kernel). The MAP scores are summarized in Table 4, where the best result for each  X  is highlighted. We can observe that the PLM indeed outperforms the standard pas-sage retrieval baseline significantly, which shows that model-ing term proximity directly using a proximity-based density function is more effective and robust than assuming fixed lengths.
 Table 4: Comparison of fixed-length arbitrary pas-sage retrieval (Psg) and PLM. 0 + 0 means that im-provements over the Psg are statistically significant. Figure 5: Sensitivity to the parameter k of multi-position strategy
We now evaluate the multi-position ranking strategy. Based on the observation in the previous section, we use the Gaus-sian kernel (  X  = 175 ) with Dirichlet smoothing (  X  = 500 ) for the PLM. We vary parameter k and plot the MAP re-sults of multi-position strategy in Figure 5. We see that the multi-position strategy does not lead to any noticeable improvement over the best position strategy (i.e., k = 1 ), and if we use a relatively large k , the performance can even degrade dramatically. Hence, given a single  X  value, the best position strategy is a robust and reasonable method for document ranking.
We now turn to the evaluation of the multi- X  strategy  X  in particular, the special case of interpolating a PLM with the whole document language model (i.e.,  X  =  X  ). To test this special case of Multi- X  strategy, we fix one  X  value to  X  , and vary the other one from 25 to 300 in increments of 25. For each  X  value, we again use the Gaussian ker-nel and the Dirichlet prior smoothing method (  X  = 500 ). The results are presented in Table 5, where we tune the interpolation coefficient  X  in the range of [0 . 0 , 1 . 0] to its op-timal value for each  X  . It shows that, when interpolated with document language models, the PLM performs more robustly and effectively. One possible explanation is that a locally focused PLM alone does not model document-level retrieval heuristics as effectively as the whole document lan-guage model does, even though the former captures term proximity heuristic better, thus balancing them will get bet-ter results. Another interesting observation is that the best results are always obtained when we use a smaller  X  value, e.g. 25 or 75, which also suggests that the PLM is bet-Figure 6: Sensitivity to  X  value of multi- X  strategy. ter at capturing local term proximity evidence rather than document-level evidence (e.g., term frequency). Table 5: The best performance of multi- X  strategy for different  X  . 0 + 0 means that improvements over the baseline KL method are statistically significant.
To further look into the sensitivity to  X  , we set R = { 75 ,  X  X  and vary  X  on all the four data sets. The re-sults are shown in Figure 6. Interestingly, for collections of long documents (i.e., FR and WT2G), we can rely more on PLMs (larger  X  ), likely because the whole document lan-guage models may contain much noise, but for collections of short-documents (i.e., TREC8 and AP88-89), the sensi-tivity curves are generally flatter and a relatively smaller  X  seems working better, suggesting that regular document lan-guage models work reasonably well without term proximity information.

We finally compare our multi- X  strategy ( R = { 75 ,  X  X  ,  X  = 0 . 8 for FR and WT2G, and  X  = 0 . 4 for TREC8 and Table 6: MAP Comparison of the multi- X  strategy and the best method proposed by Tao and Zhai.
 AP88-89) with a state-of-the-art proximity retrieval method proposed in [28]. Our parameter setting gives PLM near optimal performance. To be fair, we also use the best lan-guage modeling retrieval formula suggested in [28] (i.e., R + MinDist), and tune their parameter  X  to its optimal value to re-rank documents. We label this run as  X  R 1 + MinDist X  and report the comparison results in Table 6. Interestingly, we see both methods perform similarly on short-document collections (i.e., TREC8 and AP88-89), but our method is clearly better on long-document collections (i.e., WT2G and FR), suggesting that the proposed PLM can capture the pas-sage and proximity heuristics more effectively.
In this paper, we proposed a novel positional language model which implements both proximity heuristic and pas-sage retrieval in a unified language model. We proposed and studied four different proximity-based density functions to estimate PLMs. Experiment results show that the Gaussian density kernel performs the best, followed by Circle, Trian-gle, and Cosine. As for the smoothing of PLM, the Dirich-let smoothing method performs better than Jelinek-Mercer smoothing.

In addition, we further proposed three PLM-based doc-ument ranking strategies. We evaluated their performance and found that the multi- X  strategy performs the best. Our experiments on several standard test collections show that the proposed PLM not only outperforms the regular docu-ment language models, but also outperforms the fixed-length arbitrary passage retrieval method and a state-of-the-art proximity-based retrieval model.

As a new family of language models, the PLM opens up many interesting future research directions. One of the most interesting directions is to further study whether setting a term-specific and/or query-specific  X  can further improve performance. Another interesting direction is to study how to optimize  X  automatically based on statistics such as IDF of terms and discourse structures of documents.
We thank the anonymous SIGIR X 09 reviewers for their useful comments. This material is based upon work sup-ported by the National Science Foundation under Grant Numbers IIS-0347933, IIS-0713581, and IIS-0713571.
