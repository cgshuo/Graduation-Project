 In the language modeling approach [1][2][3][4][5], a core technique for estimating document language model is smoothing, which adjusts the maximum likelihood estima-sparseness. Zhai and Lafferty studied and compared three popular smoothing methods and their influences on retrieval performance: Jelinek-Mercer, Dirichlet and absolute discounting [5][6]. Their experimental results showed that the accuracy of smoothing is directly related to retrieval performance [5][6]. In this paper we develop a new smooth-ing method  X  GJM-2. Based on the analysis for Jelinek-Mercer method, we propose a general linear interpolated smoothing method  X  GJM, in which a document-dependent GJM. Using the count of unique terms in current document rather than the document length, we further derive GJM-2 method, an improved version of GJM-1. Experimental GJM-2 are better than the three popular methods both on short and long queries. Suppose we have a collection with total N documents, whose vocabulary is V = { w 1 , w documents according to the relevance of each document with the query. Based on the assumption of words independence, the language modeling approach views each document D as an observed sequence of words generated by a multinomial language tion model (i.e. unigram model) parameterized by  X  D . And the relevance of document D with the query Q is measured by the likelihood of Q according to the multinomial model  X  D : where c ( w , Q ) is the frequency of word w occurring in query Q . 
Clearly, the retrieval problem is now essentially reduced to a multinomial language model estimation problem. The simplest method to estimate  X  D is to utilize the maxi-mum likelihood estimator (MLE): length of D , i.e.  X  =
However, using MLE can cause serious zero probability problem due to data sparseness. A direct solution to the problem is smoothing, which adjusts the MLE so language model estimation. According to [5], there are three popular smoothing methods applied to the language modeling approaches to ad hoc IR: Jelinek-Mercer, number of unique words in document D . The collection language model  X  C is typi-cally estimated by MLE based on the whole collection, i.e. Clearly, in the context of ad hoc IR, different documents have different statistical fea-tures, such as document length etc. However, Jelinek-Mercer method doesn X  X  take this into account. An intuitive consideration is to make the coefficient  X  dependent on the document. So we propose following general Jelinek-Mercer smoothing method (GJM): where  X  D is a document-dependent coefficient. 
Ideally, GJM could make good use of the differences between the statistical fea-tures of documents and incorporate them into  X  D to improve the accuracy of the document language model estimation. However, the requirement for efficient compu-tations over large collection make it impossible to set a completely different value of  X  for each document. A reasonable tradeoff is to partition the collection into multiple  X  X uckets X  according to the commonness or similarity of statistical features between documents, and then to set the same value of  X  D to the documents in the same  X  X ucket X . So a crucial question for GJM method is: how to partition the collection, i.e. what statistical features of a document can be used to partition the collection? 
From the statistical perspective longer document should put more trust in the maximum likelihood estimator )  X  | ( the following GJM-1 smoothing method: 
Clearly, GJM-1 is equivalent to Dirichlet, which indicates that Dirichlet is a special case of GJM. The excellent performance of Dirichlet showed in [5] validates that in-language model smoothing, which further testify the feasibility of the idea of GJM. Besides document length, is there any other statistical feature to help improve the ac-curacy of language model estimation? 
Suppose there are two different documents D 1 and D 2 . The length of D 1 is the same same trust as that of D 2 . This is not what we expect. From statistical perspective, the count of observed events in D 1 is larger than in D 2 , which suggests it is more plausible to make D 1 get more trust in the maximum likelihood estimator than D 2 . 
On the other hand, a larger | D | u also implies a larger | D | to some extent, so we think that | D | u will be more appropriate than | D | as a statistical feature for partitioning col-lection in order to estimate language model more accurately. Similar to GJM-1, we set stant,  X  &gt; 0. So we get the following GJM-2 smoothing method: On the surface, GJM-2 is a typical linear-interpolated smoothing method similar to Jelinek-Mercer except that the interpolation coefficient  X  D of GJM-2 is document-dependent: when using GJM-2 to smooth document language model, the larger the number of unique terms in the document, the more trust will be put in the maximum likelihood estimator. If compared to the lin ear-interpolated form of Dirichlet method, GJM-2 is highly similar to Dirichlet except that | D | in Dirichlet method is substituted with | D | u in GJM-2. We think that when we adjust maximum likelihood estimator rately than | D |, which will be testified in our experiments. Furthermore, from the point of view of computational efficiency, clearly GJM-2 is as efficient as the three popular smoothing methods, and it is very appropriate for the task of ad hoc IR. Our goal is to evaluate whether GJM-2 can improve retrieval performance when compared to the three popular methods. We use four data sets from TREC2, TREC3, TREC7 and TREC8 ad hoc task, respectively. Similar to [5], we construct two differ-description + narrative). The short queries are mostly two or three key words, whereas the long queries have multiple whole sentences. In all our experiments we only apply stemming with a Porter stemmer for tokenization. No stop words are removed. Table 2 gives the labels for all data sets, bas ed on the collections and topics. 
In order to compare GJM-2 with the three methods, we follow the experimental methodology described in [5]: first we select a best run (in terms of non-interpolated average precision) for each method on each testing set, and then compare the average precision (AvgPrec), precision at 10 documents (P@10), and precision at 20 docu-ments (P@20) of the selected runs. The results are shown in Table 3. 
In spite of the unstable order among the old three methods, it is evident to see that: (1) On all testing sets the average precision (non-interpolated) of GJM-2 is better than the other three existing methods. (2) On most of testing collections P@10 and P@20 of GJM-2 are better than the three methods. (3) On average, GJM-2 is best among the four methods according to the three precision measures. Fig. 1 also shows their com-parisons of average precision (non-interpolat ed) on short and long queries more intuitively. In the language modeling approach to ad hoc IR, a central issue in language model es-timation is smoothing, which adjusts the maximum likelihood estimator to compen-sate for data sparseness. By generalizing the Jelinek-Mercer smoothing method, we propose the GJM method, which uses a document-dependent coefficient to control the influence of the maximum likelihood model and the collection model. Analysis shows that Dirichlet method is equivalent to GJM-1, a special case of GJM. Using the num-ber of unique terms in the document rather than the document length, we propose GJM-2 method, an improved version of GJM-1. Experimental results show that GJM-2 method can achieve better retrieval performances than the three existing popular smoothing methods both on short and long queries. 
