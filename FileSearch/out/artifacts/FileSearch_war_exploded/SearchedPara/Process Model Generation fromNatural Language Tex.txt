 Business process management is a discipline which seeks to increase the effi-ciency and effectiveness of companies by holistically analyzing and improving business processes across departmental boundaries. In order to be able to ana-lyze a process, a thorough understanding of it is required first. The necessary level of insight can be obtained by creating a formal model for a given business process.

The required knowledge for construct ing process models has to be made ex-plicit by actors participating in the process [1]. However, these actors are usually not qualified to create form al models themselves [2]. For this reason, modeling experts are employed to iteratively for malize and validate process models in col-laboration with the domain experts. This traditional procedure of extracting process models involves interviews, m eetings, or workshops [3]. It entails con-siderable time and costs due to ambiguities or misunderstandings between the involved participants [4]. Therefore, the initial elicitation of conceptual models is considered to be a knowledge acquisit ion bottleneck [5]. A ccording to Herbst [1] the acquisition of the as-is model in a workflow project requires 60% of the total time spent. Accordingly, substantial savings are possible by providing ap-propriate tool support to speed up the acquisition phase.

In this context, it is a paradox that acquisition is costly although detailed information about processes is often already available in the form of informal textual specifications. Such textual documents can be policies, reports, forms, manuals, content of knowledge management systems, and e-mail messages. Con-tent management professionals estimated that 85% of the information in com-panies is stored in such an unstructured format [6]. Moreover, the amount of unstructured text is growing at a much faster rate than structured data [7]. It seems reasonable to assume that these tex ts are relevant sources of information for the construction of conceptual models.

In this paper, we develop an approach to directly extract business process models from textual descriptions. Our contribution is a corresponding technique that does not make any assumptions about the structure of the provided text. We combine an extensive set of tools from natural language processing (NLP) in an innovative way and augment it with an anaphora resolution mechanism, which was particularly developed for our approach. The evaluation of our technique with a set of 47 text-model pairs from industry and textbooks reveals that on average 77% of the model is correctly gene rated. We furthermore discuss current limitations and directions of improvement.

The paper is structured as follows. Section 2 introduces the foundations of our approach, namely BPMN process models and natural language processing techniques. Section 3 identifies a set of l anguage processing requirements, and illustrates how they are tackled in the various steps of our generation approach. Section 4 presents our evalu ationresultsbasedonasampleoftext-modelpairs. Section 5 discusses related work before Section 6 concludes the paper. Generating models builds on understanding the essential concepts of BPMN process models and of state-of-the-art techniques for natural language processing. In this section, we introduce BPMN and then natural language processing tools.
The Business Process Model and Notat ion (BPMN) is a standard for process modeling that has been recently published in its version 2.0 [8]. It includes four categories of elements, namely Flow Objects (Activities, Events and Gateways), Swimlanes (Pools and Lanes), Artifacts (e.g. Data Objects, Text Annotations or Groups), and Connecting Objects (Se quence Flows, Message Flows and Asso-ciations). The first three are nodes, the latter ones are edges. Figure 1 shows a BPMN example of a claims handling process provided by QUT. The process is subdivided into three pools (one with two lanes) capturing the actors of the pro-cess. Activities are depicted as rounded boxes. Different events (round elements with icons) for sending and receiving mess ages affect the execution of the process. The diamond-shaped elements define sp ecific routing behavior as gateways.
A BPMN process model is typically the res ult of analyzing textual descrip-tions of a process. A claims handling process provided by QUT is described as follows:  X  X he process starts when a cu stomer submits a claim by sending in relevant documentation. The Notification department at the car insurer checks the documents upon completeness and registers the claim. Then, the Handling department picks up the claim and checks the insurance. Then, an assessment is performed. If the assessment is positive, a garage is phoned to authorise the repairs and the payment is scheduled (in this order). Otherwise, the claim is re-jected. In any case (whether the outcome is positive or negative), a letter is sent to the customer and the process is considered to be complete. X  Such information is usually provided by people working in the process and then formalized as a model by system analysts [2].

For our model generation approach, we will employ methods from compu-tational linguistics and natural language processing. This branch of artificial intelligence deals with analyzing and extracting useful information from natural language texts or speech. For our approach, three concepts are of vital impor-tance: syntax parsing, which is the determination of a syntax tree and the gram-matical relations between the parts of th e sentence; semantic analysis, which is the extraction of the meaning of words or phrases; and anaphora resolution, which involves the identification of the co ncepts which are references using pro-nouns ( X  X e X , X  X e X , X  X t X ) and certain articles ( X  X his X ,  X  X hat X ). For syntax parsing and semantic analysis, there are standard tools available.
 The Stanford Parser is a syntax parsing tool for determining a syntax tree. This tree shows the dependencies between the words of the sentence through the tree structure [10]. Additionally, each word and phrase is labeled with an appro-priate part-of-speech and phrase tag. The tags of the Stanford Parser are the same which can be found in the Penn Tree Bank [11]. The Stanford Parser also produces 55 different Stanford Dependencies [12]. These dependencies reflect the grammatical relationships between the words. Such grammatical relations pro-vide an abstraction layer to the pure syntax tree. They also contain information about the syntactic role of all elements.

There are also tools available for semantic analysis. They provide semantic relations on different levels of deta il. We use FrameNet [13] and the lexical database WordNet[14]. WordNet provides various links to synonyms, homonyms, and hypernyms for a particular class of m eaning associated wi th a synonym-set. FrameNet defines semantic relations that are expected for specific words. These relations are useful, e.g., to recognize that a verb  X  X end X  would usually go with a particular object being sent. Syntax parsers and semantic analysis are used in our transformation approach, augmented with anaphora resolution. The most important issue we are facing when trying to build a system for gener-ating models is the complexity of natural language. We collected issues related to the structure of natural language texts from the scientific literature and an-alyzed the test data, which is described in section 4. Thereby, we were able to identify four broad categories of issues which we have to solve in order to ana-lyze natural language process descr iptions successfully (see Table 1). Syntactic Leeway relates to the fact that there is a mismatch between the semantic and syntactic layer of a text. Atomicity deals with the question of how to construct a proper phrase-activity mapping. Relevance has to check whether parts of the text might be irrelevant for the generated process model. Finally, Referencing addresses the question of how to resolve relative references between words and between sentences.

Different solution strategies were applied in the works listed in Table 1 to overcome the stated problems, e.g. by constricting the format of the textual input [17], but no study considers all mentioned problems and offers a comprehensive solution strategy. Another interesting fact is that none of the works using a shallow parser shows how they deal with passive voice [15,22,23,17]. We solved this problem by using the grammatical relations of the Stanford Parser.
To obtain a structured representation of the knowledge we extract from the text, we decided to store it in a World Model , as opposed to a direct straight through model generation. This approach was also taken by most of the other works which built a similar system [17,22,18,23]. The data structure used by the approach of the University of Rio de Janeiro [23] was taken from the CREWS project [15]. The authors argue that it is suited well for this task as a scenario description corresponds to the description of a process model. Therefore, we also use the CREWS scenario metamodel as s tarting point. However, we modified several parts as, e.g., we explicitly represent connections between the elements using the class  X  X low X . Additionally, we explicitly considered traceability as a requirement. Thus, attributes relati ng an object to a sentence or a word are added to the World Model. The four main elements of our World Model are Actor, Resource, Action, and Flow. This World Model will be used throughout all phases of our transformation procedure to capture syntactic and semantic analysis results. Each phase is allowed to access, modify and add data.
The rest of this section is dedicated to analyzing and discussing the issues collected in Table 1. We will then seize the developed suggestions and reference these issues during the description of our transformation approach. Section 3.1 discusses sentence level analysis for finding actions. Section 3.2 investigates text level analysis for enriching the data stored in the world model. Finally, Sec-tion 3.3 describes the generation o f a BPMN model. While we focus on the general procedure here, we documented details of all algorithms in [25]. 3.1 Sentence Level Analysis The first step of our transformation procedure is a sentence level analysis. The extraction procedure consists of the steps that are outlined as a BPMN model in Figure 2. This overview also shows the different components upon which our transformation procedure builds and their usage of Data Sources.

The text is processed in several stages. Fi rst, a tokenization splits up the text into individual sentences. The challenge here is to distinguish a period used for an abbreviation (e.g. M.Sc.) from a period marking the end of a sentence.
Afterwards, each sentence is parsed by the Stanford Parser using the factored model for English [11]. We utilize the factored model and not the pure proba-bilistic context free grammar, because it p rovides better results in determining the dependencies between markers as  X  if X  or  X  X hen X , which are important for the process model generation. Next, comp lex sentences are split into individual phrases. This is accomplished by scanning for sentence tags on the top level of the Parse Tree and within nested prepositional, adverbial, and noun phrases.
Once the sentence is broken down into individual constituent phrases, actions can be extracted. First, w e determine whether the parsedSentence is in active or passive voice by searching for the appropriate grammatical relations (Issue 1.1). Then, all Actors and Actions are extracted by analyzing the grammatical relations. To overcome the problem of exam ple sentences mentioned earlier (Issue 3.2) the actions are also filtered. This filtering method simply checks whether the sentence contains a word of a stop word list called example indicators . Then, we extract all objects from the phrase and ea ch Action is combined with each Object. The same is done with all Actors. This procedure is necessary as an Action is supposed to be atomic according to the BPMN specification [8] and Issue 2.1. Therefore, a new Action has to be created for each piece of information as illustrated in the following example sentences. In each sentence the conjunction relation which causes the extraction of several Actors, Actions or Resources is highlighted. As a last step, all extracte d Actions are added to the World Model.  X   X  X ikewise the old supplier creates and sends the final billing to the cus- X   X  X t is given either by a sales representative or by a pre-sales employee  X   X  X t this point, the Assistant Registry Manager puts the receipt and 3.2 Text Level Analysis This section describes the text level an alysis. It analyzes the sentences taking their relationships into account. The structural overview of this phase is shown in Figure 3. We use the Stanford Parser and WordNet here, and also an anaphora resolution algorithm. During each of the five steps, the Actions previously added to the World Model are augmented with additional information.

An important part of the algorithm presented here is the determination heuris-tic for resolving relative references within the text (Issue 4.1). Existing libraries are not seamlessly integrateable with the output provided by the Stanford Parser. Therefore, we implemented a simple ana phora resolution technique for the reso-lution of determiner and pronouns. This procedure is described in detail in [25]. An experimental evaluation using our test data set showed that this approach achieved a good accuracy of 63.06%.

The second step in our analysis is the det ection of conditional markers. These markers can either be a single word like  X  X f X ,  X  X hen X ,  X  X eanwhile X  or  X  X ther-wise X , or a short phrase like  X  X n the meantime X  or  X  X n parallel X . All of these markers have specific characteristic s and can be mapped to different BPMN constructions. In order to capture this semantic information we compiled four lists, namely ConditionIndicators (exclusive gateway), ParallelIndicators (paral-lel gateway), ExceptionIndicators (for E rror Intermediate Events), and Sequen-ceIndicators (for the continuation of a branch of a gateway). These lists do not claim completeness and can be ext ended by the user, if necessary.

We can use the information gathered so far to combine the information con-tained in two different Actions. This pro cedure tackles the problem of Actions which are split up over several sentences (Issue 2.2). To consider two Actions as a candidate for a merger, a reference ha d to be established between them during the anaphora resolution phase. This reference can either directly point from the Actor or from the Object of this Action. But, for the case that the Object points to another Actor or Resource we also consider the Action which contains it as a possible candidate. Next, it is chec ked whether the objects can be merged by checking various characteristics of both Actions. If the actions truly complement each other, they can be merged and form one single action. When both Actions complement each other except for the ne gation modifier we can still enhance the information content of one action by copying information, as the initiating Actor, the Object, and/or the copula attribute. An example for such a case are these sentences:  X  X f course, asking the custom er whether he is generally interested is also important. X  and  X  X f this is not the case, we leave him alone, [...] X 
For Issue 4.2, we defined three types of textual references: forward, backward, and jump references. In order to identify those links in the text automatically, we start by comparing all actions within our World Model to one another. It is then determined whether the select ed actions can be linked or not. Within this method, we compare the following characteristics of both Actions: Copula Specifier, Negation Status, the initiating Actor (ActorFrom), the Object, the open clausal complement, and the Prepos itional Specifiers, whose head word is  X  X o X  or  X  X bout X . The elements are compared using their root form provided by WordNet. If the elements differ or an element is defined for one Action, but not for the other, the Actions cannot be merged. Otherwise, the Actions are considered equal and a link relationship can be established. Additionally, the type of the link relationship is determined and saved along with the link.
The last step of the text level analysis is the generation of Flows. A flow describes how activities are interacti ng with each other. Therefore, during the process model generation such Flows c an be translated to BPMN connecting objects. When creating the Flows we bu ild upon the assumption that a process is described sequentially and upon the information gathered in the previous steps. The word, which connected the items is important to determine how to proceed and what type of gateway we have to create. So far we support a distinction between  X  X r X ,  X  X nd/or X , and  X  X nd X . Other conjunctions are skipped. 3.3 Process Model Generation In the last phase of our approach the information contained in the World Model is transformed into its BPMN representation. We follow a nine step procedure, as depicted in Figure 3.3. The first 4 steps: creation of nodes, building of Sequence Flows, removal of dummy elements, the finishing of open ends, and the processing of meta activities are used to create an initial and complete model. Optionally, the model can be augmented by creating Black Box Pools and Data Objects. Finally, the model is laid out to achieve a human-readable representation.
The first step required for the model crea tion is the construction of all nodes of the model. After the Flow Object was g enerated, we creat e a Lane Element representing the Actor initiating the Action. If no Lane was determined for an Action, it is added to the last Lane which was created successfully as we assume that the process is described in a seque ntial manner. The second step required during the model creation is the construction of all edges. Due to the defini-tion of Flows within our World Model, this transformation is straight-forward. Whenever a Flow which is not of the type  X  X equence X  is encountered, a Gate-way appropriate for the type of the flow is created. An exception to that is the type  X  X xception X . If the World Model contains a flow of this type, an exception intermediate event is attached to the task which serves as a source and this In-termediate Event is connected to the target instead of the node itself. We then skip dummy actions, which were inserte d between gateways directly following each other.

Step four is concerned with open ends. So far, no Start and End Events were created. This is accomplished in this step. The procedure is also straight forward. We create a preceding Start ev ent to all Tasks which do not have any predecessors (in-flow = 0) and succeeding End Events to all Tasks which do not have any successors (out-flow = 0). Additionally, Gateways whose in-and out-flow is one receive an additional branch ending in an End Event. The last step in the model creation phase handles Meta-Activities (Issue 3.3). We search and remove redundant nodes directly adjacent to Start or End Events. This is required as several texts contain sentences like  X  X ...] the process flow at the customer also ends. X  or  X  X he process of  X  X inning X  a new customer ends here. X  If such sentences are not filtered, we might find tasks labeled  X  X rocess ends X  right in front of an end event or  X  X t art workflow X  following a start event. We remove nodes whose verb is contained in the hypernym tree of  X  X nd X  or  X  X tart X  in WordNet if they are adjacent to a Start or End Event.

The execution of these five steps yields a full BPMN model. As the elements of this model do not contain any position information yet, our generation procedure concludes with an automated layout algorithm. We utilize a simple grid layout approach similar to [26], enhanced with standard layout graph layout algorithms as Sugiyama [27] and the topology-shape-metric approach [28]. For the example text of the claims handling process from Section 2 we generated the model given in Figure 5. The question of how far this r esult can be consid ered to be accurate is discussed in the following section. For the validation of our approach, we co llected a test data set consisting of 47 of those text-model pairs, each including a textual process description and a corresponding BPMN models created b y a human modeler. Different sources from research and practice were incorporated into our test data set: Academic (15 models), Industry (9 models), Textbook (9 models), and Public Sector (14 models), see Table 2. While the academic pairs were provided by our university partners, the industry models are taken from two main sources. First, we gath-ered four models from the websites of t hree BPM tool vendors, namely Active VOS, Oracle, and BizAgi. Four models stem from training material of inubit AG, another one from a German BPM practitioner, and further ones from two BPMN textbooks [29,9]. Finally, we included the definition of switch processes of the Federal Network Agency of Germany in its semi-structured tabular format and the corresponding model.

To avoid unintended effects while parsing, minor corrections were applied to the texts. Some models were translated, some were converted from other modeling languages to BPMN in order to compare them. Table 2 lists character-istics of the texts and models of our data set. The table captures the following data: a unique ID, the number of models (M), the number of sentences (n), the average length of sentences (  X  l), the size of the models in terms of nodes ( | N | ), gateways ( | G | ), and edges ( | E | ). All our material is published in [25].
The evaluation results are based on the similarity (sim) between the manually and automatically created mod els. We employ the metric of Graph Edit Distance . To compute the Graph Edit Distance, the graph representation of the process models is analyzed. The labels, attributes , the structural context, and behavior are compared [32]. Afterwards a greedy graph matching heuristic [33] is employed to create pairs of nodes an d edges. We use the greedy heuristic as it showed the best performance without considerable accuracy trade-offs. After the mapping is created, a Graph Edit Distance value can be calculated given:  X  N  X  E  X  N  X  E  X  M -The mapping between the nodes of model 1 and 2 An indicator for the difference betw een the models can be calculated as: As a last step weights for the importance of the differences ( w map ), the un-mapped Nodes ( w uN ), and the unmapped Edges ( w uE ) have to be defined. For our experiments we gave the difference a s lightly higher importance and assigned w map =0 . 4and w uN = w uE =0 . 3. The overall graph edit distance then becomes: sim( m 1 ,m 2 )=1  X  ( w map  X  This value ranges between 0 and 1. For the case that all nodes could be mapped with a similarity of 1.0 the terms will also become 1.0. If the mapping is not optimal, the term in parenthesis will grow steadily and the similarity decreases. If no nodes are mapped at all, the similarity will be 0.

For our evaluation, we generated the m odel for each text and calculated the similarity metric between it and the original BPMN model. The results are shown in Table 3. Columns 2-4 show that the concepts of meta sentences, relative references, and textual jumps are important for almost all elements within our test data. The following six columns show the average values of nodes, gateways, and edges within the generated models. We can see that the transformation procedure tends to produce models which are on average 9-15% larger in size then what a human would create. This can be partially explained by noise and meta sentences which were not filtered appropriately. On the other hand, humans tend to abstract during the process of m odeling. Therefore, we often find more detail of the text also in the generated model. The results are highly encouraging as our approach is able to correctly recreate 77% of the model in average. On a model level up to 96% of similarity ca n be reached, which means that only minor corrections by a human modeler are required.

During the detailed analysis we determi ned different sources of failure, which resulted in a decreased metric value. These are noise, different levels of abstrac-tions, and processing problems within our system. Noise includes sentences or phrases that are not part of the process description, as for instance  X  X his ob-ject consists of data elements such as the customers name and address and the assigned power gauge. X  While such information can be important for the under-standing of a process, it leads to unwanted Activities within the generated model. To tackle this problem, further filtering mechanisms are required. Low similarity also results from difference in the level of granularity . To solve this problem, we could apply automated abstraction techniques like [34] on the generated model. Finally, the employed natural language processing components failed during the analysis. At stages, the Stanford Parser failed at correctly classifying verbs. For instance, the parser classified  X  X he seco nd activity checks and configures X  as a noun phrase, such that the verbs  X  X heck X  and  X  X onfigure X  cannot be extracted into Actions. Furthermore, important verbs related to business processes are not contained in FrameNet, as  X  X eport  X . Therefore, no message flow is created between report activities and a Black Box Pool. We expect this problem to be solved in the future as the FrameNet database grows. With WordNet, for instance, there is a problem with times like  X 2:00 pm X , where pm as an abbre-viation for  X  X rime Minister X  is classified as an Actor. To solve this problem a reliable sense disambiguation has to be conducted. Nevertheless, overall good results were achieved by using WordNet as a general purpose Ontology. Recently, there is an increasing interes t in the derivation of conceptual models from text. This research is mainly conducted by six different groups. Two approaches generate UML models. The Klagenfurt Conceptual Pre-design Model and a corresponding tool are used to parse German text and fill instances of a generic meta-model [35]. The stored information can be transformed to UML activity diagrams and class diagrams [18]. The transformation from text to the meta-model requires the user to make d ecisions about the relevant parts of a sentence. In contrast to that, the approac h described in [36] is fully automated. It uses use-case descriptions in a format called RUCM to generate activity di-agrams and class diagrams [17]. Yet, the system is not able to parse free-text. The RUCM input is required to be in a restricted format allowing only 26 types of sentence structures, which rely on keywords like  X  X ALIDATES THAT X  or  X  X EANWHILE X . Therefore, it can hardly be used in the initial process defini-tion phase as it would require rewriting of process-relevant documents.
The University of Rio de Janeiro focuses on the derivation of BPMN models from group stories provided in Portuguese [23]. The approach was tested with a course enrollment process modeled by students. The examples in their paper show that process models can be created s uccessfully, but a couple of their ex-hibits show that syntactical problems can occur, e.g. implicit conditions, which we explicitly tackle with our approach. The R-BPD toolkit from the Univer-sity of Wollongong uses a syntax parser to identify verb-object phrases [21]. It The result are rather BPMN snippets than fully connected models. Nevertheless, this toolkit is able to take existing models into account for cross validation.
A fifth approach is the one of Policy-Driven Process Mapping [37]. First, a procedure was developed which creates a BPMN diagram, given that data items, tasks, resources (actors), and constraints are identified in an input text document. Although the approach does not require a process description to be sequential, it does not support Pools, Data Objects, and Gateways other than an exclusive split. Furthermore, user-interact ion is required at several stages. The approach by Sinha et al. builds on a linguistic analysis pipeline [22,38]. First, text is preprocessed with a part-of-speech tagger. Next, words are anno-tated with dictionary concepts, which classify verbs using a domain ontology. Then, an anaphora resolution algorithm and a context annotator are applied. The resulting information is then transferred to a Use Case Description meta-model and later into a BPMN process model. The dictionary concepts, which are a vital part of their approach, rely on a domain ontology which has to be hand-crafted. This imposes a manual effort when transferring the system to other types of texts or languages. Instead, our approach builds on the free WordNet and FrameNet lexical databases, which are available for different languages. In this paper, we presented an automatic approach to generate BPMN mod-els from natural language text. We have combined existing tools from natural language processing in an innovative way and augmented them with a suitable anaphora resolution mechanism. The evaluation of our technique shows that for a set of 47 text-model pairs from industry and textbooks, we are able to generate on average 77% of the models correctly.

Despite these encouraging results, we still require empirical user studies. Such studies should investigate whether humans find the generated models useful and easy to adapt towards a fully accurate model. Furthermore, our system is able to read process descriptions c onsisting of full sentences. Furthermore, we assumed the description to be sequential and to contain no questions and little process-irrelevant information. Another prerequisite is that the text is grammatically correct and constituent. Thus, the parsing of structured input, like tables or texts making use of indentions, or texts which are of low quality is not possible at the moment and presents opportunities for further research.

While the evaluation conducted in this thesis evinced encouraging results different lines of research could be pursued in order to enhance the quality or scope of our process model generation procedure. As shown the occurrence of meta-sentences or noise in general is on e of the severest problems affecting the generation results. Therefore, we coul d improve the quality of our results by adding further rules and heuristics to identify such noise. Another major source of problems was the syntax parser we employed. As an alternative, semantic parsers like [39] could be investigated.

