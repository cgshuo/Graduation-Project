 Most existing works of data anonymisation target at the optimization of the anonymisation metrics to balance the data utility and privacy, whereas they ignore the effects of a requester X  X  trust level and application purposes during the data anonymisation. Our aim of this paper is to propose a much finer level anonymisation scheme with regard to the data requester X  X  trust value and specific application purpose. We prioritize the attributes for anonymisation based on how important and critical they are related to the specified ap-plication purposes and propose a trust evaluation strategy to quantify the data requester X  X  reliability, and further build the projection between the trust value and the degree of data anonymiztion, which intends to determine to what ex-tent the data should be anonymized. The decomposition algorithm is developed to find the desired anonymous solu-tion, which guarantees the uniqueness and correctness. H.2 [ Database Management ]: General, Systems Algorithms, Security Privacy, Anonymization, Trust
Data privacy and identity protection is a very important issue when databases containing huge information need to be stored and distributed for research or other purposes. For example, the National Cancer Institute initiated the Shared Pathology Informatics Network (SPIN) for researchers through-out the country to share pathology-based data sets anno-tated with clinical information to discover and validate new diagnostic tests and therapies, and ultimately to improve patient care. However, individually identifiable health in-formation is protected under the Health Insurance Portabil-ity and Accountability Act (HIPAA). The released data has to be sufficiently anonymized before being shared over the network. We consider two scenarios where two distinct data requesters require the same data set for different application purposes.

Scenario 1: The Research Center from the University requests the census data from US Census Bureau to conduct the demographic analysis in the local area.

Scenario 2: A PhD student from Faculty of Business requires the same census data from US Census Bureau to investigate or predict the potential business opportunities in the local area.

The above two scenarios show that for the same database, they may be used for different application purposes by differ-ent data requesters. On the one hand, considering the diver-sity of purposes, the requirements for individual attributes based on how important they are to request purposes are various. For example, Age and Gender attributes in the cen-sus database are essential for demographic purpose, but they are not necessary for some prediction purposes, so a priority weight associated with each attribute is valuable to indicate the importance of the attribute to request purposes. While, on the other hand, considering the variety of data requesters, the reliability of data requesters to data providers depends on the their trust evaluation. Intuitively, a research center is more reliable than an individual student, since a larger organization is usually more trustworthy than a strange in-dividual for the data provider. The trust between the data requester and data provider reflects the possibility that the data would be misused by the data requester. The more trustworthy the data requesters are, the less chance they will maliciously use the requested data. So, back to our scenario, the research center should receive the anonymized data with less anonymisation than the individual student.
Existing work on data anonymisation focuses on devel-oping effective models and efficient algorithms to optimize the trade-off between data privacy and utility [3, 2, 4, 5]. Normally, the same anonymous data is delivered to differ-ent requesters regardless of what kind of purposes the data used for, letting alone the reliability of the data requester. By specifying the requesters X  application purpose and their reliability, the result of the data anonymisation will achieve a better trade-off. Following this idea, there arises two main challenges:
Challenge 1: Since it is not always possible for data requesters to specify the attribute priorities before hand, how to automatically learn attribute priority from specific application purposes and further quantify to what extent the data should be anonymized when incorporating application purposes?
Challenge 2: Facing with different data requesters, how to accurately evaluate the data requester X  X  reliability and further build the projection between the reliability of the data requester and the desired degree of data anonymisa-tion?
In this paper, we incorporate the application purposes and trust into anonymisation process to maximize data utility for the data requester. A data requester could be an individual with general data exploration purpose or a research institute with sophisticated data mining tasks such as demographic analysis. Upon receiving a user X  X  request, the purpose of the request indicates the priorities of the attributes during the data anonymisation. Our idea is to represent the re-quirements of the application purposes by a list of attributes and weight pairs where each attribute is associated with a priority value based on how important it is to the appli-cation purposes. Next, the trust evaluation mechanism is triggered to evaluate the data requester X  X  reliability, thereby to determine the degree of the anonymized data through the projection function. Finally, by decomposing the degree of anonymisation, the anonymized data is sent back to the data requester.
The attribute priority proposed in [6] has the limitation that it requires users to specify the attribute priorities before hand. In this paper, we derive the priority automatically by investigating the dependency among all attributes. In order to capture the dependency among the attributes, we adopt the concept of entropy from information theory to measure the amount of information among, construct the indepen-dency matrix to quantify the relativity of two attributes are relative, and devise a method to automatically derive the attribute priorities.
For a certain purpose, some attributes can be determined to be useful, some are useless, while others are not sure. Let A ; A 2 ; ; A m be the attribute set, and for the purpose p , without loss of generality, suppose A 1 is most useful at-tribute and A m is the one with least usage (if there are not A 1 and A m , we can always swap the attributes to make the most useful one at the first and most useless one at the last). Then, the attribute priority of each attribute is defined as follows.

Definition 1 (Attribute Priority). Let A 1 ; ; A m be the attributes, and D = ( M I ( A i ; A j ) m  X  m ) be the normal-ized independency matrix among attributes ( 1 i; j m ). For a certain purpose p , a priority P ( A i ; p ) is assigned to each attribute A i for the purpose p ( 1 i m ). Suppose A 1 is most useful attribute and A m is the least one. Then, the priority P ( A i ; p ) ( P ( A i ) ) is de ned by the following re-cursive function:
P ( A i ) = M I ( A i ; A i +1 ) P ( A i  X  1 ) + M I ( A i with the initial condition P ( A 1 ) = 1 , P ( A m ) = 0 . Theorem 1. 0 P ( A i ) 1 , 8 1 i m .

Theorem 1 confirms the correctness of the defined at-tribute priority. As we assign the priority 1 to the most useful attribute and 0 to the least useful one, Theorem 1 guarantees that all the priorities calculated by the equation fall into the range [0,1]. With the aid of attribute priority, in the next section, we discuss how to define the degree of data anonymisation.

In some cases, attributes should be generalized only up to a certain degree or not transformed at all. Otherwise, their values become useless for an application purpose. The way we define the degree of data anonymization is different from [1], we incorporate attribute priority into the definition of data anonymization, which highlights our problem.
Let A h be the height of a domain hierarchy for attribute A , and let levels A 1 ; A 2 ; ; A h be the domain levels of A from the most general to most specific. Let the weight function w j;j  X  1 between domain level A j  X  1 and A j be pre-defined, where 2 j h . When a value is anonymized from level A p to level A q in its value generalization hierarchy ( p q ), the degree of attribute anonymisation of A is defined as:
Trust means the reliability and trustworthiness of a trusted agent X  X  behavior, which represents the data provider X  X  esti-mate of how likely the data requester is to fulfill its com-mitments. There are two main categories of trust: history-based and recommendation based. In the former, the data provider accesses trust based solely on their own experience; in the latter, trust is based on information provided by oth-ers. In this paper, we combine this two types of trust, and evaluate the trust value in three steps: (1) Calculate the trust value based on histories; (2) Calculate the trust value from commentators; (3) Combine the observed trust values from histories and recommendations.

For the data providers, it is important to determine to what extent the user can access the data if given the trust of data requesters, usually, the more trustworthy the data requester is, the less anonymous the released data should be.
Suppose that the evaluated value of trust is T , then the necessary degree of data anonymisation D through the trust projection is defined as: The trust projection is a mapping between trust and degree of data anonymisation, which reflects the fact that the higher the trust is, the less the degree of anonymisation would be, but the gradient of the trust projection is decreasing, which indicates that the degree of data anonymisation is more sen-sitive to the higher value of trust.
In this section, we answer the question of how to anonymize the data set with the specific degree of data anonymisation? by developing a novel decomposition method, which pro-vides the unique and correct anonymous solution.

Generally, if we are given the original data set and its anonymous version, it is easy to calculate the degree of anonymisation of the anonymized data set. However, it is not that easy to get the anonymous version the origi-nal data set only with the information of anonymisation degree. The naive way to solve this problem is to enu-merate all the possible anonymous views, calculate the de-gree of anonymisaztion of each view, and then find the one that matching the given anonymisation degree, however, this enumeration-based method suffers from two main problems. First of all, if there are n attributes A 1 ; A 2 ; A n in the data set and the height of the generalization hierarchy of A i is m i (1 i n ), then the number of all the possible anonymous solution is if the number of attribute becomes large. Second of all, for the different anonymous data sets, they may have the same degree of data anonymisation, and in this case, it is difficult to determine which is the desired view to deliver to the data requester. In this paper, we propose an effective decomposition approach to deal with these problems. Our approach consists of two steps. The first step is to decom-pose the degree of anonymisation of the whole data set to each attribute level, and the second step is to determine the anonymous view of each attribute.

Step 1: Let the original data set be T , which has n attributes A 1 ; A 2 ; A n , and each attribute A i associated with the priority P i (1 i n ). If the degree of anonymisa-tion of T is Deg ( T ), then we defined the degree of anonymi-sation for the attribute A i as: The degree of the data anonymisation at the attribute level is proportional to the priorities of the attributes.
Step 2: We consider how to generate the anonymous view of the attribute A i with the degree of attribute anonymisa-tion Deg ( A i ) (1 i n ).

Let A h be the height of a domain hierarchy for the at-tribute A , and let levels A 1 ; A 2 ; ; A h be the domain levels of A from the most general to most specific. When a value is anonymized from level A p to level A q in its value general-ization hierarchy ( p q ), the degree of attribute anonymi-sation of A is defined in the Definition 2.1. Since there are h levels in the domain hierarchy, the degree of anonymisation has been divided into h 1 intervals, which are [ i h  X  1 ; where 0 i h 2. For the decomposed degree of at-tribute anonymisation Deg ( A i ), it must fall into one of h 1 intervals. Without loss of generality, we assume that the value Deg ( A i ) is within the p -th interval [ p  X  1 h  X  1 p h 1, then for the attribute A i , we generalize it to the p -th in its generalization hierarchy. We do the same thing for all the attributes, and at last, we could get the anonymous data set with the degree of data anonymisation Deg ( T ).

We use Figure 1 to illustrate the correctness of the decom-position schema. We are given a data set T with the specified degree of data anonymisation deg , through our decomposi-tion schema, we could obtain the unique anonymous view T  X  of T . We can then compute the anonymisation degree Deg ( T  X  ) of T  X  . Since the value of Deg ( T  X  ) is not necessarily the same as deg , we recalculate the anonymous data set T with the anonymous degree Deg ( T  X  ), in order to make sure that our decomposition schema is correct, the anonymous view T  X  and T  X  X  should be the same.
 Figure 1: Correctness of the anonymisation degree decom-position
We presented a novel data anonymisation approach, which takes into account the reliability of data requesters and the relative attribute importance for the application purpose. We quantified the level of anonymisation through the con-cept of the degree of data anonymisation, and derived a de-composition algorithm for data anonymization. Our exper-imental results show that our data anonymisation method achieves better data utility than general approaches with regard to the trust and application purposes.
Thanks for the reviewers X  valuable comments. The work was partially done when the first author was with SAP Re-search Brisbane supported by EII-SAP PhD Research Visit Program. This research is also supported by ARC grant DP0774450 and DP0663414. [1] J. Li, R. Wong, A. Fu, and J. Pei. Anonymization by [2] A. Machanavajjhala, J. Gehrke, D. Kifer, and [3] L. Sweeney. k -anonymity: A model for protecting [4] R. Wong, J. Li, A. Fu, and K. Wang. ( ; k )-anonymity: [5] X. Xiao and Y. Tao. Anatomy: Simple and effective [6] L. Xiong and K. Rangachari. Towards
