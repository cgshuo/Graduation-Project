 A  X  Abstract Formulating Question Answering Validation as a classification problem facilitates the introduction of Machine Learning techniques to improve the overall performance of Question Answering systems. The different proportion of positive and negative examples in the evaluation collections has led to the use of measures based on precision and recall . However, an evaluation based on the analysis of Receiver Operating Characteristic (ROC) space is sometimes preferred in classifi-cation with unbalanced collections. In this article we compare both evaluation approaches according to their rationale, their stability, their discrimination power and their adequacy to the particularities of the Answer Validation task.
 Keywords Question Answering Answer Validation Evaluation 1 Introduction Question Answering (QA) systems receive a question in natural language and return small snippets of text that contain an answer to the question (Voorhees and Tice 1999 ) Traditional QA systems typically employ a pipeline approach (Moldovan et al. 2000 ), which produces a dependency among modules that is highly sensitive to error propagation.

Introducing more reasoning about the correctness of the returned answers could contribute to overcome the pipeline limitations of QA systems and improve QA results. The task of Answer Validation (AV) contributes to this type of improvement (Harabagiu and Hickl 2006 ). An AV system receives a Question and an Answer and returns a value indicating if the Answer is correct or not.

The decision about the correctness of answers is a binary classification task where answers must be classified as correct or incorrect. This binary classification task has the confusion matrix shown in Table 1 (formulae in the rest of the paper will be given according to this matrix), where each answer can be correct or incorrect (the true class) and an AV system can classify each answer as correct or incorrect (the hypothesized class). When an AV system classifies an answer as system classifies the answer as incorrect, it is said that the system rejects the answer.
The primary method for evaluating binary classification in Machine Learning is accuracy (the proportion of correct classifications). However, accuracy assumes that the class priors in the target environment will be constant and relatively balanced, what does not happen always in AV. The evaluation of AV must take into account this fact and consider the unbalanced nature of testing collections (Pen  X  as et al. 2008 ).
There are mainly two ways of performing evaluation in such cases: by means of precision , recall and its harmonic mean ( F-measure ); and another one based on the analysis of Receiver Operating Characteristic (ROC) space and the measures obtained from that analysis. Precision and recall are combined into a single value called F-measure , whereas the analysis of ROC space can be summarized into a single scalar value called Area Under the Curve ( AUC ). This work aims at determining which measure ( AUC or F-measure ) is more appropriate to evaluate AV systems analyzing the features targeted by each metric.

The rest of this paper is structured as follows. In Sect. 2 we explain the basic ideas of AUC metric, while the analysis based on Recall -Precision and F-measure is exposed in Sect. 3 . In Sect. 4 we compare these measures according to their reliability and their adequacy to AV. Finally, the conclusions are given in Sect. 5 . 2 Evaluation based on ROC analysis Receiver Operating Characteristic (ROC) analysis is a methodology from signal detection theory used also in Artificial Intelligence classification work (Friedman and Wyatt 1997 ). For binary classification problems, ROC space is a 2-dimensional plot with true positive rate ( tp rate ) (Formula 1 ) on the Y -axis and false positive rate ( fp rate ) (Formula 2 ) on the X -axis. Thus, a single confusion matrix produces a single point in ROC space.
In addition to this graphical representation, ROC performance can be reduced into a single scalar value, using the concept of ROC curve. A ROC curve of a classifier is formed from a sequence of ROC points of the classifier, including (0,0) and (1,1), connected by line segments. ROC curves are insensitive to changes in class distribution (a problem in the evaluation of AV systems). In the absence of any method to generate a sequence of ROC points, a single classifier can form the basis of a ROC curve by connecting its ROC point to points (0,0) and (1,1) (Drummond and Holte 2004 ). This is the method we use for building ROC curves with the purpose of evaluating AV systems.

Given a ROC curve, a way for giving a scalar value representing performance is to calculate the Area Under the ROC Curve (AUC) (Bradley 1997 ). Since AUC is a portion of the area of the unit square, its value will always range between 0 and 1. 3 Analysis based on precision and recall According to the definition of AV, an answer should be validated if there is enough evidence to affirm its correctness. Under this perspective, AV is the task of detecting correct answers and ensures that only correct answers will be validated. This is why we study the use of precision and recall measures over the correct answers.
Precision is the proportion of answers validated by the system that are actually correct (Formula 3 ), while recall is the proportion of correct answers detected by the system (Formula 4 ). Notice that recall and tp rate are different names for the same concept. The most used single measure that trades off precision versus recall is F-measure (Formula 5 ), which is the harmonic mean of precision and recall . 4 Precision-recall vs. ROC comparison A criticism that receives precision , recall and F-measure is that, as the fundamental class distribution changes, they will change as well, even if the fundamental system performance does not. This is one of the arguments for the use of ROC analysis for performing evaluation in classification instead of precision and recall when the collections are skewed (Provost and Fawcett 2001 ), which is common in AV (Pen  X  as et al. 2008 ). In this section we intend to compare both measures regarding particularities of the AV task.

Notice that recall and tp rate are different names for the same concept. Thus, the main difference between the two approaches is the use of precision or fp rate (besides of the way in which the final value is calculated: harmonic mean for F-measure and area for AUC ).
 For analyzing reliability, we have chosen the method described in Buckley and Voorhees ( 2000 ) for assessing stability and discrimination power. This method has been used for studying IR metrics (showing similar results with the methods based on statistics (Sakai 2006 )), as well as for evaluating the reliability of QA measures (Voorhees 2002 ). The results of reliability studies do not mean that a measure should be preferable to another. They offer information about how careful researchers must be when making conclusions comparing systems with different measures.

In order to compare the two evaluation measures, we took the data freely available from the task Answer Validation Exercise 1 (AVE) 2008 (Rodrigo et al. 2009 ) at CLEF. AVE collections were created from the real output of QA systems participating at the QA task of CLEF and they are focused on the evaluation of AV systems. These evaluation collections contain a set of pairs { Answer, Supporting Text } that are grouped by Question . Participant systems at AVE must consider each Question and classify each of its { Answer, Supporting Text } pairs as correct or incorrect.

We decided to perform our study in English because it was the language with the higher number of participating runs and the second biggest collection. 2 Given that the methods employed in our study of reliability take advantage of performing a large number of comparisons among different systems, a higher number of systems allows us to perform more comparisons and to be more confident in the results. 4.1 Discrimination power and stability In order to study and compare the reliability of F-measure and AUC , we use the method described by Buckley and Voorhees ( 2000 ) for studying stability . The more stable a measure is, the lower the probability of errors associated with the conclusion  X  X  X ystem A is better than system B X  X  is. It is important to note that results of stability do not mean that measures with a high error should not be used. These results must be used for being more careful in taking decisions when comparing systems using a measure with high error, performing more experiments than in the case of using a measure with lower error.

Moreover, this method also allows to study the number of times systems are deemed to be equivalent with respect to a certain measure, which reflects the discrimination power of the measure. The less discriminative the measure is, the more ties between systems there will be and larger difference in scores will be needed for concluding which system is better.

The method works as follows: let S denote a set of runs. Let x and y denote a pair of runs from S . Let Q denote the entire evaluation collection. Let f represent the fuzziness value, which is the percentage difference between scores such that if the difference is smaller than f , the two scores are deemed to be equivalent. We apply the algorithm in Fig. 1 to obtain the information needed for computing error rate (Formula 6 ) and proportion of ties (Formula 7 ). Error rate is used to measure stability (the lower the error rate is, the more stable the measure is), while the proportion of ties is used for measuring discrimination power (the lower the proportion of ties, the more discriminative the measure).

The intuition of calculating the error rate is as follows: We assume that for each measure the correct decision about whether run x is better than run y happens when there are more cases where the value of x is better than the value of y . The number of times y is better than x is considered as the number of times the test is misleading, while the number of times the values of x and y are equivalent is considered the number of ties.

On the other hand, it is clear that larger fuzziness values reduce the error rate but also reduce the discrimination power of a measure. Since a fixed fuzziness value might imply different trade-offs for different metrics, we decided to vary the fuzziness value from 0.01 to 0.10 (following Sakai ( 2007 )) and to draw for each measure a proportion-of-ties / error-rate curve.

Figure 2 shows the proportion-of-ties / error-rate curves for F-measure and AUC calculated using the participant runs and the test collection of English in AVE 2008 with c = 500 answers. 3 In Fig. 2 we can see how there is a consistent decrease of the error rate of both measures when the proportion of ties increases (this corresponds to increments of the fuzziness value). Furthermore, both measures present an error rate lower than 5%, what means that they are very stable measures (5% is one of the most commonly used confidence value for checking reliability of systems using stability (Sakai 2007 ) or statistical significance (Keen 1992 ).

We can obtain two different interpretations of the curves shown in Fig. 2 : 1. If we fixed the proportion of ties, the error rate for F-measure is lower than the 2. If we fixed the error rate, the proportion of ties we have for AUC is higher than
Therefore, F-measure has shown to be more stable and to have a higher discrimination power than AUC . These results do not mean that F-measure is preferable to AUC , but F-measure is more reliable than AUC . That is, researchers using AUC must be more careful when making conclusions than when using F-measure . 4.2 Adequacy to the evaluation problem The most important aspects when using different evaluation measures (more than reliability) is what these metrics are measuring and for which scenario is more suitable to use each one of them. We compare in this section AUC and F-measure according to their adequacy to the evaluation of AV systems.

The objective of an AV system is to improve the results of the QA system that is using it. For improving results, the AV system must collaborate in increasing the number of correct answers produced by the QA system, as well as reducing the number of incorrect answers. Therefore, in AV we are interested in evaluating the increase of correct answers at the same time that incorrect ones are reduced.
When an AV system receives an answer, there are four possible behaviors depending on whether the AV system accepts or rejects it and whether the answer is objective of improving QA results, and they can be ranked from the one that contributes in the highest proportion, to the one that contributes less. The preference order is: 1. Validate correct answers : this is the output with more value since it contributes 2. Reject incorrect answers : this is also a desired behavior. However, since the 3. Reject correct answers : although this is an incorrect behavior, the AV system is 4. Validate incorrect answers : this is the possibility that most contributes to get
We have studied how F-measure and AUC work assuming this order of preferences. We find differences regarding the control of the incorrect validation of answers due to the differences between precision and fp rate . A low precision means that a high amount of incorrect answers were validated incorrectly. Therefore, precision is acknowledging systems that validate a low amount of incorrect answers, while it penalizes systems that validate high amounts of incorrect answers (the worst behavior).

However, this behavior is not so well controlled by fp rate . A low fp rate does not mean to have validated a low amount of incorrect answers, but to have validated a low proportion of incorrect ones with respect to the total number of incorrect answers in the collection. We can understand better this idea following the example shown in Table 2 , which shows the confusion matrix of a participant system in AVE 2008.

According to the confusion matrix shown in Table 2 , the AV system is validating incorrectly 129 answers obtaining an fp rate of 0.14 (when 0 is the best possible value for fp rate and 1 the worst one). However, 129 answers represent about twice as many answers as those the AV system is correctly validating (68 answers), which is reflected by precision with a value of 0.35 (the best possible value for precision is 1 and the worst one is 0). Therefore, when a QA that uses for validation purposes the AV system presented in Table 2 returns an answer, there are more chances of having an incorrect answer than having a correct one. This is because, according to the results, only 35% of the answers that are validated by the AV system are actually correct. That is something we do not desire. However, fp rate is giving to the AV system a good value (0.14 when the best one is 0), while precision gives a bad value (0.35 when the best possible one is 1). Then, precision is warning us better than fp rate of the results we could achieve in QA. 4
These results reflects that F-measure is more suitable than AUC when evaluating the contribution of an AV system to the improvement of QA results. AUC would be more useful than F-measure when the purpose of the evaluation is to acknowledge the performance detecting correct and incorrect answers over unbalanced collections. 5 Conclusions In this work we have compared two different ways for evaluating Question Answering Validation (AV) as a classification problem: one based on the analysis of Receiver Operating Characteristic (ROC) space summarized in AUC measure; and another one based on precision and recall combined in F-measure . The comparison has been performed between AUC and F-measure comparing their rationale, their stability and discrimination power and their adequacy to the particularities of the AV task.

F-measure has shown to be more stable than AUC , as well as F-measure has more discrimination power than AUC . These results do not mean that AUC should be discarded for evaluating AV systems, but that researchers can be more confident in the results obtained using F-measure than in the ones obtained using AUC .
Furthermore, an evaluation based on F-measure seems to be more useful for the purposes of AV (to improve the results of Question Answering systems) than the one based on AUC. This is because the use of precision in F-measure contributes to control the incorrect validation of answers (the worst output we can obtain from an AV system). AUC is more useful when we want to acknowledge in the same way the correct classification of both correct and incorrect answers because AUC is less sensitive to changes in the class distribution of collections than F-measure . References
