 Document summaries should use a minimum number of words to express a document X  X  main ideas. As such, high quality summaries can sig-nificantly reduce the information overload many professionals in a variety of fields must contend with on a daily basis (Filippova et al., 2009), as-sist in the automated classification and filtering of documents, and increase search engines precision.
Automated summarization methods can use different levels of linguistic analysis: morphological, syntactic, semantic and dis-course/pragmatic (Mani, 2001). Although the summary quality is expected to improve when a summarization technique includes language specific knowledge, the inclusion of that knowl-edge impedes the use of the summarizer on multiple languages. Only systems that perform equally well on different languages without language-specific knowledge (including linguistic analysis) can be considered language-independent summarizers.

The publication of information on the Internet tates the importance of developing multilingual summarization approaches. There is a particu-lar need for language-independent statistical tech-niques that can be readily applied to text in any language without depending on language-specific linguistic tools. In the absence of such techniques, the only alternative to language-independent sum-marization would be the labor-intensive transla-tion of the entire document into a common lan-guage.

Here we introduce MUSE (MUltilingual Sen-tence Extractor), a new approach to multilingual single-document extractive summarization where summarization is considered as an optimization or a search problem. We use a Genetic Algorithm (GA) to find an optimal weighted linear combina-tion of 31 statistical sentence scoring methods that are all language-independent and are based on ei-ther a vector or a graph representation of a docu-ment, where both representations are based on a word segmentation.

We have evaluated our approach on two mono-lingual corpora of English and Hebrew documents and, additionally, on one bilingual corpora com-prising English and Hebrew documents. Our eval-uation experiments sought to -Compare the GA-based approach for single-document extractive summarization (MUSE) to the best known sentence scoring methods. -Determine whether the same weighting model is applicable across two different languages.

This paper is organized as follows. The next section describes the related work in statistical extractive summarization. Section 3 introduces MUSE, the GA-based approach to multilingual single-document extractive summarization. Sec-tion 4 presents our experimental results on mono-lingual and bilingual corpora. Our conclusions and suggestions for future work comprise the fi-nal section. Extractive summarization is aimed at the selec-tion of a subset of the most relevant fragments from a source text into the summary. The frag-ments can be paragraphs (Salton et al., 1997), sen-tences (Luhn, 1958), keyphrases (Turney, 2000) or keywords (Litvak and Last, 2008). Statisti-cal methods for calculating the relevance score of each fragment can be categorized into sev-eral classes: cue -based (Edmundson, 1969), key-word -or frequency -based (Luhn, 1958; Edmund-son, 1969; Neto et al., 2000; Steinberger and Jezek, 2004; Kallel et al., 2004; Vanderwende et al., 2007), title -based (Edmundson, 1969; Teufel and Moens, 1997), position -based (Baxendale, 1958; Edmundson, 1969; Lin and Hovy, 1997; Satoshi et al., 2001) and length -based (Satoshi et al., 2001).

Considered the first work on sentence scoring for automated text summarization, Luhn (1958) based the significance factor of a sentence on the frequency and the relative positions of signifi-cant words within a sentence. Edmundson (1969) tested different linear combinations of four sen-tence ranking scoring methods X  cue , key , title and position  X  X o identify that which performed best on a training corpus. Linear combinations of sev-eral statistical sentence ranking methods were also applied in the MEAD (Radev et al., 2001) and SUMMA (Saggion et al., 2003) approaches, both of which use the vector space model for text repre-sentation and a set of predefined or user-specified weights for a combination of position , frequency , title , and centroid -based (MEAD) features. Gold-stein et al. (1999) integrated linguistic and statisti-cal features. In none of these works, however, did the researchers attempt to find the optimal weights for the best linear combination.

Information retrieval and machine learning techniques were integrated to determine sentence importance (Kupiec et al., 1995; Wong et al., 2008). Gong and Liu (2001) and Steinberger and Jezek (2004) used singular value decomposition (SVD) to generate extracts. Ishikawa et al. (2002) combined conventional sentence extraction and a trainable classifier based on support vector ma-chines.

Some authors reduced the summarization pro-cess to an optimization or a search problem. Has-sel and Sjobergh (2006) used a standard hill-climbing algorithm to build summaries that max-imize the score for the total impact of the sum-mary. A summary consists of first sentences from the document was used as a starting point for the search, and all neighbours (summaries that can be created by simply removing one sentence and adding another) were examined, looking for a bet-ter summary.

Kallel et al. (2004) and Liu et al. (2006b) used genetic algorithms (GAs), which are known as prominent search and optimization meth-ods (Goldberg, 1989), to find sets of sentences that maximize summary quality metrics, starting from a random selection of sentences as the initial pop-ulation. In this capacity, however, the high com-putational complexity of GAs is a disadvantage. To choose the best summary, multiple candidates should be generated and evaluated for each docu-ment (or document cluster).

Following a different approach, Turney (2000) used a GA to learn an optimized set of parame-ters for a keyword extractor embedded in the Ex-tractor tool. 3 Or  X  asan et al. (2000) enhanced the preference-based anaphora resolution algorithms by using a GA to find an optimal set of values for the outcomes of 14 indicators and apply the opti-mal combination of values from data on one text to a different text. With such approach, training may be the only time-consuming phase in the op-eration.
Today, graph-based text representations are be-coming increasingly popular, due to their abil-ity to enrich the document model with syntactic and semantic relations. Salton et al. (1997) were among the first to make an attempt at using graph-based ranking methods in single document ex-tractive summarization, generating similarity links between document paragraphs and using degree scores in order to extract the important paragraphs from the text. Erkan and Radev (2004) and Mi-halcea (2005) introduced algorithms for unsuper-vised extractive summarization that rely on the application of iterative graph-based ranking algo-rithms, such as PageRank (Brin and Page, 1998) and HITS (Kleinberg, 1999). Their methods rep-resent a document as a graph of sentences inter-connected by similarity relations. Various sim-ilarity functions can be applied: cosine similar-ity as in (Erkan and Radev, 2004), simple over-lap as in (Mihalcea, 2005), or other functions. Edges representing the similarity relations can be weighted (Mihalcea, 2005) or unweighted (Erkan and Radev, 2004): two sentences are connected if their similarity is above some predefined threshold value. In this paper we propose a learning approach to language-independent extractive summariza-tion where the best set of weights for a linear com-bination of sentence scoring methods is found by a genetic algorithm trained on a collection of doc-ument summaries. The weighting vector thus ob-tained is used for sentence scoring in future sum-marizations. Since most sentence scoring methods have a linear computational complexity, only the training phase of our approach is time-consuming. 3.1 Sentence scoring methods Our work is aimed at identifying the best linear combination of the 31 sentence scoring methods listed in Table 1. Each method description in-cludes a reference to the original work where the method was proposed for extractive summariza-tion. Methods proposed in this paper are denoted by new . Formulas incorporate the following nota-tion: a sentence is denoted by S , a text document by D , the total number of words in S by N , the to-tal number of sentences in D by n , the sequential number of S in D by i , and the in-document term frequency of the term t by tf ( t ) . In the LUHN method, W and the total number of words in the i th cluster, re-spectively, such that clusters are portions of a sen-tence bracketed by keywords, i.e., frequent, non-
Figure 1 demonstrates the taxonomy of the methods listed in Table 1. Methods that require pre-defined threshold values are marked with a cross and listed in Table 2 together with the aver-age threshold values obtained after method eval-uation on English and Hebrew corpora. Each method was evaluated on both corpora, with dif-ferent threshold t  X  [0 , 1] (only numbers with one decimal digit were considered). Threshold val-ues resulted in the best ROUGE-1 scores, were selected. A threshold of 1 means that all terms are considered, while a value of 0 means that only terms with the highest rank ( tf, degree, or pagerank ) are considered. The methods are di-vided into three main categories X  structure -, vec-tor -, and graph -based X  X ccording to the text rep-resentation model, and each category is divided into sub-categories.

Section 3.3 describes our application of a GA to the summarization task.
 Table 2: Selected thresholds for threshold-based scoring methods 3.2 Text representation models The vector-based scoring methods listed in Ta-ble 1 use tf or tf-idf term weights to evaluate sentence importance. In contrast, representation used by the graph-based methods (except for Tex-tRank) is based on the word-based graph represen-tation models described in (Schenker et al., 2004). Schenker et al. (2005) showed that such graph representations can outperform the vector space model on several document categorization tasks. In the graph representation used by us in this work nodes represent unique terms (distinct words) and edges represent order-relationships between two terms. There is a directed edge from A to B if an A term immediately precedes the B term in any sen-tence of the document. We label each edge with the IDs of sentences that contain both words in the specified order. 3.3 Optimization X  X earning the best linear We found the best linear combination of the meth-ods listed in Table 1 using a Genetic Algorithm (GA). GAs are categorized as global search heuris-tics. Figure 2 shows a simplified GA flowchart. A typical genetic algorithm requires (1) a genetic representation of the solution domain, and (2) a fitness function to evaluate the solution domain.
We represent the solution as a vector of weights Figure 2: Simplified flowchart of a Genetic Algo-rithm for a linear combination of sentence scoring methods X  X eal-valued numbers in the unlimited range normalized in such a way that they sum up to 1 . The vector size is fixed and it equals to the number of methods used in the combination.

Defined over the genetic representation, the fit-ness function measures the quality of the repre-sented solution. We use ROUGE-1 Recall (Lin and Hovy, 2003) as a fitness function for mea-suring summarization quality, which is maximized during the optimization procedure.

Below we describe each phase of the optimiza-tion procedure in detail.

Initialization GA will explore only a small part of the search space, if the population is too small, whereas it slows down if there are too many solu-tions. We start from N = 500 randomly gener-ated genes/solutions as an initial population, that empirically was proven as a good choice. Each gene is represented by a weighting vector v w 1 , . . . , w D ments. All elements are generated from a standard normal distribution, with = 0 and  X  2 = 1 , and normalized to sum up to 1 . For this solution rep-resentation, a negative weight, if it occurs, can be considered as a  X  X enalty X  for the associated met-ric.

Selection During each successive generation, a proportion of the existing population is selected to breed a new generation. We use a truncation se-lection method that rates the fitness of each so-lution and selects the best fifth ( 100 out of 500 ) of the individual solutions, i.e., getting the maxi-mal ROUGE value. In such manner, we discard  X  X ad X  solutions and prevent them from reproduc-tion. Also, we use elitism  X  X ethod that prevents losing the best found solution in the population by copying it to the next generation.

Reproduction In this stage, new genes/solutions are introduced into the popu-lation, i.e., new points in the search space are explored. These new solutions are generated from those selected through the following genetic operators: mating, crossover , and mutation .
In mating , a pair of  X  X arent X  solutions is ran-domly selected, and a new solution is created us-ing crossover and mutation , that are the most im-portant part of a genetic algorithm. The GA per-formance is influenced mainly by these two opera-tors. New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size N is generated.
Crossover is performed under the assumption that new solutions can be improved by re-using the good parts of old solutions. However it is good to keep some part of population from one generation to the next. Our crossover operator in-cludes a probability ( 80 %) that a new and different offspring solution will be generated by calculat-ing the weighted average of two  X  X arent X  vectors according to (Vignaux and Michalewicz, 1991). Formally, a new vector v will be created from two vectors v v =  X   X  v 1 + (1  X   X  )  X  v 2 (we set  X  = 0 . 5 ). There is a probability of 20 % that the offspring will be a duplicate of one of its parents.

Mutation in GAs functions both to preserve the existing diversity and to introduce new variation. It is aimed at preventing GA from falling into lo-cal extreme, but it should not be applied too often, because then GA will in fact change to random search. Our mutation operator includes a proba-bility ( 3 %) that an arbitrary weight in a vector will be changed by a uniformly randomized factor in the range of [  X  0 . 3 , 0 . 3] from its original value.
Termination The generational process is re-peated until a termination condition X  X  plateau of solution/combination fitness such that successive iterations no longer produce better results X  X as been reached. The minimal improvement in our experiments was set to  X  = 1 . 0 E  X  21 . 4.1 Overview The MUSE summarization approach was eval-uated using a comparative experiment on two monolingual corpora of English and Hebrew texts and on a bilingual corpus of texts in both lan-guages. We intentionally chose English and He-brew, which belong to distinct language families (Indo-European and Semitic languages, respect-fully), to ensure that the results of our evaluation would be widely generalizable. The specific goals of the experiment are to: -Evaluate the optimal sentence scoring models in-duced from the corpora of summarized documents in two different languages. -Compare the performance of the GA-based mul-tilingual summarization method proposed in this work to the state-of-the-art approaches. -Compare method performance on both lan-guages. -Determine whether the same sentence scoring model can be efficiently used for extractive sum-marization across two different languages. 4.2 Text preprocessing Crucial to extractive summarization, proper sen-tence segmentation contributes to the quality of summarization results. For English sentences, we used the sentence splitter provided with the MEAD summarizer (Radev et al., 2001). A sim-ple splitter that can split the text at periods, excla-mation points, or question marks was used for the 4.3 Experiment design The English text material we used in our experi-ments comprised the corpus of summarized doc-uments available to the single document summa-rization task at the Document Understanding Con-ference, 2002 (DUC, 2002). This benchmark dataset contains 533 news articles, each accompa-nied by two to three human-generated abstracts of approximately 100 words each.

For the Hebrew language, however, to the best of our knowledge, no summarization benchmarks exist. To generate a corpus of summarized Hebrew texts, therefore, we set up an experiment where human assessors were given 50 news articles of 250 to 830 words each from the Website of the Haaretz newspaper. 8 All assessors were provided with the Tool Assisting Human Assessors (TAHA) selected and stored for later inclusion in the doc-ument extract. In total, 70 undergraduate students from the Department of Information Systems En-gineering, Ben Gurion University of the Negev participated in the experiment. Each student par-ticipant was randomly assigned ten different doc-uments and instructed to (1) spend at least five minutes on each document, (2) ignore dialogs and quotations, (3) read the whole document before beginning sentence extraction, (4) ignore redun-dant, repetitive, and overly detailed information, and (5) remain within the minimal and maximal summary length constraints (95 and 100 words, re-spectively). Summaries were assessed for quality by comparing each student X  X  summary to those of all the other students using the ROUGE evalua-1 metric (Lin and Hovy, 2003). We filtered all the summaries produced by assessors that received av-erage ROUGE score below 0 . 5 , i. e. agreed with the rest of assessors in less than 50 % of cases. Finally, our corpus of summarized Hebrew texts was compiled from the summaries of about 60 % of the most consistent assessors, with an aver-ROUGE scores of the selected assessors are dis-tributed between 50 and 57 percents.

The third, bilingual, experimental corpus was assembled from documents in both languages. 4.4 Experimental Results We evaluated English and Hebrew summaries us-ing ROUGE-1 , 2 , 3 , 4 , L, SU and W metrics, de-scribed in (2004). In agreement with Lin X  X  (2004) conclusion, our results for the different metrics were not statistically distinguishable. However, ROUGE-1 showed the largest variation across the methods. In the following comparisons, all results are presented in terms of the ROUGE-1 Recall metric.

We estimated the ROUGE metric using 10 -fold cross validation. The results of training and testing comprise the average ROUGE values obtained for English, Hebrew, and bilingual corpora (Table 3). Since we experimented with a different number of English and Hebrew documents ( 533 and 50 , re-spectively), we have created 10 balanced bilingual corpora, each with the same number of English and Hebrew documents, by combining approxi-mately 50 randomly selected English documents with all 50 Hebrew documents. Each corpus was then subjected to 10 -fold cross validation, and the average results for training and testing were calcu-lated.

We compared our approach (1) with a multilingual version of TextRank (denoted by ML TR) (Mihalcea, 2005) as the best known multilingual summarizer, (2) with Microsoft MS SUM) as a widely used commercial summa-rizer, and (3) with the best single scoring method in each corpus. As a baseline, we compiled sum-maries created from the initial sentences (denoted by POS F). Table 4 shows the comparative re-sults (ROUGE mean values) for English, Hebrew, and bilingual corpora, with the best summarizers on top. Pairwise comparisons between summa-rizers indicated that all methods (except POS F and ML TR in the English and bilingual corpora and D COV J and POS F in the Hebrew corpus) were significantly different at the 95% confidence level. MUSE performed significantly better than TextRank in all three corpora and better than the best single methods COV DEG in English and D COV J in Hebrew corpora respectively.

Two sets of features X  X he full set of 31 sen-tence scoring metrics and the 10 best bilingual a clustering analysis of the methods results on both corpora X  X ere tested on the bilingual corpus. The experimental results show that the optimized combination of the 10 best metrics is not signif-icantly distinguishable from the best single met-ric in the multilingual corpus  X  COV DEG. The difference between the combination of all 31 met-rics and COV DEG is significant only with a one-tailed p-value of 0 . 0798 (considered not very sig-nificant). Both combinations significantly outper-formed all the other summarizers that were com-pared. Table 4 contains the results of MUSE-trained weights for all 31 metrics.

Our experiments showed that the removal of highly-correlated metrics (the metric with the lower ROUGE value out of each pair of highly-correlated metrics) from the linear combination slightly improved summarization quality, but the improvement was not statistically significant. Dis-carding bottom ranked features (up to 50 %), also, did not affect the results significantly.

Table 5 shows the best vectors generated from training MUSE on all the documents in the En-glish, Hebrew, and multilingual (one of 10 bal-anced) corpora and their ROUGE training scores and number of GA iterations.

While the optimal values of the weights are ex-pected to be nonnegative, among the actual re-sults are some negative values. Although there is no simple explanation for this outcome, it may be related to a well-known phenomenon from Nu-merical Analysis called over-relaxation (Friedman and Kandel, 1994). For example, Laplace equa-tion  X  grid of points as follows: At each grid point let  X  ( n ) ,  X  ( n ) denote the n th iteration as calculated from the differential equation and its modified fi-nal value, respectively. The final value is chosen as  X  X  ( n ) + (1  X   X  )  X  ( n  X  1) . While the sum of the two weights is obviously 1 , the optimal value of  X  , which minimizes the number of iterations needed for convergence, usually satisfies 1 &lt;  X  &lt; 2 (i.e., the second weight 1  X   X  is negative) and ap-proaches 2 the finer the grid gets. Though some-what unexpected, this surprising result can be rig-orously proved (Varga, 1962).
 Table 4: Summarization performance. Mean ROUGE-1
Assuming efficient implementation, most met-rics have a linear computational complexity rela-tive to the total number of words in a document -
O ( n ) . As a result, MUSE total computation time, given a trained model, is also linear (at fac-tor of the number of metrics in a combination). The training time is proportional to the number of GA iterations multiplied by the number of indi-viduals in a population times the fitness evaluation (ROUGE) time. On average, in our experiments the GA performed 5  X  6 iterations X  X election and reproduction X  X efore reaching convergence. In this paper we introduced MUSE, a new, GA-based approach to multilingual extractive sum-marization. We evaluated the proposed method-ology on two languages from different language families: English and Hebrew. The experimen-tal results showed that MUSE significantly out-performed TextRank, the best known language-Table 5: Induced weights for the best linear com-bination of scoring metrics independent approach, in both Hebrew and En-glish using either monolingual or bilingual cor-pora. Moreover, our results suggest that the same weighting model is applicable across multiple lan-guages. In future work, one may: -Evaluate MUSE on additional languages and lan-guage families. -Incorporate threshold values for threshold-based methods (Table 2) into the GA-based optimization procedure. -Improve performance of similarity-based metrics in the multilingual domain. -Apply additional optimization techniques like Evolution Strategy (Beyer and Schwefel, 2002), which is known to perform well in a real-valued search space. -Extend the search for the best summary to the problem of multi-object optimization, combining several summary quality metrics. We are grateful to Michael Elhadad and Galina Volk from Ben-Gurion University for providing the ROUGE toolkit adapted to the Hebrew alpha-bet, and to Slava Kisilevich from the University of Konstanz for the technical support in evaluation experiments.

