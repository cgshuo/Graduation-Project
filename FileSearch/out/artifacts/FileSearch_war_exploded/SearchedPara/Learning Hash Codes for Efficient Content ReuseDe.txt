 Content reuse is extremely common in user generated medi-ums. Reuse detection serves as be the basis for many ap-plications. However, along with the explosion of Internet and continuously growing uses of user generated mediums, the task becomes more critical and difficult. In this paper, we present a novel efficient and scalable approach to detect content reuse. We propose a new signature generation algo-rithm, which is based on learned hash functions for word-s. In order to deal with tens of billions of documents, we implement the detection approach on graphical processing units (GPUs). The experimental comparison in this paper involves studies of efficiency and effectiveness of the pro-posed approach in different types of document collections, including ClueWeb09, Tweets2011, and so on. Experimen-tal results show that the proposed approach can achieve the same detection rates with state-of-the-art systems while us-es significantly less execution time than them (from 400X to 1500X speedup).
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information Search and Retrieval; H.3.7 [ Digital Libraries ]: Collection, Systems Issues Content Reuse Detection, GPUs, Learning to Hash
There is a quick expansion in the popularity of user gener-ated content in forums, microblogging sites, blogs, and other medius in recent years. These broadcast mediums provide opportunities for users to exchange content. According to the statistics, users in Twitter send 230 million tweets per day[8]. Technorati X  X  State of the Blogosphere Report also showed that there are about 126 million blogs on the Inter-net in 2010. The development of these platforms has result-ed in the number of user generated content (UGC) rapidly growing during the last few years.

While the increasing of UGC, content reuse ,whichisthe practice of using existing content components, occurs fre-quently in these mediums. It contains various forms includ-ing duplicate, near-duplicate, and partial-duplicate. Ex-act duplicate documents can be easily identified by stan-dard checksumming techniques. Near-duplicate web pages contain identical core content but are different in framing, navigation bar, advertisements, footer, or other non-content parts. Partial-duplicate, which is a more difficult problem, contains quoted phrases, sentences, or passages from other documents.

Duplicate or near-duplicate detection can help search en-gine to reduce storage costs and improve the quality of search indexes. It may also avoid users to see redundant documents in search results. Applications including plagiarism detec-tion, information flow tracking, opinion mining, and so on may benefit from the partial-duplicate detection or involve it as the basis. Along with the increasing requirements, con-tent reuse detection has received much attention in recent years. Many efficient and effective algorithms have been proposed [11, 15, 19, 23, 24, 25, 28, 33].

The challenges of content reuse detection include: 1) reuse may happen at different levels; 2) massive documents should be efficiently processed. Partial-duplicate detection may re-quire different algorithms to the approaches proposed for re-solving near-duplicate document detection. One of the main reason for this is that only a small part of a document is tak-en from others. Content reuse detection algorithms have to face with enormous documents, due to the rapid growth of the web. Thus, it is essential that content reuse detection methods should be efficient and scalable.

In this paper, we investigate a novel approach to detect sentence level content reuse by mapping sentence to a sig-nature space. Signature of a sentence is created by taking the bitwise-or of all signatures of words occurs in the sen-tence. Rather than using traditional hash functions, which do not consider statistics of words or characters, to assign hash code for each word/character, we analyze the require-ments of what the good codes should satisfy and formalize it as a constraint optimization problem. Since the task of finding optimal codes is NP hard, we relax the optimiza-tion problem and introduce a efficiency method to calculate the codes. With the signature generation method, sentences whose reuse scores are predicted to be less than a given threshold are eliminated. Experimental results show that the codes generated by the proposed method outperform state-of-the-art approaches. In order to handle millions of documents, graphical processing units (GPUs) are used to implement the detection algorithm.

The contributions of this work are as follows: 1) We out-line and discuss what makes good codes of words for content reuse detection. 2) Efficient optimization method is pro-posed based on several relaxation. 3) We provide parallel algorithm and its GPU implementation. 4) Evaluations on six large web collections in both English and Chinese are used to measure the efficiency and effectiveness.
The remaining of the paper is organized as follows: In section 2, we review a number of related work and the state-of-the-art approaches in related areas. Section 3 presents the proposed method. Experimental results in test collections and analyses are shown in section 4. Section 5 concludes this paper.
Our approach relates to three research areas: content reuse detection, learning to hash and parallel algorithm-s based on GPUs. In this section, we discuss the related work on these areas.
Content reuse detection has received much attention in the past several years. Previous studies on content reuse detection can be roughly divided into two research direc-tions: representation and efficiency. The first one focuses on representing text in different levels with or without lin-guistic knowledge. With the growth of digital documents, efficiency, has also received much more attentions.
Shingling, which was proposed by Broder [4], uses con-tiguous subsequences to represent documents. It does not rely on any linguistic knowledge. If sets of shingles extracted from different documents are appreciably overlap, these doc-uments are considered exceedingly similar, which are usually measured by Jaccard similarity. In order to reduce the com-plexity of shingling, meta-sketches was proposed to handle the efficiency problem [5].

In order to improve the robustness of shingle-like sig-natures, Theobald et al. introduced a method, SpotSigs. It provides more semantic pre-selection of shingles for ex-tracting characteristic signatures from Web documents [28]. SpotSigs combines stopword antecedents with short chains of adjacent content terms. The aim of it is to filter natural-language text passages out of noisy Web page components. They also proposed several pruning conditions based on the upper bounds of Jaccard similarity.

Chowdhury et al. proposed I-Match [7], which filters the input document based on collection statistics and compute a single hash value for the remainder text. If the documents with same hash value, they are considered as duplicates. It hinges on the premise that removal of very infrequent terms and very common terms results good document representa-tions for the near-duplicate detection task. Since I-Match signatures is respect to small modifications, Kolcz et al. [16] proposed the solution of several I-Match signatures, all de-rived from randomized versions of the original lexicon.
Different from the methods focused on document level, partial-duplicate detection was proposed by Zhang et al. [33]. They converted the task into two subtasks: sentence level near-duplicate detection and sequence matching. Except for the similarities between documents, the method can simul-taneously output the positions where the duplicated parts occur. In order to handle the efficiency problem, they im-plement their method using three MapReduce jobs.
Local text reuse detection focus on identifying the reused and modified sentences, facts or passages, rather than w-hole documents. Seo and Croft [23] analyzed the task and defined six categories of text reuse. They proposed a gen-eral framework for text reuse detection. Several fingerprint-ing techniques for the framework were evaluated under the framework.

The most similar work to ours was proposed by Kim et al. [15]. They mapped sentences into a point in a high di-mensional space and leveraged range searches in this space. However different with us, they simply use MD5 hash func-tion for each word to generate signature file. In this paper, we outline and discuss what makes a good code for content reuse detection, and propose to use learned hash codes to capture the relations between words/characters to reduce the false matches.
Extensive research on similarity search have been pro-posed in recent years. Among them hash-based methods were received more attention due to its ability of solving sim-ilarity search in high dimensional space. Recently, several researches attempted to find good data-aware hash functions through machine learning.

Hinton and Salakhutdinov proposed to train a multilayer neural network with a small central layer to convert high-dimensional input vectors into low-dimensional codes [13]. They used a two-layer network called a Restricted Boltz-mann machine(RBM) [14] to do it. Experimental results showed that it could accelerate document retrieval.
Spectral hashing [30] was defined to seek compact binary codes in order to preserve the semantic similarity between codewords. Weiss et al. defined the criterion for a good code which is related to graph partitioning and used a spectral relaxation to obtain a solution.

Norouzi and Fleet [20] introduced a method for learning similarity-preserving hash functions, which is based on la-tent structural SVM framework. They designed a specific loss function taking Hamming distance and binary quanti-zation into account.

Zhang et al. introduced Self-Taught Hashing (STH) ap-proach to semantic hashing [32]. They divided the prob-lem of finding small codes into two stages. Firstly, they used unsupervised method, binarised-LapEig, to optimal l -bit binary codes for all documents in the given corpus. The classifiers were trained to predict the l -bit code for unseen documents.

Almost all the current methods for similarity-preserving hash functions attempt to map the high dimensional data, which represents the whole document or sentence, onto bi-nary codes. In this paper, we seek good binary codes for words under the content reuse detection framework.
Graphics programming units is designed for single instruc-tion multiple data(SIMD) paradigm, which is different from general purpose microprocessors. Due to its advantages on massive parallel, high memory bandwidth, and power-ful computing capacity, GPUs have been successfully used in numerical algorithms.
Owens et al. [21] introduced their works on implement-ing three applications (protein folding simulation, scalable molecular dynamics, and calculating electrostatic potential maps). Through these examples, they demonstrated the po-tential of the GPU for delivering performance gains on real problems.

The emergence of the NVIDIA CUDA programming mod-el speed up the trend of using GPUs to accelerate algorithms. Edelkamp et al. [9] introduced their work on accelerating s-tate space search using GPUs. Linear algebra operators were also implemented to build blocks for more complex numer-ical algorithms [17]. The results of researches in sort [12], search [6], linear algebra [10], partial differential equations (PDEs) [3], and many other applications have demonstrated the performance and capabilities of GPUs.

F
The processing flow of the proposed content reuse detec-tion approach is shown in Figure 1. It consists of two distinct stages: offline and online . Given a collection of documents, the sentence extraction step splits the documents into sen-tences. Hash code generation step takes the training data calculated based on the given collections or open domain data set to generate data-aware hash codes for words or characters. Signature generation step uses the hash table to calculate hash code of sentences from both document corpus and given queries. The key algorithm of the online stage is candidate searching , which tries to filter the sentences whose reuse scores with the given query are guaranteed not bigger than a given threshold. The reuse sentences and their cor-responding reuse scores are calculate in the final step.
Many similarity metrics have been proposed for content reuse detection. Jaccard similarity has been used in various works [4, 33]. Metzler et al. proposed to use weighted word overlap similarity to measure reuse score [18]. There are also a number of works to study the effect of cosine similarity [2, 31]. However, directly use these similarity metrics to detect content reuse in large collections would be very expensive. Because of this, in recent years, hash-based methods have been carefully studied and have demonstrated their advan-tageous for near similarity search in large document collec-tions [27].

In this paper, we follow the method proposed by Kim et al. [15], which map sentence signature into a point in a high dimensional space. Within this method, each word (or character in Chinese corpus) is assigned a fixed-width bit string. The sentence signature is generated by taking bitwise-or of all signature of words in the sentence. Figure 2 shows an example of sentence signature generation process. h (  X  ) represents hash function for words. In [15], two bits were set for each word using MD5 hash function [22] for 32-bit signatures.
 resents a signature extracted from a sentence. We can map space. Then the candidate sentences can be selected based on Euclidean distance between sentences, which can be cal-culated as follow: In other words, it also represents the number of bit different between the two sentences. Given a threshold whose distances lie in the range are extracted as candidates.
As mentioned above, signatures of sentences are generated based on the hash codes of words. Therefore, how to select hash codes for words has become one of the key problems in this task. In the following parts of this section, we discuss and describe our proposed method.
The proposed approach tries to find as many as possible reuses under the given upper bound of false matches. In other words, it aims to maximize the recall rate,  X  rec ,onthe given false positives rate  X  fp . Under the same conditions, the number of bits, m , and the number, l ,ofbitssetto 1 in the signature effect the detection recall and precision. In particular, m , l , and vocabulary size should follow the equation m l  X  w [15], and l is selected as the smallest value among all candidates. Hence, in order to save the code space, words which usually occur together should have thesamehashcode.

Based on the descriptions above, we seek hash codes which should satisfy the following properties: (1) the number of bits set to 1 in the word hash code is low; (2) the number of bits to code the vocabulary should be small; (3) words which usually occur together should have same hash codes. In this paper, we formalize the good code seeking task as a constraint optimization problem.

Let { y i } n i =1 be the list of hash codes for n words. y { 0 , 1 } m represents m -bits binary vector. #( w i ) represents the number of sentences containing word w i in the given cor-pus, #( w i ,w j ) indicates the number of sentences containing both word w i and word w j . s ij measures the similarity be-tween word w i and w j , which is formulated as following: s ij equals to binary cosine similarity in which a dimension receives a score of 1 when the word appears in the sentence and 0 when it does not appear. The parameter l defines the number of bits set to 1 in the hash code. s ij can be unsuper-vised generated based on given corpus. By incorporating all the constraints together, we obtain the following problem:
The property (1) is satisfied by m k =1 y ik = l ,where l is usually set to a small number (in this work, we set l = 1 The number of bits in hash code is predefined by m to fol-low the property (2). The property (3) is implemented as the objective function. If two words usually occur togeth-er in sentences and have different hash code, it would give negative impact of the objective function.
The solving of equation (1) is 0-1 integer programming program, which is a special case of integer programming. It is known to be NP-hard. There is no easy solution for directly optimizing it. Since the number of bits required by equation (1) does not follow the restrictive assumption that the bits are uniformly distributed, the spectrum of the similarity matrix of the data can not be directly used to get the hash codes as spectral hashing [30]. Relaxation and approximation methods should be used to solve the large-scale problem.

First of all, the constraint y i  X  X  0 , 1 } m is replaced by 0 y  X  1. By ignoring the integer constraints, the objective function in (1) is differentiable. In other words, the problem in (1) is relaxed as:
Since there are usually thousands of y i and m is also bigger than 32 in practice, the number of parameters tend to be extremely large. The number of constraints is also linear in the size of vocabulary. Because of these, the problem in (2) can not be directly solved within acceptable time either.
In this work, we use an interior-point nonlinear program-ming algorithm based on a filter line search to solve the problem [29]. Based on it, the inequality constraints are converted to barrier functions which are combined with ob-jective function. We combine the following barrier function with objective function to replace the constraint 0  X  y i 1 , 1  X  i  X  n : ,where  X  is barrier parameter, which is decreased at each optimization iteration. The problem in equation 1 can now be formulated as a sequence of approximate maximization :
Algorithm 1 presents the pseudo-code for the detection part (marked as online in the figure 1) of the proposed reuse detection method. SigCol represents a collection of sentence signatures extracted from a document collection. Given SigCol and a query sentence, q ,the candidate search step tries to identify a set of candidate sentences, CSet . The candidate search step selects sentences by searching the points whose distances between p q are less than the giv-en threshold CSet is not empty, the reuse score computation step cal-culates the reuse scores between query sentence with each Algorithm 1 Pseudo-code of the Reuse Detection INPUT: a query sentence, q , a distance threshold signatures of document collection, SigCol OUTPUT: a set of detected reuse sentences, O q Candidate Selection: 1: Generate signature S q for the given query q , 2: for all S i  X  SigCol do 3: if Distance ( S q ,S i ) &lt; 4: CSet = CSet i 5: end if 6: end for 7: return CSet Reuse Score Computing: 1: for all i  X  CSet do 2: Sim q,i = Jaccard ( S q ,S i ) 3: if Sim q,i &gt; X  then 4: O q = O q &lt;i,Sim q,i &gt; 5: end if 6: end for 7: return Q q candidate based on Jaccard similarity. If the reuse score of a sentence is greater than the pre-defined threshold, the sen-tence and its corresponding document is added in the final list.

From analyzing the calculation consumption of algorithm 1, we observe that the most time consuming part is spent on step 3 of the candidate selection step. Since the number of sentences in SigCol is usually tens of millions, Distance ( S takes the most computing time of the whole algorithm. For-tunately, GPUs offered us an opportunity to accelerate the performance of the algorithm. Modern GPUs are massively parallel processors with extremely high memory bandwidth. Many operations can be performed in parallel. The distance calculation part is suitable for implementation on GPUs as it is fairly simple and consumes a huge part of computing resources.

The function executed GPUs in parallel is called kernel , which is driven by threads and grouped together in blocks and grids . In this work, we implement the step 3, 4, and 5 in candidate selection part as a kernel function. Based on the thread index and block index, different sentence signature S i will be justified in different threads. Since signature file of the whole corpus is small enough (302MB for 10 million sentences), it can be easily loaded into the global memory of modern GPUs entirely at once.

The implement of the candidate selection is shown in fig-ure 3. The first step is to copy signatures extracted from a collection into global memory. Different threads would parallel process different parts of signatures in stream pro-cessors. The step 2 and step 3 in the figure are iterated for each query. Because the bottleneck of this task on GPUs is the memory access rather than processing time, if multiple queries were processed at the same time, the acceleration ratio to CPU implementation would be further improved. Figure 3: Implementation of GPU Based Parallel Candidate Search We evaluate the proposed method with six corpora TIP-STER (Volume 1-3) 1 , ClueWeb09-T09B 2 , Tweets2011 Twit-ter 3 , SogouT 2.0 4 , Baidu Zhidao 5 ,SinaWeibo 6 .Table1 shows the statistics of the six collections. TIPSTER col-lection contains news articles, discourse passages extracted from Associated Press (AP), Wall Street Journal (WSJ) and so on. It has been used for evaluations of information re-trieval, entity extraction, and many other tasks. Tweets2011 Twitter collection is used by Trec 2011 microblog track, which contains about 16 million tweets sampled between January 23rd and February 8th, 2011 7 . TREC Catego-ry B dataset (ClueWeb09-T09B), which is a subset of the ClueWeb09, contains 50 million English pages and has been used in various TREC tracks. Since the proposed approach is language independent, we also evaluate the proposed ap-proach on three Chinese collections. Baidu Zhidao is one of the most popular community Q&amp;A site in China. We crawled a portion of question and answer pairs of all cat-egories from it, resulting in a local archive of about 33.5 million questions. Sina Weibo is the most popular and the largest Twitter-like micro-blog site in China. Messages or comments from approximate 1.78 million users are used in this work.
We set the signature lengths( m )to32inthiswork. Fol-lowing the parameters used in [15], the l is set 2 for 32 bits signature. For English collections, words are used as the basic units to assign hash codes. The basic units of Chinese collections are characters. We re-implemented the baseline qSign algorithm [15] using the MD5 [22] as hash function to assign hash codes for all the words/characters. It is labeled as X  MD5  X  X n the following tables and figures. To verify the proposed properties that good hash code should satisfy, we construct another hash code generation baseline, which set the same hash code for the words which often oc-http://www.ldc.upenn.edu/ http://boston.lti.cs.cmu.edu/Data/clueweb09/ http://trec.nist.gov/data/tweets/ http://www.sogou.com/labs/dl/t.html http://zhidao.baidu.com http://www.weibo.com
Since the data is crawled by ourselves with the tools and tweet lists provided by NIST, about 5.8% tweets are missed in our collection due to the user name change or other rea-sons. The reuse threshold  X  is set to 0.8. Table 1: Statistics of the evaluation document collections TIPSTER English 1,078,925 3.25GB Tweets2011 Twitter English 15,204,939 2.13GB ClueWeb09-T09B English 50,220,423 490.4GB Baidu Zhidao Chinese 33,497,107 22.8GB Sina Weibo Chinese 267,612,493 418.6GB
SogouT 2.0 Chinese 37,205,218 558.0GB cur together.  X  NER  X  is used to represent this method. For the proposed approach, although we have proposed several methods to reduce the computing consumption of the opti-mization problem, there would also be too many variables needed to optimize. In this work, we select top 3 , 000 word-s/characters to optimize according to their frequency. The similarity matrix is calculated based on 300,000 sentences extracted from each collection. The hash codes of the oth-er words are generated based on MD5. We use  X  OPT  X  X o represent this method in the following.

All the experiments were evaluated on a workstation with a 2.13G Intel Xeon quad-core processor, 4GB memory, and an NVIDIA Quadro 4000 graphics card with 2GB global memory and 256 stream processors. CUDA Toolkit version 4.1 is used to implement the algorithm. For sentence bound-ary detection, we used around 50 manually written rules to do it.
To compare the candidate searching with different hash code generation methods, for each corpus, we randomly se-lected 1 million sentences as evaluation data sets. As reuse detection queries, 2,000 sentences are randomly selected from them. The similarities between queries and sentences in the data set are calculated using cosine coefficient. The ground truth reuse sentences are obtained by comparing queries with all sentences in corresponding data set.

Table 2 illustrates the impact of candidate searching step with different hash code generation methods. We only list the detailed result in TIPSTER and SogouT 2.0, due to space limitations. Figure 4 summarizes results of all six corpora. In Table 2, d represents the d bits difference be-tween query and reuse sentences. The reuse threshold  X  is set to 0.8.  X #can. X  represents the number of candidate sentences, which are selected using different hash code gen-eration methods. From the results, we can observe that can-didate searching step really benefits acceleration the reuse detection system. Instead of comparing with all sentences, the step can help reduce more than 90% calculation without losing any correct reuses in most cases.

From Table 2a, we can also observe that different hash code generation methods may highly impact the performance of candidate searching step. Although the baseline method, which is also used by previous work qSign [15], already achieves good results, the simple way which merge the hash code of similar words can further give more than 50% im-provement in all bits difference level. Comparing to the baseline methods, hash codes generated based on the pro-posed learning based method achieve the best results. At the Table 3: The average number of selected candidates per sentence at different recall level. The reuse threshold  X  is set to 0.8.
 same recall level, the number of selected candidate sentences based the hash codes generated by the proposed methods is only 2.7% to 24.6% of the sentences selected based on MD5 hash code. This indicates that the proposed method can dramatically improve the effectiveness of candidate search-ing.

The precision-recall curves graph for all six collections are shown in Figure 4. The reuse threshold  X  is also set to 0.8 in all the experiments for this figure. In almost all cas-es, the proposed approach achieves the best result among the three hash code generation methods. These results also demonstrate the observations described in the previous sec-tion. Although from the view precision-recall curve graph the precision improvement can not be easily noticed, at a recall level of 0.999, the proposed approach can further re-duce around 51.9% sentences in SogouT 2.0, 55.4% in Sina Weibo and 27.7% in Baidu Zhidao over baseline method.
Table 3 shows the detailed performance of candidate search-ing step at different recall level in all six collections. The  X  X ol. X  column represents the average number of ground truth reuses per sentences. Since the recall level can on-ly be controlled by the bits difference threshold, we list the parameter d in the bracket in the third column. From the table, we can observe that quotations are common in Web collections. While news articles also contain a large number of exact quotations. At almost all recall levels, the proposed hash code generation method achieves the best performance. It means that the proposed method can benefit the perfor-mance of candidate searching step.

In the above experiments, the documents used to calcu-late similarities between words for our approach are random selected from each collection. Since in-domain data may not be pre-collected in some cases, in this experiment we evalu-ate the performance of candidate selection with hash codes Table 4: The average number of selected candidates per query sentence with in-domain and out-of-domain data. The reuse threshold  X  is set to 0.8. The recall level is set to 100%, the corresponding bits difference d are in the bracket.  X  X P-T IN X  represents hash codes generated based on in-domain data.  X  X PT OUT X  represents hash codes generated based on out-of-domain data.
 Corpus MD5 OPT IN OPT OUT TIPSTER (d=4) 6024.8 316.3 5482.1 Twitter (d=4) 488.1 477.4 492.1 ClueWeb09 (d=4) 1253.0 554.1 796.4 Baidu Zhidao (d=3) 147.2 94.5 88.1 Sina Weibo (d=2) 145.8 47.4 49.1
SogouT 2.0 (d=4) 1887.7 1005.5 1228.9 Figure 5: The overlap percentage of top frequency word-s/characters between corpora and  X  X eb 1T 5-gram Corpus X  or  X  X hinese Web 5-gram X . generated with similarities matrix calculated from open do-main data sets. We use X  X eb 1T 5-gram Corpus 8  X  X nd X  X hi-nese Web 5-gram 9  X  as the open domain data, where 5-grams are treated as sentences. They contain English and Chinese word n-grams and their observed frequency counts. Table 4 shows the performance candidate selection with these cor-pora. From the results, we can observe that in-domain data performs better than out-of-domain X  X  in most cases. Out-of-domain data works better in Chinese than in English. For the corpus Baidu Zhidao, the performance of hash codes gen-erated based on 5-grams is even better than in-domain data. We think that the main reason is that the words distribu-tions in  X  X hinese Web 5-gram X  are similar as Baidu Zhidao, which also contains documents from multiple domains. For English corpora, the performances of out-of-domain are not good as in-domains. In order to find the reason, we analysis the overlap of top frequency words. Figure 5 shows the over-lap percentage of top frequency words/characters between corpora and  X  X eb 1T 5-gram Corpus X  or  X  X hinese Web 5-gram X . From the statistics, we can observe the reason why out-of-domain data perform worse in English corpora than Chinese corpora. Distributions of words play an important
LDC Catalog No. LDC2006T13
LDC Catalog No. LDC2010T06 role for the quality of generated hash codes from different domains.
Due to the increasingly growing data, efficiency is another important issue we focused on in this paper. In this subsec-tion, we compare the running time of our approach with state-of-the-art systems. We note that the running time of our approach composes of two steps: candidate selection and post-processing. Candidate selection step can be fur-ther divided into two steps: sentence feature generation and range searching. Because feature generation time for sen-tences are equal in different methods and the calculation consumption is small, we only evaluate the time of differen-t range searching methods. To evaluate the impact of the number of selected candidates with different hash codes, we also evaluate the post-processing time.

Table 5 shows the running time of three different range searching methods. Brute force, which directly calculates all the distances between query and reference sentences, is implemented using a single thread CPU implementation and GPU implementation. We also adopt PM-Tree 10 [26] which is an indexing technique for efficient similarity searching. From the results, we can observe that indexing technique can improve searching efficiency. However, brute force method with GPU implementation can even achieve more than 1500 times speedup. Further more, the brute force methods do not need the index construction time. We think that the high memory bandwidth of GPU and naturally parallel al-gorithm are the main reason of the success of GPU imple-mentation. Figure 6 shows the running time of GPU based candidate searching. We can observe that the processing time along the y axis increases as a linear function of the size of collection.

Figure 7 shows the post processing time at different recall level. We select two corpora  X  X IPSTER X  and  X  X ogouT 2.0 X  to evaluate the time using different hash codes. Figure 8 shows the post processing time with different hash codes generation methods. From these results, we can observe that the proposed hash code generation method can benefit the execution time of post processing. The number of selected candidates through different hash code generation methods impacts the execution time. Since our proposed method can filter more negative candidates than other methods, the running time are reduced. Figure 6: Total execution time of candidate selection imple-mented with GPU for 2,000 queries.
We use the source code provided by Tom  X as Skopal. We set pivot to 20, and pageSize to 2048 in the experiments.  X  is set to 0.8. Each corpus contains 1 million sentences.
Time (S)  X  is set to 0.8. The number of bits difference is set to 3.
In this work, we propose a novel approach which improves the efficiency and effectiveness of content reuse detection in two aspects. We introduce learning to hash method for gen-erating hash codes of words/characters. We also propose to use GPU implementation to speedup the range searching task, which is the most time consuming part in candidate selection step. We evaluate the proposed approach in six different kinds of documents collections. Experimental re-sults show that our method can significantly improve the efficiency of content reuse detection and would not impact the recall any more.
The author wishes to thank the anonymous reviewers for their helpful comments. This work was partially funded by 973 Program (2010CB327900), National Natural Science Foundation of China (61003092, 61073069), Shanghai Lead-ing Academic Discipline Project (B114), and  X  X hen Guang X  project supported by Shanghai Municipal Education Com-mission and Shanghai Education Development Foundation (11CG05). [1] J. Aberdeen, J. Burger, D. Day, L. Hirschman, [2] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all [3] J. Bolz, I. Farmer, E. Grinspun, and P. Schr  X  ooder. [4] A. Z. Broder. On the resemblance and containment of [5] A. Z. Broder. Identifying and filtering near-duplicate [6] B. Bustos, O. Deussen, S. Hiller, and D. Keim. A [7] A. Chowdhury, O. Frieder, D. Grossman, and M. C. [8] L. Dugan. 230 million tweets per day, 50 million daily [9] S. Edelkamp, D. Sulewski, and C. Y  X  l  X zcel. Perfect [10] K. Fatahalian, J. Sugerman, and P. Hanrahan. [11] A. Gionis, P. Indyk, and R. Motwani. Similarity [12] N. K. Govindaraju, M. Henson, M. C. Lin, and [13] G. Hinton and R. Salakhutdinov. Reducing the [14] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast [15] J. W. Kim, K. S. Candan, and J. Tatemura. Efficient [16] A. Kolcz, A. Chowdhury, and J. Alspector. Improved [17] J. Kr  X  uger and R. Westermann. Linear algebra [18] D. Metzler, Y. Bernstein, W. B. Croft, A. Moffat, and [19] K. Muthmann, W. M. Barczy  X nski, F. Brauer, and [20] M. Norouzi and D. Fleet. Minimal loss hashing for [21] J. D. Owens, M. Houston, D. Luebke, S. Green, J. E. [22] R. Rivest. The md5 message-digest algorithm, 1992. [23] J. Seo and W. B. Croft. Local text reuse detection. In [24] N. Shivakumar and H. Garcia-Molina. Scam: A copy [25] N. Shivakumar and H. Garcia-Molina. Finding [26] T. Skopal, J. Pokorn  X y, and V. Sn  X  asel. Pm-tree: [27] B. Stein. Principles of hash-based text retrieval. In [28] M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs: [29] A. W  X  achter and L. T. Biegler. On the implementation [30] Y. Weiss, A. Torralba, and R. Fergus. Spectral [31] C. Xiao, W. Wang, X. Lin, and J. X. Yu. Efficient [32] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught [33] Q. Zhang, Y. Zhang, H. Yu, and X. Huang. Efficient
