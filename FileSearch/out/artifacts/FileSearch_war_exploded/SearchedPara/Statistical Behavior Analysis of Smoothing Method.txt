 have been used on information retrieval (IR). We can calculate the probability for the each occurred event according to its count in re-estimate the probability for each event. 
For the possible word X  X  estimation, the word sequence W max with maximum conditional probability P ( W ) in n -gram model will be expressed as: bigram model. Therefore the probability of a word bigram b will be written as: maximum likelihood estimation (MLE). In the general case of n -gram models, Eq. (2) can be rewritten as: 1.1 Cross Entropy and Perplexity Two common metrics to evaluate language model is called cross entropy and probability for the testing set P ( T ) can be described as: The entropy H ( T ) can be regarded the bit size needed to encode each word in testing set T . H ( T ) can be shown as: Perplexity PP ( T ) is defined in term of entropy H ( T ): proposed smoothing methods with the previous methods. future output of the source, the cross entropy should be small. In the general case, CH &gt;= H (where H employs the best possible language model, the source itself). 1.2 Zero Count: The Smoothing Issues with MLE. However, such method will lead to the degradation of performance. For a 
It is not reasonable and good to assign 0 to the unseen events. If we should assign certain probability to such events, how is the probability assigned? The schemes used probability obtained from MLE will be adjusted and redistributed. Such a process will improve the performance of language models. 
Although there are currently several corpora containing more than twenty millions may be either word sequences (like n -grams) or single word (like Mandarin characters or words). 1.3 Overview of Several Smoothing Methods discounting , Witten-Bell [17], and our proposed Smoothing method. 1.3.1 Additive Discount Method defined as follows: rewritten as follows: number of alphabets for a language, such as the number of Mandarin characters. According to the previous experiments [3 ], the performance was usually degraded by using add-one smoothing. 1.3.2 Witten-Bell C Wetten and Bell 1 [17]. recurring: 
The W-B C is described as: respectively. 
The discounted probability will be expressed for seen bigrams as: as: uniformly by U : The ratio may be greater or less than 1, depending on the value of S and U . 1.3.3 Proposed Smoothing Method (Y-H) heuristic. Basic Concept of our proposed smoothing method will be described: 
In case for a bigram, our method calculates the smoothed probabilities as: where d denotes a constant (0&lt; d &lt;1) and independent of U . 
When computing the smoothed probability, the proposed method don X  X  employ interpolating scheme to combine the high order models and lower order models. As mass d A /( N +1), behaviors of smoothing methods in next section. A. Property 1 which is described as follows: B is the number of types of bigrams. B. Property 2 The summation of smoothed probability P * for all the bigrams is necessarily equal to 1 on any training size N . Total smoothed probability P is summed as: where B denotes the total number of bigrams. C. Property 3 satisfy all the following inequality equations 2 : corpus of size N . 
Inequality Eq. (16) describes the concept that smoothed probability for any with c counts. D. Property 4 Comparing to the probability P prior to smoothing process, the smoothed probability P for all bigrams will be changed. Property 4 can be expressed as follows: smoothed probability for unseen bigrams. E. Property 5 same counts on training size N +1 should be decreased a bit while comparing to the Q * the incoming bigram b N+1 : 5 can be expressed as: bigrams with 0 count. smoothing methods should still comply with these properties. Based on the statistical methods. 3.1 Additive Discount Method ( c &gt;= 0) is discounted by the normalized factor N /( N+B ). For the smoothed property of all bigrams, property 3 still holds. The property 4 for additive method is analyzed as: 
According to property 4, Eq. (22) should be negative while numerator ( N -cB ) of Eq. method. 3.2 Witten-Bell Method As shown in Eq. (15) and (16), smoothed probability P * for any bigram will be (0,1), counts can be expressed as follows: does not hold. 
Finally, property 5 will be analyzed as follows. Smoothed probabilities Q * , derived as: Case I: When N = N +1, U keep unchanged and U &gt;1. Case II: When N = N +1, U = U -1 and U &gt;1. method. 3.3 Proposed Smoothing Method (Y-H) We analyze furthermore the statistical behaviors of the proposed method. As shown in can be accumulated as: 
So, property 2 does hold. The smoothed probability Q * for bigrams with c and c +1 counts on N training data is calculated as follows. For c =0 and 1, For c &gt;1: Eqs. (24) and (25), we can conclude that property 3 does hold. Original and smoothed probability for a bigram with c counts is as follows: 
As shown of Eq. (26) and (27), we can conclude property 4 does hold. Finally, we training data are calculated as: Eqs. (28) and (29), property 5 does hold in this case. As shown of Eq. (30), numerator ( U-N +2) may be less or greater than 0 in terms of N and U : ( N &lt;&lt; U ). properties. Among these smoothing methods, there isn X  X  any method which complies properties. Notations O and X denote the method does and does not comply with the proposed property, respectively. models are shown. We further discuss the relationship between the P mass and entropy entropy H of LM or not will be shown. 4.1 Data Sets and Empirical Models symbols are extracted and there are about 7M Mandarin characters in news texts. The Academic Sinica Balanced Corpus version 3.0 (ASBC) includes 316 text files distributed in different fields, 118MB memo ry size and 5.22 millions of words labeled with a POS tag. Our corpus contains totally up to 12M Mandarin characters. 
In the paper, we have constructed three models to evaluate the entropy of word unigrams model. The entropy of each method is calculated on various data size employ up to 12M Mandarin characters (unigrams and bigrams) and the 3 rd model use about up to 5M Mandarin words in ASBC corpus. 4.2 Probability Mass Assigned to Unseen Events normalized factor for each seen bigram. P mass is varied primarily with the smoothing counts ( c &gt;= 1) prior to smoothing process. 4.3 Entropy Evaluation for Three Language Models In the paper, three language models; Mandarin character unigrams, character bigrams assigned to all unseen events and entropy. Three smoothing methods are evaluated. The constant d in our method are set to 0.8, similar and the maximum entropy will happen at training size 1M for all the methods. methods through all training size N ( 1M&lt;= N &lt;=12M). 
For the Mandarin character bigram, smoothing method Add-1 also generate highest d =0.01 will obtain lowest entropy than that from all other methods. In the model, the Finally, we look at the Mandarin word unigram model (see the bottom of Figure 1). obtain almost same results and it is always lower entropy than other methods. However, our proposed smoothing methods comply with all these properties based on some reasonable conditions. 
Three Mandarin language models are constructed; character unigram, character methods to analyze the statistical behaviors. perplexity. In other paper, we will further discuss the perplexity and the performance of real NLP applications in which the smoothing method is applied into POS tagging, Mandarin word semantics or WSD issues. 
