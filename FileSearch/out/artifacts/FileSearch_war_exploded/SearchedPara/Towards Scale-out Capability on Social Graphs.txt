 The development of cloud storage and computing has facil-itated the rise of various big data applications. As a rep-resentative high performance computing (HPC) workload, graph processing is becoming a part of cloud computing. However, scalable computing on large graphs is still dom-inated by HPC solutions, which require high performance all-to-all collective operations over torus (or mesh) network-ing. Implementing those torus-based algorithms on com-modity clusters, e.g., cloud computing infrastructures, can result in great latency due to inefficient communication. Moreover, designing a highly scalable system for large so-cial graphs, is far from being trivial, as intrinsic features of social graphs, e.g., degree skewness and lacking of locality, often profoundly limit the extent of parallelism.
To resolve the challenges, we explore the iceberg of devel-oping a scalable system for processing large social graphs on commodity clusters. In particular, we focus on the scale-out capability of the system. We propose a novel separator-combiner based query processing engine which provides na-tive load-balancing and very low communication overhead, such that increasinglylarger graphs can be simply addressed by adding more computing nodes to the cluster.The pro-posed system achieves remarkable scale-out capability in processing large social graphs with skew degree distribu-tions, while providing many critical features for big data analytics, such as easy-to-use API, fault-tolerance and re-covery. We implement the system as a portable and easily configurable library, and conduct comprehensive experimen-tal studies to demonstrate its effectiveness and efficiency. H.2.4 [ Database Management ]: Systems Algorithms; Experimentation; Performance Graph Databases; Distributed Databases; Social Networks
In an era of information explosion, big data analytics have become an increasingly important component of industry, which focuses on developing special parallel computational platforms to analyze various forms of massive data. The widespread usage of these platforms has changed the land-scape of scalable parallel computing, which was long domi-nated by High Performance Computing (HPC) applications. The big data platforms support applications, different from those solved by HPC before in that (1) big data platforms rely on commodity cluster-based execution and storage in-frastructure like MapReduce; and (2) the utility computing model introduced by cloud computing makes computing re-sources available to the customer as needed, such that large scale-out capability is mandatory for the systems on cloud.
As one of the most popular data-intensive applications, the rapidly growing social networks require an efficient and scalable graph processing platform. Unfortunately, MapRe-duce, the de-facto big data processing model, and its asso-ciated technologies such as Pig [19] and Hive [31] are often ill-suited for iterative computing problems on large graphs. As a result, several graph-parallel projects [1, 17, 18, 22, 24, 27, 30, 32, 36], have been proposed to facilitate data analysis on large graphs. These projects can be generally classified into two categories:
Briefly, the existing systems provide the following choices to process large graphs. 1. Relying on the existing systems which are based on 2. Adopting the HPC solutions which provide high per-3. Extending collective communication to commodity clus-1D (Commodity)  X   X   X  2D (HPC)  X   X   X  2D (Commodity)  X   X   X  Table 1: Existing systems against requirements
D eveloping a system for processing large social graphs on large-scale commodity clusters means satisfying three re-quirements: 1. Degree-skew : the partitioning method is load-balanced when processing graphs with skew degree distributions; 2. Commodity : the system does not rely on costly networking such as torus; and 3. Scale-out : larger graphs can be addressed by simply adding more comput-ing nodes to the system. As shown in Table 1, none of the existing approaches satisfy the above three requirements. Motivated by this, we propose a novel system, named GraphSlice. GraphSlice X  X  query processing is based on a Separator-Combiner BSP model. We demonstrate that the above three requirements can be satisfied simultaneously by an elegantly designed query processing engine by which mas-sive social graphs can be efficiently processed on commodity clusters. This system provides the following features. 1. Capturing the common patterns of graph algorithms 2. Achieving scalable performance in processing large graphs 3. Synchronizing supersteps without all-collective com-4. Relying on one-to-one communication in a BSP model,
The rest of this paper is organized as follows. Section 2 introduces the background knowledge. Section 3 gives the framework of our approach. Section 4 describes the query processing engine. Section 5 presents API of our system. Section 6 discusses system implementation. Section 7 re-ports the experimental results. The related work and con-clusion is given in Section 8 and 9, respectively.
In this section, we briefly introduce the background knowl-edge on 1D partitioning and vertex-centric BSP model.
The 1D partitioning (a.k.a. distributed adjacency list, source-vertex partitioning and horizontal partitioning) is the most popular partitioning method for graph processing in distributed environments. The vertices are partitioned to workers along with their outgoing edges. Figure 1 (a) and (b) illustrate the partitions of an example graph and its distributed adjacency list, respectively. In this example, the vertices are partitioned into three partitions { A, B, C } , { D, E, F } and { G, H, I } along with their outgoing edges.
In a BSP model [34], the superstep concept is used for synchronizing the parallel execution of workers. A superstep typically consists of three parts. Then the algorithm moves to the next superstep. An algo-rithm may involves several supersteps which are executed one after another.

For example, a simple BFS algorithm on 1D partitions is illustrated in Figure 2. The BFS begins at the root vertex  X  X  X  which is initialized by the vertex value  X 0 X  as its distance to the root. In the Superstep 1 , the worker, which owns the vertex  X  X  X  , sends the messages that contains the value Figure 2: A simple BFS algorithm on 1D partitions Algorithm 1 T he Vertex API foundations of Pregel template &lt; V ertexType, EdgeType, MessageType &gt; class Vertex { }  X 1 X  as the distance of frontiers, along the outgoing edges. E very vertex which has received messages from the vertex  X  X  X  updates its vertex value to  X 1 X , and sends the messages that contains the value  X 2 X  for the next superstep. The algo-rithm iteratively updates vertex values if the received value is smaller than the current vertex value, and sends messages which contain the updated value plus  X 1 X . The algorithm terminates when there is no update in a superstep.
Pregel [18] proposes a vertex-centric API on BSP model, abbreviated as vertex-centric BSP . Writing a vertex-centric BSP program usually means overriding the virtual Com-pute() function in the Vertex class as shown in Algorithm 1, which will be executed at each active vertex in every su-perstep. The template defines three value types which are associated with vertices, edges and messages. The vertex-centric BSP model exposes all outgoing edges of a vertex to the Compute() function by providing an OutEdgeIterator . A vertex-centric BSP implementation of Single Source Shortest Path (SSSP) algorithm is shown in Algorithm 2. The ShortestPathVertex class inherits from Vertex class and implements the Compute() function. The vertex value with each vertex is initialized to a constant INF which is larger than any feasible distance. In each superstep, each vertex first receives messages via its incoming edges, and calculates the minimum value among the received messages. If the minimum is less than the current vertex value, this vertex updates it value and sends the updated value via the outgoing edges, consisting of the updated vertex value added Algorithm 2 S ingle Source Shortest Path (SSSP) class S hortestPathVertex extends Vertex &lt; int, int, int &gt; { } to the edge weight. The algorithm terminates when there is n o new updates in a superstep, after that the value of each vertex denotes the distance from the source to that vertex. This algorithm is amenable to optimization using a com-biner as discussed in [18], which is omitted here due to the interest of space.
In this paper, we propose a system GraphSlice that em-ploys novel partitioning and query processing methods to tackle degree-skew and scale-out problems, while providing an easy-to-use API to the users. It consists of three parts: 1. Partitioning. The partitioning is managed by Sepa-2. Query Processing. The query processing is based 3. API. The API provides an easy Vertex-and-Edge view,
The next three sections will introduce the system details.
In this section, we introduce the basic idea of the par-titioning method, followed by the query processing model. More implementation details will be given in Section 6.
The vertices are firstly partitioned to workers according to a user-defined Hash function. Then, edge parititioning is managed by Separators which are containers of a subset of edges outgoing from a same vertex. We provide two types of Separators to manage the outgoing edges of every vertex: PreSeparator and PostSeparator which are abbreviated for pre-communication separator and post-communication sep-arator respectively. Every vertex X  X  outgoing edges are man-aged by either PreSeparator or PostSeparator. In particular, there is an underlying degree-based mechanism which man-ages high degree vertices by PostSeparators and low degree vertices by PreSeparators.

PreSeparator is very similar to the existing 1D paritition-ing method. It is hosted by the worker which owns the source vertex of the PreSeparator. PostSeparator is more tricky. If a vertex v is managed by PostSeparator, it is indeed man-aged by at most N PostSeparators on N different workers where N is the total number of workers in the system. Each PostSeparator on a worker manages a subset of the outgo-ing edges of v which emanate to the vertices owned by that worker.

F or example, a weighted directed graph is shown in Fig-ure 3. A user-defined hash function firstly divides the ver-tices into two partitions: { A,B,C } and { D,E,F } . The sys-tem estimates the degree of every vertex to determine the type of Separators. In this example, the vertex  X  X  X , whose out-degree is four, is managed by PostSeparator, while the others are managed by PreSeparator. In particular,  X  X  X  is managed by two PostSeparators, one for edges {  X  X A X ,  X  X B X  } and the other for {  X  X D X ,  X  X E X  } . It is worth-noting that these subsets are classified by the target vertex of each edge. For instance, the edges  X  X A X  and  X  X B X  are partitioned into the same subset, as the target vertices  X  X  X  and  X  X  X  are owned by the first worker. It is similar that the edges  X  X D X  and  X  X E X  are on the second worker which owns vertices  X  X  X  and  X  X  X . The query processing engine employs a Separator-Combiner BSP model (SC-BSP). A Combiner is an instance of an ag-gregation function that aggregates the messages emanating to a same vertex. We illustrate the query processing pro-cedure by an example of the SSSP algorithm on the graph in the previous example. The SSSP algorithm starts with a root at vertex  X  X  X . In Superstep 1, the distance of root vertex  X  X  X  is initialized to  X 0 X  ; the root vertex  X  X  X  sends messages to its neighbors  X  X  X ,  X  X  X  and  X  X  X ; and finally these three neigh-bors update their distances to  X 1 X ,  X 2 X  and  X 1 X , respectively. We omit the execution of Superstep 1, but enlarge the de-tails of the execution of Superstep 2 in Figure 4. There are two workers as depicted by the large gray rectangles. One of them contains the vertices  X  X  X ,  X  X  X  and  X  X  X , while the other contains the vertices  X  X  X ,  X  X  X  and  X  X  X . In Superstep 2, the vertices  X  X  X ,  X  X  X  and  X  X  X  are the frontiers which are marked in red color, as their distances have just been updated in Superstep 1.

As shown in the example in Figure 4, the low degree ver-tices  X  X  X  and  X  X  X  are managed by PreSeparators, while the high degree vertex  X  X  X  is controlled by two PostSeparators {  X  X A X ,  X  X B X  } and {  X  X D X ,  X  X E X  } .

A superstep consists of three parts: pre-computation, com-munication and post-computation. In the pre-computation of Superstep 2, the vertex  X  X  X  broadcasts its value  X 1 X  to Superstep 2 Communication
Finished F igure 4: The query processing of Separator-Combiner BSP model: a sample superstep in the SSSP algorithm all the workers since it is manged by PostSeparators. In the meanwhile, the vertices  X  X  X  and  X  X  X  call their PreSepa-rators to conduct edgeCompute() function (the API will be discussed in the next section). In the SSSP algorithm, edgeCompute() function adds the edge weight to the ver-tex value, and sends the sum to the target vertex of the edge. As the two edges  X  X D X  and  X  X D X  have the same target ver-tex  X  X  X , the PreCombiner is called after the PreSeparators to combine the two messages. For the SSSP algorithm, the MinValueCominber merges the messages by selecting the minimum value. Then, the communication part exchanges the fours messages among two workers.

In the post-computation of Superstep 2, the PostSepara-tors of vertex  X  X  X  traverse the outgoing edges of  X  X  X  in which edgeCompute() function is called to add the edge weight to the vertex value for every edge. The messages produced by PostSeparators are combined by their destinations in the PostCombiners. Then, these combined messages are sent to vertices in the next superstep. In the beginning of Super-step 3, the distances of vertices  X  X  X  and  X  X  X  are updated, and these two vertices will become the new frontiers.
The degree threshold, which determines whether a ver-tex is manged by PreSeparator or PostSeparator, is user-configurable. By default, the threshold is the number of workers. The default setting can minimize the total message size in communication for most applications. We observe that there are some existing graph algorithms which are op-timized by degrees. The one which is similar to our proposal is [28]. Compared with the existing algorithm, our contri-bution is designing a degree-based mechanism in a parallel graph processing engine, while the complexity of the design is hidden from users. Users can use GraphSlice X  X  API to develop graph algorithms without an understanding of the underlying degree-based mechanism. In this section, we present an overview of GraphSlice X  X  API which includes the major classes and functions. The focus of GraphSlice X  X  API is providing an easy Vertex-and-Edge view to users, while the underlying degree-based mechanism for Separator and Combiner is hidden.
The Vertex class of GraphSlice introduces three new fea-tures to the vertex-centric BSP model: (1) the enum Path which describes the message-passing paths between the ver-tices, (2) the template argument SeparationType which defines a user-specified value type for intermediate results, and (3) the concept of edge compute and EdgeHandler class which handles edge compute.

As shown in Algorithm 3, users override the virtual ver-texCompute() method, which will be executed at each ac-tive vertex in every superstep. The SendMessage() method allows vertexCompute() to send messages to other vertices, and these messages will be received by the other vertices via MessageIterator in their next superstep. SendMes-sage() contains three arguments: (1) enum Path , which describes the destinations of this message, either the tar-get vertex of the outgoing edges of the current vertex or all vertices in the graph, (2) SeparationType , which is the value type that will be passed to EdgeHandler , and (3) a user-defined subclass of the EdgeHandler class.
 Algorithm 3 T he Vertex API foundations template &lt; V ertexType, EdgeType, MessageType,
SeparationType &gt; class Vertex { }
The E dgeHandler class is shown in Algorithm 4. In-stead of exposing all outgoing edges in the Vertex class, the EdgeHandler class is responsible for edge compute. Users override the virtual edgeCompute() method which receives a S eparationType value from its source vertex, produces a MessageType value and sends the MessageType value to its target vertex.
 Algorithm 4 T he Edge API foundations template &lt; E dgeType, MessageType, SeparationType &gt; class EdgeHandler { }
An example of implementing the SSSP algorithm by over-r iding the Vertex class and the EdgeHandler class is il-lustrated in Algorithm 5. For representation simplicity, we set VertexType , EdgeType , MessageType and Sepa-rationType to int which is abbreviated for Integer. By overriding the vertexCompute() function, the algorithm firstly calls the isSource( long VertexID ) function to get an initial value for the distance. The distance is initialized to zero if the vertex is the source (i.e. root) of the algorithm, otherwise it is assigned with a constant INF which is larger than any feasible distance. In each superstep, every vertex processes the received messages to calculate the minimum value among all the messages. If the minimum value is less than the current vertex value, the vertex updates its value and sends the updated value via the outgoing edges by spec-ifying OutgoingEdges as the Path in the SendMessage method. The class type ShortestPathEdge.class is also specified to indicate the EdgeHandler of this message.
The ShortestPathEdge class is the EdgeHandler in the SSSP algorithm. It adds the vertex distance to the edge weight, and returns the sum. By default, the returned value will be sent to the target vertex of the current edge as a received message in the next superstep. The algorithm ter-minates when there is no update to any vertex in a super-step, at which point the value of each vertex indicates the distance from the source to that vertex.

As shown in Algorithm 6, the Combiner class is usu-ally used for reducing the communication of an algorithm. A combiner combines the messages which are sent to the same vertex. For example, in the SSSP algorithm, only the minimum value of the messages is useful to the vertex. The combiner has been proposed in Pregel. But the GraphSlice X  X  Combiner API is different from that of Pregel X  X , due to their difference in query processing engines. In Pregel X  X  API, the combiner exposes all the messages, which are sent to the same vertex, as an iterator of messages to users. However, GraphSlice X  X  API in Algorithm 6 provides a combine() function which always combines the original message and a newly arrived message, because its SC-BSP based query pro-cessing engine combines messages in both pre-computation and post-computation.

An example of the minimum value combiner for the SSSP algorithm is shown in Algorithm 7. If the value of a newly arrived message msgToCombine is less than the original message originalMsg , the value of the original message will be replaced by the value of the newly arrived message. In consideration of ease-of-use, the Separator class is Algorithm 5 S ingle Source Shortest Path(SSSP) class S hortestPathVertex extends Vertex &lt; int, int, int, int &gt; { } class ShortestPathEdge extends EdgeHandler &lt; int, int, int &gt; { } Algorithm 6 T he Combiner API foundations template &lt; M essageType &gt; class Combiner { } usually hidden from users. We provide in Algorithm 8 a de-f ault separate() which is usually applicable for most appli-cations. The default separate() function iterates the edges in the subset, calls the edgeCompute() function of each edge, and sends the returned MessageType value to the target of each edge.

Advanced users may override the separate() function to implement advanced algorithms. The Separator class provides a set of functions to users for implementing com-plicated algorithms. We briefly introduce some important functions. The getSourceVertexID() function returns the VertexID of the source vertex of this subset of edges. The isPreSeparator() function provides the type of the sepa-rator which can be either PreSeparator ( true ) or PostSep-arator ( false ). The getNumOfSubsets() and getSub-setID() functions return the number of subsets which are responsible for the same source vertex and the ID of this subset, respectively.
We are dedicated to implementing GraphSlice as an easy-to-use, easily maintainable and scale-out capabile system on commodity clusters.
 Indexing of Graph Structure. GraphSlice organizes the graph structure in the distributed memory of workers. The vertices are distributed to workers according to a user-defined Hash function. The distributed memory holds PreSepara-vertex-centric BSP model Algorithm 7 M inimum Value Combiner class M inValueCombiner extends Combiner &lt; int &gt; { } Algorithm 8 T he Separator API foundations template &lt; E dgeType, MessageType ,SeparationType &gt; class Separator { } tors and PostSeparators with corresponding B-tree indexes r espectively. Iterating all vertices, iterating all PreSepara-tors and iterating all PostSeparators on a worker are also supported for advanced users. This capability enables users to develop  X  X hink like a graph X  algorithms [32]. Communication. There are two alternative implementa-tions of PostSeparators. The first is indexing the workers in which a high degree vertex has PostSeparators, and sending SeparationType value to the indexed workers. The sec-ond is broadcasting the SeparationType value of a high degree vertex to all the workers. When the outgoing edges of a high degree vertex are distributed across more workers, the efficiency produced by such indexes will be reduced. We implement the prototype of PostSeparators as a broadcast-ing method in order to make the code easily readable and easily maintainable.
 Online Updating. Some graph mining algorithms may need to change the graph X  X  topology in their algorithms. Online update of graph structure is supported with some constraints. GraphSlice supports setVertexValue, addVer-tex, removeVertex, addEdge and removeEdge functions in Vertex class. It also supports setEdgeValue function in EdgeHandler class.
 Persistent Storage. GraphSlice accesses persistent stor-age only for input, output and checkpoints. Either gen-eral graph formats or a specific partitioned graph format can be used as an input. General graph formats are user-configurable, which include adjacency lists, edge lists and etc. When general graph formats are used as the input, GraphSlice needs to estimate the degree of every vertex to determine the type of separators. The estimation is typically calculated by randomly sampling of edges. After the esti-mation step, the graph structure are loaded into distributed memory, while the corresponding in-memory indexes are built on the fly.

GraphSlice is able to write the graph structure on disk in either general graph formats or the specific partitioned graph format. The specific partitioned graph format organizes the graph structure into two parts: vertices and separators. It directly writes the separators on each worker X  X  local disk. This format is used as the format in checkpointing for fault tolerance and recovery.
 Fault Tolerance and Recovery. As the query processing of GraphSlice uses a BSP model, the mechanism of fault tol-erance and recovery is based on checkpointing, which is sim-ilar to Pregel X  X . According to a user-defined parameter T we make check points every T CP supersteps. In these check points, the modified graph data and the buffered messages are written to the persistent file system for failure recovery. The persistent file system adopts the hadoop distributed file system (HDFS) with a configurable replication factor; that is, the file content is replicated on the storage of multiple computing nodes for reliability.
We conduct comprehensive experiments to evaluate the performance of the proposed SC-BSP model against the vertex-centric BSP model on massive degree-skewed graphs. We implement both two models as an in-memory system based on Hadoop, Netty and Giraph on an existing HDFS cluster with 128 physical machines each of which is con-figured with two Intel Xeon E5530 2 . 4 GHz CPUs, 24GB memory and four 500GB SSDs. In the default settings, the number of workers is fixed to 512 such that each physical machine runs four instances of an SSSP algorithm.
We adopt the HPC benchmark Graph500 [2] with default settings to generate graphs with skewed distributions. The graph generator is a Kronecker generator [5]. The generated graphs are highly degree-skewed. For example, when the graph contains one billion vertices, the maximum out-degree of a vertex will be more than half billion.
 Scale-Out Capability. The experiments in Figure 5 (a) eval-uate the scale-out capability on degree-skewed social graphs. Scale-out capability means that larger graphs can be ad-dressed by simply adding more workers to a system.
Both SC-BSP model and vertex-centric BSP model are initially allocated with 32 workers with 4GB memory on each worker. With 32 workers, SC-BSP can process graphs up to 0 . 5 billion edges. In contrast, vertex-centric BSP model only can process graphs up to 0 . 1 billion edges. Al-though both of the two systems own the same amount 32  X  4GB = 128GB of distributed memory, SC-BSP model can process larger graph than vertex-centric BSP model because of its native load-balancing technique. As the maximum de-gree vertex emanates more than 10% of edges, the vertex-centric BSP model needs to keep these edges in a single worker X  X  memory due to its vertex-centric property. SC-BSP model deploys PostSeparators as a container of edges, such that edges emanating from the maximum degree vertex are managed by the PostSeparators on different workers.
We increase the number of workers to examine if the sys-tem can address larger graphs. Vertex-centric BSP model fails to process any larger graph, as the maximum degree vertex consumes too much memory on a single worker. On the contrary, SC-BSP model can address larger graphs by simply adding more workers to the system. With 512 work-ers, SC-BSP model can process a graph with up to 2.1 billion edges.
 Response Time. The response time is illustrated in Fig-ure 5 (b). Because vertex-centric BSP model cannot process any graph larger than 0.13 billion edges, we only report its performance until this graph size. Within graph size they can address, both two systems shows scalable response time with respect to the number of edges.
 Communication. The communication is measured by the average message size produced by each worker. Figure 5 (c) reports the comparison between two BSP models. It is not surprising that performance gap is up to 40 times, even when the graph size is relatively small. The PreSeparators of the SC-BSP model produce the same amount of messages as the vertex-centric BSP model. However, the PostSepa-rators exceedingly reduce the amount of messages that are produced by high degree vertices, because only the Seper-ationType value will be transfered in communication and all messages along with outgoing edges will be processed by PostSeparators in a same worker. In power law graphs, when increasing the size of graph, more and more edges will be connected to the existing high degree vertices. These in-creased edges do not produce extra messages in the SC-BSP model, thus the performance gap drastically increases with the graph size. We observe that the message size increases sublinearly with the graph size in the SC-BSP model. Dynamic SC-BSP. When graphs are relatively small, allo-cating a large amount of workers may introduce cost over-head. We propose a simple optimization to reduce the cost overhead. This optimization dynamically determines the number of workers according to the graph size. In this ex-periment, we intuitively set this ratio to 2 million edges per worker. Therefore, we allocate 16 workers to the graph with 0 . 03 billion edges and 512 worker to the graph with 1 . 07 billion edges. For the graph with 2 . 14 billion edges, we still allocate 512 workers due to reaching the constraint on the maximal number of workers. Figure 6 shows Dynamic SC-BSP can reduce up to three-fourth of the response time on relatively small graphs. Degree threshold. The experiments in Figure 7 verify the choice of the threshold to determine whether a vertex is managed by PreSeparator or PostSeparator . The result shows that the minimum message size is achieved when the threshold is equal to the number of workers, which is the default setting in our system.
This section discusses existing literature on graph-parallel computing systems, with a focus on partitioning methods of graphs and corresponding communication models of mes-sages. As real-life graphs are often too huge to be loaded into the memory of single machines, graph-parallel comput-ing systems divide the graph into partitions. Every worker loads a partition of the graph into its own memory, and communicates with the other workers to execute a parallel graph algorithm. Next, we organize the related work into two categories according to their partitioning methods. Parallel BGL [10] is a generic C++ library for research. It implements 1D partitioning method on Boost Graph Li-brary, and uses MPI for communication in a BSP model.
Pregel [18] is currently the most popular large graph pro-c essing framework. In Pregel, master is primarily respon-sible for coordinating workers, and also participates in the computation of aggregators , which is a mechanism for global communication. GPS [24] extended aggregator to a master compute mechanism, which was later adopted by Giraph [1]. In Giraph X  X  API, the MasterCompute class has a compute() function, in which users are able to read and change aggregated values from the previous superstep; ad-ditionally, each aggregator is assigned to one of the workers such that master does not have to perform any aggregation. [25] proposed to monitor the size of the subgraph of active vertices. When the active subgraph becomes small enough, it is sent to the master to perform the remaining computa-tion serially. [25] also discusses the optimization techniques including merging vertices to supervertices, cleaning edges on demand, etc.

It has been noted that the vertex-centric BSP model is unsuitable for the graph applications that require coordina-tion, e.g., graph coloring. Lately, Giraphx [30] presented a modification to the BSP model to meet the aforementioned requirement by categorizing vertices as border and internal vertices. The modification allows the direct read to worker X  X  memory for internal vertices. For border vertices, additional synchronization is implemented by adopting two alternative coordination mechanisms, e.g., dinning philosophers and to-ken ring.

The vertex-centric BSP model is easy to program, but sacrifices the flexibility of a vertex being able to access the other vertices in the same worker. Two recent studies, Gi-raph++ [32] and VB-Partitioner [14], exploited the idea of sharing information in the same worker to improve efficiency. In particular, vertices in the same worker are processed in a compute() function together. Remarkable performance improvements can be achieved by carefully designing the al-gorithms, with, however, a trade-off of the ease-of-use.
High-degree vertices in a single worker are likely impose a bottleneck when processing power-law graphs, which are commonly seen in real world. The PowerGraph [9, 16] pro-posed to use a randomized edge partitioning approach to balance workloads. As the outgoing edges of each vertex are stored across workers, a synchronization mechanism aggre-gates the vertex values across all workers for all vertices in the graph. Since the aggregation is an all-to-all collective communication and the size of aggregators is proportional to the number of vertices, the scale-out capability to the number of computing nodes is hence limited.

Among others, Trinity [29] and Facebook X  X  TAO [4] are data models for serving online queries on a large-scale graph. These systems answer graph queries with low read latency and high read availability. The applications of Trinity and TAO are quite different from the other systems which focus on high throughput offline processing. In contrast to the BSP models, [12] proposed an asynchronous model for graph processing. A comprehensive performance comparison and analysis can be found in [26]. 2D partitioning was first proposed in [37] and adopted by [7, 33]. In 2D partitioning, the outgoing edges of a set of vertices are collectively owned by all the computing nodes in a row of mesh networking. We employ a level-synchronous BFS algorithm to describe how graph algorithms run on 2D partitioning. A 2D-BFS algorithm contains two steps in each superstep: (1) Expand : Every computing node sends the current frontier of vertices that it owns to other computing nodes in the same column as an AllGather operation, and inspects the outgoing edges of vertices which belong to the gathered frontier set; and (2) Fold : Every computing node exchanges newly-discovered vertices with the other comput-ing nodes in the same row, and constructs the new frontiers for the next superstep.

The 2D solution handles load-balancing and scalability with a trade-off of communication cost. Indeed, the com-munication cost of the Fold step is equal to 1D partition-ing X  X  total communication cost in terms of total number of messages. The communication cost of the Expand step is the price we pay extra for load-balancing and scalability. Due to lacking torus networking and the a high-performance implementation of AllGather operation, deploying these al-gorithms on commodity clusters and cloud computing in-frastructures can lead to inefficiency in the form of expen-sive communication. Moreover, implementing 2D graph al-gorithms requires great human effort, conflicting with the  X  X ase-of-use X  concept of big data analytics.

Note that there is another line of research on graph com-putation on MapReduce. This category of systems, such as Pegasus [13], Cloud9 [15], Pig Latin [19], Hive [31] and SGC [21], rely on Map and Reduce tasks for graph process-ing. MapReduce is often ill-suited for graph computation problems [18], and consequently can lead to suboptimal per-formance and usability issues. Therefore, MapReduce-based systems are beyond the scope of this paper.
We introduce a parallel system called GraphSlice for large-scale graph processing on commodity clusters and cloud computing infrastructures. The major contribution of this system is enabling high parallelism and scale-out capabil-ity for graph processing in real-world applications, where graphs typically have power law degree distributions. The other usability aspects such as the easy-to-use API, fault-tolenrance, recovery and portability make GraphSlice easily configurable and usable on an existing Hadoop cluster. The performance, scalability and scale-out capability of Graph-Slice have been experimentally demonstrated, which is able to meet the requirements of large-scale graph processing. [1] Apache giraph: http://giraph.apache.org/. [2] The graph 500 list,http://www.graph500.org/. [3] A. Z. Broder, R. Kumar, F. Maghoul, P. Raghavan, [4] N. Bronson, Z. Amsden, G. Cabrera, P. Chakka, [5] D. Chakrabarti, Y. Zhan, and C. Faloutsos. R-mat: A [6] E. Chan, M. Heimlich, A. Purkayastha, and R. van de [7] F. Checconi and F. Petrini. Massive data analytics: [8] M. Faloutsos, P. Faloutsos, and C. Faloutsos. On [9] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and [10] D. Gregor and A. Lumsdaine. The parallel bgl: A [11] T. Gunarathne, J. Qiu, and D. Gannon. Towards a [12] M. Han and K. Daudjee. Giraph unchained: [13] U. Kang, C. E. Tsourakakis, and C. Faloutsos. [14] K. Lee and L. Liu. Efficient data partitioning model [15] J. Lin and C. Dyer. Data-Intensive Text Processing [16] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [17] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [18] G. Malewicz, M. H. Austern, A. J. C. Bik, J. C. [19] C. Olston, B. Reed, U. Srivastava, R. Kumar, and [20] J. Pjesivac-Grbovic, T. Angskun, G. Bosilca, G. E. [21] L. Qin, J. Yu, L. Chang, H. Cheng, C. Zhang, and [22] M. Redekopp, Y. Simmhan, and V. K. Prasanna. [23] S. Redner. How popular is your paper? an empirical [24] S. Salihoglu and J. Widom. Gps: A graph processing [25] S. Salihoglu and J. Widom. Optimizing graph [26] N. Satish, N. Sundaram, M. M. A. Patwary, J. Seo, [27] S. Seo, E. J. Yoon, J. Kim, S. Jin, J.-S. Kim, and [28] H. Shang and M. Kitsuregawa. Efficient breadth-first [29] B. Shao, H. Wang, and Y. Li. Trinity: A distributed [30] S. Tasci and M. Demirbas. Giraphx: Parallel yet [31] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, [32] Y. Tian, A. Balmin, S. A. Corsten, S. Tatikonda, and [33] K. Ueno and T. Suzumura. 2d partitioning based [34] L. G. Valiant. A bridging model for parallel [35] T. White. Hadoop -The Definitive Guide: Storage and [36] R. S. Xin, J. E. Gonzalez, M. J. Franklin, and [37] A. Yoo, E. Chow, K. W. Henderson, W. M. III, [38] M. Zaharia, M. Chowdhury, M. J. Franklin,
