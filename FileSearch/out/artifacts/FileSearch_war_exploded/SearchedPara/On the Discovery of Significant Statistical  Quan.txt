 In this paper we study market shar e rules, rules that have a certain market share statistic associated with them. Such rules are particularly relevant for decision making from a business perspective. Motivated by market share rules, in this paper we consider statistical quantitative rules (SQ rules) that are quantitative rules in which the RHS can be any statistic that is computed for the segment satisfying the LHS of the rule. Building on prior work, we present a statistical approach for learning all significant SQ rules, i.e., SQ rule s for which a desired statistic lies outside a confidence interval computed for this rule. In particular we show how resampling technique s can be effectively used to learn significant rules. Since our method considers the significance of a large number of rules in parallel, it is susceptible to learning a certain number of "fal se" rules. To address this, we present a technique that can de termine the number of significant SQ rules that can be expected by chance alone, and suggest that this number can be used to determ ine a "false discovery rate" for the learning procedure. We apply our methods to online consumer purchase data and report the results. H.2.8 [ Database Management ]: Database Applications -Data Mining. Algorithms, Management. Rule discovery, market share rule s, statistical quantitative rules, nonparametric methods, resampling. Rule discovery is widely used in data mining for learning interesting patterns. Some of the early approaches for rule learning were in the machine learning literature [11, 12, 21]. More recently there have been many algorithms [1, 25, 28, 31] proposed in the data mining literature, most of which are based on the concept of association rules [1]. While all these various approaches have been successfully used in many applications [8, 22, 24], there are still situations that these types of rules do not capture. The problem studied in this paper is motivated by market share rules, a specific type of rule that cannot be represented as association rules. Informally, a market share rule is a rule that specifies the market share of a product or a firm under some conditions. The results we report in this paper are from real user-level Web browsing data provided to us by comScore Networks. The data consists of browsing behavior of 100,000 users over 6 months. In addition to customer specific attributes, two attributes in a transaction that are used to compute the market share are the site at which a purchase was made a nd the purchase amount. Consider the example rules below that we discovered from the data: The market share for a specific site, e.g. Expedia.com, is computed as the dollar value of f light ticket purchases (satisfying the LHS of the rule) made at E xpedia.com, divided by the total dollar value of all flight ticket purchases satisfying the LHS. The discovered rules suggest that Expedia seems to be doing particularly well among the single households in the North East region (rule 2), while it cedes some market in the segment of teenagers (rule 4). Rules such as these are particularly relevant for business since they suggest natural actions that may be taken. For example, it may be worth inves tigating the higher market share segments to study if there is so mething particularly good that is being done, which is not being done in the lower market share segments. More generally,  X  X arket share X  is an example of a statistic that is computed based on the segment satisfying the antecedent of the rule. Besides market share, vari ous other quantitative statistics on the set of transactions satisfying the LHS of a rule can be computed, including mean and variance of an attribute. Prior work on learning quantitative association rules [2, 33] studied the discovery of rules with statistics such as the mean, variance, or minimum/maximum of a single attr ibute on the RHS of a rule. In this paper we generalize the structure of the rules considered in [2] to rules in which the RHS can be any quantitative statistic that can be computed for the subset of data satisfying the LHS. This statistic can even be computed based on multiple attributes. We term such rules as statistical quantitative rules ( SQ rules ). With respect to learning SQ rules from data, we formulate the problem as learning significant SQ rules that have adequate support. We define an SQ rule to be significant if the specific statistic computed for the rule lies outside a certain confidence interval. This confidence interval represents a range in which the statistic can be expected by chance alone . This is an important range to identify if the rules discovered are to be interpreted as suggesting fundamental relations hips between the LHS and the market share. For example, by chance alone if it is highly likely that the market share of Expedia is between 25% and 30% for any subset of data, then it is not at all clear that the rule relating income and Expedia X  X  market share (rule 1 in the example) is identifying a fundamental relationship between income and the market share. While prior work [6, 9] has used confidence intervals to identify significant rules, most of these approaches are either parametric or specific for binary data. Building on prior work in this paper we present a statistical approach for learning significant SQ rules that is entirely non-parametric. In particular we show how resampling techniques, such as pe rmutation, can be effectively used to learn confidence intervals for rules. Based on these confidence intervals, significant ru les can be identified. However, since our method considers the significance of a large number of rules in parallel, for a given significance level it is susceptible to learning a certain number of false rules. To address this we present an intuitive resampling technique that can determine the number of false rules, and argue th at this number can be used to determine a "false discovery rate " for the learning procedure. The practical significance of this approach is that we learn significant SQ rules from data and specify what the false discovery rate exactly is. The paper is organized as follows. We first define SQ rules in the next section. Section 3 presen ts an algorithm for computing confidence intervals and Section 4 presents an algorithm for learning significant SQ rules. In Section 5 we explain how the false discovery rate for our approach can be computed. We present detailed experimental resu lts on real web browsing data in Section 6 followed by a literature review and conclusions. In this section we define SQ ru les and significant SQ rules. Let describe segments and B = { B 1 , B 2 ,..., B m } be another set of attributes that will be used to compute various statistics that describe the segment. Let dom ( A i ) and dom ( B of values that can be taken by attribute A i and B j respectively, for any A i  X  A and B j  X  B . Let D be a dataset of N transactions where each transaction is of the form { A 1 = a 1 , A 2 = a b , B 2 = b 2 ,..., B m = b m } where a i  X  dom ( A i an atomic condition be a proposition of the form value value 2 for ordered attributes and A i = value for unordered attributes where value, value 1 , value 2 belong to the finite set of discrete values taken by A i in D . Finally, let an itemset represent a conjunction of atomic conditions.
 Definition 2.1 (SQ rule) . Given (i) sets of attributes A and B , (ii) a dataset D and (iii) a function f that computes a desired statistic of interest on any subset of data, an SQ rule is a rule of the form: where X is an itemset involving attributes in A only, D subset of D satisfying X , the function f computes some statistic from the values of the B attributes in the subset D X , and support is the number of transactions in D satisfying X .  X  Note that the statistic on the RHS of the rule can be computed using the values of multiple a ttributes. The following examples are listed to demonstrate different types of rules that an SQ rule can represent. For ease of expos ition we use the name of the desired statistic in the RHS instead of referring to it as f ( D 1. Quantitative association rules [2]: population -subset  X  mean or variance values for the subset (2.2) Quantitative association rules are a popular representation for rules in the data mining literature in which the RHS of a rule represents the mean or variance of some attribute. Example: Education = graduate  X  Mean ( purchase ) = $15.00. (2.2) is a special case of (2.1), where f ( subset ) is the mean of some attribute B in the subset of data. 2. Market share rules : MSV (Market Share Variable) is a special categorical attribute for which the market share values are to be computed. P is a special continuous variable that is the basis for the market share computation for MSV . For example, each transaction T represent a book 2 purchased online. A 1 through A attributes of the customer who makes the purchase, such as income, region of residence and household size. For each transaction, MSV is the variable indicating the online book retailer where the purchase was made. dom(MSV) may be {Amazon, Barnes&amp;Noble, Ebay} and P is the price of the book purchased. For a specific v  X  dom(MSV) a market share statistic can be computed as described below. Market share rules have the following form: where X is an itemset consisting of attributes in { A and marketshare v is a statistic that represents the market share of a specific v  X  dom(MSV) . This is computed as follows. Let D represent the subset of transactions satisfying X and D represent the subset of transactions satisfying ( X  X  MSV = v ). Then marketshare v is computed as sum ( P , D X, MSV=v where sum ( P , D ) is the sum of all the values of attribute P in the transactions in D . Market share rules naturally occur in various applications, including online purchases at various Web sites, sales applications, and knowledge ma nagement applications. The examples presented in the introduc tion are real market share rules discovered in online purchase data. The following additional examples illustrate the versatility and usefulness of market share rules.  X  Within a specific product categor y (e.g. shoes) Footlocker  X  Consider a dataset of patents associated with some area (e.g. Definition 2.1 differs from the definition of quantitative rule [2, 33] as follows. First, it is not limited to mean and variance statistics and assumes a much broa der class of statistics, including the market share statistics. Sec ond, unlike quantitative rules, the statistic of interest in the RHS of a rule can be computed based on multiple attributes. Definition 2.2 (Significant SQ rule) . For a given significance level  X   X  (0, 1), let ( stat L , stat H ) be the (1  X   X  ) confidence interval for a desired statistic, where this confidence interval represents the range in which the statistic can be expected by chance alone. An SQ rule X  X  f(D X ) = statistic, support = sup is significant if statistic lies outside the range ( stat L , stat H ).  X  The main objective of this paper is to discover all significant SQ rules. The first challenge in learning significant SQ rules is in constructing a confidence interval for the desired statistic such that this interval represents a ra nge of values for the RHS statistic that can be expected by chance alone. In the next section we present an algorithm for learning these confidence intervals. The first question that needs to be addressed is what is meant by  X  X  range for the statistic that can be expected by chance alone X . In this section we start by addressing this question and outline a procedure by which such a range can be computed. Next we will point out the computational cha llenge in implementing such a procedure for learning these intervals for several SQ rules and then outline three observations that will substantially help address the computational problems. Ba sed on these observations we present a resampling-based algorithm for computing the confidence intervals. For a given SQ rule, the desired confidence interval theoretically represents the range in which the statistic can be expected when there is no fundamental relationship between the LHS of the rule and the statistic. More precisely, since the statistic is computed from the values of the B attributes, the confidence interval represents the range in which the statistic can be expected when the A attributes are truly independent of the B attributes. Without making any parametric di stributional assumptions, such a confidence interval can be generated using the classical non-parametric technique of permuta tion. Indeed permutation-based approaches have been commonly used to generate confidence intervals in the statistics literature [16]. If R is the set of all attributes in a dataset, the basic idea in permutation is to create multiple datasets by randomly permuting the values of some attributes R i  X  R . Such a permutation would create a dataset in which R i is independent of ( R  X  R i ), but would maintain the distributions of R i and ( R  X  R i ) in the permutation dataset to be the same as the distributions of these a ttributes in the original dataset. Table 3.1 illustrates one example of a permutation dataset D  X  in which the B attributes are randomly permuted. Since a desired statistic can be computed on each permutation dataset, a distribution for the statistic can be computed based on its values from the multiple permutation datasets. A confidence interval can then be computed from this distribution. Original dataset D : Permutation dataset D  X  :
A As mentioned above, this is a commonly used procedure in non-parametric statistics. The reason th is procedure makes sense is as follows. Even if there is a relati onship between the LHS of an SQ rule and the statistic on the RHS, by holding the A attributes fixed and randomly re-ordering the values of the B attributes the relationship is destroyed and the A attributes and B attributes are now independent of each other. Repeating this procedure many times provides many datasets in which the A attributes and B attributes are independent of each other, while maintaining the distributions of the A and B attributes to be the same as their distributions in the original data set. The values for the statistic computed from the many permutation datasets is used to construct a distribution for the statistic that can be expected when the A attributes are truly independent of the B attributes. Specifically, for the same itemset X , compare the following two SQ rules in D and D  X  , First note that the supports of the rules are the same since the number of records satisfying X in the permutation dataset is the same as the original dataset. We will use this observation to build a more efficient method for computing confidence intervals shortly. A confidence interval for the rule in (3.1) can be computed using the following na X ve procedure. 1. Create permutation dataset D  X  from the original dataset D 2. Repeat step 1 N perm &gt; 1000 times 3 , sort all the N 3. The (1  X   X  ) confidence interval for the SQ rule in Equation Computing these confidence intervals for multiple candidate SQ rules creates several computational problems which we will address in this section. For exam ple, if we need to test 10,000 potential significant rules (which is a reasonably small number for data mining tasks), then we would need to repeat the above steps 10,000 times, and this means gene rating permutation datasets each permutation dataset. The following observations substantially reduce the computational complexity of the procedure. 1. Sampling can be used instead of creating permutation datasets . For the SQ rule in Equation (3.1), computing stat permutation dataset is really equivalent to computing stat on a random sample of sup D records in D . This is the case since none of the A attributes play a role in the computation of the statistic. Permuting the dataset, identifying the ( sup where X holds, and then computing th e statistic on this subset achieves the same effect as picking a random sample of sup records from D and computing the statis tic on this random subset. Hence to determine the confidence interval for the SQ rule in Equation (3.1), instead of permuting the dataset N enough to sample sup D records from D for N perm times. 2. Some of the candidate SQ rules have the same support values as other rules . Based on this observation, confidence intervals for two SQ rules with the same support can be approximated by the same interval. This is the case sin ce for a given rule the interval is generated by sampling sup D records many times and if another rule has support = sup D then the interval for that rule will be similar if the same procedure is re peated (it will not be exactly the same because of randomization). Therefore, fewer confidence intervals need to be generated. 3. It is adequate to generate a fixed number of intervals, independent of the number of rules considered . We observe that the interval for an SQ rule with support = sup approximated by an interval computed by sampling sup where sup E is  X  X easonably close X  to sup D . This is a heuristic that we use to considerably reduce the complexity of the procedure. Denote N Rule as the number of rules to be tested. If all rules have different support values, we need to construct N Instead, we would construct a fixed number N dist such that for rule  X  X  X  f ( D X ) = statistic , support = sup  X , statistic is compared with the distribution that is constructed by sampling the closest number of transactions to sup . This heuristic is more meaningful when we consider support in terms of percentage of transactions satisfying LHS of a rule, which is a number between 0 and 1. Based on the above observations, we present in Figure 3.1 algorithm CIComp for constructing N dist distributions and determining the (1  X   X  ) confidence intervals for a given significance level. In the above algorithm, N dist , N perm , and  X  are user-defined parameters.  X  is usually chosen to be 5%, 2.5% or 1%. For N and N perm , the larger they are, the more precise the distributions will be. Let N = 1000, N dist = 100, N perm = 999, and  X  = 5%. We use these numbers as an example to explain the algorithm. For step 2, the first distribution corresponds to N sample = 1/100  X  1000 = 10 transactions. Step 3 to 6 computes N 999 statistics for 10 randomly samp led transactions from dataset D . Then we sort these 999 statistics and pick  X  /2 and 1  X   X  /2 distribution, as the lower and upper thresholds for the (1  X   X  ) confidence interval. Steps 2 through 9 are repeated N times to get the desired number of distributions and confidence intervals. The computation complexity of the algorithm in Figure 3.1 is O( N  X  N perm  X  N dist ), whereas the complexity of na X ve method is O( N  X  N perm  X  N rule ). Note that N dist can be fixed to a reasonable small number, e.g. 100, whereas N rule is the number of rules that are being tested and can easily be or ders of magnitude more than N Given the distributions and conf idence intervals, discovering all significant statistical rules is straightforward. Algorithm SigSQrules is presented in Figure 4.1. Given N dist distributions constructed from the algorithm CIComp, we use the above algorithm to di scover all significant SQ rules. We continue to use the example N = 1000, N dist = 100, and N 999 to describe the steps in Figure 4. 1. Note that the attributes in A represent the attributes in the da taset that are used to describe segments for which statistics can be computed. Step 1 uses any large itemset generation procedure in rule discovery literature to generate all large itemset s involving attributes in A . The exact procedure used will depend on whether the attributes in A are all categorical or not. If they are, then Apriori algorithm can be used to learn all large itemsets. If so me of them are continuous then other methods such as the ones described in [31] can be used. Step 4 computes the statistic function for each large itemset, x . In step 5, we find out which dist ribution is to be used for significance test. For example, if support ( x ) = 23, then be round (2.3) = 2. We would compare x.stat with its corresponding confidence interval ( LowerCI [2], UpperCI [2]) in step 6. If x.stat is outside of the confidence interval, the rule is significant, and we use step 7 to calculate its 2-side p -value. If q %). The p -value is not only a value to understand how significant a rule is, but is also useful for determining the false discovery rate in Section 5. Note that the confidence interval used to test significance of a rule is approximate since we do not compute this interval for the ex act value of the support of this rule. Instead we use the closest interval (which was pre-computed as described in Section 3.2) co rresponding to this support value. In future research we will quantify the effects of this approximation. We would also like to point out th at in many cases (see below) the computation of the statistic can be done efficiently within the itemset generation procedure ( largeitems ) itself. This can be used to modify the algorithm to make it more efficient once a specific itemset generation procedure is used. This is the case if the function f that computes the statistic on transactions T is a recursive function on s , that is, Many statistics, such as mean and market share , are recursive. For example, Mean ( T 1 , T 2 ,..., T s ) = [ Mean ( T 1 , T Mean ( T s )] / s . In this section we presented an algorithm SigSQrules for generating significant SQ rules. However, as mentioned in the introduction, for any given level of significance for a rule, the fact that thousands of rules are eval uated for their significance makes it possible to discover a certain number of false rules. This is the well known multiple hypothesis testing problem [4]. While it is difficult to eliminate this problem , it is possible to quantify this effect. In the next section we discuss the problem of false discovery in detail and present an algorithm for determining the false discovery rate associated with the discovery of significant SQ rules. As mentioned above, when multiple rules are tested in parallel for significance, it is possible to learn a number of  X  X alse X  rules by chance alone. Indeed, this is a pr oblem for many rule discovery methods in the data mining literature. The false discovery rate ( FDR ) is the expected percentage of false rules among all the discovered rules. Prior work in st atistics has taken two approaches to deal with the multiple hypothesis testing problem [4, 17, 34]. One approach attempts to lower the false discovery rate by adjusting the significance level at which each rule is tested. As we will describe below, this approach is not suitable for data mining since it will result in very few ru les being discovered. The second approach assumes that a given num ber of false discoveries should be expected, and focuses on estim ating what the false discovery rate ( FDR ) exactly is. This is more us eful for data mining, since it permits the discovery of a reasona ble number of rules, but at the same time computes a FDR that can give users an idea of what percentage of the discovered rules ar e spurious. In this section, we first review key ideas related to the multiple hypotheses testing problem and then present a nonparametric method to determine false discovery rate for our procedure. For significance tests for a single rule, the significance level  X  is defined as the probability of discovering a significant rule when the LHS and RHS of the rule are actually independent of each other; in other words,  X  is the probability of a false (spurious) discovery. For example, on a random dataset where all attributes are independent, if we test 10,000 rules, then by definition of  X  , we expect 10,000  X  5% = 500 false discoveries by pure chance alone. When some of the attributes are dependent on each other, as is the case for most datasets on which rule discovery methods are used, the above approach cannot be used to get an expectation for the number of false rules. In such cases, two approaches are possible. In statistics, a measur e called familywise error rate ( FWER ) is defined as the probability of getting at least one false rule output. Most conventional appr oaches in statistics that deals with the multiple hypotheses testing problem use different methods to control FWER by lowering significance level for individual rule,  X  ind . For example, Bonferroni-type procedures would have  X  ind =  X  / the number of rules tested, which is 5% / 10,000 = 5  X  10 -6 . However, when the number of hypotheses tested is large (as is the case in data mining algorithms), extreme low  X  value, e.g. 5  X  10 -6 , will result in very few rules discovered. The other type of approach, as taken recently in [4] estimates the false discovery rate ( FDR ), the expectation of the proportion of false discoveries in all discoveries. LHS independent of RHS a b LHS dependent on RHS c d In Table 5.1, the number of rules tested is ( a + b + c + d ), out of which ( a + b ) is the number of rules where the LHS of the rules is truly independent of the RHS, and ( c + d ) is the number of rules where there is a real relationship between the LHS and the RHS of the rules. The columns determine how many tested rules are output as significant or non-significant. The two terms FDR and FWER can be defined precisely as FDR = Exp ( b / b + d ) and FWER = Prob ( b &gt;0). We adopt FDR estimation in this section because it effectively estimates false discove ries without rejecti ng too many discovered rules. However, the method proposed in the literature [4, 7, 35] for FDR cannot be used for large scale rule discovery because of the following two reasons: first, th e assumption that statistics of the rules tested are independent from each other (which some of the approaches use) is not true. For example, rules A independent. In fact a large number of rules are related to each other in rule discovery because their LHS share common conditions and RHS come from th e same attributes. Second, methods in statistics draw conc lusions based on the number of rules tested (= a + b + c + d ), however, as indicated in [25], a and c are unknown values due to the filtering by support constraint. Without making any assumptions , below we present another permutation-based method to estimate the FDR for our procedure for learning significant SQ rules. Denote N sig (  X  ) to be the number of significant rules discovered from dataset D when the significant level =  X  . In Table 5.1, N by keeping the values in attributes A intact and permuting the B attributes, we get a permutation dataset D  X  . Since we remove any relationship between A and B attributes by this procedure, all the LHS and RHS statistic of each rule tested in D  X  are independent. If we apply the significant rule discovery algorithm SigSQrules , the number of significant rules discovered from D  X  when the significant level =  X  will be one instance of false discovery, that permutation datasets, we can estimate the expectation of the number of false discoveries and thus compute a false discovery how FDR (  X  ) can be estimated in detail in the Appendix. In this section, we described the problem of multiple hypotheses testing and pointed out that for any given significance level a certain number of significant SQ rules will be discovered by chance alone. We then described an intuitive permutation based procedure to compute the false discovery rate. From a practical point of view the procedure described above can be used in conjunction with SigSQrules to discover a set of significant SQ rules and provide a number representing the percentage of these rules that are likely to be spurious. In this section we present resu lts from learning significant market share rules, a specific type of SQ rules. We started with user-level online purchase data gathered by comScore Networks, a market data vendor. The data consist of 100,000 users X  online browsing and purchase behavior over a period of six months. The market data vendor tracks all online purchases explicitly by parsing the content of all pages delivered to each user. Starting from the raw data we created a dataset of purchases where each transaction represents a purchase made at an online retailer. Attributes of the transaction include user demogr aphics, the site at which the purchase was made, the prim ary category (e.g. books, home electronics etc) of the site, th e product purchased and the price paid for the product. Within a specific category, e.g. books, significant market share rules would be particularly interesting to discover. We selected many data sets with purchases belonging to each specific category and applied our method to learn several interesting significant market share rules. For space limitations we do not present all the results, but report the results for learning market share rules for the top three retailers in the online book industry. Specifically the dataset c onsists of all transactions in which a book was purchased at any site and we use the methods presented in the paper to learn market share rules for the top 3 sites  X  Amazon.com, Barnes&amp;Nobl e and Ebay. The total number of transactions was 26,439 records and we limit the rules to having at most five items on the LHS of a rule. Among the most significant market share rules (as determined by the p -values of these rules), we picked four rules to list that were particularly interesting for each online retailer. Rule (4) for Amazon.com indicates that it is doing particularly well in households with middle-aged heads that have broadband access. The market share for Amazon.com in this segment lies significantly outside the confidence interval computed for the rule. On the other hand, rule (1) for Barnesandnoble.com shows that they are doing poorly selling to a segment which perhaps represents well educated couples . Given that this is a large segment (support = 6%), this rule suggests that they could try and examine why this is the case and how they can achieve greater penetration in this segment. In Eb ay X  X  case, all four rules are very interesting. Rule (4) indicates th at they have high market share among teenagers, while rule (3) describes a segment they clearly have trouble penetrating. For ma ny other categories too (travel and home electronics in particular) the significant SQ rules that we learned were highly interesti ng. As these examples suggest, these rules can be insightful, identify interesting segments and have significant business potential. To test how the methods perform as the minimum support and significance levels vary, for one s ite we generated significant SQ rules for many values of the minimum support and significance level parameters. Figures 6.1 a nd 6.2 show how the number of significant rules and the false disc overy rate vary with support. As the minimum support threshold is lowered the number of significant SQ rules discovered increases. However the FDR increases as the support thres hold is lowered, suggesting a tradeoff between discovering many significant rules while keeping the FDR low. A practical outcome is that it may be desirable to have higher minimum supports (to keep FDR low), but not too high that very few ru les are discovered. Figures 6.3 and 6.4 illustrate a similar tradeoff for the significance level parameter. As  X  decreases FDR is lower, but this results in fewer number of significant rules be ing discovered. Again, the implication is that it may be desirable to have a low  X  (to keep FDR low) but not too low that very few rules are discovered. Based on this general tradeoff we chose minimum support of 2% and chose an  X  of 2.5% in order to report summary results for the three sites. Table 6.1 summarizes the number of significant rules discovered and the false discovery rates of the procedure. As the values in the table and the exam ples above show, our procedure can be used effectively to learn a good set of significant SQ rules while keeping the false discovery rates reasonable. In this section we first presented compelling examples of rules discovered that illustrate the potential of learning significant market share rules. We then examined how the number of significant rules discovered and th e false discovery rate changes with the support and significance level (  X  ) parameters. The results of this analysis suggested a tradeoff between generating significant rules and keeping the fa lse discovery rate low. Based on this tradeoff we identified a specific value of the support and significance parameters and s howed the number of rules discovered for these values. We compare our work with the literature based on three aspects: rule structure, rule significance, and methodology. Rule structure . Rule discovery methods on a quantitative dataset can be traced back to [29], where rules of the form x 1 &lt; A &lt; x y &lt; B &lt; y 2 are discovered. [31] extends the structure to be conjunctions of multiple conditions on both antecedent and consequent of a rule, and propos es their discovery method based on the Apriori algorithm [1]. Although rules in [31] are important, partitions like y 1 &lt; B &lt; y 2 for continuous attributes on the RHS of a rule only gives partial descrip tion of the subset satisfying the LHS of the rule and partial descriptions sometimes are misleading. Observing this problem, [2] introduces a new structure where the consequent of a rule is Mean ( D Variance ( D X ) to summarize the behavior of the subset satisfying the antecedent. [33] further extends the form of the consequent of the rule, such that it can be of Min ( D X ), Max ( D Our rule structure is based on prior work: the antecedent is conjunctions of conditions, while the consequent can be any aggregate function f on multiple attributes to describe the behavior of the subset satisfying the antecedent. Rule significance . Any combination of attributes with conditions can potentially form a rule. Researchers use different measurements, e.g. support and confidence, to select only important rules from all possible rules. Based on the support and confidence framework, many metric s have been developed, such as gain [15], conviction [10], unexpectedness [27]. Although these metrics can be generalized to rules where the antecedent and consequent are both conjunctions of the form value 1 &lt; Attribute &lt; value 2 for quantitative datasets, they are not applicable for rules whose consequent is a function, such as Mean ( D X ), or in general, f ( D
X ). To solve this non-trivial problem, we use statistical significance tests to evaluate rules, so that the consequent of a rule is not expected by chance alone. In the data mining literature, statistical significance tests are commonly used in many applications. For example, chi-square (  X  2 ) is a statistic to test correlations between attributes in binary or categorical data, and it has been applied to discover corre lation rules [9], actionable rules [23], and contrast sets [3, 32]. Fo r sparse data, [35, 36] employ Fisher X  X  Exact Test to detect anomaly patterns for disease outbreaks. As mentioned in Secti on 3, these two tests are special cases of our significance test when we apply our significance definition to categorical data. For quantitative rules in [2], the authors use a standard Z-test to determine the significance of inequality of means between a subset D X and its complement D  X  D . [33] defines a new measurement, impact , to evaluate quantitative rules, where impact can identify those groups that contribute most to some outcome, such as profits or costs. For areas other than rule discovery, standard Z-tests with log-linear models is used in Exploratory Da ta Analysis for OLAP data cubes [30]. Our significance test is different from the above primarily because (i) our significance definition is applicable to any user-defined aggregate function f ( D X ), and (ii) we using nonparametric methods to construct distributions and confidence intervals, in which f ( D X ) is expected from random effects alone. Methodology . Nonparametric statistics is philosophically related to data mining, in that bot h methods typically make no assumptions on distributions of data or test statistics. Even with known distribution of a statis tic, nonparametric methods are useful to estimate parameters of the distribution [13]. Nonparametric methods are widely used on testing models that are built from data: as earliest in [18], the author uses randomization tests to tackle a model overfitting problem; [20] compares bootstrap and cross-validation for model accuracy estimation; for decision trees, [14, 26] use permutation tests to select attributes based on 2  X  2 contingency tables. Rule discovery is to learn local features, which is inherently different from models. Although we have seen methods using parametric hypothesis testing approach to l earning rules from dataset [5, 6], no prior work has been found on discovering large number of rules based on nonparametric significance tests. The problem of multiple hypothesis testing/multiple comparison is well known in rule discovery, a good review of which can be found in [19]. On sparse binary data, [25] shows that with proper support and confidence control, very few false rules will be discovered. However, rule discovery on quantitative data faces much more complicated ch allenges, and conventional p -value adjustment methods cannot be dir ectly applied. To solve this problem, we employ false discovery rate [4] metric to estimate the number of false rules discovered due to testing a large number of rules. In data mining, FDR has b een shown useful in [7, 36] for categorical data with known num ber of hypotheses, and we extend it to quantitative rules with resampling methods. In this paper we defined a new cat egory of rules, SQ rules, and the significance of SQ rules, on quantitative data. Then we presented a permutation-based algorithm for learning significant SQ rules. Furthermore, we show how an explicit false discovery rate can be estimated for our procedure, which makes the approach useful from a practical perspective. We presented experiments in which we discovered market share rules, a specific type of SQ rules, in real online purchase datasets and demonstrated that our approach can be used to learn interesting rules from data. We would also like to point out that it is possible to compute the false discovery rate ( FDR ) for several possible significance levels in an efficient manner (without cr eating permutation datasets for each significance level). Although a detailed presentation of this is beyond the scope of this paper, in the appendix we provide an overview of how this can be done. One main advantage of being able to do this is that significant SQ rules can be discovered at a chosen significance level that is computed from some desired FDR . Hence rather than just estimating FDR we may be able to discover significant rules given a specific FDR . However this needs to be studied in greater detail in future work. [1] Agrawal, R. and Srikant, R., Fast Algorithms for Mining [2] Aumann, Y. and Lindell, Y., A Statistical Theory for [3] Bay, S. D. and Pazzani, M. J., Detecting Change in [4] Benjamini, Y. and Hochbe rg, Y., Controlling the False [5] Bolton, R. and Adams, N., An Iterative Hypothesis-Testing [6] Bolton, R. J. and Hand, D. J., Significance Tests for Patterns [7] Bolton, R. J., Hand, D. J., and Adams, N. M., Determining [8] Brijs, T., Swinnen, G., Va nhoof, K., and Wets, G., Using [9] Brin, S., Motwani, R., and Silverstein, C., Beyond Market [10] Brin, S., Motwani, R., Ullman, J. D., and Tsur, S., Dynamic [11] Clark, P. and Niblett, T., The Cn2 Induction Algorithm, [12] Clearwater, S. and Provost, F., Rl4: A Tool for Knowledge-[13] Efron, B. and Tibshirani, R. J., An Introduction to the [14] Frank, E. and Witten, I. H., Using a Permutation Test for [15] Fukuda, T., Morimoto, Y., Mo rishita, S., and Tokuyama, T., [16] Good, P., Permutation Tests: A Practical Guide to [17] Hsu, J. C., Multiple Comparisons -Theory and Methods . [18] Jensen, D., Knowledge Discovery through Induction with [19] Jensen, D. and Cohen, P. R., Multiple Comparisons in [20] Kohavi, R., A Study of Cross-Validation and Bootstrap for [21] Lee, Y., Buchanan, B. G ., and Aronis, J. M., Knowledge-[22] Ling, C. X. and Li, C., Da ta Mining for Direct Marketing: [23] Liu, B., Hsu, W., and Ma , Y., Identifying Non-Actionable [24] Mani, D. R., Drew, J., Betz, A., and Datta, P., Statistics and [25] Megiddo, N. and Srikant, R., Discovering Predictive [26] Oates, T. and Jensen, D., Large Datasets Lead to Overly [27] Padmanabhan, B. and Tu zhilin, A., A Belief-Driven Method [28] Padmanabhan, B. and Tuzhilin, A., Small Is Beautiful: [29] Piatesky-Shapiro, G., Disc overy, Analysis, and Presentation [30] Sarawagi, S., Agrawal, R., and Megiddo, N., Discovery-[31] Srikant, R. and Agrawal, R., Mining Quantitative [32] Webb, G., Butler, S., and Newlands, D., On Detecting [33] Webb, G. I., Discovering Associations with Numeric [34] Westfall, P. H. and Young, S. S., Resampling-Based Multiple [35] Wong, W.-K., Moore, A., Cooper, G., and Wagner, M., [36] Wong, W.-K., Moore, A., Cooper, G., and Wagner, M., Let us continue to use the example N perm = 999 and  X  = 5%. On the dataset D , from the algorithm SigSQrules we generate significant rules as well as each rule X  X  p -value. Because there are N perm values in each distribution, the smallest possible p -value from the permutation tests is 1/( N perm + 1) = 0.001, and all possible p -values are S = { 1/( N perm + 1) = 0.001, 2/( N perm = 0.05 }. Let N sig [  X  ind ] be the number of significant rules whose p -value is no larger than  X  ind  X  S . For example, if there are 50 rules whose p -value = 0.001, and 30 rules whose p -value = 0.002, then N sig [0.001] = 50 and N sig [0.002] = 50 + 30 = 80. Without further permutation tests, with N sig [] we know how many rules will be discovered if we lower the significance level from  X  to  X  example, if  X  ind = 0.002, there are only N sig whose p -value is no larger than  X  ind = 0.002, therefore we expect to discover 80 rules. Similarly, for each permutation dataset D  X  , at each significance level  X  ind &lt;  X  we can compute the number of significant rules and their p -values by applying SigSQrules only once. Note that all discoveries from D  X  are false discoveries, because the relationships between A and B are removed. Let N be the number of discoveries from permutation datasets D  X  [ i ]. For example, N sig-perm [1][0.002] = 20 means we have 20 discoveries from the permutation dataset D  X  [1] at  X  ind = 0.002. We implement this procedure on multiple permutation datasets, and Median ( N significance level  X  ind on permutation datasets. Therefore, FDR (  X  ind ) = Median ( N sig-perm [][  X  ind ]) / N to estimate the expectation, wh ich conforms to nonparametric statistical considerations (med ian is the best estimator for expectation when the underlying distribution is unknown). Empirically, we showed in Figure 6.3 that FDR (  X  increasing function on  X  ind . It means that by decreasing  X  can control FDR (  X  ind ) to a smaller value. We are not always guaranteed, though, to be able to set an individual significance level such that FDR &lt; 5%. It is possible that even when we decrease  X  ind to a level that almost no rules are discovered, FDR is still much larger than 5%. In other words, there are always a large proportion of spurious rule s discovered from some datasets. For example, if attributes independent based on a test statistic , levels, and FDR  X  1. We want to point out that this is a desirable property of our method on controlling FDR , because there are many real-world datasets whose attributes are truly independent from each other. Traditional methods cannot estimate how many rules should be discovered, but w ith our technique, we can draw the conclusion that, there is no rule to be discovered because none of the rules is better than chance. This nonparametric method to estimate and control FDR is applicable to quantitative datasets and broad types of rules. 
