 1. Introduction
The World Wide Web has transcended from a read-only to a read-write web. Nowadays, anyone can actively participate in content creation by providing personal opinions, reporting an event, or commenting on a posted article. Information is uploaded in real-time; the minute an event that attracts people's attention occurs, its description becomes available on some (micro) web.

The definition of event depends on context and granularity. Dictionary.com defines an event as gathering or activity [1]. WordNet defines an event as  X  something that happens at a given place and time cognitive agent. In general, one could argue that an event is vague, since the event concept is socially defined, meaning that an event may be important to a group of people and unimportant to others.

In our work, events cover real life as well as web  X  happenings topics (e.g. the Olympic Games and all the games that take place during them). Thus, our methodology can detect any major event that triggers enough discussion in the web. In the case of real life events, we detect their virtual representations, which we we consider as future work the quantitative analysis of the difference between them.

To our understanding, an event is a  X  significant  X  action that has a duration (beginning (legal or physical), and is associated with one or more locations. An event can be described by a topic, which we define as the the mentioned topic/entity occurs in the web.

Having defined the notion of event within the context of our work, we discuss the notion of event identification (or event amount to a new or previously unidentified event. Identification may imply discovering previously unidentified events in an accumulated collection (  X  retrospective identification  X  (  X  on-line identification  X  ). Event identification comprises numerous challenges: one has to integrate information from multiple document streams, extract the spatial and temporal information associated with each document, identify and distinguish possible unknown entities, and classify the event to multiple event types.

The definition of importance is also subjective. Events that may be deemed interesting by many people are often available in various web sources. Although they are authored by different opinions, they all share common features. Documents (articles, news stories, blogposts, comments, tweets, etc.) referring to the same event are reported at time periods close to the actual event. They also contain similar information (topics and named entities) that define the reporting event. We argue that these information snippets are the principal components that indicate events. We define an event as important , if the event has affected enough people to be reported or commented on in the Web. The minimum number of reports (per time unit) can be a tunable threshold and depends on the specific application/domain. In this work, we propose a methodology to detect previously unknown important events , as reported through social media interactions. topic occurrences over some time period. In addition, we propose an efficient unsupervised algorithm that detects the important recognition, dynamic topic map discovery, topic clustering, and peak detection, in order to identify events. Our methodology is work can be summarized in the following: a) We propose a methodology that accurately detects interesting events and augments each event with semantic information pointing out the topic, the entities involved, the place, and the time period the event was observed on the web. b) We also propose an algorithm that can be adequately parameterized to accommodate different perceptions of the event concept and has expected complexity that grows linearly with the number of documents in the stream. This makes the algorithm suitable for online event detection on very large datasets. Our implementations of the proposed methodology and algorithm were evaluated on a dataset of 7 million blogposts and have outperformed other approaches in an international social event detection challenge.

The rest of the paper is organized as follows: related work on event and peak detection, as well as on topic extraction, is tunable algorithm for event detection. Section 5 evaluates the methodology through extensive experimental action and compares it against other approaches. Section 6 summarizes our work, discusses future directions, and concludes the paper. 2. Background and related work
The main objective of the event identification problem is to identify events from temporally-ordered streams of documents and organize these documents according to the events they describe. Towards this direction, numerous algorithms and techniques have been proposed, with most of them in the unsupervised learning category.

In the majority of cases, dynamic clustering techniques are used. One common approach is to model event identification as an and the document is assigned to either an existing event, or to a new event based on predefined criteria. Following a different approach, Zhang et al. [6] propose a news event detection model that speeds up the detection task by using a dynamic news
Chen et al. [10] adopt a spatial approach, which transforms document streams into feature streams of tens of thousands of features. They propose a method that clusters bursty features to form bursty events and associate each event with a power value dynamically represents documents over time and amplifies their features in proportion to their burstiness at any point in time. required the specification of a small number of parameters. Furthermore, Zeppelzauer et al. [14] proposed the combination of data based on the most reliable information (timestamps and geotags) to obtain robust event candidates and then employed additional contextual information for filtering and refinement of the event candidates.

Event identification has also been attempted through statistical methods . Ha-Thuc et al. [15] propose a scalable system that employs Latent Dirichlet Allocation in order to track events and sub-events, while excluding non-relevant (to the event of interest) text portions. On the other hand, Tang et al. [57] used latent temporal structures for complex event detection through spatio-temporal features of videos, appropriately quantized and aggregated into Bag-of-Words descriptors. Nikovshi and Jain [16] propose two fast algorithms with a computational complexity of O ( N of abrupt changes in streaming data that can operate on arbitrary unknown data distributions before and after a hypothesized change. The first algorithm is based on computing the average Euclidean distance between all pairs of data points, while the normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term re-weighting based on inverse event frequencies, and segmentation of the documents.

Chen and Chundi [18] extract hot spots of basic and complex topics, using temporal scan statistics to assign a discrepancy score maximum of functions and comparing observed and expected numbers of events in a window of width w , in order to identify intersecting paths.

Probabilistic methods have also been widely used in the event detection task. Sakaki et al. [3] built a probabilistic redundancy measure based on a mixture of language models are both effective for identifying redundant documents. In [22] ,
Li et al. proposed a probabilistic model that incorporates both content and time information for retrospective news event detection. Another interesting work is that of Traag et al. [59] who apply social event detection in massive mobile phone data using probabilistic location inference.

Classification algorithms are also extensively used in the event detection field. Yang et al. [23] use a supervised learning algorithm to classify the on-line document stream into pre-defined broad topic categories, and then perform topic-conditioned detection for documents in each topic. Following a different approach, Kumaran and Allan [24] use text classification and named entities, while exploring modifications to the document representation in a vector space-based system, in order to improve the performance of news event detection schemes.

Event detection has also been explored through graph analysis . Zhao and Mitra [25] define an event as a set of relations each node represents a social actor and each edge represents a piece of text communication that connects two actors. Events are detected by combining text-based clustering, temporal segmentation and graph cuts of social networks. Sayyadi et al. [26] hypothesize that documents describing the same event contain similar sets of keywords and the graph of keywords for a document collection contains clusters of individual events. In this context, they built a network of keywords based on their co-occurrence in documents and proposed a graph-based event detection algorithm, that uses community detection methods to detecting  X  communities  X  of documents. These candidate social events were further processed by splitting the events exceeding a predefined time range into shorter events.

Other advanced computer science techniques have also been employed. An infinite-state automaton, where bursts appear proposed. A different approach is followed by Adaikkalavan and Chakravarthy [30] , who propose SnoopIB, an event specification language and detect events based on interval-based semantics. On the other hand, Lee et al. [58] focus on Twitter for mining microblogging text stream to obtain real-time and geospatial event information. A watershed-based method with support from row contains all records of a user. These records are then sorted by time, transforming the social event detection task into a watershed-based image segmentation task. He et al. [32] consider the problem of analyzing word trajectories in both time and frequency domains, with the specific goal of identifying important and less-reported periodic and aperiodic words. In another interesting work, Weng and Lee [56] , built signals for individual words by applying wavelet analysis on the frequency-based raw from annotated photos as well as from external data sources within a framework that has a classification model at its core.
It is obvious that a significant amount of work has been done in this field. Nevertheless, none of the aforementioned efforts efficiently integrate named entity recognition and topic modeling with peak detection for discovering document streams. As discussed, the majority of approaches either cluster similar documents using text similarity metrics, thus retrospective, as well as online event detection in very large datasets. 3. Proposed methodology and requirements
Based on the above discussion, we define an event e i  X  E as a triple, where E thus NE  X  P P  X  O  X  L  X  OE  X  X  . A topic  X  i  X   X  (  X  from the Greek word w ; w 2 ;:::; w N event. Events may have more than one active period (e.g. iterative events); in this case NE and with new timestamps. According to the above definition, each event has a semantically rich representation that provides information on the entities involved, the place and the time period that it took place, and the topic of the event. Problem Statement : Given a document stream D ={ d 1 , d 2 ,... d and is depicted in Fig. 1 . The five steps are: 1. Document stream preprocessing (Spam removal, html tag removal, stemming, etc.). discovery and topic clustering. 3. Evaluation of topic clusters using semantically-aware similarity metrics. 4. Event detection. 5. Event evaluation.

In the following subsections we elaborate on each step of the proposed methodology. 3.1. Preprocessing
In order to avoid the notorious  X  garbage in , garbage out erroneous instances, and appropriately transform data. During preprocessing, HTML tags and stop-words are removed, in order to create meaningful topic maps later. Moreover, stemming is required to reduce the inflected and derived words to their stem/root form and map related words to the same stem.

Further preprocessing actions may be necessary, depending on the format and nature of the original document stream. Spam is the case, spam should be equally distributed over time. Since we are looking for entity account translation, mapping, and several other issues. 3.2. Entity and topic discovery
The second step of our methodology involves named entity recognition, topic discovery, and topic clustering. First, documents clusters, where each topic cluster is comprised of a set of N variable, while K  X  N  X  is the number of topic clusters. The selection of N 3.2.1. Named entity recognition
Since entity names form the main contextual content of a document, it is important to identify them. Named entity recognition a knowledge-based approach that uses explicit resources like hand-crafted rules and gazetteers, while the second is a dynamic approach, where a tagged corpus is used to train a supervised learning algorithm. We propose employing both methods, since their combination has shown to exhibit the best results in NER tasks [34] . 3.2.2. Topic discovery
The objective of this step is to discover sets of topics, as expressed by a stream of documents that identify their semantic content of those documents and express the semantic similarity among them. We employ topic modeling by the use of Latent
Dirichlet Allocation (LDA) for extracting semantic information from document streams [35,36] . Topic modeling is based on the assumption that each document d i can be described as a random mixture of topics and each topic as a focused multinomial distribution over terms. LDA builds a set of thematic topics from terms that tend to co-occur in a given set of documents. The result of the process is a set of N  X  topics, each expressed with a set of N per topic N W have to be defined in advance. The two parameters can be used to adjust the degree of specialization of the latent topics.

LDA discovers a mixture of topics P (  X  | d ) for each document d following another probability distribution as given in Eq. (1) . The probability of the w
P ( w i | d ), where  X  i is the latent topic, and P ( w i topic j in a document is P (  X  i = j | d ).

LDA estimates the topic  X  term distribution P ( w |  X  ) and the document over each term w i in document d i and samples a new topic j according to Eq. (2) , until the LDA model parameters converge. In Eq. (2) , C W  X  maintains a count of all topic  X  term assignments, C available terms,  X   X  i are all topic  X  term and document  X  are the hyperparameters for the Dirichlet priors. Based on these counts, the posterior probabilities of Eq. (2) are estimated in Eqs. (3) and (4) .

The Gibbs sampler sets the complexity of topic modeling to O ( N context of our method, we build a different topic model for each of the N computed at maximum from N / N E documents. LDA is generally considered a time and resource consuming process. Therefore, we propose offline building of new topic models at regular time periods and subsequent online assignment of topics to the document stream. Since we build different models for each top entity, this process can easily be distributed. 3.2.3. Topic clustering
Even though LDA is a powerful method for extracting topics from document streams, it requires a priori specification of the of N  X  , for entities with too few associated topics, LDA may be forced to produce exceedingly detailed topics, or even semantically overlapping topics (topics bearing the same meaning, expressed with different tags). On the other hand, limiting the number of topics may result in loss of vital information, as common entities may be associated with a higher number of topics than the one topics to use. In addition, the same entity may participate in different number of events in different time periods. We propose to start with a high number of topics. The exact starting value for N quantity of available data (in our evaluation presented in Section 5 we set it to 50 topics per top entity) and then dynamically determine the optimum value by merging semantically similar topics, with the help of some clustering technique. This approach will minimize information loss for common entities associated with many topics, but it will allow extensive topic merging for a) graph and b) hierarchical clustering. topic and each edge represents the similarity between two nodes. It has been shown that community-based semantics emerge from this graph representation and transformation process in similar applications [38] . A topic model TM can be regarded as a hypergraph (a bipartite graph, also known as a two-mode graph) with hyperedges. The set of vertices is partitioned into two disjoint sets: 1. Topics:  X   X   X  1 ;  X  2 ;:::;  X  N  X  ; and 2. Concepts (words-tags): C  X  w 1 ; w 2 ;::: w W N
Thus, the topic model TM is defined as TM p  X   X  C . The bipartite network is defined as H ( TM )= folding it. This can be achieved if we denote the matrix of the hypergraph as H ={ h concept w i . We define a new matrix S ={ s ij }, where s of topics weighted by the number of common concepts. The betweenness centrality score of an edge is defined as the extent to which that edge lies along shortest paths between all pairs of nodes, and is a good measure to find the edges between two communities. Inter-community edges will always obtain a high score, since the shortest paths between nodes from different communities will pass through them. Thus, by computing the edge betweenness for all edges in the graph and then removing edges k we remove, which determines the number of topics and the level of detail for each cluster, is determined in the topic cluster evaluation step discussed in Section 3.3 . concept, where the similarity of two clusters is the similarity of their most similar members.

To perform single-link text clustering it is necessary to define a representation model of textual data, a similarity measure among the documents, and a strategy for the cluster formation. The widely used space-vector model represents each document d as a vector of words, where each term is accompanied by its frequency of occurrence. Clusters are successively merged (in each distance p . The selection for the value of p is discussed next. 3.3. Evaluation of topic clusters
The results of the topic clustering process can be assessed by the use of a semantic similarity evaluation metric which can progressively lead to the optimal number of clusters for each entity. Thus, the semantic metric should recommend an appropriate Distance  X  NGD [40] , or 2) the Wikipedia-based Semantic Similarity Metric.

Keywords with similar meanings tend to be close in  X  units be farther apart. The NGD between two search terms (topics in our case), number of web pages searched by Google, f (  X  i ) and f ( number of web pages that contain both  X  i and  X  j . In practice, NGD values belong to the range [0 semantic match.

Although NGD is a reliable semantic metric, calculating the similarity of thousands of terms may turn out to be a time consuming process. As web search engines have limited throughput for computer generated queries, calculating the NGD value for a large number of documents within an acceptable time period is not feasible. Another approach, followed by many researchers, computes the semantic similarity between literals using WordNet. Nevertheless, measuring semantic similarity with hand-crafted lexical resources like WordNet, which are not available for many languages and have limited coverage may be problematic, particularly
As with the NGD concept, this approach also uses the distribution similarity as a proxy for semantic similarity, but without any in this case WP refers to the total number of Wikipedia pages. 3.4. Detection of interesting events discussed in detail in Section 4 . 3.5. Evaluation
The final step of the proposed methodology involves the evaluation of the whole process. Since a large number of events may be detected, they have to be ranked and evaluated against a ground truth event set. Alternatively, detected events can be evaluated according to user-based criteria. Evaluation results can be used to optimize the event detection algorithm. This optimization can be generic or personalized depending on whether evaluation is based on either a generally accepted ground truth or user preferences. 4. An ef fi cient unsupervised event detection algorithm
In this Section, we propose an efficient unsupervised event detection algorithm that identifies all important events (as defined in Section 1 ) available in a document stream. Our algorithm searches for peaks of entity/topic combinations in streams and generates two sets of data. The first is the set of  X  potential events in a document. For each potential event, the algorithm counts the number of occurrences, calculates average occurrence and records the first and last occurrence timestamps within a specific temporal window of length hV . If the average occurrence exceeds an upper threshold, we regard the potential event as an identified event. To avoid continuously increasing the number of potential events, the algorithm removes the ones that do not occur frequently. As a result, the required memory space and running time of the algorithm are significantly improved.

The second dataset contains only the average number of encounters for every combination of entity/topic identified as a further reduce the number of potential events. Moreover, a global metric of the average number of occurrences for each entity/ topic combination can be derived from this dataset and used as the baseline for the peak detection task.
Fig. 2 depicts a high level flowchart of the proposed event detection algorithm. First, we define the time interval to examine potential events. We define the concepts of frequent and infrequent potential events using a threshold value as discussed later in Section 4.1 . The algorithm examines one by one all the documents in the stream and extracts the named entities and topics residing in them. Subsequently, for each document it calculates all the unique entity/topic combinations and for every combination, it keeps the number of occurrences and creates a new potential event, or updates an existing one.
At regular time intervals, defined by examTimePeriod , our algorithm examines all potential events in order to decide which ones are too rare, so that they are no longer considered, and which ones are too frequent, so that they are identified as new events. 4.1. Analysis of the algorithm
The pseudocode of the proposed algorithm is listed in Algorithm 1 . Our approach connects to the stream and analyzes one document at a time (line 5). First, it tracks the number of documents analyzed during the current exam time period (line 6) in in various machines.

For topic detection, LDA can be used as discussed in Section 3.2.2 . LDA calculates for each document the probability to belong to each topic but here we only consider the topic with the highest probability. LDA is a time-consuming process, thus this approach is only practical in the case of retrospective event detection.

For online event detection, a faster approach is necessary. To increase efficiency, one can easily extract the topic of the document by examining its relevance to a set of pre-discovered topics, using one of the semantic similarity metrics discussed in
Section 3.3 . These pre-discovered topics are topic models that are computed at regular time intervals from scratch using the documents available on that particular time interval. We consider as typical interval a value between one hour and one day, events.
 Next, all available combinations of the | NE ( d i )| + | of the elements is not important and repetitions are discarded. The combinations are stored in a list named PET . The number of combinations is [41] :
To improve the time efficiency of the method for documents that mention too many entities (e.g. historic articles) and after enough experimentation we have empirically set the maximum number of entities allowed in a single blogpost to | NE ( d we consider that documents with | NE ( d i )| N 6 do not focus on any particular entity. We also set | each document is associated with one topic. Thus, the maximum number of combinations according to Eq. (6) is limited to 63.
Then, the algorithm goes through every entity/topic combination (line 10) and updates the respective potential event (lines 11 single document contains multiple combinations of entities/topics, a document can participate in multiple events. Likewise, the algorithm updates an object (line 18) or creates a new one (line 20), that holds the global average occurrences of the examined permutation.

At regular time intervals, defined by the timeUnit parameter (line 23), the list of potential events is traversed (line 24) and potential events that exhibit increased activity are retained, while potential events with minimal activity are removed from the potential event during the previous examination time period is divided by the number of documents examined in this time period (line 25) to define the relative average number of occurrences, rc documents in each investigated time unit. Such variations may be due to differences in the collection rates.
Two critical steps in the algorithm are the removal of  X  weak is detected when the average times encountered, avgRelEnc ( hV ), within the hV interval is higher than a minimum detection threshold detectionAvg (to avoid detecting very weak events) and sufficiently higher than the average times AVGInfo ( PET entities/events occur in general. The sufficiently higher concept is expressed as the product of a factor upLimit  X  destruction  X  threshold ( destructionAvg ), or sufficiently lower than the AVGInfo ( PET expressed with a factor lowLimit b 1 that is multiplied with AVGInfo ( PET (line 31) and nullifies the document per hour counter (line 32), in order to begin processing for a new time slot.
Algorithm 1. 4.2. Time complexity of the proposed method
The complexity of the proposed algorithm is an aggregation of the complexities of its five main tasks: A) named entity in offline mode for retrospective event detection, or in online mode for real time event detection.

The named entity recognition task is performed on a per document basis. Since the execution time for a single document depends on its length, which can be regarded as constant equal to the average document length, Task A's complexity for N documents is O ( N ). The complexity of Task B depends on the complexity of Gibbs sampler, which is O number of documents, N E the number of top entities for which we perform LDA analysis, N number of iterations. As discussed earlier, we perform topic modeling analysis on a per entity basis, only for the N
Thus, we can assume that it is sufficient to limit the number of topics: N usually applies [36,42] . Since N E , N  X  and I are constants, the complexity of this Task for N documents is O ( N ).
The complexity of Task C naturally depends on the clustering method employed. The complexity for graph clustering based on betweenness centrality is O ( kmN  X  ), where k is the number of edges to remove, m is the total number of edges, and N number of vertices (topics). Since the number of topics N number of edges to remove, are constant ( N  X  MAX 2  X  10 ; top entities depends only on the number of topic models to be built and is independent of N . Likewise, the complexity for the single-link hierarchical clustering is ON  X  2 [43] and independent of N . As far as Task D is concerned, the complexity for independent of N .

The operation that defines the actual online time complexity of the proposed method is the event detection algorithm. Task E the number of potential events is in the order of N times the available combinations of entities/topics (limited to 63 according the algorithm would be:
In practice though, the expected running time of the algorithm can be reduced to O ( N ), as we explain next. Let us assume that we set examTimePeriod to an appropriate value in order for the number of documents within it to be in the order of means that we check and update the list of potential events every 3162, 10,000 or 31,620 document when the stream consists of 10, 100 or 1000 millions of documents respectively, which is more than acceptable for the needs of our algorithm. We also assume that the number of potential events is significantly lower than the number of total documents by selecting a proper value for upLimit that limits the total number of detected events in the order of
Section 5 the maximum number of potential events has been identified to be much lower than values tested (plus we can always manually limit them). All things considered, it is obvious that the expected complexity of the between lines 23  X  37, which for N documents is: 5. Experimental evaluation
In this section, we provide some implementation details and evaluate the topic modeling and topic clustering steps, as well are performed with benchmark data provided by the ICWSM 2009 Data Challenge. In addition, we perform multivariate test analysis and analyze the accuracy, total number of detected events, average event duration, and execution time of detected events. Finally, we compare our methodology with other approaches in the context of an international event detection challenge (MediaEval2012). 5.1. Implementation details
In order to implement the various steps of the proposed methodology ( Fig. 1 ), a number of tools were employed. In the preprocessing phase, the Porter Stemmer [44] was applied, followed by a simple spam detection method, which was developed to remove very short messages and messages with many outgoing links. The LingPipe [45] linguistic framework was used for building the named entity recognition model, while we also employed supervised training for building statistical models that use dictionary matching and regular expression matching for improving the entity recognition process. In addition, we built a complete lexicon of organizations and locations, from publicly available open linked data, taken from DBPedia [46] and
Geonames.org ( http://www.geonames.org ). The lexicon is used for reducing the false positive results of the entity recognition model.

For grouping similar topic-maps into clusters, we employed the graph clustering method presented in [47] that is based on calculating the betweenness values of edges. The running time of this algorithm is O ( kmN O ( kN  X  2 ), while for graphs with strong community structure the complexity is even lower.

To calculate the Wikipedia-based semantic similarity, we followed the approach proposed by Kolb [48] . First, we used a simple context window of three words for counting co-occurrences. By moving the window over a Wikipedia corpus consisting of 220,000 words (resulting into 267 million tokens, obtained from http://www.linguatools.de/disco/disco-download_en.html ), we generate a co-occurrence matrix. In order to find a word's distributionally similar words, one should compare every word vector with all other word vectors. For vector comparison, Lin's information theoretic measure [49] was employed. In order to compute the overall matching score between two topics, we used the matching average method (Eq. (9) ), which calculates WSim , the number of set tokens: 5.2. Evaluation of proposed methodology and algorithm
The 3rd International AAAI Conference on Weblog and Social Media hosted the ICWSM 2009 Data Challenge. The Challenge delivered in XML format by the Spinn3r ( http://www.spinn3r.com ) company. The corpus consisted of weblogs published from
August 1, 2008 to October 1, 2008. This dataset spans over a number of big news events, like the 2008 Athens Olympics, both US posted to blogs. Thus, this dataset is an excellent test-bed for testing and evaluating the proposed methodology. 5.2.1. Data preprocessing
In the preprocessing phase we selected the English posts created in August 2008, in order to have a better overview of the events that took place during that period. This action resulted in a dataset of 12 million blogposts. The number of documents in differences in network throughput and availability, processing resources, or external variables may lead to differences in the available data. This is the main reason for using the normalization variable docCounterPerExamTimePeriod , as described in 257,373 organizations and 6,929,290 location names, respectively.

The selected documents were cleaned from XML tags and were ordered by time. Stop-word removal and stemming were operations resulted into a reduced corpus of 7 million documents with an average length of 913 characters, which was then fed to our event detection algorithm. 5.3. Entity and topic discovery  X  cluster identi fi cation and evaluation 5.3.1. Entity and topic discovery Before applying any stemming or stop-words removal, we identified the entities on the original blogpost context, since the three main classes of Persons, Organizations and Locations. Despite the fact that the number of distinct organizations and occurrences (i.e. limited number of invalid/faulty entities discovered for these categories). set of ground truth events that took place during the period we examined. Trying to keep the ground truth set as objective as list of 199 events. A small sample of that list is depicted in Table 2 .

Next, we created the topic models for a set of  X  top entities into account the size of our dataset, we regarded as top entities the ones with more than 1000 occurrences. We used this occurrences. The five most common entities along with one example topic are available in Table 3 .

Based on these entities, we built the respective topic models. Our intention was to create a relatively high number of topics and then merge the ones that were similar. For this reason, we set the number of topics to N the topic models we used typical values found in literature, such as: iterations [42] . 5.3.2. Clustering and cluster evaluation
Having created the topic models, our next task is to merge similar topics through clustering based on the edge-betweenness levels are depicted, concerning President Obama. Clusters are shown as tagclouds, which provide a versatile textual and visual representation. In the upper left quadrant of Fig. 4 , all topics form one cluster with no edges being removed by the clustering algorithm. Thus, there is only one general topic describing the nomination campaign of President Obama. As more and more edges are removed in subsequent steps of the algorithm, more clusters are created, thus more specific topics emerge. The general topic describing the campaign of President Obama remains in all four subfigures, but, as we move into higher levels of detail, topics such as the web campaign that took place during the nomination period, an interview with Pastor Rick Warren, an arrest that took place at that period, etc. begin to emerge.

A key point in our methodology is the selection of the optimal number of topic clusters using two semantic similarity metrics, the NGD and the Wikipedia based similarity. Thus, we created various sets consisting of different numbers of clusters and then evaluated each cluster set. After enough experimentation, we concluded that cluster sets having average Wikipedia-based similarity WSim near to 0.1 presented the most meaningful topics. This is a subjective criterion used by the authors; others may select some higher value of WSim to create more general topics, while others may prefer more specialized topics by selecting a lower value of WSim .
 Fig. 5 shows the results of clustering based on edge-betweenness as was applied to the topic model built on President Obama. cohesion and cluster separation for different clustering levels. Cluster cohesion is defined as the number of links between all different cluster numbers. The depicted example results from applying the agglomerative hierarchical clusterer, with different values of the partition distance.

In the case of hierarchical clustering, changing the value of partition distance causes sharp changes in the number of clusters, while, in graph clustering, changing the number of edges to remove causes smooth changes in the number of topic clusters. This other hand, in the case of hierarchical clustering, the calculation of partition distance is based only on the number of common of possible clusters. 5.4. Evaluation of the event detection algorithm
Next, we present a multivariate testing of the peak detection algorithm and we evaluate the events detected, the algorithm accuracy, and its execution time. 5.4.1. Multivariate testing
The required inputs for the event detection algorithm presented in Section 4 include the document stream, the minimum normalized average occurrences for detecting a new event ( detectionAvg ), the minimum normalized average occurrences under which potential events are destroyed ( destructionAvg ), the upper threshold for detecting a new event ( upLimit ), the lower relative average times an event was encountered ( hV ).

Our first set of experiments includes testing the behavior of the algorithm for different values of five input variables. We conducted experiments for detectionAvg =1 to 5, destructionAvg = 0.1 to 1, upLimit = 1.1 to 2.1, lowLimit = 0.5 to 0.9, and hV =1 to 48. For these values we computed the number and average duration of total detected events, as well as the number of detected ground truth events (out of 199). We consider a ground truth event to be correctly identified when our algorithm detects an event where the same entities participate and the two events overlap ( X 24 h due to different local hours and the one day event resolution of Wikipedia). Going over the representative results of the experiments, as depicted in Table 4 , one may draw various conclusions. As expected, low values of detectionAvg lead to the detection of a larger number of events and improve with an open world problem (there is no way to know for sure that a detected event is not an actual event), we are not able to more time consuming.

Fig. 7 depicts the number of identified events and the percentage of the ground truth events correctly identified. The accuracy of the algorithm reaches 90.45% (180 out of 199 ground truth events). Some examples of automatically detected events mentioned in this Table is the relative increase of the number of event occurrences in the data stream. nomination campaign of President Obama. One may notice that the time-line (that is the product of NER, topic modeling and clustering) is close to the one expected. There is a small peak on August 16 2008, when the U.S. presidential candidates John
McCain and Barack Obama were interviewed by pastor Rick Warren at Saddleback Church in Lake Forest, California. Another peak appears on August 23, when Barack Obama's campaign announced that Senator Joe Biden would be the Vice Presidential nominee. The strongest peak appears on August 27, when Obama was officially declared nominee of the Democratic Party for the we set detectionAvg = 3 and detectionAvg = 0.5.
 Fig. 9 depicts the number of documents associated with John McCain's and Barack Obama's interview by pastor Rick Warren at
Saddleback Church. This topic could be considered as a subtopic of President Obama's nomination campaign, but since we have
Obama's interview. 5.4.2. Tests on historic values and average detected event duration
Another parameter that defines the algorithm's efficiency is the interval hV of historic values to be taken into account when increasing hV results into a higher average duration of the detected events and a lower number of detected events. 5.4.3. Tests on time ef fi ciency
Next, we test the execution time of our method. As discussed in Section 4.2 , despite the fact that the complexity of the event detection method in the worst case is O ( N 2 ), the expected complexity is O ( N ). In this Section, we confirm this claim plot execution times with respect to detectionAvg and destructionAvg , respectively. In all tests, the order of the running time the detectionAvg and upLimit variables do not significantly affect the execution time.

Furthermore, we conducted a set of tests for various sizes of the input stream. The results are depicted in Fig. 13 and confirm
Core, 4 GB Ram). 5.5. Evaluation of our methodology against other approaches In order to further validate our approach, we evaluated it against other approaches in the context of the MediaEval2012 Social
Event Detection (SED) international competition [52] . The SED competition comprised three challenges on a common test dataset of images with their metadata (timestamps, tags, geotags).
 place in Hamburg (Germany) and Madrid (Spain). Soccer events, for the purpose of this task, were soccer games and social events centered around soccer such as the celebration of winning a cup. The goal of the third challenge was to find demonstration and protest events of the Indignados movement occurring in public places in Madrid. The Spanish Indignados organized a series of months or days in advance). SED provided 167,332 photos collected from Flickr.com that were captured from 2009 to 2011. All photos were originally geotagged. However, before providing the XML photo metadata archive (including any tags, geotags,
Although the SED dataset included photos augmented with metadata, we focused on textual metadata, in order to treat all photos as documents. We consider the use of visual information from our algorithm as future work. Evaluation of the submissions to the SED task performed by the organizers using ground truth that partially came from the EventMedia dataset [53] (for challenges). Two evaluation measures were used: a) the Harmonic mean (F-score) of Precision and Recall for the retrieved images, which cannot measure the number of retrieved events, nor how accurate the correspondence between retrieved images and events was, and b) the Normalized Mutual Information (NMI) that compared two sets of photo clusters (where each cluster events [55] . Both evaluation measures received values in the range [0 the ground truth results. These evaluation measures were calculated both per challenge and on aggregate (for those teams that submitted runs to all challenges).

The evaluation criteria for each submission took into account the number of detected events (out of all relevant events in the test set) and the number of correct/incorrect media detected for these events. What we were looking for was a set of photo content, and also in terms of place/time) with the event of interest.
 Table 6 presents the results of our methodology against other even t detection approaches in the MediaEval2012 Social Event created by domain experts consisted of manually selected keyword s, based on our observation of topics and aggregate statistics. from LDA produced topics, and augmented with tags of high occurrence believed to be useful.
 Topics identified automatically by LDA also provide good results, with the exception of Challenge 1. Unlike Challenges 2 and 3,
Challenge 1 is about technical events (mainly conferences) described by a diverse vocabulary and often comprising relatively few photos, thus resulting in topics that contain concepts from irrelevant photos. In any case, our methodology outperformed the other approaches in almost all Challenges and evaluation metrics, and won the 1st place in the overall competition. 6. Conclusions and future work
Throughout this paper we presented a methodology and the respective algorithm for the semantic detection of events from document streams that originate from Web Social Media. We argued that the defined concept, and we defined it in the context of our work. We presented a novel parameterizable event detection algorithm it with named entity recognition, topic modeling through LDA, topic clustering and semantic evaluation methods.
We support our thesis by extensively testing and evaluating the proposed methodology and algorithm on a dataset containing more than 7 million blogposts, as well as through an international Event Detection Challenge, where our methodology outperformed suitable for online event detection on very large datasets, with expected complexity of O ( N ).

The proposed methodology can be applied for discovering and summarizing interesting events to all web users. The incorporation of granularity. For example, one may be interested in knowing whether a major sports event is taking place, while others may be opening and closing ceremonies, etc.).
 An issue that deserves further discussion is the named entity recognition task, which is a key factor in our methodology.
Despite all the research efforts in this domain, the effective recognition of interesting named entities in previously unknown free text documents remains an open problem. Another issue we faced in our experiments was the high rate of false positives in the named entity recognition task. We were partially able to reduce the rate of false positives by using gazetteers, constructed from open linked data, however, such solution poses other difficulties. Even if we assume that the open linked data corpora (DBPedia.org, Geonames.org, etc.) contain a complete list of all location names and all major companies, incorporating such a list in our approach may result into limiting the disambiguation potential of the method. It may also limit the set of possible discovered entities only in the set of entities contained in the gazetteer. Another major problem in this area is the entity case of person entities, where maintaining a complete list of names is extremely difficult (at least for medium sized projects/ teams). An efficient solution to the above discussed problems would come from the prevail of a truly semantic web, where web pages appropriately describe the existing entities by embedding semantic information using microformats, RDFa and other technologies.

Future directions of our research include the improvement of the named entity recognition and disambiguation process and the creation of a rewarding mechanism that is based on personalized choices, in order to automatically select the optimized values of the algorithm's parameters based on user type. Moreover, fuzzy logic for the event definition and event identification may be employed. Another interesting direction is the dynamic selection of the number of topics created by LDA that will better as well as the detection of intentionally or unintentionally seriously flawed web reports of events. In the near future, we also consider integrating semantic information from more sources, such as HTML tags and microformats, as well as focusing on related of the event and the web medium where it is being reported.

References
