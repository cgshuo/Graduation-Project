 Low-dimensional topic models have been proven very use-ful for modeling a large corpus of documents that share a relatively small number of topics. Dimensionality reduc-tion tools such as Principal Component Analysis or La-tent Semantic Indexing (LSI) have been widely adopted for document modeling, analysis, and retrieval. In this pa-per, we contend that a more pertinent model for a docu-ment corpus as the combination of an (approximately) low-dimensional topic model for the corpus and a sparse model for the keywords of individual documents. For such a joint topic-document model, LSI or PCA is no longer appropriate to analyze the corpus data. We hence introduce a powerful new tool called Principal Component Pursuit that can effec-tively decompose the low-dimensional and the sparse com-ponents of such corpus data. We give empirical results on data synthesized with a Latent Dirichlet Allocation (LDA) mode to validate the new model. We then show that for real document data analysis, the new tool significantly reduces the perplexity and improves retrieval performance compare d to classical baselines.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Theory, Verification, Experimentation Principal Component Pursuit, Sparse Keywords, Latent Se-mantic Indexing, Latent Dirichlet Allocation, Perplexity
This work is partially supported by the grants NSF CCF 0964215 and ONR N000140910230
The ability to effectively model and analyze relationships among a large corpus of documents has become increasingly important for information retrieval, and for indexing, ran k-ing, and searching documents on the web [7, 15]. In the past few years, a large number of models and computational tools have been introduced in the information retrieval literatu re, and many have seen great success and led to more powerful indexing and search methods.

Nevertheless, the rapidly increasing volume and diversity of the web documents continue to demand ever more effec-tive and scalable tools for extracting useful information f rom massive collections of text data. The size of the document corpus to be analyzed is now routinely in the range of mil-lions or even billions. When dealing with such massive high-dimensional data, the curse of dimension becomes a reality: the complexity of many models and algorithms increases ex-ponentially in terms of the dimension, or their performance decreases sharply as the scale goes up. As pointed out in [3], to alleviate such a curse, modern statistical and compu -tational methods have started to leverage on the fact that natural high-dimensional data typically have low intrinsi c dimensionality.

Such low-dimensional structures have long been observed and harnessed in the analysis of document corpora. Al-though the number of documents in a corpus can be large, their topics can be very limited and highly correlated. Thus , if we try to summarize the semantics of the corpus at the topic level, the corpus data could have much lower dimen-sion (or degree of freedom) than the ambient dimension (the number of documents). Since such a low-dimensional model is important for summarizing the semantics of the corpus, earlier work such as the Latent Semantic Indexing (LSI) [8] has proposed to use Singular Value Decomposition (or Prin-cipal Component Analysis ) to fit a low-dimensional subspace to the corpus data matrix. This simple subspace model and solution by PCA have had tremendous influence on docu-ment analysis in the past two decades.

Many recent more powerful (and hence more complicated) models such as Probabilistic Latent Semantic Indexing (pLSI) [10] and Latent Dirichlet Allocation (LDA) [2] essentially fol-low the same rationale that document corpora at the topic level have few intrinsic degrees of freedom. Although pLSI and LDA are based on more precise statistical formulations, in practice, data generated according to such models clearl y exhibit an approximate low dimensional subspace structure , as shown in Figure 3(a). Mathematically, the corpus data matrix D can be viewed as low-rank matrix L 0 perturbed low-rank model L . Here, the corpus was decomposed via PCP with  X  = 5 / by small noise: To some extent, pLSI and LDA model use generative statis-tical models to justify the validity of PCA for document data analysis: if Z 0 is i.i.d. Gaussian, PCA in fact gives the opti-mal estimate of the low-dimensional subspace (or principal components) L 0 .

While most of the above topic models are trying to capture the intrinsic common relationships among all the documents , none of them is designed to model the statistics of individua l documents. Although a particular document may belong to a common topic (say on the topic of  X  X inance X ), it covers a story different from all the other financial documents in the same corpus. Any statistical deviation of this document from the common topic model allows us to distinguish it from the rest of the corpus. For instance, certain words may show up much more frequently in the document and stand out from the background topic model. One may view such words as  X  X eywords. X  Figure 1 shows one example of some such keywords detected by our method in a financial document. Obviously, discriminative statistical feature s like the keywords would be tremendously informative for us to identify any particular document inside the entire corpus, hence allow us to better index, search and rank documents [18].

In this paper, we propose to model the statistics of both the corpus topic and individual documents with a combina-tion of a topic model such as LDA and a sparse model for the keywords that occur with unusually high frequency in each document. We refer to such a model as a joint topic-document model . Similar models have previously been con-templated and explored for document analysis [5]. However, the solution given in [5] suffers from several problems. Firs t, [5] directly models the keywords and topic words via a la-tent random variable and estimates the parameters from the corpus. The definition of keywords, however, is more a sub-jective rather than objective concept. What percentage of words should be treated as the keywords of a document, 10%, 5%, or 1%? The answer varies from application to ap-plication. Our model leaves it as a free parameter to tune, embracing a broader range of applications. Moreover, al-though the inference procedure (a modified Gibbs Sampling method) used by [5] worked well in practice, there is no guarantee on the number of iterations in general.
As we will soon see, mathematically, we essentially assume that the corpus data can be written as the sum of a low-rank matrix and a sparse matrix with the low-rank component L 0 corresponding to the back-ground topic and the sparse one S 0 to the keywords (here, we assume D , L 0 and S 0 to be unnormalized term frequency matrixes). There is a major difference between this model and the LSI model discussed earlier: there Z 0 (given in (1)) introduces an entry-wise small perturbation to L 0 while here the entries of S 0 are sparse but in theory are allowed to have arbitrarily large magnitude. It is well known that decom-posing a matrix into its low-rank and sparse components is an intractable (NP-hard [4]) problem in general. So such a model, though conceptually well-motivated, would be use-less for real document analysis if we could not effectively learn the low-dimensional background topic and the sparse keywords from large corpora. Fortunately, recent break-throughs in high-dimensional convex optimization indicat e that the above decomposition can be exactly and efficiently computed under surprisingly broad conditions by solving a certain convex program, called Principal Component Pursuit (PCP) [3].

In this paper, we validate the joint topic-document model with encouraging empirical results on synthetic and real data. Our experiments demonstrate the effectiveness of PCP in recovering the low-dimensional topic model and the spars e keywords. As we will see, this new tool yields much better results in terms of both subspace distance and perplexity in identifying the latent low-dimensional topic model, and these translate into better performance in identifying key -words as well as on a model retrieval task.

We note that the goal of this paper is not to reach at a full-fledged document indexing and retrieval method that has been optimized to work better than existing methods on diverse corpora. Instead, our goal is to validate a sim-ple, new model for document analysis and to introduce a remarkable new tool that can learn such new model rather effectively. Although we have only verified the model on a corpus of moderate size and compared with classical base-line methods, the consistently positive test results and wi de improvement margins indicate that this new model indeed has great potential to enhance future document analysis, indexing, and retrieval.
In this section, we introduce the proposed joint topic-document model and new computational tool for analyzing corpus data that obey this model. As the new model is a generalization of conventional topic models, especially L SI and LDA, for completeness, we first give a brief overview of main assumptions of these methods. The overview will also discuss some limitations of these assumptions and justify t he need for a better model and analytical tool.
It is common in document analysis to assume that docu-ments are generated from a relatively small number of topics [1, 2, 8, 10]. We consider documents containing words drawn from a vocabulary V of size m . Each topic corresponds to a discrete probability distribution over V . To fix some no-tation, suppose there are r topics, and that  X  1 . . .  X  r these r distributions: each  X  i  X  R m is a nonnegative vector whose entries sum to one. For ease of notation, we introduce a matrix We will let n denote the number of documents in the cor-pus. For each j = 1 . . . n , the j -th document is generated as follows: one first chooses a set of r weights, w ( j ) ( w 1 . . . w One then forms the mixture distribution Clearly, p ( j ) is also a probability distribution over V . The words of the j -th document are considered as an iid sample final observation is the unnormalized term frequency vector : d . In particular, if the length of the document N j is large, then the law of large numbers suggests that with proper normalization, the empirical probability distribution It is convenient to further consider the matrix forms Here D  X  R m  X  n is our observation (again, note that T , W and P are all probability or weight matrices, but D is unnormalized term frequency matrix  X  X ampled X  from P ). In terms of these quantities, notice that
Since column-wise normalization does not affect the rank of D , D is approximately the same rank as P (i.e., r ) ex-cept for the small noise term and lack of sufficient sam-pling. This phenomenon has been verified empirically in real text corpora. For example, for a document set con-sisting of n = 10 , 000 articles with a vocabulary V of size m = 20 , 000, between 50 and 300 dimensions are generally sufficient [6, 11]. This has motivated the use of low-rank ap-proximation in information retrieval, under the name Latent Semantic Indexing [8]. LSI uses the singular value decom-position to form an optimal rank-r approximation where { u i } , { v i } are the first r singular vectors of D and {  X  the corresponding singular values. Interestingly, in some cases this low-rank approximation indeed improves recall [ 8], a phenomenon that can be explained under certain strong generative models for text [16].

Although the above low-dimensional model seems naive, it captures the essence of other more sophisticated topic mod-els such as the popular LDA model. Although not explicitly analyzed in [2], one can show that if each document to be generated by LDA model is sufficiently long, the mixture weights of topics of T can be computed accurately. There-fore, the corpus generated by LDA model satisfies the low-dimension model. In summary, LDA places probabilistic priors on the number of words per document and the mix-ing weights, which makes it more flexible in modeling real corpus data. Hence, in subsequent simulations and exper-iments in Section 3, we will use the LDA to generate and model our data. We also verified empirically the property of low-dimensionality of LDA here.

Note that in the following discussion, unless explicitly stated, D , L , S and all the related matrix symbols all stands for document-term matrixes whose entries indicate the num-ber of occurrence of each term.
As argued above, a low-dimensional topic model (and its low-rank approximation) provides a good representation of the commonalities of a set of documents. However, for in-dexing and retrieval tasks, the deviation of each document from the common model may actually be much more infor-mative. Normally each document will have a few terms that are used with much higher frequency than one might expect from the overall statistics of the corpus. We call these the  X  X eywords X  of the document, and model their effect on the observation D through an additive term S 0  X  R m  X  n : Since there are relatively few keywords, S 0 is a sparse matrix.
In the context of document analysis, the above model is rather natural to interpret: while the low-rank component L 0 can be viewed as the common  X  X ackground X  topics of all the documents in the corpus; the sparse component S 0 cap-ture the distinctive  X  X eywords X  or key phrases that best rep -resent the unique content of each document, see for example the words  X  X hrysler X  and  X  X rtner X  in Figure 1. The support of S 0 indicates which terms are the keywords for each doc-ument and the magnitude of its entries suggests how much each keyword stand out from the common topic model. As this model capture both the corpus topic and the content of individual documents, we call it a  X  X oint topic-document  X  model.

This model has several implications. First, as discussed above Moreover, The reason for this is simple: the rank-r approximation (4) is optimal when the matrix D is perturbed by small dense noise, say D = L 0 + Z 0 where Z 0 is i.i.d. Gaussian. However, the sparse perturbation S 0 due to the presence of keywords has a very different nature than Gaussian noise: a few of the entries are quite large, but the rest are almost zero. In this situation of large but sparse corruption, the approximatio n error k  X  L LSI  X  L 0 k can become extremely large.
It seems then, that a more pertinent solution for the joint topic-document model would be to attempt to directly com-pose the data matrix D in (5) into a low-rank component L corresponding to the topic model, and a sparse component S , corresponding to the keywords. Clearly, we can always get a low-rank L 0 by setting S 0 = D , which is obviously meaningless. Therefore, we want to minimize the number of non-zero entries in S 0 so long as L 0 is a low-rank matrix. Specifically if we know from prior knowledge that the esti-mated number of topics of the given corpus is at most r , the above idea can be translated into the following optimizatio n problem:
The Lagrange form of (6) is: It is known, however, for worst case inputs, this optimiza-tion problem is intractable (NP-hard [4]). Nevertheless, a s we will see, recent advances in the study of low-rank matrix recovery provide a computationally feasible solution that guarantees to solve a surprisingly broad class of instances . In fact, these advances are largely motivated by the interes t in relevance analysis of web data or user taste anticipation . In particular, as long as the rank of the matrix L 0 is suffi-ciently low and the matrix S 0 is sparse, one can effectively and efficiently get exactly the same solution as (7) by solving the following relaxed convex program: Here kk  X  is the nuclear norm of a matrix (i.e. the sum of its singular values) and k k 1 is the  X  1 norm of a matrix (i.e. the sum of absolute values of entries). The parameter  X  &gt; 0 bal-ances the tradeoff between sparsity of the keywords matrix S and rank of the background topic matrix L . According to the theoretical result established in [3],  X  should be of the order of  X  max { m, n }  X  1 / 2 . The above convex pro-gramming problem is dubbed Principal Component Pursuit (PCP) in [3]. Below, we will let denote the solution to (8). The recent development in con-vex optimization has produced algorithms that can solve this relaxed convex program with a computational cost not so much higher than that of the classical PCA. For com-pleteness we review one of the fastest algorithm via Alter-nating Directions [19, 20] method here to demonstrate the tractability of (8). The equation (8) can be rewritten as the Augmented Lagrange Multiplier form: where the Euclidean inner product between two matrices X and Y is defined as h X, Y i = trace( X  X  Y ).

There are two important observations that makes mini-mizing (10) quite easy and elegant. Before presenting that, let us define two operators: and Here S  X  ( ) is initially defined on single number and then we extend it to matrix by applying it to every entry in the
The recent Netflix challenge of completing movie rankings from incomplete user survey is one such example. matrix. For any matrix X , X = U  X  V  X  gives the singular value decomposition of it. The following two observations are particularly important for the minimization problem: With these two observations, in each iteration we fix S and minimize  X  with respect to L , do the contrary(fix L and minimize with respect to S ) and then update the Lagrange multiplier matrix Y based on the residual D  X  L  X  S . This procedure has been proved to converge to the global optimal point of (10) under quite board conditions [19, 20]. The above idea is summarized as Algorithm 1.
 Algorithm 1 Decomposing D into Low-rank and Sparse Parts by Alternating Directions [19, 20]
Initialize: S 0 = Y 0 = 0, &gt; 0 while not converged do end while return L, S
As we have discussed, realistic web corpora may contain millions or even billions of observations. This scale is oft en beyond the capability of traditional methods, due to the Curse of Dimensionality . On the other hand, theoretical results in [3] demonstrate that as the data dimensionality increases, the decomposing ability of (8) becomes identica l to (7) and perhaps more surprisingly, the convergence of Algorithm 1 needs even fewer iterations, arguably a Blessing of Dimensionality!
We will talk about the computational cost of Algorithm 1 briefly. Note that the Singular Value Decomposition is re-quired in each iteration. Recent research shows that solvin g (8) with this algorithm just takes approximately 10 Singu-lar Value Decomposition on D [20], meaning that the time complexity of solving (8) is almost the same as traditional Singular Value Decomposition, which can be efficiently com-puted in parallel. Our distributed implementation of Algo-rithm 1 performs efficiently even for large scale problems, e.g. , decomposing matrices as large as 10 , 000  X  50 , 000.
As we have seen, PCP offers a way to make the conven-tional PCA robust to gross corruptions with small additiona l computational cost. In the next section, we will see that this approach performs well in decomposing corpera D that are generated according to the above joint topic-document model.
In this section, we evaluate the proposed model and algo-rithm with both synthetic and real data experiments. We first show that if the corpus is generated from a topic model plus sparse document-specific keywords, Principal Compo-nent Pursuit gives a much more accurate estimate of L 0 (and hence of S 0 ) than classical PCA/LSI. We observe that real Figure 2: Singular values of the 1,000 documents from the Reuters dataset. The singular values de-cay smoothly, similarly to the idealized joint topic-document model in Figure 3(b). data such as the Reuters dataset indeed exhibit a distribu-tion that better resembles the joint topic-document model than the topic model alone. Motivated by this observation, we then compare topic models fit to the original data ma-trix to topic models fit to the matrix after PCP has removed outlying words. We will see that such preprocessing signifi-cantly reduces the perplexity of the topic model fit by LDA. Finally, we show how the low-rank and sparse components learned by the PCP algorithm can enhance retrieval perfor-mance.
We first perform a basic consistency check, to see that when the corpus is indeed generated according to a topic model plus some sparse keywords, Principal Component Pur-suit effectively estimates the two components.

We adopt the popular Latent Dirichlet Allocation model to generate the corpus. This model places probabilistic pri -ors on the number of words N j per document and the mixing weights w ( j ) . In particular, the N j are assumed to be i.i.d. samples from a Poisson(  X  ) distribution, while the w ( j ) sumed to be i.i.d. samples from a Dirichlet distribution wit h parameter vector  X   X  R r . For more details on this proba-bilistic formulation, we invite the reader to consult [2, 17 ].
In our simulation, we generate data according to the LDA model with expected document length  X  = 500 and Dirichlet parameter  X  = (0 . 1 , . . . , 0 . 1). The corpus size is n = 1 , 000, while the vocabulary is assumed to have size m = 2 , 000. We generate r = 100 topics, each distributed in accordance with Zipf  X  X  Law , which was observed on almost any corpus that is large enough [9]. For each  X  i , we generate an in-dependent random permutation of  X  i of { 1 . . . m } , and set  X  ( j )  X  1 / X  i ( j ). This is reasonable, since only a small por-tion of the vocabulary set is highly correlated with a given topic.

We first generate a corpus D LDA according to the LDA model, with no additional keywords. Figure 3(a) plots the singular values of a document matrix generated according to this model. We observe a clear sudden drop of magnitude of the r -th and ( r + 1)-th singular value, where r corresponds to the number of hidden topics. Such a distribution clearly supports the use of subspace methods such as LSI for pro-cessing the data to help identifying the topics.

However, real text datasets rarely exhibit such a clear sub-space structure. Figure 2 shows the singular value distribu -tion of a 1,000-article real dataset from the Reuters corpus . As we see, the change of singular values is more gradual and smooth, and one does not see any clear break point.
We next consider what happens if the corpus contains additional keywords, that occur more frequently than pre-dicted by the LDA model. From each article in the syntheti-cally generated corpus, we randomly select 10% of the words of each article to be keywords. We add a frequency of 10 to each (recall that since  X  = 500, this is 1 / 50 of the expected document length), forming a new data matrix Figure 3(b) shows the distribution of singular values of the corrupted LDA data. Notice that the clear break point has disappeared and the distribution has become just as smooth as the real data. This shows that the joint topic-document model is more suitable than the LDA model alone, which does not take the keywords of individual documents into consideration.

We next apply principal component pursuit to the per-turbed corpus, with weight factor  X  = 0 . 1. This yields a low-rank and sparse pair (  X  L,  X  S ), for which  X  S has 54,704 non-zero entries. Figure 3(c) plots the singular values of t he recovered matrix  X  L . Notice that the distribution of singular values becomes much closer to the original distribution in Figure 3(a). The sharp drop at r = 100 which vanished due to the presence of keywords is now clearly visible again.
We evaluate the recovery quantitatively by examining the distance between the range of the recovered low-rank sub-space and the range of the matrix T of topic distributions defined in (3). Notice that as the number of words per docu-ment becomes large, the matrix D LDA approaches range( T ) = range( L 0 ). If our goal is to infer the underlying topic mix-tures L 0 = T W that generated the data, then the distance to range( L 0 ) is a good measure of correctness.
We measure distance using the subspace angles [14]. Specif-ically, let S 1 and S 2 be two subspaces in R n with The distance between S 1 and S 2 is defined as follows where cos  X  i =  X  i and  X  i is the i th singular value of S We let  X  L LDA,LSI denote the rank r approximation to the LDA corpus D LDA obtained by singular value decomposi-tion. We let  X  L JT D,LSI denote the rank r approximation to D
JT D obtained by singular value decomposition. Finally,  X  L
JT D,P CP denotes the rank-r approximation given by ap-plying Principal Component Pursuit to the corpus D JT D . Table 1 plots the distance between the range of each of these recovered low-rank components and the range of L 0 . Notice that in the absence of additional sparse keywords, the SVD (or LSI) gives a fairly good approximation to L 0
By abuse of notion, we here use the same notation S 1 , S to represent any orthonormal bases for the two subspaces, respectively.
 Table 1: The distance between different subspaces. d (range(  X  L LDA,LSI ) , range( L 0 ))  X  1 . 86. However, when ad-ditional sparse keywords are introduced this estimate brea ks down: d (range(  X  L JT D,LSI ) , range( L 0 ))  X  11 . 1. If we instead decompose the data using principal component pursuit, the error drops significantly: d (range(  X  L JT D,P CP ) , range( L 3 . 63.

This drastic reduction clearly suggests that PCP could ef-fectively isolate sparse keywords that cause deviations fr om the low-rank topic model. Conversely, the large error in es-timating L 0 using LSI/SVD suggests that these tools may be less appropriate for such perturbed corpora. We have repeated this experiment with varying numbers of topics, documents and perturbation percentages. The reduction of the subspace angle is consistent and significant.
In this section, we further validate the proposed model with experiments on real data. Because real data lack ground truth L 0 , [2] proposed to measure the quality of a learned topic model through the perplexity perplexity( D | model) = exp Since this is a monotonic function of the likelihood of D , lower perplexity implies higher likelihood, suggesting a b et-ter fit.
In this experiment, we use a corpus consisting of 1,000 documents in the Reuters-21578 dataset. The vocabulary V consists of the 3,000 most frequent words, excluding common words in a  X  X top words X  list. This gives us a 3 , 000  X  1 , 000 data matrix D whose i, j entry is the frequency of occur-rence of word i in document j . As above, we decompose D into low-rank topics L and sparse keywords S by solving the Principal Component Pursuit problem (8).

The free parameter  X  in (8) strikes a balance between ex-tracting more keywords and using a higher-rank topic model. In our experiment we try 20 different values of  X  , This gives 20 solutions ( L ( i ) , S ( i ) ) , 1  X  i  X  20. For each L ( i ) , we train an LDA model using Gibbs sampling. We let number of topics r = 50, and use the default hyperpa-rameters  X  = 50 r = 1 ,  X  = 0 . 1 and number of iterations, 2 , 000. This yields 20 different learned LDA models. For comparison, we also generate an LDA model from the data itself with no PCP preprocessing. We compare the quality of these learned models by evaluating the likelihood of D being generated by the learned model, or, equivalently, the perplexity.

Using the package GibbsLDA++, available at http:// gibbslda.sourceforge.net/ (a) Synthetic Topic Data (LDA model) is again clearly visible.

We calculate the perplexity of this model and compare those of the models learned from above. Here we ignore the result from L (1) to L (4) and L (19) to L (20) . The reason is when  X  is too small ( i &lt; 5), S is too dense containing too many words; and when  X  is large ( i &gt; 18), S becomes too sparse (less than 1,000 entries, which means each document has less than 1 keyword on average). The perplexity as a function of  X  is plotted in Figure 4(a), and compared to the baseline perplexity (red line) without PCP.

From the result we see that in the selected range of  X  , the perplexity of the model after processed by PCP is signifi-cantly lower than that of the unprocessed data. Note that the perplexity for L (5) and L (6) is much lower than that for the original data D , with a reduction of nearly 20  X  30% in the perplexity value. The matrix S (6) is already rather sparse: it contains no more than 150 words compared with the 3 , 000 in total. Figure 1 shows some of the keywords (in bold) detected by S in two typical sample texts from the corpus.

From this experiment, we may conclude that after apply-ing PCP to the corpus data and removing a small number of outlying words from each documents, the processed corpus fit the LDA model much better.
Next, we would like to test the generalizability of the learned model for new (but similar) test data. Again, we use perplexity to measure the goodness of each model learned in explaining the new data. Since the learned LDA model is only supposed to predict the low-dimensional part of the tes t data, we will first run PCP on the test to remove outliers.
We use the same data set, Reuters-21578 with 2,000 doc-uments, obtaining a 3,000-by-2000 matrix D . But this time we take the odd columns and form a 3,000-by-1,000 subma-trix as the training data D train and use the submatrix of the even columns as the testing data D test . For both D train and D test , we use PCP to decompose them to their low rank plus sparse components: Same as the previous experiment, we choose 20 different L
Then we learn an LDA model for each L ( i ) train with exactly the same setting as before. Also we use the original unpro-cessed corpus D train to learn an LDA model and compare its perplexity with those learned from L ( i ) train . Then we would like to test the models on the test matrix M test and L ( i ) test , 1  X  i  X  20. In this inference step we set iter = 20 by default. We then calculate the perplexity in the same manner as in the first experiment.

For the same reason as above, we only select the  X  from perplexity drops significantly: in the case of i = 5 , 6, the drop is nearly 30  X  35%!
For i = 5, the matrix L (5) test contains about only 76% of the vocabulary. This may have partially contributed to the drop in perplexity. Nevertheless, for i = 6, L (6) test contains over 95% of the vocabulary, and the drop remains significant.
Even when  X  increases significantly, for example for  X  18 the support of the sparse matrix S (18) test is about 3 , 000. This means that for each document with an average length of 500 words, we only treat 3 words as outliers but the perplexity still drops a lot.

So we may conclude that the new method can effectively remove the outliers of a given corpus and the model learned after outliers being removed is much more generalizable.
In this section, we examine how so learned low-rank and sparse components encode useful information for document analysis. As we have discussed before, the low-rank matrix encodes the general topics that are contained in documents, whereas the sparse components are the document-specific terms. It is natural to ask that how well are the keywords Perplexity low-rank component of the test set with respect to this fitted model. computed by PCP agree with human X  X  intuition. It is, how-ever, not easy to verify this due to the lack of well-labeled data. Nevertheless, in the experiment below, we examine the proportion that the keywords extracted by PCP are also the words contained in the title of each document. This is a rea-sonable test, since for most articles, especially formal on es, the titles are carefully chosen to represent the most inform a-tive and document-specific part in a corpus. We therefore consider the titles as the human label for the keywords of documents.

However, since title words are usually somewhat unique to each document, many of them will not fall in the list of 3,000 most frequent words in the corpus. So we have to resample the corpus to ensure most title words are included in the list of words of interest. By combining the title words with the most frequent words in the 1,000 documents in the corpus, we obtain an extended new data matrix D of size 3 , 322  X  1 , 000. We use another matrix X of the same size to denote the matrix of title words: x ij is the number of times the i th word appears in the title of document j .
As in the previous section, we apply PCP to decompose the data matrix D into a low-rank part L and a sparse part S . We again choose 20 different  X   X  X  from 1  X  decomposition: D = L ( i ) + S ( i ) , for 1  X  i  X  20.
We first investigate the relationships between the title words and the words that appear in the sparse matrix S . We compute the correlation between the title matrix X and each of the sparse matrix S ( i ) , i = 1 , . . . , 20 in terms of two measures. We first normalize the sum of each column of S ( i ) to one and view it as a probability distribution: 1. The first measure, denoted as t 1 ( i ), is the fraction of 2. The second measure, denoted as t 2 ( i ), is the average Figure 5: Keywords justification by comparing with Title Words. Two correlation measures t 1 and t 2 between the sparse matrix S ( i ) and the title matrix X .

We plot these measures as a function of i in Figure 5. To give a sense of how sparse the keywords detected by S ( i ) are in the corpus, we have also plotted the fraction of word counts given by S ( i ) w.r.t. the total number of words in the corpus.

As we see from the plot 5, the result is rather striking: The value of t 2 ( i ) reaches its peak at i = 12 or  X  = 12  X  fraction of words detected by S (12) as keywords is about 5%. But out of the 5% keywords, more than 50% of them are title words (see Figure 1 for an example), even though we never told the algorithm which words belong to the titles! Note from the same example that it is not necessarily true that title words are the most important words of a document  X  there may well be keywords that do not appear in the title. Such words will get detected by S as well. Also, it is interesting to see in the plot 5 that t 1 ( i ) is monotonic in i which suggests that as  X  increases and S gets sparser, the percentage of title words left in the keyword list increases .
PCP decomposition actually provides us more informa-tion about the original dataset, i.e. background information versus document-specific information (or, geometrically, a proper subspace versus outliers). We have performed some preliminary experiments on information retrieval using th e two parts in a cooperative manner. Specifically, we found that a better mAP can be obtained by weighting the low-rank terms and sparse terms separately, compared with tra-ditional TF IDF strategy. We leave a comprehensive com-parisons with state-of-the-art and the possible ad-hoc str ate-gies with learning to rank (see, e.g. [12, 13]) for future work.
In this paper, we have argued that sparse and low-rank models may be more relevant for text data analysis than more traditional topic models. We have further demon-strated that Principal Component Pursuit is an effective tool for decomposing the given data into sparse and low-rank components, and suggested that this can indeed im-prove retrieval performance. However, as a new tool, its full capabilities are still far from well-understood. Alth ough we have focused on the retrieval task, we believe that this approach might be useful for a broad family of problems, including text classification, information retrieval, and doc-ument summarization, to name a few. Moreover, the scale could be extended to several order of magnitudes larger by the rapidly advancements in parallel and distributed com-puting. [1] B. Bai, J. Weston, D. Grangier, R. Collobert, [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] E. J. Cand`es, X. Li, Y. Ma, and J. Wright. Robust [4] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and [5] C. Chemudugunta, P. Smyth, and M. Steyvers.
 [6] C. H. Q. Ding. A similarity-based probability model [7] S. T. Dumais. An interdisciplinary perspective on [8] S. T. Dumais, G. W. Furnas, T. K. Landauer, [9] A. Gelbukh and G. Sidorov. Zipf and heaps laws X  [10] T. Hofmann. Probabilistic latent semantic indexing. In [11] T. Hofmann. Unsupervised learning by probabilistic [12] J.-W. Kuo, P.-J. Cheng, and H.-M. Wang. Learning to [13] T.-Y. Liu. Learning to rank for information retrieval. [14] J. Miao and A. Ben-Israel. On principal angles [15] M. Mitra and B. B. Chaudhuri. Information retrieval [16] C. H. Papadimitriou, H. Tamaki, P. Raghavan, and [17] M. Steyvers and T. Griffiths. Probabilistic topic [18] X. Wei and B. W. Croft. Lda-based document models [19] X. Yuan and J. Yang. Sparse and low-rank matrix [20] L. W. Z. Lin, M. Chen and Y. Ma. The augmentd
