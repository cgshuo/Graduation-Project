 Most information retrieval evaluation metrics are designed to measure the satisfaction of the user given the results re-turned by a search engine. In order to evaluate user satis-faction, most of these metrics have underlying user models , which aim at modeling how users interact with search engine results. Hence, the quality of an evaluation metric is a direct function of the quality of its underlying user model. This paper proposes EBU, a new evaluation metric that uses a sophisticated user model tuned by observations over many thousands of real search sessions. We compare EBU with a number of state of the art evaluation metrics and show that it is more correlated with real user behavior captured by clicks.
 H3.4 [ Information Search and Retrieval ]: Theory, Measurement, Experimentation Evaluation, Metrics, User Behavior, Click-through Data
Information retrieval evaluation metrics are designed to measure the satisfaction of users with respect to the results they receive for their queries. In the context of web search, users typically interact with the search engine by clicking and browsing a tiny fraction of the top-ranked documents returned by the engine. That is, although several relevant documents might be returned by the search engine, the user is often only influenced by the quality of scanned snippets and clicked documents. Therefore, to estimate the satisfac-tion rate of the user for a given result list, in addition to the knowledge about the relevance of documents, it is important to predict which documents are more likely to be visited by the user.

Most common evaluation metrics can be interpreted as simple user models in which the discount functions are used to assign more weights to documents that are more likely to be visited by the user. However, the user models of ex-isting metrics are often primitive and ad hoc. Furthermore, the discount function of current metrics are typically static and relevance independent (ERR [1] is an exception). That is, a relevant document at rank n is assumed to have the same likelihood of being visited by the user compared to a nonrelevant document at the same rank. Furthermore, the relevance of the document at rank n  X  1 is often ignored when computing the importance of the document at rank n .
Click logs can be used to design user models that are highly correlated with real user behavior. We propose a new user model that can be trained by using previous click logs. Inspired by the recent work on click prediction [2, 4], our user model assigns the importance of each document according to its probability of examination P ( E ) by the user and the probability that it satisfies the information need. Based on our suggested user model, we then design Expected Brows-ing Utility (EBU), 1 a new metric for evaluating information retrieval systems. EBU is an offline metric; even though we use past click data to learn parameters of our model, once these parameters are learned, our model does not need any further click information. It uses the labels of the documents retrieved in a ranked list to predict how users are expected to behave if this ranking were presented to them, and to assign the importance of documents.
Most user-centric search evaluation metrics in the liter-ature can be written as the inner-product of a relevance vector J and a discount vector D [9] (the latter is known as a utility vector in [9]).
 The relevance vector J is provided by manual judgments and can be multi-graded. The discount vector D represents the importance of each document  X  often  X  regardless of its relevance, i.e., the probability that a user will observe (or read) a document.

In the context of web search, it is plausible to assume that the importance of a document depends on its probability of
An earlier version of this work appeared in the SIGIR 2009 workshop on Understanding the User. Figure 1: The user model associated with EBU.
 Table 1: P ( C |R ) : Probability of click given the rel-evance of the document. P ( L|R ,C ) : Probability of leaving the search session given the relevance of the last clicked document.
 P ( C |R ) 0.49 0.45 0.55 0.71 0.94
P ( L|R ,C ) 0.43 0.40 0.49 0.67 0.94 getting visited or clicked by the user. While previous work [1, 9] interpreted D as a probability of examination (scan-ning), we agree with Turpin et al. [8] in that  X  X  relevant document should only make a positive contribution to a sys-tem metric if the user would actually view that document. X  We argue that clicked documents are usually more influen-tial than scanned documents in satisfying user information needs, and hence D should be measuring the probability of click. Therefore, the quality of a metric is directly depen-dent on how accurately the associated user model (hence, the discount function) estimates this probability.
Most evaluation metrics are based on user models that make simplistic and ad-hoc assumptions. Some important properties that are under-represented in IR evaluation met-rics are: metrics do not distinguish between the clicked documents and those that were only scanned. However, when a user examines a document, the user would only click on it if he is attracted by the snippet [8]. For example, the numbers in the first row of Table 1  X  extracted from a query log with available relevance labels (dataset described in the Appendix A)  X  show that the users are more likely to click on more relevant documents.
 expect the search behavior of users to change based on the relevance of the last visited document. That is, visiting a highly relevant document that perfectly satisfies the user X  X  information need (e.g. a navigational answer) shall be strongly correlated with the probability of terminating the search ses-sion. The second row in Table 1 shows the probability that the user leaves the search given the relevance of the last clicked document P ( L| C,R ). These values suggest that the users are more likely to leave ( L ), the search session after clicking ( C ), on a document with very high relevance ( R ), as expected.

Motivated by these properties, and inspired by the DBN click prediction model [2], we propose Expected Browsing Utility (EBU), a new evaluation metric based on the user model in Figure 1. A typical search session according to this user model can be described as: The user always exam-ines the top document and continues examining the ranked list of documents from top to bottom. At each rank r , the user first examines the document ( E r ) by scanning the snip-pet S r . Based on some probability related to the quality of snippet P ( C r | S r ) the user clicks on the document at rank r . Previous work suggests that probability of click is highly cor-related with the relevance of underlying documents. There-fore, we replace P ( C r | S r ) with P ( C r | R r ) in our model, where R r denotes the relevance grade of the document.
If the user clicks on the document at rank r , then the rele-vance of document is assessed. After the click, the user then leaves the search session with probability P ( L r | R r ,C continues by examining the next document. 2 Alternatively, if the document at rank r is skipped (not clicked), with probability P ( L r | C r ,E r ) the user leaves the search session, or continues scanning the next documents otherwise.
The above user model assumes that, if a document at rank r is examined, its previous document at rank r  X  1 is also examined. In addition it assumes that the first document and all the clicked documents are always scanned by the user.

In EBU, we set the discount value D of each document to its probability of getting examined and clicked. According to our user model, all the clicked documents must be examined and the probability of click for an examined document only depends on its relevance (which is reflected in the quality of its snippet). Therefore, the discount function of EBU for a document at rank r can be written as follows:
The user examines the document at rank r only if the user has examined the document at rank r  X  1 and has not then abandoned the search.
 where P ( L r  X  1 ,E r  X  1 ) varies depending on whether or not the document at rank r  X  1 has been clicked.
To simplify the notations we replace P( X = 1) with P ( X ) and P ( X = 0) with P ( X ) in our equations. ) ) Hence, Here,  X  r  X  1 is the probability of not leaving the search ses-sion after the document at rank r  X  1 is clicked, while  X  denotes the probability of click on rank r  X  1 given its exam-ination. The  X  parameter, specifies the persistence of user when skipping over documents, and in that respect is similar to the p value used in RBP.

Having derived the EBU discount function ( D EBU ) from the user model described above, the metric can be defined as the expected browsing utility that the user receives from a ranking of n documents.
 where, R r corresponds to the relevance grade of the docu-ment at rank r , and D EBU ( r ) is the discount value for the same document. Note that due to the recursive nature of user model, D EBU ( r ) depends on the probabilities of skip, click, and termination for all the documents presented before rank r .
The  X  parameter in Equation 6 is measuring the probabil-ity that the user does not leave the search session. The value of  X  can be inferred from click logs for each query-URL pair using DBN [2] or similar methodologies [5]. We assume that the probability of leaving the search session once a document is clicked, is strongly correlated with its relevance grade.
This is a simplifying but plausible assumption. Further-more, the numbers in the bottom row of Table 1 confirm that the probability of leaving the search session significantly varies for different relevance labels.

The  X  parameter in Equation 6 represents the click proba-bility for an examined document, and similar to  X  can be ap-proximated separately for each query-URL pair using DBN or maximum likelihood on click logs. Alternatively,  X  can be approximated according to the relevance of the examined document.
The strong correlation between P ( C r | E r ), and P ( C has been acknowledged in previous work [8], and is sup-ported by average numbers computed over our training data logs (the left column of Table 1). 3
Considering Equations (2) and (9), our user model as-sumes that the probability of click on a document is a func-tion of its snippet quality which itself is strongly correlated
For simplicity, we compute the average click probabilities independently for each relevance label. Figure 2: The accuracy of different discount func-tions for predicting the probability of click  X  P(click). with relevance. Therefore, we refer to P ( C | R ) as snippet quality in this paper.

Parameter  X  can be regarded as the user persistence when skipping over a document. As in RBP [7], we assume that  X  is a fixed value between [0-1]. We report the results for both  X  = 0 and leave further exploration for finding the most value of  X  as future work.
 uation metrics represent the probability of user satisfaction after visiting a document. In the presence of previous click information, these probabilities can be inferred for each rele-vance label or individually for each query-document pair [2, 3]. EBU does not force any particular choice of gain func-tion. However, it requires the following constraint to be met: gains to documents that are less likely to be viewed by the user. Assigning higher gains to such documents contradicts the basic assumption of our model that the user satisfaction is mainly affected by the clicked documents.
In order to learn the parameters of EBU, we use the train-ing data described in the appendix. We then use these pa-rameters to compute the discount function of EBU in the test set. Figure 2 compares the actual probability of click averaged over thousands of logged search sessions in the test set with the discount function of EBU. For compari-son purposes, the plot also shows the discount function of DCG [6] (with the commonly used 1 /log 2 ( r + 1) with and 1 /r discounts, where r is the rank at which a document is retrieved), RBP [7] and ERR [1] with their best parame-ters to a click log, and the actual click probabilities. For example, the probability of click for a document at rank 3 according to RBP with persistence parameter p = 0 . 4 is (0 . 4) (3  X  1) = 0 . 16. For the same document, DCG with the commonly used 1 /log 2 ( r + 1) predicts 1 /log 2 (3) = 0 . 63 chance of click on that document. The plots report the aver-age Root Mean Squared (RMS) error between the probabil-ity of click assumed by a metric and the actual probability of click. It can be seen that the probability of click predicted by some metrics is quite different than the observed average  X  X xcellent X  respectively. click probability over different sessions and EBU is much better than other metrics at predicting probability of click.
The success of EBU is mainly due to its dynamic discount function that can assign different weights to documents ac-cording to their probability of clicks and examination (com-puted based on relevance). Figure 3 shows two sample cases taken from our test logs in which the discount function of EBU behave very differently. The left plot, shows the av-erage probability of click drops significantly after visiting a perfect document at rank one. On the right hand plot how-ever, EBU assigns more weights to documents lower in the ranked lists, as they are more likely to get visited by the user since the document at rank 1 is less likely to satisfy the information need.
Most evaluation metrics in information retrieval aim at evaluating the satisfaction of the user given a ranked list of documents. These metrics are based on some underly-ing user models which are assumed to be modeling the way users search. However, most of these models are based on unrealistic and ad-hoc assumptions.

Click logs contain information about how users behave. In this paper, we showed how click logs can be used to devise enhanced evaluation measures that are more correlated with real user behavior. We first described the main shortcomings of the commonly used evaluation metrics and showed that these can be addressed by using a more sophisticated user model. We then introduced EBU, a novel web search eval-uation metric that is directly derived from this user model. In future, we would like to extend EBU to account for the topical diversity and dependency.
