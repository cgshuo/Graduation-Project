 Cross-lingual text mining aims to transfer knowledge across different languages to help applications such as cross-lingual information retrieval, summarization and categoriza-tion in cases where translation and class-la beled resources are scarce. A specific chal-lenge is to build models that capture content of comparable corpora, i.e. texts that to a varying degree contain shared and non-shar ed content. Recently, with the rapid devel-opment of online social networks, such as Facebook, MySpace, Ning, and Twitter, users have generated a huge volume of valuable multilingual l anguage resources. However, content is seldom well-paired across the languages, forming cross-lingual parallel cor-pora. For example, a user may write a lot in English about her recent travel in a blog, but write only a few sentences in French. Indeed, even in well-organized Wikipedia doc-uments, one cannot find an exact translation of a page across the different languages. However, there are many so-called  X  X omp arable X  corpora available, where the docu-ments discuss similar content in differe nt languages, but the content in one language is not an exact translation of the content in t he other language. Extracting knowledge from these comparable corpora is very valuable for cross-lingual tasks.

Current research for text categorization on bilingual corpora often focuses on learn-ing the classification model from monolingual labeled documents and their cross-lingual pairing. Few efforts are made to directly build cross-lingual models, with which the classification knowledge can be transferred across different languages.

In this paper, we address the problem of knowledge transfer across multilingual and comparable corpora. We propose a unified pr obabilistic model to simultaneously extract latent topics from the multili ngual corpora. The learned topic models capture the com-mon knowledge across the different languages which can be used in many applications. We apply the topic models to bilingual documen t classification. Our experimental results on multilingual Wikipedia data (written in Eng lish, Spanish, French and Italian) show that the proposed Bilingual Latent Dirichlet Allocation (Bi-LDA) model can effectively represent a target language without the use of any information from translation dictionar-ies. Additionally, the learned bilingual topic models yield a very strong categorization performance in target languages, for which no labeled training examples are available.
The rest of the paper is organized as follo ws: Section 2 formally formulates the problem. Section 3 explains the proposed model. Section 4 gives experimental results that validate the effectiveness of our methodology. Finally, Section 5 discusses related work and Section 6 concludes. In this section, we present several necessary definitions and then define the subproblems of knowledge transfer acros s multilingual documents addressed in this paper.
A paired bilingual document corpus can be defined as C = { ( d S 1 ,d T 1 ) ,  X  X  X  , ( d source language S and the target language T , respectively. Each document may de-scribe a number of topics. For example, the Wikipedia page of a city may describe topics related to history, culture, and tourism. Before formulating the problem, we first give the definition of a probabilistic topic model.
 Definition 1. Topic model of documents : A topic model  X  of a document collection D is a multinomial distribution of words p ( w |  X  i ) over a vocabulary V, for each  X  i represented in  X  d where d  X  D . The document collection is considered a mixture of multiple topics  X  .
 The underlying assumption of a topic model is that words in the document are sampled Therefore, words with the highest probability in the distribution represent the semantic field contained in the topic. For example, the words travel , tourist ,and hotel would represent the topic  X  X ravel X . Given this, we can define the problem of bilingual topic modeling.
 Problem 1. Bilingual topic modeling. Given a collection of paired documents in lan-eling is to learn for every document pair a set of K topics  X  , each of which defines an associated set of words in S and in T .
 The topic model  X  bridges knowledge across documents in two languages, which cre-ates many potential applications. In this work, we consider a general application, i.e. bilingual document classification. In particular, we consider how to take advantage of the topic models to transfer knowledge from one language to help document classifica-tion in another language. More specifically, let L S be a labeled document collection in the source language, in which each document d S j is annotated with a class label c  X  X  S , where Y S = { c 1 ,  X  X  X  ,c S p } denotes the label space and S p is the number of class la-bels in the source language. Let U T be a unlabeled document collection in the target language. Formally, we can define the problem of bilingual document classification as follows.
 Problem 2. Bilingual document classification. Given a labeled document collection L S in the source language S , an unlabeled document collection U T in the target lan-guage T , and the learned bilingual topic models, t he goal is to transfer the labeled supervision information from the source language to predict the label of the documents in the target language.
 Please note that although we only give the definition of bilingual document modeling and classification, the problem can easily be extended to multilingual corpora. For bilingual topic modeling, we can simply consider a general topic model as a base-line method, i.e. Latent Dirichlet Allocation (LDA) [3], to model the topic informa-tion of all bilingual documents. We propose a probabilistic topic model, referred to as Bi-LDA, to simultaneously model bilingual documents within a unified model. The model describes each pair of bilingual documents ( d S j ,d T j ) using a common mixture  X  j , thus knowledge can be transferred across different languages via the common mixture model.
 In the remainder of this section, we will first briefly review Latent Dirichlet Allocation, and then describe our proposed approach in detail. 3.1 Latent Dirichlet Allocation Recently, probabilistic topic models attracted considerable interest and have been suc-cessfully applied to text mining tasks such as information retrieval, recommendation, and text analysis [13,3]. Latent Dirichlet Allocation (LDA) [3] is a three-level Bayesian network, which models documents using a latent topic layer. In LDA, for each docu-ment d j in the corpus, a multinomial distribution  X  j over topics is first sampled from a Dirichlet distribution with parameter  X  . Second, for each word w ji , a topic z ji is chosen from  X  j and the word w ji is generated from a topic-specific multinomial  X  z ji . Thus, the generating probability of word w from document d j is: In other words, it uses the topic distribution  X  j of the document and the probability  X  w z of the topic generating word w to calculate the word X  X  probability. Directly applying LDA to our bilingual problem means that the contents of both documents of a pair have to be combined together as one big document, learning a topic model were the two languages are mixed. This approach will serve as our baseline. 3.2 Bilingual Latent Dirichlet Allocation Algorithm 3.1: B ILINGUAL LDA() for each document pair d j do Figure 1 shows the graphical representatio n of the proposed model, Bilingual Latent Dirichlet Allocation (Bi-LDA). In Algorithm 3.1 we present its generative story. For each document pair d j , a topic distribution  X  is sampled from a K -dimensional Dirichlet distribution with parameter  X  .  X  defines a distribution common to both languages ( S and T ). Then, each word w S ji in the source language is generated from a multinomial distribution  X  z S language is also sampled with a same procedure. We see that there is a one common  X  for both languages, which implies that all topics in  X  are shared across the bilingual documents.

To train this model, we used the Gibbs sampling approach. This requires two sets of formulas to converge to correct di stributions: one for each topic z S ji and one for each topic z T ji . For the first, the updating formula becomes: where n S j,k is the number of times that topic k has been sampled from the multinomial counted. v S k,  X  ,  X  is the number of times that word w S ji in language S has been generated by topic k minus one (not including the currently associated word w S ji ). In these coun-ters a dot (  X  ) means summation over all values of this variable, i.e. all topics in case of n  X  and all words in v S k,  X  ,  X  .

For z T ji , the change in formula 2 is trivial. 3.3 Cross-Lingual Document Classification For cross-lingual document classification, we are given a set of labeled documents in the source language and no labeled documents in the target language. The objective is to learn a classification model from the labeled documents of the source language and apply to the classification of documents in the target language. The task obviously cannot be achieved by the traditional method th at only uses words as features, as there is minimal word overlap between the two languages.

Our idea for cross-lingual document classification is to take advantage of the com-mon topics learned by the proposed topic models, to transfer the knowledge from the source language to the target language. Basically we first learn the topic models (either LDA or Bi-LDA) on a general bilingual corpus (e.g., Wikipedia). Then given a bilin-gual document classification task, i.e., a L S in language S and an unlabeled document collections U T in language T (in the same domain as the source documents), we use the learned topic model to infer the topi c distribution of each document in L S and U T .
Each document is then taken as a data insta nce in the classification model and the features are defined as the inferred topic dis tribution. The value of each feature of an instance (e.g., document d S j ) is the probability of th e corresponding topic k in the doc-Naive Bayes, Perceptron, Maximum Entropy, and Support Vector Machine (SVM). In this paper, we use SVM. 4.1 Experimental Setup Datasets: We conduct the experiments on two datasets, one (called Topic) for training and evaluating the topic models, and one (called Classification) for evaluating the trans-fer classification. Both datasets are downloaded from the online encyclopedia Wikipedia, from the English, Spanish, French and Italian language sections. All documents were acquired from Wikipedia dumps , which mirror the entire online encyclopedia, are regularly updated and can be downloaded freely.

For the Topic dataset we collect three bilingual corpora with paired documents by following  X  X anguage links X  between articles. Table 2 shows statistics of the Topic dataset. Note that not every article appears in each la nguage, resulting in a different content for each bilingual dataset.

The Classification dataset, which is differ ent from the Topic dataset, is collected by exploiting the category-labels of Wikipedia. Specifically, we first select 5 high level categories: books ( X  X ook X ) , films ( X  X ilm X ) , programming languages ( X  X rog X ) , sports ( X  X port X ) and videogames ( X  X ideo X ) . Then for each category, we extract up to 1,000 articles which are annotated with the category label. To acquire a workable set of doc-uments, the categories were chosen to have examples of very broad and more specific classes. A Wikipedia article can have more than one label, and these labels can be very specific. Using the hierarchy that Wikipedia provides, we extract all subcategories for the above base classes up to three sublevels. Articles are then crawled that belonged to any one of these subcategories. Since not all Wikipedias have as large a collection of articles, we sometimes collected fewer than thousand articles for Spanish, French and Italian. Table 4.1 shows the size of the Classification dataset.
 Comparison Methods: We compare the following met hods for bilingual classification:  X  SVM+LDA . This will be our baseline method. It combines each bilingual pair of  X  SVM+Bi-LDA . This method uses the proposed Bi-LDA to learn the topic distribu-The code for this process is implemented in C++ and will be publicly available along with the data sets. 4.2 Perplexity Perplexity measures a topic model X  X  capability of predicting previously unseen doc-uments. For a collection of these new documents (in our case the articles from the Classification dataset) C u , it is calculated as: A lower perplexity score means that the model has assigned a higher probability to the documents. Theoretically, if a model is good, i.e. has a low perplexity, it will be well adapted to the new documents and therefore yield a good representation. Although trained on paired bilingual documents, inference of each of these models has to hap-pen on one language at a time (as we do not have any apriori information of the test document X  X  content). Therefore we present the perplexity for both of the languages separately.

Table 4 lists the perplexity of each model for all three language pairs, averaged over a number of different parameter settings for the model. These settings are: (named settings will be used in further experiments)  X  for LDA and Bi-LDA: ranging the number of topics from 10 to 200 in steps of 10.
The difference between the perplexity of LDA and Bi-LDA can be explained easily: since the vocabulary sizes doubles by merging the two languages in the LDA model, it follows that the probability of each word halv es, which then again results in a doubling of the perplexity. 4.3 Classification Accuracy The use of knowledge transfer in a cross-lingual text categorization task is to our knowl-edge not studied in the literature. As a baseline we use LDA performed on concatenated texts (SVM + LDA) where the two vocabularies are mixed. Table 5 summarizes the per-formance of the models for each of our chosen classes, in each language pair. The F1 score is again averaged, over the same ranges of number of topics used for Table 4. It can be seen that the Bi-LDA model realizes an acceptable transfer of categorization knowledge.

In order to better assess the capab ilities of the pr oposed topic models for cross-lingually transferring knowledge, w e perform a number of additional tests. 4.4 Topic Smoothing In this subsection, we first analyze how the bilingual document classification is influ-enced by different parameters (i.e., number of topics and hyperparameters), and then present a new method, called topic smoothing, to further improve the performance of bilingual document classification.
 Effect of #topics on the categorization We train the Bi-LDA model with the number of topics varied, and each time we apply the learned model to the bilingual document classification. Figure 2 plots the F1-score of the classification results using Bi-LDA, when choosing a different number of topics apriori . We see on some categories the classification results are not very sensitive to the number of topics, except when the number is very small. But on several tasks (e.g., the classification on Italian), the results vary largely with the different number of topics.
 Topic smoothing. To deal with the above sensitivity problem, we present a strategy of topic smoothing. Basically, for Bi-LDA the smoothing method is to combine topic models that are learned with a different number of topics for bilingual document clas-sification. We train ten topic models, with the number of topics ranging from 10 to 200 with a 20 topic step. Then we apply the topic models to inference the topic distribution of the documents in the test data set, and concatenate all the different topic distributions as features of the SVM to train the classifi cation model. Specifically, different topic models are trained using different hyperparameters, and then topic distributions from the different topic models are concatenated as features to train the classification model. Table 6 shows the comparison of Bi-LDA and sBi-LDA. It can be seen that the smooth-ing method can efficiently improve (averagely +8.5%) the classification performance. As a trade-off of course, it requires substantially more computation.
 Cross-lingual text mining is a popular research topic and quite a few research works have been conducted for cross-lingual information retrieval (e.g., [12,27,21,24]); how-ever, cross-lingual classification is up till no w rarely studied. Existing methods often rely on a translation tool to bridge documents of different languages and thus transform the problem into a monolingual classification [9,22]. Cross-lingual sentiment classifi-cation in text has recently drawn the attention [ 17] relying on translation dictionaries. Some efforts have also been made to reduce the number of labeled examples in both languages using techniques such as co-trai ning [1,25]. However, these methods still rely on a well-organized parallel multilingua l corpus [11] or on a translation tool. In the work presented here we train and test on comparable corpora and do not make use of any external translation resources.

Recently, how to model the multilingual cor pora so as to discover the common and differential topics among the different languages becomes an interesting research topic and many methods have been proposed. A basic idea in these methods is to use La-tent Semantic Analysis (LSA) [19,6,26], document clustering [16], word alignment and machine translation [28], and parallel LDA [20,18,8] to find the correlation between different languages. While much progress has been made, two fundamental problems have been largely ignored. First, the bilingual corpora may be quite unbalanced, i.e., documents of the source language may not be comparable with the target language. The cross-lingual documents on the Web, in particular the Web 2.0, are freely authored by the users, thus the contents would be very different. Second, it is unclear how to model the bilingual documents simultaneously. Directly employing LSA, clustering, or LDA can only model the bilingual content in a common space, but cannot differentiate the topics specific to each language.

Another related work is transfer learning, which aims to transfer knowledge from a source domain to a related target domain. Two fundamental issues in transfer learning are  X  X hat to transfer X  and  X  X hen to transfer X . Many approaches have been proposed by reweighting instances in the source domain for use in the target domain [7]. [10] propose a locally weighted ensemble framework which can utilize different models for transfer-ring labeled information from multiple training domains. Also many works rely on new feature representations [14,15]. For example, [2] propose a method to learn a shared low-dimensional representation for multiple related tasks and the task functions simul-taneously. [23] propose to use a large amount of unlabeled data in the source domain to improve the performance on the target domai n in which there are only few labeled data. They don X  X  assume that the two domains share the class labels or topic distributions. [4] proposed a structural correspondence l earning approach to induce correspondences among features from source and target domains . There are also other approaches which transfer information by shared parameters [5] or relational knowledge. Transfer learning techniques are widely used in classification, regression, clustering and dimensionality reduction problems.

The use of bilingual topic models for transferring the category knowledge across languages is completely new. Moreover, our topic models are trained on comparable corpora which are abundantly available. In this paper we investigate knowledge transfer across multilingual corpora via latent topics. We formalize the major tasks and propose a probabilistic approach to solve the tasks. We study and compare several strategies for simultaneously modeling the content of bilingual documents. One is the bilingual topic model based on LDA. We present a sampling algorithm to learn the model. Experimental results for categorizing Wikipedia documents according to their labels demonstr ate the effectiveness of the proposed trans-fer learning approach.

There are several directions for future work. It would be interesting to develop new algorithms to automatically find the number of topics, and detect topics that are not shared between comparable documents. As the information in different languages might be very unbalanced, the numbers of topics in the documents of different languages could also be different. Another potential issue is to apply the proposed approach to other applications (e.g., recommendation, and lin k prediction across multilingual documents, cross-lingual information retrieval or cross-lingual summarization) to further validate its effectiveness.
 Wim De Smet is supported by WebInsight (K.U.Leuven-Tsinghua collaboration BIL/008/008) and the AMASS++ project (IWT-SBO 060051). Jie Tang is supported by NSFC(61073073, 60703059, 60973102), Chinese National Key Foundation Research (60933013, 61035004) and National High-tech R&amp;D Program(2009AA01Z138).

