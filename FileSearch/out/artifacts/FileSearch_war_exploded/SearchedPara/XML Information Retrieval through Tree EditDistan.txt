 XML documents are organized through s emantically meaningful elements. As content is distributed over different levels of the document structure, using this information should improve the overall search process as well as it enables the returned information to be more focused on the expressed needs. In this context, Semi-Structured information retrieval (SIR) models aim at combining content and structure search processes.

XML documents are structured through nested tags. This hierarchical organi-zation is naturally expressed through a s pecific graph representation, i.e. trees 1 , in which nodes are elements and edges hierarchical dependencies. In an XML tree, the text is located in the leaves which are the bottom nodes of the hi-erarchy. In structured retrieval, qu eries can be expressed using either Content Only constraints (CO) or both Content And Structure constraints (CAS). As for XML documents, the structural constraints expressed in CAS queries can be visualized through a tree representation, and might contain two parts: the target element indicates the tag element we want to retrieve and the rest of the structural constraints is called support or environment .
 Figure 1 shows a conversion example of an XML document and a query 2 .The target element is  X  X  X . For clarity reaso ns we shorten the tags. In real case the  X  X  X  is equivalent to the semantically richer  X  X aragraph X .

Based on these representations we propose a SIR model based on both graph theory properties and content scoring. To our knowledge, only few SIR models explicitly use tree matching algorithms between documents and queries. Most of them reduce the structure of documents to strings which impoverish the amount of information available to the matching process. Some authors such as Alilaouar et al. [1], Ben Aouicha et al. [2] and Popovici et al. [16], lose a high level of details to get back to their domain known approaches. Our approach uses the well-known tree edit distance combined with a traditional IR model for content. This allows to use the whole document str ucture to select relevant elements. Moreover to improve space and time complexity we propose three methods to summarize documents.

The rest of this paper is organized as follows: Section 2 pr esents existing tree matching algorithms and XML retrieval approaches using trees; Section 3 presents our model and finally Section 4 discusses the experiments and results obtained by our approach using the SSCAS task of the INEX 2005 campaign. In this section we will first overview so me tree matching algorithms and then give a brief survey on some SIR approaches based on tree matching. 2.1 Structural Similarities between Trees Two graphs are called isomorphic if they share the same nodes and edges. Eval-uating how isomorphic are two graphs is called graph matching. We make the distinction between approximate matching and exact matching. The first one at-tempts to find a degree of similarity between two structures while exact matching tries to validate the similarity. Because of the context of our work, we will focus here on approximate tree matching. There are three main families of approxi-mate tree matching: alignment , inclusion ,and edit distance .Asthelateroffer the most general application we will focus on this one. Tree edit distance algo-rithms [18] generalizes Levenshtein edit distance [14] to trees. The similarity is the minimal set of operations (adding, removing and relabeling) to turn one tree to another. Given two forests (set of trees) F and G ,  X  F and  X  G their rightmost removing (or adding) and relabeling, the distance d ( F, G ) is evaluated according to the following recursive lemma: Operations (a) and (b) are respectively the cost c del () of removing  X  F or  X  G while (c) is the cost c match () of relabeling the  X  F by  X  G .Later,Kleinetal.[13] reduced the overall complexity in time and space by splitting the tree structure based on the heavy path (defined in Section 3.3). Demaine et al. [6] further improved this algorithm by storing substrees scores in order to reduce calculation time. Finally Touzet et al. [8] used a decomposition strategy to dynamically select always the best nodes to recurse on between rightmost and leftmost which reduce the number of subtrees in memory. The best tree edit distance algorithms uses  X  ( nm ) on space complexity and between  X  ( n  X  log ( n ) .m  X  log ( m )) [8] and  X  ( n.m (1 + log ( n m ))) [6] on time for n and m the respective sizes in nodes of two trees T 1 and T 2 . Tree edit distance algorithms are efficient but slow on large trees. One way to overcome this issue in SIR is to reduce the document tree size by pruning or summaries. Used mainly on indexing and clustering, the latter is based on the intuition that the document underlying structure can be captured efficiently with a smaller size representation. As we will see in Section 2.2, the summarizing algorithm proposed in [4] provides satisfying results in improving the runtime. In this paper, we will apply this algorithm to XML retrieval. 2.2 Semi-structured Information Retrieval XML documents as well as CAS queries can be represented as trees. It then seems natural to apply tree matching algorithms for the document-query match-ing process. However, following INEX pro ceedings overviews ([7] and [9]), tree matching is uncommon in SIR. One can however find two main categories of retrieval approaches in the literature using XML trees matching.
 The first one uses relaxation , which means reducing the links or constraints. Relaxing is a research space expansion process. The general idea is to translate the tree structure into a set of binary weighted edges. This simplified repre-sentation allows to use traditional IR models. For example, Alilaouar et al. [1] combined relaxation and minimal covering trees. Similarly Ben Aouicha et al. [2] relaxed the whole hierarchical constraints by adding virtual edges. These edges are weighted based on the initial hierarchical distance in the document structure. The resulting arcs are then projected on a vector space and the document-query matching is done using a traditional Vector Space Model . Some other approaches as for example [5] use fuzzy closing in which a set of virtual edges representing all tree X  X  transitive relationships is created based on the closing property. One can also found approaches that directly use edit distance algorithms .Popovici et al. [16] translated documents tree structure in a set of paths and evaluated the document-query similarity using the Levenshtein [14] string edit distance. Other works using tree matching algorithms can be found in IR related domains, like error detecting and clustering. In their papers, Boobna [3] and Rougemont [17] check XML documents conformity t o their DTD thought tree edit distance. Similarly, Dalamagas et al. [4] summarize documents before applying Tai [18] algorithm to cluster them.

The fact that there are relatively few SI R models using explicitly tree match-ing algorithms could be due to the complexity of such approaches. All of the previously presented models always applie s trees conversion to restrict the prob-lem to a smaller search space. This removes a lot of details. In this paper we attempt to overcome the complexity drawback by summarizing the structure without loosing too much of the structural information. We assume that a query is composed of content (keywords) and structure con-ditions, as shown in Figure 1. The document-query similarity is evaluated by considering content and structure separately, and we then combine these scores to rank relevant elements. In this section, we first describe the content evalua-tion. We then present our subtree extraction and summary algorithms. Finally we detail our structure matching algorithm based on tree edit distance. 3.1 Content Relevance Score Evaluation First, we used a tf  X  idf (Term Frequency  X  Inverse Document Frequency [11]) formula to score the document leaf nodes according to query terms contained in content conditions. To score inner nodes, our intuition is that a node score must depend on three elements. First, it should take into account its leaves scores, that form what we call intermediate score. Second we should score higher a node located near a relevant element than a node located near an irrelevant one. Finally, there must be a way to balance the hierarchical effect on the node score. Based on these constraints we define the content score c ( n )ofanelement n as the intermediate content score of the element itself plus its father X  X  intermediate score plus all its father X  X  descendants score . Recursively, and starting from the document root: c ( n )= ( i )isthe intermediate content score part with | leaves ( n ) | the number of leaf nodes descendants of n and p ( n ) the intermediate score of the node based on evaluated using a tf  X  idf formula. ( ii )isthe neighborhood score part which allows us to convey a part of the rele-vance of a sibling node through its father a 1 . p ( a 1 ) is the intermediate score of a 1 and ( iii )isthe ancestor scores ,with c ( a 1 ) the final content score of the father a 1 minus its intermediate score. 3.2 Extracting and Summarizing Subtrees rooted by all its ancestors starting from the first one whose tag is similar to a tag in the query. In Figure 2, five subtrees are extracted.
 Even the best edit distance algorithm runs with the minimal time complexity O ( n 3 ) [8], it thus remains costly on larg e trees. In order to reduce the match-ing space, we choose Dalamagas [4] summary rules which seem to be the best compromise between preserving the overall structure and speed. As illustrated in Figure 3 these rules are  X  X emove nesting X  which is equivalent to move the subtree of a node having the same label than one of its ancestor; and  X  X emove duplicates from father-child relationship X  which removes the siblings having the same labels. Based on these rules we cr eate four different summary versions. The first one, illustrated on top, is the strict adaptation of these rules to all nodes while the second version, illustrated at the bottom, apply the summary rules only for the nodes which labels are not in the query. Thanks to this version we keep as many relevant nodes as possible while reducing the document size. Finally, in the third and fourth versions, we adapt the first two ones by keeping a record of the number of removed nodes. This will be used to multiply the edit 3.3 Structure Relevance Score Evaluation As seen in Section 2.1, the tree edit distance is a way of measuring similarity based on the minimal cost of operations to transform one tree to another. The number of subtrees stored in memory during this recursive algorithm depends on the direction we choose when applying the operations. Our algorithm is an extension of the optimal cover strategy from Touzet et al. [8]. The difference is that the optimal path is computed with the help of the heavy path introduced by Klein et al. [13]. The heavy path is the path from root to leaf which pass through the rooted subtrees with the maximal cardinality. This means that selecting always the most distant node from this path allows us create the minimal set of subtrees in memory during the recursion creating the optimal cover strategy .  X  ( n
This strategy is used on the document and the query as input to our tree edit distance algorithm (Algorithm 1). F, G are two forests (i.e. the document and the query as first input), and p F and p G are positions in O F and O G the optimal paths (i.e. paths of the optimal cover strategy ). Function O.get ( p ) returns the node in path O corresponding to position p ..

To evaluate the final structure score of a node n , we average the tree edit distances between its subtrees and all of its ancestor ones and the query. These distance are averaged by their cardinality in order to reduce the gap size between the subtrees. With Anc ( n )thesetof n ancestors; a  X  Anc ( n ); T ( a ) the subtree rooted in a ; d ( T ( a ) ,Q ) the edit distance between the tree rooted in aT ( a )and Q , the structure score s ( n ) is formally:
Algorithm 1. Edit distance using optimal paths 3.4 Final Combination The final score score ( n ) for each candidate node n extracted as explained in Section 3.2 is evaluated through the combination of the previously normalized scores  X  [0 , 1]. Then the elements correspondi ng to the target nodes are filtered and ranked. Formally, with  X   X  [0 , 1]: We evaluated our approach on both summarized and unsummarized subtrees on the INEX 2005 collection and compared our results with the official participants. 4.1 INEX Collection INEX ( Initiative for the Evaluation of XML Retrieval ) is the reference evalua-tion campaign for XML retrieval. To evaluate our approach we used the 2005 collection which is composed of 16000 XML documents from the IEEE Computer Society scientific papers. These documents have an average of 1500 elements for a hierarchical depth of 6.9.
Two main types of queries are available, namely Content Only (CO) and Con-tent And Structure (CAS). Tasks using CAS queries are centered on structural constraints and were not reconducted in later campaigns 3 . Four subtasks were proposed. To evaluate queries in which structural constraints are semantically relevant, we use in our experiments the SSCAS subtask, in which constraints are strict on the target element and its environment.
 There are two measures for the CAS subtasks [12]: Non-interpolated mean aver-age effort-precision (MAeP) which is used to average the effort-precision measure at each rank and Normalized cumulated gain (nxCG). This last one corresponds to the cumulative gain at a threshold and is based on the ideal ranking over the sum of all score to that threshold.
 Finally we will use the  X  X trict X  quantization to aggregate specificity and exhaus-tivity relevance judgments as it only takes into account fully relevant elements. 4.2 Experiments We run our algorithm with four summary versions of document and query trees. Our baseline is unsummarized version ( full ). Regarding the summary versions, dalamagas summary corresponds to the run for the exact summary rules while in partial summary nodes containing tags from the query are not summarized. The multi extension for the both previous versions are for the cases in which we keep a record of the number of removed nodes as a coefficient in the edit distance. We set the removing cost c del () to 0.5 for a label in the query and 1 otherwise and the relabeling cost c match () to 0 for similar tags (eg: section and subsection are considered as semantically similar) and 1 otherwise.

Figure 4 shows the results for the MAeP metric depending on the  X  parameter of equation (4). The best tuning for our system is around 0.5 which means an equal part of content and structure. However, if all of our solutions return better results around the same value of  X  it is clear that two of them are well over the others (more than 80% for the MaeP ): our full tree algorithm and the partial summary algorithm in which nodes that have tags equivalent to the query ones are not summarized. We further notice that the results are equivalent for these two cases which means that we can gain on runtime while keeping the score of our intial solution. Moreover, keeping t rack of the removed nodes does not im-prove our results. Even if it seems natural for the dalamagas summary in which the tree structure is strongly altered through the removal of duplicate node de-scendants, it seems more surprising for our partial summary .Itcanhoweverbe explained by the fact that it artificially increases the number of nodes for a sub-stitution score equal to 0. It means that it gives the same score for a summarized tree with several noisy children nodes than for a subtree with only the correct number of relevant nodes.Considering now efficiency, Dalamagas summary re-duces the number of nodes to 48%, while our partial summary version reduces their number to 45%. Regarding the running time, it is improved 20 times on the average.

Figure 5 shows the results for  X  = 0.5 over the various INEX 2005 metrics compared to the best participants, namely the Max Planck institute with TopX [19] a database-centered approach; IBM Haifa Research Lab [15] with a vector space model and the University of Klagenfurt [10] which also uses a vector space model. While our full tree and partial summary runs score slightly the same than the first participants for the nxCG (an average of 5% under the first while 13% over the second), our MAeP results are overall better (+45 % compared to the first one and +90% for the second) for all our runs. 4.3 Conclusions and Future Work In this paper we presented an XML retrieval approach whose main originality is to use tree edit distance. This solution allowed us to outperform other approaches on the MAeP measure by 45% while proving the usefulness of structural infor-mation in SIR process. However its main drawback, i.e. complexity, was time consuming on the runs. We overcame this issue with a new set of summary rules which allowed us to keep the effectiveness of our solution while improving effi-ciency though search space reduction. In future work we will improve our content score as well as the overall edit distance cost system in order to better use the tag semantics. Finally we are currently working on the INEX 2010 Datacentric collection which will allow us to confirm our results on a bigger and semantically richer collection.

