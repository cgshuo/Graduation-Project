 Many real world applications increasingly involve both struc-tured data and text, and entity based retrieval is an impor-tant problem in this realm. In this paper, we present an automatic query reformulation approach based on entities detected in each query. The aim is to utilize semantics asso-ciated with entities for enhancing document retrieval. This is done by expanding a query with terms/phrases related to entities in the query. We exploit Wikipedia as a large repository of entity information. Our reformulated approach consists of three major steps : (1) detect representative en-tity in a query; (2) expand the query with entity related terms/phrases; and (3) facilitate term dependency features. We evaluate our approach in ad-hoc retrieval task on four TREC collections, including two large web collections. Ex-periments results show that significant improvement is pos-sible by utilizing entity corresponding information. H.3.3 [ Information Search and Retrieval ]: Search and Retrieval -search process, query formulation Algorithms, Experimentation Entity, Query Expansion, Term Dependency, Wikipedia
One of the fundamental task of information retrieval is to search for documents that satisfy a user X  X  information need. Despite the recent advance in search quality, current appli-cations of document retrieval still rely mainly on keyword matching methods based on query terms. For example, in the query  X  Bill Gates  X , each of the words will be used as a keyword. As a consequence, documents containing the key-words  X  bill  X  and  X  gates  X  will be retrieved. However, the doc-uments or passages retrieved do not necessarily contain the information which is compatible with the information need, i.e. facts on  X  Windows  X  and  X  Microsoft  X  in this example.
In this paper, we propose an automatic query reformula-tion approach, based on entities detected in queries. Our goal is to utilize semantics associated with entities for en-hancing document retrieval. An entity is something that has a distinct, separate existence, which can be a person, a place, an organization or miscellaneous. The information associated with an entity is more abundance and less am-biguous for retrieval task than query terms. Thus it is our hypothesis that retrieval performance can benefit from in-corporating such information.
 We use a thesauri-based approach for entity detection. Wikipedia is a free online encyclopedia, it records one arti-cle for a real world entity, with information focused on this entity. It is in fact a large manually edited repository of entities. Its large volume of data, abundance of structural wikitext and high quality contents make it a perfect candi-date for this task.

Given a query, we identify the most representative entities for it. Then we collect a set of terms/phrases closely related to these entities, according to Wikipedia entity pages. Fi-nally the query is enriched by adding the most semantic related terms/phrases for new round of retrieval.
Entity Detection. Given an n-word query q ( w 1 , w 2 , ... , w n ), the entity recognition starts from q itself (n-word candidate, a string of n consecutive words in the original query); if failed, it searches in the (n-1)word sub-phrases of q . The aim of searching is to detect the most representative entities in a query, rather than fetch all the possible entities in a query. Thus the process stops when all the query terms are covered by entities , or reaches the 1-word candidates. We require p to meet the following standard to be considered as an entity : there is an entity page titled exactly by p , or p is redirecting to an entity page . Note that a term or a phrase might be associated with more than one entities, we disambiguate it by choosing the well known meaning for it. This is indicated as primary topics in most disambiguation pages.

Selecting Terms/Phrases for Expansion . We extract entity related terms/phrases from an Wikipedia page. De-tails are given in Table 1.

While there exists large number of terms/phrases related to an entity based on Wikipedia entity pages, not all of them are compatible with the user X  X  information need. For this, we propose a strategy to rank candidate terms/phrases in descending order according to their degree of similarity with the query. Only the top k candidate terms/phrases are used for expansion. Resnik X  X  information content based similarity measure[4] is used to represent similarity between two terms. Table 1: Summary of resources for query expansion. The number of entries indicate how many entities have the corresponding feature, rather than unique (entity,feature) pair.
 And the similarity between a candidate term/phrase and the query is computed as follows: where sim ( i, j ) is semantic similarity between term i and j using Resnik X  X  measure. C is set of terms in a candidate concept , and Q is set of query terms.

Utilizing Term Dependency . Query terms appear con-tiguously within a query provides stronger evidence about the information need[2]. Metzler &amp; Croft has shown that retrieval performance can benefit from incorporating term dependencies[2]. In this work, we also considering adding multi-word entity as term dependency features. In our work, an ordered term dependency feature(OTDF) requires those terms in a phrase to appear ordered with at most N-1 terms between each. Both multi-word entity and selected entity related phrases are added as OTDF. We set N = 2 in our experiments.
Both indexing and retrieval experiments were carried out using Indri search engine[1]. All collections were stemmed using Porter stemmer. Documents are retrieved using Markov Random Field retrieval model[2]. Table 2 provides a sum-mary of the TREC data sets considered.
 Table 2: Overview of TREC collections and topics
The reformulated queries consist of two parts: the origi-nal query Q , and the expanded query Q 0 .  X  and 1  X   X  are weight constraint for each part in the ranking function re-spectively. For the baseline unigram language model,  X  is 1.0. For reformulated queries, we use a fix  X  = 0 . 8 across our experiments.

Here are the methods that we use for testing: TX: ex-pand the query with top N terms according to criteria given in section 2 PX: expand the query with top N phrases se-lected according to criteria given in section 2. We compared our methods with baseline unigram model, which adds no terms/phrases to original query at all.
 Table 3: Comparison of effectiveness in terms of MAP. * indicates a significant improvement over the unigram model.(p &lt; 0.05 with a one-tailed paired t-test)
The results are shown in Table 3. Overall, compared with unigram method, experimental results using our reformula-tion approaches lead to noticeable improvements. Compar-ing among collections, our approaches is more effective for large web collections than for small collections. This is due to the fact that large collections have more variants for an entity than smaller homogeneous collections. In addition, we notice that the methods is specially helpful for original poor performance queries. This can be demonstrated by GMAP. The results for WT10g and GOV2 are similar, where con-siderable improvement is achieved in terms of GMAP, by over 50%. Comparing our two approaches, the results are mixture. For AP, Robust and GOV2, TX performs slightly better than PX in terms of MAP, but the result is opposite when compare in terms of GMAP.
In this paper, we have presented a query reformulation ap-proach based on entity detected in queries. We introduced the main steps of the approach. We have described our ap-proaches on ad-hoc retrieval experiments. Results indicate that utilizing entity corresponding information can help im-prove the quality of document retrieval.

For future work, we have two research interests. One is to focus on refining the entity detection approach. With domain knowledge, the entity detection can be more task-oriented. Another area is to explore the relations among entities and to utilize these relations for reformulation. [1] Indri. http://www.lemurproject.org/indri/ . [2] D. Metzler and W. B. Croft. A markov random field [3] J. M. Ponte and W. B. Croft. A language modeling [4] P. Resnik. Semantic similarity in a taxonomy: An
