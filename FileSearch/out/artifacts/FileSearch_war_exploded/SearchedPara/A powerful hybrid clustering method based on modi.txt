 fi ed stem cells algorithms, we called it SC-FCM algorithm, for optimum 1. Introduction
Data clustering is an important tool in many applications such as analysis ( Aitkin and Aitkin, 2011 ), math programming ( Masmoudi et al., 2010 ), image segmentation ( Taherdangkoo et al., 2010 )etc. Many approaches have been proposed for data clustering in general and speci fi c applications. One of the most important and applicable clustering algorithms is the K-means algorithm. The algorithm is an unsupervised algorithm that tries to put N data points into K points (K data points as a set of primary centers). In general, the resulted clusters should satisfy some conditions as follows:  X  where S is the entire dataset. Moreover, there should be at least one point in each cluster, so that: C j where  X  is an empty set. There are not also any data point jointly existing in two clusters which is expressed as follows: C  X  C j  X   X  j k  X  j  X  3  X 
In K-means algorithm, the clusters are formed such that the existing data in each cluster should have the minimum Euclidean distance to the center of that cluster. The Euclidean distance between x i which is i th data point and Z j which is the center of C ,isde fi ned as: E  X   X  x i  X  Z j  X   X  4  X  where  X  :  X  is the norm of vectors difference. Then, the centers of clusters are recomputed based on the fact that a center is the best represent of data points of a cluster. This process is repeated iteratively and ended when the centers do not change, namely the algorithm converges.

However, the algorithm has some limitations when the datasets are large and it may not work properly. For instance, bad selection of algorithm does not converge at all. Another fundamental problem is minimums may be long and eventually the response obtained after several repeats may not be the optima response. To overcome these drawbacks, many clustering algorithms have been introduced recently. For instance, the Genetic algorithms have been widely employed for data clustering ( Laszlo and Mukherjee, 2007 ). How-ever, many parameters should be fi xed in these algorithms and the convergence is not guaranteed.

Another unsupervised algorithm that attracted the scientists attention is Fuzzy C-mean (FCM) for data clustering ( Bezdek et al., 1984; Jain and Dubes, 1988 ). The algorithm works according to minimizing the objective function that is de fi ned as:
J  X  U ,
V , X  X  X   X  N
In above equation, U is a fuzzy membership function matrix with N rows and C columns in which N is the number of data points and C is the number of clusters. The element u ij in the i throw and j thcolumn in U , represents the degree of belongingness of the i th data point to the j th cluster, V  X  v 1 , v 2 , ::: , of centers of clusters and X  X  x 1 , x 2 , ::: , x i :::: , input data points, m is the fuzzy index that governs the in of membership grades ( m is any real number greater than 1, in is used for measuring the similarity between x i and v j . d x , v j  X  X   X  x i  X  v j  X  2  X  6  X 
In Eq. (5) , u ij is a fuzzy membership function whose value is between the interval of [0, 1] and de fi ned as follow: u u ij  X  1 ,  X  i  X  1 , ::: , N  X  7  X 
For the centers of clusters, v j ,wehave: v  X 
The process of FCM algorithm begins by initializing the centers vectors. Then u ij are computed using Eq. (7) . Next, the centers are recomputed using Eq. (8) then the process is repeated with new centers until v t  X  1 j  X  v t j o  X  and u ij t  X  1  X  u ij small value (  X  is a termination criterion between 0 and 1) and t shows the iteration. Although, the FCM algorithm is ef fi it is a very sensitive to the selection of initial values and having long convergence in the case of large datasets. To overcome these drawbacks, many clustering algorithms have been introduced recently. For instance, the Genetic algorithms have been widely employed for data clustering ( Gan et al., 2009 ). This paper ( Gan et al., 2009 ), introduced the responses that are expressed as a string of bits and the algorithm starts using a population consist-ing of a series of initial responses and obtains ideal responses and continues its process until achieving a convergence to a global minimum. It should be noted that the optimization of FCM using
Genetic algorithm increases only the speed of reaching fi response and improves a little the performance of FCM, although it solves somehow the problem of local minimum convergence.
Data clustering approaches have been also extended by the algorithms based on the social behavior of ants known as ant colony optimization (ACO) algorithms ( Kanade and Hall, 2007 ).
Kanade and Hall (2007) ) introduced an algorithm by the concept of using the increase of pheromone evaporation rate on data closer to the clusters  X  centers and achieved responses far better than previous algorithms. Some researchers have used recently particle swarm optimization (PSO) algorithm for data clustering to over-come the FCM problems ( Biswal et al., 2009 ). For instance, in works ( Biswal et al., 2009 ), data clustering was achieved by optimizing FCM based on PSO. The method demonstrated a performance much better than previous algorithms. In this the arti fi cial bee colony (ABC) algorithm have been employed for data clustering ( Zhang et al., 2010; Yan et al., 2012 ).
In this paper, we improve based on new optimization algorithm by integrating FCM algorithm in it in order to improve the performance of data clustering such as high accuracy and high speed of convergence. 2. Stem cells algorithm
Recently, stem cells algorithm has been introduced as one of the newest optimization algorithms for optimizing numerical functions ( Taherdangkoo et al., 2012a, 2012b, 2013 ). This algorithm was introduced and implemented on the basis of stem cells behavior in the human body (at the time of entering into the body). In overall, fi nding injured and weak organs is the main goal of this algorithm. Each of these organs is an optimum solution for solving all kinds of optimization problems. In order to implement this algorithm, multidimensional and self-renewal properties of stem cells are used. A brief introduction of the algorithm is as follows. An initial matrix that is composed of the problem variables, namely stem cells properties that are inherent, is formed. For example, we can point to the multidimensional property as the ability of a stem cell transforming into the marrow cells, blood cells, etc. The initial matrix is de fi ned as:
SC  X  X  SC i 1 , SC i 2 , ::: , SC iD i  X  1 , 2 , ::: , S where S represents the total number of cells participating in the implementation process of the algorithm and D is the dimension of the problem space.

In this algorithm, an initial population is selected from the members and then in subsequent iterations new members are added to the old population by a speci fi c percentage fraction. This speci fi c percentage of increment of members into the old popula-tion is determined before the algorithm implementation. Hence the algorithm starts by using only a part of the total members; If a large population was selected early during the implementation of the optimization algorithm, it would cause numerous iterations, high time consumption and sometimes trapping in local minimums.

The initial population is selected so that its distribution is uniformly and randomly extended in the problem space. Then a cost function for each cell is de fi ned as:
Cost  X 
SC i  X  X  where a is a positive random number and f i is the cost value of the solution SC i . Then the cost of each cell is normalized by: Cost
N  X  SC n  X  X  Max  X  Cost  X  SC i  X   X  Cost  X  SC n  X  X  11  X  where Cost  X  SC n  X  is the cost of the n thstem cell, Max the maximum cost among stem cells and Cost N  X  SC n  X  is the normalized cost of the n th stem cell.

Each cell that has a higher cost is a weaker cell and thus its normalized cost is lower. The ultimate parameter in determining the best stem cell (optimum solution) is the relative potency (i.e. the potential of each stem cell in differentiating between different cell types, for example bone marrow cells and blood cells). It is derived as follow:
P  X  Cost N  X  SC n  X  point of view, P n is the comparative power of the n thcell. After calculating comparative power of each cell, its value is saved in the memory of each cell. Then each of these cells shares information saved at its memory and fi nally this information is classi table from highest to the lowest order.

A part of the cells (e.g. one third of them) located at the upper range of the table are permitted to participate in self-renewal process and they form a portion of the attended population in a next iteration (e.g. 60% of the attended population in a next iteration).

The rest of the population is randomly selected from the cells that have no information from the problem space. Self-renewal process is done by the equation below: SC where t shows the each cycle (iteration) and  X  is a random number self-renewal process, which occurs similarly and reciprocally, is set for similar self-renewal and  X   X  0 : 01 for mutual self-renewal. Also in some problems where problem space dimension is high or the problem is highly complex, we could utilize both self-renewal processes simultaneously in order to prevent uniformity in reaching to optimum solution. When using this algorithm in single variable problems, the goal is the formation of an organ and the algorithm would be continued until reaching a complete organ. But when the objective is obtaining optimum values of two or more variables, the goal is to reach to two or more complete organs that justi multidimensional property of stem cells. 3. Modi fi ed stem cells algorithm
As mentioned in the previous section regarding the main algo-rithm of stem cells, self-renewal process is done either similarly or mutually. Although this process has some advantages in numerical functions, but by this method can lead to dif fi culties in applying to multi-objective functions or data classi fi cation and also convergence process slows down and the time to reach the optimum solution or ideal classi fi cation may increase up. In this work, we signi improve the algorithm  X  s performance by using Rechenberg Mutation Rule. Here, we improve the self-renewal process by modify-ing Eq. (13) : SC  X   X  t  X  1  X  X   X   X  t  X  1  X  *  X   X  t  X   X   X  t  X  1  X  X  where the initial  X  is determined before the process of algorithm implementation and is a random number in the interval of [0, 1], and  X  is also a random number in the interval of  X   X   X  ,  X  and is speci before implementing the algorithm. If the algorithm cannot improve the solution with respect to Rechenberg  X  s 1/5 rule (i.e. the ratio of successful mutations to all mutations  X  is less than 1/5), speed up the search.

In the original model of SCA, after each self-renewal process and in the next iteration, the distribution of cells was considered uniformly and randomly in the whole problem space, but in this new model, the location of each cell self-renewal is in the table in which highly informative cells have been located in the top. Then between each two cells (available space between two cells in the problem space), we use Beta distribution instead of uniform distribution in order to create self-renewal process more randomly. Beta distribution process of self-renewal cells is de fi ned as: B  X   X  ,  X   X  X  where a and b are the fi rst and second cell participated in the distribution process and whose positions are speci fi ed in the table. and  X  are two positive symmetric random numbers (  X  ,  X  4
The rest of the cells participating in the process of algorithm implementation utilizes cells that have no information about the sample space and the distribution of these cells happens uniformly and randomly. 4. Proposed method for data analysis
In order to obtain a classi fi cation with minimum error, the exact centers of clusters are needed. According to this fact that most of the classi fi cation algorithms selected the centers of clusters randomly; reaching to a convergence in classi fi cation of datasets seems to be important and is the major problem for all the clustering methods, because if datasets are very large the accuracy in reaching to an ideal response decreases. The FCM algorithm is faster than the other this paper, we used stem cells algorithm (SCA) to overcome this problem. The FCM algorithm is integrated with SCA to form a hybrid clustering algorithm called SC-FCM which uses the metrics of both FCM algorithm and SCA. SC-FCM algorithm applies FCM to the cells in the stem cells group at each iteration in order to improve value of each cell. In SC-FCM algorithm, we use a function for evaluating the fi tness function. So, the cost function is calculated mathematically as follows: f  X   X  where  X  is a constant (for example,  X   X  1) and J m is the objective function (see Eq. (5) ). The required steps for applying the proposed algorithm are de fi ned as follows:
Step 1 :De fi ning and setting the required parameters and constraints. {
Initialize SC i randomly and SC i , j within interval Evaluate f i according to Eq. (16) .

Initialize P i with a copy of SC i . }
Step 2 :De fi ning a criterion for stopping the process which is the same as the fuzzy clustering criterion is (i.e. Eq. (5) ). In this case, the process stops when the Eq. (5) is minimized or the number of iterations reaches to a prede fi ned maximum. { For i o N , normalize SC i according to Eq. (11) .
 For i o N , evaluate f i according to Eq. (16) .

Get the best stem cells. }
Step 3 :De fi ning the initial population and designating each member of the population as a response for clustering as C centers of clusters (note that the initial location of each C centers is selected randomly without any prior information). Step 4 : Calculate the fi tness value of each cells using Eq. (13) . Step 5 : Compute the criterion distance for each cells using Eq. (6) . Step 6 : Update the membership function for each cells using
Eq. (7) . { Get partition matrix U by Eq. (7) .

Refresh V by Eq. (8) . }
Step 7 :ByusingEq. (5) , the location of each data point is evaluated so that, when each data point reaches to lowest value related to each of C centers; it is put in the group of that center  X 
Step 8 : Stopping the algorithm is happened when each of the data point has the most similarity to the center of that class based on Eq. (5) which should be minimized in each class, if not, go to Step 4. 5. Experimental results
In order to evaluate the performance of the new data clustering algorithm, a set of well-known datasets has been used. They are known as Vowel, Iris, Crude Oil, Control Chart, Wood Defects, Wine, Nomao, University, and Seeds ( Blake et al., 1998 ). Table 1 shows the characteristics of each dataset including the number of objects, features and classes. We have also compared the obtained results using the proposed algorithm with those using standard K-mean algorithm, FCM algorithm, ACDE, GCUK, and VGAPS methods, Particle Swarm Optimization (PSO) and arti fi cial bee colony (ABC) algorithm. Table 2 shows the parameters used in each algorithm in our experiments. Tables 3  X  11 show the obtained results by applying mentioned clustering algorit hms on different datasets.
Moreover, the values of average, best and worst are average function value (cost function), best function value and worst function value, and these results were obtained by the algorithms for 50 independent runs on each dataset. As can be seen, the proposed algorithm demonstrates better results in obtaining lower mean value with minimum difference between min and max values for all datasets. We also computed the cost time of each algorithm in clustering of different datasets. The characteristics of the computer on which the algorithms were run are: Macintash OS, Two 2.93 GHz 6-Core Intel, 64GB Ram, and the graphic card is ATI Radeon HD 5870 1 GB.

The obtained results of cost time are shown in Tables 12  X  can be seen, the proposed algorithm has better performance among other algorithms. The cost time for each algorithm is the time when the algorithm achieves its best result. The SC-FCM took approximately less time than the other clustering algorithm to achieve to its best result. It is mostly due to the fact that SCA has less constraints, less parameters to be computed and less loops in its algorithm which causes that it achieves the results in much less iterations than other algorithms. In order to assess the speed of convergence of different clustering algorithms, we computed the curve of objective function value against the number of iterations for all mentioned clustering algorithms and for all tested datasets. Figs. 1  X  9 show these curves. As can be obviously seen, the proposed algorithm has achieved a better performance at less number of iterations for all datasets.

Figs. 1  X  9 show the objective function value versus iteration numbers which somehow show the convergence characteristic for various algorithms applied on different datasets. These algorithms for the comparison were K-means, FCM, ACDE, GCUK, VGAPS, PSO, ABC, and SC-FCM. The standard datasets on which the algorithms have been applied were Vowel, Iris, crude Oil, Control Chart, Wine, Nomao, University, and Seeds datasets.

By inspecting Figs. 1  X  9 and Tables 3  X  20 , it is concluded that both K-means and FCM clustering algorithms have approximately similar precision, convergence rate and error magnitude albeit the time of reaching fi nal response is a bit different. This holds because of the varied implementation and that FCM algorithm is a little more sophisticated than K-means.

GCUK method ( Bandyopadhyay and Maulik, 2002 ) has a much better convergence rate and lower error than FCM and K-means algorithms and this is noticeable by observing all Figures. Its ultimate response reaching time is higher than that of two others (by neglecting the response  X  s type) and this is due to the intricacy of its implementation.

VGAPS method ( Bandyopadhyay and Saha, 2008 ) has more parameters and higher complexity of implementation than K-means, FCM and GCUK that causes a higher ultimate response reaching time. Noticing Tables 14-19 for datasets 3, 5, 7, and 8, we observe that the time of reaching ultimate response in VGAPS algorithm is less than that of GCUK and this besides taking its complexity into consideration af fi rms its ability as yielding much better ultimate response than performance of GCUK. This besides taking its much better ultimate response and its complexity into consideration.

Due to Figs. 1  X  9 , VGAPS possesses a lower error rate despite the complexity of its implementation and approximately yields an ideal response. This has a lower DB measure ( Davies and Bouldin, 1979 ); this measure is a function of the ration of the sum of within-cluster scatter to between-cluster separation, and it uses both the cluster and their sample means, than that of K-means, FCM and GCUK and of an acceptable convergence rate in reaching ultimate response (the results of DB measure as shown in Tables 22  X  30 ).

PSO clustering algorithm has a lower complexity and ultimate response time compared to VGAPS but this time is comparable to other algorithms that their responses will be examined afterwards. The overall error rate of this algorithm is acceptable but regarding its unreliability in applying into large-scaled datasets, gives a much better results relative to K-means, FCM, ACDE, GCUK and VGAPS. Also it has less intricacy than VGAPS and does produce better results.

Although ACDE ( Das et al., 2008 ) method has better results than K-means, FCM and GCUK, it takes a longer time to reach the ultimate response. Noticing all fi gures, the results of this method are better compared to those of K-means, FCM and GCUK. Besides, the algorithm does display a nice performance and has a lower DB. Its error rate is just better than that of K-means and FCM.
Generally speaking it is a reliable algorithm when applying into a small-scaled data set.
 ABC algorithm possesses a higher speed in comparison with K-means, FCM, GCUK, VGAPS, ACDE and PSO and this is provable by observing time-results in Tables 12  X  20 . Its DB measure are less than those of K-means, FCM, GCUK, ACDE and PSO but higher than those of VGAPS algorithm for datasets 3 and 7 (As shown in Tables 14 and 18 ). Generally, it contains less complexity in implementation and as seen by the results, it is a reliable one except some certain times. This is the case of large-scaled multi-variant data set that has a lower precision in reaching ultimate response.

The ultimate rate of index of DB in the proposed algorithm (SC-FCM) is much lower than other clustering algorithms. This displays its ability in accurately fi nding cluster centers and hence yielding an ideal clustering. Much better convergence rate and lower error than ABC algorithm in applying to all datasets are its other two characteristics. Finally this posits the reliability of the proposed algorithm.

Short cost time in SC-FCM attributed to its less complexity, implies smaller number of variables and simpler implementation. Also it gives highly acceptable results when applying to large-scale datasets and this illustrates the reliability of the proposed algo-rithm when applying to such datasets.

Meanwhile, in order to more evaluate the performance of the proposed algorithm with respect to other clustering algorithms, the statistical tests, i.e. Friedman, Friedman Aligned, and Quade have been used. As shown in Table 21 , it can be seen that the proposed algorithm compared to other algorithms has produced less errors in clustering of datasets. 6. Discussion
Many clustering methods are composed of assumptions that should be taken into consideration prior to the implementation, for instance information about the clusters structure. Therefore the structure and performance of the presented clustering algo-rithms and comparison of them with the proposed algorithm are discussed in this section thoroughly.

K-means algorithm  X  s structure is very simple and is initiated with a bunch of data randomly selection as a centers. As obser-vable in the fi gures of this paper, it is not a well-performed algorithm in applying into large-scale datasets albeit having a high convergence rate.

FCM algorithm is the fuzzy mode of K-means algorithm and although it has a lower error compared to K-means, both display a similar performance in applying into gigantic datasets.
Generally speaking although K-means and FCM algorithms have a high convergence rate, they do not own a good precision in ultimate response (reaching an ideal clustering) as this algo-rithm has dif fi culty in fi nding cluster centers precisely. Therefore the ultimate clustering is of no good precision, includes some misclassi fi cation and contains a high error.

GCUK algorithm is considered as the combination of K-means and GA algorithms and performs the clustering process automa-tically. Of course this algorithm can detect enquisized hyper-spherical clusters within a data set. It has a good convergence in determining enquisizied hyperspherical clusters while possesses a weak convergence when clustering large-scaled datasets. The time needed for reaching response in GCUK algorithm is relatively higher compared to other evolutionary clustering algorithms and in general has an unacceptable error.

VGAPS algorithm performs the similarity measurement index by using a novel idea of measuring distances (PS-based distance). In this algorithm SYM index is considered as the cost of the chromosomes. Using adaptive mutation and appropriate crossover probabilities is of signi fi cant help to the convergence rate of this algorithm. VGAPS method yields pretty much better response with a lower rate of error compared to GCUK method.

At the end of the clustering, ACDE algorithm had lower misclassi fi cations in a relatively reasonable time, but produced contains higher error in comparison with PSO, ABC, VGAPS and SC-FCM methods.

PSO and ABC algorithms had a nice capability in reducing value of DB and also achieved data clustering results in a credible time accounting the quality of their responses. Albeit both produced low error rate, PSO method had a relatively smaller one.
Also it is worthy of mention that both PSO and ABC methods contained higher error rate and lower convergence rate compared to SC-FCM based on the tables and fi gures of Section 5 .
In addition, they hold a larger cost time in reaching an ideal clustering when compared to SC-FCM method.

As demonstrated in several research papers ( Taherdangkoo et al., 2012a, 2012b, 2013 ), SCA algorithm has a high precision and convergence rate. One of its reasons is that in SCA method the competence is de fi ned for a group of cells and not for a single one. This has a signi fi cant role in reaching to a low response time and also its quality. By using this algorithm, one of the main problems of FCM that is clustering data in a wrong direction is resolved. This occurs when initial centers (initial centers are randomly chosen from a dataset) do not converge to a correct direction.
In this paper, the shortcoming of FCM algorithm has been resolved in its best possible way by bene fi ting from the precision and high convergence rate of modi fi ed SCA algorithm in moving each cluster without any similarities with each other.
SC-FCM algorithm has an ample ability in determining opti-mum number of clusters in each data set and as it is obvious from the results obtained, there is a pretty much lower error rate relative to other clustering methods. Also this algorithm has a very high convergence rate and is able to produce the ideal response within the minimum time due to its small number of parameters. All the results in Experimental Results section prove this claim. 7. Conclusion
In this paper, we have presented a novel method for data clustering based on one of the newest optimization algorithms, i.e. modi fi ed Stem Cells algorithm and on using the principles of FCM algorithm. The important part of the proposed method is the ability of automatic inspection of the optimum number of clusters in a large-scale dataset. Therefore some new large datasets have been used for evaluating the performance of the proposed method (SC-FCM) and comparing with other clustering algorithms. It is notable that the better performance of SC-FCM algorithm in fi nding correction of clusters  X  centers compared to other clustering algorithms is of great precision and consequently leads to achieve the optimum number of clusters. While fi nding clusters number in a very large dataset seems impossible, the aforementioned bene of the proposed algorithm combined with its high convergence rate and reliability makes its usage justi fi able.

One cause of such characteristic is that the competence is not a proprietary of a single cell but is dedicated to a group of cells in reaching at an ideal clustering (formation of a single or some special organs inspired by the SCA algorithm  X  s nature).
We cannot de fi nitely claim that this proposed algorithm gives an ideal and much better response in comparison with other clustering algorithms when applying to all datasets in the world, as we have just examined its performance on 9 datasets. None-theless, based on the outcome of Section 5 , we can surely assert that the proposed method gives completely better results com-pared to other clustering algorithms. In addition, this method gives the results with a higher convergence rate in a less time, hence its reliability is veri fi ed.
 References
