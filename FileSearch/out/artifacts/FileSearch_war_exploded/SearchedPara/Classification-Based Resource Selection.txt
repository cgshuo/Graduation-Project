 In some retrieval situations, a system must search across multiple collections. This task, referred to as federated search, occurs for example when searching a distributed in-dex or aggregating content for web search. Resource selec-tion refers to the subtask of deciding, given a query, which collections to search. Most existing resource selection meth-ods rely on evidence found in collection content. We present an approach to resource selection that combines multiple sources of evidence to inform the selection decision. We de-rive evidence from three different sources: collection docu-ments, the topic of the query, and query click-through data. We combine this evidence by treating resource selection as a multiclass machine learning problem. Although machine learned approaches often require large amounts of manually generated training data, we present a method for using au-tomatically generated training data. We make use of and compare against prior resource selection work and evaluate across three experimental testbeds.
 H.3.3 [ Information Search and Retrieval ]: Miscella-neous Algorithms federated search, distributed information retrieval, resource selection, query classification
Classic information retrieval systems model search under the assumption of a centralized index. Federated search sys-tems model search across multiple, distributed collections. We focus on one subtask of federated search, resource selec-tion , the task of deciding, given a query, which collections to search. The objective of resource selection is to select a few collections whose merged ranking approximates the per-formance of a ranking generated by searching a centralized index of all collection content.

Traditional approaches to resource selection model the rel-evance of a collection by analyzing documents from the col-lection. For example, relevance can be modeled by com-paring the text in the query to the text in a collection with metrics used in document retrieval [26, 5, 23]. Relevance can also be modeled as an expectation of the number of relevant documents in a collection as in the ReDDE system [9, 21]. Parameters for these models are usually tuned manually on a small set of training queries.

In this work, we model a collection X  X  relevance based on its impact on a full-dataset retrieval, one that merges con-tent from all collections. We make the following assumption: given a query, an effective partial-dataset retrieval will re-semble a full-dataset retrieval . The idea is that we want a retrieval that merges content from a few collections to be indistinguishable from one that merges content from all collections. Because users scan results from top to bottom, collections should be prioritized by their contribution of doc-uments to the top ranks of a full-dataset retrieval. We train a classification system that models the inclusion of a collec-tion in the merged results as a function of a set of features. Training data is harvested from full-dataset retrievals con-ducted offline.

Modeling resource selection as a classification problem al-lows us to easily incorporate a diverse set of evidence as input features. This evidence can be classified into three categories. Corpus-based features derive evidence from col-lection documents. These include traditional resource se-lection metrics such as ReDDE. Query-categorical features derive evidence from the topic of the query. Finally, click-through features derive evidence from queries with clicks on collection documents. In a federated search environment, click-through data can be collected by the portal interface.
A classification-based approach provides several advan-tages over traditional approaches. First, it is flexible. De-pending on the federated search environment, different fea-tures can be easily incorporated into the model. Second, it is easy to train. As long as the system has offline access to full-dataset retrievals, more training data can be generated. Third, it is general. Although we adopt a particular ma-chine learning method in this paper, any multiclass learning method can be used to automatically tune parameters given training data. Finally, it is effective. We demonstrate that in the majority of experimental settings, a classification-based approach significantly outperforms traditional resource se-lection approaches.
Most resource selection methods share two things in com-mon: they formulate the problem as resource ranking (i.e., prioritizing collections for selection) and derive evidence from collection content (e.g., from sampled documents). Common approaches view collections (or their sampled documents) as large documents and adapt document retrieval methods to rank collections. CORI adapts INQUERY X  X  inference net document ranking approach [5]. Xu and Croft score collec-tions by the Kullback-Leibler divergence between the query and collection language models [26]. Si et al. score col-lections based on the query generation probability given the collection language model [23]. Large document models have the advantage of being relatively straight-forward adapta-tions of well studied document ranking techniques. However, they model the relevance of the entire collection. For this reason, they may favor a small, topically-focused collection (related to the query) when a larger, more topically-diverse collection contains more relevant documents. Instead of comparing the text in the query with that of the entire col-lection, Seo and Croft focus on the collection X  X  documents most similar to the query [16]. More specifically, a collection is scored based the geometric average query likelihood from its top m documents.

Methods such as GlOSS [9] and ReDDE [21] rank col-lections based on their expected number of relevant docu-ments. Similar to a large document model, GlOSS mod-els collection relevance using a query-independent collection language model. ReDDE also scores collections by their ex-pected number of relevant documents, but derives this ex-pectation using a retrieval from a centralized sample index, a mix of documents sampled from each target collection [21]. ReDDE predicts a binary relevance label for every sampled document and then assumes that every relevant sampled document represents some number of relevant documents in the collection from where it originates.

Other resource selection algorithms estimate the distribu-tion of document scores or document probabilities of rele-vance across collections [22, 19, 24]. Given these estimates, collections can be prioritized by their average document score or by a collection X  X  contribution to an approximated merged ranking. Similar to ReDDE, these methods start by issuing the query to a centralized sample index, producing a retrieval score for every sampled document. A full-collection score distribution is estimated by assuming that every scored sampled document represents some number of documents in its original collection with a similar score.
 Sometimes, collections are topically focused. Ipeirotis and Gravano exploit a topical relatedness between collections in order to minimize the negative effect of incomplete content descriptions derived from sampled documents [10]. Collec-tions are first classified into a topic hierarchy using topically-focused queries and their hit counts. A collection X  X  language model (derived from sampled documents) is smoothed with those from topically-related ones. At test time, a  X  X lat X  re-source selection method (e.g., GlOSS) is applied in a top-down fashion, descending the topic/collection hierarchy.
We cast resource selection as a multiclass classification problem. Therefore, we review some prior work on query classification. Because queries are terse, query-classification approaches often augment the query with features beyond the query string, possibly derived from query-logs [2], query click-through data [25], and documents associated with tar-get categories [17, 13, 18]. Bietzel et al. classify queries into semantic categories using an (unlabeled) query-log and a technique known as selectional preference . The query  X  X nter-est rates X  belongs to target category finance because terms  X  X nterest X  and  X  X ates X  often occur in contexts that co-occur with known finance-related terms. Shen et al. [17] and other participants of the KDD 2005 Cup [13] use corpus-based ev-idence. The query is issued to an index where every doc-ument is associated (heuristically) with a target category. Then, similar to ReDDE, the query is classified based on the number of top-ranked documents associated with each category. In later work, Shen et al. derive a soft membership of documents to target categories using term similarity [18], after augmenting the category representation with related terms using pseudo-relevance feedback. Li et al. take a different approach [12]. Instead of enriching the query rep-resentation, classifiers are trained using purely query string features. However, the amount of training data is expanded by propagating class labels to unlabeled queries using a large click-graph.

In the context of web search, vertical selection refers to the decision of whether to include content from specialized collections in web search results. In previous work, Diaz pro-poses a model for predicting whether to include news content based on user click feedback [6]. Arguello et al. address the situation where 18 verticals can be integrated into web re-sults [1]. Finally, Diaz and Arguello propose several methods for improving the performance of classification-based verti-cal selectors by incorporating implicit user feedback [7].
Given a set of n collections and a query q , a resource se-lector picks k collections from which to retrieve documents. We assume that the rankings from different collections can be merged as though all documents were centrally indexed. This separates the performance of the merging algorithm from resource selection evaluation. When k = n , rankings are equivalent to those generated from a centralized collec-tion. We refer to this as a full-dataset retrieval. When k &lt; n , we expect performance to be inferior to a full-dataset retrieval. Our objective, therefore, is to perform as well as possible for a given value k .
Our classification approach takes the form of n one-vs-all logistic regression models (one per collection). 1 Given a test query, each classifier makes a binary prediction with respect to its collection. Collections are then prioritized based on P ( Y = 1 | q ), the confidence of a positive prediction from collection C i  X  X  classifier.

Training collection-specific classifiers requires training data in the form of binary judgements on collections. If Q denotes the set of training queries and C the set of target collections, http://www.csie.ntu.edu.tw/cjlin/liblinear/ we require a function of the form, which maps query-collection pairs to +1, if C i is relevant to q , and  X  1, if C i is not relevant to q .

As mentioned in the introduction, our objective is to learn a model that selects collections based on their contribution to a full-dataset retrieval. This is based on the assumption that, on average, a full-dataset retrieval ( k = n ) outperforms a partial dataset retrieval ( k &lt; n ). Given a full-dataset re-trieval of query q , we generate a true label for every collec-tion C i  X  C as follows. With respect to C i , query q is a positive instance, +1, if more than  X  documents from C i are present in the top T full-dataset results. Otherwise, q is a negative instance,  X  1, with respect to C i .

We set T = 30 because we evaluate merged results in terms of P @ { 5 , 10 , 30 } and set  X  = 3 in order to ignore col-lections that contribute only a few documents to the top 30. We do not claim this is an optimal parameter setting. One alternative would have been to train different models using T = { 5 , 10 , 30 } when evaluating based on P @ { 5 , 10 , 30 } , re-spectively.
Our approach to resource selection is to exploit sources of evidence known to be effective in previous work. In addi-tion, we propose several other signals likely to be correlated with a collection X  X  impact on a merged retrieval. We focus on features derived from three sources of evidence: collec-tion documents, the query topic, and collection query-click behavior.

Some of the signals require conducting a retrieval from a particular index, for example from a centralized sample index, which combines documents sampled from each tar-get collection. Unless stated otherwise, all retrievals were conducted using Markov random field retrieval [14]. The parameter settings of the algorithm were taken from prior work and have been shown to perform well across various collections and tasks [14]. Corpus features derive evidence from collection documents. For example, given a query, we might prioritize collections by their number of documents retrieved. This, however, would require searching every collection, which is impracti-cal given our objective of approximating a full-dataset re-trieval by searching only a few collections. For this reason, corpus-features are derived from sampled documents. Doc-uments were sampled from a collection uniformly without replacement. In some federated search environments, col-lection documents are only accessible through a search in-terface. Prior work suggests that sampled sets of similar quality (as those obtained using uniform sampling) can be obtained using query-based sampling [4].

Corpus features correspond to three existing resource se-lection methods: CORI [5], Seo and Croft X  X  geometric aver-age approach [16] (GAVG), and the variant of ReDDE intro-duced in Arguello et al. [1], which we refer to as ReDDE.top. While these three methods derive evidence from the same source (i.e., collection samples), they model different phe-nomena. CORI and GAVG model the similarity between the query and collection text. However, CORI models the col-lection as one large query-independent bag of words, while GAVG focuses on the collection documents most similar to the query. ReDDE.top models the collection X  X  average doc-ument score in a full-dataset retrieval. We incorporate these three collection scoring functions as features to investigate their relative contribution to resource selection performance.
CORI adapts INQUERY X  X  inference net document rank-ing approach to ranking collections [5]. Here, all statistics are derived from sampled documents rather than the full collection. We use n CORI features (one per collection). tralized sample index, one that combines document samples from every collection, and scores collection C i by the ge-ometric average query likelihood from its top m sampled documents, where C sampled i is the set of documents sampled from C i P ( q | d ) is document d  X  X  query likelihood score. If fewer than m sampled documents are retrieved for a given collection, the product above is padded with P min ( q | d ), the retrieval X  X  minimum query likelihood. We use n GAVG features (one per collection).
Like GAVG, ReDDE.top issues to the query to a central-ized sample index and scores collection C i according to
ReDDE.top q ( C i ) = SF i  X  X where R sampled N denotes the top N documents in the cen-tralized sample index retrieval and SF i is the scale factor of collection C i . The scale factor quantifies the difference be-tween the size of the original collection, | C i | , and the number of documents sampled from it, | C sampled i | , We used two sets of ReDDE.top features, one set using N = 100 and a second using N = 1 , 000, for the following reason. The first set accumulates scores from the top 100 sampled documents. A collection with no documents in the top 100 receives a score of zero. This is problematic, however, if the number of collections with a non-zero score is less than k , the number of collections to be selected. To increase the number of collections with a non-zero ReDDE.top feature, we used a second set of ReDDE.top features setting N = 1 , 000. We use 2 n ReDDE.top features: n features with N = 100 and n features with N = 1 , 000. source of evidence is the topic of the query. We selected 166 topics from the Open Directory Project (ODP) hierar-chy and crawled Web documents associated with these ODP nodes. 2 These document sets were used to train logistic-regression classifiers (one per category) using unigram fea-tures. 3 Because queries are terse, instead of applying our trained classifiers directly on the query string, we apply them to documents in the centralized sample index and classify the query using a retrieval from this index. We set the value of category feature y i according to,
CAT q ( y i ) = 1 Z X where P ( y i | q ) is category y i  X  X  confidence value on document N = 100. We use 166 query category features (one per category).
Once in operation, a resource selection system has access to user feedback in the form of clicks on collection docu-ments. A click on a document can be viewed as a surrogate for document relevance. We view a click on a document as a surrogate for collection relevance, in favor of the collection from which the document originates. Click-through features exploit a possible correlation between collection relevance and the similarity between the test query and queries with clicks on collection documents.

Our approach is to model queries which result in a click on a collection document. For a collection, C i , let Q i all queries associated with a click event on a document in C i (allowing duplicate queries). We index each Q i as an individual document in a corpus of n documents. Given a query, we use the retrieval score of each collection as a feature. We use n click-through features (one per collection).
The TREC GOV2 test collection is a large crawl of the  X .gov X  portion of the Web, containing about 25M documents. The GOV2 corpus was used to construct 3 experimental federated search testbeds, varying the number of target col-lections: 1,000, 250, and 30. We refer to these testbeds as gov2.1000, gov2.250, and gov2.30, respectively. We con-structed the gov2.1000 testbed following the procedure de-scribed in Fallen and Newby [8]. While the GOV2 corpus consists of about 17,000 unique hosts (e.g., www.epa.gov), the largest 1,000 hosts contain about 90% of the GOV2 col-lection (i.e., about 22M documents). The gov2.1000 testbed was constructed by treated each of the largest 1,000 hosts as a separate collection.

The gov2.250 and gov2.30 testbeds were constructed by clustering hosts in the gov2.1000 testbed into 250 and 30 clusters, respectively, as follows. First, to represent hosts, we randomly sampled 1,000 documents from each. We de-fine a host X  X  vocabulary by all term-stems (using the Porter stemmer [15]) appearing at least 10 times in its document sample. Host-specific language models were constructed us-ing maximum likelihood without smoothing. The distance http://www.dmoz.org http://www.csie.ntu.edu.tw/cjlin/liblinear/ http://ir.dcs.gla.ac.uk/test collections/gov2-summary.htm between hosts H i and H j was computed using the Jeffrey divergence between their respective language models [11], also known as the symmetric Kullback-Leibler divergence,
D J (  X  i ||  X  j ) = X We used average-link agglomerative clustering, iteratively merging clusters according to their hosts X  average pair-wise similarity. We seeded the clustering by first combining hosts belonging to the same government entity (e.g., nih, usgs, usda, epa, uspto, nasa).
 Figure 1 shows each testbed X  X  collection size distribution. The gov2.1000 and gov2.250 testbeds have a few large col-lections and many small collections, while gov2.30 has many large collections and a few small ones. In the gov2.1000 testbed, 720 (72%) collections have fewer than 10,000 doc-uments and 438 (44%) have fewer than 5,000 documents. In the gov2.250 testbed, 131 (66%) collections have fewer than 10,000 documents. In the gov2.30 testbed, 24 (80%) collections have more than 1M documents.

As described in Section 4, we train a classification system to predict the inclusion of a collection in the merged results based on its impact on a full-dataset retrieval. To this end, we require a set of queries used to produce full-dataset re-trievals. There are multiple possibilities for selecting a set of  X  X raining X  queries (e.g., using a query-log or generating artifi-cial queries from collection text). However, one requirement is that there be enough positive instances for training for every collection. In other words, for each collection, there should be a sufficient number of queries with hits in the collection.

In this work, training queries were sampled from the AOL query-log. Recall that our three experimental testbeds con-sist of clusters of hosts from the  X .gov X  domain (1 , 000 single-ton host clusters in the case of the gov2.1000 testbed). Click events in the AOL query-log are uniquely identified by user ID, query, date/time and host URL (i.e., for the host asso-ciated with the document clicked). Therefore, it is possible to identify all AOL click events associated with any one of our 1 , 000 hosts. For each host, we estimate a query multi-nomial using the query X  X  relative frequency in click events associated with documents from the host. A set of 75,000 queries was sampled (without replacement) using a two-step iterative processes. First, a host is sampled uniformly from the set of 1 , 000 hosts. Then, a query is sampled from the host X  X  query multinomial. Hosts were sampled uniformly to favor coverage across hosts and, thereby, coverage across collections in our three testbeds. Queries were sampled ac-cording to their relative frequency in click events to favor popular queries likely to have hits in the collection.
Click-through features require simulating click events on collection documents. Click-through data collected over time was simulating also using the AOL query-log. We collected a total of 305 , 236 click events associated with our 1,000  X .gov X  hosts. There were no click events for about 25% of hosts. Of the 75% of hosts with click events, only about half had more than 50 click events. Click events associated with a query in our test set (described later) were omitted from the set of queries used for training and from those used to simulate click-through data.
The classification approach was evaluated against six single-evidence baselines, including one for every type of feature used in the classification approach. Corpus-based single-evidence baselines CORI, GAVG, and ReDDE.top score col-lections as described in Sections 5.1.1-5.1.3. ReDDE.top was used as a single-evidence baseline as follows. First collec-tions are prioritized by ReDDE.top score using N = 100. A second priority list is constructed using N = 1 , 000. If the first priority list has fewer than k collections, the remaining collections are selected from the second priority list.
We also evaluated a baseline approach that scores col-lections by query likelihood given its click-through queries, denoted by CLICK, as described in Section 5.3.
In addition to ReDDE.top, we evaluate against the orig-inal version of ReDDE [21], which estimates the number of relevant documents in a collection. ReDDE, like GAVG and ReDDE.top, conducts a retrieval from a centralized sample index and then scores collection C i according to, where P ( rel | d ) is the probability that document d is rele-vant and SF i is the scale factor of collection C i , defined by Equation 1.

ReDDE models P ( rel | d ) as a step function, based on doc-ument d  X  X  projected rank in an unobserved full-dataset re-trieval,  X  R full ( d ), according to, where | C all | = P C d  X  X  projected rank in the unobserved full-dataset ranking,  X  R full ( d ), is the sum of scale factors for collections represented by documents ranked above d in the centralized sample in-dex retrieval, R sampled , where R sampled ( d ) is document d  X  X  rank in the centralized sample index retrieval. Our baseline ReDDE algorithm cor-responds to the version referred to as modified ReDDE in [21]. Modified ReDDE ranks collections using ReDDE with  X  = 0 . 0005. A second priority list is constructed for all collec-tions with a ReDDE mass of less than 0.10 using  X  = 0 . 003. If the first priority list has less than k collections, the remain-ing collections are selected from the second priority list.
Our CATS baseline scores collections based on the simi-larity between the topical profile of the query and the top-ical profile of the collection. The query X  X  topical profile is given by normalizing Equation 2 across categories, such that P y j  X  X  CATS q ( y j ) = 1. The collection X  X  topical profile is de-fined by, The similarity between the query and collection topical pro-files is given by the Bhattacharya distance between these two distributions [3],
We are interested in the quality of document rankings produced by selecting only a few collections and combining their documents into a single ranked list. For this reason, we evaluate in terms of precision at different cut-off points, P @ { 5 , 10 , 30 } , when selecting between 1-5 collections. To focus evaluation on resource selection rather than results merging, we assume access to a function that provides the score that a centralized retrieval would have provided for ev-ery document retrieved. Given a set of collections selected, we combine their documents into a single ranked list accord-ing to each document X  X  centralized retrieval score.
We evaluate on TREC queries 701-850, used in the ad-hoc retrieval task of the Terabyte Track from 2004, 2005, and 2006. Recall that we are missing about 10% of the GOV2 collection in our testbeds, corresponding to those documents in GOV2 not originating from the 1,000 largest hosts. How-ever, all queries had at least one relevant document in our subset of GOV2, except query 703, which has no relevant documents in the full GOV2 collection.

All approaches were evaluated on all three testbeds un-der two conditions: sampling 1,000 documents and sam-pling 300 documents from each collection. We denote these 6 experimental conditions as gov2.1000.1000, gov2.1000.300, gov2.250.1000, gov2.250.300, gov2.30.1000, and gov2.30.300. Our motivation is to investigate the effect of sampled set size on resource selection performance across the classification approach and single-evidence baselines.
We evaluate resource selection based on the quality of the merged retrieval when selecting between 1-5 collections. Re-sults are presented based on P @ { 5 , 10 , 30 } . Table 1 shows results across our three testbeds when sampling 1,000 doc-uments from each collection. Table 2 shows results when sampling 300 documents from each collection. In addition, to evaluate the overall performance of federated search, we present results from centralized retrieval (denoted as  X  X ull X ), from a single index of all n collections combined.

The classification-based approach either significantly out-performs or is statistically indistinguishable from the best single-evidence baseline in all cases. In the gov2.1000.1000 condition, the GAVG and ReDDE.top baselines perform at the same level as the classification approach. We investigate how this experimental condition favors these methods in the next section.

From the performance of our single-evidence baselines, we notice two trends. First, all baselines that derive evidence from sampled documents (i.e., CORI, GAVG, ReDDE.top, and ReDDE) perform better when sampling 1 , 000 vs. 300 documents from each collection. This shows that these meth-ods are sensitive to the sampled set size. They perform bet-ter with more evidence, which is consistent with previous evaluations [20]. Second, their relative performance varies across experimental conditions. In the gov2.1000.1000 con-dition, GAVG and ReDDE.top clearly outperform CATS and CLICK in all cases. This is not true in the gov2.30.300 condition. When k = 1, in the gov2.30.300 condition, CATS and CLICK both outperform GAVG and ReDDE.top. These approaches derive evidence from different sources. GAVG and ReDDE.top derive evidence exclusively from sampled documents. CATS derives evidence from the topical simi-larity between the query and the collection. CLICK derives evidence from click-through data. Different types of evi-dence was particularly useful under different conditions.
Two results support the hypothesis that full-dataset re-trievals can be used to harvest data for training a machine learned resource selection method. First, a full-dataset re-trieval outperforms all methods, including the classification approach, in all cases. Second, the classification approach, trained on data harvested from full-dataset retrievals, per-forms at same level or better than the best single-evidence baseline in all cases.

Finally, the fact that a full-dataset retrieval outperforms all methods in all cases indicates that there is room for im-provement. We may more closely approximate the perfor-mance of a full-dataset retrieval by integrating new sources of evidence into the classification approach. The perfor-mance gap between full-dataset and federated retrieval is larger than that observed in some prior work. This may be a product of our three testbeds. Standard testbeds fre-quently used in prior work contain about 100 collections, with no collection containing more than 1M documents.
ReDDE.top and GAVG, which derive evidence from sam-pled documents, perform well in the gov2.1000.1000 exper-imental condition. In this condition, 1,000 documents were sampled from every collection. As previously mentioned, 72% of collections in the gov2.1000 testbed have fewer than 10,000 documents and 44% have fewer than 5,000 docu-ments. This means that a sample set of 1,000 documents constitutes at least 20% of the full collection for about half the collections in gov2.1000. In other words, in this condi-tion, ReDDE.top and GAVG have access to fairly complete representations for about half the collections. Furthermore, we would expect these methods to do well if these smaller collections frequently contain relevant documents. To exam-ine this, we binned collections by their number of documents and determined, for each bin, the number of times a collec-tion from the bin contained at least 10 documents relevant to a test query. These histograms are shown in Figure 2. In the gov2.1000 testbed, the smallest collections, with 1,000-10,000 documents, most often contain at least 10 documents relevant to a test query. In contrast, in the gov2.250 and gov2.30 testbeds, the collections that most often contain at least 10 documents relevant to a test query have more than 100,000 documents. Therefore, we can conclude that in the gov2.1000.1000 condition, corpus-based single-evidence baselines such as ReDDE.top and GAVG benefit from hav-ing fairly complete representations (i.e. large sampled sets relative to the collection size) for collections containing many relevant documents. In the other conditions, we see a more clear benefit from integrating multiple sources of evidence.
The classification approach integrates different types of evidence as input features. In this section, we conduct a set of feature ablation studies to test the contribution of evidence integration to the classification approach X  X  perfor-mance. We focus on experimental condition gov2.1000.1000 and gov2.30.300. Our motivation is to verify that the classi-fication approach is capable of focusing on the most reliable features under different experimental conditions. Based on the analysis from Section 7.1, in the gov2.1000.1000 condi-tion, we expect the classification approach to focus on evi-dence derived from sampled documents (i.e., CORI, GAVG, and ReDDE.top features). In the gov2.30.300 condition, we expect it to focus on other types of evidence. We individu-ally omitted each feature type (CORI, GAVG, ReDDE.top, CATS, and CLICK) and measure its contribution to per-formance based on the classifier X  X  percent decrease in pre-cision. Significance, again, is tested using a paired t-test on queries. Results are presented in Table 3. These results confirm our hypothesis. In the gov2.1000.1000 condition, in the majority of cases, omitting ReDDE.top features leads to a significant drop in performance. This is because in the gov2.1000.1000 condition, ReDDE.top has access to fairly complete representations for those collections with relevant content. On the other hand, in the gov2.30.300 condition, CLICK features are more predictive, particularly in terms of P @30. This shows that the classification approach is capable on focusing on the most reliable features depending on the condition. Also, although CORI, GAVG, and ReDDE.top features derive evidence from the same source (i.e., sampled documents), they model different phenomena. Our results show that they do not contribute equally to performance. This further motivates a feature integration approach, even when the evidence is derived from the same source.
We evaluated a classification approach to resource selec-tion against a number of single-evidence baselines, includ-ing three existing resource selection methods that have pro-duced good results in previous evaluations. Evaluation was done across six experimental conditions, varying the number of target collections and the number of documents sampled from each. The classification approach performed either at the same level or significantly better than all single-evidence baselines in all cases.

Most existing approaches to resource selection derive evi-dence from collection content. Often, the content in the col-lection is represented using sampled documents. Our eval-uation shows that these methods perform better when they have access to fairly complete representations. Their per-formance deteriorates, however, when most collections are large and sample sets are small. Our classification-based approach combines these approaches as input features along with features that capitalize on the query-collection topic similarity and click-through information. The end result is a method that is more robust. We show that when collection representation quality is poor, the classifier learns to focus on more reliable sources of evidence from training data.
We also show that full-dataset retrievals, which merge con-tent from every collection, can be used to produce data to train a machine learned approach. More training examples can be produced as long as there is (offline) access to full-dataset retrievals. This training procedure may be partic-ularly valuable in a dynamic environment where collection content is continually updated. A new model can be easily trained using a new set of full-dataset retrievals.
In this work, in order to separate results merging per-formance from resource selection evaluation, full-dataset re-trievals were produced by issuing queries to a centralized index of all collection content. In some federated search en-vironments, it may not be possible to combine collections in a single index. Future research may consider generat-ing training data using a merging algorithm that does not assume access to a single index of all collection content.
This work was supported in part by the NSF grants IIS-0841275 and IIS-0534345 and a generous gift from Yahoo!. Any opinions, findings, conclusions, and recommendations expressed in this paper are the authors X  and do not neces-sarily reflect those of the sponsors. level and a  X  at the p &lt; 0 . 005 level. level and a  X  at the p &lt; 0 . 005 level. documents to a test query.
