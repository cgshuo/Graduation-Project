 Modern part-of-speech (POS) taggers perform well on what some consider canonical language, as found in domains such as newswire, for which sufficient manually-annotated data is available. For many do-mains, such as subtitles, lyrics, and tweets, however, labeled data is scarce, if existing, and the perfor-mance of off-the-shelf POS taggers is prohibitive of downstream applications.

Furthermore, subtitles, lyrics and tweets are very heterogeneous. Subtitles span from Shakespeare to The Wire, and the lyrics of Elvis Costello are very different from those of Tupac Shakur. Twitter can be anything from teenagers discussing where to go tonight, to researchers discussed the implications of new findings. All three sources of data exhibit a very high degree of linguistic variation, some of which is due to the dialects of the speakers or authors.
In this paper, we use a corpus of POS-annotated tweets recently released by CMU, 1 consisting of semi-randomly sampled US tweets. We want to use this corpus to learn a POS tagger for subti-tles, lyrics, and tweets , which are typically associ-ated with African-American Vernacular English (AAVE). We believe our POS tagger can broaden the coverage of NLP tools, and serve as an impor-tant tool for large-scale sociolinguistic analyses of language use associated with AAVE (J X rgensen et al., 2015; Stewart, 2014), which relies on the accu-racy of these NLP tools.

We combine several recent trends in domain adap-tation, namely word embeddings, clusters, sam-pling, and the use of type constraints. Word rep-resentations learned from representative unlabeled data, such as word clusters or embeddings, have been proven useful for increasing the accuracy of NLP tools for low-resource languages and do-mains (Owoputi et al., 2013; Aldarmaki and Diab, 2015; Gouws and S X gaard, 2015). Since similar words receive similar labels, this can give the model support for words not in the training data. In this pa-per, we use word clusters and word embeddings in both our baseline and system models.

Using unlabeled data to estimate a target distribu-tion for importance sampling, or for semi-supervised learning (S X gaard, 2013), as well as wide-coverage, crowd-sourced tag dictionaries to obtain more robust predictions for out-of-domain data have been suc-cesfully used for domain adaptation (Das and Petrov, 2011; Hovy et al., 2015a; Li et al., 2012). In this paper, we use automatically-harvested tag dictionar-ies for the target variety(/-ies) in two different set-tings: for labeling the unlabeled data using a tech-nique elaborating on previous work (Li et al., 2012; Wisniewski et al., 2014; Hovy et al., 2015a), and for imposing type constraints at test time in a semi-supervised setting (Garrette and Baldridge, 2013; Plank et al., 2014a). Our best models are obtained using partially labeled training data created using tag dictionaries.
 Our contributions We present a POS tagger for AAVE-like language, mining tag dictionaries from various websites and using them to create par-tially labeled data. Our contributions include: (i) a POS tagger that performs significantly bet-ter than existing tools on three datasets contain-ing AAVE markers, (ii) a new domain adaptation algorithm combining ambiguous and cost-sensitive learning, and (iii) an annotated corpus and trained POS tagger made publicly available at https:// bitbucket.org/soegaard/aave-pos16 . For historical reasons, most of the manually anno-tated corpora available today are newswire corpora. In contrast, very little data is available for domains such as subtitles, lyrics and tweets  X  especially for language varieties such as AAVE. Learning robust models for AAVE-like language and other language varieties is often further complicated by the absence of standard writing systems (Boujelbane et al., 2013; Bernhard and Ligozat, 2013; Duh and Kirchhoff, 2005).

In this paper, we use three manually annotated data sets, consisting of subtitles from the televi-sion series The Wire , hip-hop lyrics from black American artists and tweets posted within the south-eastern corner of the United States. We do not use this data for training, but only for evaluation, so our experiments use unsupervised (or weakly super-vised) domain adaptation.

Although the language use in the three domains vary, they have several things in common: the register is very informal, and the subtitles, lyrics and tweets contain slang terms such as loc X  X  out , cheesing with and po X  , spoken language fea-tures such as uh-hum, huh and oh , phonologically-motivated spelling variations such as dat mouf , missin X  and niggas and contractions such as we X  X l and I X  X  . These features are infrequent in or absent from most commonly used training corpora for NLP.
The data was annotated by two trained linguists with experience in analyzing AAVE, using the Uni-versal Part-of-Speech tagset (Petrov et al., 2011). They obtained an inter-annotator agreement score of 93.6%. The test sections consist of 528 sentences (subtitles), 509 sentences (lyrics), and 374 sentences (tweets). In addition, we had 546 sentences of sub-titles annotated for development data. Note that we only use one domain for development to avoid overly optimistic performance estimates.

For all experiments, we use a publicly available on the 1827 tweets from the CMU Twitter Cor-pus (Gimpel et al., 2011). Note that despite the fact that the training data also comes from an informal domain, the distribution of POS tags in this data set is different from those of the test sets. For instance, the percentage of determiners in the CMU Twitter corpus is on average 4% lower than in our test do-mains, and there are 7% more pronouns in the test sets than in the CMU Twitter corpus.

We also create a large unlabeled corpus of data that is representative of our test sets. This corpus, consisting of 4.5M sentences, is created using subti-tles from the TV series The Wire and The Boondocks , English hip-hop lyrics, and tweets from the south-eastern states of the US. None of the unlabeled data overlaps with our evaluation datasets. We use this corpus for two purposes: to induce word clusters and embeddings, and to partially annotate a portion of it automatically, which we include in the training data of our ambiguous supervision model (see Section 3 below). Word representations To learn word embeddings from our unlabeled corpus, we use the Gensim im-plementation of the word2vec algorithm (Mikolov et al., 2013b; Mikolov et al., 2013a). We also learn Brown clusters from a large corpus of ditional features to our training and test sets. The word representations capture latent similarities be-tween words, but more importantly enable our tag-ging model to generalize to unseen words.
 Partially labeled data Model performance gen-erally benefits from additional data and constraints during training (Hovy and Hovy, 2012; T  X  ackstr  X  om et al., 2013). We therefore also use the unlabeled data and tag dictionaries as additional, partially la-beled training data. For this purpose, we extract a tag dictionary for AAVE-like language from various crowdsourced online lexicons.

Partial constraints from tag dictionaries have pre-viously been used to filter out incorrect label se-quences from projected labels from parallel cor-pora (Wisniewski et al., 2014; Das and Petrov, 2011; T  X  ackstr  X  om et al., 2013). We use a combination of a publicly available dump of Wiktionary 4 (Li et al., 2012), entries from Hepster X  X  glossary of musical nary by scraping UD for all words in our unlabeled corpus and extracting the part-of-speech information where available. See an example entry for the word hooch below, which has five possible parts of speech in our tag dictionary: VERB, NOUN, ADJ, PRON, ADV.

We use the tag dictionary to label the unlabeled corpus. E.g., when we see the word hooch , we assign it the label VERB/NOUN/ADJ/PRON/ADV.
 We present two ways of using this data for learning better POS models: one where the tag dictionaries are used in an ambiguously supervised setting, and one where they are used as type constraints at pre-diction time in a self-training setup.
 Ambiguous supervision Our algorithm is related to work in cross-lingual transfer (Wisniewski et al., 2014; Das and Petrov, 2011; T  X  ackstr  X  om et al., 2013) and domain adaptation (Hovy et al., 2015a; Plank et al., 2014a), where tag dictionaries are used to filter projected annotation. We use the tag dictionaries to obtain partial labeling of in-domain training data.
Our baseline sequence labeling algorithm is the structured perceptron (Collins, 2002). This algo-rithm performs additive updates passing over labeled data, comparing predicted sequences to gold stan-dard sequences. If the predicted sequence is identi-cal to the gold standard, no update is performed. We use a cost-sensitive structured perceptron (Plank et al., 2014b) to learn from the partially labeled data.
Each update for a sequence can be broken down into a series of transition and emission updates, passing over the sequence item-by-item from left to right. For a word like hooch la-beled VERB/NOUN/ADJ/PRON/ADV, we perform an update proportional to the cost associated with the predicted label. If the predicted label is not in the mined label set, e.g., PRT, we update with a cost of 1.0 (multiplied by the learning rate  X  ); if the pre-dicted label is in the mined label set, we do not up-date our model. This means that the POS model is not penalized for predicting any of the five supplied labels. We did consider distributing a small cost be-tween the candidates in the mined label sets, but this led to slightly worse performance on our develop-ment data.

In the experiments below, we also filter the par-tially labeled data by the amount of ambiguity ob-served in our labels. At one extreme, we require all words to have a single label, as in fully labeled data. Hovy et al. (2015b) also used a tag dictionary to ob-tain fully labeled data for domain adaptation. At the other end of the scale, we use all the partially labeled data, allowing up to 12 tags per words. Finally, we also experiment with using only sentences from our unlabeled data such that the tag dictionary assigns at most two (2) or three (3) labels to each word.
We also experimented with using different amounts of ambiguously labeled data. The best performing system on development data uses both Wiktionary and the tag dictionaries associated with AAVE, only 100 ambiguously labeled data points for training, a cost of 0.0 for predicting labels in the mined label sets, no threshold on ambiguity levels (but leaving only sentences covered by our tag dictionaries), the CMU Brown clusters, and 20-dimensional word2vec embeddings with a sliding window of nine (9). The results of this system are shown in Table 1 as Ambiguous .
 Self-training with type constraints Our second system uses the harvested tag dictionary for type constraints when making predictions on the unla-beled data for self-training. The search space of pos-sible labels for each word is simply restricted to the tags provided for that word by the tag dictionary.
For our self-training experiments, we experiment with pool size, but heuristically set the stopping cri-terion to be when the development set accuracy of the tagger decreases over three consecutive itera-tions. we obtained the best performance on de-velopment data using the tag dictionary without Wikipedia, using all entries for type constraints, the CMU Brown clusters, and 10-dimensional embed-dings with a window size of five (5). The results of this model are listed in Table 1 as Self-training . Pre-Normalization We also experimented with test-time pre-normalization of the input, using the normalization dictionary of Han et al. (2011), but this led to worse performance on development data. Table 1 shows the baseline accuracies, with and without clusters and embeddings, as well as the per-formance of the two developed systems described above. All results for both ambiguous supervi-sion and self-training with type constraints signifi-cantly outperform the simple baseline with p &lt; 0 . 01 (Wilcoxon). The system using ambiguous supervi-sion is also significantly better than the baseline with clusters and word embeddings on the Twitter data. The fact that we generally see worse performance on Twitter data than on the two other data set (even though the systems were trained on Twitter data) can be attributed to a higher type-token ratio.

We also provide the accuracies of three publicly available POS taggers in Table 1. The three POS systems are the bidirectional Stanford Log-linear biguous learning system outperforms all three sys-tems on all test sets.
Our improvements are primarily due to better per-formance on unseen words. Both systems improve the accuracy on OOV items for all three test sets, with the ambiguous learning system reducing the error by an average of 14%, and the self-training system reducing it by 7.7% on average. However, we also see an average increase in performance on known words of 1% for both systems. This increase is highest for tweets (2%) and around 0.5% for the subtitles and hip-hop lyrics test sets. The main rea-son for the increased overall performances of our systems is therefore the improved accuracy on OOV words. Table 2 shows that the accuracy on OOVs in-creases on all three test sets for both developed sys-tems over baseline.

The OOV words learned in these two test sets are mainly verbs such as sittin X  , gettin X  and feelin X  (g-dropped spellings), and words that are infrequent in canonical written language such as  X  X m and ho .
We observe that our systems improve perfor-mance on traditionally closed word classes such as pronouns, adpositions, determiners and conjunc-tions. These increases can be ascribed to the systems having learned from the additional information pro-vided on spelling variations such as  X  X ause , fo X  and ya and unknown entities such as dis , dat , sum .
Finally, we note that increasing the number of training examples for ambiguous learning seems to come with diminishing returns. The learning curve is presented in Figure 1. We explore several techniques to learn better POS models for AAVE-like subtitles, lyrics, and tweets from a manually annotated Twitter corpus. Our sys-tems perform significantly better than three state-of-the-art POS taggers for English, with error reduc-tions up to 55%. The improvements were shown to be primarily due to better handling of OOV words.
