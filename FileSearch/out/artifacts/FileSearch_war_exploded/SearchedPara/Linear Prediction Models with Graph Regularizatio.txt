 We present a risk minimization formulation for learning from both text and graph structures which is motivated by the problem of collective inference for hypertext document cat-egorization. The method is based on graph regularization formulated as a well-formed convex optimization problem. We present numerical algorithms for our formulation, and show that such combination of local text features and link information can lead to improved predictive accuracy. I.2.6 [ Artificial Intelligence ]: Learning Algorithms graph and relational learning, document classification, semi-supervised learning, regularization, collective inference
Hyperlinks among web documents provide extra evidence which can improve document classification accuracy. For ex-ample, Chakrabarti et al. have shown that classes of graph neighbors seeded with a text only classifier and further re-fined with an EM-like technique significantly improve Yahoo Directory classification accuracy [3]. Other options exist for aggregating neighborhood class assignments, Macskassy and Provost [8] analyze classification performance with var-ious configurations of local classifiers, relational classifiers, and collective inference methods for propagating evidence through the graph. Also see [7] for a related study by Jensen et al. Methods originating in inductive logic pro-gramming have also been applied to classification with hy-perlinks. Craven and Slattery use a combination of FOIL and Naive Bayes for classification in the WebKB data [5]. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. Getoor et al. apply probabilistic relational models to pre-diction in WebKB and Cora datasets [6].

Such relational learners are designed to exploit structure in the graphs where nodes link to members of their own class or certain other classes with high probability. While we use hyperlink and co-citation relationships, relational learners have also been applied to other data, for example, Neville and Jensen used a relational Bayesian classifier to assign companies into their industry sector [10], Segal et al. used probabilistic relational models to characterize gene expres-sion data [12], Popescul and Ungar used regression over fea-tures automatically generated from a multi-relational, in-cluding cluster relations, representation of CiteSeer for cita-tion probability estimation and conference community clas-sification [11].

A different approach, which can also take advantage of graph structure, has recently appeared in the semi-supervised learning literature. In this approach, one creates a graph us-ing unlabeled data (often nearest neighbor graph by linking points that are close to each other). Discriminative learn-ing methods such as logistic regression or support vector machines can then be directly applied using a properly de-fined regularization condition (often referred to as graph-Laplacian) on the graph. For example, see [2, 13, 17, 18]. This framework has drawn significant attention, with a fast growing number of papers appearing in the machine learning literature. An advantage of this approach is its equivalence to a form of standard large margin kernel methods with a specially designed kernel on graph. Therefore it is an ex-tension of the standard kernel method in the graph setting, and the well developed theoretical and algorithmic results for large margin methods can be readily applied here [15]. Moreover, the resulting formulation is a well-formed convex optimization problem, which has a unique and efficiently computable solution. In contrast, previously proposed link-based relational learning models either implement a proce-dure that does not solve an optimization problem (hence such procedures do not necessarily converge), or requires approximate Bayesian inference (due to the non-convexity of the underlying Bayesian formulation). Therefore at the conceptual level, this different approach for exploiting graph structure has some unique features and advantages.
The purpose of this paper is to adapt the graph regular-ization framework for large ma rgin discriminative learning to text-categorization problems on the web using text infor-mation and hyperlink structures. This leads to a new class of learning methods that complement the earlier approach from the relational learning point of view. In this frame-work, combining link and text information is closely related to combining regularizers or kernels, as we will demonstrate later. Although some theoretical aspects of such combina-tion were discussed in recent work [1], they do not lead to implementable algorithms suitable for large scale text clas-sification problems. The algorithmic and scalability issues will be investigated in this paper. We show that by com-bining text and link structures, we can improve the state of the art local regularized linear classification methods for document categorization.

The main contribution of this work is to derive these new formulations within the graph regularization framework, and then present scalable numerical algorithms. For simplic-ity, we will not directly compare performance with the pre-vious relational learning approaches. Some of them start with different classification methods (e.g. not discriminative linear classifier) and make improvements relative to those methods; therefore their baselines are not directly compat-ible with our framework. Our re gularized logistic regres-sion baseline is generally regarded as state of art for text categorization without using link information, and our con-tribution presents improvements beyond this baseline when graph structure is added within a unified framework.
We define our problem more abstractly as follows. Con-sider the problem of predicting a real-valued output y based on its corresponding input vector x .Inthispaper,weare interested in the setting of collective inference where we ob-serve a set of labeled data ( X i ,Y i )for i =1 ,...,n and a set of unlabeled data X j for j = n +1 ,...,m . The true values Y j for X j is unknown. Our goal is to estimate a functional relationship Y j  X  p ( X j )for j =1 ,...,m such that the risk P tion, and p ( x ) is a real-valued function. Slightly different from inductive inference, where ( X k ,Y k )for k =1 ,...,n are assumed to be selected from an unknown distribution, in the collective inference setting, we assume that X k ( 1 ,...,m ) are fixed. We randomly draw n out of m samples as labeled, and reveal the corresponding labels. We then test the learning performance on the remaining samples (where the labels are hidden).

In addition to the input vector X , we assume further that a graph structure is observed on the dataset X k for k = 1 ,...,m . The vertices of the graph are the nodes X k ,with edges defined between node pairs. In the context of web-classification, the nodes are web-pages, and edges are links or co-citations between web-pages. For simplicity, we treat the graph as undirected. The graph structure will be used to construct appropriate regularization conditions for p ( X described in subsequent sections. In this work, we consider the following interpretation of the graph. If two nodes X and X are linked, then X k and X are likely to have similar predictive values: p ( X k )  X  p ( X ). This information, which we shall use for the purpose of designing a regularization condition, will be explored in the paper. In the supervised setting, we use the local input vector X but ignore the graph structure. In text categorization, many classifiers such as decision trees, Naive Bayes, k -NN, have been employed [14]. It was known that sate of the art performance can be achieved by the so-called regular-ized linear classifiers that include SVM, regularized logistic regression, ridge regression, etc (for example, see [16]). In this paper, we focus on linear classifiers. Consider a fea-ture map  X  ( X ) and we are looking for linear weight w such that Y  X  p ( X )= w T  X  ( X ), where w is a parameter we want to estimate from the training data. In text categoriza-tion,  X  ( X ) is often a vector representation of a document X in the vector space model. Given a set of training data (
X i ,Y i ), we can compute a linear weight using regularized linear prediction method, where  X  p ( x )=  X  w T  X  ( x ), and where  X   X  0 is an appropriate regularization parameter.
This gives us a predictor  X  p ( x ), which we can evaluate on the test data X j ( j = n +1 ,...,m ). In this work we are mainly interested in the binary classification prob-lem, where Y  X  X  X  1 } . Given the weight vector  X  w trained from (1), the corresponding class label of X j can be as-signed as sign(  X  w T  X  ( X j )). Common loss functions include L ( f, y )=ln(1+exp(  X  fy )) (logistic regression), L ( f, y )= max(0 , 1  X  fy ) (SVM), and L ( f, y )=( f  X  y ) 2 (least squares).
In graph-based learning, we are interested in finding the predictive values f k = p ( X k )directlyfor k =1 ,...,m .In the class of methods we consider here, this can be achieved by constructing a regularization condition L on the graph and solve the following problem: In this representation, L is an m  X  m matrix. If we have a weighted graph with edges E among the nodes X k ,and weights c k,k associated with ( k, k )  X  E , then a standard way to define the regularization condition L is This regularization condition is often referred to as (un-normalized) graph Laplacian in the graph-learning litera-ture. There are different heuristics for choosing weights c k,k ,andwejusttake c k,k = 1. What is a more appro-priate definition (and whether it is better to use normalized Laplacian) is not the main topic of this work.

One may reformulate the supervised learning method (1) to have the same form as the collective learning method (2). We define a kernel function k ( x, x )=  X  ( x ) T  X  ( x ), and let the kernel gram matrix be K =[ k ( X k ,X k )] k,k =1 ,...,m Then it is known that the supervised learning method (1) and the collective inference method (4) are equivalent on the graph:  X  f k = X  p ( X k )for k =1 ,...,m . For example, see [15] for a proof. If we replace the regularizer L by the inverse of the kernel gram matrix K : L = K  X  1 , then the reformulation (4) of supervised learning and graph learning formulation (2) have the same form. Computationally, (1) is far more efficient than (4). However, the collective inference formulation is conceptually useful when we combine local information  X  ( x ) in (1) with link information in (2).
In the graph learning formulations (2) and (4), the under-lying graph is used to form regularization conditions. In (2), a graph Laplacian operator is explicitly constructed, and in (4), kernel k ( x, x )=  X  ( x ) T  X  ( x ) is used. Under this frame-work, a natural way to combine local information in (4) and link information in (2) is to combine these regularization conditions. In this paper, we consider two possible ways to do so: using linear combinations of the regularizers or linear combinations of the associated kernels.
A natural way to combine local information with global link structure is to linearly combine the regularizers:  X  f =arg min In this formulation, f k  X  f k either when k and k have sim-ilar local information contents (that is,  X  ( X k )  X   X  ( when k and k are close to each other on the graph. There-fore the resulting method effectively explores the local in-formation in addition to the link structure on the graph.
However, a disadvantage of the above method is that K is adense m  X  m matrix. The computational cost of inverting K is m 3 . Even if we use iterative algorithms, the complexity is at least O ( m 2 ), which is not practical for large scale clas-sification. Therefore we need to rewrite it in a form which can be solved more efficiently. In order to address this issue, we shall replace K by ( K +  X I ), where  X &gt; 0isasmall tuning parameter that is close to zero. The small parameter  X  is often introduced to make the system strictly positive definite, and thus more stable (for example, [17, 15]). In this work, we use a small number  X  =0 . 01 which is consis-tent with the literature. We can now consider the following formulation that contains the tuning parameter  X  : The following fact is used to reformulate (5).

Proposition 1. Let K =[  X  ( X k ) T  X  ( X k )] k,k =1 ,...,m f
T ( K +  X I )  X  1 f =min
Proof. The minimum on the right hand side can be achieved at w =( XX T +  X I )  X  1 Xf, where X =[  X  ( X 1 ) ,..., X  ( X m )] Substituting this into the right hand side, we obtain
Using Proposition 1, we can now rewrite (5) as [  X  f,  X  v,  X  w ]=argmin
This is our final formulation for combining local and link information using a linear combination of the corresponding regularizers. Note that in this approach, the role of the stabilizing parameter  X  can be regarded as adding a feature  X   X  to each specific node. The method has the following two important properties. First, it avoids the construction of the dense matrix K , and thus can be solved more efficiently. Second, the optimization problem is convex in f and w , which implies there is only one global optimal solution. In contrast, earlier works in relational learning do not lead to a global convex optimization problem, and thus are more difficult to solve.

The formulation (6) integrates the following ideas for the predictive vector f on graph: f fits well on the labeled data: P i L ( f i ,Y i ) is small; f is approximately a linear function of the local features;  X v 2 = smooth on the graph: f T L f is small.
A second way to combine local information with global link structure is to linearly combine the corresponding ker-nels, which is a standard practice in the kernel learning lit-erature. Since the kernel matrix is the inverse of the regular-ization operator (e.g. see remarks at the end of Section 4), we have the following method that linearly combines the kernels: we use the following result to rewrite this formulation.
Proposition 2. Let K = K 1 + K 2 , we have f
Proof. Consider the optimal solutions f 1 and f 2 on the right hand side. There is a Lagrangian multiplier  X   X  R m such that f 1 and f 2 minimize the unconstrained problem: Taking derivative, we obtain the condition f 1 = K 1  X  , f K 2  X  , and thus  X  =( K 1 + K 2 )  X  1 f . Substituting this repre-sentation into the right hand side of the equation in Propo-sition 2 gives the left hand side.

Now by using Proposition 2 with K 1 =  X   X  1 K and K 2 =  X   X  1 L  X  1 , we can rewrite (7) as [  X  f,  X  f ,  X  w ]=arg min Similar to (6), we may introduce a stabilizing parameter  X  and obtain the following formulation:
Equation (8) is similar to (6) except for the graph regular-ization term involving L . In (6), the regularization is on the function f = w T  X  ( X k )+ v k tion is only on the second component v k
Since regularization restricts the parameter space, we know that the formulation (8) is more expressive than (6). Instead of assuming the final predictor to be smooth on the graph as in (6), the method in (8) can be regarded as adding a feature to the predictor that is smooth on the graph. In this sense, when the quality of the graph structure is high, that is, when most links in the graph are within the same class, it is better to use (6). On the other hand, when there are many links that connect different cl asses, then (8) is preferred.
There are several ways to derive optimization procedures for this problem. In this paper, we consider the dual train-ing method which has the advantage of minimum storage requirement. Using the representation of L in (3), both (6) and(8)canbewritteninthefollowinggeneralizedform:
Let e k be the vector in R m with zero entries except for the k -thentrywhichisone. For(6),wecanlet u T =[ w T ,v T ], i =[  X  ( X i ) T , can let u T =[ w T ,v T ],  X  T i =[  X  ( X i ) T , [0 ,e k  X  e k ]. It is easy to verify that under such notation, (6) and (8) become special cases of (9). It follows that we only need to develop a computational method for (9).
Since the number of edges in E is often large, we will not be able to store all vectors  X  k,k in memory. Therefore in our computational procedure, we do not explicitly store the vectors  X  k,k in order to solve (6). However, our algorithm requires that local vectors  X  i canbestoredinmemoryfor all i =1 ,...,m . If this is not possible, then we have to use simplifications. For example, one may simply subsample the graph by removing some nodes. Another possible simplifi-cation of (9) is to compute a weight vector  X  w based on (1), and then compute a feature  X  w T  X  ( x )foreachdocument x Now, instead of using all features  X  ( x ), we use a simplified feature  X  w T  X  ( x ), which requires less memory. Therefore in the following, we assume that the local vectors  X  i can be stored in memory for all i =1 ,...,m , although we may not be able to store all  X  k,k .

A simple idea to avoid storing  X  k,k is to use the stochastic gradient descent algorithm, where we look at one data point i ,oroneedge( k, k ) at a time, and update the weight vector u based on the gradient of the cost function at the examined point. A closely related method, which we shall employ here, is to solve the dual formulation of (9). It can be shown that the weight vector  X  u in (9) can be obtained using the following dual formulation: The function L D ( a, y ) is the convex dual of L , defined as L
D ( a, y )=sup f  X  R [ af  X  L ( f, y )]. The set of variables often referred to as the dual variable, while u is the pri-mal variable. The idea of dual algorithm is to vary one  X  i or  X  k,k at a time, while keeping the remaining dual variables fixed. We always set the primal variable for each node, and a dual variable for each edge. There are two kind of dual updates, corresponding to node dual-variable update and edge dual-variable update. We have the algorithm in Table 1 for solving equation (10). The param-eter  X   X  (0 , 1) is introduced to make the algorithm more stable. The parameter is not critical, and we use 0 . 1. initialize primal weight vector u  X  0 initialize dual variables  X  i  X  0 , X  k,k  X  0 for =1 ,  X  X  X  ,L let  X  f i = u T  X  i ( i =1 ,  X  X  X  ,m )
We use three document collections: two hyperlinked web page collections, WebKB (http://www.cs.cmu.edu/ webkb/) [4] and Yahoo! Directory (http://www.yahoo.com/), and Cora (http://www.cs.umass.edu/ mccallum/code-data.html)  X  a collection of scientific articl es with citation information [9]. We have found that co-citation graphs derived from the original directed hyperlink (citation) graphs often give better performance within our framework, and in the rest of the paper it is assumed we use co-citation graphs, un-less otherwise noted. Two documents are connected by an (undirected) co-citation edge, if there exists a third page which links to both documents. The exact meaning of  X  X x-ists X  varies across the datasets we use, and will be described below. The word features are formed (after removing html tags when necessary) by converting upper-case characters to lower-case characters, removing punctuation, tokenizing by white space characters and removing stop words. We use full word forms.

The WebKB dataset consists of 8,275 web pages crawled from university web sites and belonging to seven functional, as opposed to topical, categories (course, department, fac-ulty, project, staff, student and other). The vocabulary con-sists of 20,000 most frequent words. To obtain a denser co-citation graph we have used Yahoo! X  X  database to retrieve current inlinks, where pages used to derive the co-citation graph do not necessarily belong to the set of WebKB pages. The number of (subsampled) edges in the co-citation graph used in our experiments is 1,143,716. Because we use ad-ditional information in graph generation, the results are not comparable to earlier published experiments with this dataset, and should be interpreted only in the context of comparison to word features wi thin the proposed framework.
The Yahoo! Directory dataset is the largest of three, and consists of 22,969 web pages with assignments into one of the 13 top level topical directory categories (for example, arts, business and education). The vocabulary consists of 50,000 most frequent words. The number of (subsampled) edges in the co-citation graph is 1,170,029. As in the previous dataset generation, the in-neighbor pages used in creating the Yahoo! Directory co-citation graph do not have to nec-essarily belong to the initial set of 22,969 pages and were used for graph generation only.

The Cora collection contains 30,714 computer science pa-pers with available class assignments. We experiment with the class structure containing 10 top level categories. The vocabulary consists of 20,000 most frequent words. The co-citation graph has 259,298 edges. Since in-coming citations from external papers were unavailable, and in contrast to the previous two datasets, the edges were formed using only the citations among the documents in this dataset. In addition to the above Cora data which we refer to as Cora-cocit, we also include a version of Cora with direct citations, which we call Cora-direct. There are 225,026 documents in this data, and graph contains 714,266 directed edges. The text and class labels of documents in Cora-direct which do not appear in Cora-cocit are not available.

We randomly split the labeled data into two parts: 50% for training and another 50% for testing. We draw five runs and report test set averages and standard deviations. We use the logistic regression loss function L ( f, y )=ln(1+ exp(  X  fy )). The best  X  value can be found through cross validation in the training data using (1) without the graph structure. We then fix this  X  for all other configurations (with graph structure). We take  X  =0 . 01 as mentioned earlier. In the experiments, we simply set  X  such that  X  n =0 . 01 without additional optimization. Therefore bet-ter results might be possible with optimized  X  .Wecompare the following methods: reg-comb (reg): method to combine text features with link structures using (6); ker-comb (ker): method to combine text features with link structures using (8); text-only (txt): method that only uses text features (1); graph-only (gra): method that only uses link structures (2). The computational method presented in Table 1 easily scales to the datasets used in this paper. A typical run for each dataset takes well under an hour on a standard PC. With appropriate engineering, the method is capable of handling datasets at a significantly larger scale.
 Table 2: Classification accuracy (mean  X  std-dev % ) reg 90.9  X  0.4 72.0  X  0.4 78.1  X  0.2 79.6  X  0.5 ker 89.2  X  0.5 66.5  X  0.3 77.8  X  0.2 82.0  X  0.2 txt 89.2  X  0.5 66.2  X  0.3 73.9  X  0.3 73.9  X  0.2 gra 64.0  X  0.5 40.1  X  0.3 55.8  X  0.4 78.8  X  0.1
Table 2 shows the multi-category classification accuracy for different methods across datasets. We noticed the reg-comb method consistently outperforms either text-only or graph-only methods. The performance of the ker-comb method is not as consistent as that of reg-comb. However, it never decreases the performance, while it significantly improves the text-only or graph-only baselines for the two Cora datasets. It is also not surprising that text often carries more informa-tion than the link structure. Therefore the text only method usually out-performs the graph-only method. The only ex-ception is the Cora-direct dataset, where link information turns out to be more useful than text information.
In our experiments, the reg-comb method usually achieves better performance than the ker-comb method. As we have pointed out, we did not optimize the regularization param-eter  X  using cross validation on the training data. This might have affected the performance of ker-comb. One may also study the quality of the underlying graph to obtain ad-ditional insights on what method works better with what kind of graph. Table 3 shows the binary-classification per-formance of the graph-only method (and text-only method) at the zero decision threshold, micro-averaged over the cat-egories. We use the standard performance measures for text categorization: precision, recall, and F-measure. The micro-averaged performance measures are computed using statis-tics summed over the confusion matrices of all categories. The table shows that the graphs have relatively high preci-sion, which implies that the links are reliable. As we com-mented earlier, the reg-comb method will be able to take advantage of such graphs to achieve good performance. We also observe that the graph-only method yields low recalls, implying that although the graph structures can be used to reliably label part of the document collections, many docu-ments cannot be reliably labeled by graph alone (and thus text information becomes help ful here). A special case is Cora-direct, which has relatively high precision and recall. We believe that high recall is one reason for ker-comb to perform well on this data. The precision and recall figures of the graph-only classifier give good characterizations of the underlying graph structure. They have implications on the performance of different combination methods. A use-ful direction is to develop better characterizations of graph-structures, and then design suitable combination methods accordingly. For comparison, we also include the precision (P), recall (R), and F-measure (F) figures for the text-only method in Table 3.
 Table 3: Micro-averaged Binary Classification Per-formance (mean  X  std-dev % ) P 99.6  X  0.2 90.1  X  0.5 79.8  X  0.5 83.0  X  0.3 R 58.0  X  0.4 30.0  X  0.4 33.4  X  0.4 74.0  X  0.3 F 73.3  X  0.3 45.0  X  0.5 47.1  X  0.4 78.3  X  0.3 P 90.8  X  0.6 73.8  X  0.5 89.1  X  0.4 88.7  X  0.3 R 82.8  X  0.6 50.0  X  0.3 58.4  X  0.3 57.8  X  0.3 F 86.6  X  0.6 59.6  X  0.4 70.5  X  0.2 70.0  X  0.2
Table 4 shows the predictive performance of the text-only method versus the graph-only method, where TT is the per-centage of test data predicted correctly by both methods; TF is the percentage of test data only predicted correctly by the text-only method; FT is the percentage of test data only predicted correctly by the graph-only method; and FF is the percentage of test data predicted incorrectly by both methods.
 TT 57.9 28.3 48.3 64.5 TF 31.3 37.9 25.6 9.4 FT 6.2 11.8 7.5 14.3
FF 4.6 22.0 18.6 11.8
This paper introduces a novel method for learning from both text and graph structures, applicable to the problem of collective inference for hypertext document categorization. This method is based on the graph regularization formula-tion of kernel methods, recently proposed in the machine learning literature. An advantage of this method compared to earlier approaches from the relational learning literature is that it leads to a well-formed convex optimization prob-lem. Therefore it has a global optimal solution that can be efficiently computed. Moreover, the method is equivalent to the standard kernel method, with an appropriately defined kernel based on the graph. Therefore existing theoretical and algorithmic results can be directly applied. Experimen-tal results show that combining text features and link infor-mation leads to improved accuracy in the tasks we studied. [1] A. Argyriou, M. Herbster, and M. Pontil. Combining [2] M. Belkin and P. Niyogi. Semi-supervised learning on [3] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced [4] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, [5] M. Craven and S. Slattery. Relational learning with [6] L. Getoor, N. Friedman, D. Koller, and B. Taskar. [7] D. Jensen, J. Neville, and B. Gallagher. Why [8] S. Macskassy and F. Provost. Classification in [9] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. [10] J. Neville and D. Jensen. Iterative classification in [11] A. Popescul and L. Ungar. Cluster-based concept [12] E. Segal, B. Taskar, A. Gasch, N. Friedman, and [13] M. Szummer and T. Jaakkola. Partially labeled [14] Y. Yang. An evaluation of statistical approaches to [15] T. Zhang and R. K. Ando. Analysis of spectral kernel [16] T. Zhang and F. J. Oles. Text categorization based on [17] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [18] X. Zhu, Z. Ghahramani, and J. Lafferty.

