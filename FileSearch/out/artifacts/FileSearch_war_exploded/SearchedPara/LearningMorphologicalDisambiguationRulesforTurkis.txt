 Morphological disambiguation is the task of select-ing the correct morphological parse for a given word in a given conte xt. The possible parses of a word are generated by a morphological analyzer . In Turk-ish, close to half the words in running text are mor -phologically ambiguous. Belo w is a typical word  X masal X  with three possible parses. masal +Noun+A3sg+Pnon+A cc (= the story ) masal +Noun+A3sg+P3sg+N om (= his story ) masa +Noun+A3sg+Pnon+ Nom X DB+Adj+With
The rst two parses start with the same root, masal (= story , fable ), but the interpretation of the follo wing + suf x is the Accusati ve mark er in one case, and third person possessi ve agreement in the other . The third parse starts with a dif ferent root, masa (= table ) follo wed by a deri vational suf x +l (= with ) which turns the noun into an adjecti ve. The symbol  X DB represents a deri vational boundary and splits the parse into chunks called inectional groups (IGs). 1
We will use the term featur e to refer to indi vidual morphological features lik e +Acc and +With ; the term IG to refer to groups of features split by deri va-tional boundaries (  X DB ), and the term tag to refer to the sequence of IGs follo wing the root.

Morphological disambiguation is a useful rst step for higher level analysis of any language but it is especially critical for agglutinati ve languages lik e Turkish, Czech, Hungarian, and Finnish. These lan-guages have a relati vely free constituent order , and syntactic relations are partly determined by morpho-logical features. Man y applications including syn-tactic parsing, word sense disambiguation, text to speech synthesis and spelling correction depend on accurate analyses of words.

An important qualitati ve dif ference between part of speech tagging in English and morphological dis-ambiguation in an agglutinati ve language lik e Turk-ish is the number of possible tags that can be as-signed to a word. Typical English tag sets include less than a hundred tag types representing syntac-tic and morphological information. The number of potential morphological tags in Turkish is theoret-ically unlimited. We have observ ed more than ten thousand tag types in our training corpus of a mil-lion words. The high number of possible tags poses a data sparseness challenge for the typical machine learning approach, some what akin to what we ob-serv e in word sense disambiguation.

One way out of this dilemma could be to ignore the detailed morphological structure of the word and focus on determining only the major and minor parts of speech. Ho we ver (Oflazer et al., 1999) observ es that the modier words in Turkish can have depen-dencies to any one of the inectional groups of a deri ved word. For example, in  X ma vi masal oda X  (= the room with a blue table ) the adjecti ve  X ma vi X  (= blue ) modies the noun root  X masa X  (= table ) even though the nal part of speech of  X masal X  is an ad-jecti ve. Therefore, the nal part of speech and in-ection of a word do not carry suf cient information for the identication of the syntactic dependencies it is involv ed in. One needs the full morphological analysis.

Our approach to the data sparseness problem is to consider each morphological feature separately . Ev en though the number of potential tags is un-limited, the number of morphological features is small: The Turkish morphological analyzer we use (Oflazer , 1994) produces tags that consist of 126 unique features. For each unique feature f , we tak e the subset of the training data in which one of the parses for each instance contain f . We then split this subset into positi ve and negati ve examples depend-ing on whether the correct parse contains the feature f . These examples are used to learn rules using the Greedy Prepend Algorithm (GP A), a novel decision list learner .

To predict the tag of an unkno wn word, rst the morphological analyzer is used to generate all its possible parses. The decision lists are then used to predict the presence or absence of each of the fea-tures contained in the candidate parses. The results are probabilistically combined taking into account the accurac y of each decision list to select the best parse. The resulting tagging accurac y is 96% on a hand tagged test set.

A more direct approach would be to train a single decision list using the full tags as the tar get classi-cation. Given a word in conte xt, such a decision list assigns a complete morphological tag instead of pre-dicting indi vidual morphological features. As such, it does not need the output of a morphological ana-lyzer and should be considered a tagger rather than a disambiguator . For comparison, such a decision list was built, and its accurac y was determined to be 91% on the same test set.

The main reason we chose to work with decision lists and the GP A algorithm is their rob ustness to ir-rele vant or redundant features. The input to the deci-sion lists include the suf x es of all possible lengths and character type information within a ve word windo w. Each instance ends up with 40 attrib utes on average which are highly redundant and mostly irrel-evant. GP A is able to sort out the rele vant features automatically and build a fairly accurate model. Our experiments with Nai ve Bayes resulted in a signif-icantly worse performance. Typical statistical ap-proaches include the tags of the pre vious words as inputs in the model. GP A was able to deli ver good performance without using the pre vious tags as in-puts, because it was able to extract equi valent infor -mation implicit in the surf ace attrib utes. Finally , un-lik e most statistical approaches, the resulting models of GP A are human readable and open to interpreta-tion as Section 3.1 illustrates.

The next section will revie w related work. Sec-tion 3 introduces decision lists and the GP A training algorithm. Section 4 presents the experiments and the results. There is a lar ge body of work on morphological dis-ambiguation and part of speech tagging using a va-riety of rule-based and statistical approaches. In the rule-based approach a lar ge number of hand crafted rules are used to select the correct morphological parse or POS tag of a given word in a given conte xt (Karlsson et al., 1995; Oflazer and T  X  ur, 1997). In the statistical approach a hand tagged corpus is used to train a probabilistic model which is then used to select the best tags in unseen text (Church, 1988; Hakkani-T  X  ur et al., 2002). Examples of statisti-cal and machine learning approaches that have been used for tagging include transformation based learn-ing (Brill, 1995), memory based learning (Daele-mans et al., 1996), and maximum entrop y models (Ratnaparkhi, 1996). It is also possible to train sta-tistical models using unlabeled data with the ex-pectation maximization algorithm (Cutting et al., 1992). Van Halteren (1999) gives a comprehensi ve overvie w of syntactic word-class tagging.

Pre vious work on morphological disambiguation of inectional or agglutinati ve languages include unsupervised learning for of Hebre w (Le vinger et al., 1995), maximum entrop y modeling for Czech (Haji c and Hladk  X  a, 1998), combination of statistical and rule-based disambiguation methods for Basque (Ezeiza et al., 1998), transformation based tagging for Hungarian (Me gyesi, 1999).

Early work on Turkish used a constraint-based ap-proach with hand crafted rules (Oflazer and Kuru  X  oz, 1994). A purely statistical morphological disam-biguation model was recently introduced (Hakkani-T  X  ur et al., 2002). To counter the data sparseness problem the morphological parses are split across their deri vational boundaries and certain indepen-dence assumptions are made in the prediction of each inectional group.

A combination of three ideas mak es our approach unique in the eld: (1) the use of decision lists and a novel learning algorithm that combine the statis-tical and rule based techniques, (2) the treatment of each indi vidual feature separately to address the data sparseness problem, and (3) the lack of dependence on pre vious tags and relying on surf ace attrib utes alone. We introduce a new method for morphological dis-ambiguation based on decision lists. A decision list is an ordered list of rules where each rule consists of a pattern and a classication (Ri vest, 1987). In our application the pattern species the surf ace at-trib utes of the words surrounding the tar get such as suf x es and character types (e.g. upper vs. lower case, use of punctuation, digits). The classication indicates the presence or absence of a morphological feature for the center word. 3.1 A Sample Decision List We will explain the rules and their patterns using the sample decision list in Table 2 trained to identify the feature +Det (determiner).

The value in the class column is 1 if word W should have a +Det feature and 0 otherwise. The pattern column describes the required attrib utes of the words surrounding the tar get word for the rule to match. The last (def ault) rule has no pattern, matches every instance, and assigns them +Det . This def ault rule captures the beha vior of the ma-jority of the training instances which had +Det in their correct parse. Rule 4 indicates a common exception: the frequently used word  X c X  X k X  (mean-ing very) should not be assigned +Det by def ault:  X c X  X k X  can be also used as an adjecti ve, an adv erb, or a postposition. Rule 1 introduces an exception to rule 4: if the right neighbor R1 ends with the suf x +D A (the locati ve suf x) then  X c X  X k X  should recei ve +Det . The meanings of various symbols in the pat-terns are described belo w.

When the decision list is applied to a windo w of words, the rules are tried in the order from the most specic (rule 1) to the most general (rule 5). The rst rule that matches is used to predict the classication of the center word. The last rule acts as a catch-all; if none of the other rules have matched, this rule as-signs the instance a def ault classication. For exam-ple, the ve rule decision list given abo ve classies the middle word in  X pek c X  X k alanda X  (matches rule Table 3: Symbols used in the rule patterns. Capital letters on the right represent character groups useful in identifying phonetic variations of certain suf x es, e.g. the locati ve suf x +D A can surf ace as +de, +da, +te, or +ta depending on the root word ending. 1) and  X pek c X  X k insan X  (matches rule 2) as +Det , but  X insan c X  X k daha X  (matches rule 4) as not +Det .
One way to interpret a decision list is as a se-quence of if-then-else constructs familiar from pro-gramming languages. Another way is to see the last rule as the def ault classication, the pre vious rule as specifying a set of exceptions to the def ault, the rule before that as specifying exceptions to these excep-tions and so on. 3.2 The Gr eedy Pr epend Algorithm (GP A) To learn a decision list from a given set of training examples the general approach is to start with a de-fault rule or an empty decision list and keep adding the best rule to cover the unclassied or misclassi-ed examples. The new rules can be added to the end of the list (Clark and Niblett, 1989), the front of the list (W ebb and Brkic, 1993), or other positions (Ne wlands and Webb, 2004). Other design decisions include the criteria used to select the  X best rule X  and how to search for it.

The Greedy Prepend Algorithm (GP A) is a variant of the P REPEND algorithm (W ebb and Brkic, 1993). It starts with a def ault rule that matches all instances and classies them using the most common class in the training data. Then it keeps prepending the rule with the maximum gain to the front of the gro w-ing decision list until no further impro vement can be made. The algorithm can be described as follo ws: GPA ( data ) 1 dlist  X  NIL 3 rule  X  [if TRUE then default -class ] 4 while G AIN ( rule , dlist , data ) &gt; 0 5 do dlist  X  prep end( rule , dlist ) 7 retur n dlist
The gain of a candidate rule in GP A is dened as the increase in the number of correctly classied instances in the training set as a result of prepend-ing the rule to the existing decision list. This is in contrast with the original P REPEND algorithm which uses the less direct Laplace preference func-tion (W ebb and Brkic, 1993; Clark and Boswell, 1991).

To nd the next rule with the maximum gain, GP A uses a heuristic search algorithm. Candidate rules are generated by adding a single new attrib ute to the pattern of each rule already in the decision list. The candidate with the maximum gain is prepended to the decision list and the process is repeated until no more positi ve gain rules can be found. Note that if the best possible rule has more than one extra at-trib ute compared to the existing rules in the decision list, a suboptimal rule will be selected. The origi-nal P REPEND uses an admissible search algorithm, O
PUS , which is guaranteed to nd the best possible candidate (W ebb, 1995), but we found O PUS to be too slo w to be practical for a problem of this scale.
We pick ed GP A for the morphological disam-biguation problem because we nd it to be fast and fairly rob ust to the existence of irrele vant or redun-dant attrib utes. The average training instance has 40 attrib utes describing the suf x es of all possible lengths and character type information in a ve word windo w. Most of this information is redundant or irrele vant to the problem at hand. The number of distinct attrib utes is on the order of the number of distinct word-forms in the training set. Ne vertheless GP A is able to process a million training instances for each of the 126 unique morphological features and produce a model with state of the art accurac y in about two hours on a regular desktop PC. 2 In this section we present the details of the data, the training and testing procedures, the surf ace at-trib utes used, and the accurac y results. 4.1 Training Data
Our training data consists of about 1 million words of semi-automatically disambiguated Turkish news text. For each one of the 126 unique morpho-logical features, we used the subset of the training data in which instances have the given feature in at least one of their generated parses. We then split this subset into positi ve and negati ve examples depend-ing on whether the correct parse contains the given feature. A decision list specic to that feature is cre-ated using GP A based on these examples.

Some rele vant statistics for the training data are given in Table 4. 4.2 Input Attrib utes Once the training data is selected for a particular morphological feature, each instance is represented by surf ace attrib utes of ve words centered around the tar get word. We have tried lar ger windo w sizes but no signicant impro vement was observ ed. The attrib utes computed for each word in the windo w consist of the follo wing: 1. The exact word string (e.g. W==Ali'nin) 2. The lowercase version (e.g. W=  X  ali'nin) Note: 3. All suf x es of the lowercase version (e.g. 4. Attrib utes indicating the types of characters at
Each training instance is represented by 40 at-trib utes on average. The GP A procedure is responsi-ble for picking the attrib utes that are rele vant to the decision. No dictionary information is required or used, therefore the models are fairly rob ust to un-kno wn words. One potentially useful source of at-trib utes is the tags assigned to pre vious words which we plan to experiment with in future work. 4.3 The Decision Lists At the conclusion of the training, 126 decision lists are produced of the form given in Table 2. The num-ber of rules in each decision list range from 1 to 6145. The longer decision lists are typically for part of speech features, e.g. distinguishing nouns from adjecti ves, and contain rules specic to lexical items. The average number of rules is 266. To get an esti-mate on the accurac y of each decision list, we split the one million word data into training, validation, and test portions using the ratio 4:1:1. The train-ing set accurac y of the decision lists is consistently abo ve 98%. The test set accuracies of the 126 deci-sion lists range from 80% to 100% with the average at 95%. Table 5 gives the six worst features with test set accurac y belo w 89%; these are the most dif cult to disambiguate. 4.4 Corr ect Tag Selection To evaluate the candidate tags, we need to combine the results of the decision lists. We assume that the presence or absence of each feature is an indepen-dent event with a probability determined by the test set accurac y of the corresponding decision list. For example, if the +P3pl decision list predicts YES , we assume that the +P3pl feature is present with Table 5: The six features with the worst test set ac-curac y. probability 0 . 8408 (See Table 5). If the +Fut deci-sion list predicts NO , we assume the +Fut feature is present with probability 1  X  0 . 8511 = 0 . 1489 . To avoid zero probabilities we cap the test set accura-cies at 99%.

Each candidate tag indicates the presence of cer -tain features and the absence of others. The prob-ability of the tag being correct under our indepen-dence assumption is the product of the probabilities for the presence and absence of each of the 126 fea-tures as determined by our decision lists. For ef-cienc y, one can neglect the features that are absent from all the candidate tags because their contrib u-tion will not effect the comparison. 4.5 Results The nal evaluation of the model was performed on a test data set of 958 instances. The possible parses for each instance were generated by the morpholog-ical analyzer and the correct one was pick ed manu-ally . 40% of the instances were ambiguous, which on the average had 3.9 parses. The disambiguation accurac y of our model was 95.82%. The 95% con-dence interv al for the accurac y is [0.9457, 0.9708].
An analysis of the mistak es in the test data sho w that at least some of them are due to incorrect tags in our training data. The training data was semi-automatically generated and thus contained some er-rors. Based on hand evaluation of the dif ferences be-tween the training data tags and the GP A generated tags, we estimate the accurac y of the training data to be belo w 95%. We ran two further experiments to see if we could impro ve on the initial results.
In our rst experiment we used our original model to re-tag the training data. The re-tagged training data was used to construct a new model. The result-ing accurac y on the test set increased to 96.03%, not a statistically signicant impro vement.

In our second experiment we used only unam-biguous instances for training. Decision list training requires negati ve examples, so we selected random unambiguous instances for positi ve and negati ve ex-amples for each feature. The accurac y of the result-ing model on the test set was 82.57%. The problem with selecting unambiguous instances is that certain common disambiguation decisions are never repre-sented during training. More careful selection of negati ve examples and a sophisticated bootstrapping mechanism may still mak e this approach workable.
Finally , we decided to see if our decision lists could be used for tagging rather than disambigua-tion, i.e. given a word in a conte xt decide on the full tag without the help of a morpholo gical analyzer . Ev en though the number of possible tags is unlim-ited, the most frequent 1000 tags cover about 99% of the instances. A single decision list trained with the full tags was able to achie ve 91.23% accurac y using 10000 rules. This is a promising result and will be explored further in future work. We have presented an automated approach to learn morphological disambiguation rules for Turkish us-ing a novel decision list induction algorithm, GP A. The only input to the rules are the surf ace attrib utes of a ve word windo w. The approach can be gener -alized to other agglutinati ve languages which share the common challenge of a lar ge number of poten-tial tags. Our approach for resolving the data sparse-ness problem caused by the lar ge number of tags is to generate a separate model for each morphologi-cal feature. The predictions for indi vidual features are probabilistically combined based on the accu-rac y of each model to select the best tag. We were able to achie ve an accurac y around 96% using this approach.
 We would lik e to thank Kemal Oazer of Sabanc Uni versity for pro viding us with the Turkish mor -phological analyzer , training and testing data for dis-ambiguation, and valuable feedback.
