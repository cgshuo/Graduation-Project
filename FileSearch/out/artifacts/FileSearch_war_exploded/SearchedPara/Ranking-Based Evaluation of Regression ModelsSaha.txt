
We suggest the use of ranking-based evaluation mea-sures for regression models, as a complement to the com-monly used residual-based evaluation. We argue that in some cases, such as the case study we present, ranking can be the main underlying goal in building a regression model, and ranking performance is the correct evaluation metric. However, even when ranking is not the contextu-ally correct performance metric, the measures we explore still have significant advantages: They are robust against extreme outliers in the evaluation set; and they are inter-pretable. The two measures we consider correspond closely to non-parametric correlation coefficients commonly used in data analysis (Spearman X  X   X  and Kendall X  X   X  ); and they both have interesting graphical representations, which, sim-ilarly to ROC curves, offer useful  X  X artial X  model perfor-mance views, in addition to a one-number summary in the area under the curve. We illustrate our methods on a case study of evaluating IT Wallet size estimation models for IBM X  X  customers.
The standard approach to evaluating regression mod-els on holdout data is through additive, residual-based loss functions, such as squared error loss or absolute loss. These measures are attractive from a statistical perspective as they have likelihood interpretations and from an engineering or scientific perspective because they often represent the  X  X rue X  cost of the prediction errors.

In this paper we propose a different approach to the eval-uation of regression models, through their success in rank-ing the test set observations in the correct order. There are several reasons why ranking-based evaluation of regression models is interesting. First, ranking may be the real goal in building the prediction model. That is, in some cases we may just want to be able to separate the entities which are likely to have high response from the ones which cor-respond to low response. An example is given in the case study we present in Section 5, where the goal is to iden-tify companies with high potential IT spending (IT Wal-let). These are preferred targets for vigorous sales efforts by IBM. If we assume that IBM X  X  sales resources are fixed, then identifying the largest IT Wallets, regardless of their actual numeric value, is the best support a regression model can give.
 Second, ranking-based measures are quite interpretable. The two main evaluation methods we consider allow us to draw interpretations and connections:  X  We build graphical representations similar to ROC  X  We can connect the one-number ranking performance Third, ranking-based evaluation is robust. That is, it is only mildly affected by errors  X  even gross ones  X  both in the values of features and in the measured response. In Sec-tion 4 we define a concept of  X  X valuation robustness X  and show that ranking-based evaluation methods are very ro-bust, whereas commonly used additive, residual-based eval-uation measures, like mean squared error or mean abso-lute error, are extremely non-robust for evaluation. Some residual-based methods, like median absolute error, are ro-bust, but tend to ignore completely the error measure on many observations. As such, they can be argued to be  X  X hrowing the baby with the water X . Ranking-based eval-uation, on the other hand, considers the evidence from all observations, while not allowing gross errors to affect the overall evaluation measure too strongly.

Our paper is organized as follows. In Section 2 we in-troduce an evaluation nomenclature, describe the standard, residual-based evaluation approaches, define the concept of ranking-based evaluation, and introduce our suggested ranking-based measures. We analyze these measures, their interpretations, their statistical properties, and their visu-alizations in Section 3. Section 4 is devoted to the topic of  X  X valuation robustness X   X  we define an evaluation ro-bustness measure and show that the residual-based additive methods are non-robust, while ranking-based measures are highly robust. In Section 5, we present our motivating case-study, of evaluating IT Wallet estimation models for guiding IBM X  X  Sales &amp; Marketing efforts.
In this paper we concentrate on the model evaluation phase of supervised learning of regression models. We do not consider at all the process of model building , instead as-suming that we are given a model  X  y = m ( x ) which attempts to predict a response y  X  R as a function of a feature vector x . Assume further we have a test set of size n : { x i ,y i not used in modeling, which we want to utilize to evaluate the performance of model m in predicting y . 1
The standard approach to evaluating regression model performance is based on additive measures of error, depend-ing on the residuals r i = y i  X  m ( x i ) : Some commonly used error measures are: Other approaches to regression model evaluation include Regression Error Curves [8, 1], where the model is eval-uated according to its error rate at different levels of  X  X r-ror tolerance X ; and using medians of the absolute deviations (MAD), rather than their mean, as the error measure: In this paper we consider, instead of these residual-based evaluation measures, the use of ranking-based measures, which evaluate the performance of the scoring model m ( x ) in sorting the values of y from  X  X arge X  to  X  X mall X . We as-sume, without loss of generality, that the test set is sorted in decreasing order of model scores, that is: 2 We rank the responses in the test set in decreasing order. Let s i be the rank of observation i in this order: Ranking-based evaluation uses only these ranks in evalu-ating model performance. We consider here two ranking-based evaluation measures and their interpretations. We start by defining the following intuitive statistics: The first measure simply counts how many of the pairs in the test data are ordered incorrectly by the model m ( x ) second also considers these incorrect orderings, but weighs them by the difference in their model ranks, a measure of the magnitude of error being committed.

Next, we linearly transform these two measures to put them into the range [  X  1 , 1] , where 1 corresponds to per-fect model performance ( T,R =0 ) and  X  1 corresponds to making all possible errors, thus attaining perfect reverse ranking. It is easy to verify that max( T )= n ( n  X  1) / 2 max( R )= n ( n  X  1)( n +1) / 6 . Thus we re-scale: This in fact gives us exactly Kendall X  X   X   X  and Spearman X  X  [6]  X  the measures for non-parametric correlation preva-lent in Statistical data analysis tools. We use here the nota-tion  X   X  and  X   X  , to denote that these are the sample quantities (which [6] denote by t, r respectively), as compared to the  X  X opulation X  quantities:  X  = E  X   X , X  = E  X   X  .

In what follows, we will use, in addition to  X   X  and  X   X  , their re-normalized versions:  X   X  =1 / 2+ X   X / 2 and  X   X  =1 / 2+ X   X / 2 Both of these are in [0 , 1] with 1 corresponding to perfect ranking and 0 to inverse ranking.
In this section, we present some of the interpretations and visualization approaches that we can apply to our rank-ing evaluation measures  X   X  ,  X   X  .
Of the two measures we suggest,  X   X  is the more natural target for a statistical analysis. First, we observe that be interpreted as the percentage of all pairs that are correctly ranked from the total of n ( n  X  1) / 2 pairs of observations [6]. Thus, E  X   X  is the probability of correctly ranking a randomly drawn pair of observations. This gives  X   X  a probabilistic in-terpretation, in the same spirit as the area under the ROC curve [2].

Second, we are interested the distribution of  X   X  and  X   X  the uncertainty inherent to them. The moments of  X   X  and  X   X  under the relevant null assumptions (  X  =0 and  X  =0 , respectively) are quite easy to calculate and a normal ap-proximation gives a hypothesis testing methodology for the assumption of no correlation [6]. However, testing this as-sumption  X  that the ranking by scores and by actual test set responses are independent  X  is of little interest in most cases for model evaluation. We expect any reasonable pre-diction model to create rankings that are indeed correlated with the rankings by response.

Of much more practical interest are confidence intervals for the values of evaluation measure given its sample value (the non-null case), as they represent the uncertainty in the model evaluation based on a single test set. For residual-based measures, it is typically not possible to build confi-dence intervals without parametric assumptions and/or vari-ance estimation. The non-parametric nature of  X   X  allows us to write a general expression for its variance [7]:
Var ( X   X  )= where  X  c = E  X   X  =1 / 2+1 / 2  X  and  X  cc are two properties of the ranking function (we omit here the exact complicated definition of  X  cc ). We can replace these with their sample means, and after some algebra obtain [7]:  X  Var ( X   X  )= number of observations that are  X  X oncordant X  with obser-vation i , that is, that their ranking relative to i in the test data agrees with the ranking by model scores (as plotted in Figure 1 below).

Since  X   X  is asymptotically normal [6], we can obtain an approximate, consistent 1  X   X  confidence interval for  X  as: We are not aware of a similar calculation for  X   X  .
Visualizations of ranking performance can often provide additional insights about model performances. We will use the data from the case study in Section 5 to illustrate the dif-ferent visualization approaches. This dataset was collected from a survey of 500 firms reporting the amount of money allocated in 2003 to the purchase of IT goods. The 500 ob-servations are considered the ground truth against which we wish to evaluate an IBM internal model.
Figure 1. Percent of correctly ranked pairs in-volving a particular prediction.

Starting from the largest model prediction m ( x 1 ) we cal-culate in decreasing order for each observation x i the per-centage of correctly ranked pairs ( y i ,y j ) over all Figure 1 shows this percentage of correctly ranked pairs as a function of the rank. The area above the curve is the sum of the percent incorrectly ranked pairs, which is equal to 2  X  T/n ( n  X  1) . Therefore, the area under the curve equals  X   X  . The dashed line corresponds to a locally optimal per-formance. A perfect model would show a constant perfor-mance of 100% correctly ranked pairs. But given that the model is not perfect and makes predictions that are some-times too large or too small, even a perfect prediction for a particular observation with m ( x i )= y i will have a number of inversely ranked pairs due to errors of the other predic-tions. The upper limit of the performance for a given pre-diction, keeping everything else constant, is therefore not 100% but determined by the model performance of predic-tions around it. The interpretation of the locally optimal performance is therefore the highest achievable percentage of correctly ranked pairs if m ( x i ) could be placed arbitrar-ily, given all other model predictions.

The performance in a particular region of the graph is characterized by two properties of the plot, 1) the distance of the upper envelope from the 100% line, and 2) the dis-tance of the actual performance from the upper envelope. A particular region with a performance that on average re-mains very close to the upper envelope has a nearly optimal local ranking and is only disturbed by bad predictions that were either larger or small than the predictions of the region.
In summary, this graph provides a number of relevant insights for model evaluation and analysis:  X  It shows the variability of the ranking performance for  X  It shows large outliers with less than 50 percent correct
The next two graphs are related to  X   X  . For Figure 2 we transform the original regression results into n  X  1 classi-fication results, where we discretize the observations into a binary class variable c ( i ) ( y j )=1 iff y j  X  y i for all possible cutoffs 1 &lt;i&lt;n .

For each classification c ( i ) we can evaluate the model performance using the area under the ROC curve (AUC, [2]). The probabilistic interpretation of the AUC is the prob-ability that a pair of observations with opposite class labels is ranked correctly. Since AUC i only considers pairs with different class labels under cutoff i , the number of pairs used to calculate AUC i is equal to i  X  ( n  X  i ) and the num-ber of incorrectly ranked pairs under this cutoff is therefore (1-AUC i )  X  i  X  ( n  X  i ) .

The total number of times a pair of observations will be assigned opposite class labels across all cutoffs is equal to the rank difference: a neighboring pair ( k, k +1 ) receives opposite class labels only if the cutoff is equal to k whereas the extreme pair ( 1 ,n ) will have opposite classes for all cutoffs. Given our definition in (3) this implies that i (1  X  AU C i )  X  i  X  ( n  X  i )= R .

Since each AUC i is rescaled by a different factor i  X  ( n  X 
Figure 2. AUC as a function of cutoff posi-tion above which the truth will be assigned to class 1 adjusted for the number of pairs. Note the nonlinear transformation of the x-axis. i ) , a graph of the AUC i as a function of the cutoff i would not have an area equal to  X   X  . In order to achieve a direct correspondence with  X   X  we have to allocate i  X  ( n  X  i ) AUC i by rescaling the x-axis accordingly. Figure 2 shows such a transformation of the x-axis and has an area under the curve of  X   X  . This graph confirms our earlier notion that the model performs better in ranking large outcomes and worse in ranking small outcomes.

The plot in Figure 3 is very close in spirit to a lift curve. After sorting the model predictions in decreasing order, we plot the cumulative inverse rank of the truth p = i j =1 ( n  X  s j +1) for increasing cutoffs i in percent. Using the inverse rank emphasizes the model performance on the largest predictions that is shown in the bottom left of the graph. The model performance is bounded above by the optimal ranks p i = i j =1 ( n  X  j +1) and below by the cumulative worst (inverse) ranking w i = i j =1 j . The area under the model curve is given as n i =1 ( n  X  i )( n  X  s can be shown to be equal to n 3  X  n 2  X  n  X  R + n i =1 i 2 exposing the relationships to  X   X  (see [6] for details).
A commonly used definition of robustness in model fit-ting uses the concept of fitting breakdown point . A simpli-fied version of the breakdown point definition of [5, p. 98] reads: Figure 3. Lift-curve of the cumulative rank.

With about 10% of the data the model is able to capture 15% of the cumulative rank, about 4% less than the optimal.
 Using this definition, we can show that linear regression with squared error loss has a breakdown point of 1 /n . Thus, this is a non-robust procedure  X  one corrupted data point can affect the fitted model arbitrarily badly. Linear regres-sion with absolute loss, on the other hand, has a breakdown point of  X  X lmost X  1 / 2 . This is a robust fitting procedure, since as long as less than half of the data points are cor-rupted we are guaranteed to remain  X  X easonably close X  to the uncorrupted solution (see [5] and references therein for details). As we will see below, absolute loss is not robust for evaluation.

We would like to define a similar notion of robustness for evaluation. We are not aware of any work in the literature on that topic. Here we suggest one such notion. Consider the following evaluation process: Inputs: Regression model: m : R p  X  R Test set of size n : { x i ,y i } n i =1 Continuous evaluation loss measure: L : R n  X  R n  X  R Evaluation procedure: Apply model to test set to get predictions: ( m ( x 1 ) , ..., m ( x n )) Apply evaluation measure to model: e = L ( m , y ) We are now ready to define evaluation breakdown M =  X  ). The evaluation breakdown of an evaluation met-ric L , denoted R ( L ) is the smallest % of test data points that need to be arbitrarily corrupted to guarantee that for any c&lt;M and any test data we can get L ( m , y ) &gt;c The following simple result is an immediate consequence of this definition: Proposition 1 If L is:  X  A non-negative, additive function of the residuals, that  X  Unbounded (that is, M =  X  ) Then R ( L )=1 /n for a test set of size n . That is, a single corrupted data point is enough to guarantee arbitrarily bad evaluation.
 Proof Because of the additivity, M =  X  implies sup u  X  R L ( u )=  X  as well. Given a value c&lt;  X  ,let be such that L ( u 0 ) &gt;c . Now, if we choose any observation ( x k ,y k ) and corrupt its response y k to  X  y k = m ( x k we get the evaluation score: L (  X  y k  X  m ( x k )) + i = k L ( y i  X  m ( x i ))  X  L ( u
This result illustrates that squared error loss, absolute loss and any other unbounded function of the residuals are all evaluation non-robust and have a breakdown point of 1 /n for evaluation. This is less trivially also true of the area over the REC curve (AOC), suggested by [1] as a one-number summary of model performance. The AOC can be bounded from below by an additive function:
AOC  X  and the resulting lower bound is evaluation non-robust, which clearly leads to the AOC being evaluation non-robust as well.

On the other hand, MAD (1) is clearly a robust evalua-tion measure, since corrupting almost half of the data arbi-trarily still guarantees that the median is in the uncorrupted half, and so is not significantly affected. It can be argued, though, that by ignoring completely the large absolute resid-uals, MAD is  X  X oo robust X  and does not penalize a model for making gross errors. Our suggested ranking-based evalua-tion measures are also robust. The constant M of Defini-tion 1 is finite and to get arbitrarily close to it we clearly need to corrupt almost all our data, in some cases all of it, to make sure that the order of all pairs is incorrect. More convincingly, we can derive an O ( 1 n ) bound on the effect of any outlier on the overall model evaluation score (a property not shared by any of the residual-based methods, including MAD), as follows: Proposition 2 Given a test set of size n , if we corrupt at most k observations arbitrarily, the change in both the mea-sures  X   X  and  X   X  is O (1 /n  X  k ) .
 Proof Denote the original label vector by y and the cor-rupted one by y  X  . If we consider our two measures T,R as defined in (2, 3), we can see that: | T ( y  X  )  X  T ( y ) | X  | R ( y  X  )  X  R ( y ) | X  k ( where the second calculation uses the equivalent formula-tion R = i i 2  X  i i  X  s i , proven in [6], and the fact that the change in the ranks of the corrupted observations is no more than n , while the ranks of the non-corrupted observa-tions can be changed by at most k .

Plugging this into the definitions of  X   X ,  X   X  we get:
The wallet of a customer (or potential customer) is de-fined as the total amount that a customer spends in a certain product category in a given time frame. There are many possible uses for wallet estimates, which include targeting sales/marketing actions towards large wallet customers, de-tecting partial defection of customers and rewarding sales representatives according to the share of the wallet of a cus-tomer that they attain. Recent marketing literature demon-strates that knowing the customer X  X  share of wallet is impor-tant for customer relationship management [4].

In certain industries, customer wallets can be easily ob-tained from public data. For example, in the credit card industry, the card issuing companies can calculate the wal-let size using credit records from the major credit bureaus. For most industries, however, no public wallet information is available at the customer level. In this case, a model that relates the available information about the customer to the wallet needs to be created. A common approach is to obtain
Figure 4. IT Wallet Distribution. Note that the x-axis is on a logarithmic scale. actual wallet information for a random subset of customers through a survey and build a regression model.

At IBM, a variety of approaches have been considered for estimating the wallet of customers for information tech-nology (IT) products, including heuristic approaches and predictive modeling. Given the variety of models, there was a pressing need for an objective comparison of their performance. For this reason, a survey was conducted to obtain actual 2003 wallet figures for a set of 500 companies that are customers or potential customers of IBM in the US, for three product categories: hardware, software and ser-vices. In this case study, we do not discuss building models for wallet estimation, but restrict ourselves to the important problem of evaluating and comparing models in a meaning-ful and robust fashion.
Figure 4 shows the distribution of IT wallets obtained in the survey. Note that this distribution is long-tailed, that is, there are many companies with relatively small wallet size and a few companies with very large wallet size. Therefore, evaluation measures such as mean squared error and mean absolute error can be greatly influenced by a small subset of companies that have very large wallets and for which the models are more likely to make larger absolute errors. On the other hand, measures such as median squared error can completely ignore the performance of the model on the companies with large IT wallet size, which are usually the most important customers. An approach that is often used to mitigate the effects of a skewed distribution (especially in modeling) is to transform the numbers to a logarithmic scale. This approach, however, is not adequate for the eval-uation of wallet models, since log-dollars is a unit that does not have a clear financial meaning and, therefore, cannot be used in conjunction with other financial variables such as
Table 1. The effect of the single most influen-tial observation on different measures. budgets and costs.

For some practical applications, such as targeting large wallet customers, ranking the customers according to wal-let size is all that is needed. To make this more concrete, let us assume that a certain budget of B dollars is available for targeting customers and that the cost of targeting a customer is fixed at c dollars. Then, we can target at most k = B customers. Given that the number of customers we can tar-get is fixed at k , it is easy to see that the best strategy given the wallet estimates is to target the customers with the top wallet values. Because B and c can vary over time, we need a complete ranking of customers to be able to threshold at any point.

Thus, we can argue that evaluating wallet models us-ing ranking-based measures is advantageous for at least two reasons. First, as demonstrated in Section 4, they are robust, which is especially important when we have skewed distri-butions. Second, they evaluate the appropriate performance measure, at least for targeting applications.
To illustrate the sensitiveness of different performance measures to a single observation, we calculate the root mean squared error (RMSE), mean absolute error (MAE), Kendall X  X   X   X  and Spearman X  X   X   X  measures of the model that predicts the software wallet for the companies included in the survey. Then, we recalculate the measures excluding the single company that has the largest influence on each measure. For additive measures this is the company whose estimated wallet is the furthest from the actual wallet. For ranking measures we do an exhaustive search to determine the removal that leads to the biggest change. The results are shown in Table 1.

The percent change characterizes empirically the degree to which each measure is affected by a single company. For RMSE and MAE a single company is responsible for 49.8% and 25.9% of the total loss, respectively, while for Kendall X  X   X   X  and Spearman X  X   X   X  no company is responsible for more than 2.05% of the total correlation. These empirical results are in agreement with the theoretical results from Section 4 showing that the ranking-based measures are more robust than residual-based measures.
Here, we use residual-based and ranking-based perfor-mance measures to compare two existing IBM Software Wallet models, which in this study we refer to as M 1 and M In table 2 we show the RMSE, MAE, Kendall X  X   X   X  and Spearman X  X   X   X  measures of the two models for the 500 com-panies included in the survey. According to these results, M 1 is slightly outperforming M 2 for the residual-based measures (RMSE and MAE), where smaller is better, and it is greatly outperforming M 2 for the ranking-based mea-sures (  X   X  and  X   X  ), where larger values are better.
A natural question to ask at this point is whether the dif-ferences in performance between these models are signif-icant. In order to measure statistical significance, we cre-ate 100 bootstrap samples from the original survey of 500 companies. Using the bootstrap samples, we can obtain es-timates of the standard deviation of the difference between the performance of the two models for each measure. As-suming a gaussian distribution, we compute p-values using these estimates of the standard deviation and the actual dif-ference in performance from the original sample. The last three columns of Table 2 summarize these results.

From these results, we can conclude that the difference in performance between the two models is statistically sig-nificant for the ranking-based measures and not statistically significant for the residual-based measures. This is a conse-quence of the fact that we have a few extreme outliers in the test sample. The outliers considerably affect the residual-based measures and make them unreliable and inappropri-ate for model comparison. On the other hand, the ranking-based measures are much more stable and robust to the out-liers, allowing us to be confident that model M 1 is indeed performing better than M 2 in terms of ranking.

Using the bootstrap samples, we can estimate the vari-ance of  X   X  and compare it to the analytical expression (6). The numbers (for model M 2 ) are indeed very close: the empirical estimate is 0 . 00103 and the analytical estimate is 0 . 00097 .
The graphs depicted in Section 3 use data from this case study, more specifically from model M 1 . By investigating these graphs, we can make the following observations about this model:  X  From Figures 1 and 2 we can see that the model is able  X  In Figure 1, we see that there are some examples that
Figure 5. Comparative model performance in terms of AUC across all cutoffs.

Figure 5 presents the comparative AUC X  X  as a function of cutoff position for both models. The graph shows that model M 1 is consistently outperforming model M 2 for all the cutoff points. A bootstrap analysis of the performance differences between M 1 and M 2 shows that within a 0.1 to 0.9 cutoff range M 1 outperforms M 2 more than 95% of the time. This fits in nicely with the significance results for in Table 2, which implies that the area under M 2 is signif-icantly smaller than under M 1 (given the interpretation we presented in Section 3.2).
We have presented ranking-based evaluation strategies for regression models that are often appropriate for market-ing tasks (as shown in our case study) and are more robust to outliers than traditional residual-based performance mea-sures. Other advantages of our evaluation approach include the proposed visualization methods. These can provide in-sights about local model performance and outliers. Further contributions of this work include the definition of evalu-ation robustness as a property of different evaluation mea-sures, and the presentation of confidence intervals for one of the suggested ranking measures.

