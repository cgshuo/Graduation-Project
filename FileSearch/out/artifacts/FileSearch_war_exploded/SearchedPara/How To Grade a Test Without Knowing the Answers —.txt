 Yoram Bachrach yobach@microsoft.com Microsoft Research, Cambridge, UK Tom Minka minka@microsoft.com Microsoft Research, Cambridge, UK John Guiver joguiver@microsoft.com Microsoft Research, Cambridge, UK Thore Graepel thoreg@microsoft.com Microsoft Research, Cambridge, UK Collective decision making is a well-studied topic in social choice, voting and artificial intelligence. It has long been known that decisions based on aggregating the opinions of several agents can be of higher quality than those based on the opinions of single individuals. The Condorcet Jury Theorem (de Caritat et al., 1785), dating back to the 18th century, is concerned with a group of individuals attempting to reach a binary de-cision by majority vote; it is assumed that one of the two outcomes of the vote is  X  X orrect X , and that each in-dividual independently chooses the  X  X orrect X  response with probability p . The theorem states that if p &gt; 1 then adding more agents increases the probability of making the correct decision, and the probability that the collective decision will be  X  X orrect X  approaches 1 in the limit of infinitely many participating agents. Recent technological advances make it easier to share opinions and knowledge, enabling us to harness the collective intelligence of crowds for solving tasks. Companies can use crowdsourcing to carry out busi-ness tasks, using platforms such as Amazon Mechani-cal Turk. Such services allow us to collect the opinions of many individuals, but leave open the question of how to aggregate the collected data to reach decisions. A key technique for solving tasks using collective intelligence is to obtain information from multiple sources and aggregate it into a single complete solu-tion. Consider a crowd of experts who are assigned with many similar classification tasks, such as classi-fying many news articles according to their topic ( X  X ol-itics X ,  X  X usiness X ,  X  X ntertainment X  etc.). We refer to each expert as a participant and to each classification task as a question . Suppose each participant expresses her opinion regarding the correct answer for each ques-tion, in the form of a response , chosen by her from the list of possible answers for that question. Similarly to Condorcet X  X  Jury Theorem, we make the simplify-ing assumption that for each of the questions only one answer is correct . We call such a domain a multiple problem domain . Given the responses provided by the participants regarding the various questions in a mul-tiple problem domain, how should we best determine the correct answer for each of the items? Which ques-tions are easy and which are hard? How can we find the most competent participants in the crowd? Which questions best test the ability of a participant? Given the correct answers to each of the items, it is easy to find the most competent participants, or dif-ferentiate the easy and hard questions  X  a participant who has answered almost all the questions correctly is likely to be more skilled than a participant who had very few correct responses. Typically, however, the correct answers are not known in advance  X  the whole point of crowdsourcing a classification task is to deter-mine the correct classifications for the items. A possible solution to the above problem is to first evaluate the skill of each expert by asking her to pro-vide responses to a set of items for which the correct answer is known (sometimes called a  X  X old-set X ). A prominent example of this, where all correct answers are known, is intelligence testing (Anastasi et al., 1997). Psychologists have studied human intelligence, and designed IQ tests for evaluating the aptitude of individuals. These tests have been shown to be predic-tive of a person X  X  performance in many domains, such as academic attainment and job success (Anastasi et al., 1997). Such tests typically ask participants to respond to questionnaires composed of many multiple-choice questions, and allow ranking participants ac-cording to their individual skill levels after examining the responses. The properties of responses to IQ tests have been widely studied by psychometricians, so such datasets can serve as a testing ground for exploring in-ference models for multiple problem domains.
 Our Contribution: We propose a new family of graphical models for analyzing responses in multiple problem domains and evaluate the models on a data set of completed questionnaires of a standard IQ test. The proposed framework enables us to jointly infer the correct answer for each question (when these are not known in advance), the difficulty levels of the ques-tions, and the ability of each participant. We show how the model can: determine a probability distribution over answers for a given question by aggregating the responses of participants based on their abilities and the questions X  difficulties; test the ability levels of par-ticipants efficiently by finding the best next question to ask in an adaptive way, depending on the previous responses; automatically calibrate aptitude tests from a set of questions and the responses provided by the participants, determining the relative difficulty levels of the questions and their ability to discriminate be-tween participants of similar but uneven skill levels. Measuring intelligence is a key topic in psychology. Psychologists showed that peoples X  performance on many cognitive tasks is strongly correlated, so a single statistical factor called  X  X eneral intelligence X  emerges (Anastasi et al., 1997). A measure for the per-formance of groups of people in joint tasks, called  X  X ol-lective intelligence X , was investigated (Woolley et al., 2010). This approach focuses on explicit collabora-tion and interaction between members of the crowd. Although in our setting participants do not interact directly, one can view our model as a method for infer-ring correct answers to questions given the responses of a crowd of individuals. The number of correct answers inferred can serve as a measure of the intelligence of the crowd. In this sense, our work is somewhat similar to other approaches which also use aggregated responses to IQ tests for measuring collective intelligence (Lyle, 2008; Bachrach et al., 2012; Kosinski et al., 2012). Psychometricians developed a body of theory called  X  X est theory X  which analyzes outcomes of psychologi-cal testing, such as the ability levels of participants or the difficulty of questions in a test, trying to improve reliability in such tests (Anastasi et al., 1997). One paradigm for designing tests of mental abilities, is the  X  X tem-response theory X  (Hambleton et al., 1991) (IRT for short). IRT has been used to develop high-stakes adaptive tests such as the Graduate Management Ad-mission Test (GMAT). IRT is based on the idea that the probability of a participant providing the correct response to a question is a function of both a param-eter of the question and a parameter of the item (for example, the question X  X  difficulty and the person X  X  ap-titude). When applying aptitude tests, the parameter of the person is latent (cannot be directly observed), and only its manifestation, in the form of the partici-pant X  X  responses, can be directly observed. Our frame-work relies on a probabilistic graphical model (Koller &amp; Friedman, 2009), using themes similar to IRT. Many papers deal with merging opinions, ranging from information aggregation in the semantic web (Kasneci et al., 2010) to prediction markets (Pennock &amp; Sami, 2007). Frameworks such as Probabilistic Relational Models (Getoor et al., 2007) combine a logical repre-sentation with probabilistic semantics, and allow in-ference to aggregate information and opinions. One basic method for collective decision making is voting. Voting was studied in social choice theory (Sen, 1986), which focuses on how participants can manipulate by lying about their preferences. We assume that the experts X  responses are their true opinion and focus on the inference problem. One application of our model is aggregating crowdsourced opinions. A machine learn-ing approach for doing so which does not model task difficulty was proposed in (Raykar et al., 2010) and a technique that models task difficulty but uses an EM approach was proposed in (Whitehill et al., 2009). An-other method based on graphical models is (Welinder et al., 2010). In that model questions are endowed with features, which could represent concepts or top-ics and participants have different areas of expertise matching these topics. Our model focuses on a gen-eral domain in the spirit of test theory and IRT, and does not rely on specific features. An active learning approach for labeling data was proposed in (Yan et al., 2011) and is similar to our adaptive IQ testing tech-nique. Another approach akin to ours is the TrueSkill system (Herbrich et al., 2007), which uses a graphi-cal model to estimate the relative skill levels of people based on past contests. We present a probabilistic model for analyzing multiple problem domains, which we refer to as the Difficulty-Ability-REsponse estimation model, or DARE for short. The inputs to the model are re-sponses that participants give to multiple choice ques-tions. Additional inputs may be ground truth informa-tion for some or all of the questions. The model falls into the framework of probabilistic graphical models. Such models allow structurally describing the genera-tive process assumed to underlie the observed data in terms of latent and observed random variables. In the domain of interest, information such as the correct re-sponse to a question, the ability of a participant, and the difficulty of a question are modeled as unobserved variables whereas the given response to a question by a user is viewed as an observed variable. The struc-ture of the model is determined by the conditional in-dependence assumptions made about the variables in the model. Pearl (Pearl, 1988) introduced Bayesian Networks (directed graphical models), which encode assumptions of conditional independence as a graph whose vertices represent variables and whose edges represent dependencies between variables. We use the more general notion of a factor graph, see e.g. (Koller &amp; Friedman, 2009), to describe the factorial structure of the assumed joint probability distribution among the variables. After defining the structure of the model as a factor graph and setting the observed variables to their observed values, approximate message passing al-gorithms (Koller &amp; Friedman, 2009) can infer marginal probability distributions of unknown variables of in-terest such as the correct response to a question, the ability of a participant, or the difficulty of a question. 3.1. The DARE Model We model a situation in which a set P of participants is available to answer a set Q of multiple choice ques-tions. We assume that for each question q  X  Q there are R q possible answers, only one of which, y q  X  R q , is correct. We model the process by which participants p  X  P produce responses r pq  X  R q to questions q  X  Q . We assume that: a) Every participant has an under-lying ability a p  X  R which determines her ability to determine the correct answer to questions q  X  Q . b) Each question q has an inherent difficulty d q  X  R which determines how likely it is that participants p  X  P will know the correct answer to question q .
 We propose a joint probabilistic model whose factor graph is given in Figure 1: The model has two parts, one modeling the probability of participant p knowing the correct answer to question q (left of c pq in Fig-ure 1), and one relating the true answer y q to question q to the response r pq given by participant p depending on them knowing the correct answer as represented by c pq of the answer (right of c pq in Figure 1). Knowledge of the correct answer, c pq  X  { T,F } , is modeled as an interaction of the ability a p  X  R of participant p , and the difficulty d q  X  R of question q . Specifically, it is assumed to depend on the difference t pq := a p  X  d q via: P ( c pq = T | t pq , X  q ) := Here  X  denotes the standard Gaussian density  X  ( x ) :=  X  2  X   X  1 exp(  X  x 2 / 2) and  X  denotes the (sigmoidal) cu-mulative Gaussian distribution  X ( t ) := R t  X  X  X   X  ( x ) dx ;  X  (  X  ) denotes the step function, and the precision  X  determines how discriminative question q is. The in-tegral representation emphasizes that the probability can be viewed as emerging from a binary process re-sulting from evaluating the step function  X  on variable t with added Gaussian noise of variance  X   X  1 . The response r pq is modeled as a mixture of two distri-butions. If participant p knows the correct answer to question q , c pq = T , we constrain the response r pq to match the correct answer, r pq = y q , otherwise we as-sume that r pq is sampled uniformly at random from the available answers, r pq  X  DiscreteUniform( R q ). Note how this mixture is expressed as a gate (dashed pair of boxes in Figure 1), which switches the factor connect-ing to r pq depending on the state of the variable c pq . Gates were introduced in (Minka &amp; Winn, 2008) as a powerful and flexible notation that simplifies factor-graph representations of mixture models. They can be used to represent context-dependent conditional inde-pendence relations, and are suited for implementations of approximate message passing inference.
 In order to do inference on the model, we need to define prior distributions for the variables of inter-est. We assume factorizing Gaussian priors for the abilities a p  X  Normal(  X  p , X  2 p ) and difficulties d Normal(  X  q , X  2 q ). We choose a Gaussian prior as it lets us specify a range of plausible values based on two parameters (mean and variance) per variable, and ad-mits a relatively simple approximate inference. The factorization assumption reflects the belief that a pri-ori knowing the difficulty of one question would not be informative about the difficulty of another ques-tion, and similarly for the abilities of participants. We also assume factorizing discrete uniform priors for the true answers y q  X  DiscreteUniform( R q ) and for the responses r pq  X  DiscreteUniform( R q ) for participant-question pairs. Finally, we define factorizing Gamma priors for the precision parameters  X  q  X  Gamma( k, X  ). The Gamma prior is conveniently parameterized by a shape parameter k and a scale parameter  X  , and is the conjugate prior for the precision parameter  X  :=  X   X  2 of the normal distribution if the mean  X  is known. This choice simplifies inference by approximate mes-sage passing because the posterior also takes the func-tional form of the Gamma distribution.
 Based on the above specification we defined a joint probability distribution p ( a p ,d q ,t pq , X  q ,c pq ,r specific pairs of question q and participant p . Assum-ing exchangeability of questions q and participants p we get a model with two plates as depicted in Fig-ure 1, where one plate runs over participants p  X  P and the other over questions q  X  Q (plates denote a replication of the fraction of the graphical model they contain). From a generative point of view, this models a table with | P | rows (one for each participant p ) and | Q | columns (one for each question q ), where entries are the responses r pq of participants to questions. 3.2. Probabilistic Inference We show how the model can infer quantities of interest. Generally, the data is given in the form of two incom-plete sets: A set of m participant-question-response triples R := { r p 1 q 1 ,...,r p m q m } and a set of n ground-truth question-answer pairs y := { y q 1 ,...,y q n } . One special case is when all the ground-truth question-answer pairs are known. This is the traditional test scenario as used in aptitude tests including school tests, GMAT, IQ tests, etc. Another special case is crowdsourcing domains where we may not have any ground-truth available, but can obtain participant-question-response triples depending on budget and time constraints. As shown in Section 4, provid-ing some ground-truth question-answer pairs (a  X  X old-set X ), can improve the accuracy of the inferred answers y q because it can be used to assess the abilities a p of the participants p more accurately, leading to more accurate inference on the answers y q . Generally, for every observed response r  X  pq , we set the discrete prior distribution p ( r pq ) to a single point distribution con-centrated on the observed response r  X  pq , and similarly for every known ground-truth question-answer pair y  X  q . Given the data R and y , we wish to infer sev-eral approximate marginal (posterior) distributions: the discrete distribution p ( y q | R , y ) over correct an-swers y q , which assign a probability  X  q r  X  [0 , 1] to each of the possible responses r  X  R q ; the Gaussian density p ( a p | R , y ) over abilities a p of participants p with means  X   X  p and variances  X   X  2 p ; the Gaussian den-sity p ( d q | R , y ) over difficulties d q of questions q with means  X   X  q and variances  X   X  2 q ; the Bernoulli distribution p ( c pq | R , y ) over correctness c pq of participant p  X  X  re-sponse to question q given by probabilities  X  pq ; the discrete distribution p ( r pq | R , y ) over responses r participant p to question q , which assign a proba-bility  X  pqr  X  [0 , 1] to each of the possible responses r  X  R q ; the Gamma distribution p (  X  q | R , y ) over the precision/discrimination parameter  X  q , with scale pa-rameters  X  q and shape parameters k q .
 Inference in the model is done using approximate message passing (see (Koller &amp; Friedman, 2009)). We used Infer.NET (Minka et al., 2010), a pack-age for probabilistic inference. Specifically, we used the expectation-propagation (EP) algorithm presented in (Minka, 2001). EP allows us to calculate marginal distributions of interest on a given factor graph by it-eratively calculating messages along edges that propa-gate information across the factor graph. In our case, EP provides only an approximation to the exact solu-tion because a) the underlying factor graph is loopy , and b) the messages at the junction between c pq , t pq , and  X  q are approximations, and so are the messages go-ing in and out of the gate connected to c pq . Thus EP is run iteratively until convergence, so its running time is linear in the input size (variables and observations). 3.3. Active Learning and Adaptive Testing Having a joint probabilistic model of the data has a number of advantages, including the ability to query different distributions of interest and to handle missing data in a principled way. In addition, maintaining in-formation about the uncertainty present in the model allows us to reason about the impact of future obser-vations on the model uncertainty. This idea forms the basis of active learning , a variant of which is known in the psychometrics literature as adaptive testing . Often there is a considerable cost associated with ob-taining additional data points, so one can use the model of the data to determine which measurements to take next so as to improve the inferred knowledge ac-cording to a pre-determined criterion. In the absence of problem specific information, a reasonable goal is re-ducing uncertainty in estimates of model parameters as measured by the entropy of the posterior distribu-tions, an idea put forward in (MacKay, 1992).
 Suppose we have determined a set of model parameters of interest, denoted here as a vector w . Our goal is to find a criterion by which to decide which response r pq to elicit in order to maximally reduce the posterior entropy S ( w ) of those parameters defined as: where m ( w ) is an arbitrary base measure which does not influence the outcome (see (MacKay, 1992)). We consider two posterior distributions, p m ( w ) := p ( w | R , y ) before inclusion of the new data point, and p m +1 ( w ) := p ( w | R , y ,r pq ) after inclusion of data point r pq . We then aim at maximizing the en-tropy reduction  X  S ( r pq ) := S ( p m ( w ))  X  S ( p m +1 over the choice of response r pq to elicit. Since the actual response r pq is unknown, this choice can only be guided by the expected entropy reduction E over the predictions of the model before inclusion of the new data point, i.e., based on the predictive dis-tribution p m ( r pq | R , y ) obtained by message passing. In its full generality, this active learning scheme can guide the full observation/measurement process in-cluding all possible responses r pq and ground truth answers y q . However, here we focus on the case of adaptive testing, where all the ground-truth answers y q are available, and where the goal is to determine the ability of a participant p as accurately as possi-ble, using as few questions as possible. In this spe-cial case, the parameter vector w only includes the ability a p of participant p . The posterior distribution p m ( a p ) before inclusion of the new observation is Nor-mal, p m ( a p ) := Normal( a p ;  X  p.m , X  2 p,m ), and so is the posterior distribution after inclusion, p m +1 ( a p | r pq a univariate Gaussian with parameters  X  and  X  2 is ln(2  X e X  2 ), so the entropy reduction  X  S ( r pq ) is: Thus the response minimizing posterior variance is preferred. Given participant p , for each possible ques-tion q the expectation E p lated by examining the following quantities for all pos-sible responses r pq  X  R q : a) their probabilities  X  pq , and b) the resulting posterior variances  X  2 p,m +1 ( r pq ) in the updated model. From these we compute the expected entropy reduction for each question q : We then pick the question q  X  that reduces the expected entropy the most. We empirically tested the DARE model discussed in Section 3.1 using a dataset of responses to a standard intelligence test, called Raven X  X  Standard Progressive Matrices (SPM) (Raven), which falls within the cat-egory of multiple choice domains. It consists of sixty questions, each of which consists of a matrix of shapes with one element missing and eight possible answers. Each answer is a possible shape that completes the ma-trix, but only one answer is correct. A sample item, similar 1 to those in SPM is shown in Figure 2. SPM is one of the most popular intelligence tests, and was used both for research and clinical purposes.
 The sample consisted of 120 individuals who filled SPM for its standardization in the British market in 2006 (Raven). The mean number of correct responses, called  X  X aw score X , was 99 . 57 (STD=14 . 16). 4.1. Unobserved Correct Answers First, we investigate the DARE model X  X  ability to han-dle missing correct answers y q . In this case the model allows us to compute the probability p ( y q | R , y ) that a given answer y q is correct. To minimize the probabil-ity of error we select the mode of that distribution as the model X  X  answer for that question. When provided with the responses of all 120 participants the DARE model correctly infers the correct responses for 46 of the questions. Note, that the number of errors is not too surprising because some items in the test were very difficult and few participants answered them correctly. The highest raw score was fifty so even the top scoring participant answered ten items incorrectly.
 We can calculate a participant X  X  raw IQ score with re-spect to the true correct answers y  X  q or with respect to the predicted  X  X orrect X  answers  X  y q , and we refer to the latter score as the model raw score . In crowdsourcing situations when the correct answer for each question is unknown, one can use the model raw scores as an esti-mate of participants X  abilities. Figure 3 shows a scatter plot, in which each point represents a participant; the position on the x-axis represents the participant X  X  raw IQ score, and the position along the y-axis their model raw score. As Figure 3 indicates, there is a very strong correlation between the true raw IQ scores and model raw scores ( R 2 = 0 . 7243), and the difference between the two scores is rather small across all participants. One can think of DARE as an aggregator that re-ceives the responses of a crowd of participants, and outputs the inferred answer for each question. Ex-isting work (Lyle, 2008; Bachrach et al., 2012) tests simpler aggregators using IQ test data. The former uses majority voting, and the latter does consider the ability levels of participants but assumes all items to be of equal difficulty. Another possible simplifying as-sumption, not examined in this earlier work, is that all the participants have equal ability. In contrast, in the DARE model, the probability of a participant to know the correct answer depends both on the difficulty d q of the question and the ability a p of the participant, with the above scenarios as special cases.
 We refer to the model with different question difficul-ties as the question model , and the model with different participant abilities as the participant model . We ex-amine how such simplifications affect the model X  X  abil-ity to infer correct answers as a function of the amount of available data. Figure 4 shows how well the ques-tion, participant and DARE models perform in this regard. For any given crowd size, shown on the x-axis, we randomly selected 10 , 000 subsets of participants of that size. For each such crowd we inferred the cor-rect answer  X  y q to each question using the model, and used the number of questions for which the inferred answer  X  y q was equal to the true correct answer y  X  q as a measure of the model X  X  performance. The y-axis is the quality of each model, averaged over the 10 , 000 sam-pled crowds. Figure 4 shows the ability of all models to infer correct answers increases with the amount of data. It also shows that DARE outperforms the sim-pler models. Interestingly, only modeling participants ability of is better than only modeling question diffi-culty (which is equivalent to majority vote).
 Crowdsourcing Data: To examine the appli-cability of our model to crowdsourcing, we tested our model on the TREC 2011 Crowdsourcing Track dataset (Lease &amp; Kazai, 2011), generated by crowd-sourced workers which classified search engine re-sponses for queries (relevant / irrelevant). Each query-document pair is a  X  X uestion as workers must deter-mine if the document is relevant for the query. This dataset is sparse, as most workers only examined few  X  X uestions, and all  X  X uestions have at most 10 answers in total, and includes ground-truth judgements. We isolated the 369  X  X uestions with the most answers (8 per  X  X uestion), and the 84 workers who answered the most  X  X uestions (at least 30 answers each). Our anal-ysis shows that DARE slightly outperforms majority voting on this dataset. Majority voting gets 206 ques-tions correct, while DARE gets 210 correct. We also tested how well DARE estimate participants skills, similarly to Figure 3. Although for this crowdsourcing dataset the resulting scatter plot is quite noisy ( r 2 of 0.79), it is similar to the one in Figure 3. 4.2. Partial Information on Correct Answers We now examine situations where participants are first tested on a  X  X old-set X  of questions for which the cor-rect answer is known. Consider choosing i questions and making the correct answer to these questions ob-servable to the model. This does not reveal the cor-rect answer to the remaining | Q |  X  i questions, but it does allow the model to better estimate the abil-ity levels of the participants, which in turn allows the model to better infer the correct answer to these re-maining items. Figure 5 shows this effect in DARE. The x-axis represents the number of  X  X evealed X  ques-tions and the y-axis represents the proportion of the remaining questions for which the model inferred the right answer. For each number i of  X  X evealed X  items, we sampled 100 , 000 crowds of 20 participants and i revealed questions (uniformly at random), and the lo-cation on the y-axis is the average proportion of the remaining questions for which the model inferred the right answer over this sample. As the figure shows, having a larger  X  X old-set X  increases the model X  X  ability to infer the correct response for the remaining items. 4.3. Adaptive Skill Testing We now show how DARE can be used for adaptive skill testing. Given a budget of b questions to ask, our goal is to infer the participants X  ability levels. We use DARE to estimate a participant X  X  raw IQ score after only observing this participant X  X  responses to a set of  X  X sked X  questions (revealed responses). In a static approach, for each budget b we choose a specific question set used for all the participants, and measure the RMSE in estimated raw IQ across all participants. To choose the best set of questions of size b for the static approach, one must enumerate over all possible b question sets of size b , and use the one minimiz-ing the error. This is intractable when | Q | is large, so we heuristically choose the question set. We selected static question set for a given budget b by choosing questions that equally partition the participant pop-ulation in terms of the fraction of participants who solved the question 2 For example, with a budget b = 2 we selected a question that roughly two thirds of the participants solved correctly and one that roughly one third of the participants solved incorrectly.
 We also implemented the adaptive testing scheme of Section 3.3 and compared it to the baseline static ap-proach. Under the adaptive approach, the next ques-tion to ask depends on the participant X  X  response to earlier questions, so we reveal the participant X  X  re-sponses one at a time. To measure the RMSE for a given budget b , we simulated the adaptive process for each of the participants and averaged the errors across all participants. Figure 6 shows RMSEs for the static and adaptive approaches for different budget levels. It shows the adaptive approach has a smaller error in its inferred ability levels for any given budget. 3 We presented the DARE model for inferring the cor-rect answers, difficulty levels of questions and abil-ity levels of participants in multiple problem domains. Our evaluation of the model shows that joint infer-ence of these quantities is possible to a high level of accuracy and that it is indeed possible to grade a test without knowing the answers. We showed that in our setting modeling participants X  ability levels is more im-portant than questions X  difficulty levels, that including a  X  X old-set X  helps, and that active learning leads to more efficient testing.
 Our approach is subject to several limitations. Our evaluation used an IQ dataset, whereas crowdsourc-ing tasks may exhibit different properties, such as a greater homogeneity in task difficulty levels. Also, we assume that participants answer to the best of their ability, but participants may be selfish agents with varying motives. For a game theoretic treatment of such issues see (DiPalantino &amp; Vojnovic, 2009; Gao et al., 2012).
 Many questions are open for future research. Are there better models for aggregating responses, or mod-els better tailored to other domains? How can one tractably compute the optimal non-adaptive test for a given population? Can we use similar models to infer the ability levels of individuals when only their perfor-mance within the context of a group is known?
