 Trees are fundamental data structures used to represent very different objects such as proteins, HTML documents, or interpretations of natural language utterances. Thus, many areas  X  for example, biology (Vert, 2002; Hashimoto and natural language processing (Collins &amp; Duffy, 2001; Gildea &amp; Jurafsky, 2002; Pradhan et al., 2005; MacCartney et al., 2006)  X  fostered extensive research in methods for learning classifiers that leverage on these data structures. Tree kernels (TK), firstly introduced in (Collins &amp; Duffy, 2001) as specific convolution kernels (Haussler, 1999), are widely used to fully exploit tree structured data when learn-feature spaces have been proposed (see (Shin et al., 2011) tion of their execution time. Kernel machines compute many times TK functions during learning and classifica-tion. The original tree kernel algorithm (Collins &amp; Duffy, 2001), that relies on dynamic programming techniques, has a quadratic time and space complexity with respect to the size of input trees. Execution time and space occupation are still affordable for parse trees of natural language sen-tences that hardly go beyond the hundreds of nodes (Rieck et al., 2010). But these tree kernels hardly scale to large training and application sets.
 As worst-case complexity of TKs is hard to improve, the biggest effort has been devoted in controlling the average execution time of TK algorithms. Three directions have been mainly explored. The first direction is the exploita-tion of some specific characteristics of trees. For example, it is possible to demonstrate that the execution time of the original algorithm becomes linear in average for parse trees of natural language sentences (Moschitti, 2006). Yet, the tree kernel has still to be computed over the full underlying The second explored direction is the reduction of the un-derlying feature space of tree fragments to control the ex-ecution time by approximating the kernel function. The feature selection is done in the learning phase. Then, for in the kernel computation by selecting subtrees headed by specific node labels (Rieck et al., 2010) or the smaller se-lected space is made explicit (Pighin &amp; Moschitti, 2010). In these cases, the beneficial effect is only during the clas-sification and learning is overloaded with feature selection. The third direction exploits dynamic programming on the whole training and application sets of instances (Shin et al., 2011). Kernel functions are reformulated to be computed using partial kernel computations done for other pairs of trees. As any dynamic programming technique, this ap-proach is transferring time complexity in space complexity. In this paper, we propose the distributed tree kernels (DTK) novel method to reduce time and space complexity of tree kernels. The idea is to embed feature spaces of tree frag-ments in low-dimensional spaces, where the computation is approximated but its worst-case complexity is linear with respect to the dimension of the space. As a direct embed-ding is impractical, we propose a recursive algorithm with linear complexity to compute reduced vectors for trees in the low-dimensional space. We formally show that the dot product among reduced vectors approximates the original tree kernel when a vector composition function with spe-cific ideal properties is used. We then propose two approx-imations of the ideal vector composition function and we study their properties. Finally, we empirically investigate the execution time of DTKs and how well these new ker-nels approximate original tree kernels. We show that DTKs are faster, correlate with tree kernels, and obtain a statisti-cally similar performance in two natural language process-ing tasks.
 properties for DTKs. Section 3 introduces the DTKs and proves their properties. Section 4 compares the complex-ity of DTKs with other tree kernels. Section 5 empirically investigates these new kernel algorithms. Finally, section 6 draws some conclusions. 2.1. Notation and Basic Idea Tree kernels (TK) (Collins &amp; Duffy, 2001) have been pro-posed as efficient methods to implicitly compute dot prod-ucts in feature spaces R m of tree fragments. A direct com-putation in these high-dimensional spaces is impractical. Given two trees, T 1 and T 2 in T , tree kernels TK ( T 1 perform weighted counts of the common subtrees  X  . By construction, these counts are the dot products of the vec-tors representing the trees, ~ T 1 and ~ T 2 in R m , i.e.: Vectors ~ T encode trees T as forests of active tree fragments F ( T ) . Each dimension ~ X  i of R m corresponds to a tree frag-ment  X  i . The trivial weighting scheme assigns  X  i = 1 to dimension ~ X  i if tree fragment  X  i is a subtree of the original tree T and  X  i = 0 otherwise. Different weighting schemes are possible and used. Function I , that maps trees in T vectors in R m , is: where I maps tree fragments into related vectors of the standard orthogonal basis of R m , i.e., ~ X  i = I (  X  i ) . To reduce computational complexity of tree kernels, we want to explore the possibility of embedding vectors ~ T  X  R m into smaller vectors for an approximated but faster and explicit computation of these kernel functions. The direct embedding f : R m  X  R d is, in principle, possible with techniques like singular value decomposition or random indexing (Sahlgren, 2005), but it is again impractical due to the huge dimension of R Then, our basic idea is to look for a function b F : T  X  R that directly maps trees T into small vectors these latter distributed trees (DT) in line with Distributed Representations (Hinton et al., 1986). The computation of similarity over distributed trees is the distributed tree ker-nel (DTK): As the two distributed trees are in the low dimensional space R d , the dot product computation, having constant complexity, is extremely efficient. Computation of func-done once for each tree and outside of the learning algo-rithms. We also propose a recursive algorithm with linear complexity to perform this computation. 2.2. Distributed Trees, Distributed Tree Fragments, here examine the properties required of b F so that DTKs are also approximated computations of TKs, i.e.: To derive these properties and describe function b F , we show the relations between the traditional function I : T R m that maps trees into forests of tree fragments, in the tree fragments feature space, I : T  X  R m that maps tree frag-ments into the standard orthogonal basis of R m , the linear embedding function f : R m  X  R d that maps ~ T into a smaller vector b F .
 Equation 2 presents vectors ~ T with respect to the standard orthonormal basis E = { ~e 1 ...~e m } = { ~ X  1 ...~ X  m } of tributed tree
T = f ( ~ T ) = f ( X where each The linear function f works as a sort of approximated ba-sis transformation , mapping vectors ~ X  of the standard ba-sis E into approximated vectors them. As it a distributed tree fragment (DTF). The set of vectors e E = { mal basis of R m embedded in R d . Then, these two prop-erties should hold: Property 1 (Nearly Unit Vectors) A distributed tree frag-ment tor: 1  X  &lt; || Property 2 (Nearly Orthogonal Vectors) Given two differ-ent tree fragments  X  1 and  X  2 , their distributed vectors are nearly orthogonal: if  X  1 6 =  X  2 , then | As vectors idea is that  X  by means of a function b f (  X  ) = f ( I (  X  )) that composes f and I . Using this function to obtain distributed tree frag-ments This latter equation is presented with respect to the active tree fragments forest F ( T ) of T , neglecting vectors where  X  i = 0 . It is easy to show that, if properties 1 and 2 hold for function b f , distributed tree kernels approximate tree ker-nels (see Equation 4). Johnson-Lindenstrauss Lemma (JLL) (Johnson &amp; Linden-strauss, 1984) guarantees that the embedding function f : R m  X  R d exists. It also points out the relation between the desired approximation of Property 2 (Nearly Orthogonal Vectors) and the required dimension d of the target space, for a certain value of dimension m . This relation affects how well DTKs approximate TKs (Equation 4).
 Knowing that f exists, we are presented with the following issues:  X  building a function b f that directly computes the dis- X  showing that distributed trees Once the above issues are solved, we need to empirically show that Equation (4) is satisfied and that computing DTKs is more efficient than computing TKs. These latter points are discussed in the experimental section. 3.1. Computing Distributed Tree Fragments from This section introduces function b f for distributed tree frag-ments and shows that, using an ideal vector composition 1 and 2. The basic blocks needed to represent trees are their nodes. We then start from a set N  X  R d of nearly orthonormal vectors representing nodes. Each node n is mapped to a vector tistically nearly orthonormal, their elements ( domly drawn from a normal distribution N (0 , 1) and they are normalized so that || of Johnson-Lindenstrauss Lemma in (Dasgupta &amp; Gupta, 1999)). Actual node vectors depend on the node labels, so that label.
 Tree structure can be univocally represented in a  X  X lat X  for-mat using a parenthetical notation. For example, the tree in Fig. 1 is represented by the sequence (A (B W1)(C (D W2)(E W3))) . This notation corresponds to a depth-first visit of the tree, augmented with parentheses so that the tree structure is determined as well.
 Replacing the nodes with their corresponding vectors and introducing a vector composition function : R d  X  R d  X  R , the above formulation can be seen as a mathematical expression that defines a representative vector for a whole tree. The example tree would then be represented by vector  X  = ( Then, we formally define function b f (  X  ) , as follows: Definition 1 Let  X  be a tree and N the set of nearly orthog- X  b f ( n ) = ; n if n is a terminal node, where ; n  X  X   X  b f (  X  ) = ( ; n b f (  X  c  X  b f (  X  1 ... X  k ) = ( b f (  X  1 ) b f (  X  2 ... X  k )) if  X  We here introduce the ideal properties of the vector com-position function , such that function b f (  X  i ) has the two desired properties.
 The definition of the ideal composition function follows: Definition 2 The ideal composition function is : R d  X  R d  X  R d such that, given ; a , and a vector vectors in N by applying , the following properties hold: 2.1 Non-commutativity with a very high degree k 1 2.2 Non-associativity: 2.3 Bilinearity:
Approximation Properties 2.4 || 2.5 | 2.6 | The ideal function cannot exist. Property 2.5 can be only statistically valid and never formally as it opens to an infi-nite set of nearly orthogonal vectors. But, this function can be approximated (see Sec. 5.1). Having defined the ideal basic composition function , we can now focus on the two properties needed to have DTFs as a nearly orthonormal basis of R m embedded in R d , i.e., Property 1 and Property 2.
 For property 1 (Nearly Unit Vectors), we need the follow-ing lemma: Lemma 1 Given tree  X  , vector b f (  X  ) has norm equal to 1. This lemma can be easily proven using property 2.4 and knowing that vectors in N are versors.
 For property 2 (Nearly Orthogonal Vectors), we first need to observe that, due to properties 2.1 and 2.2, a tree  X  gener-ates a unique sequence of application of function in b f (  X  ) representing its structure. We can now address the follow-ing lemma: Lemma 2 Given two different trees  X  a and  X  b , the corre-sponding DTFs are nearly orthogonal: | b f (  X  a )  X  b f (  X  Proof The proof is done by induction on the structure of  X  and  X  b .
 Basic step Let  X  a be the single node a . Two cases are possible:  X  the single node b 6 = a . Then, by the properties of vectors in N , | b f (  X  a )  X  b f (  X  b ) | = | ; a  X  2.5, | b f (  X  a )  X  b f (  X  b ) | = | Induction step Case 1 Let  X  a be a tree with root production a  X  a 1 ...a and  X  b be a tree with root production b  X  b 1 ...b h b f (  X  a 1 ... X  a cases: If a 6 = b , | Property 2.6. Else if a = b , then  X  a 1 ... X  a k 6 =  X  b as  X  a 6 =  X  b . Then, as | b f (  X  a 1 ... X  a k )  X  b f (  X  erty 2.6.
 Case 2 Let  X  a be a tree with root production a  X  a 1 ...a and  X  b =  X  b 1 ... X  b h be a sequence of trees. The expected property becomes | b f (  X  a )  X  b f (  X  b ) | = | ( ( b f (  X  b 1 ) b f (  X  b 2 ... X  b 2.6.
 Case 3 Let  X  a =  X  a 1 ... X  a k and  X  b =  X  b 1 ... X  b h be two sequences of trees. The expected property becomes | b f (  X  a )  X  b f (  X  b ) | = | ( b f (  X  a 1 ) b f (  X  a 2 b f (  X  b 2 ... X  b  X  | b f (  X  a )  X  b f (  X  b ) | &lt; by Property 2.6. Else, if  X  | b f (  X  a 2 ... X  a pothesis, | b f (  X  a )  X  b f (  X  b ) | &lt; by Property 2.6. 3.2. Recursive Algorithm for Distributed Trees This section discusses how to efficiently compute DTs. We focus on the space of tree fragments implicitly defined in (Collins &amp; Duffy, 2001). This feature space refers to sub-trees as any subgraph which includes more than one node, with the restriction that entire (not partial) rule productions must be included. We want to show that the related dis-tributed trees can be recursively computed using a dynamic programming algorithm without enumerating the subtrees. We first define the recursive function and then we show that it exactly computes DTs. The structural recursive formulation for the computation of distributed trees where N ( T ) is the node set of tree T and s ( n ) represents the sum of distributed vectors for the subtrees of T rooted in node n . Function s ( n ) is recursively defined as follows:  X  s ( n ) = ~ 0 if n is a terminal node.  X  s ( n ) = ; n ( ; c 1 + As for the classic TK, the decay factor  X  decreases the With dynamic programming, the time complexity of this (where d is the size of the vectors in R d ). The overall theorem we need is the following.
 Theorem 3 Given the ideal vector composition function , the equivalence between equation (5) and equation (6) holds, i.e.: According to (Collins &amp; Duffy, 2001), the contribution of of nodes in  X  . Thus, we consider  X  i = demonstrate Theorem 3 by showing that s ( n ) computes the weighted sum of vectors for the subtrees rooted in n (see Theorem 5).
 Definition 3 Let n be a node of tree T . We define R ( n ) = {  X  |  X  is a subtree of T rooted in n } We need to introduce a simple lemma, whose proof is triv-ial.
 Lemma 4 Let  X  be a tree with root node n . Let c 1 ,...,c be the children of n . Then R ( n ) is the set of all trees  X  ( n, X  1 ,..., X  m ) such that  X  i  X  R ( c i )  X  X  c i } . Now we can show that function s ( n ) computes exactly the weighted sum of the distributed tree fragments for all the subtrees rooted in n .
 Theorem 5 Let n be a node of tree T . Then s ( n ) =
X Proof The theorem is proved by structural induction. Basis. Let n be a terminal node. Then we have R ( n ) =  X  . Thus, by its definition, s ( n ) = ~ 0 = X Step. Let n be a node with children c 1 ,...,c m . The inductive hypothesis is then s ( c i ) = X Applying the inductive hypothesis, the definition of s ( n ) and the property 2.3, we have s ( n ) = = n X = X ... b f (  X  m )
X DTKs have an attractive constant computational complex-ity. We here compare their complexity with respect to the traditional tree kernels (TK) (Collins &amp; Duffy, 2001), the fast tree kernels (FTK) (Moschitti, 2006), the fast tree ker-nels plus feature selection (FTK+FS) (Pighin &amp; Moschitti, 2010), and the approximate tree kernels (ATK) (Rieck et al., 2010). We discussed basic features of these kernels in the introduction.
 Table 4 reports time and space complexity of the kernels in learning and in classification . DTK is clearly competi-tive with respect to other methods, since both complexities are constant, according to the size d of the reduced feature space. In these two phases, kernels are applied many times by the learning algorithms. Then, a constant complexity is extremely important. Clearly, there is a trade-off between the chosen d and the average size of trees n . A comparison among execution times is done applying these algorithms to actual trees (see Section 5.3). In this section we propose two approximations of the ideal composition function , we investigate on their appropri-whether these concrete basic composition functions yield to effective DTKs, and, finally, we evaluate the computa-tion efficiency by comparing average computational exe-cution times of TKs and DTKs. For the following experi-ments, we focus on a reduced space R d with d = 8192 . 5.1. Approximating Ideal Basic Composition Function We consider two possible approximations for the ideal composition function : the shuffled  X  -product and shuf-fled circular convolution . These functions are defined as follows: where:  X  is the element-wise product between vectors and is the circular convolution (as for distributed representa-tions in (Plate, 1995)) between vectors; p 1 and p 2 are two different permutations of the vector elements; and  X  is a normalization scalar parameter, computed as the average norm of the element-wise product of two vectors. Properties 2.1, 2.2, and 2.3 hold by construction. The two permutation functions, p 1 and p 2 , guarantee Prop. 2.1, for a high degree k , and Prop. 2.2. Property 2.3 is inherited from element-wise product  X  and circular convolution . Properties 2.4, 2.5 and 2.6 can only be approximated. Thus, we performed tests to evaluate the appropriateness of the two considered functions.
 Property 2.4 approximately holds for since approximate norm preservation already holds for circular convolution, whereas uses factor  X  to preserve norm. We empiri-cally evaluated this property. Figure 2(a) shows the average norm for the composition of an increasing number of basic vectors (i.e. vectors with unitary norm) with the two basic composition functions. Function behaves much better than .
 Properties 2.5 and 2.6 were tested by measuring similari-ties between some combinations of vectors. The first ex-periment compared a single vector of several other vectors, as in property 2.5. Both functions resulted in average similarities below 1%, independently of the number of vectors in test property 2.6 we compared two compositions of vectors a except for the first one. The average similarity fluctuates around 0, with performing better than ; this is mostly notable observing that the variance grows with the number of vectors in performed, with all the vectors in common except for the last one, yielding to similar results. In light of these results, seems to be a better choice than , although it should be noted that, for vectors of di-mension d , is computed in O ( d ) time, while takes O ( d log d ) time. 5.2. Evaluating Distributed Tree Kernels: Direct and In this section, we evaluate whether DTKs with the two concrete composition functions, DTK and DTK , ap-proximate the original TK (as in Equation 4).We perform two sets of experiments: (1) a direct comparison where we directly investigate the correlation between DTK and TK values; and, (2) a task based comparison where we com-pare the performance of DTK against that of TK on two natural language processing tasks, i.e., question classifica-tion (QC) and textual entailment recognition (RTE). For the experiments, we used standard datasets for the two NLP tasks of QC and RTE.
 For QC, we used a standard question classification train-ing and test set 2 , where the test set are the 500 TREC 2001 test questions. To measure the task performance, we used a question multi-classifier by combining n binary SVMs ac-cording to the ONE-vs-ALL scheme, where the final output class is the one associated with the most probable predic-tion.
 For RTE we considered the corpora ranging from the first challenge to the fifth (Dagan et al., 2006), except for the fourth, which has no training set. These sets are re-ferred to as RTE1-5. The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800, 800/800, ditional task of pair-based entailment recognition, where a pair of text-hypothesis p = ( t,h ) is assigned a pos-itive or negative entailment class. For our comparative analysis, we use the syntax-based approach described in (Moschitti &amp; Zanzotto, 2007) with two kernel function schemes: (1) PK S ( p 1 ,p 2 ) = K S ( t 1 ,t 2 ) + K S ( h and, (2) PK S + Lex ( p 1 ,p 2 ) = Lex ( t 1 ,h 1 ) Lex ( t K ture between the text and the hypothesis and K S is realized with TK , DTK , and DTK . In the plots, the different PK S kernels are referred to as TK , DTK , and DTK whereas the different PK S + Lex kernels are referred to as TK + Lex , DTK + Lex , and DTK + Lex . classic TK, we considered the Spearman X  X  correlation of their values computed on the parse trees for the sentences contained in QC and RTE corpora. Table 2 reports results and shows that DTK does not approximate adequately TK for  X  = 1 . This highlights the difficulty of DTKs to cor-rectly handle pairs of large active forests , i.e., trees with many subtrees with weights around 1. The correlation improves dramatically when parameter  X  is reduced. We can conclude that DTKs efficiently approximate TK for the  X   X  0 . 6 . These values are relevant for the applications as we will also see in the next section.
Accuracy
Accuracy We performed both QC and RTE experiments for different values of parameter  X  . Results are shown in Fig. 3 and 4 for QC and RTE tasks respectively.
 For QC, DTK leads to worse performances with respect to TK, but the gap is narrower for small values of  X   X  0 . 4 (with DTK better than DTK ). These  X  values produce DTK and DTK is similar to TK . Differences are not statistically significant except for for  X  = 0 . 4 where DTK behaves better than TK (with p &lt; 0 . 1 ). Statis-tical significance is computed using the two-sample Stu-dent t-test. DTK + Lex and DTK + Lex are statisti-cally similar to TK + Lex for any value of  X  . DTKs are a good approximation of TKs for  X   X  0 . 4 , that are the values where TK s have the best performances in the tasks. 5.3. Average computation time Computation time (ms) We measured the average computation time of FTK (Mos-chitti, 2006) and DTK (with vector size 8192) on trees from the Question Classification corpus. Figure 5 shows the rela-tion between the computation time and the size of the trees, computed as the total number of nodes in the two trees. As expected, DTK has constant computation time, since it is computation time for FTK, while being lower for smaller trees, grows very quickly with the tree size. The larger are the trees considered, the higher is the computational advan-tage offered by using DTK instead of FTK. In this paper we proposed the distributed tree kernels (DTKs) as an approach to reduce computational com-plexity of tree kernels. Having an ideal function for vector composition, we have formally shown that high-dimensional spaces of tree fragments can be embedded in low-dimensional spaces where tree kernels can be directly computed with dot products. We have empirically shown that we can approximate the ideal function for vector com-position. The resulting DTKs correlate with original tree kernels, obtain similar results in two natural language pro-cessing tasks, and, finally, are faster.

