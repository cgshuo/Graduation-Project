 Suppose that a medical center has decided to use machine lear ning techniques to induce a diagnostic tool from records of previous patients. The center aims to ob tain a comprehensible model, with low expected test costs (the costs of testing attribute values) and high expected ac curacy. Moreover, in many cases there are costs associated with the predictive er rors. In such a scenario, the task of the inducer is to produce a model with low expected test costs and low expected misclassification costs . A good candidate for achieving the goals of comprehensibili ty and reduced costs is a decision tree model. Decision trees are easily interpretable becaus e they mimic the way doctors think [13][chap. 9]. In the context of cost-sensitive classificat ion, decision trees are the natural form of representation: they ask only for the values of the featur es along a single path from the root to a leaf. Indeed, cost-sensitive trees have been the subject o f many research efforts. Several works proposed learners that consider different misclassificati on costs [7, 18, 6, 9, 10, 14, 1]. These meth-ods, however, do not consider test costs. Other authors desi gned tree learners that take into account test costs, such as IDX [16], CSID3 [22], and EG2 [17]. These m ethods, however, do not consider misclassification costs. The medical center scenario exemp lifies the need for considering both types of cost together: doctors do not perform a test before consid ering both its cost and its importance to the diagnosis.
 Minimal Cost trees, a method that attempts to minimize both t ypes of costs simultaneously has been proposed in [21]. A tree is built top-down. The immediate red uction in total cost each split results in is estimated, and a split with the maximal reduction is sel ected. Although efficient, the Minimal Cost approach can be trapped into a local minimum and produce trees that are not globally optimal. For example, consider a problem with 10 attributes a The cost of a of misclassification. This may hide their usefulness, and mi slead the learner to fit a large expensive tree. The problem is intensified if a gain (e.g., a local measures would fail in recognizing the relevance of a preferred. The Minimal Cost method is appealing when resour ces are very limited. However, it requires a fixed runtime and cannot exploit additional resou rces. In many real-life applications, we are willing to wait longer if a better tree can be induced. For example, due to the importance of the model, the medical center is ready to allocate 1 week to learn it. Algorithms that can exploit more time to produce solutions of better quality are called anyti me algorithms [5].
 One way to exploit additional time when searching for a tree o f lower costs is to widen the search space. In [2] the cost-sensitive learning problem is formul ated as a Markov Decision Process (MDP) and a systematic search is used to solve the MDP. Although the algorithm searches for an optimal strategy, the time and memory limits prevent it from always fi nding optimal solutions.
 The ICET algorithm [24] was a pioneer in searching non-greed ily for a tree that minimizes both costs together. ICET uses genetic search to produce a new set of costs that reflects both the original costs and the contribution each attribute can make to reduce misclassification costs. Then it builds a tree using the greedy EG2 algorithm but with the evolved cos ts instead of the original ones. ICET was shown to produce trees of lower total cost . It can use additional time resources to produce more generations and hence to widen its search in the space of cost s. Nevertheless, it is limited in the way it can exploit extra time. Firstly, it builds the final tre e using EG2. EG2 prefers attributes with high information gain (and low test cost). Therefore, when t he concept to learn hides interdepen-dency between attributes, the greedy measure may underesti mate the usefulness of highly relevant attributes, resulting in more expensive trees. Secondly, e ven if ICET may overcome the above prob-lem by reweighting the attributes, it searches the space of p arameters globally, regardless of the context. This imposes a problem if an attribute is important in one subtree but useless in another. To illustrate the above consider the concept in Figure 1 (right ). There are 10 attributes of similar costs. Depending on the value of a all attributes will have a low gain. Because ICET assigns cos ts globally, they will have similar costs as well. Therefore, ICET will not be able to recognize which a ttribute is relevant in what context. Recently, we have introduced LSID3, a cost-insensitive alg orithm, which can induce more accurate trees when given more time [11]. The algorithm uses stochast ic sampling techniques to evaluate candidate splits. It is not designed, however, to minimize t est and misclassification costs. In this work we build on LSID3 and propose ACT , an Anytime Cost-sensitive Tree learner that can exploit additional time to produce trees of lower costs. Applying th e sampling mechanism to the cost-sensitive setup, however, is not trivial and imposes severa l challenges which we address in Section 2. Extensive set of experiments that compares ACT to EG2 and t o ICET is reported in Section 3. The results show that ACT is significantly better for the majorit y of problems. In addition ACT is shown to exhibit good anytime behavior with diminishing returns. The major contributions of this paper are: (1) a non-greedy algorithm for learning trees of lower c osts that allows handling complex cost structures, (2) an anytime framework that allows learning t ime to be traded for reduced classification costs, and (3) a parameterized method for automatic assigni ng of costs for existing datasets. Note that costs may also be involved during example acquisit ion [12, 15]. In this work, however, we assume that the full training examples are in hand. Moreov er, we assume that during the test phase, all tests in the relevant path will be taken. Several t est strategies that determine which values to query for and at what order have been recently studied [21] . These strategies are orthogonal to our work because they assume a given tree. Offline concept learning consists of two stages: learning fr om labelled examples; and using the induced model to classify unlabelled instances. These two s tages involve different types of cost [23]. Our primary goal in this work is to trade the learning ti me for reduced test and misclassification costs. To make the problem well defined, we need to specify how to: (1) represent misclassification costs, (2) calculate test costs, and (3) combine both types o f cost.
 To answer these questions, we adopt the model described by Tu rney [24]. In a problem with | C | different classes, a classification cost matrix M is a | C | X | C | matrix whose M penalty of assigning the class c the test costs of a particular case, we sum the cost of the test s along the path from the root to the appropriate leaf. For tests that appear several times we cha rge only for the first occurrence. The model handles two special test types, namely grouped and delayed . Grouped tests share a common cost that is charged only once per group. Each test also has an extra cost charged when the test is actually made. For example, consider a tree path with tests l ike cholesterol level and glucose level. For both values to be measured, a blood test is needed. Clearl y, once blood samples are taken to measure the cholesterol level, the cost for measuring the gl ucose level is lower. Delayed tests are tests whose outcome cannot be obtained immediately, e.g., l ab test results. Such tests force us to wait until the outcome is available. Alternatively, we can t ake into account all possible outcomes and follow several paths in the tree simultaneously (and pay for their costs). Once the result of the delayed test is available, the prediction is in hand. Note th at we might be charged for tests that we would not perform if the outcome of the delayed tests were ava ilable. In this work we do not handle delayed costs but we do explain how to adapt our framework to s cenarios that involve them. Having measured the test costs and misclassification costs, an important question is how to combine them. Following [24] we assume that both types of cost are giv en in the same scale. Alternatively, Qin et. al. [19] presented a method to handle the two kinds of c ost scales by setting a maximal budget for one kind and minimizing the other.
 ACT, our proposed anytime framework for induction of cost-s ensitive trees, builds on the recently introduced LSID3 algorithm [11]. LSID3 adopts the general t op-down induction of decision trees scheme (TDIDT): it starts from the entire set of training exa mples, partitions it into subsets by testing the value of an attribute, and then recursively builds subtr ees. Unlike greedy inducers, LSID3 invests more time resources for making better split decisions. For e very candidate split, LSID3 attempts to estimate the size of the resulting subtree were the split to t ake place and following Occam X  X  razor [4] it favors the one with the smallest expected size. The est imation is based on a biased sample of the space of trees rooted at the evaluated attribute. The s ample is obtained using a stochastic version of ID3, called SID3 [11]. In SID3, rather than choosi ng an attribute that maximizes the information gain  X  I (as in ID3), the splitting attribute is chosen semi-randoml y. The likelihood that an attribute will be chosen is proportional to its informati on gain. LSID3 is a contract algorithm parameterized by r , the sample size. When r is larger, the resulting estimations are expected to be more accurate, therefore improving the final tree. Let m = | E | be the number of examples and n = | A | be the number of attributes. The runtime complexity of LSID3 is O ( rmn 3 ) [11]. LSID3 was shown to exhibit a good anytime behavior with diminishin g returns. When applied to hard concepts, it produced significantly better trees than ID3 an d C4.5. ACT takes the same sampling approach as in LSID3. However, three major components of LSI D3 need to be replaced for the cost-sensitive setup: (1) sampling the space of trees, (2) e valuating a tree, and (3) pruning. Obtaining the Sample. LISD3 uses SID3 to bias the samples towards small trees. In AC T, however, we would like to bias our sample towards low cost trees. For th is purpose, we designed a stochastic version of the EG2 algorithm, that attempts to build low cost trees greedily. In EG2, a tree is built top-down, and the attribute that maximizes ICF (Informatio n Cost Function) is chosen for splitting a node, where, ICF ( a ) = 2  X  I ( a )  X  1 / (( cost ( a ) + 1) w ) .
 In Stochastic EG2 (SEG2), we choose splitting attributes se mi-randomly, proportionally to their ICF. Due to the stochastic nature of SEG2 we expect to be able to esc ape local minima for at least some of the trees in the sample. To obtain a sample of size r , ACT uses EG2 once and SEG2 r  X  1 times. Unlike ICET, we give EG2 and SEG2 a direct access to context-b ased costs, i.e., if an attribute has already been tested its cost would be zero and if another attr ibute that belongs to the same group has been tested, a group discount is applied. The parameter w controls the bias towards lower cost attributes. While ICET tunes this parameter using genetic s earch, we set w inverse proportionally to the misclassification cost: a high misclassification cost re sults in a smaller w , reducing the effect of attribute costs. One direction for future work would be to tu ne w a priori.
 Evaluating a Subtree. As a cost insensitive learner, the main goal of LSID3 is to max imize the expected accuracy of the learned tree. Following Occam X  X  ra zor, it uses the tree size as a preference bias and favors splits that are expected to reduce the final tr ee size. In a cost-sensitive setup, our goal is to minimize the expected cost of classification. Followin g the same lookahead strategy as LSID3, we sample the space of trees under each candidate split. Howe ver, instead of choosing an attribute that minimizes the size, we would like to choose one that mini mizes costs. Therefore, given a tree, we need to come up with a procedure that estimates the expecte d costs when classifying a future case. This cost consists of two components: the test cost and misclassification cost.
 Assuming that the distribution of future cases would be simi lar to that of the learning examples, we can estimate the test costs using the training data. Given a t ree, we calculate the average test cost of the training examples and use it to approximate the test co st of new cases. For a tree T and a set of training examples E , we denote the average cost of traversing T for an example from E (average testing cost) by tst-cost ( T, E ) . Note that group discounts and delayed cost penalties do not need a special care because they will be incorporated when calcula ting the average test costs. Estimating the cost of errors is not obvious. One can no longe r use the tree size as a heuristic for pre-dictive errors. Occam X  X  razor allows to compare two consist ent trees but does not provide a mean to estimate accuracy. Moreover, tree size is measured in a diff erent currency than accuracy and hence cannot be easily incorporated in the cost function. Instead , we propose using a different estimator: the expected error [20]. For a leaf with m training examples, of which e are misclassified the ex-pected error is defined as the upper limit on the probability f or error, i.e., EE ( m, e, cf ) = U where cf is the confidence level and U is the confidence interval for binomial distribution. The ex -pected error of a tree is the sum of the expected errors in its l eafs. Originally, the expected error was used by C4.5 to predict whether a subtree performs better tha n a leaf. Although it lacks theoretical basis, it was shown experimentally to be a good heuristic. In ACT we use the expected error to approximate the misclassification cost. Assume a problem wi th | C | classes and a misclassification cost matrix M . Let c be the class label in a leaf l . Let m be the total number of examples in l and m (the right most expression assumes uniform misclassificati on cost M The expected error of a tree is the sum of the expected errors i n its leafs. In our experiments we use cf = 0 . 25 , as in C4.5. In the future, we intend to tune cf if the allocated time allows. Alternatively, we also plan to estimate the error using a set-aside validati on set, when the training set size allows. To conclude, let E be the set of examples used to learn a tree T , and let m be the size of E . Let L be the set of leafs in T . The expected total cost of T when classifying an instance is: Having decided about the sampler and the tree utility functi on we are ready to formalize the tree growing phase in ACT. A tree is built top-down. The procedure for selecting splitting test at each node is listed in Figure 2 (left), and exemplified in Figure 2 ( right). The selection procedure, as formalized is Figure 2 (left) needs to be slightly modified wh en an attribute is numeric: instead of iterating over the values the attribute can take, we exami ne r cutting points, each is evaluated with a single invocation of EG2. This guarantees that numeri c and nominal attributes get the same resources. The r points are chosen dynamically, according to their informat ion gain.
 Costs-sensitive Pruning. Pruning plays an important role in decision tree induction. In cost-insensitive environments, the main goal of pruning is to sim plify the tree in order to avoid overfitting. A subtree is pruned if the resulting tree is expected to yield a lower error. When test costs are taken into account, pruning has another important role: reducing costs. It is worthwhile to keep a subtree only if its expected reduction to the misclassification cost is larger that the cost of its tests. If the misclassification cost was zero, it makes no sense to keep any split in the tree. If, on the other hand, Procedure ACT-C HOOSE -A TTRIBUTE ( E, A, r ) the misclassification cost was very large, we would expect si milar behavior to the cost-insensitive setup. To handle this challenge, we propose a novel approach for cost-sensitive pruning. Similarly to error-based pruning [20], we scan the tree bottom-up. For each subtree, we compare its expected total cost to that of a leaf. Formally, assume that e examples in E do not belong to the default class. 1 We prune a subtree T into a leaf if: A variety of experiments were conducted to test the performa nce and behavior of ACT. First we describe and motivate our experimental methodology. We the n present and discuss our results. 3.1 Methodology We start our experimental evaluation by comparing ACT, give n a fixed resource allocation, with EG2 and ICET. EG2 was selected as a representative for greedy learners. We also tested the per-formance of CSID3 and IDX but found the results very similar t o EG2, confirming the report in [24]. Our second set of experiments compares the anytime beh avior of ACT to that of ICET. Be-cause the code of EG2 and ICET is not publicly available we hav e reimplemented them. To verify the reimplementation results, we compared them with those r eported in literature. We followed the same experimental setup and used the same 5 datasets. The res ults are indeed similar with the basic version of ICET achieving an average cost of 49.9 in our reimp lementation vs. 49 in Turney X  X  paper [24]. One possible reason for the slight difference may be th e randomization involved in the genetic search as well as in data partitioning into training, valida ting, and testing sets.
 Datasets. Typically, machine learning researchers use datasets from the UCI repository [3]. Only five UCI datasets, however, have assigned test costs [24]. To gain a wider perspective, we developed an automatic method that assigns costs to existing datasets randomly. The method is parameterized with: (1) cr the cost range, (2) g the number of desired groups as a percentage of the number of attributes, and (3) sc the group shared cost as a percentage of the maximal marginal cost in the and 4 datasets that represent hard concept and have been used in previous research. The online appendix 3 gives detailed descriptions of these datasets. Two version s of each dataset have been created, both with cost range of 1-100. In the first g and sc were set to 20% and in the second they were set to 80% . These parameters were chosen arbitrarily, in attempt to co ver different types of costs. In total we have 55 datasets: 5 with costs assigned a s in [24] and 50 with random costs. Cost-insensitive learning algorithms focus on accuracy an d therefore are expected to perform well Figure 3: Illustration of the differences in performance between ACT and ICET for misclassification costs when testing costs are negligible relative to misclassifica tion costs. On the other hand, when testing costs are significant, ignoring them would result in expensi ve classifiers. Therefore, to evaluate a cost-sensitive learner a wide spectrum of misclassificatio n costs is needed. For each problem out of the 55, we created 4 instances, with uniform misclassificati on costs mc = 10 , 100 , 1000 , 10000 . Normalized Cost. As pointed out by Turney [24], using the average cost is probl ematic because: (1) the differences in costs among the algorithms become sma ll as misclassification cost increases, bine average costs for different misclassification costs. T o overcome these problems, Turney sug-gests to normalize the average cost of classification by divi ding it by the standard cost , defined as ( T C + min i (1  X  f i )  X  max i,j ( M i,j )) , The standard cost is an approximation for the maximal cost in a given problem. It consists of two components: (1) T C , the cost if we take all tests, and (2) the misclassification cost if the classifier achieves only the ba se-line accuracy. f of class i in the data and hence (1  X  f Statistical Significance. For each problem, one 10 fold cross-validation experiment h as been con-ducted. The same partition to train-test sets was used for al l compared algorithms. To test the statistical significance of the differences between ACT and ICET we used two tests. The first is t-test with a  X  = 5% confidence: for each method we counted how many times it was a s ignifi-cant winner. The second is Wilcoxon test [8], which compares classifiers over multiple datasets and states whether one method is significantly better than the ot her (  X  = 5% ). 3.2 Fixed-time Comparison For each of the 55  X  4 problem instances, we run the seeded version of ICET with its default parameters (20 generations), 4 EG2, and ACT with r = 5 . We choose r = 5 so the average runtime of ACT would be shorter than ICET for all problems. EG2 and ICE T use the same post-pruning mechanism as in C4.5. In EG2 the default confidence factor is u sed (0.25) while in ICET this value is tuned using the genetic search.
 Table 1 lists the average results, Figure 3 illustrates the d ifferences between ICET and ACT, and Figure 4 (left) plots the average cost for the different valu es of mc . The full results are available in the online appendix. Similarly to the results reported in [24] ICET is clearly better than EG2, because the latter does not consider misclassification cost s. When mc is set to 10 and to 100 ACT significantly outperforms ICET for most datasets. In these c ases ACT was able to produce very small trees, sometimes consist of one node, neglecting the a ccuracy of the learned model. For mc set to 1000 and 10000 there are fewer significant wins, yet it i s clear that ACT is dominating: the number of ACT wins is higher and the average results indicate that ACT trees are cheaper. The Wilcoxon test, states that for mc = 10 , 100 , 10000 , ACT is significantly better than ICET, and that for mc = 1000 no significant winner was found.
 When misclassification costs are low, an optimal algorithm w ould produce a very shallow tree. When misclassification costs are dominant, an optimal algor ithm would produce a highly accurate tree. Some concepts, however, are not easily learnable and e ven cost-insensitive algorithms fail to achieve perfect accuracy on them. Hence, with the increas e in the importance of accuracy the normalized cost increases: the predictive errors affect th e cost more dramatically. To learn more about the effect of accuracy, we compared the accuracy of ACT to that of C4.5 and ICET mc values. Figure 4 (mid-left) shows the results. An important property of both ICET and ACT is their ability to compromise on accuracy when needed. ACT X  X  flexibi lity, however, is more noteworthy: from the least accurate method it becomes the most accurate o ne. Interestingly, when accuracy is extremely important both ICET and ACT achieves even better a ccuracy than C4.5. The reason is their non-greedy nature. ICET performs an implicit lookahe ad by reweighting attributes according to their importance. ACT performs lookahead by sampling the space of subtrees under every split. Among the two, the results indicates that ACT X  X  lookahead is more efficient in terms of accuracy. We also compared ACT to LSID3. As expected, ACT was significan tly better for mc  X  1000 . For mc = 10000 their performance was similar. In addition, we compared the studied methods on nonuniform misclassification costs and found ACT X  X  advanta ge to be consistent. 3.3 Anytime Comparison Both ICET and ACT are anytime algorithms that improve their p erformance with time. ICET is expected to exploit extra time by producing more generation s and hence better tuning the parameters for the final invocation of EG2. ACT can use additional time to acquire larger samples and hence achieve better cost estimations. A typical anytime algorit hm would produce improved results with the increase in resources. The improvements diminish with t ime, reaching a stable performance. To examine the anytime behavior of ICET and ACT, we run each of them on 2 problems, namely Breast-cancer-20 and Multi-XOR-80, with exponentially in creasing time allocation. ICET was run results show a good anytime behavior of both ICET and ACT. For both algorithms, it is worthwhile to allocate more time. ACT dominates ICET for both domains an d is able to produce trees of lower costs in shorter time. The Multi-XOR dataset is an example fo r a concept with attributes being important only in one sub-concept. As we expected, ACT outpe rforms ICET significantly because the latter cannot assign context-based costs. Allowing ICE T to produce more and more generations (up to 128) does not result in trees comparable to those obtai ned by ACT. Machine learning techniques are increasingly being used to produce a wide-range of classifiers for real-world applications that involve nonuniform testing c osts and misclassification costs. As the complexity of these applications grows, the management of r esources during the learning and clas-sification processes becomes a challenging task. In this wor k we introduced a novel framework for operating in such environments. Our framework has 4 major ad vantages: (1) it uses a non-greedy approach to build a decision tree and therefore is able to ove rcome local minima problems, (2) it evaluates entire trees and therefore can be adjusted to any c ost scheme that is defined over trees. (3) it exhibits good anytime behavior and produces significantl y better trees when more time is avail-able, and (4) it can be easily parallelized and hence can bene fit from distributed computer power. To evaluate ACT we have designed an extensive set of experime nts with a wide range of costs. The experimental results show that ACT is superior over ICET and EG2. Significance tests found the differences to be statistically strong. ACT also exhibited good anytime behavior: with the increase in time allocation, there was a decrease in the cost of the lea rned models. ACT is a contract anytime algorithm that requires its sample size to be pre-determine d. In the future we intend to convert ACT into an interruptible anytime algorithm, by adopting th e IIDT general framework [11]. In addition, we plan to apply monitoring techniques for optima l scheduling of ACT and to examine other strategies for evaluating subtrees.
 [1] N. Abe, B. Zadrozny, and J. Langford. An iterative method for multi-class cost-sensitive [2] V. Bayer-Zubek and Dietterich. Integrating learning fr om examples into the search for diag-[3] C. L. Blake and C. J. Merz. UCI repository of machine learn ing databases, 1998. [4] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth . Occam X  X  Razor. Information [5] M. Boddy and T. L. Dean. Deliberation scheduling for prob lem solving in time constrained [6] J. Bradford, C. Kunz, R. Kohavi, C. Brunk, and C. Brodley. Pruning decision trees with [7] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees . [8] J. Demsar. Statistical comparisons of classifiers over m ultiple data sets. Journal of Machine [9] P. Domingos. Metacost: A general method for making class ifiers cost-sensitive. In KDD , 1999. [10] C. Elkan. The foundations of cost-sensitive learning. In IJCAI , 2001. [11] S. Esmeir and S. Markovitch. Anytime learning of decisi on trees. Journal of Machine Learning [12] R. Greiner, A. J. Grove, and D. Roth. Learning cost-sens itive active classifiers. Artificial [13] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, [14] D. Margineantu. Active cost-sensitive learning. In IJCAI , 2005. [15] P. Melville, M. Saar-Tsechansky, F. Provost, and R. J. M ooney. Active feature acquisition for [16] S. W. Norton. Generating better decision trees. In IJCAI , 1989. [17] M. Nunez. The use of background knowledge in decision tr ee induction. Machine Learning , [18] F. Provost and B. Buchanan. Inductive policy: The pragm atics of bias selection. Machine [19] Z. Qin, S. Zhang, and C. Zhang. Cost-sensitive decision trees with multiple cost scales. Lecture [20] J. R. Quinlan. C4.5: Programs for Machine Learning . Morgan Kaufmann, 1993. [21] S. Sheng, C. X. Ling, A. Ni, and S. Zhang. Cost-sensitive test strategies. In AAAI , 2006. [22] M. Tan and J. C. Schlimmer. Cost-sensitive concept lear ning of sensor use in approach and [23] P. Turney. Types of cost in inductive concept learning. In Workshop on Cost-Sensitive Learning [24] P. D. Turney. Cost-sensitive classification: Empirica l evaluation of a hybrid genetic decision
