 Real -lif e dat a minin g a pplication s ar e interes tin g becaus e the y ofte n presen t a di fferen t set o f problem s fo r dat a miners . On e pa tient s databases . Valuabl e le sson s ar e learn t fro m thi s app lication . I n par ticular , w e disc ove r tha t th e ofte n neglecte d pre -proce ssin g a nd post -proce ssin g step s i n knowledg e disc over y ar e th e mos t critical element s i n determinin g th e su cce ss o f a ho w w e ca rry ou t knowledg e disc over y o n thi s diabe tic pa tien t database , th e interes tin g i ssue s tha t hav e surfaced , a s we ll a s th e le sson s w e have learn t fro m thi s a pplication . W e w ill describ e a semi -automa tic m ean s fo r cl eanin g th e diabe tic p atien t database , and presen t a step -by -ste p approach t o hel p th e hea lth doctor s explor e thei r dat a a nd t o understa nd th e disc overe d rule s be tter. In Singapore , abou t 10 percen t o f th e po pula tio n is diabe tic . Thi s dis eas e ha s man y sid e e ffect s suc h a s highe r ris k o f ey e disease , highe r ris k o f ki dne y fa ilure , a nd othe r comp lications . However , earl y detec tio n o f th e disease a nd prope r car e managemen t ca n mak e a di fference . T o comba t thi s dis ease , Singapor e intro duce d a regula r scr eenin g program fo r th e diabe tic pa tient s i n 1992 . Pa tien t information , c linica l symptoms , eye -dis eas e diagnosi s and tr eatment s ar e capture d int o a database . Afte r eigh t y ear s o f dat a co llection , a whole wea lth o f informa tio n ha s b een gathered . Thi s l ead s natura lly t o th e a pplicatio n o f knowledg e disc over y and dat a minin g tec hnique s t o discove r interes tin g pa ttern s tha t understa nd mor e abou t th e diabe tic dis eas e o r t o fi nd ou t somethin g specia l abou t a par ticula r pa tien t po pula tion . A lthoug h knowledg e disc over y i n database s has reporte d man y su cce sse s i n domain s suc h a s fra ud detec tion , targete d marke tin g etc ., w e fo und tha t th e a pplication o f dat a minin g tec hnique s t o h ealth secto r ha s b een rela tivel y fe w i n comparison . W e be liev e thi s i s primar ily due t o tw o r easons . First, th e data capture d b y h ealth clinic s ar e typica lly very noisy . Man y o f th e pa tien t record s contai n typ ogra phica l e rror s, mi ssin g values , o r wron g informa tio n such a s str eet name s o r dat e o f birt h etc ; a nd worse , man y record s ar e i n fac t dup licat e records . Cl eanin g thes e dat a take s treme ndou s amo unt o f effor t a nd time . I n add ition , man y o f mining . The y n eed t o b e transforme d t o mor e meaningfu l attri bute s befor e minin g ca n proc eed . Seco nd, th e hea lth doctor s ar e usua lly t oo bus y s eein g pa tient s eac h day . The y ca nno t a ffor d th e tim e o r th e energ y t o siev e thro ug h th e thousa nds o f rule s generate d b y som e state -of -the -ar t minin g tec hnique s o n th e diabe tic pa tien t database . T hus , it i s importan t t o presen t th e disc overe d rule s i n som e easy -to -understa nd fashion . In thi s paper , w e w ill demonstrat e ho w w e a ddre ss thes e concerns . T o overcom e the proble m o f nois y data , w e hav e develope d a semi -automa tic dat a cleanin g system . Th e syste m reconc ile s databas e forma t di fference s b y a llowin g th e doctor s t o specif y th e ma ppin g betw een a ttri bute s i n di fferen t format style s a nd th e encodin g scheme s used . Onc e the forma t di fference s hav e bee n reconc iled , th e proble m o f iden tifyin g a nd rem ovin g dup licat e record s is a ddre ssed . T o resolv e the proble m o f t oo man y rule s generate d b y th e state -of -the -ar t minin g tec hniques , we a ppl y a user -oriente d approach tha t pr ovide s step -by -ste p explora tio n o f th e data i n orde r t o be tte r understa nd th e disc overe d pa tterns. Th e medica l domai n o ffer s a fer tile gro und fo r dat a minin g app lications . However, the y hav e bee n fe w medica l da ta minin g app lication s as compare d t o othe r domains . [ 14] reporte d thei r experienc e i n tryin g to automa ticall y ac quir e medica l knowledg e fro m c linica l databases . The y di d som e e xperiment s o n thr ee medica l database s a nd th e rule s i nduce d ar e use d t o compare ther e i s n o direc t in volvemen t o f th e medica l doctors . [ 11] focuse d on th e need s t o generat e rule s tha t doe s no t violat e th e prio r knowledg e o f th e domai n experts . Th e tec hniqu e wa s app lie d to a Alzheime r X  X  dis eas e database . Thes e a pplication s focu s o n th e genera tio n o f understandabl e rules . Wh ile it i s importan t to generat e understandabl e rules , it i s als o importan t to th e medica l doctor s t o hav e a complet e picture o f a ll the rules that ex ist i n the database. This leads naturally to association rule mining. The problem with association rules is, however, that t here a re often too many of them, and they also contain a large a mount of redund ancy [8]. Past research in dealing with this problem can b e described with the following approaches: (a) Discover all rules first and then allow the user to query and (b) Use constraints to constrain the mining process to g enerate (c) Find un expected rules. This approach first asks the user to The first two approaches did n ot work well for our application becau se the doctors often d o not know w hat they were looking for and thus could n ot give templates. The third approach works to a limited extend b ecause the doctors are unable to specify too many existing beliefs. We noticed that t he doctors are quite unwilli ng to specify any existing knowledge or r equirement because it i s a mental burden to them. All t he previous work in medical domain focus on the rule generation ph ase itself. In our experiences, we find that t he process of reaching the stage where und erstan dable rules can b e generated and the process after the rules have been generated are, equally if not, more important than the rule generation stage itself. More e mphasis needs to be given to the pre -processing stage a nd the post -processing stage. We e mbarked on the mining o f diabetic patient database project in early 1999 . We were given the diabetic patient database which contains about 200 ,000 screening records captured from 1992 -1996 . Each record h as 60 fields. A preliminar y analysis of the database reveals patient details s uch as their identification number 1 , race, sex, date of birth, duration of diabetes, and the date of screening. Each screening involves an eye e xamination to detect signs of diabetes -associated eye diseas es s uch as retinopathy, maculapathy and age -related eye diseases s uch as cataract and glycoma. The outcome of the e ye e xamination is also recorded in the screening record. Given the "garbage in, garbage out" principle, it i s crucial for us to clean the d iabetic patient database first before mining can proceed. A few observations were made in the cleaning process: 1 Every S ingaporean h as a unique identity card nu mber, which is similar to the social security number in the United States. (a) In real -life, due to the rapid p ace of software a nd h ardware (b) With each format change, inconsistencies creep in such as (c) Many attribute fields are left blank , particularly in medical To tackle these problems, we implemented a data cleaning system. This s ystem allows a user to define mapp ings between attributes in d ifferent formats, the e ncoding schemes used, and whether the a ttributes are to be kept i n the cleaned d atabase. With this s pecification, a final stand ardized format schema is generated. Based on this s tandardized format schema, each of the database files are transformed accordingly into this s tandardized format. However, one major problem persists in the data a fter this s tandardization step  X  dup licate records. Having two o r more records referrin g to a single screening session of a patient not only contributes to the problem of handling ever -increasing amount of data, but also leads to the mining o f inconsistent or inaccurate information that is obviously und esirable. Note that a patient can h ave more than one screening at different dates. A Sorted Neighbourhood Method (SNM) is proposed to remove dup licate records [2]. The major steps are a s follows: Step 1 . Choose one or more fields that uniquely identify each Step 2 . Sor t the database a ccording to the chosen fields. Step 3 . Compare the chosen fields of the records within a sliding Step 4 . User verifies the dup licates detected and true dup licates Additional t echniques are e mployed to pre -process the data records before sorting them so as to increase the likelihood of the potentially dup licate records being brought t o a close neighbourhood. These techn iques includ e scrubb ing data fields using external source files to remove typograph ical errors and the use of abbreviations, tokenizing data fields and then sorting the tokens in the data fields. For example, supp ose Record 1 and Record 2 are dup licates containing the same data in all t he fields e xcept for the Name field which is { Tan Lay -Hoon } and { Lay -Hoon Tan } respectively. If the Name field is used to sort t he database, then Record 1 and 2 will become very far apart; hence Record 1 and 2 will not be detected as dup licates. Sorting the tokens in the Name field will cause the Name field of Record 1 and 2 to become { Lay -Hoon Tan }. Utili zing these techn iqu es in our data cleaning system, we must first choose a n application -specific key to sort the database before the removal of dup licate records. Thi s key is a sequ ence of a subset of attributes, or sub -strings within the a ttributes, which h as s ufficient discriminating power in identifying likely cand idates for matching. There is no rule specifying how the key should b e designed bu t it i s important t ha t users s hould choose the fields that contain representative information of the record to sort the database so that "potentially matching" records will be moved to within a close neighborhood. In the diabetic patient database, we judiciously choose a set of attributes that will uniquely identify a patient's s creening record. This key consists of the patient X  X  identification nu mber ( NRIC) and the date of screening (DO S). The NRIC number is unique to all Singaporeans or Singapore permanent residents. It is similar to the social security number of the United States of America. We pre -process these key fields with a reliable e xternal source file containing the NRIC and n ame of persons. This is to remove any typographical errors in the NRIC or name fields in th e database. The database is then sorted b ased on this key. Next, pair wise comparison of nearby records is carried out. Figure 1 shows how a fixed -size sliding window is employed to limit t he scope of record comparisons. Supp ose the size of the window is w records, then every new record entering the window is compared with the previous w -1 records to find "matching records", that is, records containing similar key values. The first record in the window then slides out of the window.
 Figure 1: Record comparison with a sliding wind ow of size 3 There a re three types of dup li cate records: true dup licates, false dup licates and dup licates that require further investigation. The attribute names and values have been encrypted to ensure confidentiality of data. The key used in this case is the combination of NRIC and DO S. Table 1 s hows a pair of true dup licates T1 and T2 where both the records contain the same data values in all t he fields. Table 2 illustrates the characteristics of false dup licates: (a) Records F1 and F2 h ave the same values in fields s uch as (b) Fields s uch as CLIN, case type (C -TYPE), and screening (c) The last field, REDR, which contains the r eviewing The reason for such false dup licate detection is because NRIC and DO S combined together is not really a key. The key should be {NRIC, DO S, CLIN}. Howev er, it i s rarely that a person will screening. For efficiency reasons, we choose {NRIC, DO S} as a key. Records s uch as F1 and F2 are the e xceptional cases. Finally, Table 3 shows two records V1 and V2 that contain the same data in all t he fields except for the screening result field HDU R. In this case, clarification from the e xpert i s needed to find out whether the value of HDU R is 6 or 5.
 The e xperimental studies in [4] demonstrated that 70 % of du plicate records are detected and removed as a result of the additional pre -processing steps. Note that i n p ractice, it i s inherently difficult t o detect all the dup licates in a database. Table 3: Verification n eeded to confirm if Record V1 a nd Once the data has been cleaned, the mining process began. Many data mining techniques are now available to discover patterns in data. In general, the discovered p atterns fall i nto o ne of the following types: classification p atterns, association p atterns, sequential patterns, and spatial -temporal patterns. In the diabetic database a pplication, we focus on the mining o f classification and association p atterns. Classification p atterns provide a description of the characteristics of the population h aving certain d iabetic -related eye disease. They are then u sed to predict whether a new patient i s likely to have the e ye disease. Association p atterns provide a list of symptoms or treatments that often occur together, which give a complete picture of the relationships in the domain. A state -of -art data mining tool that integrates classification with association rule mining (CB A) is used to find all such p atterns [7]. We used minimum supp ort of 1% and minimum confidence of 50% as s uggested b y the doctors to mine a ssociation rules. Approximately 700 rules are generat ed in total. The doctors were totally overwhelmed. Some kind of post -processing is needed to help the doctors und erstand these rules. Our exploration mining methodology aims to g ive the doctors a better und erstanding o f their data a nd the discovered p atter ns by helping the doctors to step through the massive a mount of information in stages. In the first stage, basic demographic information about t he patients will be presented to the doctors. The goal here is to allow the doctors to have some basic idea a bout the impact of various s ingle a ttributes, such as age, race, etc on the disease population. The information is presented in h istogram graph s for easy digestion. Two types of information can b e displayed: the absolute proportion (see Figure 2) or the re lative proportion (see Figure 3). In Figure 3, we display the proportion of the different race -age populations having the e ye disease. The race -age group with the highest disease proportion is assigned to be 100 % while the rest of the group p roportions are adjusted accordingly. Each bar r epresents an age group while the different colors within a bar denote different races. This provides a comm on b asis for comparison. In one glance, the doctor is able to tell which factor dominates in terms of the number of patients having a particular eye disease a nd h ow much are the deviations among the group s over the years.
 Once the doctors have obtained some basic idea of the demographic distributions, they are ready to investigate a ll significant correlations that exis t i n the data. This marks the beginning o f phase two o f the e xp loration mining. During this phase, the doctors are interested in kn owing which set of symptoms often occur together and that i f they do, it often implies the presence of some e ye disease. Such correlations can be found u sing association rule mining. Association rule mining is comm only stated as follows [1]: Let I = { i 1 , ..., i n } be a set of da ta fields , and D be a set of data records. Each d ata record consists of a sub set of attribu tes/fields i n I . An association rule is an implication of the form X  X  Y , where X  X  I , Y  X  I , and X  X  Y =  X  . The rule X  X  Y holds in D with confidence c % if c % of data cases in D that supp ort X also supp ort Y . The rule has s upp ort s % in D if s % of the data case in D c ontains X  X  Y . The problem of mining association rules is to generate a ll association rules that have supp ort and confidence greater than the user -specified minimum supp ort and minimum confidence.
 Table 4 shows the breakdown of the number of association rules generated p er year using the CB A mining tool [7] with a fixed class attribute, namely, the type of eye disease. The doctors are overwhelmed b y the many rules and most of them do not make sense to the doctors. This is because these rules only indicate that there e xist statistical relationships between the a ttributes, such as  X  X he e xistence of A and B implies the e xistence of C X . However they do not s pecify whether the presence of A and B causes C, which is what the doctors are interested in. Further analysis is needed to determine the potential factors of diabetic eye disease. This is the third ph ase of exploration mining. [15] studied h ow casual str uctures can b e determined from association rules. A LCD algorithm is proposed for this purpose [15]. The LCD algorithm is a polynomial ti me, constraint -based algorithm. It uses tests of variable dependence, independence, and conditional independence to res trict t he possible causal relationships between variables. Underlying this technique is the Markov condition [16].
 Definition 1 ( Markov Cond ition) 
Let A be a nod e in a causal Bayesian n etwork, and let B be any nod e that is not a d escendan t of A in the c ausal network. Then the Markov c ond ition ho lds if A and B are independent, cond itioned on the parents of A.
 Assuming the Markov condition, we can make the following causal claims. For instance supp ose we know that A has no cause. Then if B is dependent on A, B must be caused b y A, although p ossibly indirectly. If we have a third variable C dependent on b oth A and B, then the three variables lie a long a causal chain. Variable A, since it has no cause, is at the head of the chain, but we do not know w hether B causes C or vice versa. If, however, A and C become independent conditioned on B, then we can conclude, by the Markov condition, that B causes C. Now supp ose two v ariables B and C are independent, but each is correlated with A. then B and C are not a cau sal path, but A is on a causal path with b oth of them, implying either both are ancestors of A or both are descendants of A. If B and C become dependent when conditioned on A, then b y the Markov condition, they cannot be descendants of A, so we can conclud e that B and C are causes of A. In our diabetic application, we modified the LCD algorithm (see Figure 4) to determine causal relationships involving multiple factors. The a lgorithm assumes tests for dependence a nd conditional independence using  X  2 stati stical test . The definitions of dependence a nd conditional ind epend ence a re given as follows.  X  Dependence test  X  Independence test  X  Conditional independence test Appl ying these definitions in our modified LCD algorithm, we obtain the results as s hown in Table 5. Our doctors agree that the factors we discovered make sense a nd that they are helpful t o allow the doctors to have better und erstand ing o f how multiple attribu tes interact with each other in the diabetic database. However, all t he a bove techniqu es give only a bird eye X  X  view of the whole domain. In order for the doctors to g ain an overview or summ ary of the disease patterns, we have e mbarked on the general and exception rules mining (see more details of this technique in [9]). A new representation, called general and exception rule representation, is proposed. This new representation consists of two parts: the general rules and the exception rules. The general rules give the und erlying trend in the data while the e xception rules give us the a bn ormaliti es to these trends. The a dvantage of this new representation is that it is highly intuitive a nd can easily draw the user X  X  attention to the 
Inpu t: A set M of rules of the form (A -&gt;B), where A contains 2 or more a ttributes cond itions. A set S of rules of the form (C -&gt;B) where C is a single a ttribu te condition. 
Outpu t: A list of causal relationships s upp orted b y the data for e ach rule (A -&gt;B) in M end for interesting rules/patterns. The e xception rule mining program takes as inpu t t he prunn ed tree generated b y C4.5 [11] and performs the following check to find the e xception rules:  X  It condu cts chi -square test for each n ode of the tree, if it i s  X  If this general rule candidate does not has any parent node  X  For each general rule or sun -general rule node, check the leaf  X  For those nodes that does not belong t o the a bove three After showing these rules to the doctors, they find the rules useful. In p articular, they find the e xception rules especially helpful in und erstanding some of the special subp opulation that exhibits trends which are contrary to t he main p opulation. Table 3 gives a summ ary of the number of general and exception rules discovered. From the table, we see that the doctors do have some feelings about the general trends. In fact, the general rules s erve to confirm what they know. On the other hand, there a re quite a number of exception rules that are unkn own to the doctors. They agree that these e xception rules are worth investigating further. The issues of preprocessing and p ostprocessing (before a nd after rule generation) have largely been ignored b y the data mining research comm unity. Yet these issues are critical to the success of any real -life a pplications. To deal with these issues, we have proposed the use of a semi -automatic data cleaning system for cleaning the noi sy data a nd a e xp loration mining strategy for easy und erstanding o f the rules generated b y state -of -the -art data mining techniques. Our doctors confirm that many of rules discovered conform to the trends that they have observed in their practices. However, they are surprised b y some of the e xception rules and express interest in investigating them further. Our thanks to Dr Shanta C Emm anu el, Dr Paul Goh, and Dr Jonathan Phang o f the Family Health Service, Ministry of Health, Singapore, for providing us the data a s well as their active involvement in the project. [1] Agrawal R., Imielinski T. and Swami, A.,  X  X ining [2] Hernandez M. and Stolfo S.,  X  X he merge/pu rge problem for [3] Klemetinen, M., Mannila, H., Ronk ainen, P., Toivonen, [4] Lee M. L., Lu H., Ling T.W. and Ko Y.K.,  X  X leansing Data [5] Liu B., Hsu W.,  X  X ost -analysis of learned rul es, X  AAA I , [6] Liu B., Hsu W., and Chen S.,  X  X sing g eneral impressions to [7] Liu B., Hsu W., and Ma Y.,  X  X ntegrating classification and [8] Liu B., Hsu W., Ma Y.,  X  X runing and Summ arizing the [9] Liu B., Hu, M., and Hsu W.,  X  X ulti -level organization and [10] Pazzani M. J., Manu S., Shankle W. R.,  X  X eyond Concise [11] Quinlan R.,  X  X 4.5: A program for machine learning X . [12] Srikant, R., Vu, Q. and Agrawal, R.,  X  X ining association [13] Silberschatz A. and Tuzhilin A.,  X  X hat makes patterns [14] Tsumoto S.,  X  X utomated Discovery of Plausible Rules [15] Silverstein C., Brin S., Motwani R., Ullma n J.,  X  X calable [16] Spirtes P., Glymour C., and Scheines R.  X  X ausation, 
