 to post on servers or to redistribute to lists, requires prior specific SET keeps track of frequent itemsets, and the size of the 0-1 programming system grows rapidly as the number of such itemsets grows. Finally, if the size constraint changes, a new 0-1 programming system must be solved. This require-ment makes it hard to address the cost-constrained selection where there is no size constraint. 
The rest of the paper is organized as follows. Section 2 outlines our approach and the issues to be addressed. Sec-tion 3 provides an overview of a web page ranking algorithm, HITS. In Section 4, we present the detail of our ranking method. In Section 5, we apply the ranking method to solve the two selection problems. Section 6 presents an empirical study. Finally, we conclude the paper. 
Our ranking method is motivated by the mutually rein-forcing relationship of hubs and authorities adopted in the web pages ranking algorithm HITS [8]. The well known search engine Google (http://www.google.com) has exploited this relationship for ranking query results on the web. 
The main idea of HITS is to regard the presence of a hyperlink from page i to page j as conferring authority (on a given topic) to page j. HITS ranks web pages by exploiting the mutually reinforcing relationship of hubs and authorities exhibited by such hyperlinks: a good hub is a page that points to many good authorities; a good authority is a page that is pointed to by many good hubs. The fixed point, i.e., the converged values, of these goodnesses gives the hub weight and authority weight of web pages. 
We draw an analogy between the cross-selling effect and the mutual reinforcement of hub weight and authority weight. An item corresponds to a web page, and the influence of sales between two items corresponds to a hyperlink between two pages. An item gets "authority weight" if it is a "necessary buy" for other items, in that a customer buying other items tends to buy this item. This is analogous to a web page getting authority weight if it is a "necessary destination" of other pages, i.e., pointed by other pages. An item gets "hub weight" if it is an "introductory buy" for other items, in that a customer buying this item tends to buy other items. This is analogous to a web page getting hub weight if it is a "directory page", i.e., points to other pages. 
However, this analogy does not quite solve our ranking problem. 
In this paper, we first address these issues and present a ranking method in the presence of cross-selling effect. We then apply this ranking method to solve the two selection problems. Finally, we present a method for estimating the profitability of a selection of items before applying it to real life applications. We devote this section to review the HITS algorithm in [8]. HITS (Hyperlink-Induced Topic Search) is an algorithm for ranking the relevance of web pages on a given topic. Given a topic, HITS first identifies a (possibly very large) base set of pages that are potentially related to the topic. HITS then computes the hub/authority weights of each page in the base set based on the mutually reinforcing relationship of hubs and authorities: a good hub is a page that points to many good authorities; a good authority is a page that is pointed to by many good hubs. This relationship is exploited via an iterative algorithm that updates numerical weights for each page. The details are given below. 
Each page i in the base set is associated with a non-negative authority weight a( i) and a non-negative hub weight h(i). The pages i with larger a(i) and h(i) weight are "bet-ter" authorities and hubs. Before each iteration, the weights of each type are normalized so that their squares sum to 1: Epa(i) 2 = 1 and Eph(i) 2 = 1. In one iteration, HITS up-dates a(i) to be the sum of the hub weight of all pages j in the base set that have a hyperlink to i: and updates h(i) to be the sum of the weights of all pages j that i points to: This iterative step continues until a(i) and h(i) converge to stable values. The convergence follows from a property of the matrix formulation below. 
Suppose that all pages in the base set are numbered by 1,2,...,n. The adjacency matrix B of the base set is an n x n matrix with entries of either 0 or 1. The matrix entry B[i,j] is set to 1 if page i has a hyperlink to page j; otherwise, it is set to 0. The hub weights and authority weights are represented by vectors h =&lt; hi,... ,h X  &gt; and a =&lt; al,...,an &gt;. The above update rule can now be written as follows: Unfolding these equations once, corresponding to the first iteration, we obtain: After k iterations, we have Linear algebra indicates that the sequence of hub/anthority weights converges to the principal eigenvectors of BB T and BTB, respectively [8]. Furthermore, the converged weights are independent of the choice of initial weights [8]. For a more comprehensive background of linear algebra, we refer the reader to [7]. 
There are two ways to compute the hub/anthority weights: either perform the iterative update of hub/authority weights, as in Equations (1) and (2), until the weights become sta-ble, or apply the standard eigen analysis packages such as [9] to BB T and BTB to find their principal eigenvectors. Web pages are ranked by the authority weight. [8] pointed out that the iterative update converges rapidly, usually af-ter 20 iterations. One advantage of the iterative update is that the user can control the number of iterations, therefore, controlling the tradeoff between resources and optimality. 
We adopt the hub-anthority idea of HITS to compute the overall profitability of items. Unlike web pages where hy-perlinks are given, we need to model links between items and the strength of such links. The presence of individual profit of items presents another new factor. It is a challenge to incorporate these new requirements in a meaningful way and still maintain the convergence of hub-authority weights. 
Infrequent occurrences often represent random behaviors and should not be counted on to maximize profit. We specify a minimum support to exclude infrequent items. Precisely, the support of an item i, denoted supp(i), refers to the per-centage of transactions that contain the item. Similarly, the support of a pair i and j, denoted supp(i, j), can be defined. An item or a pair is drequent if its support is not less than some minimum support minsupp. The set of frequent items forms the nodes in our graph. We assume that the number of frequent items is much larger than the number of items to be selected. 
Next, we determine the potential cross-selling links be-tween frequent items. Consider frequent items i and j, not necessarily distinct. A link i ~ j represents the cross-selling effect from i to j. The meaning and strength of this link are given by the confidence of i ~ j, denoted conf(i ~ j), de-fined as the percentage of the transactions that contain i also contain j, that is, supp(i,j)/supp(i). The following under-standings about conf(i ~ j) are relevant to the subsequent discussion: 
For example, on one extreme, if conf(i ~ j) = 1, j always occurs whenever i does, in which case j is fully necessary for i. If we do not select j, we will lose not only the profit of j, but also the profit of i. We can recognize this necessity by crediting the authority weight of j by the profit of i. On the other extreme, if cond(i ~ j) = O, j never occurs when i has no contribution to the authority weight of j. 
We create links as follows. For each pair of frequent items i and j, not necessarily distinct, we create a link i ~ j if supp(i,j) &gt; minsupp and conf(i,j) &gt; minconf, minsupp ensures that i and j occur together frequently, minconf ensures that there is enough cross-selling effect from i to j. Note that every frequent item i has a link to itself, i.e., i --~ i, because conf(i, i) = 100%. 
The cross-selling effect of a link i ~ j depends not only on the confidence conf(i ~ j), but also on the individual profit of i. The term "individual profit of i", denoted prod(i), refers to the recorded profit of i in all transaction. For in-stance, ifj is flxlly necessary for i, i.e., conf(i ~ j) = 100%, and if the individual profit of i is $500, not selecting j will result in the loss of $500 of i. If the individual profit of i is $5000 instead, the loss will be 10 times of that, and for that reason, the authority weight of j should be 10 times as high. 
To take the role of individual profit into account, an imme-diate approach is treating it as the initial authority weight in HITS. Unfortunately, this is equivalent to ignoring the indi-vidual profit because the final weights computed by HITS, which are completely determined by hyperlinks, are inde-pendent of the choice of initial weights [8]. In a sense, HITS finds the densest communities of hubs and authori-ties, thereby, ignoring any initial weights. 
We solve this problem by "incorporating" the individual profit into links. In particular, if there is a link i ~ j, we credit the authority weight of j by prod(i) x conf(i, j). Intuitively, prod(i) x cony(i, j) represents the part of prod(i) lost by not selecting j, which should be viewed as the credit for the authority weight of j, just as a hyperlink to a page j is viewed as the credit for the authority weight of j. With this consideration, we modify the update Equation (1) as and modify the update Equation (2) as While Equation (3) has the above "crediting authority inter-pretation", Equation (4) has the following "rewarding hub interpretation": prod(i) x cond(i, j) rewards i as a good hub which makes sense. Alternatively, prod(j) x conf(j, i) and prod(i) x conf(i, j) can be viewed as the "quality" of links j --~ i and i -* j, and a better quality gives more endorse-ment of the hub/authority reinforcement. The corresponding matrix formulation is straightforward. We set the entry B[i,j] in the matrix B as follows. If there is a link i --~ j, If there is no link i ~ j, Note that B[i, i] = prod(i) for all items i. B is called the cross-selling matrix. Since BB T and BTB are symmetric, 654 We illustrate the algorithm using an example 
EXAMPLE 4.1. Given the following information of frequent prof(X) = $5, prof(Y) = $1, proS(Z) = $0.1, conf(X ~ Y) = 0.2, con.f(Y ~ X) = 0.06, conf(X ~ Z) = 0.8, conf(Z ~ X) = 0.2, conf(Y ~ Z) = 0.5, conf(Z --} Y) = 0.375. The hub weight is given by the principal eigenvector of Interestingly, this ranking is different from the ranking X, 
We apply the new ranking method to solve the two selec-
For the size-constrained selection, the ranking of items 
For the cost-constrained selection, the selection of items The question is: where should the ranked list be cut off. 
Suppose that we can estimate the total profit of a set of 
Estimating the profitability of selected items is an impor-new transactions with only the selected items available, this information can only be estimated. We present an estima-tion model based on the confidence and support observed in the given transactions. 
Consider a database of transactions T and a set of selected items S. We estimate the profit generated by S as follows. 
For each transaction, t, in the database, let t' represents the items selected in the transaction t and d represents the items not selected in the transaction t. If d is empty, all items in t are selected, and the profit of t remains unchanged. If t' is empty, no item of t is selected, and t generates no profit. The other case is that d and t' are non-empty. In this case, we estimate the profit of the selected items in t' as follows. 
First, for each transaction t, we find the confidence conf(--~d -~a) for each item a in t'. This confidence estimates the prob-ability that the customer will not buy item a given that the items in d are not selected. -~d --~ -~a are called loss rules. Suppose that the original profit of item a in t is prof(a, t). 
We estimate the loss of the profit on item a in transaction t by conf(~d -~ -~a) x prof(a,t), due to the absence of the items in d. Thus, the estimated profit of item a in t' is prof(a,t)(1 -conf(-,d ~ -~a)). The estimated profit of t is the sum of the estimated profit for all items in t'. The estimated profit of S is the sum of the estimated profit for all transactions in the database. This is summarized below. DEFINITION 5.1. The estimated profit of a selection S is To evaluate our method, we implemented HAP, PROF-
SET, and the naive approach in C++. We used mathematic software packages [9, 6] for the eigen analysis of HAP and for the 0-1 programming of PROFSET. The same result was also obtained using iterative computation. The first dataset is acquired from a large drug store in 
Vancouver over a period of 3 months. This dataset car-ries 26,128 different items and 193,995 sales transactions. A transaction contains 2.86 items on average. The relatively short transaction length implies that the cross-selling effect is limited in this dataset. Each item in a transaction is asso-ciated with a profit, computed by ~price x quantity -cost) of the item. The "cost" refers to the cost of the item it-self, which is different from the selection cost in the cost-constrained selection problem. Every item has a positive profit, with the total profit of $1,006,970. 
We also generated a synthetic database of stronger cross-selling effect. This dataset is generated using the IBM syn-thetic data generator [1] with the following parameters: 1,000 items, 10,000 transactions, 10 items per transaction on av-erage, and 4 items per frequent itemset on average. We generated the profit of items for a single quantity as follows: 80% of items have a medium profit ranging from $1 to $5, 10% of items have a high profit ranging from $5 to $10, 10% of items have a low profit ranging from $0.1 to $1. This is a simplified version of the normal distribution. The exact profit of each item is determined by randomly picking a dol-lar amount from the respective profit range. We consider only single quantity sales for each item. The total profit in this dataset is $317,579. 
We measure the profitability of each method by the esti-mated profit of selected items in Definition 5.1 
The drug store dataset. The results on the drug store dataset are shown in Figure 3 for minimum support of 0.1%, and in Figure 4 for minimum support of 0.05%. The profit generated and items selected are in the percentage of total profit and total number of items in the dataset. As the num-ber of selected items increases, HAP enlarges its lead over the other two methods. However, the lead is limited because with the average of 2.86 items per transaction, there is no strong cross-selling effect between items. The cross-selling effect increases to some extent at the lower minimum sup-port of minimum support of 0.05%. At the same selection size, the smaller minimum support generates a larger profit, due to more cross-selling effect considered. The y/x (for profit/item) ratio indicates the "profit effectiveness" of se-lected items. When very few items are selected, this ratio is high because only the most profitable few items will be selected. 
The synthetic dataset. The results for the synthetic dataset are shown in Figure 5 for the minimum support of 0.5%, and in Figure 6 for the minimum support of 0.1%. 
At the minimum support of 0.1%, all three methods per-form similarly because there are only 15 frequent pairs (and few frequent itemsets). If this happens, there is not much association among items, and HAP and PROFSET degen-erate to the naive approach, i.e., selecting items based on their individual profit. However, at the minimum support of 0.1% (Figure 6), with 11,322 frequent pairs or links, the cross-selling effect becomes strong and HAP generates a con-siderably higher profit than the other methods after 1/3 of the items are selected. This increase comes from a better selection based on the overall profitability of each item in the presence of cross-selling effect. 
Surprisingly, PROFSET performed worse than the naive approach on this dataset. One reason is that, as pointed out in the Introduction, maximal frequent itemsets used by PROFSET do not necessarily represent strong associations. 
This is particularly so when the minimum support is low, in which case PROFSET tends to select long itemsets that have a small support. In the extreme case of the lowest minimum support, for example, PROFSET will select the whole transaction to represent a "purchase intention". A consequence of this purchase intention model is that, as the 
Figure 3: The drug store dataset, minimum sup-port=0.1% 
Figure 5: port=0.5% 
Figure 4: The drug store dataset, minimum sup-port=0.05% 
Figure 6: port=0.1% minimum support decreases, PROFSET often selects many items that are not strongly associated. On the other hand, such items do not necessarily have high profit themselves. 
As a result, PROFSET loses to the naive approach. 
Our work benefits from HITS's mutual reinforcing rela-tionship between hubs and authorities considered for web page. However, several unique requirements in the context of item ranking present new issues to deal with. In par-ticular, the profit of items and the strength of cross-selling effect are new factors in our problems, but there is no obvi-ous place to model them in the web page ranking framework. 
Another challenge is that any modification to the web page ranking model must preserve the convergence of the itera-tive computation. The work presented can be considered as a generalization of the hub-authority web page ranking in that nodes/links have non-uniform weights to start with. We have shown that decision making problems often require such a generalized modeling. The two item selection prob-lems considered are such examples. [1] R. Agrawal. Ibm synthetic data generator. In [2] R. Agrawal, T. Imilieuski, and A. Swami. Mining [3] R. Agrawal and R. Srikant. Fast algorithm for mining [4] T. Brijs, B. Goethals, G. Swinnen, K. Vanhoof, and [5] T. Brijs, G. Swinnen, K.Vanhoof, and G. Wets. Using [6] CPLEX. Ilog cplex. In [7] G. Golub and C. F. V. Loan. Matrix computations. [8] J. M. Kleinberg. Authoritative sources in a hyperlink [9] MINILAB. Minilab. In http://www.minilab.com/. 
