 Transfer learning has bene tted many real-world applica-tions where labeled data are abundant in source domains but scarce on the target domain. As there are usually mul-tiple relevant domains where knowledge can be transferred, Multiple Source Transfer Learning (MSTL) has recently at-tracted much attention. Most existing MSTL methods work in an oine fashion in that they have to store all the data on the target domain before learning. However, in some time-critical applications where the data arrive sequentially in large volume, a fast and scalable online method that can transfer knowledge from multiple source domains is much needed. To achieve this end, in this paper, we propose a new framework of Online Multiple Source Transfer Learn-ing (OMS-TL). The framework is based on a convex opti-mization problem where knowledge transferred from multi-ple source domains are guided by the information on the target domain. The proposed method is fast, scalable and enjoys the theoretical guarantees of standard online algo-rithms. Extensive experiments are conducted on three real-life data sets. The results show that the performance of OMS-TL is close to that of its oine counterpart, which bears comparable performance to existing baseline methods. Furthermore, the proposed method has great scalability and fast response time.
 H.4 [ Information Systems Applications ]: Miscellaneous Online Learning, Transfer Learning, Multiple Source Transfer learning refers to the scenario that given a learning task on a target domain, knowledge is extracted from one or several source domains to help the learning task on the target domain. Transfer learning is mostly applicable when labels on the target domain are limited or too expensive to tra nsferred from multiple source domains are guided by the information on the target domain. The learning is cast as a convex optimization problem where the global minimum is found. Based on this oine MSTL method, a fast and scal-able online algorithm is then developed, which enjoys the theoretical guarantee of standard online algorithms. Exten-sive experiments are conducted on three life-time data sets. The results show that the performance of OMS-TL is close to that of its oine counterpart, whose performance is com-parable to existing MSTL baseline methods. Furthermore, the proposed method has great scalability and fast response time.
 The major contributions of this paper are: Assume there are k source domains. The s-th source domain is the feature vector, y s i is the corresponding label, and n s is the total number of samples for source domain s . The source domains are assumed to be xed and known before the learning happens. The target domain has a few labeled data D l = ( x i ; y i ) j n l i =1 which are known beforehand. Data arrive in sequence, i.e., f ( x t ) j t = 1 ; :::; T g . The goal is to develop a target classi er f T that can predict the label of the test data on the target domain, using knowledge extracted from source domains and a few target labeled data. Note that di erent from traditional online learning, no label will be revealed after the data point arrives. The core of OMS-TL is a fast and scalable online MSTL method. To develop such a method, we rst propose an of-ine MSTL method that combines knowledge from source domains and information on the target domain by using convex optimization. An online MSTL method is then de-veloped based on the optimization framework that has no-regret performance guarantee with regard to its oine method. 3.1 Multi-Source Transfer Learning From source domains, there are many possible ways to trans-fer knowledge. In this paper, we focus on the decision level where, for each source domain, a classi er is trained and makes predictions on the testing data of the target domain. To transfer knowledge, we rst represent the predictions from multiple source domains using a bipartite graph. Sup-pose there are c classes and k source domains, the prediction from each source corresponds to c groups where each group represents one class. For T testing data points on the tar-get domain, we know that each data point must belong to k groups. This formulates a natural representation of bipar-tite graph. We use the following toy example to illustrate the bipartite graph representation.

T o estimate the probability of data points in each class, we minimize the following objective function: where h i = imum when q jz and f iz are as close to each other as pos-sible, which implies the consensus assumption that a group j corresponds to class z if the majority of data points in this group belong to class z . Meanwhile a data point cor-responds to class z if the majority of the groups it belongs to correspond to class z . The second and third terms in Eq. 1 maintain the constraints that F and Q don't devi-ate much from their initial labels, where and denote the con dence over the two constraints. The last term in Eq. 1 implements the smoothness assumption on the target domain, which encourages label smoothness over all data points in that similar examples tend to have similar labels. Note that the objective function in Eq. 1 is convex , which indicates that we can nd the global minimum of Eq. 1.
We take the Block Coordinate Descend method to solve the objective function in Eq. 1 iteratively. At each iteration, we x Q , then the Hessian matrix with regard to F can be shown to be positive de nite, indicating that  X  J ( Q t 1 ; F ) = 0 gives the unique minimum of the objective function in terms of F . Similarly, xing F , we can verify that the cor-responding Hessian matrix is also positive de nite, there-fore we can have the unique minimum of J ( F; Q ) in term of Q . By Proposition 2.7.1 in [1], the iterative algorithm converges to the stationary point. The algorithm is shown in Algorithm 1.
 A lgorithm 1 The Iterative Algorithm Input: A n v , P v c , Y n c , W n n parameter , , ,  X  Output: F
H ere Dv = diag f ( Kv = diag f ( elements of a matrix. D v and D n are the normalization fac-tors. K v acts as constraints for the group nodes. ^ L is the Laplacian derived from W : ^ L = D w W where D w is a diagonal matrix with its ( i; i ) element equal to the sum of the i -th row of W .

Since the objective function in Eq. 1 is convex and Algo-rithm 1 converges to a stationary point, we can thus nd the global minimum of Eq. 1 through Algorithm 1. Al-gorithm 1 indicates how information propagates. During each iteration, F , the probability of each data point in each class, is updated based on the balance of smoothness as-sumption and consensus of source domains (line 4). Such I t is easy to see that R  X  ( T ) J inst ( T ) J ( f ; q ) R ( T ). Based on Eq. 4, we have lim sup T !1 R  X  ( T ) 0. Therefore, we can conclude that the optimal solution to the objective function in Eq. 2 has no-regret guarantee with regard to the optimal solution to Eq. 1.

Lem ma 1 shows that if we can nd the optimal solution to Eq. 2, then the performance of the online algorithm con-verges to that of the oine algorithm when T increases. Now we present an ecient algorithm to nd the optimal solution.
 Ecient Online Algorithm We iteratively solve F and Q . At the d -th iteration, we set the partial derivatives to 0 and obtain the unique minimum of the objective function with respect to f t and q j respectively: Note that f 1 ; :::; f t 1 are xed. The online algorithm works as follows: When a new sample arrives, a prediction is made based on two parts: the source domains and the target do-mains. The source domains provide the prediction in terms of the average of the probability of the groups that the sam-ple belongs to. The target domain yields the prediction in terms of the weighted average of past prediction following smoothness assumption. Once a prediction on the sample is made, we in turn update group probability Q . The proce-dure stops when it converges. The overall online algorithm is as follows: A lgorithm 2 The Online MSTL Algorithm Input: Received data points x 1 ; :::; x t 1 , P v c , Y n c , incom-ing data point x t parameter , , ,  X  Output: f t Ti me Complexity Analysis: In Algorithm 2, line 1 takes O ( k ) to compute where k is the number of source domains. Line 2 takes O ( t ) to compute, where t is the number of data points received so far. Line 5 takes O ( v ) to compute since f ; :::; f t 1 is xed during time t . For line 6, we can rewrite Eq. 7 into follows: where both the numerator and the denominator can be split into two parts: the summation over the previous t 1 data points, and the information carried by current data point x t . Therefore, the update can be done incrementally by storing the summation over the existing data points. The compu-tation of Eq. 7 is thus O (1). Therefore line 6 takes O ( v ) to compute where v is the number of groups. Suppose the
Cardiac Arrhythmia Detection . The ECG data sets in the CAD problem are from MIT-BIH database [10]. We randomly picked 13 patients' ECG data (time-series) and each patient's data consists of around 1008 to 1416 samples of 39 dimensional feature vectors, belonging to two classes: arrhythmia and normal heart beats. When learning on one patient, we transfer knowledge from all the other patients.
Spam Email Filtering . The email spam data set was re-leased by ECML/PKDD 2006 discovery challenge Its task B contains 15 di erent users' email box, each of which has dif-ferent word distributions. The task is to build a spam email lter for each individual user by transferring knowledge from all the other users (sources). The number of normal emails outnumbers that of spam emails in that the percentage of spam emails is roughly 25%.

Intrusion Detection . The KDD cup 99 data set consists of a series of TCP connection records for a local area net-work. Each example in the data set corresponds to a network connection, which is labeled as either normal or an attack. Attacks fall into four main categories: DOS, R2L, U2R, and Probing. We created three data sets, each of which contains a large set of randomly chosen normal examples and a set of attacks from one category. The transfer learning scenario is to learn a classi er on the target task domain by transferring knowledge from the other task domains.

Evaluation Metric: The three real-life data sets all have imbalanced label distributions, i.e., one class dominates the other class. Therefore, we calculate Receiver Operating Characteristics (ROC) curve and assess the quality of the ROC curve by Area Under the Curve (AUC) . The higher, the better.

Baseline Methods: To show the advantages of the pro-posed oine MSTL method, we compare it with the follow-ing baseline methods: CRC [9] transfers knowledge from multiple source based on the maximization of the consen-sus among sources; MDA [3] computes the weight for each source domain based on the smoothness assumption and co unterpart. In the experiments, we observe the correspond-ing behavior. From the experiments, we notice that the per-formance of online algorithm is getting close to oine algo-rithm when T increases. We also observe some gap between bu ering technique and online algorithm without bu ering. This illustrates the trade-o of bu ering technique: it gains better scalability but loses some performance gains.
Scalability Study : In this part, we examine the scala-bility of the proposed online MSTL method. Figure 3 shows the scalability of the oine algorithm, online algorithm and online algorithm with bu ering technique on selected sam-ples from three datasets. For the rst two plots in Figure 3, the di erence between the oine algorithm and the on-line algorithm is so big that we plot the running time in log space. The remaining two plots are in linear space. It is clear that the growing rates of oine algorithm is quadratic while the online algorithm grows in linear time. When the bu er window is full, the bu ering technique will grow in constant time. Also the online algorithm is fast: In CAD problem, each data point only needs less than 0 : 05 second to make a prediction. For each email, it consumes less than 1 second. In network intrusion task, each data point takes less than 0 : 2 second.

Figure 4 shows the impact of bu ering window size. As we can see from the results, larger bu ering size brings better performance. Meanwhile, it also brings longer computation time, which is a typical trade-o of using bu er.
Parameter Sensitivity : There are three parameters, i.e., , and , in the OMS-TL that have in uence on the performance. indicates the con dence over the ini-tial group labels. The group labels represent the knowledge transferred from source domain. In the MSTL, the knowl-edge transferred from source domains are not always helpful, therefore is set to be low to produce satisfactory perfor-mance. On the other hand, represents the con dence over the training data on the target domain. Those labeled data should be trusted, and thus should be set a high value. For , it denotes the con dence over the information from the unlabeled data. The value of depends on whether the structure of the data on the target domain follows the smoothness assumption. Figure 5 shows the experimental results on the parameter sensitivity of the proposed method (due to space limitation, we only show one the example). When we conduct experiments, we vary one parameter (e.g. ) while the other parameters remain xed. As we can see from the gure, the performance tends to decrease when increases, indicating that there are irrelevant sources in the source domains. Increasing will magnify their impact, therefore decreasing the performance. For , we notice that the performance reaches a plateau after 50, showing that the training data on the target domain contribute to the performance increase. For , it's a little more complicated than the other two parameters because parameter that is too small or too large damages the performance. This vari-ance of performance is associated with the degree of how we trust the smoothness assumption on the target data. Multiple source transfer learning transfers knowledge from multiple source domains to a target domain where labeled data are hard or expensive to collect. Existing MSTL ap-proaches work in an oine fashion in that it has to store all the data before learning. In this paper, we explored a real scenario that online decision has to be made on fast arriving
