
Historically, a number of seemingly diverse ensemble schemes have been proposed, but recently efforts have been made to explain their properties under within a common framework, which can be viewed as statistical sampling from a model space where adequate model coverage is achieved via appro-priate randomization of the learning set and/or the learning process [8][3]. Decision trees have been used predominantly as the ensemble base learner, and approaches based on ran-dom perturbations of the training set (bagging [6] and arcing [7]) received particularly much attention. Adaboost [11], a technique related to arcing that does not explicitly depend of randomization, has been especially well acclaimed. 
Dietterich [9], instead of randomizing the training set, ran-domized the process of tree construction whereby, when de-ciding how to best split a tree node, all possibilities are ranked according to their utility, and a split is chosen at random from a small set of the most promising candidates I . Note that such an approach requires a complete training set (with all features) to be present for splitting each node of the tree. A different variant of randomization, which relaxes this constraint, is due to Amit et al. [3] and Breiman [8]. In [3] it is suggested to built each tree using random sub-sets of all possible attribute-threshold combinations when splitting each node of the tree. Breiman [8] proposed a sim-ilar process, where a random subset of attributes is selected prior to executing each split (the two approaches are equiv-alent when all features are binary). In both [3] and [8] the authors also propose unifying frameworks for different types of classifiers ensembles (under the name of Multiple Ran-domized Classifiers (MRCL) in [3] and under the term of Random Forests in [8]). 
The ideas outlined in this paper are directly based on the types of randomization proposed by Amit et al. [3] and Breiman [8]. In the general discussion we will focus on the architecture of Random Forests, whose algorithmic implementation clearly puts emphasis on random selection of input features. We will then treat Randomized Boosting of [3] as a natural extension of Random Forests, and use Randomized Boosting as the platform for our experiments. 
It is interesting to note that the idea of using random subsets of features in classifier system design has a long his-tory. Bledsoe and Browining [5] applied it to built optical character-recognition systems using random access memo-ries, by randomly interconnecting the address lines of each memory module to the image matrix. Their architecture be-came known as the N-tuple network [15] (since N randomly selected features were utilized by each model), and has been used with success in several pattern recognition applications (e.g., [1][2]). As will be shown, under certain circumstances, the new architectures such Random Forests can be seen to extend the basic N-tuple idea. 
Although the definition of a Random Forest, as stated in [8], is quite general and has much in common with that of MRCL, we focus on its special instance Forest-RI, which is XFor axis-parallel splits, a candidate combines the attribute (i.e., feature) to be split with a particular threshold value at which the attribute's range is to be divided. 
Since the order in which the tree models are created is 
To summarize, in cases where the complete training set  X  Step 1: choosing a random subset of F features (as-suming that the feature-reduced training set overcomes the RAM limitations);  X  Step 2: building K random trees by sampling from the pre-selected features only;  X  Step 3: repeating steps 1 and 2 for a predefined num-ber of times, or till the desired level of accuracy, or a convergence criterion has been reached. For parallel implementations, each sequence of steps 1 and 2 might be executed on a separate machine. 
A suitable value of F might determined via cross-validation 
The popular Adaboost algorithm [11] (and its arcing vari-6 of the most populous categories {acq, com, earn, econ, engr, garl}, where documents belonging to multiple cate-gories were removed, leaving the 6,089 training documents and 3,044 test documents. For pre-processing, all charac-ters were converted to lower case, and words were defined as sequences of characters delimited by whitespace, which produced N = 6,502 unique features. Words are treated as binary attributes, i.e., only the presence/absence of a word in a document was taken into account. 
The text categorization performance was measured by means of the macro-and micro-averaged F1 metric, often used in the information-retrieval community, defined as where precision is the ratio of the correctly classified doc-uments for a given class to the total number of documents classified as belonging to that class, while recall is the ratio of the correctly classified documents for a given class to the total number of documents belonging to that class. In the case of macro-averaging, F1 is first measured when distin-guishing each class from all others and then the results are averaged. Conversely, in micro-averaging, the contingency tables corresponding to all one-against-others classifiers are merged and then the overall average is computed. More details can be found in [18]. 
Given this data set, three sets of experiments were per-formed: 
Standard Adaboost.MH achieved the micro and macro-averaged F1 test-set accuracy of 0.92 and, 0.95, respectively. Little change in accuracy occurred beyond 500 iterations, and for all subsequent experiments the overall number of boosting iterations (however divided between blocks, etc.) was kept at 500. For Randomized Boosting, the dependence of the test-set F1 measure on the number of boosting iterations and dif-ferent choices of F is depicted in Figures 1 and 2 where, instead of showing the absolute values of F, we used their relative values, with the total number of features used as a reference, i.e., a = F/N, where c~ E (0, 1). Note that for = 1 Randomized Boosting is equivalent to deterministic 
