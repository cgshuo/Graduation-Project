 In spite of the initialization problem, the Expectation-Maximization (EM) algorithm is widely used for estimating the parameters in several data mining related tasks. Most popular model-based clustering techniques might yield poor clusters if the parameters are not initialized properly. To reduce the sensitivity of initial points, a novel algorithm for learning mixture models from multivariate data is intro-duced in this paper. The proposed algorithm takes advan-tage of TRUST-TECH (TRansformation Under STability-reTaining Equilibra CHaracterization) to compute neigh-borhood local maxima on likelihood surface using stability regions. Basically, our method coalesces the advantages of the traditional EM with that of the dynamic and geometric characteristics of the stab ility regions of th e corresponding nonlinear dynamical system of the log-likelihood function. Two phases namely, the EM phas e and the stability region phase, are repeated alternatively in the parameter space to achieve improvements in the maximum likelihood. Though applied to Gaussian mixtures in this paper, our technique can be easily generalized to any other parametric finite mix-ture model. The algorithm has been tested on both synthetic and real datasets and the improvements in the performance compared to other approaches are demonstrated. The ro-bustness with respect to initialization is also illustrated ex-perimentally.
Finite mixtures allow a probabilistic model-based ap-proach to unsupervised learning [10] which plays an impor-tant role in predictive data mining applications. One of the most popular methods used for fitting mixture models to the observed data is the Expectation-Maximization (EM) algo-rithm which converges to the maximum likelihood estimate of the mixture parameters locally [4, 6]. The usual steepest descent, conjugate gradient, or Newton-Raphson methods are too complicated for use in solving this problem [19]. EM has become a popular method since it takes advantages of problem specific properties. EM based approaches have been successfully used to solve problems that arise in vari-ous other applications [12, 2].

In this paper, we consider the problem of learning pa-rameters of Gaussian Mixture Models (GMM). Fig 1 shows data generated by three Gaussian components with different mean and variance. Note that every data point has a proba-bilistic (or soft) membership that gives the probability with which it belongs to each of the components. Points that belong to component 1 will ha ve high proba bility of mem-bership for component 1. On the other hand, data points be-longing to components 2 and 3 are not well separated. The problem of learning mixture models involves not only es-timating the parameters of these components but also find-ing the probabilities with whic h each data point belongs to these components. Given the number of components and an initial set of parameters, EM algorithm can be applied to compute the optimal estimates of the parameters that maxi-mize the likelihood of the data given the estimates of these components. However, the main problem with the EM al-gorithm is that it is a  X  greedy  X  method which is very sensi-tive to the given initial set of parameters. To overcome this problem, a novel two phase algorithm based on stability re-gion analysis is proposed. The main research concerns that motivated the new algorithm presented in this paper are :  X  EM algorithm for mixture modeling converges to a lo- X  There are many other promising local optimal solu- X  Model selection criteria usually assumes that the
Figure 1. Data generated by three Gaussian components. The problem of learning mix-ture models is to obtain the parameters of these Gaussian components and the mem-bership probabilities of each datapoint.  X  Some regions in the search space do not contain
Of all the concerns mentioned above, the fact that most of the local maxima are not distributed uniformly [16] makes it important for us to develop algorithms that not only help us to avoid searching in the low-likelihood regions but also emphasize the importance of exploring promising sub-spaces more thoroughly. This subspace search will also be useful for making the solution less sensitive to the initial set of parameters. In this paper, we propose a novel two phase algorithm for estimating the parameters of mixture models. Using concepts of dynamical systems and EM algorithm si-multaneously to exploit the problem specific features of the mixture models, our algorithm obtains the optimal set of pa-rameters by searching for the global maximum on the like-lihood surface in a systematic manner.

The rest of this paper is organized as follows: Section 2 gives some relevant background about various methods pro-posed in the literature for solving the problem of learning mixture models. Section 3 discusses some preliminaries about mixture models, EM al gorithm and stability regions. Section 4 discusses our new framework and the details of our implementation are given in Section 5. Section 6 shows the experimental results of our algorithm on synthetic and real datasets. Finally, Section 7 concludes our discussion with future research directions.
Although EM and its variants have been extensively used for learning mixture models, several researchers have ap-proached the problem by identifying new techniques that give good initialization. More generic techniques like de-terministic annealing [16], genetic algorithms [13] have been applied to obtain a good set of parameters. Though, these techniques have asymptotic guarantees, they are very time consuming and hence cannot be used for most of the practical applications . Some problem specific algo-rithms like split and merge EM [17], component-wise EM [6], greedy learning [18], incremental version for sparse representations[11], parameter space grid [8] are also pro-posed in the literature. Some of these algorithms are either computationally very expensive or infeasible when learning mixtures in high di mensional spaces [8]. Inspite of all the expense in these methods, ver y little effort has been taken to explore promising subspaces within the larger parameter space. Most of these algorithms eventually apply the EM algorithm to move to a locally maximal set of parameters on the likelihood surface. Simpler practical approaches like running EM from several random initializations, and then choosing the final estimate that leads to the local maximum with higher value of the likelihood are also successful to certain extent [15].

Though some of these methods apply other additional mechanisms (like perturba tions [5]) to escape out of the local optimal solutions, systematic methods are yet to be developed for searching the subspace. The dynamical sys-tem of the log-likelihood function reveals more information on the neighborhood st ability regions and t heir correspond-ing local maxima [3]. Hence, the difficulties of finding good solutions when the error surface is very rugged can be overcome by adding stability region based mechanisms to escape out of the convergence zone of the local maxima. Though this method might introduce some additional cost, one has to realize that existing approaches are much more expensive due to their stochastic nature. Specifically, for a problem in this context, where there is a non-uniform distri-bution of local maxima, it is difficult for most of the meth-ods to search neighboring regions [20]. For this reason, it is more desirable to apply TRUST-TECH based Expecta-tion Maximization (TT-EM) algorithm after obtaining some point in a promising region. The main advantages of the proposed algorithm are that it:  X  Explores most of the neighborhood local optimal solu- X  Acts as a flexible interface between the EM algorithm  X  Allows the user to work with existing clusters obtained  X  Helps the expensive global methods to truncate early.  X  Exploits the fact that promising solutions are obtained
We now introduce some necessary preliminaries on mix-ture models, EM algorithm and stability regions. First, we describe the notation used in the rest of the paper:
Lets assume that there are k Gaussians in the mixture model. The form of the probability density function is as follows: where x =[ x 1 ,x 2 , ..., x d ] T is the feature vector of d dimensions. The  X  k  X  X  represent the mixing weights .  X  rep-resents the parameter set (  X  1 , X  2 , ... X  k , X  1 , X  2 , ... X  is a univariate Gaussian density parameterized by  X  i (i.e.  X  and  X  i ):
Also, it should be noticed that being probabilities  X  i must satisfy
Given a set of n i.i.d samples X = { x (1) ,x (2) , .., x the log-likelihood corresponding to a mixture is
The goal of learning mixture models is to obtain the pa-rameters  X  from a set of n data points which are the samples of a distribution with density given by (1). The Maximum Likelihood Estimate (MLE) is given by : where  X   X  indicates the entire par ameter space. Since, this MLE cannot be found analytically for mixture models, one has to rely on iterative procedures that can find the global maximum of log p ( X|  X ) . The EM algorithm described in the next section has been used successfully to find the local maximum of such a function [9].
The EM algorithm assumes X to be observed data. The missing part, termed as hidden data, is a set of n labels Z = { z (1) , z (2) , .., z ( n ) } associated with n samples, indi-cating which component produced each sample [9]. Each z was produced by the i th component. Now, the complete log-likelihood (i.e. the one from which we would estimate  X  if the complete data Y = {X , Z} is
The EM algorithm produces a sequence of estimates {  X ( t ) ,t =0 , 1 , 2 , ... } by alternately applying the follow-ing two steps until convergence:  X  E-Step : Compute the conditional expectation of the  X  M-Step : The estimates of the new parameters are
Several variants of the EM algorithm have been exten-sively used to solve this problem. The convergence prop-erties of the EM algorithm for Gaussian mixtures are thor-oughly discussed in [19]. The Q  X  function for GMM is given by:
Q ( X  |  X ( t )) = where
The maximization step is given by the following equa-tion : where  X  k is the parameters for the k th component. Because of the assumption made that each data point comes from a single component, solving the above equation becomes trivial. The updates for the maximization step in the case of GMMs are given as follows:
This section mainly deals with the transformation of the original log-likelihood function into its corresponding non-linear dynamical system and introduces some terminology pertinent to comprehend our algorithm. This transformation gives the correspondence betw een all the critical points of the s -dimensional likelihood surface and that of its dynami-cal system. For the case of spherical Gaussian mixtures with k components, we have the number of unknown parameters s =3 k  X  1 . For convenience, the m aximization problem is transformed into a minimization problem defined by the following objective function : where f ( X ) is assumed to be in C 2 ( s , ) .
 Definition 1  X   X  is said to be a critical point of (13) if it sat-isfies the following condition A critical point is said to be nondegenerate if at the crit-ical point  X   X   X  s , d T  X  2 f (  X   X ) d =0 (  X  d =0 ). We construct the following gradient system in order to locate critical points of the objective function (13): where the state vector  X  belongs to the Euclidean space , and the vector field f : s  X  s satisfies the sufficient condition for the existence and uniqueness of the solutions. The solution curve of Eq. (15) starting from  X  at time t =0 is called a trajectory and it is denoted by  X ( X  ,  X  ):  X  s A state vector  X  is called an equilibrium point of Eq. (15) if f ( X ) = 0 . An equilibrium point is said to be hyperbolic if the Jacobian of f at point  X   X  has no eigenvalues with zero real part. The gradient system for the log-likelihood func-tion in the case of spherical Gaussians is constructed as fol-lows : where  X  X   X  X   X  X 
For simplicity, we show the construction of the gradient system for the case of spherical Gaussians. It can be easily extended to the full covariance Gaussian mixture case. It should be noted that only (k-1)  X  values are considered in the gradient system because of the unity constraint. The dependent variable  X  k is written as follows: Definition 2 A hyperbolic equilibrium point is called a (asymptotically) stable equilibrium point (SEP) if all the eigenvalues of its corresponding Jacobian have negative real part. Conversely, it is an unstable equilibrium point if some eigenvalues have a positive real part.

An equilibrium point is called a type-k equilibrium point if its corresponding Jacobian has exact k eigenvalues with positive real part. The stable ( W s (  X  x ) )and unstable ( W u (  X  x ) ) manifolds of an equilibrium point, say  X  fined as:
The task of finding multiple local maxima on the log-likelihood surface is transformed into the task of finding multiple stable equilibrium poi nts on its corresponding gra-dient system. The advantage of our approach is that this transformation into the corresponding dynamical system will yield more knowledge about the various dynamic and geometric characteristics of t he original surface and leads to the development a powerful method for finding improved solutions. In this paper, we are particularly interested in the properties of the local maxima and their one-to-one cor-respondence to the stable equ ilibrium points. To compre-hend the transformation, we need to define energy func-tion . A smooth function V (  X  ): s  X  s satisfying  X  V ( X ( X  ,t )) &lt; 0 ,  X  x/  X  X  set of equilibrium points (E) and t  X  + is termed as energy function.
 Theorem 3.1 [3]: f ( X ) is a energy function for the gradi-ent system (15).
 Definition 3 A type-1 equilibrium point x d (k=1) on the practical stability boundary of a stable e quilibrium point x s is called a decomposition point.
 Definition 4 The practical stability region of a stable equi-librium point x s of a nonlinear dynamical system (15), de-noted by A p ( x s ) and is the interior of closure of the stability region A ( x s ) which is given by : The boundary of practical stability region is called the practical st ability boundary of x s and will be denoted by  X  X  p ( x s ) . Theorem 3.2 asserts that the practical stability boundary is contained in the union of the closure of the sta-ble manifolds of all the decomposition points on the practi-cal stability boundary. Hence, if the decomposition points can be identified, then an explic it characterization of the practical stability boundary can be established using (21). This theorem gives an explicit description of the geometri-cal and dynamical structure of the practical stability bound-ary.
 Theorem 3.2 (Characterization of practical stability boundary)[7]: Consider a negative gradient system de-scribed by (15). Let  X  i , i=1,2,... be the decomposition points on the prac tical stab ility boundary  X  X  p ( x s ) stable equilibrium point, say x s .Then stability boundary. Points highlighted on the stability boundary (  X 
The dotted lines indicate the convergence of the EM algorithm. d space and their corresponding log-likelihood values.
Our approach takes advantage of TRUST-TECH (TRansformation Under STability-reTaining Equilibra CHaracterization) to compute neighborhood local maxima on likelihood surface using stability regions. Originally, the basic idea of our algorithm was to find decomposition points on the practical stability boundary. Since, each decomposition point connect s two local maxima uniquely, it is important to obtain the saddle points from the given local maximum and then move to the next local maximum through this decomposition point [14]. Though, this procedure gives a guarantee that the local maximum is not revisited, the computational expense for tracing the stability boundary and identifying the decomposition point is high compared to the cost of applying the EM algorithm directly using the exit point without considering the decomposition point. One can use the saddle point tracing procedure described in [14] for applications where the local methods like EM are much expensive.
Our framework consists mainly of two phases which are repeated in the promising subs paces of the parameter search space. It is more effective to use our algorithm at only these promising subspaces which are usually obtained by stochas-tic global methods. The first phase is the local phase (or the EM phase) where the promising solutions are refined to the corresponding locally optimal parameter set. The second phase which is the main contribution of this paper, is the stability region phase, where the exit points are computed and the neighborhood solutions are systematically explored through these exit points. Fig. 2 shows the different steps of our algorithm both in (a) the parameter space and (b) the function space.

This approach can be treated as a hybrid between global methods for initialization and the EM algorithm which gives the local maxima. One of the main advantages of our ap-proach is that it searches th e parameter space more deter-ministically. This approach differs from traditional local methods by computing multiple local solutions in the neigh-borhood region. This also enhances user flexibility by al-lowing the users to choose between different sets of good clusterings. Though global methods give promising sub-spaces, it is important to explore this subspace more thor-oughly especially in problems like parameter estimation. Algorithm 1 describes our approach.

In order to escape out of this local maximum, our meth-ods needs to compute certain promising directions based on the local behaviour of the function. One can realize that generating these promising directions is one of the impor-tant aspects of our algorithm. Surprisingly, choosing ran-dom directions to move out of the local maximum works Algorithm 1 Stability Region based EM Algorithm Input: Parameters  X  ,Data X , tolerance  X  ,Step S p Output:  X  MLE Algorithm: Apply global method and store the q promising solutions
 X  while  X  init =  X  do end while  X  well for this problem. One might also use other direc-tions like eigenvectors of the Hessian or incorporate some domain-specific knowledge (like information about priors, approximate location of clust er means, user preferences on the final clusters) depending on the application that they are working on and the level of computational expense that they can afford. We used random directions in our work because they are very cheap to comput e. Once the promising direc-tions are generated, exit poi nts are computed along these directions. Exit points are points of intersection between any given direction and the practical stability boundary of that local maximum along that particular direction. If the stability boundary is not encountered along a given direc-tion, it is very likely that one might not find any new local maximum in that direction. With a new initial guess in the vicinity of the exit points, EM algorithm is applied again to obtain a new local maximum. Our program is implemented in MATLAB and runs on Pentium IV 2.8 GHz machine. The main procedure imple-mented is TT EM described in Algorithm 2. The algo-rithm takes the mixture data and the initial set of parame-ters as input along with step size for moving out and tol-erance for convergence in th e EM algorithm. It returns the set of parameters that correspond to the Tier-1 neighbor-ing local optimal solutions. The procedure eval returns the log-likelihood score given by (4). The Gen Dir procedure generates promising directions from the local maxima. Exit points are obtained along these generated directions. The procedure update moves the current parameter to the next parameter set along a given k th direction Dir [ k ] .Someof the directions might have one of the following two prob-lems: (i) Exit points might not be obtained in these direc-tions. (ii) Even if the exit point is obtained it might con-verge to a less promising solution. If the exit points are not found along these directions, search will be terminated af-ter Eval MAX number of evaluations. For all exit points that are successfully found, EM procedure is applied and all the corresponding neighborhood set of parameters are stored in the Params [] 1 . Since, different parameters will be of different range, care must be taken while multiplying with the step sizes. It is important to use the current esti-mates to get an approximation of the step size with which one should move out along each parameter in the search space. Finally, the solution with the highest likelihood score amongst the original set of parameters and the Tier-1 solu-tions is returned.
 Algorithm 2 Params[ ] TT EM ( P set, Data, T ol, Step ) Val = eval ( Pset ) Dir []= Gen Dir ( Pset )
Eval MAX = 500 for k =1 to size ( Dir ) do end for
Return max ( eval ( Params []))
Our algorithm has been tested on both synthetic and real datasets. The initial values for the centers and the covari-ances were chosen uniformly random. Uniform priors were chosen for initializing the components. For real datasets, the centers were chosen randomly from the sample points.
Figure 4. Graph showing likelihood vs Eval-uations. A corresponds to the original local maximum (L=-3235.0). B corresponds to the exit point (L=-3676.1). C corresponds to the new initial point in the neighboring stability region (L=-3657.3) after moving out by  X   X . D corresponds to the new local maximum (L=-3078.7). A simple synthetic data with 40 samples and 5 spherical Gaussian components was generated and tested with our al-gorithm. Priors were uniform and the standard deviation was 0.01. The centers for the five components are given as follows:  X  1 =[0 . 30 . 3] T ,  X  2 =[0 . 50 . 5] T ,  X  3 =[0  X  4 =[0 . 30 . 7] T and  X  5 =[0 . 70 . 3] T .

The second dataset was that of a diagonal covariance case containing n = 900 data points. The data generated from a two-dimensional, three-component Gaussian mix-ture distribution with mean vectors at [0  X  2] T , [0 0] T and same diagonal covariance matrix with values 2 and 0.2 along the diagonal [16]. All the three mixtures have uni-form priors. Fig. 3 shows various stages of our algorithm and demonstrates how the clusters obtained from existing algorithms are improved using our algorithm. The initial clusters obtained are of low quality because of the poor ini-tial set of parameters. Our algorithm takes these clusters and applies the stability region step and the EM step simul-taneously to obtain the final result. Fig. 4 shows the value of the log-likelihood during t he stability region phase and the EM iterations.

In the third synthetic dataset, a more complicated over-lapping Gaussian mixtures are considered [6]. The param-eters are as follows:  X  1 =  X  2 =[  X  4  X  4] T ,  X  3 =[22] and  X  4 =[  X  1  X  6] T .  X  1 =  X  2 =  X  3 =0 . 3 and  X  4 =0
Two real datasets obtained from the UCI Machine Learn-ing repository [1] were also used for testing the performance of our algorithm. Most widely used Iris data with 150 sam-ples, 3 classes and 4 features was used. Wine data set with 178 samples was also used for testing. Wine data had 3 classes and 13 features. For these real data sets, the class labels were deleted thus treating it as unsupervised learning problem. Table 2 summarizes our results over 100 runs. The mean and the standard deviations of the log-likelihood val-ues are reported. The traditional EM algorithm with random starts is compared against our algorithm on both synthetic and real data sets. Our algorithm not only obtains higher likelihood value but also produces it with high confidence. The low standard deviation of our results indicates the ro-bustness of obtaining the global maximum. In the case of the wine data, the improvements with our algorithm are not much significant compared to the other datasets. This might be due to the fact that the dataset might not have Gaussian components. Our method assumes that the underlying dis-tribution of the data is mixture of Gaussians. Table 3 gives the results of TRUST-TECH compared with other methods like split and merge EM and k-means+EM proposed in the literature.

Table 3. Comparison of TRUST-TECH-EM with other
It will be effective to use our algorithm for those solu-tions that appear to be promising. Due to the nature of the problem, it is very likely that the nearby solutions surround-ing the existing solution will be more promising. One of the primary advantages of our method is that it can be used along with other popular methods available and improve the quality of the existing solutions. In clustering problems, it is an added advantage to perform refinement of the final clusters obtained. Most of the focus in the literature was on new methods for initializatio n or new clustering tech-niques which often do not take advantage of the existing results and completely start the clustering procedure  X  from scratch  X . Though shown only for the case of multivariate Gaussian mixtures, our technique can be effectively applied to any parametric finite mixture model.

Table 4 summarizes the average number of iterations taken by the EM algorithm for the convergence to the lo-cal optimal solution. We can see that the most promising solution produced by our TRUST-TECH methodology con-verges much faster. In other words, our method can effec-tively take advantage of the fact that the convergence of the EM algorithm is much faster for high quality solutions. This is an inherent property of the EM algorithm when applied to the mixture modeling problem. We exploit this property of the EM for improving the efficiency of our algorithm. Hence, for obtaining the Tier-1 solutions using our algo-rithm, the threshold for the number of iterations can be sig-nificantly lowered.

Table 4. Number of iterations taken for the convergence
A novel stability region based EM algorithm has been in-troduced for estimating the parameters of mixture models. The EM phase and the stability region phase are applied al-ternatively in the context of the well-studied mixture model parameter estimation problem. The concept of stability re-gion helps us to understand the topology of the original log-likelihood surface. Our method computes the neighborhood local maxima on likelihood surface using stability regions of the corresponding nonlinear dynamical system. The al-gorithm has been tested successfully on various synthetic and real datasets and the improvements in the performance are clearly manifested. Some properties of the EM algo-rithm about the rate of convergence have been exploited ef-ficiently.

Our algorithm can be easily extended to popularly used k-means clustering technique. In the future, we plan to work on applying these stability region based methods for other widely used EM related parameter estimation problems like training Hidden Markov Models, Mixture of Factor Analyz-ers, Probabilistic Principal C omponent Analysis, Bayesian Networks etc. We would also plan to extend our technique to Markov Chain Monte Carlo strategies like Gibbs sam-pling for the estimation of mixture models.
 [1] C.L. Blake and C.J. Merz. UCI repos-[2] C. Carson, S. Belongie, H. Greenspan, and J. Malik. [3] H.D. Chiang and C.C. Chu. A systematic search [4] A. P. Demspter, N. A. Laird, and D. B. Rubin. Max-[5] G. Elidan, M. Ninio, N. Friedman, and D. Schuur-[6] M. Figueiredo and A.K. Jain. Unsupervised learning [7] J. Lee and H.D. Chiang. A dynamical trajectory-based [8] J.Q.Li. Estimation of Mixture Models . PhD thesis, [9] G. McLachlan and T. Krishnan. The EM Algorithm [10] G. J. McLachlan and K. E. Basford. Mixture mod-[11] R. M. Neal and G. E. Hinton. A new view of the EM [12] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. [13] F. Pernkopf and D. Bouchaffra. Genetic-based EM al-[14] C. K. Reddy and H.D. C hiang. A stability boundary [15] S. J. Roberts, D. Husmeier, I. Rezek, and W. Penny. [16] N. Ueda and R. Nakano. Deterministic annealing EM [17] N. Ueda, R. Nakano, Z. Ghahramani, and G.E. Hin-[18] J. J. Verbeek, N. Vlassis, and B. Krose. Efficient [19] L. Xu and M. I. Jordan. On convergence properties [20] B. Zhang, C. Zhang, and X. Yi. Competitive EM algo-
