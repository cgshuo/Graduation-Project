 GRAVIR-INRIA-CNRS, 655 Avenue de l X  X urope, Montbonnot 38330, France We consider the problem of estimating and tracking the 3D configurations of complex articulated objects from monoc-ular images, e.g. for applications requiring 3D human body pose or hand gesture analysis. There are two main schools of thought on this. Model-based approaches presuppose an explicitly known parametric body model, and estimate the pose by either: (i) directly inverting the kinemat-ics, which requires known image positions for each body part (Taylor, 2000); or (ii) numerically optimizing some form of model-image correspondence metric over the pose variables, using a forward rendering model to predict the images, which is expensive and requires a good initial-ization, and the problem always has many local minima (Sminchisescu &amp; Triggs, 2003). An important sub-case is model-based tracking , which focuses on tracking the pose estimate from one time step to the next starting from a known initialization, based on an approximate dynamical model (Bregler &amp; Malik, 1998, Sidenbladh et al., 2002). In contrast, learning based approaches try to avoid the need for accurate 3D modelling and rendering, and to capitalize on the fact that the set of typical human poses is far smaller than the set of kinematically possible ones, by estimating (learning) a model that directly recovers pose estimates from observable image quantities (Grauman et al., 2003). In particular, example based methods explicitly store a set of training examples whose 3D poses are known, and estimate pose by searching for training image(s) sim-ilar to the given input image, and interpolating from their poses (Athitsos &amp; Sclaroff, 2003, Stenger et al., 2003, Mori &amp; Malik, 2002, Shakhnarovich et al., 2003). In this paper we take a learning based approach, but in-stead of explicitly storing and searching for similar training examples, we use sparse Bayesian nonlinear regression to distill a large training database into a single compact model that generalizes well to unseen examples. We regress the current pose (body joint angles) against both image descrip-tors (silhouette shape) and a pose estimate computed from previous poses using a learned dynamical model. High di-mensionality and the intrinsic ambiguity in recovering pose from monocular observations makes the regression nontriv-ial. Our algorithm can be related to probabilistic tracking, but we eliminate the need for: (i) an exact body model that must be projected to predict an image; and (ii) a pre-defined error model to evaluate the likelihood of the ob-served image signal given this projection. Instead, pose is estimated directly, by regressing it against a dynamics-based prediction and an observed shape descriptor vector. Regressing on shape descriptors allows appearance varia-tions to be learned automatically, enabling us to work with a simple generic articular skeleton model; while including an estimate of the pose in the regression allows the method to overcome the inherent many-to-one projection ambigui-ties present in monocular image observations.
 Our strategy makes good use of the sparsity and generaliza-tion properties of our nonlinear regressor, which is a variant of the Relevance Vector Machine (RVM) (Tipping, 2000). RVM X  X  have been used, e.g. , to build kernel regressors for 2D displacement updates in correlation-based patch track-ing (Williams et al., 2003). Human pose recovery is sig-nificantly harder  X  more ill-conditioned and nonlinear, and much higher dimensional  X  but by selecting a suffi-can still obtain enough information for successful regres-sion (Agarwal &amp; Triggs, 2004a).
 Our motion capture based training data models each joint as a spherical one, so formally, we represent 3D body pose by 55-D vectors x including 3 joint angles for each of the 18 major body joints. The input images are reduced to 100-D observation vectors z that robustly encode the shape of a human image silhouette. Given a temporal sequence of observations z ing sequence of pose vectors x At each time step, we obtain an approximate preliminary pose estimate  X  x ing a dynamical model learned by linear least squares re-gression. We then update this to take account of the ob-servations z  X  x = r (  X  x , z )  X  learned from a set of labelled training examples { ( z ear combination r ( x , z )  X  P scalar basis functions {  X  tiated Gaussian kernels). The learned regressor is regular in the sense that the weight vectors a control over-fitting, and sparse in the sense that many of them are zero. Sparsity occurs because the RVM actively selects only the  X  X ost relevant X  basis functions  X  the ones that really need to have nonzero coefficients to complete the regression successfully.
 Previous work: There is a good deal of prior work on hu-man pose analysis, but relatively little on directly learning 3D pose from image measurements. (Brand, 1999) models a dynamical manifold of human body configurations with a Hidden Markov Model and learns using entropy minimiza-tion. (Athitsos &amp; Sclaroff, 2000) learn a perceptron map-ping between the appearance and parameter spaces. Human pose is hard to ground truth, so most papers in this area use only heuristic visual inspection to judge their results. How-ever, the interpolated-k -nearest-neighbor learning method of (Shakhnarovich et al., 2003) used a human model ren-dering package (P OSER from Curious Labs) to synthesize ground-truthed training and test images of 13 degree of freedom upper body poses with a limited (  X  40  X  ) set of ran-dom torso movements and view points, obtaining RMS es-timation errors of about 20  X  per d.o.f. In comparison, our regression algorithm estimates full 54 d.o.f. body pose and orientation  X  a problem whose high dimensionality would really stretch the capacity of an example based method such as (Shakhnarovich et al., 2003)  X  with mean errors of only of training and test images from different viewpoints, but rather than using random synthetic poses, we used poses taken from real human motion capture sequences. Our re-sults thus relate to real poses and we also capture the dy-namics of typical human motions for temporal consistency. The motion capture data was taken from the public website www.ict.usc.edu/graphics/animWeb/humanoid . (Howe et al., 1999) developed a Bayesian learning frame-work to recover 3D pose from known image locations of body joint centres, based on a training set of pose-centre pairs obtained from resynthesized motion capture data. (Mori &amp; Malik, 2002) estimate the centres using shape con-text image matching against a set of training images with pre-labelled centres, then reconstruct 3D pose using the al-gorithm of (Taylor, 2000). Rather than working indirectly via joint centres, we chose to estimate pose directly from the underlying image descriptors, as we feel that this is likely to prove both more accurate and more robust, pro-viding a generic framework for estimating and tracking any prespecified set of parameters from image observations. (Pavlovic et al., 2000, Ormoneit et al., 2000) learn dynami-cal models for specific human motions. Particle filters and MCMC methods have widely been used in probabilistic tracking frameworks e.g . (Sidenbladh et al., 2002). Most of the previous learning based methods for human track-ing take a generative, model based approach, whereas our approach is essentially discriminative. To improve resistance to segmentation errors and occlu-sions, we use a robust representation for our image ob-servations. Of the many different image descriptors that could be used for human pose estimation, and in line with (Brand, 1999, Athitsos &amp; Sclaroff, 2000), we have chosen to base our system on image silhouettes. There are two main problems with silhouettes: ( i ) Artifacts such as shadow attachment and poor background segmentation tend to distort their local form. This often causes problems when global descriptors such as shape moments are used, as in (Brand, 1999, Athitsos &amp; Sclaroff, 2000), because each lo-cal error pollutes every component of the descriptor. To be robust, shape descriptors must have good spatial local-ity. ( ii ) Silhouettes make several discrete and continuous degrees of freedom invisible or poorly visible. It is diffi-cult to tell frontal views from back ones, whether a person seen from the side is stepping with the left leg or the right one, and what are the exact poses of arms or hands that fall within (are  X  X ccluded X  by) the torso X  X  silhouette (see fig. 1). These factors limit the performance attainable from silhouette-based methods.
 Histograms of edge information are a good way to encode local shape robustly (Lowe, 1999). Here, we use shape con-texts (histograms of local edge pixels into log-polar bins) (Belongie et al., 2002) to encode silhouette shape quasi-locally over a range of scales, making use of their locality properties and capability to encode approximate spatial po-sition on the silhouette  X  see (Agarwal &amp; Triggs, 2004a). Unlike Belognie et al , we use quite small image regions (roughly the size of a limb) to compute our shape contexts, and for increased locality, we normalize each shape con-text histogram only by the number of points in its region. This is essential for robustness against occlusions, shad-ows, etc . The shape context distributions of all edge points on a silhouette are reduced to 100-D histograms by vec-tor quantizing the 60-D shape context space using Gaussian weights to vote softly into the few histogram centres nearest to the contexts. This softening allows us to compare his-tograms using simple Euclidean distance rather than, say, Earth Movers Distance (Rubner et al., 1998). Each image observation (silhouette) is thus finally reduced to a 100-D quantized-distribution-of-shape-context vector, giving rea-sonably good robustness to occlusions and to local silhou-ette segmentation failures. The 3D pose can only be observed indirectly via ambiguous and noisy image measurements, so it is appropriate to start by considering the Bayesian tracking framework in which our knowledge about the state (pose) x tions up to time t is represented by a probability distribu-tion, the posterior state density p ( x Given an image observation z the corresponding pose x x servation model that predicts z x . Unfortunately, when tracking objects as complicated as the human body, the observations depend on a great many factors that are difficult to control, ranging from lighting and background to body shape and clothing style and tex-ture, so any hand-built observation model is necessarily a gross oversimplification.
 One way around this would be to learn the generative model p ( z | x ) from examples, then to work backwards via its Ja-cobian to get a linearized state update, as in the extended Kalman filter. However, this approach is somewhat indirect, and it may waste a considerable amount of effort modelling appearance details that are irrelevant for predicting pose. Instead, we prefer to learn a  X  X iscriminative X  (diagnostic or anti-causal) model p ( x | z ) for the pose x given the obser-vations z  X  c.f . the difference between generative and dis-criminative classification, and the regression based trackers of (Jurie &amp; Dhome, 2002, Williams et al., 2003). Similarly, in the context of maximum likelihood pose estimation, we would prefer to learn a  X  X iagnostic X  regressor x = x ( z ) i.e . a point estimator for the most likely state x given the observations z , not a generative predictor z = z ( x ) . Unfortunately, this brings up a second problem. In monocu-lar human pose reconstruction, image projection suppresses most of the depth (camera-object distance) information, so the state-to-observation mapping is always many-to-one. In fact, even when the labelled image positions of the pro-jected joint centers are known exactly, there may still be some hundreds or thousands of kinematically possible 3D poses, linked by  X  X inematic flipping X  ambiguities ( c.f . e.g . (Sminchisescu &amp; Triggs, 2003)). Using silhouettes as im-age observations allows relatively robust feature extraction, but induces further ambiguities owing to the lack of limb labelling: it can be hard to tell back views from front ones, and which leg or arm is which in side views. These ambi-guities make learning to regress x from z difficult because the true mapping is actually multi-valued. A single-valued least squares regressor will tend to either zig-zag erratically between different training poses, or (if highly damped) to reproduce their arithmetic mean (Bishop, 1995), neither of which is desirable. Introducing a robustified cost func-tion might help the regressor to focus on just one branch of the solution space so that different regressors could be learned for different branches, but applying this in a heav-ily branched 54-D target space is not likely to be straight-forward.
 To reduce the ambiguity, we can take advantage of the fact that we are tracking and work incrementally from the pre-vious state x assumption of discriminative tracking is that state informa-tion from the current observation is independent of state in-formation from previous states (dynamics): The pose reconstruction ambiguity is reflected in the fact that the likelihood p ( x it is obtained by using Bayes X  rule to invert the many-to-one generative model p ( z | x ) ). Probabilistically this is fine, but to handle it in the context of point estima-tion / maximum likelihood tracking, we would in princi-ple need to learn a multi-valued regressor for x then fuse each of the resulting pose estimates with the esti-mate from the dynamics-based regressor x stead, we adopt the working hypothesis that given the dy-namics based estimate  X  or any other rough initial esti-mate  X  x of the observation-based estimates is at all likely a poste-riori. Thus, we can use the  X  x solution X  for the observation-based reconstruction x Formally this gives a regressor x serves mainly as a key to select which branch of the pose-from-observation space to use, not as a useful prediction of x be nonlinear and well-localized in  X  x further, if  X  x a dynamical model), we can use a single regressor of the same form, x pendence on  X  x structing an observation-estimate x with  X  x In this section we detail the regression methods that we use for recovering 3D human body pose. Poses are represented as real vectors x  X  R m . For a full body model, these are 55-dimensional, including 3 joint angles for each of the 18 the true human pose degrees of freedom, but it corresponds to our motion capture based training data, and our regres-sion methods handle such redundant output representations without problems. 4.1. Dynamical (Prediction) Model Human body dynamics can be modelled fairly accurately with a second order linear autoregressive process, x  X  x der dynamical estimate of x ( c.f . e.g . (Agarwal &amp; Triggs, 2004b)). To ensure dynamical stability and avoid over-fitting, we actually learn the autore-gression for  X  x where I is the m  X  m identity matrix. We estimate A and B by regularized least squares regression against x mizing k k 2 with the regularization parameter  X  set by cross-validation to give a well-damped solution with good generalization. 4.2. Likelihood (Correction) Model Now consider the observation model. As discussed above, the underlying density p ( x ing to the pervasive ambiguities in reconstructing 3D pose from monocular images, so no single-valued regression function x for x managed to learn moderately successful pose regressors x = x ( z ) , they tend to systematically underestimate pose angles (owing to effective averaging over several possible solutions) and to be subject to occasional glitches where the wrong solution is selected (Agarwal &amp; Triggs, 2004a). Although such regressors can be combined with dynamics-based predictors, this only smooths the results: it cannot remove the underlying underestimation and  X  X litchiness X . In default of a reliable method for multi-valued regression, we include a non-linear dependence on  X  x observation-based regressor. Our full regression model also includes an explicit  X  x tribution of the dynamics to the overall state estimate, so the final model becomes x error to be minimized, and:  X  x t  X  C  X  x t + Here, {  X  basis functions for the regression, and d sponding R m -valued weight vectors. For compactness, we gather these into an R p -valued feature vector f ( x , z )  X  (  X  1 ( x , z ) , . . . ,  X  p ( x , z )) &gt; ( d 1 , . . . , d p ) instantiated-kernel bases of the form: where ( x independent Gaussian) kernels on x -space and z -space, K Building the basis from Gaussians based at training exam-ples in joint ( x , z ) space forces examples to become rel-evant only if they have similar estimated poses and simi-lar image silhouettes. It is essential to choose the relative widths of the kernels appropriately. In particular, if the x -kernel is chosen too wide, the method tends to average over (or zig-zag between) several alternative pose-from-observation solutions, which defeats the purpose of includ-ing  X  x in the observation regression. On the other hand, by locality, the observation-based state corrections are effec-tively  X  X witched off X  whenever the state happens to wander 0. Initialize A with ridge regression. Initialize the run-ning scale estimates a scale = k a k for the components or vectors a . 1. Approximate the  X  log k a k penalty terms with  X  X uadratic bridges X   X  ( a /a scale ) 2 + const (the gradients match at a scale ); 2. Solve the resulting linear least squares problem in A ; 3. Remove any components a that have become zero, up-date the scale estimates a scale = k a k , and continue from 1 until convergence.
 too far from the observed training examples x x -kernel is set too narrow, observation information is only incorporated sporadically and mistracking can easily occur. Fig. 2 illustrates this effect, for an x -kernel a factor of 10 narrower than the optimum. The method initially seemed to be sensitive to the kernel width parameters, but after select-ing optimal parameters by cross-validation on an indepen-dent motion sequence we observed accurate performance over a sufficiently wide range of both the kernel widths: a tolerance factor of  X  2 on  X  The coefficient matrix C in (3) plays an interesting role. Setting C  X  I forces the correction model to act as a differ-ential update on  X  x largely observation-based state estimates with only a la-tent dependence on the dynamics. An intermediate setting, however, turns out to give best overall results. Damping the dynamics slightly ensures stability and controls drift  X  in particular, preventing the observations from disastrously  X  X witching off X  because the state has drifted too far from the training examples  X  while still allowing a reasonable amount of dynamical smoothing. Usually we estimate the full (regularized) matrix C from the training data, but to get an idea of the trade-offs involved, we also studied the effect of explicitly setting C = s I for s  X  [0 , 1] . We find that a small amount of damping, s overall, maintaining a good lock on the observations with-out losing too much dynamical smoothing (see fig. 3.) This simple heuristic setting gives very similar results to the full model obtained by learning an unconstrained C . 4.3. Relevance Vector Regression The regressor is learned using a Relevance Vector Machine (Tipping, 2001). This sparse Bayesian approach gives sim-ilar results to methods such as damped least squares / ridge regression, but selects a much more economical set of ac-tive training examples for the kernel basis. We have also tested a number of other training methods (including ridge regression) and bases (including the linear basis). These are not reported here, but the results turn out to be relatively in-sensitive to the training method used, with the kernel bases having a slight edge.
 RVM X  X  take either individual parameters or groups of pa-rameters a (in our case, columns of A ), and impose  X  log k a k regularizers or priors on each group. Rather than using the (Tipping, 2000) algorithm for training, we use a continuation method based on successively approxi-mating the  X  log k a k regularizers with quadratic  X  X ridges X   X  ( k a k /a scale ) 2 chosen to match the prior gradient at a running scale estimate for a . The bridging functions allow parameters to pass through zero if they need to, without too much risk of premature trapping at zero. The algorithm is sketched in fig. 4. Regularizing over whole columns (rather than individual components) of A ensures a sparse expan-sion, as it swaps entire basis functions in or out. We conducted experiments using a database of motion cap-ture data for an m = 54 d.o.f. body model (3 angles for each of 18 joints, including body orientation w.r.t. the camera). We report mean (over all angles) RMS (over time) absolute difference errors between the true and estimated joint angle vectors, in degrees: The training silhouettes were created by using Curious Labs X  P OSER to re-render poses obtained from real human motion capture data, and reduced to 100-D shape descrip-tor vectors as in  X  2. We used 8 different sequences totalling about 2000 instantaneous poses for training, and another two sequences of about 400 points each as validation and test sets.
 The dynamical model is learned from the training data ex-actly as described in  X  4.1, but when training the obser-vation model, we find that its coverage and capture ra-dius can be increased by including a wider selection of  X  x values than those produced by the dynamical predictions. Hence, we train the model x = x tion of  X  X bserved X  samples (  X  x (2)) and artificial samples generated by Gaussian sampling N ( x t ,  X ) around the training state x t . The observation corresponding to x based part of the regressor to rely mainly on the observa-tions, i.e . on recovering x z , using  X  x choose. The covariance matrix  X  is chosen to reflect the local scatter of the training examples, with a larger variance along the tangent to the trajectory at each point to ensure that phase lag between the state estimate and the true state is reliably detected and corrected.
 Fig. 5 illustrates the relative contributions of the different terms in our model by plotting tracking results for a mo-tion capture test sequence in which the subject walks in a decreasing spiral. (This sequence was not included in the training set, although similar ones were). The purely dy-namical model (2) provides good estimates for a few time steps, but gradually damps and drifts out of phase. (Such damped oscillations are characteristic of second order linear autoregressive dynamics, trained with enough regulariza-tion to ensure model stability). At the other extreme, using observations alone without any temporal information ( i.e . C = 0 and K x = 1 ) provides noisy reconstructions with occasional  X  X litches X  due to incorrect reconstructions. Pan-els (c),(f) show that joint regression on both dynamics and observations gives smoother and stabler tracking. There is still some residual misestimation of the hip angle in (c) at around t =140 and t =380 . Here, the subject is walking di-rectly towards the camera (heading angle  X   X  0  X  ), so the only cue for hip angle is the position of the corresponding foot, which is sometimes occluded by the opposite leg. Even humans have difficulty estimating this angle from the sil-houette at these points.
 Fig. 6 shows some silhouettes and corresponding maximum likelihood pose reconstructions, for the same test sequence. The 3D poses for the first two time steps were set by hand to initialize the dynamical predictions. The average RMS esti-mation error over all joints using the RVM regressor in this the same basis gives similar errors, but has much higher storage requirements. The Gaussian RVM gives a sparse regressor for (3) involving only 348 of the 1927 training ex-amples, thus allowing a significant reduction in the amount of training data that needs to be stored. Reconstruction re-sults on a test video sequence are shown in fig. 7. The re-construction quality demonstrates the generalized dynami-cal behavior captured by the model as well as the method X  X  robustness to imperfect visual features, as a naive back-ground subtraction method was used to extract somewhat imperfect silhouettes from the images.
 In terms of computational time, the final RVM regressor al-ready runs in real time in Matlab. Silhouette extraction and shape-context descriptor computations are currently done offline, but would be doable online in real time. The (of-fline) learning process takes about 26 min for the RVM with  X  2000 data points, and about the same again for (Matlab) Shape Context extraction and clustering.
 The method is reasonably robust to initialization errors. Al-though the results shown in figs. 5 and 6 were obtained by initializing from ground truth, we also tested the effects of automatic (and hence potentially incorrect) initialization. In an experiment in which the tracker was automatically ini-tialized at each time step in turn using the pure observation model, then tracked forwards and backwards using the dy-namical tracker, the initialization lead to successful tracking in 84% of the cases. The failures were the  X  X litches X , where the observation model gave completely incorrect initializa-tions. We have presented a method that recovers 3D human body pose from sequences of monocular silhouettes by direct nonlinear regression of joint-angles against histogram-of-shape-context silhouette shape descriptors and dynamics based pose estimates. No 3D body model or labelling of image positions of body parts is required. Regressing the pose jointly on image observations and previous poses al-lows the intrinsic ambiguity of the pose-from-monocular-observations problem to be overcome, thus producing sta-ble, temporally consistent tracking. We use a kernel-based Relevance Vector Machine for the regression, thus selecting a sparse set of relevant training examples as exemplars. The method shows promising results on tracking unseen video sequences, giving an average RMS error of 4 . 1  X  per body-joint-angle on real motion capture data.
 Future work: We plan to investigate the extension of our regression based system to a complete discriminative Bayesian tracking framework, including multiple hypothe-ses and robust error models. We would also like to include richer features, such as internal edges in addition to silhou-ette boundaries to reduce susceptibility to poor image seg-mentation.
 This work was supported by the European Union projects VIBES and LAVA.
