 School of Information, Computer and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University, Pathumthani, Thailand 1. Introduction
Nowadays publishing news documents on the Internet has become a common tool for mass commu-nication. Due to the large amount of online news documents, effective management of these resources has become a challenging goal for researchers in the field of information retrieval, text categorization and text mining. Towards this, the most common functions include clustering and classifying news doc-uments [4,7,26,35,39], extracting semantic relationships between entities from news documents [37,40, 41], detecting event from news stories [3,18,28,34], and discovering meaningful relations among news documents [20,30,38]. Among these tasks, discovering news relations has recently been focused among several studies. Some works have applied association rule mining to find relations among documents due to its performance and scalability [21,32]. While given N documents, cluster-based association discov-ery requires the calculation of all possible combination of documents, i.e., the complexity of O (2 N ) ,the association-rule-mining approach utilizes a criterion of minimum support to control the searching space, resulting in the complexity of O ( N L ) ,where L is the length of the longest pattern. However, an in-evitable issue of association rule mining is triggered by the large number of rules generated [5,22,23,27, 31]. In the past, several works introduced various approaches to solve the problem of these tremendous association rules, which may be classified into filtering, constraint-based mining and ranking.
Up to present, most previous works have applied single objective measure (such as support, confi-dence or lift) or single subjective measure (such as preference, personal value or cultural value) or their combinations, to filter, constrain, or rank the discovered rules. However, they still focus on a uniform function for the whole ranking interval. Intuitively, only a single objective function for the whole range of candidates may not be enough to prioritize the rules. It is worth exploring multiple objective functions for rule prioritization. Towards such issues, this paper presents a method to selectively use different ob-jective functions (association measures) for different ranking regions to improve the ranking mechanism. So-called region-based ranking, the proposed approach, sets a specific weight for each different region (range) of the relation strength in order to maximize classification performance. This work also deals with two main issues, i.e., (1) how to determine suitable points of weight scheme change in order to form regions, and (2) which weight scheme we should assign to each region. In addition, when the domain is changed to other areas such as medical, environmental, and industrial, it does not affect the proposed method. Since the method applied an association rule mining technique to discover news relations, the repeated occurrence of terms in the documents is the primary key to form the news relations. That is, the proposed method is domain-independent in the sense that the method can achieve equivalent accu-racy in another domain without any more hand-labeled training examples and can be applied in many applications.

In the remainder of this paper, related works are provided in Section 2. Section 3 introduces a frame-work of news relation discovery. Section 4 describes news relation types. A formalism of association-based approach is presented in Section 5. In Section 6, weighted non-directional association measures used for the ranking mechanism and a region-based ranking approach are proposed. Details of an eval-uation method in this work, including preparation of our evaluation dataset, and criteria of evaluation, are explained in Section 7. Section 8 describes experimental settings and experimental results with their discussions. Finally, conclusions and future works are made in Section 9. 2. Related works
Association rule mining is a well-known data mining technique for knowledge discovery, proposed by Agrawal [2]. Its primary purpose is to find frequent patterns in the form of rules from a database. The frequent patterns describe a repeated appearance of items in a transaction. Recently, association rule mining or its derivatives have been applied in finding relations among documents [32,33]. By en-coding documents as items, and terms in the documents as transactions, mined are a number of frequent patterns, each of which represents a set of documents that share common terms more than a threshold, called support. Thereafter, as a further step, a set of frequent rules can be found based on these frequent patterns with another threshold, namely confidence. Sriphaew and Theeramunkong [32,33] proposed an approach to mine relations in scientific research publications by using association rule mining with support-confidence framework by extending a concept of traditional association rule mining to mine frequent itemsets on the database with real-valued instead of only item existences. This method could discover a set of topically similar documents as high quality relations. In this work, a novel evaluation method is presented based on an evaluation by benchmark citation matrix, using referencing the ACM database.

Although an association rule mining method provides an efficient way of document relation discovery, a huge set of extracted rules or meaningless relations may be generated. Many research works have been performed on topics related to the solutions of these tremendous association rules, including filtering, constraint-based mining, and ranking approaches.

The first approach filters out meaningless rules from a large number of generated rules using a thresh-old or criterion. A number of works provide a mechanism to detect duplicated rules or redundant rules and then sift meaningless rules [5,16,25]. An earlier work by [25] defined a so-called direction setting (DS) concept and applied  X  2 test, for independence assessment, to split a set of discovered rules into DS rules and non-DS rules, where the former rules give a summary of the behavior of the discov-ered associations while the latter ones provide additional details. To give more precise and probabilistic quantification of the interaction among subrules, Jaroszewicz and Simovici [16] presented a Maximum Entropy approach to express interestingness of a rule. Ashrafi et al. [5] proposed another method to eliminate redundant rules under the consideration of both fixed antecedent rules and fixed consequence rules.

As for the second approach, instead of generating a large number of rules and then filtering uninter-esting rules out, several works tried to establish a set of constraints to generate only the rules that satisfy such predefined constraints [6,12,17,36]. A classification of association rule types and a framework of defining templates to represent predefined constrained were established before mining association rules in [6]. This work discussed the characteristics of six types of association rules, i.e., (1) simple association rules, (2) filtered association rules, (3) mining-constrained association rules, (4) clustering association rules, (5) discretized association rules, and (6) rules referencing hierarchies. However, no implementa-tion issues were raised. Zaki [36] presented a method to mine a set of closed frequent itemsets which were much smaller than a set of traditional frequent itemsets but equivalent in terms of expressiveness. His work showed that mining a set of closed frequent itemsets could prevent us from the generation of non-redundant association rules. However, the concept of closed frequent itemsets seemed useful when the mined database was dense, but might not be effective in the cases of a sparse database. As a the-oretical database-oriented approach, Jeudy et al. [17] discussed the MINE RULE operators, similar to rule templates, towards the design of inductive databases dedicated to the rule constraints. Moreover, similar to Zaki X  X  work [36], they proposed an approach on condensed representations for the frequent itemsets, called the  X  -free and closed itemsets, to optimize single association rule mining queries as well as sophisticated post-processing and interactive rule mining. As another recent work, to reduce compu-tational complexity of the itemset search space, Do et al. [12] proposed a category-based constraints to limit the generation of associations by modifying the Apriori algorithm to dynamically check the con-straints on the categories of items. By the category-based Apriori algorithm, they reduced computational complexity of the mining process by allowing most of the subsets of the final itemsets in the category to pass or not pass as a group.

Instead of hard decision, a more flexible solution, the third approach, is to provide a mechanism to rank a large number of generated rules in order to locate meaningful rules on the top of the list [8,10, 19,23]. Towards ranking of association rules, Li and Cercone [23] introduced a rough-set-based process by first generating potential subsets of attributes that are important, called reducts, then performing a mining process on each reducts, and evaluating rule importance by finding common mined rules which occur frequently across different reducts. The rule importance was defined separately from common as-sociation measures, such as support, confidence or lift, by considering the frequency the rule occurs in a set of reducts. As an application to decision support, Choi et al. [10] proposed a method to use busi-ness values collected by the Analytic Hierarchy Process (AHP), to prioritize mined association rules. The rule importance was determined by not only objective measures such as support, confidence and interest factor, but also subjective measures, in the form of business values such as recency (time as-pect) and monetary value (financial aspect). The rule quality could be improved by considering both objective criteria and subjective preferences of users but it suffered with considerable human interaction to find out the weights among multiple criteria. To incorporate subjective measures defined on domain knowledge, Chen [8] presented a method to utilize a non-parametric approach called Data Envelopment Analysis (DEA), to estimate and rank the efficiency of association rules with multiple criteria using lin-ear programming. However, the method still considered one objective function for all candidates. In [19], Kazienko extended classical association rules to transitively generate indirect associations. The proposed IDARM* algorithm was applied to extract complete indirect association rules with their important mea-sure, confidence, using pre-calculated direct rules. Applied to find relations among web documents, both direct and indirect rules were joined into one set of complex association rules, which could be used to recommend related web pages. 3. A framework of news relation discovery
To discover news relations, we introduce a framework based on association rule mining techniques, as presented in Fig. 1. There are three main steps, i.e., (1) preprocessing, (2) mining process, and (3) verifi-cation process. In the preprocessing (1) step, two main subtasks are term segmentation (1 . 1) and stopword elimination (1 . 2) . As the term segmentation, a lengthy document will be broken down into a sequence of terms in order to enable us to use terms as representatives to process the document. Typically, it is possible to define terms (or words) in the form of single words, compound words, and named entities. To implement the term segmentation, we can use a general dictionary and a specific named entity dictio-nary (or named entity pattern matching mechanism) with one of segmentation schemes such as longest matching scheme and maximum matching scheme. After segmenting each document in the document set ( { d 1 ,d 2 ,...,d n } ) into a sequence of terms, a stopword list can be applied to eliminate terms which are commonly used and do not contribute to the meaning of the documents, such as functional words, articles and prepositions. The remaining terms are used as representatives to express the documents. In the second step, the documents which are represented by a set of terms, except stopwords, are used for the association rule mining process (2) . In this process, it is possible to use one of alternative options, which are varied by term representation, term weighting and association measure. As the result, we will specified by the relation strength. In general, among a large number of rules can be generated, only some of them are potential ones. To this end, an additional process, namely verification process (3) ,can be applied to filter or rank a set of rules based on a subjective or another objective criterion in order to obtain more proper association rules. In this verification step, it is possible to use either a ranking method (3 . 1) or a machine-learning technique (3 . 2) . In this work, we focus on the ranking method while the machine-learning technique is left as our future work. 4. News relation types
Most tasks on finding document relations judged stories to be either relevant or non-relevant, i.e., two classes ( X  X es X  and  X  X o X ). In our task, to discover more meaningful relations, three main types of news relations are classified based on the relevance of news events:  X  X ompletely related X  (CR),  X  X omehow related X  (SH) and  X  X nrelated X  (UR) [20].

A CR relation stands for two news documents when they exactly describe an identical event, usually published, at the (almost) same time period, by two news reporters from different publishers or reported as a repeating news. However, news documents with a CR relation may hold different headlines, different phrases or expressions and different writing styles.
 A SH relation may exist between two news documents with either one of three following conditions. As the first condition, namely  X  X ubnews X  (SN), two news documents describe the same event but one may have more details than the others. The second condition, called  X  X eries X  (SE), represents the situa-tion that two news documents form a series of events reported in different time sequences. For the last condition, named  X  X imilar theme X  (ST), two news documents hold similar topics or themes. Any ST-type relations may involve two different events, or they are mixtures of multiple events where some of them are overlapped.

A UR relation indicates the situation that two news documents are absolutely unrelated in their con-tents, or even their topics or themes.

Table 1 illustrates an example of pa irs of news documents for each of re lation types, showing published date, publisher, news category, and news headline. 5. An association-based approach for news relations discovery
Unlike traditional association rule mining, mining document relations, including news relations, re-quires handling non-binary data. Towards this, in [20,21,33], the conventional support, confidence, con-viction, and lift were generalized to cope with weighted items and then used as association measures. A formulation of these association measures on news relation discovery can be summarized as follows. Assume that I = { i 1 , i 2 ,..., i m }isasetof m news documents (items), T = { t 1 , t 2 , ..., t n }isasetof n terms (transactions), a news itemset X = { x 1 , x 2 , ..., x k }  X  I is a set of k news documents, and a news itemset Y = { y 1 , y 2 ,..., y l }  X  I is a set of l news documents. The generalized support of X  X  Y ( sup ( X  X  Y ) ), the generalized confidence of X  X  Y ( conf ( X  X  Y ) ), the generalized conviction of X  X  Y ( conv ( X  X  Y ) ), and the generalized lift of X  X  Y ( lift ( X  X  Y ) ) are shown in Figs 2(a), Z = X  X  Y = { z 1 , z 2 , ..., z k + l }  X  I , composed of k news documents in the X and l news documents in the Y . By this method, the discovered relations are in the form of  X  X  X  Y  X , where X as well as Y is a set of news documents. The relation  X  X  X  Y  X  with a high association value implies that the news documents in X has a relationship with the news documents in Y .

To elaborate calculation of the generalized association values, let X  X  use an example given in Table 2. In d 3.6, 3.5 and 0.8 for the document d 3 and 0.0, 4.1 and 1.9 for the document d 4 . However, the term weight can be arbitrary, such as term frequency or its normalized form. Consider relations among { d 1 , d 3 }and { d 4 }. The generalized support, confidence, conviction, and lift can be computed as shown in Figs 3(a), (b), (c), and (d), respectively. In this task, the result shows that { d 1 ,d 3 } X  X  d 4 } has a support of 0.25 with a moderate confidence of 0.60, and a high conviction of 1.25 and a high lift of 1.20. Therefore, this rule seems interesting or important.

In practical, we need a suitable term weighting in order to obtain the desired news relations. Different term weights trigger different association values (i.e., relation strength) for each mined relation. For ex-ample, it is possible to represent a document using a bag of single words (unigrams), each of which is given by a weight determined by the frequency of its occurrences. Based on this assumption, the relation strength between two news documents can be obtained by finding the number of overlapping terms in these two documents. Intuitively, the more overlapping terms exist between two documents the more related these two documents are. Although this setting seems reasonable, it may overstate the relation among two large documents with a plentiful overlapping terms, even they have no relation. Alternatively, the relation strength between two documents may be defined by the ratio of the number of overlapping terms over the number of terms in the larger (or smaller) document. Here, the former represents the situ-ation that unigram is used as term representation basis, binary frequency as term weighting, and support as association measure while the latter models a similar case except confidence is used as association measure. In more general cases, we can use bigram, trigram, or other term encodings for term repre-sentation basis. In the same way, it is possible to apply binary term frequency, term frequency, inverse document frequency, or any weighting schemes for term weighting. Any other association definition, such as conviction, lift, leverage, or any measures for rules [13] is an alternative for association measure.
Even no restriction on these factors, in this work, we focus on unigram (UG) and bigram (BG) as term representation basis, binary term frequency weighting (BF), term frequency weighting (TF), and their modification with inverse document frequency weighting (BFIDF, TFIDF) as term weighting, and confidence (CONF), conviction (CONV), and lift ( LIFT) as association meas ure. While UG exploits single terms, it usually has high coverage but yields high term ambiguity. In contrast, since BG considers two neighboring terms as a unit, it handles compound words and then partially solves the ambiguity of words. As primitive term weighting, BF is simply defined by the existence or non-existence of a term in a document while TF is modeled by the frequency of a term in the document. IDF is often used in complementary with TF, to promote a rare term, which occurs in very few documents as an important word. It is usually calculated as a logarithm of the total number of documents in the collection ( N ) divided by the number of documents containing the term ( DF ) . Therefore, BFIDF and TFIDF of a term are defined as BF  X  log( N/DF ) and TF  X  log( N/DF ) , respectively. As for the association measure, the support, confidence, conviction and lift are generalized as summarized in Figs 2(a), (b), (c), and (d). 6. Weighted non-directionalization association measure and region-based ranking
In the past, although a number of previous works [20,21] explored a suitable factor setting that dis-covered high-quality relations, they still had some limitations, especially ones related to the definition of association measures as follows. First, they had a strict condition on the definition of association measures where the min() function was applied to aggregate the association measures of X  X  Y and that of Y  X  X into a non-directional measure. Second, they assumed a single association measure for relation quality measurement. Towards a flexible environment, in this work, we present two extensions i.e., (1) weighted non-directional association measure and (2) region-based ranking. The first extension is to replace a strict condition of the min() function with a more general weighting function for aggre-gating the association measures of the rule into a non-directional measure. The second extension is to strengthen the rule ranking mechanism with a so-called region-based ranking method to exploit different association measures for different regions in order to rank the mined relations. 6.1. Weighted non-directionalization association measure
For the first extension, a replacement of a more general weighting function instead of a strict condition of the min() function is required to combine the association measures of the rule X  X  Y and that of Y  X  X into a non-directional measure. Like previous works [20,21], the task in this work focuses on relation of either completely related, somehow related, or unrelated, without concern of detailed semantics of somehow related (subnews, series, and similar theme). Hence, considering this task, relation strength between X and Y can be defined with regardless of the direction of the associations of X  X  Y or Y  X  X since the order of X and Y in the relation seems trivial. Theoretically, lift is originally a non-lift as it is. Unlike lift, since the original confidence and conviction possess directions, as shown in Fig. 2 (b) and (c), a method to unify these directional measures is needed. In [20,21], the combination was done by simply using a min() function. However, for more generalization, the definitions of association measures are modified to include weighting between the association value of X  X  Y and that of Y  X  X . To this end, weighted non-directional association measures for confidence and conviction, denoted by conf N and conv N , are formulated below. where  X  provides a weight to balance between the minimum and maximum values of the association measures, defined by
When  X  is set to 1, conf N ( X,Y, X  ) becomes conf N min ( X,Y ) and conv N ( X,Y, X  ) is replaced by conv N min ( X,Y ) . This simple condition is the one used in [20,21]. Since the minimum value is used, a large value is obtained when both X  X  Y and Y  X  X have a high association value. As an ex-treme case, when X and Y are identical, conf N ( X,Y, X  ) will be 1 whereas conv N ( X,Y, X  ) becomes an infinity value. The min() function seems suitable to find strong relations, especially almost identical cases, where the values of conf N ( X,Y, X  ) and conv N ( X,Y, X  ) are very high. However, it cannot handle well when a small value is obtained. Especially for cases of subsumption where Y is subsumed by X or vice versa, the min() function may return a small value, when X (or Y ) is much larger, and then cannot express this relation.

In contrast, the zero value of  X  makes conf N ( X,Y, X  ) and conv N ( X,Y, X  ) correspond to conf N max ( X,Y ) and conv N max ( X,Y ) , respectively. For this case, a large value is obtained when X subsumes Y or vice versa. However, even we get the max value of either 1 (in cases of confidence) or infinity value (in cases of conviction), we cannot conclude either X and Y are identical or one of them are just subsumed by the other.

To grasp this characteristics, thi s paper proposes an adaptive mechanism to utilize a region information to set different suitable  X   X  X  for different situations. This method assigns different weights to represent relation strength of a rule in each region in order to optimize ranking performance in that region. That is, in this work, the value of  X  in Eqs (1) and (2) can be varied. It is effective to use  X  =1 when the minimum value of confidence or conviction ( conf N min ( X,Y ) or conv N min ( X,Y ) ) is high while a smaller  X  should be set for the opposite cases. By this heuristic, a strong relationship among documents can be found when  X  =1 while a moderate relationship or subsumption relationship among documents (some-how related) can be detected with a smaller  X  . In other words, to obtain the highest performance, we may need to use different values for  X  with respect to relation strength. To handle these characteristics, we have proposed a region-based ranking method to utilize different association measures for different regions as described in next section. 6.2. Region-based ranking
As for the second extension, to release a strict measurement of relation quality, a method to improve rule ranking mechanism, a so-called region-based ranking, is proposed. As described in the previous section, it is probable to set different weights to different ranges (regions) of the relation strength in order to maximize the performance. In this method, first the mined relations are ranked by relation strength under a preliminary criterion in order to be preliminarily partitioned into a number of regions (in this work, three regions each of which virtually represents CR, SH, or UR) and then second different weights ( 0  X  1 ) can be assigned to these three regions in order to finally rank rules. As the ranking process, two main issues are (1) how to determine the suitable points of weight change to form regions and (2) which weight we should assign to each region.

For the first issue, the most na X ve approach is to plot the performance of each method and then find the point that the performance becomes reversed. Another approach is to use a standard measure commonly used in information retrieval, to determine the turnover point of performance. One popular measure is F-measure [9,29], which handles the tradeoff between precision (P) and recall (R). Defined as 2 PR P + R ,F-measure expresses a harmonic mean of precision and recall. By plotting F-measure with respect to rank positions, we can find the condition with the maximum performance. The F-measure is used to find the rank position and its corresponding association measur e which obtains the highe st value. Such positions can be considered as the boundaries, virtually representing (1) the boundary of CR vs. SH, and (2) that of SH vs. UR, constructing three regions; CR, SH and UR. At this point, for each range of association (relation) strength, it is possible to determine a suitable weighting value (  X  )forit.

For the second issue of the assigned weight for each region, it is possible to explore each possible weighing candidate one by one to find the most suitable one. As the next step, the ranking process scores relations in each region, with different objective functions, where different weights (i.e.,  X  in Eq. (1) or Eq. (2)) are set for these three regions. To implement this, the  X   X  X  of the three regions are explored with a step of 0.1 in the range of 0.0 to 1.0 in order to find the optimal weighting among the weighted non-directional confidence and the weighted non-directional conviction. 7. Evaluation methodology 7.1. Evaluation dataset
Currently there is no standard dataset of news relations in Thai, available as a benchmark for the purpose of performance assessment. Due to this circumstance, we have constructed an evaluation dataset from 811 Thai news documents collected from three news online sources (Dailynews (313 documents), Komchadluek (207 documents), and Manager online (291 documents)) 1 during August 14 X 31, 2007, consisting of politics, economics, and crime categories.

Table 3 displays the statistics of the constructed news collection, characterized by their categories and sizes (the number of words in a document). The news documents were collected equally from three categories; 266 (91 + 87 + 88) politics, 250 (107 + 41 + 102) economics, and 295 (115 + 79 + 101) crime news documents. Most news documents (161 + 357 + 193 = 711 documents) contain fewer than 300 words, and news documents from Dailynews tend to be shorter than those from the other two sources.
As our evaluation collection, a set of 1,132 news relations (in this work, a pair of news documents) are distinctly selected from each top-k relation sets of twenty four conditions where their association values are high (high confidence, high conviction, or high lift). Such twenty four conditions were initiated in our previous works [20,21], by combining two term representation bases (UG and BG), four term weightings (BF, TF, BFIDF, and TFIDF), and three association measures (CONF, CONV, and LIFT). More details of them can be found in Section 8.2.2. For each of 1,132 re lations, three assessors (freque nt news readers) read and judged whether the two news documents are related with each other by one of the predefined relation types, i.e., CR, ST, SE, SN, and UR. Through discussion sessions, the assessors attempted to share the common understanding before starting to do the task. Here, every news relation was judged by these three assessors, where the final decision was done by voting. In the non-majority cases, the assessors were requested to reconsider their decisions until the final decision could be concluded. Finally, the types of the 1,132 news document relations were determined. They are summarized in Table 4. In the table, we can observe that most relations are SH-type (571 relations) and UR-type (496 relations) with few CR-type relations (65 relations). The set of the SH-type relations are composed of 75 SN-type, 199 SE-type, and 297 ST-type relations. The collected relations may involve two news documents from an identical news publisher (528 relations) or those from two distinct news publishers (604 relations). Two news documents with a CR-type relation usually are published in the same time period (maximum time interval = 7 days, mode time interval = 0 day, and average time interval = 0.48 days). Like the CR-type, a SN-type relation often occurs in the case that two news documents are published in the same time period (maximum time interval = 4 days, mode time interval = 0 day, and average time interval = 0.32 days). The other types, i.e., SE, ST and UR, are found in the cases of a relatively longer time interval (maximum time interval = 16, 17, and 15 days, mode time interval = 1, 3, and 2 days, and average time interval = 2.17, 5.77, and 6.11 days). As our initial work, we have focused on three main types, i.e., CR, SH, and UR, by ignoring the subtypes of SH-type relations. 7.2. Evaluation criterion
The quality of ranking discovered news relations is evaluated by comparing the ranks obtained from the proposed method with those of human judgment. The evaluation method applied a pairwise compar-ison technique [11] since the pairwise comparison has been applicably used for determining the relative order of ranking of items.
 where
Equation (7) indicates a so-called rank-order mismatch (ROM), the value of which ranges between 0 and 1. Here, the ROM value is calculated by dividing a mismatch score ( M ( A, B ) )withthemis-match score of the worst case ( N  X  ( N  X  1) 2 ). The worst case occurs when a ranked list A is arranged in the completely reverse order compared to the other ranked list B . For this case, the ROM becomes the maximum value, i.e., 1. In Eq. (8), M ( A, B ) , the mismatch score, indicates the number of rank mismatches between two ranked lists, say A and B , given a set of N objects to be ranked. The mis-match score expresses how much the ranked list A does not match with the ranked list B .The r A ( k ) and r B ( k ) are the respective ranks of the k -th objects based on the ranked lists A and B , respectively. | both the ranked list A and the ranked list B . Otherwise, it becomes 1 when the rank of object i and that of object j are in the reversed order for the ranked list A and the ranked list B . The mismatch score, M ( A, B ) , are the summation of all rank mismatches between the ranked list A and the ranked list B .
In our work, the ROM is calculated by setting A to the ranked list produced by the system and B to the ranked list suggested by human, denoted by ROM h . If all news relations are ranked by the system in the same order with the human judgment, the ROM value becomes 0. Implicitly, the ROM value reflects the amount of incorrect relation types suggested by the system, compared to the human judgment. 8. Experiments 8.1. Experimental settings
To examine performance of the proposed methods, we have conducted five experiments using 1,132 news relations collected from 811 Thai news documents. The first experiment preliminarily investi-gates the size of a document when different term representation bases and different term weightings are applied. The second experiment targets an exploration of the detailed performance of twenty four combinations, generated by combining two term representation bases, four term weightings, and three association measures. We investigate their performances on the top-k ranks. After that, we select a num-ber of optimal combinations and explore their performance in detail. The optimal combinations are used in the subsequent experiments in order to show how the proposed region-based approach can help im-prove quality of news relation discovery. In the third experiment, we preliminarily study the performance of ranking on any pairs of relation types; CR vs. SH, CR vs. UR and SH vs. UR, with no region. This experiment aims to investigate which association measure is the best in ranking between a pair of re-lation types. As the fourth experiment, we study the performance of ranking on all relation types, i.e., CR vs. SH vs. UR, with uniform weighting and no region construction. This experiment helps clarifying the ranking performance when only one weighting is applied for ranking and then indicates the neces-sity of region construction. The result will be used as a baseline for our proposed method. In the last experiment, we study the method to create the CR, SH and UR regions. With these preliminary regions, another measure is applied to rank each region.

For evaluation, the performance is evaluated at different k  X  X  with a step of 25 of the top-k ranks in a ranked list generated from each individual method by using the ROM value compared to human judg-ment (ROM h ). Concretely, the evaluation is proceeded by creating the ranked list of relations ordered by its association measure (a score suggested by the system) and the other ranked list of relations suggested by human judgment (i.e., 0.0 for  X  X nrelated X , 0.5 for  X  X omehow related X , and 1.0 for  X  X ompletely re-lated X  relations). The mismatch score between these two ranked lists is calculated. For the best case, all news relations are ranked by the system in the same order with the human judgment, obtaining ROM h of 0. On the contrary, the value of ROM h is 1 (or 100%) when arranging in the completely reverse or-der. To simply evaluate the performance of proposed method, we consider a special case of one single antecedent and one single consequent, the news document in X relates to the news document in Y .
Similar to the traditional association rule mining [1,2], minimum support is defined to filter out trivial rules. As the implementation of mining news relations, we applied FP-Tree as the mining algorithm since its efficiency was shown in several literatures [14,15,24]. Note that, due to an unlimited upper bound of conviction and lift values, we have performed value normalization to the range between 0 and 1 in order to handle measures uniformly. 8.2. Experimental results 8.2.1. Preliminarily investigating document sizes of varied term representation bases and term
To grasp the charact eristics of each setting, we ha ve done preliminary investi gation by measuring the size of a document when the setting is applied. The result is shown in Table 5. Using 811 news documents for this investigation, we find the size of the smallest document (denoted by  X  X inimum X ), the size of the largest document (denoted by  X  X aximum X ), and the average size of all 811 documents (denoted by  X  X verage X ). When we use different term representation bases and term weightings, the document sizes are varied, resulting in different values of minimum, maximum, and average. For example, the value of 33.00 in the  X  X inimum X  row of the  X  X G-BF X  column indicates that there are 33 distinct words (single terms) in the document which has the smallest number of distinct words. The value of 39.00 in the  X  X inimum X  row of the  X  X G-TF X  column presents that the smallest document includes 39 words (single terms) in total. For the  X  X inimum X  row of the  X  X G-BFIDF X  column points that the smallest document in terms of BFIDF weighting has 38.32 as the summation of weights of all terms in that document. For the  X  X inimum X  row of the  X  X G-TFIDF X  column points that the smallest document in terms of TFIDF weighting has 46.49 as the summation of weights of all terms in that document. When bigram (BG) is utilized in place of unigram (UG), the values in the  X  X inimum X  row expresses the size of the smallest document when it is represented by bigram. For example, the values of 70.00 in the  X  X inimum X  row of the  X  X G-BF X  column shows that there are 70 distinct bigrams (two consecutive terms) in the smallest document. The values in the  X  X inimum X  row for the  X  X G-TF X ,  X  X G-BFIDF X  and  X  X G-TFIDF X  represent the size of the smallest document when  X  X G-TF X ,  X  X G-BFIDF X  and  X  X G-TFIDF X  are applied as term representation basis and term weighting, respectively. Furthermore, the  X  X aximum X  row indicates the size of the largest document, and the  X  X verage X  row means the average size of all documents (811 documents).

From Table 5, some observations can be done as follows. Firstly, the number of bigrams is approxi-mately 2.10 X 3.58 times of that of unigrams. For example, the average number of bigrams in the BG-BF is 301.81 while the average number of unigrams in the UG-BF is 121.58, which is 2.48 times higher. The average number of bigrams in the BG-TF is 399.89 which is 2.00 times greater than that of UG-TF (199.76). It is 3.58 (669.57/187) for the ratio of BG-BFIDF to UG-BFIDF. The ratio of BG-TFIDF to UG-TFIDF is 2.79 (826.58/296.00). These ratios indicate that the number of single terms can increase up to 4 times when terms represented by two consecutive terms. Secondly, the number of distinct words is approximately 1.23 X 1.58 times of the number of total words. Therefore, we can say that the terms is rarely duplicated in a document. For example, the average document size of UG-TF is 199.76 while that of UG-BF is 121.58. This is, the number of terms in UG-TF is around 1.64 times more than that of UG-BF. The ratio of the average number of terms in UG-TFIDF to UG-BFIDF is 1.58 (296.00/187.13). It is 1.32 (399.89/301.81)for the ratio of BG-TF to BG-BF. For the ratio of BG-TFIDF to BG-BFIDF, it is 1.23 (826.58/669.57). Thirdly, the size of documents with IDF representation is around 1.54 X 1.55 times to the size of documents without IDF representation in the case of unigrams. It is around 2.07 X 2.22 times in the cases of bigrams. For example, the ratio of UG-BFIDF to UG-BF is 1.54 (187.13/121.58). It is 1.48 (296.00/199.76) for UG-TFIDF to UG-TF. The ratio of BG-BFIDF to BG-BF is 2.22 (669.57/301.81) while it is 2.07 (826.58/399.89) for BG-TFIDF to BG-TF. It can be observed that the weight of term increases when inverse document frequency is used in complementary with BF or TF. 8.2.2. Investigating performance of varied combinations of three factors
The second experiment explores the detailed performance of possible combinations among setting of three factors, i.e., (1) term representation basis, (2) term weighting, and (3) association measure. The experiment presents an empirical investigation on how term representation basis, term weighting, and association measure affect the quality of discovered news relations. Exploration of possible combina-tions among settings of these three factors is made empirically to find the best setting towards discovery of high-quality relations among news documents. Even there have been plentiful of possible settings for these factors, this work focuses on unigram (UG) and bigram (BG) as term representation basis, bi-nary term frequency weighting (BF), term frequency weighting (TF), and their modification with inverse document frequency weighting (BFIDF, TFIDF) as term weighting, and confidence (CONF), conviction (CONV), and lift (LIFT) as associat ion measure. Here, the generalized confidence, generalized convic-tion, and generalized lift, as shown in Figs 2(b), (c), and (d), are used. In total, twenty four settings are investigated.

The performance evaluation is conducted by comparing the top-k results of each combination to those of the others using the rank-order mismatch compared to human judgment (ROM h ). Table 6 shows the ROM values of twenty four methods in each top-k rank. For the top-50 mined relations, the combination of bigram, term frequency with inverse document frequency, and confidence (BG-TFIDF-CONF), as well as bigram, term frequency with inverse docum ent frequency, and conviction (BG-TFIDF-CONV), appears to be the most effective since it has the lowest ROM values of 0.41%. In the same range, the condition of unigram, term frequency with inverse document frequency, and confidence (UG-TFIDF-CONF) and that of unigram, term frequency with inverse document frequency, and conviction (UG-TFIDF-CONV) also perform well with the low ROM v alues of 0.49% for top-50. This result indicates that the combinations which include term frequency with inverse document frequency are effective for upper ranks. On the other hand, in the upper ranks (top-50), the methods with lift as their measures obtain more than 10.50% ROM, indicate that lift is not good at finding good relations at these upper ranks. Moreover, if the methods with lift are compared with themselves, they obtain improved performance in the middle ranks (top-100 to top-500) but become worse in the lower ranks ( &gt; top-500). Compared to the methods with confidence or conviction, the methods with lift are not good at the upper ranks ( &lt; top-500) but perform well in the lower ranks ( &gt; top-500). For instance, the method with unigram, term frequency with inverse document frequency, and lift (UG-TFIDF-LIFT) performs the best on the top-1100 rankings by giving the lowest ROM values of 9.63%. These results indicate that bigram with either confidence or conviction is effective in the upper ranks while unigram with lift has a good performance in the lower rank, and term frequency with inverse document frequency is the most effective term weighting for both cases. These optimal combinations, i.e., (1) BG-TFIDF-CONF, (2) BG-TFIDF-CONV, and (3) UG-TFIDF-LIFT, are selected to be explored in the remaining experiments to show how the proposed region-based approach can help improve quality of news relation discovery. 8.2.3. Ranking on two relation types with no region
The third experiment explores the performance of the association measures in ranking on any pairs of relation types; CR vs. SH (later, CR_SH), CR vs. UR (later, CR_UR), and SH vs. UR (later, SH_UR). This experiment tries to take only two types into consideration by ignoring the other type in order to investigate effects of weights, assigned to the association measures, on the relation types. In Fig. 4, the three graphs show the results of CR_SH, SH_UR and CR_UR, in order.

The graph of CR_SH indicates that  X  =1 . 0 ( conf N min and conv N min ) achieves the lowest ROM h (both 4.77% at top-150 ) for the higher ranks ( 150) while  X  =0 . 5 ( 0 . 5 conf N min +0 . 5 conf N max and ranks ( &gt; 150), i.e., 0.33% for confidence and 0.34% for conviction at top-636 (the largest k )for  X  =0 . 5 , and 0.44% for confidence and 0.43% for conviction at top-636 for  X  =0 . 0 . As another observation, lift performs worse for the whole range. Its ROM h values are higher than the other methods X  ones, that is, it is not good in differentiating between CR and SH.
 Compared to CR_SH, the classification among SH and UR (SH_UR) seems harder. We got quite high ROM h , i.e., 9.95% X 14.07%, when we consider the whole ranks. In this case, confidence and conviction with  X  =0 . 5 perform the best in almost the whole ranks. Moreover, lift tends to be better than confidence and conviction with  X  =0 . 0 or 1 . 0 , but it is worse for  X  =0 . 5 . However, lift performs well in the whole ranks (top-1067) with 9.89% ROM h . At top-1067 , confidence and conviction gain ROM h of 14.07% and 13.85% for  X  =1 . 0 , 9.95% and 9.86% for  X  =0 . 5 , 10.10% and 11.27% for  X  =0 . 0 .Forthelast case, the graph of CR_UR implies that  X  = 0.5 or  X  = 0.0 shows the better performance than  X  =1 . 0 . That is, at upper ranks (top-75),  X  = 0.5 obtains ROM h values of 0.18% for confidence and 0.29% for conviction, and  X  = 0.0 gains 0.90% for confidence and 0.83% ROM h for conviction while  X  = 1.0 gets 1.15% for confidence and 1.08% ROM h for conviction. For the whole ranks (top-561, the largest k ),  X  = 0.5 obtains ROM h values of 0.003% for confidence and 0.01% for conviction, and  X  = 0.0 gains 0.02% for confidence and 0.01% ROM h for conviction while  X  = 1.0 gets 0.08% for both confidence and conviction. Like the case of CR_SH, lift yields worse performance than the others in the case of CR_UR.

Another four observations can be made as follows. Firstly, it is easily to rank the CR and UR relations with almost no mis-ranking among these two relations, as the ROM h values are lower than 1.60%. Secondly, SH and UR relations are the most difficult combination to be ranked. It seems hard to separate SH relations from UR relations. Thirdly, CR_SH obtains a lot of ranking errors (a high ROM h )atthe higher ranks while the number of ranking errors decreases in the whole ranks. This implies that a lot of confusions, especially several SH relations appear at the higher ranks. This phenomenon also occurs for CR_UR cases but with much lower ROM values. Fourthly, unlike CR_SH and CR_UR graphs, the SH_UR graph displays a reversed ranking error figure. There is a small number of ranking errors at the higher ranks while the whole ranks obtain more errors. This result suggests that a lot of confusions between SH and UR relations occur when we consider the whole ranks, even if may be good for the upper ranks. It implies that it is the most difficult to rank SH and UR relations in the suitable order when we consider the whole ranks, compared to CR_SH and CR_UR in the whole ranks. Such errors appear when the SH relations are located in lower ranks and then mixed with UR relations. The SH relations may be identified as UR relations since there exist synonyms between two news documents which are not able to grasp similarity among texts.
 8.2.4. Ranking on all relation types with no region
To indicate the necessity of region construction, all three types of relations are taken into consideration for investigating overall effects of weights on relation ranking. This experiment investigates the perfor-mance of ranking on all relation types, i.e., CR vs. SH vs. UR, with uniform weighting and no region construction.

The graphs in Fig. 5 show the results of ranking all relation types by using a uniform weight without region classification. Since some measures are good for only some regions, we need to know a good measure for the whole region. Towards this, an average ROM h is calculated by the integral of ROM h over the whole range divided by the total number of ranks (1132). In the legend, the weight (  X  )isshown behind each association measure along with an average ROM h calculated from the whole range. Based on the graphs, the following observations can be made.

Firstly, the conf N min as well as conv N min (  X  =1 . 0 ) achieves the lowest ROM h for the upper ranks ( 175), i.e., 0.00% X 5.45% for confidence and 0.00% X 5.49% for conviction, while the conf N (  X  =0 . 5 ) and conv N (  X  =0 . 5 ) gains the lowest ROM h for almost the lower ranks ( &gt; 175), i.e., 3.71% X 9.92% for confidence and 3.82% X 9.91% for conviction.

Secondly, some other measures, i.e., conf N max (  X  = 0.0), conv N max (  X  = 0.0), and lift are also a good weighting for the lower ranks ( 800), i.e., 9.09% X 9.74% for confidence, 8.86% X 9.53% for conviction, and 9.32% X 10.58% for lift. Nevertheless, for whole range, lift obtains lower performance than conf N max and conv N max .

Thirdly, considering the whole range, there is a lot of ranking errors (a high ROM h ) at the higher (between 50 and 175) and lower ranks ( &gt; 500) but the number decreases at the middle ranks (between 175 and 500).

Fourthly, although the conf N (  X  =0 . 5 )and conv N (  X  =0 . 5 ) are the best weighting for the whole range with the lowest average ROM h (6.65% for confidence and 6.79% for conviction), they seem not good for the higher ranks ( 175), while the conf N min as well as conv N min (  X  =1 . 0 ) is the worst weighting for the whole range with the highest average ROM h (9.17% for confidence and 9.10% for conviction) they perform very well for the higher ranks ( 175).

All observed characteristics give us a potential to separate ranks into several regions and then use different weightings for each region.
 8.2.5. Region-based ranking on all relation types
The last experiment investigates the ranking performance when different weights are assigned to dif-ferent regions. The performance of ranking is investigated for all relation types, i.e., CR vs. SH vs. UR, with varied weightings and region classification. According to the results of the previous experiment, it and conv N max (  X  = 0.0) is proper for the lower ranks. To maximize the performance of ranking, we have applied different weightings for different regions. Composed of three steps, first we have ranked the list of relations based on conf N min (or conv N min )aswellas conf N max (or conv N max ). Second, we select the top-k relations ranked by conf N min (or conv N min ) and choose the bottom-m relations ordered by conf N max (or conv N max ). Third, the remaining relations (at the middle) which do not exist in top-k relations and bottom-m relations are further ranked by another suitable criterion. In this experiment, the weighting of the middle region is investigated in the range of 0.0 and 1.0. To simplify the graph, we display only the cases of 0.0, 0.5 and 1.0. The classification of regions is driven by both automatic and manual settings.
As an automatic setting, the region is constructed by observing the ranks where F-measure achieves the highest value. To determine the boundary fairly, we have performed boundary selection by three dif-ferent datasets which are randomly selected by 50% of each of relation types (CR, SH, or UR) from our evaluation dataset. The best boundaries with the highest values of F-measure for the top-k relations or-dered by conf N min and the bottom-m relations ranked by conf N max are shown in Table 7. For each dataset, a rank position of relations is plotted relative to F-measure. To form the regions, two suitable bound-aries (top-k and bottom-m positions) are identified by their rank positions which have the maximum F-measure, each of which is matched to the corresponding association value, i.e., the top-k position is paired with conf N min value and the bottom-m position is tied with conf N max value. As a result, they are the second dataset, and conf N min of 39.27 and conf N max of 15.28 for the third dataset. By considering the average values, the first region (top-k relations) is positioned at conf N min of 37.86 and the last region (bottom-m relations) is located at conf N max of 14.84. The remaining range is defined as the middle re-gion. By matching the value at the boundary to the corresponding rank position, the first, second, and last regions are partitioned into 1 X 42 ranks, 43 X 406 ranks, and 407 X 1132 ranks, respectively.
Figure 6 shows ranking performance (% ROM h ) where three regions are highlighted with two vertical lines. In the legend, the weights for the first region, the second region, and the last region are shown as the three numbers in a parenthesis, followed by the average ROM h over the whole range. Here, the numbers of CR-, SH-and UR-type relations are indicated for each region. The experimental result shows that the region classification gives us a potential to adjust weights in order to increase the performance. The figure evidences that the optimal  X  values for the first region, the second region, and the last region are 1.0, 0.5 and 0.0, respectively. This optimal weighting combination makes us obtain the lowest average ROM h , i.e., 6.57% for confidence and 6.46% for conviction. By considering each region, this weighting has a ROM h lower than 1.0 in the first region, a ROM h between 1.0 and 7.0 in the second region and a ROM h between 4.0 and 9.5 in the last region.
 In the manual setting, we vary the left boundary between 50 and 200 with a step of 50 but we fix the right boundary at 406, which is the best location when we apply the F-measure as mentioned above. According to the results of the graphs in Fig. 7, it is possible to conclude that placing the boundary more on the left makes us obtain better performance. When the left boundary is placed at the point after top-50, the performance of the first region becomes better but of the second region seems degraded. As a conclusion, the lowest average ROM h can be obtained compared to other setting, i.e., 6.59% for confidence and 6.50% for conviction when the left boundary is 50 and the weight is (1, 0.5, 0). Then we vary the right boundary between 300 and 450 with a step of 50 when the left boundary is fixed to 50, which is the best left boundary beforehand. The results in Fig. 8 indicate that ranking performance is good when the right boundary is less than 400. That is, the best performances (the lowest average ROM h ) are 6.57% for confidence and 6.48% for conviction when the right boundary is 400 and the weight is (1, 0.5, 0). Moreover, if we place the right boundary after top-400, the performance degrades. For example, when setting the right boundary to 450 and the weight to (1, 0.5, 0), we obtain the average ROM h of 7.10% and 6.80% for confidence and conviction, respectively, and even worse for the weight of (1, 1, 0), we gain 9.07% and 8.65% for confidence and conviction, respectively.

As conclusions, the above result of manual boundary setting implies that the left boundary should be set to top-50 while the right boundary should be placed around top-400 . Compared to the automatic setting, the boundaries assigned by the manual setting are close to ones obtained from the automatic setting. This result implies that our automatic boundary setting, region-based approach, can provide suitable regions for setting different weights in ranking mechanism. 8.3. Error analysis
In order to analyze the errors obtained in the proposed methods, we investigate the relations which are mis-ranked in each region. Table 8 shows the confusion matrix between three regions when the regions are split based on automatic region setting as shown in Fig. 6, i.e., 1 X 42, 43 X 406, and 407 X 1132. It can be concluded as follows.

The twenty four CR relations located in the SH region (i.e., (a)), are ranked between 45 X 144. The main reason why CR relations are ranked in the lower ranks are as follows. Firstly, any two news documents with a same topic may use synonyms which trigger few overlapping terms among these documents, and usually they occupy similar sizes. Secondly, any two completed-related news documents may have different levels of event details. In many cases, a news document explains an event in details while the other may provide only a summary or conclusion. Thirdly, whereas two news documents may mention the same event, they may have different facts.

Only one pair of somehow-related news documents is ranked in the CR region (i.e., (b)) since they share many terms or named entities, such as person names and place names, but mention different events. In some cases, it is not always correct to imply a CR relation when two news documents share common terms.
 Analyzing mis-ranking between SH and UR, i.e, 276 SH-to-UR misclassifications (c) and 46 UR-to-SH misclassifications (d), it was found that they may be triggered by reasons similar to those between SH and CR. Any two unrelated news documents may be detected as somehow-related news documents when they are in the same category, such as economic news, and share several common terms in that category, such as  X  X inance X ,  X  X ndex X ,  X  X ax X ,  X  X arket X , and  X  X tock X . These terms are not recognized as stopwords but they frequently occur in economics news. With this circumstance, two news documents with several common economic terms may be mis-recognized as the somehow-related relation. Any two somehow-related news documents may be recognized as unrelated news when they do not share enough common terms.

A number of solutions towards these problems include improving weighting schemes, handling se-mantic, and applying a news category-dependent stopword list. Moreover, the presence of the same con-cepts, e.g., synonym and polysemy, should be considered since they provides more practical similarity. The latent semantic indexing or some disambiguation methods can be applied to solve this problem. 9. Conclusions and future works
This paper presented a method to apply association rule mining to discover relations among news doc-uments. There were three important contributions in this work. First, a framework of association-based approach for news relation discovery was introduced. By encoding news documents as items, and terms in the news documents as transactions, a number of frequent patterns were mined. Each of frequent patterns represented a set of news documents that shared common terms more than a threshold, called support. Thereafter, as a further step of our work, a set of frequent rules was found based on these fre-quent patterns with several thresholds, namely confidence, conviction and lift. Since mining news rela-tions involved handling non-binary data, the modified definitions of confidence, conviction, and lift were adopted as generalized confidence, generalized conviction, and generalized lift, respectively. Second, a study of factor effects on quality of discovered news relations was explored. Three factors, i.e., term representation basis, term weighting, and association measure, were investigated to select appropriate factor combinations. Under such suitable factor combinations, three types of news relations (completely related, somehow related, unrelated) were retrieved. Third, a so-called region-based ranking method was proposed to enhance the performance of news relation discovery. The method sorted the mined relations under a preliminary criterion to form a number of regions Then, for each region, the method ranked the relations with different ranking criteria to recommend a final rank of each news relation.
An evaluation was done on a set of 1,132 news relations with three levels of relation strength: com-pletely related, somehow related, and unrelated relations mined from 811 news documents. The region-based method achieved the best performance by setting the weighting of the first, the second and the third region to 1, 0.5, and 0, respectively with the average ROM h of 6.57% and 6.46% for confidence and conviction, consecutively. By comparing to the non-region method when the weights are set to 0, 0.5, and 1, the experimental results showed that, for the whole range, the proposed method improves the performance with the average ROM h improvement of 11.87%, 1.21% and 28.32% for confidence, and 11.91%, 4.83% and 29.04% for conviction, respectively.

As future works, the following topics are interesting for exploration. First, to grasp the detailed re-lations among news documents, the semantic analysis such as synonym handling and discourse and pragmatic analysis, as well as the usability of latent semantic indexing or some disambiguation meth-ods is needed. Second, to improve the classification performance, a proper term weighting is required for determining term importance. For instance, we can give more weights to the terms placed in the news headline than to the terms positioned in the body because they convey a central information of news story. Third, to improve performance of ranking, a hybrid method which selectively uses different criteria for different areas is beneficial. Last, to increase the automation of discovering process, it is possible to apply machine learning techniques for finding an optimal feature setting. This would reduce the dependency of having domain expert available in judging the results every time when the data is changed. As the applicability for real cases, we may add the predicted results which have high confi-dence to increase the number of examples in the evaluation dataset (training data). This process includes the co-training and self-training techniques which are being considered for further work.
 Acknowledgements
This work was granted by Strategic Scholarships Fellowships Frontier Research Networks 2006 from the Commission on Higher Education and The National Electronics and Computer Technology Center (NECTEC) under project number NT-B-22-KE-38-54-01. Also thanks to the National Research Univer-sity Project of Thailand Office of Higher Education Commission and a Research Grant sponsored by the Bangchak Petroleum Public Company Limited (BCP), Thailand.
 References
