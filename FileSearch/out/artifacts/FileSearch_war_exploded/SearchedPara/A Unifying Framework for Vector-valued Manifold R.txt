 H`a Quang Minh minh.haquang@iit.it Loris Bazzani loris.bazzani@iit.it Vittorio Murino vittorio.murino@iit.it that we prove here.
 Notation : the definition of f as given by is adopted because it is also applicable when W is an infinite-dimensional Hilbert space. For W = R m , mutation of the matrix M in (Rosenberg et al., 2009) when they give rise to the same semi-norm.
 The adjoint operator E  X  C, x : Y l  X  X  K is thus The operator E  X  C, x E C, x : H K  X  X  K is then with C  X  C : W  X  X  .
 Proof of Theorem 1 . Denote the right handside of reproducing property, E C, x satisfies for all b  X  X  l , since C  X  b i  X  X  . Thus operator S x satisfies for all w  X  X  u + l . Thus For an arbitrary f  X  X  K , consider the orthogonal de-Then, because || f 0 + f 1 || 2 H result just obtained shows that Thus the minimizer of (2) must lie in H K, x . 1.2. Proofs for the Least Square Case Proposition 1. The minimization problem (3) has a a  X  X  are given by l X  Also K  X  x 1  X  i  X  l : which gives the formula l X  Similarly, for l + 1  X  i  X  u + l , which is equivalent to This completes the proof.
 Proposition 2. tively, and y l +1 =  X  X  X  = y u + l = 0 .
 Proof of Proposition 2 . This is straightforward to matrix formulation described in the main paper. Proposition 3. For C = c T  X  I P , c  X  R m , M W = equations (12) in Proposition 2 is equivalent to where diagonal being 1 and the rest being 0 .
 flattening operation (:) in MATLAB. To compute the matrix Y T C , note that by definition being y i , with Y
C is of size P  X  ( u + l ) m , whereas C Pm  X  ( u + l ). the point evaluation operator We are not assuming that the functions in H are de-that  X  x  X  X  , there is a bounded linear operator f and its least square version f Following are the corresponding Representer Theorem recover Theorem 1 and Proposition 1, respectively. Theorem 2. The minimization problem (25) has a vectors a i  X  X  , 1  X  i  X  u + l .
 Proposition 4. The minimization problem (26) has a a  X  X  are given by ing example. Thus one cannot make a statement about f W = R . Let G : X  X X  X  R be continuous and for f  X  X  . One has the reproducing kernel similar behavior. We show that the matrix A in Propo-to verify the correctness of an implementation of our algorithm. At  X  =  X  , for each pair ( x,t ), we have and form of the coefficients a i  X  X  for the case We have For  X  I = 0, we have B = which is Equivalently, The inverse of B in this case has a closed form:
 H`a Quang Minh minh.haquang@iit.it Loris Bazzani loris.bazzani@iit.it Vittorio Murino vittorio.murino@iit.it Reproducing kernel Hilbert spaces (RKHS) and kernel-based methods have been by now established as among the most powerful paradigms in machine learn-ing and statistics, with numerous practical applica-tions, see e.g. (Sch  X olkopf &amp; Smola, 2002; Shawe-Taylor &amp; Cristianini, 2004). While most of the literature on kernel methods so far has focused on scalar-valued functions, RKHS of vector-valued functions have re-ceived increasing research attention in machine learn-ing recently, from both theoretical and practical per-spectives, see e.g. (Micchelli &amp; Pontil, 2005; Carmeli et al., 2006; Reisert &amp; Burkhardt, 2007; Caponnetto et al., 2008; Brouard et al., 2011; Dinuzzo et al., 2011; Kadri et al., 2011; Minh &amp; Sindhwani, 2011). The goal of the present work is to fuse together two learning approaches for Semi-Supervised Learning in a general vector-valued RKHS framework, where the hy-pothesis spaces are RKHS of vector-valued functions. The first approach for Semi-Supervised Learning that we consider is Manifold Regularization (Belkin et al., 2006), which attempts to learn the geometry of the in-put space from the given unlabeled data. Generaliza-tions from the scalar case to the vector-valued setting include (Brouard et al., 2011), where a vector-valued version of the graph Laplacian L is used, and (Minh &amp; Sindhwani, 2011), where L can be a general symmet-ric, positive operator, including the graph Laplacian. The vector-valued setting allows one to capture possi-ble dependencies between output variables by the use of, for example, an output graph Laplacian.
 The second approach for Semi-Supervised Learning that we consider is Multi-view Learning. In the multi-view approach (Brefeld et al., 2006; Sindhwani &amp; Rosenberg, 2008; Rosenberg et al., 2009; Saffari et al., 2010), different hypothesis spaces are employed to con-struct target functions based on different aspects of the input data. These target functions not only need to agree with the labeled data, but also need to agree with each other on the unlabeled data. One scenario where this approach can be applied (Rosenberg et al., 2009) is when the input data can be decomposed as x = ( x 1 ,...,x m ), with each x i representing one view . One can then construct a target function for each view and fuse them together to obtain the final solution. The formulation we present in this paper gives a uni-fied learning framework for the case the hypothesis spaces are vector-valued RKHS. Our formulation is general, encompassing many common algorithms as special cases, including both Vector-valued Manifold Regularization and Multi-view Learning. For the case of least square loss function, we give a closed form solu-tion which can be implemented efficiently. Our numer-ical experiments were performed using a special case of our framework, namely Vector-valued Multi-view Learning, with promising results on several particu-larly challenging multi-class categorization problems. To the best of our knowledge, this work is the first attempt to present a unified general learning frame-work whose components have been only individually and partially covered in the literature.
 Organization. We start by giving a review of vector-valued RKHS in Section 2. We state the general opti-mization problem we wish to solve in Section 3, along with various special cases, the Representer Theorem, and Proposition 1, which gives the explicit solution for the least square case. We then describe Vector-valued Multi-view Learning and its implementation in Section 4, with empirical experiments in Section 5. Proofs for all mathematical results in the paper are given in the Supplementary Material. In this section, we give a brief review of RKHS of vector-valued functions 1 , for more detail see e.g. (Carmeli et al., 2006; Micchelli &amp; Pontil, 2005; Capon-netto et al., 2008; Minh &amp; Sindhwani, 2011). In the following, denote by X a nonempty set, Y a real, sepa-rable Hilbert space with inner product  X  X  ,  X  X  Y , L ( Y ) the Banach space of bounded linear operators on Y . Let Y X denote the vector space of all functions f : X  X  X  . A function K : X  X  X  X  L ( Y ) is said to be an operator-valued positive definite kernel if for each pair ( x,z )  X  X   X X , K ( x,z )  X  = K ( z,x ), and for every finite set of points { x i } N i =1 in X and { y in Y . Given such a K , there exists a unique Y -valued RKHS H K with reproducing kernel K , which is con-structed as follows. For each x  X  X and y  X  Y , form a function K x y = K ( .,x ) y  X  X  X defined by Consider the set H 0 = span { K x y | x  X  X , y  X  Y}  X  Y
X . For f = P N define the inner product which makes H 0 a pre-Hilbert space. Completing H 0 by adding the limits of all Cauchy sequences gives the Hilbert space H K . The reproducing property is Sampling Operators. For each x  X  X  , let K x : Y  X  H
K be the operator with K x y defined as above, then which implies that so that K x is a bounded operator. Let K  X  x : H K  X  X  be the adjoint operator of K x , then from (2), we have From this we deduce that for all x  X  X  and all f  X  X  K , || f ( x ) || Y  X || K  X  x |||| f || H that is the sampling operator S x : H K  X  X  defined by is bounded. Let x = ( x i ) l i =1  X  X l , l  X  N . For the sampling operator S x : H K  X  Y l defined by S x ( f ) = ( f ( x i )) l i =1 , for any y = ( y i ) l i =1  X  X  l , Thus the adjoint operator S  X  x : Y l  X  X  K is given by and the operator S  X  x S x : H K  X  X  K is given by Data-dependent Semi-norms. Let ( x 1 ,...,x u + l ) ric, positive operator, that is  X  y,My  X  Y u + l  X  0 for all y  X  X  u + l . For f  X  X  K , let f = ( f ( x 1 ) ,...,f ( x pressed as an operator-valued matrix M = ( M ij ) u + l i,j =1 of size ( u + l )  X  ( u + l ), with each M ij : Y  X  X  being a linear operator, so that We can then define the following semi-norm for f , which depends on the x i  X  X : This form of semi-norm was utilized in vector-valued manifold regularization (Minh &amp; Sindhwani, 2011). In this section, we state the general minimization prob-lem that we wish to solve, which includes Vector-valued Manifold Regularization and Multi-view Learn-ing as special cases.
 Let the input space be X , an arbitrary non-empty set. Let Y be a separable Hilbert space, denoting the out-put space. Assume that there is an unknown probabil-ity measure  X  on X  X Y , and that we have access to a random training sample z = { ( x i ,y i ) } l i =1  X  X  x i of l labeled and u unlabeled examples.
 Let W be a separable Hilbert space. Let K : X X X  X  L ( W ) be an operator-valued positive definite kernel and H K its induced Reproducing Kernel Hilbert Space of W -valued functions.
 Let M : W u + l  X  W u + l be a symmetric, positive op-erator. For each f  X  X  K , let Let V : Y  X Y  X  R be a convex loss function. Let C : W  X  Y be a bounded linear operator, with C  X  : Y  X  X  its adjoint operator.
 The following is the general minimization problem that we wish to solve: with regularization parameters  X  A &gt; 0,  X  I  X  0. Let us give a general multi-view learning interpretation of our framework. If each input instance x has many views, then f ( x )  X  W represents the output values from all the views, constructed by their correspond-ing hypothesis spaces. These values are combined by the operator C to give the final output value in Y , which is not necessarily the same as W . In (9), the first term measures the error between the final output Cf ( x i ) for x i with the given output y i , 1  X  i  X  l . The second summand is the standard RKHS regularization term. The third summand, Multi-view Manifold Reg-ularization, is a generalization of vector-valued Mani-fold Regularization in (Minh &amp; Sindhwani, 2011) and Multi-view Point Cloud regularization in (Rosenberg et al., 2009): if there is only one view, then it is sim-ply manifold regularization; if there are many views, then it consists of manifold regularization along each view, as well as consistency regularization across dif-ferent views. We describe one concrete realization of this term in Section 4.2.
 Remark 1 . The framework is readily generalizable to the case the point evaluation functional f ( x ) is re-placed by a general bounded linear operator -we de-scribe this in the Supplementary Material. 3.1. Representer Theorem The minimization problem (9) is guaranteed to al-ways have a unique global solution. The following is a natural generalization of the Representer Theorem in (Minh &amp; Sindhwani, 2011).
 Theorem 1. The minimization problem (9) has a vectors a i  X  X  , 1  X  i  X  u + l . 3.2. Least Square Case For the case V is the least square loss function, we solve the following problem, which has an explicit solution: The following is a generalization of Proposition 1 in (Minh &amp; Sindhwani, 2011).
 Proposition 1. The minimization problem (10) has a a  X  X  are given by l X  for 1  X  i  X  l , and for l + 1  X  i  X  u + l . 3.3. Operator-valued Matrix Formulation The system of equations (11) and (12) can be refor-mulated in matrix form, which is more readable and more convenient to implement efficiently.
 Let K [ x ] denote the ( u + l )  X  ( u + l ) operator-valued matrix whose ( i,j ) entry is K ( x i ,x j ). Let J l entries on the main diagonal are the identity operator I : W  X  X  , with the rest being 0. Let C  X  C : W u + l  X  W u + l be the ( u + l )  X  ( u + l ) diagonal matrix, with each diagonal entry being C  X  C : W  X  W . Let C  X  : with each diagonal entry being C  X  : Y  X  W . Then Proposition 1 is equivalent to Proposition 2. ( C  X  C J W ,u + l l K [ x ] + l X  I MK [ x ] + l X  A I ) a = C sidered as column vectors in W u + l and Y u + l , respec-tively, and y l +1 =  X  X  X  = y u + l = 0 . 3.4. Special Cases Vector-valued Regularized Least Squares. If C  X  C = I : W u + l  X  X  u + l , then (13) reduces to If u = 0,  X  I = 0, and  X  A =  X  , then we have One particular case for this scenario is when W = Y and C : Y  X  Y is a unitary operator, that is C  X  C = CC  X  = I . If Y = R n and C : R n  X  R n is real, then C is an orthogonal matrix. If C = I , then we recover the Regularized Least Squares algorithm.
 Vector-valued Manifold Regularization. Let W = Y and C = I . Then we obtain the minimiza-tion problem for vector-valued Manifold Regulariza-tion (Minh &amp; Sindhwani, 2011): f Scalar Multi-view Learning. In this section, we show that the scalar multi-view learning formulation of (Sindhwani &amp; Rosenberg, 2008; Rosenberg et al., 2009) can be cast as a special case of our framework. Let Y = R and k 1 ,...,k m be real-valued positive defi-nite kernels on X  X X , with corresponding RKHS H k i of functions f i : X  X  R , with each H k i represent-ing one view. Let f = ( f 1 ,...,f m ), with f i  X  H k i Let c = ( c 1 ,...,c m )  X  R m be a fixed weight vec-tor. In the notation of (Rosenberg et al., 2009), let f = ( f 1 ( x 1 ) ,...,f 1 ( x u + l ) ,...,f m ( x 1 ) ,...,f The objective of Multi-view Point Cloud Regulariza-tion (formula (4) in (Rosenberg et al., 2009)) is for some convex loss function V , with  X  i &gt; 0, i = 1 ,...,m , and  X   X  0. Problem (17) admits a natural formulation in vector-valued RKHS. Let then f = ( f 1 ,...,f m )  X  X  K : X  X  R m , with By the reproducing property, we have We can now recast (17) into This is a special case of (9), with W = R m , Y = R , and C : R m  X  R given by
Cf ( x ) =  X  c ,f ( x )  X  R m = c 1 f 1 ( x )+  X  X  X  + c m f The vector-valued formulation of scalar multi-view learning has the following advantages: (i) The kernel K is diagonal matrix-valued and is ob-viously positive definite. In contrast, it is nontrivial to prove that the multi-view kernel of (Rosenberg et al., 2009) is positive definite.
 (ii) The kernel K is independent of the c i  X  X , unlike the multi-view kernel of (Rosenberg et al., 2009), which needs to be recomputed for each different set c i  X  X . (iii) One can recover all the component functions f i  X  X  using K . In contrast, in (Sindhwani &amp; Rosenberg, 2008), it is shown how one can recover the f i  X  X  only when m = 2, but not in the general case. Another special case of our formulation, which is the focus of subsequent sections, is vector-valued multi-view learning. For a general separable Hilbert space Y , let W = Y m and C 1 ,...,C m : Y  X  Y be bounded each f i ( x )  X  Y , we can define the operator C = [ C 1 ,...,C m ] : Y m  X  X  by This gives rise to a vector-valued version of multi-view learning, where outputs from m views, each one being a vector in the Hilbert space Y , are linearly combined. Consider the following setting of vector-valued multi-view learning, which we apply to the problem of multi-class categorization in Section 5. Let the input space be X , a non-empty subset, and the output space be Y = R P , P  X  N . Let the number of views be m  X  N . For the multi-class classification problem, P is the number of classes. For each y i , 1  X  i  X  l , in the labeled training sample, y i = (  X  1 ,..., 1 ,...,  X  1), with 1 at the k th location if x i is in the k th class. Let W = Y m = R Pm . The hypothesis space H K of functions with values in W = R Pm is induced by a positive definite matrix-valued kernel K : X  X X  X  R
Pm  X  Pm , that is for each pair ( x,t )  X  X   X X , K ( x,t ) is an Pm  X  Pm matrix. For each function f  X  H K , f ( x ) = ( f 1 ( x ) ,...,f m ( x )), where f i ( x )  X  Y = the value corresponding to the i th view.
 In Proposition 2, J W ,u + l l is a diagonal matrix of size Pm ( u + l )  X  Pm ( u + l ), with the first Pml en-tries on the main diagonal being 1, the rest being 0, R 4.1. The Combination Operator In the present context, the bounded linear operator C : W  X  X  is a matrix of size P  X  Pm . This operator transforms the output vectors obtained from the m views f i  X  X  in R Pm into an output vector in R P . The simplest form of C is the average operator:
Cf ( x ) = Let  X  denote the Kronecker tensor product. For m  X  N , let e m = (1 ,..., 1) T  X  R m . The matrix C is then More generally, we consider a weight vector c = ( c 1 ,...,c m ) T  X  R m and define C as
C = c T  X  I P , with Cf ( x ) = 4.2. Multi-view Manifold Regularization We decompose the multi-view manifold regularization term  X  I  X  f ,M f  X  W u + l in (Eq. 9) into two components  X  where M B ,M W : W u + l  X  W u + l are symmetric, pos-itive operators, and  X  B , X  W  X  0. We call the first term between-view regularization , which measures the consistency of the component functions across differ-ent views, and the second term within-view regulariza-tion , which measures the smoothness of the component functions in their corresponding views. We describe next two choices for M B and M W .
 Between-view Regularization. Let This is the m  X  m matrix with ( m  X  1) on the diagonal and  X  1 elsewhere. Then for a = ( a 1 ,...,a m )  X  R m , If each a i  X  R P , then we have a  X  R Pm and We define M B by Then M B is a diagonal block matrix of size Pm ( u + l )  X  Pm ( u + l ), with each block ( i,i ) being M Pm . For f =  X  f ,M B f  X  For P = 1, this is precisely the Point Cloud regular-ization term for scalar multi-view learning (Rosenberg et al., 2009; Brefeld et al., 2006). In particular, for m = 2, we have M 2 = which is the Point Cloud regularization term for co-regularization (Sindhwani &amp; Rosenberg, 2008). Within-view Regularization. One way to define M
W is via the graph Laplacian. For view i , 1  X  i  X  m , let G i be a corresponding undirected graph, with symmetric, nonnegative weight matrix W i , which in-duces the scalar graph Laplacian L i , a matrix of size ( u + l )  X  ( u + l ). For a vector a  X  R u + l , we have Let L be the block matrix of size ( u + l )  X  ( u + l ), with block ( i,j ) being the m  X  m diagonal matrix given by Then for a = ( a 1 ,...,a u + l ), with a j  X  R m , we have If a j  X  R Pm , with a i j  X  R P , then a T ( L  X  I P ) a = Define  X  f ,M W f  X  . The i th summand in the sum P m i =1 is precisely a man-ifold regularization term within view i .
 Single View Case. When m = 1, M m = 0 and M
B = 0. In this case, we simply carry out manifold regularization within the given single view, using M W . 4.3. Numerical Implementation This section is devoted to giving an efficient numerical implementation of Proposition 2 while still preserving the closed form of the solution. Assume that each input x is decomposed into x = ( x 1 ,...,x m ) for the m different views. We define K ( x,t ) as a block diagonal matrix, with the ( i,i )th block given by where k i is a scalar-valued kernel, such as the Gaussian kernel. Then K ( x,t ) is a matrix of size Pm  X  Pm . Simplification via the Kronecker Tensor Prod-uct. Let G ( x,t ) be the m  X  m diagonal matrix, with and G [ x ] be the ( u + l )  X  ( u + l ) block matrix, where block ( i,j ) is the m  X  m matrix G ( x i ,x j ). Then and the matrix K [ x ] is Proposition 3. For C = c T  X  I P , c  X  R m , M W = L  X  I P , M B = I u + l  X  ( M m  X  I P ) , the system of linear equations (13) in Proposition 2 is equivalent to where which is of size ( u + l ) m  X  ( u + l ) m , A is the matrix of size ( u + l ) m  X  P such that a = vec( A T ) , and Y is the matrix of size ( u + l ) m  X  P such that C  X  y = size ( u + l )  X  ( u + l ) , with the first l entries on the main diagonal being 1 and the rest being 0 .
 Evaluation on a Testing Sample. Let v = { v 1 ,...,v t }  X  X be an arbitrary set of testing input examples, with t  X  N . Let f z , X  ( v ) = Let K [ v , x ] denote the t  X  ( u + l ) block matrix, where the t  X  ( u + l ) block matrix, where block ( i,j ) is the m  X  m matrix G ( v i ,x j ). Then f z , X  ( v ) = K [ v , x ] a = ( G [ v , x ]  X  I P ) a = vec( A sample, we have G [ v , x ] = G [ x ].
 Algorithm 1 combines all the steps in this section. Algorithm 1 R P -valued, m -view, semi-supervised least square regression and classification Input : -Training data z = ( x i ,y i ) l i =1  X  ( x i ) l labeled examples and u unlabeled examples. -Number of views: m . Output dimension: P . -Testing example: v .
 Parameters :  X  A , X  B , X  W ; a kernel k i for each view and its parameters; the weight vector c .
 Procedure : -Compute kernel matrix G [ x ] on input set x = ( x i ) u + l i =1 according to (40). -Compute matrix C according to (26). -Compute graph Laplacian L according to (34). -Compute matrices B , Y C according to Proposition 3. -Solve system of linear equations BA = Y C . -Compute kernel matrix G [ v, x ] between v and x . Output : f z , X  ( v ) = vec( A T G [ v, x ] T )  X  R Pm . R P -regression: return Cf z , X  ( v )  X  R P .
 P -way classification: return index of max( Cf z , X  ( v )). This section provides an empirical analysis of the pro-posed framework and its particular instance described in Section 4. We tested our method on two different multi-class categorization tasks, namely object recog-nition and bird species categorization, using challeng-ing, publicly available datasets (Fei-Fei et al., 2006; Wah et al., 2011). For these problems, each view of an input example is one type of features of that example. These experiments demonstrate that: 1) the multi-view regularization terms both contribute to improve learning performance; 2) multi-view learning achieves significantly better performance compared to single-view learning; 3) our method gives comparable perfor-mance with other state-of-the-art methods.
 Parameters. These were fixed for all of the experi-ments unless explicitly stated otherwise. In particular,  X 
A = 10  X  5 ,  X  B =  X  W = 10  X  6 , and the weight vector c of the combination operator was set to be uniform. The graph Laplacians of Eq. 34 were computed with the kernel matrices as weight matrices.
 Object Recognition. We evaluated the pro-posed method on the Caltech-101 dataset (Fei-Fei et al., 2006) using the features, kernel matrices, and evaluation protocol proposed in (Vedaldi et al., 2009), available at http://www.robots.ox.ac.uk/ ~ vgg/software/MKL/ . The appearance descriptors are made up of 4 features (views): PHOW gray and color, geometric blur (GB), and self-similarity (SSIM). For the first three features, a three-level pyramid of spatial histograms were built (Vedaldi et al., 2009). First, we analyzed the contributions of each of the between-view (Eq. 32) and within-view (Eq. 38) regu-larization terms in (Eq. 9). A subset of 10 images for each class were randomly selected, with half used as la-beled data l c = 5 and the other half as unlabeled data u c = 5 (see Table 1, last column). We also tested the proposed method in the one-shot learning setup, where the number of labeled images is one per class l c = 1 (see Table 1, third column). The testing set consisted of 15 images per category. For this test, we selected the features at the bottom of each pyramid, because they give the best performance in practice. We can see from Table 1 that both the between-view and within-view regularization terms contribute to increase the recognition rate, e.g. with l c = 1 the improvement is 2 . 35%. As one would expect, the improvement result-ing from the use of unlabeled data is bigger when there are more unlabeled data than labeled data, which can be seen by comparing the third and forth columns. We observed that averaging views in the combina-tion operator is not the optimal choice. For c = provement of 1 . 02% with respect to the last row, last column of Table 1. This suggests that c should be ei-ther jointly optimized within the proposed framework or cross-validated. We leave this to a future work. To demonstrate that multi-view learning is able to combine features properly, we report in Table 2 the performance of each feature independently and of the proposed method with all 10 views combined. The minimum improvement with respect to the view that gives the best results (PHOW gray L2) is 4 . 77% (sec-ond column) and 4 . 71% (last column).
 The last test we performed on the Caltech-101 dataset used the features at the bottom level of the pyramid in a supervised setup with 15 images per category for training. The results obtained (see Table 3) are com-parable with other state-of-the-art techniques. This is very encouraging, because the loss function V in the present implementation of our framework is the least square loss, while most existing methods use SVM that is known to reach very good classification performance in practice. We leave to future work an implementa-tion using the SVM loss function, which will be able to combine the advantages of our framework with the classification power of the SVM.
 Bird Species Categorization. Next, we exam-ined the Caltech-UCSD Birds-200-2011 dataset (Wah et al., 2011), which is particularly challenging be-cause it contains 11 , 788 annotated images of 200 very similar bird species. We consider as two views an appearance-based feature and an attribute-based fea-ture. For the appearance-based view, we extracted the bag of feature descriptors based on PHOW gray features (Vedaldi et al., 2009) and computed the  X  kernel. For the attribute-based view, we used the 312-dimensional binary vector provided in (Wah et al., 2011) and computed the Gaussian kernel; these cor-respond to 312 attributes for each image, which were not exploited as features in (Wah et al., 2011). The parameters of both kernels were set to be the me-dian pairwise distances among training points. Fig-ure 1 shows the classification accuracies of single-view learning (in red and green) of each feature and 2-view learning (in blue) when increasing the number of la-beled data in the training set with a fixed number of unlabeled data. The best categorization accuracy reported so far in (Wah et al., 2011) is 6 . 94% (with l c = 5). Their pipeline, consisting of a bird detec-tor, an appearance-based descriptor and SVM, outper-forms PHOW because they performed bird localization while we extracted features from the whole image. The most important fact is that 2-view learning is always able to combine both appearance-based and attribute-based features, giving better results compared to each feature taken as single view.
 Computational complexity. The system of linear equations (43) in Proposition 3 is of size m ( u + l )  X  m ( u + l ), has a unique solution, is simple to implement and is efficient. For the experiments that we report in this paper, the main computation cost is in computing the different kernel matrices. With 4 views, using the same setup of Table 2, the proposed algorithm took 30 . 79 sec. and 0 . 48 sec. 2 for training (3060 images) and testing (1530 images), respectively (given the pre-computed kernel matrices). With 2 views, it took 4 . 95 sec. (training) and 0 . 17 sec. (testing). We have presented a general vector-valued RKHS formulation for Semi-Supervised Learning, of which Vector-valued Manifold Regularization and Multi-view Learning are special instances. The results we have ob-tained demonstrate that this is a promising venue for further research exploration. Our future work includes an implementation of the SVM loss function within our framework, the optimization of the combination oper-ator, and connection to Multiple Kernel Learning. Belkin, M., Niyogi, P., and Sindhwani, V. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research , 7:2399 X 2434, 2006. Brefeld, U., G  X artner, T., Scheffer, T., and Wrobel, S. Efficient co-regularised least squares regression. In
Proceedings of the International Conference on Ma-chine Learning(ICML) , 2006.
 Brouard, C., D X  X lche-Buc, F., and Szafranski, M.
Semi-supervised penalized output kernel regression for link prediction. In Proceedings of the Interna-tional Conference on Machine Learning , 2011.
 Caponnetto, A., Pontil, M., C.Micchelli, and Ying, Y. Universal multi-task kernels. Journal of Machine Learning Research , 9:1615 X 1646, 2008.
 Carmeli, C., Vito, E. De, and Toigo, A. Vector-valued reproducing kernel Hilbert spaces of integrable func-tions and Mercer theorem. Analysis and Applica-tions , 4:377 X 408, 2006.
 Christoudias, M., Urtasun, R., and Darrell, T. Bayesian localized multiple kernel learning. Univ. California Berkeley, Berkeley, CA , 2009.
 Dinuzzo, F., Ong, C.S., Gehler, P., and Pillonetto, G.
Learning output kernels with block coordinate de-scent. In ICML , 2011.
 Fei-Fei, L., Fergus, R., and Perona, P. One-shot learn-ing of object categories. Pattern Analysis and Ma-chine Intelligence, IEEE Transactions on , 28(4):594  X 611, april 2006.
 Gehler, P. and Nowozin, S. On feature combination for multiclass object classification. In Proceedings of the 12th IEEE International Conference on Computer Vision , 2009.
 Kadri, H., Rabaoui, A., Preux, P., Duflos, E., and
Rakotomamonjy, A. Functional regularized least squares classification with operator-valued kernels. In Proceedings of the International Conference on Machine Learning(ICML) , 2011.
 Micchelli, C. A. and Pontil, M. On learning vector-valued functions. Neural Computation , 17:177 X 204, 2005.
 Minh, H.Q. and Sindhwani, V. Vector-valued manifold regularization. In ICML , 2011.
 Reisert, M. and Burkhardt, H. Learning equivari-ant functions with matrix valued kernels. J. Mach. Learn. Res. , 8:385 X 408, 2007.
 Rosenberg, D., Sindhwani, V., Bartlett, P., and
Niyogi, P. A kernel for semi-supervised learning with multi-view point cloud regularization. IEEE Sig. Proc. Mag. , 26(5):145 X 150, 2009.
 Saffari, A., Leistner, C., Godec, M., and Bischof, H.
Robust multi-view boosting with priors. In Proceed-ings of the European Conference on Computer Vi-sion , 2010.
 Sch  X olkopf, B. and Smola, A. Learning with kernels:
Support Vector Machines, Regularization, Optimiza-tion, and Beyond . The MIT Press, Cambridge, 2002. Shawe-Taylor, J. and Cristianini, N. Kernel Methods for Pattern Analysis . Cambridge University Press, 2004.
 Sindhwani, V. and Rosenberg, D. An RKHS for multi-view learning and manifold co-regularization. In
Proceedings of the International Conference on Ma-chine Learning (ICML) , 2008.
 Vedaldi, A., Gulshan, V., Varma, M., and Zisserman,
A. Multiple kernels for object detection. In Pro-ceedings of 12th IEEE International Conference on Computer Vision , 2009.
 Wah, C., Branson, S., Welinder, P., Perona, P., and
Belongie, S. The Caltech-UCSD Birds-200-2011 dataset. 2011. URL http://www.vision.caltech. edu/visipedia/CUB-200-2011.html .
 Yang, J., Li, Y., Tian, Y., Duan, L., and Gao, W.
Group-sensitive multiple kernel learning for object categorization. In Proceedings of the 12th IEEE In-
