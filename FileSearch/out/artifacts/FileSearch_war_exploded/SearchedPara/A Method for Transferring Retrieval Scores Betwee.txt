 We present a method for projecting retrieval scores across two corpora with a shared, parallel corpus.
 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Retrieval models General Terms: Algorithms Keywords: regularization, cross-lingual retrieval
In many retrieval scenarios, the collection of retrievable documents consists of several, disjoint sub-collections. This is generally referred to as distributed information retrieval or federated search. We focus on the situation where each sub-collection uses a unique vocabulary. For example, we might have a sub-collection of text documents written in english, a sub-collection of text documents written in french, and another sub-collection of images.

Given a query, we often are able to score documents in one sub-collection but not in others. When our sub-collections consist of documents written in different languages, this is known as cross-lingual retrieval. In cross-lingual informa-tion retrieval, a user is interested in documents written in a foreign or target language and provides a query in her native or source language. Traditional approaches to this problem usually perform some sort of query translation from the source to the target language [2]. When sub-collections consist of documents in different media, this is known as cross-media retrieval.

We focus on transferring the scores from one sub-collection to another sub-collection. We accomplish this by scoring source parallel documents and using these scores as the basis for regression in the target collection. Like other methods, we only require a parallel corpus. However, we do not trans-late the query and hence do not require a second retrieval.
Formally, we have a target collection of n t documents with a vocabulary size of m t . Some relatively small number, n  X  Work conducted at the Center for Intelligent Information Retrieval.
 of the target documents have been translated into the source language with a vocabulary size of m s . Sets of translated collections are common in the machine translation commu-nity and are referred to as parallel corpora. More generally, we only require some representation of the target documents in the source vocabulary. For example, we may have cap-tions associated with images. We will further assume that, given a query in the source language, we have some method for scoring the source language documents.

Transferring scores between collections is a process of scor-ing the source parallel documents and then assuming that the n s parallel target documents should have the same score. If the user were interested in retrieving the parallel tar-get documents, the retrieval process could terminate at this stage. However, the user is more often interested in those target documents which do not have source translations. We will score these non-parallel target documents by using the score information from the parallel documents.

Assume that the translated documents are all indexed identically from 0 to n s for both corpora. Additionally, we will assume that we have an n t  X  n t affinity matrix, W for the target collection of documents. An affinity matrix contains the similarity information between all pairs of doc-uments in the target collection. For text documents, similar-ity measures have been studied in the context of topic link detection as well as clustering for many languages. More generally, we can use any kernel defined on documents of the target collection. We process W t by keeping only the k -nearest neighbors of each document and then making this matrix symmetric. Further let D be the n t  X  n t normalizing matrix such that D ii = P j W ij .

Let the n t  X  1 vector f t contain the scores transferred to the target corpus documents. We would like to search over the space of all n t  X  1 vectors to find a vector for which similar documents X  X s represented by W t  X  X ave very simi-lar scores X  X s measured by some score similarity function X  subject to the constraint that the projected scores for the first n s elements are similar to the original retrieval scores. We represent the dissimilarity of scores of related documents using the function S ( f t ); we represent the dissimilarity of scores of the first n s documents with the original retrieval scores using the function E ( f t , y s ) where y s is the n vector of source collection scores. We linearly combine these into a composite function, where  X  is a scalar parameter combining both objectives. The constraints are defined as, where y t = [ y T s 0 T ] T is a vector of projected scores and  X  = I  X  D  X  1 / 2 t W t D  X  1 / 2 t is known as the combinatorial Laplacian. The combinatorial Laplacian as a measure of score similarity has been previously used successfully in the situation of document re-ranking [1]. The closed form solu-tion for computing f  X  is,
Let  X  t refer to a language model over the target vocabu-lary; similarly,  X  s models the source language. If we have a query in the source language, we score each source paral-lel document, d , according to the query likelihood, P ( Q |  X  The cross-lingual relevance model is estimated as [2], where Z is a normalizing constant. With the cross-lingual relevance model, we are applying the score for a source doc-ument to the parallel target document, allowing us to build a relevance model in the target language using source doc-ument scores as the interpolation weights. This solves our problem of not having a query in the target language. We then score a document by its cross-entropy with P ( w |  X 
Interestingly, we can show that scoring by cross-lingual relevance models is very similar to our method of transfer-ring scores. The proof follows from combining the relevance model estimation and cross-entropy scoring and rearranging summations. The resulting scores, f t , can be are related to the original target corpus scores according to, where y t is composed of P ( Q |  X  s d ) scores and A t is an n affinity matrix based on inter-document cross-entropy be-tween the target documents. This is a single step of an iterative version of Equation 2.
We compared the performance of our method to cross-lingual relevance models (CLRM) using a cross-lingual re-trieval task involving a source query written in English and a target collection written in Mandarin [3]; machine trans-lated TDT5 documents were used as a parallel corpus [2]. We indexed and retrieved english parallel documents using the open source Indri retrieval system. We indexed Man-darin documents using Indri, treating each character as a word. We used character tf.idf vectors and cosine similarity for computing W t . We perform 10-fold cross-validation to tune free retrieval parameters. We used the paired t-test to measure statistical significance.

We present results in Table 1. Perhaps due to the the-oretical similarity of the approaches, there is no statistical difference between our method and CLRM. However, our method tends to perform better at low-recall areas. There-fore, in Table 2, we evaluate each algorithm by the precision for the top k documents.
 Table 1: Cross-lingual relevance models compared to transferring scores.
 Table 2: Cross-lingual relevance models compared to transferring scores.

These preliminary results indicate an interesting direction for information retrieval research for several reasons. First, our perspective is general and can be applied to cross-lingual retrieval, distributed information retrieval, and cross-media retrieval. Second, our method generalizes a previous, high-quality retrieval method and allows us to study its components X  computing source collection scores and interpolating tar-get collection scores X  X ndependently. Third, unlike previous query-translation approaches, we do not need to expand the query into a potentially large query, allowing us to optimize retrieval engines for short queries instead of having to handle short and long queries. Finally, in addition to links between the two corpora, we only require a kernel to be defined on the target corpus. This means that, in cases where a query translation is ill-specified or target corpus retrieval is poor but we understand a kernel, we can still apply our algorithm.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by the Defense Advanced Research Projects Agency under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are the au-thor and do not necessarily reflect those of the sponsor.
