 Being able to predict the prominence or pitc h accent status of a word in con versational speech is impor -tant for implementing text-to-speech in dialog sys-tems, as well as in detection of prosody in con versa-tional speech recognition.

Pre vious investigations of prominence prediction from text have primarily relied on rob ust surf ace fea-tures with some deeper information structure fea-tures. Surf ace features lik e a word X  s part-of-speech (POS) (Hirschber g, 1993) and its unigram and bi-gram probability (Pan and McK eown, 1999; Pan and Hirschber g, 2000) are quite useful; content words are much more lik ely to be accented than function words, and words with higher probability are less lik ely to be prominent. More sophisticated linguis-tic features have also been used, generally based on information-structura l notions of contr ast , focus , or given-ne w . (Hirschber g, 1993).

For example, in the Switchboard utterance be-low, there is an intrinsic contrast between the words  X  X  omen X  and  X  X en X , making both terms more salient (w ords in all capital letters represent promi-nent tok ens):
Similarly the givenness of a word may help deter -mine its prominence. The speak er needs to focus the hearer X  s attention on new entities in the discourse, so these are lik ely to be realized as prominent. Old en-tities, on the other had, need not be prominent; these tendencies can be seen in the follo wing example.
While pre vious models have attempted to cap-ture global properties of words (via POS or unigram probability), the y have not in general used word identity as a predicti ve feature, assuming either that current supervised training sets would be too small or that word identity would not be rob ust across gen-res (Pan et al., 2002). In this paper , we sho w a way to capture word identity in a feature, accent ratio , that works well with current small supervised train-ing sets, and is rob ust to genre dif ferences.
We also use a corpus which has been hand-labeled for information structure features (including given/ne w and contrast information) to investigate the relati ve usefulness of both linguistic and shallo w features, as well as how well dif ferent features com-bine with each other . For our experiments we use 12 Switchboard (God-fre y et al., 1992) con versations, 14,555 tok ens in to-tal. Each word was manually labeled for presence or absence of pitch accent 1 , as well as additional features including information status (or givenness), contrast and animac y distinctions, (Nissim et al., 2004; Calhoun et al., 2005; Zaenen et al., 2004), fea-tures that linguistic literature suggests are predicti ve of prominence (Bolinger , 1961; Chafe, 1976).
All of the features described in detail belo w have been sho wn to have statistically significant correla-tion with prominence (Brenier et al., 2006). Inf ormation status The information status (IS), or givenness, of discourse entities is important for choosing appropriate reference form (Prince, 1992; Gundel et al., 1993) and possibly plays a role in prominence decisions as well (Bro wn, 1983). No pre vious studies have examined the usefulness of information status in natur ally occurring con versa-tional speech. The annotation in our corpus is based on the givenness hierarchy of Prince: first mentions of entities were mark ed as new and subsequent men-tions as old . Entities that are not pre viously men-tioned, but that are generally kno wn or semantically related to other entities in the preceding conte xt are mark ed as med iated. Ob viously , the givenness an-notation applies only to referring expressions, i.e. noun phrases the semantic interpretation of which is a discourse entity . This restriction inherently limits the power of the feature for prominence prediction, which has to be performed for all classes of words. Complete details of the IS annotation can be found in (Nissim et al., 2004).
 Kontrast One reason speak ers mak e entities in an utterance prominence is because of information structure considerations (Rooth, 1992; Vallduv  X  X  and Vilkuna, 1998). That is, parts of an utterance which distinguish the information the speak er actually says from the information the y could have said, are made salient, e.g. because that information answers a question, or contrasts with a similar entity in the conte xt. Several possible triggers of this sort of salience were mark ed in the corpus, with words that were not kontrasti ve (in this sense) being mark ed as bac kgr ound :  X  contr astive if the word is directly dif ferentiated  X  subset if it refers to a member of a more general  X  adverbial if a focus-sensiti ve adv erb such as  X  corr ection if the speak er intended to correct or  X  answer if the word completes a question by the  X  nonapplic for filler phrases such as  X  X n fact X ,  X  X 
Note that only content words in full sentences were mark ed for kontrast, and filler phrases such as  X  X n fact X  and  X  X  mean X  were excluded. A com-plete description of the annotation guidelines can be found in (Calhoun et al., 2005).
 Animacy Each noun and pronoun is labeled for the animac y of its referent (Zaenen et al., 2004). The cate gories include concr ete , non-concr ete , human , org anizations, place , and time .
 Dialog act Specifies the function of the utterance such as statement , opinion , agree , reject , abandon ; or type of question ( yes/no, who, rhetoric )
In addition to the abo ve theoretically moti vated features, we used several automatically deri vable word measures.
 Part-of-speech Two such features were used, the full Penn Treebank tagset (called POS) , and a col-lapsed tagset (called BroadPOS) with six broad cat-egories (nouns, verbs, function words, pronouns, ad-jecti ves and adv erbs).
 Unigram and bigram probability These features are defined as log ( p tively and their values were calculated from the Fisher corpus (Cieri et al., 2004). High probability words are less lik ely to be prominent.
 TF .IDF This measure captures how central a word is for a particular con versation. It is a function of the frequenc y of occurrence of the word in the con ver-sation ( n tain the word in a background corpus ( k ) and the number of all con versations in the background cor -pus ( N ). Formally , TF .IDF1 = n also used a variant, TF .IDF2 , computed by normal-izing TF .IDF1 by the number of occurrences of the most frequent word in the con versation. TF .IDF2 = TF .IDF1 /max ( n values are important in the con versation and are more lik ely to be prominent.
 Stopw ord This is a binary feature indicating if the word appears in a high-frequenc y stopw ord list from the Bo w toolkit (McCallum, 1996). The list spans both function and content word classes, though nu-merals and some nouns and verbs were remo ved. Utterance length The number of words.
 Length The number of characters in the words. This feature is correlated with phonetic features that have been sho wn to be useful for the task, such as the number of vowels or phones in the word.
 Position from end/beginning The position of the word in the utterance divided by the number of words that precede the current word.
 Accent ratio This final (ne w) feature tak es the  X  X emorization X  of pre vious productions of a given word to the extreme, measuring how lik ely it is that a word belongs to a prominence class or not. Our feature extends an earlier feature proposed by (Yuan et al., 2005), which was a direct estimate of how lik ely it is for the word to be accented as observ ed in some corpus. (Yuan et al., 2005) sho wed that the original accent ratio feature was not included in the best set of features for accent prediction. We belie ve the reason for this is the fact that the original ac-cent ratio feature was computed for all words, even words in which the value was indistinguishable from chance (.50). Our new feature incorporates the sig-nificance of the prominence probability , assuming a def ault value of 0.5 for those words for which there is insuf ficient evidence in the training data. More specifically , AccentR atio ( w ) = where k is the number of times word w appeared accented in the corpus, n is the total number of times the word w appeared, and B ( k, n, 0 . 5) is the probability (under a binomial distrib ution) that there are k successes in n trials if the probabil-ity of success and failure is equal. Simply put, the accent ratio of a word is equal to the esti-mated probability of the word being accented if this probability is significantly dif ferent from 0.5, and equal to 0.5 otherwise. For example, AccentRa-tio(you)=0.3407 , AccentRatio(education )=0 .866 6 , and AccentRatio(pr obably)=0 .5 .

Man y of our features for accent prediction are based only on the 12 training con versations. Other features, such as the unigram, bigram, and TF*IDF features, are computed from lar ger data sources. Ac-cent ratio is also computed over a lar ger corpus, since the binomial test requires a minimum of six occurrences of a word in the corpus in order to get significance and assign an accent ratio value dif fer -ent from 0.5. We thus used 60 Switchboard con ver-sations (Ostendorf et al., 2001), annotated for pitch accent, to compute k and n for each word. For our experiments we used the J48 decision trees in WEKA (W itten and Frank, 2005). All the results that we report are from 10-fold cross-v alidation on the 12 Switchboard con versations.

Some pre vious studies have reported results on prominence prediction in con versational speech with the Switchboard corpus. Unfortunately these studies used dif ferent parts of the corpus or dif ferent label-ings (Gre gory and Altun, 2004; Yuan et al., 2005), so our results are not directly comparable. Bear -ing this dif ference in mind, the best reported results to our kno wledge are those in (Gre gory and Altun, 2004), where conditional random fields were used with both textual, acoustic, and oracle boundary fea-tures to yield 76.36% accurac y.

Table 1 sho ws the performance of decision tree classifiers using a single feature. The majority class baseline (not accented) has accurac y of 58%. Accent ratio is the most predicti ve feature: the accent ratio classifier has accurac y of 75.59%, which is two per -cent net impro vement abo ve the pre viously kno wn best feature (unigram). The accent ratio classifier assigns a  X  X o accent X  class to all words with accent ratio lower than 0.38 and  X  X ccent X  to all other words. In Section 4 we discuss in detail the accent ratio dic-tionary , but it is worth noting that it does correctly classify even some high-frequenc y function words lik e  X  X he X ,  X  X e X ,  X  X o X  or  X  X p X  as accented. 3.1 Combining featur es We would expect that a combination of features would lead to better prediction when compared to a classifier based on a single feature. Several past studies have examined classes of features. In order to quantify the utility of dif ferent specific features, we ran exhausti ve experiments producing classifiers with all possible combinations of two, three, four and fi ve features.

As we can see from figure 1 and table 2, the clas-sifiers using accent ratio as a feature perform best, for all sizes of feature sets. Moreo ver, the increase of performance compared to a single-feature classi-fier is very slight when accent ratio is used as fea-ture. Kontrast seems to combine well with accent ratio and all of the best classifiers with more than one feature use kontrast in addition to accent ratio. This indicates that automatic detection of kontrast can potentially help in prominence prediction. But the gains are small, the best classifiers without kon-trast but still including accent ratio perform within 0.2 percent of the classifiers that use both.
On the other hand, classifiers that do not use ac-cent ratio perform poorly compared to those that do, and even a classifier using fi ve features (unigram, broad POS, tok en length, position from beginning and bigram) performs about as well as a classifier using solely accent ratio as a feature. Also, when accent ratio is not used, the overall impro vement of the classifier gro ws faster with the addition of new features. This suggest that accent ratio pro vides rich information about words beyond that of POS class and general informati veness. 2
Table 2 gives the specific features in ( n + 1) -feature classifiers that lead to better results than the best n -classifier . The figures are for the classifiers performing best overall. Interestingly , none of these best classifiers for all feature set sizes uses POS or unigram as a feature. We assume that accent ratio captures all the rele vant information that is present in the unigram and POS features. The best classifier with fi ve features uses, in addition to accent ratio, kontrast, tf.idf, information status and distance from the beginning of the utterance. All of these features con vey some what orthogonal information: seman-Table 2: Performance increase augmenting the ac-cent ratio classifier . tic, topicality , discourse and phrasing information respecti vely . Still, all of them in combination im-pro ve the performance over accent ratio as a single feature only by one percent.

Figure 1 sho ws the overall impro vement of clas-sifiers with the addition of new features in three sce-narios: overall best, best when kontrast is not used as a feature and best with neither kontrast nor ac-cent ratio. The best classifier with fi ve features that do not include kontrast has accent ratio, broad POS, word length, stopw ord and bigram as features and has accurac y of 76.28%, or just 0.27% worse than the overall best classifier that uses kontrast and in-formation status. This indicates that while there is some benefit to using the two features, the y do not lead to any substantial boost in performance. Strik-ingly , the best classifier that uses neither accent ra-tio nor kontrast performs very similarly to a classi-fier using accent ratio as the only feature: 75.82% for the classifier using unigram, POS, tf.idf1, word length and position from end of the utterance. 3.2 The po wer of linguistic featur es One of the objecti ves of our study was to assess how useful gold-standard annotations for comple x lin-guistic features are for the task of prominence pre-diction. The results in this section indicate that an-imac y distinctions (concrete/non-conc ret e, person, time, etc) and dialog act did not have much power Figure 1: Performance increase with the addition of new features. as indi vidual features (table 1) and were never in-cluded in a model that was best for a given feature set size (table 2).

Information status is some what useful and ap-pears in the overall best classifier with fi ve features (table 2). But when compared with other classifiers with the same number of features, the benefits from adding information status to the model are small. For example, the accent ratio + information status classifier performs 0.23% better than accent ratio alone, but so does the classifier using accent ratio and tf.idf. There are two reasons that can explain why the givenness of the referent is not as helpful as we might have hoped. First of all, the informa-tion status distinction applies only to referring ex-pressions and has undefined values for words such as verbs, adjecti ves or function words. Second, in-formation status of an entity influences the form of referring expression that is used, with old items be-ing more lik ely to be pronominalized. In the numer -ous cases where pronominalization of old informa-tion does occur , features such as POS, unigram or accent ratio will be sensiti ve to the change of infor -mation status simply based on the lexical item.
Kontrast is by far the most useful linguistic fea-ture. It is used in all of the best classifiers for any feature set size (table 2). It applies to more words than givenness does, since salience distinctions can be made for any part-of-speech class. Still, not all words were annotated for kontrast either , and more-over kontrast only captures one kind of semantic salience. This is particularly true of discourse mark-ers lik e  X  X specially X  or  X  X efinitely X : these would ei-ther be in sentence fragments that weren X  t mark ed for kontrast, or would probably be mark ed as  X  X ack-ground X  since the y are not salience triggers in a se-mantic sense. As we can see from figure 1, clas-sifiers that use kontrast perform only slightly better than others that use only  X  X heaper X  features. Contrary to our initial expectations, both classes in the accent ratio dictionary (for both low and high probability of being prominent) cover the full set of possible POS cate gories. Tables 3 and 4 list words in both classes (with words sorted by increasing accent ratio in each column). The  X  X o accent X  class is dom-inated by function words, but also includes nouns and verbs. One of the dra wbacks of POS as a fea-ture for prominence prediction is that normally aux-iliary verbs will be tagged as  X  X B X , the same class as other more contentful verbs. The informati veness (unigram probability) of a word would distinguish between these types of verbs, but so does the accent ratio measure as well.

Furthermore, some relati vely frequent words such as  X  X oo X ,  X  X o w X ,  X  X oth X ,  X  X o X ,  X  X es X ,  X  X lse X ,  X  X  ow X  have high accent ratio, that is, a high probability for accenting. Such distinctions within the class of func-tion words would not be possible on the basis of in-Table 3: Accent ratio entries with low prominence probability . formati veness, POS, or even information structure features. Another class lik e that is words lik e  X  X es X ,  X  X kay X ,  X  X ure X  that are mostly accented by virtue of being the only word in the phrase.

Some rather common words,  X  X ot X  for example, are not included in the accent ratio dictionary be-cause the y do not exhibit a statistically strong pref-erence for a prominence class. The accent ratio clas-sifier would thus assign class  X  X ccented X  to the word  X  X ot X , which is indeed the class this word occurs in more often.

Another fact that becomes apparent with the in-spection of the accent ratio dictionary is that while certain words have a statistically significant prefer -ence for deaccenting, there is also a lot of variation in their observ ed realization. For example, personal pronouns such as  X  X  X  and  X  X ou X  have accent ratios near 0.33. This means that every third such pronoun was actually realized as prominent by the speak er. In a con versational setting there is an implicit con-trast between the two speak ers, which could partly explain the phenomenon, but the situations which prompt the speak er to realize the distinction in their Table 4: Accent ratio values for words with high probability for being accented. speech will be the focus of a future linguistic inves-tigation.

Kontrast is helpful in predicting  X  X ccented X  class for some generally low ratio words. Ho we ver, even with its help, production variation in the con versa-tions cannot be fully explained. The follo wing ex-amples from our corpus sho w low accent ratio words (that, did, and, have, had) that were produced as prominent.
The examples attest to the presence of variation in production: in the first utterance, for example, we see the words  X  X id X ,  X  X nd X  and  X  X hat X  produced both as prominent and not prominent. Intonational phras-ing most probably accounts for some of this varia-tion since it is lik ely that even words that are typ-ically not prominent will be accented if the y occur just before or after a longer pause. We come back to this point in the closing section. While accent ratio works well for our data (Table 2), a feature based so strongly on memorizing the status of each word in the training data might lead to problems. One potential problem, suggested by Pan et al. (2002) for lexicalized features in general, is whether a lexical feature lik e accent ratio might be less rob ust across genres. Another question is whether our definition of accent ratio is better than one that does not use the binomial test: we need to investigate whether these statistical tests indeed im-pro ve performance. We focus on these two issues in the next two subsections.
 Binomial test cut-off As discussed abo ve, the original accent ratio feature (Yuan et al., 2005) was based directly on the frac-tion of accented occurrences in the training set. We might expect such a use of raw frequencies to be problematic. Given what we kno w about word dis-trib utions in text (Baayen, 2001), we would expect about half of the words in a big corpus to appear only once. In an accent ratio dictionary without binomial test cut-of f, all such words will have accent ratio of either exactly 1 or 0, but one or even few occurrences of a word would not be enough to determine statis-tical significance. By contrast, our modified accent ratio feature uses binomial test cut-of f to mak e the accent ratio more rob ust to small training sets.
To test if the binomial test cut-of f really impro ved the accent ratio feature, we compared the perfor -mance on Switchboard of classifiers using accent ratio with and without cut-of f. The binominal test impro ved the performance of the accent ratio fea-ture from 73.49% (Yuan et al. original version) to 75.59% (our version).

Moreo ver, Yuan et al. report that their version of the feature did not combine well with other features, while in our experiments best performance was al-ways achie ved by the classifiers that made use of the accent ratio feature in addition to others. A cross-genr e experiment: broadcast news In a systematic analysis of the usefulness of dif fer -ent informati veness, syntactic and semantic features for prominence prediction, Pan et al. (2002) sho wed that wor d identity is a powerful feature. But the y hy-pothesized that this would not be a useful feature in a domain independent pitch accent prediction task. Their hypothesis that word identity cannot be a ro-bust across genres would obviously carry over to ac-cent ratio. In order to test the hypothesis, we used the accent ratio dictionary deri ved from the Switch-board corpus to predict prominence in the Boston Uni versity Radio corpus of broadcast news. Using an accent ratio dictionary from Switchboard and as-signing class  X  X ot accented X  to words with accent ra-tio less than 0.38 and  X  X ccented X  otherwise leads to 82% accurac y of prediction for this broadcast news corpus. If the accent ratio dictionary is built from the BU corpus itself, the performance is 83.67%. 3 These results indicate that accent ratio is a rob ust enough feature and is applicable across genres. In this paper we introduced a new feature for promi-nence prediction, accent ratio. The accent ratio of a word is the (maximum lik elihood estimate) prob-ability that a word is accented if there is a signifi-cant preference for a class, and 0.5 otherwise. Our experiments demonstrate that the feature is power -ful both by itself and in combination with other fea-tures. Moreo ver, the feature is rob ust to genre, and accent ratio dictionaries can be used for prediction of prominence in read news with very good results.
Of the linguistic features we examined, kontrast is the only one that is helpful beyond what can be gained using shallo w features such as n-gram prob-ability , POS or tf.idf. While the impro vements from kontrast are relati vely small, the consistenc y of these small impro vements suggest that developing auto-matic methods for approximating the gold-standard annotation we used here, similar to what has been done for information status in (Nissim, 2006), may be worthwhile. An automatic predictor for kontrast may also be helpful in other applications such as question answering or textual entailment.
 All of the features in our study were text-based. There is a wide variety of research investigating phonological or acoustic features as well. For exam-ple Gre gory and Altun (2004) used acoustic features such as duration and ener gy, and phonological fea-tures such as oracle (hand-labeled) intonation phrase boundaries, and the number of phones and sylla-bles in a word. Although acoustic features are not available in a text-to-speech scenario, we hypothe-size that in a task where such features are available (such as in speech recognition applications), acous-tic or phonological features could impro ve the per -formance of our text-only features. To test this hy-pothesis, we augmented our best 5-feature classifier which did not include kontrast with hand-labeled in-tonation phrase boundary information. The resulting classifier reached an accurac y of 77.45%, more than one percent net impro vement over 76.28% accurac y of the model based solely on text features and not in-cluding kontrast. Thus in future work we plan to in-corporate more acoustic and phonological features.
Finally , prominence prediction classifiers need to be incorporated in a speech synthesis system and their performance should be gauged via listening experiments that test whether the incorporation of prominence leads to impro vement in synthesis.
