 Fondazione Bruno Kessler  X  IRST Fondazione Bruno Kessler  X  IRST Fondazione Bruno Kessler  X  IRST
We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner. In particular, we use a combination of basic it uses a considerably smaller number of training examples than other methods. 1. Introduction
A significant challenge in man ynatural language processing tasks is to reduce the need for labeled training data while maintaining an acceptable performance. This is espe-ciall ytrue for word sense disambiguation (WSD) because when moving from the some-what artificial lexical-sample task to the more realistic all-words task it is practically impossible to collect a large number of training examples for each word sense. Thus, man ysupervised approaches, explicitl ydesigned for the lexical-sample task, cannot be applied to the all-words task, even though the yexhibit excellent performance. This has led to the somewhat paradoxical situation in which completel ydifferent methods have been developed for the two tasks, although the yrepresent two sides of the same coin. based on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo,
Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006). In particular, we explored the following research directions: (1) independentl ymodeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (2) exploiting external knowledge acquired from unlabeled data, with the purpose of drasticall yreducing the amount of labeled training data. The first direction is based on the linguistic assumption that syntagmatic and domain (associative) relations are crucial for representing sense distinctions, but the yare originated b ydifferent phenomena.
Regarding the second direction, one can hope to obtain a more accurate prediction b ytaking into account unlabeled data relevant to the learning problem (Chapelle, sample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a large amount of unlabeled training data, as well as the usual labeled training data. However, at that time, we were the onl yteam to use the unlabeled data (Strapparava, Gliozzo, and Giuliano 2004).
 information in order to define a complete kernel for WSD. The rest of the article is organized as follows. In Section 2, we provide a general introduction to the kernel methods, in which we give the basis for understanding our approach. Exploiting kernel methods, we can define and combine individual kernels representing information from different sources in a principled way. After this introductory section, in Section 3 we present the kernels that we developed for WSD. This includes a detailed description of the individual kernels and the wa ywe define the composite ones. We present our experiments in Section 4. The results obtained on a range of lexical-sample tasks and on the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclu-sions and some directions for future research. 2. Kernel Methods
Kernel methods are a popular machine learning approach within the natural lan-guage processing community. They are theoretically well founded in statistical learn-ing theor yand have shown good empirical results in man yapplications (Vapnik 1999; Cristianini and Shawe-Taylor 2000; Sch  X  olkopf and Smola 2002; Shawe-Taylor and Cristianini 2004).
 into two parts. The yfirst embed the input data in a suitable feature space, and then use a linear algorithm to discover nonlinear patterns in the input space. Typically, the mapping is performed implicitl yb ya so-called kernel function . The kernel function is a similarit ymeasure between the input data that depends exclusivel yon the specific data type and domain. A typical similarity function is the inner product between feature vectors. Characterizing the similarit yof the inputs pla ys a crucial role in determining the success or failure of the learning algorithm, and it is one of the central questions in the field of machine learning.
 (e.g., vectors, texts, or parse trees) and outputs a real number characterizing their similarity, with the property that the function is symmetric and positive semi-definite. That is, for all x i , x j  X  X satisfies where  X  is an (implicit) mapping from X to an (inner product) feature space or kernel perceptrons as the interface between the algorithm and the data. The kernel function is then the onl ydomain specific element of the s ystem, while the learning algorithm is a general purpose component.
 is to map the set of training data into a high-dimensional feature space function  X  : X  X  F , and construct a separating hyperplane with maximum margin (i.e., 514 the minimum distance between the hyperplane and data points) in that space. The use of an appropriate non-linear transformation  X  of the input yields a nonlinear decision boundar yin the input space. Kernel functions make possible the use of feature spaces with an exponential or even infinite number of dimensions. Instead of performing the explicit feature mapping  X  , one can use a kernel function, which permits the (efficient) computation of inner products in high-dimensional feature spaces without explicitly carrying out the mapping  X  . This is called the kernel trick in the machine learning literature (Boser, Guyon, and Vapnik 1992).
 bine individual kernels to form composite ones. Of course, not ever ysimilarit yfunction is a valid kernel because, b ydefinition, kernels should be equivalent to some inner product in a feature space. The function K : X  X  X  X  R is a valid kernel provided that its kernel matrices 1 are positive semi-definite 2 for all training sets S = so-called finitel ypositive semi-definite propert y. Note that defining similarit ymeasures b ymeans of kernels ma ybe more intuitive than performing the explicit mapping in the feature space. Furthermore, this formulation does not require the set X to be a vector space: for example, we shall define kernels that take strings as input.
 functions that onl yimplicitl ycorrespond to a feature mapping  X  . Another consequence ite ones. This will allow us to integrate heterogeneous sources of information in a simple and effective way. We shall use the following properties of kernels to define our compos-providing an explicit feature mapping  X  : X  X  R n ; (2) defining a similarit yfunction that is symmetric and positive semi-definite; and (3) composing different valid kernels, using the closure properties of kernels. This forms the basis for the approach described in the following section. 3. Kernel Methods for WSD
Our approach to WSD consists of representing linguistic phenomena independentl yand then defining a combination method to integrate them. As described in the previous sec-tion, the kernel function is the onl ytask-specific component of the learning algorithm.
Thus, to develop a WSD system, we only need to define appropriate kernel functions to represent the domain and syntagmatic aspects of sense distinction and, second, exploit the properties of kernel functions to define a composite kernel to combine and extend the individual kernels.
 syntagmatic kernels. The former family, described in Section 3.1, models the domain aspects of sense distinction; it is composed of the domain kernel ( K words kernel ( K BoW ). The latter, described in Section 3.2, represents the syntagmatic aspects of sense distinction; it is composed of the collocation kernel ( K of-speech kernel ( K PoS ). Finally, Section 3.3 describes the composite kernel for WSD. 3.1 Domain Kernels
It has been shown that domain information is fundamental for WSD (Magnini et al. 2002). For instance, the (domain) polysemy between the computer science and the medicine senses of the word virus can be solved b yconsidering the domain of the context in which it appears. Gliozzo, Strapparava, and Dagan (2004) proposed a WSD method that exploits onl ydomain information.
 a kernel function that estimates the domain similarit ybetween the contexts of the words to be disambiguated. The simplest method to estimate the domain similarity between two texts is to compute the cosine similarit yof their vector representations in the vector space model (VSM). The VSM is a k -dimensional space text t j is represented b ya vector t j , where the i th component is the term frequenc yof the term w i in t j . However, such an approach does not deal well with lexical variability and ambiguity. For instance, despite the fact that the sentences He is affected by AIDS and HIV is a virus express closely-related concepts, their similarity is zero in the VSM because the yhave no words in common (the yare represented b yorthogonal vectors).
On the other hand, due to the ambiguit yof the word virus , the similarit ybetween the sentences The laptop has been infected by a virus and HIV is a virus is greater than zero, even though the yconve yver ydifferent messages.
 use it to define a domain VSM in which texts and terms are represented in a uniform way. A DM is composed of soft clusters of terms. Each cluster represents a semantic domain, that is, a set of terms that often co-occur in texts having similar topics. A DM is represented b ya k  X  k rectangular matrix D , containing the degree of association among terms and domains, as illustrated in Table 1.
 represented in the standard VSM into the vector t j in the domain VSM.
 as follows: 3 516 where t j is represented as a row vector, I IDF is a k  X  k diagonal matrix such that i
IDF ( w i ), and IDF ( w i ) is the inverse document frequenc yof w relations among terms. For example, the similarit yof the two sentences He is affected by AIDS and HIV is a virus is ver yhigh, because the terms AIDS , HIV ,and virus are strongl yassociated with the medicine domain.

Net Domains (Magnini and Cavagli ` a 2000), or b yperforming a term-clustering process on a (large) corpus. However, the second approach is more attractive because it allows us to automaticall yacquire DMs for different languages and domains.
 tion (SVD) to acquire DMs from a corpus represented b yits term-b y-document matrix
T , in a unsupervised way. 4 SVD decomposes the term-by-document matrix T into three matrixes T V  X   X   X  k U T , where V and U are orthogonal matrices (i.e., V U T U = I ) whose columns are the eigenvectors of TT T and T is the diagonal k  X  k matrix containing the highest k k eigenvalues of T , and all the remaining elements set to 0. The parameter k is the dimensionalit yof the domain VSM and can be fixed in advance. Under this setting, we define the domain matrix D as follows: where I N is a diagonal matrix such that i N i , i = 1
V  X  columns of the matrix D and the ydo not have an explicit name. B yusing a small number of domains, we can define a ver ycompact representation of the DM and, con-sequently, reduce the memory requirements while preserving most of the information.
There exist ver yefficient algorithms to perform the SVD process on sparse matrices, allowing us to perform this operation on large corpora in a ver ylimited time and with reduced memor yrequirements. 6 between the contexts of the words to be disambiguated. It is a variant of the latent semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to define an explicit mapping D : R k  X  R k from the classical VSM into the domain VSM. The domain kernel is explicitl ydefined as follows: where D is the domain mapping defined in Equation (2). words features from a wide window of text around the words to be disambiguated.
Based on this representation, we define a linear kernel called the bag-of-words kernel ( K
BoW ). K BoW is a particular case of the domain kernel in which D = I in Equation (2), where I is the identit ymatrix. The BoW kernel does not require a DM; therefore, it can be applied to the strictl ysupervised settings, in which external knowledge is not available.
 supervised learning process; it will be compared and combined with the standard bag-of-words approach in Section 4. In the following section, we shall see that domain models are also useful for defining soft-matching collocation kernels. 3.2 Syntagmatic Kernels
Collocations (such as bigrams and trigrams) extracted from the local context of the word to be disambiguated are typically used to capture syntagmatic relations (Yarowsky 1994). However, traditional approaches to WSD fail to represent non-contiguous or shifted collocations, and fail to consider lexical variability. For example, suppose we have to disambiguate the verb to score in the sentence Ronaldo scored the first goal ,given the labeled example The football player scored two goals in the second half as training. A traditional approach has no clues to return the right answer because the two sentences have no features in common.
 b yrepresenting (non-contiguous) collocations and exploiting external lexical knowl-edge sources to define non-zero measures of similarit ybetween words (soft-matching criteria). In this formulation, words taken in their context are compared b ykernels that sum the number of common (non-contiguous) collocations of words, considering lexical variability, and part-of-speech tags, avoiding an explicit feature mapping that would lead to an exponential number of features.
 to compute the inner product among images of strings in high-dimensional feature space using dynamic programming techniques. The gap-weighted subsequences kernel is one of the most general types of kernel based on sequences. Roughly speaking, it compares two strings b ymeans of the number of contiguous and non-contiguous substrings of a given length the yhave in common. Non-contiguous occurrences are penalized according to the number of gaps the ycontain. Formall y, let  X  be an al-phabet of |  X  | symbols, and s = s 1 s 2 ... s | s | be a finite sequence over  X  (i.e., s i | s | ). Let i = [ i in s ; we will denote as s [ i ]  X   X  n the subsequence s i necessaril yform a contiguous subsequence of s ; for example, if s is the sequence  X  X onaldo scored the first goal X  and i = [2,5], then s [ i ] is  X  X cored goal X . The length weighted subsequences kernel of length n is indexed by I = X  given by 518 where 0 &lt; X  1 is the deca yfactor used to penalize non-contiguous subsequences. associate kernel is defined as
To evaluate K n more efficiently, we use the recursive formulation based on a dynamic programming implementation (Lodhi et al. 2002; Saunders, Tschach, and Shawe-Taylor 2002; Cancedda et al. 2003). It is defined in the following equations: where K n and K n are auxiliar yfunctions with a similar definition to K the computation. Based on these definitions, K n can be computed in O ( n this recursive definition, it turns out that computing all kernel values for subsequences of lengths up to n is not significantl ymore costl ythan computing the kernel for n only. that operate at word and part-of-speech tag level. In particular, following the approach proposed b yCancedda et al. (2003), it is possible to adapt sequence kernels to operate at word level b yinstancing the alphabet  X  with the vocabulary
Moreover, we restrict the generic definition of the gap-weighted subsequences kernel to recognize collocations in the local context of a specified word. The resulting kernel, called the n-gram collocation kernel ( K n Coll ), operates on sequences of lemmata around a specified word l 0 (i.e., l  X  3 , l  X  2 , l  X  1 , l 0 , l estimate the number of common (sparse) subsequences of lemmata (i.e., collocations) between two examples, in order to capture syntagmatic similarity. Analogously, we define the part-of-speech kernel ( K n PoS ) to operate on sequences of part-of-speech tags and (15), respectively.

Both kernels depend on the parameter n , the length of the non-contiguous subse-bigrams in the local context of a word. Finally, the syntagmatic kernel is defined as
To solve this problem, external lexical knowledge is fed into the supervised learning process, allowing us to define the soft-matching collocation kernel. In particular, we de-fine two alternative soft-matching criteria by exploiting synonymy relations in WordNet and DMs acquired from corpora. Both criteria are based on the assumption that every word in a sentence can be substituted b yanother preserving the original meaning, if these words are paradigmatically related (e.g., synonyms, hyponyms, or domain related words). For example, if we consider as equivalent the terms Ronaldo and football player , first goal , providing a strong evidence to disambiguate the verb to score in the second sentence.
 matching gap-weighted subsequences kernel is now calculated recursivel yusing Equa-tions (7) X (9), (11), and (12), replacing Equation (10) b ythe equation: and modifying Equation (13) to: where a xy are entries in a similarit ymatrix A between terms. In order to ensure that the resulting kernel is still valid, A must be positive semi-definite.
 based on WordNet Synonymy and Domain Proximity, respectively. To show that the similarit ymatrices are positive semi-definite, we use the following result. Proposition 1 Amatrix A is positive semi-definite if and onl yif A = B T The proof is given in Shawe-Taylor and Cristianini (2004).

WordNet Synonymy. The first soft-matching criterion is based on WordNet a similarit ymatrix between words. In particular, we substitute two words if the yare synonyms. To this end, a word is represented as vector whose dimensions are associated 520 with the synsets. Formally, we define the term-by-synset matrix S as the matrix whose rows are indexed b ythe terms and whose columns are indexed b ythe s ynsets. The ( i , j )th entr yof S is 1 if the synset s j contains the term w
S gives rise to the similarit ymatrix A = SS T between terms. Because A can be re-semi-definite.

Domain Proximity. The second soft-matching criterion exploits the domain models intro-duced in Section 3.1 to define a similarit ymatrix between words. Once a DM has been defined b ythe matrix D , the domain space is a k dimensional space, in which both texts and terms are represented b ymeans of domain vectors, that is, vectors representing the domain relevances among the linguistic object and each domain. The domain vector w for the term w i  X  V is the i th row of D , where V = { w the corpus. The term-by-domain matrix D gives rise to the similarit ymatrix A = DD between terms. It follows b yProposition 1 that A is positive semi-definite. and trigrams of lemmata and part-of-speech tags typically used as features in WSD. 3.3 Composite Kernel
Having defined all the individual kernels representing syntagmatic and domain aspects of sense distinction, we can define the composite kernel to combine and extend the individual kernels. The closure properties of the kernel functions allows us to define the composite kernel as where K l is a valid individual kernel. The individual kernels are normalized X  X his plays an important role in allowing us to integrate information from heterogeneous feature spaces.

Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empiricall yshown the effec-tiveness of combining kernels in this way: The composite kernel consistently improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source.
 learning, we defined two WSD kernels, K wsd and K wsd . The yare completel yspecified b y the n individual kernels that compose them in Equation (19).

K wsd is composed by K Coll , K PoS ,and K BoW ;
K wsd is composed by K Coll , K PoS , K BoW ,and K D .
 The onl ydifference between the two is that K wsd uses the domain kernel K external knowledge while K wsd onl yuses the labeled training data. 4. Evaluation
Experiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds 2004). First of all, we conducted a preliminar yset of experiments on the Catalan, English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.
Second, in order to show the general applicabilit yof the proposed method, we evalu-ated the system on the English all-words task; the results are presented in Section 4.2. customized to embed our own kernels. The parameters were optimized b yfive-fold cross-validation on the training set. 4.1 Lexical-Sample Tasks
In this section, we report the evaluation of our method on the Catalan, English, Italian, and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004). Table 2 describes the tasks we have considered. For each task, it summarizes the number of words to be disambiguated, the mean polysemy, the size of the labeled training set, the size of the test set, and the size of the unlabeled training set, respectively. For the
Catalan, Italian, and Spanish tasks, we acquired the DMs from the unlabeled corpora made available b ythe task organizers. For the English task, we used a DM acquired from the British National Corpus (BNC) as the task organizers have not provided an yunlabeled training data. The objectives of these experiments are to (a) estimate the impact of different knowledge sources in WSD; (b) stud ythe effectiveness of the kernel combination; (c) understand the benefits of plugging external information in a supervised framework; and (d) verif ythe portabilit yof our methodolog yto different languages. 4.1.1 Results. Table 3 reports the results of the individual kernels K K PoS and their combinations K wsd and K wsd (the baselines for the tasks are reported in
Table 5). In our experiments, the parameters n and  X  (see Equation (5)) are optimized b yfive-fold cross-validation. For K n Coll , we obtained the best results with n = 2and  X  = 0 . 5. For K n PoS , n = 3and  X   X  0. The domain cardinality k was set to 50. Table 4 shows the performance of the syntagmatic kernel in different configurations: hard and soft matching. As a baseline, we report the result of a standard approach consisting of explicit bigrams and trigrams of words and part-of-speech tags around the words to be disambiguated (Yarowsk y1994). We evaluated the impact of the domain kernel on the overall performance b ycomparing the learning curves of K lexical-sample tasks. Figure 1 shows the results of our experiments. The points of the learning curves are obtained b ysampling the same percentage of training examples for 522 each word. Finally, Table 5 summarizes the results we obtained, providing a comparison with the state of the art. 4.1.2 Discussion. Table 3 shows that domain information and syntagmatic information are crucial for WSD, and their combination significantl youtperforms the individual kernels, showing the effectiveness of the kernel combination method.
 and the composite kernel K wsd that makes use of domain information outperforms the one K wsd based onl yon the labeled training data, demonstrating our assumption (see Section 3).
 trigrams) in an yconfiguration (hard-/soft-matching). The soft-matching criteria further improve the classification performance. It is interesting to note that the domain proxim-ity obtained better results than WordNet synonymy (note that we do not have a Catalan or a Spanish WordNet). The different results observed for Italian and English using the domain proximit ysoft-matching criterion are probabl ydue to the small size of the unlabeled English corpus.
 small number of examples. It is worth noting, as reported in Table 5, that K the same performance as K wsd using about half of the labeled training data. This result shows that the proposed semi-supervised learning approach consisting of acquiring domain models from unlabeled corpora is effective, as it allows us to drasticall yreduce the amount of labeled training data and provide a viable solution for the knowledge acquisition bottleneck problem in WSD.
 tasks of Senseval-3, further improving the state of the art b y0.4% to 8.2% for English and Italian, respectively. Finally, we have demonstrated the language independency of our approach. The DMs have been acquired for different languages from different unlabeled corpora b yadopting exactl ythe same methodolog y, without requiring an y external lexical resource or ad hoc rule. 4.2 All-Words Task
Encouraged b ythe excellent results obtained on the lexical-sample tasks, we evaluated our approach on the all-words task, in which a ver ysmall amount of labeled training 524 data is typically available. We performed the evaluation on the English all-words task of Senseval-3 (Snyder and Palmer 2004). The test set was extracted from two Wall Street
Journal articles and one text from the Brown Corpus. The test set consists of 945 words (2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses. The inter-annotator agreement rate in the preparation of the corpus was approximatel y72.5%. The most frequent (MF) baseline using the first WordNet sense heuristic obtained 60.9%.
Net 1.7.1 as sense repository; (2) SemCor, 9 considering onl ythose words appearing in the Senseval-3 all-words data set X  X e extracted about 61,700 tagged examples that constitute the onl ylabeled training set exploited b ythe s ystem; and (3) the BNC, from which we extracted the unlabeled training data. 4.2.1 Results. We trained 734 word-expert classifiers on the SemCor corpus. The labeled examples for each classifier range from a minimum of one example to a maximum of 2,275 examples. We return a random sense for those words that have no training examples in SemCor. 10 We have acquired two DMs, one from the BNC (i.e., same we used in the lexical-sample task) and one from SemCor (i.e., slightl ybetter performance with the latter.
 and their composite kernels K wsd ,  X  K bnc D and  X  K sem stand the results obtained, we performed an evaluation on the subset of the test set for which at least one training example is available in SemCor. Evaluating onl yon these words the performance increases from 65.2% to 70.0%, and the most frequent baseline becomes 65.7%. Tables 7 and 8 present a more detailed analysis that considers results grouped according to the amount of training available and the mean polysemy of the words in the test set, excluding from the data set the monosemous words. Table 7 shows of  X 
K sem wsd on those words that have a given number of training examples. This evaluation is limited to the best composite kernel  X  K sem wsd . 4.2.2 Discussion. We compared our approach with the three best systems that par-ticipated in the English all-words task of Senseval-3. The best system (Decadt et al. 2004) has comparable performance (65.2) to ours; however, it uses a larger training set composed of 563,129 sense-tagged words. The training corpus was built b ymerging
SemCor, and English lexical-sample and all-words data sets taken from all the previous editions of Senseval. The system proposed by Mihalcea and Faruque (2004) scored sec-ond (64.6). The dimension of their training set is comparable to ours; however, the yalso use additional information drawn from WordNet to derive semantic generalizations using syntactic dependencies. Finally, the third system (Yuret 2004) obtained 64.1 using a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks).
 limited amount of unlabeled data is sufficient to improve the overall performance, and the use of unlabeled data taken from the training set helps to slightl yimprove the overall performance. However, the domain model can be acquired from a different corpus (e.g., the BNC) without significantl yaffecting the overall performance. ambiguate with good accurac y( F 1 = 76%) words with a number of training examples that ranges from 1 to 10, outperforming the most frequent baseline b y3%. This is an interesting result given the extremel ysmall number of training examples available. On the other hand, the more training is available for a given word, the more polysemous that word is. Nevertheless, the algorithm always outperforms the baseline and has a more significant difference for increasing values of the mean polysemy (from 3% to 16%). These results, together with the ones obtained in the lexical sample tasks, show that the domain kernel is able to boost the overall performance when little training data are available, as well as with enough training data. The benefit is even more pronounced for the latter case, even though the disambiguation task is more complex due to the high polysemy of highly frequent words. 5. Conclusions This article summarizes the results of a word expert semi-supervised algorithm for
WSD based on a combination of kernel functions. First, we evaluated our methodology 526 on four lexical-sample tasks of Senseval-3, significantl yimproving the state of the art for all of them. In particular, we demonstrated that using external knowledge inside a supervised framework is a viable methodolog yto reduce the amount of training data required for learning. In our approach, the external knowledge is represented b ymeans of domain models automaticall yacquired from corpora in a totall yunsupervised wa y.
Then, we applied the method so defined to the English all-words task of Senseval-3, achieving state-of-the-art performance while requiring less labeled training data compared to the other systems we have found in the literature.
 duced b ya parser. In the framework of kernel methods, this expansion can be done b y adding a tree kernel (i.e., a kernel function that evaluates the similarit yamong parse trees) to our composite kernel. However, the performance achieved is close to the upper bound, if we consider the inter-annotator agreement as an indication of the upper-bound performance.
 solution for developing a sense-tagging system. Indeed, we tested the system on the
English lexical-sample task of SemEval 2007, still obtaining state-of-the-art performance (Pradhan et al. 2007). Therefore, we plan to make available an optimized version of our system, and to exploit it for ontology learning, textual entailment, and information retrieval.
 Acknowledgments References
