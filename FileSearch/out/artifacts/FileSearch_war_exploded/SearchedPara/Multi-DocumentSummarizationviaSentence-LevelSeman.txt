 Multi-do cumen t summarization aims to create a compressed summary while retaining the main characteristics of the orig-inal set of documen ts. Man y approac hes use statistics and mac hine learning techniques to extract sen tences from doc-umen ts. In this pap er, we prop ose a new multi-do cumen t summarization framew ork based on sen tence-lev el seman-tic analysis and symmetric non-negativ e matrix factoriza-tion. We rst calculate sen tence-sen tence similarities using seman tic analysis and construct the similarit y matrix. Then symmetric matrix factorization, whic h has been sho wn to be equiv alen t to normalized spectral clustering, is used to group sen tences into clusters. Finally , the most informativ e sen-tences are selected from eac h group to form the summary . Exp erimen tal results on DUC2005 and DUC2006 data sets demonstrate the impro vemen t of our prop osed framew ork over the implemen ted existing summarization systems. A further study on the factors that bene t the high perfor-mance is also conducted.
 H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al| Clustering ; I.2.7 [ Arti cial Intelli-gence ]: Natural Language Pro cessing| Text clustering Algorithms, Exp erimen tation, Performance Multi-do cumen t summarization, Sen tence-lev el seman tic anal-ysis, Symmetric non-negativ e matrix factorization
Multi-do cumen t summarization is the pro cess of generat-ing a generic or topic-fo cused summary by reducing docu-men ts in size while retaining the main characteristics of the original documen ts [21, 27]. Since one of the problems of data overload is caused by the fact that man y documen ts share the same or similar topics, automatic multi-do cumen t summarization has attracted much atten tion in recen t years. With the explosiv e increase of documen ts on the Internet, there are various summarization applications. For example, the informativ e snipp ets generation in web searc h can assist users in further exploring [31], and in a Qestion/Answ er sys-tem, a question-based summary is often required to pro vide information ask ed in the question [14]. Another example is short summaries for news groups in news services, whic h can facilitate users to better understand the news articles in the group [28].

The ma jor issues for multi-do cumen t summarization are as follo ws [32]: rst of all, the information con tained in dif-feren t documen ts often overlaps with eac h other, therefore, it is necessary to nd an e ectiv e way to merge the docu-men ts while recognizing and remo ving redundancy . In En-glish to avoid rep etition, we tend to use di eren t word to describ e the same person, the same topic as a story goes on. Thus simple word-matc hing types of similarit y suc h as cosine can not faithfully capture the con ten t similarit y. Also the sparseness of words between similar concepts mak e the simi-larit y metric unev en. Another issue is iden tifying imp ortan t di erence between documen ts and covering the informativ e con ten t as much as possible [25]. Curren t documen t summa-rization metho ds usually involve natural language pro cessing and mac hine learning techniques [29, 2, 34], suc h as classi -cation, clustering, conditional random elds (CRF) [4], etc. Section 2 will explicitly discuss these existing metho ds.
In this pap er, to address the above two issues, we prop ose a new framew ork based on sen tence-lev el seman tic analy-sis (SLSS) and symmetric non-negativ e matrix factorization (SNMF). Since SLSS can better capture the relationships be-tween sen tences in a seman tic manner, we use it to construct the sen tence similarit y matrix. Based on the similarit y ma-trix, we perform the prop osed SNMF algorithm to cluster the sen tences. The standard non-negativ e matrix factoriza-tion(NMF) deals with a rectangular matrix and is thus not appropriate here. Finally we select the most informativ e sen tences in eac h cluster considering both internal and ex-ternal information. We conduct exp erimen ts on DUC2005 and DUC2006 data sets, and the results sho w the e ectiv e-ness of our prop osed metho d. The factors that bene t the high performance are further studied.

The rest of the pap er is organized as follo ws. Section 2 dis-cusses the related work of curren t metho ds in multi-do cumen t summarization. Our prop osed metho d including SLSS and SNMF algorithm is describ ed in Section 3. Various exp er-imen ts are set up and the results are sho wn in Section 4. Section 5 concludes.
Multiple documen t summarization has been widely stud-ied recen tly. In general, there are two types of summariza-tion: extractiv e summarization and abstractiv e summariza-tion [16, 15]. Extractiv e summarization usually ranks the sen tences in the documen ts according to their scores calcu-lated by a set of prede ned features, suc h as term frequency-inverse sen tence frequency (TF-ISF) [26, 20], sen tence or term position [20, 33], and num ber of keyw ords [33]. Ab-stractiv e summarization involves information fusion, sen-tence compression and reform ulation [16, 15]. In this pap er, we study sen tence-based extractiv e summarization.
Gong et al. [12] prop ose a metho d using laten t seman tic analysis (LSA) to select highly rank ed sen tences for summa-rization. [11] prop oses a maximal marginal relev ance (MMR) metho d to summarize documen ts based on the cosine simi-larit y between a query and a sen tence and also the sen tence and previously selected sen tences. MMR metho d tends to remo ve redundancy , however the redundancy is con trolled by a parameterized mo del whic h actually can be automat-ically learned. Other metho ds include NMF-based topic speci c summarization [24], CRF-based summarization [29], and hidden Mark ov mo del (HMM) based metho d [5]. Some DUC2005 and DUC2006 participan ts achiev e good perfor-mance suc h as Language Computer Corp oration (LCC) [1], that prop oses a system com bining the question-answ ering and summarization system and using k-Nearest Neigh bor clustering based on cosine similarit y for the sen tence selec-tion. In addition, some graph-ranking based metho ds are also prop osed [22, 9]. Most of these metho ds ignore the de-pendency syn tax in the sen tence level and just focus on the keyw ord co-o ccurrence. Thus the hidden relationships be-tween sen tences need to be further disco vered. The metho d prop osed in [13] groups sen tences based on the seman tic role analysis, however the work does not mak e full use of clus-tering algorithms.

In our work, we prop ose a new framew ork based on sen tence-level seman tic analysis (SLSS) and symmetric non-negativ e matrix factorization (SNMF). SLSS can better capture the relationships between sen tences in a seman tic manner and SSNF can factorize the similarit y matrix to obtain meaning-ful groups of sen tences. Exp erimen tal results demonstrate the e ectiv eness of our prop osed framew ork.
Figure 1 demonstrates the framew ork of our prop osed ap-proac h. Giv en a set of documen ts whic h need to be sum-marized, rst of all, we clean these documen ts by remo ving formatting characters. In the similarit y matrix construction phase, we decomp ose the set of documen ts into sen tences, and then parse eac h sen tence into frame(s) using a seman tic role parser. Pairwise sen tence seman tic similarit y is cal-culated based on both the seman tic role analysis [23] and word relation disco very using WordNet [10]. Section 3.2 will describ e this phase in detail. Once we have the pairwise sen tence similarit y matrix, we perform the symmetric ma-trix factorization to group these sen tences into clusters in the second phase. Full explanations of the prop osed SNMF algorithm will be presen ted in section 3.3. Finally , in eac h cluster, we iden tify the most seman tically imp ortan t sen-tence using a measure com bining the internal information (e.g., the computed similarit y between sen tences) and the external information (e.g., the given topic information). Sec-tion 3.4 will discuss the sen tence selection phase in detail. These selected sen tences nally form the summary .
After remo ving stemming and stopping words, we trunk the documen ts in the same topic into sen tences. Simple word-matc hing types of similarit y suc h as cosine can not faithfully capture the con ten t similarit y. Also the sparseness of words between similar concepts mak e the similarit y metric unev en. Thus, in order to understand the seman tic meanings of the sen tences, we perform seman tic role analysis on them and prop ose a metho d to calculate the seman tic similarit y between any pair of sen tences.
A seman tic role is \a description of the relationship that a constituen t plays with resp ect to the verb in the sen-tence" [3]. Seman tic role analysis plays very imp ortan t role in seman tic understanding. The seman tic role lab eler we use in this work is based on PropBank seman tic annotation [23]. The basic idea is that eac h verb in a sen tence is lab eled with its prop ositional argumen ts, and the lab eling for eac h par-ticular verb is called a \frame" . Therefore, for eac h sen tence, the num ber of frames generated by the parser equals to the num ber of verbs in the sen tence. There is a set of abstract argumen ts indicating the seman tic role of eac h term in a frame. For example, Arg0 is typically the actor, and Arg1 is the thing acted upon. The full represen tation of the ab-stract argumen ts [23] and an illustrativ e example are sho wn in Table 1.
Giv en sen tence S i and S j , now we calculate the similarit y between them. Supp ose S i and S j are parsed into frames resp ectiv ely. For eac h pair of frames f m 2 S i and f S , we disco ver the seman tic relations of terms in the same seman tic role using WordNet [10]. If two words in the same seman tic role are iden tical or of the seman tic relations suc h rel: the verb Arg0: causer of motion Arg1: thing in motion Arg2: distance moved Arg3: start point Arg4: end point Arg5: direction ArgM-LOC: location ArgM-EXT: exten t ArgM-TMP: time ArgM-DIS: discourse connectiv es ArgM-PNC: purp ose ArgM-AD V: general-purp ose ArgM-MNR: manner ArbM-NEG: negation mark er ArgM-DIR: direction ArgM-MOD: mo dal verb ArgM-CA U: cause Example :
Sentenc e : A num ber of marine plan ts are harv ested commercially in Nova Scotia.

Label : A j Arg1 num ber j Arg1 of j Arg1 marine j Arg1 plan ts j Arg1 are j -harv ested j rel commercially j ArgM-MNR in j ArgM-LOC Nova j ArgM-LOC Scotia j ArgM-LOC . j -Table 1: Represen tation of argumen ts and an illus-trativ e example. as synon ym, hypern ym, hyponym, meron ym and holon ym, the words are considered as \related" .

Let R m and R n be the seman tic roles in f m and f n , re-spectiv ely. Assume R m R n . Let f r 1 ; r 2 ; :::; r k g be the set of K common seman tic roles between f m and f n , T m ( r the term set of f m in role r i , and T n ( r i ) be the term set of f similarit y between T m ( r i ) and T n ( r i ) as: where Then the similarit y between f m and f n is Therefore, the seman tic similarit y between S i and S j can be calculated as follo ws.

Eac h similarit y score is between 0 and 1. Thus, we com-pute pairwise sen tence similarit y for the given documen t col-lection and construct the symmetric seman tic similarit y ma-trix for further analysis.
Most documen t clustering algorithms deal with a rectan-gular data matrix (e.g., documen t-term matrix, sen tence-term matrix) and they are not suitable for clustering pair-wise similarit y matrix. In our work, we prop ose the SNMF algorithm to conduct the clustering in the second phase. It can be sho wn that the simple symmetric nonnegativ e matrix factorization approac h is equiv alen t to normalized spectral clustering.
Giv en a pairwise similarit y matrix W , we want to nd H suc h that where the matrix norm jj X jj 2 = ij X 2 ij is the Frob enius norm. To deriv e the updating rule for Eq.(4) with non-negativ e constrain ts, h ij 0, we introduce the Lagrangian multipliers ij and let L = J + ij ij H ij . The rst order KKT condition for local minima is Note that @J @H = 4 W H + 4 HH T H: Hence the KKT condi-tion leads to the xed point relation: Using gradien t descen t metho d, we have plicativ e updating rule for SNMF: Hence, the algorithm pro cedure for solving SNMF is: given an initial guess of H , iterativ ely update H using Eq.(7) until con vergence. This gradien t descen t metho d will con verge to a local minima of the problem.
SNMF has sev eral nice prop erties that mak e it a powerful tool for clustering. First, one of the nice prop erties of the SNMF algorithm is its inheren t abilit y for main taining the near near-orthogonalit y of H . Note that Minimizing the rst term is equiv alen t to enforcing the or-thogonalit y among h s : h t s h t 0. On the other hand, since W HH T , where j h j is the L 1 norm of vector h . Hence jj h s jj 0. Therefore, we have
The near-orthogonalit y of columns of H is imp ortan t for data clustering. An exact orthogonalit y implies that eac h row of H can have only one non-zero elemen t, whic h leads to the hard clustering of data objects (i.e., eac h data ob-ject belongs to only 1 cluster). On the other hand, a non-orthogonalit y of H does not have a cluster interpretation. The near-orthogonalit y conditions of SNMF allo w for \soft clustering" , i.e., eac h object can belong fractionally to mul-tiple clusters. This usually leads to clustering performance impro vemen t [7].
Another imp ortan t prop erty is that the simple SNMF is equiv alen t to sophisticated normalized cut spectral cluster-ing. Spectral clustering is a principled and e ectiv e ap-proac h for solving Normalized Cuts [30], a NP-hard opti-mization problem. Giv en the adjacen t matrix W of a graph, it can be easily seen that the follo wing SNMF where
W = D 1 = 2 W D 1 = 2 ; D = diag ( d 1 ; ; d n ) ; d i = is equiv alen t to Normalized Cut spectral clustering. It can also be sho wn that SNMF is equiv alen t to Kernel K-means clustering and is a special case of 3-factor Nonneg-ativ e matrix factorization. These results validate the clus-tering abilit y of SNMF.

Kernel K-means Clustering: For clustering and clas-si cation problems, the solution is represen ted by K non-negativ e cluster mem bership indicator matrix: H = ( h 1 ), where For example, the nonzero entries of h 1 indicate the data points belonging to the rst cluster. The objectiv e function of K -means clustering is where f k is the cluster cen troid of the k -th cluster C points, i.e., f k = i 2 C function of Kernel K-means with mapping x i ! ( x i ) is where k is the cen troid in the feature space. Using cluster indicators, for K -means and Kernel K-means, the clustering problem can be solv ed via the optimization problem where H is the cluster indicator and W ij = ( x i ) T ( x the kernel. For K -means, ( x i ) = x i , W ij = x T i x j
Note that if we imp ose the orthogonalit y constrain t on H , then J 1 = arg min In other words, SNMF of W = HH T is equiv alen t to Kernel K-means clustering under the orthogonalit y constrain ts on H .

Nonnegativ e Matrix Factorization (NMF): SNMF can also be view ed as a special case of 3-factor nonnega-tive matrix factorizations. The 3-factor nonnegativ e matrix factorization is prop osed to sim ultaneously cluster the rows and the columns of the input data matrix X [8] Note that S pro vides additional degrees of freedom suc h that the low-rank matrix represen tation remains accurate while F gives row clusters and G gives column clusters. This form gives a good framew ork for sim ultaneously clustering the rows and columns of X [6, 18]. An imp ortan t special case is that the input X con tains a matrix of pairwise similarities: X = X T = W . In this case, F = G = H; S = I . This reduces to the SNMF:
After grouping the sen tences into clusters by the SNMF algorithm, in eac h cluster, we rank the sen tences based on the sen tence score calculation as sho wn in Eqs.(15, 16, 17). The score of a sen tence measures how imp ortan t a sen tence is to be included in the summary .
 where F 1 ( S i ) measures the average similarit y score between sen tence S i and all the other sen tences in the cluster C and N is the num ber of sen tences in C k . F 2 ( S i ) represen ts the similarit y between sen tence S i and the given topic T . is the weigh t parameter.
We use the DUC2005 and DUC2006 data sets to test our prop osed metho d empirically , both of whic h are open benc h-mark data sets from Documen t Understanding Conference (DUC) for automatic summarization evaluation. Eac h data set consists of 50 topics, and Table 2 gives a brief description of the two data sets. The task is to create a summary of no more than 250 words for eac h topic to answ er the informa-tion expressed in the topic statemen t.

Num ber of documen ts 25 50 25 relev ant to eac h topic
In order to compare our metho ds, rst of all, we imple-men t four most widely used documen t summarization base-line systems: For better evaluating our prop osed metho d, we also imple-men t alternativ e solutions for eac h phase of the summariza-tion pro cedure as listed in Table 3.

Measuremen t Similarit y based Within-Cluster M p = F 1 ( S i ) M 1 = M 2 = Table 3: Di eren t metho ds implemen ted in eac h phase. Remark: S i is the i th sen tence in the clus-ter, and the calculation of F 1 ( S i ) and F 2 ( S i ) is the same as describ ed in section 3.4.

In Table 3, the keyw ord-based similarit y between any pair of sen tences is calculated as the cosine similarit y. The pa-rameter in M p is set to 0.7 empirically , and the in uence of will be discussed in Section 4.4.4. Note that in our exp erimen ts, both similarit y matrix generation phase and sen tence extraction phase use the same type of similarit y measuremen ts. Thus, we have 22 implemen ted summariza-tion systems: 18 by varying metho ds in eac h phase, and 4 baselines. In section 4.4, we will compare our prop osed metho d with all the other systems.
We use ROUGE [19] toolkit (version 1.5.5) to measure our prop osed metho d, whic h is widely applied by DUC for performance evaluation. It measures the qualit y of a sum-mary by coun ting the unit overlaps between the candidate summary and a set of reference summaries. Sev eral au-tomatic evaluation metho ds are implemen ted in ROUGE, suc h as ROUGE-N, ROUGE-L, ROUGE-W and ROUGE-SU. ROUGE-N is an n-gram recall computed as follo ws. where n is the length of the n-gram, and ref stands for the reference summaries. Count match ( gram n ) is the maxi-mum num ber of n-grams co-o ccuring in a candidatae sum-mary and the reference summaries, and Count ( gram n ) is the num ber of n-grams in the reference summaries. ROUGE-L uses the longest common subsequence (LCS) statistics, while ROUGE-W is based on weigh ted LCS and ROUGE-SU is based on skip-bigram plus unigram. Eac h of these evaluation metho ds in ROUGE can generate three scores (recall, precision and F-measure). As we have similar con-clusions in terms of any of the three scores, for simplicit y, in this pap er, we only rep ort the average F-measure scores generated by ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-W and ROUGE-SU to compare our prop osed metho d with other implemen ted systems.
First of all, we compare the overall performance between our prop osed metho d (using SLSS and SNMF) and all the other implemen ted systems. Table 4 and Table 5 sho w the ROUGE evaluation results on DUC2006 and DUC2005 data sets resp ectiv ely. We clearly observ e that our prop osed metho d achiev es the highest ROUGE scores and outp er-forms all the other systems. In section 4.4.2, 4.4.3 and 4.4.4, we evaluate eac h phase of our prop osed metho d and try to analyze all the factors that our metho d bene ts from.
Actually , instead of using similarit y matrix, man y sum-marization metho ds directly perform on the terms by sen-tences matrix, suc h as the LSA and NMFBase whic h are implemen ted as baseline systems in our exp erimen ts. In fact, LSA and NMF give con tinuous solutions to the same K-means clustering problem [7]. Their di erence is the con-strain ts: LSA relaxes the non-negativit y of H , while NMF relaxes the orthogonalit y of H . In NMFbase or LSA, we treat sen tence as vectors and clustering them using cosine similarit y metric (since eac h documen t is normalized to 1, j d 1 d 2 j 2 = 2 2 cos ( d 1 ; d 2)). From Table 4 and 5, we can see the results of LSA and NMFbase are similar and all of these metho ds are not satisfactory . This indicates that sim-ple word-matc hing types of similarit y suc h as cosine can not faithfully capture the con ten t similarit y.

Therefore, we further analyze the sen tence-lev el text and generate pairwise sen tence similarit y. In the exp erimen ts, we compare the prop osed sen tence-lev el seman tic similarit y with the traditional keyw ord-based similarit y calculation. In order to better understand the results, we use Figure 2 and 3 to visually illustrate the comparison. Due to space limit, we only sho w ROUGE-1 results in these gures.
 Figure 2: Metho ds comparison in similarit y matrix construction phase using ROUGE-1 on DUC2006 data set the metho ds used in eac h phase. For example, \ KM + M (keyw ord)" represen ts that keyw ord-based similar-in Table 3.
 Figure 3: Metho ds comparison in similarit y matrix construction phase using ROUGE-1 on DUC2005 data set
The results clearly sho w that no matter whic h metho ds are used in other phases, SLSS outp erforms keyw ord-based similarit y. This is due to the fact that SLSS better captures the seman tic relationships between sen tences.
Now we compare di eren t clustering algorithms in Fig-ure 4 and 5. We observ e that our prop osed SNMF algo-rithm achiev es the best results. K-means and NMF meth-ods are generally designed to deal with a rectangular data matrix and they are not suitable for clustering the similarit y matrix. Our SNMF metho d, whic h has been sho wn to be equiv alen t normalized spectral clustering, can generate more meaningful clustering results based on the input similarit y matrix.
 Figure 4: Di eren t clustering algorithms using ROUGE-1 on DUC2006 data set
Figure 6 and Figure 7 demonstrate the in uence of the weigh t parameter in the within-cluster sen tence selection phase. When = 1 (it is actually metho d M 1 ), only internal information coun ts, i.e. the similarit y between sen tences. And = 0 represen ts that only the similarit y between the sen tence and the given topic is considered (metho d M 2 ). We Figure 5: Di eren t clustering algorithms using ROUGE-1 on DUC2005 data set Figure 6: Study of weigh t parameter using ROUGE-1 on DUC2006 data set Figure 7: Study of weigh t parameter using ROUGE-1 on DUC2005 data set gradually adjust the value of , and the results sho w that com bining both internal and external information leads to better performance.
In this pap er, we prop ose a new multi-do cumen t summa-rization framew ork based on sen tence-lev el seman tic analy-sis (SLSS) and symmetric non-negativ e matrix factorization (SNMF). SLSS is able to capture the seman tic relationships between sen tences and SNMF can divide the sen tences into groups for extraction. We conduct exp erimen ts on DUC2005 and DUC2006 data sets using ROUGE evaluation meth-ods, and the results sho w the e ectiv eness of our prop osed metho d. The good performance of the prop osed framew ork bene ts from the sen tence-lev el seman tic understanding, the clustering over symmetric similarit y matrix by the prop osed SNMF algorithm, and the within-cluster sen tence selection using both internal and external information.
 The pro ject is partially supp orted by a researc h gran t from NEC Lab, NSF CAREER Aw ard IIS-0546280, and IBM Fac-ulty Researc h Aw ards. [1] http://www-nlpir.nist.go v/pro jects/duc/pubs/. [2] M. Amini and P. Gallinari. The use of unlab eled data [3] D. Arnold, L. Balk an, S. Meijer, R. Humphreys, and [4] C. M. Bishop. Pattern Recognition and Machine [5] J. Conro y and D. O'Leary . Text summarization via [6] I. S. Dhillon. Co-clustering documen ts and words [7] C. Ding and X. He. K-means clustering and principal [8] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [9] G. Erk an and D. Radev. Lexpagerank: Prestige in [10] C. Fellbaum. Wor dNet: An Electronic Lexic al [11] J. Goldstein, M. Kan trowitz, V. Mittal, and [12] Y. Gong and X. Liu. Generic text summarization [13] S. Harabagiu and F. Lacatusu. Topic themes for [14] T. Hirao, Y. Sasaki, and H. Isozaki. An extrinsic [15] H. Jing and K. McKeo wn. Cut and paste based text [16] K. Knigh t and D. Marcu. Summarization beyond [17] D. D. Lee and H. S. Seung. Algorithms for [18] T. Li. A general mo del for clustering binary data. In [19] C.-Y. Lin and E.Ho vy. Automatic evaluation of [20] C.-Y. Lin and E. Hovy. From single to multi-do cumen t [21] I. Mani. Automatic summarization . John Benjamins [22] R. Mihalcea and P. Tarau. A language indep enden t [23] M. Palmer, P. Kingsbury , and D. Gildea. The [24] S. Park, J.-H. Lee, D.-H. Kim, and C.-M. Ahn. [25] D. Radev, E. Hovy, and K. Mckeown. Introduction to [26] D. Radev, H. Jing, M. Stys, and D. Tam.
 [27] B. Ricardo and R. Berthier. Modern information [28] G. Sampathsampath and M. Martino vic. A Multilevel [29] D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. [30] J. Shi and J. Malik. Normalized cuts and image [31] A. Turpin, Y. Tsega y, D. Hawking, and H. Williams. [32] X. Wan, J. Yang, and J. Xiao. Manifold-ranking based [33] W.-T. Yih, J. Goodman, L. Vanderw ende, and [34] H. Zha. Generic summarization and keyphrase
