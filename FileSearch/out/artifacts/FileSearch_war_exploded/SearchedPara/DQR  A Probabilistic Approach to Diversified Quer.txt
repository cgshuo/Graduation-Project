 Web search queries issued by casual users are often short and with limited expressiveness. Query recommendation is a popular tech-nique employed by search engines to help users refine their queries. Traditional similarity-based methods, however, often result in re-dundant and monotonic recommendations. We identify five basic requirements of a query recommendation system. In particular, we focus on the requirements of redundancy-free and diversified rec-ommendations. We propose the DQR framework, which mines a search log to achieve two goals: (1) It clusters search log queries to extract query concepts, based on which recommended queries are selected. (2) It employs a probabilistic model and a greedy heuris-tic algorithm to achieve recommendation diversification. Through a comprehensive user study we compare DQR against five other recommendation methods. Our experiment shows that DQR out-performs the other methods in terms of relevancy, diversity, and ranking performance of the recommendations.
 H.2.8 [ Database Management ]: Database Applications X  Data min-ing Query recommendation, query concept, diversification
The effectiveness of keyword-based search engines, such as Google and Bing, depends largely on the ability of a user to formulate proper queries that are both expressive and selective . An expressive query clearly and unambiguously describes a user X  X  search intent, while a selective query results in a relatively small set of match-ing documents. Properly formulated queries bring users directly to the desired information, which make search engines powerful tools for tapping into the wealth of knowledge accessible through the world-wide-web. In practice, translating human thoughts into con-cise sets of keywords to form queries is never straightforward [3]. This is particularly true for search engine users, who are mostly un-trained casual users. In many cases, casual users have very limited background knowledge about the information they are searching for. To assist users in formulating queries, modern search engines are equipped with machine intelligence. This includes techniques like automatic query completion [8] and query recommendation [6, 12, 15, 16]. For the latter, given a user query, a search engine de-duces the search intent of the user and recommends a set of queries that are more expressive and selective than the original user input. Our goal is to study the various properties and requirements of a query recommendation system and to propose a novel approach to achieve the desired requirements.

Web search queries are usually very short, typically with only one or two keywords each [21]. Short queries lead to two issues. First, they are often ambiguous . For example, the query  X  X aguar X  can refer to a big cat, an automobile brand-name, or an operat-ing system. In [18], it is reported that up to 23.6 % of web search queries are ambiguous. Ambiguity weakens the expressiveness of queries because the search engine may not get the users X  hidden search intents. This causes poor retrieval results.
 The second issue is that short queries are often not specific enough. For example, a user issuing the query  X  X isney X  may really be in-terested only in  X  X isney Pictures X . Without knowing the hidden intent, a search engine would return web pages on Disney X  X  theme parks, stores, and cartoon characters in addition to those on the film-making studios. Although there is no ambiguity in the mean-ing of the keyword  X  X isney X , it is too general a query for the search engine to pinpoint the specific information the user is interested.
In recent years, research has been carried out to tackle the prob-lems of query ambiguity and weak selectivity. One prominent ap-proach is query recommendation. Given a query, e.g.,  X  X aguar X , a search engine provides a list of recommended queries such as  X  X aguar Cat X ,  X  X aguar Car X , and  X  X aguar Mac X . This helps the user refine his query by picking the most relevant recommendation.
Many of existing query-recommendation techniques are based on query similarity. Given an input query  X  q , a recommender sys-tem suggests a set of queries that it has frequently seen and which are the most similar to  X  q . The various similarity-based methods differ in the way they measure query similarity. The rationale be-hind this approach is that similar queries reflect the same or similar search intent, and it is likely that the search intent is more prop-erly expressed by the frequently seen queries, which are the re-sults of the collective wisdom of the community. We observe that similarity-based methods are not always the best approach. This is because queries that are the most similar to  X  q tend to be similar among themselves. This may lead to redundancy and monotonicity in the recommendation.

Recommendation redundancy refers to the situation where some recommended queries are of equivalent meaning, describing almost the same search intent. For example, consider the input query  X  X ec-ommendation X  and two recommended queries returned by the Bing search engine:  X  X etter of recommendation examples X  and  X  X ecom-mendation letter samples X . Obviously, the recommended queries refer to the same information and they are likely to give very simi-lar search results. They are thus redundant.

Recommendation monotonicity refers to the situation where the recommended queries all refer to search intents of a common pre-dominant interpretation of the input query keywords. For example, consider the input query  X  X aguar X , the recommendation { X  X aguar parts X ,  X  X aguar sedan X ,  X  X aguar used car X ,  X  X aguar dealer X  X  all fol-low the interpretation of  X  X aguar as an automobile manufacturer X . The recommendation is thus monotonic and it could miss out the real search intent of the user, such as  X  X aguar as an animal X .
We propose that a good query recommender should go beyond pure similarity and towards providing redundancy-free and diver-sified recommendations. To avoid redundancy, we highlight the notion of query concept (see Figure 1). In our model, a query concept is a set of queries that reflect the same or similar search intent. For example, the queries  X  X etter of recommendation exam-ples X  and  X  X ecommendation letter sample X  both describe the same search intent and therefore they should belong to the same query concept. An important pre-processing step of our query recom-mendation system is to group (previously seen) queries into query concepts. Given an input query  X  q , our system first selects and ranks a small set of relevant concepts. A representative query from each of the selected concepts is then extracted. These extracted queries form a list Y of ranked recommended queries. Through query con-cept selection, we are able to avoid recommendation redundancy. This is because only one (representative) query from each relevant concept is returned to the user. Other redundant queries, which belong to the same concept, are not extracted.

To diversify recommendation, the concept selection component (Figure 1) employs a probabilistic model whose goal is to max-imize the likelihood that the recommended concepts (or equiva-lently their representative queries) cover as many hidden search in-tents and interpretations of the input query keywords as possible. For example, the recommendation Y 1 : { X  X aguar Cat X ,  X  X aguar Car  X ,  X  X aguar Mac X ,  X  X aguar NFL X  X  is superior to the recommenda-tion Y 2 : { X  X aguar Parts X ,  X  X aguar Dealer X ,  X  X aguar sedan X ,  X  X aguar used cars X  X , because Y 1 covers more interpretations of the query keyword  X  X aguar X . It is therefore more likely that at least one of the recommended queries is relevant to the hidden search intent of the user.

To extract query concepts and to construct the probabilistic model for recommendation diversification, we mine search logs. A search log contains historical records, each of which registers the details of a web search conducted by a user. Table 1 shows some sample records extracted from the AOL search engine X  X  search log [17] Each record includes a query string, an anonymous user ID by whom the query was formulated, a query submission time, and the URL subsequently clicked by the user (if done). To distinguish user queries for which recommendations are to be made from those his-torical queries recorded in the search log, we call the former input queries (denoted by  X  q ) and the latter log queries or simply queries.
Before we proceed to describe the details of our approach, let us enumerate the properties that a good query recommendation sys-tem should observe: [Relevancy] Recommended queries should be semantically rele-vant to the user search query. [Redundancy Free] The recommendation should not contain re-dundant queries that repeat similar search intents. [Diversity] The recommendation should cover search intents of different interpretations of the keywords given in the input query. [Ranking] Highly relevant queries should be ranked first ahead of less relevant ones in the recommendation list. [Efficiency] Query recommendation provides online help. There-fore, recommendation algorithms should achieve fast response times.
Here we summarize the major contributions of our work: (1) We study the main features that a query recommender should possess. These include relevancy, redundancy-free, diversity, rank-ing, and real-time response. Our approach, called DQR, is the first to address all the 5 requirements. (2) We highlight the notion of query concept to tackle the recom-mendation redundancy problem. We propose a clustering method to achieve query concept construction. (3) We propose a concept-based p robabilistic query recommenda-tion model, which considers both the requirements of relevancy and diversity. (4) We present a comprehensive empirical evaluation of our ap-proach against a number of recommendation methods using real search engine datasets. The results show that our approach is both highly effective and efficient in suggesting relevant and diversified recommendations.
Query recommendation is an active research topic. In this sec-tion we give a brief account of some representative works on gen-eral web search recommendation techniques (Section 2.1). Since our approach uses concept extraction to avoid recommendation re-dundancy and we aim to achieve recommendation diversification, we highlight some related work on recommendation diversification (Section 2.2) and concept mining (Section 2.3).
There are a number of studies on search-log-based query rec-ommendation methods [2, 4, 6]. Many of these studies derive click-through bipartite graphs (CTBG) from search logs to measure
Each log record is associated with many fields. We are displaying only those fields that are of our interest. query similarity. The set of queries ( Q ) and the set of clicked URLs ( D ) in a search log form two disjoint sets of vertices in a CTBG. An edge connects a query q  X  Q and an URL d  X  D if q and d appear together in a log record. Figure 2 shows the CTBG derived from the search log shown in Table 1. Assuming that the URLs a user clicked reflect his search intent, queries that share more clicked URLs are considered more similar.
Based on a similarity measure, queries are clustered. For exam-ple, in [2], each query is associated with a term vector that is de-rived from the contents of the web pages referred to by its clicked URLs. The similarity of two queries is given by the cosine simi-larity of the queries X  term vectors. The k-means algorithm is then applied to cluster the queries. Given an input query  X  q , the cluster C to which  X  q belongs is determined. Queries in the cluster are then ranked and the highly ranked queries are returned in the recommen-dation. In [4], a similar cluster-based recommendation approach is proposed, except that an agglomerative clustering method applied on the CTBG is used instead of k-means. In [21], the similarity of two queries is measured by a combination of the clicked URLs they share as well as the keywords shared by their query strings. A density-based clustering algorithm (DBSCAN) is then applied to form query clusters. Our approach is in sharp contrast to the above methods. Although we also cluster queries, the purpose is not to recommend queries that are in the same cluster of the input query. Instead, each cluster forms one query concept and only one rep-resentative query from each cluster (concept) is considered in the recommendation procedure to avoid redundancy.

Some studies apply various probabilistic models for query rec-ommendation. For example, in [12, 15, 16], a query graph is con-structed from a search log in which vertices represent queries in the log. An edge connecting two queries is weighted by a transition probability , which is related to the similarity of the two queries. A hitting time algorithm , which performs a random walk, is then exe-cuted on the graph. The hitting time h ( q j | q i ) is the expected num-ber of steps it takes to reach a query vertex q j from a starting vertex q . In [12], given an input query  X  q , queries q j  X  X  with the smallest hitting times h ( q j |  X  q ) are recommended. One weakness of the hit-ting time approach is that query graphs are huge. For example, the query graph derived from the AOL search log [17] is about 100 GB. Typically, either a depth-first search or a breadth-first search on the query graph is executed to obtain a reduced graph for the execution of the hitting time algorithm. This is computationally expensive and it works against the real-time requirement of query recommendation.
In [7], Carbonell et al. introduce Maximal Marginal Relevance (MMR) as a technique to diversify results in traditional document retrieval systems. Given a keyword query, MMR selects relevant documents incrementally. When a relevant document d is selected, MMR updates the relevancy of each remaining document d by computing a penalty, which is based on the similarity between d and d . The next relevant document picked by MMR is then based on the updated relevancy scores. By doing so, MMR favors docu-ments that are not only relevant to the query, but are also not so sim-ilar to those that have already been included in the retrieval result. In [15], Ma et al. study diversified query recommendation. Sim-ilar to MMR, a recommendation list is constructed incrementally. They employ a reversed hitting time algorithm to ensure that the next query selected for recommendation is not very similar to those that have already been selected. Like other hitting-time-based algo-rithms, their method suffers from high computational cost in pro-cessing huge query graphs. Our approach is similar to that of [15] in that we also construct the recommendation list incrementally. However, we pick recommended queries at the query concept lev-els to avoid recommendation redundancy. Also, instead of using query graphs, we propose a probabilistic model that allows recom-mendations to be determined much more efficiently to satisfy the real-time requirement.
In traditional IR, concepts are often extracted from unstructured text collections to improve retrieval accuracy. Techniques such as LSI [9] are often employed. However, the literature on mining query concepts from search log queries remains sparse.

In [6], Cao et al. propose CACB, which employs a one-pass clustering algorithm to cluster search log queries into query con-cepts. In Section 4.1 we will give the details of this clustering al-gorithm, point out its potential weaknesses, and propose a method to improve the clustering results. In [11], Fonseca et al. extract query concepts from query relation graphs (QRGs). The idea is to first identify user sessions from the search log (e.g., based on the queries X  submission times). Search log queries that frequently co-occur in the same sessions are considered highly associated, and so they are connected by an edge in the QRG. Subsets of queries that are strongly connected in the QRG are taken as query concepts.
A search log R consists of a number of log records. Given a log record r  X  R ,weuse r q , r u , r t ,and r d to denote the query, the user who issued the query, the query submission time, and the clicked URL of that record, respectively. We use Q , U ,and D to denote the set of all unique queries, the set of all unique user ID X  X , and the set of all unique clicked URLs, respectively, in the search log.
Given an input query  X  q issued by a user u , we say that a recom-mended query q matches  X  q if user u finds q relevant to his search intent. Note that this concept of matching is subjective to the user, and there could be more than one query matching  X  q . Without re-ferring to a particular user, we use e q to denote the event that an arbitrary user finds q matching his input query  X  q . Similarly, given a set of recommended queries Y  X  Q ,weuse e Y to denote the event that at least one query in Y matches  X  q .Weuse p ( e p ( e Y ) to denote the proba bilities of the corresponding events. P ROBLEM S TATEMENT 1. (Diversified Query Recommendation) Given an input query  X  q and an integer m , the diversified query rec-ommendation problem is to find a set Y  X  Q of m queries that maximizes t he pr obability p ( e Y ) . The diversified query recommendation problem is a difficult one. First, the number of queries in Q is huge. For example, there are more than 10 million queries in the AOL dataset. Even picking, say, m = 10 recommended queries from Q involves a huge search space. Second, it is not appa rent how the probability p ( e timated. To strike a balance between efficiency and recommenda-tion quality, we propose a heuristic approach called DQR. DQR consists of an offline component and an online component. Fig-ure 1 shows the framework. For the offline component, queries in the search log are clustered. Each cluster forms a query concept. For the online component, given an input query  X  q , DQR employs a probabilistic model to find a set Y c of m query concepts. For each concept C i  X  Y c , a representative query q i is selected from the cluster of C i .These m selected queries, one from each concept in Y c , form the recommendation Y . In this section we first discuss our offline concept mining algorithm (Section 4.1), followed by our probabilistic model for online concept selection and recommenda-tion construction (Section 4.2).
We group similar queries in the search log to form query con-cepts. This step is done for two purposes. First, as we have ex-plained in the introduction, by recommending at most one query from each cluster, we avoid recommendation redundancy. Second, the number of query concepts is much smaller than the number of queries in a search log ( | Q | ). Selecting m query concepts (for rec-ommendation) thus involves a much smaller search space than the problem of selecting m queries out of Q . This allows the online recommendation process to be done efficiently enough to satisfy the real-time requirement of the recommendation problem.
To cluster queries, we need a similarity measure. We use the set of URLs as the features of queries. Each query q i  X  thus represented by a | D | -dimensional vector cally, given a query q i andanURL d j , we follow [10] and assign the user-frequency-inverse-query-frequency (UF-IQF) score as the component weight of q i for dimension d j . This weight, w ( q is given by the formula: where N u ( q i ,d j ) is the number of unique users who have issued query q i and subsequently clicked URL d j ,and N q ( d j ber of queries that led to the clicking of URL d j :
N u ( q i ,d j )= |{ u  X  U | X  r  X  R,r u = u, r q = q i ,r d We further normalize the weights to w ( q i ,d j ) by
The distance , d ( q i ,q j ) , between two queries q i and q sured by the Euclidean distance of their normalized vectors. We also use s ( q i ,q j ) to denote a similarity measure of the queries: d ( q i ,q j )= ||
Before we describe our clustering algorithm, let us point out two requirements: (1) Adaptability : The clustering algorithm should be able to automatically determine the number of clusters in a search log, since this number is unknown beforehand. (2) Efficiency :The algorithm has to be efficient because the number of queries in the search log is huge.
 With these requirements k-means, for example, is not suitable. Not only does it require the number of concepts be known, it also requires numerous iterations through the search log queries during the clustering process. As an example, we have applied k-means to cluster the queries in the AOL search log. The algorithm did not terminate in two days.

In view of the efficiency requirement, a one-pass clustering algo-rithm is proposed in [6]. The algorithm maintains a set of clusters (initially C =  X  ). The algorithm then scans the search log queries. For each query q i , the algorithm locates the cluster C  X  X  the  X  closest  X  X o q i . If adding q i to C does not violate a compactness requirement of C , q i is added to C ;otherwise, q i forms a cluster by itself and this new cluster is added to C .

This one-pass algorithm, although very efficient, is highly sen-sitive to the order in which log queries are scanned. As a simple example, Figure 3(a) shows three queries and their pairwise dis-tances. Suppose we impose the following compactness require-ment:  X  X he average pairwise distance of queries in a cluster clustering result is C 1 = {{ q 1 ,q 2 } , { q 3 }} . Note that q 3 is not added to the cluster { q 1 ,q 2 } because doing so will violate the com-pactness requirement. We observe that a better clustering result is C = {{ q 1 } , { q 2 ,q 3 }} because q 2 is closer to q 3 than it is to q 1 .It
Figure 3(b) further illustrates this problem when the one-pass al-gorithm is applied to cluster a number of queries. We note that if the compactness requirement is set too loose, the one-pass algo-rithm runs the risk of putting two far points (e.g., q 1 and q 2 ) into the same cluster, leading to sub-optimal clustering. By limiting to one pass through the data, the algorithm cannot rectify the clustering result. This argues for a more stringent compactness requirement. However, if the requirement is too stringent, we will end up with a large number of very small clusters, which is not desirable ei-ther. Our solution is to use hierarchical clustering: we start with a very stringent compactness requirement to make sure that only points that are very close among themselves form a cluster. We then merge these clusters by gradually relaxing the compactness requirement. Algorithm 1 shows our clustering method.

Given a query cluster C , we use the diameter measure ( L ( C ) ) proposed in [22] as our cluster compactness measure: We use  X  ( C ) to represent the centroid of cluster C . Our algo-rithm maintains a set of clusters C and the set of their centroids, P = {  X  ( C ) | C  X  X } . It also maintains a compactness requirement L ( C )  X  L  X  C  X  X  ,where L is a compactness bound that changes from 0 (initially) to L max with an increment step of L  X  erations of the algorithm. Initially, each query q i  X  Q forms a cluster { q i } by itself. This is equivalent to having a compactness requirement of L ( C ) = 0 for each cluster. In each iteration of the algorithm, we group the centroids (in P ) using the one-pass algo-rithm of [6] with the compactness bound L . If a set of centroids are grouped together by the one-pass algorithm, we merge their corresponding clusters to form a bigger cluster. At the end of an iteration, we update the set of cluster centroids ( P ), increase the compactness bound L by L  X  , and repeat until L exceeds the maxi-mum bound L max . Figure 4 illustrates our clustering algorithm.
We have argued that a query clustering algorithm should not ex-ecute too many iterations because the number of log queries are typically huge. By controlling the value of the compactness bound increment L  X  , we can control the number of clustering iterations, Algorithm 1: Clustering Algorithm
Input : query set Q = { q 1 ,q 2 , ..., q | Q | } , increment step L Output : a set of clusters C = { C i | C i  X  Q }
Initialization: C X  X { q i }| q i  X  Q } ; P X  Q ; L  X  0 ; repeat until L&gt;L max ; return C Figure 4: Clustering illustration: (1) Some queries. (2) Grouped with and thus the clustering efficiency. We remark that the choice of the maximum bound L max is more of an engineering effort. One way to determine a  X  X ood X  L max is to perform a user study on the recommender X  X  effectiveness under a few values of L max .Fortu-nately, our experiment results show that our DQR approach gives very good performance over a wide range of L max values so it is not critical that an optimal value of L max be found. Let C be the set of query concepts extracted from the search log. Given an input query  X  q , we construct a list of recommended queries Y with a two-step process. First, we use a probabilistic model to select a set Y c  X  X  of m recommended query concepts. Then, Y is constructed by selecting a representative query from each of the concepts in Y c .

Recall that in Section 3 we defined the notion of a recommended query q matches a user input query  X  q . Since the first step of our recommendation procedure operates at the query concept level, we extend our notion of matching to the concept level as well. In par-ticular, given a user u who has issued an input query  X  q , we say that a query concept C  X  X  matches  X  q if user u finds some queries in C relevant to his search intent. We use e C to denote the event that an arbitrary user u finds C matches  X  q . Note that, depending on the perspective of the user, there could be more than one concept C matching  X  q . Furthermore, given a set Y c  X  X  of query concepts we use e Y c to denote the event that an arbitrary user u finds some concepts in Y c matching  X  q . With these notations, the first step (con-cept selection) of our two-step recommendation procedure can be performed as followed: [Concept Selection] Given an input query  X  q ,let C  X  q query concept  X  q belongs 2 . Since the user has expressed his search intent with the input query  X  q , the concept C  X  q is deemed relevant to the user X  X  search intent. To recommend queries selected from other concepts, our heuristic algorithm finds a set Y c  X  X  X  X  of m query concepts such that p ( e Y c | e C  X  q ) is maximized. To better explain the model and the algorithm, we first give a few definitions.
After a user issues a query q to a search engine, he may click a set of URLs, s , returned by the search engine. We use I : q to denote such an interaction, which is captured by a set of records in the search log. For example, the first two records in the search log shown in Table 1 shows that a user issued the query  X  X aps X  and subsequently clicked two URLs: maps.yahoo.com and maps.google.com . We call s the click-set of the interaction I . Note that records that constitute an interaction all share the same user ID, query string, and query-submission time. We use I to denote the set of all such interactions found in the search log R .Weuse CS to denote the set of all click-sets found in R .
 For example, from the log in Table 1, we get four queries:
Q = q 1 three click-sets: and six interactions:
There are |C| X  1 m or O ( |C| m ) ways of picking m concepts into the set Y c out of the pool of query concepts C X  X  C  X  q } Instead, we apply a greedy strategy and construct the set Y mentally.

Let Y c (which is initially empty) be a set of concepts our algo-rithm has picked so far. We want to add more concepts to Y concept at a time) until the selection contains m concepts. At each step, our greedy heuristic picks the concept C  X  X  X  X  C  X  q that maximizes the probability increment:
To estimate this value, we make one reasonable assumption: we assume that the search intent of a user, who issues a query q to a search engine, is truly reflected by the URLs he would click when presented with the search results of q . This has two implications:
I MPLICATION 1. Given concepts C a and C b ,andaclick-set s , we have p ( e C b | e s ,e C a )= p ( e C b | e s ) ,where e event that the user clicks the URLs in s . That is, knowing that con-cept C a matches the input query  X  q does not add more information to estimating p ( e C b ) given that we already know the user would click the click-set s .
If  X  q has previously appeared in the search log, we simply look up which concept in C contains  X  q .If  X  q is a new query previously unseen, we find the query q  X  Q whose content (such as the key-words contained in the query strings) is the most similar to that of  X  q and use the concept q belongs as C  X  q .
I MPLICATION 2. Given two concepts C a , C b and a click-set s , we have p ( e C a ,e C b | e s )= p ( e C a | e s ) p ( e of C a matching  X  q and the event of C b matching  X  q are conditionally independent given e s .
 With the conditional independence assumption, we derive p ( e ) ity across all click-sets in CS : Then, = = Substituting Equation (7) into Equation (6), we get
To evaluate the right-hand side of Equation (8) we need to es-timate p ( e s | e C  X  q ) , p ( e C | e s ) ,and p ( e C j sider the set of interactions recorded in the search log. Note that if he finds the concept C  X  q relevant to his search intent. We estimate this probability by c ounting the number of interactions in involve some query of the cluster C  X  q (we denote this number by (
NI ( C  X  q ) )), and the number of those interactions that involve click-set s (denoted by NI ( C  X  q ,s ) ). We then approximate the probability p ( e s | e C  X  q ) by the fraction NI ( C  X  q ,s ) / NI
Similarly, p ( e C j | e s ) is estimated by Note that the computation of all these probabilitie s can be done of-fline. With these quantities, the online estimation of  X ( Y can be done efficiently (see Section 5.5.4). Finally, the concept C that gives the largest  X ( Y c ,C,C  X  q ) is added to the set Y repeat this process until Y c contains m concepts. Algorithm 2 sum-maries the procedure.
 Algorithm 2: Diversified Concept Recommendation Input : m , C  X  q , C Output : a concept list Y c = &lt;C 1 ,C 2 ,  X  X  X  ,C m &gt;
X X  X  X  X  C  X  q } ; Y for i  X  1 to m do return Y c
With the m concepts selected, the next step of DQR is to pick one representative query from each concept to form the recommen-dation list Y . We pick representative queries by popularity votes. More specifically, given a concept C , its representative query is the one that is issued by the largest number of distinct users in the search log among all the queries in C . For example, from the search log of Table 1, we see that the query  X  X aps search X  has been issued by two distinct users, while the query  X  X aps X  has been issued by just one. The former is therefore more popular. Finally, the queries in Y are ranked according to the order in which their corresponding concepts are selected into the set Y c , e.g., if a concept is added to Y c first, then its representative query will be ranked first in Y .
In this section we present the experimental results for evaluating our DQR method. We first briefly describe the datasets used in Section 5.1. Then we describe five other recommendation methods against which we compare DQR in Section 5.2. In Section 5.3 we compare the recommendations made by the various methods for an example input query to highlight the differences of the methods. Finally, in Section 5.4, we give a comprehensive analysis of the various methods with a user study.
Our experiments were done on two real datasets: (1) AOL is a search log collected from the AOL search engine [17], and (2) SOGOU is the search log of the Chinese search engine Sogou follow the method employed in [13, 20] to clean the data. This in-volves: (1) removing non-alpha-numerical characters from query strings except for  X . X  (dot) and  X   X  (space), (2) removing redundant space characters, and (3) removing queries that appear only once in the search log. (These queries are likely typos.) Similarly, we ap-plied the cleaning steps to the SOGOU dataset except that we retain Chinese characters in SOGOU queries. Table 2 shows the statistics of the datasets.

Besides our DQR method, we have implemented five other query recommendation methods for the comparison study.
Available at http: // www.sogou.com / labs / dl / q-e.html. [SR (Similarity-based Ranking)] Given an input query  X  q ,we first find the log query q  X  Q whose query string is the most similar to that of  X  q . (Since a search log typically consists of a large number of queries, it is very likely that  X  q already exists in Q .This is true especially for frequently asked queries [6, 12, 16]. In this case, q =  X  q .) The top m queries q  X  Q  X  X  q } with the smallest distances d ( q,q ) (see Equation 3) are put into the recommendation list Y . These queries are sorted in increasing value of d ( q,q ) .SR is purely similarity-based method. It considers recommendation relevancy but does not consider redundancy or diversity. [MMR (Maximal Marginal Relevance)] The second recom-mendation method adopts the idea of MMR from [7]. The rec-ommendation set Y is constructed incrementally. Initially, Y = Recommended queries are added to Y one at a time. Given a Y that contains some k&lt;m queries, we define the marginal rele-vance score MR ( q ) of each query q  X  Q  X  Y by where 0  X   X   X  1 is a constant parameter 4 and s ( q i ,q j the similarity of two queries q i and q j (Equation 3). The MR score thus favors queries that are similar to the input query (represented by q ) but penalizes those that are similar to some query q that is already selected and put in Y . The query q that gives the highest MR ( q ) score is added to Y . The process repeats until Y contains m recommended queries. The queries in Y are sorted according to the order in which they are added to Y . MMR considers both rec-ommendation relevancy and diversity. However, it does not operate at the concept level and does not consider redundancy. [CACB (Context-Aware Concept-Based Method)] CACB, pro-posed in [6], is based on search sessions. CACB first scans the search log to identify user search sessions. Queries are considered to be in the same session if they are issued by the same user and their submission times all fall within a small window. (In many studies a window size of about 30 minutes is used.) Moreover, CACB clusters log queries using the one-pass algorithm proposed in [6] (see Section 4.1) to derive query concepts. Given a user search session W , CACB extracts the sequence of queries issued in W . These queries are then mapped to their query concepts to be the query sequence found in a session. This sequence is trans-formed to C 1  X  C 2  X  C 3  X  C 4 ,where C i is the concept q belongs. If successive concepts in the concept sequence are the same, they are merged. For example, if C 2 = C 3 , then the concept
Given an input query  X  q , CACB first finds the concept C  X  belongs. It then selects the top m concepts that follow C  X  quently according to the concept sequences derived from the search log. Finally, the most popular query from each such selected con-cept is put into the recommendation list Y .

Like DQR, CACB uses query concepts and thus it combats rec-ommendation redundancy. However, it does not consider diversity. Moreover, its one-pass clustering algorithm, as we have explained, may give low-quality result in concept extraction. [DQR-ND (DQR with No Diversity)] DQR-ND is exactly the same as DQR except that it does not consider diversity. When pick-ing a concept C to be included into the concept set Y c , DQR-ND maximizes the following objective function:
In our experiments, we set  X  =0 . 4 , which gives the best overall performance for MMR over a wide range of  X  values we tested. Comparing Equation 10 with Equation 8, we see that the term lected in Y c are not taken into account. We compare the perfor-mance of DQR and DQR-ND to evaluate the effectiveness of our probabilistic model in achieving recommendation diversity. [DQR-OPC (DQR with One Pass Clustering)] DQR-OPC also follows the same framework of DQR except that it extracts query concepts by employing the one-pass clustering algorithm proposed in [6] instead of Algorithm1 we proposed. We compare the perfor-mance of DQR and DQR-OPC to evaluate the effectiveness of our clustering algorithm in query concept extraction.
 Table 3 summarizes the features of the various methods.
We illustrate the dif ferences of the recommendation methods by comparing their recommendation lists for the query  X  X ahoo X  using the AOL dataset. Table 4 shows the results. In the table, we also show the recommendation made by Google Search 5 . We remark that a direct comparison against Google search is difficult because we do not have Google X  X  log or the details of Google X  X  recommen-dation algorithm. Nonetheless, Google X  X  recommendation serves as an interesting reference.

The first thing we notice from Table 4 is that many queries rec-ommended by SR and MMR are  X  X ypo queries X  6 , while there are no typo queries recommended by CACB, DQR-ND, or DQR. The reason is that the latter three methods employ query concept mining and recommend only representative queries from relevant concepts. Since a representative query is the most popular one within its con-cept, it is unlikely a typo. The list recommended by DQR-OPC also contains some typo queries, showing that the one-pass cluster-ing algorithm is less than perfect in extracting query concepts. The results show the importance of mining query concepts for recom-mender systems in avoiding typo queries.

Next we observe that queries recommended by SR are all rel-evant to the input query, but they are all redundant. This shows the weakness of a pure similarity-based recommender that does not perform concept extraction. The list recommended by MMR, which does not operate at the concept level either, also contains re-dundancy. By properly clustering queries into concepts, the lists recommended by DQR-ND and DQR are redundancy-free. Minor redundancy is also found in the lists of CACB and DQR-OPC, such as  X  X yspace X  and  X  X yspace.com X ; and  X  X ahoo search engine X  and  X  X ahoo search X . This shows that the one-pass clustering algorithm sometimes fails to group redundant queries into the same concept.
If we remove redundant queries from MMR X  X  list, MMR X  X  rec-ommendation is quite diverse. For example, the queries  X  X ahoo web messenger X ,  X  X ahoo maps X ,  X  X ahoo canada X  and  X  X ahoo fi-nance X  cover different aspects of  X  X ahoo X . This emphasis of diver-sity, however, could overshoot at times. For example,  X  X lay 13 X  and  X  X innesotajaycees X  do not seem relevant to the input query. CACB
The recommendation made by Google varies with time. The list shown is the one we obtained at the time of writing.
Although we have removed queries that appear only once in the log to remove typo queries, the typo queries shown in Table 4 are quite common and so are not removed by our cleaning procedure. offers a very interesting list. Most of the recommended queries do not match the user X  X  search intent ( X  X ahoo X ). However, those queries point to other interesting sources that are similar to yahoo services. Recall that CACB X  X  objective is to predict auser X  X next query, the recommendation it makes does not necessarily has to be relevant to the user X  X  input query.

DQR X  X  recommendation is very diversified. This diversity is weakened in DQR-ND. For example,  X  X ahoo search X  is a key func-tion of  X  X ahoo.com X , and  X  X ign in yahoo id X  is a step of logging into  X  X ahoo mail X . They thus cover the same aspects. By apply-ing Equation 8, DQR considers the concepts that are already in the set Y c when it adds more concepts to Y c . This further diversifies the recommendation and allows DQR to replace the query  X  X ign in yahoo id X  by, e.g.,  X  X ahoo dictionary X  in its recommendation.
Google only recommended 8 queries among which we see some redundancy, e.g.,  X  X ahoo messenger download X  and  X  X ahoo mes-senger X  are similar. This example shows that DQR is able to give a better recommendation despite it was trained with a much smaller search log.
To evaluate the performances of the recommendation methods, we carried out a user study using the AOL dataset. We invited 12 people to judge the recommendations given by the six methods. We selected 60 test queries from the AOL search log that we thought the judges could comprehend. These queries are listed in [1]. We then randomly divided the 12 judges into two groups of 6 each. The 60 test queries were also randomly divided into two groups of 30 queries each. Each group of 6 judges were assigned 30 test queries. For each test query, each judge was presented with the recommen-dation given by a particular recommendation method. The judges were asked to evaluate the recommendations presented to them. We followed the user study procedure employed in [12] in assigning a set of recommendations to each judge to evaluate. Table 5 shows this assignment. The set of 30 queries were further randomly di-vided into 6 sets: Query-Set-1 to Query-Set-6, each containing 5 queries. The entry marked with a in Table 5, for example, shows that Judge No. 3 was presented with the recommendations given by the MMR method for the queries of Query-Set-6. Note that the judges were not told which methods were used to generate the rec-ommendations. The evaluation assignment (Table 5) ensured that each judge evaluated recommendations that cover all 30 queries and all 6 methods. Also, no judge evaluated more than one recom-mendation (given by different methods) of the same query. This is purposely done to avoid bias induced by the evaluation order.
To evaluate a recommendation, a judge was given an evaluation form. Table 6 shows an example. On the form, the judge was shown the test query (e.g.,  X  X nterview X ) and a ranked list of 10 rec-ommended queries (e.g.,  X  X nterview preparation X ). For each rec-ommended query, the judge had to label it with a relevancy score: irrelevant (0), partially relevant (1), and relevant (2). Also, for those partially relevant or relevant recommended queries, the judge had to group them so that queries sharing the same search intent are put into the same group. An example evaluation done by a judge is indicated in Table 6 by marks. For example, the evaluation shows that the judge found four search intents from the recommen-dation that are relevant to the test query  X  X nterview X . In particular, the judge regarded the recommended queries  X  X nterview questions X  and  X  X nterview answers X  to be of the same search intent.
We compare the quality of the recommendations given by the six methods in terms of relevancy , diversity and ranking performance . The results shown in this section are those for the AOL dataset. We have also conducted a similar study on the SOGOU dataset. The conclusions drawn for SOGOU is very similar to those of AOL .Due to space limitation, we report the results on SOGOU in [1].
Table 7 shows the statistics of the relevancy scores the meth-ods received. The first three rows show the score label distribu-(score 1) or relevant (score 2) recommended queries in an evalua-tion. (Recall that there are 10 recommended queries on each evalu-average score of all the recommended queries.

From the results, we see that SR receives very good relevancy scores. In particular, its recomme ndations contain the least percent-age of irrelevant queries (score 0) among all the six methods. This is not surprising because SR only recommends queries that are the most similar to input queries. About 1/3 of MMR X  X  recommended queries are irrelevant, and there are only slightly more than 1/3 of them are considered straightly relevant by the judges. This shows that MMR sacrifices too much relevancy to achieve diversity. The relevancy scores of CACB are even lower. As we have explained, the objective of CACB is to predict the next query a user is likely to issue following his current input query in a user session. Since the search intent of a user could drift over a session, CACB X  X  rec-ommendation does not necessarily have to be relevant to the orig-inal input query. This explains CACB X  X  low relevancy scores even though its recommendations are sometimes interesting (Table 4). The relevancy scores of the three DQR methods are very good and are comparable to that of SR. In particular, more than 60% of their recommended queries are rated straightly relevant. These ratings are even better than SR X  X . The reason is that queries recommended by the DQR-based methods are representatives of query concepts. These representative queries are usually better-worded, more ex-pressive and noise-free. Therefore, it is more likely that the judges found them straightly relevant. Among the three methods, DQR-ND gets slightly more  X 2 X  score s (63%). This is because DQR-ND focuses more on similarity than diversity. DQR-OPC has slightly fewer  X 2 X  scores (61%) because the one-pass clustering algorithm sometimes gives inferior concept extraction. Overall, the relevancy performance of DQR is very good. For example, on average, 8.6 ered either partially relevant or relevant to the judges.
Next, we analyze how well the recommendations made by the various methods cover distinct relevant search intents. Recall that a judge had to group non-irrelevant recommended queries on an evaluation form into distinct search intents. We define the mea-sure Intent-Coverage @ k (or IC @ k for short) [23] as the number of search intents so identified among the top k recommended queries on an evaluation form. For example, consider Table 6, we have IC @4 equals 3 because three search intents were identified by the judge from the top 4 recommended queries given. Note that irrele-vant queries are ignored and IC @ k is upper-bounded by k . Figure 5 shows the average Intent-Coverage of the recommendations made by the six methods as k varies from 1 to 10.

From the figure, we see that SR has a low intent coverage. Even though most of SR X  X  recommended queries are relevant, they are highly redundant and cover only a few search intents. Although CACB X  X  recommendations tend to cover different aspects, however, they suffer from low relevancy. Therefore, the number of relevant distinct search intents given by CACB is small. This explains its low IC @ k scores. MMR, which focuses on diversification, has a larger intent coverage than CACB. However, many of its recom-mended queries are irrelevant, which keeps MMR X  X  IC @ k scores low. DQR gives the best intent coverage. This shows that the prob-abilistic model DQR uses in diversifying recommended queries works very well. For example, its score at k =10 shows that on average the 10 recommended queries given by DQR cover 7 distinct relevant search intents. DQR-ND, which does not employ Equation 8 to improve recommendation diversity has lower IC @ k scores than DQR. Fortunately, DQR-ND uses Algorithm 1 to form query concepts, and selects relevant concepts from which represen-tative queries are obtained to compose recommendation lists. With high-quality concepts formed, representative queries recommended to a user tend to distinguish among themselves. This results in a reasonably good intent coverage . The importance of high-quality query concepts in diversified query recommendation is further re-flected by the performance of DQR-OPC. From Figure 5, we see that DQR-OPC has lower IC @ k scores than those of DQR-ND and DQR. This is because, in some cases, the one-pass clustering algo-rithm fails to put queries that cover the same search intent into the same cluster. This may result in DQR-OPC recommending rep-resentative queries which, although come from different clusters, indeed are covering the same search intent.
Next, we analyze how well the methods rank the recommended queries. Intuitively, relevant queries should be ranked higher in the recommendation list. We consider two ranking measures, namely, Normalized Discounted Cumulative Gain (NDCG) [5, 14] and Mean Reciprocal Rank (MRR) [19]. The NDCG @ k score is defined as: where l i is the relevancy score (0, 1, or 2) of the i th ommended query, and Z k is a normalization factor so chosen such that the optimal ranking X  X  NGCG @ k score is 1. Intuitively, the NDCG @ k score summarizes the relevancy scores of the k highest-ranked recommended queries in such a way that a relevant query contributes more to the score if the query is ranked higher in the commendation list.

Given an input query  X  q ,let Y (  X  q ) be a ranked list of recom-mended queries determined by a recommendation method. We use rank j ( Y (  X  q )) to denote the rank of the j th relevant query (score 2) in Y (  X  q ) 7 . For example, if the 1 st and 2 nd recommended queries in Y (  X  q ) are not relevant but the 3 rd and the 4 th are, then 3 and rank 2 ( Y (  X  q )) = 4 . (If Y (  X  q ) contains fewer than j relevant queries, rank j ( Y (  X  q )) is defined as  X  .) We extend the MRR score proposed in [19] to the MRR @ h score, which is defined as: Essentially, the MRR @ h score summarizes the ranks of the first h relevant queries in the recommendation list Y (  X  q )  X  X largerscore indicates that the first h relevant queries are ranked higher in the list. Note that MRR @ h is bounded by H h = h i =1 1 /i ,whichis the h -th harmonic number.

Figures 6 and 7 show the NDCG @ k and MRR @ h scores of the recommendation methods. In Figure 7, the optimal values (i.e., H h ) are also shown for reference. From the figures, we see that CACB and MMR give relatively poor ranking performance. This is because many of their recommended queries are not relevant and these queries are sometimes ranked high in the recommendation lists, pushing the relevant ones down the lists. The three DQR-based methods generally give better ranking scores than SR. In particular, their MRR @ h scores are always higher than those of SR. This is due to the fact that they use query concepts and rec-ommend representative queries. These representative queries give high-quality recommendations which lead to more relevant (score 2) top-ranked recommended queries than SR does. Finally, we see that the ranking scores of DQR-OPC is slightly lower than those of DQR and DQR-ND. This is due to the less-than-perfect cluster-ing achieved by the one-pass algorithm, which occasionally leads
We have also considered counting both partially relevant queries (score 1) and relevant queries (score 2) as well. The conclusion drawn is similar so we skip the details of this alternative. to the recommendation of irrelevant concepts at the top of the rec-ommendation lists. Recall that the DQR method constructs the selected concept set Y c incrementally. Specifically, it iterates m times where m is the number of recommended queries, and in each iteration the con-cept C with the largest  X ( Y c ,C,C  X  q ) is picked. To determine  X ( Y c ,C,C  X  q ) a sum over the set of all clicked-sets (Equation 8). This might sound computationally expensive. Fortu-nately, the values p ( e s | e C  X  q ) and p ( e C | e s C and click-sets s . An index can be built offline to help DQR iden-tify the few non-zero terms of the summation during online query recommendation. The recommendation can thus be done very effi-ciently. For example, we tested the efficiency performance of DQR written in Java on a Core 2 Duo 2.83GHz PC. In the experiment, 1000 test queries were randomly picked from the search log as in-put queries. The maximum and the average processing times were 630ms and 51ms, respectively.
In this paper we investigated the problem of recommending queries to better capture the search intents of search engine users. We iden-tified five important requirements of a query recommendation sys-tem, namely, relevancy, redundancy-free, diversity, ranking-performance, and efficiency. We discussed the weaknesses of existing recom-mendation methods and proposed DQR. DQR mines a search log to extract query concepts based on which recommendations are made. DQR also employs a probabilistic model to achieve recommen-dation diversification. Through a comprehensive user study, we showed that DQR performed very well w.r.t. a number of measures that cover the five requirements of a recommender system. This project was supported by HKU Research Grant 201011159070. Reynold Cheng was supported by the Research Grants Council of Hong Kong (GRF Project 711309E). We would like to thank the anonymous reviewers for their insightful comments. [1] http://www.cs.hku.hk/research/techreps/ [2] R. A. Baeza-Yates, C. A. Hurtado, and M. Mendoza. Query [3] R. Baraglia, C. Castillo, D. Donato, F. M. Nardini, [4] D. Beeferman and A. L. Berger. Agglomerative clustering of [5] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [6] H.Cao,D.Jiang,J.Pei,Q.He,Z.Liao,E.Chen,andH.Li.
 [7] J. G. Carbonell and J. Goldstein. The use of mmr, [8] P.-A. Chirita, C. S. Firan, and W. Nejdl. Personalized query [9] S.C.Deerwester,S.T.Dumais,T.K.Landauer,G.W.
 [10] H. Deng, I. King, and M. R. Lyu. Entropy-biased models for [11] B. M. Fonseca, P. B. Golgher, B. P X ssas, B. A. Ribeiro-Neto, [12] J. Guo, X. Cheng, G. Xu, and H. Shen. A structured [13] J. Guo, X. Cheng, G. Xu, and X. Zhu. Intent-aware query [14] K. J X rvelin and J. Kek X l X inen. Cumulated gain-based [15] H. Ma, M. R. Lyu, and I. King. Diversifying query [16] Q. Mei, D. Zhou, and K. W. Church. Query suggestion using [17] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [18] M. Sanderson. Ambiguous queries: test collections need [19] E. M. Voorhees. The TREC-8 question answering track [20] X. Wang and C. Zhai. Learn from web search logs to [21] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user queries [22] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: An [23] X. Zhu, J. Guo, X. Cheng, P. Du, and H. Shen. A unified
