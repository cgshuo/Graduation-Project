 Outlier (or novelty) detection is one of the typical unsupervised learning tasks. It aims at deciding on whether or not an observed sample is  X  X trange X  based on some distance metric to the rest of the data. Change detection is similar to outlier detection, which is typically formulated as a statistical test for the probability distribution of a data set under some online settings.

In many practical data analysis problems, however, the problem of change detection appears with a slightly different motivati on. For example, a marketing researcher may be interested in comparing the current list of customers X  profile with a past list to get in-formation about changes. Here, detecting the ch anges itself is not of particular interest. What the researcher really wants is the detailed information about how they changed.
In this paper, we formulate this practically important problem, which we call change analysis . In contrast to change detection, we focus on developing a general framework of how to describe a change between two data sets. Clearly, the change analysis prob-lem is an unsupervised learning task in nature. We assume that we are given two data sets, each of which contains a set of unlabel ed vectors. Our goal is to find some diagno-sis information based on the comparisons between the two data sets, without using side information about the internal structure of the system. The main contribution of this paper is to show that this essentially unsupervised problem can be solved with super-vised learners .

To date, the problem of comparing two data sets has been addressed in various ar-eas. For example, the two-sample test [1,2,3], which is essentially to tell whether or not two (unlabeled) data sets are distinct, has a long history in statistics [4]. Another example is concept drift analysis [5,6,7], which basically addresses changes in super-vised learners when the (labeled) training data set changes over time. However, most of the existing approaches have a serious drawback in practice in that they focus almost only on whether or not any change exists. As mentioned before, in most of the practical problems, what we really want to know is which variables could explain the change and how.

The layout of this paper is as follows. I n Section 2, we describe the definition of the change analysis problem, and give an overview of our approach. Unexpectedly, this unsupervised problem can be solved using supervised learners, as explained in Section 3 in detail. Based on these sections, in Section 4, we present experimental results using real data to show the proposed approach is promising. Finally, we give a brief review of related work in Section 5, and conclude the paper in Section 6. In this section, we define a task of change analysis somewhat formally, and give an overview of our approach. 2.1 The Change Analysis Problem and X B  X { x (1) B , x (2) B ,..., x ( N B ) B } ,where N A and N B are the numbers of data items feature space.

This paper addresses two problems about these data sets. The first one is the change detection problem, which is basically the same as the two-sample problem: Definition 1 (change detection problem). Given nonidentical data sets X A and X B , tell whether or not the difference is significant, and compute the degree of discrepancy between X A and X B .
 Note that, unlike concept drift studies, we focus on unlabeled data in this problem. The second problem we address is the change analysis problem , which is stated as follows: Definition 2 (change analysis problem). Given nonidentical data sets X A and X B , output a set of decision rules that explain the difference in terms of individual features 1 . Since no supervised information is given in getting the decision rules, this is an unsu-pervised learning task.
To understand the difference between these two problems, let us think about lim-itations of the two-sample test, which has been thought of as a standard approach to the change detection problem. The two-sample test is a statistical test which attempts to detect the difference between two data sets. Formally, it attempts to decide whether P A = P B or P A = P B ,where P A and P B are probability distributions for X A and X B , respectively. In statistics, two-sample tests are classified into two categories [4]. The first category is the parametric method, where a parametric functional form is ex-plicitly assumed to model the distribution. In practice, however, such density modeling is generally difficult, since the distribution of real-world data does not have a simple functional form. In addition, even if a good parametric model such as Gaussian had been obtained, explaining the origin of the difference in terms of individual variables is generally a tough task, unless the variables are independent.

The second category is the nonparametric method, which allows us to conduct a statistical test without density modeling. If our interest were to detect only the discrep-ancy between two data sets, distance-like m etrics such as the maximum mean discrep-ancy [3], the Kolmogorov-Smirnov statistic [1], energy-based metrics [8], and nearest neighbor statistics [2] are available for solving the change detection problem. However, these methods are not capable of handling the change analysis problem. While some of the two-sample tests offer asymptotic distributions for the data in such limit as large number of samples, it is generally very hard to answer the change analysis problem in practice. This is because, first, such a distr ibution is an asymptotic distribution, so it generally cannot be a good model for real-wor ld data, where, e.g., the number of sam-ples is finite. Second, since the nonparametr ic approach avoids density modeling, little information is obtained about the internal structure of the data. 2.2 Overview of Our Approach Considering the limitations of the two-sample test, we propose a simple approach to these two problems. Our key idea is just as follows: Attach a hypothetical label +1 to each sample of X A ,and  X  1 to each sample of X B . Then train a classifier in a supervised fashion. We call this classifier the virtual classifier (VC) hereafter.

Figure 1 shows a high-level overview of our approach, where and indicate la-bels of +1 and  X  1 , respectively. In our approach, if two data sets actually have the dif-ferences, they should be correctly classified by the classifier. Thus a high classification accuracy p indicates a difference between X A and X B . For example, if P A = P B ,the classification accuracy will be about 0 . 5 when N A = N B .However,if p is significantly larger than 0.5, we infer that the labels make difference, so P A = P B .

In addition, to solve the change analysis problem, we take advantage of the inter-pretability of classification algorithms. For example, the logistic regression algorithm gives the weight of each feature representin g the importance. For another example, if the decision tree is employed, variables appearing in such nodes that are close to the root should have a major impact. In this way, we can get decision rules about the changes from the VC.

The advantages of this VC approach are as follows: First, it can solve the change detection and analysis problem at the same time. The classifier readily gives the degree of change as the classification accuracy, and also provides diagnosis information about changes through its feature selection functions. Second, the VC approach does not need density estimation, which can be hard especially for high dimensional data. Finally, the VC approach allows us to evaluate the significance of changes simply by a binomial test. This is an advantage over traditional nonparametric two-sample tests, which have focused on asymptotic distributions that hold only in some limit. This section presents details of our supervised learning approach to change analysis. For notations, we use bars to denote data sets including the hypothetical labels, such as  X  prediction accuracy of VCs is represented by p . 3.1 Condition of No Change Suppose that we are given the combined data set  X  X  X   X  X A  X   X  X B , and a binary clas-sification algorithm L. We train L using  X  X , and evaluate the classification accuracy p , making use of k -fold cross validation (CV). In particular, randomly divide  X  X into k equi-sized portions, leave out one portion for test, and use the remaining ( k  X  1) por-tions for training. The overall prediction accuracy p is computed as the average of those of the k classifiers.
 If P A = P B , classification of each of the samples in  X  X by L can be viewed as a Bernoulli trial. Thus the log-likelihood of N A + N B trials over all the members of  X  X will be under the assumption of i.i.d. samples, where q is the probability of the class A. By differentiating this w.r.t. q , and setting the derivative zero, we have the maximum likeli-hood solution of this binomial process as q = N A / ( N A + N B ) . Since the classification accuracy p should be max { q, 1  X  q } , we see that p is given by where the subscript represents binomial.

If P A = P B , so the information of the class label s is important, the classification accuracy will be considerably higher than p bin . Specifically, the larger the differences they have, the higher the prediction accuracy will become. One of the major features of our VC approach is that it enables u s to evaluate the significance of p via a binomial test. Consider a null hypothesis that the prediction accuracy is given by p bin , and assume N
A &gt;N B for simplicity. For a value of the confidence level  X &gt; 0 , we reject the null hypothesis if where N = N A + N B . This means that the class labels are so informative that p is sufficiently higher than p bin . If we parameterize the critical probability as (1+  X   X  ) p bin , the condition of no change is represented as For a numerical example, if N = 1000 and p bin =0 . 5 , the 5% and 1% confidence levels correspond to  X  0 . 05 =0 . 054 and  X  0 . 01 =0 . 076 , respectively. For relatively large N , Gaussian approximation can be used for computing  X   X  [4]. 3.2 Change Analysis Algorithm Once the binomial test identifies that the difference between X A and X B is significant, we re-train L (or another type of classification algorithm) using all the samples in  X  X .If some features play a dominant role in the classifier, then they are the ones that charac-terize the difference. As an example, imag ine that we have employed the C4.5 decision trees [9] as L. The algorithm iteratively identifies the most important feature in terms of information gain, so such features that app ear closest to the root will be most important. Thus focusing on such nodes amounts to feature selection, and the selected features are the ones that explain the difference. In this way, feature selection and weighting func-tions of L are utilized in change analysis.

We summarize our change analysis algorithm in Fig. 2. The first half (1-3) essentially concerns change detection by evaluating the significance of the changes through the binomial test, while the second half (4-5) addresses change analysis. As shown, there are two input parameters,  X  and k . 3.3 Application to Labeled Data While the algorithm in Fig. 2 is for unlabeled data, we can extend the algorithm for labeled data. This extension is practically important since it enables us to do change analysis between classifiers. Suppose that we are given a classification algorithm M, WetrainMbasedonD A and D B to obtain classifiers M A and M B , respectively. What we wish to solve is a change analysis problem between M A and M B : Output a set of decision rules that explain the difference between M A and M B in terms of individual features.

To solve this, we create unlabeled data sets based on the following strategy. For each are inconsistent, then we put the sample into a set X A , otherwise into X B . Scanning all the samples, we have two unlabeled data sets X A and X B . By construction, X A characterizes the inconsistencies between M A and M B , while X B characterizes their commonalities. Thus, by making use of the change analysis algorithm in Fig. 2 for these X A and X B , detailed information about the inconsistencies will be obtained. In our context, the quantity works as the degree of the inconsistencies between M A and M B (or D A and D B ), where N inc represents the number of samples whose predictions are inconsistent.

When the number of possible values for the target variable is small, it is useful to ex-tend the change analysis algorithm to include multi-class classification. As an example, sistent set X A into two subsets X A1 and X A2 . Here, X A1 consists of the inconsistent samples whose original prediction is +1 but cross-classification gives  X  1 . Similarly, X
A2 consists of the inconsistent samples that make a transition from  X  1 to +1 .Then we apply a three-class classification algorithm L to classify X A1 ,X A2 ,andX B . Finally, we examine the resulting classifier for each type of disagreement. We evaluated the utility of the VC approach for change analysis using synthetic as well as real-world data. In the following experiments, we used  X  =0 . 05 and k =10 unless otherwise noted. For a classification algorithm L, we mainly used the C4.5 decision trees (DT) algorithm implemented as J48 in Weka [9], which has a parameter named minN umObj meaning the minimum number of instances per leaf. To see the degree of linear separability between the two data sets, we additionally used logistic regression (LR) also implemented as Logistic in Weka. Two parameters in Logistic (a ridge parameter and the maximum iterations) wer e left unchanged to the default values (10  X  8 and infinity, respectively). 4.1 Synthetic Data We conducted two experiments based on synthetic data with N A = N B = 500 .For this number of samples, the critical accuracy is given by 0.527 (  X  0 . 05 =0 . 054 ). In both of the experiments, the ten features were independently generated based on zero-mean Gaussians.

For the first experiment, the data sets X A and X B were designed so that P A and P B had a significant difference. In X A , the standard deviations (denoted by  X  ) were set to be 1.0 except for Attr 1 (the first feature), where  X  was set to be 4.0. On the other hand, in X B , all the  X  s were 1.0 except for Attr 2 (the second feature), where  X  was set to be 4.0. Figure 3 (a) shows the marginal distribution of this data set in the Attr 1 -Attr 2 space. Our goal is to pick up Attr 1 and Attr 2 as features that are responsible for the difference.

We conducted change analysis for this data set with minN umObj =10 for DT. The estimated prediction accuracy p computed by 10-fold CV was 0.797 (DT), which far exceeds the critical accuracy. This means that the two data sets were correctly judged as being different. Figure 3 (b) represents the DT as the VC, where the labels of the ellipses and the edges show the split variables and the decision rules, respectively. The shaded boxes enclose the class labels as wel l as (1) the number of instances fallen into the node and (2) the number of misclassified instances in the form of (1) / (2) .The latter is to be omitted when zero. The decision boundaries found by the DT are shown by the lines in Fig. 3 (a). Clearly, the model learned the intended nonlinear change between Attr 1 and Attr 2 . Note that, when LR was used as L, 10-fold CV gave only p =0 . 505 , which is below the critical accuracy. This result clearly shows the crucial role of nonlinear decision boundaries.

For the second experiment, P A and P B were designed to be the same. In both X A and X B , all the  X  s were set to be 1.0 except for Attr 2 ,where  X  =4 . 0 . Figure 4 shows the marginal distribution corresponding to Fig. 3 (a). In contrast to the first experiment, the DT model naturally showed a low p of 0.500, indicating that the differences were not statistically significant. This result sh ows that our approach using DT generates a valid classifier with statistical significance only when the data set contains a difference between classes. 4.2 Spambase Data Spambase is a public domain data set in UCI Machine Learning Repository [10]. While the original data contains spam and non-spam email collections, we used only the 1,813 instances belonging to the spam email set. The features consist of fifty-five continuous values of word and symbol statistics. We divided the spam set into halves, X A and X
B , keeping the original order of the instances unchanged. In this setting, the critical accuracy is 0.520 (  X  0 . 05 =0 . 039 ). We performed change analysis for X A and X B to see if there was any hidden shift in the data. We used minN umObj =2 for DT.
Interestingly, the 10-fold CV produced a rather high prediction accuracy of p = 0 . 548 (DT), which is higher than the critical accuracy. According to the VC, the ma-jor features were the frequencies of the words  X  X du X ,  X 85 X , and  X  X p X , although space limitation does not permit showing the output DT. Considering the additional fact that LR produced just p =0 . 515 , we conclude that the spam class in Spambase has some nonlinear changes on the word frequencies, which are difficult to find using a linear model like LR. This result is of particular practical importance, since it suggests that learning algorithms that depend on the order of the training samples might tend to have considerable biases. 4.3 Enron Email Data The Enron email data set is an archive of real email at the now defunct Enron Corporation, and no class labels are available [11]. We used the year 2001 subset that contains 272,823 email messages in a bag-of-words representation [12]. We separated the data into the first (1H) and the second (2H) halves of this year, and generated feature vectors by including the 100 and 150 most frequent words in each pe riod. Meaningless zer o vectors including none of the selected feature words were omitted. Each half was further divided into halves to allow comparison on quarterly basis. We conducted change analysis within either 1H or 2H with numM inObj =1 , 000 . For example, in the analysis of 2H, X A and X B roughly correspond to the data in the third (3Q) and fourth (4Q) quarters, respectively.
Table 1 shows the estimated predictio n accuracies. We see that both LR and DT mark accuracies much higher than the critical accuracies. To explore the details of the differences, we picked the 2H data, and did change analysis to obtain the DT in Fig. 5, where top 5 nodes from the root have been selected, comparing between 100-and 150-word models. The notation of the trees are the same as Section 4.1, although the rank of each feature has been added here ( X  X ccess X  is 44t h frequent, etc.). The threshold values represent the occurrence numbers of featur e words in each email. Since we followed the simple frequency-based feature generation strategy, the 150-word tree tends to include such words that bear particular meanings.

We see that  X  X osition X  is at the root node in the 150-word model in spite of its less frequency (144th rank). Enron went bankrupt at the end of 2001. If we imagine what had been talked about by the employees wh o were doomed to lose their job position, this result is quite suggestive. In addition, we see that  X  X eff X  and  X  X avis X  are dominant features to characterize the 4Q data. Interestingly, the name of CEO of Enron in 2001 was Jeffrey Skilling, who unexpectedly resigned from this position on August 2001 af-ter selling all his stock options. Many employees must have said something to him at the moment of the collapse. For Davis, there was a key person named Gray Davis, who was California X  X  Governor in the course of the California electricity crisis in the same year. It may result from his response to the investigation of Enron in 4Q. Note that the VC has discovered these key persons without any newspaper information, demonstrating the utility in studying the dynamics of complex systems such as Enron. 4.4 Academic Activities Data As an example of application to labeled an d categorical data, we performed change analysis for  X  X cademic activities X  data collected in a research laboratory. This data set time index and y ( s ) represents a binary label of eith er  X  X  X  (meaning important) or  X  X  X  (unimportant). Each of the vectors x ( s ) includes three categorical features, title , group , and category , whose values are shown in Table 2.

Since the labels are manually attached to x ( s ) s by evaluating each activity, it greatly depends on subjective decision-making of the database administrator. For example, some administrators might think of PAKDD papers as very important, while other might not. Triggered by such events as job rotations of the administrator and revisions of eval-uation guidelines, the trend of decision-making is expected to change over time. The purpose on this analysis is to investigate when and what changes have occurred in the decision criteria to select importance labels.
 We created 14 data subsets by dividing the data on quarterly basis, denoted by D 1 , D 2 ,..., D 14 . First, to see whether or not distinct concept drifts exist over time, we com-puted the inconsistency score  X  (see Eq. (4)) between neighboring quarters. Specifically, we think of D A and D B as D t and D t +1 for t =1 , 2 , ..., 13 . For M, we employed decision trees. Figure 6 shows the inconsistency score  X  for all the pairs. We see that two peaks appear around t =5 and t =10 , showing clear concept drifts at those pe-riods. Interestingly, these peaks correspond to when the administrator changed off in reality, suggesting the fact that the handover process did not work well.

Next, to study what happened around t =5 ,wepickedD 5 and D 6 for change anal-ysis. Following the procedure in Section 3.3, we obtained the VC as shown in Fig. 7. Here, we used a three-class DT based on three sets X A1 , X A2 and X B ,where X A1 includes samples whose predicted labels make a transition of Y  X  N. The set X A2 includes samples of N  X  Y, while X B includes consistent samples of Y  X  Y. I f w e focus on the leaves of  X  X Y X  in Fig. 7 representing the transition from N to Y, we find interesting changes between D 5 and D 6 : The new administrator at t =6 tended to put more importance on such academic activities as program and executive committees as well as journal editors.
 One might think that there can be a simpler approach that two decision trees M 5 and M 6 are directly compared, where M 5 and M 6 are decision trees trained only within D 5 and D 6 , respectively. However, considering com plex tree structures of decision trees, we see that direct comparison between diff erent decision trees is generally difficult. Our VC approach provides us a direct means o f viewing the difference between the classifiers, and is in contrast to such a naive approach. The relationship between supervised classi fiers and the change detection problem had been implicitly suggested in the 80 X  X  [2], where a nearest-neighbor test was used to solve the two-sample problem. However, it did not address the problem of change anal-ysis. In addition, the nearest-neighbor classifier was not capable of explaining changes, since it did not construct any explicit classification model. FOCUS is another framework for quantifying the deviation between the two data sets [13]. In the case of supervised learning, it constructs two decision trees (dt-models) on each data set, then expands them further until both trees converge to th e same structure. The differences between the numbers of the instances which fall into the same region (leaf) indicate the devia-tion between the original data sets. In high-dimensional settings, however, the models should become ineffective since the size of the converged tree increases exponentially therefore the method requires substantial computational cost and massive instances.
Graphical models such as Bayesian networks [14] are often used in the context of root cause analysis. By adding a variable indicating one of the two data sets, in princi-ple Bayesian networks allow us to handle change analysis. However, a graphical mod-eling approach inevitably requires a lot of training data and involves extensively time-consuming steps for graph structure learning. Our VC approach allows us to directly explain the data set labels. This is in contrast to graphical model approaches, which basically aim at modeling the joint distribution over all variables.

In stream mining settings, handling concept drift is one of the essential research is-sues. While much work has been done in this area [5,6,7], little of that addresses the problem of change analysis. One of the exceptions is KBS-stream [15] that quantifies the amount of concept drift, and also provid es a difference model. The difference model of KBS-Stream tries to correctly discriminate the positive examples from the negative examples in the misclassified examples under the current hypothesis. On the other hand, our VC tries to correctly discriminate the misclassified examples by the current hypoth-esis against the correctly classified example s. Both models are of use to analyze concept drift, but the points of view are slightly different.

Other studies such as ensemble averagin g [16] and fast decision trees [17] tackle problems which are seemingly similar to but essentially different from change analysis. We have proposed a new framework for the change analysis problem. The key of to tell the two data apart if they came from two different data sources. The resulting classifier is a model explaining the differences between the two data sets, and analyzing this model allows us to obtain insights about the differences. In addition, we showed that the significance of the changes can be statistically evaluated using the binomial test. The experimental results demonstrated that our approach is capable of discovering interesting knowledge about the difference.

For future work, although we have used only decision trees and logistic regression for the virtual classifier, other algorithms also should be examined. Extending our method to allow on-line change analysis and regression models would also be interesting re-search issues.

