 Selection of genes that are differentially expressed and criti-cal to a particular biological process has been a major chal-lenge in post-array analysis. Recent development in bioin-formatics has made various data sources available such as mRNA and miRNA expression profiles, biological pathway and gene annotation, etc. Efficient and effective integration of multiple data sources helps enrich our knowledge about the involved samples and genes for selecting genes bearing significant biological relevance. In this work, we studied a novel problem of multi-source gene selection: given multiple heterogeneous data sources (or data sets), select genes from expression profiles by integrating information from various data sources. We investigated how to effectively employ in-formation contained in multiple data sources to extract an intrinsic global geometric pattern and use it in covariance analysis for gene selection. We designed and conducted ex-periments to systematically compare the proposed approach with representative methods in terms of statistical and bio-logical significance, and showed the efficacy and potential of the proposed approach with promising findings.
 H.2.8 [ Database Management ]: Database Applications X  data mining ; I.2.6 [ Artificial Intelligence ]: Learning; I.5.2 [ Pattern Recognition ]: Design Methodology X  feature eval-uation and selection ; J.3 [ Life and Medical Sciences ]: Biology and Genetics Algorithms, Experimentation, Measurement Gene selection, information integration, bioinformatics
Decades of research have proven that cancer is a heteroge-nous cellular disorder caused by the deregulation of many interacting cellular pathways that generate tumor forma-tion and growth. Functional genomics and proteomics tech-niques, such as global-expression profiling and molecular in-teraction screen, are being used to explore how these genes work together to trigger (or maintain) growth, survival, pro-gression, and invasiveness of malignant cells. Microarrays allow biological researchers to examine the messenger RNA (mRNA) expression levels of tens of thousands of genes at once, and possibly to analyze the expression profiles of an en-tire genome. Despite these advances of microarray technolo-gies, bottlenecks still exist, primarily in identification and selection of genes critical to the biological phenotype and processes of particular interests. Although steady progress has been made recently in statistical data analysis [16, 21], biomedical researchers using microarrays still face daunting challenges in understanding and determining the biological significance of  X  X ide data X  with tens of thousands of genes but relatively small numbers of samples. For example, fold differences identified via statistical analysis offers limited or inaccurate selection of biological features, leading to inaccu-rate groups of genes among different types of cancer [19, 25]. In order to elucidate mechanisms underlying oncogenesis or other biological processes, much remains to be explored on how to organize and interpret microarrays based on both biological relevance and statistical significance. Since the number of samples cannot be increased considerably in the near future, additional data sources can be exploited to en-hance our understanding of the data in hand.

In this work, we studied a novel problem of gene selection using multiple heterogeneous data sources. It is a problem arising from a study of bioinformatical cancer research in which cancerous samples are collected with both mRNA and microRNA (miRNA) (i.e., two different data sources) with gene properties and relationships being provided by gene function annotation [14], gene ontology annotation [4] and biology pathway (i.e., another three data sources). Our goal is to find pertinent genes from the mRNA expression profiles (henceforth, the target) with the help of four additional data sources. The additional data sources fall into two categories: (1) the data sources about genes -gene function annotation, gene ontology annotation and biology pathway; and (2) the data sources about relationships among samples -miRNA and miRNAs expression profiles sharing the same set of sam-ples. Recent studies suggest the role of miRNA as both oncogenes and anti-oncogenes [11, 28]. For instance, miR -15 and miR -16 have been shown to induce apoptosis by tar-geting BCL2 [6], acting as anti-oncogenes, whereas miR-21 targets the tumor suppressor gene TPM1 and blocks apopto-sis, thereby functioning as oncogenes and promoting tumor growth [23]. Studies also reveal that miRNA profiles are surprisingly informative for depicting sample relationships in terms of separating tissues of cancer and noncancer, as well as different types of cancers [19, 13]. It is, however, not trivial to integrate data sources describing the relationships among samples as well as data source describing the rela-tionships among genes. Since disparate data sources contain information of different perspectives, effectively integrating them for gene selection remains a challenge. To address the problem, we proposed a framework for multi-source gene se-lection. We focus on unlabeled data in this work. Thus the proposed framework is unsupervised by definition. We later showed that if class label is available, the framework can be adapted to handle labeled data in a straightforward way.
Gene selection is also known as feature selection [17] in machine learning. Although various approaches for unsu-pervised feature selection have been proposed [12, 32, 8], the potential of using multiple heterogeneous data sources for unsupervised feature selection has not been explored to help biological investigation. We notice that gene priori-tization using multiple data sources is previously studied in [3]. However, it is essentially a classification problem in which genes are treated as instances and only data sources describing genes relationships are used. Therefore despite its significance, gene selection using multiple heterogeneous data sources is a problem that has yet been addressed. Be-fore presenting the framework for solving the problem, we first provide notations used in this work.
In the paper we assume the given mRNA expression pro-file D containing d genes and n samples. We use F 1 , . . . , F to denote the d genes and f 1 , . . . , f d the corresponding vec-tors in the profile. We use X = ( x 1 , x 2 , . . . , x n the n samples, and for each sample we have, x i  X  R d . In fea-ture selection, F 1 , . . . , F d are also called features, f are the corresponding feature vectors. Besides the mRNA expression profile, we also have h G additional data sources, D 1 , . . . , D G h G depicting the relationships among genes; and h
S additional data sources, D S 1 , . . . , D S lationships among samples. The representation for the re-lationships among genes and samples can be heterogenous. In the work, we use spectral graph theory [5] as a tool to explore the geometric structure of a data. Let G denote a graph, its similarity matrix is denoted by W ( i, j ) = w Let d denote the vector: d = { d 1 , d 2 ,..., d n } , where d = P n k =1 w ik , the degree matrix D of the graph G is de-fined by: D ( i, j ) = d i if i = j , and 0 otherwise. Given D and W , the Laplacian matrix L and the normalized Laplacian matrix L are defined as: L = D  X  W ; L = D  X  1 2 LD  X  1 2 is well known that the leading eigenvectors of L and L form soft cluster indicators of the data [29]. We use S + to denote the space of symmetric positive semidefinite matrices; K , a kernel matrix; C , the covariance matrix; 1 , the vector with all elements equal to 1; and I , the identity matrix.
Given multiple data sources carrying information about gene and sample relationships with heterogeneous represen-tations, we exploit both types of information for effective gene selection. The key to the problem necessitates a com-mon way to represent knowledge that meets the following requirements: (1) information can be easily extracted from both types of information; (2) information can be combined for integration; and (3) information can be effectively used for gene selection. Hence, we propose to use a geometric pattern among samples as the common representation. We show: (1) given relationships among genes and the mRNA expression profiles, we can use the relationships to obtain a geometric pattern of samples; (2) upon obtaining local geo-metric patterns from various data sources, we can combine them to form a global pattern; and (3) using the obtained global pattern we can effectively select genes that bear both biological and statistical relevance. As shown in Figure 1, given multiple heterogeneous data sources, we first extract local sample geometric patterns from each data source, then combine them to form a global pattern, based on which dis-criminative genes can be identified. We first show how to select genes with a given geometric pattern of samples. We propose Geometry-Dependent Covariance and employ it for gene selection. In the following, we adopt the notations from feature selection, and use features and genes interchange-ably.
 Figure 1: Integration of multiple heterogeneous data sources for gene selection: Local geometric patterns are extracted from data sources and are combined to form a global pattern for identifying relevant genes. Let X = ( x 1 , ..., x n ) be a data set containing n samples. The sample covariance matrix [27] of X is given by: Here  X x = 1 n P n k =1 x k and C  X  S + is an unbiased estima-tor of the covariance matrix. C captures the correlation be-tween all possible feature pairs. Let C i,j be the ij -th element of C . It measures the covariance between features F i and F , and is given by: C i,j = 1 n  X  1 f i  X   X  f i  X  1 T f j Here,  X  f i = P n k =1 f ik . Assuming data has been centralized:  X x = 0, it can be verified that  X  i,  X  f i = 0, and we have: To simplify the notation, in the following, we assume X has been centralized. In this work we propose to adjust the covariance measures between features according to a given geometric pattern of samples. Given W , the similarity ma-trix that captures the geometric pattern of samples, and L , the corresponding Laplacian matrix. We give the concept of the geometry-dependent sample covariance.

Definition 1. Geometry-Dependent Sample Covariance with L . Given f i and f j , the feature vectors of features F and F j , and L , a Laplacian matrix, the Geometry-Dependent Sample Covariance between F i and F j is given by: where  X  : S +  X  S + is a prescribed spectral matrix func-tion which is induced from an non-increasing real function of positive input,  X  (  X  ) : (0 ,  X  )  X  (0 ,  X  ) . Let A  X  S + , A = U  X  U T be the singular value decomposi-tion (SVD) [10] of A , so U T U = I and  X  = diag (  X  1 , . . . ,  X  Here diag ( a 1 , . . . , a n ) denotes the diagonal matrix with a its i -th diagonal element. We can calculate  X  ( A ) by  X  ( A ) = U  X  non-increasing and an example is to define  X  (  X  ) = 1  X  + where &gt; 0. This gives  X  ( L ) = ( L + I )  X  1 .
In Equation (3), f and L are used to calculate the co-variance measure, however the scale of the two factors can affect the measure arbitrarily. An effective way to handle this issue is to apply normalization on f and L . Let the normalized feature vector 1 of f be defined as: We can define a geometry-dependent covariance measure us-ing normalized Laplacian matrix L :
Definition 2. Geometry-Dependent Sample Covariance with L . Given e f i and e f j , the normalized feature vectors of F and F j , and L , the normalized Laplacian matrix, Geometry-Dependent Sample Covariance between F i and F j is: In the two definitions, we assume data has been centralized. We have the following theorem for  X  C and e C :
Theorem 1. b C and e C have following properties: 1 . Assume  X  (0) = 0 and all features have unit norm 2 , let W = 1 n  X  11 T , we have b C = e C =  X  (1)  X  C , where C is the standard covariance matrix defined in Equation (1). 2 . b C and e C are symmetric positive semidefinite.
The form of normalization is commonly used in spectral dimension reduction and clustering, e.g. Laplacian Eigen-map [1]. Basically, the feature vectors are adjusted accord-ing to their nearby data density before normalization.
It can be verified that if  X x 6 = 0, the proposition still can hold when  X  (0) = 0. Also if features are not normalized, the proposition still holds for b C but not e C .
 Proof : The second property can be easily verified, here we provide a proof for the first one. Given the definition of W , it can be verified that D = I , therefore L = L . Since all features have unit norm, we have e f i = f i which means b C = e C . Let L = U  X  U T be the SVD of L, it can be verified  X  = diag (1 , . . . , 1 , 0), and 1 is the n  X  1 repeat eigenvalues of L . Thus we have  X ( L ) =  X  (1)  X  L , also b C = e C = XLX. Since L = I  X  1 n  X  11 T , we have L = L 2 . Substituting in L 2 and by noticing XL = X  X   X x1 T , we have:
The theorem says that (1) by setting W in a special form, the geometry-dependent covariance matrix is equivalent to a scaled standard covariance matrix; and (2) since any sym-metric positive semidefinite matrix is a valid covariance ma-trix, we see b C and e C are valid as covariance matrices.
Given two feature vectors, f i and f j , geometry-dependent covariance measures their covariance by considering geomet-ric pattern captured by the Laplacian matrix L . To see this, let L = U  X  U T , U = ( u 1 , . . . , u n ), we have To compare f i and f j , geometry-dependent covariance first projects the two vectors into a space spanned by ( u 1 , . . . , u then weight the transformed vectors by  X  (  X  1 ) 1 2 , . . . ,  X  (  X  Let a i = u T i f , where a i measures how well f and u i each other. We can write weighted transformed vector as: Each eigenvalue of L measures the consistency between the corresponding eigenvector and the given geometric pattern captured by L [29]. The bigger the value, the more inconsis-tent the eigenvector. A non-increasing  X  (  X  ) ensures to assign small weights to eigenvectors which are inconsistent to the given geometric pattern. If a feature vector f only aligns well to inconsistent eigenvectors, the weighting mechanism ensures that all elements in f w will be small. Thus, in covari-ance calculation, the inner product between f w and another weighted transformed vector will be small, even if the two vectors align well. This explains why the proposed measure is geometry-dependent . Similar analysis holds for e C .
The global geometry pattern obtained from multiple data sources reflects some intrinsic relationship among instances. Therefore, genes supporting this relationship need to be con-sistent with the obtained pattern [32]. With the global ge-ometry pattern, one can build a geometry-dependent co-variance matrix, based on which features can be selected using two methods: (1) sorting the diagonal of the covari-ance matrix and choosing the features with the biggest vari-ances; and (2) applying sparse principle component analysis (SPCA) [7] to select a set of features that can maximally retain the total variance, assuming that all features have similar scales. According to Equation (7), the diagonal el-ements of b C is given by: b C i,i = P n i =1 a 2 k  X  (  X  discussed, for b C i,i to achieve big value, f i must align well to the eigenvectors that are consistent with the given geo-metric pattern depicted by L . In other words, a bigger b indicates that f i is more consistent with the given geometric pattern. Therefore, selecting features according to the first method is equivalent to selecting features which are con-sistent with the given geometric pattern. Similar analysis holds for e C , where all features are automatically normalized to have similar scale. Since the first method measures fea-ture relevance individually, it may select a feature set with redundancy. The second method, applying SPCA, consid-ers the interaction among features, and is able to select a feature set containing less redundancy. Gene selection aims to identify all relevant genes as candidates for examination. Hence, the first method is more proper becuase the process of removing statistical redundancy may cause the removal of potentially biologically relevant genes.
The calculation of geometry-dependent covariance involves a spectral matrix function induced from a non-increasing real function, which may require a full eigen decomposition on L or L , and has a time complexity of O ( n 3 ), where n is the number of samples. This does not cause problems for gene selection, since the involved data are usually of small sample size. However, if the sample size is large, calculating  X ( L ) or  X ( L ) will be expensive. With the following theorem, we show that for the geometry-dependant covariance defined with L , given W  X  S + 3 , we are able to obtain  X ( L ) in O ( n with a special  X  (  X  ). The theorem can be verified via the fact that the i -th eigenvalue and eigenvector of D  X  1 2 W D  X  given by 1  X   X  i and u i of L , and u 1 = D
Theorem 2. Given  X  (  X  ) defined as: We have where  X  is the diagonal matrix with  X  i,i = D 1 2 f i
Given relationships among samples, it is straightforward to extract geometric patterns. We now show how to extract a geometric pattern of samples using gene relationships and study how to construct a global pattern via linearly combin-ing obtained local patterns.
Given relationships among genes, we propose to use the relationships to construct a covariance matrix C g for genes, which can be used to calculate the Mahalanobis distance [20] among samples to reveal sample geometric patterns:
For example, W is provided by a kernel matrix. This con-dition ensures the eigenvalues of L is bounded by 1 from above, so that  X  (  X  ) is valid.
 In Equation (10), x , y  X  R d are gene expression profile of two samples. By incorporating C g for calculating sample distance, we adjust the sample geometric pattern by con-sidering the given relationships among genes. In real appli-cations, the relationships among genes are usually specified by: graphs (or kernels), e.g. biological pathway and protein-protein interaction. Or can be derived from gene descrip-tions, e.g. gene annotation [4]. In the second case, each gene is described by several terms from a fixed vocabulary. If we treat genes as features, C g can be obtained directly via Equation (1). However in the first case, the explicit ex-pression of genes is unavailable, therefore in the case we can not obtain C g directly. To handle the issue, we propose to calculate an embedding [2] from a given affinity matrix W or kernel matrix K and use the obtained embedding to construct C g with Equation (1).

Specifically, given an affinity matrix W , we propose to construct commute time embedding [18], which is shown to be effective in real applications [29]. Let L = D  X  W and L = U  X  U T be the SVD of L , the embedding of F 1 , . . . , F are given by Y =  X  + 1 2 U T , where each column of Y corre-sponding to an f i . By transposing Y we obtain the explicit X EM in Equation (1), we can obtained a C g .

Theorem 3. Given an affinity matrix W depicting rela-tionships among genes, using commute time embedding, C g is given by: Similarly, we can show that given a kernel matrix K , using kernel PCA [22] for constructing embedding, we have C g is given by: K I  X  1 l U 11 T U T K , where K = U  X  U T .
Given multiple local geometric patterns, the global pat-tern can be obtained by linearly combining local patterns [33]. In the equation, W G i s are the geometric patterns extracted from relationships among genes; W S i s are the ones extracted from relationships among samples, and  X  G i and  X  S j are the combination coefficients, which can be assigned by domain experts according to their domain knowledge 4 [33], or, if the label information is available, learned automatically via convex optimization based on a set of kernel matrices carry-ing the information about the local geometric patterns. We refer readers to literature for comprehensive study on the research issues of kernel combination [15, 31].
The above technical discussion has paved way for us to propose a framework for M ulti-S ource G ene S election: MSGS . The detail of the framework can be found in Algorithm 1. It consists of three major steps: (1) obtaining local geometry pattern from each data source (Lines 3-5); (2) combining the local patterns to construct a global geometry pattern (Line
This provides a way to incorporate domain knowledge 6); and (3) using the global pattern to form a geometry-dependent covariance matrix and selecting genes (Lines 7,8). Below we give analysis on time complexity for MSGS . Algorithm 1 MSGS : Multi-Source Gene Selection 1: Input : D G 1 . . . D G h G , D S 1 . . . D S h S and X 2: Output : List gene -the selected gene list 3: for each D i  X  D G 1 . . . D G h G , D S 1 . . . D S h S 4: construct W G i s and W S j s, the local geometric patterns; 5: end for 6: obtain global pattern W from W G i s and W S j s; 7: form the geometry-dependent covariance matrix C ; 8: select genes according to C and form List gene ; 9: return List gene
Since the representation of the h G + h S data sources are heterogenous, the time complexity of extracting local pat-terns from data sources can vary greatly. Assuming for data sources D G 1 . . . D G h G , each data sources provides us an affin-ity matrix depicting gene relationships and involving l genes, then constructing C g using Equation (11) and calculating its (pseudo) inverse requires O ( l 3 ) operations. Computing Ma-halanobis distance among n 2 pairs of instances and forming a RBF kernel require O ( l 2 n 2 ) operations. Crossing the h data source, the total cost is O ( l 2 n 2 + l 3 ) h G . Assuming for D S 1 . . . D S h S , each data source has d features depicting the same set of n samples, and we use RBF kernel to represent the local sample geometric pattern. The time complexity of forming a RBF kernel on each data source is O ( dn 2 ). And crossing h S data source, the cost is O h s n 2 d . Therefore, the total cost of the first step is O ( l 2 n 2 + l 3 ) h Assuming we linearly combine W i s with a set of prescribed combination coefficients, the cost is O ( h G + h S ) n 2 the method specified in Theorem 2 to form e C , the cost is O ( dn 2 + d 2 ). Selecting genes based on e C requires O ( d log d ) operations, if we use gene variance; or O ( d 3 ), if we use the SPCA approach proposed in [7]. Hence the overall time com-plexity of MSGS is O ( l 2 n 2 + l 3 ) h G + n 2 dh S + d gene variance; otherwise, O ( l 2 n 2 + l 3 ) h G + n 2 dh
We empirically evaluate the performance of MSGS for multi-source gene selection in terms of statistical significance and biological relevance. Correspondingly, we choose accu-racy and hit ratio 5 as performance measures. MSGS is an unsupervised gene selection algorithm using multi-source data. Hence, label information is only used after select-ing genes to calculate accuracy during the testing phase to measure how good selected genes are. In addition, we also measure robustness of selected genes by varying the num-ber of classes, if class information is used in gene selection. The results presented in this section are based on e C , the geometry-dependent sample covariance matrix defined with L . This is because in the experiment we found e C provides more robust performance than b C , i.e., the normalization
For accuracy, we first filter the data with selected genes and build a classifier on the filtered data, then obtain its accuracy as performance measure. For hit ratio, given a list of genes, we check how many of the genes are cancer related by using the gene function annotation information provided by Ingenuity Pathways Analysis (IPA) system [14]. step does lead to better performance. We next introduce the data sets used in this work.
Human Cancer Data . It consists of five heterogeneous data sources. Two sets of gene expression profiles from a mixture of 88 normal and cancerous tissue samples 6 : a miRNA expression profile for 151 human miRNAs and a mRNA expression profile for 16,063 human mRNAs [19, 13]. miRNA profile provides relationships among samples and it is observed in [19] that comparing with mRNA, miRNA ex-pression profiles is of more power in terms of discriminating cancer from noncancer tissues as well as cancer of different types of tissues. Three gene information profiles are pro-vided: Gene Function Annotation , Biological Pathway and Gene Ontology Annotation . The three profiles are gener-ated as follows. (1) Gene Function Annotation : we used the name of involved tissues (e.g., colon, lung, . . . ) as key-words to search in IPA system [14] for cancer related pro-cesses, for each matching process, we add all genes involved in the process to the graph and connected them with edges. This resulted in a graph containing 535 genes. (2) Biological Pathway : with the mRNA expression profile, we used the IPA system to infer the relevant pathways, which results in about 40 networks of molecules, connected these networks and obtained a graph involving 571 genes. (3) Gene On-tology Annotation : 68 Gene Ontology annotation [4] terms, related to cell cycle, cell growth, differentiation, apotopo-sis, cancer development and so on, were provided by do-main experts. According to whether a term is assigned to a gene, we obtained a matrix with a size of 16063  X  68, which can be used to compute a covariance matrix among genes. The three profiles provide relationships information among genes. Using the five profiles, we obtained two sets of data: 2C-DATA and 4C-DATA. 2C-DATA contains all 88 tissue samples of the original data and class label is assigned to a sample according to whether the sample is a normal or a tumor tissue. 4C-DATA contains 33 tissue samples from 4 types cancerous tissues, Mesothelioma, Uterus, Colon and Pancreas , each has at least 7 samples. A summary of the two data sets is given in Table 1.

Table 1: A summary of the Human Cancer Data.
In the experiment, we compare gene selection using single (target) data source with using multiple data sources. We
Tissues involved:colon, pancreas, kidney, bladder, prostate, ovary, uterus, lung, mesothelioma, melanoma and breast. chose 3 feature selection algorithms for gene selection using single data sources. They are two unsupervised algorithms, Laplacian Score [12] and PathSPCA [7], and one supervised algorithms, ReliefF [24]. Experiments are performed in the Matlab environment. We use RBF kernel to represent the local and global geometric patterns. A domain expert deter-mined two sets of combination coefficients for linear combi-nation of the local geometric patterns: COMB-1, using the combination coefficients (0.0, 0.5, 0.5, 0.0, 0.0) for 2C-DATA and (0, 0.3, 0.4, 0.3, 0) for 4C-DATA; and COMB-2, using (0.1, 0.3, 0.3, 0.2, 0.1) for 2C-DATA and (0.1, 0.2, 0.4, 0.2, 0.1) for 4C-DATA. The usefulness of each data source can also be determined by checking the quality of the genes se-lected by MSGS using the geometric pattern extracted from the data source. We also tried to use class label to learn com-bination coefficients via a kernel learning approach proposed in [31]. We apply 1NN classifier to data with selected genes, and use its accuracy to measure the quality of the gene set. Reported results are based on averaging the results from 10 trials of experiments.
Experimental results are organized in terms of two sets of multi-source data with different numbers of classes. When label information is available, we study how to incorporate it in MSGS and its effect. We examine the biological relevance of the genes selected by MSGS . In the following, MSGS -VAR stands for selecting genes using gene variance, and MSGS -SPCA for using the sparse PCA method.
Figure 2-(a) compares unsupervised baseline gene selec-tion algorithms using mRNA profiles with MSGS -VAR us-ing five individual profiles as well as the combinations of them. The results show that by using miRNA profiles, COMB-1 and COMB-2, MSGS -VAR selects genes that pro-vide the best accuracy which are significantly better than those achieved by genes selected using baseline algorithms. Table 2 shows the detailed accuracy results as well as hit ratio. According to hit ratio, averagely, using COMB-2, the gene list provided by MSGS -VAR containing the most known cancer relevant genes (7), which is following by us-ing COMB-1 (6.6), miRNA (6.6) and Function Annotation (5). According to hit ratio, Biology Pathway is also helpful (3.6). The averaged number of selected known cancer re-lated genes is 5.8 for SPCA and 3.2 for Laplacian score. We notice that the first two genes selected by MSGS -VAR with mRNA profile, Function Annotation, COMB-1 and COMB-2 are all known to be cancer relevant. We will provide bio-logical relevance analysis for selected genes later. The obser-vations suggest that (1) the geometric pattern obtained from miRNA profile indeed possesses better discriminative power, which is consistent with the findings in [19], and (2) given a geometric pattern with higher quality, MSGS -VAR can select better genes. This support the use of multiple data sources in gene selection, and show that MSGS is effective. (3) By combining multiple heterogeneous data sources we are able to achieve better performance than using any indi-vidual data source. This is consistent with the observations in [15]. Figure 2-(c) plots the performance of MSGS -VAR when different combination coefficients are used to combine local geometry patterns obtained from miRNA profile and Function Annotation. We observe that the highest accuracy is achieved by using the two data sources together. This in-dicates the existence of complimentary information, which helps to improve the estimation of the geometric pattern of the underlying model. Similar trends can be observed in the experimental results of MSGS -SPCA. Figure 2-(d, e) and Table 2 contain the results on 4C-DATA. We obtained largely similar observations as those from using 2C-DATA. Since the number of samples becomes smaller but the number of classes becomes larger, we observe that the unsupervised baseline algorithms perform worse comparing with on 2C-DATA. However, the performance of MSGS using miRNA profile, Function Annotation, Bi-ological Pathway, COMB-1 and COMB-2 are consistently good in terms of accuracy and hit ratio. This indicates that MSGS can effectively select relevant genes given high quality geometric patterns.
When label information is available, we can incorporate it in MSGS in two ways: (1) using a supervised metric learning algorithm to learn a geometric pattern on the data and input the obtained pattern into MSGS . In this work we implement MSGS -OLDA: we use OLDA [30] to learn a geometric pat-tern and feed it into MSGS -VAR. (2) Learning the combina-tion coefficient automatically with a supervised kernel learn-ing algorithm and input the combined pattern into MSGS . In this work we implement MSGS -kerCB: we use the approach proposed in [31] to learn the combination coefficients 7 . For comparison we also include a supervised selection algorithm ReliefF [24] in the experiment. The upper part of Tabel 3 shows the performance of supervised gene selection algo-rithms on 2C-DATA. For accuracy, MSGS -OLDA performs the best (0.92). For average hit ratio, MSGS -KerCB per-forms the best (5.6), while we observed that the averaged hit ratio of MSGS -VAR with COMB-2 is 7.

Furthermore, we experimented how robust supervised gene selection is by applying the genes selected on 2C-DATA by supervised algorithms and MSGS -VAR to 4C-DATA to check their discriminative power. The results are shown in the lower part of Table 3 and Figure 2-(f). We observe that the genes selected by MSGS -VAR using miRNA pro-file, COMB-1 and COMB-2 can discriminate cancer from different types of tissues, however, those selected by super-vised algorithms cannot. This finding suggests that (1) re-sults of supervised feature selection hinge upon the target concept and the change of class information can results in the selection of different genes; and (2) the geometric pat-tern corresponding to miRNA profiles as well as COMB-1 and COMB-2 is relatively stable -genes selected by MSGS can discriminate data of different numbers of classes, indi-cating that it may be consistent with intrinsic structure of the underlying model and more robust. MSGS is an un-supervised algorithm and obtains intrinsic patterns that do not vary with class definitions from multiple data sources. This is not so for supervised algorithms: with different class definitions, different sets of genes will be selected, which is clearly shown in Table 3.
The learnt coefficients on 2C-DATA is: (0,0.11,0.89,0,0) genes using sparse PCA method.
In the experiments above, we used hit ratio to measure biological relevance of selected genes. In order to closely ex-amine biological relevance of selected genes, we performed a further study in which our biologist collaborators examined the top 20 genes selected by MSGS -VAR with COMB-2 on 2C-DATA. 8 It turned out that 17 of the top 20 genes were experimentally confirmed to be cancer related, except for SMNT , LAD1 and LMOD1 . Detailed information of the se-lected genes can be found in Table 4. Among them, nine were already annotated to be related to cancer or tumor in the IPA system. The other genes are found to be differen-tially expressed in unique or several cancer cell lines and are supported by literature. Enzymes involved in metabolism, such as FABP1 and GPX2 (well-known oxidoreductase re-sponsive to oxidative stress) were selected. FABP1 plays an important role in lipid metabolism and investigations have demonstrated altered systemic lipid metabolism in cancer patients [9]. GPX2 is one of the major cellular antioxidants as biomakers for reactive oxygen species (ROS) producing in animal, and plants. Hydrogen peroxides, one of ROS, has been shown to act as tumor promoters [26]. Several tu-mor suppressor genes were also identified, including FHL1 ,
The reason is that this list results in high accuracy on both 2C-DATA and 4C-DATA and its hit ratio is also high. SPARCL1 and SYNPO2 . Interestingly, most of these genes encode transmembrane proteins, implicating their role in signal transduction during cancer development. Thus, our analyses show that multi-source gene selection is able to se-lect genes bearing both statistical significance and biological relevance.

The obtained results on Human Cancer Data confirm the efficacy of multi-source gene selection algorithm MSGS as well as the potential of multi-source gene selection. The MSGS software will be made available for research purposes.
Multi-source gene selection presents a new way of dealing with wide data. It leverages information from disparate data sources aiming to form a global pattern describing intrin-sic structures embedded in multi-source data. We proposed an unsupervised gene selection algorithm for multi-source data, MSGS . Five data sources are used in this study which demonstrated that MSGS is effective in multi-source gene selection in terms of statistical significance and biological relevance. In addition, we discussed the nuanced roles of su-pervised and unsupervised gene selection. Furthermore, the identified genes through our multi-data analysis bear im-portant biological relevance, highlighting a new way of gene selection, and offering guidance for experimental biologists. to their relevance scores assigned by MSGS -VAR (from highest to lowest). Table 3: Upper: accuracy and hit ratio by super-vised algorithms. MSGS -VAR with COMB-2 is un-supervised and listed for comparison. Lower: accu-racy on 4C-DATA with genes selected by from 2C-DATA. It shows that genes selected by MSGS -VAR are discriminative on both 2C and 4C DATA, but not so for those selected by supervised algorithms. [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for [2] Y. Bengio, O. Delalleau, N. L. Roux, J.-F. Paiement, [3] T. D. Bie, L. C. Tranchevent, L. Oeffelen, and [4] E. Camon, etal. The gene ontology annotation (goa) [5] F. Chung. Spectral graph theory . AMS, 1997. [6] A. Cimmino, etal. mir-15 and mir-16 induce apoptosis [7] A. d X  X spremont, F. Bach, and L. E. Ghaoui. Optimal [8] J. Dy. Unsupervised feature selection. In H. Liu and [9] C. Gercel-Taylor, D. L. Doering, F. B. Kraemer, and [10] G. H. Golub and C. F. Van Loan. Matrix [11] J. Hagan and C. Croce. Micrornas in carcinogenesis. [12] X. He, D. Cai, and P. Niyogi. Laplacian score for [13] J. C. Huang, etal. Using expression profiling data to [14] Ingenuity-Systems. Ingenuity pathways analysis. [15] G. R. G. Lanckriet, etal. Jordan. Learning the kernel [16] T. Li, C. Zhang, and M. Ogihara. A comparative [17] H. Liu and H. Motoda, editors. Computational [18] L. Lovasz. Random walks on graphs: A survey. [19] J. Lu, etal. Microrna expression profiles classify [20] P. Mahalanobis. On the generalized distance in [21] H. Pingzhao, G. Bader, D. Wigle, and A. Emili. [22] B. Scholkopf, A. Smola, and K.-R. Muller. Nonlinear [23] M.-L. Si, etal. mir-21-mediated tumor growth. [24] M. R. Sikonja and I. Kononenko. Theoretical and [25] C. Sima and E. R. Dougherty. What should be [26] T. J. Slaga, etal. Skin tumor-promoting activity of [27] M. R. Spiegel. Theory and Problems of Probability and [28] H. Tazawa, etal. Tumor-suppressive mir-34a induces [29] U. von Luxburg. A tutorial on spectral clustering. [30] J. Ye. Characterization of a family of algorithms for [31] J. Ye, J. Chen, and S. Ji. Discriminant kernel and [32] Z. Zhao and H. Liu. Spectral feature selection for [33] D. Zhou and C. Burges. Spectral clustering and
