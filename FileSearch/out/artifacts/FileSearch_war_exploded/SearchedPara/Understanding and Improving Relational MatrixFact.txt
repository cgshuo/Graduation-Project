 Matrix factorization techniques such as the singular value decomposition (SVD) have had great success in recommender systems. We present a new perspective of SVD for con-structing a latent space from the training data, which is justified by the theory of hypergraph model. We show that the vectors representing the items in the latent space can be grouped into (approximately) orthogonal clusters which cor-respond to the vertex clusters in the co-rating hypergraph, and the lengths of the vectors are indicators of the represen-tativeness of the items. These properties are used for making top-N recommendations in a two-phase algorithm. In this work, we provide a new explanation for the significantly bet-ter performance of the asymmetric SVD approaches and a novel algorithm for better diversity in top-N recommenda-tions.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  information filtering recommender system, matrix fa ctorization, hypergraph
In a recommender system, there are two types of entities, i.e. users and items. The users give weighted connections, or ratings, to the items. These connections can be modeled as a rating matrix R where the rows represent the users and the columns represent the items. The entry R ( i, k ) is the rating given by user i to item k (if the rating exists). The goal of the recommender system is to predict unseen items that might be rated with high scores. A common assumption for making predictions is that certain latent factors are associated with each user and each item, and a user gives ratings according to her latent factors and the latent factors of the items. Figure 1: Recommender system using matrix fac-torizations.

For example, in the Lastfm dataset, the items are the artists in a music website. The latent factors could be the music genres. The recommender system first obtains the genres of the artists, and the users X  interests on different genres. Then the system makes recommendations by align-ing the user X  X  interests to the genres of the artists.
For ratings modeled as the matrix R , various matrix fac-torization techniques have been applied to extract the latent factors and make recommendations. Figure 1 shows a typ-ical recommender system using matrix factorization. The old ratings given by existing users are first decomposed into multiple matrices to extract the latent factors. When new users come into the system, their ratings are combined with the extracted latent factors to make recommendations.
Sarwar et al. [17] proposed one of the early works that use the low-rank Singular Value Decomposition (SVD) approx-imation to replace the missing values in the original rating matrix, i.e. R  X  U l S l V l where the sizes of the matrices are R trix S l contains the l biggest singular values of R ,andthis low-rank approximation is also called the truncated SVD . This approach relies on the fact that the rank-l truncated SVD approximation  X  R = U l S l V l minimizes the Frobenius norm  X  R  X  R F among all the rank-l matrices, and we hope that the matrix  X  R would generalize the correct ratings to the missing entries in R . Note that imputation of missing values is implicitly required when computing the truncated SVD. We can represent user i with the vector  X  u i = S  X  1 / 2 and item k with the vector  X  v k = S  X  1 / 2 l V l ( i,  X  dicted rating from user i to item k is then  X  R ( i, k )=  X  where  X  u i and  X  v k canbeconsideredaslatentvectors.
During the Netflix prize competition, Funk [5] proposed an iterative way to compute the low-rank SVD approxima-tion on a very big matrix that minimizes only the Frobenius norm on the non-empty entries in the original rating ma-trix. Based on Funk X  X  work, Paterek [14] suggested a new model to reduce the number of parameters that need to be stored in the recommender system. In Paterek X  X  approach, a truncated SVD is computed as before, but only the right singular vectors in V l are taken into the second step. In the second step, item k is represented by v k which is the k -th column of V l , while user i  X  X  latent vector is computed by a linear combination of the v k  X  X  whose corresponding items are rated by user i . Finally the rating prediction is the in-ner product of a user X  X  latent vector and an item X  X  latent vector. Koren [9] pointed out that this  X  X symmetric SVD X  which takes only the right singular vectors actually works better in prediction accuracy. However, besides the empiri-cal studies, the reason of such improvement for asymmetric SVD is not clear.

The main contributions of this paper include (1) anew explanation of the (approximate) orthogonal structure of the latent space constructed from the asymmetric SVD, which is used as an intermediate step in a top-N recommender system, and (2) a novel algorithm based on a hypergraph formulation that brings more diversity in the recommenda-tions by applying normalizations in the asymmetric SVD.
In the reminder of the paper, we start with the hyper-graph representation and the normalized hypergraph cut. We show that the asymmetric SVD approach follows the hypergraph learning scheme, and there is an (approximate) orthogonal structure associated with the right singular vec-tors. Based on the lengths of the latent vectors obtained from the right singular vectors, the items can be categorized into anchor items and non-anchor items. Finally we present atop-N recommendation algorithm that utilizes the above structures, and conduct experiments to compare our algo-rithm with the state-of-the-art approaches.
Suppose we have two sets of entities: the user set Y = { y 1 ,y 2 , ..., y m } of size m , and the item set Z = { z 1 of size n . The items in set Z belong to s clusters { C 1 C } .Eachcluster C j is a subset of Z and C j  X  C j =  X  for j = j .

Taking the Lastfm dataset as an example again, let the set Y be the set of users and the set Z be the set of artists. Clusters or partitions over Z can be established according to the genres of the artists. For example, C 1 includes the rock music artists, while all the country music artists are in C . Eachuserintheset Y , on the other hand, is assumed to have a preference over music genres. If the user likes rock music, she would listen to the artists from C 1 with high probability. If the user does not like country music, she would rarely listen to the artists from C 2 .
The mutually exclusion between clusters of items is the basic assumption in our analysis. Each cluster can be con-sidered as a latent factor by which the ratings are generated. In some cases, however, an artist could play in multiple gen-res. This does not pose a problem to our basic assumption, because we can create a cluster (latent factor) that corre-sponds to a combination of genres. In fact, if the intuition behind the latent factors follows some categorical attributes, e.g. genres, languages, professions, etc., or a combination of categorical attributes, our basic assumption always holds. Figure 2: The hypergraph representation of the rat-ings in Figure 1 and the induced graph.
The matrix R contains the ratings between the users and the items, which can be seen as relations between two sets. For simplicity, we assume that all the ratings are bigger than zero, thus the empty entries in the m  X  n rating matrix R can be represented by zeros. If we ignore the actual ratings in
R , we can obtain a binary matrix X =( R &gt; 0) that represents only the relations, i.e. X ( i, k )=1if R ( i, k otherwise X ( i, k ) = 0, and the resulting matrix X can be precisely represented by a hypergraph.

A hypergraph is an extension of a graph for modeling re-lations. In a graph, an edge connects exactly two vertices, while in a hypergraph a hyperedge connects any number of vertices. For a binary relation matrix X , the items are con-sidered as the vertices of a hypergraph, and a user is repre-sented by a hyperedge which contains all the items (vertices) that have been rated by the user. Following the tradition of visualizing a hypergraph in literatures, Figure 2 shows an example of the hypergraph representation. The matrix X is also called the incident matrix of the hypergraph. The ver-tex degree (of an item) is the sum of the k -thcolumnof X i.e. deg( z k )= i X ( i, k ). The hyperedge degree (of a user) is the sum of the i -th row of X , i.e. deg( y i )= k X (
Recall that we assume there are some clusters of items, and these clusters correspond to the latent factors. Thus ex-tracting the latent factors is equivalent to finding clusters of items (vertices) in the hypergraph. There is a wide range of studies for clustering the vertices in a hypergraph [8, 1, 22]. In this paper, we focus on one particular line of studies that convert a hypergraph into an approximated graph so that the graph clustering algorithms can be applied to the hy-pergraph. Zhou et al. proposed a transformation to convert a hypergraph into a graph based on the Normalized Hyper-graph Cut (NHC) [22] (others include the clique-expansion, the start-expansion [1], and the hyperedge-expansion [15]). We denote the transformation proposed in [22] as  X  X HC X , and use it to find clusters in the hypergraph.

By applying the NHC transformation, the vertices of the hypergraph are copied into a new graph. Then each hyper-edge is converted into a clique of weighted edges in the new graph that connect all the vertices in the original hyperedge. The weights of the edges in the clique are normalized by the hyperedge degree so that the contribution of each hyperedge to the new graph is the same. With a linear combination of all the cliques from all the hyperedges, the new graph can be seen as an approximation of the hypergraph in the sense that any two vertices connected by some hyperedges are also connected in the new graph. We call this new graph the induced graph (see Figure 2).
One can show that the adjacency matrix of the induced graph is X D  X  1 e X ,where X is the transpose of X and D e =diag( X 1 ) is the hyperedge degree matrix. D e is ac-tually the diagonal matrix of row sums of X ,and 1 is an all-ones vector of proper length. With the NHC transfor-mation, it is proposed to use the normalized Laplacian of the induced graph to find clusters of vertices [22], where the normalized Laplacian matrix L is defined as Matrix I is an identity matrix, and D v =diag 1 X is the vertex degree matrix.

As in standard spectral graph theory [4], the clustering algorithm involves computing the leading (smallest) l eigen-values {  X  1 , X  2 , ...,  X  l } of L and the corresponding eigenvec-tors [ f 1 , f 2 , ..., f l ]= F . Thenavertex(oranitem z be represented by a vector  X  k that is the k -thcolumnof F (note that F is of size n  X  l ). We call the l -dimensional vector  X  k the latent vector ,andthe l -dimensional space the latent space . Once the latent vectors are computed, one can apply any clustering algorithm (such as k-means) in the la-tent space to find clusters of vertices.

This scheme of spectral technique (the list of eigenvalues are often called the spectrum of the matrix) has been widely used in many areas. It has close connections to the Principal Component Analysis (PCA), SVD, and many other matrix factorization techniques. In the next section, we show that the NHC transformation of a hypergraph is equivalent to a SVD, and under certain conditions there is a structure with the resulting latent vectors. Note that although the actual ratings are ignored in the hypergraph representation, they will be used again in the top-N recommendation algorithm presented in Section 4.
The normalized Laplacian L is a symmetric matrix. Let We can rewrite L = I  X   X  X  X  X . The biggest l eigenvalues of  X  X  X  X are exactly { 1  X   X  1 , 1  X   X  2 , ..., 1  X   X  l } responding eigenvectors are still [ f 1 , f 2 , ..., f l ]= we decompose the matrix  X  X by the full SVD  X  X = USV , where U and V are unitary matrices, and S is a rectangular diagonal matrix. The biggest l singular values in S are ex-actly { V (right-singular vectors) are the eigenvectors of  X  X  X  X Therefore, instead of computing the eigen-decomposition of L , we can obtain F from the SVD of  X  X , i.e. F is the sub-matrix of the first l columns of V .

Since we are looking for the right singular vectors of  X  X sociated with the largest l singular values, computing F can be done by the truncated SVD  X  X  X  U l S l V l = U l S l Thus the NHC transformation is equivalent to the asymmet-ric truncated SVD of  X  X . We call the column vectors in the  X  X rofiles X  of the items.

There are many advantages of using SVD to compute the  X   X  X  (latent vectors) rather than the eigen-decomposition of L . Firstly, the matrix  X  X is usually sparse, but  X  X  X  X be non-sparse. When n is large, the computational cost and storage cost of the eigen-decomposition might be impracti-cal. Secondly, there are existing approaches to implement SVD incrementally, which allows us to just compute the mi-nor changes when modifying  X  X .
We first consider a special case where all the items in one cluster are rated by the same set of users. In other words, each cluster C j is associated with a subset of users and these users have rated all the items in C j . We call this special case the  X  X ull concentrated case X  because it can be modeled with a beta distribution with a small concentration parameter. If there are s clusters, the matrix  X  X can be written as where the column vectors in one cluster are all the same. We call the items in the s clusters the anchor items because they define the cluster centers and remain clustered in the latent space.

In spectral graph theory, if the graph contains s discon-nected components, one can show that the latent vectors ob-tained from the first s eigenvectors of the graph Laplacian are aligned with the axes of the s -dimensional latent space [13]. When the disconnected components are connected by some weak links, the latent vectors are still aligned with approximately ort hogonal lin es [20]. Howev er, the induced graph of a hypergraph usually does not consist of more than one connected component. Figure 3 shows an example of a hypergraph in full concentrated case. The adjacency matrix of the induced graph i s not a diagonal block matrix (not even approximately), so we need to develop a new analysis to support the clustering algorithm for hypergraphs. Figure 3: Examples of the incident matrix and the latent vectors with the full concentrated case and noisy cases. The noises are added by randomly flip-ping some entries of X . Approximately orthogonal structures can be observed in the noisy cases. In the charts of latent vectors, the gray lines are from the origin to the cluster centers.

If the rank of  X  X is s , i.e. the anchor item profiles  X  linearly independent, the truncated SVD  X  X = U l S l V l an exact decomposition when l = s . Recall that the latent vectors in F can be obtained from F = V l . The following statement shows the structure of F .
Proposition 1. If  X  X is in the form of Eq. (3) and  X  x are linearly independent, the latent vectors F =[  X  1  X  X  X   X  given by the truncated SVD  X  X = U s S s F (columns of F are normalized to 1) can be grouped by the clusters  X  k = for  X  z k  X  C j .The  X  k  X  X  from the same cluster are identical (denoted as  X  j ). Furthermore, we have  X  j  X  j =0 for  X  j j for proof.)
If all the items are anchor items, i.e. in the form of Eq. (3) and having linearly independent  X  x j  X  X , the latent vectors would be in exactly s clusters whose centers are orthogonal toeachother(seeFigure3).Thelengthofthelatentvector is the reciprocal of the cluster size. Unlike the graph case, the latent vectors computed from a hypergraph are usually not aligned with the axes of the latent space, but up to a rotation that depends on the cluster sizes and the distribu-tion of non-zero entries in X . Nevertheless, the orthogonal structure of the latent vectors still provides us a similar the-oretical foundation for clustering vertices as in the standard spectral graph theory.
The assumption that all the items are anchor items is too strict. We usually have some item profiles which are combi-nations of some other item profiles. For example, there are pure  X  X ciFi X  books and pure  X  X omance X  books that attract distinct sets of readers (the anchor items). But one book might have the  X  X ciFi X  element and the  X  X omance X  element at the same time, so it could attract readers from both sets. The item profile of this book is apparently a combination of two profiles.

Besides the anchor items, we now consider the non-anchor items that are combinations of anchor items. We assume that the weights of the combinations are always non-negative, which implies that only additions of profiles are allowed, but not subtractions. If there are n non-anchor items and s clusters of anchor items,  X  X canberewrittenas  X  X =[  X  x 1  X  X  X  X  X  X   X  x n where the profile of non-anchor item z k is denoted as  X  ( k  X  X  1 , 2 , ..., n } ), and the profile of an anchor item from cluster C j is still denoted as  X  x j .Notethat  X  x k is not an exact linear combination of the anchor items, because when combining the profiles of the anchor items, there might be redundant entries. For example, if  X  x 1 = [1100] ,  X  x [0110] and  X  x k is a combination of  X  x 1 and  X  x 2 ,  X  be[1110] insteadof[1210] because the incident matrix X is always a binary matrix (normalizations are ignored in this example for simplicity). Thus we need to subtract the redundant entries when summing the anchor item profiles: where I ( z k ) is the set of anchor items from which z k structed, and r denotes the column vector that consists of the redundant entries in the profiles of the anchor items. Obviously all entries of r are non-negative.

Denote the latent vector of an anchor item from cluster C as  X  j , and the latent vector of a non-anchor item z k as  X  By the SVD on  X  X ,  X  k can be expressed by the combination If the entries in r are small enough to be considered as resid-uals, or in other words the anchor items are associated with disjoint sets of users, we could still expect approximately or-thogonal structure between the latent vectors of the anchor items.

Proposition 2. If the anchor items are associated with disjoint sets of users, i.e.  X  x j  X  x j =0 for  X  j, j  X  X  j = j , the latent vectors of the anchor items are approxi-mately orthogonal  X  j  X  j  X  0 for  X  j = j .

The conclusion of Proposition 2 follows from the fact that  X  x j  X  x j =  X  j S 2 l  X  j =0,andthefirst l singular values in S l are approximately the same when the cluster sizes are similar.

In practice, we can consider the anchor items as the most representative items of each cluster. For example, the pure  X  X ciFi X  books are the anchor items of the  X  X ciFi X  cluster, be-cause they have the most unique features that define what is  X  X ciFi X . The anchor items are not necessarily the most popular items or the most informative items in each cluster, but the items that are most distant from all the other clus-ters and attract a very unique group of users. Usually the anchor items are in the long-tail part, i.e. less popular.
Although the anchor items exhibit the approximately or-thogonal structure, we do not know which items are the an-chor items. The following result suggests that it is possible to distinguish the anchor items and the non-anchor items by examining the length of the latent vectors.

Proposition 3. For a non-anchor item z k ,denote I ( z k ) the set of anchor items from which z k is constructed. Sup-pose that the anchor items in I ( z k ) are non-overlapping, i.e.  X  x j  X  x j =0 for  X  j, j  X  I ( z k ) ,j = j , and the cluster sizes of the anchor items are similar, i.e. the lengths of the an-chor latent vectors are similar (denoted as  X  j 2  X  b 0 for  X  j  X  I ( z k ) ,where  X  2 is the l 2 norm). We have  X  k 2 (See appendix for proof.)
This result states that the length of a non-anchor latent vector is smaller than the lengths of the anchor latent vectors from which the former is constructed. Therefore, we can find the anchor items by sorting the lengths of the latent vectors in descending order.

In order to verify the conclusions in Propositions 2 and 3, we compute the latent vectors by the rank-l truncated SVD of  X  X ,andselect10  X  l items whose latent vectors have the longest lengths. Then the selected latent vectors are grouped into clusters by the k-means algorithm. By the conclusions of the Propositions, the cluster centers of the selected latent vectors should be approximately orthogonal. In Figure 4, we show the results from a real dataset with NHC and SVD. Because there is no hyperedge-degree and vertex-degree normalizations in SVD, the longest latent vec-tors are not necessarily the anchor items. We can observe that there exists an approximately orthogonal structure in the NHC results, while such structure is less clear with SVD.
There are existing works about the categorization of the items or users based on their impact on the recommender system, e.g. [12]. Our classification of anchor and non-anchor items provides a new perspective in the context of latent space. In summary, (1) if the latent vectors are all orthogonal, each corresponds to a different anchor item (the full concentrated case). (2) if they are not, there is an or-thogonal subset such that each latent vector in this sub-set corresponds to a different anchor item, and all the oth-ers (non-anchor items) are approximately in the subspace spanned by the orthogonal subset. (3) generally, the anchor items are rated by different groups of users.
 Figure 4: The selected anchor items with longer la-tent vector lengths in the dataset BookCrossing (see Section 6 for the description of the dataset). (a) The anchor items are shown in red x-markers. (b) The cosine distance matrix of the clustered anchor items. Sc is the sum of average distances between clusters minus the sum of average distances within clusters, normalized by the largest possible value with a per-fect orthogonal structure. A Sc value close to 1 sug-gests a better orthogonal structure.
We now present a new algorithm based on the NHC Lapla-cian and the above analysis. Once the latent vectors are computed for the items, we model the user y i  X  X  interests (or latent profile) as another vector  X  i inthesamelatentspace. Thepredictedscoreforuser y i on item z k is P ( y i ,z k )=  X   X  k , and the vector  X  i should take the values such that the predictor P coincides with the existing ratings (or as close as possible).

In order to simulate the scenario where the recommenda-tions are made for new users and evaluate the algorithm, we split the rating matrix R into three parts Firstly the users are separated into two subsets. Y m is the subset of old users, and Y t is the subset of new users. The sub-matrix R m contains all the ratings from the users in and the ratings in R m are used for constructing the hyper-graph and computing the latent vectors. The ratings from the users in Y t are further split into two parts: R tr and R tr contains the  X  X nown X  ratings of the new users. Based on R tr and the latent vectors, we can predict or recommend more items for the users in Y t . Then the predictions are evaluated by the ratings in R te . Note that only R m and R tr are taken as the inputs of the algorithm. R te is used for evaluation.

Since our algorithm works with the hypergraph transfor-mation and SVD, we call it HSVD , which contains the follow-ing steps: Step 1 : Compute the l -dimensional NHC transformation F by the truncated SVD  X  X m  X  U l S l F ,where  X  X m is computed from the binary matrix X m =( R m &gt; 0) (see Eq. (2)). Normalize each of the l columns of F to 1 ( l 2 norm).
Step 2 :Foreachuser y i  X  Y t , we choose the  X  i that min-imizes the error R tr ( i,  X  )  X   X  i F 2 2 . The predicted score vector is  X  r i =  X  i F . In matrix form, the full prediction matrix is  X  R te =  X  F ,where  X  is obtained by solving the linear system F  X  = R tr in a least-squares sense.
Step 3 :Foreachuser y i  X  Y t , recommend the top-N unrated items with the highest scores in  X  R te .
There are several normalizations in the HSVD algorithm which did not exist in previous matrix factorization meth-ods. When computing  X  X m in step 1, the hyperedge-degree normalization (from the induced graph transformation it-self) and the vertex-degree normalization (from the normal-ized Laplacian of the induced graph) ensure that the anchor latent vectors would have longer lengths. By Proposition 1, the normalization of the columns of F in step 1 is also indispensable for producing an (approximately) orthogonal structure in the latent space.

When computing the  X  i in step 2, two parts are actually taken into consideration, which is illustrated in Figure 5. Firstly we consider the latent vectors of the items which are rated by user y i (the triangle points in Figure 5). We would like to choose a  X  i that is close to these rated  X  k  X  X . Since the row vector R ( i,  X  ) is weighted by the ratings,  X  i be even closer to the  X  k  X  X  with higher ratings, which can be Because every non-anchor item is approximately a linear combination of some anchor items (e.g.  X  3 is a combina-tion of  X  1 and  X  5 in Figure 5),  X  i should be in the sub-space spanned by the latent vectors of the underlying an-chor items. Secondly, for those  X  k  X  X  that are not rated by user y i (the circle points in Figure 5),  X  i should stay as far from them as possible, ideally orthogonal to these un-rated latent vectors. The orthogonality between the clus-ter centers of the anchor items suggests that it is possible to find a  X  i which is orthogonal to the anchor items that the user y i is not interested in. This can be expressed as  X  together, it suggests that in matrix form the  X  i  X  X  can be obtained by solving the linear system F  X  = R tr in a least-squares sense, which is in the step 2 of the HSVD algorithm. Figure 5: Illustration of the HSVD algorithm in 3d latent space. The dashed lines are cluster centers of the anchor items.
Proposition 3 shows that the lengths of the anchor latent vectors are longer than the non-anchor latent vectors, even if the anchor items are less popular. For a fixed  X  i ,becausethe score  X  R te ( i, k ) is simply the inner product of  X  i and anchor items of longer length would have bigger chance to be selected in the top-N recommendations. This would allow us to diversify the recommendation list and recommend more less-popular items.
The main computational cost of our algorithm comes from the truncated SVD routine. Usually the numerical method for truncated SVD on matrix  X  X is in an iterative manner, and operated as an eigen-decomposition of the Hermitian matrix  X  X  X  X . The running time of the latter problem de-pends on the complexity of each iteration and the total num-ber of iterations. In each iteration the basic operation con-tains two matrix vector multiplications  X  X  X  Xv where v dense vector. If the number of non-zero entries in  X  X (or the number of ratings in R )is M , the complexity of each itera-tion would be O ( M ). On the other hand, the total number of iterations to converge depends on the gap between the l largest singular value and the l + 1 largest singular value. A bigger gap leads to a smaller number of iterations [6, 11]. If the number of iterations is n I , the overall complexity of the HSVD algorithm is O ( n I M ), which is linear to the number of ratings. The parameter l (dimensionality of the truncated SVD) is considered as a constant, which can be determined by cross-validation in practice. In the introduction, we have mentioned the SVD [17], the SVD on non-empty entries [5], and the asymmetric SVD [14, 9] for recommender systems. Another matrix factorization technique that is widely used in recommender systems is the non-negative matrix factorization (NMF). In NMF, the rating matrix is approximately decomposed into two low-rank, sparse and non-negative matrices R  X  GH such that R  X  GH 2 is minimized [7, 16]. The idea behind NMF is that the set of orthogonal latent factors controls the values in R , and each latent factor takes one axis in the latent space. This idea is essentially similar to the HSVD since we have shown that the anchor items also exhibit an orthogonal structure in the latent space, although they are not aligned to the axes.

In order to compare our algorithm with the existing ap-proaches, we adapt the SVD and the NMF into our setting, i.e. split the rating matrix into three parts as in Eq. (7) and use the sub-matrix R = R m R tr to compute the SVD or NMF, then evaluate the results on R te . The SVD and NMF algorithms in this setting are denoted as SVDR and NMFR . To make a better baseline, we also apply the SVDR and NMFR on R m and predict with R tr in an asymmetric manner, i.e. follow the same procedures in Section 4 but replace  X  X with R m (or replace F with the matrix H in NMF). These methods are denoted as ASVDR and ANMFR , where the leading  X  A  X  means  X  X symmetric X .

We use the routine implemented by TimelyDevelopment for computing the SVD on non-empty entries [18], which is denoted as SVDN .Notethat SVDN is exactly the same as SVDR except for the SVD computation routine.

For the asymmetric SVD, we use the approach proposed in [14]. In this approach, a decomposition is first computed by SVDN R  X  U l S l V l ,where V l =[  X  1  X  X  X   X  n ] .Thena user y i is represented by the currently rated items:  X  i  X  R ( i, k )= a k +  X  i  X  k for each unrated item, where a k is the average rating of item z k . We call this method ASVDN .
In a top-N recommender system, the primary goal is to discover the unrated items that might be liked by a user [3]. We first test the algorithm X  X  ability to discover the unrated items, which is denoted as the scenario PREDICT-ALL .The rating matrix is split into three parts as in Eq. (7), and we randomly select R tr to have 5 ratings in each row (for each user). The remaining ratings in the Y t part are assigned to R te . This setting simulates the situation where new users just come into the system with only a few ratings, which is denoted as PREDICT-ON-5 . Similarly, we create another setting PREDICT-ON-20 where each row of R tr has 20 ratings to simulate the users with more available ratings. Thus in the scenario PREDICT-ALL , there are two settings, namely PREDICT-ON-5 and PREDICT-ON-20 . Notethatalltheitems in R te (rated or unrated) could be recommended in the top-N recommendation list. In all the experiments, R tr and R te are randomly selected in 5 different runs.

To test the results, we check if the set of recommended items coincides with the liked items (ratings higher than the median) for each user in the corresponding row of R te .Let  X  R i and R i denote these two sets for user y i .Wemeasure the average precision Table 1: Average precisions in PREDICT-ALL .Theleft column of each dataset: PREDICT-ON-5 , the right col-umn: PREDICT-ON-20 . The bold number indicates the method that performs significantly better than the others ( p -value &lt; 0 . 05 in paired t-test).
 Table 2: Average precisions in RANK-CANDIDATES .(see the caption of Table 1 for more details)
In the scenario PREDICT-ALL , any item which is not in R tr can be considered in the recommendation list. But some-times we already know a set of candidate items that a user has suggested implicitly. For example, a user has viewed a list of pages of artists, but did not make any further ac-tions like purchase a CD or download a track. In this case, we can limit our recommendations on the candidate items and only pick up the items that are truly liked by the user, which could be simulated by only recommending the rated items in R te . We try to rank these items such that the liked ones are ranked higher. This scenario is denoted as RANK-CANDIDATES . The same measure (precision defined in Eq. (8)) is evaluated in RANK-CANDIDATES .

We have tested all the methods listed in Section 5 with the above settings on three datasets. Lastfm 1 contains the relations between users and artists collected from last.fm , and the ratings in Lastfm are the actual counts of plays with which a user has listened to an artist. YahooMusic 2 is also in the music domain, but includes relations between users and tracks. The ratings in YahooMusic are in the range of 1 to 100. The last dataset BookCrossing contains ratings from users to books [23]. The range of ratings in BookCrossing is from 1 to 10. In each dataset, we take a subset such that the induced graph of the hypergraph representation is connected. When the dataset contains a hypergraph with multiple connected components, we could simply apply the algorithm to each connected component respectively. Figure 6: The distribution of popularity of the items (in the PREDICT-ALL scenario with N =20 ). The x-axis (popularity) is in log-scale. The mean and median of the popularity of the items are also shown in each sub-figure. Top: BookCrossing .Bottom: YahooMusic . Table 1 and Table 2 show the results of PREDICT-ALL and RANK-CANDIDATES respectively (with N = 20). In the first case, our proposed method performs best within the mu-sic domain, and shows results as good as the best methods with BookCrossing . The approaches ignoring the empty values ( SVDN and ASVDN ) does not perform very well. These methods are originally designed to minimize the root mean squared error on a testing set of known ratings, which is not fully compatible with the scenario PREDICT-ALL .Inthe
Available at http://mtg.upf.edu/node/1671
Yahoo! Webscope dataset ydata-ymusic-kddcup-2011-track2 http://labs.yahoo.com/Academic_Relations RANK-CANDIDATES , no method performs significantly better than the others over all datasets. But the NMF-based method NMFR and two asymmetric methods ASVDN and HSVD are al-ways as good as the best one.

In recommender systems, we are mainly interested in rec-ommending non-popular (long-tail) items, because the users are usually already aware of the popular items from other sources [19]. For the purpose of diversifying recommenda-tions, we measure the popularity distribution of the items in  X  R N i and  X  R N i  X  R i , i.e. the set of all recommended items and the set of successfully recommended items (Figure 6). The popularity of an item z k is the number of ratings in the k -th column of the rating matrix, i.e. deg( z k ). We observe that HSVD produces more diverse recommendations with the same (or better) level of precision. Especially with BookCrossing , there are much more less-popular recommen-dations from HSVD compared to SVDR and NMFR .

Figure 7 explains why our approach produces more di-verse recommendations. By using SVD without normaliza-tion, the lengths of the anchor items are not necessarily the longest ones. In fact, we can observe that there is a strong correlation between the popularity and the length of the latent vector with ASVDR , which would promote the popu-lar items in the recommendations. In our approach HSVD , the longest latent vectors correspond to the anchor items of smaller popularity. Figure 7: The length of the latent vectors and the popularity of all the items in BookCrossing .
In this work, we present a new perspective of SVD for constructing a latent space from the training data, which is justified by the transformation of normalized hypergraph cut. We show that the latent vectors representing the items in the latent space can be grouped into (approximately) or-thogonal clusters, and the lengths of the vectors can be used to distinguish the anchor items and the non-anchor items. These properties are then utilized for making top-N recom-mendations in the proposed algorithm HSVD . Instead of ex-plicitly constructing the clusters, the recommendations are generated by solving a linear system. We provide new expla-nations for the significantly better performance of the asym-metric SVD approaches and better diversity in top-N rec-ommendations. Experiments conducted with three dataset on two domains also confirm our analysis.

Future works could include further studies of the latent space structures, and engineering methods that exploit such structures. For example, we can explicitly extract the anchor items to construct some clusters, and analyze the features of a non-anchor item by studying the underlying anchor clus-ters. It is also possible to include additional information, e.g. contextual information [2] and social information [21], or rating bias corrections [10] into our scheme for further improvements. [1] S. Agarwal, K. Branson, and S. Belongie. Higher order [2] L. Baltrunas, B. Ludwig, and F. Ricci. Matrix [3] P.G.Campos,F.D  X   X ez, and M. S  X  anchez-Monta  X  n  X es. [4] F. Chung. Spectral Graph Theory .American [5] S. Funk. Netflix update: Try this at home, http:// [6] G. H. Golub and C. F. Van Loan. Matrix [7] P. Hoyer. Non-negative matrix factorization with [8] G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. [9] Y. Koren. Factorization meets the neighborhood: a [10] Y. Koren, R. Bell, and C. Volinsky. Matrix [11] D. Mavroeidis. Mind the eigen-gap, or how to [12] B. K. Mohan, B. J. Keller, and N. Ramakrishnan. [13] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral [14] A. Paterek. Improving regularized singular value [15] L. Pu and B. Faltings. Hypergraph learning with [16] S. Rendle and L. Schmidt-Thieme. Online-updating [17] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [18] TimelyDevelopment. http://www.timelydevelopment [19] S. Vargas and P. Castells. Rank and relevance in [20] L.Wu,X.Ying,X.Wu,andZ.Zhou.Line [21] Q. Yuan, L. Chen, and S. Zhao. Factorization vs. [22] D. Zhou, J. Huang, and B. Scholkopf. Learning with [2 3] C. Ziegler, S. McNee, J. Konstan, and G. Lausen. Proof for Proposition 1 :
Firstly, it can be shown that  X  k =  X  k if z k ,z k  X  C j because the k -thcolumnandthe k -th column of S s F must be the same to obtain the same  X  x k and  X  x k ,which implies that  X  k =  X  k . Secondly, we consider the full SVD of  X  X and list the rows of V corresponding to items in C It is always possible to find a linear combination of the first s columns of V (denoted as [ f 1 , f 2 , ..., f s ]= F ) such that Ft j = 0  X  X  X  1  X  X  X  0 ,where t j are the coefficients, and the 1 X  X  on the right hand side correspond to the items in C . Because all the columns in V are orthogonal to each other, a column is also orthogonal to a linear combination of some other columns (e.g. Ft j ). Thus the entries in each Since all the rows in V are also orthogonal to each other, we have V ( j ) 1 V ( j ) 1 = 0, which implies that  X  j  X  j =0. Proof for Proposition 3 :
By Eq. (6) we know that  X  k = j  X  I ( z residual r is zero because the involved anchor items are non-overlapping). Thus  X  k 2 2 = j  X  I ( z items are non-overlapping, we have j  X  I ( z The above equation can be written as Following the proof for Proposition 1, it can be shown that  X   X  where I  X  1 ( j ) is the set of non-anchor items that contain a fraction of an anchor item in cluster C j .Sinceeach  X  p is a linear combination of the  X  j  X  X  of the anchor items, and we know from Proposition 2 that  X  j  X  j  X  0for  X  j = j ,itis easy to show that  X  j  X  j &lt; 0. Applying this result to Eq. (10), it concludes to  X  k 2 &lt;b 0 .
