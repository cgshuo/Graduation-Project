 Compression is widely exploited in retrieval systems, such as search engines and text databases, to lower both retrieval costs and system latency. In particular, compression of repositories can reduce storage requirements and fetch times, while improving caching. One of the most effective tech-niques is relative Lempel-Ziv, RLZ, in which a RAM-resident dictionary encodes the collection. With RLZ, a specified document can be decoded independently and extremely fast, while maintaining a high compression ratio. For terabyte-scale collections, this dictionary need only be a fraction of a per cent of the original data size. However, as originally described, RLZ uses a static dictionary, against which encod-ing of new data may be inefficient. An obvious alternative is to generate a new dictionary solely from the new data. However, this approach may not be scalable because the combined RAM-resident dictionary will grow in proportion to the collection.

In this paper, we describe effective techniques for extend-ing the original dictionary to manage new data. With these techniques, a new auxiliary dictionary, relatively limited in size, is created by interrogating the original dictionary with the new data. Then, to compress this new data, we com-bine the auxiliary dictionary with some parts of the original dictionary (the latter in fact encoded as pointers into that original dictionary) to form a second dictionary. Our results show that excellent compression is available with only small auxiliary dictionaries, so that RLZ can feasibly transmit and store large, growing collections.
 E.4 [ Coding and Information Theory ]: [Data compaction and compression]; H.3.3 [ Information Storage and Re-trieval ]: Information Search and Retrieval X  Search process Repository compression; dictionary representation; encod-ing; document retrieval
Compression is a key component of large-scale informa-tion retrieval systems, such as web search engines [1, 4, 8, 21, 36]. It offers improved utilization of disk, efficient index processing, and, overall, an order-of-magnitude increase in volume of data that a single machine can handle. Compres-sion of stored documents helps to improve caching in a single server [34] and to reduce the cost of replicating or trans-mitting collections between geographically separated data centers [11].

Compression of large text collections has been extensively studied for decades, but new approaches continue to ap-pear [5, 9, 13, 15, 26]. Amongst these, relative Lempel-Ziv (RLZ) [15] is one of the most effective compression tech-niques for large repositories. RLZ offers both good com-pression effectiveness and extremely fast atomic retrieval of individual documents. It has the particular advantage that it assumes only that the material being stored contains re-dundancy in the form of repeated strings: there is no re-quirement, for example, that the collection consist of text in a certain language. As we describe later, RLZ exploits a RAM-resident dictionary that consists of a single long string, against which efficient encoding and decoding of a collection canbeperformed. Incontrasttoadhocmethodsbased on zlib 1 and similar libraries, only the document of inter-est need be retrieved and decoded; in addition, decoding is dramatically faster.

Existing methods for repository compression often assume that the collection is static [9, 26]. However, in many situa-tions, the data collection is dynamic; in this paper, we focus on the scenario of a growing data collection. We assume that the collection grows in large increments, or tranches ,eachof which is around a gigabyte, or perhaps even a terabyte. Al-though the new data may arrive in smaller increments, the retrieval system formally adds it to the collection (in com-pressed form), or transmits it, only when the threshold for a the tranche is reached. We refer to this tranche-by-tranche expanding collection as an incremental collection , and we re-fer to the problem of compressing an incremental collection as incremental compression .

In this paper, we consider the following two common sce-narios, each well served by incremental compression.
Local archiving: A single server stores an incremental col-zlib.net Figure 1: Two incremental corpus compression scenarios.
Remote transmission: A primary server replicates its col-
In each of these scenarios, we desire fast per-document retrieval as well as a good compression ratio, and thus RLZ is a suitable choice. With the adoption of RLZ, however, the goals for these scenarios are slightly modified. When updating , we additionally wish to avoid excess memory con-sumption, and thus should limit the size of the dictionary. When transmitting , we additionally wish to avoid excess data transmission, and thus should represent the dictionary (for the new tranche), as well as the tranche itself, as com-pactly as possible.

In this paper, we propose that, to compress a new tranche, the dictionary for existing tranches be exploited selectively. As we discuss in the next section, considered alone, this ex-isting dictionary may not compress a subsequent tranche well. However, a wholly new dictionary (or sequence of wholly new dictionaries) may require too much space. We therefore propose methods for creating a small auxiliary dictionary for each new tranche, based on the dictionary built for previous tranches. Our results show that excel-lent compression can be maintained with relatively limited additional space requirements. We also describe a highly efficient method X  X ndeed, an optimal method, under rea-sonable assumptions X  X or compactly describing which parts of the existing dictionary are used in compressing the new tranche.

In summary, we show in this paper (i) that incremental compression in which previous tranches assist in compress-ing new tranches is not only possible, but also feasible; (ii) a principled method for compactly describing a subset of the fragments of a large file; and (iii) through our experimen-tal results, that our new method informs better dictionary construction principles for the original RLZ algorithm. To-gether, these outcomes significantly extend the value and application of RLZ for practical compression of large collec-tions.
 Figure 2: The RLZ scheme: concatenation, sampling, and factorization.
In this section we review RLZ, previous work on pruning of RLZ dictionaries, and related work on compression of (incremental) collections.
RLZ [15] is an effective dictionary-based compression al-gorithm for large collections (such as of web documents or genomes) where there are discoverable long repeated strings. In brief, the mechanism of RLZ is that a text dictionary (a continuous long string) is generated from the collection, whichisthenusedtoparse( factorize ) the collection text into a sequence of substrings, each of which can be found in the dictionary. String matching is the key component of such relative Lempel-Ziv factorization [19], and can be accelerated by ancillary data structures, such as the suffix array [20].

Figure 2 illustrates the principles of RLZ. Consider a text collection comprising three documents, D 1 , D 2 ,and D 3 The documents in the collection are concatenated into a long string [step 1 ], which is sampled at regular intervals to form an external, but RAM-resident, dictionary [step 2 ]. Each document is then factorized against the dictionary into a se-quence of fragments [step 3 ], each of which is represented as a (position, length) factor , a pointer into the dictionary. The factors of each document are then encoded using standard byte-oriented codes. In this paper, following the settings in previous work [31], ZLIB encodes positions and VBYTE [32] encodes lengths.

RLZ provides excellent compression because random sam-pling is an effective way of collecting repeated strings. If a string is common, it is likely to be represented in the dictio-nary. (It is also likely to be present more than once in the dictionary, an issue that we examine under the topic of prun-ing, below.) The dictionary thus provides a highly effective resource against which the collection can be compressed.
RLZ provides fast random access to individual documents for two reasons. First, the dictionary is static (in contrast to the adaptive dictionaries used in the classic Lempel-Ziv (LZ) family [35]), and thus exists as a simple array of bytes. In current computer architectures, decoding a factor X  X hich may contain hundreds of bytes X  X equires only a single copy instruction, and is thus extremely efficient. Second, during encoding, document boundaries are respected, so that the end of a document is always the end of a factor, and thus each individual document is a separate stored entity. Figure 3: Redundancy between existing dictionary D 1 and new dictionary D 2 . For clarity, factors whose length ex-ceeds 1024 are omitted (they contribute only  X  0.77 MB of the 1 GB).

In this paper, we use the notation F ( C , D )todenotethe sequence of factors produced by factorizing a collection C against a dictionary D . The compressed size of F ( C , D represented as |F ( C , D ) | c ,while R ( C , D ) denotes the corre-sponding compression ratio of RLZ: the ratio of |F ( C , D to the (uncompressed) collection size |C| .
 perform well on incremental collections [15], where the initial dictionary is generated from the first tranche of data. How-ever, compared to a dictionary generated from the whole collection, we found in preliminary experiments that this compression effectiveness can degrade substantially.
We should therefore generate a dictionary from the data in the new tranche. An obvious solution is to sample a dictionary solely from this tranche. However, our prelim-inary investigations revealed that this new dictionary and the existing dictionary have much in common; ignoring the existing dictionary completely is therefore wasteful.
To illustrate this, the initial 50 GB of the GOV2 dataset [7] serves as the first tranche, with the next 50 GB of GOV2 serving as the second tranche. (In this paper, a gigabyte (GB) is 2 30 bytes, with KB and MB similarly defined.) From each tranche, we generate a 1-GB RLZ dictionary: these are called D 1 and D 2 . Finally, to identify the overlap be-tween D 1 and D 2 , we determine F ( D 2 , D 1 ). Figure 3 shows the cumulative coverage of D 2 by common factors, as a func-tion of factor length. Large factors indicate  X  X edundancy X : these sections of D 2 are covered well by D 1 . Figure 3 shows, for example, that 40 % of D 2 is covered by strings in D 1 (factors) whose length is at least 128 bytes.

We conclude that D 1 is helpful for compressing the new tranche and that we need only a small auxiliary dictionary specific to the new tranche, provided it has little overlap with D 1 . These observations lead to the new methods in this paper.
 Dictionary pruning. The original RLZ study [15] reported that, in many situations, a significant portion of dictionary is never referred to. We infer that the original scheme pro-duces dictionaries with significant redundancy. Subsequent studies [16, 31] investigated pruning of the RLZ dictionary to reduce redundancy in the sampled dictionary, and there-fore provide better compression. Hoobin et al. [16] proposed removing the blocks X  X ixed-size chunks, each say 1 KB X  that are least frequently referred to. Tong et al. [31] sub-sequently introduced a pruning algorithm (called CARE) that selects candidate prunable segments from the dictio-nary and removes those segments that their heuristic sug-gests contribute least to compression effectiveness. Due to its demonstrated superiority, we implement the CARE algo-rithm for pruning in our experiments.
Compression techniques have been adopted in multiple basic components of information retrieval systems [9, 33]. Applications include compressed textual web documents [9], compressed full-text indexes [24], compressed inverted in-dexes [37], and compressed permuterm indexes [10]. Our concern here is with compression of the stored documents, for which RLZ is one of the most effective techniques. String substitution. Common-string substitution is a typ-ical method used to compress text collections [28]. To en-code large repositories, long repeated strings are identified and then delta encoding is applied [26]. As a practical in-dustrial example, Google adopted such a technique [2] for handling of long repeated strings in the collection data of their Bigtable system [6]. RLZ can also be regarded as an application of string substitution, though the substitutions refer to an external dictionary rather than to previous parts of the collection itself.
 Versioned collections. Versioned collections X  X napshots of Wikipedia at different times, for example, comprising multiple versions of each document X  X an be compressed ef-fectively due to inter-version redundancy [13, 14]. In partic-ular, delta compression, the core idea in which is to encode one file in terms of another, performs well [17, 18, 25]. In-cremental corpus compression, as described in this paper, is a different problem. A new tranche does not necessarily comprise modified versions of material already in the collec-tion. Although delta compression can also be applied to non-versioned collections, the inter-file dependencies inevitably slow document decompression and retrieval significantly. pression has been studied in a range of other contexts. Mof-fat et al. [23] applied a word-based compression scheme to semi-static compression. They proposed methods to help the model to handle new words that occur in the updates to the collection. With a model derived from a relatively small sample of the text corpus, their scheme can effec-tively compress a large text database. Brisaboa et al. [3] also described adaptive vocabulary schemes for incremental collections. However, in common with other schemes based on parsing of text into words and non-words, compression performance is relatively poor, decoding is not fast, and ap-plication is limited to specific kinds of data.

Hoobin et al. [15] argued that RLZ also works well in dynamic environments. The reported performance is rea-sonable, but, as we show in this paper, substantial improve-ments are available if alterations to the dictionary are al-lowed. Remote replication. In the context of transmission of large repositories to remote servers, which is similar to our trans-mitting scenario, existing schemes usually incorporate de-duplication. They then use delta encoding to reduce the volume of transferred data [27, 29, 30]. As for versioned collections, de-duplication sets up inter-file dependencies, which makes atomic document retrieval much slower than it is in RLZ.
In this section, we introduce a new algorithm for the up-dating scenario (Figure 1(a)). In the context of RLZ com-pression, the optimization problem is:
In the rest of the section, tranche C 2 iscompressedwitha dictionary comprising auxiliary dictionary D  X  concatenated with D 1 . Combined dictionaries of this form will be denoted by D  X  : Table A.1 is a glossary for this section.
Because the size of the ancillary data structure for string search, for example a suffix array, is proportional to the dictionary size, we ignore this constant-factor overhead and focus solely on dictionary size when consider budget B .
The easiest way to generate an auxiliary dictionary, D  X  , for the new tranche, C 2 , would be to sample only from C The sampling technique is the same as in the standard RLZ scheme (Figure 2). We regard this new tranche-only dictio-nary, D s 2 , as a baseline for our study. Figure 3 suggests that this D s 2 would have much in common with D 1 .Toderivean auxiliary dictionary D  X  from D s 2 with  X  X resh X  material, we therefore consult D 1 .
 to sample a large dictionary D from tranche C 2 (say of sim-ilar size to D 1 ) and remove the long strings that it shares with D 1 . Obviously, we cannot remove all the redundancy between the dictionaries, because even a singleton byte in Algorithm 1 CuD approach for generating D  X  .
 Input: Tranche C 2 , Existing dictionary D 1 , Threshold  X  Output: Auxiliary dictionary D  X  for C 2 of size  X  B  X  X D 1 1: F X  Factorize ( C 2 , D 1 ) 2: F  X   X  X  f | f  X  X  and | f | X   X  } 3: F  X   X  factors in F  X  that are adjacent in C 2 to other 4: C L 2  X  concatenationoffactorsin F  X  5: return D  X   X  Sample ( C L 2 ,B  X  X D 1 | ) is a (redundant) copy of the same symbol in D 1 .Thus we only consider removing common substrings longer than some threshold  X  ; we find that the average factor length in
F ( D , D 1 ) is a good initial choice for  X  . The remaining material in D could serve as D  X  , and thus its size may be controlled by the choice of  X  . In preliminary experiments, we found that such D  X  leads to a minor improvement in com-pression over the commensurate baseline dictionary ( D s 2 Since our subsequent innovations perform considerably bet-ter than this  X  X uD X  approach, we save space and omit these preliminary results.
 performs poorly because its D is sampled uniformly from C It therefore fails to capture sufficiently those segments of that D 1 struggles to encode, and leads to many small fac-tors. Matching our intuition, experiments confirm that the presence of a large number of small factors in a factorization indicates poor compression. Our goal is thus to create a D that leads to fewer (longer) factors. We therefore identify more carefully the parts of C 2 that D 1 encodes poorly: that is, those areas with many small factors. From F ( C 2 , D we concatenate (runs of) the short factors (whose length is below threshold  X  )toform C L 2 . Algorithm 1 (CuD) X  X  final step is to sample C L 2 to derive the auxiliary dictionary The CuD functions Factorize () in line 1 and Sample () in line 5 refer to the synonymous RLZ procedures (Figure 2).
Short factors in F ( C 2 , D 1 ) indicate segments of C 2 are poorly compressed by D 1 . However, were we to add an isolated short factor to our new dictionary (one between two long factors), this would not really assist in compression as it would remain a short factor. Instead, as detailed in line 3 of Algorithm 1, we include only runs of at least two adjacent (b) The fraction of D 1 that comprises targets of the factors of F ( C 2 , D 1 )oflength at least  X  .
 (Note that C L 2 here is exactly the same as C 2 ,because Figure 5: Redundancy between existing dictionary D 1 and new dictionary D 2 sampled from C L 2 . For clarity, factors whose length exceeds 100 are omitted (they contribute only  X  26.8KBofthe1GB). short factors in the new dictionary. Concatenated, these constitute helpful longer strings in the factorization of against the new combined dictionary.

Figure 4 illustrates some properties of the relationship be-tween C 2 and D 1 as a function of  X  on the Wikipedia dataset (a 251-GB English Wikipedia snapshot extracted from the ClueWeb09 dataset 2 ). As shown in Figure 4(a), even when allowing factors up to twice the average size to be candi-dates, less than 37 % of the content in C 2 is considered as a sampling source for D  X  . Referring to Figure 4(b), with a threshold  X  that is twice the average factor length, only 61 % of D 1 is involved in compressing the 63 % of C 2 that is compressed  X  X ell X . The well-compressed parts of C 2 are those with long factors, those that are not picked to form F , and hence C L 2 .

To illustrate the effectiveness of our approach, we con-sider the factor lengths of a CuD-like dictionary, factorized lemurproject.org/clueweb09 Table 3: Results of compressing against the 1000-MB static initial dictionary on 10-GB tranches from the initial 50 GB of GOV2, for the updating scenario.
 R ( C 2 , D 1 )( % ) 12.38 16.57 17.97 18.57 18.97 against the original dictionary D 1 . The cumulative distri-bution in Figure 5 is derived from the same tranches as in Figure 3. Here, however, we sample a 1-GB dictionary D 2 from the 22.1-GB C L 2 that emerged from the CuD algorithm. (In this analysis, we need not restrict the dictionary to the smaller size expected of an auxiliary dictionary.)
Figure 5 shows that the redundancy between D 2 and D 1 is dramatically less than that between D 2 and D 1 in Figure 3. Indeed, there are no long factors, and so we have successfully captured more patterns from the poorly compressed portions of
C 2 ,onwhich D 1 and D 2 fail.
We first show, on a relatively small dataset, that in prac-tice our CuD method performs better than the C 2 -only base-line. We divide the initial 50 GB of the GOV2 collection into five 10 GB tranches; the first tranche serves as the ini-tial collection (Round 0), and there are update Rounds 1, 2, 3, and 4.

With CuD, dictionary D  X   X  comprises a small D  X  2 ,gener-ated by CuD algorithm, together with D 1 . For comparison, our baseline is D s  X  which is a small dictionary D s 2 sampled directly from C 2 , combined with D 1 .Wereportthecom-pression ratio for each tranche, R ( C 2 , D  X   X  ), and also for the whole collection in each round, R total . In every multi-round experiment, C 1 , D 1 , and so forth are updated foreachnew tranche. For example, when considering CuD, D  X   X  in round becomes D 1 in round i + 1. Results for the CuD approach are in Table 1 and those for the baseline are in Table 2. The The settings of threshold  X  are the same as in Table 1. (Note that C L 2 here is exactly the same as C 2 ,because threshold  X  in CuD is set to twice the average factor length of F ( C 2 , D 1 ) (each tranche has its own  X  ).

The CuD method is more effective than the baseline in terms of both the compression ratio for each tranche and the compression of the whole collection. Moreover, compres-sion improves as the threshold increases; briefly, if  X  equals the average factor length, then the resulting R ( C 2 , D  X  Round 4 is 12.44 %, which drops to 12.18 % (result shown in Table 1) when  X  is doubled.

For comparison, in Table 3, we also show the results of using only the initial (static) dictionary D 1 to compress the incremental collections. These results confirm the need to tailor RLZ to the incremental corpus scenario.

To see how our scheme performs on a larger dataset, we di-vide the first 250 GB of the Wikipedia collection into five 50-GB tranches. Tables 4 and 5 together show an even greater win by CuD over the baseline. In particular, on the fifth tranche our compression ratio (7.781 %) is, relatively speak-ing, 11.9 % better than the baseline achieves (8.833 %).
To further demonstrate CuD X  X  quality, we consider an ad-versarial experiment in which a dictionary of size B is sam-pled directly and only from tranche C 2 . Surprisingly, the results on the Wikipedia tranches, as shown in Table 6, are inferior to the results of CuD in Table 4.

In addition, recent work of Tong et al. [31] shows that in general a pruned dictionary compresses more effectively than a commensurate sampled dictionary. Thus we consider one further baseline, quite similar to that of Table 5. Here, however, the auxiliary dictionary is  X  X ampled and pruned X  (with CARE) instead of just sampled. That is, for each up-date round, a 1000 MB D s 2 is sampled first from C 2 and is then pruned to a 250 MB D k 2 , which serves as the auxil-iary dictionary. As shown in Table 7, the resulting tranche compression ratios ( R ( C 2 , D k  X  )) are indeed significantly bet-ter than those in Table 5, though still inferior to CuD. In fact, should D  X  2 in Table 4 be generated from C L 2 first and then CARE-pruned, then even better performance could be achieved by CuD. To save space, we omit the details. Never-theless, we observe that the competing methods in Table 6 and in Table 7 have an unfair advantage over CuD, as they Table 6: Performance of compressing each tranche with a newly generated commensurate dictionary. D o 2 is completely generated from C 2 and ignores D 1 . The Wikipedia tranches are the same as in Tables 4 and 5.
 Table 7: Performance of compressing each tranche with an auxiliary dictionary that is a 1000 MB sample from C 2 ,and then pruned to D k 2 . The dataset used is the same as Table 5. Round violate the memory constraint: |D 1 | + |D  X  | X  B . Even then, CuD outperforms each of them!
Finally, there is some evidence that the CuD algorithm compresses the Wikipedia collection about as well as RLZ with a static dictionary from the whole collection. With a static dictionary, the compression ratio for all 251 GB with a2GBdictionaryis8.688%.CuDhowevercompressesthe initial 250 GB with an overall ratio of 8.501 %. Though the last 1 GB is missing from the latter experiment, this suggests CuD might inform better dictionary construction principles for the original RLZ algorithm. Furthermore, since CuD does not require recompression of previously compressed ma-terial, the retrieval speed of RLZ is unaffected.
We now turn to the second application of incremental compression, the transmitting scenario (Figure 1(b)). In some settings, incremental compression for remote trans-mission is quite similar to that for local archiving (Sec-tion 3). If the remote receiver has sufficient memory, that is B&gt;&gt; |D 1 | , then we can simply adopt the CuD approach. However, we might already have saturated the available memory, B = |D 1 | , and then even a tiny D  X  cannot be added to D 1 ; in this section, we offer a new approach. We still generate a compact auxiliary dictionary D  X  , but this time we do not combine it with all of D 1 . Rather, we retain only the  X  X seful X  parts of D 1 when processing C 2 . In this context, we formulate the problem as: Here, |D  X  | c and |D 1 | c are the sizes of compact represen-tations of D  X  and D 1 , respectively (there is a glossary in Table A.2).

To retrieve files quickly, uncompressed dictionaries D  X  and D 1 must reside in RAM. Hence the memory constraint cannot be |D 1 | c + |D  X  | c  X  B . During transmission, however, the dictionaries should be compressed. Again the size of the ancillary data structure for string search, for example a suf-fix array, is proportional to the dictionary size, so we ignore this constant-factor overhead and focus solely on dictionary size. Moreover, this ancillary structure can be derived from the dictionary, and need not be transmitted. To simplify the presentation, in the rest of this section, we assume that B = |D 1 | . The two obvious baselines for processing C 2 are (i) D 1 = D 1 with D  X  =  X  ,and(ii) D 1 =  X  with D  X  = D s 2 , where D s is sampled from C 2 . However, the results in the previous section show that such extreme choices are unlikely to yield optimal performance. That is, each of the original dictio-naries ( D 1 and D s 2 ) is valuable when compressing C 2 findings in previous work [31] that effective pruning helps to retain the most  X  X aluable X  parts of the dictionary, we in-troducetheCnPschemetogenerateboth D 1 and D  X  (Algo-rithm 2). First, a relatively large dictionary D s 2 is sampled from tranche C 2 and is concatenated to the existing dictio-nary D 1 to form a even larger dictionary D . To simplify the presentation in the rest of this section, we assume |D s 2 though of course it is the final dictionary, D 2 , that must fit in the bound B . We then apply a principled pruning method to reduce D to D 2 , whose size is B . In Algorithm 2, Prune () in line 3 corresponds to the CARE pruning approach [31]. Dictionary D 2 as a whole is the RAM-resident dictionary for the new tranche C 2 . However, to update efficiently the remote server with the new tranche, we must describe the dictionary in a compressed form, and must thus consider the Algorithm 2 The CnP approach for incremental compres-sion in the transmitting scenario.
 Input: Incremental collection C 2 , Existing dictionary D 1: D s 2  X  Sample ( C 2 ,B ) 2: D  X  X  1 + D s 2 3: D 2  X  Prune ( C 2 , D ,B ) 4: D p 1  X  X  1 -derived parts in D 2 6: F ( C 2 , D 2 )  X  Factorize ( C 2 , D 2 ) 7: Transmit compressed F ( C 2 , D 2 ) and compact represen-Figure 6: Obtaining D 1 and D  X  (denoted by D p 1 and D p the diagram). two constituent components of D 2 : D 1 (from D 1 )and D  X  (from D s 2 ), represented as D p 1 and D p 2 in Figure 6, respec-tively.
Suppose we have already derived a dictionary D 2 for tranche C 2 ; and suppose this D 2 contains some strings from the existing dictionary (denoted by D p 1 ) and some content drawn from the new tranche (denoted by D p 2 ). The remote server should construct D 2 from a description of each of and D p 2 .Bydesign, D p 2 ought to comprise material that the remote server does not yet have, and thus this material must be transmitted.

However, sending D p 1 in its entirety, even in a compressed form, is unnecessary. Given that the receiver already has a copy of D 1 , we save bandwidth by describing D p 1 in terms of the substrings of D 1 that it represents. (We assume here that D p 1 is concatenated with D p 2 , thence no information is required to describe the relative locations of these sub-dictionaries.)
Consider the example in Figure 6. Dictionary D p 1 is de-rived from three runs of bytes in D 1 ; this  X  X ncluded X  mate-rial is shown in gray. There are also three white runs of  X  X xcluded X  or pruned material. An obvious way to represent these is as a sequence of pairs: each pair consists of the po-sition of the start of each gray run together with its length. The positions and lengths then need to be encoded. Al-though in general a Golomb code is suitable for compressing a sequence of positions in an array of known size, in this case we have additional information (factor lengths) that could be incorporated into the compression, but are ignored.
An alternative is to regard the sequence of gray runs as a sequence of lengths, of known total length, and the se-quence of white runs as, likewise, a sequence of lengths of known total length. We assuming that whether each posi-Dictionary sizes are |D 1 | = |D s 2 | = 1000 MB. |D p 2 | the size of Golomb-encoded run-length information for recovering
D
D 2 Ratio ( % ) Transmit ) 12.45 0.48 0.020 12.97 ) 7.89 0.42 0.004 8.31 nary sizes are |D 1 | = |D s 2 | = 1000 MB. The meanings of and |D p 2 | c are the same as Table 8.

D 2 Ratio ( % ) Transmit ) 8.96 0.09 0.003 9.05 ) 4.99 0.05 0.001 5.04 ) 12.63 0.13 0.002 12.76 ) 12.86 0.14 0.002 13.00 ) 9.68 0.11 0.002 9.79 tion is the start of a run X  X n a concatenation of gray (or white) runs X  X s an observation from an independent but identical, Bernoulli trials. A sequence of a known number of run lengths of known total length almost forms a geometric distribution: this can be bitwise optimally represented by a Golomb code [12, 22].

To confirm that a Golomb code is appropriate, we exam-ined 126 points from the empirical cumulative distribution of the run lengths generated by two tranches of GOV2. We plotted these quantiles against quantiles from a geometric distribution whose parameter was derived from the observed mean run length. This Q-Q plot is very close to linear, and has correlation 0.995.

Moreover, in our experiments, we found that this ap-proach yields a compact code, requiring for example only 3.2 MB to describe about 2 million runs of gray in a 1000 MB dictionary; this was far less than was required by other meth-ods we explored.
As we do in the updating scenario, in this (transmitting) scenario, we evaluate our algorithm (here, CnP) on both small and large datasets. We take two consecutive tranches from the prefix of GOV2, called G 1 and G 2 , and two from the prefix of the Wikipedia dataset, called W 1 and W 2 .In our first experiment, reported in Table 8, each tranche is 25 GB; in our second experiment, reported in Table 9, each tranche is 100 GB. In each experiment, we investigate both a homogeneous setup, with the two tranches from the same source, and a heterogeneous setup, with two tranches from different sources.

Since our goal is to minimize the total data transmitted we outline a new metric, the Transmit Ratio (TR). The TR consists of three parts, each of which is reported as a ratio to the uncompressed size of the tranche C 2 : 1. R ( C 2 , D 2 ): The compression ratio of C 2 . 2. |D p 2 | c / |C 2 | : The size ratio of an efficient representation 3. |D p 1 | c / |C 2 | : The size ratio of an efficient representation
As we can see in Tables 8 and 9, CnP, which takes advan-tage of both D 1 and D s 2 during constructing D 2 , outperforms each of the two extreme baselines. The gap in performance is especially apparent with homogeneous datasets, and for heterogeneous datasets, the baseline that only uses D 1 per-forms extremely badly. Besides, we can also observe that the Golomb code does a good job in representing D p 1 .
As an example, on the large Wikipedia tranches, CnP achieves a Transmit Ratio of 9.054 %, relatively speaking, this is 8.9 % better than the baseline dictionary derived solely from C 2 . Thus, we regard the two strategies (CnP and CuD) together as a comprehensive solution to the in-cremental compression problem.
Relative Lempel-Ziv factorization is a fast and effective compression scheme for large repositories. As initially de-signed, it maintains a static dictionary in RAM against which a static collection can be efficiently encoded and de-coded. Previous work has explored compressing new mate-rial with a static dictionary, but in some cases, as confirmed in our experiments, this leads to poor outcomes. A na  X   X ve solution is to generate a new dictionary solely from the new tranche, thus adding a dictionary with each tranche, which is unlikely to be sustainable.

We have described new methods for RLZ compression of new material based on relatively small extensions to the orig-inal dictionary. Our work considers two practical incremen-tal collection scenarios: archiving and transmitting large in-cremental repositories while maintaining fast retrieval and reasonable RAM requirements; in these scenarios, the col-lection is extended by addition of tranches of new material. We exploit the observation that it is likely that a significant portion of a dictionary derived from a new tranche shares many common strings with a dictionary derived from exist-ing tranches. We therefore describe a scheme that success-fully generates an effective and compact auxiliary dictionary, based on consideration of the existing dictionary.
We focus on methods that require no recompression of previously compressed material, and have no impact on the extremely impressive decompression speed that can be achieved by RLZ. We concede that the compression times of CuD and CnP are larger than the baseline, but only because they each invoke RLZ twice: the fundamental principles of RLZ are unchanged.

We show that such a combined new dictionary can achieve much better compression ratio (almost 12 % relatively speaking) than if only the previous dictionary is used, and helps avoid pathological cases that arise when the charac-ter of the data is changed. We have also shown that rep-resentation of a large number of substrings of the original dictionary can be highly compact, through application of Golomb codes. For a comparable compression ratio, our combined new dictionary requires much less memory than that required by the na  X   X ve baseline. If the newly generated dictionaries are commensurate in size, our method dramati-cally outperforms the na  X   X ve solution. Overall, we show that new material can be added to a repository with only limited cost.
This work is partially supported by The Australian Re-search Council, NSF of China (61373018, 11301288), Pro-gram for New Century Excellent Talents in University (NCET-13-0301) and Fundamental Research Funds for the Central Universities (65141021). Jiancong Tong would also like to thank The China Scholarship Council (CSC) for the State Scholarship Fund. [1] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [2] J. L. Bentley and M. D. McIlroy. Data compression [3] N. R. Brisaboa, A. Fari  X na, G. Navarro, and J. R. [4] S. B  X  uttcher, C. L. A. Clarke, and G. V. Cormack. [5] A. Cannane and H. E. Williams. A general-purpose [6] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. [7] C. Clarke, N. Craswell, and I. Soboroff. Overview of [8] W. B. Croft, D. Metzler, and T. Strohman. Search [9] P. Ferragina and G. Manzini. On compressing the [10] P. Ferragina and R. Venturini. The compressed [11] G. Franc` es, X. Bai, B. B. Cambazoglu, and R. A. [12] S. W. Golomb. Run-length encodings. IEEE [13] J. He, H. Yan, and T. Suel. Compact full-text [14] J. He, J. Zeng, and T. Suel. Improved index [15] C. Hoobin, S. J. Puglisi, and J. Zobel. Relative [16] C. Hoobin, S. J. Puglisi, and J. Zobel. Sample [17] J. J. Hunt, K.-P. Vo, and W. F. Tichy. Delta [18] P. Kulkarni, F. Douglis, J. D. LaVoie, and J. M. [19] S. Kuruppu, S. J. Puglisi, and J. Zobel. Relative [20] U. Manber and E. W. Myers. Suffix arrays: A new [21] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [22] A. Moffat and A. Turpin. Compression and Coding [23] A. Moffat, J. Zobel, and N. Sharman. Text [24] G. Navarro and V. M  X  akinen. Compressed full-text [25] Z. Ouyang, N. D. Memon, T. Suel, and [26] A. Peel, A. Wirth, and J. Zobel. Collection-based [27] P. Shilane, M. Huang, G. Wallace, and W. Hsu. [28] J. A. Storer and T. G. Szymanski. Data compression [29] T. Suel, P. Noel, and D. Trendafilov. Improved file [30] D. Teodosiu, N. Bj X rner, Y. Gurevich, M. Manasse, [31] J. Tong, A. Wirth, and J. Zobel. Principled dictionary [32] H. E. Williams and J. Zobel. Compressing integers for [33] I. H. Witten, A. Moffat, and T. C. Bell. Managing [34] J. Zhang, X. Long, and T. Suel. Performance of [35] J. Ziv and A. Lempel. A universal algorithm for [36] N. Ziviani, E. Silva de Moura, G. Navarro, and R. A. [37] J. Zobel and A. Moffat. Inverted files for text search B Budget for RAM-resident dictionary
F ( C , D ) Factorization of (collection) C against (dictio-
R ( C , D ) The corresponding compression ratio of RLZ
C 1 , C 2 The existing and new tranches D 1 , D 2 Dictionaries, from C 1 and C 2 (respectively) D  X  Auxiliary dictionary for C 2 D  X  Concatenation of D 1 and D  X 
D A dictionary sampled directly from C 2 ,which D s 2 A dictionary sampled directly from C 2 D k 2 Dictionary CARE pruned from D s 2 D  X  2 Auxiliary dictionary that is the outcome of the D  X   X  Concatenation of D 1 and D  X  2 (  X  caneitherbe  X  Length threshold for identifying  X  X mall X  factors F  X  Sequence of all  X  X mall X  factors from C 2 2 Concatenation of all  X  X mall X  factors from C 2 D 2 Dictionary sampled from C L 2 (for Figure 5)
D 1 Dictionary from the initial tranche (baseline
D o 2 Dictionary generated from C o 2 , which is large |F ( C , D ) | c The compressed size of F ( C , D ) (using
D 1 Smaller dictionary extracted from D 1 |D 1 | c The size of Golomb-encoded representation |D  X  | c Thesizeof7-zip-compressed D  X  D Concatenation of D 1 and D  X  D p 1 CnP-pruned D 1 from D 1 D p 2 CnP-pruned D  X  from D s 2 G 1 ,G 2 The first and second tranche of GOV2
W 1 ,W 2 The first and second tranche of Wikipedia
