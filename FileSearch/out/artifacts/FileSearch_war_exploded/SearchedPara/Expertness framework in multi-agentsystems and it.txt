 Intelligent Systems Laboratory, Department of Computer Engineering, Sharif University of Technology, Tehran, Iran 1. Introduction One of the influential challenges in multi-agent systems as well as human societies is the Credit Assignment (CA) problem. The credit assignment problem concerns determining how the success of a system X  X  overall performance is due to the various contributions of system X  X  components [35], and distributing the credit among components is crucial. A simple model for the CA problem can be in the cooperation of some agents trying to achieve a goal (the goal may be solving a problem), and the method of the distribution of the messages or feedbacks of the environment among agents. The multi-agent credit assignment challenge has single-agent equivalents: temporal CA, and structural CA. In temporal CA, a credit is given to an agent with a delay (delayed reward); and the agent is faced with the question of which of his actions merit the granted credit.  X  X emporal credit assignment X  [1] tries to distribute credit across time. In structural CA, a credit is offered to the agent and the agent is encountered with the question of which part of his knowledge/action leads to reward/punishment and therefore how the feedback influences the previous knowledge. In other words,  X  X tructural credit assignment X  [8] means the distribution of the received feedback among different states and actions. Different approaches are suggested by Sutton [1] for solving the problem of temporal CA in the reinforcement learning method. Some of them are based on actor-critic and Q-learning [31]. Several researchers try to map structural CA to temporal CA and then solve it with the aid of temporal CA solutions [2]. In psychology, the problem is approached under the name of algorithms of social credit assignment [3].

Another approach in multi-agent systems is knowledge-based credit assignment [4]. In one method, credits assigned to agents are based on the values of a measure. If the expertness measure (as defined in [4]) are smaller than a given threshold, the agents are punished otherwise they are rewarded. In another method, when the team is rewarded, two thresholds are used, and agents are divided into three groups. The first group is punished, the second group is given neither reward nor punishment, and the third group is the one who deserves reward. The second group is added as a difference to the first method, because of the doubt about their right/wrong behavior, and eligibility for reward or punishment is unknown. The rest of this paper is organized as follows; in Section 2 we have a brief overview of related works. In Section 3, some definitions and background related to the problem of credit assignment are given. In Section 4, the model is introduced, and in Section 5 the proposed method is discussed. Then we empirically evaluate the effectiveness and accuracy of the algorithms in Section 5. Finally, the paper is concluded in Section 6. 2. Related work
The credit assignment problem is first defined by Minsky [35]:
The credit assignment issue is encountered in some main categories in research, for example  X  X empo-ral credit assignment X ,  X  X tructural credit assignment X ,  X  X ocial credit assignment X , and  X  X ulti-agent credit assignment X .  X  X emporal credit assignment X  [1] tries to distribute credit across time.  X  X tructural credit assignment X  [8] simply means distribution of the received feedback between different states and ac-tions.  X  X ocial credit assignment X  tries to assign credit according to the psychological theories [3]; and  X  X ulti-agent credit assignment X  distributes credit in a group of agents (one of the techniques being  X  X nowledge-based credit assignment X ). The following paragraphs give a brief introduction to these and other categories of credit assignment in the research.

Temporal credit assignment : In temporal CA, a credit is given to an agent with delay; and the agent is faced with the question of which of his last actions merit the granted credit. Solutions to temporal CA are categorized into two groups: solutions based on temporal difference like Q-learning and other derived algorithms. They are algorithms for solving the problem of delayed rewards. The second category relates to the eligibility tr aces method, and it will be talked next [1]. The problem of credit assignment in the presence of delayed feedback is also investigated in [14].

Structural credit assignment : In structural CA, a credit is offered to the agent and the agent is en-countered with the question of which part of his knowledge is credit worth-giving, that is which state or state/action pair should be rewarded/punished. The simplest method is the update of the state in which experience has taken place. Q-learning [31] and profit sharing [32] are examples of performing such tasks. In profit sharing the new value is dependent on the last value, learning rate, and received feed-back. But in Q-learning, with the parameter  X  , the value of the next state is also used in the calculation of the current state value.

The other solution is gained by storing the number of references (parameter e) to each state. The higher This approach is used in TD(  X  ). The eligibility trace for state s at time t is denoted with e t ( s )  X  + . In the equation below, s is a non-terminal state,  X  is the discount rate and  X  is the parameter introduced with TD(  X  ) .

Two popular methods for calculating  X  X  X  are as follows:
Social credit assignment : Based on the psychological attribution theory, a computational approach to social credit assignment is presented in [3]. Attribution theory (in psychology) is the study of feelings or characteristics that are projected onto others, or predicting the success or failure based on how people explain their own or others X  past performance. They use coercion to determine the responsible agent, and intention and forecast-ability in assigning the intensity of the credit/blame. They adopt a simple categorical model of intensity assignment, though one could readily extend the model to a numeric value by incorporating probabilistic rules of inference. If the responsible agent predicts the consequence while acting, the intensity assigned is high. If the responsible agent does not foresee the consequence, the intensity is low [3].

Knowledge-based credit assignment : In [4], based on the value of one of the criteria that is computed for each agent, if the values are lower than a threshold, the agents will be punished, otherwise rewarded. The author proposed that some agents can be punished even if the whole group is rewarded, and vice versa. In another suggested method, when the team receives feedback, two thresholds are used, and agents are divided into three groups. The first group is punished, the second group is given neither reward, nor punishment, and third group receives reward. The second group is added as a difference with the last method, because we doubt about the right/wrong behavior of this group. In addition, the idea that punishing the agent when he does not deserve punishment has worse effect than rewarding him when he deserves punishment is proposed. The inspiration of punishing some agents when reward is given to the team is extracted from the human society counterpart, for which there is a possibility that a group of agents, as a whole, receives the reward, although some agents do not perform their task properly, and thus deserve punishment. [4]
In some studies, the given feedback is split into two parts, global and partial, namely observational and vicarious reinforcement [19]. Some experimental studies have been carried out in order to compare different policies in different situations, which is better than the other [18]. Some researchers argue that global reward does not scale well to increasingly difficult problems because the learners do not have sufficient feedback tailored to their own specific actions [17]. In [20], agents use a Kalman filter to extract the agent X  X  contribution from the teammates X  contribution and compute the agent X  X  share in the global reward. In [22], averaging rewards over discounted rewards are preferred, and thus is given great priority of immediate rewards rather than future rewards. In [23], neuro-biologically plausible solutions are investigated in the credit assignment problem for neural networks. The problem of credit assignment in horizontal or vertical power division is investigated in [24]. In [25], the challenge in evolutionary methods is discussed, in which the genetic algorithm faces the difficult credit assignment problem of evaluating how a single gene in a chromosome contributes to the full solution. And the difficulty of evaluating the effectiveness of neurons in the network population is discussed in [33]. Credit assignment is also used to reduce learning interference in conventional CMAC [34].

In [26], a reinforcement learning mechanism as a model for recurrent choice is proposed and extended to account for skill learning.

In [27], organizing knowledge components are investigated so as to enable accurate and efficient struc-tural credit assignment over the resulting structure. To explore the temporal credit assignment problem, event-related potentials (ERPs) are recorded as participants performing sequential decision tasks [28]. In [29] the interaction of explicit and implicit learning with internal and external state information in credit assignment problem is investigated. The issue is also studied in Neuronal Population Learn-ing [30].

The problem is also solved in hierarchical classification [8]. Some have tried to unify the structural and temporal challenges [9]. Credit assignment is also introduced in rule discovery systems based on genetic algorithms [10]. This issue is also discussed in modular or hierarchical reinforcement learning systems in a multiple model-based reinforcement learning architecture [11]. The concept  X  X odular reward X  is introduced in [11], which is calculated from the temporal difference of the module gating signal and the value of the succeeding module. It also has applications in learning grammatical rules for inferring a context-free grammar from a set of sample sentences, in which a number implying the strength is associated to each rule taking into account its usefulness in correctly classifying the test sentences [12].
Another form of learning is related to CO llective IN telligence (or COIN) [13]. A COIN is a large multi-agent system where there is little or no centralized communication/control; and there is a provided-world utility function (or  X  X onderful life utility X  instead of local and global rewards [21]) that rates the possible histories of the full system. In COIN approaches, each agent runs a reinforcement learning algorithm. COIN design problems were specifically successful in packet-routing, the leader-follower problem, and in variants of the El Farol bar problem.

A distinct class of multi-agent learning algorithms is multi-agent reinforcement learning, surveyed in [36], which has provided a taxonomy for reinforcement learning tasks according to the type of task, fully cooperative (in static and dynamic domains), fully competitive, and mixed types (static and dy-namic). For a detail discussion of several algorithms classified in these groups, please refer to the [36]. In the method that we proposed in this paper, we tried to convert the task of multi-agent learning into multiple single agent learni ng tasks, this way each agent can pursue i ts learning tasks i ndependent of oth-ers. Beside all the advantages independence has, the speed of learning can be different between agents. 3. Problem definition: Multi-agent credit assignment problem
The multi-agent credit assignment problem, as shown in Fig. 1, is defined as a group of agents co-operating with each other, in order to achieve a goal. Environment is the location in which agents have interaction with each other and also with the environment itself, and can be categorized as dynamic or static, partially or totally observable, stochastic or deterministic, multi-agent or single-agent, episodic or sequential, and discrete or continuous [7]. From the viewpoint of the environment, teamwork is observed as a single task, and thus the environment returns a single scalar value as team reinforcement. Now the group is confused regarding which portion of the reinforcement belongs to which agent, in order for each agent to use individual reinforcement learning methods like Q-learning. We will assume (without loss of generality) that there exists a special agent, named critic , whose job is to distribute the feedback accord-ing to agents X  long term behaviors. Critic is a special agent who has the wisdom of credit distribution ( Credit is defined as the distributed feedback among agents). The presumption of the existence of critic agents does not restrict the solution, because we can think that every agent can have an internal critic in his mind, and the team reinforcement is given to every agent. The internal critic himself can decide what portion of feedback belongs to him. As shown in Fig. 1, the state vector is given to every agent ( State is defined as the condition of each agent in the environment), and uncertainty (noise) can be added to the agents X  actions. Of course, the effect of noise can be studied in different parts of the architecture.
The whole process is as follows: the environment is in state s while the group of agents has the op-the number of agents. This vector of actions can be combined with a vector of noise, [ n 1 .. n i .. n N ] and a number which implies the degree of satisfaction from the received actions. Generally, feedback may be everything, and not necessarily a numeric value. Here, in particular, feedback is a single scalar and of converting a scalar to a vector of scalars is devoted to the critic, and with the r i s given to the agents, they have the chance to learn in an individual reinforcement learning environment, and this sequence of actions and decisions continues until the end of the episode. The critic evaluates the knowledge of agents by one of the six criteria which are defined in [4 X 6], and discussed next. 4. Learning capability
In this section, the proposed method for critic improvement over time will be explained. Suppose agents are cooperatively working in an environment and there is a distinct agent, named  X  X ritic X , whose responsibility is t o distribute received feedback among agents. T his agent is thinking of maintaining an array of coefficients (a sign of agents X  knowledge of encountered state), and improving the coefficients according to the amount of joint feedback received. The following equations try to explain the idea.
In Eq. (3), f total t is a scalar that shows the total amount of feedback received when a group of agents choose a group of actions toward the environment. c i t is the coefficient that critic maintains which shows the degree of maturity (degree of non-random selection of the actions, where exploitation instead of exploration is used) of the agent work; and f i t is the amount of feedback for the i-th agent.
A constraint is maintained for the coefficients, in which the sum of the coefficients should be one, or for short, they are normalized. (According to the following equation)
And the method of learning for the critic is as follows: where f i t is computed by one of the criteria that is discussed next (related to agent i ) ,  X  is the learning Superscripts identify the agent number; subscripts denote the time. The solution has the advantage of adapting to the environment; as the amount of reward is changed, this method adjusts the coefficients with the change.

Six criteria introduced in [4 X 6] are used to provide feedback to the critic agent, namely normal expert-ness, certainty, correctness, efficiency, GSR, and LR, which are explained next. 5. The domain
The experiment which is about to test the idea, is a one-step game, in which 5 agents want to add two 5-digit numbers  X  one pair of single digits for each agent. The digits are 0 to 4, inclusive. The addition doesn X  X  produce any carry. At every episode, until obtaining the correct answers by the agent to all the 5  X  5 questions, a pair of 5-digit numbers is given to the agents by the environment, and he should suggest the action. Each agent has the action set {0, 1, 2,. . . , 8}. When they suggest an action, the environment will give them a reward in the format of a single scalar. The reward is such that every correct action that is suggested by the agent is rewarded by + 4. If all the agents suggest the correct answer total reward will be + 20. For example, in case of 12304 + 33410 for which the result is 45714, if the agents suggest 45814, the reward would be 16. The logic of calculation of the reward is unknown to the critic, and he should try to understand how to distribute the reward among the agents. The domain is also used in experiments conducted in [4]. 6. Experimental results
In this section, the results of the method introduced in the last section on a domain discussed are reported. The task is designed such that every agent learns 35% of its task by individual learning (the criterion for the percent of learning is the ratio of the number of learnt states to the total number of states), and the rest will be learned in a cooperative learning environment. The existence of the individual learning before cooperative learning is a common assumption, as it always occurs in the society. When a group of people want to perform a group task, they have the chance of learning at least part of the task individually. In later experiments the individual learning phase is removed and the new experiments without it are also reported.

Six criteria which are the source of the feedbacks are examined in six different experiments. The results are compared in each section with two different critics, equal critic and random critic. The equal critic assigns equal coefficients to the agents; and the random critic assigns random coefficients to the agents. All the coefficients (for either equal critic or random critic) normalize such that the Eq. (4) is satisfied.

All of the experiments are performed in 100 episodes, and the condition of finishing the episode is that the group of agents finds the goal, which is the correct answer of the sum of 2 five-digit numbers. 6.1. Experiment set I: Comparison of the method with equal and random critics 6.1.1. Experiment I: Comparison in terms of normal expertness
The first criterion to be discussed is normal expertness. It works with the number of rewards and punishments that the agent encounters during an episode. It states that in the interaction of the agent with the environment, more reward and less punishment shows that the agent is a better expert, and is thus more skillful. After all, when the agent is performing exploratory movements, the number of punishments is more and the number of rewards is less than the time of exploitation. Expertness is experience in receiving rewards, he is probably an expert in that area (for example in that special state), and by receiving punishment, he might be less expert because he made some mistakes. This is related to the history of the agent, and the history should be maintained in order to judge with this criterion. E ( s, a ) is the amount of expertness that the agent gains. N r ( s, a ) is the number of rewards that he gained until now, and N p ( s, a ) is the number of punishments.

Figure 2 shows the results of critic learning with Normal Expertness measure. As can be seen, there are 3 experiments. The one with random coefficients tries to choose coefficients in a random manner. Another experiment with equal coefficients always distributes feedbacks equally among agents, and the third uses normal expertness to determine the coefficients of distributing feedbacks. These three tests are compared according to normal expertness, and the agent with normal expertness learning capability was better than the other three. 6.1.2. Experiment II: Comparison in terms of certainty
The next indicator to be measured is certainty. According to certainty, an attempt is made to measure how certain the agent is when he selects the action. This is explained with the following equation. Since the exponent function is used, the numerator and denominator are always positive. The certainty is measured as follows for each state s and action a , Q-values will be used in the Q-tables that will be updated according to the following equation:
In Eq. (9), s is state and the sum in the denominator is over all actions a that can be selected in state s .

There is a special parameter T in Eq. (9), denoting temperature that can be selected according to the degree of precision that Q-values have (in allusion to chemical kinetics). For high temperatures (
T  X  X  X  ), all actions have nearly the same probability, and the lower the temperature, more expected rewards affect the probability. For a low temperature ( T  X  0 + ), the probability of the action with the highest expected reward tends to be 1.

For calculating T, the preceding equation is used. In this equation, episode is the episode number ranging from 1 to 100, T_INITIAL is 10, T_MIN is 1, and has an increasing nature which is a must in the exploration-exploitation trade-off.

This is one of the criteria independent of history, and depends on the current Q-value. As can be seen in Fig. 3, in the case of the critic with the equal coefficients and the critic with random coefficients, no improvement is observed over time. They start with a certainty value of 10 and remain at 10. However, in the case of the critic with learning capability, numerical values show that it starts with the value 10, and then improves over time (with some spikes). Although some spikes are observed, the trendline shows improvement over time. 6.1.3. Experiment III: Comparison in terms of efficiency
The next criterion to be discussed is efficiency. It is based on the hypothesis that whenever a critic doesn X  X  know if the agent should be rewarded or punished, the best thing is not to encourage/discourage him and only give him the feedback zero. This way, if we have some non-zero assignments, it means that the critic has some knowledge of the assignment, and thus we have improvement. In Efficiency criterion Eq. (10), the assignments done by the critic are intended not to equal zero and be purposeful and have an effect on the knowledge of the agent, that is, either to punish or encourage him by negative or positive reinforcements. Instead of zero itself, a distance to zero can be estimated. The philosophy behind efficiency is that, since the critic agent pursues a cautious behavior, whenever he is unsure of how correct the value to be assigned to the agents is, he avoids assigning it. Because experiments show that giving wrong positive or negative rewards to the agents misguides them toward their goal, when not sure about the correctness of the rewards, it is better not to assign anything. Consequently when a non-zero assignment is encountered, it means a rigid decision is made by the critic and that indicates an improvement on the trend of the assignment.

According to the preceding discussion, the efficiency criterion is defined, where | . | is the number of assignments made. As can be seen in Fig. 4, the critic with learning capability has more efficiency than the other two competing agents. 6.1.4. Experiment IV: Comparison in terms of correctness
Another way to guide the critic agent is through the correctness criterion. The correctness wants to measure how many correct assignments the critic made. Thus, the more correct assignments the critic made, the more rewards she received, and the more she was ensured that the assignment was truly made. Correct assignments are defined when the value of the assigned feedback and the real value of the reward that should be given to the agent differed by a threshold which will be determined as a parameter. Thus, this criterion needs setting a parameter and also maintaining the history.

Figure 5 shows the capability of the learning critic to improve his correct assignments over time. As can be seen, critics with ra ndom and equal coefficients reach a corr ectness value and re main there until the 100 th episode. 6.1.5. Experiment V: Comparison in terms of group support ratio
Group Support Ratio (or for short GSR), is a team-based criterion. If the credit assignment among agents has good qua lity, the rate of reward-tak ing will be increased because agents are learning how to do their tasks and thus improvement can be seen the overall performance of the team.

GSR focuses on a window of n = 100 previous interactions with the environment (since we want to compute the percentage ( = 100) of group supporting), and calculates the percentage of interactions in which the group got the reward. Group support ratio ( GSR ), as given in Eq. (12), observes a short history of the past n steps. Between the last 100 steps, the steps which reward (positive reinforcement) is given to the agent are calculated and the more rewards the agent receives, the more probable for him to receive big rewards in the future.

The performance of the learning critic is evaluated through GSR in Fig. 6. As can be seen, in 100 episodes, the critic finally learned the task co mpletely, and reached the GSR value of 100% or 1. The critics who carry out th e task wisely reach the complete GSR in 100 episodes, much better tha n random or equal agents. 6.1.6. Experiment VI: Comparison in terms of learning ratio
The goal of this experiment is to study the learning ratio in the expertness framework. Learning Ratio ( LR ), as shown in Eq. (13), is the proportion of number of states in which their correct action has the highest value to the total number of states. This criterion does not take history into account, and does not need any parameter to be tuned.
 where | states | is the number of states existing in the problem, in this environment the number is 5  X  5 of states that has a single unique action with maximum Q-value, and that action is the correct action in that state. | . | denotes the number of elements in the set (i.e. | {0,1,2} | is 3.).

Let LR i be the learning ratio for the i-th agent; n is the number of agents in the group, and the learning ratio of the group in time t denoted by LR t is the average of the learning ratio of the agents.
As can be shown in the Fig. 7, the critic with the learning capability has reached much better results than the other competing critic agents.
 6.2. Experiment Set II: The effect of noise
In order to evaluate the expertness framework on noisy environments, computer experiments are con-ducted. In these experiments, the method proposed in Section 5 is evaluated in a noisy environment according to six performance criteria proposed in [4 X 6]. In experiments I to VI, the effect of noise on normal expertness, certainty, efficiency, correctness, group support ratio, and learning ratio is reported, respectively. In experiment VII, the effect of noise on the number of tries is evaluated. The implemented noise is such that the content of Q-table will randomly change every episode. The noise is designed such that 20% of the entries in the Q-table (almost 45 out of 225 entries of the table) will change 1.8 times their correct value; and thus an attempt is made to investigate how much our method is noise-tolerant. In section 5.1 the domain used for experiments is explained. 6.2.1. Experiment I: The effect of noise on the normal expertness criterion
Figure 8 shows the effect of noise on the average normal expertness value of five agents. 100 episodes are experimented as the frame of the test. During this time, as can be seen until the 50 th episode, no distinct effect can be detected, but after that time, it seemed that the cumulative noise causes a degree of distance (value of normal expertness) between the noisy and clean environment. The diagram shows the normal expertness of the agents after 100 episodes for noisy environment is 9% lower than the environment without noise. Also the effect of noise on the critic with equal coefficients and on random coefficients is shown. In the case of equal coefficients, the feedback is distributed equally among agents. In random coefficients, the feedback is distributed randomly among agents such that the sum of given feedbacks equals the received feedback. 6.2.2. Experiment II: The effect of noise on the certainty criterion
Figure 9 shows the effect of noise in the environment with certainty criterion. As can be seen, except for the values that rise sharply, the certainty criterion behaves well in noisy environments and both of the diagrams start from around 10 (the value of 10 is due to individual learning), and at the end of 100 episodes both are around 35. The effects of noise on the critic with equal coefficients and on random coefficients are also shown. 6.2.3. Experiment III: The effect of noise on the efficiency criterion Figure 10 shows the effect of noise on the learning critic measured using the efficiency criterion. This figure shows that the learning critic has better performance in clean environment than in noisy environment. In this figure, a 13% decrease in efficiency criterion is observed in episode 100. As the amount of noise is high, a 13% decrease is negligible with respect to the degree of input noise. Non-zero credit assignment is a metric showing the ability of the critic to distribute credit, and the amount of reduction in noisy environment is significant. The effects of noise on the critic with equal coefficients and on random coefficients are also shown. 6.2.4. Experiment IV: The effect of noise on the correctness criterion
Figure 11 denotes the effect of noise on the learning critic measured correctness criterion. The critic got more correctness confirmation in clean environment than noisy environment; the clean environment in 100 episodes got 1200, and got almost 960 in noisy environment, where 20% reduction is observed. The reason for this reduction is the cumulative effect of noise from the start of the test until the specified episode. Correctness values in random and equal agents are also shown in the diagram. 6.2.5. Experiment V: The effect of noise on the group support ratio criterion
Figure 12 shows GSR value for agents operating in noisy and clean environments. As can be seen, the clean environment gains more GSR than the noisy one from initial episodes, and the distance between the two graphs increases as the episodes passed. Near the final episodes, 18% reduction can be observed in the diagram. 6.2.6. Experiment VI: The effect of noise on the learning ratio criterion
This experiment evaluates 100 episodes. The reason for choosing that number is that the agents in the clean environment have the opportunity of reaching the learning ratio near one. As observed in similar setting, the agents in a noisy environment reach the learning ratio near one, showing the ability to overcome the noisy environment. Figure 13 shows the effect of noise in the environment in which the critic agent uses LR criterion Eq. (9). As can be observed the effect of noise on the system is so low that two graphs in almost all of the episodes improve with each other; and thus noise has no significant effect on the learning process with LR criterion.
 6.2.7. Experiment VII: The effect of noise on the number of tries The goal of this experiment is to evaluate the number of tries in noisy and noiseless environments. By the number of tries, we mean the responses that the group of agents attempt in order to finish the episode. Figure 14 shows the number of tries each agent carried out in order to proceed and complete 100 episodes. The names of the criteria are written in shorthand form, i.e. LR is learning ratio Eq. (13), GSR is group support ratio Eq. (12), Eff is efficiency Eq. (10), Cer is certainty Eq. (7), Cor is correctness Eq. (11) and Nrm is shorthand for expertness Eq. (6). In each group, left bars show the number of tries when noise does not exist, and right bars indicate the number of tries when the mentioned noise exists. As can be seen, there is almost a 27% increase in the number of tries needed to complete 100 episodes. Since the degree of change is to some extent high, 27% change is reasonable; and the amount of change in the real environment is less than this percentage in order for the performance to be better. 6.3. Experiment se t III: The effect of i ndividual learning
In this section, the independence of the method from individual learning is investigated. All of the experiments conducted until now is done with a bias of individual learning at the beginning. In this set, experiments are performed to examine the two conditions in presence and absence of individual learning. 6.3.1. Experiment I: Effect of individual learning on learning critic with normal expertness
In the Fig. 15, two kinds of cooperation, one with individual learning and the other without individual learning is studied. As can be seen, the two diagrams start with each other, and end in approximately the same point, but are different in that the normal expertness of the learning without individual learning is lower than the one with individual learning. However, the final node for both of them is the same. Two other agents exist, random and equal agent. Two curves (random and equal) are shown in Fig. 15, which show that they are much lower than the other 2 agents. It is shown that normal expertness is independent of individual learning. 6.3.2. Experiment II: Effect of individual learning on learning critic with certainty
Figure 16 shows the effect of individual learning on the learning critic, and the two tests, one with individual learning and the other without it. The trend of the diagrams without attention to the spikes shows the improvement on the certainty value. Random and equal agents have much less certainty in comparison to cooperative learning. 6.3.3. Experiment III: Effect of individual le arning on learning critic with efficiency
To study the effect of individual learning on the next criterion,  X  X fficiency X , two tests are executed one with individual learning, and one without it.

In this experiment, an attempt is made to guide the critic by means of efficiency. As it is observed in Fig. 17, individual learning has no effect on the efficiency of the critic. However, random and equal agents have low values of efficiency.
 6.3.4. Experiment IV: Effect of individual learning on learning critic with correctness
As it can be seen in Fig. 18, in the case of correctness, individual learning makes a big difference for the case, and with individual learning, a lot more correctness percentage are obtained than without individual learning, and random and equal agents do not have any promotion over time. 6.3.5. Experiment V: Effect of individual learning on learning critic with group support ratio
As shown in Fig. 19, GSR value for the agents with individual learning and without it is investigated. 100 episodes have shown that the start of the curves is the same. After some episodes, it seems that the space between the two curves is increased. However, after 100 episodes the GSR value finally comes close to one, meaning that after 100 episodes, almost all the interactions include a reward. It seems that the speed of GSR enhancement is more when the agents have an individual learning phase. 6.3.6. Experiment VI: Effect of individual lear ning on learning critic with learning ratio
Figure 20 shows the effect of individual learning on the learning ratio of the agent. As can be seen, the starting point of the curve with individual learning shows the percent of individual learning, which is 30%. The rate of increase in the LR in the curve with individual learning is higher than the curve without individual learning. 6.3.7. Experiment VII: Comparison of different criteria
The next experiment tries to compare the six criteria with each other, in terms of number of tries. This is the number of efforts that agents made in order to finish the episode. Since the condition for finishing the episode is that the group of agents reaches the goal, the number of tries that they made for its end and the speed of its reduction are important. As can be expected, the number of tries without individual learning in each of the criteria is more than the case with individual learning. In the case of cooperative learning with and without individual learning, Learning Ratio is more successful in guiding the critic, and thus the learning ratio is more preferable. The results are reported in Fig. 21. 7. Conclusions and future works
This paper discussed the multi-agent credit assignment problem, and stated that when the role of the agents in teamwork is not obvious and the environment does not have clear information about the agents, there is a necessity for the existence of a responsible for the distribution of feedback among agents, and as this problem has lots of dynamics, an attempt was made to model this dynamicity with a critic agent who shares the feedback among agents. He will try to adapt his knowledge to the performance of the agents. Some criteria are used to guide him through his task. Results show the superiority of the learning critic compared with their two rivals according to the six criteria used. Additionally, the study of noise on the proposed method and the effect of individual learning should be studied, and it is clarified that the two methods are not really dependent on the bias of individual learning. Some measures are used for credit distribution, and a comparison of amount of noise tolerance and the effect of individual learning on them are performed, beside some requirements of parameter tuning (maybe some hints exist in the environment that guide the tuning of parameters), the amount of noise tolerance and necessity of individual learning can be used to select among six measures.

The future works include the introduction of more criteria in the domain, implementation of the method in more complex domains and more in-depth analysis of several kinds of noise in different parts of the model. Mathematical analysis of our approach is also among the topics of future research. In addition, empirical tests will be performed in more complex domains which include physical robots navigating in the real environment.

Finally, there should be ways to include and use the domain information in the credit assignment problem, and it is predicted to improve the performance of the system that credit assignment existed in.
The expertness framework can be applied to different problems of resource allocation, decision sup-port systems and problems of hierarchical reinforcement learning and semi-Morkov decision processes. References
