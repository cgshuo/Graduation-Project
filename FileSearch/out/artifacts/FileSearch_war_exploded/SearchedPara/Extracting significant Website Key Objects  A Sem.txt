 1. Introduction
The rapid growth of the World Wide Web, the assembly of large-scale volumes of Web data, and ever exponentially increasing applications have led to the development of ever smarter approaches to extract patterns and build knowledge with the aid of artificial intelligence techniques. These techniques have been used, together with information technology, in a wide range of applications. This is where semantics, social network analysis, Web structure, content, usage, and other aspects have already been and will increasingly be included in many application domains.
 One of such domains is related to how Web users browse the
Webpages and Websites looking for information. Often, they require and there is a greater possibility of them staying or returning to the Website if they find the content they are searching for in a Website. For this, Website administrators intend to reach the highest user base they can, therefore it is within their interest to provide accurate and correct content ( Vela  X  squez and Palade, 2008 ).

However, different difficulties a re present in this application domain. On the one hand, Web users X  interests often change and it is often unclear to assume at first sight what the users X  interests are.
On the other hand, whether the conte nt has been correctly presented is a relevant question for any Website administrator. Furthermore, the content may be presented in several formats, which could stem from free text to images or videos. In this sense, not only it is unclear what is the content that users are looking for, but also their preferences in terms of the format that should be considered.
A typical Website is composed primarily of a free text being formatted within the limitations imposed by the HTML standard. However, they also consist of other data formats such as images, videos, etc. An example of this is the most successful sites of the so-called Web 2.0 such as You Tube where the main focus of interest lies in Web videos. A drawback to this trend is that the formats shown above do not provide information regarding their content which can easily be retrieved by a computer and, therefore, a small degree of content analysis can be carried out in relation to them.
In Web mining, several techniques have been created to discover the problem stated above, focusing mainly in text-based Websites presentation formats, images, or flash animations present on the most successful Websites nowadays. Taking this into consideration the main idea is to define WebObjects which could represent any content in a Website independently of the format in which it is presented and todiscoverwhichofthisWebObjectsattractsuserinterest.These objects are named Website Key Objects.

The main contribution of this work is a methodology that enables the extraction of significant Website Key Objects, following a Seman-tic Web mining approach. In this case, Semantic Web mining will be considered as using data mining algorithms in order to extract relevant information from the Se mantic Web representation of a given Website. Specifically, the idea is to extract a new relation between structured component s from a Website (WebObjects), represented by a simple core ontology. This relation is extracted from the Web user X  X  perspective, represe nted by their collected sessions, from which patterns are extracted.

This paper is organized as follows. Section 2 provides an overview on related work for significant Website objects extrac-tion. In Section 3 the proposed Semantic Web mining approach to find Website Key Objects is presented. Then, in Section 4 an application of our work for a Chilean Geographical Information
Systems company 1 is described. Finally, conclusions and future work are presented in Section 5. 2. Related work
Different approaches have been previously used to find relevant information for end-users in Web mini ng applications. In this context, traditional techniques, such as We b content, usage and structural mining, have been taken into account by most researchers. Further-more, Web semantic mining techniques have been proposed for extracting relevant information fro m a given Website. In this section, the main contributions on traditional Web mining techniques, as well as semantic approach, are reviewed. 2.1. Significant information extraction using Web mining
Significant information extraction from Web content has been a major focus for many researchers, where different degrees of information, such as words, text passages, or WebObjects, have been taken into account. Furthermore, many methodologies have been proposed, and some of the most relevant approaches will be discussed in the following.

The methodology created by Vela  X  squez (to appear) and Vela  X  squez et al. (2005) for finding Website keywords forms the basis of this work, in which information retrieval and Web usage mining techni-ques were used within the Knowledge Discovery in Databases (KDD) ( Fayyad et al., 1996 ) framework, to find the keywords that define the search process for a group of users. The process described is based on five fundamental steps. The first one is associated with the Vector
Space Model definition from a given Website ( Salton et al., 1975 )and the processing of Weblogs, in order to include the end-user informa-tion. Afterwards, this methodology focusses on finding the relation-ship between the page interest and time spent, as well as selecting the most important pages from the ex tracted user sessions. Finally, by using different clustering techniques (particularly k -Means and Kohonen Self-Organizing Feature Maps), the process to discover
Keywords in clusters takes place. According to Vela  X  squez (to appear) , this methodology can be used in order to improve a given Website information to enhance the general content of the Website.
The attempts for attracting users to Websites have been made since the Web became a massive source of information, and the study of usability in Websites has been one of the most widespread research domains. One of the first approaches to create Website usability patterns is the Common User Access (CUA) proposed by Berry (1998) . Another approach described by Nielsen (2006) focuses in how to present the content in terms of typography, design, presentation of elements and other end-user properties associated with their interaction with visualization components. These patterns have shown to be effective, but they l ack information about the direct feedback for the users of the site.
 Several researches have been done lately in the field of
WebObjects. In this section some of these methodologies will be described focusing mainly on three of them: WebPage Element
Classification ( Burget and Rudolfova  X  , 2009 ), Named Objects ( Tiwary et al., 2009 ), and Entity extraction from the Web ( Urbansky et al., 2008 ).
 posed by Burget and Rudolfova  X  (2009) focuses on the fact that in normal pages found across the World Wide Web, most additional information such as copyright notices or advertisement influences in a negative manner the results of the Web. To avoid this, a method for detecting the interesting areas in a Webpage from a human reader approach is created. This is accomplished by dividing a Webpage into visual blocks and detecting the purpose of each block based on their visual features.
 of the approach presented by Tiwary et al. (2009) , where the perception of a Webpage is obtained through the intention of users. This intention delivers information for both the user and the Webmaster and is the basis of Named Objects which allows the mining of patterns within Websites by using a Web Design
Pattern approach. This named objects are used as the basis of mining methods which allows Web Content Mining.
 et al. (2008) , which uses Concepts, Attributes and Entities as input data. By modelling this using an ontology, facts from generic structures and formats are extract ed. Afterwards a self-supervised learning algorithm automatically estimates the precision of these structures.
 developed from different perspectives. One of the leading approaches was proposed by Gao et al. (2005) , whose research is based on determining which is the information on a given Web that is most interesting to end-users. In this approach, informa-tion retrieval techniques ( Jr and Ziviani, 2004 ) are used along with Web usage mining to infer the user preferences found in objects.
Also, microformats ( Khare and C -elik, 2006 ) have been previously used as a mechanism to add semantics into a given Website, and improving the information extraction from the usage data of different Websites ( Plumbaum et al., 2009 ). 2.2. Significant information extraction using Semantic Web mining relevant patterns from the Semanti c Web, different techniques have been proposed. One of the first approaches ( Li and Zhong, 2003 ) presents an ontology representation of user profiles in order to design efficient Web mining models. This approach is one of the first to be considered oriented towards the correct characterization of user profiles according to a semantic representation.
 ogy towards a more flexible and capable ontology for further applications related to the usage of Web user profiles for different
Web mining applications. Furthermore, in Li and Zhong (2007) , introduced pattern taxonomy and Ontology mining into a general semantic representation.
 oped, one of the main difficulties for their correct analysis is the comparison between their different formats. In Sahami (2006) ,an approach to set a common representation between WebObjects using natural language processing and semantics to set their differences has been proposed.
 mining, Chambers et al. (2006) present WebLearn, whose objec-tive is to identify WebObjects by using the semantic content of the written language surrounding WebObjects.
 of the semantic information for WebObjects is one of the main consideration. Likewise, in Berendt et al. (2002) ontology learning ( Maedche and Staab, 2001 ; Poon and Domingos, to appear; Tsoi et al., 2009 ) approaches must be considered in terms of applying
Semantic Web mining approaches. These approaches are based on basic pattern recognition techniques to extract, prune, refine, and reuse of Web information to set the basics for ontology learning from an architecture structure.

Most of the ontology learning approaches are based on textual analysis of documents ( Zavitsanos et al., 2010 ), where using prob-abilistic reduction techniques the semantic representation of a given corpus is identified. Different approaches have been used in the past to extract ontologies from document-based environments, where one of the most promising automatic techniques are based on latent semantic analysis tools ( Paa X  et al., 2004 ), where using topic model-ling ( Blei et al., 2003 ) and other information theoretic approaches ( Papadimitriou et al., 1998 ; Baeza-Yates and Ribeiro-Neto, 1999 )have been widely used. 3. A methodology to extract Website Key Objects In this section, the proposed approach to extract the Website
Key Objects is presented. Firstly, the problem definition and the general notation of terms are introduced. Secondly, Web content and Web usage mining terms and methodologies used to achieve the Website Key Object definition are presented. Finally, the core methodology and main contribution of this work are detailed. 3.1. Problem definition and general notation
In the following, WebObjects and Website Key Objects terms are presented, as well as the proposed ontological representation and mathematical notation. Likewise, the Website Key Object (WSKO) extraction problem is introduced. 3.1.1. WebObjects
WebObjects may represent structured text or any other multi-media format present in a Website. In order to process their content using computer software it is necessary to include metadata that describes them. The definition given in a later section allows a human to comprehend the nature of a WebOb-ject. However, a computer is unable to understand this so the following definition of a WebObject is introduced.

Definition 1 ( WebObject ).  X  X  X t is a structured group of words or a multimedia file present within a Webpage that has metadata for describing its content X  X .

The implementation of WebObjects can be made in several ways because it relies heavily on the ontology used to describe them. In this work a simple ontology was introduced based on the work by the MPEG to include metadata in videos. In this sense an
XML document will be associated with each WebObject present in a Webpage. Despite the complex semantic analysis of multi-media, metadata is used to define the WebObject within it. In this sense, a set of N objects is defined by x  X  { x 1 , y , x object is defined by a set of M concepts, c  X  { c 1 , y , c
In our approach, the usage of metadata to describe WebOb-jects will be considered as the basis to constitute the information source, in order to build a vectorial representation of its content.
However, the end-user X  X  point of view will be considered as the principal research topic of this approach. Therefore, the contents of the Website and Weblogs are combined for processing. 3.1.2. Website Key Objects Having described what WebObjects are, we introduce the term Website Key Object (WSKO) as follows, Definition 2 ( Website Key Objects ).  X  X  X ebObjects or groups of WebObjects that attract the Web users attention X  X .

Key Objects can be considered as elements on a given Website that provide knowledge of both content and formats that appear interesting to end-users. Enhancements can be made in presenta-tion as well as in content when Key Objects are identified, and used to improve the structure of a Website.

In order to accomplish this, and to propose which objects are the ones that must be taken into account when a given Website is re-engineered, the extraction of Key Objects problem must be solved. Definition 3 ( Website Key Object Extraction ). The Website Key Object (WSKO) Extraction problem is defined as setting an order relation $ KO between a list of WebObjects from a given Website, taking into account the relevance of WebObjects for the Website users.

In order to achieve an accurate WSKO extraction, the Website can be represented as a core-ontology ( Stumme et al., 2006 ), from which concepts and its relations will be used as input to the WSKO extraction process.

In general terms, the Key Objects can be represented as an order relation from the end-user perspective, where each object X  X  relevance is inferred from the usage of the given Website. Definition 4 ( Key Objects core ontology ). According to Stumme et al. (2006) , a Website Key Objects X  core ontology is represented by the tuple O :  X  X  C , r C , s , r R , A  X  with the following properties: A set of concepts defined by the set C as,
The concept hierarchy (or organizational part-of hierarchy) interpreted that a given Website ( s ) is represented by Web-pages ( w ), each Webpage  X  w i A w  X  can be represented as the composition of WebObjects ( x ), and each WebObject ( x j characterized by a composition of WebConcepts ( c ).

Set of part-of relations R  X f r 1 , r 2 , r 3 g where r 1 r 2  X  page-has-object, and r 3  X  object-has-WebConcepts{ nn }.
The signature s is defined for relations r i A R according to s  X  r 1  X  X  X  s , w i  X  , s  X  r 2  X  X  X  w i , x j  X  , s  X  r 1
Relations X  hierarchy  X  r R  X  in this case will be represented by the identity, given the flat relation between concepts. Logical axioms are represented by the empty set A  X  | .
In this case, Logical axioms ar e not necessary given that the ontology concepts interaction will be considered as static, whose representation is sufficiently formalized by using an Resource Description Framework (RDF) ( Klyne and Carroll, 2004 )schema. Further developments on using the extracted Key Objects for decision making, such as the introduced by Chambers et al. (2006) could be considered as future work.

Previous ontology can be described as a graphical representa-tion, where the Website, Webpages, Webobjects, and their Web-Concepts are related according to the relation set R and their hierarchy r C , as shown in Fig. 1 .

The proposed ontology sets a common ground to characterize each object, independently from the original format from which it was created. By using this ontology, it is possible to make a pairwise conceptual comparison between objects, disregarding their original format. Overall, WebConcepts can be represented as keywords used by the Website administrator to define a given object. Likewise,
WebMetaConcepts will be considered as groups of keywords or categories, associated with a more general concept than the one X  X  used in the set of WebConcepts. However, WebMetaConcepts will not be considered in this work as part of the ontology and will be used as categories.
 The structure depicted in Fig. 1 can be represented as a Directed
Acyclic Graph which leads to a simple model of a given Website. This model allows to reflect the main interactions between the ontology X  X  components, such as the hierarchy between Websites, Webpages,
WebObjects, and WebConcepts. 3.2. Comparing WebObjects
In order to compare two WebObjects, we consider that each of them is represented by a group of concepts that defines a given object X  X  content. Considering this shared representation among all objects,adistanceforcomparingobjects do : j c jj c j - X  0 , 1 is proposed by using a given distance measure (e.g. an edit distance).
Given two objects x i and x j such that j x i j X  N and j x concept of the object x i . We need to find a representation in which all WebObjects can be compared. A compare function that could be used, but not limited to, is based on the edit distance between the pairwise alignment between WebConcepts of objects x i x , 8 x i , x j A x , i a j represented by Algorithm 1.
 Algorithm 1. Pairwise Concept Alignment Between Two WebObjects.
 Require : x i , x j A x , t
Ensure : Aligned objects { x i , x j } 1: seq  X  x i , x j  X   X  0 2: for c k A f x i -c g do 3: for c l A f x j -c g do 4: if c k .Equals( c l ) then 5: seq  X  x i , x j  X   X  seq  X  x i , x j  X  X  1 6: else if c k . Synonym( c l ) then 7: seq  X  x i , x j  X   X  seq  X  x i , x j  X  X  0 : 5 8: end if 9: if seq  X  x i , x j  X  4 t then 10: align  X  x i , x j , k , l  X   X  Pair concept c k with c 11: end if 12: end for 13: end for
Once all the concepts are paired, the object concepts are ordered in such a way that every concept is in the same relative position in relation to each object. Then a string that consists of a symbol representing each concept is created to represent each object. This string has the structure shown in the expression (1), x  X  WebConcept 1 , ... , WebConcept N ) x  X  c 1 , ... , c where c k A f x i -c g represents a set of all concepts in object x
Equals and Synonym functions are defined according to a word comparison whose outputs are True if compared words are equals or synonyms, or its output is otherwise False . each representing a certain category, two objects may be compared by using an edit distance. The diffe rences noted by using this distance will introduce a notion of conceptu al similarity between each object as their symbols represent their fundamental concepts. For this, the idea is to pair the most similar concepts between each object and then compare the objects based on a comparison of this paring. 3.3. Sessionization and approximated time spent in WebObject behavior from Weblog analysis through the application of a sessionization process to the Weblog. The process applied is taken from Vela  X  squez (to appear) , where he combines the approach proposed by Berendt and Spiliopoulou (2001) , with a stemming process ( Porter, 1980 ) of the Website.
 page a host requested and a timestamp for the request. By reconstructing the user sessions, it is possible to determine how much time each user spends on a given Webpage. However, it is not possible to determine how much time that user spends in a certain object within that page.
 user spends an equal amount of time in each object that defines a page. If this assumption is made, the analysis that could define
Website Key Objects would be merely the analysis of pages browsed by users and a definition drawn on the basis of the most popular objects. To avoid this, an approximation of the time spent by each user is obtained by making a survey over a controlled group of users. The purpose of this survey is to analyze which objects where more appealing to users in each page, so a grade was awarded to every object in a given Webpage.
 interesting within a certain page for each user. Using this and weighing it with the time spent by every user on that particular page gives an approximation of the time spent by users in every object within a Webpage. 3.4. Important Object Vector Visitor Vector (OVV) whose components store the objects visited and time spent by the user during his/her session, can be defined as follows,
Definition 5 ( Object Visitor Vector (OVV) ). y  X  X  X  ~ x 1 , ~ T 1  X  , ... ,  X  ~ x n , ~ T n  X  by the user during the session seeing each object.
 By selecting the i objects from OOV, the Important Object Vector (IOV) is created as shown in Definition 6.
 Definition 6 ( Important Object Vector (IOV) ).

C  X  y  X  X  X  X  l 1 , m 1  X  , ... ,  X  l i , m i  X  where  X  l i , m i  X  is the component that represents the i th most important objects and the percentage of time spent on it by session.

Let d , g be two IOV, a similarity measure between them is calculated by using the following expression: so  X  d , g  X  X  1 i where do : j c jj c j - X  0 , 1 is the similarity between two WebOb-jects (e.g. an edit distance). 3.5. User sessions clustering
Different clustering techniques can be applied to create user session clustering. In this work, two algorithms will be used and will be cross-checked to prove that the created clusters are similar. These algorithms are Kohonen X  X  Self-Organizing Feature
Maps (SOFM) ( Kohonen et al., 2001 ) and k -Means ( MacQueen, 1967 ; Hartigan and Wong, 1979 ).

The SOFM machine learning algorithm is a special type of neural network where a typically two-dimensional grid of neu-rons is ordered so it reflects changes made in the n -dimensional vector that neurons represent. In this particular case, these vectors can be considered as IOVs. SOFM works with the concept of neighborhoods among neurons, where within the grid, some neurons are considered as neighbors and furthermore changes in one neuron will affect their neighbors.

SOFM requires rules for updating the weights of neurons, this is achieved generically by rule (3), m  X  t  X  1  X  X  m i  X  t  X  X  h ci  X  t  X  X  x  X  t  X  m i  X  t  X  X  3  X  where m ( n ) is the weight, h ( n ) is a monotone decreasing function depending on the radius of the neighborhood, and x ( t ) is the example presented to the network. In our case, special considera-tion should be taken for IOVs composed by times and objects. The latter cannot be weighted straightforward, for which a vector of differences between objects in a SOFM X  X  neuron is given (Eq. (4)),
D  X f do  X  x i , x j  X g N , M i , j  X  1 , i a j  X  4  X  3.6. Website Key Object Extraction Finally, according to previously described terms, the Website Key Object Extraction process is introduced in Algorithm 2. Algorithm 2. Website WebObject Extraction.
 Require: x , T
Ensure: $ KO 1: align ( x ) according to Algorithm 1 2: Compute D according to Eq. (4) 3: Using T  X f T 1 , ... , T n g , determine OVVs and IOVs according 4: Clusterize objects according to D and similarity measure 5: for each cluster k A K do 6: for each x i A k do 7: count i  X  count i  X  1 8: end for 9: end for 10: $ KO  X  Ordered list according to count i , 8 i A f 1 , ... , j x jg
As described in Algorithm 2, for a given Website, an initial ontology learning step must be realized. In this work, the core ontology proposed is based on a simple representation of the Website according to their Webpages, WebObjects, and WebCon-cepts for each object. This can be developed by using both automatic or manual processing, whose structure can be easily represented by an XML schema or an RDF-like ontology repre-sentation. Afterwards, all objects are compared with their respec-tive concepts by using a pairwise alignment procedure, and then an edit distance can be computed (e.g. Levenshtein, 1966 ) and the Levenshtein distance is computed.

Once the distance matrix between all objects is computed, by using the Webpages X  Weblogs, all objects are grouped together according to their relevance for the end-users. Finally, after con-cluding the clustering of different objects, their respective frequency was determined by clustering algorithms. This methodology X  X  per-formance can be tested against a su rvey where different end-users vote for the most relevant Objects within each Webpage. 4. Practical application
The site chosen to test the proposed WSKO Extraction approach belongs to a Chilean geographical information systems service provider, known as DMapas. 2 The site is written comple-tely in Spanish and is composed by 27 static Webpages. Its content is represented by free-text, images, and flash animations. Weblogs correspond to the month of June 2007, composed of 31.756 requests. In particular, this site has the following characteristics:
All pages address different information, and if two pages share similar information it is presented with a different focus.
The users are interested in a certain set of pages and not interested in the remainder.

The Website is maintained by a Webmaster who can choose if a page stays in the site based primarily on its success to attract users attention.

Given this, the Website ontological representation of objects, the similarity between them, the sessionization process, the approximate time spent in each object, the clustering process, and the extracted Website Key Objects are presented with their respective results and discussion as follows.
 As described in Section 3, for the given Website (in this case DMapas), an initial ontology learning step must be realized. It can be developed by using a manual process performed by Web-masters and experts on this Website. This core ontology was translated into an RDF-like representation. Afterwards, all objects were compared with their respective concepts, which were defined by end-users, by using a pairwise alignment procedure, and then the Levenshtein distance is computed. Once the distance matrix between all objects are computed, by using the Webpages X 
Weblogs, all objects are grouped together according to their relevance according to end-users. Finally, their frequency on different clusters determined by clustering algorithms, and Web-site Key Objects can were retrieved.

The complete evaluation of previously described steps, as well as results obtained, and discussion of relevant points, are exten-sively presented in the following section. 4.1. Site objects and ontological representation
The site has 40 objects, out of which 26 are composed of free text within tables, 11 as images and three as flash animations.
Three hundred and forty-four WebConcepts were associated with these objects and the WebConcepts categorized into one of the 12 categories created (WebMetaConcepts). Depending on the context in which an object is positioned, two of their defining concepts can belong to different categories even if they are identical.
The Web ontology is represented by an XML schema, whose structure is based on the ontology for the DMapas Website ( Fig. 2 ). An example of this representation is presented as follows.
This representation was extended over the 40 WebObjects in all 27 Webpages of the selected Website. This XML representation supported the core ontology O described in Section 3.1.2, and was built completely manually. For further applications in large-scale
Websites, automatic ontology learning and extraction ( Tsoi et al., 2009 ), among other ontology engineering techniques ( Poon and
Domingos, to appear ), could be considered as an extension of this work.
 process began proving that the similarity measure created was suitable enough to compare two objects at a conceptual level. This proof took into consideration a dataset of four objects, two of which where flash animations depicting demos of Geographic
Information Systems (GIS) solutions that the Company provides, the remaining two where free text within tables. The first describes GIS from a technical point of view, defining what they are and how they operate. The other shows information about the company, their owners, and employees.
 which shows how similar these objects are from the users X  point of view. The tests that were performed given as results in Table 1 , consisted of similarities in the range [0,1] where do  X  x means that objects x i and x j are identical.
 calculations are correct, and provides results according to the conceptual similarities given by experts. 4.2. Sessionization process language using a reactive strategy ( Vela  X  squez and Palade, 2008 ) o ?xml version  X  }1 : 0} encoding  X  }UTF 8} ? 4 o xsd : schema targetNamespace  X  }http : == www : dmapas : com = core} o = xsd : schema 4 which was limited to 30 min per session. The process considered replacing the pages with objects leading to an expansion of the
Weblog. This was considered due to the fact that requests for one page can represent one or more objects with their corresponding times spent by users, provided by the results of the survey. The result of the sessionization process provided 12.608 sessions.
After this, all the objects with navigation time zero were elimi-nated, resulting in 5.866 sessions over 19.282 requests. This provided an average of 3.29 objects per session.

To create the IOV, the number of requests considered in each session was calculated by taking the mean number of objects per session and adding the standard deviation which lead to all sessions having six or more requests. Only 815 out of the 12.608 sessions applied to this constraint, which was softened to all sessions consisting of five or more requests. Therefore, the final number of used sessions was 1.463. 4.3. Approximated time spent in each WebObject
A survey was taken over a group of 10 users, two of them were the experts who had an extensive knowledge of the Website, four users were DMapas customers who had visited the Website but had a partial knowledge of it, the rest were new users who had seen the site for the first time. This ensured that a diverse users were surveyed. Before the survey was taken, every user was introduced to every object in the Web page, and all their characteristics were clearly defined and explained.
 After the objects were introduced, three questions were asked.
The first question for each user was  X  X  X hich object was the most appealing to you within the whole Website? X  X , the second ques-tion was to make a top 20 list with all the objects of the site being the first most appealing for them. Finally for each page which had two or more objects, 10 points must be awarded between all of them having the most points the object which was the most appealing to the user within a certain page. Table 2 is an example about the answers given by one user. It was applied on Webpages with two or more objects, because in the case one of object per page, the assumption is the object concentrates its whole atten-tion of the user during his/her visit to the page, i.e., the time spent in the page is the same that in the object.

Information in Table 2 is interpreted as follows: one page can contain two or more objects (until four), the column Favorite shows the favorite object. Regarding the last four columns, each score was given by users. For instance, the page ID 17 has the objects IDs 15 and 16. From these two objects, the favorite one for the user is the ID 15 because 8 points were given, where only 2 points were given to object ID 16.

By using the survey X  X  information, the average of points assigned per WebObject were calculated. This information was used to distribute the time spent per page by user, obtained from Weblogs, over all objects during the user session according to their weights. After applying the sessionization process and by using the objects X  weights, both Object Visited Vectors (OVVs) and Important Object Vectors (IOVs) can be created and used as input for clustering algorithms. 4.4. Clustering process
The clustering algorithms developed in this work were SOFM and k -Means. Both algorithms were implemented over an Intel T2300 Core Duo running at 1.63 GHz with 1GB RAM on Windows XP operational system. The SOF M network was implemented in Python using f 12 12 , 14 14 , 18 18 , 24 24 g neurons in the chart with a toroidal topology, The k -Means algorithm was implemented in 1.42 using the number of clusters extracted from the best SOFM representation. In terms of comput ational time, by using this archi-tecture and programming languages, the SOFM algorithm ran approximately 2 h, while k -Means took approximately 15 min. 4.4.1. Clustering results
The main algorithm for obtaining the clustering over WebOb-jects was SOFM. However, these results were cross-checked with k -Means. By using SOFM, in the best clustering approach for the 24 24 neurons architecture, nine clusters were clearly identified as illustrated in Fig. 3 .

WebObjects that belonged to each cluster for the 24 24 neurons SOFM can be seen in Table 3 .

It is important to notice that each cluster was formed by either one or two neurons in the chart so they may have 5 or 10 objects that represent them. In some cases an object can appear twice in a cluster. Then each of the objects was labelled with its main concept, and then, a label for each cluster was created, which described the main content for each cluster shown in Table 3 .
Furthermore, it can be inferred from Table 3 that all objects in cluster 9 are related to technical i nformation about GIS and carto-graphy that DMapas company provides. Analogously, the remained clusters where labelled according t o the main content of the objects they contain. This result was checked using the k -Means algorithm, which discovered five well-defined clusters, presented in Table 4 .
Five clusters ( k  X  5) were chosen according to the evaluation of k
A  X  2 , 12 . The evaluation began k  X  12, which is the number of clusters generated by the SOFM algorithm, plus a slack of +3.
Then, the k parameter evaluated was decreased until all the clusters found were acceptable in terms of their interpretation. 4.5. Website Key Objects
In order to discover the Website Key Objects, all objects present in every cluster were counted. These results are presented in Table 5 , where objects which appeared the most were con-sidered to be the Website Key Objects.
 text format, two as flash animations, and only one corresponds to an image. They focus on a small part of the company X  X  site omitting a large quantity of information that administrators assumed to be very useful to users.
 4.6. Verification of Website Key Objects
The accuracy of the extracted Key Objects by the algorithm was proved by comparing the results with a survey taken by a controlled group of users. They looked at five pages, two of them with Website Key Objects, and the others were extracted ran-domly from the site. Next, users were asked to answer which of the five Webpages shown was the most appealing for them. Table 6 shows the results of the Website Key Object effectiveness.

Web users showed a positive receptivity towards pages con-taining Key Objects, considering them explicitly interesting and with relevant information. This means that for these users, WebObjects showed significant information, demonstrating that
Website Key Objects can be used for attracting the Web user X  X  attention.

Finally, as a benchmark, Web users were asked about which objects shown in Webpages can be considered as more relevant. These results are shown in Table 7 .

When comparing results of this survey with the algorithm X  X  extracted Key Objects, a difference of only two objects is detected, which leads to a match of 80% between the algorithm X  X  detected objects and those preferred by the controlled group. If the analysis is extended to the top 15 objects, the accuracy rises to 87%, which shows a positive relation between the algorithm and the survey results. 5. Conclusions In this work, a methodology for identifying Website Key
Objects is introduced. Website Key Objects are the most appe-aling objects for users within a Website. This methodology is a generalization of a prior developed by Vela  X  squez et al. (2005) for identifying Website Keywords. Our approach is based on the fact that there is a correlation between the time spent by a user in a certain page during a session and the interest the user has in its content.

In order to develop this methodology a definition of a WebOb-ject was created, and particularly a definition for Website Key Objects, which are those objects in a Website that drives the attention of users. The definition of these objects enables the characterization of the conceptual content represented by a simple core ontology. The Website Key Object Extraction Problem (WSKOP) is aimed towards the definition of a new relation between the core ontology X  X  WebObjects. This relation is an ordered list of Website Key Objects, according to the Web user preferences.

This characterization delivers a common ground over which any object can be defined with no restrictions regarding the format in which it is presented to the user. This allows a clear conceptual definition for each object. Additionally, by using this ontology a similarity measure was introduced which enables a quantifiable conceptual comparison between two WebObjects, even though these might not share the same format.

The proposed methodology was applied in a real Website, for which its Key Objects were extracted, whose results were com-pared with Web user surveys. Results showed that our approach is similar by at least 80% of those Website Key Objects described by Web users, which leads us to the conclusion that an automatic approach which is scalable and easy to implement could enhance the Webmaster X  X  labor on what information and what format should be considered to update a given Website.

The knowledge acquired from the application of the metho-dology to a real site allows a Webmaster to know the preferences of the user base. It also enables the possibility of enhancing the Website by empowering the information that users are looking for and also presenting it in an appealing format. 5.1. Future work
The methodology to discovering Website Key Objects relies heavily on two factors, an ontology used to define WebObjects and an approximation used to determine how much time a user spent looking for a certain object within a Webpage. By using privacy-preserving data mining, an end-user profiling algorithm could be included to improve the modelling and inclusion of different types of users to extract a more representative list of Key Objects. Also, the information gathered by eye-tracking devises could be included as Web usage data to extend the possibility to analyze more complex Websites.

Also, in this work, all metadata was created manually mainly taking into consideration that the test Website was composed of static pages and that its cardinality was small. The model used in this work is a relatively basic and easy to implement ontology.
The strong development of metadata applied to the Web allows for the creation of more advanced metadata models which enables the creation of a more complex a nd expressive ontology (e.g. considering the hierarchy between WebConcepts). This would result in a more precise definition of an object which would lead to a more precise comparison between two objects. Furthermore, in this work metadata was incorporated manually to the test Website. This could be made in reasonable time because the test Website of a limited size and of static nature. If a larger or dynamical site were used this task would have been time consuming.
 Acknowledgments This work was supported partially by the FONDEF project DO8I-1015 and the Millennium Institute on Complex Engineering
Systems (ICM: P-05-004-F, CONICYT: FBO16). Authors would like to thank anonymous reviewers for their interesting and useful comments on the preliminary version of this article.
 References
