 Datasets available for prediction tasks are growing over time, resulting in increasing scale in all their measurable dimensions: separate from the issue of the growing number of examples m and features d , they are also growing in the number of classes k . Current multi-class applications such as web advertising [6], textual document categorization [11] or image annotation [12] have tens or hundreds of thousands of classes, and these datasets are still growing . This evolution is challenging traditional approaches [1] whose test time grows at least linearly with k .
 At training time, a practical constraint is that learning should be feasible, i.e. it should not take more than a few days, and must work with the memory and disk space requirements of the available hardware. Most algorithms X  training time, at best, linearly increases with m , d and k ; algorithms that are quadratic or worse with respect to m or d are usually discarded by practitioners working on real large scale tasks. At testing time, depending on the application, very specific time constraints are necessary, usually measured in milliseconds, for example when a real-time response is required or a large number of records need to be processed. Moreover, memory usage restrictions may also apply. Classical approaches such as One-vs-Rest are at least O ( kd ) in both speed (of testing a single example) and memory. This is prohibitive for large scale problems [6, 12, 26].
 In this work, we focus on algorithms that have a classification speed sublinear at testing time in k as well as having limited dependence on d with best-case complexity O ( d e (log k + d )) with d e d and d e k . In experiments we observe no loss in accuracy compared to methods that are O ( kd ) , further, memory consumption is reduced from O ( kd ) to O ( kd e ) . Our approach rests on two main ideas: firstly, an algorithm for learning a label tree : each node makes a prediction of the subset of labels to be considered by its children, thus decreasing the number of labels k at a logarithmic rate until a prediction is reached. We provide a novel algorithm that both learns the sets of labels at each node, and the predictors at the nodes to optimize the overall tree loss, and show that this approach is superior to existing tree-based approaches [7, 6] which typically lose accuracy compared to O ( kd ) approaches. Balanced label trees have O ( d log k ) complexity as the predictor at each node is still Algorithm 1 Label Tree Prediction Algorithm Input: test example x, parameters T .

Let s = 0 . -Start at the root node repeat until | ` s | = 1 -Until this uniquely defines a single label.

Return ` s . linear in d . Our second main idea is to learn an embedding of the labels into a space of dimension d e that again still optimizes the overall tree loss. Hence, we are required at test time to: (1) map the test example in the label embedding space with cost O ( dd e ) and then (2) predict using the label tree resulting in our overall cost O ( d e (log k + d )) . We also show that our label embedding approach outperforms other recently proposed label embedding approaches such as compressed sensing [17]. The rest of the paper is organized as follows. Label trees are discussed and label tree learning algorithms are proposed in Section 2. Label embeddings are presented in Section 3. Related prior work is presented in Section 4. An experimental study on three large tasks is given in Section 5 showing the good performance of our proposed techniques. Finally, Section 6 concludes. A label tree is a tree T = ( N, E, F, L ) with n +1 indexed nodes N = { 0 , . . . n } , a set of edges E = labeled with index 0. The edges E are such that all other nodes have one parent, but they can have an arbitrary number of children (but still in all cases | E | = n ). The label sets indicate the set of labels to which a point should belong if it arrives at the given node, and progress from generic to where there are only k leaf nodes, one per class, and hence any two nodes i and j at the same depth cannot share any labels, ` i  X  ` j =  X  , and joint label trees that can have more than k leaf nodes. Classifying an example with the label tree is achieved by applying Algorithm 1. Prediction begins at the root node ( s = 0 ) and for each edge leading to a child ( s, c )  X  E one computes the score of the label predictor f c ( x ) which predicts whether the example x belongs to the set of labels ` c . One takes the most confident prediction, traverses to that child node, and then repeats the process. Classification is complete when one arrives at a node that identifies only a single label, which is the predicted class.
 Instances of label trees have been used in the literature before with various methods for choosing the parameters ( N, E, F, L ) . Due to the difficulty of learning, many methods make approximations such as a random choice of E and optimization of F that does not take into account the overall loss of the entire system leading to suboptimal performance (see [7] for a discussion). Our goal is to provide an algorithm to learn these parameters to optimize the overall empirical loss (called the tree loss ) as accurately as possible for a given tree size (speed).
 We can define the tree loss we wish to minimize as: where I is the indicator function and of the final prediction for x , i.e. the number of loops plus one of the repeat block when running Algorithm 1. The tree loss measures an intermediate loss of 1 for each prediction at each depth j of the label tree where the true label is not in the label set ` b is the max over these losses, because if any one of these classifiers makes a mistake then regardless of the other predictions the wrong class will still be predicted. Hence, any algorithm wishing to optimize the overall tree loss should train all the nodes jointly with respect to this maximum. We will now describe how we propose to learn the parameters T of our label tree. In the next subsection we show how to minimize the tree loss for a given fixed tree ( N, E and L are fixed, F is to be learned). In the following subsection, we will describe our algorithm for learning N, E and L . 2.1 Learning with a Fixed Label Tree Let us suppose we are given a fixed label tree N, E, L chosen in advance. Our goal is simply to standard approach of minimizing the empirical loss over the data, while regularizing our solution. We consider two possible algorithms for solving this problem.
 Relaxation 1: Independent convex problems The simplest (and poorest) procedure is to consider the following relaxation to this problem: where C j ( y ) = 1 if y  X  ` j and -1 otherwise. The number of errors counted by the approximation cannot be less than the empirical tree loss R emp as when, for a particular example, the loss is zero for the approximation it is also zero for R emp . However, the approximation can be much larger because of the sum.
 One then further approximates this by replacing the indicator function with the hinge loss and choos-convex problem: minimize where we also added a classical 2-norm regularizer controlled by the hyperparameter  X  . In fact, this can be split into n independent convex problems because the hyperplanes w i , i = 1 , . . . , n , do not interact in the objective function. We consider this simple relaxation as a baseline approach. Relaxation 2: Tree Loss Optimization (Joint convex problem) We propose a tighter minimization of the tree loss with the following: When  X  is close to zero, the shared slack variables simply count a single error if any of the pre-dictions at any depth of the tree are incorrect, so this is very close to the true optimization of the tree loss. This is measured by checking, out of all the nodes that share the same parent, if the one containing the true label in its label set is highest ranked. In practice we set  X  = 1 and arrive at a convex optimization problem. Nevertheless, unlike relaxation (1) the max is not approximated with a sum. Again, using the hinge loss and a 2-norm regularizer, we arrive at our final optimization problem: subject to constraints (2) and (3). 2.2 Learning Label Tree Structures The previous section shows how to optimize the label predictors F while the nodes N , edges E and label sets L which specify the structure of the tree are fixed in advance. However, we want to be able to learn specific tree structures dependent on our prediction problem such that we minimize the Algorithm 2 Learning the Label Tree Structure Train k One-vs-Rest classifiers  X  f 1 , . . . ,  X  f k independently (no tree structure is used).
For each internal node l of the tree, from root to leaf, partition its label set ` l between its chil-maximizing: subject to constraints preventing trivial solutions, e.g. putting all labels in one set (see [4]).
This optimization problem (including the appropriate constraints) is a graph cut problem and it can be solved with standard spectral clustering, i.e. we use A as the affinity matrix for step 1 of the algorithm given in [21], and then apply all of its other steps (2-6).

Learn the parameters f of the tree by minimizing (4) subject to constraints (2) and (3). overall tree loss. This section describes an algorithm for learning the parameters N , E and L , i.e. optimizing equation (1) with respect to these parameters.
 The key to the generalization ability of a particular choice of tree structure is the learnability of the label sets ` . If some classes are often confused but are in different label sets the functions f may not be easily learnable, and the overall tree loss will hence be poor. For example for an image labeling task, a decision in the tree between two label sets, one containing tiger and jaguar labels versus one containing frog and toad labels is presumably more learnable than (tiger, frog) vs. (jaguar, toad). In the following, we consider a learning strategy for disjoint label trees (the methods in the previous section were for both joint and disjoint trees). We begin by noticing that R emp can be rewritten as: y , but we predict a different node at the same depth leading to a prediction not in the label set of j . Intuitively, the confusion of predicting node i instead of j comes about because of the class confusion between the labels y  X  ` i and the labels  X  y  X  ` j . Hence, to provide the smallest tree loss we want to group together labels into the same label set that are likely to be confused at test time. Unfortunately we do not know the confusion matrix of a particular tree without training it first, but as a proxy we can use the class confusion matrix of a surrogate classifier with the supposition that the matrices will be highly correlated. This motivates the proposed Algorithm 2. The main idea is to recursively partition the labels into label sets between which there is little confusion (measuring confusion using One-vs-Rest as a surrogate classifier) solving at each step a graph cut problem where standard spectral clustering is applied [20, 21]. The objective function of spectral clustering penalizes unbalanced partitions, hence encouraging balanced trees. (To obtain logarithmic speed-ups the tree has to be balanced; one could also enforce this constraint directly in the k -means step.) The results in Section 5 show that our learnt trees outperform random structures and in fact match the accuracy of not using a tree at all, while being orders of magnitude faster. An orthogonal angle of attack of the solution of large multi-class problems is to employ shared representations for the labelings, which we term label embeddings. Introducing the function  X  ( y ) = we would like to find a linear embedding E ( y ) = V  X  ( y ) where V is a d e  X  k matrix assuming that labels y  X  X  1 , . . . , k } . Without a tree structure, multi-class classification is then achieved with: where W is a d e  X  d matrix of parameters and S (  X  ,  X  ) is a measure of similarity, e.g. an inner product or negative Euclidean distance. This method, unlike label trees, is unfortunately still linear with respect to k . However, it does have better behavior with respect to the feature dimension d , with O ( d e ( d + k )) testing time, compared to methods such as One-vs-Rest which is O ( kd ) . If the embedding dimension d e is much smaller than d this gives a significant saving.
 There are several ways we could train such models. For example, the method of compressed sensing [17] has a similar form to (5), but the matrix V is not learnt but chosen randomly, and only W is learnt. In the next section we will show how we can train such models so that the matrix V captures the semantic similarity between classes, which can improve generalization performance over random choices of V in an analogous way to the improvement of label trees over random trees. Subsequently, we will show how to combine label embeddings with label trees to gain the advantages of both approaches. 3.1 Learning Label Embeddings (Without a Tree) We consider two possibilities for learning V and W .
 Sequence of Convex Problems Firstly, we consider learning the label embedding by solving a se-quence of convex problems using the following method. First, train independent (convex) classifiers f same as the first two steps of Algorithm 2. Then, find the label embedding vectors V i that minimize: subject to the constraint V &gt; DV = I where D ii = P j A ij (to prevent trivial solutions) which is the same problem solved by Laplacian Eigenmaps [4]. We then obtain an embedding matrix V where similar classes i and j should have small distance between their vectors V i and V j . All that remains is to learn the parameters W of our model. To do this, we can then train a convex multi-class classifier utilizing the label embedding V : minimize where || . || F RO is the Frobenius norm, subject to constraints: Note that the constraint (6) is linear as we can multiply out and subtract || W x i || 2 from both sides. At test time we employ equation (5) with S ( z, z 0 ) =  X  X | z  X  z 0 || .
 Non-Convex Joint Optimization The second method is to learn W and V jointly, which requires non-convex optimization. In that case we wish to directly minimize: and || V i || X  1 ,  X  i  X  0 , i = 1 , . . . , m . We optimize this using stochastic gradient descent (with randomly initialized weights) [8]. At test time we employ equation (5) with S ( z, z 0 ) = z &gt; z 0 . 3.2 Learning Label Embedding Trees In this work, we also propose to combine the use of embeddings and label trees to obtain the ad-vantages of both approaches, which we call the label embedding tree . At test time, the resulting label embedding tree prediction is given in Algorithm 3. The label embedding tree has potentially O ( d e ( d + log ( k ))) testing speed, depending on the structure of the tree (e.g. being balanced). Algorithm 3 Label Embedding Tree Prediction Algorithm Input: test example x, parameters T .
 Compute z = W x . -Cache prediction on example
Let s = 0 . -Start at the root node repeat -Traverse to the most until | ` s | = 1 -Until this uniquely defines a single label.
 Return ` s .
 To learn a label embedding tree we propose the following minimization problem: subject to constraints: This is essentially a combination of the optimization problems defined in the previous two Sections. Learning the tree structure for these models can still be achieved using Algorithm 2. Multi-class classification is a well studied problem. Most of the prior approaches build upon binary classification and have a classification cost which grows at least linearly with the number of classes k . Common multi-class strategies include one-versus-rest, one-versus-one, label ranking and Deci-sion Directed Acyclic Graph (DDAG). One-versus-rest [25] trains k binary classifiers discriminating each class against the rest and predicts the class whose classifier is the most confident, which yields a linear testing cost O ( k ) . One-versus-one [16] trains a binary classifier for each pair of classes and predicts the class getting the most pairwise preferences, which yields a quadratic testing cost should get the highest score, which yields a linear testing cost O ( k ) . DDAG [23] considers the same are reported to perform similarly in terms of accuracy [25, 23].
 Only a few prior techniques achieve sub-linear testing cost. One way is to simply remove labels the classifier performs poorly on [11]. Error correcting code approaches [13] on the other hand represent each class with a binary code and learn a binary classifier to predict each bit. This means that the testing cost could potentially be O (log k ) . However, in practice, these approaches need larger redundant codes to reach competitive performance levels [19]. Decision trees, such as C4.5 [24], can also yield a tree whose depth (and hence test cost) is logarithmic in k . However, testing complexity also grows linearly with the number of training examples making these methods impractical for large datasets [22].
 Filter tree [7] and Conditional Probability Tree (CPT) [6] are logarithmic approaches that have been introduced recently with motivations similar to ours, i.e. addressing large scale problems with a thousand classes or more. Filter tree considers a random binary tree in which each leaf is associated with a class and each node is associated with a binary classifier. A test example traverses the tree from the root. At each node, the node classifier decides whether the example is directed to the right or to the left subtree, each of which are associated to half of the labels of the parent node. Finally, the label of the reached leaf is predicted. Conditional Probability Tree (CPT) relies on a similar paradigm but builds the tree during training. CPT considers an online setup in which the set of classes is discovered during training. Hence, CPT builds the tree greedily: when a new class is encountered, it is added by splitting an existing leaf. In our case, we consider that the set of classes are available prior to training and propose to tessellate the class label sets such that the node classifiers are likely to achieve high generalization performance. This contribution is shown to have a significant advantage in practice, see Section 5. Finally, we should mention that a related active area of research involves partitioning the feature space rather than the label space , e.g. using hierarchical experts [18], hashing [27] and kd-trees [5]. Label embedding is another key aspect of our work when it comes to efficiently handling thousands of classes. Recently, [26] proposed to exploit class taxonomies via embeddings by learning to project input vectors and classes into a common space such that the classes close in the taxonomy should have similar representations while, at the same time, examples should be projected close to their class representation. In our case, we do not rely on a pre-existing taxonomy: we also would like to assign similar representations to similar classes but solely relying on the training data. In that respect, our work is closer to work in information retrieval [3], which proposes to embed documents  X  not classes  X  for the task of document ranking. Compressed sensing based approaches [17] do propose to embed class labels, but rely on a random projection for embedding the vector representing class memberships, with the added advantages of handling problems for which multiple classes are active for a given example. However, relying on a random projection does not allow for the class embedding to capture the relation between classes. In our experiments, this aspect is shown to be a drawback, see Section 5. Finally, the authors of [2] do propose an embedding approach over class labels, but it is not clear to us if their approach is scalable to our setting. We consider three datasets: one publicly available image annotation dataset and two proprietary datasets based on images and textual descriptions of products.
 ImageNet Dataset ImageNet [12] is a new image dataset organized according to WordNet [14] where quality-controlled human-verified images are tagged with labels. We consider the task of annotating images from a set of about 16 thousand labels. We split the data into 2.5M images for training, 0.8M for validation and 0.8M for testing, removing duplicates between training, validation and test sets by throwing away test examples which had too close a nearest neighbor training or validation example in feature space. Images in this database were represented by a large but sparse vector of color and texture features, known as visual terms, described in [15].
 Product Datasets We had access to a large proprietary database of about 0.5M product descriptions. Each product is associated with a textual description, an image, and a label. There are  X  18 thousand unique labels. We consider two tasks: predicting the label given the textual description, and predict-ing the label given the image. For the text task we extracted the most frequent set of 10 thousand words (discounting stop words) to yield a textual dictionary, and represented each document by a vector of counts of these words in the document, normalized using tf-idf. For the image task, images were represented by a dense vector of 1024 real values of texture and color features.
 Table 1 summarizes the various datasets. Next, we describe the approaches that we compared. Flat versus Tree Learning Approaches In Table 2 we compare label tree predictor training meth-ods from Section 2.1: the baseline relaxation 1 ( X  X ndependent Optimization X ) versus our proposed relaxation 2 ( X  X ree Loss Optimization X ), both of which learn the classifiers for fixed trees; and we compare our  X  X earnt Label Tree X  structure learning algorithm from Section 2.2 to random struc-tures. In all cases we considered disjoint trees of depth 2 with 200 internal nodes. The results show that learnt structure performs better than random structure and tree loss optimization is superior to independent optimization . We also compare to three other baselines: One-vs-Rest large margin classifiers trained using the passive aggressive algorithm [9], the Filter Tree [7] and the Conditional Probability Tree (CPT) [6]. For all algorithms, hyperparameters are chosen using the validation set. The combination of Learnt Label Tree structure and Tree Loss Optimization for the label predictors is the only method that is comparable to or better than One-vs-Rest while being around 60  X  faster to compute at test time.
 For ImageNet one could wonder how well using WordNet (a graph of human annotated label sim-ilarities) to build a tree would perform instead. We constructed a matrix C for Algorithm 2 where C ij = 1 if there is an edge in the WordNet graph, and 0 otherwise, and used that to learn a label tree as before, obtaining 0.99% accuracy using  X  X ndependent Optimization X . This is better than a random tree but not as good as using the confusion matrix, implying that the best tree to use is the one adapted to the supervised task of interest. Table 2: Flat versus Tree Learning Results Test set accuracies for various tree and non-tree meth-ods on three datasets. Speed-ups compared to One-vs-Rest are given in brackets.
 Embedding and Embedding Tree Approaches In Table 3 we compare several label embedding methods: (i) the convex and non-convex methods from Section 5; (ii) compressed sensing; and (iii) the label embedding tree from Section 3.2. In all cases we fixed the embedding dimension d e = 100 . The results show that the random embeddings given by compressed sensing are inferior to learnt embeddings and Non-Convex Embedding is superior to Sequential Convex Embedding, presumably as the overall loss which is dependent on both W and V is jointly optimized. The latter gives results as good or superior to One-vs-Rest with modest computational gain (3  X  or 10  X  speed-up). Note, we do not detail results on the product descriptions task because no speed-up is gained there from embedding as the sparsity is already so high, however the methods still gave good test accuracy (e.g. Non-Convex Embedding yields 38.2%, which should be compared to the methods in Table 2). Finally, combining embedding and label tree learning using the  X  X abel Embedding Tree X  of Section 3.2 yields our best method on ImageNet and Product Images with a speed-up of 85  X  or 142  X  respectively with accuracy as good or better than any other method tested. Moreover, memory usage of this method (and other embedding methods) is significantly less than One-vs-Rest. We have introduced an approach for fast multi-class classification by learning label embedding trees by (approximately) optimizing the overall tree loss. Our approach obtained orders of magnitude speedup compared to One-vs-Rest while yielding as good or better accuracy, and outperformed other tree-based or embedding approaches. Our method makes real-time inference feasible for very large multi-class tasks such as web advertising, document categorization and image annotation. We thank Ameesh Makadia for very useful discussions.
