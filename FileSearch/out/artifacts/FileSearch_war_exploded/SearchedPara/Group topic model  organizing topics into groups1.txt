 Tian Tian Abstract Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.
 Keywords Topic modeling Latent Dirichlet allocation Group Variational inference Online learning Document clustering 1 Introduction Large text document collections have recently become readily available online. Systematic analyses of these collections are significantly meaningful to various domains. Consider, for example, scientific article archives. (1) We want to organize the articles by subject, and help users explore the archives. This is commonly a substantial multi-label classification ommend the most professional reviewers.

Although these problems have been well studied, they are still significantly challenging problems in machine learning research. This is because: (1) the text document collections semantics and any correlations between documents. Statistical modeling research has ments (Koller and Friedman 2009 ). In particular, topic modeling approaches (Blei 2012 ) have provided a realizable avenue for expressing the latent semantics and hidden structures of documents. As a result, these approaches have been widely used in different application domains.

Latent Dirichlet allocation (LDA) (Blei et al. 2003 ) is acknowledged as one of the most defines a Dirichlet prior beyond all the document-topic distributions, so it does not suffer semantic index (PLSI) approach (Hofmann 1999 ). Recently, researchers have proposed Chang and Blei 2010 ), and applying it to other kinds of data (Li and Perona 2005 ; Sivic et al. 2008 ).

In this paper, we focused on a topic modeling approach and investigated relaxing the assumptions of LDA. Intuitively, we know that larger document collections may contain more latent topics. To capture the latent semantics, all documents in LDA are represented puter-related topics, whereas chemistry articles prefer to cover the three chemistry-related to chemistry), as a reasonable method to tackle the  X  X  X orced topic X  X  problem.
By considering the discussions above, we developed an extension of the LDA model, namely group latent Dirichlet allocation (GLDA). In GLDA, there are two kinds of topics: local topics and global topics. A local topic corresponds to a topic that only occurs in the corpus. Closely related local topics are clustered together as latent groups. Each document corresponding topic-word distributions. Based on the latent groups, GLDA can model the documents using the most related topics, rather than constraining each document to all the GLDA. Additionally, we developed an online inference algorithm to model large-scale data. evaluate the proposed model. Our experimental results demonstrate that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.
 2 Topic modeling approach In this section, we review the history of topic modeling approaches. Table 1 summarizes several important notations used in this paper.
 model for latent semantic analysis (Deerwester et al. 1990 ). However, it suffers from two They also developed an effective variational inference algorithm to infer the model. As a result, LDA is in widespread use. As shown in Fig. 1 a, the generative process of LDA is summarized as follows: 1. For each topic k 2. For each document d in the corpus W further uncover more sophisticated structures in the documents. Traditionally, the exten-sions of LDA focus on four fundamental assumptions (Blei 2012 ):  X  X  X ag of words X  X ,  X  X  X ag of documents X  X ,  X  X  X ixed topics X  X , and  X  X  X ndependent topics X  X . 1.  X  X  X ag of words X  X  is an exchangeable assumption that the orders of words in documents 2.  X  X  X ag of documents X  X  is also an exchangeable assumption that the orders of documents 3. The  X  X  X ixed topics X  X  assumption signifies that the number of topics in LDA is fixed and There are other modified topic models that relax various assumptions with respect to LDA, for example, as the spherical topic model (Reisinger et al. 2010 ) and sparse topic unrealistic to force each document to associate with the same K topics. Considering this analysis, they proposed a cluster based topic model (CTM), which organizes topics into different groups and individualizes each group using the group-specific Dirichlet prior of the document-topic distribution. For each document, CTM first generates a group indicator group. Based on CTM, Xie and Xing ( 2013 ) further introduced global topics to capture the global semantics, and proposed the multi-grain cluster topic model (MGCTM). As shown in Fig. 1 b, for each word, MGCTM must select between local and global topics, and then generate local or global topics using its choice. In our work, we investigated how to relax the assumptions of LDA, and propose the GLDA model. The proposed GLDA model defines local topics specific to a group as a solution to the  X  X  X orced topic X  X  problem, and defines global topics to capture the background semantics. It samples the document-topic and the global prior. The GLDA representation is less ambiguous than MGCTM. More topics in terms of different groups. There are more detailed discussions in Sect. 3.5 . 3 Proposed approach In this section, we first introduce the GLDA model, and then propose the procedures for inference, parameter estimation and online learning. Finally, we compare MGCTM and GLDA in detail. 3.1 GLDA In LDA, all documents are represented by the same K topics. This results in the  X  X  X orced topic X  X  problem, which has two aspects: (1) in practice, the documents belonging to dif-ferent groups might only involve some topics, but they are forced to cover all topics (an example is shown in Sect. 1 ); and (2) LDA has no mechanism to cover the background semantics in a corpus, so the semantics must fill in each specific topic. For instance, the words that cover the background semantics are commonly ubiquitous and frequently occur duction X  X  to  X  X  X etwork X  X  and  X  X  X rganic chemistry X  X . This behavior reduces the expressiveness of the topics.
 To address the  X  X  X orced topic X  X  problem mentioned above, we extended LDA to the bution p , which can be used to generate the group indicator for the documents; (2) each To formalize the generation process for document d , we first choose a group indicator g d from the distribution p . We combine the local Dirichlet prior of group g d with the global Then, we sample the document-topic distribution h d over the local topics with respect to a . The words are then generated as in LDA.

As shown in Fig. 1 c, the generative process of GLDA is as follows: 1. For each topic k 2. For each document d in the corpus W
We can summarize model parameters as U  X  p ; a  X  l  X  c C c  X  1 ; a  X  g  X  ; b procedure to generate topics. For each document, if a group is chosen then only the topics in this group can be used to describe the document. On other hand, GLDA introduces the concept of global topics to gather the words that describe the background semantics. This helps to purify the specific topics. 3.2 Inference approximate estimation.

The basic idea behind variational inference is to use Jensen X  X  inequality to approach the tightest lower bound on the log likelihood. To achieve this, we introduced the variational and nodes in GLDA. That is, where ~ a d fg D d  X  1 and ~ b k multinomial distribution parameters.
We transformed the task of finding the tightest lower bound on the log likelihood into the problem of maximizing the following lower bound: which is described in the Appendix .

We use the fixed point method to maximize this lower bound with respect to the free updating rules are: gamma function; and W  X  X  is the digamma function Then, where The full variational inference procedure is summarized in Algorithm 1. 3.3 Parameter estimation In this section, we consider the parameter estimation for GLDA. Given a corpus, we wish to optimize the model parameters ( U ) using a maximum likelihood estimation. Again, the variational parameters ( X ) and model parameters ( U ).

Similar to Algorithm 1, the variational EM algorithm is summarized in Algorithm 2. In multinomial parameter p is updated using:
Comparison with asymmetric LDA GLDA organizes topics into groups. To specialize default an asymmetric model. It seems similar with the best version of asymmetric LDA, suggested in (Wallach et al. 2009a ), so we have stated the relationships between the two models. When inferring a document d , GLDA might equal to AS form LDA with a certain are totally different for different documents. In other words, we believe that GLDA and asymmetric LDA are two disparate models. 3.4 Online learning In this section, we extend Algorithm 2 to an online inference algorithm (Online GLDA) for inference (SVI) (Hoffman and Wang 2013 ; Hoffman and Blei 2010 ), where each iteration uses only a mini-batch of the documents to generate a stochastic gradient, and a stochastic optimization algorithm is used to learn the global parameters of interest.
 update the global parameters given a learning rate q t as follows: updating rule: 8 &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; : in (Hoffman and Blei 2010 ): r mization of p because vich 2003 ), when updating p : and ; hi is the inner product.

The Online GLDA is summarized in Algorithm 3. 3.5 Comparison with MGCTM MGCTM and GLDA have some similarities, so we have investigated the relationships between the two topic models. In fact, both represent documents using the local topics of the groups and global topics. However, the relationships between the two kind topics are different in MGCTM and GLDA.

A graphical model representation is shown in Fig. 1 b. We reviewed the generative process of MGCTM (Xie and Xing 2013 ) as follows: For each document d , first select a group g d from the distribution p . Then sample a local topic distribution h l g Dirichlet prior a  X  l  X  g h assigned a global topic. Finally a word is generated as in LDA.
Let pt g  X  k  X  X  be the probability of generating the k th global topic. In MGCTM pt g  X  k  X  X  X  p d d ; n  X  0 j x d pt g  X  k j h g  X  X  and its expectation is:
In contrast, GLDA samples the document-topic distribution from the combination Di-We can transform Eq. ( 12 ) into: Comparing this with Eqs. ( 11 ) and ( 13 ), we found that the second terms are the same. dependent on the different groups, which is not the case in MGCTM. We argue that the assumption in GLDA is reasonable. For example, computer science articles may be nat-urally more willing to cover common knowledge topics (i.e., global topics) than chemistry articles. In particular, we argue that this consideration is more significant when modeling collections that contain many latent groups. 4 Experiment In this section, we present our results when evaluating GLDA on two problem domains, i.e., topic modeling and document clustering. 4.1 Dataset We considered two widely used offline datasets: 1 20-NewsGroups (20-NG) and WebKB. 20-NG is a balanced dataset. It contains 18,821 documents, which are equally divided into 20 related categories. We used 11,293 documents as the training data, and the remaining 7,528 documents as the testing data. WebKB contains 4,199 documents, which consists of four categories. In contrast to 20-NG, it is an unbalanced dataset, where the largest cat-egory contains 1,641 documents and the smallest category only contains 504 documents. We selected 2,803 documents for training, and used the remaining 1,396 documents for testing.
 We also chose an online collection. We randomly downloaded 3M documents from Wikipedia (Wiki) using the implementation 2 in (Hoffman and Blei 2010 ). We then processed these documents using a standard vocabulary of 7,700 words. We used 2,000 randomly selected documents from the collection for testing. 4.2 Topic modeling We evaluated the topic modeling performance of GLDA across the three selected corpora. 2003 , CTM Wallach 2008 , and MGCTM Xie and Xing 2013 ) as performance baselines. We downloaded the public version of LDA 3 and implemented in-house codes for CTM and MGCTM. For fair comparisons, we estimated all of the hyper-parameters of these approaches using the variational EM method, and estimated the GLDA using Algorithm 2. In terms of the online collection (Wiki), we used Online LDA 2 (Hoffman and Blei 2010 )as the baseline and GLDA was estimated using Algorithm 3. All these models used the AS (Wallach et al. 2009a ) form (asymmetric topic Dirichlet prior and symmetric word the Newton X  X aphson algorithm (Blei et al. 2003 ). The symmetric word Dirichlet prior b was fixed at 0.01.

Naturally, we can consider the topic model as a special probability density function for generating a corpus. So the topic modeling performance can be evaluated by the likelihood perplexity scores of the held-out test data. The perplexity, used by convention in language modeling, is equivalent to the inverse of the geometric mean per-word likelihood. A lower perplexity represents a higher performance. Given corpora W and W test , the perplexity is defined as: 4.2.1 Qualitative evaluation We fit GLDA to two versions of the 20-NG datasets. One is the original 20-NG with stop words, and the other is a filtered 20-NG that has removed stop words 5 (378 in total). For both versions, we set C  X  20 ; Kl  X  5 and Kg  X  20.
 Table 2 illustrates the 10 most popular words for three global topics learnt by GLDA. The global topics learnt from the original 20-NG are almost filled by stop words. Because semantics. In other words, GLDA successfully captured the common semantics. The results are clearer for the filtered 20-NG. We observed that Global Topic 1 is about article writing; Global Topic 2 is about time; Global Topic 3 is about both writing and time. These global topics obviously show the common semantics, and can be generated in all documents.

Table 3 shows the local topics from two estimated groups learnt by GLDA. Obviously, system and network, respectively. In Group 2, the local topics are about sports, including baseball, hockey and game. Although the results for the original 20-NG were affected by semantics for each group.

Overall, we found that the two-stage generation of GLDA had a positive influence when capturing the semantics. On one hand, the local semantics were first organized at a coarse work). On the other hand, the common semantics were covered by the global topics. This framework effectively modeled the corpus, even with stop words. 4.2.2 Quantitative evaluation of offline collections We tested the perplexity scores of the offline collections with different numbers of topics. follows: For MGCTM and GLDA, we fixed C  X  20 and Kg  X  20, and set of total topics. For WebKB, in both MGCTM and GLDA, we fixed C  X  4 and Kg  X  32, total topics.
 The results for 20-NG are shown in Fig. 3 . GLDA performed better than LDA and CTM. For LDA, there was a conflict. A larger K (more topics) is required to uncover the complex semantics in large document collections, but many documents naturally only Our experimental results confirmed this analysis. The LDA performed better when K  X  40, and the performance deteriorated for larger K . CTM organizes topics into different groups. Its performance increases with the growth of K . Unfortunately, the CTM lacks the mechanism to distinguish local and global topics. So its peak performance is even worse than LDA for the 20-NG dataset.
 Compared with MGCTM, GLDA performed better with respect to the perplexity metric. They both performed worse for Kl  X  1 ; 2. Because: (1) a small number of local topics are exaggerate the influence of global topics (i.e., one man X  X  loss is another X  X  gain). GLDA outperformed MGCTM for Kl [ 2, e.g., 3,190 in GLDA and 3,620 in MGCTM for Kl  X  6, and 3,048 in GLDA and 3,323 in MGCTM for Kl  X  8. We argue that this is because GLDA considers the relationships between local and global topics in terms of the different point.

As shown in Fig. 4 , GLDA also performed better for WebKB. It performed better than the two simpler models (i.e., LDA and CTM) and slightly outperformed the state-of-the-art MGCTM. For the two simpler topic models, CTM outperformed LDA except when topics to capture the local semantics. In particular, we found that the gap between MGCTM groups, in contrast to MGCTM. However, WebKB contains fewer groups ( C = 4) than 20-NG ( C = 20). So MGCTM approaches GLDA in this case.

We also investigated how to set the number of local and global topics in GLDA. We averaged perplexity performance for different Kl and Kg using the WebKB dataset. The topic modeling performance was not significantly sensitive to the number of topics, and the variations were not very abrupt. Larger Kl and Kg resulted in a better performance than small values, e.g., the best performance was achieved when Kl = 32 and Kg = 32, and the worst was when Kl = 8, 16 and Kg = 8. More importantly, we found that the performance requires more global topics to describe the background semantics that are ubiquitous to all the documents. 4.2.3 Quantitative evaluation on Wiki Comparion with MGCTM Because of the similarities between MGCTM and (non-online) GLDA, we attempted to further compare the two models using a larger collection. To this end, we randomly selected 50,000 documents from the entire 3M Wiki collection (mini-Wiki) for model training, and evaluated MGCTM and GLDA on the test data that con-tained the 2,000 documents mentioned above.

Because the true number of groups in Wiki is unknown, we tested the perplexity scores using different numbers of groups. For both models, we fixed Kl = 10 and Kg = 20, and set C  X  2 ; 3 ; ... ; 10. The results are shown in Fig. 6 . We can see that GLDA performed better than MGCTM in most cases. When there was a small number of groups (e.g., C = 2, 3, 4), the gap between the two models was relatively small. As C increased, GLDA rapidly diverges from the other model. As discussed in Sect. 3.5 , the main difference between the two models is that GLDA considers the relationships between local and global topics in terms of the different groups, but MGCTM does not. In other words, we argue that GLDA is superior to MGCTM with a relatively large value of C . These empirical results support this view, as expected.

Online learning we evaluated the performance of Online GLDA on the entire 3M Wiki collection. We set the mini-size M = 100 and 500. We fixed K = 100 for the Online LDA (Hoffman and Blei 2010 ), and C = 8, Kl = 10 and Kg = 20 for the online GLDA. The following learning rate is chosen, where the delay s and forgetting rate j are set as 1,024 and 1, respectively.
The results are shown in Fig. 7 . Obviously, Online GLDA outperformed Online LDA. It improved by 150 when M = 100 and approximately 180 when M = 500. This is because a cluster the relevant topics together. This experimental result shows that GLDA is useful for large-scale data.
 4.3 Document clustering clustering. We evaluated the document clustering performance of the proposed GLDA model using the filtered 20-NG and WebKB datasets. For both datasets, we removed the words that have occurred less than 10 times. 4.3.1 Metric documents using the clustering algorithm and the true labels. In our experiments, we used two common metrics (Cai et al. 2011 ; Zhang et al. 2011 ): clustering accuracy (AC) and normalized mutual information (NMI). For both metrics, a larger score represents a better performance.

The AC is used to evaluate the final clustering performance. Given a document d , let e y d computed by: Plummer 1986 ).

NMI is originally used to measure the statistical information shared between two dis-tributions. Let e Y be the set of clusters obtained by the clustering algorithm and Y be the true set of labels. Their mutual information is defined as: label y i and cluster e y j at the same time. Here, we normalize MI Y ; e Y using: cluster set e Y . 4.3.2 Performance We selected several baseline algorithms: non-negative matrix factorization (NMF), entropy weighting K-Means (EWKM) (Jing et al. 2007 ), LDA (Blei et al. 2003 ), CTM (Wallach 2008 ), and MGCTM (Xie and Xing 2013 ). For the LDA, we followed the experimental and b  X  0 : 01. For the CTM, we set the number of topics to 120 for the 20-NG dataset and to 40 for the WebKB dataset. In MGCTM and the proposed GLDA model, we used 10 local topics for each group and 20 global topics for the 20-NG dataset, and 32 local topics for each group and 32 global topics for the WebKB dataset. Following the settings in (Xie and Xing 2013 ), MGCTM initialized the variational document-group distributions with the clustering results of LDA and randomly initialized the other parameters. For GLDA, we randomly initialized all the parameters (Ran-GLDA). For all the approaches, we averaged significance levels for the GLDA and the baselines.

Table 4 shows the results for the 20-NG dataset. Obviously, the proposed GLDA model achieved the highest scores in both the AC and NMI metrics. GLDA performed much tively when compared to the topic modeling approaches. It outperformed LDA by approximately 5 % in AC and 6 % in NMI, and outperformed CTM by approximately 8 % in AC and 11 % in NMI. Ran-GLDA was approximately 0.5 % better in AC and 0.3 % in NMI than the state-of-the-art MGCTM. More importantly, GLDA outperformed MGCTM by approximately 3 %,in both AC and NMI.

Table 5 illustrates the results for the WebKB dataset. As for the 20-NG dataset, GLDA outperformed all the other approaches on the two metrics. For example, GLDA was approximately 4 % better than LDA in AC, and approximately 3.5 % better than CTM in NMI. GLDA outperformed the state-of-the-art MGCTM (approximately 0.3 % better in AC, and 0.6 % better in NMI). Ran-GLDA performed slightly worse than MGCTM (i.e., 0.5 % in AC and NMI), because there are no optimal initial parameters for Ran-GLDA. and 5 . We can clearly see that the proposed GLDA model was statistically superior to the compared algorithms in most cases [i.e., 20-NG (11/12) and WebKB (9/12)]. GLDA was clearly better than NMF, EWKM, LDA and CTM, and was slightly superior to MGCTM. We can also see that the standard deviations of the scores from GLDA were smaller. This further validates the robustness of GLDA. 4.3.3 Study on the number of topics We investigated the effect of the number of topics on document clustering. Figure 8 illus-trates the AC and NMI performance for the WebKB dataset, with different Kl and Kg .We better performance is achieved when Kl and Kg are both large. In particular, the performance deteriorated when Kl [ Kg (e.g., the worst scores were obtained when Kl = 24, 28, 32 and suggest the following settings in GLDA: (1) relatively larger Kl and Kg ; and (2) Kl Kg . 5 Conclusion In this paper, we developed GLDA as an extension to the LDA model. The highlight of GLDA to cover the background semantics. In contrast to existing techniques, GLDA considers the variational inference algorithm to model the offline corpora, and further extended an online learning algorithm for GLDA for a large-scale collection and true online data.
We used extensive experiments to evaluate the proposed GLDA model. We compared the topic modeling performance to traditional topic models for both offline and online cases. We also evaluated GLDA for document clustering. Our experimental results demonstrated that GLDA can achieve a state-of-the-art topic modeling performance, and also has a competitive clustering performance when compared with state-of-the-art clustering approaches.
In the future, we hope to develop extensions of GLDA using nonparametric methods, which can adaptively determine the number of groups and topics. It may also be useful to apply GLDA to basic tasks such as classification and sentiment analysis.
 Appendix In this appendix, we derive the variational inference w.r.t GLDA. In terms of the varia-Jensen X  X  inequality: follows: free variational parameters ~ p ; ~ h ; ~ a ; ~ b :
L ~ p ; ~ h ; ~ a ; ~ b j p ; a  X  l  X  ; a  X  g  X  ; b where / c k corresponds to the k th / of group c and:
Now we derive the update rules w.r.t the four free variational parameters one by one. 1. For ~ p : we know that contain ~ p d ; c and adding the Lagrange multipliers k , we obtain:
Compute the derivative with respect to ~ p d ; c as follows:
Setting Eq. ( 24 ) to zero, so: 2. For ~ h : we know that terms that contain ~ h d ; n ; k and adding the Lagrange multipliers k , we obtain:
Taking the derivative with respect to ~ h d ; n ; k , we obtain:
Setting Eq. ( 27 ) to zero, we can obtain: 3. For ~ a : in Eq. ( 20 ), the terms that contain ~ a d ; k are as follows:
The corresponding derivative is: where W 0  X  X  is the derivative of W  X  X  function.

Setting Eq. ( 30 ) to zero, we can yield the maximum at: 4. For ~ b : in Eq. ( 20 ), the terms that contain ~ b k ; v are as follows:
When k is a local topic that belongs to group c, its corresponding derivative is:
When k is a global topic, its corresponding derivative is:
Setting the Eqs. ( 33 ) and ( 34 ) to zero, we can yield the maximums at: References
