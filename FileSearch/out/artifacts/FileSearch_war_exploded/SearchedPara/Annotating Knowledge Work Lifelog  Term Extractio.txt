 We present a system that supports review of a knowledge work lifelog as an activity history. Since knowledge workers often review their own activity histories, gathering each user X  X  activities on his/her terminal as a lifelog is a promising approach. However, readability of the stored lifelog is a large problem of lifelog-based application. We propose a term extraction method to add annotation labels to the stored lifelog for supporting knowledge workers, exploiting text data acq uired from desktop activities. Our prototype system monitors a us er X  X  desktop activities after combining raw events, and then extracts possible annotation labels with LDA and C-value techniques from documents and text data in sensor events. In this paper, we introduce a lifelogging module and a lifelog annotation method based on term extraction techniques. According to an empirical evaluation for three weeks, we found that the current method is useful for one-week review. H.5.2 [ Information Interfaces and Presentation ]: User Interfaces; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval.
 Human Factors, Experimentation. Knowledge work, lifelog, term extraction. Knowledge workers often review their own activity histories to write their daily or weekly reports, search some specific materials such as mails, schedules, and documents, or approximately calculate how much time they used for each task. However, it is difficult to review the activities as an aggregated history since these materials are scattered. To place the activities on time series data, gathering each user X  X  activities on his/her terminal as a lifelog is a promising approa ch. Nowadays, lifelogging as a history of daily activity is beco ming easier, because many sensor devices are installed in person al terminals such as personal computers, portable phones, and smartphones. However, it is difficult to review the sensor or operation events that occur through users X  actions because users should interpret what they actually did from  X  X aw X  large-scale lifelogs. We believe that users would like to review their lifelogs if this problem were solved. One solution is to shrink detailed activities into adequate volumes and annotate them. Each annotation works as a summary of an activity and a search query so that users can interpret or search their activities easily. In the knowledge work domain, we can exploit text data for a nnotation, e.g., titles of emails, file names or content of documents. We use a term extraction method to consider topics and readability. Figure 1 shows a screenshot of ou r system. A user X  X  lifelog events are shown in a timeline from top to bottom of the window. Each lifelog event includes the title of the activity, an icon indicating annotation. When the user pushes one or more tags, the timeline is refined and only activities with these tags are shown. In this paper, we introduce a lifelogging module and a lifelog annotation method based on te rm extraction techniques. Several researchers have endeavored to support review of knowledge work activities. The two principal approaches are search support and task classification. The search support approach aims to help knowledge workers increase their efficiency by reducing the time and effort required to access relevant content or activities. FALCON is a system that integrates email and calendar functions with a recorded meetings application [1]. Using each pe rson X  X  microphone input, a method of searching documents used in a meeting without mail or calendar information is proposed [7]. The task classification approach aims to gather related applications or contents for each task so that a user can find the material needed for a selected task. An application to classify materials or documents from the office work history, namely, TaskPredictor2, is proposed [9]. However, there is little discu ssion on the automatic addition of readable annotations to review the document browsing history as an activity history. In regard to information extraction, various research has been done on extracting important terms from stored documents or browsed webpages. For example, there are techniques for extracting importa nt terms from a user X  X  web browsing history [6]. We need an adequate term extraction method for reviewing knowledge work activity. In regard to requirements for these annotations, the ability to classify an activity into a class and terms with higher readability is needed. Figure 2 shows the flow of lifelog generation. Firstly, the logger application logs sensor events and operation events as a background process. Currently, our system supports about 90 events for Windows and Android pl atforms. These events include sensor events for internal sensors such as accelerometers, GPS (global positioning system), neighbor Bluetooth terminals or WiFi access points, speech events with the pitch tracking method [10], the transportation method (walking, running, or vehicles) [3], and operation events for desktop activ ities such as office document operations, web browsing, ma il operations, active window changes, mouse events, and keyboard events. For a part of applications, e.g., Mi crosoft Office applications or Internet Explorer, the logger application automatically attaches to these applications and events are detected by application APIs. These events are input into the sensor event buffer. In our current system, sensor events detected on an Andr oid terminal is merged to the events of a Windows PC by th e import module. Figure 3 shows a screenshot of the sensor event viewer for an Android terminal. Secondly, the lifelog event det ection module generates lifelog events from combinations of sensor events in the sensor event buffer. For example, a lifelog event  X  X ail compose X  is generated from  X  X tart mail compose X  and  X  X  end mail X  sensor events, and a lifelog event  X  X resentation X  is generated from  X  X lideshow X  and  X  X peech X  sensor events. Currently, our system supports about 40 lifelog events. Detected lifelog events are stored in the lifelog database. The text of each conten t is also stored in the content database. Thirdly, the keyword extraction module extracts terms for annotation from content text. As the content text, we can use window titles, browsed web documen ts, mail contents, content of documents, file names, and so on. The detail of this process is described in the following section. Extracted terms are annotated to the corresponding lifelog events. Finally, the lifelog viewer show s annotated lifelog events as shown in Figure 1. To support review of knowledge wo rk activities, annotation terms should be typical topic terms of a user X  X  activities and interpretable for the user. In the proposed system, we combine LDA (latent Dirichlet allocation) [2] for extracting typical topic terms and C-value [4] calculation for extracting readable terms. Figure 4 shows the flow of term extraction. This process includes three steps: preprocess, important term estimation, and collocation extraction. As preprocess, documents are converted into a series of morphemes by morphological anal ysis. We use MeCab [5] for the Japanese morphological analysis module. LDA [2] is a language generation model that considers a set of D documents as a mixture of latent K topics. Each latent topic is a multinomial distribution with W words. The parameter  X  that controls the generation model of la tent topics is expressed as a Dirichlet distribution Dir( X ) , and  X   X  words for document j are modeled with topic  X   X  X  X   X (Multinomial=  X  ) and a term  X   X (Multinomial  X  method [8] for the parameter es timation. We use a number of topics with the smallest perp lexity after a hundred sampling iterations. Then, terms with higher occurrence probabilities for each topic are extracted as term candidates. Independent morpheme extract ed with LDA does not have readability for a user to interpret the user X  X  activities because of the small length. We use the C-value [4] calculation technique as a readable collocation extraction method. A C-value score for a collocation a is calculated as where t ( a ) means the occurrence frequency of longer collocations, and c ( a ) means the number of these collocations. Finally, collocations that have high C-value scores and include terms extracted through the LDA ar e extracted as annotation, as shown in Figure 1. We empirically investigated to what degree our annotation method satisfied users. First, we let eight subjects (knowledge workers in the IT domain) collect their lifelogs for three weeks. Second, the system extracted up to twenty terms for each target duration: 1, 3, 7, 14, or 21 day(s). We used mail titles, file names of Microsoft Office documents, and the text of the first page of these files for the input text afte r our preliminary investigation. Then, each user rated annotations from three perspectives: meaningfulness , remindability , and usefulness for refinement . Meaningfulness means to what degr ee annotations are meaningful as words or collocations at three levels: meaningful (3); somewhat (2); meaningless (1). We also introduced remindability, which means to what degree annotati ons remind users of concrete activities. This score indicates usefulness for reminding users of their activity histories at three levels: remindable (3); somewhat (2); not remindable (1). Usefulne ss for refinement means to what degree annotations are useful for refinement of lifelog events at three levels: useful (3); some what (2); not useful (1). Figure 5 and Figure 6 summarize the results of our experiments with 2,531 events and 553 annotations. Figure 5 shows the average scores for target durations. From remindability scores in Figure 5, current method is most useful for seven-day data, though there is no significant difference. This result means that the current method is most useful for one-week review. However, refinement scores are totally low. After checking evaluated terms, we found that some terms indi cating transient events, e.g.,  X  X onfirm, X   X  X pplication form, X  and  X  X uestionnaire, X  are rated with high remindability scores and lo w refinement scores. How to detect these transient terms is included in our future work. Figure 6 shows average scores of individual users for terms extracted from seven-day lifelogs . From Figure 6, we found that there is great variability among us ers. There are two reasons. One is the difference of criterion for evaluation itself. The other is the difference of activities among users. User 6, whose score is the lowest, mainly used a spreadsheet application. Since each word in spreadsheet files is not useful for reminder or refinement, the average scores for the subject is low. From Figure 5 and Figure 6, we f ound that the current method is most useful for one-week review , though we should refine the extraction method continuously. During the demonstration session, users can see what kinds of sensor events are detected fo r Windows-based and Android-based clients, and what kinds of terms are annotated to the lifelog with a lifelog viewer application on a Windows PC. We use a Windows PC and an Android smartphone for the demonstration. In this paper, we propose a term extraction method to add annotation labels to the stored lifelog for supporting knowledge workers, exploiting text data acq uired from desktop activities. Our prototype system monitors a us er X  X  desktop activities after combining raw events, and then extracts possible annotation labels with LDA and C-value techniques from documents and text data in sensor events. According to an empirical evaluation with eight subjects, we found that the current method is useful for one-week review. Our future work includes the refinement of our term extraction method and introduction of interpersonal contexts such as communication history [7]. [1] Bjellerup, P., Cama, K. J., Desikan, M., G uo, Y., Kale, A. G., [2] Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent [3] Cho, K., Iketani, N., Setoguc hi, H., and Hattori, M. 2009. [4] Frantzi, K. T. and Ananiadou, S. 1996. Extracting nested [5] Kudo, T., Yamamoto, K., Mats umoto, Y. 2004. Appliying [6] Matsuo, Y., Fukuta, H., and Ishizuka, M. 2002. Browsing [7] Okamoto, M., Iketani, N., Ni shimura, K., Kikuchi, M., Cho, [8] Porteous, I., Newman, D., Ihler, A., Asuncion, A., Smyth, P., [9] Shen, J., Irvine, J., Bao, X. , Goodman, M., Kolibaba, S., [10] Talkin , D., 1995. A Robust Algorithm for Pitch Tracking. In 
