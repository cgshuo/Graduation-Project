 Mathematical Information Retrieval concerns retrieving in-formation related to a particular mathematical concept. The NTCIR-11 Math Task develops an evaluation test collection for document sections retrieval of scientific articles based on human generated topics. Those topics involve a combi-nation of formula patterns and keywords. Another task in NTCIR-11 is the optional Wikipedia Task, which provides a test collection for retrieval of individual mathematical for-mula from Wikipedia based on search topics that contain exactly one formula pattern. We developed a framework for automatic query generation and immediate evaluation. This paper discusses our dataset preparation, our topic genera-tion and evaluation methods, and summarizes the results of the participants, with a special focus on the Wikipedia Task.
Math Information Retrieval (MIR) is a growing field. Re-cent publications (e.g., [4 , 6, 10]) show that there is a signif-icant demand for enhancement in Mathematical Knowledge Management. In order to compare different approaches and measure their performance, test collections are needed. At the CICM 2012 conference in Bremen, Germany, the first  X  X IR happening X  took place with two participants, 10,000 arXiv documents and a dataset size of 293 MB. In 2013, the NTCIR-10 Math pilot task [2 ] for MIR attracted 6 par-ticipants and used 100,000 arXiv documents with a dataset size of 63GB. Based on the gathered experience, MIR qual-ified for a main task at NTCIR-11 [3 ] which took place in 2014 with 8 participants and 8 million document sec-tions of 174GB in total. Additionally, the newly introduced Wikipedia Task was appreciated by the participants. We ex-pect that the automated feedback and evaluation framework will lower the entrance level for participants and attract even more participants in the future. In this first section, we pro-vide a general introduction to MIR, list some applications of MIR, and explain why MIR is fundamentally different from other IR tasks, such as text retrieval and xml retrieval. In Section 2, we describe how the Wikipedia dataset was pre-pared and augmented. In Section 3 and 4, we describe the query design and evaluation process respectively. In Section 5, we present the participating teams and the performance their MIR systems. In Section 6 we give a future outlook for the Wikipedia Task.
 The use cases for MIR are diverse. They include applicable theorem search, plagiarism detection, related work search, patent search, and search in Excel spreadsheets [ 5]. Some of the fundamental concepts that relate back to tree structure search can be used for code-search or search for chemical formulae.

For the Wikipedia Task, the focus is on information needs that involve mathematics that is naturally expressed us-ing mathematical expressions. With regard to the afore-mentioned applications, those information needs can be ex-pressed as mathematical expressions, combination of expres-sions and keywords, or keywords only. While the main task uses all of these combinations to retrieve documents, the Wikipedia Task provides exactly one formula per topic to describe an information need.
The test collection used at NTCIR-10 in 2013 was based to html5 [8 ]. The arXiv is a vast and expanding source of knowledge for researchers and experts in highly specialized domains. However, neither math search engine developers (participants) nor the assessors that evaluate the search en-gine results usually are domain experts in all the topics ad-dressed in the arXiv publications. Some topics discussed in the research papers are so specialized that it becomes im-possible for participants to get even a preliminary idea of the content and to decide on the relevance of a formula with respect to a topic (i.e., the underlying information need). This fact adds additional complexity to debugging and test-ing of the Math search engines. In contrast to the arXiv Figure 1: Distribution (blue  X + X ) and mean (red  X  X  X ) of the number of distinct formulae and their frequencies. For example the expression n occurs 2988 times and there are about 280 000 formulae that occur only once. dataset, the Wikipedia encyclopaedia contains most of the mathematical world knowledge explained in simple terms. While this knowledge is not sufficient for new research, it is perfectly suitable as a test corpus for math search compe-titions. This knowledge simplifies debugging and testing of the math search engines and enables the participants to test their systems on a dataset that is easier to understand and contains all formulae they are already familiar with. The English Wikipedia contains about 30k encyclopaedic arti-cles with mathematical formula. Those are written using the T E X-like input format texvc. Even though the syntax in texvc is restricted and does not allow to write Turing com-plete programs, as it is possible with T E X, T E X is neither the optimal way to represent Mathematics on the web nor to search for formula. In contrast, MathML was designed to serve the aforementioned purposes.

Schubotz and Wicke [7 ] compare different conversion meth-ods and identify the L A T E xml converter as the most reason-able solution with content MathML support for Wikipedia. A majority of the participating systems use content for the search task. Therefore, both tasks (arXiv and Wikipedia) use L A T E xml to convert the original user input to MathML with parallel content and presentation markup and the orig-inal input as annotation.

In order to generate stable and unique references to each individual formula used in Wikipedia, we created an unique index, from which one can derive many interesting statistics about the usage of mathematics within Wikipedia. For ex-ample, Figure 1 shows the frequency distributions of the for-mulae. One use case we published at formulasearchengine. com is an auto-completion list for T E X commands based on the usage statistics that we gathered by tokenizing the fre-quency ordered formulae. People editing Wikipedia articles about math with mobile devices will benefit from this fea-ture.
From our experience with the Math pilot task at NTCIR-10, we draw the following conclusions: 1) for each query there should be at least one relevant hit in the dataset; 2) Figure 2: Density of the topics with regard to the frequency of seed formula f ( t ) and the number of query variables in the topic q ( t ) . The blackest box corresponds to the highest count (up to 41 topics) and the lightest grey corresponds to only 1 topic. the semantics of query variables should be well-defined; 3) only the information that would be exposed to a MIR sys-tem should be communicated to the participants; 4) the rel-evance criteria for each topic should be independent of the topic author and assessor. While the main task addressed these issues with human intuition, we chose a different ap-proach for the Wikipedia Task that does not involve humans.
We developed a program that generates queries in three simple steps. At first, the so called seed formula is chosen based on random selection from our mathindex. In a sec-ond step, we inject query variables. In contrast to the main task, where humans chose a meaningful name for the query variable, we call our query variables x 0 , x 1 , . . . . Finally, we generate the NTCIR-11 xml Topics [ 1] using L A T E xml . Our relevance criterion is to find a formula similar to the seed. By our naming convention for the query variable and the absence of a topic title we ensure that no information is ex-posed to the participants that is not intended to be used according to the topic specification, i.e., MIR systems can-not use the name of query variables for relevance ranking. Our method generates two meta-information pieces f ( t ) and dicates how many exact matches (based on exact matches on the original T E X input) for each seed are contained in the dataset, and q ( t ) is the number of query variables used. This allows for an a priori classification of the search topics based on f and q . For simplicity, we partition the set of generated topics T (Figure 2) into the 4 following groups: Easy topics without query variables and exactly one precise match E = { t  X  T : f ( t ) = q ( t )  X  1 = 1 } ; variable top-ics with query variables but only one exact match for the underlying seed V = { t  X  T : f ( t ) = 1  X  q ( t ) &gt; 0 } ; fre-quent topics without query variables but with non-unique seeds F = { t  X  T : q ( t ) = 0  X  f ( t ) &gt; 1 } and hard top-ics that contain query variables and non-unique seeds H = used in NTCIR-11 the following cardinalities were given: | E | = 41 , | V | = 27 , | F | = 24 , | H | = 8  X  X  T | = 100 .
For retrieval tasks with one known good result, a typical evaluation measure is the mean reciprocal rank (mrr) [9 ]. In addition, our automatic evaluation software calculates mean average precision in the first k hits for different levels of k , and counts the number of found seeds referred to as success
Participant v ariable h ard to tal ea sy freq TU B Technische Universit  X  at
Berlin (Germany) 4 4 8 7 46 KW ARC Jacobs University
Bremen (Germnay) 1 1 5 0 67 RM HS Richard Montgomery
High School (USA) 1 1 5 0 01
RI T Rochester Institute of Technology (USA) 1 7 4 6 3 83
M IAS Masaryk University (Czech republic) 1 9 4
TU W Vienna University of Technology (Austria) 5 3 8 8 54
N II National Institute of Informatics (Japan) 9 4 1 00 67 in the rest of this paper, which corresponds to the recall for k  X   X  . The evaluation tool performs two types of evalua-tions, a page-centric evaluation that regards a hit as correct if the seeding page was found, and the formula-centric eval-uation, which assumes that a hit is correct, if a formula with exactly the same T E X input was found. To avoid over-fitting, only aggregated results are displayed as feedback to the par-ticipants. Thus the participants get feedback on how their systems performed on average for all topics, but they do not know how the systems performed on an individual topic or on a topic category. We observed that the intermediate feedback feature was highly appreciated by the participants, because it helped them to identify and fix bugs in their soft-ware. We observed that participants submitted 3 to 5 times until they were satisfied with the results. Some participants submitted subsequent runs under different names. This jus-tifies the reports by the participants that the submission system helped to improve MIR systems. Some teams have improved their mrr by 50% or more.
We had seven participants from five countries and in total 56 runs with about 2 million hits. The results of the evalu-ation described in the former section are listed in Table 1. A detailed overview of the participants and their MIR ap-proaches can be found in [ 3]. The best result with regard to success was submitted by TUW and NII. Both runs found 97% of the topics according to the page-centric evaluation. With mrr = 82% the ranking of TUW is slightly better com-pared to Nii (74.5%). For the formula-centric evaluation Nii has the best success with 94% and mrr = 82%. The best TUW run achieves a mrr value of 88% at a sucess rate of 93%. Table 1 shows that there is a high correlation between the topic category and the system performance. Further-more, all teams that submitted more than one run got very good results for the easy topics. The difference between page and formula exact evaluation is not significant.

As shown in Figure 3, the performance results from dif-ferent teams vary more than different runs submitted by the same team. The MIaS team runs (circles) show the well known behavior that mrr (or precision) decays with growing success (or recall respectively).

We observe that all topics except one were found by two or more participants, even if the formula exact evaluation method is used. The known good result for query 99 ? x 0 ? x can be verbalized as  X  X ny fraction X , was found only by one team at rank 8983. More interesting is that 4 teams assigned a high rank to the result x y y + z page in contrast to the first mathematical expression 1 2 the Wikipedia article titled fraction that was not ranked very high.
We discussed the NTCIR-11 Math Wikipedia Task that lowers the entrance barrier for new participants to Math Information Retrieval and broadens the scope of NTCIR Math tasks to encyclopedic applications. We presented three main technology contributions, integrated in the MediaWiki MathSearch extension. First, we developed methods to con-vert Wikipedia dumps (in any language) to the main task data format including content and presentation MathML . Second, we developed a method for automated search pat-tern generation with example hits. Third, our extension provides a fully automated evaluation framework with real time feedback at submission time and a comparative evalu-ation for multiple submissions including hit pooling.
For the future, we plan to continue development and im-provement of the platform with regard to the following as-pects. We will allow for user feedback for results with regard to relevance for the entries submitted by the systems. While it X  X  questionable that volunteers can be found to evaluate the results, participants can evaluate their own results which will be helpful for system tuning. Furthermore, we will dis-play some basic similarity scores for each hit and will allow users to create their own search topics. The queries used for NTCIR-11 will stay available for training and testing. A query exact feedback will be displayed for new submissions for the old topics. We intend to attract more participants and found a Math Search interest group made of mathemati-cians, scientists and people from the traditional information retrieval community. Due to the continuously available por-tal, participants will be able to test new features whenever they are ready. We will publish new queries on demand, and synchronize participants with the NTCIR events.
 support of Akiko Aizawa, Michael Kohlhase, Goran Topic and Marcus Leich. This work has been supported through grants by the German Science Foundation (DFG) as FOR 1306 Stratosphere and by the German Ministry for Educa-tion and Research as Berlin Big Data Center BBDC (funding mark 01IS14013A). [1] Formats for topics and submissions for the math2 task [2] Akiko Aizawa, Michael Kohlhase, and Iadh Ounis. [3] Akiko Aizawa, Michael Kohlhase, Iadh Ounis, and [4] Michael Kohlhase, Helena Mihaljevic-Brandt, Wolfram [5] Michael Kohlhase, Corneliu Prodescu, and Christian [6] Matthias S. Reichenbach, Anurag Agarwal, and [7] Moritz Schubotz and Gabriel Wicke. Mathoid: [8] Heinrich Stamerjohanns, Michael Kohlhase, Deyan [9] Ellen M. Voorhees. The TREC-8 Question Answering [10] Keita Del Valle Wangari, Richard Zanibbi, and
