 Existing recommender systems usually model items as static  X  un-changing in attributes, description, and features. However, in do-mains such as mobile apps, a version update may provide substan-tial changes to an app as updates, reflected by an increment in its version number, may attract a consumer X  X  interest for a previously unappealing version. Version descriptions constitute an important recommendation evidence source as well as a basis for understand-ing the rationale for a recommendation. We present a novel frame-work that incorporates features distilled from version descriptions into app recommendation. We use a semi-supervised topic model to construct a representation of an app X  X  version as a set of latent topics from version metadata and textual descriptions. We then dis-criminate the topics based on genre information and weight them on a per-user basis to generate a version-sensitive ranked list of apps for a target user. Incorporating our version features with state-of-the-art individual and hybrid recommendation techniques signif-icantly improves recommendation quality. An important advantage of our method is that it targets particular versions of apps, allowing previously disfavored apps to be recommended when user-relevant features are added.
 H.3.3 [ Information Search and Retrieval ]: Information filtering, Search process Version sensitive, Recommender systems, Mobile apps, App store
Mobile applications (apps) are now a part of our daily life, cre-ating economic opportunities for companies, developers, and mar-keters. While the growing app market 1 has provided users with a motley collection of apps, their sheer number leads to information overload, making it difficult for users to find relevant apps.
Recommender systems have been deployed to alleviate the over-load of information in app stores by helping users find relevant apps [30, 31, 6, 15, 32]. Existing recommender systems typically apply one of two methods: collaborative filtering (CF), which rec-ommends items to target users based on other similar users X  prefer-ences, or content-based filtering (CBF), which recommends based on the similarity of an item X  X  content and the target user X  X  interests. However, unlike conventional items that are static, apps change and evolve with every revision. Thus, an app that was unfavorable in the past may become favorable after a version update. For exam-ple, Version 1.0 of App X did not interest a user at first, but a recent update to Version 2.0  X  which promises to provide the function-ality of high definition (HD) video capture  X  may arouse his in-terest in the revised app. A conventional recommender system that regards an app as a static item would fail to capture this impor-tant detail. This is why it is vital for app recommender systems to process nascent signals in version descriptions to identify desired functionalities that users are looking for.

We focus on the uniqueness of the app domain and propose a framework that leverages on version features ; i.e. , textual descrip-tions of the changes in a version, as well as version metadata. First, with the help of semi-supervised topic models that utilize these fea-tures, we generate latent topics from version features. Next, we discriminate the topics based on genre information and use a cus-tomized popularity score to weight every unique genre-topic pair. We then construct a profile of each user based on the topics, and finally compute a personalized score of recommending the latest version of an app to a target user. Furthermore, we show how to integrate this framework with existing recommendation techniques that treat apps as static items.

Figure 1 provides an overview of our approach. App X has five different versions (1.0, 1.1, 1.2, 2.0, and 3.0). Each version is characterized by a set of latent topics that represents its contents, whereby a topic is associated with a functionality, such as the abil-ity to capture HD videos. For instance, Version 1.0 has Topics 1, 2, and 4; whereas Version 3.0 only has Topic 5. At the same time, based on a user X  X  app consumption history, we can model which topics they are interested in. Therefore, if Bob has a keen interest in Topic 5, the chance that he adopts App X at Version 3.0 would be higher because Topic 5 attracts Bob X  X  interest. Likewise, there is a higher chance of both Alex and Clark adopting App X at Ver-sion 1.2 because Topics 1 and 3 attract their interests.

We show that the incorporation of version features complements other standard recommendation techniques that treat apps as static items, and this significantly outperforms state-of-the-art baselines. them by identifying which topics they are interested in (on the right). Our experiments identify which topic model best utilizes the avail-able version features to provide the best recommendation, and ex-amine the correlation between various version metadata and recom-mendation accuracy. Furthermore, we provide an in-depth micro-analysis that investigates: (i) whether our approach recommends relevant apps at the most suitable version, (ii) what information can we gather by scrutinizing the latent topics, and (iii) which is the most influential version-category. To the best of our knowledge, this is the first work that investigates version features in recom-mender systems. Our contributions are summarized as follows:
Collaborative filtering (CF) has been widely studied. This tech-nique can be classified into neighborhood models [5, 8, 23] and latent factor models [10, 12, 9]. Matrix factorization (MF) is one of the most successful realizations of latent factor models and is superior to the neighborhood models [25, 13]. MF associates users and items with several latent factors where observed user-item rat-ings matrix is their product [32].

On the other hand, content-based filtering (CBF) has been ap-plied mostly in textual domains such as news recommendation [28] and scholarly paper recommendation [18, 26, 24]. In recent years, topic models such as latent Dirichlet allocation (LDA) [4] have been used to provide an interpretable and low-dimensional repre-sentation of textual documents for recommender systems. For ex-ample, Ramage et al. [19] used a variant of LDA to characterize microblogs in order to recommend which users to follow, while Moshfeghi et al. [17] also employed LDA to combine content fea-tures and emotion information for movie recommendation.
In order to deal with the recent rise in the number of apps, works on mobile app recommendation are emerging. Some of these works focus on collecting additional information from the mobile device to improve recommendation accuracy. Xu et al. [30] investigated the diverse usage behaviors of individual apps by using anonymized network data from a tier-1 cellular carrier in the United States. Yan and Chen [31] and Costa-Montenegro et al. [6] constructed app recommender systems by analyzing the usage patterns of users. Other works utilize external information to improve recommenda-tion quality. Zheng et al. [34] made use of GPS sensor information to provide context-aware app recommendation. Lin et al. [15] uti-lized app-related information on Twitter to improve app recommen-dation in cold-start situations. Yin et al. [32] considered behavioral factors that invoke a user to replace an old app with a new one, and introduced the notion of  X  X ctual value X  (satisfactory value of the app after the user used it) and  X  X empting value X  (the estimated satisfactory value that the app may have), thereby regarding app recommendation as a result of the contest between these two val-ues. While the above works recommend apps that are similar to a user X  X  interests, Bhandari et al. [2] proposed a graph-based method for recommending serendipitous apps.
Works in information retrieval (IR) have also handled items that change and evolve. For example, past works have also viewed Web pages as entities that evolve over time. Keyaki et al. [11] explored XML Web documents ( e.g. , Wikipedia articles) that are frequently updated, and proposed a method for fast incremental updates for efficient querying of XML element retrieval. This is different from our work as they dealt with items only, whereas our method also generates personalized recommendation of items for users, i.e. , our work also considers the users. Furthermore, our work focuses on a secondary item unit  X  version updates, which is a separate entity from primary item ( i.e. , the app). Liang et al. [14] proposed a method to capture the temporal dynamics of relevant topics in micro-blogs ( e.g. , Twitter) where a topic centers around a certain theme such as the U.S. presidential election or Kate Middle-ton X  X  baby which, in the micro-blogging community, may change quickly with time. Our work differs from theirs as the  X  X tems X  in their system are the topics, which is an indefinite discourse. Apps, on the other hand, are definite items that users download and use. Wang and Zhang [27] explored the problem of recommending the right product at the right time, which uses a proposed opportu-nity model to explicitly incorporate time in an e-commerce rec-ommender system. Their work explores the time of purchase, and does not focus on items that change with time. Figure 2: An app X  X  changelog chronicles the details of every version update; shown here is an excerpt of the Tumblr app changelog. Version updates typically include new features, en-hancements, and/or bug fixes.

The works mentioned at the outset (Sections 2 and 2.1) can be characterized as static item recommendation, as the items do not undergo any change or evolution. In contrast, our work focuses on items that evolve. Furthermore, as shown later in Section 5, our work complements these techniques that treat items as static. In addition, our work differs from the previous works in time-sensitive recommendation (Section 2.2) in that the nature and requirements of app recommendation differs from the retrieval of Web articles and topic recommendation in micro-blogs.
Our framework processes the version texts and metadata to de-cide whether a particular version of an app entices a target user. We first generate latent topics from version features using semi-supervised topic models in order to characterize each version. Next, we discriminate the topics based on genre metadata and identify important topics based on a customized popularity score. Follow-ing that, we incorporate user personalization, and then compute a personalized score for a target user with respect to an app and its version. Our system then recommends the top k target apps: where an app a and its specific version v are treated as a tuple that characterizes a document d , and is scored with respect to a target user u . Lastly, we explain how to integrate this framework with existing recommendation techniques.
App versioning is the process of assigning unique version num-bers to unique states of the app. Within a given version number cat-egory ( e.g. , major , minor ), these numbers are generally assigned in increasing order and correspond to new developments in the app. Figure 2 shows an example of an app X  X  changelog that consists of four different version updates (or version-snippets ) in reverse or-der: Versions 2.0, 1.2.1, 1.2, and 1.1. Hereafter, we will use the terms  X  X ersion-snippet X  and  X  X ocument X  interchangeably to refer to the textual description of each version update.

Versions are identified using a conventional numbering structure of  X  X .Y.Z X  where X, Y, and Z represent major , minor , and mainte-nance categories, respectively: 1. Major  X  Major versions indicate significant changes to the Figure 3: The 40 pre-defined genre labels on Apple X  X  iOS app store (as of January 2014). The bottom set are gaming sub-genres and only appear on gaming apps.
 2. Minor  X  The minor version category is often applied when 3. Maintenance  X  The maintenance version category is asso-Hereafter, we will use  X  X ersion-category X  as shorthand for version number category.

Besides textual descriptions and version-categories, each version-snippet is also associated with the following information: 1. Genre Mixture  X  Every app is assigned to a subset of pre-2. Ratings  X  It is commonly known that user ratings are di-
In order to find an interpretable and low-dimensional representa-tion of the text in the version-snippets (or documents), we focus on the use of topic modeling algorithms (topic models). A topic model takes a collection of texts as input and discovers a set of  X  X opics X   X  recurring themes that are discussed in the collection  X  and the degree to which each document exhibits those topics.
 We first explore the use of two different topic models: (i) latent Dirichlet allocation (LDA) [4] and (ii) Labeled-LDA (LLDA) [20], which are unsupervised and semi-supervised topic models, respec-tively. We also investigate a corpus-enhancing strategy of incorpo-rating version metadata directly into the corpus prior to the appli-cation of topic models. This is to improve the quality of the topic distribution discovered by the topic models.
LDA is a well-known generative probabilistic model of a cor-pus; it generates automatic summaries of latent topics in terms of: (i) a discrete probability distribution over words for each topic, and further infers (ii) per-document discrete distributions over topics, which are respectively defined as: where z , d , and w denote the latent topic, the document, and a word, respectively.

However, a limitation of LDA is that it cannot incorporate  X  X b-served X  information as LDA can only model the text in version de-scriptions, i.e. , LDA is an unsupervised model. In the context of our work, this means that we cannot incorporate the observed version metadata ( e.g. , version-category and genre mixture) into the latent topics. This leads us to Labeled-LDA (or LLDA), an extension to LDA that allows the modeling of a collection of documents as a mixture of some observed,  X  X abeled X  dimensions [20], representing supervision.

LLDA is a supervised model that assumes that each document is annotated with a set of observed labels. It is adapted to account for multi-labeled corpora by putting  X  X opics X  in one-to-one correspon-dence with  X  X abels X , and then restricting the sampling of topics for each document to the set of labels that were assigned to the docu-ment. In other words, these labels  X  instead of topics  X  play a di-rect role in generating the document X  X  words from per-label distri-butions over terms. However, LLDA does not assume the existence of any global latent topics, only the document X  X  distributions over the observed labels and those labels X  distributions over words are inferred [21]. This makes LLDA a purely supervised topic model.
Although LLDA appears to be a supervised topic model ini-tially, depending on the assignment of the set of labels to the doc-uments, it can actually function either as an unsupervised or semi-supervised topic model. To achieve an unsupervised topic model like LDA, we first disregard all the observed labels (if any) in the corpus, and then model K latent topics as labels named  X  X opic 1 X  through  X  X opic K  X  and assign them to every document in the col-lection. This makes LLDA mathematically identical to traditional LDA with K latent topics [19]. On the other hand, to achieve a semi-supervised topic model, we first assign every document with labels named  X  X opic 1 X  through  X  X opic K  X  for the unsupervised portion, and then use the observed labels 4 (that are unique to each document) for the supervised portion.

The semi-supervised method of implementing LLDA allows us to quantify broad trends via the latent topics (as in LDA) while at the same time uncover specific trends through labels associated with document metadata. In our work, we treat the version cat-egories and genre mixture as observed labels, and rely on semi-supervised LLDA to discover the words that are best associated with the different version-categories and genres, respectively. Algorithm 1 How to create  X  X seudo-terms X  from metadata and in-corporate them into the corpus (in pseudocode). 1: For each doc  X  X  X  in corpus: 3: verText = d.getText(); 4: verCategory = d.getVersionCategory(); 7: appId = d.getAppId(); 8: genres = getGenres(appId); 11: verText += genres +  X , X  + verCategory; 12: d.setText(verText);
Similar to LDA, LLDA generates the topic-word and document-topic distributions in Equations (2) and (3), respectively, allow-ing us to obtain the mixture weights of topics for every document. Hereafter, semi-supervised LLDA will be the default LLDA model, and we will also use the terms  X  X opic X  and  X  X abel X  interchangeably.
Aside from employing topic models, we identify another way of incorporating metadata into the latent topics. Inspired by how hashtags are used in Twitter to add content to Twitter messages, we create  X  X seudo-terms X  from the metadata and incorporate them into the set of documents before performing topic modeling. These pseudo-terms can be identified by their  X # X  prefix. Algorithm 1 shows how metadata in the form of pseudo-terms are automatically  X  X njected X  into the corpus of version-snippets, as we want to asso-ciate these pseudo-terms with the latent topics.

Because LDA and LLDA generate automatic summaries of top-ics in terms of a discrete probability distribution over words for each topic [20], incorporating pseudo-terms into the corpus allows the topic models to learn the posterior distribution of each pseudo-term (in addition to the natural words) in a document conditioned on the document X  X  set of latent topics. Incorporating these unique pseudo-terms will help in getting topic distributions that are more consistent with the nature of version-snippets. Note that the differ-ence between using the enhanced-corpus and the normal corpus is that the former allows both the words and pseudo-terms to be as-sociated with the latent topics, while the latter only allows (natural language) words to be associated with the latent topics. To differen-tiate between the normal corpus and the enhanced-corpus, we add the prefix  X  X nj X  X ction to LDA and LLDA; in shorthand,  X  X nj+LDA X  and  X  X nj+LLDA, X  respectively, to denote these approaches.
We can now model each version-snippet (or document) as a dis-tribution of topics. However, we do not know which topics are important for recommendation. For example, if we knew that users prefer a topic that is related to the promise of high-definition (HD) display support, we would rather recommend an app that includes HD display support in its latest version update over similar apps that do not. Therefore, the importance of each topic differs from app to app, and this is a key contribution of our work.

Furthermore, apps are classified into various genres; each genre works differently to the same type of version update. For example, a version update that offers HD display support would be more en-ticing and relevant on a game app instead of a music app. Later, in Section 5.2, we will show how the inclusion of genre informa-tion significantly improves the recommendation accuracy. Because of this, our method includes genre information by default. Table 1 Table 1: Genre-topic weighting matrix, where g and z denote a genre and a latent topic, respectively. Every genre-topic pair has a unique weight from weighting scheme. Also, x  X  { LDA, inj+LDA, LLDA, and inj+LLDA } .
 shows how we uniquely weight every genre-topic pair with mul-tiplicative weight w x , where x  X  { LDA, inj+LDA, LLDA, and inj+LLDA } . Note that each genre has a different distribution of importance weights with respect to the set of latent topics.
To compute the weight w , we first introduce a measurement for  X  X opularity X  for a document. We use a variant of the popularity measurement detailed in [33] whereby the popularity is reflected by the votes it receives; as intuitively, the more positive votes it re-ceives, the more popular it is and vice versa. While one may argue that an item receiving a large number of votes (whether they are positive or not) is popular, in this work, we define popular items as those that are  X  X iked X  by the majority of the service users, whereby a  X  X ike X  translates to a rating of 3 and above on the 5-point Likert scale, whereas a  X  X islike X  is a rating of 2 and below.

We formally define the popularity score  X  ( d ) that outputs a value between 0 and 1 , which factors user ratings into account: where pv d and nv d denote the number of positive and negative ratings of document d , respectively.

We use this popularity score to define the importance weight of a genre-topic pair, w x : where Z is the set of all K topics, D ( g ) is the set of all documents that belongs to genre g ,  X  ( d ) is the popularity score of document d , p ( z | d ) is the document-to-topic distribution in Equation (3), and x  X  { LDA, inj+LDA, LLDA, and inj+LLDA } . The denominator is used solely for normalization. In other words, w x is discrimi-nated by the genre, and information from the ratings, along with the distribution of topics, are used to identify its weights.
To incorporate personalization, we need to know each user X  X  preference with respect to the set of latent topics. We determine this importance by analyzing the topics present in the apps that a user u has previously consumed. To compute this factor with re-spect to a latent topic z , we define the following equation: where p ( z | d ) is the document-to-topic distribution defined in Equa-tion (3) and D ( u ) is the set of documents consumed by user u . As in Equation (5), the denominator is solely for normalization.
Finally, we calculate the score defined by Equation (1). We com-bine the document-to-topic distribution defined in Equation (3), the weighting schemes defined by Equation (5), the user-personalization factor defined by Equation (6), and compute the score as follows: where d , u , and z are the document, target user, and latent topic, re-spectively, w x (  X  ) denotes the weighting schemes (where x  X  {LDA, inj+LDA, LLDA, and inj+LLDA}), genre ( d ) is the genre of docu-ment d , p ( z | d ) is the document-to-topic distribution in Equation (3), and p ( z | u ) is the probability that the target user u prefers topic z . Thus, for each app, we calculate its score based on its latest version to see if it should be recommended.
Our work aims at exploring how version features can improve the recommendation accuracy of existing recommendation techniques such as CF and CBF. A simple way to integrate version features with the other recommendation techniques is to use a weighted combination scheme, but we also explore a more advanced ap-proach, Gradient Tree Boosting (GTB) [7], which is a machine learning technique for regression problems that produces a predic-tion model in the form of an ensemble of prediction models. We show the results of GTB in our work as it is more superior.
For each of the users, we fit a GTB model to their training data (for each app in the training data that a user has consumed). Each training sample contains the prediction scores of the various recom-mendation techniques and the actual rating value of the user for the particular app. Note that for our version-sensitive recommendation (VSR) score, we map the score of the version-snippet to the app. We assume a recommendation technique  X  such as CF and CBF or any other  X  provides a probability of the likelihood of user u consuming or downloading app a . The features given to GTB are a set of probability scores of each of the recommendation techniques, VSR, CF, and CBF; the output of GTB is a predicted score between 0 and 5. The predicted ratings are then ranked in reverse order for recommendation.
We preface our evaluation proper by detailing: 1) how we con-structed our dataset, 2) how we chose our evaluation metric, 3) our setting for the dataset, and 4) the baselines that we compare our approach against.
We constructed our dataset by culling from the iTunes App Store and AppAnnie 6 . The dataset consists of the following elements: 1. App Metadata . App metadata consists of an app ID, title, 2. Version Information . For each app, we utilize a separate 3. Ratings . For each version, we utilize yet another crawler
We further process the dataset by selecting apps with at least 5 versions, documents ( i.e. , version-snippets) with at least 10 ratings, and users who rated at least 20 apps. With these criteria enforced, our dataset consists of 9,797 users, 6,524 apps, 109,338 versions, and 1,000,809 ratings. We then perform a 5-fold cross validation, where in each fold, we randomly select 20% of the users as tar-get users to receive recommendations. For each target user, we first remove 25% of their most recent downloaded apps, by de-fault. Additionally, among the training data, 70% is used for train-ing the latent topics while the remaining 30% is used for the train-ing of GTB. Recommendation is evaluated by observing how many masked apps are recovered in the recommendation list.
Our system ranks the recommended apps based on the ranking score. This methodology leads to two possible evaluation metrics: precision and recall. However, a missing rating in the training set is ambiguous as it may either mean that the user is not interested in the app, or that the user does not know about the app ( i.e. , truly miss-ing). This makes it difficult to accurately compute precision [26]. But since the known ratings are true positives, we believe that re-call is a more pertinent measure as it only considers the positively rated apps within the top M , namely, a high recall with a lower M will be a better system.

As previously done in [26, 15], we chose Recall@ M as our pri-mary evaluation metric. Let n u and N u be the number of apps the user likes in the top M and the total number of apps the user likes, respectively. Recall@ M is then defined as their ratio: n compare systems using average recall, where the average is com-puted over all test users.
For the number of topics K of LDA and LLDA, we experimented on a series of K values between 100 to 1200 for each topic model, and selected the K that maximizes the recall in each model. For the  X  and  X  hyperparameters of LDA and LLDA, we used a low  X  -value of 0.01 as we want to constrain a document to contain only a mixture of a few topics; likewise, we used a low  X  -value of 0.01 to constrain a topic to contain a mixture of a few words. For the parameters of GTB, we used the default values in scikit-learn 8 , whereby we employed 500 trees, a depth level of 3, and the least square for the loss function.
We considered two state-of-the-art recommendation techniques as baselines: (i) probabilistic matrix factorization (PMF) [22] which represents collaborative filtering (CF); and (ii) latent Dirichlet allo-cation (LDA) [3] which represents content-based filtering (CBF).
PMF has been widely used in previous works [22, 1, 16] as an implementation of CF as it is highly flexible and easy to extend. On the other hand, LDA has been used in previous works [3, 17, 26, 15] as an implementation of CBF as it effectively provides an inter-pretable and low-dimensional representation of the items. Note that in the context of our experiments, LDA X  X  implementation of CBF Figure 4: For each of the four topic models, we experimented with various K between K =100 and K =1200, and show a sub-sampled chart of K intervals that are fixated at Recall@100. uses the apps X  descriptions as documents  X  not the version fea-tures. Besides pure CF and CBF, we also show the recommendation accuracy obtained by hybrid of individual techniques, namely, (i) CF+CBF, (ii) CF+VSR, (iii) CBF+VSR, and (iv) CF+CBF+VSR, where VSR represents our version-sensitive recommendation ap-proach proposed in Section 3.
We first show the recommendation accuracy evaluated with re-call by varying the number of latent topics K , and then show how recall is affected when we exclude an app X  X  genre information. Af-ter which, we show the performance of the four topic models pro-posed in Section 3.2. Finally, we compare our approach with other recommendation techniques, including hybrid methods described in Section 4.4.
We optimize the number of topics, K , for our VSR approach with respect to our four new topic models. Figure 4 shows the re-call when varying K for LDA, inj+LDA, LLDA, and inj+LLDA, respectively. We observe that K =1000 gives the best recall scores for all four models, and that the recall scores generally show a steep increase towards the optimum ( i.e. , between K =600 and K =1000), and then gradually decline once K exceeds this optimum ( i.e. , be-tween K =1000 and K =1200). K =1000 may be seen as a large number of topics, but as observed by [29], larger datasets like ours (we have 109,338 documents) may necessitate a larger number of topics to be modeled well. Additionally, as we had previously con-strained both hyperparameters of the topic models to be small (re-sulting in low topic-mixture per document), more topics are needed to represent the set of documents.
Our framework allows each genre to assign different weights to identical latent topics. In order to determine the importance of genre information, we compare the recommendation accuracies between models with and without genre information. Both vari-ants are based on the best-performing model (inj+LLDA). Figure 5 shows that the variant incorporating genre outperforms the plain model with a statistical significance at p &lt; 0.01. We conclude that genre information is an important discriminatory factor, as each genre weights the same type of version update differently. For ex-ample, a version update that offers the support for HD displays would be more attractive and relevant to a game app instead of a music app. Therefore, by discriminating the genres, we assign Figure 5: Recall scores between the inj+LLDA model that uses genre information and another that does not.
 Figure 6: Recall scores of different topic modeling schemes with K =1000 as the optimal number of topics. more relevant weights, which results in better recall. As such, we use genre information in all of the subsequent experiments.
Figure 6 shows the performance of the five different topic model variants: (i) supervised-LLDA ( i.e. , without K latent topics), (ii) LDA, (iii) inj+LDA, (iv) LLDA, and (v) inj+LLDA. So that we can compare unsupervised, supervised, and semi-supervised mod-els, we added supervised-LLDA for the purpose of completeness.
We see that recall is consistently improved as the basic LDA model is incrementally enhanced through inj+LDA, LLDA, and inj+LLDA. Between the inj+LDA and inj+LLDA models that use the enhanced-corpus ( cf . Section 3.2.2) and the LDA and LLDA models that do not, we observe that the enhanced-corpus generally provides better recall, with inj+LLDA showing more significant performance against LLDA. Furthermore, both models of LLDA ( i.e. , LLDA and inj+LLDA) consistently outperform the pure LDA models, which shows that semi-supervised LLDA models are su-perior to LDA, which is due to LLDA X  X  ability to quantify broad trends via latent topics while at the same time uncovering specific trends through observed metadata.

We added supervised-LLDA as a baseline for this specific eval-uation, but we see that it performs worst among all the baselines. The reason why supervised-LLDA is the worst model despite hav-ing  X  X upervision X  is that it does not have sufficient topics to prop-erly capture the essence of the corpus.

As inj+LLDA is the best-performing model among the topic mod-els we have tested, we use it in subsequent comparisons. We see that use of version metadata improves recall, as the three models Figure 7: Recall scores of our version-sensitive model (VSR) against other individual recommendation techniques.
 Figure 8: Recall scores of various combinations of recommen-dation techniques. that utilize metadata ( i.e. , inj+LDA, LLDA, and inj+LLDA) con-sistently outperform the LDA model that only utilizes the text from version-snippets.
Figure 7 shows the recall scores of the three individual tech-niques  X  VSR, CF, and CBF  X  where the VSR approach uses inj+LLDA at the optimal settings of K =1000. While VSR under-performed against CF, it does outperform CBF. We believe this is because the textual features in the app descriptions are noisy [15], resulting in poor recommendation. Thus, among the content-based recommendation approaches of the app domain, version features are promising replacements for app descriptions.

Figure 8 shows the combination of individual techniques us-ing GTB. We observe that combining VSR with CBF or CF ( i.e. , CBF+VSR or CF+VSR) improves both CF or CBF alone. This sug-gests that version features are a good complement to the traditional recommendation techniques that treat apps as static items. As ver-sion features focus on the unique differences between various states of an app, they play a natural complementary role for CF or CBF alone. In addition, we have further confirmed that feature-wise, version features are better content descriptions as CF+VSR outper-forms CF+CBF. Furthermore, we note that the best performing hy-brid is CF+CBF+VSR, though it is roughly on par with CF+VSR. Finally, the hybrid methods CF+VSR and CF+CBF+VSR outper-form the pure CF model with a statistical significance of p &lt; 0.01 at Recall@50. Figure 9: Comparison of normalized score among past (current  X  1 to  X  7), current, and future (current +1 to +7) versions.
We examine the experimental results obtained by the use of ver-sion features in detail. First, we perform an in-depth study that compares a recommended version against previous and future ver-sions of the same app. Next, we perform a micro-analysis on indi-vidual latent topics and investigate the terms that are found in each topic. Finally, we investigate the effect of injecting more complex version-category information.
From our dataset, we only know which version of an app a tar-get user has downloaded. However, we do not know whether the user has or has not seen previous versions of the app before down-loading the current version. For example, we only know that Bob downloaded AngryBirds Version 2.1 but we do not know whether:
Hence, we need to consider the situation where a user did not download a target app earlier even though it might be available for download; and that it was only after a version-update did the app attract him. For this reason, based on every app that each target user in the training set downloaded, we input the current version ( i.e. , the version which the target user downloaded) as well as the previous and future versions of the same app, and find out whether our system can recommend the exact version that the target user downloaded.

In order to conduct a fair study, we have to take into account the fact that every app has different number of version updates. For example, some apps may only have 5 different version updates while others may have as many as 20 version updates. To solve this problem, we fit the versions of every app into three sets of bins: The first set of bins denotes the previous versions ( i.e. , bins #1 to #7), the second set of bins denotes the version of the app that a user has downloaded ( i.e. , bin #8), the last set of bins represent the future versions ( i.e. , bins #9 to #15). Then, for every app that a target user has consumed, we calculate the score for each version (explained in Section 3.5), and enter the score into the respective bins. Finally, we normalize the score of every bin.

Figure 9 shows the normalized score of this analysis for all tar-get users in the training set. We observe that our approach favors the current version ( i.e. , the one that was downloaded by the target users) the most, thereby indicating that our VSR model effectively targets the version of an app that maximizes its chances of being acquired by the target user. This also reflects that apps tend to go through a series of revisions before being generally favorable; after which the subsequent versions show a decline in general interest, and this suggests the peripheral nature of the subsequent revisions.
To further understand why injecting pseudo-terms into the cor-pus improves recommendation accuracy, we perform a micro anal-ysis by exploring the latent topics discovered by inj+LLDA. We selected the three most important latent topics based on the expec-tation of each latent topic over the set of training data. Note that each latent topic contains a set of words as well as the injected pseudo-terms.

Figure 10 shows the three topics. We observe that every topic coincides with a certain theme. In addition, from the pseudo-terms found in the topic, we can discern the kind of version-category and genre mixture information the topic belongs to. For exam-ple, Topic #385 contains words like  X  X etina X  and  X  X esolution X , cor-rectly suggesting that the update is display-related. In addition, we observe what genres of apps most likely have such updates, which are the Utilities and Productivity apps (in red). Furthermore, we observe that updates in Topic #385 are strongly related to the version-category minor (in blue). On the other hand, Topic #47 is associated with navigation and traveling , as the genre-related pseudo-terms (in red) suggests. The top natural language words found in Topic #47 also agree with the hashtags, in that the re-lated updates include improvements in mapping and routing, and that the updates also include alerts and notifications with regards to traveling-related information, such as fuel, points of interests (POIs), and accidents. Finally, as we recall that inj+LLDA allows the incorporation of  X  X bserved X  labels as topics, the third topic is related to the  X  X edical genre X  label and it is closely associated with apps in the neighboring  X  X ealth &amp; Fitness X  genre. This  X  X bserved X  label/topic mainly deals with providing users visual reports (such as graphs and charts) about their personal health (such as periods and pregnancy) as well as the provision of personal tracking and re-minders. We observe that the injected pseudo-terms act as a guide for inj+LLDA X  X  inferencing process, which contributes to better la-tent topic generation. It also helps in understanding the topics fur-ther as the metadata ( i.e. , version-categories and genre mixture) that is imbued in the topics gives users a more comprehensible under-standing of the topics.
To verify the importance of various version-categories ( i.e. , ma-jor, minor, and maintenance), we calculate their respective scores based on (i) the topic-word distribution from the topic model, and (ii) the importance score of the latent topics (Section 3.3), which is essentially: P g  X  G P z  X  Z w inj+LLDA ( g,z )  X  p ( w = m | z ) , where m represents one of the strings:  X #major X ,  X #minor X , or  X #main-tenance. X  Note that p ( w | z ) is the topic-word distribution in Equa-tion (2). Also note that the equation must be normalized, which results in the score being between 0 and 1.

The importance of each of the three version-categories are as fol-lows: (i)  X #major X : 0.128, (ii)  X #minor X : 0.656, and (iii)  X #mainte-nance X : 0.216. It is evident that the  X  X inor X  version category is the one that is generally more favorable. This is because major updates tend to be buggy, while minor or maintenance updates after a major update would likely fix the bugs that occurred in the major release, leading to higher user satisfaction. The reason why minor performs better than maintenance ( i.e. , 0.656 vs 0.216) is that a minor update Figure 11: List of standard and advanced hashtags for corpus-injection.
 Figure 12: Recall scores between the use of  X  X tandard X  and  X  X dvanced X  version-categories. typically introduces important bug fixes or functionalities, which is more appreciable than a maintenance update that resolves trivial issues of the app.

As version-categories are valuable features, we hypothesize that the recommendation accuracy can be improved if we further aug-ment the version-categories. More specifically, as we previously only considered three standard version-categories: #major, #mi-nor, and #maintenance, we consider improving the recommenda-tion performance by injecting a more comprehensive list of version-categories into the corpus (as in Figure 11). Figure 12 shows the comparison between the standard and such an advanced set of version-categories (both models using inj+LLDA). Incorporating the ad-vanced version-categories improves recommendation accuracy, as instead of identifying only 3 standard version-categories, we can discriminate among 6 additional scenarios. The additional details and specifications given by advanced version-categories effectively improve recommendation accuracy. We observe that advanced version-category model outperforms the standard model, particularly at the lower (more important) app recommendation ranks ( X  X  X ), although not statistically significantly so.

A more comprehensive modeling of version may be promising, and as such, since there is evidence that the sequence of versions would help, we plan to model the sequence of versions in future work.
In this paper, we leverage the unique properties in the app do-main and explored the effectiveness of using version features in app recommendation. Our framework utilizes a semi-supervised variant of LDA that accounts for both text and metadata to char-acterize version features into a set of latent topics. We used genre information to discriminate the topic distributions and obtained a recommendation score for an app X  X  version for a target user. We also showed how to combine our method with existing recommen-dation techniques. Experimental results show that genre is a key factor in discriminating the topic distribution while pseudo-terms based on version metadata are supplementary. We observed that a hybrid recommender system that incorporates our version-sensitive model statistically outperforms a state-of-the-art collaborative fil-tering system. This shows that the use of version features com-plements conventional recommendation techniques that treat apps as static items. We also performed a micro-analysis to show how our method targets particular versions of apps, allowing previously disfavored apps to be recommended.

In our future work, we plan to investigate the use of a vari-ant of the tf-idf scheme to further vary the weights of the latent topics, allowing us to reward less common topics. We also plan to investigate more advanced techniques such as treating versions as inter-dependent and using a decaying exponential approach to model how versions are built upon one another in sequence. This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.
