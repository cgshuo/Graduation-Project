 With the explosive growth of the Web, it becomes more and more difficult for surfers to find valuable pages in such huge repository. Consequently, search en-gines come forth to help them to retrieve appropriate and valuable web pages. as conventional information retrieval systems where only relevance scores are utilized to sort pages for a certain query. Whereas, researchers found that this scheme merely led to a poor result in the web. The top-ranking pages were often not the most valuable ones and sometimes were even rubbish. In other words, high relevance score does not mean high quality.
 pages by their importance. PageRank [2] and HITS [6] are two of the most popular algorithms, which utilize hyperlinks to compute the importance of each page. Taking relevance and rank into account, the quality of top-ranking retrieval pages can be improved by much.
 perlinks to measure the importance of pages are referred to as link analysis algorithms in the literature. The hyperlink between two web pages is treated as a kind of recommendation from source to destination. If there is a hyperlink from page A to page B, we believe A endorses B for its importance. Hence, the Web can be considered as a tremendous voting system. With the continuous iteration of voting, each page will get a stable measurement of its importance eventually. analysis algorithms, which consider each hyperlink to be identical in sense of recommendation. However, we argue it is not the best way of utilizing hyper-links although it has worked well. Optimally, different hyperlinks should have different weights in the voting process. For example, a hyperlink from the portal of a website to a common page should have stronger recommendation than a hyperlink from the common page to the portal. And we believe that the weight of a hyperlink should be decided by the level properties of its two ending pages. rithms, named level-based link analysis . Compared to previous algorithms, each hyperlink will be assigned a weight to express its strength of recommendation. By applying this concept, almost all previous link analysis algorithms can be refined with only a little modification to the adjacent matrix of web graph. In following sections, we will show how to combine traditional link analysis methods with this level-based concept in details.
 previous works to show the common process of link analysis. In Section 3, we describe the level-based link analysis in details. The experiments and correspond-ing results are shown in Section 4. Finally, we give the concluding remarks and future works in Section 5. We might feel that hyperlinks make up a great part of the Web from the saying  X  X he Web is a hyperlinked environment X  [6]. In the literature, link analysis algo-rithms have shown their success in measuring the importance of pages. Among them, PageRank and HITS are two of the widely-recognized representatives. definitions first. In many works, the Web were modelled as a directed graph where V = { 1 , 2 ,  X  X  X  ,n } is the collection of nodes, each of which represents a page; and E = { &lt;i,j&gt; | i, j  X  V } is the collection of edges, each of which represents a hyperlink. For example, &lt;i,j&gt; means a hyperlink from page i to page j .
 A ij = 0. This matrix is the core component of link analysis algorithms. 2.1 HITS The HITS algorithm assigns two numeric properties to each page, called au-thority score and hub score. The higher authority score a page has, the more important it will be. If a page points to many pages with high authority score, it will obtain a high hub score. If a page is pointed by many pages with high hub score, it will obtain a high authority score symmetrically. Hub scores and authority scores exhibit a mutually reinforcing relationship. We can obtain the two scores of each page in an iterative manner.
 hub scores of the web graph respectively. Without regard to normalization, the iteration process can be formulated as follows [9]: will be the principal eigenvectors of A T A and AA T respectively, when A T A as well as AA T has unique principal eigenvector [5]. 2.2 PageRank The PageRank algorithm assigns one numeric property, called PageRank, to each page to represent its importance. This algorithm simulates a random walk process in the web graph. Suppose there is a surfer in an arbitrary page of the Web. At each step, he/she will transfer to one of the destination pages of the hyperlinks on the current page with probability  X  , or to another page in the whole graph with probability 1  X   X  . This process can also be formulated in an iterative manner. probability matrix A . Then the above random walk can be represented as where U is a uniform probability transition matrix, all elements of which equal of the whole web graph. It can be computed through the below iterative process: A In this section, we illustrate the concept of level-based link analysis. First, we discuss how to compute the weight of each hyperlink so as to define the level-based adjacent matrix. Then we show how to add this concept to existing link analysis algorithms. 3.1 Weight of the Hyperlink As aforementioned, the existing link analysis methods treat all hyperlinks iden-tically in sense of recommendation. However, as we know, the Web is not orga-nized with a flat structure but multi-level structure. Thus, hyperlinks should be treated non-identically. Then comes the problem of how to define the difference between two hyperlinks. To tackle it, we make use of the level properties of pages in the website. In particular, this can be illustrated by Fig.1, where a website is denoted by a tree; the circles denote pages; the solid lines denote hyperlinks while the dash lines denote the organization structure.
 hyperlinks pointing to j from i 1 and i 2 respectively. Denote the level property of page i by l i .If i is on the highest level, let l i = 1. And l i increases by one when i goes down to the next level of the tree. Here we use w j | i to represent the weight of the hyperlink from i to j and use anc ( i, j ) to denote the ancestor of these two pages.
 question is which hyperlink is stronger in sense of recommendation with respect to j , the one from i 1 or from i 2 . To answer this question, we design two intuitive and reasonable rules as follows.
 Rule 1. In the case shown in Fig.1(a) , where the joint-ancestor of i 1 and j is recommendation than &lt;i 2 ,j &gt; , i.e. tiously. Once a hyperlink appears in it, it means that the destination of the hyperlink is endorsed with much deliberation of the author. Thus, the hyperlink would probably have strong recommendation. So, we claim that the hyperlink from a higher-level page will have larger weight than the hyperlink from a lower one when other conditions are the same.
 Rule 2. In the case shown in Fig.1(b) , where the joint-ancestor m 1 of i 1 and has stronger recommendation than &lt;i 1 ,j &gt; , i.e. focuses on the sports (its URL is http://www.***sports.com). And m 1 is the entry point of one sub-topic of that site. For example, the URL of m 2 is* http://www.***sports.com/football. As i 1 and j are in the same site and share the same topic, it is very common that there is a hyperlink between them. However, such a hyperlink may have more organizational sense than recommen-dation. Comparatively, the hyperlink between i 2 and j would represent more recommendation.
 perlink as follows: not come out a realization of (8). In this case, we construct a virtual page in the higher level than any existing pages, denoted by m c (the dotted circle in Fig.1(c)). We consider this virtual page as the parent of the pages in the top level, or the root of all web sites. The level property of this virtual page is denoted by l 0 . To guarantee the denominator in (8) is not zero, let l 0 =0 . 1. matrix  X  L as follows: final-version of level-based adjacent matrix after taking more information into consideration as shown in the next sub section. 3.2 Level-Punishment Intuitively, the higher level a page is on, the more important it is. Therefore, a page X  X  importance should be punished by its level property l i . Take HITS for example. After replacing A by  X  L , (4) and (5) can be rewritten as follows: introduce it into the calculation of (10) and (11). Then we have If define we can obtain pared with (4) and (5), we only replaced A by L in LBHITS, where algorithms, we can always obtain the level-based version of the original link analysis algorithms accordingly. We omit the corresponding deductions here for simplicity.
 3.3 Convergence of Level-Based Link Analysis In this subsection, we give the proofs of the convergence of LBHITS and level-based PageRank(LBPR). For other level-based link analysis algorithms, the proofs are similar.
 Lemma 1. If A is a symmetric matrix and x is a vector not orthogonal to the principal eigenvector of A , then when the principal eigenvector of A is unique. And x  X  equals to the unique prin-cipal eigenvector [5] .
 Theorem 1 (Convergence of LBHITS). Replacing A by L in (4) and (5) , a and h will converge to a  X  L and h  X  L respectively.
 Proof. Let h (0) denote the arbitrary initial value of authority. Then after k steps of iteration, we can easily obtain In terms of above lemma, because h (0) is an arbitrary value, we suppose it is not orthogonal to the principal eigenvector of LL T . Hence, h ( k ) converges to a limit h . In the same way, L T h (0) can be also considered to be not orthogonal to the principal eigenvector of L T L so that a ( k ) converges to a limit a  X  L . Theorem 2 (Convergence of LBPR). Replacing A by L in computing LBPR,  X  will converge to  X   X  L .
 Proof. After replacement, we can obtain Because L is finite and non-negative, L is finite and non-negative, too. Besides, U is a uniform probability transition matrix with any element positive so that L must be finite and absolutely positive. Thereby L is an irreducible probability transition matrix. It must have stationary distribution which can be computed as follows.
 In our experiments, the topic distillation task of TREC2003 web track was used to evaluate our algorithms. The data corpus in this task was crawled from .gov domain in 2002. It contains 1,247,753 documents, 1,053,111 of which are html files. We only used these html files in our experiments. (provided by the TREC committee) for each query ranged from 1 to 86 with an average of 10.32.
 construct the sitemap. After that, we will describe the implementation of the baseline algorithms and then show the improved retrieval performance after in-troducing the link-based analysis to the retrieval framework. 4.1 Sitemap Construction The sitemap is usually referred to as a regular service which is provided by many websites to represent their organizational structures. However, in our ex-periments, the sitemap is a data structure and contains more information than its original. The sitemap records the level properties of pages and the hierar-chical structure of the website. In other words, it must record the parent-child relationship of pages. There is a constraint here that each page can have only one parent. In our current implementation, we define the sitemap strictly as a tree. A typical sitemap is showed in Fig. 2. Because the sitemap is implicit in many websites, we need an algorithm to construct it from the relationship among web pages automatically. For this purpose, we make use of the URLs of each page to construct sitemap according to the following rules. 1. For the URL whose format is like http://www.abc.gov/.../index.*, we regu-2. For those pages with only one-level URL such as http://www.usgs.gov/ and 3. For those pages with multi-level URL, we will find a parent for them. For ex-website. We acknowledge the above process is not very accurate, but sitemap construction is not the focus of our paper although we can foresee that the better sitemap we have, the more effective our level-based link analysis will be. 4.2 Algorithm Confirmation In web information retrieval system, when a query is submitted, we firstly com-pute the relevance score of each page with respect to the query and select top n relevant pages. Secondly, we integrate the relevance score and the rank score linearly into the final score that is used to resort the top n pages, formulating as follows: The retrieval result without regard to rank scores is called baseline, where the mean average precision(MAP) is 0.1367 and the precision at ten(P@10) is 0.108. Compared with the best result of TREC2003 participants (with MAP of 0.1543 and P@10 of 0.1280), this baseline is reasonable.
 values. To illustrate the advantage of our level-based link analysis algorithm, we choose HITS for comparison.
 of HITS, the size of the root set is 200 and the in-link parameter is 50. Besides, the intrinsic links are removed before computing authority and hub scores. We implement the HITS algorithm strictly according to the above descriptions. and in-link parameter as in the original HITS algorithm. However, we don X  X  remove intrinsic links. The reason for removing intrinsic links is that  X  X ntrinsic links very often exist purely to allow for navigation of the infrastructure of a site X  [6]. However, as we have punished the weight of these intrinsic links in our algorithm thus we don X  X  need to remove them at all. 4.3 Experimental Result In this subsection, we listed the retrieval performance of both HITS and LBHITS on the TREC 2003 topic distillation task.
 top-1000 pages for evaluation. Both MAP and P@10 are used to evaluate the performance of the algorithms.
 The curves of both HITS+Relevance and LBHITS+Relevance converge to the baseline when  X  =1.
 has already been better than HITS, although the corresponding performance is very low. After combining with relevance scores, both HITS+Relevance and LB-HITS+Relevance get improvement over the baseline. For LBHITS+Relevance, the best P@10, 0.136, is achieved when  X  =0 . 68. This is better than HITS+Relevance and the baseline by 13.3% and 25.9% respectively. And the best MAP, 0.162, is achieved when  X  =0 . 77. This corresponds to 7% and 18.5% improvements over HITS+Relevance and the baseline.
 2003 while LBHITS+Relevance sometimes performs better than the best result. For the best case, LBHITS+Relevance achieves 6.3% and 5.0% improvements over the best result of TREC 2003 in sense of MAP and P@10 respectively. cordance with our theoretical analysis in the previous sections. At the end of this section, we place the retrieval performance of all algorithms tested in our experiments in a table for a comprehensive comparison. In this paper, we refine the previous link analysis algorithms by introducing the level properties of pages. We discuss the reasoning and propose how to define the weight of the hyperlink based on the level properties of its two endings. Through our experiments, we prove that the level-based link analysis can give rise to higher retrieval performance than the previous algorithms.
 First, how to define the weight of the hyperlink to better represent the influence of the level property is still a challenge. In section 3, we just discover the trend of the weight with respect to the changing endings. However, it is just a na  X   X ve attempt. There must be some more precise and effective approaches to represent the strength of recommendation of the hyperlink. Second, as we have shown, the level property can improve the performance of rank. It is natural that we want to know whether it can improve the performance of relevance as well. Because the Web is naturally organized with multi-level structure, we believe that the level property should influence every aspect of researches on the Web, including the relevance and many other basic components.

