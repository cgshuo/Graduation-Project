 Bin Zhao zha obinhere@hotmail.com Fei Wang feiw ang03@mails.tsinghua.edu.cn Changsh ui Zhang zcs@mail.tsinghua.edu.cn Clustering (Duda et al., 2001; Shi &amp; Malik, 2000; Ding et al., 2001) aims at dividing data into groups of sim-ilar objects, i.e. clusters. Recen tly, motiv ated by the success of large margin metho ds in supervised learn-ing, (Xu et al., 2004) prop osed maximum margin clus-tering ( MMC ), which borrows the idea from the sup-port vector machine theory and aims at  X nding the maxim um margin hyperplane which can separate the data from di X eren t classes in an unsup ervised way. Technically , what MMC does is to  X nd a way to label the samples by running an SVM implicitly , and the SVM margin obtained would be maximized over all possible labelings (Xu et al., 2004). However, unlik e supervised large margin metho ds which are usually for-mulated as convex optimization problems, maximum margin clustering is a non-c onvex integer optimization problem, which is much more di X cult to solve. Several attempts have been made to solve the maxi-mum margin clustering problem in polynomial time. (Xu et al., 2004) and (Valizadegan &amp; Jin, 2007) made several relaxations to the original MMC problem and reform ulated it as a semi-de X nite programming ( SDP ) problem. However, even with the recen t advances in interior point metho ds, solving SDP s is still computa-tionally very expensiv e. Consequen tly, the algorithms can only handle very small datasets containing several hundreds of samples. More recen tly, Zhang et al . uti-lized alternating optimization techniques to solve the MMC problem (Zhang et al., 2007), in which the max-imum margin clustering result is obtained by solving a series of SVM training problems. However, there is no guaran tee on how fast it can converge and the algo-rithm is still time demanding on large scale datasets. Moreo ver, the metho ds describ ed above can only han-dle binary clustering problems (Zhao et al., 2008), and there are signi X can t complications to deriving an e X ec-tive maximum margin clustering approac h for the mul-ticlass scenario 1 . Therefore, how to e X cien tly solve the multiclass MMC problem to make it capable of clus-tering large scale dataset is a very challenging researc h topic.
 In this paper, we prop ose a cutting plane multi-class maximum margin clustering algorithm CPM3C . Speci X cally , the algorithm constructs a nested se-quence of successiv ely tighter relaxations of the origi-nal multiclass MMC problem, and each optimization problem in this sequence could be e X cien tly solved us-ing the constr ained concave-c onvex procedure ( CCCP ). Moreo ver, we show that the computational time of CPM3C scales roughly linearly with the dataset size. Our experimen tal evaluations on several real world datasets show that CPM3C performs better than ex-isting MMC metho ds, both in e X ciency and accuracy . The rest of this paper is organized as follows. Section 2 will show the CPM3C algorithm in detail, and the time complexit y analysis of CPM3C will be presen ted in section 3. Section 4 presen ts the experimen tal re-sults on several real world datasets, followed by the conclusions in section 5. We will formally presen t the cutting plane multiclass maximum margin clustering ( CPM3C ) algorithm in this section. 2.1. Multiclass Maxim um Margin Clustering Maximum margin clustering ( MMC ) extends the the-ory of support vector machine ( SVM ) to the un-supervised scenario. Speci X cally , given a point set X = f x 1 ;  X  X  X  ; x n g and their labels y = ( y 1 ; : : : ; y f 1 ; : : : ; k g n , SVM de X nes a weight vector w p for each class p 2 f 1 ; : : : ; k g and classi X es sample x by y tained as follows 2 (Crammer &amp; Singer, 2001) where the data samples in X are mapp ed into a high (possibly in X nite) dimensional feature space, and by using the kernel trick, this mapping could be done im-plicitly . However, in those cases where kernel trick cannot be applied, it is possible to compute the co-ordinates of each sample in the kernel PCA basis (Sch X olkopf et al., 1999) according to kernel K . There-fore, throughout the rest of this paper, we use x i to denote the sample mapp ed by the kernel function. Instead of  X nding a large margin classi X er given labels on the data as in SVM , MMC targets to  X nd a label-ing that would result in a large margin classi X er (Xu et al., 2004). That is to say, if we subsequen tly run an SVM with the labeling obtained from MMC , the mar-gin would be maximal among all possible labelings. multiclass MMC could be form ulated as follows: where P n i =1  X  i is divided by n to better capture how the regularization parameter  X  scales with the dataset size. Di X eren t from SVM , where the class labels are given and the only variables are ( w 1 ; : : : ; w k ), MMC targets to  X nd not only the optimal weight vectors, but also the optimal labeling vector y  X  . 2.2. Cutting Plane Algorithm In this section, we will reform ulate problem (2) to re-duce the number of variables. Speci X cally , Theorem 1 Problem (2) is equivalent to where I (  X  ) is the indicator function and the label for sample x i is determine d as Proof. We will show that for every ( w 1 ; : : : ; w k ) the smallest feasible P n i =1  X  i are identical for problem (2) and problem (3), and their corresp onding labeling vec-tors are the same. For a given ( w 1 ; : : : ; w k ), the  X  problem (2) can be optimized individually . According to the constrain t in problem (2), As the objectiv e is to minimize 1 value for  X  i is and we denote the corresp onding class label by y (1) i . Without loss of generalit y, we assume the following relationship r x i ) g X  1, therefore, y Similarly , for problem (3), the optimal value for  X  i is and the class label could be calculated as Therefore, the objectiv e functions of both optimiza-tion problems are equiv alent for any ( w 1 ; : : : ; w k the same optimal  X  i , and consequen tly so are their op-tima. Moreo ver, their corresp onding labeling vectors y are the same. Hence, we proved that problem (2) is equiv alent to problem (3). 2 By reform ulating problem (2) as problem (3), the num-ber of variables involved is reduced by n , but there are still n slack variables  X  i in problem (3). De X ne e p as the k  X  1 vector with only the p -th elemen t being 1 and others 0, e 0 as the k  X  1 zero vector and e as the all one vector. To further reduce the number of vari-ables involved in the optimization problem, we have the following theorem Theorem 2 Problem (3) can be equivalently formu-lated as problem (11), with  X   X  = 1 1 : : : ; k and each constr aint c is represente d as a k  X  n matrix c = ( c 1 ; : : : ; c n ) .
 Proof. To justify the above theorem, we will show that problem (3) and problem (11) have the same objectiv e value and an equiv alent set of constrain ts. Speci X cally , we will prove that for every ( w 1 ; : : : ; w the smallest feasible  X  and P n i =1  X  i are related by  X  = 1 n P n i =1  X  i . This means, with ( w 1 ; : : : ; w tions to problem (3) and (11) respectiv ely, and they result in the same objectiv e function value. For any given ( w 1 ; : : : ; w k ), the  X  i in problem (3) can be optimized individually and the optim um is achieved as where we assume the relation in (7) holds.
 Similarly for problem (11), the optimal  X  is Since each c i are indep enden t in Eq.(13), they can be optimized individually . Therefore, Hence, for any ( w 1 ; : : : ; w k ), the objectiv e functions for problem (3) and problem (11) have the same value given the optimal  X  and  X  i . Therefore, the optima of the two optimization problems are the same. 2 Putting theorems 1 and 2 together, we could there-fore solve problem (11) instead to  X nd the same max-imum margin clustering solution, with the number of variables reduced by 2 n  X  1. Although the number of variables in problem (11) is greatly reduced, the number of constrain ts increases from nk to ( k + 1) n . The algorithm we prop ose in this paper targets to  X nd a small subset of constrain ts from the whole set of constrain ts in problem (11) that ensures a su X cien tly accurate solution. Speci X cally , we emplo y an adapta-tion of the cutting plane algorithm (Kelley , 1960) to solve problem (11), where we construct a nested se-quence of successiv ely tighter relaxations of problem (11). Moreo ver, we can prove theoretically (see sec-tion 3) that we can always  X nd a polynomially sized subset of constrain ts, with which the solution of the relaxed problem ful X lls all constrain ts from problem (11) up to a precision of  X  . That is to say, the remain-ing exponen tial number of constrain ts are guaran teed to be violated by no more than  X  , without the need for explicitly adding them to the optimization problem (Tso chantaridis et al., 2005). Speci X cally , the CPM3C algorithm keeps a subset  X  of working constrain ts and computes the optimal solution to problem (11) subject to the constrain ts in  X . The algorithm then adds the most violated constrain t in problem (11) into  X . In this way, a successiv ely strengthening appro ximation of the original MMC problem is constructed by a cut-ting plane that cuts o X  the curren t optimal solution from the feasible set (Kelley , 1960). The algorithm stops when no constrain t in (11) is violated by more than  X  . Here, the feasibilit y of a constrain t is measured by the corresp onding value of  X  , therefore, the most vi-olated constrain t is the one that results in the largest  X  . Since each constrain t in problem (11) is represen ted by a k  X  n matrix c , then we have Theorem 3 De X ne p  X  = arg max p ( w T p x i ) and r  X  = constr aint could be calculate d as follows Proof. The most violated constrain t is the one that would result in the largest  X  . As each c i in the con-strain t is indep enden t, in order to ful X ll all constrain ts in problem (11), the value of  X  is as follows where t i =( w T 1 x i ; : : : ; w T k x i ) T . Since c i c selects the largest elemen t of the vector e  X  w T p  X  x i z + t i , which could be calculated as 1  X  ( w T p  X  x i  X  w T Therefore, the most violated constrain t c that results in the largest  X   X  could be calculated as in Eq.(14). 2 The CPM3C algorithm iterativ ely selects the most vi-olated constrain t under the curren t weight vectors and adds it into the working constrain t set  X  until no vio-lation of constrain ts is detected. Moreo ver, if a point more, as in the objectiv e function of problem (11), there is a single slack variable  X  that measures the clus-tering loss. Hence, we could simply select the stopping criterion as all samples satisfying the inequalit y (15). Then, the appro ximation accuracy  X  of this appro xi-mate solution is directly related to the training loss. 2.3. Enforcing the Class Balance Constrain t In 2-class maximum margin clustering , a trivially \op-timal" solution is to assign all patterns to the same class, and the resultan t margin will be in X nite (Xu et al., 2004). Similarly , for the multiclass scenario, a large margin can always be achieved by eliminat-ing classes (Xu &amp; Schuurmans, 2005). Therefore, we add the following class balance constrain ts to avoid the trivially \optimal" solutions where l  X  0 controls the class imbalance. Therefore, multiclass maximum margin clustering with working constrain t set  X  could be form ulated as follows Before getting into details of solving problem (17), we  X rst presen t the CPM3C approac h in Algorithm 1. Algorithm 1 Cutting Plane Multiclass MMC
Initialize  X  =  X  repeat until ( w 1 ; : : : ; w k ) satis X es c up to precision  X  2.4. Optimization via the CCCP In each iteration of the CPM3C algorithm, we need to solve problem (17) to obtain the optimal classifying hyperplanes under the curren t working constrain t set  X . Although the objectiv e function in (17) is convex, the constrain ts are not, and this makes problem (17) di X cult to solve. Fortunately , the constr ained concave-convex procedure (CCCP) is just designed to solve those optimization problems with a conca ve-con vex objectiv e function under conca ve-con vex constrain ts (Smola et al., 2005). In the following, we will show how to utilize CCCP to solve problem (17).
 The objectiv e function in (17) and the second con-strain t are convex. Moreo ver, the  X rst constrain t is, al-though non-con vex, the di X erence of two convex func-tions. Hence, we can solve (17) with CCCP . Notice that while 1 convex, it is a non-smo oth function of ( w 1 ; : : : ; w To use CCCP , we need to calculate the subgradients : Given an initial point ( w (0) 1 ; : : : ; w (0) k ), CCCP com-placing 1 constrain t with its  X rst order Taylor expansion at By substituting the above  X rst-order Taylor expansion into problem (11), we obtain the following quadr atic programming (QP) problem: Moreo ver, the dual problem of (20) is a QP problem with j  X  j + 2 variables and could be solved in polyno-mial time, where j  X  j denotes the total number of con-strain ts in  X . Putting everything together, according to the form ulation of the CCCP (Smola et al., 2005), we solve problem (17) with the approac h presen ted in Algorithm 2, where we set the stopping criterion in CCCP as the di X erence between two iterations less than  X  % and set  X  % = 0 : 01, which means the curren t Algorithm 2 Solve problem (17) with CCCP
Initialize w p = w 0 p for p = 1 ; : : : ; k . repeat until stopping criterion satis X ed. objectiv e function is larger than 1  X   X  % of the objec-tive function in last iteration, since CCCP decreases the objectiv e function monotonically . 2.5. Theoretical Analysis We provide the following theorem regarding the cor-rectness of the CPM3C algorithm.
 Theorem 4 For any dataset X = ( x 1 ; : : : ; x n ) and to problem (11) with the class balanc e constr aint, then our CPM3C algorithm returns a point ( w 1 ; : : : ; w k ;  X  ) and satis X es the class balanc e constr aint. Moreover, the corresponding objective value is better than the one corresponds to ( w  X  1 ; : : : ; w  X  k ;  X   X  ) . Based on the above theorem,  X  indicates how close one wants to be to the error rate of the best classifying hyperplanes and can thus be used as the stopping cri-terion (Joac hims, 2006). In this section, we will provide analysis on the time complexit y of CPM3C . For the high-dimensional (say, d-dimensional) sparse data commonly encoun tered in applications like text mining and bioinformatics, we assume each sample has only s  X  d non-zero features, i.e. , s implies the sparsit y, while for non-sparse data, by simply setting s = d , all our theorems still hold. Theorem 5 Each iteration of CPM3C takes time O ( snk ) for a constant working set size j  X  j . Moreo ver, for the binary clustering scenario, we have the following theorem Theorem 6 For any  X  &gt; 0 ,  X  &gt; 0 , and any dataset X = f x 1 ; : : : ; x n g with samples belonging to two dif-ferent classes, the CPM3C algorithm terminates after adding at most R numb er indep endent of n and s .
 It is true that the number of constrain ts can poten-tially explo de for small values of  X  , however, experi-ence with CPM3C shows that relativ ely large values of  X  are su X cien t without loss of clustering accuracy . Since the number of iterations in CPM3C (with k = 2) is bounded by R and each iteration of the algorithm takes time O ( snk ) ( O ( sn ) for the binary clustering scenario), we arriv e at the following theorem Theorem 7 For any dataset X = f x 1 ; : : : ; x n g with n samples belonging to 2 classes and sparsity of s , and any  X xed value of  X  &gt; 0 and  X  &gt; 0 , the CPM3C algo-rithm takes time O ( sn ) to conver ge.
 For the multiclass scenario, experimen tal results shown in section 4 also demonstrate that the compu-tational time of CPM3C scales roughly linearly with the dataset size n . In this section, we will validate the accuracy and e X -ciency of the CPM3C algorithm on several real world datasets. Moreo ver, we will also analyze the scaling be-havior of CPM3C with the dataset size and the sensi-tivity of CPM3C to  X  , both in accuracy and e X ciency . All the experimen ts are performed with MATLAB 7.0 on a 1.66GHZ Intel Core TM 2 Duo PC running Win-dows XP with 1.5GB main memory . 4.1. Datasets We use eight datasets in our experimen ts, which are selected to cover a wide range of prop erties: Dig-its, Letter and Satellite from the UCI repository , MNIST 3 , 20 newsgroup 4 , WebKB 5 , Cora (Mc-Callum et al., 2000) and RCVI (Lewis et al., 2004). In order to compare CPM3C with other MMC algo-rithms which can only perform binary clustering, we choose the  X rst two classes from Letter and Satel-lite . For the 20 newsgroup dataset, we choose the topic rec which contains autos, motor cycles, base-ball and hockey from the version 20-news-18828. For WebKB , we select a subset consists of about 6000 web pages from computer science departmen ts of four schools (Cornell, Texas, Washington, and Wisconsin). For Cora , we select a subset containing the researc h paper of sub X eld data structure (DS), hardw are and architecture (HA), machine learning (ML), operating system (OS) and programming language (PL). For RCVI , we use the data samples with the highest four topic codes (CCA T, ECA T, GCA T and MCA T) in the \Topic Codes" hierarc hy in the training set. 4.2. Comparisons and Clustering Results Besides our CPM3C algorithm, we also implemen ts some other comp etitiv e algorithms and presen t their results for comparison. Speci X cally , we use K-Means (KM) and Normalized Cut (NC) as baselines, and also compared with Maxim um Margin Cluster-ing (MMC) (Xu et al., 2004), Generalized Maxi-mum Margin Clustering (GMC) (Valizadegan &amp; Jin, 2007) and Iterativ e Supp ort Vector Regres-sion (SVR) (Zhang et al., 2007) which all aim at clustering data with the maxim um margin hyperplane. Technically , for k-means , the cluster centers are ini-tialized randomly . For NC , the implemen tation is the same as in (Shi &amp; Malik, 2000), and the width of the Gaussian kernel is set by exhaustiv e searc h from the distance between any two data points in the dataset. Moreo ver, for MMC and GMC , the implemen tation is the same as in (Xu et al., 2004; Xu &amp; Schuurmans, 2005) and (Valizadegan &amp; Jin, 2007) respectiv ely. Fur-thermore, the implemen tation code for SVR is down-loaded from http://www.cse.ust.hk/  X  twinsen and the initialization is based on k-means with randomly se-lected initial data centers, and the width of the Gaus-sian kernel is set in the same way as in NC . In the experimen ts, we set the number of clusters equal to the true number of classes k for all the clustering algorithms. To assess clustering accuracy , we follow the strategy used in (Xu et al., 2004) where we  X rst take a set of labeled data, remo ve the labels for all data samples and run the clustering algorithms, then we label each of the resulting clusters with the major-ity class according to the original training labels, and  X nally measure the number of correct classi X cations made by each clustering. Moreo ver, we also calculate the Rand Index (Rand, 1971) for each clustering re-sult. The clustering accuracy and Rand index results are summarized in table 2 and table 3 respectiv ely, where the results for k-means and iterative SVR are averaged over 50 indep enden t runs and `-' means the corresp onding algorithm cannot handle the dataset in reasonable time. Since GMC and iterative SVR can only handle binary clustering problems, we also pro-vide experimen ts on several 2-class problems: Let-ters , Satellite , autos vs. motorcycles (Text-1) and baseball vs. hockey (Text-2) . Moreo ver, for the UCI-Digits and MNIST datasets, we enumerate all 45 possible class pairs, and report the average clus-tering results. Furthermore, as the MMC and GMC algorithms can only handle datasets with no more than a few hundred samples, we perform experimen ts on UCI Digits and focus on those pairs (3 vs 8, 1 vs 7, 2 vs 7, 8 vs 9, 0689 and 1279) that are di X cult to di X eren tiate. From the tables we can clearly observ e that our CPM3C algorithm can beat other comp eti-tive algorithms on almost all the datasets. 4.3. Speed of CPM3C Table 4 compares the CPU-time of CPM3C with other comp etitiv e algorithms. According to the ta-ble, CPM3C is at least 18 times faster than SVR , 200 times faster than GMC . As reported in (Valizadegan &amp; Jin, 2007), GMC is about 100 times faster than MMC . Hence, CPM3C is still faster than MMC by about four orders of magnitude. Moreo ver, as the sample size in-creases, the CPU-time of CPM3C grows much slower than that of iterative SVR , which indicates CPM3C has much better scaling prop erty with the sample size than SVR . Finally , CPM3C also performs much faster than conventional kmeans , which is a very appealing result. As for the Ncut metho d, since the calculation of the similarit y matrix is very time consuming and usually takes several hours on the text datasets, we do not report the time it spends here.
 4.4. Dataset size n vs. Speed In the theoretical analysis section, we state that the computational time of CPM3C scales linearly with the number of samples. We presen t numerical demonstra-tion for this statemen t in  X gure 1, where a log-log plot of how computational time increases with the size of the data set is shown. Speci X cally , lines in the log-log plot corresp ond to polynomial growth O ( n d ), where d is the slope of the line. Figure 1 shows that the CPU-time of CPM3C scales roughly O ( n ), which is consisten t with the statemen t in section 3. 4.5.  X  vs. Accuracy &amp; Speed Theorem 6 states that the total number of iterations involved in CPM3C is at most R higher  X  , the algorithm migh t converge fast. However, as  X  is directly related to the training loss in CPM3C , we need to determine how small  X  should be to guar-antee su X cien t accuracy . We presen t in  X gure 2 and  X gure 3 how clustering accuracy and computational time scale with  X  . According to  X gure 2,  X  = 0 : 01 is small enough to guaran tee clustering accuracy . The log-log plot in  X gure 3 veri X es that the CPU-time of CPM3C decreases as  X  increases. Moreo ver, the em-pirical scaling of roughly O ( 1 O ( 1  X  2 ) in the bound from theorem 6. We prop ose the cutting plane multiclass maximum margin clustering ( CPM3C ) algorithm in this paper, to cluster data samples with the maxim um margin hy-perplane. Preliminary theoretical analysis of the algo-rithm is provided, where we show that the computa-tional time of CPM3C scales linearly with the sample size n with guaran teed accuracy . Moreo ver, experi-mental evaluations on several real world datasets show that CPM3C performs better than existing MMC metho ds, both in e X ciency and accuracy .
 This work is supp orted by the projects (60721003) and (60675009) of the National Natural Science Founda-tion of China.

