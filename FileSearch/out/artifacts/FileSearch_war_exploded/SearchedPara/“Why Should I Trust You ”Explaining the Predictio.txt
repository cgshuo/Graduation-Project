 Machine learning is at the core of many recent ad-vances in natural language processing. Unfortunately, the important role of humans is an oft-overlooked as-pect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models into products that need to be shipped, a vital concern remains: if the users do not trust a model or a prediction, they will not use it . It is important to differentiate between two different (but related) defi-nitions of trust: (1) trusting a prediction , i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model , i.e. whether the user trusts a model to behave in rea-sonable ways if deployed  X  X n the wild X . Both are directly impacted by how much the human under-stands a model X  X  behavior, as opposed to seeing it as a black box. Recent resurgence of neural networks has resulted in state-of-art models whose working is quite opaque to the user, exacerbating this problem.
A common surrogate for ascertaining trust in a model is to evaluate accuracy on held-out annotated data. However, there are several ways this evaluation can go wrong. Data leakage, for example, defined as the unintentional leakage of signal into the training (and validation) data that would not occur in the wild (Kaufman et al., 2011), potentially increases accu-racy. Practitioners are also known to overestimate the accuracy of their models based on cross validation (Patel et al., 2008), as real-world data is often signifi-cantly different. Another particularly hard to detect problem is dataset shift (Candela et al., 2009), where training data is different than test data. Further, there is frequently a mismatch between that metrics that we can compute and optimize (e.g. accuracy) and the actual metrics of interest such as user engagement and retention. A practitioner may wish to choose a less accurate model for content recommendation that does not place high importance in features related to  X  X lickbait X  articles (which may hurt user reten-tion), even if exploiting such features increases the accuracy of the model in cross validation.

In this paper, we describe a system that explains why a classifier made a prediction by identifying useful portions of the input. It has been observed that providing an explanation can increase the ac-ceptance of computer-generated movie recommen-dations (Herlocker et al., 2000) and other automated systems (Dzindolet et al., 2003), and we explore their utility for NLP. Specifically, we present:  X  LIME, an algorithm that can explain the predic-tions of any classifier, by approximating it locally with an interpretable model.  X  SP-LIME, a method that selects a set of representa-tive explanations to address the  X  X rusting the model X  problem, via submodular optimization.  X  A demonstration designed to present the benefits of these explanation methods, on multiple NLP classification applications, classifier algorithms, and trust-related tasks. By  X  X xplaining a prediction X , we mean presenting vi-sual artifacts that provide qualitative understanding of the relationship between the instance X  X  components (e.g. words in text) and the model X  X  prediction. Ex-plaining predictions is an important aspect in getting humans to trust and use machine learning effectively, provided the explanations are faithful and intelligible. We summarize the techniques here; further details and experiments are available in Ribeiro et al. (2016). Local Interpretable Model-Agnostic Explanations We present Local Interpretable Model-agnostic Ex-planations ( LIME ). The overall goal of LIME is to identify an interpretable model over the inter-pretable representation that is locally faithful to pre-dictions of any classifier . It is important to distin-guish between features and interpretable data repre-sentations, the latter is a representation that is under-standable to humans, regardless of the actual features used by the model. A possible interpretable represen-tation for text is a binary vector indicating the pres-ence or absence of a word, even though the classifier may use more complex (and incomprehensible) fea-tures such as word embeddings. We denote x  X  R d as the original instance, and x 0  X  X  0 , 1 } d 0 to denote a binary vector for its interpretable representation.
Formally, we define an explanation as a model g  X  G , where G is the class of linear models, such that g ( z 0 ) = w g  X  z 0 Note that g acts over ab-sence/presence of the interpretable components , i.e. we can readily present it to the user with visual ar-tifacts. We let  X ( g ) be a measure of complexity (as opposed to interpretability ) of the explanation g  X  G . For text classification, we set a limit K on the number of words included, i.e.  X ( g ) =  X  1 [ k w g k
Let the model being explained be denoted f , i.e. f ( x ) is the probability (or a binary indicator) that x belongs to a certain class. We further use  X  x ( z ) as a proximity measure between an instance z to x , so as to define locality around x . Finally, let L ( f,g,  X  x ) be a measure of how unfaithful g is in approximat-ing f in the locality defined by  X  x . We use the lo-cally weighted square loss as L , as defined in Eq. (1) , where we let  X  x ( z ) = exp (  X  D ( x,z ) 2 / X  2 ) be an exponential kernel on cosine distance D .
 L ( f,g,  X  x ) = In order to ensure both interpretability and local fidelity , we minimize L ( f,g,  X  x ) while having  X ( g ) be low enough to be interpretable by humans. We approximate L ( f,g,  X  x ) by drawing samples, weighted by  X  x . Given a sample z 0  X  { 0 , 1 } d 0 (which contains a fraction of the nonzero elements of x 0 ), we recover the sample in the original repre-sentation z  X  R d and obtain f ( z ) , which is used as a label for the explanation model. Given this dataset Z of perturbed samples with the associated labels, we optimize Eq. (2) to get an explanation  X  ( x ) by first selecting K features with Lasso (Efron et al., 2004), forward selection or some other method, and then learning the weights via least squares.
 Submodular Pick for Explaining Models Although an explanation of a single prediction pro-vides some understanding into the reliability of the classifier to the user, it is not sufficient to evaluate and assess trust in the model as a whole. We propose to give a global understanding of the model by ex-plaining a set of individual instances. Even though explanations of multiple instances can be insightful, these instances need to be selected judiciously, since users may not have the time to examine a large num-ber of explanations. We represent the number of explanations humans are willing to look at a bud-get B , i.e. given a set of instances X , we select B explanations for the user to inspect. We construct an n  X  d 0 explanation matrix W that represents the local importance of the interpretable components for each instance, i.e. for an instance x i and explana-tion g i =  X  ( x i ) , we set W i = | w g each component j in W , we let I j denote the global importance, I j =
While we want to pick instances that cover the im-portant components, the set of explanations must not be redundant in the components they show the users, i.e. avoid selecting instances with similar explana-tions. We formalize this non-redundant coverage intuition in Eq. (3) , where coverage C , given W and I , computes the total importance of the features that appear in at least one instance in a set V . The pick problem thus consists of finding the set V, | V | X  B that achieves highest coverage.
 The problem in Eq. (4) is maximizing a weighted cov-erage function, and is NP-hard (Feige, 1998). Due to submodularity, a greedy algorithm that iteratively adds the instance with the highest coverage gain of-fers a constant-factor approximation guarantee of 1  X  1 /e to the optimum (Krause and Golovin, 2014). Using this explanation system that is capable of providing visual explanations for predictions of any classifier, we present an outline for a demon-stration using different NLP tasks, models, and user interactions. The complete source code and documentation for installing and running the demonstration is available at https://github. com/uw-mode/naacl16-demo , which uses the code for explaining classifiers available as an open-source python implementation at https:// github.com/marcotcr/lime .
 Applications We explore three NLP classification tasks, which dif-fer in the types of text they apply to and predicted cat-egories: politeness detection for sentences (Danescu-Niculescu-Mizil et al., 2013), multi-class content classification for documents (20 newsgroups data), and sentiment analysis of sentences from movie re-views (Socher et al., 2013). We explore classifiers for these tasks that vary considerably in their underlying representation, such as LSTMs (Wieting et al., 2015), SVMs, and random forests, trained on bag of words or on word embeddings.

We will outline the specific user interactions using a running example of the 20 newsgroups dataset, and the interfaces for the other applications will look simi-lar. In particular, we focus on differentiating between  X  X hristianity X  from  X  X theism X , and use an SVM with an RBF kernel here. Although this classifier achieves 94% held-out accuracy, and one would be tempted to trust it based on this, the explanation for an instance shows that predictions are made for quite arbitrary reasons (words  X  X osting X ,  X  X ost X  and  X  X e X  have no connection to either Christianity or Atheism). 3.1 Explaining Individual Predictions The first part of the demo focuses on explaining in-dividual predictions. Given an instance that the user either selects from the dataset or writes their own piece of text, we provide the set of words that are im-portant for the prediction according to the classifier of their choosing. The interface for the user is shown in Figure 1, which embedded in an iPython notebook. For most datasets and classifiers, our current system can produce an explanation in under three seconds, and some simple further optimizations can be used to produce near-instant explanations. 3.2 Explaining and Comparing Models For the second part of the demonstration, we provide explanations of different models in order to compare them, based on explanations of a few selected in-stances. The interface presents the explanations one at a time, similar to that in Figure 1. By default the in-stances are selected using our submodular approach (SP-lime), however we also allow users to write their own text as well, and produce explanations for the classifiers for comparison. 3.3 Improving Classifiers As the final demonstration, we consider a simple ver-sion of feature engineering. Specifically, we initiate importance of the word for  X  X hristianity X , and magenta  X  X theism X . each user with a classifier trained using all features, including both noisy and correct ones. We then show explanations of the classifier to the users, and ask them to select which words to remove from the clas-sifier (see Figure 2 for the interface). Given this feedback, we retrain the classifier and provide the users with a score of how well their classifier per-formed on a hidden set, along with a leader board of the accuracy of all the participants. We argue that trust is crucial for effective human interaction with machine learning based NLP sys-tems, and that explaining individual predictions is important in assessing trust. We present a demonstra-tion for LIME, a modular and extensible approach to faithfully explain the predictions of any model in an interpretable manner, and SP-LIME, a method to select representative and non-redundant explanations, providing a global view of the model to users. The user interactions on multiple applications and clas-sifiers span a variety of trust-related tasks: getting insights into predictions, deciding between models, and improving untrustworthy models.
 We would like to thank Kevin Gimpel, John Wieting, and Cristian Danescu-Niculescu-Mizil for making their classifiers and datasets available for use. This work was supported in part by the TerraSwarm Re-search Center, one of six centers supported by the STARnet phase of the Focus Center Research Pro-gram (FCRP) a Semiconductor Research Corporation program sponsored by MARCO and DARPA.
