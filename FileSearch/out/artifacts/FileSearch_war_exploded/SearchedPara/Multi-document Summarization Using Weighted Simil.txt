 Radev et al. [12] mentioned three points to be considered for the multi-document summarization: (i) recognizing and removing redundancy, (ii) identifying important difference between documents, (iii) ensuring the coherence of summaries. The redun-dancy represents how many terms or concepts are repeated in a document while the diversity or the difference represents how many terms or concepts are different from each other [2]. The coherence represents how many summaries are readable and rele-vant to the user [8]. 
The recent studies for document summarization are as follows: Gong and Liu pro-posed the method using the Latent Semantic Analysis (LSA) technique to semanti-cally identify important sentences for summary creation [3]. However, it may extract less meaningful sentences since the term weights of the semantic feature vector in latent semantic space may be negative [16]. Sassion proposed a multi-document summarization method based on topics. His method summarizes documents by re-moving irrelevant sentences one by one from set of candidate sentences until a user X  X  specified compression ratio [14]. This method has a weakness that it cannot select most meaningful sentences because it uses simply the cosine similarity between can-didate sentences which are extracted by using n-gram. Park et al. proposed a generic document summarization method based on NMF [9]. Park et al. proposed a query based summarization method by using NMF [10]. This method extracts sentences using the cosine similarity between a query and semantic features, whereas it does not document summarization method based on clustering using NMF [11]. This method clusters the sentences and extracts sentences using the cosine similarity measure be-tween a topic and semantic features. This method improves the quality of summaries and avoids the topic to be deflected in th e sentence structure by clustering sentences and removing noise, whereas it may extract more or less similar but meaningless sentences from documents and does not consider the coherence with respect to ex-tracted sentences. 
In this paper, we propose a new topic based multi-document summarization method using weighted similarity between topic and non-negative semantic features which are obtained by means of clustering and non-negative matrix factorization (NMF). The NMF can represent individual object as the non-negative linear combina-human X  X  cognition that human use only the addition of non-negative data when they recognize an object as the combination of part information. This method can deal with a large volume of information efficiently since original non-negative matrix is decomposed into sparsely distributed representation of two non-negative matrices [6, 7, 15, 16]. 
The proposed method has the following advantages. First, the semantic feature vectors and the semantic variable vectors are sparse with non-negative values. Sen-tences can be decomposed into intuitively comprehensible semantic features having a few terms. The inherent structure of documents can be analyzed into a linear combi-nation of semantic features. Therefore, it can select more meaningful sentences clos-est to the topic. Second, it can avoid the bias ed inherent structure of documents to be reflected in summaries since it deals with redundant information and removes noise by clustering sentences. Therefore, it can improve the accuracy of the document summarization. Third, it can enhance the co herence by sorting extracted sentences in the order of their rank. 
The rest of the paper is organized as follows: Section 2 describes the non-negative matrix factorization. In Section 3, a new multi-document summarization method is introduced. Section 4 shows the evaluation and experimental results. Finally, we con-clude in Section 5. In this paper, we define the matrix notation as follows: Let X * j be j  X  X h column vector 
Non-negative matrix factorization is to decompose a given m  X  n non-negative ma-trix A into multiplication of a m  X  r non-negative semantic feature matrix (NSFM), W , (1). where r is usually chosen to be smaller than m or n so that the total sizes of W and H are smaller than that of the matrix A . 
We use the Frobenius norm as an objective function to satisfy the approximation condition WH A =
We keep updating W and H until ) , ( H W E  X  converges under the predefined threshold or exceeds the number of repetition. The update rules are as follows: In this section, we propose a new topic-based multi-document summarization method. The proposed method consists of the preprocessing phase, the clustering phase, and the sentence extraction phase. We explain three phases in full, respectively. 
In the preprocessing phase, after given documents are decomposed into a set of sentences, we remove stop-words, and perform words stemming and weight calcula-tion. Then we construct the term-frequency matrix for documents [13].

We use K -means to cluster sentences. K -means clustering is a partition algorithm that the cosine similarity measure with respect to the matrix A as shown in Equation (4). Besides, since these are non-negative values, it follows that 0  X  sim ( )  X  1. Hence, 0  X  d ( )  X  1. The weighted matrix of i X  X h cluster of sentences, C i , is a subset of column vectors of matrix A . C i and C j are disjoint and satisfy the following property: 
We construct matrices W i and H i by applying NMF algorithm with respect to sen-tence matrices of k  X  clusters after removing noise, respectively, as shown in Equation (7). 
The j  X  X h column vector i j C * of matrix C i is a weighted vector for j  X  X h sentence, and feature vector i l W * in sentence i j C * is i lj H . 
The semantic analysis of non-negative matrix W and H is as follows: semantic fea-ture matrix W is composed of groups of semantically related terms. This characteristic can discriminate the same term to be used in a different meaning in the context. Be-sides, semantic variable matrix H represents weights of semantic features denoting tional to the value of the semantic variable for the semantic feature. We can quantita-variables. 
Equation (9) is a formula that denotes the weighted similarity measure between mantic feature in sentences. Hence, we multiply the ratio of summation of weights of the semantic feature to reflect the importance of the semantic feature in sentences. We high but unimportant. The sentence, in which the weight of the semantic feature se-lected as the most important one is the largest, is extracted as one of the summary. 
To estimate the number of sentences extracted from each cluster, we define the topic-cluster similarity measure between the topic and the cluster. sentences are extracted from the cluster. The proposed multi-document summarization algorithm is as follows: In step 5.a, we select the semantic feature vector i p W * most similar to topic T by using chiefly p  X  X h semantic feature vector. proposed method. The ROUGE has been applied by Document Understanding Confer-ence (DUC) for performance evaluation [1]. The DUC is the international conference for performance evaluation of the proposed system by comparing manual summaries document summarization. The testing data of DUC 2005 consist of 50 topics and 25 to 50 documents related with each topic [5] 
We conducted the performance evaluation of the topic-based multi-document summarization using given 50 topics of DUC 2005. ROUGE includes five automatic evaluation methods such as ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S, and ROUGE-SU [1]. 
We evaluated four different summarization methods such as the LSA , the Kmeans , the Clustering + NMF , and the proposed method. In Table 2, the LSA denotes Gong X  X  method [3]. The Kmeans denotes the multi-document summarization method using Clustering + NMF denotes the multi-document summarization method using NMF and K -means clustering [11]. Table 2 shows the f -measure values of four methods using ROUGE evaluation. The proposed method shows the best performance whereas the LSA shows the lowest performance. The Clustering + NMF shows better performance than the Kmeans . Be-cause the Clustering+NMF generates more meaningful summary by removing noise and redundancy and reflecting the inherent semantics of documents whereas the Kmeans may produce poor summary since it remove only simple noise. 
The proposed method shows better performance than other methods. It generates more meaningful summary by reflecting the inherent semantics of documents without noise and redundancy, and it prevents from extracting the less meaningful sentences since the semantic feature whose cosine similarity with respect to a topic is high but meaningless is not selected by using the weighted similarity measure. This paper presents a new topic based multi-document summarization method. In the method, it is important to extract sentences which are common to all documents and relevant to a given topic, and to remove noise and redundant information for improv-ing the quality of summaries. The advantages of the proposed method are as follows. First, it can represent documents by means of intuitively comprehensible form since it uses very sparse semantic features and semantic variables with non-negative values. prevent from extracting more or less similar but meaningless sentences. Second, it removes the redundancy of sentences within a cluster and identifies the important difference of sentences between clusters. So, it can avoid the biased inherent structure of documents to be reflected in summaries. 
In the future, we will study the term re-weighting method of semantic features and semantic variables to summarize documents. We anticipate that re-weighting terms racy of document summarization. Acknowledgements. This research was supported by the MIC(Ministry of Information and Communication), Korea, under the ITRC(Information Technology Research Center) support program supervised by the IITA(Institute of Information Technology Assessment). 
