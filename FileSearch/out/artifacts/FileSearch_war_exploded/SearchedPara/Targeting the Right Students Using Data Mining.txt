 The education domain offers a fertile ground for many interesting and challenging data mining applications. These applications can help both educators and students, and improve the quality of education. In this paper, we present a real-life application for the Gifted Education Programme (GEP) of the Ministry of Education (MOE) in Singapore. The application involves many data mining tasks. This paper focuses only on one task, namely, selecting students for remedial classes. Traditionally, a cut-off mark for each subject is used to select the weak students. That is, those students whose scores in a subject fall below the cut-off mark for the subject are advised to take further classes in the subject. In this paper, we show that this traditional method requires too many students to take part in the remedial classes. This not only increases the teaching load of the teachers, but also gives unnecessary burdens to students, which is particularly undesirable in our case because the GEP students are generally taking more subjects than non-GEP students, and the GEP students are encouraged to have more time to explore advanced topics. With the help of data mining, we are able to select the targeted students much more precisely. Target selection, scoring, data mining application in education. The education domain offers many interesting and challenging applications for data mining. First, an educational institution often has many diverse and varied sources of information. There are the traditional databases (e.g. students X  information, teachers X  information, class and schedule information, alumni information), online information (online web pages and course content pages) and more recently, multimedia databases. Sec ond, there are many diverse interest groups in the educational domain that give rise to many interesting mining requirements. For example, the administrators may wish to find out information such as admission requirements and to predict the class enrollment size for timetabling. The students may wish to know how best to select courses based on prediction of how well they will perform in the courses selected. The alumni office may need to know how best to perform target mailing so as to achieve the best effort in reaching out to those alumni that are likely to respond. All these applications not only contribute towards the education institute delivering a better quality education experience, but also aid the institution in running its administrative tasks. With so much information and so many diverse needs, it is foreseeable that an integrated data mining system that is able to cater for the special needs of an education institution will be in great demand In this paper, we present a real-life application for the Gifted Education Programme (GEP) of the Ministry of Education (MOE), Singapore. Singapore is a small city-state, where human resources are the only source of natural resources. Education is viewed as a critical factor in contributing to the long-term economic well-being of the country. The government of Singapore believes in the importance of maximizing the potential of each and every individual student. As such, the MOE treats the daunting task with great importance as the responsibilities of educating the country X  X  future leaders fall heavily on their shoulders. GEP is a programme initiated by MOE, and is aimed at discovering talented individuals as early as possible so as to further nurture them with a set of specially designed courses. In Singapore, the mainstream students generally start their education with 6 years of primary school education. At the end of the 6 years, students sit for their first nationwide streaming exams, Primary School Leaving Exams (PSLE) before being promoted to the next level. Following the primary school comes 4 years of secondary school education before the students sit for the next nationwide streaming exams, Ordinary Level Exams (O-level). From here each student has a choice on how to purse his/her further education. The student could go straight into tertiary education by joining a polytechnic or could further study for another 2 years in a junior college (JC) and sit the Advanced Level Exams (A-level), which will eventually land him/her a place in a University. The GEP students are selected when they are in their primary schools. The selection is done with a series of tests. All students interested in joining the GEP program are invited to take the tests. Exceptional students identified through the tests are offered places in the GEP programme. In this project, we were given the GEP students' demographic data and school performance data over the past few years. We were requested to do a data mining application, which has a number of separate tasks. The main objective of this application is to validate both the selection process and the effectiveness of the program. Some of the tasks that the MOE is interested in include profiling of the GEP students based on different grouping criteria and multi-angled comparison of students who were admitted into the GEP and those who did not make it and also a performance analysis of students in the GEP with special interest in how further improvements can be achieved. In this paper, we focus only on one specific task in the application, selecting students for remedial classes. The GEP is are interested in improving the A-level exam results of their students by selecting the weak students to attend remedial classes. There are two main considerations. On one hand, it is undesirable to select too many students to take part in the remedial classes as this increases the load on both the students and the teachers. Asking these students to take more classes is particularly undesirable b ecause the GEP students are already taking more lessons than those students not in the GEP program, and the GEP students are encouraged to have more time to explore advanced topics in science and technology. On the other hand, if we miss the right students for remedial classes, they may end up doing poorly in their A-level exams (the expectations from the MOE on these students are very high). Our task is to evaluate the traditional selection method and to suggest improvements based on findings from data mining. In this paper, we will describe a data mining based method for selecting the right students for remedial classes. The key component of this method is a new scoring function (called SBA [21]) that is based on association rules [1]. This method has yielded some exceptionally promising results. It outperforms the traditional method significantly. The paper is organized as follows: In the next section, we introduce the traditional selection method to select the weak students for remedial classes. In Section 3, we present our data mining technique used for the task, which is a new scoring method based on association rules. In Section 4, we apply this technique to our task. We will see that simply applying this technique is far from sufficient. Our data mining results must be combined with some problem specific methods and knowledge in order to produce the required results. Section 5 evaluates our new method and compares with various other techniques. Section 6 concludes the paper. Gifted (also called GEP) students in Singapore are identified in programme focuses on both the primary and secondary school levels. In the Ministry of Education, a Gifted Education branch keeps track of all the GEP students over the years. All the exam records related to GEP students are kept in a database. From past experiences and previous studies, the Gifted Education branch knows that GEP students generally perform well in their O-level exams. Only 14% of the students have not done as well as expected. However, the percentage of students not meeting the expectations in their A-level exams rises to 31% (the student  X  s performance at A-level is measured by an aggregated score computed from individual subject scores using a complex formula). Over the years, these percentages are reasonably consistent. The immediate tension is to reduce the discrepancies between these two statistics. MOE hopes to provide JCs with information on how to help the identified weak students so that remedial classes can be provided to these students. Out of all subjects, 8 O-level subjects have been identified that are related to the A-level exams that they would like to provide remedial classes. Out of these 8 subjects, 3 of them are language subjects, and the rest are science and mathematics subjects. The O-level examination result of each subject has been studied individually, and a subject cutoff mark for attending the remedial class of the subject is imposed. If a GEP student does not do better than a subject cutoff mark in his/her O-level exam, he/she will be recommended to attend the remedial class of the subject. In our application, we show that this cutoff method selects too many students for remedial classes. In the rest of the paper, we also refer this method as traditional method , as it is commonly used in most educational institutions. After careful study and consultation with our domain experts, we designed the following two steps to solve the problem: 1. Identify the potential weak students. 2. Select the courses that each weak student is recommended The first step is clearly a data-mining task, while the second step experts  X  advice is needed because they do not want our met hod to depart too far from the normal practice. Otherwise, both the students and teachers will complain that they do not understand what is going on in the computer. Step 1 can be seen as a classification problem. However, it is not suitable to use a classifier to predict who will definitely perform poorly in his/her A-level exams b ecause the accuracy is too low. For example, using the O-level results, the classification system C4.5 [29] can only identify half of the weak students, and the CBA system [19] can only identify two-third of the weak students. From our experiences and experiments, we found that a scoring method is more appropriate. Instead of assigning each student a definite class ('weak' or 'non-weak' student), a scoring model assigns a probability estimate to each student to express the likelihood that he/she will do poorly in the A-level exams (note that a student  X  s performance at A-level is measured by an aggregated score). Then, the subsequent processing has the flexibility to choose a certain subset of the students for remedial classes. This will be clear later. Below, we present our new scoring technique. We call it the SBA technique (Scoring Based on Associations). Although there already exist scoring methods based on decision trees and also the Bayesian model, it is shown in [21] (our IBM Research Report) that in general our new method SBA outperforms these traditional techniques substantially. Later in the evaluation section, we will also see that SBA performs better than C4.5 and Na  X  ve Bayesian based scoring methods in our application. We present the SBA method in this section. Since the SBA method is based on association rules, we begin the presentation by introducing the concept of association rule mining. We then discuss the problems and solutions in using the association rules for scoring. Finally, we give the scoring function adopted by SBA. Association rules are an important class of regularities that exist in databases. Since it was first introduced in [1], the problem of mining association rules has received a great deal of attention [e.g., 1, 2, 5, 11, 13, 25, 30, 33, 34]. The classic application of association rules is the market basket analysis [1, 2]. It analyzes how the items purchased by customers are associated. An example association rule is as follows, This rule says that 10% of customers buy cheese and beer together, and those who buy cheese also buy beer 80% of time. The association rule mining model can be stated as follows: Let I (the database), where each transaction d is a set of items such that d  X  I . An association rule is an implication of the form, X  X  Y , where X  X  I , Y  X  I , and X  X  Y =  X  . The rule X  X  Y holds in the transaction set D with confidence c if c % of transactions in D that support X also support Y . The rule has support s in D if s % of the transactions in D contains X  X  Y . Given a set of transactions D (the database), the problem of mining association rules is to discover all association rules that have support and confidence greater than or equal to the user-specified minimum support (called minsup) and minimum confidence (called minconf). To use association rules for scoring purposes, we need to solve a number of problems, which are discussed below. Solutions to these problems are also proposed. See also [21]. Mining association rules from a relational table : In our applications, we need to mine association rules from a relational table (rather than a set of transactions) as our task uses this form of data. For association rule mining to work on this type of data, we need to discretize each numeric attribute into intervals since association rule mining only takes categorical values or items. After discretization, we can treat each data case (record) in the dataset as a set of ( attribute , value ) pairs and a class label. An ( attribute , value ) pair is an item . With this transformation, each data case becomes a transaction. An existing association rule-mining algorithm can be applied to the dataset. Discretization of continuous attributes will not be discussed in this paper (see [10]). In traditional association rule mining, any item can appear on the left-hand-side or the right-hand-side of a rule. For scoring, we have a fixed class attribute with two classes. Thus, we are only interested in rules that use a single class on their right-hand-sides. That is, we only mine association rules of the form: from the rest of the attributes. The class that we are interested in is often called the positive class (e.g., the  X  weak  X  student class). The other class is called the negative class (e.g.,  X  non-weak  X  student class). We say a rule is large if it meets the minimum support. Using an existing association rule-mining algorithm (e.g., [2]) to mine this type of rules is straightforward. We simply find all large itemsets (or rules) of the form: In the mining process, each iteration adds a new item to every itemset. That is, in the first iteration we find all itemsets of the form &lt; item 1 , C i &gt;. In the second iteration, we find all itemsets of the form &lt; item 1 , item 2 , C i &gt;, and so on. Problems with minsup and minconf: Traditional association rule mining uses a single minsup and a single minconf in the mining process. This is not appropriate for our task because the class distribution of our data can be quite imbalanced. Let us discuss the problems with minsup first. Using a single minsup causes the following problems:  X  If the minsup is set too high, we may not find those rules that  X  In order to find rules that involve the minority class, we have While a single minsup is inadequate for our application, a single minconf also causes problems. For example, in a database, it is known that only 5% of the students are weak students and 95% are non-weak students. If we set the minconf at 96%, we may not be able to find any rule of the  X  weak  X  class because it is unlikely that the database contains reliable rules of the  X  weak  X  class with such a high confidence. If we set a lower confidence, say 50%, we will find many rules that have the confidence between 50-95% for the  X  non-weak  X  class and such rules are meaningless (see also [3]). We solve these problems by using different minsups and minconfs for rules of different classes. For minimum supports, we only require the user to specify one total or overall minimum support (called t_minsup ), which is then distributed to each class according to the class distribution in the data as follows: where f ( C i ) is the number of C i class cases in the training data. | D | is the total number of cases in the training data. The reason for using this formula is to give rules with the frequent (negative) class a higher minsup and rules with the infrequent (positive) class a lower minsup. This ensures that we will generate enough rules with the positive class and will not produce too many meaningless rules for the negative class. For minimum confidence, we use the following formula to automatically assign minimum confidence to each class: The reason for using this formula is that we should not produce rules of class C i whose confidence is less than f ( C such rules make no sense. Although we have different minsups and minconfs for rules of different classes, no change needs to be made to the original association rule mining algorithm [2] as the downward closure property [2] still holds for mining &lt; item 1 , ... , item reason is that item sets of different classes do not interact. Pruning of association rules (optional): It is well known that many association rules are redundant and minor variations of others. Those insignificant rules should be pruned. Pruning can remove a huge number of rules with no loss of accuracy (see [21] for more details). It also improves the efficiency of scoring. Our pruning function uses the pessimistic error rate based method in C4.5 [28]. Note that the error rate of a rule is 1  X   X  the confidence of the rule  X  . The technique prunes a rule as follows: If rule r  X  s estimated error rate is higher than the estimated error rate of rule r  X  (obtained by deleting one condition from the conditions of r ), then rule r is pruned. Note that when r is a 1-condition rule of the form, x  X  y , then r  X  is,  X  y , which has no condition. See [28] for the detailed computation of the pruning method. Pruning can be done very efficiently b ecause if r is a rule then r  X  must also be a rule. An important point to note is that when attempting to prune a rule have been pruned previously. Then, the procedure needs to go back to the rule that prunes r  X  , and uses that rule to prune r . Using association rules for scoring: The key feature of association rule mining is its completeness, i.e., it aims to find all rules in data. This presents a great opportunity to design good scoring functions by making use of the rules, i.e., we have abundant of information. However, it also represents a challenge because when we want to score a data case, there are often many rules that can be applied. Different rules may give different information, and many of them even give conflicting information. For example, one rule may say that the data case should belong to the positive class with a probability of 0.9, while another rule may say that it should belong to the negative class also with a probability of 0.9. The question is which rule we should trust. This is not a problem for traditional classification systems because they typically have only one answer. In the case of association rules, we have many answers. In the next sub-section, we focus on this issue and present a score function that makes use of all rules. After the rules are generated (from training data), we can use them to score the new (or test) data. Since each rule is attached with a support and a confidence, it is thus easy to design a method to score the data. To design the best scoring method based on association rules, however, is very difficult, because there are an infinite number of possible methods. Below, we describe a heuristic technique that is both effective and efficient (see [21] for detailed evaluation results). In SBA, we aims to achieve the following effects: 1. When there are many confident positive class rules that can 2. When the positive class rules that cover the data case are Basically, we try to push data cases toward both ends. If this is done reliably, we will achieve good ranking results. We now present the proposed scoring function. In the context of association rules, we have the following types of useful information:  X  Confidence: Rule confidence is an essential piece of  X  Support: Rule support, to certain extent, reflects the  X  Two types of rules: There are two types of rules that we We postulate the following general scoring function, which is a weighted average taking into account of the above information (the support information will appear in the weights). Given a data and 1 inclusively. where: Now the problem is what should be the weights. Since we want to achieve the two desirable effects discussed above, the weights should reflect their needs. We have two pi eces of information about each rule to use: s upport and confidence. We performed a large number of experiments, and found that for both the negative weight and the positive weight, the combination of both confidence and support performs the best. That is: where conf i and sup i are the original confidence and support of the positive class rule i , and where conf j and sup i are the original confidence and support of of negative class rules (which often have high supports and high confidences). We performed many experiments to determine k and found that when k = 3, the system performs the best (see [21]). It is important to note that to compute the weight for a negative class rule, we do not convert the rule to a positive rule and then use the support and confidence in the positive class. Instead, we still use their original support and confidence in the negative class. This helps us to achieve the two effects discussed above. See [21] for the justification and explanation of the formulas. [21] also gives the comparison results of SBA and other scoring methods. Finally, in ranking, when more than one data case has the same score, we compute a priority value ( P ) using the following formula: where | POS | and | NEG | are the numbers of rules in POS and NEG respectively. This formula uses supports of the rules to calculate the priority. Basically, we give those data cases with higher positive supports and lower negative supports higher priorities. Note that when a data case does not satisfy any rule (i.e., POS = NEG =  X  ), we assign S = 0 and P = 0. Our scoring method assigns each student a likeli hood value to express the chance that he/she will be a weak student (i.e. not meeting a certain pre-defined standard) in the A-level exams. Hence, if a student receives a high score from the scoring system, it means that this student is likely to do badly and could thus be a target for remedial classes. After scoring and ranking, we can identify a list of weak students. We then select the courses that each potentially weak student has to attend. Our new technique for selecting the right students to attend remedial classes consists of the following steps: 1. Train the scoring model (e.g., SBA) using the training data 2. Score each student using the model constructed 3. Allocate groupings to each student based on the scores 4. Recommend remedial classes to the students in each gr oup The first two tasks above are relatively straightforward. We can simply execute the SBA system on the training data (step 1), which will then assign scores to the unseen test data (step 2). In the following 2 subsections (section 4.1 and 4.2), we will describe the techniques employed in the last two steps. Our technique for selecting courses for students is based on allocating students to different groups using their scores. This helps us discriminate the students based on how likely their group is going to do badly. The question is "how many groups should we allocate and how to decide the boundary of each gr oup?" Based on our experiments using the training data and the suggestions from the domain expert, we decided to use 3 distinct groups, needy, normal, and fine. Our domain experts do not like to have too many groups b ecause it makes things very complex and difficult to understand. In this domain (or any education domain), the users are particularly concerned with the complexity and the fairness, as they have to explain every action to the students and the teachers. Next, we map the group boundaries. This is done by inspecting the results of our score models using 5-fold cross validation on the training data. We noticed that SBA can identify weak students very well in the first 20% of top ranking students, and in the next 30% there are a small number of weak students, and in the last 50%, almost all the students are good students. Thus, our three groups have 20%, 30% and 50% of the students respectively. With the students allocated to the groups, we can now easily implement discriminating policies to these groups. The aim of such policies is to heighten the chance of a weak student attending remedial classes while lessening the chance of capable students attending remedial classes. Students allocated to the needy group are those that carry the highest chance of doing badly in their A-level exams. These students should have the highest chance of attending remedial classes. Students allocated to the normal group should have less chance while students allocated to the fine group should have the least chance. We then use the cutoff mark for each subject used in the traditional met hod as the base to decide the cutoff mark for each course subject in each gr oup in our method. For the needy group, we use a lower cutoff mark for each course subject, i.e., we make it more likely for those students in this group to attend remedial classes. For the normal group, we use the same cutoff mark as the traditional method. For the fine group, we use a higher cutoff mark for each subject, i.e., we make it harder for these students to take remedial classes. The exact cutoff mark for each course subject of each gr oup is obtained from experiments and verified by the domain experts. This verification was important because our domain experts do not want our method to depart too far from their proposed method so that they can easily explain to the students and the management. This section compares the results of our data mining based method with the traditional method. In order to perform this comparison, we need to define some criteria, which are meaningful to our domain experts. Our criteria are similar to the precision and recall measures in information retrieval. Below, we define two precision measures, and one recall measure. Basically, they measure whether the teaching effort will be targeted at the right students. One main goal in this project is to target the right students with the right remedial classes. To measure how well our method performs, we define a concept called unit effort, e . Definition: a unit effort (e) is a single class attended by a particular student. Alternatively, it is a single student taught by a teacher in a particular class. Basically, the unit effort can be seen as a unit workload of a student or a teacher. For example, if a student attends a class, we say 1 unit effort is put on this student. Our users are genuinely interested in putting more effort towards teaching potentially weak students than to the strong students. A good model should thus allow MOE to select the right students for the right remedial classes. We use the following performance measure, precision_by_effort , to measure this. Definition: precision_by_effort (P e ) is the ratio of the total unit efforts ( E w ) spent on weak students to the total unit efforts ( E spent in teaching all the students in the remedial classes, i.e., where E w and E t are computed as follows ( s is the number of students attending remedial classes, k i is the number of remedial classes taken by student i , j is a remedial class taken by a particular student, and n is the number of actual weak students). We can see that a higher P e means that more teaching efforts are spent on the weak students. However, P e alone is not sufficient. Our users would also like to teach as many potentially weak students as possible without burdening those good students. In other words, a good model should allow MOE to identify more weak students amongst all the students identified for remedial classes. We define this performance measure as precision_by_student.
 Definition: precision_by_student ( P s ) is the ratio of the total number of weak students (T w ) attending remedial classes to the total number of students (T t ) attending remedial classes, i.e., Clearly, a higher P s means that we are teaching more weak students in our remedial classes. The above two measures are precision measures. We also need a recall measure since our users are also interested in sending as many potentially weak students as possible for remedial classes. A good technique should simply allow MOE to identify as many weak students as possible. We define this performance measure as recall (by students). Definition: recall (R) is the ratio of the total number of weak students ( T w ) attending remedial classes to the total number of weak students ( T wa ) in the data, i.e., Note that it is not meaningful to define a recall measure by effort because it is not clear what is the maximum effort required by the weak students. Our users agreed that the three measures above are sufficient. We now present the comparison results. Note that we also used another two scoring techniques in our experiments to show that SBA gives superior results. One scoring method is based on the decision tree system C4.5 [29] (which we call C4.5-score), and the other is based on the Na  X  ve Bayesian technique (which we call NB-score). MOE gave us 4 years of data for building the model. These 4 years of data are also used in determining the cutoff marks in the traditional method. They hold the latest year  X  s student results in O-level and A-level as the test data. We perform many experiments on the 4 years data to determine the number of groups, group boundaries and also the exact cutoff marks of each course subject for each gr oup (see also Section 4.1 and 4.2). Note that for different scoring systems, these experiments are carried out separately. Thus, their group boundaries and cutoff marks of course subjects for each gr oup may be different. The objective is to maximize their performances in terms of the measurements defined in Section 5.1. Table 1 shows the results of various systems. The holdout set has one year of GEP students (153 of them), and out of these 153, 45 of them did not do well in their A-level exams. From this table we can make the following observations:  X  Examination of the Precision_by_Effort measurement  X  SBA is also better than the other two scoring methods. Note To explain why SBA performs better than other systems, let us see the rankings of the weak students produced by various systems. To save space, after scoring and ranking, we divide each ranked list equally into 10 bins. Table 2 shows the weak students in each bin produced by each scoring system. Clearly, we see that the SBA scoring method is superior to others. It does the best in pushing those weak students to the top. For example, SBA captures more weak students in its top 3 bins as compared to the other two scoring techniques. The two bottom bins of SBA also have no weak students, while the other systems all have some weak students in the bottom bins, which is undesirable. In this paper, we reported a real-life application in an education domain. It aims to select the potentially weak students for remedial classes. We showed that by using a data mining technique we achieve much better results. This reduces the burden on both students and teachers. In our future work, we plan to build an application specific system that can be used by any educational institution to select the right students for various purposes. These students can be good students, weak students or students with special needs, etc. We are grateful to the Gifted Education Branch of the Ministry of Education, Singapore, in particular Director Dr. Bee Geok Tan for initiating this project and for her constant involvement and help. We thank Yiyuan Xia for modifying the C4.5 program to produce the C4.5-score system. We also thank Chew Lim Tan, Wynne Hsu, and Huan Liu for useful discussions. The project is funded by a research grant from National Science and Technology Board of Singapore and National University of Singapore under RP3981678. [1] Agrawal, R., Imielinski, T., Swami, A.  X  Mining association [2] Agrawal, R. and Srikant, R.  X  Fast algorithms for mining [3] Bayardo, R., Agrawal, R, and Gunopulos, D. "Constraint-[4] Bayardo, R., Agrawal, R.  X  Mining the most interesting [5] Brin, S. Motwani, R. Ullman, J. and Tsur, S.  X  Dynamic [6] Chan, P. K., and Stolfo, S. J.  X  Towards scaleable learning [7] Dietterich, T and Baskiri, G.  X  Solving multiclass learning [8] Dong. G. Zhang, X. Wong, L. Li, J. 1999.  X  CAEP: [9] Fawcett, T., and Provost, F.  X  Combining data mining and [10] Fayyad, U. M. and Irani, K. B. "Multi-interval [11] Fukuda, T. Morimoto, Y. Morishita, S and Tokuyama, T. [12] Gehrke, J., Ganti, V., Ramakrishnan, R. and Loh, W. [13] Han, J. and Fu, Y.  X  Discovery of multiple-level association [14] Hughes, A. M. The complete database marketer: second-[15] Kohavi, R., John, G., Long, R., Manley, D., and Pfleger, K. [16] Kubat, M. and Matwin, S.  X  Addressing the curse of [17] Lee, W., Stolfo, S. J., and Mok, K. W.  X  Mining audit data [18] Ling, C. and Li C. "Data mining for direct marketing: [19] Liu, B., Hsu, W. and Ma, Y.  X  Integrating classification and [20] Liu, B., Hsu, W. and Ma, Y.  X  Mining association rules with [21] Liu, B., Ma, Y., Wong, C K. and Yu, P.  X  Target selection [22] Mannila, H., Pavlov, D and Smyth, P  X  Prediction with local [23] Meretkis, D. &amp; Wuthrich, B.  X  Extending na  X  ve bayes [24] Merz, C. J, and Murphy, P. UCI repository of machine [25] Ng. R. T. Lakshmanan, L. Han, J.  X  Exploratory mining and [26] Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., and [27] Piatetsky-Shapiro, G. and Massand, B.  X  Estimating [28] Provost, F., and Fawcettt, T.  X  Analysis and visualization of [29] Quinlan, R. C4.5 : program for machine learning. Morgan [30] Rastogi, R. and Shim, K. 1998.  X  Mining optimized [31] Shafer, J., Agrawal, R. &amp; Mehta, M. "SPRINT: A scalable [32] Sanjeev, A. P. and Zytkow, J. "Discovering Enrollment [33] Toivonen, H.  X  Sampling large databases for association [34] Tong, A. Lu, H., Han, J. and Feng, L.  X  Break the barrier of 
