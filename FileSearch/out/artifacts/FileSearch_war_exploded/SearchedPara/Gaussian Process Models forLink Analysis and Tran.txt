 In many scenarios the data of interest consist of relational observations on the edges of networks. Typically, a given finite collection of such relational data can be represented as an M  X  N matrix Y = { y i,j } , which is often partially observed because many elements are missing. Sometimes accompanying Y are attributes of nodes or edges. As an important nature of networks, { y i,j } are highly inter-dependent even conditioned on known node or edge attributes. The phenomenon is extremely common in real-world data, for example, The goal of this paper is to design a Gaussian process (GP) [13] framework to model the depen-dence structure of networks, and to contribute an efficient algorithm to learn and predict large-scale relational data. We explicitly construct a series of parametric models indexed by their dimension-ality, and show that in the limit we obtain nonparametric GP priors consistent with the dependence of edge-wise measurements. Since the kernel matrix is on a quadratic number of edges and the computation cost is even cubic of the kernel size, we develop an efficient algorithm to reduce the computational complexity. We also demonstrate that transfer learning has an intimate connection to link prediction. Our method generalizes several recent transfer learning algorithms by additionally learning a task-specific kernel that directly expresses the dependence between tasks. The application of GPs to learning on networks or graphs has been fairly recent. Most of the work in this direction has focused on GPs over nodes of graphs and targeted at the classification of nodes [20, 6, 10]. In this paper, we regard the edges as the first-class citizen and develop a general GP framework for modeling the dependence of edge-wise observations on bipartite, undirected and directed graphs. This work extends [19], which built GPs for only bipartite graphs and proposed an algorithm scaling cubically to the number of nodes. In contrast, the work here is more general and the algorithm scales linearly to the number of edges. Our study promises a careful treatment to model the nature of edge-wise observations and offers a promising tool for link prediction. 2.1 Modeling Bipartite Graphs We first review the edge-wise GP for bipartite graphs [19], where each observation is a measurement on a pair of objects of different types, or under a pair of heterogenous conditions. Formally, let U and V be two index sets, then y i,j denotes a measurement on edge ( i, j ) with i  X  U and j  X  V . In the context of transfer learning, the pair involves a data instance i and a task j , and y i,j denotes the label of data i within task j . The probabilistic model assumes that y i,j are noisy outcomes of a real-valued function f : U X V  X  R , which follows a Gaussian process GP ( b, K ) , characterized by mean function b and covariance (kernel) function between edges where  X  and  X  are kernel functions on U and V , respectively. As a result, the realizations of f on a and covariance K =  X   X   X  , where  X  means Kronecker product. The dependence structure of edges is decomposed into the dependence of nodes. Since a kernel is a notion of similarity, the model ex-and f ( i 0 , j 0 ) .
 It is essential to learn the kernels  X  and  X  based on the partially observed Y , in order to capture the dependence structure of the network. For transfer learning, this means to learn the kernel  X  between data instances and the kernel  X  between tasks. Having  X  and  X  is it then possible to predict those missing y i,j based on known observations by using GP inference.
 Theorem 2.1 ([19]) . Let f ( i, j ) = D  X  1 / 2 P D h K (( i, j ) , ( i 0 , j 0 )) =  X ( i, i 0 ) X ( j, j 0 ) .
 Theorem (2.1) offers an alternative view to understand the model. The edge-wise function f can be decomposed into a product of two sets of intermediate node-wise functions, { g k }  X  which are i.i.d. samples from two GP priors GP (0 ,  X ) and GP (0 ,  X ) . The theorem suggests that the GP model for bipartite relational data is a generalization of a Bayesian low-rank matrix factorization F = HG &gt; + B , under the prior H  X  X  M  X  D (0 ,  X  , I ) and G  X  X  N  X  D (0 ,  X  , I ) . When D is finite, the elements of F are not Gaussian random variables. 2.2 Modeling Directed and Undirected Graphs In this section we model observations on pairs of nodes of the same set U . This case includes both directed and undirected graphs. It turns out that the directed graph is relatively easy to handle while deriving a GP prior for undirected graphs is slightly non-trivial. For the case of directed graphs, we let the function f : U  X U  X  R follow GP ( b, K ) , where the covariance function between edges is and C : U X U  X  R is a kernel function between nodes. Since a random function f drawn from the can be modeled. The covariance function Eq. (2) can be derived from Theorem (2.1) by setting that { g k } and { h k } are two independent sets of functions i.i.d. sampled from the same GP prior GP (0 , C ) , modeling the situation that each node X  X  behavior as a sender is different but statistically related to it X  X  behavior as a receiver. This is a reasonable modeling assumption. For example, if two papers cite a common set of papers, their are also likely to be cited by a common set of other papers. For the case of undirected graphs, we need to design a GP that ensures any sampled function to be symmetric . Following the construction of GP in Theorem (2.1), it seems that f is symmetric if g k  X  h k for k = 1 , . . . , D . However a calculation reveals that f is not bounded in the limit D  X  X  X  . Theorem (2.2) shows that the problem can be solved by subtracting a growing quantity D 1 / 2 C ( i, j ) as D  X  X  X  , and suggests the covariance function With such covariance function , f is ensured to be symmetric because the covariance between f ( i, j ) and f ( j, i ) equals the variance of either.
 Theorem 2.2. Let f ( i, j ) = D  X  1 / 2 P D is a collection of random variables independently following the same distribution, and has the mean C ( i, j ) . The covariance function is Cov ( f ( i, j ) , f ( i 0 , j 0 )) = 1 D Interestingly, Theorem (2.2) recovers Theorem (2.1) and is thus more general. To see the connection, let h k  X  GP (0 ,  X ) and g k  X  GP (0 ,  X ) be concatenated to form a function t k , then we have t  X  X P (0 , C ) and the covariance is For i, i 0  X  X  and j, j 0  X  X  , applying Theorem (2.2) leads to Theorems (2.1) and (2.2) suggest a general GP framework to model directed or undirected relation-ships connecting heterogeneous types of nodes. Basically, we learn node-wise covariance functions, like  X  ,  X  , and C , such that edge-wise covariances composed by Eq. (1), (2), or (3) can explain the happening of observations y i,j on edges. The proposed framework can be extended to cope with more complex network data, for example, networks containing both undirected links and directed links. We will briefly discuss some extensions in Sec. 6. We consider the regression case under a Gaussian noise model, and later briefly discuss extensions corresponding quantities of the latent function f , and K be the | O | X | O | matrix of K between edges having observations, computed by Eq. (1)-(3). Then observations on edges are generated by directed/undirected graph case we let  X  i =  X  i for any i  X  X  . f can be analytically marginalized out, the marginal distribution of observations is then where  X  = {  X , b, K } . The parameters can be estimated by minimizing the penalized negative log-has the form: Gradient-based optimization packages can be applied to find a local optimum of  X  . However the computation can be prohibitively high when the size | O | of measured edges is very big, because the tens of thousands or even millions. A slightly improved algorithm was introduced in [19], with a complexity O ( M 3 + N 3 ) cubic to the size of nodes. The algorithm employed a non-Gaussian approximation based on Theorem (2.1) and is applicable to only bipartite graphs.
 We reduce the memory and computational cost by exploring the special structure of K as discussed  X  z We turn the problem of optimizing K into the problem of optimizing X = [ x 1 , . . . , x M ] &gt; and K = UU &gt; , where U is an | O | X  L matrix, L  X  | O | , therefore applying the Woodbury identity C in the bipartite graph case and the directed graph case, respectively there are where the rows of U are indexed by ( i, j )  X  O . For the undirected graph case, we first rewrite the kernel function
K (( i, j ) , ( i 0 , j 0 )) =  X  x i  X  x j , x i 0  X  x j 0  X  +  X  x i  X  x j , x j 0  X  x i 0  X  = = and then obtain a simple form for the undirected graph case The overall computational cost is at O ( L 3 + | O | L 2 ) . Empirically we found that the algorithm is efficient to handle L = 500 when | O | is about millions. The gradients with respect to U can be found in [12]. Further calculation of gradients with respect to X and Z can be easily derived. Here we omit the details for saving the space. Finally, in order to predict the missing measurements, we only need to estimate a simple linear model f ( i, j ) = w &gt; u i,j + b i,j . 3.1 Incorporating Additional Attributes and Learning from Discrete Observations There are different ways to incorporate node or edge attributes into our model. A common practice is to let the kernel K ,  X  , or  X  be some parametric function of attributes. One such choice is the RBF function. However, node or edge attributes are typically local information while the network itself is rather a global dependence structure, thus the network data often has a large part of patterns that are independent of those known predictors. In the following, via the example of placing a Bayesian prior on  X  : U X U  X  R , we describe a flexible solution to incorporate additional knowledge. Let  X  0 be the covariance that we wish  X  to be apriori close to. We apply the prior p ( X ) = 1 and use its negative log-likelihood as a regularization for  X  : where  X  is a hyperparameter predetermined on validation data, and  X   X  1 is a small number to be optimized. The energy function E ( X ) is related to the KL divergence D KL ( GP (0 ,  X  0 ) ||GP (0 ,  X  + by the dimensionality, then E ( X ) can be derived from a likelihood of  X  as if each dimension of the attributes is a random sample from GP (0 ,  X  +  X   X  1  X  ) . If the attributes are nonlinear predictors we can conveniently set  X  0 by a nonlinear kernel. We set  X  0 = I if the corresponding attributes are absent. ` ( X ) , ` ( C ) and ` ( K ) can be set in the same way.
 The observations can be discrete variables rather than real values. In this case, an appropriate like-lihood function can be devised accordingly. For example, the probit function could be employed inference techniques, e.g. Laplace approximation, can be applied to finding a Gaussian distribution that approximates the true likelihood. Then, the marginal likelihood (8) can be written as an explicit expression and the gradient can be derived analytically as well. Transfer Learning : As we have suggested before, the link prediction for bipartite graphs has a tight f : U  X V  X  R consists of N node-wise functions f j : U  X  R for j = 1 , . . . , N . If we fix Baysian model that assumes multiple tasks sharing the same GP prior [18]. In particular, the negative logarithm of p in a regularization framework [3], if the log-determinant term is replaced by a trace regularization is convex with jointly { f j } and  X  . The GP approach differs from the regularization approach in two aspects: (1) f j are treated as random variables which are marginalized out, thus we only need to estimate  X  ; (2) The regularization for  X  is a non-convex log-determinant term. Interestingly, because log |  X  | X  tr(  X  )  X  M , the trace norm is the convex envelope for the log-determinant, and thus the two minimization problems are somehow doing similar things. However, the framework introduced in this paper goes beyond the two methods by introducing an informative kernel  X  between tasks. From a probabilistic modeling point of view, the independence of { f j } conditioned on  X  is a restrictive assumption and even incorrect when some task-specific attributes are given (which means that { f j } are not exchangeable anymore). The task-specific kernel for transfer learning has been recently introduced in [4], which however increased the computational complexity by a factor of N 2 . One contribution of this paper on transfer learning is an algorithm that can efficiently solve the learning problem with both data kernel  X  and task kernel  X  .
 Gaussian Process Latent-Variable Model (GPLVM) : Our learning algorithm is also a generaliza-Eq. (9) is equivalent to the form of GPLVM, where Y is a fully observed M  X  N matrix, the mean B = 0 , and there is no further regularization on  X  . GPLVM assumes that columns of Y are conditionally independent given  X  . In this paper we consider a situation with complex dependence of edges in network graphs.
 Other Related Work : Getoor et al. [7] introduced link uncertainty in the framework of probabilistic the block structure of links. Link prediction was casted as structured-output prediction in [15, 2]. Statistical models based on matrix factorization was studied by [8]. Our work is similar to [8] in the
MMMF results, and the fourth row is of the bilinear results. sense that relations are modeled by multiplications of node-wise factors. Very recently, Hoff showed in [9] that the multiplicative model generalizes the latent-class models [11, 1] and can encode the transitivity of relations.
We set the dimensionality of the model via validation on 10% of training data. In cases that the additional attributes on nodes or edges are either unavailable or very weak, we compare our method with max-margin matrix factorization (MMMF) [14] using a square loss, which is similar to singular value decomposition (SVD) but can handle missing measurements. 5.1 A Demonstration on Face Reconstruction
A subset of the UMist Faces images of size 112  X  92 was selected to illustrate our algorithm, which consists of 10 people at 10 different views. We manually knocked 10 images off as test cases, as presented in Figure 1, and treated each image as a vector that leads to a 103040  X  10 matrix with 103040 missing values, where each column corresponds a view of faces. GP was trained by setting
L 1 = L 2 = 4 on this matrix to learn from the appearance relationships between person identity and pose. The images recovered by GP for the test cases are presented as the second row of Figure 1-right (RMSE=0.2881). The results of MMMF are presented as the third row (RMSE=0.4351). We also employed the bilinear models introduced by [16], which however does not handle missing data of a matrix, and put the results at the bottom row for comparison. Quantitatively and perceptually our model offers a better generalization to unseen views of known persons. 5.2 Collaborative Filtering
Collaborative filtering is a typical case of bipartite graphs, where ratings are measurements on edges of user-item pairs. We carried out a serial of experiments on the whole EachMovie data, which includes 61265 users X  2811718 distinct numeric ratings on 1623 movies. We randomly selected selection was carried out 20 times independently.

For comparison purpose, we also evaluated the predictive performance of four other approaches: 1) Movie Mean: the empirical mean of ratings per movie was used as the predictive value of all the predictive value of the users X  rating on all movies; 3) Pearson Score: the Pearson correlation coefficient corresponds to a dot product between normalized rating vectors. We computed the Gram matrices of the Pearson score with mean imputation for movies and users respectively, and took attributes in this experiment and carried out least square regression on observed entries. 4) MMMF.
The optimal rank was decided by validation. Table 1: Test results on the EachMovie data. The number in bracket indicates the rank we applied. The The results of these approaches are reported in Table 1. The per-movie average yields much better results than the per-user average, which is consistent with the findings previously reported by [5]. The improvement is noticeable by using more components of the Pearson score, but not significant. The generalization performance of our algorithm is better than that of others. T-test showed a signif-icant difference with p-value 0.0387 of GP over MMMF (with 15 dimensions) in terms of RMSE. It is well worth highlighting another attractiveness of our algorithm  X  the compact representation of factors. On the EachMovie data, there are only three factors that well represent thousands of items individually. We also trained MMMF with 3 factors as well. Although the three-factor solution GP found is also accessible to other models, MMMF failed to achieve comparable performance on this case (i.e., see results of MMMF(3)). In each trial, the number of training samples is around 2.25 million. Our program took about 865 seconds to accomplish 500 L-BFGS updates on all 251572 parameters using an AMD Opteron 2.6GHz processor. 5.3 Text Categorization based on Contents and Links We used a part of Cora corpus including 751 papers on data structure (DS), 400 papers on hardware and architecture (HA), 1617 on machine learning (ML) and 1575 on programming language (PL). We treated the citation network as a directed graph and modeled the link existence as binary labels. Our model applied the probit likelihood and learned a node-wise covariance function C , L = 50  X  50 , which composes an edge-wise covariance K by Eq. (2). We set the prior covariance C 0 by the linear kernel computed by bag-of-word content attributes. Thus the learned linear features encode both link and content information, which were then used for document classification. We compare several other methods that provide linear features for one-against-all categorization using SVM: 1) CONTENT: bag-of-words features; 2) LINK: each paper X  X  citation list; 3) PCA: 50 components by PCA on the concatenation of bag-of-word features and citation list for each paper. We chose the dimensionality 50 for both GP and PCA, because their performances both saturated when the dimensionality exceeds 50. We reported results based on 5-fold cross validation in Table 2. GP clearly outperformed other methods in 3 out of 4 categories. The main reason we believe is that our approach models the in-bound and out-bound behaviors simultaneously for each paper . In this paper we proposed GPs for modeling data living on links of networks. We described solu-tions to handle directed and undirected links, as well as links connecting heterogenous nodes. This work paves a way for future extensions for learning more complex relational data. For example, we be undirected. Based on the feature representations, Eq.(10)-right for directed links and Eq.(12) for undirected links, the covaraince is K (( i, j ) , ( i 0 , j 0 )) = 1 / which indicates that dependence between a directed link and an undirected link is penalized com-pared to dependence between two undirected links. Moreover, GPs can be employed to model multiple networks involving multiple different types of nodes. For each type, we use one node-wise covariance. Letting covariance between two different types of nodes be zero, we obtain a huge block-diagonal node-wise covariance matrix, where each block corresponds to one type of nodes. This big covariance matrix will induce the edge-wise covariance for links connecting nodes of the same or different types. In the near future it is promising to apply the model to various link prediction or network completion problems.
 [1] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing, Mixed membership stochastic block [2] S. Andrews and T. Jebara, Structured Network Learning. NIPS Workshop on Learning to [3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learn-[4] E. V. Bonilla, F. V. Agakov, and C. K. I. Williams. Kernel multi-task learning using task-[5] J. Canny. Collaborative filtering with privacy via factor analysis. International ACM SIGIR [6] W. Chu, V. Sindhwani, Z. Ghahramani, and S. S. Keerthi. Relational learning with gaussian [7] L. Getoor, E. Segal, B. Taskar, and D. Koller. Probabilistic models of text and link structure [8] P. Hoff. Multiplicative latent factor models for description and prediction of social networks. [9] P. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data. to [10] A. Kapoor, Y. Qi, H. Ahn, and R. W. Picard. Hyperparameter and kernel learning for graph [11] C. Kemp, J. B. Tenenbaum, T. L. Griffiths, T. Yamada, and N. Ueda. Learning systems of [12] N. Lawrence. Gaussian process latent variable models. Journal of Machine Learning Research , [13] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning . The MIT [14] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative [15] B. Taskar, M. F. Wong, P. Abbeel, and D. Koller. Link prediction in relational data. Neural [16] J. B. Tenenbaum and W. T. Freeman. Separating style and content with bilinear models. Neural [17] Z. Xu, V. Tresp, K. Yu, and H.-P. Kriegel. Infinite hidden relational models. International [18] K. Yu, V. Tresp, and A. Schwaighofer. Learning Gaussian processes from multiple tasks. [19] K. Yu, W. Chu, S. Yu, V. Tresp, and Z. Xu. Stochastic relational models for discriminative link [20] X. Zhu, J. Lafferty, and Z. Ghahramani. Semi-supervised learning: From gaussian fields to
