 In recent years, machine learning (ML) has been used more and more to solve complex tasks in dif-ferent disciplines, ranging from Data Mining to In-formation Retrieval or Natural Language Processing (NLP). These tasks often require the processing of structured input, e.g., the ability to extract salient features from syntactic/semantic structures is criti-cal to many NLP systems. Mapping such structured data into explicit feature vectors for ML algorithms requires large expertise, intuition and deep knowl-edge about the target linguistic phenomena. Ker-nel Methods (KM) are powerful ML tools (see e.g., (Shawe-Taylor and Cristianini, 2004)), which can al-leviate the data representation problem. They substi-tute feature-based similarities with similarity func-tions, i.e., kernels, directly defined between train-ing/test instances, e.g., syntactic trees. Hence fea-ture vectors are not needed any longer. Additionally, kernel engineering, i.e., the composition or adapta-tion of several prototype kernels, facilitates the de-sign of effective similarities required for new tasks, e.g., (Moschitti, 2004; Moschitti, 2008).
 The tutorial aims at addressing the problems above: firstly, it will introduce essential and simplified the-ory of Support Vector Machines and KM with the only aim of motivating practical procedures and in-terpreting the results. Secondly, it will simply de-scribe the current best practices for designing ap-plications based on effective kernels. For this pur-pose, it will survey state-of-the-art kernels for di-verse NLP applications, reconciling the different ap-
