
Hidenao Abe 1 ,ShusakuTsumoto 1 ,MihoOhsaki 2 , and Takahira Yamaguchi 3 In recent years, huge data are easily stored on information systems in natural sci-ence, social science and business domains, developing information technologies. With these huge data, people hope to utilize them for their purposes. Besides, data mining techniques have been widely known as a process for utilizing stored data on database systems, combining different kinds of technologies such as database technologies, statistical methods and machine learning methods. Es-pecially, IF-THEN rules, which are produced by rule induction algorithms, are discussed as one of highly usable and readable output of data mining. However, to large dataset with hundreds attributes including noises, the process often ob-tains many thousands of rules. From such huge rule set, it is difficult for human experts to find out valuable knowledge which are rarely included in the rule set.
To support such a rule selection, many efforts have done using objective rule evaluation indices such as recall, precision, and other interestingness measure-ments (we call them  X  X bjective indices X  l ater). However, it is also difficult to estimate a criterion of a human expert with single objective rule evaluation in-dex, because his/her subjective criterion such as interestingness and importance for his/her purpose is influenced by the amount of his/her knowledge and/or a passage of time.

To above issues, we have been developed an adaptive rule evaluation support method for human experts with rule evaluation models, which predict experts X  criteria based on objective i ndices, re-using results of evaluations of human ex-perts. In Section 3, we describe the rule evaluation model construction method based on objective indices. Then we prese nt a performance comparison of learn-ing algorithms for constructing rule evaluation models in Section 4. With the results of the comparison, we discuss about the availability of our rule evaluation model construction approach. Many efforts have done to select valuable rules from mined large rule set based on objective rule evaluation indexes. Some of these works suggest the indexes to discover interesting rules from such a large amount of rules.

Focusing on interesting rule selection w ith objective indexes, researchers have developed more than forty objective indexes based on number of instances, probability, statistics values, information quantity, distance of rules or their at-tributes, and complexity of a rule[11, 21, 23]. Most of these indexes are used to remove meaningless rules rather than to discover really interesting ones for a human expert, because they can not include domain knowledge. In contrast, a dozen of subjective indexes estimate how a rule fits with a belief, a bias or a rule template formulated beforehand by a human expert. Although these sub-jective indexes are useful to discover really interesting rules to some extent due to their built-in domain knowledge, they depend on the precondition that a hu-man expert is able to clearly formulate his/her interest. Although interestingness indexes were verified their availabilities on each suggested domain, nobody has validated their availabilities on the other domains or/and characteristics related to the background of a given dataset.

Ohsakiet.al[15]investi gated the relation between objective indexes and real human interests, taking real data mining results and their human evaluations. In this work, the comparison shows that it is difficult to predict real human interests with a single objective index. Based on the result, they indicated the possibility of logical combination of the objective indexes to predict real human interests more exactly. We considered the process of modeling rul e evaluations of human experts as the process to clear up relationships between the human evaluations and features of input if-then rules. With this consideration, we decided that the process of rule evaluation model construction can be implemented as a learning task. Fig.1 shows the process of rule evaluation model construction based on re-use of human evaluations and objective indices for each mined rule.

At the training phase, attributes of a meta-level training data set is obtained by objective indices such as recall, precisi on and other rule evaluation values. The human evaluations for each rule are joined as class of each instance. To obtain this data set, a human expert has to evaluate the whole or part of input rules at least once. After obtaining the training data set, its rule evaluation model is constructed by a learning algorithm. At the prediction phase, a human expert receives predictions for new rule s based on their values of the objective indices. Since the task of rule evaluation models is a prediction, we need to choose a learning algorithm with higher a ccuracy as same as curre nt classification problems. To predict human evaluation labels of a new rule based on objective indices more exactly, we have to construct a rule evaluation model, which has higher predictive accuracy.

In this section, we firstly present the result of an empirical evaluation with the dataset from the result of a meningitis data mining[9]. Then to confirm the performance of our approach on the other datasets, we evaluated the five algorithms on four rule sets from four kinds of UCI benchmark datasets [10]. With the experimental results, we discuss about the following three view points: accuracies of rule evaluation models, ana lyzing learning curves of the learning algorithms, and contents of learned rule evaluation models.

As an evaluation of accuracies of rule evaluation models, we have compared predictive accuracies on th e whole dataset and Leave-One-Out. The accuracy of a validation dataset D is calculated with correct ly predicted instances Correct ( D ) as Acc ( D )=( Correct ( D ) / | D | )  X  100, where | D | means the size of the dataset. Recalls of class i on a validation dataset is calcul ated with correctly predicted instances about the class Correct ( D i )as Recall ( D i )=( Correct ( D i ) / | D i | )  X  100, where | D i | means the size of instances with class i . Also the precision of class i is calculated with the siz e of instances predicted i as P recision ( D i )= ( Correct ( D i ) /P redicted ( D i ))  X  100.

As for learning curves, we obtained le arning curves about accuracies to the whole training dataset to evaluate whether each learning algorithm can perform in early stage of a process of rule evalu ations. Accuracies from randomly sub-sampled training datasets are averaged with 10 times trials on each percentage of subset.

Looking at elements of the rule evaluation models on the meningitis data mining result, we consider the characteristics of objective indices, which are used in these rule evaluation models.

To construct a dataset to learn a rule evaluation model, values of objective indices have been calculated for each rul e, taking 39 objective indices as shown in Table1. Thus each dataset for each rule set has the same number of instances as the rule set. Each instance consists of 40 attributes including the class attribute.
To these dataset, we applied five learning algorithms to compare their perfor-mance as a rule evaluation model construction method. We used the following learning algorithms from Weka[22]: C4.5 decision tree learner[18] called J4.8, neural network learner with back propagation (BPNN)[12], support vector ma-chines (SVM) 1 [17], classification via linear regressions (CLR) 2 [3], and OneR[13]. 4.1 Constructing Rule Evaluation Models on an Actual Datamining In this case study, we have taken 244 rules, which are mined from six dataset about six kinds of diagnostic problems as shown in Table2. These datasets are consisted of appearances of meningitis patients as attributes and diagnoses for each patient as class. Each rule set was mined with each proper rule induction al-gorithm composed by a constructive meta-learning system called CAMLET[9]. For each rule, we labeled three evaluations (I:Interesting, NI:Not-Interesting, NU:Not-Understandable), according to evaluation comments from a medical expert.
 Comparison on Classification Performances. In this section, we show the result of the comparisons of accuracies on the whole dataset, r ecall of each class label, and precisions of each class label. Since Leave-One-Out holds just one test instance and remains as the training dataset repeatedly for each instance of a given dataset, we can evaluate the performance of a learning algorithm to a new dataset without any ambiguity.

The results of the performances of the five learning algorithms to the whole training dataset and the results of Leave-One-Out are also shown in Table3. All of the accuracies, Recalls of I and NI, and Precisions of I and NI are higher than predicting default labels.

Comparing with the accuracy of OneR, the other learning algorithms achieve equal or higher performance with combin ation of multiple objective indices than sorting with single objective index. Looking at Recall values on class I, BPNN have achieved the highest performance. As for the other algorithms, they show lower performance than OneR, because t hey have tended to be learned classifi-cation patterns for the major class NI.
 The accuracies of Leave-One-Out shows ro bustness of each learning algorithm. These learning algorithms have achieved from 75.8% to 81.9%. However, these learning algorithms have not been able to classify the instances with class NU, because it is difficult to predict a minor class label in this dataset. Learning Curves of the Learning Algorithms. Since the rule evaluation model construction method needs evaluations of mined rules by a human ex-pert, we have investigated learning curves of each learning algorithm to estimate minimum training subset to construct a valid rule evaluation model. The upper table in Fig.2 shows accuracies to the whole training dataset with each subset of training dataset. The percentages of achievements for each learning algorithm, comparing with the accuracy with the whole dataset, are shown in the lower chart of Fig.2.

As shown in these results, SVM and CLR, which learn hype-planes, achieves grater than 95% with only less than 10% of training subset. Although decision tree learner and BPNN could learn better classifier to the whole dataset than these hyper-plane learners, they need mo re training instances to learn accurate classifiers.

To eliminate known ordinary knowledge from large rule set, it is needed to classify non-interesting rules correctly. The right upper table in Fig.2 shows percentages of recalls on NI. The right lower chart in Fig.2 also shows the per-centages of achievements on recall of NI, comparing with the recall of NI on the whole training dataset. Looking at this result, we can eliminate NI rules with rule evaluation models from SVM and BPNN even if there is only 10% of rule evaluations by a human expert. This is guaranteed with no less than 80% precisions of all learning algorithms.
 Rule Evaluation Models on the Actual Datamining Result Dataset.
 In this section, we present rule evaluat ion models to the whole dataset learned with OneR, J4.8 and CLR, because they are represented as explicit models such as a rule set, a decision tree, and a set of linear models.

Fig.3 shows rule evaluation models on the actual data mining result: The rule set of OneR is shown in Fig.3(a), Fig.3(b) shows the decision tree learned with J4.8, and Fig.3(c) shows linea r models to classify each class.

Looking at indices used in learned rule evaluation models, they are not only the group of indices increasing with a correctness of a rule, but also they are used some different groups of indices on different models. Almost indices such as YLI1, Laplace Correct ion, Accuracy, Precision, R ecall, Coverage, PSI and Gini Gain are the former type of indices on the models. The later indices are GBI and Peculiality, which sums up differen ce of antecedents between one rule and the other rules in the same ruleset. This corresponds to the comment from the human expert. He said that he evaluated these rules not only correctness but also his interest based on his expertise. 4.2 Constructing Rule Evaluation Models on Artificial Evaluation We have also evaluated our rule evaluation model construction method with rule sets from four datasets of UCI Machine Learning Repository to confirm the lower limit performances on probabilistic class distributions.

We selected the following four datasets: Mushroom, Heart, Internet Adver-tisement Identification (called InternetAd later) and Letter. With these datasets, we obtained rule sets with bagged PART, which repeatedly executes PART[4] to bootstrapped training sub-sample datasets.

To these rule sets, we calculated the 3 9 objective indices as attributes of each rule. As for the class of these datasets, we set up three class distributions with multinomial distribution. Table4 shows us the datasets with three different class distributions. The class distribution for  X  X istribution I X  is P =(0 . 35 , 0 . 3 , 0 . 3) where p i is the probability for class i . Thus the number of class i in each instance D j become p i D j . As the same way, the probability vector of  X  X istribution II X  is P =(0 . 3 , 0 . 5 , 0 . 2), and  X  X istribution III X  is P =(0 . 3 , 0 . 65 , 0 . 05). Accuracy Comparison on Classification Performances. To above datasets, we have attempted the five learning algorithms to estimate whether their clas-sification results can go to or beyond the accuracies of just predicting each de-fault class. The left table of Table5 shows the accuracies of the five learning algo-rithms to each class distribution of the three datasets. As shown in Table5, J48 and BPNN always work better than just pre dicting a default class. However, their performances are suffered from probabilistic class distributions to larger datasets such as Heart and Letter.
 Evaluation on Learning Curves. As same as evaluations of learning curves on the meningitis rule set, we have estimated the minimum training subsets for a valid model, which works better than just predicting a default class.
The right table in Table5 shows sizes of minimum training subsets, which can be constructed more a ccurate rule evaluation models than percentages of a default class by each learning algorithm. To smaller dataset, such as Mushroom and InternetAd, they can construct valid models with less than 20% of given training datasets. However, to larger dataset, they need more training subsets to construct valid models, because their performances with whole training dataset fall to the percentages of default class of each dataset as shown in the left table in Table5. In this paper, we have described rule evaluation support method with rule eval-uation models to predict evaluations for an IF-THEN rule based on objective indices, re-using evaluations of a human expert.

As the result of the performance comparison with the five learning algo-rithms, rule evaluation models have ach ieved higher accuracies than just pre-dicting each default class. Considering t he difference between the actual evalua-tion labeling and the artificial evaluation labeling, it is shown that the medical expert evaluated with noticing particu lar relations between an antecedent and a class/another antecedent in each rule. In the estimation of robustness to a new rule with Leave-One-Out, we have achieved more than 75.8% with these learning algorithms. On the evaluation with learning curves to the dataset of the actual datamining result, SVM and CLR have achieved more than 95% of achievement ratio compared to the accura cy of the whole training dataset with less than 10% of subset of the training dataset with certain human evaluations. These results indicate the availability of this rule evaluation support method for ahumanexpert.

As future work, we will introduce a selection method of learning algorithms to construct a proper rule evaluation model according to each situation. We also apply this rule evaluation support method to estimate other data mining result such as decision tree, rule set, and co mmittee of them with objective indices, which evaluate whole mining results.
