 In this paper we address the issue of using local embeddings for data visualization in two and three dimensions, and for classification. We advocate their use on the basis that they provide an efficient mapping procedure from the original di-mension of the data, to a lower intrinsic dimer~sion. We depict how they can accurately capture the user's perception of similarity in high-dimensional data for visualization pur-poses. Moreover, we exploit the low-dimensional mapping provided by these embeddings, to develop new classification techniques, and we show experimentally that the classifica-tion accuracy is comparable (albeit using fewer dimensions} to a number of other classification procedures. 
During the last few years we have experienced an explo-sive growth in the amount of data that is being collected, leading to the creation of very large databases, such as com-mercial data warehouses. New applications have emerged that require the storage and retrieval of massive amounts of data; for example: protein matching in biomedical appli-cations, fingerprint recognition, meteorological predictions, and satellite image repositories. 
Most problems of interest in data mining involve data with a large number of measurements (or dimensions). The re-duction of dimensionality can lead to an increased capability of extracting knowledge from the data by means of visual-ization, and to new possibilities in designing efficient and possibly more effective classification schemes. Dimension-ality reduction can be performed by keeping only the most important dimensions, i.e. the ones that hold the most infor-mation for the task at hand, and/or by projecting some di-*Supported by NSF CAREER Award 9984729, NSF IIS-9907477, and the DoD. tSupported by NSF CAREER Award 0133825 permission and/or a fee. SIGKDD '02 Edmonton. Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. mensions onto others. These steps will improve significantly our ability to visualize the data (by mapping them in two or three dimensions), and facilitate an improved query time, by refraining from examining the original multi-dimensional data mad scanning instead their lower-dimensional "sum-m axles". 
For visualization, the challenge is to embed a set of ob-servations into a Euclidean feature-space, that preserves as closely as possible their intrinsic metric structure. For clas-sification, we desire to map the data into a space whose di-mensions clearly separate members from different classes. 
Recently, two new dimensionality reduction techniques have been introduced, namely Isomap [15] and LLE [26]. These methods attempt to best preserve the local neighbor-hood of each object, while preserving the global distances "through" the rest of the objects. They have been used for visualization purposes, by mapping data into two or three di-mensions. Both methods perform well when the data belong to a single well sampled cluster, and fail to nicely visualize the data when the points axe spread among multiple clus-ters. In this paper we propose a mechanism to avoid this limitation. 
Furthermore, we show how these methods could be used for classification purposes. Classification is a key step for many tasks in data mining, whose aim is to discover un-known relationships and/or patterns from large set of data. A variety of methods has been proposed to address the prob-lem. A simple and appealing approach to classification is the K-nearest neighbor method [21]: it finds the K-nearest neighbors of the query point xo in the dataset, and then predicts the class label of x0 as the most frequent one oc-curing in the K neighbors. However, when applied on large datasets in high dimensions, the time required to compute the neighborhoods (i.e., the distances of the query from the points in the dataset) becomes prohibitive, making answers intractable. Moreover, the curse-of-dimensionality, that af-fects any problem in high dimensions, causes highly biased estimates, thereby reducing the accuracy of predictions. 
One way to tackle the curse-of-dimensionality-problem for classification is to consider locally adaptive metric techniques, with the objective of producing modified local neighbor-hoods in which the posterior probabilities are approximately constant ([10, 11, 7]). A major drawback of locally adap-tive metric techniques for nearest neighbor classification is the fact that they all perform the K-NN procedure multi-645 pie times in a feature space that is tranformed by means of weightings, but has the same number of dimensions as the original one. Thus, in high dimensional spaces these techniques become very costly. Here, we propose to overcome this limitation by applying K-NN classification in the reduced space provided by locally linear dimensionality reduction techniques such as Isomap and LLE. In the reduced space, we can construct and use efficient index structures (such as [2]), thereby improving the performance of the K-NN technique. However, in order to use this approach, we need to compute an explicit mapping function of the query point from the original space to the reduced dimensionality space. 
Our contributions can be summarized as follows:  X  We analyze the LLE and Isomap visualization power through an experiment, and show that they perform well only when the data are comprised of one, well sampled, clus-ter. The mapping gets significantly worse when the data are organized in multiple clusters. We propose to overcome this limitation by modifying the mapping procedure, and keeping distances to both closest and farthest objects. We demonstrate the enhanced visualization results.  X  To tackle with the curse-of-dimensionality problem for classification we combine the Isomap procedure with locally adaptive metric techniques for nearest neighbor classifica-tion. In particular, we introduce two new techniques, Weighte-dIso and Iso+Ada. By modifying the transformation per-formed by the Isomap technique to take into considera-tion the labelling of the data, we can produce homogeneous neighborhoods in the reduced space, where better classifica-tion accuracy can be achieved.  X  Through extensive experiments using real data sets we demonstrate the efficacy of our methods, against a number of other classification techniques. The experimental findings corroborate the following conclusions: 1. WeightedIso and Iso+Ada achieve performance results 2. WeightedIso and Iso+Ada allow to considerably re-
Numerous approaches have been proposed for dimension-ality reduction. The main idea behind all of them is to keep a lossy representation of the initial dataset, which nonethe-less retains as much of the original structure as possible. 
We could distinguish two general categories: 1. Local or Shape preserving 2. Global or Topology preserving 
In the first category we could place methods that do not try to exploit the global properties of the dataset, but rather attempt to 'simplify' the representation of each object re-gardless of the rest of the dataset. If we are referring to time-series, the selection of the k-features should be such that the selected features retain most of the information ("energy") of the original signal. For example, these fea-tures could be either the first coefficients of the Fourier de-composition ([1, 9]), or the wavelet decomposition ([5]), or even some piecewise constant approximation of the sequence 
The second category of methods has mostly been used for visualization purposes, with the objective of discover-ing a parsimonious spatial representation for the dataset. The most widely used methods are Principal Component Analysis (PCA) [16], Multidimensional Scaling (MDS), and Singular Value Decomposition (SVD). MDS focuses on the preservation of the original high-dimensional distances, for a 2-dimensional representation of objects. The only assump-tion made by MDS is the existence of a monotonic relation-ship between the original and projected pairwise distances. Finally, SVD can be used for dimensionality reduction by finding the projection that restores the largest possible orig-inal variance, and ignoring those axes of projection which contribute the least to the total variance. 
Other methods that enhance the user's visualization abil-ities have been proposed in [19, 8, 4, 14]. 
Lately, another category of dimensionality reduction tech-niques has appeared, namely Isomap [15] and LLE [26]. In this paper we will refer to such category of techniques as Local Embeddings (LE). These methods attempt to preserve as well as possible the local neighborhood of each object, while preserving the global distances "through" the rest of the objects (by means of a minimum spanning tree). Figure 1: Mapping in 2-dimensions of the SCURVE dataset using SVD, LLE and ISOMAP. 
Most of the dimensionality reduction techniques fail to capture the neighborhood of data, when points lie on a man-ifold (manifolds are fundamental to human perception [18]). Local Embeddings attempt to tackle this problem. 
Isomap is a procedure that maps high-dimensional objects into a lower dimensional space (usually 2-3 for visualization 
I. Calculate the K closest neighbors of each object 2. Create the Minimum Spannin 9 Tree (MST) distances of the updated distance matriz 3. Run MDS on the new distance matr~z. 4. Depict points on some lower dimension. 
Locally Linear Embedding (LLE) also attempts to recon-
We depict the potential power of the above methods with 
Both LLE and ISOMAP present a meaningful mapping 
In addition, we observe that the quality of the mapping 
The observed "overclusterin9" effect can be mitigated if 
Therefore, for visualizing large, clustered, dynamic datasets 1. Map the current dataset using the k/2 closest objects and the k/2 farthest objects. This will separate clearly the clusters. 2. For any new points that are added in the database, As suggested by the previous experiment the new incremen-tal mapping will be adequately accurate. 
In a classification problem, we are given J classes and N training observations. The training observations consist of q feature measurements x = (xl,.  X   X  , Xq) E ~q and the known class labels, y, y = 1,...,J. The goal is to predict the class label of a given query x0. It is assumed that there ex-ists an unknown probability distribution P(x, y) from which data are drawn. To predict the class label of a given query x0, we need to estimate the class posterior probabilities 
The K nearest neighbor classification method [13, 20] is a simple and appealing approach to this problem: it finds the K nearest neighbors of xo in the training set, and then pre-dicts the class label of xo as the most frequent one occurring in the K neighbors. K nearest neighbor methods are based on the assumption of smoothness of the target functions, which translates to locally constant class posterior proba-bilities. It has been shown in [6] that the one nearest neigh-bor rule has asymptotic error rate that is at most twice the Bayes error rate, independent of the distance metric used. 
However, severe bias can be introduced in the nearest neighbor rule in a high dimensional input feature space with finite samples ([3]) . The assumption of smoothness becomes invalid for any fixed distance metric when the input obser-vation approaches class boundaries. One way to tackle this problem is to develop locally adaptive metric techniques, with the objective of producing modified local neighbor-hoods in which the posterior probabilities are approximately constant. The common idea in these techniques ([I0, Ii, 7]) is that the weight assigned to a feature, locally at a given query point q, reflects its estimated relevance to predict the class label of q: larger weights correspond to larger capabil-ities in predicting class posterior probabilities. 
A major drawback of locally adaptive metric techniques for nearest neighbor classification is the fact that they all perform the K-NN procedure multiple times in a feature space that is tranformed by means of weightings, but has the same number of dimensions as the original one. In high di-mensional spaces, then, these techniques become very costly. Here, we propose to overcome this limitation by applying the K-NN classification in the lower dimensional space provided by Isomap, where we can construct efficient index structures. 
In contrast to global dimensionality reduction techniques like SVD, the Isomap procedure has the objective of reduc-ing the dimensionaRity of the input space while preserving the local structure of the dataset as much as possible. This feature makes Isomap particularly suited for being combined with nearest neighbor techniques, that rely on the queries' local neighborhoods to address the classification problem. 
The mapping performed by Isomap, combined with the label information provided by the training data, can help us reduce the curse-of-dimensionality effect. We take into consideration the non isotropic characteristics of the input feature space at, different locations, thereby achieving more accurate estimations. Moreover, since we will perform near-est neighbor classification in the reduced space, this process will result in a boosted efficiency. 
When computing the distance between two points for clas-sification, we desire to consider the two points close to each other if they belong to the same class, and far from each other otherwise. Therefore, we arm to compute a transfor-mation that maps similar observations, in terms of class pos-terior probabilities, to nearby points in feature space, and observations that show large differences in class posterior probabilities to distant points in feature space. We derive such a transformation by modifying step 1 of the Isomap procedure to take into consideration the labelling of points. 
We proceed as follows. We first compute the K near-est neighbors of each data point x (we set K = 10 in our experiments). Let us denote with K~arn, the set of nearest neighbors having the same class label as x. We then "move" each nearest neighbor in Ksa,~ closer to x by rescaling their Euclidean distance by a constant factor (set to 1/10 in our experiments). This mapping construction is summarized in Figure 5. 
In constrast to visualization tasks, where we wish to pre-serve the intrinsic metric structure for neighbors as much as possible, here we wish to stretch or constrict such metric in order to derive homogeneous neighborhoods in the trans-formed space. Our mapping construction arms to achieve this goal. Once we have derived the map into d dimensions, we apply K-NN classification in the reduced feature space to classify a given query x0. We first need to derive the query's coordinates in d dimensions. To achieve this goal, we learn an explicit mapping f : ~q --~ ~d using the smooth inter-polation technique provided by radial basis function (RBF) networks [12, 23], applied to the known corresponding pairs obtained as output in Figure 5. Figure 4: Linear combination of three Gaussian Ba-sis Functions. 
An RBF neural network solves a curve-fitting approxima-tion problem in a high-dimensional space. It involves three different layers of nodes. The input layer is made up of source nodes. The second layer is a hidden layer of high enough dimension. The ouput layer supplies the response of the network to the activation patterns applied to the in-put layer. The transformation from the input space to the hidden-unit space is nonlinear, whereas the transformation 
The training phase constitutes the optimization of a fit-1. Mapping Contruction (Figure 5); 2. Network Contruction (Figure 6); 3. Classification (Figure 7). 
In our experiments we also explore an alternative pro- X  Input: Training data T = {(x, y)}{v  X  Execute on the training data the Isomap procedure modified as follows:  X  Output: Set of N pairs {(x, Xd)}l N, where xd corre-sponds to x mapped into d dimensions.  X  Input: Training data {(x, xd)}l N  X  Output: RBF network NET. 
Classification:  X  Input: RBF network NET, {Xd, Y}l N, query Xo  X  Output: Classification label for x0. 
We compare several classification methods using real data: 
Procedural parameters for each method were determined and Vowel. Cardinalities, dimensions, and number of classes for each data set are summarized in Table 1. 
Vowel 528 ] 10 11 ten 2fold c-v 
Tables 2 and 3 show the (cross-validated) error rates for the ten methods under consideration on the seven real data sets. The average error rates for the smaller data sets (i.e., Iris, Sonar, Glass, Liver, and Lung) were based on leave-one-out cross-validation, and the error rates for Image and Vowel were based on ten two-fold-cross-validation, as summarized in Table 1. 
In Figure 9 we plot the error rates obtained for the Weighte-dIso method for different values of reduced dimensionality d (up to 15), and for each data set. We can observe an "elbow" shaped curve for each data set, where the largest improve-ments in error rates are found when d increases from two to three and four. This means that, through our mapping transformation, we are able to achieve a good discrimination level between classes in low dimensional spaces. As a conse-quence, it becomes feasible to construct indexing structures that allow a fast nearest neighbor search in the reduced fea-ture space. In Tables 2 and 3, we report the lowest error rate obtained with the WeightedIso technique for each data set. We use the d value that gives the lowest error rate for each data set to run the Iso+Ada technique, and report the corresponding error rates in Tables 2 and 3. We apply the remaining eight techniques in the original q-dimensional feature space. 
Different methods give the best performance on different data sets. Iso+Ada gives the best performance on three data sets (Iris, Image, and Lung), and is close to the best performer in the remaining four data sets. A large gain in performance is achieved by both Iso+Ada and WeightedIso for the lung data. The data for this problem are extremely sparse in the original feature space (only 32 points with 56 dimensions). Both the WeightedIso and Iso+Ada techniques reach an error rate of 34.4% in a two-dimensional space. 
It is natural to ask the question of robustness. That is, how well a particular method m performs on average in sit-uations that are most favorable to other procedures. We capture robustness by computing the ratio bm of its error rate em and the smallest error rate over all methods being compared in a particular example: Thus, the best method m* for that example has bin* = 1, and all other methods have larger values b~ &gt;_ 1, for m m*. The larger the value of bin, the worse the performance of the ruth method is in relation to the best one for that example, among the methods being compared. The dis-tribution of the br~ values for each method m over all the examples, therefore, seems to be a good indicator concern-ing its robustness. For example, if a particular method has an error rate close to the best in every problem, its bm val-ues should he densely distributed around the value 1. Any method whose b value distribution deviates from this ideal distribution reflects its lack of robustness. 
Figure 8 plots the distribution of bm for each method over the seven simulated data sets. For each method we stack the seven bm values. We can observe that the ADAMENN technique is the most robust technique among the meth-ods applied in the original q-dimensional feature space, and Iso+Ada is capable of achieving the same performance. The b values for both methods, in fact, are always very close to 1 (the sum of the values being slightly less for Iso+Ada). Therefore Iso+Ada shows a very robust behavior, achieved in feature spaces much smaller than the original one, upon which ADAMENN has operated. The WeightedIso tech-nique also shows a robust behavior, still competitive with the adaptive techniques that operates in the original feature space. C4.5 is the worst performer. Its poor performance is likely due to estimates with large bias and variance, due to the greedy strategy it employes, and to the partitioning of the input space in disjoint regions. 
Figure 9: Error rate for the Weightedlso method as a function of the dimensionality d of the reduced feature space. data visualization and classification. We have analyzed the 
LLE and Isomap techniques, and enhanced their visualiza-tion power for data scattered among multiple clusters. Fur-thermore, we have tackled the curse-of-dimensionality prob-lem for classification by combining the Isomap procedure with locally adaptive metric techniques for nearest neigh-bor classification. Using real data sets we have shown that our methods provide the same classification power as other methods, but in a much lower dimensional space. There-fore, since the proposed methods considerably reduce the di-mensionality of the original feature space, efficient indexing data structures can be employed to perform nearest neigh-bor search. [2] N. Beckmann, H. Kriegel, and R. Schnei. The r * -tree: an [3] R. Bellman. Adaptive Control Processes. Princeton Univ. [10] J. Friedman. Flexible Metric Nearest Neighbor [11] T. Hastie and R. Tibshirani. Discriminant Adaptive [12] S. Haykin. Neural Networks: A Comprehensive Foundation. [13] T. Ho. Nearest Neighbors in Random Subspaces. Lecture [14] A. Inselberg and B. Dimsdale. Parallel coordinates: A tool [15] J. C. L. J. B. Tenenbaum, V. de Silva. A global geometric [16] I. T. Jolliffe. Principal Component Analysis. [17] E. Keogh, K. Chakrabarti, S. Mehrotra, and M. Pazzani. [18] H. S. S.. D. D. Lee. The manifold ways of perception. [19] R. C. T. Lee, J. R. Slagle, and H. Blum. A triangulation [20] D. Lowe. Similarity Metric Learning for a Variable-Kernel [21] G. McLachlan. Discriminant Analysis and Statistical [22] C. Merz and P. Murphy. UCI Repository of Machine [23] T. Poggio and F. Girosi. Networks for approximation and [24] M. Polito and P. Perona. Grouping and dimensionality [25] J. Quinlan. C4.5: Programs for Machine Learning. [26] S. R.. L. Saul. Nonlinear dimensionality reduction by 
