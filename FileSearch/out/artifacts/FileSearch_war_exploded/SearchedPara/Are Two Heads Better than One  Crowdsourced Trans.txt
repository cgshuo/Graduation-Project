 Statistical machine translation (SMT) systems are trained using bilingual sentence-aligned parallel corpora. Theoretically, SMT can be applied to any language pair, but in practice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, English-Chinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or  X  X ow resource X  languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collect-ing parallel corpora for minor languages has be-come an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel frag-ments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, lit-tle consideration has been given to creating par-allel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commit-ment.

Recently, crowdsourcing has opened the possi-bility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site into different languages us-ing volunteers (TechCrunch, 2008). DuoLingo turns translation into an educational game, and translates web content using its language learners (von Ahn, 2013).

Rather than relying on volunteers or gamifica-tion, NLP research into crowdsourcing transla-tion has focused on hiring workers on the Ama-zon Mechanical Turk (MTurk) platform (Callison-Burch, 2009). This setup presents unique chal-lenges, since it typically involves non-professional translators whose language skills are varied, and since it sometimes involves participants who try to cheat to get the small financial reward (Zaidan and Callison-Burch, 2011). A natural approach for trying to shore up the skills of weak bilinguals is to pair them with a native speaker of the tar-get language to edit their translations. We review relevant research from NLP and human-computer interaction (HCI) on collaborative translation pro-cesses in Section 2. To sort good translations from bad, researchers often solicit multiple, redundant translations and then build models to try to predict which translations are the best, or which transla-tors tend to produce the highest quality transla-tions.

The contributions of this paper are:  X  An analysis of the difficulties posed by a two- X  A new graph-based algorithm for selecting In the HCI community, several researchers have proposed protocols for collaborative translation efforts (Morita and Ishida, 2009b; Morita and Ishida, 2009a; Hu, 2009; Hu et al., 2010). These have focused on an iterative collaboration between monolingual speakers of the two languages, facil-itated with a machine translation system. These studies are similar to ours in that they rely on na-tive speakers X  understanding of the target language to correct the disfluencies in poor translations. In our setup the poor translations are produced by bilingual individuals who are weak in the target language, and in their experiments the translations
Another significant difference is that the HCI studies assume cooperative participants. For in-stance, Hu et al. (2010) recruited volunteers from the International Children X  X  Digital Library (Hour-cade et al., 2003) who were all well intentioned and participated out a sense of altruism and to build a good reputation among the other volunteer translators at childrenslibrary.org . Our setup uses anonymous crowd workers hired on Mechanical Turk, whose motivation to participate is financial. Bernstein et al. (2010) characterized the problems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnec-essary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify pro-cess. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc-tions to problematic regions that had been identi-fied by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good.

Most NLP research into crowdsourcing has fo-cused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert an-notations are similar to expert annotations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibra-tion data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applica-tions (Callison-Burch and Dredze, 2010).

Although hiring professional translators to cre-ate bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 mil-lion words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of ac-tive learning to further reduce the cost of transla-tion (Ambati et al., 2010; Ambati, 2012; Blood-good and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps neces-sary to build a persistent multilingual workforce on MTurk.

This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could ap-proach the level of professional translators. They solicited multiple redundant translations from dif-Urdu translator: English post-editor: LDC professional: ferent Turkers for a collection of Urdu sentences that had been previously professionally translated by the Linguistics Data Consortium. They built a model to try to predict on a sentence-by-sentence and Turker-by-Turker which was the best transla-tion or translator. They also hired US-based Turk-ers to edit the translations, since the translators were largely based in Pakistan and exhibited er-rors that are characteristic of speakers of English as a language. Zaidan and Callison-Burch (2011) observed only modest improvements when incor-porating these edited translation into their model. We attempt to analyze why this is, and we pro-posed a new model to try to better leverage their data. Setup We conduct our experiments using the data collected by Zaidan and Callison-Burch (2011). This data set consists 1,792 Urdu sen-tences from a variety of news and online sources, each paired with English translations provided by non-professional translators on Mechanical Turk.
Each Urdu sentence was translated redundantly by 3 distinct translators, and each translation was edited by 3 separate (native English-speaking) ed-itors to correct for grammatical and stylistic er-rors. In total, this gives us 12 non-professional English candidate sentences (3 unedited, 9 edited) per original Urdu sentence. 52 different Turkers took part in the translation task, each translating 138 sentences on average. In the editing task, 320 Turkers participated, averaging 56 sentences each. For comparison, the data also includes 4 differ-ent reference translations for each source sentence, produced by professional translators.

Table 1 gives an example of an unedited trans-lation, an edited translation, and a professional translation for the same sentence. The transla-tions provided by translators on MTurk are gen-erally done conscientiously, preserving the mean-ing of the source sentence, but typically con-tain simple mistakes like misspellings, typos, and awkward word choice. English-speaking editors, despite having no knowledge of the source lan-guage, are able to fix these errors. In this work, we show that the collaboration design of two heads X  non-professional Urdu translators and non-professional English editors X  yields better trans-lated output than would either one working in iso-lation, and can better approximate the quality of professional translators.
 Analysis We know from inspection that trans-lations seem to improve with editing (Table 1). Given the data from MTurk, we explore whether this is the case in general: Do all translations im-prove with editing? To what extent does the in-dividual translator and the individual editor effect the quality of the final sentence? Figure 1: Relationship between editor aggressive-ness and effectiveness. Each point represents an editor/translation pair. Aggressiveness (x-axis) is measured as the TER between the pre-edit and post-edit version of the translation, and effective-ness (y-axis) is measured as the average amount by which the editing reduces the translation X  X  TER gold . While many editors make only a few changes, those who make many changes can bring the translation substantially closer to professional quality.

We use translation edit rate (TER) as a mea-sure of translation similarity. TER represents the amount of change necessary to transform one sen-tence into another, so a low TER means the two Figure 2: Effect of editing on translations of vary-ing quality. Rows reflect bins of editors, with the worse editors (those whose changes result in in-creased TER gold ) on the top and the most effective editors (those whose changes result in the largest reduction in TER gold ) on the bottom. Bars re-flect bins of translations, with the highest TER gold translations on the left, and the lowest on the right. We can see from the consistently negative  X  TER gold in the bottom row that good editors are able to improve both good and bad translations. sentences are very similar. To capture the quality ( X  X rofessionalness X ) of a translation, we take the average TER of the translation against each of our gold translations. That is, we define TER gold of translation t as where a lower TER gold is indicative of a higher quality (more professional-sounding) translation.
We first look at editors along two dimensions: their aggressiveness and their effectiveness. Some editors may be very aggressive (they make many changes to the original translation) but still be in-effective (they fail to bring the quality of the trans-lation closer to that of a professional). We measure aggressiveness by looking at the TER between the pre-and post-edited versions of each editor X  X  translations; higher TER implies more aggressive editing. To measure effectiveness, we look at the change in TER gold that results from the editing; negative  X  TER gold means the editor effectively improved the quality of the translation, while pos-itive  X  TER gold means the editing actually brought the translation further from our gold standard.
Figure 1 shows the relationship between these two qualities for individual editor/translation pairs. We see that while most translations re-quire only a few edits, there are a large number of translations which improve substantially after heavy editing. This trend conforms to our intu-ition that editing is most useful when the transla-tion has much room for improvement, and opens the question of whether good editors can offer im-provements to translations of all qualities.
To address this question, we split our transla-tions into 5 bins, based on their TER gold . We also split our editors into 5 bins, based on their effec-tiveness (i.e. the average amount by which their editing reduces TER gold ). Figure 2 shows the de-gree to which editors at each level are able to im-prove the translations from each bin. We see that good editors are able to make improvements to translations of all qualities, but that good editing has the greatest impact on lower quality transla-tions. This result suggests that finding good ed-itor/translator pairs, rather than good editors and good translators in isolation, should produce the best translations overall. Figure 3 gives an exam-ple of how an initially medium-quality translation, when combined with good editing, produces a bet-ter result than the higher-quality translation paired with mediocre editing. The problem definition of the crowdsourcing translation task is straightforward: given a set of candidate translations for a source sentence, we want to choose the best output translation.
This output translation is the result of the com-bined translation and editing stages. Therefore, our method operates over a heterogeneous net-work that includes translators and post-editors as well as the translated sentences that they pro-duce. We frame the problem as follows. We form two graphs: the first graph ( G T ) represents Turk-ers (translator/editor pairs) as nodes; the second graph ( G C ) represents candidate translated and right was produced by a different editor. Order reflects the TER TER gold on the top. Some translators receive low TER gold scores due to superficial errors, which can be the best translation after being revised by a good editor. post-edited sentences (henceforth  X  X andidates X ) as nodes. These two graphs, G T and G C are com-bined as subgraphs of a third graph ( G T C ). Edges in G T C connect author pairs (nodes in G T ) to the candidate that they produced (nodes in G C ). To-gether, G T , G C , and G T C define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define for-mally as follows.

Let G denote the heterogeneous graph with nodes V and edges E . Let G = ( V , E ) = ( V
T ,V C ,E T ,E C ,E T C ) . G is divided into three subgraphs, G T , G C , and G T C . G C = ( V C ,E C ) is a weighted undirected graph representing the can-didates and their lexical relationships to one an-other. Let V C denote a collection of translated and edited candidates, and E C the lexical simi-larity between the candidates (see Section 4.3 for details). G T = ( V T ,E T ) is a weighted undirected graph representing collaborations between Turk-ers. V T is the set of translator/editor pairs. Edges E
T connect translator/editor pairs in V T which share a translator and/or editor. Each collabora-tion (i.e. each node in V T ) produces a candidate (i.e. a node in V C ). G T C = ( V T C ,E T C ) is an unweighted bipartite graph that ties G T and G C together and represents  X  X uthorship X . The graph G consists of nodes V T C = V T  X  V C and edges E
T C connecting each candidate with its authoring translator/post-editor pair. The three sub-networks ( G T , G C , and G T C ) are illustrated in Figure 4. 4.1 Inter-Graph Ranking The framework includes three random walks, one on G T , one on G C and one on G T C . A random walk on a graph is a Markov chain, its states be-ing the vertices of the graph. It can be described by a stochastic square matrix, where the dimen-sion is the number of vertices in the graph, and the entries describe the transition probabilities from one vertex to the next. The mutual reinforcement framework couples the two random walks on G T and G C that rank candidates and Turkers in iso-lation. The ranking method allows us to obtain a global ranking by taking into account the intra-/inter-component dependencies. In the following sections, we describe how we obtain the rankings on G T and G C , and then move on to discuss how the two are coupled.

Our algorithm aims to capture the following in-tuitions. A candidate is important if 1) it is similar to many of the other proposed candidates and 2) it is authored by better qualified translators and/or post-editors. Analogously, a translator/editor pair is believed to be better qualified if 1) the editor is collaborating with a good translator and vice versa and 2) the pair has authored important candi-dates. This ranking schema is actually a reinforced process across the heterogeneous graphs. We use two vectors c = [  X  ( c )] | c | X  1 and t = [  X  ( t )] | t | X  1 denote the saliency scores  X  ( . ) of candidates and Turker pairs. The above-mentioned intuitions can be formulated as follows:  X  Homogeneity. We use adjacency matrix sentence to translate. [ M ] | c | X | c | to describe the homogeneous affinity between candidates and [ N ] | t | X | t | to describe the affinity between Turkers. where c = | V C | is the number of vertices in the candidate graph and t = | V T | is the number of ver-tices in the Turker graph. The adjacency matrix [ M ] denotes the transition probabilities between candidates, and analogously matrix [ N ] denotes the affinity between Turker collaboration pairs.  X  Heterogeneity. We use an adjacency matrix [  X  between the output candidate and the producer Turker pair from both of the candidate-to-Turker and Turker-to-candidate perspectives.
All affinity matrices will be defined in the next section. By fusing the above equations, we can have the following iterative calculation in matrix forms. For numerical computation of the saliency scores, the initial scores of all sentences and Turk-ers are set to 1 and the following two steps are alternated until convergence to select the best can-didate.

Step 1 : compute the saliency scores of candi-dates, and then normalize using ` -1 norm.
Step 2 : compute the saliency scores of Turker pairs, and then normalize using ` -1 norm. where  X  specifies the relative contributions to the saliency score trade-off between the homogeneous affinity and the heterogeneous affinity. In order to guarantee the convergence of the iterative form, we must force the transition matrix to be stochastic and irreducible. To this end, we must make the c and t column stochastic (Langville and Meyer, 2004). c and t are therefore normalized after each iteration of Equation (4) and (5). 4.2 Intra-Graph Ranking The standard PageRank algorithm starts from an arbitrary node and randomly selects to either fol-low a random out-going edge (considering the weighted transition matrix) or to jump to a random node (treating all nodes with equal probability).
In a simple random walk, it is assumed that all nodes in the transitional matrix are equi-probable before the walk starts. Then c and t are calculated as: and where 1 is a vector with all elements equaling to 1 and the size is correspondent to the size of V C or V
T .  X  is the damping factor usually set to 0.85, as in the PageRank algorithm. 4.3 Affinity Matrix Establishment We introduce the affinity matrix calculation, in-cluding homogeneous affinity (i.e., M,N ) and heterogeneous affinity (i.e.,  X  W,  X  W ).

As discussed, we model the collection of can-didates as a weighted undirected graph, G C , in which nodes in the graph represent candidate sen-tences and edges represent lexical relatedness. We define an edge X  X  weight to be the cosine similarity between the candidates represented by the nodes that it connects. The adjacency matrix M describes such a graph, with each entry corresponding to the weight of an edge.
 where F ( . ) is the cosine similarity and c is a term vector corresponding to a candidate. We treat a candidate as a short document and weight each term with tf.idf (Manning et al., 2008), where tf is the term frequency and idf is the inverse docu-ment frequency.

The Turker graph, G T , is an undirected graph whose edges represent  X  X ollaboration. X  Formally, let t i and t j be two translator/editor pairs; we say that pair t i  X  X ollaborates with X  pair t j (and there-fore, there is an edge between t i and t j ) if t i and t j share either a translator or an editor (or share both a translator and an editor). Let the function I ( t i ,t j ) denote the number of  X  X ollaborations X  ( # col ) between t i and t j .
 Then the adjacency matrix N is then defined as
In the bipartite candidate-Turker graph G T C , the entry E T C ( i,j ) is an indicator function denot-ing whether the candidate c i is generated by t j :
Through E T C we define the weight matrices  X  W ities of transitions from c i to t j and vice versa: We are interested in testing our random walk method, which incorporates information from both the candidate translations and from the Turk-ers. We want to test two versions of our pro-posed collaborative co-ranking method: 1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collab-orations.
 Metric Since we have four professional transla-tion sets, we can calculate the Bilingual Evalu-ation Understudy (BLEU) score (Papineni et al., 2002) for one professional translator (P1) using the other three (P2,3,4) as a reference set. We repeat the process four times, scoring each pro-fessional translator against the others, to calculate the expected range of professional quality transla-tion. In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations. Therefore, each number reported in our experi-mental results is an average of four numbers, cor-responding to the four possible ways of choosing 3 of the 4 reference sets. This allows us to compare the BLEU score achieved by our methods against the BLEU scores achievable by professional trans-lators.
 Baselines As a naive baseline, we choose one candidate translation at random for each input Urdu sentence. To establish an upper bound for our methods, and to determine if there exist high-quality Turker translations at all, we compute four Table 2: Overall BLEU performance for all methods (with and without post-editing). The highlighted result indicates the best performance, which is based on both candidate sentences and Turker information. oracle scores. The first oracle operates at the seg-ment level on the sentences produced by transla-tors only: for each source segment, we choose from the translations the one that scores highest (in terms of BLEU) against the reference sen-tences. The second oracle is applied similarly, but chooses from the candidates produced by the collaboration of translator/post-editor pairs. The third oracle operates at the worker level: for each source segment, we choose from the translations the one provided by the worker whose transla-tions (over all sentences) score the highest on average. The fourth oracle also operates at the worker level, but selects from sentences produced by translator/post-editor collaborations. These or-acle methods represent ideal solutions under our scenario. We also examine two voting-inspired methods. The first method selects the translation with the minimum average TER (Snover et al., 2006) against the other translations; intuitively, this would represent the  X  X onsensus X  translation. The second method selects the translation gen-erated by the Turker who, on average, provides translations with the minimum average TER.
 Results A summary of our results in given in Ta-ble 2. As expected, random selection yields bad performance, with a BLEU score of 30.52. The oracles indicate that there is usually an acceptable translation from the Turkers for any given sen-tence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and rather reflect the true po-tential of the collected translations. On average, the reference translations give a score of 42.38. To put this in perspective, the output of a state-of-the-Figure 5: Effect of candidate-Turker coupling (  X  ) on BLEU score. art machine translation system (the syntax-based variant of Joshua) achieves a score of 26.91, which is reported in (Zaidan and Callison-Burch, 2011). The approach which selects the translations with the minimum average TER (Snover et al., 2006) against the other three translations (the  X  X onsen-sus X  translation) achieves BLEU scores of 35.78.
Using the raw translations without post-editing, our graph-based ranking method achieves a BLEU score of 38.89, compared to Zaidan and Callison-Burch (2011) X  s reported score of 28.13, which they achieved using a linear feature-based classi-fication. Their linear classifier achieved a reported both translators and editors. In contrast, our pro-posed graph-based ranking framework achieves a score of 41.43 when using the same information. This boost in BLEU score confirms our intuition that the hidden collaboration networks between candidate translations and transltor/editor pairs are indeed useful.
 Parameter Tuning There are two parameters in our experimental setups:  X  controls the probability of starting a new random walk and  X  controls the coupling between the candidate and Turker sub-graphs. We set the damping factor  X  to 0.85, fol-lowing the standard PageRank paradigm. In order to determine a value for  X  , we used the average BLEU, computed against the professional refer-
Table 3: Variations of all component settings. ence translations, as a tuning metric. We experi-mented with values of  X  ranging from 0 to 1, with a step size of 0.05 (Figure 5). Small  X  values place little emphasis on the candidate/Turker coupling, whereas larger values rely more heavily on the co-ranking. Overall, we observed better performance with values within the range of 0.05-0.15. This suggests that both sources of information X  the can-didate itself and its authors X  are important for the crowdsourcing translation task. In all of our re-ported results, we used the  X  = 0.1.
 Analysis We examine the relative contribution of each component of our approach on the overall performance. We first examine the centroid-based ranking on the candidate sub-graph ( G C ) alone to see the effect of voting among translated sen-tences; we denote this strategy as plain ranking . Then we incorporate the standard random walk on the Turker graph ( G T ) to include the structural in-formation but without yet including any collabo-ration information; that is, we incorporate infor-mation from G T and G C without including edges linking the two together. The co-ranking paradigm is exactly the same as the framework described in Section 3.2, but with simplified structures.
Finally, we examine the two-step collaboration based candidate-Turker graph using several varia-tions on edge establishment. As before, the nodes are the translator/post-editor working pairs. We investigate three settings in which 1) edges con-nect two nodes when they share only a transla-tor, 2) edges connect two nodes when they share only a post-editor, and 3) edges connect two nodes when they share either a translator or a post-editor. These results are summarized in Table 3.

Interestingly, we observe that when modeling the linkage between the collaboration pairs, con-necting Turker pairs which share either a transla-tor or the post-editor achieves better performance than connecting pairs that share only translators or connecting pairs which share only editors. This result supports the intuition that a denser collabo-ration matrix will help propagate saliency to good translators/post-editors and hence provides better predictions for candidate quality. We have proposed an algorithm for using a two-step collaboration between non-professional trans-lators and post-editors to obtain professional-quality translations. Our method, based on a co-ranking model, selects the best crowdsourced translation from a set of candidates, and is capable of selecting translations which near professional quality.

Crowdsourcing can play a pivotal role in fu-ture efforts to create parallel translation datasets. In addition to its benefits of cost and scalabil-ity, crowdsourcing provides access to languages that currently fall outside the scope of statistical machine translation research. In future work on crowdsourced translation, further benefits in qual-ity improvement and cost reduction could stem from 1) building ground truth data sets based on high-quality Turkers X  translations and 2) identify-ing when sufficient data has been collected for a given input, to avoid soliciting unnecessary redun-dant translations.
 This material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled  X  X rowdsourcing Translation X  (con-tract D12PC00368). The views and conclusions contained in this publication are those of the au-thors and should not be interpreted as represent-ing official policies or endorsements by DARPA or the U.S. Government. This research was sup-ported by the Johns Hopkins University Human Language Technology Center of Excellence and through gifts from Microsoft, Google and Face-book.

