 Modern applications of text mining often deal with large collections of documents that cover diverse sets of topics. Various topic modeling techniques have been developed in recent decades to discover these topics automatically and present visualizations that capture the spectrum of themes in a corpus. In a real-world setting, however, analysts are often interested in grasping the nature of the discourse around a particular concept or entity rather than understanding the corpus as a whole.
 Such a task may be difficult to perform when dealing with social media texts. Social microblog systems are populated with millions of noisy, content-poor documents discuss large variety of subjects and concepts. The short, unedited nature of social media texts complicates applications of common topic model-ing approaches [ 1 ][ 2 ][ 3 ][ 4 ][ 5 ][ 6 ] and makes extraction of interesting patterns especially difficult.
 In this paper, we propose the new Semi-Supervised Microblog-hLDA (SS-Micro-hLDA) model that learns topics (defined as probability distributions over words) from microblog data in a way that allows for sets of interesting keywords (referred to as supervisory word sets from here on) to influence the topic learning process. To make the job of interpreting the learned topics easier, we require our approach to organize topics as hierarchies. This is motivated by well-known works in cognitive research that suggest that hierarchies may be instrumental in enhancing human sense making [ 7 , 8 ].
 lic by the TREC project and show that our model produces more interpretable and coherent topic models when measured in terms of PMI-Score against TSSB, Recursive-CRP, Constrained-hLDA and hLDA. Further, we test our approach and related approaches using information entropy and show that our model learns topic hierarchies that are more subject-focused than those produced by TSSB, Recursive-CRP, Constrained-hLDA and hLDA.
 research in the area of topic modeling in general and microblog topic modeling in particular. Section 3 offers an analysis of topic modeling challenges in social stream data and describes the new Semi-Supervised Microblog-hLDA model designed to overcome these challenges. In Section 4 , we discuss data sets and experiments that were used to evaluate how well our new topic modeling app-roach performed as compared with other approaches. Section 5 concludes the paper and outlines future work. Discovering hidden relationships between words may be accomplished using a number of different techniques. Matrix factorization approaches such as Latent Semantic Indexing (LSI) [ 9 ] and Non-Negative Matrix Factorization (NMF) [ 10 ] have been used to infer latent relationships between terms. While matrix fac-torization may be employed for topic discovery, approaches based on Latent Dirichlet Allocation (LDA) [ 11 ] have become very popular in recent years. This popularity has often been attributed to the flexibility and modularity of LDA, which easily lends itself to extensions and generalizations that accommodate many types of relationships in data [ 12 ].
 assumption and represents documents as probability distributions over These topics are, in turn, viewed as probability distributions over sions and generalizations, one of its major limitations is that users must select the number of topics K before the approach can be used. This requirement makes the approach quite rigid, as it cannot accommodate influx of new data [ 13 ]. To make topic modeling more flexible, LDA machinery was modified in [ 13 ]touse the Chinese Restaurant Process (CRP) [ 14 ]. CRP relaxes the fixed of LDA by assuming an infinite number of topics and postulating that words are generated from topics chosen according to the following distribution: where m i is the number of words assigned to topic i ,  X  is a parameter and total number of words seen so far. The formulation in Equation 1 removes the need to know K apriori as it assigns a non-zero probability to choosing a new topic. This allows the number of discovered topics to grow as new data arrives. To improve interpretability of discovered topics, the work by Blei et al. on Hierar-chical LDA (hLDA)[ 13 ] attempted to learn organized topic hierarchies. The hLDA generative probabilistic model assumes that words in a document are generated from an infinitely branched tree of height L according to a document-specific mix-ture model. In hLDA, each node of the tree is associated with a single topic. To learn topic and tree structure from data, sampling is often used by first choosing an L -level path c d for each document d according to Equation 2 . where w  X  d and c  X  d are words and paths of documents other than respectively words and paths of all documents; z is the topic assignments. Once the path is found, topic assignments for words are approximated by sampling [ 1 ]. Including partial supervision in topic modeling has been an active area of research in recent years. Approaches such as the one proposed in [ 15 ]and extended in [ 16 ] work by defining sets of possible topic assignments for each word type (referred to as topic-in-sets ) and modifying the Gibbs sampler to constrain possible topic choices for words according to these topic-in-sets . These approaches are flexible in that they allow certain  X  X eed words X  to help focus topic discovery for words and documents from underrepresented or noisy top-ics. Unfortunately, specifying topic-in-sets early in the topic discovery process is only meaningful if the number of topics is known ahead of time. This limits the usefulness of these approaches as they cannot be applied in a non-parametric settings, such as the nested CRP or the Hierarchical LDA.
 Semi-Supervised hLDA (SSHLDA) proposed in [ 17 ] introduces partial super-vision into the Hierarchical LDA learning process by restricting the initial struc-ture of the topic tree to known hierarchies of labels and then allowing the nested CRP process to discover new branches in the tree with stochastic sampling, as in hLDA.
 Constrained-hLDA is particularly relevant to this work because, as in this paper, it focuses on hierarchical topic modeling in microblogs. Specifically, Constrained-hLDA experimented with Chinese microblogs and showed significant improvement in terms of held-out log-likelihood. As noted by the authors, much of the improve-ments were realized by an additional heuristic aimed specifically at microblog data, which restricted word-level assignments during sampling. Their relied on the doc-ument frequency function, which returned the number of documents containing a word in a corpus, as well as upper and lower inclusion boundary thresholds and part-of-speech indicators.
 The novel model discussed in the next section improves upon topic-in-set -based approaches, such as the one proposed in [ 15 ], by allowing for partial super-vision and guidance to be applied to hierarchical topic learning in a way that is non-parametric with respect to the number of topics. The new model further improves on recently proposed hierarchical semi-supervised approaches in that it incorporates supervision in a way that does not require an existing label hier-archy (as in SSHLDA) nor does it necessitate the initial supervisory hierarchy to be learned by other means (such as FP-Tree in Constrained-hLDA). We motivate our model by imagining that topics are not atomic constructs, but are rather comprised of levels of topic specificity. That is, we consider that microblog posts pertain to a single conceptual theme (such as the presidential election or the World Cup), and that each theme contains a number of stages or levels of specificity. For example, when discussing the World Cup, one microblog message may express excitement about the fact of the World Cup X  X  existence, while another post may speak about an outcome of a particular match or the role of a specific player. In both cases, messages may be set to belong to a  X  X orld Cup X  theme, but the former message is surely more general than the latter one. hLDA to sample levels according to a uniform distribution, rather than a multi-nomial one. 3.1 Generative Process for Semi-Supervision We, then, take our approach a step further and attempt to discover a way to allow semi-supervised focus to be introduced into topic modeling. That is, we imagine a user interested in a particular subject area supplies a topic modeling algorithm with few keywords or phrases about the subject. The user, then, expects the algorithm to highlight her keywords and phrases by restricting them to a single position in the resulting topic tree. Further, the user may expect the approach to discover topics around the given subject area (siblings, parents, etc.) providing the user with further insights into her area of interest.
 lined in Figure 1 . It assumes that for each social media collection, there exists a parallel corpus of short phrases, which has a bearing on how microblog posts are generation. We treat the parallel corpus as a collection of word phrases and refer to these phrases as supervisory word-sets . We, then, imagine that these supervi-sory word sets are themselves generated with a random generative process. collection of supervisory word-sets, such that w s  X  W sup | W sup | is the number of supervisory word-sets. The process starts by generating S supervisory word-sets in step 3 .Instep 3(c)c , supervisory words are aggregated into the set W sup , which is used in later steps to ensure that supervisory words may only be generated on paths associated with supervisory word sets. In step 3d , leaf nodes of paths chosen for each of the supervision word sets are aggregated into a set L sup . The resulting collection of S paths is used in later steps to ensure that words in supervisory sets may only emerge from paths associated with those supervisory sets. Having generated the supervisory sets, the process begins to produce docu-ment content in step 4 . First, the process draws a number multinomial distribution parameterized by an L -sized vector an index s into the set L sup is drawn from a distribution parameterized by an | L | -dimensional vector w . Then, the process chooses a path for each document by deterministically selecting the first x nodes from the 4e ) and then allowing the CRP to randomly choose nodes from ( to the leaf level L (step 4f ). It is important to note that the realization step 4a amounts to no supervision, since all paths share the root node. The resulting path is used to generate words in the document. For each word, the process draws z  X  X  1 ,...,L } from a uniform distribution. Once the level assign-ment is known, set V is constructed in step 4(g)c and initially contains all words in vocabulary V except for all supervisory terms of set W assignment is on the path associated with some supervisory word-set, the supervi-sory words of that set are added to V . Then, the | V | -dimensional word proportions vector associated with the chosen topic multiplies a diagonal matrix, which con-tains zeros in elements of the diagonal that correspond to indices of words not found in vector V . The multiplication in step 4(g)f has the effect of allowing supervisory words to be generated only from a single hierarchy path. The resulting unnormal-ized parameter vector is used to randomly select words in a way identical to hLDA. 3.2 Inference Posterior inference tries to visualize hidden process structures by repeatedly adjusting its mental vision of them to better fit the actual observations. In our application, observations consist of two collections  X  1) a corpus of microblog messages and, 2) a number of user-specified supervisory word sets. Given these observations, we are interested in learning the shape of the hidden topic hierarchy and the word proportions for the nodes in the hierarchy.
 First, a random tree scaffolding is constructed by many hierarchical random walks. Then, for each microblog message, an L -level path through the scaffolding is selected. This is followed by a path node selection and subsequent counters updated for both path and the node. These counters are used in later stages to approximate hierarchy makeup and parameters.
 sen, the algorithm knows that all words in the message were generated by the particular path, but still needs to determine which member of the path was responsible for which words. This is a challenge as our model postulates a uni-form distribution over levels for each document, which means that the posterior inference algorithm cannot learn level assignments from data, as in hLDA. To have a reasonable chance of intelligently approximating the hidden structure, the inference procedure may consider the following argument.
 ment draws equally many words from each of the levels, but gives no guidance as to how to determine which level of the hierarchy generated which of the words. If the hidden word distributions at each level were known, the inference algo-rithm could simply choose a node with the highest probability of a given word. However, since these distributions are unknown, the inference procedure may consider the following dichotomy regarding word proportions in nodes of the hidden tree  X  1) all distributions are of the same (or similar) shape and, 2) all distributions are not of the same (or similar) shape.
 with everyday common sense  X  obviously, language texts, such as social media posts, discuss a variety of subjects. Therefore, we must conclude that words are distributed unequally among paths and, consequently, path nodes. With that, words that are favored by  X  X opular X  nodes (nodes that appear on many paths) must appear more frequently than words from unpopular nodes. Again arguing from the observations, because empirical laws (e.g.: the Zipf X  X  Law [ 18 ]) suggest that words are distributed according to the inverse power-law, there must be few  X  X opular X  nodes and many  X  X npopular X  ones. Then, in graph-theoretic terms, since, by definition, there are fewer higher-level nodes than lower-level ones, the higher-level nodes (those closer to the root) must be the  X  X opular X  ones and the lower-level (towards leafs) nodes must be relatively  X  X npopular X .
 With that, the level assignment task is straight forward. Given a word, the algorithm may simply consult a frequency table and determine its corpus-level rank. Then, if the word ranks first, the word must be associated with the root node, whereas if its ranked last, it gets assigned to the leaf node of the given document path.
 Naturally, the above raises the question of what to do if the rank is some-where between first and last. We tackle this challenge by partitioning the corpus frequency table into L buckets (one for each hierarchy level) in such a way as to place few highly ranked words into the top-level bucket and many very infre-quent terms into leave level one. This intuition is quantified by assigning words to levels during sampling according to the following equation: where N is the number of distinct words and rank ( w ) is the rank of word the corpus frequency table. The equation captures our intuition by exponentially increasing bucket sizes towards the leaf level.
 For an illustrative example, when considering a 3-level hierarchy ( and a 1000 term vocabulary ( | V | = 1000), Equation 4 will associate the 10 most frequent terms with the root level, next 90 with the intermediate level, and 900 least frequent ones with the leaf nodes. Then, for the same hierarchy, if the word  X  X he X  were the most frequent word in a corpus containing 1000 unique terms, its rank would necessarily be 1 and Level  X  the  X  = log 3 the root level. If, however, the word  X  X nique X  were the only word to appear just once in the corpus, it would be ranked 1000 and its level would be computed as Level During inference, we approximate each word X  X  position with the value of Level w for each observed word w by deterministically selecting level assignments with the help of Equation 4 . 3.3 Inference with Semi-Supervision We outline the supervised inference procedure by recalling that, in addition to a document corpus, observations in the SS-Micro-hLDA also contain collections of supervisory words. Since SS-Micro-hLDA uses the same generative approach for both the supervisory and the document corpora, same sampling procedure may apply. The restriction that supervisory words may originate from only a single path (step 4(g)f in Figure 1 ) implies that documents containing supervisory words must have been generated from paths that share a prefix with paths to leafs associated with supervisory word sets.
 and without replacement selecting a node from a set of hierarchy leafs for each supervisory set w s  X  W sup . This results in a collection of tuples S w Then, for each i th tuple S i  X  S sup , words in its word set w on the path to c i according to word ranks as specified by Equation 4 . checking whether any words in the document are found in any supervisory set and constraining the path selection to go through the corresponding node. Once, the path is known, word assignments are sampled according to Equation 4 . The proposed model was tested with two datasets  X  the Tweets2011 Twitter Collection made available through the TREC project [ 19 ] and a collection of user comments on a popular Reddit news and social networking site, which we manually collected by monitoring the site X  X  programmatic API end-points. The Twitter data set consisted of 16 million Twitter messages sampled in early months of 2011. The Reddit collection was comprised of 51 to articles posted in Reddit subsections (known as subreddits ) labeled /gaming , /politics and /sports .
 special topical annotations known as hashtags . While users often misplace or misspell hashtags or abuse the hashtag notation (i.e.: some messages may contain more hashtags than actual text), with no standard corpus available, our approach was tested on collections of carefully select tagged messages.
 the Tweets2011 collection and assembled corpus-level hashtag counts. We then selected those hashtags that appeared in at least 1000 messages in the cor-pus. The resulting 34 hashtags were used to construct the corpus by retaining only those messages that contained the frequent tags. The data set was further restricted to those Twitter messages that contained only a single hashtag. This was done to control noise with the intuition that messages with just a single hashtag are more likely to be focused on a particular subject. 4.1 PMI-Score Evaluation To compare performance of our approach to others, we expressed our interest in the Egyptian revolution and the major American Football sporting event by con-structing two supervisory sets  X  {  X  X rotests X ,  X  X gypt X  } and  X  X teelers X  } . We then trained topic models using semi-supervised and unsupervised variants of our approach 1 as well as the Constrained-hLDA and hLDA (to serve as a baseline) and compared resulting models in terms of the PMI-Score [ 20 ]. The PMI-Score measure was chosen in favor of other metrics, such as such as perplex-ity or log-likelihood, as this measure has been reported by numerous researchers ([ 21 ],[ 20 ],[ 22 ]) to correlate well with human interpretation of topic models. PMI-Score is motivate by the observation that human evaluation of topic models is often conducted by considering the top n representative words for each topic. The PMI-Score aims to provide quantitative approximation of human evaluation by considering the Pointwise Mutual Information for the top as quantified by Equation 5 .
 where w is the topic, w i and w j are i th and j th ranked words in topic w , number of  X  X op words X  selected (for example, n =10 top words), Evaluation results using ten-fold cross-validation are outlined in Figure 2 . The figure reports PMI-Scores for hierarchies of different heights and shows that SS-Micro-hLDA outperforms other approaches for deeper hierarchies. All models appeared to perform similarly in terms of the PMI-Score for shallower hierarchies (number of levels less than 5). This is expected as shallow hierarchies do not allow for deep specialization in topic structures. 4.2 Information Entropy Evaluation While the PMI-Score evaluation presented above tested topic models in terms of their interpretability, the metric did not measure how well sections of hierarchies focused on particular topical areas. That is, in hierarchical topic learning, it is expected that siblings are somehow conceptually related to one another. For example, topics on  X  X ogs X  and  X  X ats X  may be expected to appear under the general topic on  X  X ammals X , while  X  X pples X  and  X  X ranges X  should occur under the general topic heading on  X  X ruits X . If a hierarchical topic modeling approach were to place the  X  X ogs X  topic under the  X  X ruits X  heading, a human analyst would likely find such a placement in error even if the top words of the topic were coherent and interpretable.
 abilities of each node as proportional to the number of times a node appeared on any document X  X  path. We then computed Shannon X  X  information entropy [ 23 ] given as
H ( C )=  X  d p ( c d | testdata ) log ( p ( c d | testdata random variable taking on values of all possible paths. The information entropy quantity may be interpreted by considering that, in a focused hierarchy, test docu-ments on the same topic would likely be concentrated in a particular area of the hier-archy, their placement being more predictable and implying lower entropy. On the other hand, classification using an unfocused hierarchical model would place doc-uments more evenly across the entire hierarchy, resulting in higher entropy. There-fore, we would expect the information entropy of a focused hierarchy to be lower than that of an unfocused one.
 We only present results for the Twitter #egypt and Reddit #sports test samples because of space considerations. In Figure 3 , information entropy for the test data using SS-Micro-HLDA model is lower than that of other models for deeper hierarchies, suggesting a more focused topic tree. This is particularly encourag-ing as deeper hierarchies provide a way for analysts to focus on a particular area among a potentially large number of topics. In this paper, we developed an algorithm to infer hierarchical topic models around specific concepts that may be of interest to analysts. We evaluated our new algorithm using a large, publicly available collection of microblog messages and showed that the proposed method outperformed other approaches in terms of the PMI-Score. As PMI-Score has been shown to relate favorably to topic interpretability by humans, this evaluation suggests that our new approach pro-duces highly meaningful topic models.
 existing state-of-the-art topic modeling on a static data set, our approach is not designed for continuous operation on stream data. In our future work, we will focus on developing an approach to handle streaming social media messages with the goal of tracking and monitoring social discourse over time.

