 Universidad de Buenos Aires Columbia University
Constantine the Philosopher University and Institute of Informatics, Slovak Academy of Sciences
We present a series of studies of affirmative cue words X  X  family of cue words such as  X  X kay X  or  X  X lright" that speakers use frequently in conversation. These words pose a challenge for spoken dialogue systems because of their ambiguity: They may be used for agreeing with what the in-terlocutor has said, indicating continued attention, or for cueing the start of a new topic, among other meanings. We describe differences in the acoustic/prosodic realization of such functions in a corpus of spontaneous, task-oriented dialogues in Standard American English. These results are important both for interpretation and for production in spoken language applications. We also assess the predictive power of computational methods for the automatic disambiguation of these words. We find that contextual information and final intonation figure as the most salient cues to automatic disambiguation. 1. Introduction C
UE PHRASES are linguistic expressions that may be used to convey explicit information about the discourse or dialogue, or to convey a more literal, semantic contribution.
They aid speakers and writers in organizing the discourse, and listeners and readers in processing it. In previous literature, these constructions have also been termed discourse markers, pragmatic connectives, discourse operators, and clue words. Examples of cue phrases include now, well, so, and, but, then, after all, furthermore, however, in consequence, as a matter of fact, in fact, actually, okay, alright, for example ,and incidentally . natural language processing tasks, including anaphora resolution (Grosz and Sidner 1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986;
Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995).
Furthermore, correctly determining the function of cue phrases using features of the surrounding text can be used to improve the naturalness of synthetic speech in text-to-speech systems (Hirschberg 1990).
 cue words (hereafter, ACWs), and that include alright , mm-hm , okay , right ,and uh-huh , inter alia. These words are frequent in spontaneous conversation, especially in task-oriented dialogue, and are heavily overloaded: Their possible discourse/pragmatic functions include agreeing with what the interlocutor has said, displaying interest and continued attention, and cueing the start of a new topic. Some ACWs (e.g., alright , okay ) are capable of conveying as many as ten different functions, as described in Section 3.
Whereas ACWs thus form a subset of more general classes of utterances which have been studied in more general studies of cue words , cue phrases , discourse markers , feedback utterances , linguistic feedback , acknowledgments , grounding acts , our focus is on this par-ticular subset of lexical items which may convey an affirmative response X  X ut which may also convey many different meanings. The disambiguation of these meanings we believe is critical to the success of spoken dialogue systems.
 in particular by finding descriptions of the acoustic/prosodic characteristics of their different functions, and by assessing the predictive power of computational methods for their automatic disambiguation. This knowledge should be helpful in spoken language generation and understanding tasks, including interactive spoken dialogue systems and applications doing off-line analyses of conversational data, such as meeting segmenta-tion and summarization. For example, spoken dialogue systems lacking a model of the appropriate realization of different uses of these words are likely to have difficulty in understanding and communicating with their users, either by producing cue phrases in a way that does not convey the intended meaning or by misunderstanding users X  productions.
 tion 3 we describe the materials used in the present study from the Columbia Games
Corpus. Section 4 presents a statistical description of the acoustic, prosodic, and con-textual characteristics of the functions of ACWs in this corpus. In Section 5 we describe results from a number of machine learning experiments aimed at investigating how accurately ACWs may be automatically classified into their various functions. Finally, in Section 6 we summarize and discuss our main findings. 2. Previous Work
Cue phrases have received extensive attention in the computational linguistics litera-ture. Early work by Cohen (1984) presents a computational justification for the impor-tance of cue phrases in discourse processing. Using a simple propositional framework for analyzing discourse, Cohen claims that, in some cases, cue phrases decrease the number of operations required by the listener to process  X  X oherent transmissions X ; in other cases, cue phrases are necessary to allow the recognition of  X  X ransmissions which would be incoherent (too complex to reconstruct) in the absence of clues X  (page 251).
Reichman (1985) proposes a model of discourse structure in which discourse com-prises a collection of basic constituents called context spaces , organized hierarchically according to semantic and logical relations called conversational moves . In Reichman X  X  model, cue phrases are portrayed as mechanisms that signal context space boundaries, specifying the kind of conversational move about to take place. Grosz and Sidner (1986) introduce an alternative model of discourse structure formed by three interrelated 2 components: a linguistic structure, an intentional structure, and an attentional state. In this model, cue phrases play a central role, allowing the speaker to provide information about all of the following to the listener:
In a corpus study of spontaneous conversations, Schiffrin (1987) describes cue phrases as syntactically detachable from a sentence, commonly used in initial position within utterances, capable of operating at both local and global levels of discourse, and having a range of prosodic contours. As other authors, Schiffrin observes that cue phrases provide contextual coordinates for an utterance in the discourse X  X hat is, they indicate the discourse segment to which an utterance belongs. However, she suggests that cue phrases only display discourse structure relations; they do not create them. In a critique of Schiffrin X  X  work, Redeker (1991) proposes defining cue phrases as phrases  X  X ttered linkage of the upcoming utterance with the immediate discourse context X  (page 1169). performed by Hirschberg and Litman (Hirschberg and Litman 1987, 1993; Litman and
Hirschberg 1990), which focus on differentiating between the discourse and sentential senses of single-word cue phrases such as now , well , okay , say ,and so in American
English. When used in a discourse sense, a cue phrase explicitly conveys information about the discourse structure; when used in a sentential sense, a cue phrase instead conveys semantic information. Hirschberg and Litman present two manually devel-oped classification models, one based on prosodic features, and one based on textual features. This line of research is further pursued by Litman (1994, 1996), who incorpo-rates machine learning techniques to derive classification models automatically. Litman uses different combinations of prosodic and text-based features to train decision-tree and rule learners, and shows that machine learning constitutes a powerful tool for developing automatic classifiers of cue phrases into their sentential and discourse uses.
Zufferey and Popescu-Belis (2004) present a similar study on the automatic classification of like and well into discourse and sentential senses, achieving a performance close to that of human annotators.
 the Conversational Analysis (CA) literature describes items it terms linguistic feedback or acknowledgments . These include not only the computational linguists X  cue phrases but also expressions such as I see or oh wow , which CA research describes in terms of attention, understanding, and acceptance by the speaker of a proposition uttered by another conversation participant (Kendon 1967; Yngve 1970; Duncan 1972; Schegloff 1982; Jefferson 1984). Such items typically occur at the second position in common adjacency pairs and include backchannels (also referred to as continuers ), which  X  X xhibit on the part of [their] producer an understanding that an extended unit of talk is underway by another, and that it is not yet, or may not be (even ought not yet be) complete; [they take] the stance that the speaker of that extended unit should continue talking X  (Schegloff 1982, page 81), and agreements , which indicate the speaker X  X  agreement with a statement or opinion expressed by another speaker. Allwood,
Nivre, and Ahlsen (1992) distinguish four basic communicative functions of linguistic feedback which enable conversational partners to exchange information: contact , perception , understanding ,and attitudinal reactions . These correspond respectively to whether the interlocutor is willing and able to continue the interaction, perceive the message, understand the message, and react and respond to the message. Allwood,
Nivre, and Ahlsen posit that  X  X imple feedback words, like yes , [...] involve a high degree of context dependence X  (page 5), and suggest that their basic communicative function strongly depends on the type of speech act, factual polarity, and information status of the immediately preceding communicative act. Novick and Sutton (1994) propose an alternative categorization of linguistic feedback in task-oriented dialogue, which is based on the structural context of exchanges rather than on the characteristics of the preceding utterance. The three main classes in Novick and Sutton X  X  catalogue are: (i) other  X  ackn , where an acknowledgment immediately follows a contribution by other speaker; (ii) self  X  other  X  ackn , where self initiates an exchange, other eventually completes it, and self utters an acknowledgment; and (iii) self + ackn , where self includes an acknowledgment in an utterance independently of other  X  X  previous contribution. in our class of ACWs in the psycholinguistic literature in studies of grounding  X  the process by which conversants obtain and maintain a common ground of mutual knowledge, mutual beliefs, and mutual assumptions over the course of a conversation (Clark and Schaefer 1989; Clark and Brennan 1991). Computational work on grounding has been pursued for a number of years by Traum and colleagues (e.g., Traum and Allen 1992; Traum 1994), who recently have described a corpus-based study of lexical and semantic evidence supporting different degrees of grounding (Roque and Traum 2009).
Prosodic characteristics of the responses involved in grounding have been studied in the Australian English Map Task corpus by Mushin et al. (2003), who find that these utterances often consist of acknowledgment contributions such as okay or yeh produced with a  X  X on-final X  intonational contour, and followed by speech by the same speaker which appears to continue the intonational phrase. Studies by Walker of informa-tionally redundant utterances (IRUs) (Walker 1992, 1996), utterances which express  X  X  proposition already entailed, presupposed or implicated by a previous utterance in the same discourse situation X  (Walker 1993a, page 12), also include some of our
ACWs, such as IRU prompts (e.g., uh-huh ), which, according to Walker,  X  X dd no new propositional content to the common ground X  (Walker 1993a, page 32). Walker adopts the term  X  X ontinuer X  from the Conversational Analysis school to further describe these prompts (Walker 1993a). Walker describes some intonational contours which are used to realize IRUs in generation in Walker (1993a) and in Walker (1993b), examining 63 IRU tokens and finding five different types of contour used among them.
 recognition and understanding tasks in American English, Jurafsky et al. (1998) present a study of four particular discourse/pragmatic functions, or dialog acts (Bunt 1989;
Core and Allen 1997), closely related to ACWs: backchannel, agreement, incipient speakership (indicating an intention to take the floor), and yes-answer (affirmative answer to a yes X  X o question). The authors examine 1,155 conversations from the Switch-board database (Godfrey, Holliman, and McDaniel 1992), and report that the vast ma-jority of these four dialogue acts are realized with words like yeah , okay ,or uh-huh . They find that the lexical realization of the dialogue act is the strongest cue to its identity (e.g., backchannel is the preferred function for uh-huh and mm-hm ), and report preliminary results on some prosodic differences across dialogue acts: Backchannels are shorter in duration, have lower pitch and intensity, and are more likely to end in a rising intonation than agreements. Two related studies, part of the same project, address the automatic classification of dialogue acts in conversational speech (Shriberg et al. 1998; Stolcke 4 et al. 2000). The results of their machine learning experiments, conducted on the same subset of Switchboard used previously, indicate a high degree of confusion between agreements and backchannels, because both classes share words such as yeah and right .
They also show that prosodic features (including duration, pause, and intensity) can aid the automatic disambiguation between these two classes: A classifier trained using both lexical and prosodic features slightly yet significantly outperforms one trained using just lexical features.
 at arbitrary locations in conversation; rather, it mostly occurs at or near transition-relevance places for turn-taking (Sacks, Schegloff, and Jefferson 1974; Goodwin 1981).
Ward and Tsukahara (2000) describe, in both Japanese and American English, a region of low pitch lasting at least 110 msec which may function as a prosodic cue inviting the realization of a backchannel response from the interlocutor. In a corpus study of Japanese dialogues, Koiso et al. (1998) find that both syntax and prosody play a central role in predicting the occurrence of backchannels. Cathcart, Carletta, and Klein (2003) propose a method for automatically predicting the placement of backchannels in Scottish English conversation, based on pause durations and part-of-speech tags, that outperforms a random baseline model. Recently, Gravano and Hirschberg (2009a, 2009b, 2011) describe six distinct prosodic, acoustic, and lexical events in American English speech that tend to precede the occurrence of a backchannel by the interlocutor. examine here have seldom, if ever, been an object of study in themselves, as a separate subclass of cue phrases or dialogue acts. Some have attempted to model other types pragmatic functions that may be conveyed through ACWs, but which may also be conveyed through other types of expressions (e.g., agreements may be communicated by single words such as yes or longer cue phrases such as that X  X  correct ). Subsets of ACWs have been studied in very small corpora, with some proposals about their prosodic and functional variations. For example, Hockey (1993) examines the prosodic variation of two ACWs, okay and uh-huh (66 and 77 data points, respectively) produced as full intonational phrases in two spontaneous task-oriented dialogues. She groups the F contours visually and auditorily, and shows that instances of okay produced with a high-rise contour are significantly more likely to be followed by speech from the other speaker than from the same speaker. The results of a perception experiment conducted by Gravano et al. (2007) suggest that, in task-oriented American English dialogue, contextual information (e.g., duration of surrounding silence, number of surrounding words) as well as word-final intonation figure as the most salient cues to disambiguation of the function of the word okay by human listeners. Also, in a study of the function of intonation in Scottish English task-oriented dialogue, Kowtko (1996) examines a corpus of 273 instances of single-word utterances, including affirmative cue words such as mm-hm , okay , right , uh-huh ,and yes . Kowtko finds a significant correlation between discourse function and intonational contour: The align function (which checks that the listener X  X  understanding aligns with that of the speaker) is shown to correlate with rising in-tonational contours; the ready function (which cues the speaker X  X  intention to begin a new task) and the reply-y function (which  X  X as an affirmative surface and usually indicates agreement X ; Kowtko 1996, page 59) correlate with a non-rising intonation; and the acknowledge function (which indicates having heard and understood) presents and different languages have distinct ways of realizing different discourse/pragmatic functions, so it is unclear how useful these results are for American English. course markers, linguistic feedback, and dialogue acts do shed light on the particular subset of utterances we are studying, and although there is some information on par-ticular lexical items we include here in our study, the class of ACWs itself has received little attention. Particularly given the frequency of ACWs in dialogue, it is important to identify reliable and automatically extractable cues to their disambiguation, so that spoken dialogue systems can recognize the pragmatic function of ACWs in user input and can produce ACWs that are less likely to be misinterpreted in system output. 3. Materials The materials for all experiments in this study were taken from the Columbia Games
Corpus, a collection of 12 spontaneous task-oriented dyadic conversations elicited from 13 native speakers (6 female, 7 male) of Standard American English (SAE). A detailed description of this corpus is given in Appendix A. In each session, two subjects were paid to play a series of computer games requiring verbal communication to achieve joint goals of identifying and moving images on the screen. Each subject used a separate laptop computer; they sat facing each other in a soundproof booth, with an opaque curtain hanging between to allow only verbal communication.
 of dialogue in the corpus. Trained annotators orthographically transcribed the re-cordings and manually aligned the words to the speech signal, yielding a total of 70,259 words and 2,037 unique words in the corpus. Additionally, self repairs and certain non-word vocalizations were marked, including laughs, coughs, and breaths.
For roughly two thirds of the corpus, intonational patterns and other aspects of the prosody were identified by trained annotators using the ToBI transcription framework (Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994). 3.1 Affirmative Cue Words in the Games Corpus Throughout the Games Corpus, subjects made frequent use of affirmative cue words :
The 5,456 instances of affirmative cue words alright , gotcha , huh , mm-hm , okay , right , uh-huh , yeah , yep , yes ,and yup account for 7.8% of the total words in the corpus. Because the usage of these words seems to vary significantly in meaning, we asked three labelers to independently classify all occurrences of these 11 words in the entire corpus into the ten discourse/pragmatic functions listed in Table 1.
 that our categories of Agr and BC differ primarily in that Agr is defined as indicating belief in or agreement with the interlocutor (e.g., a response to a yes X  X o question), whereas BC indicates only continued attention. 1 6 transcript and speech source. The guidelines used by the annotators are presented in
Appendix B. Appendix C includes some examples of each class of ACWs, as labeled by our annotators. Inter-labeler reliability was measured by Fleiss X  X   X  (Fleiss 1971) as
Substantial at 0.745. 2 We d e fi n e t h e majority label of a token as the label chosen for that token by at least two of the three labelers; we assign the  X ? X  label to a token either when its majority label is  X ? X , or when it was assigned a different label by each labeler. Of the 5,456 affirmative cue words in the corpus, 5,185 (95%) have a majority label other than  X ?. X  Table 2 shows the distribution of discourse/pragmatic functions over ACWs in the whole corpus. 3.2 Data Downsampling
Some of the word/function pairs in Table 2 are skewed to contributions from a few speakers. For example, for backchannel ( BC ) uh-huh , as many as 65 instances (44%) are from one single speaker, and the remaining 83 are from seven other speakers. In cases like this, using the whole sample would pose the risk of drawing false conclusions on the usage of ACWs, possibly influenced by stylistic properties of individual speakers.
Therefore, we downsampled the tokens of ACWs in the Games Corpus to obtain a balanced data set, with instances of each word and function coming in similar propor-tions from as many speakers as possible. Specifically, we downsampled our data using the following procedure: First, we discarded all word/function pairs with tokens from fewer than four different speakers; second, for each of the remaining word/function pairs, we discarded tokens (at random) from speakers who contributed more than 25% of its tokens. In other words, the resulting data set meets two conditions: For each word/ function pair, (a) tokens come from at least four different speakers, and (b) no single speaker contributes more than 25% of the tokens. The two thresholds were found via a grid search, and were chosen as a trade-off between size and representativeness of the data set. With this procedure we discarded 506 tokens of ACWs, or 9.3% of such words in the corpus. Table 3 shows the resulting distribution of discourse/pragmatic functions over ACWs in the whole corpus after downsampling the data. The  X  measure of inter-labeler reliability was practically identical for the downsampled data, at 0.751. 3.3 Feature Extraction
We extracted a number of lexical, discourse, timing, phonetic, acoustic, and prosodic features for each target ACW, which we use in the statistical analysis and machine learning experiments presented in the following sections. Tables 4 through 8 summarize the full feature set. For simplicity, in those tables each line may describe one or more features. Features that may be extracted by on-line applications are marked with letter
O ; this is further explained later in this section.
 8 of the target word ( w ), the word immediately preceding w , and the word immediately following w (see Table 4). POS tags were labeled automatically for the whole corpus using Ratnaparkhi, Brill, and Church X  X  (1996) maxent tagger trained on a subset of the
Switchboard corpus (Charniak and Johnson 2001) in lower-case with all punctuation removed, to simulate spoken language transcripts. Each word had an associated POS tag from the full Penn Treebank tag set (Marcus, Marcinkiewicz, and Santorini 1993), and one of the following simplified tags: noun, verb, adjective, adverb, contraction, or other.
 a maximal sequence of words surrounded by silence longer than 50 msec. A turn is a maximal sequence of IPUs from one speaker, such that between any two adjacent IPUs there is no speech from the interlocutor. 3 , 4 Boundaries of IPUs and turns are computed automatically from the time-aligned transcriptions. A task in the Games Corpus cor-responds to a simple game played by the subjects, requiring verbal communication to achieve a joint goal of identifying and moving images on the screen (see Appendix A for a description of these game tasks). Task boundaries are extracted from the logs collected automatically during the sessions, and subsequently checked by hand. Our discourse features are intended to capture discrete positional information of the target word, in relation to its containing IPU, turn, and task. temporal nature, such as the duration (in milliseconds) of w and its containing IPU and turn, or the duration of any silence before and after w . These features also contain information about the target word relative to the other speaker X  X  speech, including the duration of any overlapping speech, and the latencies between w  X  X  conversational turn and the other speaker X  X  preceding and subsequent turns.

Pitrelli, Beckman, and Hirschberg 1994), which consists of annotations at four time-linked levels of analysis: an orthographic tier of time-aligned words; a tonal tier describing targets in the fundamental frequency (F0) contour; a breakindextier indicating degrees of juncture between words; and a miscellaneous tier , in which phenomena such as disfluencies may be optionally marked. The tonal tier describes events such as pitch accents , which make words intonationally prominent and are realized by increased F0 height, loudness, and duration of accented syllables. A given word may be accented or not and, if accented, may bear different tones, or different degrees of prominence, with respect to other words. Five types of pitch accent are distinguished in the ToBI system for American English: two simple accents H* and L* , and three complex ones,
L*+H , L+H* ,and H+!H* .An L indicates a low tone and an H , a high tone; the the lexical item bearing the accent. Some pitch accents may be downstepped ,suchthat the pitch range of the accent is compressed in comparison to a non-downstepped accent.
Downsteps are indicated by the  X  !  X  diacritic (e.g., !H* , L+!H* ). Break indices define two levels of phrasing: Level 3 corresponds to Pierrehumbert X  X  (1980) intermediate phrase and level 4 to Pierrehumbert X  X  intonational phrase. Level 4 phrases consist of one or more level 3 phrases, plus a high or low boundary tone ( H% or L% ) indicated in the tonal tier at the right edge of the phrase. Level 3 phrases consist of one or more pitch accents, aligned with the stressed syllable of lexical items, plus a phrase accent , which also may be high ( H-)orlow( L-). For example, a standard declarative contour consists 10 of a sequence of H* pitch accents ending in a low phrase accent and low boundary tone ( L-L% ); likewise, a standard yes X  X o question contour consists of a sequence of L* pitch accents ending in H-H% . These are illustrated in Figure 1. In our study, prosodic version of the labels, considering only high and low pitch targets (i.e., H* vs. L* for pitch accents, H-vs. L-for phrase accents, and H% vs. L% for boundary tones), and simplified break indices (0 X 4). These are listed in Table 6.

Praat toolkit (Boersma and Weenink 2001). These include pitch, intensity, stylized pitch, ratio of voiced frames to total frames, jitter, shimmer, and noise-to-harmonics ratio (NHR) (see Table 7). Pitch features capture how high the speaker X  X  voice sounds or how low. Intensity is correlated with how loud the speaker sounds to a hearer. The voiced-frames ratio roughly approximates the speaking rate. Jitter and shimmer correspond to variability in the frequency and amplitude of vocal-fold vibration, respectively. NHR is the energy ratio of noise to harmonic components in the voiced speech signal. Jitter, shimmer, and NHR correlate with perceptual evaluations of voice quality, such as harsh, whispery, creaky, and nasalized, inter alia. Pitch slope features capture elements of the intonational contour, and were computed by fitting least-squares linear regression models to the F 0 data points extracted from given portions of the signal, such as a full word or its last 200 msec. This procedure is illustrated in Figure 2, which shows the pitch track of a sample utterance (blue dots) with three linear regressions, com-puted over the whole utterance (solid black line), and over the final 300 and 200 msec ( X  X  X  and  X  X  X  dashed lines, respectively). We used a similar procedure to compute intensity slopes (which capture changes in perceived loudness) and stylized pitch slopes (which capture more coarse-grained characteristics of the intonational contour).
Stylized pitch curves were obtained using the algorithm provided in Praat: Look up the pitch point p that is closest to the straight line L that connects its two neighboring points; if p is further than four semitones away from L , end; otherwise, remove p and start over.
 12 pitch or final pitch slope, are not comparable across genders because of the different pitch ranges of female and male speakers X  X oughly 75 X 500 kHz and 50 X 300 kHz, respec-tively. Therefore, before computing those features we applied a linear transformation to the pitch track values, thus making the pitch range of speakers of both genders approximately equivalent. We refer to this process as gender normalization . All other normalizations were calculated using z -scores: z = ( x surement to be normalized (e.g., the duration of a particular word), and  X  and  X  are the mean and standard deviation of a certain population (e.g., all instances of the same word by the same speaker in the whole conversation).
 ognizer based on the Hidden Markov Model Toolkit (HTK) (Young et al. 2006), using three corpora as training data: the TIMIT Acoustic-Phonetic Continuous Speech Corpus (Garofolo et al. 1993), the Boston Directions Corpus (Hirschberg and Nakatani 1996), and the Columbia Games Corpus. With this recognizer, we obtained automatic time-aligned phonetic transcriptions of each instance of alright , mm-hm , okay , right , uh-huh , and yeah in the corpus. To improve accuracy, we restricted the recognizer X  X  grammar to accept only the most frequent variations of each word, as shown in Table 9. We extracted our phonetic features, such as phone and syllable durations, from the resulting time-aligned phonetic transcriptions. The remaining five ACWs in our corpus ( gotcha , huh , yep , yes ,and yup ) had too low counts to contain meaningful phonetic variation; thus, we did not compute phonetic features for those words.
 which the target word was produced, along with the identity and gender of both speakers (Table 8). These features were solely intended for searching for speaker or dialogue dependencies.
 extracted from the speech signal only up to the IPU containing the target ACW. In
Tables 4 through 8 these features are marked with letter O tures can be computed automatically in real time by state-of-the-art speech processing applications, although it should be noted that all of our lexical and discourse features strongly rely on a speech recognizer output, which typically has a high error rate for spontaneous productions. All on-line features are also available in off-line conditions; the remaining features (those not tagged O in Tables 4 through 8) are normally available only in offline conditions. We distinguish online features for the machine learning exper-iments described in Section 5, in which we assess, among other things, the usefulness of information contained in different feature sets, simulating the conditions of actual on-line and off-line applications.
 tions of ACWs. Subsequently, we experiment with machine learning techniques for the automatic classification of the function of ACWs, training the models with different combinations of features. 4. Characterizing Affirmative Cue Words
In this section we present results of a series of statistical tests aimed at identifying con-textual, acoustic, and prosodic differences in the production of the various discourse/ pragmatic functions of affirmative cue words. This kind of characterization is important both for interpretation and for production in spoken language applications: If we can find reliable features that effectively distinguish the various uses of these words, we can hope to interpret them automatically and generate them appropriately. 4.1 Position in IPU and Turn
We begin this analysis by looking at the discourse position of the various discourse/ pragmatic functions of ACWs. Because these words help shape, or at least reflect, the structure of conversations, we expect to find positional differences between their functions. Figure 3 shows the distribution of the six most frequent ACWs in the corpus ( alright , okay , yeah , mm-hm , uh-huh ,and right )withrespecttotheirpositionintheirIPU.
An IPU-initial word is one that occurs in the first position in its corresponding IPU; that is, it is preceded by at least 50 msec of silence and followed by another word. An IPU-final word occurs last in its IPU. An IPU-medial word is both immediately preceded and followed by other words. Lastly, a single-word IPU is an individual word both preceded and followed by silence. Figure 3 also depicts the distribution of discourse/ pragmatic functions within each of these four categories. For example, roughly 40% of all tokens of alright in the corpus occur as IPU initial; of those, about half are agreements ( Agr ), half are cues to beginning discourse segments ( CBeg ), and a marginal number convey other functions.
 position in the corresponding conversational turn. Turn-initial , turn-medial ,and turn-final words, and single-word turns are defined analogously to the four IPU-related categories defined previously, but considering conversational turns instead of IPUs. 14 of ACWs in the Games Corpus. Only a minority of these words occur as IPU medial instances do occur in such positions X  X ainly tokens with the literal modifier ( Mod ) meaning, but also tokens used to check with the interlocutor ( Chk ), which take place at the end of a turn (and thus, of an IPU).
 right in all possible positions within the IPU and the turn; for mm-hm and uh-huh , agreements occur mostly as full conversational turns. Nearly all backchannels ( BC ) occur as separate turns, with only a handful of exceptions: In four cases, the backchannel is followed by a pause in which the interlocutor chooses not to continue speaking, and the utterer of the backchannel takes the turn; in two other cases, two backchannels are uttered in fast repetition (e.g., uh-huh uh-huh ).
 pattern similarly. The first such pair consists of mm-hm and uh-huh , which show very similar distributions and are realized almost always as single-word turns, as either Agr or BC . The second pair of words with comparable patterns of IPU and turn position are alright and okay . These are precisely the only two ACWs used to convey all ten discourse/pragmatic functions in the Games Corpus (recall from Table 2). This result suggests that the lexical items in these two pairs may be used interchangeably in conversation. The word yeah presents a pattern analogous to that of alright and okay , albeit with fewer meanings.
 position of ACWs between their functional types, as well as between their lexical of discourse features in the automatic classification of the function of ACWs. Given the observed positional differences, we expect these features to play a prominent role in such a task. 4.2 Word-Final Intonation
Shifting our attention to acoustic/prosodic characteristics of ACWs, we examine next the manner in which word-final intonation varies across ACW functions. First we look at two categorical variables in the ToBI framework which capture the final pitch incur-sion: phrase accent and boundary tone. Figure 5 shows the distribution of ToBI labels for each of the six most frequent ACWs and their corresponding functions (see Section 3.3 for a description of the ToBI labeling conventions). The distributions for alright , okay , right ,and yeah depart significantly from random ( alright : Fisher X  X  Exact test, p = 0.0483; okay : Pearson X  X  Chi-squared test,  X  2 (24) = 261, p  X  0; right : Pearson,  X  yeah : Fisher, p  X  0). 5 , 6 For right , considering just its discourse/pragmatic functions (i.e., excluding its Mod instances), the distribution also significantly differs from random (Fisher, p  X  0). On the other hand, the distributions for mm-hm and uh-huh do not depart significantly from random. 16 preference for a high-rising ( H-H% in the ToBI conventions) or low-rising ( L-H% )pitch contour towards the end of the word. Those two contours account for more than 60% of the backchannel instances of mm-hm , okay , uh-huh ,and yeah . For the other ACWs there are not enough instances labeled BC in the corpus for statistical comparison.
The predominance of H% found for backchannels is consistent with the openness that such boundary tone has been hypothesized to indicate (Hobbs 1990; Pierrehumbert and Hirschberg 1990). The utterer of a backchannel understands that (i) there is more to be said, and (ii) it is the speaker holding the conversational turn who must say it.
 ( L-L% ) or plateau final intonation ( [!]H-L% ) in the case of alright , okay , right ,and yeah .
The L% boundary tone is believed to indicate the opposite of H% , a sense of closure , separating the current phrase from a subsequent one (Pierrehumbert and Hirschberg 1990). In our case, by agreeing with what the speaker has said, the listener indicates that enough information has been provided and that any subsequent phrases may refer to a different topic. In other words, such closure might mean that the proposition preceding the ACW has been added to the current context space (Reichman 1985), or that a new focus space is about to be created (Grosz and Sidner 1986).
 the other lexical items, with a distribution of final intonations that closely resembles that of backchannels. In particular, over 60% of the Agr tokens of mm-hm and uh-huh are produced with final rising intonation (either L-H% or H-H% ). As we will see in the following sections, the realization of mm-hm and uh-huh as Agr or BC seems to be very similar along several dimensions besides intonation.
 beginning of a new discourse segment, either combined with an agreement function ( PBeg )orinitspureform( CBeg ). These two functions typically have a falling ( L-L% )or sustained ( [!]H-L% ) final pitch contour. Additionally, the instances of okay and yeah used to cue a discourse segment ending ( PEnd ) tend to be produced with a L-L% contour, and also with [!]H-L% in the case of okay . This predominance of L% for ACWs conveying a discourse boundary function is consistent with the previously mentioned closure that such boundary tone is believed to indicate.
 tor (the Chk function) is right , as illustrated in the following exchange: A: and the top X  X  not either, right ? B: no A: okay
Such instances of right in the corpus normally end in a high-rising pitch contour, or H-H% . This fact is probably explained by the close semantic resemblance of this construction to yes X  X o questions, which typically end in the same contour type (Pierrehumbert and Hirschberg 1990).
 intonation may also be studied by measuring the slope of the word-final pitch track (see Section 3.3 for a description of how pitch slopes are calculated). A high positive value of pitch slope corresponds to a rising intonation; a value close to zero, to a flat intonation; a high negative value, to a falling intonation. Final pitch slope has the ad-vantage of being automatically computable; ToBI labels, on the other hand, still must be manually annotated X  X lthough ongoing research may change this fact in the near future (Rosenberg and Hirschberg 2009; Rosenberg 2010a, 2010b). Therefore, it is important to verify that the results obtained using ToBI labels X  X f they are to be of practical use X  are also observable when considering numeric measures such as pitch slope. Figure 6 shows, for the same ACWs and functions discussed earlier, computed over the second half of the word and over its final 100 and 200 msec, and gender-normalized as described in Section 3.
 functions confirms that the observations made previously for categorical prosodic fea-tures also hold when considering numeric features such as pitch slope, thus making the likelihood that such observations will be of practical use in actual systems. For okay , the three measures of word-final pitch slope are significantly higher for backchannels ( BC ) than for all other functions, and significantly lower for CBeg than for Agr , BC , and PEnd ( RMANOVA for each of the three variables: between-subjects p &gt; 0 . 3, within subjects p  X  0; Tukey test confidence: 95%). 8 BC tokens of yeah are also significantly higher than Agr , with similar p-values. Figure 6 shows that BC instances of mm-hm 18 and uh-huh also have comparably high final pitch slopes. Again, for mm-hm we find no significant difference in final pitch slope between agreements and backchannels. pitch, the RMANOVA tests yield between-subjects p-values of 0.01 or lower, indicating substantial speaker effects. In other words, even though the general tendency for these tokens, as indicated by both the numeric and categorical variables, seems to be to end in a high-rising intonation, there is evidence of different behavior for some individual speakers, which keeps us from drawing general conclusions about this pragmatic func-tion of right . 4.3 Intensity
The next feature we find to vary significantly with the discourse/pragmatic function of ACWs is word intensity. Commonly referred to as loudness or volume, intensity generally functions to make words more salient or prominent. Figure 7 shows the maximum and mean intensity for the most frequent ACWs and functions, computed over the whole word and speaker normalized using z -scores.

ACWs. For okay and yeah , both maximum and mean intensity are significantly lower for instances cueing the end of a discourse segment ( PEnd ) than instances of all other functions (for both variables and both words, RMANOVA tests report between-subjects p &gt; 0 . 4 and within-subjects p  X  0; Tukey 95%). For ACWs cueing a beginning discourse segment, the opposite is true. Instances of alright and okay labeled CBeg or PBeg have a maximum and mean intensity significantly higher than all other functions (for alright , a
RMANOVA test reports between-subjects p &gt; 0 . 12 and within-subjects p results are consistent with previous studies of prosodic variation relative to discourse structure, which find intensity to increase at the start of a new topic and decrease at the end (Brown, Currie, and Kenworthy 1980; Hirschberg and Nakatani 1996). Because by definition CBeg / PBeg ACWs begin a new topic and CEnd / PEnd end one, it is then not surprising to find that the former tend to be produced with higher intensity, and the latter with lower.
 tween their unique functions, agreement ( Agr ) and backchannel ( BC ). Recall from the previous section that we find no differences in final intonation either. This further suggests that these two lexical types tend to be produced with similar acoustic/prosodic features, independently of their function. 4.4 Other Features
For the remaining acoustic/prosodic features analyzed, we find a small number of sig-nificant or approaching-significance differences between the functions of ACWs. These differences are related to duration, mean pitch, and voice quality. The first set of findings corresponds to the duration of ACWs, normalized with respect to all words with the same number of syllables and phonemes uttered by the same speaker. For alright and okay , instances cueing a beginning ( CBeg ) tend to be shorter than the other functions (for both words, RMANOVA : between-subjects p &gt; 0 . 5, within-subjects p &lt; 0 . 05, Tukey 95%). We also find tokens of right used to check with the interlocutor ( Chk )tobe on average shorter than the other two functions of right ( p &gt; 0 . 7, within-subjects p = 0 . 001; Tukey 95%). Note that these two functions are rel-atively simple: CBeg calls for the listener X  X  attention, and is frequently conveyed with a filled pause ( uh , um ); Chk asks the interlocutor for confirmation, which may alter-natively be achieved via a high-rising intonation. Thus, it is not surprising that these functions take less time to be realized than other more pragmatically loaded functions, such as agreement.
 differences for okay and yeah . Instances labeled PEnd (agreement and cue ending discourse segment) present a higher mean pitch than the other functions (for both words, RMANOVA : between-subjects p &gt; 0 . 6, within-subjects p &lt; 0 . 01; Tukey 95%). This is rather unexpected, because as noted in Section 4.2 around 70% of PEnd ACWs in the corpus end in a L% boundary tone, and thus they would plausibly be uttered with a low pitch level. What our data indicate, however, is that speakers tend to reset and raise their pitch range when producing PEnd instances of ACWs.
 okay show a lower shimmer over voiced portions when starting a new segment ( CBeg ) (
RMANOVA : between-subjects p &gt; 0 . 9for alright ,p = 0 . 09 for okay ; within-subjects p &lt; 0 . 001 for both words). Also, okay and yeah present a lower noise-to-harmonics ratio (NHR) for backchannels ( RMANOVA : between-subjects p &gt; 0 . 3for okay ,p = 0 . 04 for yeah ; within-subjects p &lt; 0 . 005 for both words). A lower value of shimmer and NHR has been associated with the perception of a better voice quality (Eskenazi, Childers, and Hicks 1990; Bhuta, Patrick, and Garnett 2004). Our results suggest, then, that voice quality may constitute another dimension along which speakers vary their productions to convey the intended discourse/pragmatic meaning. Notice though that for these two variables some of the between-subjects p-values are low enough to suggest significant 20 speaker effects. Therefore, our results related to differences in voice quality should be considered preliminary. 5. Automatic Classification of Affirmative Cue Words
In this section we present results from machine learning (ML) experiments aimed at investigating how accurately affirmative cue words may be classified automatically into their various discourse/pragmatic functions. If spoken dialogue systems are to interpret and generate ACWs reliably, we must identify reliable cues. With this goal in mind, we explore several dimensions of the problem: We consider three classification tasks, simulating the conditions of different speech applications, and study the perfor-mance of different ML algorithms and feature sets on each task. We note that previous studies have attempted to disambiguate between the sentential and discourse uses of cue phrases such as now , well ,and like , in corpora containing comparable numbers of instances of each class. For ACWs in the Games Corpus dialogues, sentential uses are rare, with the sole exception of right . Therefore, disambiguating between discourse and sentential uses appears to be less useful than distinguishing among different discourse functions.
 ( Agr , BC , CBeg , PBeg , CEnd , PEnd , Mod , BTsk , Chk , Stl ; see Table 1), a critical task for spoken dialogue systems seeking to interpret user input in general. The second task involves identifying instances of these words used to signal the beginning ( CBeg , PBeg in our labeling scheme) or ending ( CEnd , PEnd ) of a discourse segment, which is im-portant for applications that must segment speech into coherent units, such as meeting browsing systems and turn-taking components of spoken dialogue systems. The third task consists in identifying tokens conveying some degree of acknowledgment: ( Agr ,
BC , PBeg ,and PEnd ), a function especially important to spoken dialogue systems, for which it is critical to know that a user has heard the system X  X  output.
 tions such as spoken dialogue systems process information as it is generated, having access to a limited amount of context, normally up to the last IPU uttered by the user.
On the other hand, off-line applications, such as meeting transcription and browsing systems, have the whole audio file available for processing. We simulate these two conditions in our experiments, assessing how the limitations of online systems af-fect performance. We also group the features described in Section 3.3 into five sets X  through 8 X  X o determine the relative importance of each feature set in the various classification tasks. For example, this approach allows us to simulate the conditions of the understanding component of a spoken dialogue system, which can use only the information up through the current IPU to detect the function of a user X  X  ACW. Such a system may have access only to ASR transcription or it may have access to acoustic and prosodic information; we note that our analysis does not take into account the pos-sibility that transcriptions are likely to contain some errors. Our approach also allows us to simulate a text-to-speech (TTS) system which might be used to produce a spoken version of an on-line chat room. In order to choose the appropriate acoustic/prosodic realization of each ACW, the TTS system will first need to determine its function based on features extracted solely from the input text (in our taxonomy, LX and DS ). different characteristics: the decision tree learner C4.5 (Quinlan 1993), the propositional rule learner R IPPER (Cohen 1995), and support vector machines (SVM) (Cortes and
Vapnik 1995; Vapnik 1995). We use the implementation of these algorithms provided in the W EKA machine learning toolkit (Witten and Frank 2000), known respectively as
J48 ,JR IP JR IP JR IP ,and SMO . We also use 10-fold cross-validation in all experiments. 5.1 Classifiers and Feature Types
To assess the predictive power of the five feature types ( LX , DS , TM , AC ,and PH )we exclude one type at a time and compare the performance of the resulting set to that of the full model. Table 10 displays the error rate of each ML classifier on the general task, classifying any ACW into any of the most frequent discourse/pragmatic functions ( Agr , BC , CBeg , PEnd , Mod , Chk ). Table 11 shows the same results for the other two tasks: the detection of a discourse boundary function X  X ue beginning ( CBeg PBeg ), cue ending ( CEnd , PEnd ), or no-boundary (all other labels); and the detection of an acknowledgment function X  Agr , BC , PBeg ,or PEnd , vs. all other labels. 22 types. The subsequent five lines show the performance of models with just four fea-ture types, excluding one feature type at a time, and the following five lines show the performance of models with exactly one feature type X  X hese are two methods for assessing the predictive power of each feature set. For the error rates of our classifiers, the  X  symbol indicates that the given classifier performs significantly worse when trained on a particular feature set than when trained on the full set. indicates that the difference between SVM and the given classifier, either C4.5 or Ripper, for the general classification task, the three models trained on all but lexical features perform significantly worse than the respective full models; also, the performance of
C4.5 is significantly worse than SVM, and the difference between Ripper and SVM is not significant.
 well as two estimates of the error rate of human labelers. We consider two types of baseline: one a majority-class baseline, and one that employs a simple rule based on word identity. In the general classification task, the majority class is Agr ,andthe best performing word-based rule is huh  X  Chk , mm-hm  X 
Mod , others  X  Agr . For the identification of a discourse boundary function, the majority class is no-boundary, and the word-based rule also assigns no-boundary to all tokens. For the detection of an acknowledgment function, the majority class is acknowledgment, and the word-based rule is right , huh others  X  acknowledgment.

Our first estimate compares the labels assigned by each labeler and the majority labels as defined in Section 3.1. Because each labeler X  X  labels are used for calculating both the error rate and the gold standard, this estimate is likely to be over-optimistic. Our second estimate considers the subset of cases in which two annotators agree, and compares those labels with the third labeler X  X . Tables 10 and 11 show that these two estimates yield similar results; for PEnd and Chk , there are not enough counts for computing the F-measure of estimate 2.
 individual ACW function, for the general task. The highest F-measures correspond to Agr , BC , CBeg ,and Mod , precisely the four functions with the highest counts in the
Games Corpus. For PBeg and Chk the F-measures are much lower (and equal to zero for the four remaining functions, not included in the table) due very likely to their low counts, which prevent a better generalization during the learning stage. Future research could investigate incorporating boosting and bootstrapping techniques to reduce the negative effect on classification of low counts for some of the discourse/pragmatic functions of ACWs.
 to, the other two classifiers whenever acoustic features ( AC ) are taken into account together with other feature types. When used alone, though, acoustic features per-form poorly in all three tasks. Moreover, when acoustic features are excluded, SVM X  X  accuracy is comparable to, or worse than, C4.5 and Ripper. This is probably due to the fact that SVM X  X  mathematical model is better suited to exploit larger amounts of continuous numerical variables, and thus makes a difference when including acoustic features.
 feature type, as shown by the significantly lower performance resulting from removing any of the feature types from the full model X  X he sole exception is the phonetic type ( PH ), whose removal in no case negatively affects the accuracy of any classifier. C4.5 and Ripper, on the other hand, appear to take more advantage of some feature types than others. For the third task, lexical ( LX ) and discourse ( DS ) features apparently have more predictive power for both C4.5 and SVM than the other types. Note also that for the second and third tasks, the error rates of our full-model SVM classifiers closely approximate the estimated error rates of human labelers.
 our full-model SVM classifier achieves the best overall results. To take a closer look at the performance of this model, we compute its F-measure for the discourse/pragmatic functions of each individual lexical item, as shown in Table 12. We observe that the classifier achieves better results for word X  X unction pairs with higher counts in the
Games Corpus, such as yeah -Agr or right -Mod (cf. Table 2). Again, the low counts for the remaining word X  X unction pairs may prevent a better generalization during the learning stage, a problem that could be attenuated in future work with boosting and bootstrapping techniques. 24 5.2 Session-Specific and ToBI Prosodic Features
When including session-specific features in the full model, such as identity and gender of both speakers (see Table 8), the error rate of the SVM classifier is significantly reduced for the general task (13.3%) and for the discourse boundary function identification task (6.4%) (Wilcoxon, p &lt; 0 . 05). For the detection of an acknowledgment function, the error rate is not modified when including those features (4.5%). This suggests the existence of speaker differences in the production of at least some functions of ACWs that may be exploited by ML classifiers. Finally, the inclusion of categorical prosodic features based on the ToBI framework, such as type of pitch accent and break index on the target word (see Table 5), does not improve the performance of the SVM-based full models in any of the classification tasks. 5.3 Individual Features
To estimate the importance of individual features in our classification tasks, we rank them according to an information-gain metric. We find that for the three tasks, lexical ( LX ), discourse ( DS ), and timing ( TM ) features dominate. The highest ranked features are the ones capturing the position of the target word in its IPU and in its turn. Lexical identity and POS tags of the previous, target, and following words, and duration of the target word, are also ranked high. Acoustic features appear lower in the ranking; the best performing ones are word intensity (range, mean, and standard deviation), pitch (maximum and mean), pitch slope over the final part of the word (200 msec and second half), voiced-frames ratio, and noise-to-harmonics ratio. All phonetic features are ranked very low. Note that, whereas durational features at the word level are ranked high, durational features at the phonetic level are not, because the latter only capture the duration of each phone relative to the word duration X  X pparently not an informative attribute for these classification tasks. These results confirm the existence of large positional differences across functions of ACWs, as seen in Section 4. Additionally, whereas several acoustic/prosodic features extracted from the target word contain use-ful information for the automatic disambiguation of ACWs, it is positional information that provides the most predictive power. 5.4 Online and Offline Tasks
To simulate the conditions of online applications, which process speech as it is produced by the user, we consider a subset of features that may typically be extracted from the speech signal only up to the IPU containing the target ACW. These features are marked in Tables 4 through 8 with letter O . With these features, we train and evaluate an SVM classifier for the three tasks described previously. Table 13 shows the results, comparing the performance of each classifier to that of the models trained on the full feature set, which simulate the conditions of off-line applications. In all three cases the on-line model performs significantly worse than its offline correlate, but also significantly better than the baseline (Wilcoxon, p &lt; 0 . 05).
 solely text-based features X  X hat is, only features of lexical ( LX ) or discourse ( DS )types.
Text-based models simulate the conditions of spoken dialogue systems with no access to acoustic and prosodic information, or generation systems attempting to realize text-based exchanges in speech. They reflect the importance of text information alone in training such systems to recognize the function of ACWs on-line and off-line and to produce appropriate realizations from limited or full transcription.
 corresponding models that use the whole feature set, but they still outperform the baseline models in all cases (Wilcoxon, p &lt; 0 . 05). Finally, the off-line text-based models also outperform their on-line correlates in all three tasks (Wilcoxon, p &lt; 0 . 05). These results indicate the important role that other classes of cues play in recognition, while indicating the level of performance we can expect from TTS systems which have only text available. 5.5 Backchannel Detection
The correct identification of backchannels is a desirable capability for speech processing systems, as it would allow us to distinguish between two quite distinct speaker inten-tions: the intention to take the conversational floor, and the intention to backchannel.
ACWs in the corpus into backchannels vs. the rest, using information from the whole conversation. In such a task, an SVM classifier achieves a 4.91% error rate, slightly yet significantly outperforming a word-based baseline ( mm-hm , uh-huh no-BC ), with 5.17% (Wilcoxon, p &lt; 0 . 05).
 speaker contribution immediately after (or even while) it is uttered, and certainly without access to any subsequent context. The Games Corpus contains approximately 6,700 turns following speech from the other speaker, all of which begin as potential backchannels and need to be disambiguated by the listener. Most of these candidates can be trivially discarded using a simple observation about backchannels: By definition 26 they are short, isolated utterances, and consist normally in just one ACW. Of the 6,700 candidate turns in the corpus, only 2,351 (35%) begin with an isolated ACW, including 753 of the 757 backchannels in the corpus. 12 Thus, an on-line classification procedure would only need to identify backchannels in those 2,351 turns. At this point, we ex-plore using a binary classifier for this task. The same word-based majority baseline described earlier achieves an error rate of 11.56%. An SVM classifier trained on features extracted from up to the current IPU (to simulate the on-line condition of a spoken dialogue system) fails to improve over this baseline, achieving an error rate of 11.51%, not significantly different from the baseline. A possible explanation for this might be that backchannels seem to be difficult to distinguish from agreements in many cases, previous section, the positional and acoustic/prosodic similarities of tokens with these two functions for mm-hm and uh-huh , for example. Shriberg et al. (1998) report the same difficulty in distinguishing these two word functions. We conclude that further research is needed to develop novel approaches to this crucial problem of spoken dialogue systems. 5.6 Comparison with Previous Work what we believe to be the most relevant results from related studies. Note, however, that comparing these results directly to the results of our classification experiments is difficult because the type of corpora, definitions used, features examined, and/or methodology employed vary greatly among the studies. The current study focuses exclusively on the discourse/pragmatic functions of ACWs whereas other studies have either a broader or narrower scope.
 investigate. Litman describes the automatic classification of cue words in general (in-cluding, e.g., now , well , say ,and so ), classifying these into discourse and sentential uses using a corpus of monologue. In this classification task, which is not performed in our study, the best results are reached by decision-tree learners trained on prosodic and text-based features, with an error rate of 13.8%.
 experiments on the automatic disambiguation of dialogue acts (DA) on 1,155 sponta-neous telephone conversations from the Switchboard corpus, labeled using the DAMSL (Dialogue Act Markup in Several Layers) annotation scheme (Core and Allen 1997). For the subtask of identifying the Agreement and Backchannels tags collapsed together, the authors report an error rate of 27.1% when using prosodic features, 19.0% when using features extracted from the text, and 15.3% when using all features. Other DA classi-fications also include some of the functions of ACWs discussed in our current study.
For instance, Reithinger and Klesen (1997) employ a Bayesian approach for classifying 18 classes of DAs in transcripts of 437 German dialogues from the VERBMOBIL corpus (Jekat et al. 1995). The DA tags examined include Accept, Confirm, and Feedback, all of which are related to the functions of ACWs discussed here. For the Accept DA tag, the authors report an F-measure of 0.69; for Feedback, 0.48; and for Confirm, 0.40. These experiments are repeated on transcripts of 163 English dialogues from the same corpus, yielding an F-measure of 0.78 for the Accept DA tag, and 0 for the other two tags due to data sparsity.
 type of task, Core (1998) experiments with hand-coded decision trees for classifying five high-level dialogue act classes, including A GREEMENT following the DAMSL annotation scheme. On 19 dialogues from the TRAINs corpus (discussions related to solving transportation problems), Core reports an accuracy of 70% for both the Agreement and the Understanding DA classes, using only the previous utterance X  X  DAMSL tag as a feature in the decision trees. This use of DA context in classifying ACWs would appear to be promising, assuming an accurate automatic classification of all DAs in the corpus.
 text-based features for automatically predicting eight different speech acts derived from a taxonomy called Verbal Response Modes (VRM). The experiments are conducted on transcripts of 1,368 utterances from 14 dialogues in English. For the Acknowledg-ment speech act (which  X  X onveys receipt of or receptiveness to other X  X  communication; simple acceptance, salutations; e.g., yes  X  [page 37]), the classifier yields an F-measure of 0.75.
 in their methodology, and in the domain they examine. However, we expect this brief summary to serve as a general frame of reference for our own classification results. 6. Discussion
In this work we have undertaken a comprehensive study of affirmative cue words, a subset of cue phrases such as okay , yeah ,or alright that may be utilized to convey as many as ten different discourse/pragmatic functions, such as indicating continued attention to the interlocutor or cueing the beginning of a new topic. Considering the high frequency of ACWs in task-oriented dialogue, it is critical for some spoken language processing applications such as spoken dialogue systems to model the usage of these words correctly, from both an understanding and a generation perspective. of the various discourse/pragmatic functions of ACWs. The most notable contrasts in acoustic/prosodic features relate to word final intonation and word intensity. Backchan-nels typically end in a rising intonation; agreements and cue beginnings, in a falling intonation. Cue beginnings tend to be produced with a high intensity, and cue endings with a very low one. Other acoustic/prosodic features X  X uration, mean pitch, and voice quality X  X lso seem to vary with the word usage. Our findings related to final intonation are consistent with previous results obtained by Hockey (1993) and Jurafsky et al. (1998) for American English. For Scottish English, Kowtko (1996) reports a non-agreement function. Kowtko also reports observing all types of final intonation in her  X  X cknowledge X  function, whose definition overlaps both our agreements and backchan-nels. Thus, we find no apparent contradictions between Kowtko X  X  results for Scottish English and ours for American English.
 includes instances conveying each of the ten identified meanings, and this item shows the highest degree of variation along the acoustic/prosodic features we have examined. 28
We speculate from this finding that the more ambiguous an ACW, the more a speaker needs to vary acoustic/prosodic features to differentiate its meaning.
 positional differences across functions, such as the position of the word in its con-versational turn, or whether the word is preceded and/or followed by silence. Such large differences bring support to Novick and Sutton X  X  (1994) claim that the discourse/ pragmatic role of these expressions strongly depends on their basic structural context.
For example, in Novick and Sutton X  X  words, an ACW in turn-initial position is  X  X learly not serving as a prompt for the other speaker to continue X  (page 97).
 as now , well ,or like , present the problem as a binary classification task: Each cue word has either a discourse or a sentential sense (e.g., Litman 1996; Zufferey and Popescu-Belis 2004). In the study of automatic classification of ACWs presented in Section 5 we show that for spoken task-oriented dialogue, the simple discourse/sentential distinction is insufficient. In consequence, we define two new classification tasks besides the general task of classifying any ACW into any function. Our first task, the detection of an acknowledgment function, has important implications for the language management component in spoken dialogue systems, which must keep track of which material has reached mutual belief in a conversation (Bunt, Morante, and Keizer 2007; Roque and
Traum 2009). Our second task, the detection of a discourse segment boundary func-tion, should help in discourse segmentation and meeting processing tasks (Litman and
Passonneau 1995). Our SVM models based on lexical, discourse, timing, and acoustic features approach the error rate of trained human labelers in all tasks, while our auto-matically computed phonetic features offer no improvement. Previous studies indicate that the pragmatic function of ambiguous expressions may be effectively predicted by models that combine information extracted from various sources, including lexical and prosodic (e.g., Litman 1996; Stolcke et al. 2000). Our results support this, and extend the list of useful information sources to include discourse and timing features that may be easily extracted from the time-aligned transcripts.
 nations of feature sets, in an attempt to simulate the conditions of different applications. Models that are trained using features extracted only from the speech signal up to the
IPU containing the target word simulate on-line applications such as spoken dialogue systems with access to acoustic/prosodic features. Although such models perform worse than  X  X ff-line X  models, which make use of left and right context, they still sig-nificantly outperform our baseline classifiers. Models that simulate the conditions of current spoken dialogue systems with access only to lexical features (although perhaps errorful) and TTS systems synthesizing spoken conversations, which have access only to features extracted from the input text, also significantly outperform our baseline classifiers.
 pear to contain very few instances of backchannel responses from either conversational partner. On the system X  X  side, the absence of this important element of spoken com-munication may be due to the difficulty of detecting appropriate moments where a backchannel response would be welcome by the user. Recent advances on that research topic (Ward and Tsukahara 2000; Cathcart, Carletta, and Klein 2003; Gravano and
Hirschberg 2009a) have encouraged research on ways to equip systems with the ability to signal to the user that the system is still listening (Maatman, Gratch, and Marsella 2005; Bevacqua, Mancini, and Pelachaud 2008; Morency, de Kok, and Gratch 2008) X  for example, when the user is asked to enter large amounts of information. On the user X  X  side, an important reason for not backchanneling may lie in the unnaturalness of such systems, often described as  X  X onfusing X  or even  X  X ntimidating X  by users, as well as their inability to recognize backchannels as such. Nonetheless, recent Wizard-of-Oz experiments conducted by Hjalmarsson (2009, 2011) show that humans appear to react to turn-management cues produced by a synthetic voice in the same way that they react to cues produced by another human. This important finding suggests that users of spoken dialogue systems could be cued to produce backchannel responses, for example to determine if they are still paying attention. In that case, it will be crucial for systems to be able to distinguish backchannels from other pragmatic functions (Shriberg et al. 1998). In Section 5.5 we present results on the task of automatically identifying backchannel ACWs from the other possible functions. Our models improve over the baseline in an off-line condition (e.g., for meeting processing tasks), but fail to do so in an on-line setting (e.g., for spoken dialogue systems). Practically all of the confusion of this on-line model comes from misclassifying agreements ( Agr ) as backchannels ( BC ) and vice versa. The reliability of our human labelers for distinguishing these two classes was measured by Fleiss X  X   X  at 0.570, a level considerably lower than the 0.745 achieved for the general labeling task, which indicates that the backchannel identification task is difficult for humans as well, at least when they are not engaged in the conversation itself but only listening to it after the fact. Although we asked our annotators to distinguish the agreement function of ACWs from  X  X ontinued attention, X  there are clearly cases where people disagree about whether speakers are indicating agreement or not. In future research we will investigate this issue in more detail, given the relevance of on-line identification of backchannels in spoken dialogue systems.
 affirmative cue words in a large corpus of SAE task-oriented dialogue. The corpus on which our experiments were conducted, rich in ACWs conveying a wide range of discourse/pragmatic functions, has allowed us to systematically investigate many dimensions of these words, including their production and automatic disambiguation.
Besides the value of our findings from a linguistic modeling perspective, we believe that incorporating these results into the production and understanding components of spoken dialogue systems should improve their performance and increase user satisfac-tion levels accordingly, getting us one step closer to the long-term goal of effectively emulating human behavior in dialogue systems.

The C OLUMBIA G AMES C ORPUS is a collection of 12 spontaneous task-oriented dyadic conversations elicited from native speakers of Standard American English. The cor-pus was collected and annotated jointly by the Spoken Language Group at Columbia
University and the Department of Linguistics at Northwestern University. In each of the 12 sessions, two subjects were paid to play a series of computer games requiring verbal communication to achieve joint goals of identifying and moving images on the screen. Each subject used a separate laptop computer and could not see the screen of the other subject. They sat facing each other in a soundproof booth, with an opaque curtain hanging between them, so that all communication was verbal. The subjects X  speech was not restricted in any way, and it was emphasized at the session beginning that the game was not timed. Subjects were told that their goal was to accumulate as many points as possible over the entire session, since they would be paid additional money for each point they earned. 30 A.1 Game Tasks Subjects were first asked to play three instances of the C shown cards with one to four images on them. Images were of two sizes (small or large) and various colors, and were selected to contain primarily voiced consonants, which facilitates pitch track computation (e.g., yellow lion , blue mermaid ). There were two parts to each Cards game, designed to vary genre from primarily monologue to dialogue. 10 cards (Figure A1a). Player A was asked to describe the top card on her pile, while
Player B was asked to search through his pile to find the same card, clicking a button when he found it. This process was repeated until all cards in Player A X  X  deck were matched. In all cases, Player B X  X  deck contained one additional card that had no match in Player A X  X  deck, to ensure that she would need to describe all cards.
 screen (Figure A1b), all initially face down. As the game began, the first card on one player X  X  (the D ESCRIBER  X  X ) board was automatically turned face up. The Describer was told to describe this card to the other player (the S matching card from the cards on his board. If the Searcher could not find a card exactly matching the Describer X  X  card, but could find a card depicting one or more of the objects on that card, the players could decide whether to declare a partial match and receive points proportional to the numbers of objects matched on the cards. At most three cards were visible to each player at any time, with cards seen earlier being automatically turned face down as the game progressed. Players switched roles after each card was described and the process continued until all cards had been described. The players were given additional opportunities to earn points, based on other characteristics of the matched cards, to make the game more interesting and to encourage discussion. a final game, the O BJECTS game. As in the Cards game, all images were selected to have likely descriptions which were as voiced and sonorant as possible. In the Objects game, each player X  X  laptop displayed a game board with 5 to 7 objects (Figure A1c).
Both players saw the same set of objects at the same position on the screen, except for one (the TARGET ). For the D ESCRIBER , the target object appeared in a random location among other objects on the screen; for the F OLLOWER , the target object appeared at the bottom of the screen. The Describer was instructed to describe the position of the target object on her screen so that the Follower could move his representation to the same location on his own screen. After players negotiated what they believed to be their best location match, they were awarded 1 to 100 points based on how well the Follower X  X  target location matched the Describer X  X .
 the subjects always acted as the Describer, and the other one as the Follower. In the following four tasks their roles were inverted: The subject who played the Describer role in the initial four tasks was now the Follower, and vice versa. In the final six tasks, they alternated the roles with each new task.
 A.2 Subjects and Sessions Thirteen subjects (six women, seven men) participated in the study, which took place in
October 2004 in the Speech Lab at Columbia University. Eleven of the subjects partici-pated in two sessions on different days, each time with a different partner. All subjects reported being native speakers of Standard American English and having no hearing impairments. Their ages ranged from 20 to 50 years (mean, 30.0; standard deviation, 10.9), and all subjects lived in the New York City area at the time of the study. They were contacted through the classified advertisements Web site craigslist.org . totaling roughly 9 hours of dialogue in the corpus. Of those, 70 minutes correspond to the first part of the Cards game, 207 minutes to the second part of the Cards game, and 258 minutes to the Objects game. Each subject was recorded on a separate channel of a DAT recorder, at a sample rate of 48 kHz with 16-bit precision, using a Crown head-mounted close-talking microphone. Each session was later downsampled to 16 kHz, 16-bit precision, and saved as one stereo .wav file with one player per channel, and also as two separate mono .wav files, one for each player.
 pus and manually aligned the words to the speech signal, yielding a total of 70,259 words and 2,037 unique words in the corpus. Additionally, self repairs and certain non-word vocalizations were marked, including laughs, coughs, and breaths. Intonational patterns and other aspects of the prosody were identified using the ToBI transcription framework (Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994):
Trained annotators intonationally transcribed all of the Objects portion of the corpus (258 minutes of dialogue) and roughly one third of the Cards portion (90 minutes). Appendix B: ACW Labeling Guidelines
These guidelines for labeling the discourse/pragmatic functions of affirmative cue words were developed by Julia Hirschberg,  X tefan Be  X  nu X , Agust X n Gravano, and Michael Mulley at Columbia University.
 Classification Scheme
Most of the labels are defined using okay , but the definitions hold for all of these words: clue about the function of a word, label it as ? . [Mod] Literal Modifiers: In this case the words are used as modifiers. Examples:  X  X  think that X  X  okay . X   X  X t X  X  right between the mermaid and the car. X   X  X eah, that X  X  right . X  32 [Agr] Acknowledge/Agreement: The function of okay that indicates  X  X  believe what you said X , and/or  X  X  agree with what you say. X  This label should also be used for okay after another okay or after an evaluative comment like  X  X reat X  or  X  X ine X  in its role as an acknowledgment. 13 Examples: A: Do you have a blue moon? B: Yeah .
 A: Then move it to the left of the yellow mermaid.

B: Okay , gotcha . Let X  X  see... (Here, both okay and gotcha are labeled Agr .) [CBeg] Cue Beginning: Thefunctionof okay that marks a new segment of a discourse or a new topic. Test: could this use of okay be replaced by  X  X ow X ? [PBeg] Pivot Beginning: (Agr+CBeg) When okay functions as both a cue word and as an
Acknowledge/Agreement. Test: Can okay be replaced by  X  X kay now X  with the same pragmatic meaning? [CEnd] Cue Ending: Thefunctionof okay that marks the end of a current segment of a discourse or a current topic. Example:  X  X o that X  X  done. Okay . X  [PEnd] Pivot Ending: (Agr+CEnd) When okay functions as both a cue word and as an
Acknowledge/Agreement, but ends a discourse segment. [BC] Backchannel: The function of okay in response to another speaker X  X  utterance that indicates only  X  X  X  X  still here / I hear you and please continue. X  [Stl] Stall: Okay used to stall for time while keeping the floor. Test: Can okay be replaced by an elongated  X  X m X  or  X  X h X  with the same pragmatic meaning?  X  X o I yeah I think we should go together. X  [Chk] Check: Okay used with the meaning  X  X s that okay? X  or  X  X s everything okay? X  For example,  X  X  X  X  stopping now, okay ? X  [BTsk] Back from a task:  X  X  X  X e just finished what I was doing and I X  X  back. X  Typical case: One subject spends some time thinking, and then signals s/he is ready to continue the discourse.

Special Cases (1)  X  X kay so X  /  X  X kay now X  /  X  X kay then X  / and so forth, where both words are uttered together, okay seems to convey Agr ,and so / now / then seems to convey CBeg . Because we do not label words like so , now ,or then ,welabel okay as PBeg . (2) If you encounter a rapid sequence of the same word several times in a row, all of them uttered in one  X  X urst X  of breath, mark only the first one the corresponding label, and label the others with  X ? X . Example:  X  okay yeah yeah yeah  X  should be labeled as:  X  okay :Agr yeah :Agr yeah :? yeah :? X . Appendix C: ACW Labeling Examples This appendix lists a number of examples of each type of ACWs from the Columbia
Games Corpus, as labeled by our annotators. Each ACW is highlighted and annotated with its majority label. Overlapping speech segments are embraced by square brackets, and additional notes are given in parentheses.
 A: it X  X  aligned to the f-to the foot of the M&amp;M guy like to the bottom of the iron B: okay Agr lines up A: yeah Agr it X  X  it X  X  almost it X  X  just barely like over B: okay Agr A: the tail B: mm-hm BC A: of the iron B: mm-hm BC A: is past the it X  X  a little bit past the mermaid X  X  body A: when you look at the lower left corner of the iron B: [ okay BC ] A: [where] the turquoise stuff is [and you] B: [ mm-hm BC ] A: know the bottom point out to the farthest left for that region A: the blinking image is a lawnmower B: okay BC A: and it X  X  gonna go below the yellow lion and above the bl-blue lion B: mm-hm BC A: the bottom black part is almost aligned to the white feet of the M&amp;M guy B: [ okay Agr ] A: [ yeah PEnd ] (end-of-task) A: okay CBeg um the blinking image is the iron A: okay CBeg it X  X  uh the l-I guess the lime that X  X  blinking A: nothing lined up real well B: yeah Agr that X  X  right Mod A: that was good okay CEnd A: that X  X  awesome B: you X  X e still the ace alright CEnd A: his beak X  X  kinda orange right Chk B: uh-huh Agr
A: you can X  X  see any of that 34
A: that X  X  like a smaller amount than it is on the right Mod B: [ right Agr ] A: okay Agr A: the lower right Mod corner B: yeah Agr the lower right Mod corner A: let X  X  start over B: okay Agr A: okay PBeg so you have your crescent moon A: but not any of the yellow [part] B: [ okay PBeg ] so would the top of the ear be aligned to like where
A: the like head of the lion to like the where the grass shoots out there X  X  that X  X  a significant
B: okay PBeg so there X  X  definitely a bigger space from the blue lion to the lawnmower than there A: alright ? I X  X l try it (7.81 sec) okay BTsk B: okay CBeg the owl is blinking A: that thing is gonna be like (0.99 sec) okay Stl (0.61 sec) one pixel to the right Acknowledgments References 36 38
