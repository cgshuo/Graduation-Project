 1. Introduction
In information retrieval (IR), one generally distinguishes between retrieved and non-retrieved items on the one hand and between relevant and non-relevant items on the other. Very generally, the goal of any IR method is to retrieve only relevant items  X  often, but not always, as many as possible. There exists a large collection of measures to evaluate the performance of a given method. In the context of this article, we assume binary relevance, where an item is either relevant or not relevant. The two best known IR evaluation measures are precision and recall. Precision is defined as where ret denotes the set of retrieved items and rel denotes the set of relevant items. Recall is defined as
Precision and recall are generally regarded as complementary ( Buckland &amp; Gey, 1994 ). In other words, one needs both to adequately assess the performance of a retrieval system. The reasons are well-known: it is, for instance, easy to achieve max-imum recall by simply retrieving all items available in the system.

Several derived measures have been proposed that summarize precision and recall into one single number. Here, we study one of them, the so-called F -measure or F -score. The F -score is defined as the harmonic mean of precision and recall:  X  either precision or recall is low, F will be low as well; this is not the case when using the arithmetic or geometric mean.
To the best of our knowledge, the foundation for the F -score was laid by van Rijsbergen (1974) and van Rijsbergen (1979) , who introduced an IR effectiveness measure E (p. 128). The measure E also occurs in (Salton &amp; McGill, 1983, p. of the sets ret and rel: precision. Eq. (3) uses F 1 with b =1or a = 1/2, such that precision and recall have the same weight.
 a relevance score, such that the items considered most likely to be relevant are presented first to the user. One may thus construct a curve of any evaluation measure as a function of the number of retrieved items t = |ret|. In this article we con-
F -score curves have a remarkable shape: a sharp concave increase, followed by a longer  X  usually convex  X  decrease. The maximum F -score is thus reached where the increase becomes a decrease; we refer to this maximum as the tipping point . Thus, if one accepts the F -score as a valid retrieval evaluation measure, the optimal situation occurs at the tipping point. The theoretical results offer a partial explanation of the empirically found regularities.
 f  X  is always below.

IR and link prediction experiments. In Section 3 , the shape of the F -score curves is partially explained using a theoretical we present the conclusions. 2. Empirical results the same table can also be used for other applications, it is possible to apply R and P (and hence F ) outside information re-cation, where we first encountered the regularities described in this paper. Similar results would likely have been obtained from other applications of these measures. In other words, although the paper mainly uses IR terminology, one should keep in mind that these measures can be used outside IR as well. 2.1. Information retrieval experiment from a usually large repository of unstructured and heterogeneous documents, such as the Web. Our retrieval experiment uses a total of 100 queries from the Text Retrieval Evaluation Conference (TREC) Web track (queries 1 X 50 from 2009 and egory B data set, which contains approximately 50 million web pages in English crawled between January and February 2009. different retrieval models from the language modeling (LM) framework ( Croft &amp; Lafferty, 2003 ): (i) LM with Dirichlet smoothing, (ii) LM with Jelinek X  X ercer smoothing, and (iii) LM with two-stage Dirichlet and Jelinek X  X ercer smoothing. Dirichlet includes a parameter l , and Jelinek X  X ercer includes a parameter k . We tune these parameters using 5-fold validation separately for each query set on Zhai and Lafferty X  X  (2002) tuning ranges: l 2 {100,500,800,1000,2000,3000, 4000,5000,8000,10,000}, k 2 {0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99}. For all results, we record the number of relevant documents, as well as the number of retrieved items that are relevant at each step of 10 retrieved documents (up to 1500). This enables us to determine the evolution of F as a function of the number of retrieved items. Query number 95 was excluded from the subsequent analysis, because no documents were assessed relevant for this query. In the case of query number 40, only 128 documents were retrieved, because that query contains one very rare word ( X  X  X ichworks X  X ).
It is practically difficult to show resulting F -score curves for all 100 information needs in the three retrieval models. We therefore give an overview of the variety of results and some overall properties. Fig. 1 shows the curve of the F -score as a function of the number of retrieved items for five queries (Q19, Q35, Q46, Q74, and Q86) using LM with Dirichlet smoothing. The five queries have been selected to represent the broad variety of resulting curves. Q35 and Q46 first exhibit a concave increase to a maximum, followed by a largely convex decrease. Q35 is slightly less regular than Q46, in that the first part of clear local maxima. Q19 X  X  curve is again similar, although the decreasing part seems to remain virtually horizontal. Never-nally, Q74 does not really obey the expected pattern (according to the theoretical analysis, Section 3 ), mostly because initially the F -score remains at zero. Curves like the one for Q74 occur mainly in those cases where the number of relevant documents is very small. The tipping points in Fig. 1 are reached at different numbers of retrieved documents, which is due to the fact that each curve corresponds to a separate query.

We do not show the curve of the F -score as a function of the number of retrieved items for the above five queries using LM with two-stage smoothing, because it is very similar to Fig. 1 . However, Jelinek X  X ercer smoothing yields curves that overall in the two Figures. However, in most cases the maximum F -score is lower for Jelinek X  X ercer smoothing than for the other two models.
 in Section 3 . However, it can be seen that curves of individual queries may deviate from the expected shape. This is mostly items  X  although ideally it is.

The experiment also reveals that maximum F -scores may vary a lot across different queries. Fig. 3 illustrates the variety across all queries. Since the results are approximately normally distributed, the mean or median of maximum F -scores can be is the mean of R -Precisions for each individual query in the run.

Another approach would be to create an aggregated F -score curve by taking the average over all queries for each number is, however, unclear whether this is indeed a viable and methodologically sound approach. 2.2. Link prediction experiment
We briefly introduce the link prediction application. Essentially, link prediction tries to predict the future state of an ing network, which is the input to the prediction process. The prediction constructed on the basis of the training network is simply known as the predicted network. To assess the quality of the prediction, one can compare the predicted network with an actual later snapshot, called the test network. Predicted and unpredicted links are analogous to retrieved and non-re-trieved items respectively; links present and not present in the test network are analogous to relevant and non-relevant items respectively. Because some links are more likely than others, we can order them in decreasing order of likelihood, sim-ilar to relevance ranking. Taken together, these elements imply that link prediction can be evaluated using IR evaluation measures, such as precision and recall.
 berg, 2007 ). All three cases clearly show a steep concave increase to a maximum, followed by a slower decrease. The decrease is completely convex for the Graph distance and Katz predictors; Simrank X  X  decreasing part, however, appears to start out concave and then turn into convex. Overall, the curves for link prediction are more regular than those for IR for this data set, an observation partially due to the different characteristics of the ClueWeb09 and AcadBib data sets. there can be little doubt that Simrank is outperformed by the other two, since the former X  X  curve is below the others X  throughout the chart. Graph distance is generally very similar to Katz, but reaches a slightly lower tipping point. Note that observation.

Similar results can be obtained using other predictors and data sets. We point out that Fig. 5 is truncated on the right, such that the environment around the tipping point is more clearly visible; the same convexly decreasing trend actually continues up to about 600,000 predictions. Some predictors  X  such as those that are based on common neighboring nodes  X  yield only a limited number of predictions. In those cases, it is possible that the curve ends before the tipping point is reached. 3. Theory: the F -score curve in different retrieval situations In this section, we address the question why specific shapes such as those found in Fig. 1, 4 and 5 occur.
Egghe (2008) presents curves of the measures precision, recall, fallout and miss for some generalized retrieval situations, which were originally introduced by Buckland and Gey (1994) . One can distinguish between perverse retrieval (first return all dom retrieval (randomly return items without regard for their relevance), and normal retrieval (the situation where the den-one tries to return only relevant items but inevitably makes some errors. Perverse, perfect and random retrieval are not very realistic, but they are theoretically valuable because they represent extreme cases of IR.

We will build upon Egghe X  X  results to describe the F -score curve for these retrieval situations. First, we briefly summarize the theoretical framework we are working in (see Egghe (2008) for full details). We have t = |ret| retrieved items. We assume a given query or IR problem, such that the number of relevant items, denoted as  X  = |rel|, is fixed. The entire system has N function H is differentiable and denote H 0 = h . It follows that Here, h is the retrieval density function, specifying the density of relevant items at each number of retrieved items t . In the continuous setting of Egghe (2008) , the F -score is then equal to 1 Note, again, the equivalence of the F -score (7) with the Dice coefficient (4) .
 Since And since  X   X  3.1. Perverse retrieval the relevant ones.
 3.2. Perfect retrieval
Where t =  X  , the curve is continuous, but not differentiable. 3.3. Random retrieval The F -score curve is then described by:
This yields a curve as shown in Fig. 6 . 3.4. Normal retrieval
Egghe (2008) defines normal retrieval as the retrieval situation where the following conditions hold.
Furthermore, the density of relevant items decreases as one retrieves more items: Informally, Eq. (18) states that there are more relevant items among higher ranked items than among lower ranked ones. This is a very natural requirement for normal retrieval situations, related to van Rijsbergen X  X  (1974) concept of decreasing marginal effectiveness. In theory it should not matter if one trades an increase in precision (or recall) for an equally sized decrease in recall (or precision). In reality however, there is no perfect trade-off: the marginal effectiveness of precision (or recall) is decreasing as one retrieves more items. Assumptions (16) X (18) can be interpreted as a consequence of the prob-items is a decreasing function of t .

To the three assumptions (16) X (18) we add the extra assumption that there is at least one relevant item in the system:
This assumption is needed to avoid some cases of division by zero. Note that this assumption is also needed if one wants to avoid division by zero in the definition of recall. We remark that some of Egghe X  X  calculations also rely on the assumption h  X  X ( t ) &lt; 0; since this assumption is not needed for our purposes, we do not use it here.
 We now consider the question of the curve X  X  shape. The first derivative is: The sign of F 0 ( t ) is determined by the expression between brackets. We now examine the start and end of the curve. As t approaches 0, h ( t ) approaches 1. This follows from (16) and the continuity of h . Moreover, as t approaches 0, approaches 0 (assuming (19) ). So for small t , we have Hence, F 0 ( t ) &gt; 0 and F ( t ) is increasing.
 At the end of the curve, t approaches N . Here, h ( t ) approaches 0 and as follows from (19) . For large t , we have Hence, F 0 ( t ) &lt; 0 and F ( t ) is decreasing.
 point. Note, however, that we have not been able to show that there exists exactly one such maximum. It is theoretically possible that there is more than one maximum.
 We now consider the second derivative of F ( t ). From (20) , it follows: and F ( t ) ends concavely iff 4. Discussion concavely and then decreases, and has a maximum. However, some aspects of the empirically found curves are not explained by the model. These are: (1) in most cases, the curve appears to decrease convexly rather than concavely, (2) the decreasing predictors) reach a maximum around the same value of t (number of retrieved documents or predictions).
 the results of normal retrieval will (to a certain extent) resemble the results of perfect retrieval. Indeed, the three unex-plained phenomena can also be found in the case of perfect retrieval. First, the decreasing part of a perfect retrieval curve is always convex. Second, the tipping point of the perfect retrieval curve is reached when  X  items have been retrieved. Since empirical data). Note that the other two extreme cases, random and perverse retrieval, reach a maximum only when t = N  X  retrieval.
 tipping point. For this reason, and because the tipping point occurs around the same value of t (and close to  X  ) we suggest that, if one wants to compare F -scores in a ranked retrieval situation, one can best compare the respective tipping points. tence of one individual query, rather than (as is customary in IR) aggregating over many queries. Moreover, it may be the case that observed rankings for individual queries are quite different from the continuous model  X  especially for those cases original work; the model is certainly a simplification but it has the advantage of highlighting some generally expected reg-complete solution, but possible approaches to these problems include determining the maximum F -score for each query and aggregating over that using a simple mean, analogous to mean average precision (MAP) or constructing an aggregated F -score curve and finding its maximum. More research would be necessary to determine the best approach.

There exists a significant amount of IR literature on modeling the distributions of scores of relevant and non-relevant doc-process. From the early work of Swets (1963) , who proposed to model score distributions to find an optimal threshold for separating relevant from non-relevant documents, to the more recent work of Dai, Kanoulas, Pavlu, and Aslam (2011) , who used score distributions models for inferring precision-recall curves, various combinations of statistical distributions have been proposed; some focus more on their goodness of fit to some set of empirical data, while others consider more their distributions. First, we do not take scores and their distributions directly into account, only the resultant ranking. Second, since we make only general assumptions and do not try to fit any particular score distribution, our theoretical part is rather studied here. 5. Conclusions
We have shown empirically that the F -score as a function of the number of retrieved items typically has a distinct shape with a clear maximum  X  the tipping point. Since the same shape occurs for different applications, data sets and IR strategies, that other IR performance measures may yield other results.

The empirical findings have spurred a theoretical investigation within the continuous framework of Egghe (2008) . This retrieval. It can be shown that perfect and normal retrieval lead to the distinct shape with a tipping point, which was found empirically. Interestingly, the model seems to allow for a greater variety of shapes than found empirically. The empirical re-sults are qualitatively closer to perfect retrieval than to random or perverse retrieval.

We close with some suggestions for future research. It would be interesting to examine F -score curves for more data sets to get a better view of possible variations that our experiments may have missed. Are there, for instance, empirical results that do yield a concavely ending F -score curve? Also, there exist other derived IR performance measures. What is their behavior as a function of the number of retrieved items? Finally, an important but difficult problem remains the question how a system should estimate the point at which F (or another performance measure) is maximized, if no relevance infor-mation is available.
 Acknowledgements
We are grateful to Leo Egghe for his suggestions and his help regarding Section 3.4. Ronald Rousseau and two anonymous reviewers provided useful comments, which helped to improve the paper.
 References
