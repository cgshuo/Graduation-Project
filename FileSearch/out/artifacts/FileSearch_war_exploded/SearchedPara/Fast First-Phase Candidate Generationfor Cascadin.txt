 Current search engines use very complex ranking functions based on hundreds of features. While such functions return high-quality results, they create efficiency challenges as it is too costly to fully evaluate them on all documents in the union, or even intersection, of the query terms. To address this issue, search engines use a series of cascading rankers, starting with a very simple ranking function and then ap-plying increasingly complex and expensive ranking functions on smaller and smaller sets of candidate results. Researchers have recently started studying several problems within this framework of query processing by cascading rankers; see, e.g., [5, 13, 17, 51].

We focus on one such problem, the design of the initial cascade. Thus, the goal is to very quickly identify a set of good candidate documents that should be passed to the sec-ond and further cascades. Previous work by Asadi and Lin [3, 5] showed that while a top-k computation on either the union or intersection gives good results, a further optimiza-tion using a global document ordering based on spam scores leads to a significant reduction in quality. Our contribu-tion is to propose an alternative framework that builds spe-cialized single-term and pairwise index structures, and then during query time selectively accesses these structures based on a cost budget and a set of early termination techniques. Using an end-to-end evaluation with a complex machine-learned ranker, we show that our approach finds candidates about an order of magnitude faster than a conjunctive top-k computation, while essentially matching the quality.
Search engines are continuously optimizing their ranking functions in order to improve result quality. This is usually achieved through more and more complex ranking functions based on large sets of features, including features derived from text, link structure, past queries, and online or propri-etary data sets and knowledge bases through various data extraction and mining techniques. However, these complex ranking functions create significant performance challenges for the engines, as evaluating them on large numbers of doc-uments is very expensive. Given the billions of queries that have to be processed each day, it would not be feasible to apply such functions directly on all documents passing the initial Boolean filter, even in the conjunctive case.
To address this challenge, all current major engines appear to be using a cascading approach to query processing that approximates the results of such ranking functions through a series of increasingly more complex intermediate functions. Thus, after query analysis and rewriting, the search engine first applies a very simple ranking function, similar to BM25 or a Cosine measure, that only uses a few features and can be very efficiently applied on large numbers of candidates. We refer to this process as the first cascade. Next, in the sec-ond cascade, a somewhat more complex function based on a larger set of features is applied to, say, the top few thousand results from the first cascade. In the third and further cas-cades, even more complex functions are applied to smaller and smaller sets of surviving candidates of the previous cas-cades. This approach was described and formalized in [51].
Thus, the goal of the cascading approach is to return (al-most) the same results as we would get from applying the complex function on all candidates, at a fraction of the com-putational cost. Its proper implementation, however, poses several challenges that have recently received some atten-tion in the literature [5, 13, 17, 32, 40, 51]. In particular, work has focused on four distinct challenges: (1) How to design good sequences of increasingly complex ranking func-tions and associated cutoffs (number of results kept for the next cascade) [51]; (2) how to efficiently apply a complex ranking function to candidates by using early-exit strategies [13]; (3) how to design ranking functions for the first cas-cades that preserve many good candidates for subsequent cascades, as opposed to focusing on how to order a few top results [17]; (4) how to implement the first cascade ef-ficiently through an optimal choice of Boolean filters and various early-termination techniques [3, 5].

We focus on the last challenge, which is important as the first cascade is executed on large numbers of candidates. The results in [3, 5] indicated that for the first cascade, a conjunctive filter does essentially as well as a disjunctive one, while saving a lot of time. However, a naive attempt to use a global document ordering to avoid a complete conjunctive traversal of the index structures resulted in significant losses in end-to-end result quality.

In this paper, we propose an approach that runs about an order of magnitude faster than even highly optimized con-junctive and disjunctive top-k computations, while achieving essentially the same end-to-end result quality. More pre-cisely, given a complex ranking function that needs to be approximated by a cascading approach, an inverted index structure for a document collection, and a training query trace, we show how to build an auxiliary layer of index struc-tures, and how to select which parts of this layer to consult on a submitted query, in order to obtain high-quality candi-dates with a limited computational and space budget.
In particular, our contributions are as follows: 1. We design a framework that builds specialized single-2. We propose an online selection algorithm based on a 3. We provide an end-to-end evaluation of our proposed
The remainder of the paper is organized as follows: Sec-tion 2 presents some background and discusses related work. In Section 3 we give a high-level overview of our approach, and present our solutions for several components. Next, Section 4 outlines the experimental setup, and Section 5 presents the experimental evaluation of the proposed frame-work. Finally, Section 6 provides some concluding remarks.
In this section, we provide some background on inverted indexes, query processing and early termination techniques, and cascade ranking architectures, and discuss related work.
Inverted Indexes: Commercial search engines perform query processing based on the widely used inverted index [58]. Given a collection of N documents, each document is assigned a unique identifier (docID) from 0 to N -1. The inverted index consists of a set of inverted lists and a lexi-con. In particular, for each distinct term t in the collection, there is an inverted list L t . Each L t is a list of postings specifying the documents that t appears in. Typically, each posting contains the docID of a document containing t and the frequency of t in the document; however, there may also be other information, such as the positions of the term oc-currences in the document, or a precomputed impact score. The lexicon contains for each unique term in the collection a pointer to the corresponding inverted list. Inverted index compression is crucial for search engine performance and many techniques have been proposed [6, 45].

Index Layout: There are many ways to organize the in-verted lists. In document-sorted indexes, each list is sorted by docID, resulting in small delta gaps (d-gaps) between consecutive docIDs that lead to a smaller compressed size [45]. Impact-sorted indexes sort the postings in each list in decreasing order of impact scores. Thus, high-scoring query results tend to be located near the front of the lists, poten-tially enabling a smart query processing algorithm to skip most of the rest of the list. However, this approach leads to poor compressibility compared to document-sorted indexes, and may require random lookups into lists for docIDs that score high on one query term but low on others. Finally, impact-layered indexes split each inverted list into a small number of layers based on impact scores. Our approach uses two layers, where the first layer of high-impact postings is sorted by impact, and the second layer by docID to allow for efficient random lookups.

Index Traversal: During query processing, index struc-tures can be traversed in different ways [50]. In Document-at-a-time (DAAT), each list has a pointer to a current post-ing and one document is processed at a time; then pointers are moved forward in docID space. The top results are usu-ally maintained in a min-heap structure. In Term-at-a-time (TAAT) traversal, a list is fully traversed before accessing the next one. Partially scored documents are kept as accu-mulators in a hash table or other structure; this structure can be a bottleneck if it grows beyond the CPU caches. Lastly, there are hybrids between DAAT and TAAT. Note that DAAT is mainly suitable for document-sorted indexes, while TAAT works well with impact-sorted ones.
The simplest form of query processing applies a Boolean filter (AND/OR) on the inverted lists of the query terms, and then ranks all documents passing the filter. A good ranking function should (a) provide a good approximation of the relevance of a document with respect to a query and (b) be efficiently computable using the information stored in the index. Well-known examples include Cosine measures and BM25 [6]. Most of these simple ranking schemes have the property that the score of a document d for the full query q is the sum (or other simple combination) of per-term scores; i.e., score ( q,d ) = P t  X  q s ( t,d ), where s ( t,d ) is the impact score of term t in d .

Early Termination: A na  X   X ve query processing approach is inefficient, and ends up decompressing and accessing large parts of the inverted lists. To improve on this, researchers have proposed many early termination (ET) algorithms that try to find good results while accessing and scoring only a small part of the relevant inverted lists. ET algorithms are called safe if they always return the same results as an exhaustive algorithm, and unsafe otherwise [48]. ET tech-niques are widely used in commercial engines and academic systems, and include the following approaches:
Our approach here is unsafe, and based on dynamic prun-ing with impact-layered structures for both terms and term pairs (intersections of two terms), as described later.
In modern commercial engines, query processing is based on cascading ranking schemes [10, 13, 51], where each cas-cade includes a ranker that provides candidate documents to subsequent cascades. The first cascade is usually based on a very simple ranking function that is evaluated on large parts of the index structure to get an initial set of candi-dates. Thus, this function must be very fast, while providing a reasonably high-quality set of candidates. Subsequent cas-cades are executed on fewer candidates using more complex and expensive ranking functions. The challenge in designing such cascading architectures is to select a set of cascades and associated ranking functions that achieves high end-to-end quality at low cost.

Cascading setups are crucial for the performance and qual-ity of modern commercial engines, and a number of papers have recently focused on this setup [4, 5, 13, 17, 32, 40, 51]. We focus on optimizing the efficiency of the first cascade in such architectures, a problem recently studied in [4, 5].
We now discuss the relationship of our approach to pre-vious work. The particular problem we consider is based on the setup in [5, 4, 17]. Thus, we have a two-phase cas-cading architecture, where the first phase obtains an initial candidate set of, say, several hundred or thousand docu-ments, while the second phase reranks these candidates us-ing a more sophisticated ranker based on dozens or hundreds of features. Our goal is to design very fast ET algorithms for the first phase that achieve end-to-end quality compara-ble to more exhaustive approaches. This is essentially the problem addressed by Asadi and Lin in [4, 5].

In particular, [5] investigates the efficiency/effectiveness trade-off for various first-phase candidates generation ap-proaches. They experiment with conjunctive WAND [10], disjunctive WAND, and two conjunctive algorithms that first obtain the intersection and then rerank results based on BM25 or spam score, and conclude that conjunctive WAND provides the best trade-off. Work in [4] shows how to accel-erate the intersection-based approaches using Bloom Filters. Our contribution here is to provide a method that achieves quality comparable to their best methods at lower cost.
Another relevant recent work [52] proposes a document prioritization method for selective evaluation of documents that achieves a better efficiency/effectiveness balance in the first cascade. The running times reported in [52] are sig-nificantly slower than ours, though some of the ideas could potentially be used to further optimize the lookup phase of our approach.

Our algorithm is based on a layered index organization and performs a limited-depth access to impact-sorted single-term and term-pair structures, followed by random lookups. As such, it is closely related to the well-know FA algo-rithm proposed by Fagin [23], and also to ET algorithms for impact-layered indexes introduced in [39]. There are many subsequent papers that further developed and often com-bined these approaches to solve various IR ranking problems, including, e.g., [2, 7, 9, 33, 47, 48].

We note that [28, 29] describe a number of access and lookup strategies for top-k query processing in database ap-plications. One algorithm that is somewhat similar to our approach is the MPro algorithm in [28], which seeks to min-imize lookup costs through sorted access. Work in [48] sug-gested methods for selecting the access depth into the avail-able single-term impact-sorted lists. However, [48] focused only on single-term impact-sorted lists and did not provide cost-based query processing algorithms in the context of cas-cading rankers. Our approach is different from both of the above as we are proposing a framework for constructing ad-ditional impact-sorted index structures, including pairwise structures, based on query term distributions and posting quality models and subject to space constraints, plus an on-line depth selection algorithm and lookup strategies.
Our approach relies heavily on term-pair index structures, introduced in [34] and subsequently studied in a number of papers such as [11, 15, 26, 30, 44, 54, 56, 57]. Our work is most closely related to [30], which also applies the approach in [23] to pairwise structures. The main difference is our focus on cascading ranking schemes, and our framework for optimizing index structures and index traversals based on a limited space and access cost budget.

Also related is the work in [11], which proposes building single and pairwise impact-sorted lists that are then com-pletely accessed during query processing. Though related, our work is different in two ways: While [11] and earlier work in [44, 54, 56, 57] assume a ranking function that directly takes proximity into account, we assume a more complex function in a cascading setup. Also, the resulting auxiliary structures in [11] are much larger than the basic index size; in contrast, our query language and posting quality models allow us to achieve high speed with only a limited increase in size. We note that our results could potentially be improved by adding special pairwise structures for terms occurring close to each other in a document, as done in [11, 54].
Finally, [1, 27] study techniques for learning better index structures given a set of documents and queries. In par-ticular, [27] can be seen as essentially learning an ordering of index postings that is better than the  X  X atural X  impact-based ordering used, e.g., in [23]. We note that this issue is orthogonal to our approach, and could be combined to possi-bly yield additional benefits. Lastly, while we construct our first-layer structures using off-line preprocessing, one could also approach this via a suitable caching mechanism, such as those in [25, 38, 46] for other types of structures. Finally, our improvements are in addition to any speedups achieved through result caching, since our query traces do not have significant numbers of repeated queries.
We now define and discuss our problem setup, give a high-level description of our approach, and then provide more details about the various steps that are involved.
We are given a complex ranking function CF , a simple ranking function SF used as the first cascade, and a rank cutoff c for the first cascade, meaning that only c results from the first cascade will be evaluated by the complex ranking function, which will then return the top k , k  X  c , results to the user. Our goal is to implement the first cascade to run as fast as possible without significantly decreasing end-to-end result quality. In our implementation, we allow unsafe early termination techniques, that is, the c results we give to CF may be different than those obtained from an exhaustive top-c computation using SF .

We measure quality in two ways: (i) Overlap@(k,c) , mean-ing how many of the top-k results that would be returned by an exhaustive application of CF (i.e., c cf =  X  or at least fairly large) are returned with our first cascade implemen-tation that evaluates c candidates and (ii) NDCG@k , which is the normalized discounted cumulative gain that considers the order of the results.

Problem Discussion: Note that while the above defini-tion assumes only two cascades, SF and CF , this does not really limit our approach as long as any additional interme-diate cascades do a good job at approximating CF , i.e., do not lose too many good results among those nominated by SF for further processing. We believe this is a reasonable assumption in practice, and assume that subdividing the second cascade into further cascades is a separate problem. For the same reason, the reported running times are only for the first cascade, as the cost of the second cascade should only depend on c .

While an unsafe implementation of the first cascade could in principle achieve better quality than a safe disjunctive or conjunctive top-c computation using SF , this is not really expected. Thus, our goal is to do (almost) as well as these two choices, shown to be good in [3, 5], while being much faster than these and other non-safe competitors.
We now describe our approach, which starts with an ex-isting inverted index for the collection that could be used to run queries using SF . We then create additional auxiliary index structures to quickly identify promising candidates. We create two kinds of structures, single-term structures, and term-pair, or pairwise, structures, which together make up the first layer of the index, while the complete inverted index 1 makes up the second layer. For the single-term struc-tures in the first layer, we choose, for each inverted list, some number of high-scoring postings, and arrange them by im-pact score in decreasing order. The pairwise structures are obtained by intersecting two inverted lists, and keeping a certain number of high-scoring two-term postings, where the score of such a posting is the sum of the impact scores of the two constituent postings. These structures are also sorted in decreasing order of impact score. We later discuss how to select which postings to put into the first layer, based on query traces and impact scores.

When a query enters, the first cascade is now executed by first selecting and accessing some prefix of the much smaller relevant structures in the first layer of the index. After-wards, we perform a limited number of lookups into the second layer, to obtain additional scores for some promising documents for which we have found partial scores in the first layer. Finally, we identify c documents for further evalua-tion by CF . We show the overall index structure in Figure 1, where a query  X  X og cat mouse X  is processed.

For our problem setup, there are various technical prob-lems to address. In particular, when building the first layer, we need to decide how deep we should build the single-term structures, and for which pairs of terms we should build a pairwise structure and up to what depth. The goal is for the structures to be deep enough to find most good results, Except for short lists that are completely in the first layer. Figure 1: Our index structure, with first layer on the left and second layer on the right. In the top left are single-term structures, and in the bottom left are pairwise structures, each sorted by decreasing im-pact score. For a query  X  X og cat mouse X  our method might decide to access a certain prefix (shown in red) of each relevant single-term list, and of one of the available pairwise structures (in this case, for  X  X at X  and  X  X ouse X ), based on some access budget. but not so deep that there is a large increase in overall index size. When a query enters, we need to decide which of the applicable index structures to consult and up to what depth  X  always using all potentially relevant structures in the first layer up to their full depth would not be efficient. We also need to decide what lookups into the second layer should be performed to identify the c results to be forwarded to the complex ranker for full evaluation. We later propose and evaluate solutions for all these problems.

Overall, our approach has the following steps that need to be implemented. During indexing, we have two steps:
Later, when a query enters the system, we execute the following sequence of steps:
We now describe the two steps in the index construction in more detail. First, we build two models, one for query and query term distribution, and one to model the quality con-tributions of different parts of the index structures. These models are then stored for later use during index building and query processing.

Modeling: For the first model, we use standard Lan-guage Modeling tools, in particular the MIT Language Mod-eling (MITLM) toolkit 2 , based on Kneser-Ney smoothing. We train these models on part of our query trace (distinct from any queries used in the evaluation), to obtain estimates of two probabilities, p ( t ), the probability that term t occurs in a random incoming query, and p ( t 1 ,t 2 ), the probability that both t 1 and t 2 occur in such a query. We refer to these as our query language models .

For the second model, given a posting p for a term t that has rank r in its list (i.e., has the r -th highest impact score in its list), we want a rough estimate of the likelihood that the posting belongs to a top-k result under the complex ranker CF , given a random query containing t . This is done by issuing training queries and, for each posting in one of the query term lists, storing its rank, the length of its inverted list, and whether it is part of a top-k result for the query. We bucketize the list lengths and relative ranks within lists into dozens of ranges (classes) each. Then we aggregate our data into a two-dimensional table A where A [ i,j ] estimates the probability that a posting belonging to list length class i and relative rank class j (which might, say, correspond to a list length between 1000 and 1500 and rank between 120 and 160) leads to a top-k result. This approach gives sufficiently accurate predictions, while allowing extremely fast lookups during index building and query processing to get an estimate of p (top-k | t ), the likelihood that a posting is part of a top-k result given that its term t is part of a query.
We then repeat the process for term intersections, where we create a table where for each posting in the intersection of two terms t 1 and t 2 , we use the length of the intersection, and the rank of the posting in the intersection, to estimate p (top-k | t 1 ,t 2 ), the likelihood that a pairwise posting is part of a top-k result given that both t 1 and t 2 occur in a query. We refer to these models for single lists and term intersec-tions as posting quality models .

Index Building: Given a space budget, our next goal is to build a first layer containing term and term-pair postings that are likely to lead to top-k results under random queries. To do this, we allocate separate space budgets to the single-term and pairwise structures in the first layer. For single-term lists, we greedily pick postings from the highest ranks of the inverted lists to add to the first layer. That is, we try to pick postings with the highest value of p ( t )  X  p (top-k | t ). Since our estimate for this value based on the models is expected to be a monotonically decreasing step function in each list, we can sort each list by increasing rank, and select chunks of postings with equal value from the beginning of
Available at https://code.google.com/p/mitlm/ the lists until the budget is exhausted using a heap to decide from which list to pick.

We repeat this greedy selection process for pairwise post-ings. Since there is a huge number of term pairs, we first restrict the space by only considering intersections for terms t and t 2 with p ( t 1 ,t 2 )  X   X  for some small  X  . Then these intersections are created and sorted by impact, and we again select chunks of postings from the beginning of the intersec-tions based on our estimates of p ( t 1 ,t 2 )  X  p (top-k | t the budget is exhausted.

All structures in the first layer are kept sorted from highest to lowest impact score. The single-term postings are of the form (docID, impact), while the term-pair posting layout is (docID, impact t 1 , impact t 2 ). Note that for single-term structures, we do not remove the postings in the first layer from the second layer, but create a copy of the postings, so this uses extra space. The reason is that we only access a limited amount of the first-layer structure, and thus we need to make sure that a lookup into the second layer can retrieve all postings. An exception are very short lists, of length less than 100, where we always move the entire list into the first layer and access it fully on any query containing the term; thus, these lists do not increase space usage (though their overall size is small). We also added an additional rule that limited the depth of any selected single-term and pairwise structure to the maximum access depth for queries, typically several thousand postings, as any posting deeper than the access depth would never be used anyway.
We now describe the steps involved in query processing: the online greedy depth selection, the query processor, the second-layer lookups, and the final selection of candidates.
Online Greedy Depth Selection: Given a query, we first identify all relevant structures available in the first layer. This usually includes all single-term structures for the query, since our language model assigns non-zero probabilities to all terms and the first postings tend to have very high values of p (top-k | t ), especially for short lists. Only some of the pairwise structures will typically be available for a query.
For each query, we have a cost budget that determines how much of the relevant structures we can access. For ex-ample, we might have a budget b = 1000, meaning that we can only access a total of b postings from the structures. Then for each structure we select a (possibly empty) prefix of postings. This is done greedily using a heap, as during in-dex building, except that we select chunks of postings based on p (top-k | t ) and p (top-k | t 1 ,t 2 ), respectively, without mul-tiplying by p ( t ) and p ( t 1 ,t 2 ) (since at this point the query already contains the terms). We note that we could perform various refinements to this approach, by charging different costs for pairwise versus single-term postings, or assigning different budgets to queries based on their difficulty.
Query Processor: We now run a fast and simple query processor on the selected structures and their correspond-ing depths. This processor copies the selected prefixes of the first-layer structures into an array and then runs a fast Radix Sort to sort postings by docID. A subsequent scan then aggregates the impact scores for each docID, and cre-ates a bit filter for each docID stating which terms may re-quire lookups into the second layer. During the scan we also filter redundant lookups as follows: Suppose a posting with docID 7 and impact 2 . 9 was found in the prefix of the list for  X  X at X , and that we have also accessed all pairwise post-ings for the pair  X  X at dog X  with score 2 . 1 or higher. Then we do not have to perform a lookup into the  X  X og X  list in the second layer for docID 7  X  if such a posting existed we would have seen it as part of a pairwise posting.

We initially implemented a TAAT query processor using a hash table for the accumulators. However, we found that the sorting-based approach was much faster, by a factor of 2 to 3. Such a sorting-based approach is possible because we fix the access depth for each structure at the start of the query.

Second Layer Lookups and Final Candidate Selec-tion: Next, we check the candidates and their accumulated scores, where most of these scores are partial, and many may have been only seen for one of the query terms. We now de-cide for which of these candidates we should perform lookups into the second layer to complete their scores, subject to a budget on the number of lookups (say, a few thousand per query). This is done using the accumulated partial score, as this provides a strong signal for relevance. We select the candidates with the highest partial scores, by running a randomized approximate selection algorithm where we first draw a sample of the impact scores, sort this sample, pick a suitable threshold from the sample, and then keep all candi-dates with impact above this score. Finally, we perform all necessary lookups for these candidates into the second layer, and keep the c candidates with highest completed BM25 scores, to be submitted to CF .
In this section, we describe the data sets, ranking models, evaluation metrics, and setup of our experiments.
 Datasets: All our experiments were conducted on the ClueWeb09B collection, which consists of 50 , 220 , 423 doc-uments, 86 , 532 , 822 distinct terms, and 17 billion postings. For evaluation, we used the TREC 2009 Million Query Track (40k), which we refer to as Million09. Our training set for the modeling step includes 30k queries selected at random from Million09, while the testing set for performance evalu-ation consists of 3k from the remaining 10k queries. Table 1 shows the query lengths for the testing query set. Table 1: Query length distribution for the 3 k testing
The TREC 2010 to 2012 Web Track topics (150) were used for training the machine-learned complex ranker CF . For the language models, we used linear interpolation of a model for the training queries of the query set with a model for a randomly selected sample of 1 . 5 million documents, using the MIT Language Modeling (MITLM) toolkit.

Ranking models: We selected the BM25 ranking scheme as our first ranker, as it is widely used as a simple ranker in the literature and satisfies the desired properties of being both computationally fast and providing a reasonable set of initial candidates.

Recent studies [31, 51] show that ranking schemes ob-tained using learning-to-rank methods with dozens or hun-dreds of features outperform traditional bag-of-words mod-els in terms of quality. There are a number of learning-to-rank tools that are available. We decided to use Lamb-daMart [53] to learn our complex ranker CF , as it is consid-ered one of the most effective learning-to-rank models [22, 35]. We trained on the 150 queries from TREC 2010 to 2012 based on standard features from the literature [5, 36, 49]. Table 2 lists a subset of these features. The anchor text features were extracted using the data from [18], while the spam and pagerank values are from [16]. The distribution feature refers to the dispersion of term occurrences across a specific document, when the document is split into pieces of fixed size, say 100 terms.

Evaluation metrics: The main aspects in the design of a scalable web search architecture include quality, time, and space. Thus, we evaluated the proposed framework on these three aspects. We measure the end-to-end quality of the proposed methods with Overlap@(k,c) and NDCG@k. In particular, the end-to-end effectiveness evaluation within the cascading ranker setup is performed as follows. In the first cascade, the top-c documents are obtained based on our method and then, in the second cascade, the CF is applied to these documents in order to return the final top-k .
For the effectiveness of the first cascade, we measure Over-lap@(k,c) as the fraction of top-k documents obtained when applying CF to all top 2000 results of a safe disjunctive BM25, that are also found among the c candidates com-puted by our algorithm. While it was not feasible to apply the complex ranker to all documents in the union of query terms, we found in preliminary experiments that beyond the top 2000 there was little change in the final top-k . This choice is also directly supported by the recommendations in [35]. Thus, an overlap of 1 . 0 for a query means that all top-k results were found among the c candidates of our method. We use k = 10 unless stated otherwise.

We evaluated the speed of our methods using average query latency (in milliseconds) for generating the candi-dates. That is, we measure the time elapsed from when a query arrives until the time when the top-c candidates are ready to be evaluated by CF . (We do not count the time for applying CF as it is the same for all methods.) The space overhead was measured as the percentage of a baseline full index.

Index: The second layer was indexed and compressed using a version of PForDelta [59] proposed in [55]. The al-gorithms were implemented using C++ and compiled using gcc with -O3 optimization. The experiments were conducted on a single core of a 2 . 27Ghz Intel Xeon (E5520) CPU. All data structures and indexes are memory-resident.

Parameters: Overall, the proposed framework utilizes the following parameters: (a) the access cost budget, i.e., the number of postings to access per query, (b) the num-ber of documents to perform lookups on, (c) the number of candidates to forward to CF , c , and (d) the space budget, i.e., the amount of additional space beyond a standard in-dex. During selection of the term-pair structures in the first layer, we considered only pairs with probability (according to the language model of MITLM) at least 1 . 99  X  10  X  16 the Million09 query trace. Next, we present the experimen-tal evaluation of our approach based on these parameters.
In this section, we present the experimental evaluation of our methods in terms of effectiveness, efficiency, and space.
In the first experiment, we evaluate the proposed candi-date generation algorithm under the assumption that there is no space budget; i.e., all possible first layer single-term and pairwise structures are available at query time. Al-though this scenario is unrealistic, as the space overhead of the pairwise structures can be very large, it shows the potential of the proposed method.

We compare our method against a na  X   X ve baseline which, given a cost budget b , accesses all relevant structures of the first layer at equal depth. Thus, if there are 5 available structures for a specific query, and the cost budget is 5 k postings, the baseline would access the first 1 k postings in each structure. Note that this approach is similar to the Fixed method proposed in [48]. We evaluate three versions of this na  X   X ve approach, for the cases where only single-term, only pairwise, and both types of structures are available, to show that both types of structures give benefit. More-over, we implemented a clairvoyant selection algorithm that knows apriori which of the docIDs in the postings in the first layer result in top-k results, and then selects prefixes of the structures in an optimal way, thus giving an upper bound on the quality that can be achieved with any depth-limited access scheme on impact-sorted structures. This algorithm is included to observe how close to optimal our algorithm is.
Setup: We assume unlimited space budget and use the following parameter settings and selection strategies: c is 500, the algorithms exhaustively perform lookups on all do-cIDs seen in the accesses of the first-layer structures (thus the number of lookups is only bounded by the access depth), and the c candidates are selected based on highest BM25.
Effectiveness: Figure 2 shows Overlap@(500 , 10), i.e., the fraction of correct top-10 results preserved within the c = 500 candidates, as the access cost budget varies from 500 to 20000, and with the first layer consisting of different structures. Obviously, the clairvoyant algorithm achieves the best quality for all access budgets, as it is as an up-per bound of our method. On the other hand, we observe that for the na  X   X ve method, having only pairwise structures consistently outperforms having only single-term ones, but having both achieves the best quality, which is 0 . 8864 when the cost budget is 2000. The Greedy selection algorithm out-performs all na  X   X ve methods and achieves quality close to the optimal clairvoyant one, even with moderate access budget. For instance, the quality is 0 . 946 for the 2000 access bud-get. Thus, more than 94% of the same top-10 results are returned. Finally, as the cost budget increases, quality in-creases for all algorithms, since we consider more postings as candidates. In particular, Greedy achieves quality really close to Clairvoyant for the 5000 access budget.

Efficiency: In Table 3, we report the average query pro-cessing time in milliseconds of the Greedy algorithm when the access budget varies from 500 to 20000. We observed that the access budget is a very good proxy for query pro-cessing time in the case of unlimited lookups, and thus we only report the time for Greedy; the numbers for the na  X   X ve and clairvoyant algorithms with the same access budget are very similar. According to Table 3, we see that it is possible Figure 2: Effectiveness of the first-phase selection al-to achieve overlap close to 0 . 946 for access budget 2000, in 0 . 51ms. As the access budget increases, query processing becomes slower, as we process more postings. Thus, both quality and speed are very good when there is no space con-straint. Next, we evaluate if comparable numbers can be obtained with limited space.
 Table 3: Efficiency of the Greedy algorithm with various
In the next experiment, we drop the assumption of un-limited space budget and focus on more realistic scenarios. More specifically, we allow a specific percent of space over-head for the first-layer structures over the full second-layer index. Given various space budgets, we evaluate the Greedy algorithm in terms of quality and speed. In this experiment, we use the setup and parameters of the previous experiment, and assume that the first layer consists of both single-term and pairwise structures. We still do all lookups to complete the partial scores of all docIDs seen in the first layer.
Effectiveness: In Table 4, we present the effectiveness of the Greedy algorithm (measured by Overlap@(500 , 10)) when a specific percentage of space overhead is allowed for the first layer structures, and the access cost budget is 2000 and 5000. For the single-term structures, we decided to keep up to 2000 and 5000 postings of every list, which correspond to 7 . 1% and 9 . 9% of the full index, respectively. On top of this small fixed space overhead, we have a limited space budget for pairwise structures that is allocated according to the Greedy allocation algorithm. The reported space in Table 4 includes only the pairwise structures in the first layer. First, we observe that our proposed method works quite well even with limited space budget, with moderate quality loss. (We look at NDCG numbers later.) Increasing the space budget of pairwise structures to more than 50% does not seem to provide significant quality gains unless a lot of space is available. When the access cost is larger, again better quality is achieved as more candidates are considered. Table 4: Effectiveness of the Greedy algorithm
A space overhead of 57 . 1% (7 . 1% singles and 50% pairs) for 2000 access budget, and 59 . 9% for 5000 is acceptable given the significant performance speedup that we achieve. For example, in the Maguro system [42], a 20x index size increase is justified for a 3x performance improvement.
Efficiency: Table 5 shows the average query processing time of the Greedy algorithm for various space budget, when access budget is 2000 or 5000. As mentioned before, the ac-cess budget provides a reliable proxy for performance and this is evident in Table 5. The Greedy algorithm requires on average 0 . 551ms and 1 . 055ms, for 2000 and 5000 access budget, respectively, still assuming no limit on lookups. As the space budget increases, the performance for both access budgets becomes better. The reason is that more high qual-ity postings appear in the pairwise structures, which results in fewer lookups for the missing terms.
 Table 5: Efficiency of the Greedy algorithm with varying
In the previous experiments, all algorithms exhaustively perform lookups in the second layer. However, as we will see, the lookup selection policy plays an important role in the performance. Thus, we now look at better lookup strategies.
First, we investigate how query processing costs are dis-tributed. Table 6 shows the average time overhead of each part of the query processing cost for 5000 access budget and 3000 lookups. Recall that query processing includes the online greedy depth selection, the radix sort, the scan for aggregating scores and lookup pruning, selection by sam-pling, lookups into the second layer, and another selection to get the c candidates. From Table 6, it is obvious that lookups are a large part of the total cost, as they involve decompression and access to many random blocks.

Instead of exhaustively performing all lookups, we keep only a certain number m of candidates for lookups, based on partial BM25 score. To do this, we perform a randomized approximate selection as described earlier. We evaluate the quality and the speed of this lookup strategy for access bud-gets of 2000 and 5000 and a 0 . 5 pairwise space budget on top of the single-term structures of each access budget. Figure 3 presents the Overlap@(500 , 10) and the average query pro-cessing time for several configurations of m lookups, under both access budgets. More specifically, we allow lookups of { 500 , 1 k, 2 k, 3 k, 5 k } candidates for access budget 5000, and { 500 , 750 , 1 k, 1 . 5 k, 2 k } for access budget 2000 (from left to right). We see that better performance can be achieved with the proposed lookup method at the cost of some quality loss, and thus there is a trade-off between quality and time. Figure 3: Effectiveness/Efficiency evaluation of our
Next, we compare the speed of our approach with other recently published methods. While top-10 query process-ing can be done in a few milliseconds [19], selecting the top 500 is much more expensive for most methods. Table 7 presents running times for different top-500 candidate gen-eration methods. We implemented all methods in C++ and ran them on the ClueWeb09B data set. For safe methods, we pick BMW-LB, the fastest disjunctive method to our knowl-edge from [19], and BMA, the fastest conjunctive method from [21]. For unsafe methods, we implemented the best approach from [52], which is the tree-based Priority with pruning, and the best method from [43], BMW-CS, which uses 10% of the index as first layer. Our method with 5k-3k setup is much faster, since it only evaluates at most 5k postings, which means that less than 5k documents for each query are evaluated, while other methods usually evaluate many more documents. Both our online and offline post-ing selection mechanism contributes significantly to its good performance. BMA has the second fastest speed, since it is a conjunctive algorithm that only evaluates a limited num-ber of documents. BMW-CS performs BMW on the first layer, which is the top 10 percent of the full index based on impact score. BMW-CS is different from our approach as we use pairwise structures in addition to singles in the first layer, and they do not have a fixed access budget per query or the greedy depth selection techniques. Their speed is much slower than ours, while still faster than BMW-LB. Priority is slower than BMW-LB, for top-500. For space overhead, BMA and BMW-CS both use less than 5% of the index size to store the Max-Block index. The two tiers in BMW-CS are disjoint, so the first tier takes no extra space. BMW-LB takes about 25% to cache the LB index, while priority takes no extra space. Our method with 5k-3k setup takes 59.9% extra space. Note that commercial search en-gines are often willing to accept significant space overheads for relatively small improvements in speed[42].

We now evaluate the effectiveness of our methods. Table 8 shows the Overlap@(500 , 10) for all the methods, and for Table 7: Running times and space overhead of different our approach with three different setups. BMW-LB has the best result, and BMA is almost as good, but slightly worse. This means that conjunctive query processing is almost as good as disjunctive in terms of quality, which was also shown in [5]. In Table 9, we rerank the results of all the methods according to the complex ranker. BMW-LB+CF is much better than BMW-LB, which means that the complex ranker performs well on identifying more relevant results. The 5k-3k setup achieves consistently the best quality among our methods at all cutoff levels, and it X  X  also slightly better than the other two unsafe methods for both Overlap and NDCG. Overall, we see that under NDCG, our approach can very quickly, in less than a millisecond, identify results that are almost as good as a safe disjunctive approach (BMW-LB). M ethod / cutoff 1 5 10 20 100 200 BM W-LB+CF 0 . 267 0 . 253 0 . 259 0 . 252 0 . 275 0 . 292 BM W-CS+CF 0 . 249 0 . 241 0 . 236 0 . 222 0 . 245 0 . 270 P riority+CF 0 . 261 0 . 242 0 . 239 0 . 243 0 . 252 0 . 269
Table 9: NDCG at various cutoff levels on Million 09 .
Next, we test the impact of query length on speed and quality, using various configurations of our methods. Table 10 presents the average query processing time (ms) when varying the query length, whereas Table 11 reports the cor-responding Overlap@(500 , 10) for each configuration. As query length increases, the query processing time also in-creases, since more lookups into more structures are per-formed. On the other hand, quality decreases, because top results in longer queries tend to be deeper inside the impact-sorted list (a basic fact in top-k query processing shown in Fagin X  X  theoretical analysis of his algorithms [23]). Table 10: Efficiency when varying query length on the
Table 12 shows the impact of the number of candidates returned by our candidate generation algorithm (cutoff c ) on Overlap@( c, 10) for the 5000-3000 setup. As the cutoff c increases, the quality increases, untill flattening around 500. This justifies our choice of c = 500 throughout the paper. Table 11: Effectiveness in Overlap@ (500 , 10) when we
Table 12: Effectiveness when we vary candidates c .
In this paper, we have proposed a fast first-phase can-didate generation approach for cascading ranking architec-tures. Our framework builds an auxiliary layer of index structures (single-term and pairwise structures) based on models for query term frequency and posting quality, which is then selectively accessed at query time based on a cost budget and using early termination techniques. The exper-imental evaluation shows that the proposed framework can find candidates about an order of magnitude faster than con-junctive or disjunctive top-k computations, with little loss in quality.

Future work includes the addition of specialized structures for phrases and proximity into our framework. We also ex-pect some improvements from further optimization of the query processor and lookup mechanism, e.g., by adding bit vectors, or Bloom filters as in [4], or by using more complex rules to decide which lookups to perform (possibly based on ideas similar to [52]).
 This research was supported by NSF Grant IIS-1117829  X  X f-ficient Query Processing in Large Search Engines X , and by a grant from Google.
