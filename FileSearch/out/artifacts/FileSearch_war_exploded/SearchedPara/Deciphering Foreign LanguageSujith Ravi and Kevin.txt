 Bilingual corpora are a staple of statistical machine translation (SMT) research. From these corpora, we estimate translation model parameters: word-to-word translation tables, fertilities, distortion pa-rameters, phrase tables, syntactic transformations, etc. Starting with the classic IBM work (Brown et al., 1993), training has been viewed as a maximiza-tion problem involving hidden word alignments ( a ) that are assumed to underlie observed sentence pairs ( e,f ): Brown et al. (1993) give various formulas that boil P ( f,a | e ) down to the specific parameters to be es-timated.

Of course, for many language pairs and domains, parallel data is not available. In this paper, we address the problem of learning a full transla-tion model from non-parallel data , and we use the learned model to translate new foreign strings. As successful work develops along this line, we expect more domains and language pairs to be conquered by SMT.

How can we learn a translation model from non-parallel data? Intuitively, we try to construct trans-lation model tables which, when applied to ob-served foreign text, consistently yield sensible En-glish. This is essentially the same approach taken by cryptanalysts and epigraphers when they deal with source texts.

In our case, we observe a large number of foreign strings f , and we apply maximum likelihood train-ing: Following Weaver (1955), we imagine that this cor-pus of foreign strings  X  X s really written in English, but has been coded in some strange symbols, X  thus:
The variable e ranges over all possible English strings, and P ( e ) is a language model built from large amounts of English text that is unrelated to the foreign strings. Re-writing for hidden alignments, we get: Note that this formula has the same free P ( f,a | e ) parameters as expression (2). We seek to manipulate these parameters in order to learn the same full translation model. We note that for each f , not only is the alignment a still hidden, but now the English translation e is hidden as well.
A language model P ( e ) is typically used in SMT decoding (Koehn, 2009), but here P ( e ) actually plays a central role in training translation model pa-rameters. To distinguish the two, we refer to (5) as decipherment , rather than decoding.

We can now draw on previous decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly non-deterministic mappings and very large substitution tables.

The contributions of this paper are therefore:  X  We give first results for training a full transla- X  We develop novel methods to deal with large-Before we tackle machine translation without par-allel data, we first solve a simpler problem X  X ord substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in the natural language (plaintext) sequence is substituted by a cipher token, according to a substi-tution key. The key is deterministic X  X here exists a 1-to-1 mapping between cipher units and the plain-text words they encode.
 For example, the following English plaintext se-quences: may be enciphered as: according to the key:
The goal of word substitution decipherment is to guess the original plaintext from given cipher data without any knowledge of the substitution key.
Word substitution decipherment is a good test-bed for unsupervised statistical NLP techniques for two reasons X (1) we face large vocabularies and corpora sizes typically seen in large-scale MT problems, so our methods need to scale well, (2) similar deci-pherment techniques can be applied for solving NLP problems such as unsupervised part-of-speech tag-ging.

Probabilistic decipherment: Our decipherment method follows a noisy-channel approach. We first model the process by which the ciphertext sequence c = c 1 ...c n is generated. The generative story for decipherment is described here: 1. Generate an English plaintext sequence e = 2. Substitute each plaintext word e i with a cipher-We model P ( e ) using a statistical word n-gram English language model (LM). During decipher-ment, our goal is to estimate the channel model pa-rameters  X  . Re-writing Equations 3 and 4 for word substitution decipherment, we get:
Challenges: Unlike letter substitution ciphers (having only 26 plaintext letters), here we have to deal with large-scale vocabularies (10k-1M word types) and corpora sizes (100k cipher tokens). This poses some serious scalability challenges for word substitution decipherment.
We propose novel methods that can deal with these challenges effectively and solve word substi-tution ciphers: 1. EM solution: We would like to use the Expecta-2. Bayesian decipherment: We also propose a
In the next two sections, we describe these meth-ods in detail. 2.1 Iterative EM We devise a method which overcomes memory and running time efficiency issues faced by EM. Instead of instantiating the entire channel model (with all its parameters), we iteratively train the model in small steps. The training procedure is described here: 1. Identify the top K frequent word types in both 2. Extend the plaintext and ciphertext vocabular-3. Instantiate a new (2 K +1)  X  (2 K +1) channel 4. Goto Step 2 and repeat the procedure, extend-
Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decod-ing e that maximizes P ( e )  X  P  X  ing the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Grif-fiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage un-certainty about model parameters and allow one to incorporate prior knowledge during inference.
Here, we propose a novel decipherment approach using Bayesian learning. Our method holds sev-eral other advantages over the EM approach X (1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient infer-ence even when we use higher-order n-gram LMs, (3) there are no memory bottlenecks since the full channel model and derivation lattice are never in-stantiated during training, and (4) prior specification allows us to learn skewed distributions that are useful here X  X ord substitution ciphers exhibit 1-to-1 cor-respondence between plaintext and cipher types.
We use the same generative story as before for decipherment, except that we use Chinese Restau-rant Process (CRP) formulations for the source and channel probabilities. We use an English word bi-gram LM as the base distribution ( P 0 ) for the source model and specify a uniform P 0 distribution for the channel. 1 We perform inference using point-wise Gibbs sampling (Geman and Geman, 1984). We de-fine a sampling operator that samples plaintext word choices for every cipher token, one at a time. Using the exchangeability property, we efficiently score the probability of each derivation in an incremental fashion. In addition, we make further improvements to the sampling procedure which makes it faster. Smart sample-choice selection: In the original sampling step, for each cipher token we have to sam-ple from a list of all possible plaintext choices (10k-1M English words). There are 100k cipher tokens in our data which means we have to perform  X  10 9 sampling operations to make one entire pass through the data. We have to then repeat this process for 2000 iterations. Instead, we now reduce our choices in each sampling step.
 Say that our current plaintext hypothesis contains English words X, Y and Z at positions i  X  1 , i and i +1 respectively. In order to sample at position i , we choose the top K English words Y ranked by P ( X Y Z ) , which can be computed offline from a statistical word bigram LM. If this probability is 0 (i.e., X and Z never co-occurred), we randomly pick K words from the plaintext vocabulary. We set K = 100 in our experiments. This significantly reduces the sam-pling possibilities (10k-1M reduces to 100) at each step and allows us to scale to large plaintext vocab-ulary sizes without enumerating all possible choices at every cipher position. 2 Parallelized Gibbs sampling: Secondly, we paral-lelize our sampling step using a Map-Reduce frame-work. In the past, others have proposed parallelized sampling schemes for topic modeling applications (Newman et al., 2009). In our method, we split the entire corpus into separate chunks and we run the sampling procedure on each chunk in parallel. At the end of each sampling iteration, we combine the samples corresponding to each chunk and collect the counts of all events X  X his forms our cache for the next sampling iteration. In practice, we observe that the parallelized sampling run converges quickly and runs much faster than the conventional point-wise sampling X  X or example, 3.1 hours (using 10 nodes) versus 11 hours for one of the word substitution ex-periments. We also notice a higher speedup when scaling to larger vocabularies. 3
Decoding the ciphertext: After the sampling run has finished, we choose the final sample and ex-tract a trained version of the channel model P  X  ( c | e ) from this sample following the technique of Chi-ang et al. (2010). We then use the Viterbi algo-rithm to choose the English plaintext e that maxi-mizes P ( e )  X  P  X  2.3 Experiments and Results Data: For the word substitution experiments, we use two corpora:  X  Temporal expression corpus containing short  X  Transtac corpus containing full English sen-
The cipher data was originally generated from En-glish text by substituting each English word with a unique cipher word. We use the plaintext corpus to build an English word n-gram LM, which is used in the decipherment process.
 Evaluation: We compute the accuracy of a particu-lar decipherment as the percentage of cipher tokens that were correctly deciphered from the whole cor-pus. We run the two methods (Iterative EM 4 and Bayesian) and then compare them in terms of word substitution decipherment accuracies.
 Results: Figure 1 compares the word substitution results from Iterative EM and Bayesian decipher-ment. Both methods achieve high accuracies, de-coding 70-90% of the two word substitution ciphers. Overall, Bayesian decipherment (with sparse priors) performs better than Iterative EM and achieves the best results on this task. We also observe that both methods benefit from better LMs and more (cipher) training data. Figure 2 shows sample outputs from Bayesian decipherment. We now turn to the problem of MT without par-allel data. From a decipherment perspective, ma-chine translation is a much more complex task than word substitution decipherment and poses several technical challenges: (1) scalability due to large corpora sizes and huge translation tables, (2) non-determinism in translation mappings (a word can have multiple translations), (3) re-ordering of words or phrases, (4) a single word can translate into a phrase, and (5) insertion/deletion of words.
Problem Formulation: We formulate the MT de-cipherment problem as X  X iven a foreign text f (i.e., foreign word sequences f 1 ...f m ) and a monolingual English corpus, our goal is to decipher the foreign text and produce an English translation.

Probabilistic decipherment: Unlike parallel train-ing, here we have to estimate the translation model P ( f | e ) parameters using only monolingual data. During decipherment training, our objective is to es-timate the model parameters  X  in order to maximize the probability of the foreign corpus f . From Equa-tion 4 we have:
For P ( e ) , we use a word n-gram LM trained on monolingual English data. We then estimate param-eters of the translation model P  X  ( f | e ) during train-ing. Next, we present two novel decipherment ap-proaches for MT training without parallel data. 1. EM Decipherment: We propose a new transla-2. Bayesian Decipherment: We introduce a novel 3.1 EM Decipherment For the translation model P  X  ( f | e ) , we would like to use a well-known statistical model such as IBM Model 3 and subsequently train it using the EM algorithm. But without parallel training data, EM training for IBM Model 3 becomes intractable due to (1) scalability and efficiency issues because of large-sized fertility and distortion parameter tables, and (2) the resulting derivation lattices become too big to be stored in memory.
 Instead, we propose a simpler generative story for MT without parallel data. Our model accounts for (word) substitutions, insertions, deletions and local re-ordering during the translation process but does not incorporate fertilities or global re-ordering. We describe the generative process here: 1. Generate an English string e = e 1 ...e l , with 2. Insert a NULL word at any position in the En-3. For each English word token e i (including 4. Swap any pair of adjacent foreign words 5. Output the foreign string f = f 1 ...f m , skipping
We use the EM algorithm to estimate all the pa-rameters  X  in order to maximize likelihood of the foreign corpus. Finally, we use the Viterbi algo-rithm to decode the foreign sentence f and pro-duce an English translation e that maximizes P ( e )  X  P Linguistic knowledge for decipherment: To help limit translation model size and deal with data spar-sity problem, we use prior linguistic knowledge. We use identity mappings for numeric values (for ex-ample,  X 8 X  maps to  X 8 X ), and we split nouns into morpheme units prior to decipherment training (for example,  X  X EARS X   X   X  X EAR X   X +S X ).
 Whole-segment Language Models: When using word n-gram models of English for decipherment, we find that some of the foreign sentences are decoded into sequences (such as  X  X HANK YOU TALKING ABOUT ? X ) that are not good English. This stems from the fact that n-gram LMs have no global information about what constitutes a valid English segment. To learn this information auto-matically, we build a P ( e ) model that only recog-nizes English whole-segments (entire sentences or expressions) observed in the monolingual training data. We then use this model (in place of word n-gram LMs) for decipherment training and decoding. 3.2 Bayesian Method Brown et al. (1993) provide an efficient algorithm for training IBM Model 3 translation model when parallel sentence pairs are available. But we wish to perform IBM Model 3 training under non-parallel conditions, which is intractable using EM training. Instead, we take a Bayesian approach.

Following Equation 5, we represent the transla-tion model as P  X  ( f,a | e ) in terms of hidden align-ments a . Recall the generative story for IBM Model 3 translation which has the following formula:
P  X  ( f,a | e ) =
The alignment a is represented as a vector; a j = i implies that the foreign word f j is produced by the English word e i during translation.
 Bayesian Formulation: Our goal is to learn the probability tables t (translation parameters) n (fer-tility parameters), d (distortion parameters), and p (English NULL word probabilities) without parallel data. In order to apply Bayesian inference for de-cipherment, we model each of these tables using a Chinese Restaurant Process (CRP) formulation. For example, to model the translation probabilities, we use the formula: t  X  ( f j | e i ) = where, P 0 represents the base distribution (which is set to uniform) and C history represents the count of events occurring in the history (cache). Similarly, we use CRP formulations for the other probabilities ( n , d and p ). We use sparse Dirichlet priors for all these models (i.e., low values for  X  ) and plug these probabilities into Equation 8 to get P  X  ( f,a | e ) . Sampling IBM Model 3: We use point-wise Gibbs sampling to estimate the IBM Model 3 parameters. The sampler is seeded with an initial English sample translation and a corresponding alignment for every foreign sentence. We define several sampling oper-ators, which are applied in sequence one after the other to generate English samples for the entire for-eign corpus. Some of the sampling operators are de-scribed below:  X  TranslateWord( j ): Sample a new English word  X  SwapSegment( i 1 ,i 2 ): Swap the alignment  X  JoinWords( i 1 ,i 2 ): Eliminate the English word
During sampling, we apply each of these opera-tors to generate a new derivation e,a for the foreign text f and compute its score as P ( e )  X  P  X  ( f,a | e ) . These small-change operators are similar to the heuristic techniques used for greedy decoding by German et al. (2001). But unlike the greedy method, which can easily get stuck, our Bayesian approach guarantees that once the sampler converges we will be sampling from the true posterior distribution.
As with Bayesian decipherment for word sub-stitution, we compute the probability of each new derivation incrementally, which makes sampling ef-ficient. We also apply blocked sampling on top of point-wise sampling X  X e treat all occurrences of a particular foreign sentence as a single block and sample a single derivation for the entire block. We also parallelize the sampling procedure (as de-scribed in Section 2.2). 5
Choosing the best translation: Once the sampling run finishes, we select the final sample and extract the corresponding English translations for every for-eign sentence. This yields the final decipherment output. 3.3 MT Experiments and Results Data: We work with the Spanish/English language pair and use the following corpora in our MT exper-iments:  X  Time corpus: We mined English newswire  X  OPUS movie subtitle corpus: This is a large Both Spanish/English sides of TRAIN are used for parallel MT training, whereas decipherment uses only monolingual English data for training LMs. MT Systems: We build and compare different MT systems under two training scenarios: 1. Parallel training using: (a) MOSES , a phrase 2. Decipherment without parallel data using: Evaluation: All the MT systems are run on the Spanish test data and the quality of the result-ing English translations are evaluated using two different measures X (1) Normalized edit distance score (Navarro, 2001), 6 and (2) BLEU (Papineni et al., 2002), a standard MT evaluation measure. Results: Figure 3 compares the results of vari-ous MT systems (using parallel versus decipherment training) on the two test corpora in terms of edit dis-tance scores (a lower score indicates closer match to the gold translation). The figure also shows the cor-responding BLEU scores in parentheses for compar-ison (higher scores indicate better MT output).
We observe that even without parallel training data, our decipherment strategies achieve MT accu-racies comparable to parallel-trained systems. On the Time corpus, the best decipherment (Method 2a in the figure) achieves an edit distance score of 28.7 (versus 4.7 for MOSES). Better LMs yield bet-ter MT results for both parallel and decipherment training X  X or example, using a segment-based En-glish LM instead of a 2-gram LM yields a 24% re-duction in edit distance and a 9% improvement in BLEU score for EM decipherment.

We also investigate how the performance of dif-ferent MT systems vary with the size of the training data. Figure 4 plots the BLEU scores versus training sizes for different MT systems on the Time corpus. Clearly, using more training data yields better per-formance for all systems. However, higher improve-ments are observed when using parallel data in com-parison to decipherment training which only uses monolingual data. We also notice that the scores do not improve much when going beyond 10,000 train-ing instances for this domain.

It is interesting to quantify the value of parallel versus non-parallel data for any given MT task. In other words,  X  how much non-parallel data is worth how much parallel data in order to achieve the same MT accuracy?  X  Figure 4 provides a reasonable an-swer to this question for the Spanish/English MT task described here. We see that deciphering with 10k monolingual Spanish sentences yields the same performance as training with around 200-500 paral-lel English/Spanish sentence pairs. This is the first attempt at such a quantitative comparison for MT and our results are encouraging. We envision that further developments in unsupervised methods will help reduce this gap further. Our work is the first attempt at doing MT with-out parallel data. We discussed several novel deci-pherment approaches for achieving this goal. Along the way, we developed efficient training methods that can deal with large-scale vocabularies and data sizes. For future work, it will be interesting to see if we can exploit both parallel and non-parallel data to improve on both.
 This material is based in part upon work supported by the National Science Foundation (NSF) under Grant No. IIS-0904684 and the Defense Advanced Research Projects Agency (DARPA) through the Department of Interior/National Business Center un-der Contract No. NBCHD040058. Any opinion, findings and conclusions or recommendations ex-pressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of the Interior/National Business Center.

