 ORIGINAL PAPER Sebasti X n Pe X a Saldarriaga  X  Christian Viard-Gaudin  X  Emmanuel Morin Abstract Today, there is an increasing demand of efficient archival and retrieval methods for online handwritten data. For such tasks, text categorization is of particular inter-est. The textual data available in online documents can be extracted through online handwriting recognition; however, this process produces errors in the resulting text. This work reports experiments on the categorization of online handwrit-ten documents based on their textual contents. We analyze the effect of word recognition errors on the categorization performances, by comparing the performances of a catego-rization system with the texts obtained through online hand-writing recognition and the same texts available as ground truth. Two well-known categorization algorithms (kNN and SVM) are compared in this work. A subset of the Reuters-21578 corpus consisting of more than 2,000 handwritten doc-uments has been collected for this study. Results show that classification rate loss is not significant, and precision loss is only significant for recall values of 60 X 80% depending on the noise levels.
 Keywords Text categorization  X  Noisy text  X  Document categorization  X  Online handwriting recognition 1 Introduction The categorization of electronic, ascii, 1 documents is a well-known research area and has been thoroughly studied [ 1 ]. Given the widespread use of mobile devices such as cell phones, PDAs and pen computers in today X  X  world, large amounts of online handwritten data are being created. In order to provide reliable applications for efficient archival, retrieval and management of such data, text categorization research has to be extended to online documents.

Among the management facilities that text categorization can provide, there are automatic document organization, doc-ument routing or filtering and automatic indexing for topic retrieval. This supposes that we should be able to solve the problem of document categorization from an input signal being online handwriting. One way to address this problem is to pipeline the output of a handwriting recognition sys-tem into the input of a text categorization (TC) algorithm. However, since the output texts of an online handwriting recognition engine contain errors, which will induce errors in subsequent processing stages of categorization applica-tions, a careful work has to be carried out to study to what extent the various linguistic resources that guide the recogni-tion process and subsequently the extraction of the relevant terms, act on the categorization task.

Online documents may have a rich internal structure that can include text, graphics, equations and other non-textual elements. As for typeset document images [ 2 ], categorization could be performed using non-textual features, thus bypass-ing online recognition, but textual features remain critical and need further exploration.
The retrieval of handwritten document images has been addressed in recent years by keyword spotting-based approaches [ 3  X  10 ]. Word spotting approaches are unsuitable for natural language processing (NLP) technologies. They are unable to consider morphological variants of the same word as a single term or measure the discriminating power of terms for instance. Because NLP applications such as TC or automatic summarization attempt to derive information from text, recognition is an unavoidable process. Recogni-tion is a gateway from online signals to meaningful text.
The following study will focus on classification using tex-tual features extracted from handwriting recognition results that may be noisy. Therefore, the research problem intro-duced in this paper lies within the larger area of noisy text analytics. The state-of-the-art presented below will exceed the limits of handwritten text categorization and highlights the multiple facets of noisy text categorization . In particular, different sources of noise are considered including typeset document images, synthetic data and off-line handwriting. A survey on noisy TC is given in Sect. 2 .
 The remainder of this paper is organized as follows: in Sect. 3 , we present the underlying recognition engine, and various strategies that enable a transcribed version from an online handwritten document to be produced. The resulting text documents are used as input for the text categoriza-tion engine described in Sect. 4 . The corpus collected for this study, consisting of more than 2,000 online documents, is introduced in Sect. 5 . Section 6 reports and discusses results on handwriting recognition and categorization of the handwritten dataset. Performances of noisy and clean TC are compared, and the results of significance tests are discussed. Finally, Sect. 7 summarizes and concludes this paper. 2 Noisy text categorization: a survey Due to the inherent difficulty of collecting large handwritten data sets, little research has been done on categorization of handwritten texts. Most of the research on noisy text cate-gorization has so far been conducted with texts produced by optical character recognition (OCR) systems from machine-printed texts [ 11  X  14 ], which are quite efficient as long as the quality of the original document is not degraded.

Ittner et al. [ 11 ] performed categorization of techni-cal report pages using the Rocchio algorithm [ 15 ]. Their results suggested that by employing texts affected by the same source of noise in training and test phases, acceptable levels of precision and recall can be achieved.

As a way to circumvent noise, Junker and Hoch [ 12 ] applied an n-gram-based categorization method [ 16 ], appar-ently tolerant to textual errors, over German business let-ters and technical report abstracts. Their general goal was to investigate which feature type should be preferred in order to achieve optimal effectiveness with OCR texts. Several strate-gies are presented including feature sets constructed with all the words, all but stopwords, words after morphological anal-ysis, 3, 4, 5-grams. This work presents encouraging results but no comparison with the ground truth texts is reported.
Taghva et al. [ 13 ] investigated the effects of OCR errors on a Naive Bayes classifier applied to documents of the Nuclear Regulatory Commission. They observed that term selection techniques reduced the influence of OCR errors and improved categorization results. However, this conclu-sion seems related to the moderate error rates (14%) and the number of categories actually analyzed (6 out of 52). We can note that the noise levels observed for handwritten documents are higher than those usually reported for typeset documents.
A more recent work [ 14 ] studied the impact of feature transformations on an OCRed subset of the Reuters-21578 benchmark collection [ 17 ]. The document image corpus was obtained by scanning 750 printed texts with resolutions rang-ing from 130 to 300 dpi. However, since their work focuses on the use of transformed features, Murata et al. [ 14 ] do not report results stating a difference between clean and noisy text categorization.

To our knowledge, little research addresses the problem of categorization of handwritten documents [ 18  X  20 ]. Vinc-iarelli X  X  experiments on handwritten text categorization [ 18 ] used support vector machines (SVM) [ 21 ] on a subset of 200 documents from the Reuters-21578 corpus manually writ-ten by a single writer, the SVM were trained over clean digital texts. Several experiments using synthetic noisy data obtained through OCR simulation [ 22 ] were also reported. The use of synthetic data allows the study of noise at preset word error rate levels (from 10 to 45%).

Koch [ 19 ] used a k-Nearest Neighbours (kNN) algorithm [ 23 ] on a corpus of French business letters. Text features are extracted by spotting keywords from an existing lexi-con of relevant terms that is manually constructed. His work showed that a rejection strategy in the keyword recognition engine improved categorization results. Koch states that little performance loss is expected when using handwritten docu-ments instead of clean documents, but no significance tests are presented to support such a conclusion.

Recently, categorization based on latent semantic anal-ysis (LSA) [ 24 ] and cosine similarity has been applied to prehospital care reports [ 20 ]. The goal of Milewski et al. [ 20 ] was to perform lexicon reduction using topic labels, thereby improving handwriting recognition performances, and the authors do not report results on categorization. Actually, it is not clearly stated whether the authors have the categories associated with each document at their disposal or not.
Our contribution differs from the above-mentioned works in several aspects. Firstly, this is to the best of our knowledge, the first attempt to apply text categorization to online hand-writing. Secondly, our experiments are performed on one of the biggest existing online handwriting databases intended for categorization. Also, the collected database uses a stan-dardized benchmark collection as supporting ground truth. Thirdly, unlike previous works using handwriting as an input source [ 18 , 19 ], handwritten documents are used in both train-ing and test phases. Finally, categorization is evaluated using an extensive set of measures, furthermore, significance tests are provided in order to assess the performance difference between clean and noisy text categorization. 3 Handwriting recognition Online handwriting recognition is the process of automati-cally converting texts that have been handwritten with elec-tronic devices such as tablets and digital pens, into a character stream usable within text-processing applications. Compared to off-line documents, the most important characteristic of online documents is that they capture a temporal sequence of strokes, whereas off-line documents are raw bitmap images where handwriting has to be extracted from the background pixels. Online documents, also known as digital ink, consist of a sequence of points, corresponding to the writing tool trajectory, which is sampled during the writing process (see Fig. 1 ).

In this work, as we work with a set of online documents, an online recognition engine is used. The online recogni-tion process is shown in Fig. 2 . The recognition engine of MyScript Builder 2 is used in our experiments.

Traditionally, recognition systems achieve poor recogni-tion performances without linguistic resources. Linguistic resources give prior knowledge to the recognizer about what it is likely to recognize [ 25 ]. Hence, according to the lin-guistic knowledge which is attached to the recognizer, the recognition performances can vary a lot.

MyScript Builder offers the possibility to create specific linguistic resources and also contains two convenient stan-dard built-in resources:  X  lk-text is composed of a standard lexicon of English  X  lk-free has an extended out-of-lexicon recognition capa-3.1 Measuring recognition errors Recognizers are not perfect, and recognized documents often contain noise , i.e. errors induced by the recognition process. The word error rate (WER) is a common performance mea-sure of a handwriting recognition system.

In TC, a preprocessing step is performed before the actual categorization takes place (see Fig. 3 ). This process can cor-rect errors in the recognized texts such as the recognition of inflected forms of a word into another form of the same word (e.g., dollar as dollars ), or confusion of stopwords with other stopwords (e.g., is as in ) (see Sect. 4.1 ). In this sense, the WER is an overestimation of noise because no account is taken of the effect that different types of errors may have on the document representation.

Vinciarelli [ 18 ] proposed to measure noise at the term level, i.e. after the stemming and the stopword removal pro-cess. The term error rate (TER) provides a better estimation of the noise in the categorization context. However, not all the terms in a document are used to build its vector repre-sentation. The loss of relevant terms, which are the subset of terms used to build the vector space, is more important than the loss of other terms.

A measure that estimates the proportion of relevant terms incorrectly extracted is thus necessary: the relevant term error rate (RTER). The RTER is defined as follows: RT E R = 1  X  where rt f ( i ) and rt f ( i ) are the frequencies of the relevant term i in the clean and recognized text, respectively, and M is the total number of relevant terms.
 The TER and the RTER are complementary measures. The TER helps measure the number of terms present in the train set, and how many of these potential terms are lost. Once the relevant terms are selected, the RTER can give us a hint about how many relevant terms are properly recognized. Note that when no feature selection is performed TER = RT E R .
In this paper, we report the noise level of recognized docu-ments in terms of WER, TER and RTER for our handwritten corpus. 4 Text categorization Text categorization is the task of assigning one or multiple labels to a document based on its content. More formally, for a given document d i and a category c j , text categorization can be defined by a function f such as: f : d where D = d 1 , d 2 ,..., d | D | is a set of documents, and C = c
A TC system can be used in two different ways: docu-ment-pivoted or category-pivoted. Given a document d i , one might want to find the set B  X  C containing all the cate-gories under which d i should be categorized. This is called document-pivoted categorization (DPC). Conversely, given a category c j , one might want to find all the d i  X  D belonging to c j . In this case, we perform category-pivoted categoriza-tion (CPC). Even if this distinction is more pragmatic than conceptual it can determine the choice of the evaluation mea-sures of a TC system.

TC can be seen as a pattern recognition problem, and machine-learning approaches can approximate f using a set of labeled documents. In this case, an appropriate set of fea-tures has to be selected, the so-called relevant terms, which will be used to define a vectorial representation, according to the VSM approach [ 26 ]. 4.1 Term selection and indexing In order to transform texts into a suitable representation for machine learning algorithms, four operations prior to cate-gorization are performed as shown in Fig. 3 .

The first step, tokenization, is the process of splitting char-acter streams into a sequence of words. During stopword removal, all the words assumed to carry no relevant informa-tion (called stopwords ) are removed. Stopwords are typically functional words (articles, prepositions, pronouns, etc.). For this task we used the FreeWAIS [ 27 ] stopword list.
The next step is stemming, or suffix stripping [ 28 ]. By stemming, the words indexed , indexing and indexation will have a single representative form: index . Stemming also reduces the number of distinct terms needed to represent a set of documents.

After stemming, we need to map the resulting sequence of terms into vectors. As shown in Fig. 4 , each vector component accounts for a term of the feature space . The feature space is composed of a given number of relevant terms extracted from the training set using Forman X  X  round robin algorithm [ 29 ] over category specific scores obtained with the  X  2 statistic [ 30 ].

Each term is weighted by a statistical measure such as the normalized tf  X  id f score ( w i )[ 31 ]. w where f ( i ) is the frequency of term i in a document, N the number of documents in the collection, M the number of distinct relevant terms in the collection and n i the number of documents in the collection containing i .
The vectors resulting from the indexing process can be used as input for categorization algorithms. In this paper, we compare two different categorization methods: a kNN algo-rithm [ 23 ] and SVM [ 21 ]. 3 4.2 Performance evaluation Performance evaluation for categorization systems is usually based on a test reference collection and on one or several evaluation measures. In the following discussion, we only cover the most common effectiveness measures: recall (  X  ) and precision (  X  ). The description of the test reference cor-pus is covered in Sect. 5 .

Let c be a category, R its set of relevant documents defined as such by an expert, and A be the set of the documents identi-fied as belonging to c by the system. The recall and precision measures can be derived from these sets as follows.
Recall is the proportion of correctly classified documents within all the documents of a given category ( c ).  X ( c ) =
Precision is the proportion of correctly classified docu-ments within all the documents classified under a given cat-egory ( c ).  X ( c ) =
An exhaustive picture of the system behavior is usually presented by interpolated precision versus recall curves [ 32 ]. Precision versus recall curves show the evolution of function of  X  for 11 standard recall levels [ 0 , 10 100% ] .

These measures are defined for a single category. None-theless, categorization algorithms are evaluated over several distinct categories. To evaluate categorization over several categories, we can average category-specific measures in two ways: micro-and macro-averaging.

Macro-averaging gives equal weight to each category; scores for each category are computed separately before being averaged. On the other hand, micro-averaging gives equal weight to every document, thus, micro-averaged scores are heavily affected by the performances on frequent catego-ries.
From a DPC point of view, where documents are available at different moments in time, an evaluation should be done on a document basis. Moreover, as we work with single label documents; micro-averaged recall and precision are equal. Hence, a single accuracy measure will be given as system evaluation when DPC is performed.

From the CPC perspective, the system behavior will be illustrated using precision versus recall curves. Furthermore, in a more application oriented approach, precision at position n plots will also be presented (see Sect. 6 ). 5 Corpora The handwritten documents used in this study are a subset of the Reuters-21578 newswire collection [ 33 ]. The Reuters-21578 is a well-known benchmark collection widely used in TC research [ 17 ]. The collection is composed of economic and business newswires that appeared in 1987. The news-wires are labeled into 135 categories. Out of these 135 cate-gories, only 90 are represented in both training and test sets and the 10 most represented categories account for about 90% of the collection.

The newswires cover different fields such as corpo-rate acquisitions, agricultural (coffee, sugar, etc.) or energy (crude, gasoline, etc.) commodities and interest rates for instance.

The database has been split into train and test sets follow-ing the ModApt X  split [ 34 ] and the documents not belonging to the top ten categories were discarded. A total of 2,000 documents were randomly chosen from the train set and 500 from the test set. These documents were used to collect hand-written data. Currently, we dispose of 2,029 documents out of these 2,500; they have been collected over a period of four months from more than 1,500 writers. The handwrit-ten newswires were collected using a digital pen and Anoto paper. Figure 5 shows several samples from our online data set and the corresponding categories.

Table 1 summarizes the number of documents per category for the training and the test sets. The categorization experi-ments reported in this paper are based on these documents and their electronic counterparts. 6 Experiments and results The first part of this section reports the results of handwriting recognition performed on the dataset previously described. The remainder of this section reports categorization results and significance tests. Categorization has been performed on both recognized and clean versions of the documents, and the performance difference has been measured from a statistical point of view. 6.1 Recognition The recognition engine previously described is used to per-form the transcription of the handwritten documents. For each handwritten document, the ground truth is also avail-able, but only at the full text level. Hence, we compute WER and TER, by aligning the recognized word sequence with the ground truth using a string-to-string edit distance [ 35 ]. Tables 2 and 3 show the performance of the recognition en-gine according to the linguistic knowledge resource involved.
Unsurprisingly, lk-text clearly outperforms lk-free. The latter better recognizes short words. Hence, a substantial loss of information after stopword removal and stemming is observed. It is well known that the performances of a rec-ognition system can be improved by incorporating statistical information at the word-sequence level [ 25 ], i.e. a language model. In the absence of such knowledge, which is the case for lk-free, the output of a recognition system is known to be very poor. Since lk-text works on a word basis, recog-nition of terms and words is substantially better. Moreover, for lk-text, it is worthwile to note that this resource contains no prior linguistic knowledge specific to these kind of doc-uments, which feature a lot of acronyms, abbreviations, and out of lexicon terms. Some examples of such terms can be seen in Fig. 5 a: revs for revenues, shr for shares or AMRC for American Recreation Centers, for instance.

The lexical coverage of this resource is 70.64% 4 and the minimum expected WER is 17.88%. 5 The WER, TER and RTER, which are obtained can be considered as low.

When comparing Tables 2 and 3 , it can be seen that the training and the test sets behave very similarly with respect to WER and TER, which is consistent with the random selec-tion of these two subsets. Of course, RTER is only applicable for the test set, since the relevant terms are defined using the  X  2 statistic, from among the recognized terms of the training set.

Concerning the test set, in addition to the two generic resources already mentioned, namely lk-free and lk-text, we have also introduced an additional resource, called lk-text which includes a dedicated lexicon competing with the gen-eral lexicon of lk-text. This lexicon is composed of all the words corresponding to a relevant term extracted from the lk-text training set. This way, the recognition of relevant terms is enhanced. Our goal is to decrease the RTER, thus increasing the chances of correct matching at categorization time. By using this new resource, the RTER is significantly decreased: nearly 5%, which corresponds to a relative improvement of 25%.

Two-dimensional visualization of the documents can pro-vide an intuitive view of the impact of noise in the vector space. Assuming all documents share at least a small subset of terms, we can compute a symmetric affinity matrix using the inner product between documents. For each document, we only keep the 10 largest similarities, i.e., its local neigh-borhood, then we use the Large Graph Layout [ 36 ] algorithm to visualize the matrix as a graph.
In Fig. 6 , we can see that lk-text and lk-text + preserve the structure of the original data, while lk-free preserves only the local neighborhood of the largest cate-gory (earn). Regardless of the error rates, one can intuitively expect more degradations in TC performance for lk-free, in particular, when learning algorithms use similarity or dis-tance measures, like the kNN method. 6.2 Categorization The categorization results obtained on the clean and noisy sets are reported below. It is worth noting that the textual training sets that are used for extracting the relevant terms are actually the output of the recognition system using dif-ferent resources, except for the first line of Table 4 where ground truth is used.

The parameters of the classifiers have been tuned to achieve maximum accuracy. This has been done on the clean training set. Ninety percentage of the set is used for train-ing of the categorization models, then categorization is per-formed on the remaining 10% set with several parameters until maximum accuracy is obtained. The optimal parame-ters for the kNN algorithm are 15 neighbors and 300 relevant terms, while 1,000 relevant terms are optimal for the SVM classifier.

The results presented in Table 4 are obtained in a docu-ment-pivoted manner, i.e., by sorting the category confidence scores per test document and assigning the top-ranking cate-gory to documents. When compared with the clean version of the texts, an accuracy loss of about 10% is observed using the lk-free resource regardless of the categorization method. For the time being, this can be explained by the poor capability of this resource to extract the relevant terms; it has to be related with the 51.85% of RTER. The recognition results obtained with lk-free are not suitable for efficient TC. On the other hand, lk-text performs well with both methods. Performance loss is about 2% for SVM and 1% for kNN when compared with clean texts.

Unfortunately, there X  X  no clear contribution of lk-text + categorization performance despite the boosting in the rele-vant term extraction performance. This means that RTER is not sufficient to assess the performances of the categorization system. Figure 7 confirms the poor predictive quality of the RTER. For each category, we plot the accuracy against the RTER, the error bars indicate the performances obtained with the clean set. The higher the RTER, the lower the accuracy should be with respect to the baseline score. However, such behavior is not observed. Moreover, for some categories, the baseline score is below the plotted point, which means that accuracy improves. Note that this only happens for categories with few documents where one document can ac-count for 10% of the total accuracy.

The RTER can tell us how many relevant terms are pre-served, but not which percentage of the document transcrip-tion they account for. The measures used so far (WER, TER and RTER) are focused on substitution errors, and neglect the noise introduced by insertions and deletions. The impact of these errors can be observed on the coverage plan (CP) [ 18 ]. However, we are only concerned about the relevant terms whereas Vinciarelli [ 18 ] used all the terms. Figure 8 shows term recall (TR, relevant terms preserved), against term precision (TP, relevant terms actually corresponding to relevant terms in the clean text) plots. We focused on doc-uments whose predicted category changed when compared to the results obtained with the clean versions of the texts. Unlike in [ 18 ], we cannot ensure that a document that achieves a TP and TR of 100% is categorized identically. This is particularly obvious for kNN, and the reason is that noise modified the underlying structure of the data. The proportion of documents having 100% TP is very high for kNN whereas it is very low for SVM. This happens because the feature space size is bigger for SVM. Despite having doc-uments with lower TP, SVM behaves more consistently, i.e., plus signs appear less often. This can be considered as a more robust generalization property of SVM with respect to noise in the training set.

We have to keep in mind that the relevant terms are selected based on the results of recognition on the training set: at that stage, the lk-text + resource cannot be used since relevant terms are not yet known, meaning that we could be very efficient in recognizing these terms on the test set, but due to recognition errors on the training set, the relevant term selection would not be very good.

By using only the relevant terms in the CP, we implic-itly take into account the representativity of the terms with respect to the categories. The Information Gain Plan (IGP) [ 18 ] explicitly take into account which terms are more rep-resentative of documents of individual categories, however, it relies on a particular measurement method of term use-fulness, and whenever any other measure is used for term selection, the relationship between the IGP and TC perfor-mance will not hold. Furthermore, in our context, the IGP will also be impacted by the noise present in the training set, hence, the goodness criterion score will not be as reliable as it would be when using clean texts for training.

The precision versus recall curves presented in Fig. 9 are obtained in a category-pivoted fashion, i.e., by ranking the document confidence scores per category. Precision versus recall curves can be calculated by considering the number of true positives in the ranked list above a given recall point [ 32 ]. The main advantage of ranking-based evaluation is to provide an upper bound on the possible performances. Whereas setting a threshold [ 37 ] lead us to a single mea-sure not giving a comprehensive description of the system X  X  behavior.
 Figure 9 a shows that micro-averaged performances of SVM and KNN with clean texts are close for recall lev-els up to 60%, above this level SVM is noticeably better. lk-text + performances confirm again that the RTER is not a good indicator of noise. lk-text obtains acceptable results with both methods for recall values up to 50%.

For recall values up to 80%, Fig. 9 b shows no clear distinction between kNN and SVM macro-averaged preci-sion for clean texts. Yet, in most situations, SVM outperforms kNN.

The results presented so far are oriented to give a gen-eral description of the system performance. In application environments, where the final decision is to be taken by a human expert, a system might simply rank the answer set according to the estimated scores of documents with respect to a category c . Since the human expert would just examine the top-ranked documents instead of the entire document set, precision can be computed at given document cutoff values. Such a measure is called precision at position n . According to this measure, a good system is supposed to rank all the rel-evant documents at top positions, i.e., to have high precision at position n .

The curves presented in Fig. 10 evaluate a different aspect of the system behavior. Precision at n assumes that the user is interested in finding only the first n documents belonging to a category c , whereas in precision versus recall curves, the user is assumed to be interested in finding all the documents of c .

As seen in Fig. 10 ,at n = 10, the precision is between 90 and 96% for SVM, and between 89 and 98% for kNN. This means that, on average, at least 9 documents out of the top 10 actually belong to the category specified by the user.
When using precision at position n , kNN and SVM do not show the same differences as in Fig. 9 a, b because the per-formance is influenced by the low recall points of the curves where the performances of the configurations are closer to each other.

We can see that kNN with lk-text always performs equals or better than with the clean set. With lk-text + , this is only the case for n  X  4. For SVM, the differences are only clear at specific values of n , the systems behave more consistently with respect to noise. We can also observe that lk-text + not enhance the capability of the system to rank relevant doc-uments first.

For this measure, there is no distinction between micro-and macro-average. Since the size of the answer set | A | responds to n .

Generally speaking, kNN suffer less from the impact of noisy texts than SVM. This can be a result of kNN optimal feature set parameterization. Tables 5 and 6 show the feature set overlapping rate between feature sets extracted from the different datasets.

We can see that overlapping rates between lk-text and clean are almost the same for kNN and SVM, this aspect can-not explain the higher impact of noisy texts on SVM, rather than the nature of the SVMs themselves [ 38 ]. Furthermore, the overlapping rate between lk-free and clean is higher for kNN, yet accuracy loss with lk-free is slightly higher for kNN (10.64%) than for SVM (8.91%). Referring back to Fig. 8 we can see that degradations in performance do not only depend on the capability of the system to better recognize relevant terms in the test set. More complex interactions exist when we use noisy texts in the training phase because the struc-ture of the data can significantly change due to recognition errors.

SVM can work and yield better results with many relevant features [ 39 ]; however, the effect of feature space dimension-ality on the SVM performance has only been investigated using electronic texts. On the other hand, the accuracy of kNN algorithms significantly drops in domains where many data attributes are irrelevant [ 40 ] or with high dimension-ality [ 41 ]. Furthermore, the choice of a set of features that is discriminating and non-overlapping across categories is a decisive factor [ 41 ].

However, these empirical facts do not systematically hold when dealing with noisy texts. The size of the feature space, and Forman X  X  round robin algorithm [ 29 ], designed to make a discriminating feature space across categories, cannot by themselves explain why kNN is less affected by recogni-tion errors. One fact that needs to be taken into account is that performances are less degraded with kNN because some documents tipped over into the correct category thanks to noise (see Fig. 8 ).
Another important aspect is that, since our goal was to compare the performances of the system for the same task (i.e. categorization of the clean set), the optimal parameters were estimated using the clean set, without guarantee that such parameters are optimal for noisy data. 6.3 Significance tests In order to assess which performance loss is acceptable or small in a statistical sense, we conducted several Wilcoxon rank tests [ 42 ] on categorization results. By comparing per-formances with this test, we can tell whether a significant loss is observed when we use the noisy datasets instead of the clean documents.

The null-hypothesis H 0 claims that there is a significant difference between clean and noisy text categorization. The alternative hypothesis H 1 states that no significant difference exists. H 0 is rejected when the Wilcoxon test results in a p -value &lt; 0 . 05, then, the alternative hypothesis is accepted at the 95% confidence level.

Table 7 shows the results of the significance tests based on the paired accuracy values of individual categories. Accura-cies of each noisy dataset are compared to those obtained with the clean dataset. A significant loss is observed for lk-free with both methods while no significant loss is ob-served with the other datasets.

Considering that half the relevant terms are lost, perfor-mances obtained with lk-free are rather good. Still, a signif-icant loss is observed when we compare lk-free results with performances obtained on clean documents. This confirms our previous observations about the suitability of the lk-free resource for efficient TC.

Tables 8 and 9 show the results of the Wilcoxon test for precision versus recall curve comparison. Paired precision values for each recall level are compared between the noisy datasets and the ground truth set.

The results presented above show that a significant loss is observed for every noisy set with SVM. Recognition errors may result in irrelevant features, and SVM can be very sen-sitive to them [ 38 ]. However, if we look at Fig. 9 a, b, we can see that at some points, the curves are closer to each other. If we want to know at which recall levels a significant loss exists, we have to compare curves at a given point. The tests presented in Tables 10 and 11 are based on the paired precision values of individual categories: for a given recall level, precision is computed for each category, and pairwise comparisons performed.

The results show that for recall values greater than 20 X  40% depending on the TC method, the system is unable to handle documents where half the relevant information is lost: once again this confirms that the lk-free resource is not suited for effective TC. As for lk-text and lk-text + , a significant loss is observed for recall levels between 60 and 80% with SVM. This is the reason why micro-and macro-precision-based significance tests conclude a significant loss for SVM with all the noisy sets.

The results of the Wilcoxon test for precision at position n comparison (see Table 12 ) can seem odd to say the least. There is no significant loss with SVM, whereas in Tables 8 and 9 , a significant loss was observed. A significant loss also exists for kNN and lk-free, this is not surprising if we consider the position of the corresponding curve in Fig. 10 . The Wilcoxon test yields a significant difference for kNN and lk-text as well. This cannot be considered as a significant loss since the corresponding curve is always equal to or better than kNN/clean X  X  one.

These results have to be related to the fact that precision at position n is a measure that is heavily influenced by the high-est positions in the ranking (i.e. low recall points of curves in Fig. 9 a, b). This measure only evaluates the ability of the sys-tem to rank relevant documents earlier, and not throughout the entire collection. 7 Conclusion In this paper, we presented a thorough study of the im-pact of noise using two different text categorization algo-rithms. By noise, we mean word insertions, deletions and substitutions produced during the recognition process of a handwritten text. The proposed online handwritten text cate-gorization engine consists of a handwriting recognition sys-tem pipelined into the input of a text categorization engine. Previous works [ 18 ] reported similar categorization systems and showed that categorization of noisy texts can be per-formed using models trained over clean electronic texts. Whether effective categorization can be done using models trained over noisy texts is one of the questions we address in this paper.

A subset of the Reuters-21578 dataset has been used as ground truth and great efforts have been made to collect an online handwritten version of it. Several linguistic resources are used within the online recognition engine resulting in dif-ferent levels of noise, when compared to the original clean set. First, categorization is performed on the ground truth dataset and its effectiveness is measured from several points of view. The same process is repeated over all the noisy ver-sions of the dataset.

Significance tests based on pair-wise comparison be-tween performances obtained with clean and noisy texts were performed. The results showed that no significant loss is observed with respect to performance achieved on the ground truth set. Effective TC, using state-of-the-art categorization methods and state-of-the-art online handwriting recogni-tion engines, can be performed using categorization models trained over noisy texts obtained through recognition.
However, a deep understanding of the relationship between the level and type of noise that is present in doc-uments corresponding to the output of an online handwriting recognition system and its effects on the categorization is still lacking. Experimental results showed that noise has a deep impact on the structure of the data and that every stage in the pipeline suffers from recognition errors: from tokenization to relevant term selection. Very complex interactions appear when noisy documents are used in the training phase, and existing measures do not suffice to describe such phenom-ena. Several aspects require more exhaustive studies, such as the impact of noise on the term usefulness measures, and categorization models. As we dispose by now of a signifi-cant handwritten data set, complementary experiments will be conducted to further investigate these issues.
 References
