 There is a notable interest in extending probabilistic gener-ativemodelingprinciplestoaccommodateformorecomplex structureddatatypes. Inthispaperwedevelopagenerative probabilistic model for visualizing sets of discrete symbolic sequences. The model, a constrained mixture of discrete hidden Markov models, is a generalization of density-based visualization methods previously developed for static data sets. We illustrate our approach on sequences representing web-log data and chorals by J.S. Bach.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Algorithms, Design, Theory Keywords: Hidden Markov model, latent space models, topographic mapping, EM algorithm
Topographic visualisation techniques have been an im-portant tool in multi-variate data analysis and data min-ing. Generative probabilistic approaches [2, 3, 7] have been developed and demonstrated to offer numerous advantages over non-probabilistic alternatives in terms of a flexibleand technically sound framework that makes various extensions possible in a principled manner.

However, in their current form, most of these methods make an assumption that observation data can be repre-sented in the form of a set of i.i.d. unstructured numerical vectors. Whilethismaybeareasonableassumptioninsome cases, many of the most recent practical problems face us with the notion of structured observation types. This cre-ates new challenges for data analysis research. Algorithms that are able to discover structure in sample sets of such structuredentitiesneedtobedeveloped. Practicalexamples include various user profiling tasks, where, in the simplest case, one observation consists of a log traceleft bya useras a result of interacting with an electronic environment.
Therehasbeenanotableinterestinextendingprobabilis-ticgenerativemodelingprinciplestoaccommodate formore complex structured data types [4, 6, 13, 14]. The work in [4] essentially provides a method of visualisation of user navigation sequences based on clustering of Markov chains. Probabilistic clustering of hidden Markov models have also been developed [13] and applied to user navigation mod-elling in [14]. In addition to clustering models, the work in [6] proposes a computationally efficient convex distributed dynamicmodel for profilingand prediction of dynamicuser activity.

However, none of these methods provide features for a topographic organization ofthesemorecomplexdatatypes. Topographicmappingsrequirenonlineardistributedmodels in order to preserve the core of the information contained inthepossibly complex andheterogeneous setofstructured observations in a 2D visualisation plane.

Thenecessityofvisualisingnon-vectorialstructuredtypes of data has been recognised in the literature and there are non-probabilistic,SOM-basedapproachestoe.g. visualising time series [5, 9]. In [5], a self organising map of first order Markov chains is developed and it is pointed out that the Euclidean distance measures initially employed in the non-probabilisticSOMmethodarenotappropriateforclustering or visualisation based on probabilistic models. A Kullback-Leibler divergence is then employed within the SOM, how-ever, this heuristic does not follow from a consistent model formulation.

In this paper we develop a consistent generative proba-bilistic model for visualizing sets of discrete symbolic se-quences,whereanappropriatedivergencemeasureisdefined by the noise model 1 . The model is a constrained mixture of discrete hidden Markov models (HMM) [11]), where the constraint, introduced in the spirit of [2, 7], provides the model with topographic organisation capabilities.
The remainder of the paper is organised as follows: Sec-tion2introducesthemodelandthetrainingalgorithm. Sec-tion 3 provides experimental illustration of the method on two real world data sets: melodic lines of chorales by J.S. Bachandwebnavigationsequences. Weconcludethepaper withabriefsummaryofthekeyideasandfindingsinsection 4.
Empirical Kullback-Leibler divergence between the un-known true distribution that generated the data item (se-quence) and the reference hidden Markov model. Consider aset ofsymbolic sequencesoverthealphabet of S symbols, S = { 1 , 2 , ..., S } . The sequences can represent e.g. melodic lines, or traces of web users requested by a population of users. The n -th sequence will be denoted by s length of the n -th sequence. The lengths of the sequences may vary. Consider further an (L=2)-dimensional latent space [  X  1 , 1] 2 . The aim is to represent each sequence as usingthelatentspace,suchthattheimportantoverallchar-acteristicsofthesetosequencesarerevealed. Anaturalway ofachievingthisistoimposeamaximumentropy(uniform) distribution over the latent space. For tractability reasons it is convenient to discretize the latent space into a regu-lar grid of C points x 1 , ..., x C . These sample (grid) points are analogous to the nodes of a Self Organising Map. With each grid point x c , we associate a generative distribution over sequences p ( s | x c ).

Assuming the sequences s ( n ) , n =1: N , were indepen-dently generated, the data likelihood of our model is
In order account for temporal dependency the sequences, we let the noise terms p ( s | x c ) to take the form of hidden Markov models with K hidden states [11]: where h is theset of all T n -tuplesoverthe K hiddenstates.
The (logarithm of the) data likelihood needs now to be maximised. As there are hidden variables in the model, an EM-type solution will be adopted. According to the EM methodology, the expectation of the complete log like-lihood needs to be maximised. The complete latent-centre-conditionaldatadistributionfactorisesintoseveralmultino-mials: In orderto havetheHMMcomponents topologically organ-ised  X  e.g. on a two-dimensional equidistant grid  X  we will, in the spirit of [2, 7], constrain the mixture of HMMs, byrequiringthattheHMMparametersbegeneratedthrough a parameterised smooth nonlinear mapping from the latent space into the HMM parameter space. In particular where
The expectation (with respect to the posterior distribu-tion over the hidden variables, given the observed data) of the complete data log likelihood (relative likelihood [2]) is Q =
Substituting the above quantities and solving stationary equations w.r.t. all parameters, we obtain the following al-gorithm: uniform prior distribution of latent classes encourages full use of the available visualization space
Havingtrained themodel on a set of sequences s (1) , s (2) ..., s ( N ) , each sequence can now be represented by a point in the latent space  X  the mean of the posterior distribution over the latent space, given that sequence: This way, each sequence is mapped to one point in the 2D visualisationspace. Becausedynamicmodelshavebeenem-ployed as the noise models of the generative topographic mapping, dynamic structure of the sequences is the main featurethatdeterminesthenotionof X  X loseness X  ofsequence representations in the latent space.
Intheexperimentsreportedbelowthelatentspacecentres x c werepositionedonaregular10  X  10squaregrid( C =100) and there were M =16 basis functions  X  i . The basis func-tions were spherical Gaussian functions of the same width  X  =1 . 0. The basis functions were centred on a regular 4  X  4squaregrid,reflectinguniformdistributionofthelatent classes. We account for a bias term by using an additional constant basis function  X  17 ( x )=1.

Free parameters of the model were randomly initialized in the interval [  X  1 , 1]. Training consisted of repeating EM integrations. Typically,thelikelihoodlevelledupafter30-50 EM cycles. In this experiment we visualize a set of 100 chorales by J.S. Bach [10]. We extracted themelodic lines  X  pitches are represented in the space of one octave, i.e. the observation symbol space consists of 12 different pitch values. Tempo-ral structure of the sequences is the essential feature to be considered when organizing the data items in any sensible manner.

Figure1showstheposteriormeanmappingobtainedwith our model. The method has essentially discovered the nat-ural topography of the key signatures, corroborated with similarities of melodic motives. The upperrightregion con-tains the mapping of melodic lines that utilise keys with sharps. Coming towards the center region of the visualisa-tionspacewegraduallyfindkeyswithlessandlesssharpsto natural keys. There are no sharps or flats in thecenter and upperrightregions of theplot, whilethelower regionof the plot is concerned with flats. The number of flats increases fromlefttoright. Themelodiescancontainsharpsandflats otherthanthoseincludedintheirkeysignatureduetoboth modulation and ornaments.

More interesting is to observe sub-groupings created ac-cording to melodic motives (patterns). The three melodic lines closest to the left boundary of the plot all contain a very characteristic and tense expressive melodic pattern g-f#-bb-a (that has interval of 4-), motive that can only be found in minor keys and is in general employed rather infrequently and with good musical reason only. Interest-ingly, the two closest points to those three do also contain an alleviated version of the same pattern ( g-f#-g-a-bb-a ), where the interval of 4-( f#-bb ) is not explicit. Moreover, theclosest point of thecluster from theupper region of the plot has (after a key modulation) a reversed form of this motive ( g-f#-e-d#-e ) that is far not as tensive as the ones previouslymentioned. The(thistimedescending)4-( g-d# ) is now  X  X esolved X  (to e ) within the motive.

To conclude, the benefit of the topographical representa-tion is twofold: (1) More generally, it offers a means of or-ganisinganotherwisepossiblytedioussetofdatastructures in an intuitively understandable compact form and (2) for this specific example it also has the potential of providing a way of automatically addressing the phenomenon known as enharmony[1]. Enharmonyreferstothesituation whenthe same physical pitch value may have different musical inter-pretations as a function of the context (key and temporal structure). For example, a c# and a db are both encoded with the same number in a MIDI file, and are indeed phys-ically the same frequency in a tempered intonation system. Thisisachievedbyfeaturingthe context inwhichthepitches appear. Correctly recognising enharmonincsis essential e.g. in automatic chord annotation.
The data considered in this experiment is a subset of the msnbc.com user navigation collection initially employed in [4] and also used in [6]. There are 1,480 browsing sessions totalling 119,667 page requests in this data set. The ses-sions consists of navigation patternsbyusers who visitedat least 9 of the 17 page categories (frontpage, news, tech, lo-cal,opinion,on-air,misc,weather,msn-news,health,living, business, msn-sports, sports, summary, bbs, travel). From the analysis in [6] using Markov chains, where the state space consisted of the above page categories, it transpires, that state repetition is a common feature of all browsing behaviours. Therefore, in this experiment, rather than con-centrating on individual page categories, we consider a dif-ferent alphabet. It will allow us to visually detect browsing behaviours in terms of the user X  X   X  X unger for novelty X . To thisend,wehavedefinedthreesymbols:  X  1  X : repeatthelast categoryrequest X  2  X : returntocategoryrequestedtwomoves before,  X  3  X : all the other cases.

The browsing sessions are visualised in figure 2. To un-derstand the plot better we also show the state transition probabilities and the emission probabilities of each of the 10  X  10 = 100 hidden Markov models underlying the visu-alization system. State transition structures are shown in figure 3(a) as a grid of maps of K  X  K =2  X  2 state tran-emission probabilities in a grid of S  X  K =3  X  2matrices p ( s | k, x c ).

Strongstructureofemissionprobabilitiesisclearlyvisible in figure 3(b). The second hidden state is devoted almost exclusively to symbol 1 - X  X epeat the last category request X . Indeed, as mentioned earlier, category repetition is a com-mon feature of all browsing behaviours (see [6]). In gen-eral, the first hidden state takes care of symbol 3 that en-compassesallbrowsingbehavioursdifferentfrom X  X epeatthe last categoryrequest X  X nd X  X eturntocategoryrequestedtwo moves before X . Such patterns can potentially include non-trivial navigation histories. Symbol 2 ( X  X eturn to category requested two moves before X ) is less frequent than symbols 1 and 3 and is usually explained by the first state.
While the emission structure in figure 3(b) tells us about the marginal frequencies of symbols in sequences captured by the underlying HMMs, it is the state-transition struc-ture in figure 3(a) that determines the temporal correla-tions between the symbols, i.e. the lengths of continuous blocks of symbols generated from one state. The stronger is the self-loop p ( h t = k | h t  X  1 = k, x c ) in state k of HMM c , the longer blocks of symbols favoured by that state (i.e. withhigheremissionprobabilities p ( s | k, x c ))areadmissible. Transition probabilities close to 1 /K =0 . 5 indicateapossi-bility for symbol production arising from highly oscillating hidden states.

Moving from top to bottom of the latent space, we ob-serve almost independent hidden states, with little chance of mutual transitions. Then the first state loses mass in its self-transition in favour of the second state, which becomes almostatrapstate. Towardsthebottomofthelatentspace, the first state recovers its power through a band of mixing patternsoverthetwohiddenstates. Themixingisstrongest in the vicinity of the lower-left corner of the latent square.
Top of the plot is reserved for sequences containing long blocks of consecutive 1 s and subsequences of 2 sand 3 s. Rightpartofthetopclustercontainssequenceswithpoten-tiallylongblocksof 1 sand 2 s.Sequencesmappedintheleft part of the top cluster contain potentially long blocks of 1 s and 3 s. Sequencescorrespondingtobrowsingwithinasingle categoryarerepresentedinthecenteroftheplot. Lower-left corner is devoted to sequences of broad exploratory brows-ingbehaviour,whereallkindsofmovesarepossible,without longer persistent blocks of the same type of navigation be-haviour. Denseclusterofsequencesinthemiddleofthebot-tompartof theplotrepresentsnavigation patternscontain-ing long periods of staying within the same category (con-secutive 1 s), interleaved with long periods of possibly non-trivial inter-topic search (consecutive 3 s). Such sequences can also be found in the left part of the top cluster. In this sense,thereisahintofcylindricalorganization ofthelatent space.
We have presented a generative probabilistic model for visualizing sets of discrete symbolic sequences. The model is essentially a constrained mixture of hidden Markov mod-els that allows us to represent non-Markovian dynamical structures in a two-dimensional visualisation plane. For-mally the model is a generalization of density-based visual-izationmethodspreviouslydevelopedforstaticdatasets[7]. We illustrated the model on sequences representing web-log data and chorals by J.S. Bach. We experimented with hid-denMarkovnoisemodelswithmorethantwohiddenstates, subsequence contained in the projected sequence. the web navigation experiment. the Bach chorals experiment. however, the structure extracted by two-state models was already rich and interesting enough.

Ourmodelconstitutesaprincipledapproachtovisualiza-tionofsetsofsymbolicsequences. Probabilisticformulation enables the model to deal with e.g missing data, hierarchy building, or model selection in a consistent manner. Vi-sualization plots can be naturally interpreted by plotting the state-transition and emission structures of the hidden Markov noise models corresponding to local regions of the latent space. In this sense, hidden Markov models are a much more viable option than standard fixed-order Markov noise models  X  going beyond the first-order Markov chain structure can prohibitively increase the number of states.
When the task we are facing is, for example, building a good probabilistic model for a given data set of sequences, without any concern for data visualization, then a suit-able approach may be to use e.g. mixtures of HMMs, with appropriately chosen number of mixture components us-ing a model selection technique. On the other hand, for modelbased visualization of sequential data, we may use many HMM components, but constrain them with a tight two-dimensional grid neighborhood structure. Such a con-strained mixture ofHMM maynot beable tocompete with appropriatelyconstructed(probablysmaller)unconstrained mixtureofHMMonthegroundsofdensitymodeling,butit is suitable for data visualization and importantly, the tight grid topology prevents constrained models with many com-ponents (suitable for high-quality visualization) from exces-sivelyoverfittingthedata. Theissueofdataexplanationvs. datapredictioniscoverede.g. in[12]. Toevaluatetendency of our model to overfit the training data, we split the 100 Bach chorals into training and test sets containing 80 and 20 sequences, respectively. The model is trained solely on trainingsequences. Figure4showstheevolutionofnegative log-likelihood (NLL) per symbol measured on the training and test sets. After the 12th training epoch, the test set NLL stops decreasing, but crucially, due to the constrained nature of our model, it does not tend to increase at the expense of better modeling of the training data. Scaling is, however, an issue. The E-step complexity is O ( NCTK 2 ). Obviously, a generative topographic formula-tionwithfixed-orderMarkovchainswouldbemuchcheaper, since the noise models would not involve hidden variables. On the other hand, as argued above, the flexibility and in-terpretability of such formulations would be compromised. Also, themodelcan betrained on a subsampleof theavail-able data and then used to visualize the whole data set. We are currently working on speeding up computations in ourmodelthroughavarietyofcheaperapproximationtech-niques.
