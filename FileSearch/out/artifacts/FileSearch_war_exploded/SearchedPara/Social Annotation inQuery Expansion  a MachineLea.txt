 Automatic query expansion technologies have been proven to be effective in many information retrieval tasks. Most existing approaches are based on the assumption that the most informative terms in top-retrieved documents can be viewed as context of the query and thus can be used for query expansion. One problem with these approaches is that some of the expansion terms extracted from feedback documents are irrelevant to the query, and thus may hurt the retrieval performance. In social annotations, users pro-vide different keywords describing the respective Web pages from various aspects. These features may be used to boost IR performance. However, to date, the potential of social annotation for this task has been largely unexplored. In this paper, we explore the possibility and potential of social annotation as a new resource for extracting useful expansion terms. In particular, we propose a term ranking approach based on social annotation resource. The proposed approach consists of two phases: (1) in the first phase, we propose a term-dependency method to choose the most likely expan-sion terms; (2) in the second phase, we develop a machine learning method for term ranking, which is learnt from the statistics of the candidate expansion terms, using ListNet. Experimental results on three TREC test collections show that the retrieval performance can be improved when the term ranking method is used. In addition, we also demon-strate that terms selected by the term-dependency method from social annotation resources are beneficial to improve the retrieval performance.
 H.3.3 [ Information Search and Retrieval ]: Search and Retrieval X  search process, query formulation Algorithms, Performance, Design, Experimentation Query Expansion, Social Annotation, Learning to Rank
Queries submitted to search engines always contain very few keywords or phrases, which are generally insufficient to fully describe a user X  X  information need. To solve this problem, query expansion (QE) has been widely used [3, 27, 8, 18]. Among all the methods, pseudo-relevance feed-back (PRF) via query expansion has proven to be effective in many information retrieval (IR) tasks [25, 14]. The ba-sic assumption of PRF is that the top-ranked documents in the initial retrieval result contain many useful terms that can help describe the information need better. However, this assumption is often invalid [4] which can result in a negative impact on the retrieval performance. With a rapid development of social communities, a large amount of manu-ally generated annotations, the so-called tags, have emerged which can be a potential resource for query expansion.
In recent years, with the rise of Web 2.0 technologies, so-cial annotations have become a popular way to allow users to contribute descriptive metadata for Web information, such as Web pages and photos. A series of studies have been done on exploring the social annotations for folksonomy [16], rec-ommendation [22], semantic Web [26], Web search [11, 2, 28] etc. Positive impact has been found in these studies. However, it is not clear whether social annotations can be of help as a resource for query expansion. As described in [26], social tags, which link to each other through Web resource, often share similar topics. Thus, a set of tags attached with the same resource (e.g. Web page, photo, production) could help better understand it.

In this paper, we propose a novel query expansion method based on social annotations which are used as the resource of expansion terms. Given a query, a large number of can-didate expansion terms (words or phrases) will be chosen to convey users X  information needs. We propose a term-dependency method to select the candidate expansion terms based on two term-dependence assumptions of query terms: 1) full independence, 2) sequential dependence. From our initial results of this method, some of the candidate ex-pansion terms are indeed unrelated to the original query. To solve this problem, we develop a novel learning to rank method to rank the candidate terms according to their po-tential impact on retrieval effectiveness. Once the ranking list is obtained, the top ranked terms will be selected to expand the original query.

The contributions of this paper are as follows: 1) we con-duct extensive experiments to evaluate the potential of so-cial annotations as a resource of expansion terms for query expansion, 2) in expansion terms selection process, we inves-tigate two term-dependence assumptions for selecting useful expansion terms from social annotations, 3) according to the impact of expansion terms selected based on these two as-sumptions, we develop a learning to rank approach for the novel expansion terms ranking: the list of expansion terms pertaining to a query is seemed as an instance , and the sta-tistical properties of terms in social annotations are used as the features for learning a ranking model.

The remainder of our paper is organized as follows. Sec-tion 2 reviews some related work. Section 3 explores the potential of social annotations as a new resource of expan-sion terms. Section 4 proposes the expansion term ranking method based on the social annotation sample. In Section 5, we report the experimental results and list some discussions about our work. Finally, we conclude the paper and discuss future work in Section 6.
Automatic query expansion technique has been widely used in IR. Among all the approaches, pseudo-relevance feedback (PRF) has been shown more effective by reformu-lating the original query using expansion terms from pseudo-relevant documents. Traditional PRF has been implemented in different retrieval models: vector space model [21], proba-bilistic model [20], relevance model [12], mixture model [32] and so on. Meanwhile, large amounts of research has been conducted to improve traditional PRF by using passages instead of documents [31], by using a local context anal-ysis method [27], by using a query-regularized estimation method [25], by using latent concepts [18], and by using a clustered-based re-sampling method for generating pseudo-relevant documents [14]. These methods follow the basic assumption that the top-ranked documents from an initial search contain many useful terms that can help discriminate relevant documents from irrelevant ones.

Despite the large number of studies, a crucial question is the expansion terms determined in traditional ways from the pseudo-relevant documents are not all useful [4]. Some stud-ies focus on using an external resource for query expansion. They found one of the query expansion failure reasons is the lack of relevant documents in the local collection. There-fore, the performance of query expansion can be improved by using a large external collection. Several external col-lection enrichment approaches have been proposed, such as search engine query logs [9], some thesauruses (e.g. Word-net) [7], Wikipedia [29] etc. Our work follows this strategy of a query expansion approach using an external collection as a resource of query expansion terms.

Recently, many researches have focused on the social an-notations in large part motivated by their increasing avail-ability across many Web-based applications. P. Mika [19] proposed a tripartite model of actors, concepts and instances for semantic emergence. Heymann et. al [10] found that the social annotation data had a good coverage of interesting pages on the Web. X. Wu et al. [26] explored machine un-derstandable semantics from social annotations in a statisti-cal way and applied the derived emergent semantics to dis-cover and search shared Web bookmarks. Hotho et al. [11] proposed Adapted PageRank and FolkRank to find commu-nities within the folksonomy but have not applied them to Web search. Bao et al. [2] proposed to measure the similar-ity and popularity of Web pages from Web users X  perspective by calculating SocialSimRank and SocialPageRank. Xu et al. [28] proposed a personalized search framework to uti-lize folksonomy for personalized search. Carman et al. [6] explored how useful tag data might be for improving search results, but they focused mainly on data analysis rather than retrieval experiments.

Different from the above work, we investigate the capabil-ity of social annotations in improving the retrieval perfor-mance as a promising resource for query expansion.
In this section, we briefly introduce the advantage of social annotations for our study, and then investigate the potential of the social annotations for filtering irrelevant terms for query expansion.
In social annotation services like Delicious [26], they al-low users to annotate and categorize Web resources with the keywords (called tags). These tags are freely chosen by the users without a pre-defined taxonomy or ontology. A single resource could be annotated with several tags from many disparate users. For example, the tags such as  X  X onference X ,  X  X esearch X ,  X  X cm X ,  X 2011 X , and  X  X igir X  X re used by many users to annotate Web page about SIGIR 2011 homepage. As we can see, tags can be the good keywords for describing the respective Web page from various aspects. Moreover, differ-ent tags describing the same Web resource are semantically related to some extent.

An annotation typically consists of at least three parts: the URL of the resource (e.g. a Web page), one or more tags, and the user who created the annotation. Thus we abstract it as a triple as follows: which means that user i has annotated URL k with tag j . In this paper, we focus on what resource annotated with what tags and do not care much about who annotated the resource.
A social annotation service (Delicious) has the potential to give us a great deal of data about Web pages. We collected a sample of Del.icio.us 1 data by crawling its website during March 2009. The data sample consists of 7,063,028 tags on 2,137,776 different URLs with 280,672 different tags.
In our study, we mainly utilize Web pages, tags, and rela-tionship between them. Thus, the social annotation sample is organized as one article per annotation by filtering out the user information. Each article will effectively summarize the most important information of each annotation. Based on our analysis of the social annotation structure, we divide each social annotation article into four fields as shown in Table 1.

Before the experiments, we perform two data preprocess-ing processes: 1) some tags only reflect personal require-ment, such as  X  toread  X ,  X  @read  X , etc. We remove some of them manually. 2) Some users may concatenate several cor-relative terms to form one tag, e.g.  X  java/programming  X , http ://delicious.com  X  Ira q war  X ,  X  news -business-nance  X . We split this kind of tags with the help of delimiters.
After analyzing a large amount of social annotations, we find that tags are usually semantically related to each other if they are used to annotate the same or related resources for many times. Intuitively, the social annotation collection can be seen as a manually edited thesaurus which provides a wealth of information of relevant terms.

In order to evaluate the potential usefulness of social an-notation sample as the expansion term resource, we will con-sider all the terms extracted from the social annotation sam-ple using the term co-occurrence method.

The term co-occurrence is usually used to measure how often terms appear together in the text window. In our experiment, the text window is defined as one annotation field (e.g. tags, title ). For a query term q j and a candidate term t i in the social annotation sample S , the co-occurrence value is defined as follows: cooc ( t i ,q j ) = wh ere N is the sum of articles in social annotation sample S , tf ( . | f ) is frequency of term appears in field f .
Base on this equation, terms with highest cooc ( t i ,q j chosen for the original query term q j as the candidate ex-pansion terms. The final expansion terms can be selected as follows: coof where idf is computed as log ( N/df ), N is the sum of articles in social annotation sample, and df is the number of articles which contain term t .

Inspired by the work of Cao et al. [4], we will test each of these terms to see its impact on the retrieval effectiveness. In order to make the test simpler, we make the following simpli-fications: 1) Each expansion term is assumed to act on the query independently from other expansion terms; 2) Each expansion term is added into the query with equal weight  X  (it is set at 0.01 or -0.01). Based on these simplifications, we measure the performance change due to the expansion term e by the ratio: where MAP ( Q ) and MAP ( Q  X  e ) are respectively the MAP of the original query and expanded query (expanded with e ).
In our previous work [30], we conducted some preliminary experiments. In our experiment, we set the threshold to Table 2: Proportions of each group of expansion terms selected from feedback documents Collec tion Good Terms Neutra l Terms Bad Terms Robu st 2004 16.5 7% 66.0 5% 17.3 8% Tabl e 3: Proportions of each group of expansion terms selected from social annotation sample Collec tion Good Terms Neutra l Terms Bad Terms
Robu st 2004 18.4 7% 60.0 5% 21.4 8% 0 . 005. It mean s good (or bad) expansion term which can improve (or hurt) the effectiveness should produce a perfor-mance change such that | chg ( e ) | &gt; 0 . 005.
Now, we will examine whether the candidate expansion terms from social annotation sample are good terms. Our verification is made on three TREC collections: AP, WSJ and Robust 2004. We respectively consider 150 queries for each collection and 100 expansion terms for each query with largest probabilities from pseudo-feedback documents and with largest co-occurrence values from social annotation sam-ple. Table 2 and 3 respectively show the proportion of good, bad and neutral expansion terms from feedback documents and social annotation collection for all the queries.
Comparing Table 2 and 3, we can see that the proportion of good terms extracted from social annotation sample is higher than it extracted from pseudo-feedback documents. It means that social annotations have the potential to pro-duce more good terms as the resource for query expansion. From Table 2 and 3, all the proportions of bad terms on all three collections are higher than that of the good ones X . This means while the proportion of good terms increases, social annotation sample indeed introduces more bad terms into the expansion process. The research of Cao et al. [4] shows that the retrieval effectiveness can be much improved if more good expansion terms are added to the original queries. The challenge now is to develop an effective method to correctly select the good terms in the expansion process.
The challenge to select good terms for query expansion consists of two parts: (1) how to select the candidate ex-pansion terms from the whole social annotation sample, (2) how to distinguish the different importance of these terms.
For the first part, we propose a term-dependency method to select the candidate expansion terms. When the candi-date expansion terms selected, we use a novel learning to rank method to rank the candidate terms. Once the rank-ing list is obtained, the top ranked terms in the list will be selected to expand the original query.
It is well known that the dependencies between terms exist in most queries. For example, in Topic 63 ( X  X achine trans-lation X ), occurrences of certain pairs of terms are correlated. The fac t that either one occurs provides strong evidence that the other is also likely to occur.

Eq. (3) assumes that the selection of each query term is determined independently, lacking consideration of latent term relations. Most work on modeling term dependencies in the past has analyzed three different underlying dependence assumptions: full independence , sequential dependence [23], and full dependence [17].

The full independence variant which assumes query terms are independent underlies many retrieval models. Eq. (3) is based on this assumption. The second variant we considered is the sequential dependence. In this variant we assume the dependence between neighboring query terms. Based on this assumption, we generalize Eq. (3) as follows: where n is the sum of query terms in original query Q .
The full dependence variant assumes all the query terms are in some way dependent on each other. Under this as-sumption, each combination of query terms should be con-sidered. In the experiments, the results using this variant are not satisfactory. One of the possible reasons is that some irrelevant term group may be introduced into the term se-lection procedure.

Therefore, we only consider the first two assumptions and apply the following function with linear interpolation, weighted by a parameter  X  .
Using the term-dependency method, we select the can-didate terms with the highest coof scores. For expansion term selection, the term-dependency method will be shown effective in Section 6.
A key idea of our term ranking approach is that one can generalize the knowledge of expansion terms from the past candidate ones to predict effective expansion terms for the novel queries. In order to do this, it requires a method for estimating a candidate term for each training query given examples of its relevant or irrelevant terms.

Intuitively, for query expansion technology, the relevant expansion terms are beneficial to improve the retrieval per-formance. Inspired by the work of Cao et al. [4], the chg ( e ) of expansion term e described in Section 3.3 could reflect its potential impact on retrieval effectiveness. Suppose that query q i has k expansion terms, the relevance label of ex-pansion term e j (1  X  j  X  k ) is defined as follows: where label ( e j ) = 1 reflects term e j is relevant to query q and label ( e j ) = 0 reflects term e j is irrelevant.
In our experiment, we use three TREC collections (see Ta-ble 4 in Section 5.1), with 150 queries for each collection. We divide these queries into three groups of 50 queries. In the training dataset, each query has a term list which ranks the candidate expansion terms according to chg ( e ). To generate the development dataset, we label each term with binary rel-evance judgments (relevant or irrelevant) according to Eq. (7). For each term in the dataset (training, development or test), we represent it with the features described in the next section.
This section describes our current feature set for the ex-pansion term. We utilize the statistical features of single term and also term pair. Each expansion term is represented by a feature vector.

Useful statistical features include those already used in traditional methods such as term frequency (TF), document frequency (DF). In order to capture the relationship between the candidate expansion terms and the original query terms, we consider the co-occurrences of them. The social annota-tion article described in Section 3.2 consists of four fields ( url, title, frequency, tags ), and we did not use the infor-mation in the url field in the experiment. Obviously, the importance of a term appearing in the title may be different than its appearance in the tags . We calculate the statis-tical features separately in title , tags , and the whole arti-cle. Each expansion term is represented by a feature vector: [ f in title field ( f title ). The others can be defined similarly.
The first features are the classic statistics of term fre-quency. In the experiment, a social annotation article described in Section 3.2 is defined as a document, and df of the expan-sion term e is the number of documents containing e in the specific field. The definitions of df features are:
The co-occurrence features are used to estimate the rela-tionship between high frequency terms more reliably. There-fore, we define the co-occurrence feature as follows: wh ere C ( q i ,e | f title ) is the frequency of co-occurrences of query term q i and the expansion term e in title field.
A stronger co-occurrence relation for an expansion term is with two query terms together. [1] has shown that this type of co-occurrence relation is much better because it can take into account some query contexts. wh ere  X  is the set of possible query term pairs. The frequency field denotes the annotation frequency of Web page, while it also reflects the popularity of Web page. A popular Web page will introduce many more popular tags which may be the good descriptor. Notice that we only develop the distinctive property for terms in tags field. where frequency ( e | f tags ) denotes frequency of the Web page contains the term e in tags field.

Our feature space is constructed of statistical features and the unique feature from social annotation sample. Given examples of target term weights paired with corresponding features, the following work is to predict the relevant score of expansion term with given the features. We accomplish this via the ListNet of learning to rank approaches.
ListNet [5] is a feature-based learning to rank approach, which minimizes a listwise ranking loss function based on the probability distribution on permutations. It utilizes result lists as instances in the learning procedure. Based on the neural network model[13], the gradient descent approach[15] is used to minimize the K-L divergence loss. ListNet outputs a linear ranking model that is used to predict the relevance score of a new object. In this paper we use this approach to learning a ranking model for the term ranking.
We now give a general description on learning to rank a set of expansion terms according to their effectiveness.
Assume that, a set of queries Q = { q 1 ,q 2 ,  X   X   X  ,q m given. Each query q i is associated with a set of possible usable terms T i = { t i; 1 ,t i; 2 ,  X   X   X  ,t i;n i } , where t the j -th term is relevant to the i -th query and n i denotes the sizes of T i .

Each list of terms T i is associated with a list of relevance judgment y i = { y i; 1 ,y i; 2 ,  X   X   X  ,y i;n i } , where y extent to which the candidate term y i;j is relevant to the original query q i . The measure to score the degree is defined as follows: wh ere  X  ( . ) is a performance measure function, in our experi-ment, we use MAP as the measure function. The assumption is that the higher performance improvement is observed for the combination of t i;j and q i the stronger relevance exists between them.

In training, the training set can be denoted as  X  = { ( x y The feature vector x i;j =  X  ( q i ,t i;j ) is created from each query-term pair ( q i ,t i;j ), i = 1 , 2 ,  X   X   X  ,m and j = 1 , 2 ,
For each feature vector x i;j , the ranking function f out-puts a score f ( x i;j ). Furthermore, we will obtain a list of tors x i . The objective of learning is formalized as minimiza-tion of the total losses with respect to the training data. where L is a loss function.

In the term ranking, when a new query q  X  and its asso-ciated terms t  X  are given, we construct feature vectors x from them and use the trained ranking function to assign scores to the terms t  X  . Finally we select the top k terms to expand the original query. We use a learning method for optimizing the loss function based on top gamma probabil-ity, with Neural Network as model and Gradient Descent as optimization algorithm [5].

Based on the Neural Network model  X  , we denote the ranking function as f ! . For a feature vector x i;j , f ! assigns a score to it. When we use Cross Entropy as metric, the loss function becomes:
I n our experiments, we implemented ListNet with  X  = 1.With some derivation, when  X  = 1 we have:
For simplicity, we use a linear Neural Network model in our experiments.
We used three standard TREC collections in our experi-ments: AP88-90(Associated Press); WSJ87-90(Wall St. Jour-nal); and Robust2004 (the dataset of TREC Robust Track Collec tion #Docs Train T opics Dev. Top ics Test T opics Robu st 2004 528, 155 301-350 351-400 401-450 Table 5: Performance comparisons of baseline mod-els for all the topics on AP, WSJ, Roust2004 collec-tions starte d in 2003). Table 4 shows the details of these collec-tions. For each dataset, we split the topics into three parts: the training data for the rank learner, the development data to estimate the parameters, and the test data.

Retrieval effectiveness is measured in terms of Mean Av-erage Precision (MAP) for top 1000 documents. When an original query Q is given, a set of M candidate expansion terms will be selected from social annotation sample by term-dependency method described in Section 4.1. Accord-ing to their potential impact on retrieval effectiveness, the ranking model will re-rank the M terms to form a new term ranking list. Once the ranking list is obtained, the top k terms will be selected to form an expansion query Q exp . In the experiments, the Indri 2.6 search engine [24] is used as our basic retrieval system.
In the experiments we select three baseline models; one is the query-likelihood language model ( QL ), the second is Lavrenko X  X  relevance model ( RM ) [12] implemented in Indri, the third is the expanded query model by relevance model using the oracle expansion terms selected from docu-ments ( RM+Oracle ). In relevance model RM, it retrieves a set of N documents and forms an expanded query Q exp by adding the top k most likely terms. Note that in RM+Oracle model, we select top k expansion terms with high chg ( e ) ac-cording to Eq. (4) for each query. The expanded query is formed with the following structure:
For the RM and RM+Oracle methods, we fixed the pa-rameters as follows: N = 10, k = 50 and  X  fb = 0 . 5, since we achieve relative good performance under this setting in general.

Table 5 and 6 respectively show the performance (MAP) for all the topics and all the test topics on three TREC collections. Comparing Table 5 and 6, the improvements over the QL model for all the topics and the test topics are on the same levels. In order to facilitate comparison with term ranking model, we only use the test topics to do the retrieval experiments.

As can be seen from Table 6, the relevance model can sig-nificantly improve the retrieval performance over the query likelihood language model. Compared with the results of the RM and RM+Oracle methods, the retrieval effective-ness can be much improved if the oracle expansion terms Table 6: Performance comparisons of baseline mod-els for all the test topics on AP, WSJ, Roust2004 collections are add ed. This shows the usefulness of correctly selecting the expansion terms with the high potential of improving the retrieval effectiveness. The MAP of the RM+Oracle method represents the upper bound retrieval effectiveness we can expect to obtain using expansion terms extracted from pseudo-relevance documents. These three models serve as the baseline for the following experiments.
We now turn our attention to our proposed method utiliz-ing the social annotation collection ( SA ) as the resource of query expansion. In order to extract the effective expansion terms from the social annotation collection, we propose an unsupervised method and a supervised learning model.
For the unsupervised method, we investigate the method based on two different term dependence assumptions: full independence and sequential dependence . In Section 6.1 and 6.2, we evaluate the performance of the unsupervised method, and also demonstrate the social annotation collection could be a good resource for extracting useful expansion terms.
For the supervised learning model, we examine the quality of the term ranking model, and then investigate the perfor-mance of the term ranking model for query expansion task.
We first evaluate the performance of the method based on full independence ( SA+FI ). Note that the SA+FI method selects the top k terms with highest co-occurrence accord-ing to Eq. (3). From Table 7, we can see that the SA+FI method enhance the retrieval performance over the QL model on all the collections. This indicates that the expansion terms social annotations provided is closely related to the query. Compared with the RM model, the SA+FI method does not work as well as it. Although social annotations may produce more good terms by the SA+FI method, more bad terms is also introduced into the query. That is the most likely reason for the poor performance of the SA+FI method.

The SA+FI method assumes the query terms are indepen-dent to each other. Under this assumption, some irrelevant terms may be selected by the SA+FI method. For example, on AP, Topic 159 ( X  X lectric car development X ) scores a mean average precision (MAP) of 0.3100 in the QL model, and in the SA+FI method the MAP of Topic 159 is increased to 0.3657. Using this method, the top ranked expansion terms of Topic 159 are  X  X ar X ,  X  X lectric X ,  X  X evelopment X ,  X  X ro-gram X ,  X  X nergy X ,  X  X uto X ,  X  X ool X ,  X  X esign X ,  X  X reen X  and  X  X v X . Although the MAP is increased after adding the expansion terms, some irrelevant terms (e.g.  X  X rogram X ,  X  X ool X , and Table 7: Performance comparisons of SA+FI, SA+TD, SA+TDW and SA+Oracle for all the test topics on AP, WSJ, Robust2004 collections.
 Figure 1: Performance on different weights for SA+TD on AP, WSJ and Robust2004 collections  X  X esign X ) may also hurt the retrieval performance. To exam-ine the possible impact with the irrelevant terms, we remove the irrelevant terms from the set of expansion terms manu-ally, and add the same number of terms to Topic 159. After this processing, the MAP is increased to 0.4101. We can see that the retrieval performance can be much improved if fewer irrelevant terms are added. Our problem now is to develop an effective method to extract good terms from the social annotation collection.
In Section 4.1, we take into account the dependencies be-tween query terms, and propose a term-dependency method ( SA+TD ) to select more relevant terms for original queries. The parameter  X  in Eq. (6) is used to measure the impact of term-dependency in the term selection phase. When  X  = 0 , the term selecting method becomes the SA+FI method. Figure 1 shows the results of assigning different weight on three TREC collections.

As can be seen in Figure 1, the performance is improved when weight  X  is at the higher level (  X   X  0 . 6). On the other hand, when  X  &gt; 0 . 5, the MAP of the SA+TD method is lower than the SA+FI method X  X . It means the dependen-cies between query terms can be useful in the term selection process, but it will hurt the performance if much query term dependency information is used. Finally, we set the param-eter  X  = 0 . 2.

In the SA+FI and SA+TD methods, the expansion terms are added into the original query with the same weight. It means all the expansion terms have the same importance with each other. In fact, the co-occurrence of expansion term calculated by Eq. (6) may reflect the importance to the query Q . In the SA+TDW method, we will obtain the top k terms for query expansion, whose weights are normal-ized before being added into original queries. In addition, similar to the RM+Oracle method, we select top k terms with high chg ( e ) extracted from the social annotation col-lection to add into the original query, and the results of this method ( SA+Oracle ) represent the upper bound retrieval effectiveness we can expect to obtain using expansion terms extracted from the social annotation collection.
As we can see from Table 7, both the SA+TD and SA+TDW methods achieve better performance than the SA method in the expansion term selection. Term-dependency meth-ods can effectively select relevance terms for the original query. Compared with the two term-dependency methods, the SA+TDW method performs much better. For this method, top k terms with high co-occurrence score are selected to expand the query. This shows that the co-occurrence score could be used to reflect the importance of the expansion term. Using the oracle expansion terms extracted from so-cial annotation collection, the performance can also be much improved than other methods.
 Comparing Table 6 and 7, we can see the SA+FI and SA+TD methods do not work as well as the RM model does. On AP and WSJ, the SA+TDW method gets lower performance than the RM model does, but on Robust2004, the performance is improved over the RM model. In a well-tuned relevance model (RM), the expansion terms are weighted according to the distribution they are sampled from collection. That is one of the reasons the SA+TD method gets weak performance. Compared with the results of the SA+Oracle and RM+Oracle methods, the SA+Oracle method achieve better performance. It indicates that the social annotation collection is a good resource for query ex-pansion. Based on these results, the refinement of the ex-pansion term selection process for queries could be expected to further improve retrieval performance. Our goal now is to develop an effective method to select more relevance ex-pansion terms. Let us now examine the quality of our term ranking model. In ranking performance evaluation, there are many IR evalu-ation measures: such as Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP). In our experiments, we take more attention to the relevance of the top k terms. Therefore, we adopt P @ n as the main mea-sures for the performance evaluation of ranking model. The formula of P @ n is defined as: i.e. , the portion of relevant terms in the top-n result list. P @ n is sensitive to the amount of relevance terms returned, and thus is helpful to measure the performance gain on pre-cision of the ranking model.

In our experiment, we use three TREC collections. For each collection, we use 50 queries to training the ranking learner. In training set, each query has a term list which ranks 150 expansion terms according to chg ( e ) which reflects the term X  X  impact on retrieval effectiveness. To evaluate the performance of ranking learner, according to the methods described in Section 4.2, we label the expansion term e in the development dataset with two levels of relevance judg-ments: relevance and irrelevance. In test dataset, we rank Table 8: Term ranking accuracies in terms of P @ n on AP, WSJ and Robust2004 collections the expa nsion terms order by the co-occurrence score calcu-lated by Eq. (6). This term list is named TermList-TD . The TermList-TD list could be re-ranked to form a new term list ( TermList-Learning ) by ranking learner. Finally, we rep-resent each expansion term with the features described in section 4.3.

Table 8 shows the performance of our ranking learner on different collections. As mentioned before, an expansion term list ( TermList-TD ) could be obtained according to the co-occurrence score using term-dependency method. Actu-ally, the SA+TD method selected the top k terms from this term list to expand the original query. Our ranking model learns to re-rank the term list produced by term-dependency method to form a new term list ( TermList-Learning ).
From Table 8, for the effective expansion term selection, the ranking learner outperforms the term-dependency method. On AP and Robust2004 datasets, the ranking learner works much better than it works on WSJ dataset. Table 3 shows that the proportion of good terms on WSJ is lower than that on the other two datasets. It may cause that the total of relevance terms in the term list produced by the term-dependency method is lower. This is one of the reasons that the performance on WSJ is not much improved like the other two datasets. In the next section, we will investigate the usefulness of the selected expansion terms from the new re-ranked lists ( TermList-Learning ) for query expansion.
As described in Section 4.5, given a test query, a possi-ble expansion term list is obtained using term-dependency method, and then the ranking learner will re-rank the term lists according to their impact on retrieval effectiveness. The term ranking process performs a further selection of expan-sion terms from those proposed by term-dependency method. After re-ranking, we use the top ranked terms in the new re-ranked list as the final expansion terms.

For the term re-ranked lists, we mentioned two possible ways to use the term ranking results. The first method (SA+Learning) is to add the top ranked 50 terms with the same weight to original queries. In our term ranking ex-periments, we are interested to know not only is a term is good, but also the extent to which it is good. The rank-ing learner will assign a score to all the expansion terms. This score is useful to enhance the weight of an expansion term in the final query model. Therefore, the second method (SA+LearningW) is to add the top 50 terms with different weights, note that their weights are normalized before being interpolated with the original query model.
 Table 9 shows the results obtained with both methods. From this table, we can see that both of the two methods Table 9: Performance comparisons of SAR and SARW for all the test topics on AP, WSJ, Roust2004 collections.
 Figure 2: Performance on different weights for SA+TD on AP, WSJ and Robust2004 collections, k = 50 . improve the effectiveness. In comparison, although on WSJ and Robust2004 the improvements with the SA+LearningW method are smaller, they are steady on all the three collec-tions.
 Comparing Table 7 and 9, the improvements with the SA+Learning and SA+LearningW methods are statistically significant. Our explanation is that, since the term rank-ing learner performs better (shown in Table 8), some top ranked expansion terms could improve the performance sig-nificantly. On the other hand, it means that the relevant expansion terms may be re-ranked in the higher position of the list and the learners have the ability of recommending the most likely terms to the original queries.

Comparing Table 6 and 9, the SA+Learning and SA+LearningW methods outperform the RM model, it shows that the ex-pansion terms selected from re-ranked term lists is more relevance than them extracted from pseudo-relevance doc-uments. Although the the SA+LearningW method does not work well as the RM+Oracle method on AP and WSJ datasets, there is a smaller gap between them. However, on Robust2004 dataset, the SA+LearningW method performs better than the RM+Oracle method. Our explanation is that, for the RM and RM+Oracle methods, the improve-ment over the QL model on the Robust2004 dataset is not as significant as them on AP and WSJ datasets. It means the pseudo-relevance documents produced more noisy expansion terms for the RM model. It could reduce this influence by using social annotation collection as the resource of expan-sion terms. However, it is also a difficult task to explore an effective method to mining relevance terms from this new re-source. This experiments show that the term ranking model has the ability of selecting more relevant terms for the orig-inal queries. Figure 3: Sensitivity of SA+TD to parameter ( k ) on AP, WSJ and Robust2004 collections,  X  fb = 0 . 5 .
Both the RM and the SA methods have parameters k and  X  fb . We tested these methods with 10 different values of k : the number of expansion terms: 10, 20,..., 100. For  X  fb (weight for original query), we tested with 9 different values: 0.1, 0.2,..., 0.9. Figure 2 shows the performance on different weights for original query on AP, WSJ and Ro-bust2004 collections. When  X  fb is smaller than 0.6, the performance remains steady. However, when  X  fb continues to increase, the performance drops sharply. We conjecture that the main reason is the impact on retrieval effectiveness of expansion terms reduces with  X  fb increasing. From an-other perspective, it also proves the usefulness of expansion terms. Figure 3 shows the sensitivity of SA+TD to k . For the parameter k (the number of expansion terms), when k is around 50, we can achieve the best performance on all collec-tions. When we use more expansion terms for PRF ( k &gt; 50), the performance changed is not marked. For efficiency, we suggest to choose top 50 terms for query expansion in our proposed method. The results for other methods are simi-lar, and it shows that setting k = 50 and  X  fb = 0 . 5 works best among all the values tested.
In this paper, we have explored the potential of social an-notation as a new resource for query expansion. For TREC topics, we measure the importance of expansion terms ac-cording to their impact on the retrieval performance, which selected from social annotation sample. Our experiments show that the expansion terms extracted from social anno-tation are significantly better than those from the feedback documents, using a simple statistical method. In the expan-sion term selection phase, in addition to the full independent assumption, the sequential dependence of query terms has been taken into account, which captures more relevant terms that are beneficial to retrieval performance. Another contri-bution of this paper is that we develop a machine learning approach for query expansion, which learns to rank a set of expansion terms according to their impact on the retrieval performance. We also show that our ranking approach works satisfactorily on the different TREC collections.
This study suggests several interesting research avenues for our future investigations. For the term selection method, because of its weak performance in our experiment the full dependence variant of query terms is not used. In fact, the query context factors could bring significant improvements in retrieval effectiveness. This means that there is still much room to improve the retrieval performance. For the term ranking model, we plan to explore more distinctive features in social annotation sample for the learning process. We leave these limitations as our future work.
This work is supported by grant from the Natural Sci-ence Foundation of China (No.60673039 and 60973068) , the National High Tech Research and Development Plan of China (No.2006AA01Z151), National Social Science Foun-dation of China (No.08BTQ025), the Project Sponsored by the Scientific Research Foundation for the Returned Over-seas Chinese Scholars, State Education Ministry and The Research Fund for the Doctoral Program of Higher Educa-tion (No.20090041110002). [1] J. Bai, J. Y. Nie, H. Bouchard, and G. Cao. Using [2] S. Bao, G. Xue, X. Wu, Y. Yu, B. Fei, and Z. Su. [3] C. Buckley, G. Salton, J. Allen, and A. Singhal. [4] G. Cao, J. Y. Nie, and S. Robertson. Selecting good [5] Z. Cao, T. Qin, T. Y. Liu, M. F. Tsai, and H. Li. [6] M. Carman, M. Baillie, R. Gwadera, and F. Crestani. [7] K. Collins-Thompson and J. Callan. Query expansion [8] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A [9] H. Cui, J. R. Wen, J. Y. Nie, and W. Y. Ma. Query [10] P. Heymann, G. Koutrika, and H. Garcia-Molina. Can [11] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme. [12] V. Lavrenko and W. B. Croft. Relevance based [13] Y. LeCun, L. Bottou, G. B. Orr, and K. R. M  X  uller. [14] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based [15] L. Mason, J. Baxter, P. Bartlett, and M. Frean. [16] A. Mathes. Folksonomies -cooperative classification [17] D. Metzler and W. Croft. A markov random field [18] D. Metzler and W. B. Croft. Latent concept expansion [19] P. Mika. Ontologies are us: A unified model of social [20] S. E. Robertson, S. Walker, M. Beaulieu, M. Gatford, [21] J. Rocchio. Relevance feedback in information [22] Y. Song, Z. Zhuang, H. Li, Q. Zhao, J. Li, W. C. Lee, [23] M. Srikanth and R. Srihari. Biterm language models [24] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. [25] T. Tao and C. Zhai. Regularized estimation of mixture [26] X. Wu, L. Zhang, and Y. Yu. Relevance based [27] J. Xu and W. Croft. Query expansion using local and [28] S. Xu, S. Bao, B. Fei, Z. Su, and Y. Yu. Exploring [29] Y. Xu, J. F. Jones, and B. Wang. Query dependent [30] Z. Ye, J. Huang, S. Jin, and H. F. Lin. Exploring [31] D. Yeung, C. Clarke, G. Cormack, T. Lynam, and [32] C. Zhai and J. Lafferty. Model-based feedback in the
