 The goal of focused crawling is to use limited resources to effectively discover web pages related to a specific topic rather than downloading all accessibl e web documents. The major challenge in focused crawling is how to effectively determine each hyperlink X  X  capability of leading to target pages. To compute this capability, we 1 present a novel approach, called Navigational Rank (NR) . In general, NR is a kind of two-step and two-direction credit propagation approach. Compared to existing methods, NR mainly has three advantages. First, NR is dynamically updated during the crawling progress, which can adapt to different website structures very well. Second, when the crawling seed is far away from the target pages, and the target pages only constitute a small portion of the whole website, NR shows a significant performance advantage. Third, NR computes each link X  X  capability of leading to target pages by considering both the target and non-target pages it leads to. This global knowledge causes a better performance than only using target pages. We have performed extensive experiments for performance evaluation of the proposed approach using two groups of large-scale, real-world datasets from two different domains. The experime ntal results show that our approach is domain-independent a nd significantly outperforms the state-of-arts. H.3.3 [ Information Systems ]: Information Storage and Retrieval  X  Search Process, Information Filtering, Retrieval Models Algorithms, Experiment ation, Performance Focused Crawling, Navigational Rank, Personalized PageRank Although there is a lot of information on the web, people are often only interested in finding a few pages in a certain domain. For example, a student may be only want to collect all the pages of online courses related to her/hi s research area, and a marketing professional in an enterprise may be only interested in finding all the product pages from the websites of competitors. Focused crawling [1] has been proposed as a solution for domain-specific web resource discove ry, and its main goal is to use limited resources to efficiently and effectively get web pages related to a specific topic, without downloading the entire websites. A focused crawler usually maintains a priority queue of all pages whose URLs are discove red but their contents are not yet downloaded by their capability that leads to the target pages relevant to a special topic that a focused crawler attempts to find.). Then, the crawler always downloads the top ranked in the queue. Consequently, how to determine each hyperlink X  X  capability of leading to target pages is the key issue in focused crawling research. Many approaches [3, 5] have been proposed for focused crawling. They mainly have three disadvantages: supervised learning based approaches attempt to learn the capability of leading to the target pages for each web page in the training websites. However, the structures of different websites are very different on the web. Consequently, these supervised learning based methods cannot adap t to the different varieties of the web well. 2. They cannot work well when the crawling seeds (e.g., the home page of a website) are far away from the target pages and the target pages constitute a small portion (e.g., less than 3%) of the whole website. Under this situ ation, only a crawler follows the correct crawling path, it can find the target pages most quickly and minimizes the cost of downl oading the non-target pages. 3. They usually ignore the non-target pages. These existing approaches focus on the target pages but the non-target pages are usually ignored. However, such non-target pages are important in the propagation of the capability that leads to the target pages, and should have been utilized. To overcome the drawbacks of the existing approaches, we propose a novel approach, called Navigational Rank (NR), to estimate the capability of each hyperlink in leading to target pages. In general, NR is a kind of link-based, two-step and two-directory credit propagation approach (see Section 2). Compared to existing works [3, 5] , our approach mainly has three advantages: 1. We dynamically recalculate the NR value (see section 2) for each page in the sub-graph which is formed by the downloaded pages. The NR value becomes more and more accurate while the crawling is going on. Consequently , it can adapt to the structure difference among websites. 2. NR emphasizes the global connectivity of the web graph. Unlike some previous work [3, 5] in which the credits are assigned to the pages on the crawling paths, NR globally distributes the credits to identify the useful intermediates more accurately. As a result, even if the target pages are far away from the seeds and only constitute a small portion of the whole website, our approach can find them efficiently and effectively. 3. NR computes each link X  X  capability of leading to target pages by considering both the target and non-target pages it leads to. For example, imagine a page that lists all the pages on the web, it is not useful for the purpose of focused crawling. Although this page can immediately lead to every target page, this page also leads to all non-target pages. To solve this problem, NR uses average, instead of summation in the existing methods [2, 4], to aggregate the ranks. We evaluate the performance of our method using two groups of large-scale, real-world datasets from two differe nt domains (see Section 3). These tasks pose some particular challenges. (1) The target pages are sparse  X  by our experimental estimate, only 3% web pages are targets. (2) The target pages may be far away from the seed page (e.g., the home page of a website). The experiment results show that the focused crawling based on NR is independent of special domain knowledge and outperforms the state-of-art approaches. More over, our experiments have been conducted on large-scale web page s of several entire websites, with scale much larger than most previous studies. Consequently, the scalability of our approach is much better than most of previous studies. In general, NR is a kind of link-based, two-step and two-direction credit propagation algorithm. It works in this way: 
First, we utilize a crawler to download a small set of pages using the standard Breadth-First Strategy ( BFS ) [3] and create an initial graph by these downloade d pages. From these downloaded pages, the URL of some un-downloaded pages is parsed and inserted into the crawling queue. 
Second, we compute the NR scores of all downloaded pages based on the created graph using the first-step computation of NR (see Section 2.1). The credit propagation direction is from offspring to their ancestors. 
Third, the NR scores of all downloaded pages are propagated to the un-downloaded target pages using the second-step computation of NR (see Section 2.2). The credit propagation direction is from ancestors to their offspring. Finally, all un-downloade d pages in the crawling queue are ranked by their NR scores. Then, a new round crawling is started and the top ranked are downloaded first. Then, the above process is repeated to recalculate each page X  X  NR on the expanded graph until some termination criteria, such as the number of pages which has reached a certain predefined threshold, is matched. In this paper, we model the pages of a website by a directed graph G = (V, E). In G, a vertex v represents a web page and an edge e represents a hyperlink between two pages. V and E are the number of links that point to and out of a node v . We assume that each node v is assigned a weight p(v) by a classifier, to indicate the relevance of the page. The weight can be a binary or real number according to the classifier. In this paper, we train a Multinomial Naive Bayes classifier using Weka (http://www.cs.waikato.a c.nz/ml/weka/). As a matter of fact, this content-based classifier can be trained by any classification algorithm. 
Given such a graph with vertex weights, for each v in V, the first-step computation of Navigational Rank NR(v) is calculated by the following formula (1): number of hyperlinks that point to u . d reflects how we evaluate the initial weight. It is usually a small but not negligible constant. In our work, we chose d=0.2 acco rding to the experiments on two groups of real-world large-scale datasets. p(v) is determined by a Multinomial Naive Bayes classifier. Its value is 1 if it is a target page. Otherwise, its value is 0. Similar to the definition of Personalized PageRank ( PPR ) [2] or Topical-sensitive PageRank ( TPR ) [4], the first-step computation of NR is also a typical iterative process. The convergence of Algorithm 1 can be proved in a similar way as the proof of [4]. This is also verified in our experiments on two groups of real-world datasets. 
Although the definition of NR is similar to PPR or TPR , there are two significant differences between them. 
First, the credit propagation direction is different in the direction is from the offspring to the ancestor. But in PPR and TPR , the order is reversed. This is because their purposes are different: PPR attempts to identify the authority of a page, but NR tries to measure the capability of leading to target pages, which is used as the crawling priority of the hyperlinks. The second important difference is that NR ( v ) is computed by PPR or TPR . The intuition of this choice is that a node is not only rewarded by pointing to relevant nodes (with high scores) but also penalized by pointing to irreleva nt nodes (with low scores), with the scores recursively defined. One rationale is that if a node points to all pages, its score shou ld not be very high as it does not help us distinguish between ta rget and non-target pages. In addition, NR ( v ) is normalized by being divided by di ( u ). Therefore, if a node has several ancestors, it propagates the same score to each of its ancestors with the summation to NR(v). In this way, we avoid over-emphasizing the influence of a node when it has a large number of incoming links. Our experimental results show that this intuition works well in practice. According to the definition of NR , the direction of credit propagation is from offspring to th eir ancestors. If a node points to target pages directly or in few hops, it will be assigned a relatively high NR score. Hence, the hub pages or the intermediate pages which point to target pages may be identified with high NR scores. these hub pages and the intermediate pages, it may quickly reach the target pages. This intuition is verified by our experiments (see Section 3). If each page X  X  NR value is known, we may download a website based on each page X  X  NR score. That is, we keep a priority queue of all the discovered but not yet downloaded pages by their NR scores. The crawler always downloads the top ranked from the queue. However, we ca nnot directly apply NR in focused crawling due to two restrictions: 1. The computation of NR scores relies on the knowledge of the web graph, which is unavailable before the crawling ends. Obviously, even the relevance score p(v) of a page is unknown before it is downloaded. 2. For every un-downloaded page, it has no out-links since the corresponding pages have not been downloaded and parsed. The value of NR of those pages will always be 0 according to the first-step definition of NR . It is therefore not practical for determining which un-downloaded page is more li kely to lead to target pages. To solve the above problem, we apply NR in the following way: First, we utilize a crawler to download a small set of pages (e.g., 2000) using the standard Breadth-First Strategy and create an initial sub-graph by these downl oaded pages. C onsequently, the required graph knowledge is avai lable although it is a small partition the whole graph. Second, we compute the NR scores of all downloaded pages using the formula (1) based on th e created sub-graph (see above Section). Finally, the NR scores of all downloaded pages are propagated to their neighbors by the second-step computation of NR . The second-step computation of NR is defined as the following formula (2): number of links that points out u . As formula (1), we chose d=0.2 according to the experiments on two groups of real-world large-scale datasets. NR 1 (v) is the NR value of vertex v that is got in the after the second-step computation of NR . Similar to the first-step computation of NR , the second-step computation of NR is also a typical iterative process. Similar to the definition of PPR , the credit propagation direction is also from the ancestor to offspring. Altho ugh based on the same intuition, the main difference is that NR 2 ( u ) is computed by taking the average of NR 2 ( u )/ do ( u ), but not the summation used in PPR . 
After the second step of NR credit propagation, the newly parsed but not yet downloaded pages in the neighborhood can be assigned higher NR scores, and will be first crawled in the next crawling cycle. The above processes (the first-step computing  X  the second-step computing  X  downloading top ranked un-downloaded page in the crawling queue) are repeated to recalculate each page X  X  NR on the expanded graph until some te rmination criteria happens, for example, the number of pages has reached a certain predefined threshold (We choose this number as 2000 in our experiments.). or the important immediate pages on the correct crawling path are identified. After the second-step computation, the most possible target pages in the un-downloade d pages are identified. Hence, after this kind of two-step and two direction credit propagation, NR can provide an effective focused crawling approach. 
We verify our NR approach in two different domains: we apply it to discover online course materials from five university websites and the hardware configur ation pages of server products from three computer company websites. We use the open source search engine  X  Nutch (http://nutch.apache.org/) in our experiments. We first conduct a fa irly complete crawling of these websites (the crawling depth = 15). The downloaded pages are analyzed by a Multinomial Naive Bayes classifier to identify if they are course-related. Next we use human assistance to further purify the results produced by the classifier. 
Table 1 and Table 2 summarize the total page number and total target page number of each website after manual labeling (we only obtained a small fraction of IBM website because the crawler we use does not download dynamic pages.). From Table 1 and Table 2, we can see that the target pages are sparse: there are a large amount of pages in each website, but only a small portion (less than 3%) of them are target pages. efficiency of different approaches on different data sets separately. Next, we use their average value to compare the efficiency of different approaches. We compare NR with Reinforcement Learning ( RL ) [5] and Context-Graph ( CG ) [3] methods, which are two well-known focused crawling methods and are reported with good performance. In addition, we also compare our method with the most relevant method, PPR . Similar to the definition of NR , PPR cannot be directly applied in fo cused crawling. Consequently, we apply its variation, Dynamic Personalized PageRank ( DPPR ) [2] for focused crawling task. Similar to NR , DPPR value of each page is also periodically recalculated until some termination criteria happens. The computation of Q value in [5] requires an exponential time. We use the following approxima tion based on our understanding of the description in [5]: from each target page, we assign credit to the pages on the shortest path from the target page to the root page (i.e. the seed), and the credit decreases exponentially away from the target page. Each node X  X  Q value is the summation of the credits it receives. To emulate the context-graph approach, we use the shortest distance from each page to any target page as the number in our experiment is 5, ex cluding the root seed layer. In addition, we implement DPPR method and take Breadth-First Search ( BFS ) as the baseline benchmark. The experimental results are shown in Figure 1 and Figure 2. In Figure 1 and Figure 2, we can observe that NR significantly outperforms DPPR , RL , CG , and BFS . 
Figure 1. The comparison of crawling efficiency of different 
Figure 2. The comparison of crawlin g efficiency of different The explanation of the poor performance of RL and CG in our experiments is that: (1) In our application, the seed page is far away from the target pages. There are more than 6 hops on average from the root to the target pages in our experimental datasets. And the target pages only constitute a small portion (less than 3%) of the whole website. Under this situation, the crawler should follow the correct crawling path to discover the target pages as soon as possible. NR attempts to identify the important hub pages on the correct crawling path by the first-step computation of NR and the most possible target pages in the un-downloaded pages by the second-step computation of NR . Hence, then the target pages are far away from the root, NR and find them promptly. (2) RL and CG are supervised-based methods, which are heavily dependent of the similarity betw een the training websites and test websites. However, the differen ce among different websites is huge. Hence, RL and CG cannot work well in this case. NR is periodically updated to adapt the difference among different websites. discover the targets, which tend to perform not well in this case. But NR leverage the global knowledge of the whole web sites, both the target and the non-target pages are taken into account. By examining these commercial websites more carefully, we find that their structures are flatter than the structures of university websites, and the target pages are located at shallow levels. This is in contrast to the characteristics of the course pages in university websites. Consequently, BFS based methods should work well for such crawling tasks. But still, we observe that NR performs better than the other methods. From this set of experiments, we observe that the efficiency of focused crawling depends on the structure of the websites. Focused crawling techniques devel oped in this paper perform the best by comparison when the target pages are more cluttered and buried deep in the websites hierarchy. Focused crawling is a very im portant web resource discovery technology because the challenge of web scalability still exists even in the era of cloud computing. In this paper, we propose a novel focused crawling approach  X  NR. In general, it is a kind of link-based, two-step and two-direction credit propagation approach. Our experiments show that the proposed approach is domain independent and outperform s the state-of-art methods especially when the crawling seed is far away from the target pages, and the target pages only constitute a small portion of the website. Moreover, our experime nts have been conducted on large-scale web pages of several entire websites. The scalability of our approach is much better than most of previous studies. . [1] S. Chakrabarti, M. van den Be rg, B. Dom. Focused crawling: [2] S. Chakrabarti. Dynamic personalized PageRank in entity-[3] M. Diligenti, F. Coetzee, S. Lawrence, C. L. Giles, and M. [4] T. H. Haveliwala. Topic-sensitive PageRank. In Proceedings [5] J. Rennie and A. K. McCallum. Using reinforcement learning 
