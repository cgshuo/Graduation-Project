 Many dynamic optimization problems can be cast as Markov decision problems (MDPs) and solved, in principle, via dynamic programming. Unfortunately, this approach is frequently untenable due to the  X  X urse of dimensionality X . Approximate dynamic programming (ADP) is an approach which attempts to address this difficulty. ADP algorithms seek to compute good approximations to the dynamic programing optimal cost-to-go function within the span of some pre-specified set of basis functions. The approximate linear programming (ALP) approach to ADP [1, 2] is one such well-recognized approach.
 The program employed in the ALP approach is identical to the LP used for exact com-the low-dimensional subspace spanned by the basis functions used. The resulting low di-mensional LP implicitly restricts attention to approximations that are lower bounds on the lishing approximation guarantees for the approach, the restriction to lower bounds leads while remaining computationally tractable? Motivated by this question, the present paper presents a new linear program for ADP we call the  X  X moothed X  ALP (or SALP). The SALP may be viewed as a relaxation of the ALP wherein one is allowed to violate the ALP constraints for any given state. A user defined  X  X iolation budget X  parameter controls particular, we are able to establish strong approximation guarantees for the SALP; these guarantees are substantially stronger than the corresponding guarantees for the ALP. The number of constraints and variables in the SALP scale with the size of the MDP state space. We nonetheless establish sample complexity bounds that demonstrate that an appropriate  X  X ampled X  SALP provides a good approximation to the SALP solution with a tractable number of sampled MDP states. This sampled program is no more complex than the  X  X ampled X  ALP and, as such, we demonstrate that the SALP is essentially no harder to solve than the ALP.
 We present a computational study demonstrating the efficacy of our approach on the game of Tetris. The ALP has been demonstrated to be competitive with several ADP approaches for Tetris (see [3]). In detailed comparisons with the ALP, we estimate that the SALP provides an order of magnitude improvement over controllers designed via that approach for the game of Tetris. Our setting is that of a discrete-time, discounted infinite-horizon, cost-minimizing MDP t , according to the transition probability kernel P a mapping that determines the action at each time as a function of the state. Given each given by where,  X   X  (0 , 1) is the discount factor. Denote by P matrix for the policy  X  , whose ( x,x 0 ) th entry is P whose x th entry is g ( x, X  ( x )) . Then, the cost-to-go function J equation T The Bellman operator T can be defined according to TJ = min function J  X  is the unique solution to Bellman X  X  equation and that a corresponding optimal policy  X   X  is greedy with respect to J  X  ; i.e.,  X   X  satisfies TJ  X  = T Bellman X  X  equation may be solved exactly via the following linear program: (1) Here,  X   X  R X is a vector with positive components that are known as the state-relevance lent to the set of linear constraints J ( x )  X  g ( x,a ) +  X  P refer to (1) as the exact LP .
 Note that if a vector J satisfies J  X  TJ , then J  X  T k J (by monotonicity of the Bellman operator), and thus J  X  J  X  (since the Bellman operator is a contraction with unique fixed J  X  is the unique optimal solution to the exact LP (1).
 For problems where X is prohibitively large, an ADP algorithm seeks to find a good ap-where each  X  of basis functions, one seeks an approximation of the form J J (2) The geometric intuition behind the ALP is illustrated in Figure 1(a). Supposed that r ALP is a vector that is optimal for the ALP. Then the approximate value function  X  r ALP will a point-wise lower bound to the true cost-to-go function in the span of  X  . Figure 1: A cartoon illustrating the feasible set and optimal solution for the ALP and SALP, in the case of a two-state MDP. The axes correspond to the components of the value function. A careful relaxation from the feasible set of the ALP to that of the SALP can yield an improved approximation. The J  X  TJ constraints in the exact LP, which carry over to the ALP, impose a strong proximations that are lower bounds to J  X  at every point in the state space . In the case it may be the case that constraints arising from rarely visited or pathological states are a lower bound on the optimal cost-to-go function, but rather a good approximation to J  X  . require a uniform lower bound may allow for better overall approximations to the optimal of this sort in general? The smoothed approximate linear program (SALP) is given by: (3) s ( ALP constraint. The parameter  X   X  0 is a non-negative scalar. The parameter  X   X  R X is that the SALP forms the basis of a useful ADP algorithm in large scale problems: This section is dedicated to a theoretical analysis of the SALP. The overarching objective Our analysis will present two types of results: First, we prove approximation guarantees (Sections 4.1 and 4.2) that will indicate that the SALP computes approximations that are of comparable quality to the projection of J  X  on the linear span of  X  . Second, we show (Section 4.3) that an implementable  X  X ampled X  version of the SALP may be used to approximate the SALP with a tractable number of samples. All proofs can be found in the technical appendix.
 Idealized Assumptions: Given the broad scope of problems addressed by ADP algo-constraints as there are states, or absent that, knowledge of a certain idealized sampling distribution, so that one can then proceed with solving a  X  X ampled X  version of the ALP. Our analysis of the SALP in this section is predicated on the knowledge of an idealized ticular, we will require access to samples drawn according to the distribution  X  The distribution  X  runs under the optimal policy  X   X  . We note that the  X  X ampled X  ALP introduced by de Farias 4.1 A Simple Approximation Guarantee We present a first, simple approximation guarantee for the following specialization of the SALP in (3): (4) Before we proceed to state our result, we define a useful function: (5) ` ( of the solution to (5). The following Lemma characterizes l ( r, X  ) : Lemma 1. For any r  X  R K and  X   X  0 : (i) ` ( r, X  ) is a bounded, decreasing, piecewise linear, convex function of  X  . (ii) ` ( r, X  )  X  (1 +  X  ) k J  X   X   X  r k (iii)  X  guarantee: Theorem 1. Let 1 be in the span of  X  and  X  be a probability distribution. Let  X  r be an optimal solution to the SALP (4) . Moreover, let r  X  satisfy r  X   X  argmin Then, In this case, we have from Lemma 1 that ` ( r  X  , 0)  X  (1 +  X  ) k J  X   X   X  r  X  k and Van Roy [1]; we recover their approximation guarantee for the ALP. Next observe that, from (iii), if the set  X ( r  X  ) is of small probability according to the distribution  X   X  r of the approximation produced by r  X  is large for only a small number of states), we thus the intuition (shown via Figure 1) that the SALP will permit closer approximations to J  X  than the ALP.
 The bound in Theorem 1 leaves room for improvement: The next section will present a substantially refined approximation bound. 4.2 A Better Approximation Guarantee With the intent of deriving stronger approximation guarantees, we begin this section by use a weighted max norm defined according to: k J k  X  approximation error in a non-uniform fashion across the state space and in this manner we will consider solving the following SALP: (6) maximize It is clear that (6) is equivalent to (4) for a specific choice of  X  . We then have: Theorem 2. Let  X  , { y  X  R |X| : y  X  1 } . For every  X   X   X  , let  X  (  X  ) = max Then, for an optimal solution ( r SALP ,  X  s ) to (6) , we have: a closely related result shown by de Farias and Van Roy [1] for the ALP. In particular, de Farias and Van Roy [1] showed that given an appropriate weighting (or in their context,  X  X yapunov X ) function  X  , one may solve an ALP, with  X  in the span of the basis functions  X  ; the solution to such an ALP then satisfies: provided  X  (  X  )  X  1 / X  . Selecting an appropriate  X  in their context is viewed to be an allows us to view the SALP as automating the critical procedure of identifying a good Lyapunov function for a given problem. 4.3 Sample Complexity Our analysis thus far has assumed we have the ability to solve the SALP, a program with the SALP is well approximated by the solution to a certain  X  X ampled X  program which we now describe: Let  X  X = { x 1 ,x 2 ,...,x from X according to the distribution  X  which we call the sampled SALP: (7) Here N  X  R m is a parameter set chosen to contain the optimal solution to the SALP (6), SALP satisfies, with high probability, the approximation guarantee presented for the SALP solution in Theorem 2.
 Let us define the constant B , sup to the diameter of the region N . We then have: Theorem 3. Under the conditions of Theorem 2, let r SALP be an optimal solution to the SALP (6) , and let  X  r SALP be an optimal solution to the sampled SALP (7) . Assume that states S satisfies Then, with probability at least 1  X   X   X  2  X  383  X  128 , Theorem 3 establishes that the sampled SALP provides a close approximation to the solution of the SALP, in the sense that the approximation guarantees we established for the SALP are approximately valid for the solution to the sampled version with high probability. The This number depends linearly on the number of basis functions and the diameter of the program. Exploiting the fact that the ALP has a small number of variables, de Farias and Van Roy [2] establish a sample complexity bound for a sampled version of the ALP analogous (7). The number of samples required for this sampled ALP to produce a good approximation to the ALP can be shown to depend on the same problem parameters we that case is identical to the sample complexity bound established here up to constants and an additional multiplicative factor of B/ (for the sampled SALP). Thus, the two sample complexity bounds are within polynomial terms of each other and we have established that the SALP is essentially no harder to solve than the ALP.
 This section places the SALP on solid theoretical ground by establishing strong approxima-tion guarantees for the SALP that represent a substantial improvement over those available for the ALP and sample complexity results that indicated that the SALP was implementable via sampling. We next present a computational study that tests the SALP relative to other ADP methods (including the ALP) on a hard problem (the game of Tetris). Theoretical results suggest that design of an optimal Tetris player is a difficult problem [4 X 6]. Tetris represents precisely the kind of large and unstructured MDP for which it is particularly relevant. Moreover, Tetris has been employed by a number of researchers as a testbed problem [3, 7 X 9]. We follow the formulation of Tetris as a MDP presented by Farias and Van Roy [3]. The SALP methodology was applied as follows: Basis functions. We employed the 22 basis functions originally introduced in [7]. State sampling. Given a sample size S , a collection  X  X  X  X  of S states was sampled. These (8) was solved. This program is a version of the original SALP (3), but with sampled empirical barrier methods, even for large values of S .
 Evaluation. Given a vector of weights obtained by solving (8), the performance of the corresponding policy is evaluated via Monte Carlo simulation over 3 , 000 games of Tetris. Performance is measured in terms of the average number of lines cleared in a single game. sampled states) is shown in Figure 2. It provides experimental evidence for the intuition expressed in Section 3 and the analytic result of Theorem 1: Relaxing the constraints of the ALP by allowing for a violation budget allows for better policy performance. As the the performance peaks, and we get policies that is an order of magnitude better than ALP, and beyond that the performance deteriorates. Figure 2: Average performance of SALP for different values of the number of sampled states S and the violation budget  X  .
 Table 1 summarizes the performance of best policies obtained by various ADP algorithms. Note that all of these algorithms employ the same basis function architecture. The ALP and Table 1: Comparison of the performance of the best policy found with various ADP methods. any of the ADP algorithms in Table 1 discover. Using a heuristic optimization method, Their method is computationally intensive, however, requiring one month of CPU time. In the method is unable to find a policy for Tetris that scores above a few hundred points. bounds derived in Sections 4.1 and 4.2 are approximation guarantees, which provide bounds on the approximation error given by the SALP approach versus the best approximation pos-performance guarantees . These provide bounds on the performance of the resulting SALP of the SALP are possible. Rather than solving a large linear program, such an algorithm in a manner reminiscent of stochastic approximation algorithms like TD-learning. However, a sample path SALP variation would inherit all of the theoretical bounds developed here. The design and analysis of such an algorithm is an exciting future direction. [1] D. P. de Farias and B. Van Roy. The linear programming approach to approximate [2] D. P. de Farias and B. Van Roy. On constraint sampling in the linear programming [3] V. F. Farias and B. Van Roy. Tetris: A study of randomized constraint sampling. In [5] H. Burgiel. How to lose at Tetris. Mathematical Gazette , page 194, 1997. [6] E. D. Demaine, S. Hohenberger, and D. Liben-Nowell. Tetris is hard, even to approx-[8] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming . Athena Scientific, [9] S. Kakade. A natural policy gradient. In Advances in Neural Information Processing [11] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and
