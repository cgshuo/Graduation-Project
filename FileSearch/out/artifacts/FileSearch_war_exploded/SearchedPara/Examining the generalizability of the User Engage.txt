 1. Introduction
Assessing search systems usually takes a systems-based approach that uses standard information retrieval measures such as precision, recall, and cumulative gain ( J X rvelin, 2011 ) to evaluate the output from the system; in interactive information retrieval, evaluation is extended to encompass aspects of user interactivity, where elements of the interaction are measured, including number of queries used and the mean size of a query; performance measures such as number of relevant docu-ments retrieved; and usability measures such as ease of use, effort and preference, among a host of metrics ( Kelly, 2009 ).
But how is the user experience assessed? In studies of interactive information retrieval, user experience is rarely addressed except in post task questionnaires that inquire about user satisfaction, which is generally evaluated using a single Likert-scaled question:  X  X  X ow satisfied were you ... ? X  X  (see reports from the Interactive Track at TREC and INEX as examples). This one-dimensional approach is a limited and imprecise assessment of the rich, multi-dimensional experience that is so typical of user interactivity with any digital product including search engines and exploratory search systems.  X  Increasingly, researchers are looking to experience-based frameworks as a means of understanding Human Information-
Interaction (HII) ( O X  X rien, 2011b ). The concept of user experience (UX) gained prominence in e-commerce and was further popularized in human computer interaction by Norman (2002) . UX, defined as  X  X  X  person X  X  perceptions and responses that result from the use or anticipated use of a product, system or service X  X  ( ISO, 2008 ), represents a more holistic way of approaching people X  X  interactions with technologies than usability:
An experience is an episode, a chunk of time that one went through X  X ith sights and sounds, feelings and thoughts, motives and actions; they are closely knitted together, stored in memory, labelled, relived and communicated to others ( Hassenzahl, 2011, p. 8 ).

UX examines the quality of information interactions from the perspective of the user. Like usability, UX is outcomes-based, but this outcome may be tangible (e.g., using your smartphone to text a friend) or intangible (e.g., sharing a joke with and feeling connected to that friend). UX also places an emphasis on process. Several researchers have emphasized the idea physical and social aspects of experience ( Laurel, 1993; McCarthy &amp; Wright, 2004 ).

If we begin to view IIR as an experience, then we must re-examine how we measure information searching and retrieval, moving beyond standard metrics of efficiency, effectiveness and user satisfaction to incorporate measures of fulfilment, play and engagement ( McCarthy &amp; Wright, 2005 ). This concept is especially important in exploratory search, which emphasizes mation need that changes as the searcher encounters and incorporates new information within an information space for the purposes of knowledge acquisition and personal growth. Given the richness of exploratory searching, traditional IIR metrics must be complemented by measures that address learning, discovery, enjoyment and engagement ( White &amp; Roth, 2009 ). In addition to developing such measures, we must also ensure that they are rigorously tested and meet standards of reliability, validity and generalizability in order to accurately reflect the user experience.

The focus of our work has been to define and measure user engagement. User engagement  X  X  X xplain[s] how and why attend to and become involved in the experience, and the user X  X  overall evaluation of the salience of the experience ( O X  X rien and Toms, 2008 ). Engagement depends on the depth of participation the user is able to achieve with respect to each experiential attribute.

The multidimensional nature of user engagement makes it challenging to measure. While we are very comfortable measuring concrete events, such as the number of errors a user makes when interacting with a system or how long it takes to find the answer to a factual search query, we are less firmly seated when it comes to activities for which there are no vis-ible or physical outcomes. Since only the user can evaluate the level of engagement experienced during an interaction with a system, a subjective approach is needed in the development of measures for this construct. We elected to develop a questionnaire, which takes assessment  X  X  ... away from the usual product-centred towards a more experiential evaluation X  X  (video games, e-shopping, e-learning, and web searching), and two large-scale survey studies conducted in the e-shopping domain ( O X  X rien and Toms, 2008; O X  X rien and Toms, 2010a ), we developed the User Engagement Scale (UES). The UES is a 31-item questionnaire that taps into six dimensions of experience: Aesthetic Appeal, Novelty, Focused Attention, Felt Involvement, Perceived Usability, and Endurability (i.e., the users X  overall impression of the experience).

As part of the scale development and evaluation process, we are interested in generalizing the UES to different research environments, including exploratory search, with the ultimate goal of producing a reliable and valid instrument that can assess user engagement in IIR settings. In the work reported here, we administered the UES to a large group of searchers who interacted with an exploratory search interface to perform decision-making tasks. In the following sections, we elabo-rate on issues inherent in the measurement in IIR, exploratory search, and user engagement, describe the current study, and present our findings. In light of our results, we discuss implications for the revision, validation, and use of the UES. 2. Literature Review 2.1. Measurement in interactive information retrieval and exploratory search
There are four basic classes of measures commonly employed in interactive information retrieval (IIR): contextual, inter-action, performance, and usability ( Kelly, 2009 ). Contextual measures include demographic and socio-cognitive variables (e.g., topic familiarity and search experience), as well as the nature of the search or work task and the setting in which the information interaction occurs. Interaction measures are collected during an IIR session, and are based on users X  search strategies (e.g., query construction, number of queries) and their interactions with retrieved documents. Performance measures, such as precision and recall, and usability measures, which gauge users X  perceptions of the system and their inter-actions with it, are outcome oriented and used to evaluate the success of an IIR session ( Kelly, 2009 ). Several researchers have examined the relationship between these four classes of measures. For example, Su (1992) found that usability measures (e.g., users X  confidence and satisfaction with the completeness and precision of the retrieved results), along with the value of the search results (i.e., utility), were highly correlated with users X  evaluations of system success. Additionally,
Al-Maskari and Sanderson (2010) found significant relationships between user satisfaction and user effectiveness (e.g., completeness, accuracy), and user satisfaction and system effectiveness (e.g., precision, recall, relevance). Such studies underscore the user as a fundamental component of IIR evaluation and the need to account for user perceptions and actions in the measurement model.
 The usability measures typically seen in IIR studies pertain to users X  attitudes toward the IR system or the search results.
Although users X  may be asked about their confidence in their search abilities or results, level of topical knowledge before and after the search, etc., they are not asked about their affective reactions to the experience , specifically their emotional responses to various stages of the interaction ( Kelly, 2009 ). Applying an affective, experiential framework to IIR may be increasingly important to the burgeoning area of exploratory search, which is as much about the journey as the destination information-seeking problem context that is open-ended, persistent, and multi-faceted; and to describe information-seeking focuses heavily on task completion and system performance, the outcomes of exploratory search, such as learning, knowledge discovery and personal growth, are less tangible.

As such, traditional measures of IIR, such as efficiency and effectiveness, may not be adequate for evaluating exploratory search. White and Roth (2009) highlight the challenges inherent in evaluating exploratory search interactions, where users may experience varying degrees of uncertainty during the search process and interact with different systems over multiple search sessions. They state that evaluation remains an understudied area of exploratory search, and approaches to its measurement must take into account user behaviours and cognition, as well as subjective assessments of user satisfaction, search tasks, content, and felt engagement. We are interested in the engagement piece of this measurement puzzle. 2.2. User engagement
User engagement is a quality of user experience that describes a positive human X  X omputer interaction. User engagement has been equated with user satisfaction ( Quesenbery, 2003 ), but previous work has demonstrated that it is much more than this ( O X  X rien and Toms, 2008 ). While engagement encompasses users X  attitudes toward systems (e.g., usability, aesthetic of activity ( Laurel, 1993; Norman, 1986 ) during system use. Through a systematic review of interdisciplinary literature and an exploratory study with technology users, we articulated a number of engagement attributes, including perceived atten-tion, challenge, feedback, control, novelty, interest, motivation, and affective and sensory appeal ( O X  X rien and Toms, 2008 ).
Contextual variables may also be important in user engagement, specifically the social nature of interacting with technolo-gies for the purposes of sharing information or experiences with other people ( O X  X rien, 2011b ).

In addition to the attributes of engagement, we developed a process-based model to describe the way in which engage-ment fluctuates during the course of an interaction. Users experience a point of engagement, a period of sustained engage-ment, disengagement, and re-engagement during system use; at each of these stages and with different types of systems, the attributes of engagement may manifest to different degrees ( O X  X rien and Toms, 2008 ). For example, the aesthetic appeal of a search system may be more important at the beginning of an interaction in order to capture users X  attention, whereas e-shoppers may require high levels of aesthetic appeal throughout a shopping encounter in order to evaluate and form attachments to products. 2.3. Measuring user engagement
Given the multifaceted nature of user engagement and its emphasis on the interaction process, it is an appropriate con-struct to apply in exploratory search environments. Recent research has focused on pinpointing behavioural indicators of user engagement in large-scale web studies. Using metrics such as dwell time, session utility, interaction duration, number of distinct and returning users, number of page visits, bounce rate, and short-and long-term interaction time ( Lehmann, that take into account the diversity of websites and users. However, not all of these performance metrics may be useful indi-cators of user engagement with a particular website or for a particular user ( Lehmann et al., 2012 ). Self-report measures can complement and aid in the interpretation of behavioural metrics, taking context and individual differences into account. However, self-report measures are susceptible to demand effects, social desirability, and acquiescence ( Kelly, Harper, &amp;
Landau, 2008 ). Even more problematic,  X  X  X ost of the questionnaires and scales that are used in IIR do not have established report instrument to measure user engagement and to test its reliability, validity, and generalizability in information-rich environments.

The User Engagement Scale (UES) built on existing research by Webster and Ho (1997) and Jacques et al. (1995) that explored engagement with educational multimedia systems. Both sets of researchers developed and administered question-naires that included some attributes of engagement (e.g., users X  perceptions of challenge, attention, feedback, variety, curi-osity, and intrinsic interest). However, our previous research indicated that there may be additional attributes that inform engagement, such as positive affect, Endurability, aesthetic and sensory appeal, and interactivity ( O X  X rien and Toms, 2008 ).
This led us to question the completeness of previous questionnaires and whether there were potential interdependencies amongst the attributes, resulting in the construction of our own scale.

The scale development process, as prescribed by DeVellis (2003) , is explained in depth in O X  X rien and Toms (2010a) , but engagement, and items (i.e., statements to which users would respond) to support each attribute from existing scales and interviews with video gamers, online shoppers, web searchers, and e-learners ( O X  X rien and Toms, 2008 ). This process resulted in approximately 400 items, which were systematically assessed to remove duplicates, clarify wording, and evaluate the  X  X  X it X  X  of the item with the construct it was intended to measure. Following this screening process, items were pre-tested.
The remaining 186 items were administered in a web-based survey to 440 general e-shoppers. Reliability analysis assessed data reduction technique, identified the most parsimonious set of items and grouped these together according to six under-lying factors or dimensions ( Table 1 ).

Secondly, we tested the validity and factor structure of the scale with a sample of 802 shoppers of a specific online book retailer (who requested anonymity). This analysis used Structural Equation Modelling (SEM) to perform Confirmatory Factor
Analysis (CFA) and Path Analysis (PA). The purpose of CFA was to verify the six-factor structure of the scale, while PA exam-ined the relationships amongst the factors ( Fig. 1 ). PA showed that Perceived Usability mediated the relationships between
Aesthetic Appeal, Novelty, Focused Attention, Felt Involvement and Endurability. In other words, Perceived Usability was an important variable in predicting e-shoppers lasting impressions of the experience as worthwhile, rewarding, etc. Aesthetic
Appeal and Novelty were predictors in the model, indicating whether users would chose to invest their attention and become involved in the e-shopping encounter. Overall, users X  level of Felt Involvement predicted perceptions of system usability and overall evaluations of the experience (i.e., Endurability).

The outcome of this scale development and evaluation process in the e-shopping domain was a 31-item instrument with six underlying dimensions or factors (see Table 1 ). 2.4. Applications of the User Engagement Scale (UES)
Scale development is a longitudinal process, and an important consideration for any measure is its generalizability, or its components of it) has been administered in studies with different types of technologies, including an archival webcast system ( O X  X rien &amp; Toms, 2010b ), the social networking application, Facebook ( Banhawi &amp; Mohamad Ali, 2011 ), and a simulated travel agency website (Hyder, 2011), and has been examined for its reliability and validity in these settings.
O X  X rien and Toms (2010b) examined the factor structure of the UES in a study that asked participants to perform fact find-ing and content summary tasks using a multimedia webcast system. Exploratory Factor Analysis (EFA) resulted in six factors, but the composition of these factors was different than in previous research ( O X  X rien and Toms, 2010a ). While Aesthetics,
Focused Attention, Novelty and Endurability remained stable constructs, the Felt Involvement sub-scale was eliminated and the Perceived Usability sub-scale became two factors: one factor contained affective items (e.g.,  X  X  X iscouraged) and the other consisted of items relating to the effort required to use the application. The clustering of affective items related to the Perceived Usability of the system was also observed in the work of Banhawi and Mohamad Ali (2011) who adminis-tered the UES to a sample of Facebook users. In this study, EFA revealed a four-factor model: Focused Attention, Perceived
Usability (affective items only) and Aesthetic Appeal emerged as distinct factors. However, items from the Endurability, Nov-elty and Felt Involvement sub-scales loaded together on one factor, suggesting that evaluations of Facebook as worthwhile, fun, and stimulating were perceived along the same dimension. This same four-factor model emerged when O X  X rien (2010) examined the relationship between e-shoppers UES scores and shopping motivations using an exploratory, rather than confirmatory, factor analysis approach.

In addition to studies that have looked at the factor structure of the UES, Hyder (2011) investigated the criterion validity of the UES in his study of user engagement with a simulated online travel agency. The UES was combined with items from other scales, and some items within the sub-scales were extracted and grouped with items from different UES constructs. For example, the Aesthetics sub-scale and four items from the Perceived Usability sub-scale were tested as  X  X  X ntecedents of engagement; X  X  items from the Focused Attention and Novelty sub-scales formed a measure of  X  X  X uriosity, X  X  and four of the
Endurability items formed part of the  X  X  X easurement of value X  X  (p. 352). UES items were used successfully with other psy-chometric and behavioural measures to examine website engagement. In addition, Hyder examined the relationship be-tween engagement variables and, concurrent with our work ( O X  X rien and Toms, 2010a ), found that aesthetics predicted
Focused Attention, involvement, and elements of Perceived Usability (control, challenge). In addition, elements of Endurabil-ity were embedded in Hyder X  X  outcome measures of  X  X  X erceived value X  X  and  X  X  X eturn intention. X  X 
In summary, applications of the UES in different research environments have demonstrated the reliability of the Aesthetic and Focused Attention sub-scales. Perceived Usability has been an internally consistent sub-scale, though there is some evi-dence to suggest that affective and cognitive items may be distinct dimensions in some circumstances. In one study, Novelty items loaded together ( O X  X rien &amp; Toms, 2010b ), while in other research these items combined with Felt Involvement and
Endurability items to form one factor ( Banhawi &amp; Mohamad Ali, 2011; O X  X rien, 2010 ). Hyder X  X  (2010) results demonstrated significant correlations between UES items and other psychometric scales and behavioural measures, and found similar pre-dictive relationships between engagement variables. Thus, some aspects of the UES have been generalized across contexts, while others have not. 2.5. Current study The current study examines the generalizability of the UES in an exploratory search environment. We administered the
UES to participants using an exploratory search system in a laboratory setting to complete complex search tasks. Consider-ation of the contextual elements of any one human-information interaction quickly highlights the potential impact of task were completing research-generated tasks in a laboratory setting with a novel interface, so we might expect to see the same factor solution as observed in this setting. However, in an exploratory search environment, we might also expect to see the same six-factor structure that emerged in the e-shopping domain, since tasks conducted in both of these environments in-volved gathering information (i.e., comparing products or information sources) and using a combination of searching and browsing strategies.

In this paper, we investigate the internal consistency and factor structure of the UES based on users X  perceptions of their experiences with a specific exploratory search system, with the goal of demonstrating its generalizability to the exploratory search domain. Based on previous studies, we expected the Aesthetic Appeal and Focused Attention sub-scales to remain stable, but were less certain about the composition of the Perceived Usability sub-scale, or whether Novelty, Felt Involve-ment and Endurability would emerge as distinct factors. 3. Method
The UES was administered at the end of a large laboratory experiment that examined how people performed complex imental tasks were randomly assigned to participants; each task was designed so that more than one page was required to respond to the task and participants were expected to make and submit their decision identifying which pages were most suitable to address the search scenario. 3.1. Search application
The wikiSearch system has been used in several studies and is fully described in Toms, McCay-Peet, and Mackenzie (2009) , which is summarized here. The interface (see Fig. 2 ) was  X  X  X lattened X  X  into in a three-column representation of the multiple webpages that are typically found in search systems. It provided access to a version of Wikipedia that was used in the INEX Interactive Track.

The three-sections of the interface pertained to (from left to right) task-based activities, search procedures, and detailed views of documents. The Task (extreme left) section contained the assigned experimental task, a  X  X  X ookbag X  X  to collect infor-mation and a Notebook or Answer pane. The second column, the Search section, contained a search box, a history section to display all queries entered and pages viewed, and the search results, which appeared as an abbreviated list but featured mouseover access to page descriptions. The third column, Page Display, contained the content, a scrollable wiki page with internal and external links and a text box with Suggested Pages that linked to other articles. The Suggested Pages links were created dynamically by entering the first paragraph of the displayed page as a search query. The intent behind the design was to follow some of Shneiderman X  X  (1998) design principles including reducing the number of mouse-clicks, leaving the user in control, and reducing memory load. 3.2. Task
The 12 tasks used required participants to make a decision between two options based on a set of criteria. Tasks were crafted so that the participants would either look at two items holistically or using a set of pre-ordained criteria. Each task is trying to decide between two dog breeds, Siberian Husky and German Shepherd; X  X  and asked participants to make a choice between the two alternatives. The tasks followed the two patterns below:
Type A .  X  X  X our friend has decided that he wants to get a dog. He X  X  never owned a dog before and he is trying to decide between two dog breeds, Siberian Husky and German Shepherd. Using the information you find in Wikipedia, would you recommend that your friend get a Siberian Husky or a German Shepherd? Provide at least three reasons for your decision X  X .
Type B .  X  X  X our friend has decided that he wants to get a dog. He X  X  never owned a dog before and he is trying to decide between two dog breeds, Siberian Husky and German Shepherd. Using the information you find in Wikipedia, you offer problems?  X  Which dog has the longer lifespan? Based on this information, would you recommend your friend select a Siberian Husky or a German Shepherd? X  X 
Participants were assigned three of these tasks of either Type A or Type B. Tasks were randomly assigned but counter balanced across the participant group. 3.3. User Engagement Scale (UES)
The wording of the UES (see O X  X rien and Toms, 2010a , for the complete set of questions) was modified to fit the current search environment, i.e., the word,  X  X  X hop X  X  or  X  X  X hopping X  X  was replaced with  X  X  X earch X  X  or  X  X  X earching. X  X  The UES must be evaluated in its current form for the purposes of validity; thus only small adaptations were made to the items. Participants indicated, for each question, the extent to which they agreed with each statement about their web searching experience and use of wikiSearch using a 7-point Likert scale from strongly disagree (1) to strongly agree (7). 3.4. Participants
Participants ( N = 381) ranged in age from 18 to 64, though 73.8% were 18 X 24 years old ( M = 21, SD = 7.1), and the second largest group were between the ages of 25 and 34 (12.6%). There were approximately the same number of males (52.8%) as females (47.1%). Most participants were students, although some indicated they were also employed in other capacities (13.1%). The majority of participants ( N = 344, 90.3%) indicated that they used search engines one or more times every day. In addition, they were frequent users of Wikipedia: almost three quarters of the sample used Wikipedia one or more times per week ( N = 201; 52.8%) or per day ( N = 74; 19.4%); the remaining participants said they used Wikipedia one or more sity email lists and signs advertising the study. 3.5. Procedure
Data collection took place in a laboratory setting in a seminar room where five to ten people could participate individually using the same laptop model. The experiment was presented using a modified version of WiIRE (Web Interactive Information
Retrieval Experimentation) ( Toms, Freund, &amp; Li, 2004 ) which contained study instructions, the consent form, and demo-graphic, pre-task, post-task and post-session questionnaires. The system automatically assigned tasks to each participant, provided a tutorial of the wikiSearch system, and integrated the wikiSearch interface with the experiment. The WiIRE system guided participants seamlessly through the process such that no researcher engagement was required. The UES was admin-istered post-session after participants completed three complex search tasks. At the end of the study, participants were thanked for their time, debriefed and given a small honorarium. A research assistant was present at all times to introduce the study, respond to questions, and oversee participants X  activities. The research reported here examines only the analysis of UES data. 3.6. Data analysis
The data was collected into a mySQL database, and exported to SPSS for analysis. While 427 participants were processed, some participants X  data was removed because they were pilot participants, had technical difficulties, did not complete the experiment, or did not fully engage in what they were asked to do. After screening the data, 381 respondents remained who on average completed each task in approximately 7 1/2
In preparation for data analysis, eight of the UES items were reverse-coded. Next, descriptive statistics were examined, namely the frequencies of valid responses, means and standard deviations of each item, and inter-item correlations ( DeVellis, 2003 ). Response rates to the majority of items ranged from 96.8% (12 missing values) to 100% (no missing values).
The exception to this was the Focused Attention item,  X  X  X uring this search experience, I let myself go, X  X  which was not answered by 31 (8.2%) of the participants. This item uses a colloquial expression that may not have resonated with all participants. However, more than 90% of participants did respond to the item and thus it was retained at this point in the analysis.

In order to include all 381 cases in the analysis, we elected to replace missing values using regression imputation. This method was selected after reviewing various imputation methods. We used random regression imputation since linear regression imputation may affect the shape of the distribution and relationships between variables not included in the regression model ( Durrant, 2005; Little &amp; Rubin, 1989 ). Although regression imputation relies on a predicted rather than actual value, this disadvantage was not as significant as those of other methods, such as using the variable mean to replace missing values, which has a tendency to compress variable distributions ( Durrant, 2005; Little &amp; Rubin, 1989 ).
Item means ranged from 2.56 to 6.29 on the 7-point Likert scale and standard deviations ranged from 0.83 to 1.39; thus the data demonstrated some variability but few responses toward the  X  X  X xtremes X  X  of the scale. Examining the inter-item correlations, several observations were made. Firstly, there were low to moderate (&gt;0.1 X &lt;0.6) significant correlations amongst items from the Endurability (EN), Aesthetics (AE), Felt Involvement (FI), and Novelty (NO) sub-scales. Secondly,
Focused Attention (FA) and Perceived Usability (PUs) exhibited low to moderate correlations (some of which were signif-icant) with other sub-scales, but there were negative associations between items from these sub-scales. Namely, the
Focused Attention (FA) item,  X  X  X uring this search experience, I let myself go X  X  was negatively and significantly correlated with PUs items,  X  X  X sing wikiSearch was mentally taxing X  X  ( r = 0.18, p = 0.000) and  X  X  X his search experience was demand-ing X  X  ( r = 0.16, p = 0.001); this was also true for the FA item,  X  X  X hen I was searching, I lost track of the world around me X  X  ( r = 0.12, p =0.01; r = 0.16, p = 0.001, respectively). As a result of this finding and the lower response rate for these FA items compared to other items, we removed them from the analysis. However, the existence of other negative correlations (though non-significant) between items from these two sub-scales indicated that there may be issues with inter-correla-tion analysis of the sub-scales. 4. Results 4.1. Reliability analysis
The reliability of the sub-scales was assessed using Cronbach X  X  alpha; values ranging from 0.7 to 0.9 were considered optimal ( DeVellis, 2003 ). Table 2 displays the Cronbach X  X  alpha, mean, and standard deviation values of each sub-scale.
Sub-scales are defined here according to the six factors and the items associated with these factors in O X  X rien and Toms (2010a) . AE, PUs, EN, FA and FI demonstrated  X  X  X ery good X  X  to  X  X  X xcellent X  X  values for Cronbach X  X  alpha and no items were eliminated from these sub-scales. The alpha value for the NO sub-scale (0.69) was in the  X  X  X inimally acceptable X  X  range. Eliminating the item,  X  X  X  continued to use wikisearch out of curiosity, X  X  improved the internal consistence of the sub-scale.
Means were calculated by summing participants X  ratings of items within each subscale and dividing by the total number of items for that sub-scale; these individual scores were then calculated to obtain means and standard deviations for each sub-scale. The means for AE, FA, FI and NO were appropriate for a 7-point Likert scale (i.e., toward the mid-point), but the average ratings of the EN ( M = 4.18, SD = 0.88) and PUs ( M = 5.61, SD = 0.75) were relatively high. The Shapiro X  X ilk test of normality indicated that the data were not normally distributed.

Next, correlations amongst the UES sub-scales were examined ( Table 3 ). Significant correlations were observed between most sub-scales. Low to moderate associations (&lt;0.5) demonstrated that the sub-scales should remain distinct during factor analysis; correlations above 0.5, as observed between EN and PUs, EN and FI, and NO and FI, indicated that some of the items within these sub-scales may load on more than one factor. There was a negative correlation between the FA and PUs sub-scales ( r = 0.01, p = 0.7). This indicated that these sub-scales represented distinct dimensions of user experience, but that their relationship may require further exploration. 4.2. Factor analysis
A visual examination of the scree plot and eigenvalues indicated a four or five factor solution (see Fig. 3 ). Principal axis factor analysis (PAF) with promax rotation was performed. PAF was selected as the method of extraction because the data were not normally distributed. With regard to rotation, orthogonal rotations (e.g., varimax) are commonly performed and noted for simplicity. However, Reise, Waller, and Comfrey (2000) advocate for the use of oblique rotation, including promax.
They reason that, in oblique rotations, factors are permitted to correlate, which cannot only provide researchers with valu-able information about the nature of the relationships amongst factors, but is also a more realistic portrait of psychological to generalizability studies.

The Kaiser X  X eyer X  X lkin Measure of Sampling Adequacy (KMO = 0.92) indicated that there were distinct factors amongst ships amongst the factors. In the first iteration, a five-factor solution was obtained that accounted for 60.8% of the total variance. In the five-factor solution, the PUs items and one EN item loaded on Factor 1. However, four PUs items also loaded on Factor 5:  X  X  X  found wikiSearch confusing to use X  X  (0.3);  X  X  X sing wikiSearch was mentally taxing X  X  (0.49);  X  X  X his search expe-and the small amount of variance (3.9%) the fifth factor contributed to the model, a four-factor solution was specified.
The four-factor model ( Table 4 ) accounted for 56.97% of the total variance. The PUs items and one EN item formed Factor 1, accounting for 31.4% of the variance. The remaining EN, FI and NO items loaded together on Factor 2, which contributed 13.1% to the total variance. AE items (Factor 3) contributed to 7.75% of the variance. FA items made up Factor 4, accounting for 4.68% of the total variance. There were some variables that loaded on multiple factors, specifically the FA item,  X  X  X  was absorbed in my search task X  X  (Factors 2 and 4), and three EN items,  X  X  X  would recommend wikiSearch to my friends and fam-ily, X  X   X  X  X  consider my search experience a success, X  X  and  X  X  X earching using wikiSearch was worthwhile X  X  (Factors 1 and 2). Although these variables loaded more strongly on one factor, there presence on two factors is worthy of further discussion.
The internal consistency of the resulting factors was examined ( Table 5 ). The Cronbach X  X  alpha values were in the very good range, suggesting that the resulting factors were reliable. The means and standard deviations remained unchanged for the Aesthetics and Focused Attention Factors, but were recalculated for Factors 1 and 2. Averages were based on participants X  ratings for the nine items that comprised each of these factors.

Correlation analysis demonstrated significant moderate correlations amongst the resulting factors ( Table 6 ). The excep-tion to this was the negative, non-significant relationship between FA and PUs. It is interesting to note that FA and PUs were both correlated with the other dimensions, but not with each other. 4.3. Multiple regression analysis
The factor analysis of the wikiSearch data produced a four-factor solution, rather than the six-factor solution obtained in the e-shopping domain. As such, it was not possible to confirm the path model obtained in previous research using structural equation modelling ( Fig. 1 ). However, we were interested in how the four factors related to each other. Based on the corre-lation analysis of the four factors, we selected Factors 1 (PUs), 3 (AE) and 4 (FA) as predictor variables and Factor 2 (NO, FI, and EN) as the criterion variable. The simultaneous method was used to determine if Factors 1, 3 and 4 would successfully predict Factor 3. The regression model was statistically significant, with the three predictor variables accounting for 55% of the variance in the criterion variable (Adjusted R 2 = 0.55; F predictor variable to the model (Standardized Beta Coefficient) and to the criterion variable ( t value). 5. Discussion 5.1. Factor structure of the UES The original sub-scales of the UES (AE, FA, FI, PUs and NO) demonstrated good internal consistency prior to factor analysis.
However, PAF resulted in a four-or five-factor model. After observing that PUs items pertaining to cognitive effort loaded with other PUs items on Factor 1 and formed their own factor (5), we specified a four-factor solution. While FA, AE, and PUs remained distinct factors, items from the NO, FI, and EN sub-scales converged to form one factor.

Table 8 compares the findings of studies that have employed the UES in its entirety. The four-factor model that emerged in the current study is different from the six-factor model first observed in the general shopping environment ( O X  X rien and
Toms, 2010a ), but concurs with the findings of Banhawi and Mohamad Ali (2011) and O X  X rien (2010) . Across all applications of the UES, AE, FA, and PUs have remained integral factors. However, Banhawi and Mohamad Ali (2011) found that only the affective items (e.g.,  X  X  X  felt frustrated ...  X  X ) made up the PUs component of Facebook use, and O X  X rien and Toms (2010b) observed PUs items loading on two distinct factors that distinguished affective and cognitive components of multimedia webcast use. In three administrations of the UES ( Banhawi &amp; Mohamad Ali, 2011; O X  X rien, 2010 ; current study), NO, FI and EN items merged to form one factor.
 The Novelty sub-scale contains items pertaining to curiosity in the content of the application and interest in the task. The
Felt Involvement sub-scale addresses users X  Felt Involvement in their task and their overall assessment of the experience as fun. Lastly, Endurability items ask users to consider the outcome of their experience with the application, i.e., whether it was successful, worthwhile, and rewarding, as well as whether they would recommend it to family and friends. These sub-scales have demonstrated good internal consistency prior to factor analysis in the majority of studies that have employed the UES.
One exception to this was the NO sub-scale in O X  X rien X  X  (2010) study of e-shopping. However, these items have consistently loaded on one factor across e-shopping ( O X  X rien, 2010 ), social networking ( Banhawi &amp; Mohamad Ali, 2011 ), and wikiSearch applications. This finding has motivated us to consider revising the scale in order to enhance its validity. However, we must determine whether the underlying issue is the factor structure of the UES, or the ability of the Novelty, Felt Involvement, and Endurability items to adequately represent these constructs ( Reise et al., 2000 ).
 Firstly, the items that comprise the NO, FI and EN sub-scales focus on users X  evaluation of their experience (e.g., Felt the application X  X  usability (PUs) or Aesthetic Appeal (AE), or their ability to reach a state of flow (FA), are equated with the value and success they ascribe to the interaction. Thus, Novelty, Felt Involvement and Endurability may not be distinct dimensions of the experience, but form one dimension that characterizes their felt engagement in the task and with the experience as a whole.

A second possibility is the nature of the NO, FI and EN items. Previous research has suggested that each of these dimen-sions encourages user engagement. Endurability, which represents users X  evaluation of system success ( DeLone &amp; McLean, re-engaging with technologies they felt involved with or that provided something new ( O X  X rien and Toms, 2008 ). Other studies have demonstrated that Felt Involvement and Novelty are powerful predictors of engagement. For example, in a longitudinal laboratory experiment that tested different interfaces of a digital newspaper, Toms (2000) found that partici-pants read more novel items regardless of the web interface. More recently, O X  X rien (2011a) found that Novelty and Felt
Involvement played a strong role in maintaining user engagement with online news content, whereby newsreaders were prone to disengage from news stories that did not offer anything new on a topic or that they could not relate to on a personal and Endurability as facets of user experience, but the ability of the UES items to represent them as distinct constructs. UES items mostly pertain to users X  perceptions of the system and their experience using it to perform a task, be it shopping, searching, etc. Yet few items pertain to the content of the system, and this may be a severe shortcoming, since both Novelty ven, 2008 ). 5.2. Relationships amongst the factors
Due to the four-factor model that emerged, we were unable to fit the wikiSearch data to the path model ( Fig. 1 ) developed in the e-shopping context ( O X  X rien and Toms, 2010a ). However, we examined the correlations amongst the factors and used multiple regression analysis to understand more about how the factors of the UES are related. We observed significant, positive correlations amongst most of the factors, which is consistent with previous research ( O X  X rien and Toms, 2010a ). However, the negative relationship between Focused Attention and Perceived Usability warrants further discussion.
The negative correlation between FA and PUs is inconsistent with our research in the e-shopping domain, but similar to alyi, 1990 ), characterized by temporal dissociation and complete absorption in an activity. However, perceived control, shown to relate positively to usability. Thus the negative association between these two factors is surprising. One explana-tion for this may be situational: both the wikiSearch and webcast studies were conducted in laboratories with researcher assigned tasks. The nature of the setting and the tasks may have made a state of flow difficult to achieve. Indeed, in the current study, the average PUs rating ( M = 5.6) was significantly higher than that of FA ( M = 3). In naturalistic environments that are not constrained by time or externally motivated tasks, flow and usability may act in a more complementary manner.
Despite the negative association between Perceived Usability and Focused Attention, both play an important role in over-all experience. Multiple regression analysis, using Factor 2 (Novelty/Felt Involvement/Endurability) as the criterion variable showed that Aesthetics, Perceived Usability, and Focused Attention all contributed to the criterion variable, and that the model was not as strong when one of these predictor variables was removed. As a result, it is important not to discount the contribution of both usability and Focused Attention to user experience. 5.3. Implications for scale validation and revision The administration of the UES across different applications has resulted in three relatively stable factors: Perceived
Usability, Aesthetics and Focused Attention, and three sub-scales (Novelty, Felt Involvement, Endurability) merging to form one factor. Our ultimate goal is to produce a reliable, valid psychometric scale that can be used to gauge the level of engage-ment users feel when interacting with an information system. Given the findings of this and other studies, we see the way forward as a combination of scale revision and delineation of what is being measured, i.e., validation. In this section, we discuss these objectives at the item, dimension, and scale levels. 5.3.1. UES items
The number of items retained after reliability and factor analyses varied across different applications of the UES. This may experience, I let myself go, X  X  and  X  X  X hen I was searching, I lost track of the world around me X  X  were eliminated during data onate with all participants. However, similar expressions were present in FA items that were not eliminated, such as  X  X  X  was these items may have been eliminated is that they did not suit the context. The average FA score was low compared to the means of other sub-scales, especially PUs, which may suggest that participants were more focused on the functionality of the continued to use WikiSearch out of curiosity, X  X  was removed during the reliability analysis. In this case, participants may have continued to use the search system because they had no alternative for finishing the task. Such items may be more appro-priate for exploratory search studies conducted in the field where there are no parameters around the system being used or task being executed.

Hence, at the item level, we need to investigate the generalizability of individual items to different research contexts, specifically whether the wording of items reflect Western ways of describing an experience and the applicability of items to both laboratory and more naturalistic settings. However, since different items have been eliminated in each administra-included in the Novelty factor in the webcast study ( O X  X rien &amp; Toms, 2010b ), potentially because the wikiSearch interface had functional tools to assist with search whereas the Webcast interface had tools that juxtaposed its information objects in different and perhaps unusual ways. Therefore, we recommend continuing to use all 31 items and using statistical tech-niques to determine the items that are most salient to user engagement in each circumstance. 5.3.2. Dimension level
The factor structure and regression analysis provide insight into the UES at the dimension level. Firstly, the results of the factor analysis in this and other studies indicate that NO, EN and FI require further investigation. Specifically, we must exam-ine the UES items that represent these constructs in order to ascertain whether they truly capture each dimension, or if the quality of user experiences is best captured in four factors. In other words, should these be distinct constructs of engagement, or do they belong together? We speculate that content will play an important role in how users think about novelty and involvement with respect to task ( O X  X rien, 2011a ), and that the relationship between content and engagement should be explored in greater depth and be better reflected in the UES.

Secondly, the relationship between PUs and FA has been positive in some studies, and negative in others, including the current work. Prior research would suggest that a system must be usable in order to support absorption and engagement ( Finneran &amp; Zhang, 2003; Huang, 2003; O X  X rien and Toms, 2008; Webster et al., 1993 ). However, the relationship between these two dimensions may be predicated on the needs of the user: searchers who have pragmatic goals may not be inter-ested in being in a state of flow during their interactions, while those with more hedonic needs  X  or longer time periods to spend engaged in an interaction -may report a stronger link between these constructs.

In addition, although the PUs items consistently factored together across studies, there have been instances where the affective and cognitive items loaded separately ( Banhawi &amp; Mohamad Ali, 2011; O X  X rien &amp; Toms, 2010b ). Originally, we distinguished affect as a distinct attribute of user engagement ( O X  X rien and Toms, 2008 ), but during the process of scale development and analysis, these items became integrated into other sub-scales. It may be worth revisiting the possibility of an affective sub-scale, or using an established affective scale with the UES. Another possibility is that affective and cog-nitive aspects of experience will be more relevant depending upon the application. For example, e-learning environments may be more cognitively demanding whereas social networking sites may elicit more emotional responses. Uncertainty plays a major role in exploratory search, where information needs shift over the course of interaction as searchers articulate their goals, acquire knowledge, and reformulate their search trajectories ( White &amp; Roth, 2009 ). Thus, affect and cognition may also fluctuate, as users move from uncertain to  X  X  X ureka X  X  moments: engagement may vary over the course of the expe-rience and negative engagement may be an important part of the process.

Lastly, we considered additional attributes of engagement in earlier work ( O X  X rien and Toms, 2008 ) that did not emerge as distinct factors in the e-shopping environment ( O X  X rien and Toms, 2010a ), but may be of value in IIR. One example is inter-activity, which Huang (2003) deemed  X  X  X ey to creating experiential flow ... and is the most important attribute for demanding hedonic Web performance X  X  (p. 433). Interactivity, however is also a multi-dimensional element that may be construed as the way in which an experience is achieved. As IIR interfaces become increasingly complex, incorporating additional tools, multi-media and social media features, it will be imperative to understand the relationship between interactive features and user engagement with those features. Another variable to consider is user motivation, which has been shown to predict some attributes of engagement ( O X  X rien, 2010 ), and may be tied to information needs, and may as indicated earlier, impact flow.
In order to development a more holistic measure of IIR engagement, the UES may need to be extended or augmented with additional measures. 5.3.3. Scale level As part of the refinement process, we also feel that it is important to look more closely at what is being measured by the
UES, especially in the context of IIR and exploratory search. Hassenzahl X  X  (2011) framework of user experience includes hedonic and pragmatic aspects and system and user-specific variables that coalesce to form an evaluation of an experience.
The UES includes items that relate to how the user perceived the system (e.g., PUs and AE sub-scales), their state of mind during system use (e.g., FA sub-scale), and their overall evaluation of the experience (e.g., EN sub-scale); in addition, the items are a blend of the pragmatic (e.g.,  X  X  X  found this system confusing to use X  X  [PUs]) and hedonic (e.g., This experience was fun [FI]). Therefore, the UES is a holistic instrument for assessing user experience.

However, some of the UES items relate not only to the system and the user, but to the task that is being accomplished as part of the interaction, e.g.,  X  X  X  felt involved in the search tasks. X  X  Previous user engagement research has shown that users X  motivation and interest is intertwined with task, whether it is tangible (e.g., purchasing a book), or intangible (e.g., playing a computer game for enjoyment and escape) ( O X  X rien and Toms, 2008 ). In addition, task influences users X  system preferences.
Jacques et al., 1995 examined participants X  use of educational multimedia and found that visually based media, such as ani-mations, photographs, and videos, were more engaging for browsing tasks, whereas text-based media were favoured for search tasks. In IIR, searching and browsing are embedded in larger activities that shape users X  expectations and use of sys-tems ( Ruthven, 2008 ), suggesting that the relationship between engagement and task should be delineated further.
In addition to examining engagement at the task (versus user and system) level, the role of content in engagement must be more clearly articulated. Recent research has shown that online news users may associate a dimension such as Novelty with content (i.e.,  X  X  X how me new information about a story I have been following X  X ), whereas others may be more interested tion retrieval, content is paramount, with the focus on the quality and relevance of the retrieved results. The exploratory search process is characterized by periods of exploration and uncertainty, knowledge acquisition and insight, focused search-ing and the synthesis of resources ( White &amp; Roth, 2009 ). The success of an exploratory search is defined by its ability to  X  X  X larify vague information needs, learn from exposure to information in document collections, and investigate solutions information retrieval, and must be investigated more thoroughly with user engagement: Does engagement fluctuate depending on the users X  evolving understanding of the content they are seeking, or the availability and presentation of that content from the information system? How do users X  judgements of relevance, interest, credibility, etc. equate with their perceived engagement?
As a result, we need to look at the relationship between user, system, content, and task in order to enhance the validity of the UES. To improve the criterion validity of the scale, we could develop additional items to ensure that system, user, task and content are represented by the scale and examine the relationship between these sets of items. In addition, we must look at the UES in conjunction with other measures in order to explore its concurrent validity. Although questionnaires play an pose challenges to researchers trying to collect a  X  X  X rue X  X  picture of experience ( Kelly et al., 2008 ). Thus, triangulated approaches to measurement are imperative. In IIR in general, and exploratory search more specifically, these measures may be behavioural, physiological, or subjective. With regard to the former, recent research has adopted metrics such as dwell time, number of page views, number of distinct and returning users, and time spent interacting with a website over (e.g., electrodermal activity, heart rate) are an emerging area of evaluation in information science, and have been used in human X  X omputer interaction research to explore affective responses to systems ( Mahlke &amp; Minge, 2008 ). Lastly, subjective measures, such as those traditionally used to assess the relevance of retrieved results (c.f., Ruthven, 2008 ), would illuminate the relationship between felt experience and content, while cognitive variables (e.g., cognitive style, perceptual speed) (c.f.,
Al-Maskari &amp; Sanderson, 2011 ) would provide further insight into the role of individual differences in user engagement. 5.4. Limitations
This study was situated in a laboratory setting, where participants interacted with the wikiSearch system to accomplish researcher-generated tasks. Thus, the general set-up of the study may be a limitation. To evoke engagement in more con-trolled settings, we may need to expend more effort developing the simulated task scenario ( Borlund, 2000 ), or provide users with more choice in and control over their search tasks, since motivation and intrinsic interest are important qualities of engagement ( Jacques et al., 1995; O X  X rien and Toms, 2008 ) and IIR ( Ruthven, 2008 ). We may also consider more naturalistic environments and longitudinal designs for evaluating search engagement ( Kelly, 2009; White &amp; Roth, 2009 ).
The majority of participants were familiar with search engines and Wikipedia. However, the wikiSearch system introduced new features for retrieving and managing results. Participants were given a tutorial before beginning their exper-imental tasks, and previous studies have not uncovered any issues with the use of this novel interface (see Toms et al., 2009 ).
Thus we do not believe that the interface especially contributed to the findings. Indeed, PUs subscale items, were on average higher than the mid-point.

Unlike in other studies, the UES was completed post-session after participants completed the tutorial, demographic, pre-and post-task questionnaires, and three search tasks. They may have been experiencing fatigue at this point in the study, which may have led to satisficing, whereby they may have focused on one aspect of the experience rather than the whole experience as instructed ( Kelly et al., 2008 ).

Lastly, the questionnaire items were modified from the initial one used in the e-shopping study. Did the subtle (and what we believed to be accurate) changes lead to a different interpretation of an item? See the following pairs:
Example 1. e-shopping:  X  X  X hopping on this website was worthwhile. X  X  wikiSearch:  X  X  X earching using wikiSearch was worthwhile. X  X 
Example 2. e-shopping:  X  X  X  blocked out things around me when I was shopping on this website. X  X  wikiSearch:  X  X  X  blocked out things around me when I was using wikiSearch. X  X 
In the these examples, e-shoppers may have focused on their shopping experience  X  the interaction of shopping task with the technology  X  at an online bookstore; the wikiSearch participants may have only focused on the technology when answer-ing what we perceived as the same item. The challenging aspect of adapting an instrument constructed for one purpose is in making valid and reliable modifications. At the same time it highlights the tight integration of task, technology and content. 6. Conclusion
In the current study, we administered the User Engagement Scale (UES) to users of an interactive search system and con-trasted these results with previous administrations of the Scale in e-shopping, webcast, and social networking environments.
To date, the context in which the UES has been administered has varied in several important ways, including the setting (lab-oratory versus online), sample (university pool versus general public), task (assigned versus self-generated), and time lapse between task completion and responding to the survey (immediately versus within six months). According to Serenko and
UES, as we have seen differences in the number of items retained across contexts and in the factor structure. However, three sub-scales (Perceived Usability, Focused Attention, and Aesthetic Appeal) have demonstrated stability across several studies.
The configuration of items from the Novelty, Felt Involvement, and Endurability sub-scales have been less straightfor-ward, and there is a need to review the items that make up these sub-scales to ensure they adequately reflect the constructs they represent, or if the UES is a four-factor instrument. This is an important next step, as it will determine how researchers who wish to use the UES will calculate UES sub-scale and overall scores.

In this paper, we examined the generalizability of the UES to a new, exploratory search environment. The four-factor model that emerged led us to consider scale revision and further validation activities at the item, dimension, and overall scale levels. We provide recommendations for improving the UES, including investigating the representativeness of items for constructs and with non-Western users, solidifying the dimensions that make up user engagement, delineating the rela-tionship between user, system, task, and content aspects of user experience; and examining the relationship between the
UES and other measures. Scale development and evaluation is a longitudinal process, and only by testing the UES in different environments and under different circumstances can we hone its psychometric properties and produce a reliable, valid and generalizable instrument for understanding users X  experiences with information systems.

The complexity of search requires more holistic metrics to assess the users X  experience. In this regard, the UES is a useful tool for gauging the pragmatic and hedonic facets of exploratory search. Although we are recommending improvements to the UES, it is useful in its current form for evaluating users X  level of engagement along a number of user and system dimensions.
 Acknowledgements Research supported by Social Sciences and Humanities Research Council (SSHRC) Grant to O X  X rien and GRAND Network of
Centres of Excellence grants to O X  X rien and Toms, under the nGAIA Project. The wikiSearch experiment was conducted at the iLab, Dalhousie University, Canada and funded by Grants to Toms: Canada Research Chairs, Canada Foundation for Innova-tion, and the National Science and Engineering Research Council, and also from the Social Sciences and Humanities Research
Council Canada to M. Haufner &amp; B. Detlor (McMaster University) and E. Toms, and V. Trifts (Dalhousie University). The fund-ing sources had no role in the design, conduct or analysis of this work. The authors acknowledge the work of Chris Jordan,
Sam Hall, and Tayze Mackenzie who built the wikiSearch system and re-created WiIRE, and of Lab Manager, Lori McCay-Peet, and research assistants, Jason Smith, Alexandra MacNutt, Emilie Dawe, Samantha Dutka, Jennifer Weldon, and Sarah Gilbert. References
