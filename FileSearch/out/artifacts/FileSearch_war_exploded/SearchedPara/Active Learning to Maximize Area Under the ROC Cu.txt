
In active learning, a machine learning algorithm is given an unlabeled set of examples U , and is allowed to request labels for a relatively small subset of U to use for training. The goal is then to judiciously choose which examples in U to have labeled in order to optimize some performance criterion, e.g. classification accuracy. We study how active learning affects AUC. We examine two existing algorithms from the literature and present our own active learning al-gorithms designed to maximize the AUC of the hypothesis. One of our algorithms was consistently the top performer, and Closest Sampling from the literature often came in sec-ond behind it. When good posterior probability estimates were available, our heuristics were by far the best.
In active learning, a learning algorithm A is given an un-labeled set of examples U , and is allowed to request labels for a relatively small subset of U to use for training. The goal of active learning is to choose which examples in U to have labeled in order to optimize some performance cri-terion, e.g. classification accu racy. Applications of active learning include those in which unlabeled data are plenti-ful and there are only enough resources to label relatively few of them. Such data sets include web pages, biological sequences, and images.

ROC (Receiver Operating Curve) analysis has attracted high attention in machine lear ning research in the last few years. Due to its robustness in imprecise environments, ROC curves have been advocated and gradually adopted as an alternative to classical machine learning metrics such as misclassification rate. ROC has long been used in other fields such as in signal detection [10] and medical diagno-sis [11] to describe the trade-off between true positive rate (TPR) and true negative rate (TNR) of a two-class classifi-cation model. In machine learning, people usually use TPR vs. false positive rate (FPR) to generate the plot. The area under the ROC curve (AUC) is of particular interest in that both strong statistical properties and real world experiments have indicated it superior to em pirical misclassification rate as an evaluation measure [7]. AUC has also been used as a tool to select or construct models [15, 22, 21]. When the class distribution and cost functions are skewed or un-known, significant advantages have been observed.
TPR and FPR depend on the classifier function h and the threshold  X  used to convert h ( x ) to a binary predic-tion. One thus plots TPR vs. FPR as a curve by varying the threshold  X  , resulting in the ROC curve. The area under the curve (AUC) indicates the pe rformance of this classifier: the larger the better (an AUC of 1.0 indicates a perfect rank-ing). There is also a strong connection between AUC and a statistical ranking test: AUC is the same as the Wilcoxon-Mann-Whitney statistic [11], which is an unbiased probabil-ity estimate that a randomly drawn positive example would be ranked higher than a randomly drawn negative example.
We study how active learning affects AUC. We exam-ine two existing algorithms from the literature: Roy and McCallum X  X  [23] Error Reduction Sampling (ERS) algo-rithm (designed to directly minimize prediction error) and the  X  X losest Sampling X  method [24, 26] (sampling clos-est to the decision boundary; well-known as a simple, fast, high-performance active sampl er). We also present our own active learning algorithms designed to maximize the AUC of the hypothesis. One of our algorithms (ESTAUC) was consistently the top performer, and Closest often came in second behind it. When good posterior probability esti-mates were available, ESTAUC and another of our heuris-tics (RAR) were by far the best.

The rest of this paper is organized as follows. In Sec-tion 2 we review related work. Then in Section 3 we present our algorithms and compare them to ERS, Closest, and ran-dom sampling in Section 4. We conclude in Section 5.
In order to achieve labeling efficiency, an active learner tries to select the most informative example from the unla-beled pool U with respect to some performance measure. A typical performance measure that has been extensively studied in active learning is expected accuracy. One basic idea [6] is to select examples that effectively shrink the cur-rent version space (the set of hypotheses consistent with the training data). As discussed by Tong and Koller [26], such a heuristic would be probabilistically optimal if it could al-ways exactly halve the version space each time A makes a query. Strong theoretical results using this idea are yielded by the Query by Committee algorithm [9, 25].

The  X  X losest Sampling X  method [24, 26] (sometimes called  X  X ncertainty sampling X ) can be thought of as a heuristic to shrink the version space. It greedily selects points that are closest to the current decision boundary. The intuition behind this is that points that are closest to the current decision boundary are points that algorithm A is most uncertain about. By labeling these, A can have bet-ter knowledge about the correctness of its decision. Tong and Koller [26] explain why this method often works: in a large margin classifier such as a support vector machine (SVM), the current hypothesis lies approximately  X  X n the center X  of the version space and by choosing an example (a hyperplane in version space) that is closest to it, it also cuts the version space approximately in half. However, if all the candidate points are very close to or lying on the decision hyperplane, it seems reasonable to explore diversity among these examples [4, 20]. Also, focusing on examples near the decision boundary prevents exploration of the feature space for regions of examples that the current hypothesis misclas-sifies [2, 19].

Another approach [9, 12, 14, 23] is to select exam-ples that are helpful in building up confidence in low fu-ture error. It is impossible to know the exact future er-ror without knowing the target concept, but approximations make this method feasible. For example, Roy and McCal-lum [23] suggest to directly minimize the expected error on the dataset. They started by fixing a loss function and then estimated the change in loss of the classifier when a candi-date example x  X  U and its label were added to L . Specifi-cally, when log loss is used, Roy and McCallum X  X  algorithm would choose argmin where y x is the true label of example x , Y is the set of la-bels, P ( y | x ) is the true posterior probability of label y given instance x ,and  X  P xy x ( y | x ) is the estimate of the posterior by the model trained on L  X  X  ( x, y x ) } .Since P ( y | x ) is unknown, Roy and McCallum used their esti-mate  X  P in its place. Since y x is also unknown, they consid-ered adding x with each label individually, then combined the two loss estimates weighted by their posterior estimate. Thus for log loss, they selected argmin where  X  P ( y | x ) is the posterior estimate of the current model (i.e. the one trained on L ). Because the candidate model X  X  posterior estimate is used in place of the true pos-terior probability in the loss function, ERS selects those ex-amples that maximize the sharpness of the learner X  X  poste-rior belief about the unlabeled examples [23].
 Related to Roy and McCallum X  X  work, Nguyen and Smeulders [18] chose examples that have the largest con-tribution to the current expected error: they built their clas-sifiers based on centers of clusters and then propagated the classification decision to the other samples via a local noise model. During active learning, the clustering is ad-justed using a coarse-to-fine s trategy in order to balance be-tween the advantage of large clusters and the accuracy of the data representation. Yet another approach [17] is spe-cific to SVM learning. Conceptually, in SVM learning if we can find all the true support vectors and label all of them, we will guarantee low future error. Mitra et al. assigned a confidence factor c to examples within the current deci-sion boundary and 1  X  c to examples outside each indicat-ing the confidence of whether they are true support vectors, and then chose those examples probabilistically according to this confidence.

The third category of activ e learning approaches con-tains active learning algorithms that try to quickly  X  X oost X  or  X  X tabilize X  an active learner. Active learning is unstable, especially with limited labeled examples, and the hypothe-sis may change dramatically each round it sees a new exam-ple. One way to boost active learning algorithms is simply combining them in some way. For example, the algorithm COMB [2] combines three different active learners by find-ing and fast-switching to the one that currently performs the best. Osugi et al. [19] adopted a similar approach with a simpler implementation and focused on how to balance the exploration and exploitation of an active learner. In their implementation the empirical difference between the cur-rent hypothesis and the previous one is used as a criterion to decide whether exploration should be further encouraged. In other exploration-based active learning, Xiao et al. [28] studied the problem of active learning in extracting use-ful information in commercial games, in which  X  X ecision-boundary refinement sampling X  (analogous to Closest sam-pling) and  X  X efault rule sampling X  (analogous to random sampling) mechanisms are each used half of the time.
Since each of the above algor ithms attempts to directly optimize some measure of performance (e.g. minimizing uncertainty or minimizing future prediction error), one would expect such algorithms to tend to increase AUC as a side effect. The purpose of our work is to assess how well some algorithms do just that, as well as presenting algo-rithms designed to directly maximize AUC.
We now describe our active learning algorithms designed to maximize AUC. Throughout this section, we let h ( x )  X  R denote the current hypothesis X  s confidence that example x is positive (the larger the value, the higher the confidence in a positive label). The value of h need not be a probability estimate except in one of our algorithms (ESTAUC).
In our first heuristic, Rank Climbing (RANC), we use the current hypothesis h to rank all examples in L  X  U where L is the set of labeled examples used to train h ,and U is the unlabeled pool. The examples are ranked in descending or-der according to the confidences h ( x ) .Let x be the lowest ranked example from L with a positive label. We select for labeling the lowest ranked unlabeled example that is ranked higher than x : In the unlikely event that there is no example in U ranked higher than x , RANC chooses the highest-ranked example from U .

In our second heuristic, Rank Sampling (RANS), we again use the current hypothesis h to rank all examples in L  X  U in descending order according to their confidences. Let x u be the highest ranked example from L with a neg-ative label, and x be the lowest ranked example from L with a positive label. The example that we choose to label is selected uniformly at random from the set C = { x  X  U : h ( x ) &gt;h ( x ) and h ( x ) &lt;h ( x u ) } .If C is empty then we repeatedly change x u to be the next highest ranked example from L ,and x to be the next lowest ranked example from L until C is non-empty.

Since AUC is proportional to the number of negative ex-amples ranked below positive examples, RANC and RANS attempt to find an unlabeled negative example that ranks above positive examples. RANC assumes that the lowest-ranked example above x is the most likely to be negative, while RANS makes a random selection to reduce sensitivity to noise.

For our next algorithm, first assume that we know the labels of the examples in U . Then the example from U that we choose to label is the one that most improves the AUC on the unlabeled examples, where the AUC is computed by the formula of Hanley and McNeil [11] given below. More precisely, we would choose argmax where y x is the true label of example x , h xy x ( x ) is the confidence of the hypothesis trained on L  X  X  ( x, y x ) } evaluated on x , U ( h xy x ,x )= { x  X  U : h xy x ( x ) &lt; h xy x ( x ) } is the subset of examples in U that have con-fidence less than that of x when evaluated with h xy x , P = { x  X  U : y x =+ } , N = { x  X  U : y x =  X  X  , and I (  X  )=1 if its argument is true and 0 otherwise. Since the denominator is independent of x , we instead can use the unnormalized AUC: argmax Since we do not know the true labels of the examples in U , we adapt the approach of Roy and McCallum [23] and use probability estimates de rived from the hypothesis h x place of the indicator functions. Further, since we do not yet know the label of the candidate point we are consid-ering labeling, we compute (1) using each possible label and weight them according to our posterior probability es-timates of each label: argmax where h x + ( x ) is the confidence of the hypothesis trained on L  X  X  ( x, +) } and evaluated on x . In addition,  X  P ( is the probability of predicting y  X  X  + ,  X  X  given x by hy-pothesis h trained on L ,and  X  P xy ( y | x ) is the probabil-ity of predicting y given x by hypothesis h xy trained on L  X  X  ( x, y ) } . The probability estimates may come from e.g. na  X   X ve Bayes or from logistic regression with an SVM. We refer to this approach as Maximizing Estimated AUC (ESTAUC).

Our fourth heuristic, Rank Reinforcing (RAR), focuses on the ranking induced by the hypothesis. RAR uses the current model h to label all examples in U and uses this labeling in the computation of the AUC of U as ranked by h x + and h x  X  . Specifically, the example RAR chooses is where f ( x ,x )= I ( h ( x ) &gt; X  ) I ( h ( x ) &lt; X  ) and the threshold used to map h (  X  ) to a binary label. Ties can be broken as follows. Let T  X  U be the set of examples involved in the tie. We can break ties either by applying ESTAUC over T or by summing the margins of pairs of relevant examples in T : argmax RAR amounts to choosing x  X  U such that h x most re-inforces h  X  X  ranking of the unlabeled examples to the ex-tent that examples that h predicts as positive remain ranked higher than examples that h predicts as negative. If implemented as stated above, the ERS, ESTAUC, and RAR heuristics could be slow due to the need to repeat-edly retrain the hypotheses. There are, however, several techniques that allow for speeding up the execution time of these heuristics without harming performance. The first technique is to filter the set of candidate examples that are under consideration for labeling. This can be accomplished through random sampling, or by using a faster active learn-ing heuristic to rank the examples in the unlabeled pool and choosing the most promising ones as the candidates. We found that a candidate pool of 100 examples filtered by Closest Sampling produced very good results for ESTAUC. Using a classifier that is capab le of incremental and decre-mental updates also reduces execution time as it removes the necessity of rebuilding the classifier each time a can-didate point is evaluated. For example, both na  X   X ve Bayes and SVMs are capable of increm ental and decremental up-dates [5] . Experiments were carried out on 8 data sets from the UCI Machine Learning Repository [3], and one dataset derived from the United States Postal Service (USPS) handwritten digit recognition dataset. (To make the latter data set binary-labeled, from the USPS dataset we only used examples with the digits  X  3  X  X r X  8  X .) Information on each dataset is sum-marized in Table 1, including the ratio of positive to nega-tive examples ( P/N ). Since AUC was the metric used in evaluating performance, all data sets are two-class.
In addition to the four algorithms of Section 3, tests were run with Closest Sampling, Roy and McCallum X  X  log loss Error-Reduction Sampling (ERS) method, and a random sampler. The heuristics were evaluated using the SVM Se-quential Minimal Optimization (SMO) as the base learner. All experiments were run with 15 and 100 examples in the initial labeled training set L 0 . The heuristics were imple-mented in Java within the Weka machine learning frame-work [27]. We used the Weka implementation for SMO, applying Weka X  X  logistic regression to get probability esti-mates when needed.

We used k -fold cross validation in our tests. Ten folds were used on all of the datasets except Breast Cancer, where seven folds were used due to the small size of the dataset. Our testing methodology is summarized below. 1. For each partition P i , i  X  X  1 , 2 ,..., 10 } , set aside 2. Report the average of the results of all tests.
For ESTAUC, RAR, and ERS, the set of instances under consideration for labeling was reduced to 100 candidates to increase speed. How ever, all instances in U were used for estimating AUC. The sized-100 subset of U was cho-sen as follows. First, the examples in U were ranked by Closest from least to most certain. Then the top 100 most uncertain examples were used as the candidate pool 1 .Asa control we also introduced a new heuristic called Random-CS that selects an example to label uniformly at random from the Closest-filtered candidate set, whereas Random chooses uniformly at random from all of U .

Learning curves were constructed to evaluate the behav-ior of the algorithms. These curves display the change in performance of the heuristics as they make queries. To construct the curves we plotted the AUC achieved by each heuristic on the test set against the size of L after each query is made. The AUC value plotted was the mean over all tests (10 for each fold). AUC was plotted on the y -axis, and the size of the current labeled set was on the x -axis.
Paired-t tests were performed to establish the signifi-cance at which the heuristics differ. Using a paired-t test is valid because AUC is approximately normally distributed when the test set has more than ten positive and ten nega-tive examples [13]. We compared all heuristics pairwise at each query, and determined the maximum confidence level at which the difference between them was significant. We used cutoffs at the 0.60, 0.70, 0.80, 0.90, 0.95, 0.975, 0.99, and 0.995 confidence levels. It is not feasible to report the paired-t results for all experiments, but they will be men-tioned where appropriate. In addition, they are used in one of our summary statistics.

Significance is established between two heuristics by taking the median confidence level at which they differ across all queries. So for example, if algorithm A has an ad-vantage over B significant at the 0.80 level when | L | =20 , an advantage significant at 0.70 when | L | =21 ,anadvan-tage at 0.90 at | L | =22 , no advantage at | L | =23 ,andif B has an advantage significant at the 0.95 level at | L | =24 , then the sorted sequence of significance values for A over B vantage of A over B of 0 . 70 . We used the median because it is insensitive to outliers, and because it requires that a heuristic be significantly better on at least half of the queries for it to be considered significantly better overall.
Because we have done a broad analysis of active learning heuristics, there are a large number of results to report. Re-sults are generally displayed using learning curves, but with so many it is difficult to get a handle on the big picture. To aid in this endeavor we also make use of three summary statistics.

The first statistic we refer to as the ranked performance of the heuristics. With this statistic we establish a ranking over heuristics on a dataset taking the paired-t tests into ac-count. We rank each heuristic according to how many of the other heuristics it is significantly better than, based on the median significance level over all queries. With n heuris-tics the best heuristic will receive a rank of 1 and the worst arankof n . Therefore, if heuristic A performs significantly worse than heuristic B , but is significantly better than all others, it gets a rank of 2 . It is also possible for a heuristic to have a rank range rather than a single value. This occurs when the difference between it and another heuristic is not significant. As an example, if heuristic C is significantly worse than two of the heuristics, and there is no significant difference between C and two other algorithms, then C will receive a rank of 3 X 5. In general, a heuristic A can be con-sidered significantly better than a heuristic B if there is no way for B to be ranked higher than A within the established ranking scheme.

The rank performance statistic is also summarized across all of the datasets by displaying the mean rank and num-ber of wins for each heuristic. An algorithm X  X  mean rank is simply the mean of its lower and upper ranks across all datasets. A win is awarded on each dataset for the heuris-tic that receives a rank of 1. In the case where multiple heuristics have a 1 in their rank range (i.e. there is no sig-nificant difference between them), then partial credit is as-signed to each weighted by the width of its rank range. Let Q = { q 1 ,...,q n } be the set of heuristics that have a 1 in their rank range, and r ( q i ) be the width of the rank range for heuristic q i . The win credit earned W ( q i ) for heuristic q is W ( q i )=(1 / ( r ( q i )  X  1) / n j =1 1 / ( r ( q j
One of the primary aims of active learning is to reduce the amount of training data needed to induce an accurate model. To measure this we define the target AUC as the mean AUC achieved by random sampling for the final 20% of the queries. We then report the minimum number of examples needed by each algor ithm to achieve the target AUC. We also report the data utilization ratio , which is the number of examples needed by each heuristic to reach the target AUC divided by the number needed by random sam-pling. In the event that a heuristic does not reach the target AUC we simply report that the minimum number of exam-ples needed is greater than the size of L after the last query round. This measure reflects how efficiently a heuristic uses the data, but may not reflect large changes in performance in the later query rounds. This metric is similar to one used by Melville et al. [16] and Abe et al. [1]. To summarize over all datasets we also report the median data utilization ratio and number of wins for each heuristic.

Our last summary statistic is the area under the learning curve above Random, which is the difference between the area under the learning curve for a heuristic and that of Ran-dom. A negative value for this statistic indicates that the heuristic on average performed worse than Random. The area under the learning curve for a heuristic is calculated as the sum of the AUC achieved by a heuristic over all query rounds. It is more sensitive to the overall performance of the heuristics throughout the learning process than the previous two statistics. To summarise across all datasets we also re-port the mean area above random achieved by a heuristic as well as the number of wins. 4.2.1 UCI Data In the first set of experiments, we used SMO as the base classifier and 15 examples in the initial labeled set. ES-TAUC performed better than Closest sampling overall. The Random-CS heuristic is also a strong performer, scoring worse than ESTAUC, but better than Closest. RAR and ERS do not perform better than either Closest or Random-CS. Figure 1 shows the learning curves for the Ionosphere dataset. ESTAUC is significantly better than all of the other heuristics from 45 to 94 labeled examples at the 0.6 confi-dence level or greater. Since it X  X  difficult to discern the finer differences between curves in Figure 1, we look to Tables 2 X  4 for the summary statistics. ESTAUC is clearly the winner on the ranked performance metric with a win credit that is more than twice that of its nearest competitor. On the data utilization table, Closest receives the same number of wins as ESTAUC, but ESTAUC does better on all of the datasets that Closest doesn X  X  win. On the area under the learning curve, Random-CS actually achieves a slightly higher mean area than ESTAUC even though ESTAUC has the max area on more datasets. Generally, behind ESTAUC, Closest, and Random-CS, we find ERS with RASP close behind, then Random, then RAR, then RANC. Finally, we note that de-spite the similarities in form between ESTAUC and ERS, the strong differences in perfo rmance between them indi-cate that they are making very different choices of examples to label.

In results not shown, we f ound little change in the rela-tive performance of the algorithms when starting with 100 examples in the initial labeled set rather than 15. Again ES-TAUC was the top performer on all metrics, though it did have more stiff competition from ERS and Closest in this case, mainly due to the fact that there is less room for im-provement when starting with 100 labeled examples.
Overall we see that ESTAUC is the best heuristic that we tested on these datasets. While there is an additional cost associated with ESTAUC due to the need to train multiple SVMs, this can be mitigated via the use of incremental and decremental updates and considering only a filtered subset of
U . Further, even without an incremental update SVM, the amount of time needed to choose an example to label with ESTAUC (as well as ERS), while greater than that needed for Closest, was still quite small (10 X 15 seconds) for most data sets. The only exception was USPS, where ESTAUC and ERS each took about one minute to choose an unlabeled example due to the large number of attributes, while Closest was nearly instantaneous. However, the bulk of this additional time was spent training SMO, so an incre-mental SVM would mitigate this significantly. Further, even one minute is not at all large relative to the amount of time it takes an oracle to label the example: Consider for instance how long it would take for a human labeler to classify a web page or a biological sequence. 4.2.2 Synthetic Data Given how ERS, ESTAUC, and RAR are defined with re-spect to probability estimates, we wanted to see how well they would do if given perfect probability estimates. To do this we developed two synthetic datasets. The synthetic data was created from two overlapping ten-dimensional Gaus-sian distributions: N + (  X  + , X  2 I 10 ) and N  X  (  X   X  , X  2 I where  X  2 =12 . 5 and I 10 is the 10  X  10 identity matrix. For the first dataset ( Gaussian 0,5 ),  X  + =(0 ,..., 0) and  X  (5 ,..., 5) , and for the second dataset ( Gaussian 0,10 ),  X  + =(0 ,..., 0) and  X   X  =(10 ,..., 10) . The classifica-tion of each example generated by N + is positive and that of those generated by N  X  is negative. We then ran ESTAUC and RAR with the true Gaussian pdfs P in place of the es-timates  X  P . We also ran ERS with the true pdf in place of the first  X  P ( y | x ) term (recall that the second  X  P term is the classifier itself, so we did not change it).

Without exception, ESTAUC and RAR dominate the other heuristics on the Gaussian data when using either 15 (shown) or 100 (omitted) initial labeled examples. Figure 2 shows the performance of the active learning heuristics for a representative experiment. Interestingly, ERS did not gain a similar benefit from having perfect probability estimates. The summary statistics for these experiments can be found in Tables 5 X 7.

These results clearly demonstrate the very strong poten-tial of ESTAUC and RAR if given good probability esti-mates. However, any probability model that we might gen-erate will necessarily be based on the labeled training data. In active learning this is generally a relatively small set of data. Obviously, it is difficult to generate high quality prob-ability estimates from such a small training set.
Area under the ROC curve is an important measure of learning performance. We studi ed how active learning af-fects AUC, including studying some algorithms from the literature and introducing four new algorithms for active learning designed to maximize AUC. We evaluated all these algorithms using SMO as the base classifier. Overall, we found that ESTAUC was the top performer. Further, there is strong evidence that if good pr obability estimates are avail-able, then ESTAUC and RAR will perform very well.
We are currently experimenting with a na  X   X ve Bayes base classifier in place of SMO. Future work includes extending this work to multiclass problems and to study the minimiza-tion of the lower envelopes of cost curves [8], an alternative to ROC curves.

The authors thank Nick Roy for his helpful discussions and the reviewers for their useful comments. This work was funded in part by NSF grant CCR-0092761. This work was completed in part using the Research Computing Facility at the University of Nebraska.
 labeled points.
 ing examples needed to achieve the target AUC.
 when starting with 15 labeled points.
 15 labeled points.
 of training examples needed to achieve the target AUC. data when starting with 15 labeled points.

