 Real applications in many domains such as pattern recognition, computer vision and data mining often lead to very high dimensional data. Dimensionality Reduction (DR) is an effective and widely used approach to deal with such high dimensional data, due to its potential of mitigating the so-called  X  X urse of dimensionality X . Up to now, many di-mensionality reduction methods (DRs) have b een developed, such as Principal Compo-nent Analysis (PCA) [1], Isomap [2], Lo cally Linear Embedding (LLE) [3], Laplacian Eigenmap (LE) [4] and supervised Linear Discriminant Analysis (LDA) [5].

Although existing unsupervised DRs have different motivations and concrete imple-mentations, they obtain the desired low dime nsional coordinates by preserving a certain property of original data, whether global or local. For example, property that PCA pre-serves is the global variance and Isomap aims to preserve the geometric distance on the intrinsic manifold. While some other approaches try to preserve a certain local property of data, such as locality that LE tries to p reserve and neighborhood relationship that LLE aims to preserve.

However, cluster structure as a key property of data, which reflects intrinsic distri-bution of data and plays a crucial role in further analyzing and utilizing data [6], has been ignored by these popular DRs. Here the term  X  X luster structure X  means the natural aggregation of similar objects and natural separation of dissimilar objects in concept level. Hence, it is appealing to devise DR method for clustering that can preserve or enhance cluster structure of the original high dimensional data in the transformed low dimensional embedded space.
 directly on high dimensional data is still a challenging problem. A natural solution is to transform data into a low dimensional compact space through aforementioned DRs such as PCA before clustering. However, due to the intrinsic gap between clustering and existing DRs, which are not designed originally for clustering, clustering structure of original data can X  X  be well preserved and may be even destroyed in the transformed low dimensional space.

Therefore, in this paper, a novel DR approach for clustering is proposed. In the pro-posed method, both nonlinear DR and clustering task can be achieved simultaneously. Firstly, according to graph embedding theo ry [7], a weighted graph which can charac-terize manifold structure of the whole data is constructed. Then the desired low dimen-sional coordinates are represented as linear combinations of smooth functions defined on data graph, which are the eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix of data graph. The key idea behind this is that the learned low dimensional coordinates should be as smooth as possible with respect to the data man-ifold. Finally optimal combination coeffici ents and cluster assignment matrix can be computed by maximizing between -cluster scatter and minimizing within-cluster scat-ter simultaneously as well as preserving smoothness of cluster assignment matrix with respect to data manifold. constructed, where the ( i, j ) entry w ij of affinity matrix W measures similarity be-Gaussian similarity as Ref. [8] is adopted.

According to [9], smoothness of a function f : X X  R on graph G can be measured by: The smaller  X  ( f ) is, the smoother f is. This measure penalizes large changes over data points that have large similarities. It is the same as manifold assumption that nearby points are likely to have the same labels.

Let L = D  X  W be the Laplacian matrix of graph G ,where D is diagonal matrix with element d ii = j w ij . Then the smoothness of function f defined in Eq. (1) can be also expressed as: G is identical to an n -dimensional vector f . Thus one can also regard an n -dimensional vector as a function on graph G .
 be the pair of eigenvalue and eigenvector of L ,where 0  X   X  1  X   X  2  X  X  X  X  X  X   X  n and v i has been normalized to 1. Then smoothness of v i is: This means that eigenvalue  X  i measures smoothness of eigenvector v i : the smaller  X  i is, the smoother v i is.

From definition of L , one can see that L is positive semidefinite. Thus these n eigen-vectors of L are mutual orthogonal and form the orthogonal basis of function space on graph G . Then for any function f on G , it can be expressed as: Note that  X  1 =0 and v 1 = e/ So v 1 is a trivial function on graph G . Then we rewrite Eq. (4) as: where m n .The T 0 , T 1 and T 2 are called the trivialness component, smoothness component and noise component of function f , respectively. Discarding noise compo-component T 0 is just to translate function and has no essential effect. Hence represent-ing a function as T 1 term in fact imposes smoothness assumption on function. This regularization technique is called spectral regularization [10]. Here we also introduce larization proposed in Ref. [10]. The convenience of this variation will be given in the following section. 3.1 Problem Formulation and Its Solution 1 , 2 ,...,K ) in DR procedure. Denote the cluster assignment matrix by P =( p ij )  X  B  X  K where p matrix  X  P =( X  p ij ) ,where  X  p ij = p ij / C where I K is the K  X  K identity matrix.
 i th column vector is the new low dimensional representation of sample x i . According dimensional space, for each vector f i in F , we would like it to be a smooth function onthedatagraph G . This implies that we can only use the smoothness component T 1 function of graph G : where V m =( v 2 ,v 3 ,...,v m +1 ) .Then F =( V m A ) T where A =( a 1 ,a 2 ,...,a d ) .
Since v T i e =0( i =2 , 3 ,...,n ) ,wehave Fe =( V m A ) T e =0 , which implies that in new low dimensional space, the samples ar e automatically centered at origin. This property can significantly simplify computation of total scatter matrix S t and between-cluster scatter matrix S b in the transformed space, which can be expressed as: As stated in section 1, besides obtaining low dimensional embedding, the main goal of our method is to preserve or even enhance cluster structure of data in low dimen-sional space. This is implemen ted by maximizing between-cluster scatter and minimiz-ing within-cluster scatter simultaneously: To make the problem well defined, we impose orthogonal constraint A T A = I d on combination coefficient A . Hence the above trace ratio problem can be reduced to the following constrained trace maximization problem: Another consideration for cluster assignment matrix is that points with high similarities are much likely to have the same cluster label. Mathematically, it is equal to minimize the following regularization term on  X  P : Combining the objective in Eqs. (8) and (9), then we can derive the final optimization objective for the proposed method: where  X &gt; 0 is a trade-off parameter to balance the two terms. Since the discrete con-straint of  X  p ij makes the problem an NP-hard problem, we will relax it to be continuous value as many spectral clustering algorithms do [11].

As one can see, the objective in Eq. (10) depends on both A and  X  P , which depend on each other. We cannot give a closed-form solution. Here we optimize them in an iterative manner. In other words, we will optimize the objective with respect to one variable while fixing the other one. This procedure repeats until convergence.
For a fixed  X  P , optimal combination coefficients A can be obtained by solving the following trace maximization problem: 1 , 2 ...,d ) is the i th largest eigenvector of matrix V T m  X  P  X  P T V m .
Similarly, for a fixed A , optimal relaxed assignment matrix  X  P can be computed by maximizing the following: max After obtaining continuous optimal scaled cluster assignment matrix, we can use K -means or spectral rotation to obtain dis crete cluster assignment matrix [12].
We refer to the proposed method as Nonlinear Discriminative Embedding for Clus-tering via Spectral Regularization (NDECSR), whose algorithm procedure are summa-rized in the following: 3.2 Convergence Analysis In this subsection, we will study the convergence of our NDECSR algorithm. Denote the objective function value in each iteration by then we have Theorem 1.
 Theorem 1. The objective function J (  X  P q ,A q ) satisfies the following inequality: Proof. According to our NDECSR algorithm,  X  P q +1 and A q +1 satisfy:
A  X 
P Then we have: J (  X  P q ,A q )  X  J (  X  P q ,A q +1 )  X  J (  X  P q +1 ,A q +1 ) . Theorem 1 means that the objective function monotonously increases as the iteration number q . Another fact is that the objective function has upper bound under constraints in (10). Therefore objective function will converge in limited iterations. Then we can further obtain the following Theorem which gives the convergency of A q and  X  P q : Theorem 2. The matrix sequence { (  X  P q ,A q ) } obtained by NDECSR is convergent. Proof. From Theorem 1, we can assume there exist Q iterations when the objective function converges. Then we have: Substituting (16) into (20) and by using (18) and (19), we can obtain that: =  X  X  X  orthogonal matrix O such that: A =  X  X  X  orthogonal matrix O such that:  X  P Hence A Q +1 and A Q have the same column space. So do  X  P Q +1 and  X  P Q . Note that in step 3(2) and 3(4) of our NDECSR algorithm, we have reshaped A q and  X  P q for orthogonal transformation invariance. Thus A Q +1 = A Q and  X  P Q +1 =  X  P Q . This means that when the objective function converg ences, the obtained coefficient matrix A q and relaxed assignment matrix  X  P q also converge. In this section, the clustering performance of NDECSR is compared with K -means clustering (KM), Normalized Cut (NCut) [13], Discriminative K-means (DKM) [14] and Spectral Embedded Clustering (SEC) [8 ] via systematic experiments. Moreover, the visualization results of NDECSR are also compared with two popular manifold learning algorithms: LLE [3] and LE [4]. 4.1 Data Sets Experiments are conducted on three categories of data sets, which are popular bench-mark data sets covering a wide range of applications.
 UCI data: Three UCI data set Ionosphere, Iris and Wine are used in our experiments. Image data: We perform experiments on four image data sets: ORL face data set 1 , which are the representatives of four different image recognition problems. For ORL face data, the images are resized to 32  X  32. For MNIST data set we only select five digits  X 1 X , X 2 X , X 3 X , X 4 X  and  X 5 X  and 300 images per digit. The Coil-20 data set contains 20 objects and each object has 72 images with size 32  X  32. In the original scene category forest, high way and tall building. We use the feature called Spatial Envelope [16] to represent each scene image, the f eature is a 960-dime nsional vector. TDT2 4 . For 20newsgroup data set, we random choose 2380 documents from the topic rec which contains autos, motorcycles, baseball and hockey, and delete the words that occur less than 10 times in all these 2380 documents. The final TF-IDF representa-tion of each document is a 4477-dimensional v ector. For TDT2 data set, we select all documents from categories 20013, 20070, 20044 and 20076, which are the categories that contain 4th, 5th, 6th and 7th most documents, respectively. They are total 1931 documents and we also delete the words tha t occur less than 5 times, the final TF-IDF representation of each documen t is a 7906-dimensional vector.

Table 1 summarizes the details of data sets used in our experiments. For the random selected subset data, we conduct 10 random experiments to obtain the average results. 4.2 Clustering Evaluation and Parameter Selection Two standard clustering evaluation metrics Clustering Accuracy (ACC) and Normalized Mutual Information (NMI) are used in our experiments to evaluate the performance of the involved clustering algorithms.
Since many clustering algorithms have one or more parameters to be tuned, in order to compare fairly, we run these algorithms unde r different parameters, and report their best average results. For all clustering algorithms, the number of clusters is set to be equal to the true number of classes for all data sets.

For all graph-based algorithms, such as Normalized Cut, SEC and our NDECSR method, the width parameter  X  in Gaussian similarity is automatically selected by self-tuning spectral clustering method [17] and neighborhood size k is tuned from { 5, 10, 15, 20, 50, 100 } .

For our NDECSR method, we set m =min(#Dim , 15) , where #Dim is the dimen-sionality of original data. The dimensionality of the desired low dimensional space d is selected from { 3, 5, 10, 15 } and no greater than m for all data sets. The trade-off parameter  X  is tuned from { 10  X  6 , 10  X  3 , 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 10 3 } .
For SEC, there are two trade-off parameters  X  and  X  .AsthewayinSEC[8],weset  X  =1 , and tune  X  from { 10  X  10 , 10  X  7 , 10  X  4 , 10  X  1 ,1, 10 2 , 10 5 , 10 8 } .
Given the constructed affinity graph and the number of clusters, there is no other pa-rameter in Normalized Cut. DisKmeans needs t o determine a regularization parameter  X  . We also tune it from { 10  X  10 , 10  X  7 , 10  X  4 , 10  X  1 ,1, 10 2 , 10 5 , 10 8 } .
For all methods that produce the continuous assignment matrix, we obtain the final cluster results using the spectral rotation method introduced in [12]. 4.3 Clustering Performance The clustering performance of involved al gorithms are compared in Table 2 and Table 3, in both of which the best results are highli ghted in boldface. From these comparisons, one can see that our NDECSR method outperforms KM, NCut, DKM and SEC in most cases. Especially on Ionosphere and MNIST 12345 data set, our NDECSR method can significantly improve the clustering performance of the other methods. This demon-strates that learning both smooth low dimensional coordinates and smooth cluster ma-trix with respect to the data manifold can improve the clustering performance. 4.4 Clustering Performance vs. Parameters There are three parameters that should be specified manually in our NDECSR algo-rithm. We have also performed extensive experiments to evaluate the sensitivity of clus-tering performance of NDECSR to these parameters. Due to space limitations, we only plot the ACC and NMI versus the parameters on wine and MNIST 12345 data set in will result in poor performance. Fortunately, for all data sets, we find that under the parameter setting  X  =0 . 001 , k =10 , d =10 , our NDECSR method can always obtain good performance. So we can use this param eter setting for simplicity in practice. 4.5 Comparison on the Embeddings of original high dimensional data. While the traditional clustering algorithms such as NCut, DKM and SEC have no such an ability. They can either obtain the linear em-bedding or a fixed dimensionality embedding. Here we compare the 3D-embedding obtained by our method with two popular manifold learning algorithms: LLE [3] and LE [4]. The 3D embeddings of three algorithms on MNIST 12345 and Scene data set are shown in Fig. 3 and 4. As can be seen, our NDECSR method can preserve the cluster structure better than LLE and LE.
 Since traditional dimensionality reduction are developed originally for classification or recovering the geometric structure of data (known as manifold), there exists intrinsic separation between existing DRs and clustering. On the other hand, existing clustering algorithms still have some problems when directly working on high dimensional data. Therefore, in this paper, a novel dimensionality reduction method for clustering called NDECSR is proposed. The key property of the proposed method is that the nonlinear dimensional reduction and clustering task can be achieved simultaneously in a unified framework. To our best knowledge, this is the first method that can yield both nonlinear embedding and cluster result in a unified framework in the literature. Extensive exper-iments on three data sets from UCI machin e learning repository and real world image and text document data sets demonstrate its ef fectiveness as a cluster algorithm as well as its effectiveness as a nonlinear dimensionality reduction method.
