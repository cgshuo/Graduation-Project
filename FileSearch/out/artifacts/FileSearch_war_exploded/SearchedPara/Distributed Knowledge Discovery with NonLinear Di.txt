 During the last decade, the evolution of the internet as well as the emergence of novel applications, such as peer-to-peer (P2P) systems, has led to an unprece-dented information explosion. Information is distributed among network nodes, making the cost of centralizing and processing data prohibitive. Consequently, distributed data mining (DDM) has emerged as a highly challenging task.
Dimensionality reduction (DR) is an important step of data mining as high dimensional data lead to the degradation of query processing performance, a phenomenon known as the curse of dimensionality [8]. Thus typical tasks, such as clustering or classification, become in effective. DR is then required in order to decrease the number of dimensions and reveal potentially interesting structures in data. With the advent of DDM, distributed dimensionality reduction (DDR) has emerged as a necessity in many applications.

A prominent such application is knowledge discovery from text collections dis-tributed in a P2P network. Latest theore tical and experimental evidence point out that documents lay on a non linear high dimensional manifold ([5],[3]). Con-sequently, non linear dimensionality reduction (NLDR) is necessary in order to recover the low dimensional structure. Although numerous DDR algorithms have been proposed, all assume that data la y on a linear space. Thus there is the need for definition of distributed NLDR techniques.

To this end, we introduce Distributed Isomap (D-Isomap). D-Isomap corre-sponds to the decentralized version of the well known NLDR algorithm Isomap [18]. D-Isomap has been specifically de signedandtunedinordertobeappli-cable in large scale, struct ured P2P networks. We evaluate its performance and assess its viability and suitability for dist ributed environments through extensive experiments on artificial and real world datasets.

The contribution of this work is manifold. In section 2, we provide a review of the Isomap and DDR families of algorithms. In section 3 we introduce D-Isomap, a distributed NLDR algorithm which to the best of our knowledge is the first of its genre. Furthermore, we provide a cost model that assesses the computational and network resources required for the embedding of a dataset in a low dimensional space with D-Isomap. Finally, in section 4, we demonstrate the non linear nature of our approach through extensive experiments on well known non linear manifolds and justify its applicability in mining document collections distributed in P2P networks. DR algorithms are usually classified with respect to the way they manage data ([16]). Linear algorithms assume that high dimensional data lay on a linear or approximately linear manifold of significantly lower dimensionality. On the other hand, non linear methods assume that such linearity do es not exist and operate on small fractions of the high dimensional manifold that can be perceived as locally linear. Due to space limitations, in the remaining of this section, we focus on the Isomap algorithm and its variations while in the end we provide a short overview of prominent DDR methods and motivate the need for a distributed NLDR approach.
Isomap [18] is a non linear technique that operates on points X  pairwise geodesic distances. Isomap first constructs a nearest neighbor (NN) graph, where each point is represented as a node having edges to its k NN points. Edges are weighted according to the Euclidean distance of the points connecting. Global pairwise distances are calculated based on the shortest paths between all points (geodesic distances). The low dimensional mapping is derived by applying classic metric multidimensional scaling [19](MDS) on the geodesic distance matrix.

Isomap deficiencies to deal with curved manifolds or project large datasets gave rise to extensions such as C-Isomap and L-Isomap [16]. C-Isomap employs a different edge weighting scheme taking into account the mean distance of each point from its NNs. L-Isomap on the other hand attempts to address the ex-cessive memory requirements of MDS by introducing Landmark MDS (LMDS). LMDS applies MDS on a set of sampled points and uses triangulation for the projection of the remaining dataset. Another problem of Isomap is the defini-tion of non connected NN graphs. In such cases the algorithm operates on the largest connected component and discar ds the rest. A solution is provided by Incremental Isomap [20] (I-Isomap) which guarantees the construction of a fully connected graph and is able to update the embedding when data is inserted or deleted.

DDR algorithms assume data distributed across a set of nodes and the ex-istence of some kind of network organizat ion scheme. The simplest case, where organization exists by construction, are structured P2P networks. In such net-works, a protocol (usually based on distributed hast tables -DHT) ensures that any peer can efficiently route a search t o a peer that has a specific file. Examples include Chord [17] and CAN [15]. In unstructured networks, the organization may be induced by means of physical topology (i.e. a router) or by means of self-organization [11]. In both cases however, a node undertakes all computa-tions that have to be done centrally. The most prominent approaches in the area are adaptations of PCA ( [9], [13], [14]). Two distributed alternatives of Fastmap [1] have also been proposed, but their application relies heavily on the synchronization of the network elements thus can only be applied in control-lable laboratory environments. Recen tly, K-Landmarks [1 1] has appeared as a promising solution for DDR in unstructured P2P networks.

Unfortunately, all these methods are linear, in the sense that they assume that data lay on a linear or approximately linear low dimensional manifold. However, latest results point out that data usually lay on a non linear manifold ( [5], [3]) thus linear methods fail to provide adequate results. Consequently, there is an apparent need for decentralized NLDR tec hniques. To the best of our knowledge, D-Isomap is the first attempt towards this direction. D-Isomap capitalizes on the basic steps of Isomap and applies them in a net-work context, managing to successfully retrieve the underlying manifold while exhibiting tolerable network cost and computational complexity. In the rest of this section we present in details each step of the algorithm and review the cost induced by its application in a structured P2P network. Throughout the analysis we assume that N points, residing in R d , are distributed in a P2P network of M peers. Each peer stores N i points ( M i =1 N i = N ). The objective is to recover the manifold residing in R n using a graph defined by the k NNs of each point. 3.1 Data Indexing and Nearest Neighbours Retrieval The first step of Isomap necessitates the definition of a kNN graph for each point. The latter, although applied in a distributed environment, should yield results of accuracy approximating that of a centralized approach. This, in conjunction with our initial goal for low network cost, highlights the need for a structured, DHT based, P2P network like Chord. Chord is a P2P lookup protocol where peer identifiers are arranged in a circl e. Each node has a successor and a pre-decessor. The successor of a peer is the next node in the identifier circle when moving clockwise. On the other hand, the predecessor, is the next peer in the identifier circle when moving counter-clockwise. A message in Chord may require to traverse O ( logM ) hops before reaching its destination.

In order to enable rapid lookup of points similar to each peer X  X  local data we consider locality sensitive hashing [2] (LSH) that hashes similar points to the same bucket with high probability. LSH defines L hash tables, each related with a mapping function g i ,i =1 ...L .Each g i is defined by f hash functions h () ,j =1 ...f , randomly chosen from the same family of LSH functions H . Every h i,j () maximizes the collision probability for data points that are close to each other in the original space. Since we measure similarity based on the euclidean distance, we use h r,b ( x )= rx + b w ,where x is the point under process, r is a random vector which coordinates are defined by the Gaussian distribution and w , b random numbers with the property b  X  [0 ,w ).

The mapping of hash identifiers to peer identifiers is accomplished by em-ploying a similarity preserving transformation that depicts a vector from R f in R 1 [6]. For a given vector x , LSH produces an f -dimensional vector; the l 1 norm of this vector defines a similarity preserving mapping to R 1 . Additionally, it can be proved that the obtained l 1 values are generated from the normal distribution N
The simplest way to retrieve the kNNs of a point p is to aggregate from all hash tables the points hashed in the same bucket as p . Afterwards, retrieve the actual points, calculate their distances from p and retain the kNNs. In order to reduce the required messages we halt the procedure as soon as ck points have been retrieved (in our experiments we set c = 5). Additionally, for each point, we define a range bound p that enables a queried peer to return only a subset of the points that indexes using Theorem 1. We use as bound the mean distance that a point exhibits from the points of its local dataset.
 Theorem 1. Given f hash functions h i = r i x T + b i w where r i is an 1 xn random vector, w  X  N , b i  X  [0 ,w ) ,i =1 ...f , the difference  X  of the l 1 norms of the projections x f , y f of two points x , y  X  R n is upper bounded by f i =1 | r i | x  X  y w where x  X  y is the points X  euclidean distace.
 Proof: Since | a | X  X  b | X | a  X  b | X | a + b | X | a | + | b | we derive l 1 ( x f )  X  | y | since w and | r i | are positive. In parallel, for any two vectors a, b we know that ab T  X  a b .Consequently,  X   X  1 w A | x  X  y | T  X  1 w A x  X  y . Based on the latter we obtain  X   X  A x  X  y w The first part of the procedure is presented in Algorithm 1. At first, Each peer, hashes its local points and transmits the derived l 1 values to the corresponding peers. This procedure yields a network cost of O ( N i L ) messages per peer or a total of O ( NL ) messages. The process of recovering the kNNs of a point requires ck messages thus is upper bounded by O ( ckN ). Time requirements on peer level Algorithm 1. Data indexing and kNN retrieval are O ( N i Lf + N i klogk ) induced by the hashing and ranking procedure. Finally memory requirements are O ( N i k ), due to retaining the NNs of each point. 3.2 Distributed Geodesic Distances Definition Each point p has successfully recovered the lo cation of its kNNs and created the corresponding graph G p . Now, each peer should identi fy the shortest paths (SP) from its local points to all points in the dataset using only local information ( j =1 G j ). For this, we will use distributed SP computation techniques, exten-sively used for network routing purposes. A well known algorithm is the Distance Vector Routing (DVR) or Distributed Bellman-Ford (DBF) which is core part of many internet routing protocols such as RIP, BGP, ISO and IDRP [10].
For every p oint p , its host peer maintains a distance vector DIST [ p ]that stores the distances from p to all points of the dataset. Initially, only the cells corresponding to p  X  X  NNs are populated while the rest are set to  X  . The proce-dure is initiated by sending the DIST [ p ] to all peers maintaining p  X  X  NNs. Each receiver evaluates its current SPs to points appearing in DIST [ p ]andifanew SP is identified updates distance vector DIST [ q ]-where q is a point in p  X  X  set of NNs-and goes back to the sending step. The process is repeated until no update takes place, thus SPs h ave been computed.

The algorithm is asynchronous and does not have an explicit termination criterion. However it is self-terminating ( [10]) since message transmissions will halt as soon as no updates take place. Consequently, each peer, after waiting time t to receive a message considers the pro cess finalized. In order to guarantee the Algorithm 2. Definition of geodesic distances creation of a connected SP graph we substitute in the end all remaining  X  values with five times the largest local geodesic distance. Based on the description, the algorithm is presented in Algorithm 2.

DBF resembles a progressive range search where each point p learns in loop i distance information from points that are i edges away in the graph. There-fore, DBF execution requires O ( kDN 2 ) messages, where D is the diameter (the longest path) of the network. In our case, D depicts the number of peers that maintain parts of a single shortest path (from any point p to any point q ), thus D = M and the network cost is upper bounded by O ( kMN 2 ). Although the latter is relatively large, efficient implementation of the procedure can signif-icantly reduce the total overhead. This can be accomplished by transmitting only updated SP information in the form ( p source ,p destination ,dist ). Memory re-quirements are low, O ( N i N ) due to retaining the local points X  distances in main memory throughout computations. Finally, time requirements are O ( M ). 3.3 Approximating the Multidimensional Scaling At this point, each peer has retrieved th e SP distances of its own points to the rest of the dataset. The final step is to apply eigendecomposition on the global distance matrix, which is essentially the MDS step of Isomap. Although several methods for parallel computation of this procedure exist (i.e. [7]), they exhibit excessive network requirements, making their application infeasible. An approach that yields zero messages yet rather satisf actory results is the approximation of the global dataset at peer level with landmark based DR techniques. Instead of trying to map all data simultaneously to the new space, landmark-based DR algorithms use a small fraction of points and project them in the new space. Based on the assumption that these points remain fixed (landmarks in the new space), the rest of the dataset is projected using distance preservation techniques. Algorithm 3. Distributed Isomap
Two approaches directly applicable in our case are LMDS [16] and FEDRA [12]. LMDS operates by selecting a set of a landmark points, with the constraint a&gt;n and projects them in the new space with the use of MDS. Afterwards, a distance-based triangulation procedure, which uses as input distances to already embedded landmark points, determines the projection of the remaining points. FEDRA behaves similarly to LMDS however selects exactly n landmarks. LMDS requires O ( naN i + a 3 )timeand O ( aN i ) space while FEDRA requires O ( nN i ) and O ( n 2 ) respectively. The salient characteristic of this step is that by using as landmarks the local points of a peer we manage to kill two birds with one stone. On one hand we embed the local dataset in R n while simultaneously each peer derives an approximation of the global dataset. Consequently, each node is able to access global kno wledge locally.

A potential failure may appear if the landmarks in a peer are not sufficient for the embedding to take place (i.e. for LMDS a&lt;n ). In such case, a network wide landmark selection process can be applie d. The simplest way, is to assign a peer with the role of aggregator and then all peers transmit at least n M local points. Landmarks are randomly selected from the accumulated points and transmitted back to all nodes thus inducing O ( nN M ) network load. Based on the previous analysis we derive D-Isomap and present it in Algorithm 3. The application of D-Isomap requires O ( N i Lf + N i klogk )timeand O ( n 2 + N i ( N + k )) space per peer and a total of O ( NL + kMN 2 ) messages from all peers.

A final issue is related to the addition or deletion of data. Upon the arrival of a point, we apply Algorithm 1 and derive its kNNs. Afterwards, the SPs can be easily obtained using the fact that given a set of nodes in a graph, i.e shortest path from s to e is the one minimizing the overall distance. Therefore, we relay on the retrieved k -NNs and calculate the SPs of the new point from all local landmarks. Finally, we obtain its embedding through LMDS or FEDRA. The procedure requires O ( ck ) messages. The case of deletion is much simpler, since the host node will only transmit message ( point id ,del ) and force the deletion of the point from the bucket of the indexing peer. On the other hand, the arrival or departure of peers is handled by the Chord protocol itself. In this section we present the experimental evaluation of D-Isomap, which indeed verifies the expected performance and promotes it as an attractive solution for hard DDR and DDM problems. We carried out two types of experiments. First, we compared the manifold unfolding capability of D-Isomap in a distributed context against Isomap and L-Isomap in various network settings. In the second set of experiments, we evaluated D-Isomap X  X  performance against Isomap, L-Isomap and LSI [4] in numerous supervised and usupervised DDM experiments using a medium sized text collection. The obtained results prove the suitability and viability of our algorithm for DDM problems, where each node holds a subset of the available information.

In the first set of experiments we employed three 3 D non linear manifolds, namely the Swiss Roll, Helix and 3D Clusters each consisting of 3000 points (Figures 1(a), 1(b), 1(c)). In the case of the Swiss Roll an NLDR algorithm should unfold the roll into a parallelogram while in the case of Helix it should extract a circle. Concerning the 3D Clust ers, we target in retaining the cluster structure in the new space. Each dataset was randomly distributed in a network of M peers ( M =10 , 15 , 20 , 25 and 30). Depending on the dataset, we varied the value of k ; for the Swiss Roll we set k = 8 and progressively augmented it by 2 until 14. For the 3D Clusters we started from k = 6 and reached 12 using the same step. For Helix we ranged k from2to6withastepof1.Inallexperiments we set c =5, L = 10, f =10and w = 16.

We assess the quality of D-Isomap by comparing the produced manifold (on peer level) against those produced centrally by Isomap and L-Isomap. For L-Isomap we set a = 300 in all experiments. In the subsequent graphs, D F -Isomap or D F indentifies D-Isomap configured with FEDRA and D L -Isomap or D L , D-Isomap deployed with LMDS. We used MATLAB R2008a for the implemen-tation of the algorithms and E2LSH [2] for LSH. Due to space limitations, we report only a subset of the experiments 1 .

In Figure 2 we present the required number of messages for the projection of each dataset with D-Isomap as a fraction of the worst case network cost ( WorstCaseBound ) as derived by Section 3.3. First we validated the bound of Theorem 1 with the Swiss Roll. The results (Figures 2(a), 2(b)) indicate a re-duction in the number of messages; consequently we employed the bounded version of the algorithm for all experiments. Figures 2(b), 2(c) and 2(d) provide the network load for the projection of each dataset with D-Isomap. The results highlight that D-Isomap behaves better in terms of network cost as the network size grows. The reason is simple; as th e network grows, the buckets retained by each peer are smaller, therefore the messages are reduced. Moreover, mes-sages are not affected seriously by changes in k so we observe a reduction in the percentage as k grows larger.

Figures 3(a), 3(b) depict the results obtained from Isomap and L-Isomap when applied on Swiss Roll for k = 8. Both algorithms have successfully revealed the underlying manifold. D L -Isomap also recovered the co rrect 2D structure (Figure 3(c)) without being affected by the limited size of local data (only 3.3% of the global dataset). We report only one case of failure, for M =30and k =14where the embedding was skewed due to inappropriate selection of NNs. D F -Isomap produces acceptable results howe ver of lower quality compared to D L -Isomap (Figure 3(d)). This is due to the fact that FEDRA operates using only 2 points while LMDS employs the whole local dataset at each node.

Similar quality results were obtained from D L -Isomap during the evaluation of Helix. Our algorithm managed to recover the circle structure of Helix (Figures 3(g), 3(h)) providing results comparable to L-Isomap (Figure 3(f)) and Isomap (Figure 3(e)). The inability of D F -Isomap to work with a limited number of landmark points was more evident this time, producing an arc instead of a circle. The effectiveness of D-Isomap was proved when it was applied on the 3D Clusters dataset. Unlike Isomap and L-Isomap that failed to produce a connected graph (Figures 3(i), 3(j)), D L -Isomap successfully managed to replicate the cluster structure in the new space (Figures 3(k)-3(l)) since by construction produces connected graphs. Again, D F -Isomap failed to recover the whole cluster structure and preserved only three out of five clusters.

The results obtained from 3D Clusters inspired the application of D-Isomap on a DDM problem. As evaluation dataset, we used the titles of all papers pub-lished in ECDL, ECML/PKDD, FOCS, KDD, SIGMOD, SODA and VLDB conferences between 2006 and 2008 2 . The dataset consists of 2167 papers, rep-resented as 4726-dimensional vectors using a TF-IDF populated vector space model [4]. We randomly distributed the dataset among M peers ( M = 10, 15, 20, 25 and 30) and embedded it in 10, 15, 20, 25 and 30 dimensions. We used the same values for L , f , a , c and w as before and ranged k from8to14witha step of 2. The embedded datasets from each peer were used as input for clas-sification and clustering. We employed F -measure ( [4]) in order to assess the quality of the results. In all experiments we report the relative quality amelio-in the low dimensional dataset over the F -measure ( F m,orig ) obtained in the original case.
 D F -Isomap and D L -Isomap were compared against Isomap, L-Isomap and LSI. For the central algorithms, reported values correspond to the mean of 10 executions. All results have been validat ed with a 10-fold cross validation. For D-Isomap we applied the same methodology on each peer level and report the mean value obtained across nodes. The statistical significance of D-Isomap X  X  results has been verified by a t-test with confidence level 0 . 99. We employed k-Means and k-NN [4] for clustering and c lassification respectively; for k-NN we set kNN = 8 in all experiments. Although this may not be optimal, it does not affect our results, since we report the r elative performan ce of the classifier.
Table 1(a) provides the clustering results obtained using k = 8 for the defini-tion of the NNs for D-Isomap, Isomap and L-Isomap. The results highlight the applicability of D-Isomap in DDM problems as well as the non linear nature of text corpuses. Both flavours of our algorithm produce results marginally equal and sometimes superior to central LSI. The low performance of Isomap and L-Isomap should be attributed to the definition of non-connected NN graphs. Table 1(b) provides the classification results obtained for the same value of k . D-Isomap is outperformed only by central LSI while in cases ameliorates the quality of k-NN. The latter comprises an experimental validation of the curse of dimensionality. The network load induced by D-Isomap in this experiment is provided in 2(e). In this paper we have presented D-Isomap, a novel distributed NLDR algorithm which to the best of our knowledge is the first attempt towards this direction. We presented in details each step of the p rocedure and assessed the requirements posed to each network node by its application. Through extensive experiments we validated the capability of D-Isomap to recover linear manifolds from highly non linear structures. Additionally, we highlighted its applicability in DKD problems through experiments on a real world text dataset. The high quality results inspire us to pursue the extension D-Isomap towards P2P document retrieval and web searching.

