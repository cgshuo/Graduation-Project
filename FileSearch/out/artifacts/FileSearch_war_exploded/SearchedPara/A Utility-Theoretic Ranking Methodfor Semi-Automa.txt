 In Semi-Automated Text Classification (SATC) an automatic classifier  X   X  labels a set of unlabelled documents D , follow-ing which a human annotator inspects (and corrects when appropriate) the labels attributed by  X   X  to a subset D 0 with the aim of improving the overall quality of the labelling.
An automated system can support this process by ranking the automatically labelled documents in a way that maxi-mizes the expected increase in effectiveness that derives from inspecting D 0 . An obvious strategy is to rank D so that the documents that  X   X  has classified with the lowest confidence are top-ranked. In this work we show that this strategy is suboptimal. We develop a new utility-theoretic ranking method based on the notion of inspection gain , defined as the improvement in classification effectiveness that would derive by inspecting and correcting a given automatically labelled document. We also propose a new effectiveness measure for SATC-oriented ranking methods, based on the expected reduction in classification error brought about by partially inspecting a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the pro-posed measure, our ranking method can achieve substan-tially higher expected reductions in classification error. I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Infor-mation filtering; Search process ; I.2.7 [ Artificial Intelli-gence ]: Natural Language Processing X  Text analysis Text classification, supervised learning, semi-automated text classification, cost-sensitive learning, ranking  X 
In order to correctly interpret the plots in Figures 1 and 2, this document should be printed in colour.
 Suppose an organization needs to classify a set D of textual documents under classification scheme C , and suppose that D is too large to be classified manually, so that resorting to some form of automated text classification (TC) is the only viable option. Suppose also that the organization has strict accuracy standards, so that the level of effectiveness obtain-able via state-of-the-art TC technology is not sufficient. In this case, the most plausible strategy to follow is to classify D by means of an automatic classifier  X   X  (which we assume here to be generated by training a supervised learner on a training set Tr ), and then to have a human editor inspect the results of the automatic classification, correcting mis-classifications where appropriate 1 . The human annotator will obviously inspect only a subset D 0  X  D (since it would not otherwise make sense to have an initial automated clas-sification phase), e.g., until she is confident that the overall level of accuracy of D is sufficient. We call this scenario semi-automated text classification (SATC).

An automatic TC system may support this task by rank-ing, after the classification phase has ended and before the inspection begins, the classified documents in a such a way that, if the human annotator inspects the documents start-ing from the top of the ranking and working down the list, the expected increase in classification effectiveness that de-rives from this inspection is maximized. This paper is con-cerned with devising good ranking strategies for this task.
One obvious strategy is to rank the documents in ascend-ing order of the confidence scores generated by  X   X , so that the top-ranked documents are the ones that  X   X  has classi-fied with the lowest confidence 2 . The rationale is that an increase in effectiveness can derive only by inspecting mis-classified documents, and that a good ranking method is simply the one that top-ranks the documents with the high-est probability of misclassification, which (in the absence of other information) we may take to be the documents which  X   X  has classified with the lowest confidence.

In this work we show that this strategy is, in general, suboptimal. Simply stated, the reason is that, when we deal with imbalanced TC problems (as most TC problems indeed
In the rest of this paper we will simply write  X  X nspect X  to actually mean  X  X nspect and correct where appropriate X .
We call this strategy  X  X bvious X  because of the evident sim-ilarities between SATC and active learning (see Section 6), where this strategy is an often-used baseline. However, to the best of our knowledge, the application of this ranking method (or of any other ranking method, for that matter) to SATC has never been discussed in the literature. are) and, as a consequence, choose an evaluation measure  X  such as F 1  X  that caters for this imbalance, the improvements in effectiveness that derive from correcting a false positive or a false negative may not be the same.

The contributions of this paper are the following. First, we develop a new utility-theoretic ranking method for SATC based on the notion of inspection gain , i.e., the improvement in effectiveness that would derive by correcting a given type of mistake (i.e., false positive or false negative). Second, we propose a new evaluation measure for SATC, and use it to evaluate our experiments on a standard dataset. The results show that, with respect to the confidence-based base-line method above, our ranking method is substantially more effective.

The rest of the paper is organized as follows. Section 2 sets out preliminary definitions and notation. Section 3 describes our utility-theoretic strategy for ranking the auto-matically labelled documents, while Section 4 describes the effectiveness measure we propose for this task. Section 5 reports the results of our experiments in which we compare the different ranking strategies by simulating the work of a human annotator that inspects variable portions of the clas-sified test set. Section 6 reviews related work, while Section 7 concludes by charting avenues for future research. This paper focuses on semi-automated (multi-class) multi-label TC. Given a set of textual documents D and a pre-defined set of classes C = { c 1 ,...,c m } , multi-label TC is usually defined as the task of estimating an unknown tar-get function  X  : D  X  C  X  { X  1 , +1 } , that describes how documents ought to be classified, by means of a function  X   X  : D X C  X  { X  1 , +1 } called the classifier 3 . Here, +1 and  X  1 represent membership and non-membership of the docu-ment in the class. Each document may thus belong to zero, one, or several classes at the same time. Multi-label TC is usually accomplished by generating m independent binary classifiers  X   X  j , one for each c j  X  C , each entrusted with de-ciding whether a document belongs or not to a class c j .
In this paper we will restrict our attention to classifiers  X   X  j that, aside from taking a binary decision D ij  X  X  X  1 , +1 } on a given document d i , also return a confidence estimate C ij , i.e., a numerical value representing the strength of their belief in the fact that D ij is correct (the higher the value, the higher the confidence). We formalize this by taking a binary classifier to be a function  X   X  j : D  X  (  X  X  X  , +  X  ) in which the sign of the returned value D ij  X  sgn (  X   X  j ( d { X  1 , +1 } indicates the binary decision of the classifier, and the absolute value C ij  X  |  X   X  j ( d i ) | represents its confidence in the decision.
 For the purposes of this paper we also assume that (the well-known harmonic mean of precision and recall) is the chosen evaluation measure, where  X   X  j ( Te ) indicates the result of applying  X   X  j to the test set Te and TP j , FP and TN j indicate the numbers of true positives, false pos-itives, false negatives, and true negatives in Te . Note that F 1 is undefined when TP j = FP j = FN j = 0; in this case
Consistently with most mathematical literature we use the caret symbol ( X ) to indicate estimation. we take F 1 (  X   X  j ( Te )) = 1, since  X   X  j has correctly classified all documents as negative examples.

We also use TP ( ij ) as a shorthand to indicate that  X   X  is a true positive, and use FP ( ij ), FN ( ij ), and TN ( ij ) with analogous meanings. In this paper the set of unlabelled documents that the classifier must automatically label (and rank) in the  X  X perational X  phase will be represented by the test set Te . For the moment being, let us concentrate on the binary case, i.e., let us assume there is a single class c j that needs to be separated from its complement c j . The policy we propose for ranking the automatically labelled documents in  X   X  j makes use of a function U j ( d i ) that estimates the utility, for the aims of increasing F 1 (  X   X  j ( Te )), of manually inspecting the label D ij attributed to d i by  X   X  j .

Given a set  X  of mutually disjoint events, a utility function is defined as a sum P  X   X   X  P (  X  ) G (  X  ), where P (  X  ) represents the probability of occurrence of event  X  and G (  X  ) represents the gain obtained if event  X  indeed occurs.

Upon submitting document d i to classifier  X   X  j , a positive or a negative decision can be returned. If a positive deci-sion is returned (i.e., D ij = +1) then the mutually disjoint events TP ( ij ) and FP ( ij ) can occur, while if this decision is negative (i.e., D ij =  X  1) then the mutually disjoint events FN ( ij ) and TN ( ij ) can occur. We thus naturally define the two utility functions with U + j ( d i ) addressing the case of a positive decision and U j ( d i ) the case of a negative decision. We also define as a function embracing both positive and negative decisions. We equate G ( FP ( ij )) in Equations 2 with the average in-crease in F 1 (  X   X  j ( Te )) that would derive by manually inspect-ing the label attributed by  X   X  j to a document in FP j . We call this the inspection gain of a member of FP j . From now on we will write G ( FP j ) instead of G ( FP ( ij )) so as to reflect the fact that the inspection value is the same for all members of FP j . Analogous arguments apply to G ( TP ( ij )), G ( FN ( ij )), and G ( TN ( ij )).

Quite evidently, G ( TP j ) = G ( TN j ) = 0, since when the human annotator inspects the label attributed to d i by  X  and finds out it is correct, she will not modify it, and the value of F 1 (  X   X  j ( Te )) will thus remain unchanged. This means that Equations 2 simplify to G ( FP j ) (resp., G ( FN j )) evaluates instead to the average in-crease in F 1 (  X   X  j ( Te )) obtained by correcting a false positive (resp., a false negative). It is easy to see that, in general, G ( FP j ) 6 = G ( FN j ). In fact, if a false positive is corrected, the increase in F 1 is the one deriving from removing a false positive and adding a true negative, i.e., where by F FP 1 (  X   X  j ) we indicate the value of F 1 that would derive by correcting all false positives of  X   X  j ( Te ), i.e., turn-ing them into true negatives. Conversely, if a false negative is corrected, the increase in F 1 is the one deriving from re-moving a false negative and adding a true positive, i.e., where by F FN 1 (  X   X  j ) we indicate the value of F 1 that would derive by turning all the false negatives of  X   X  j ( Te ) into true positives. We derive the probabilities P (  X  ) in Equations 4 by assum-ing that the confidence scores C ij generated by  X   X  j can be trusted (i.e., that the higher C ij , the higher the probability that D ij is correct), and by applying to C ij a generalized logistic function f ( z ) = e  X z / ( e  X z + 1). This results in The generalized logistic function has the effect of monotoni-cally converting scores ranging on (  X  X  X  , +  X  ) into real values in the [0.0,1.0] range. When C ij = 0 (this happens when  X   X  j has no confidence at all in its own decision D ij P ( TP ( ij ) | D ij = +1) = P ( FP ( ij ) | D ij = +1) = 0 . 5 and the probability of correct classification and the probability of misclassification are identical. Conversely, we have This means that, when  X   X  j has a very high confidence in its own decision D ij , the probability that D ij is wrong is taken to be very low.

The reason why we use a generalized version of the logistic function instead of the standard version (which corresponds to the case  X  = 1) is that using this latter within Equations 7 would give rise to a very high number of zero probabilities of misclassification, since the standard logistic function con-verts every positive number above a certain threshold (  X  36) to a number that standard implementations round to 1 even by working in double precision. By tuning the  X  parameter (the growth rate ) we can tune the speed at which the right-hand side of the sigmoid asymptotically approaches 1, and we can thus tune how evenly Equations 7 distribute the con-fidence values across the [0.0,0.5] interval. How we optimize the  X  parameter is discussed in Section 5.1. One problem that needs to be tackled in order to compute G ( FP j ) and G ( FN j ) is that the contingency cell counts TP FP j , and FN j are not known, and thus need to be esti-mated 4 . In order to estimate  X  j  X  { TP j , FP j , FN make the assumption that the training set and the test set are independent and identically distributed. We then per-form a k -fold cross-validation on the training set: if by TP we denote the number of true positives for class c j result-ing from the k -fold cross-validation on Tr , the maximum-likelihood estimate of TP j is  X  TP ML j = TP Tr j  X | Te | / | Tr | ; the same holds for  X  FP ML j and  X  FN ML j .

However, these maximum-likelihood cell count estimates (noted  X   X  ML j ) need to be smoothed, so as to avoid zero counts. In fact, if  X  TP ML j = 0, it would derive from Equation 5 that there is nothing to be gained by correcting a false positive, which is counterintuitive. Similarly, if  X  FP ML j = 0, the very notion of F FP 1 (  X   X  j ) would be meaningless, since it does not make sense to speak of  X  X emoving a false positive X  when there are no false positives; the same goes for  X  FN ML j .
A second reason why the  X   X  ML j need to be smoothed is that, when | Te | / | Tr | &lt; 1, they may give rise to negative values for G ( FP j ) and G ( FN j ), which is obviously counter-intuitive. To see this, note that the  X   X  ML j may not be integers (which is not bad per se, since the notions of precision, re-call, and their harmonic mean intuitively make sense also when we allow the contingency cell counts to be nonnega-tive reals instead of the usual integers), and may be smaller than 1 (this happens when | Te | / | Tr | &lt; 1). This latter fact is problematic, both in theory (since it is meaningless to speak of, say, removing a false positive from Te when  X  X here are less than 1 false positives in Te  X ) and in practice (since it is easy to verify that negative values for G ( FP j ) and G ( FN may derive).

Smoothing has extensively been studied in language mod-elling for speech processing [3] and for ad hoc search in IR [24]. However, the present context is slightly different, in that we need to smooth contingency tables, and not (as in the cases above) language models. In particular, while the  X   X  j are the obvious counterpart of the document model re-sulting from maximum-likelihood estimation, there is no ob-vious counterpart to the  X  X ollection model X , thus making the use of, e.g., Jelinek-Mercer smoothing problematic. A fur-ther difference is that we here require the smoothed counts not only to be nonzero, but also to be  X  1 (a requirement not to be found in language modelling).

Smoothing has also been studied specifically for the pur-
We will disregard the estimation of TN j since it is un-necessary for our purposes, given that F 1 (  X   X  j ( Te )) does not depend on TN j . pose of smoothing contingency cell estimates [1, 21]. How-ever, these methods are inapplicable to our case, since they were originally conceived for contingency tables character-ized by a small (i.e.,  X  1) ratio between the number of ob-servations (which in our case is | Te | ) and the number of cells (which in our case is 4); our case is quite the opposite. Ad-ditionally, these smoothing methods do not operate under the constraint that the smoothed counts should all be  X  1, which is a hard constraint for us.

For all these reasons, rather than adopting more sophisti-cated forms of smoothing, we adopt simple additive smooth-ing (also known as Laplace smoothing ), a special case of Bayesian smoothing using Dirichlet priors [24] which is ob-tained by adding a fixed quantity to all the  X   X  ML j . As a fixed quantity we add 1, since it is the quantity that all our cell counts need to be greater or equal to for Equations 5 and 6 to make sense. We thus leave the study of more sophisticated smoothing methods to future work.

However, it should be noted that we apply smoothing in an  X  X n demand X  fashion, i.e., we check if the contingency table needs smoothing at all (i.e, if any of the  X   X  ML j and we smooth it only if this is the case. Our function U j ( d i ) of Section 3.1 is thus obtained by plug-ging Equations 5 and 6 into Equations 4.

At this point, it would seem sensible to propose ranking, for each c j  X  C , all the automatically labelled documents in Te in decreasing order of their U j ( d i ) value. Unfortu-nately, this would generate |C| different rankings, and in an operational context it seems implausible to ask a human an-notator to scan |C| different rankings of the same documents (this might mean reading the same document |C| times in order to validate its labels). As suggested in [6] for active learning, it seems instead more plausible to generate a single ranking, according to a score U ( d i ) that is a function of the |C| different U j ( d i ) scores. In such a way, the human anno-tator will scan this single ranking from the top, validating all the |C| different labels for d i before moving on to another document. As the criterion for generating the overall utility score U ( d i ) we use total utility , corresponding to the simple sum Our final ranking is thus generated by sorting the test doc-uments in descending order of their U ( d i ) score.
From the standpoint of computational cost, this technique is O ( | Te | X  ( |C| + log | Te | )), since the cost of sorting the test documents by their U (  X  ) score is O ( | Te | log | Te | ), and the cost of computing the U (  X  ) score for | Te | documents and |C| classes is O ( | Te | X |C| ). No measures are known from literature for evaluating the effectiveness of a SATC-oriented ranking method  X  . We here propose such a measure, which we call expected normalized error reduction (noted ENER  X  ). Let us first introduce the notion of residual error at rank n (noted E  X  ( n ), which we assume to range on [0,1]), defined as the error that is still present in the document set Te after the human annotator has inspected the documents at the first n rank positions in the ranking generated by  X  . The value of E  X  (0) is the initial error generated by the automated classifier, and the value of E  X  ( | Te | ) is 0. We will hereafter call n the inspection length .
 We next define error reduction at rank n to be i.e., a value in [0,1] that indicates the error reduction ob-tained by a human annotator who has inspected the docu-ments at the first n rank positions in the ranking generated by  X  ; 0 stands for no reduction, 1 stands for total elimination of error.
 Example plots of the ER  X  ( n ) measure are displayed in Figures 1 and 2, where different curves represent differ-ent ranking methods  X  0 , X  00 ,... , and where, for better con-venience, the x axis indicates the fraction n/ | Te | of the test set that has been inspected rather than the number n of inspected documents. By definition all curves start at the origin of the axes and end at the upper right corner of the graph. Higher curves represent better strategies, since they indicate that a higher error reduction is achieved for the same amount of manual inspecting effort.

The reason why we focus on error reduction, instead of the complementary concept of  X  X ncrease in accuracy X , is that er-ror reduction has always the same upper bound (i.e., 100% reduction), independently of the initial error. In contrast, the increase in accuracy that derives from inspecting the documents does not always have the same upper bound. For instance, if the initial accuracy is 0.5 (with accuracy values ranging on [0,1]), then an increase in accuracy of 100% is in-deed possible, while this increase is not possible if the initial accuracy is 0.9. This makes the notion of increase in ac-curacy problematic, since different datasets and/or different classifiers give rise to different initial levels of accuracy. So, using error reduction instead of increase in accuracy  X  X or-malizes X  our curves, i.e., allows a meaningful comparison of curves obtained on different datasets and after different classifiers have been used.

Since (as stated in Section 2) we use F 1 for measuring ef-fectiveness, as a measure of classification error we use E (1  X  F 1 ). In order to measure the overall effectiveness of a ranking method across the entire set C of categories, we compute two versions of ER  X  ( n ), one based on microaver-aged E 1 (denoted by E  X  1 ) and one based on macroaveraged E 1 ( E M 1 ). E  X  1 is obtained by (i) computing the class-specific values TP j , FP j and FN j , (ii) obtaining TP as the sum of the TP j  X  X  (same for FP and FN ), and then (iii) applying is instead obtained by computing the class-specific E 1 val-ues and averaging them across the c j  X  X . The two versions of ER  X  ( n ) will be indicated as ER  X   X  ( n ) and ER M  X  ( n ). One problem with ER  X  ( n ), though, is that the expected ER  X  ( n ) value of the random ranker is fairly high 5 , since it
That the expected ER  X  ( n ) value of the random ranker is | Te | is something that we have not tried to formally prove. However, that this holds is supported by intuition and is unequivocally shown by Monte Carlo experiments we have amounts (for both ER  X   X  ( n ) and ER M  X  ( n )) to n | Te | ference between the ER  X  ( n ) value of a genuinely engineered ranking method  X  and the expected ER  X  ( n ) value of the random ranker is particularly small for high values of n , and is null for n = | Te | . This means that it makes sense to fac-tor out the random factor from ER  X  ( n ). This leads us to define the normalized error reduction of ranking method  X  as NER  X  ( n ) = ER  X  ( n )  X  n | Te | , with the two versions noted as NER  X   X  ( n ) and NER M  X  ( n ). However, NER  X  ( n ) is still unsatisfactory as a measure, since it depends on a specific value of n (which is undesirable, since our human annotator may decide to work down the ranked list as far as she deems suitable). Following [18] we assume that the human annotator stops inspecting the ranked list at exactly rank n with probability P s ( n ). We can then define the expected normalized error reduction of ranking method  X  on a given document set Te as with the two versions indicated as ENER  X   X  and ENER M  X  . Different probability distributions P s ( n ) can be assumed. In order to base the definition of such a distribution on a plausible model of user behaviour, we here make the as-sumption (along with [15]) that a human annotator, after inspecting a document, goes on to inspect the next docu-ment with probability p (also called persistence in [15]) or stops inspecting with probability (1  X  p ), so that
P s ( n ) = p It can be shown that, for a sufficiently large value of | Te | , the expected number of documents that the human annotator will inspect as a function of p asymptotically tends to 1 The value  X  = 1 | Te | (1  X  p ) thus denotes the expected fraction of the test set that the human annotator will inspect as a function of p .

Using this distribution entails the need of determining a realistic value for p . A value p = 0 corresponds to a situation in which the human annotator only inspects the top-ranked document, while p = 1 indicates a human annotator who inspects each document in the ranked list. Unlike in ad hoc search, we think that in a SATC context it would be unrealistic to take a value for p as given irrespective of the size of Te . In fact, given a desired level of error reduction, when | Te | is large the human annotators need to be more persistent (i.e., characterized by higher p ) than when | Te | is small.

Therefore, instead of assuming a predetermined value of p we assume a predetermined value of  X  , and derive the value of p from the equation  X  = 1 | Te | (1  X  p ) . For example, in a certain application we might assume  X  = . 2 (i.e., that the average human annotator inspects 20% of the test set). In this case, if | Te | = 1 , 000, then p = 1  X  1 . 2  X  1000 if | Te | = 10 , 000, then p = 1  X  1 . 2  X  10000 = . 9995. In the exper-iments of Section 5 we will test all values of p corresponding to values of  X  in { . 05 ,. 10 ,. 20 } . run on our datasets; see Figures 1 and 2 for a graphical representation.

Note that the values of ENER  X  are bounded above by 1, but a value of 1 is not attainable. In fact, even the  X  X erfect ranker X  (i.e., the ranking method that top-ranks all mis-classified documents, noted Perf ) cannot attain an ENER  X  value of 1, since in order to achieve total error elimination all the misclassified documents need to be inspected anyway, which means that the only condition in which ENER Perf might equal 1 is when there is just 1 misclassified docu-ment. We do not try to normalize ENER  X  by the value of ENER Perf since ENER Perf cannot be characterized ana-lytically, and depends on the actual labels in the test set. Let  X  be a dataset partitioned into a training set Tr and a test set Te . In each experiment reported in this paper we adopt the following experimental protocol: 1. For each c j  X  X  2. For every ranking policy  X  tested For Step 1b we have used k = 10.

The optimization method we use for Step 1(b)ii consists in picking the value of  X  that minimizes the average (across the c j  X  X  ) absolute value of the difference between Pos the number of positive training examples of class c E [ Pos Tr j ], the expected number of such examples as resulting from the probabilities of membership in c j computed in the k -fold cross-validation. That is, we pool together all the training documents classified in the k -fold cross-validation, and then we pick This is a much faster parameter optimization method than the traditional method of picking the value that has per-formed best in k -fold cross-validation since, unlike the lat-ter, it does not depend on the ranking method  X  . Therefore, this method spares us from the need of ranking the training set several times, i.e., for each combination of a tested value of  X  and a ranking method  X  .

As the learner for generating our classifiers  X   X  j a boosting-based learner called MP-Boost [5]. Boosting-based methods have showed very good performance across many learning tasks and, at the same time, have strong jus-tifications from computational learning theory. MP-Boost a Monte Carlo method with 50 random trials. Higher curves are better. is a variant of AdaBoost.MH [19] optimized for multi-label settings, which has been shown in [5] to obtain considerable effectiveness improvements with respect to AdaBoost.MH . MP-Boost generates a classifier  X   X  j where sgn (  X   X  j ( d resents the binary decision as to whether d i belongs to c and |  X   X  j ( d i ) | represents the confidence in this decision. In all our experiments we set the S parameter of MP-Boost (representing the number of boosting iterations) to 1000.
As dataset we have used the Reuters-21578 corpus. It consists of a set of 12,902 news stories, partitioned (accord-ing to the  X  X odApt  X e X  split we have adopted) into a training set of 9,603 documents and a test set of 3,299 documents. The documents are labelled by 118 categories; the average number of categories per document is 1.08, ranging from a minimum of 0 to a maximum of 16; the number of posi-tive examples per class ranges from a minimum of 1 to a maximum of 3964. In our experiments we have restricted our attention to the 115 categories with at least one positive training example. This dataset is publicly available is probably the most widely used benchmark in text clas-sification research, which allows other researchers to easily replicate the results of our experiments.

In all the experiments discussed in this paper stop words have been removed, punctuation has been removed, all let-ters have been converted to lowercase, numbers have been removed, and stemming has been performed by means of Porter X  X  stemmer. Word stems are thus our indexing units. Since MP-Boost requires binary input, only their presence/ absence in the document is recorded, and no weighting is performed. http://www.daviddlewis.com/resources/testcollections/ ~reuters21578/ As the baseline for our experiments we use the confidence-based strategy discussed in Section 1, which corresponds to using our utility-theoretic method with both G ( FP ) and G ( FN ) set to 1. As discussed in Footnote 2, while this strategy has not explicitly been proposed before, it seems a reasonable, common-sense strategy anyway.

While the confidence-based method will act as our lower bound, we have also run  X  X racle-based X  methods aimed at identifying upper bounds for the effectiveness of our utility-theoretic method, i.e., at assessing the effectiveness of  X  X de-alized X  (albeit non-realistic) systems at our task.

The first such method (dubbed Oracle1 ) consists of  X  X eek-ing X  at the actual values of TP j , FP j , and FN j in Te , us-ing them in the computation of G ( FP j ) and G ( FN j ), and running our utility-theoretic method as usual. Oracle1 thus indicates how our method would behave were it able to  X  X er-fectly X  estimate TP j , FP j , and FN j . The difference in effec-tiveness between Oracle1 and our method will thus be due to (i) the performance of the smoothing method adopted, and (ii) possible differences between the distribution of the documents across the contingency table cells in the training and in the test set.

In the second such method ( Oracle2 ) we instead peek at the true labels of the documents in Te , which means that we will be able to (a) use the actual values of TP and FN j in the computation of G ( FP j ) and G ( FN j in Oracle1 ), and (b) replace the probabilities in Equations 4 with the true binary values (i.e., replacing P ( x ) with 1 if x is true and 0 if x is false), after which we run our utility-based ranking method as usual. The difference in effectiveness between Oracle2 and our method will be due to factors (i) and (ii) already mentioned for Oracle1 and to our method X  X  across the 10 parts. (obvious) inability to perfectly predict whether a document was classified correctly or not. Figure 1 illustrates the results of our experiments on the Reuters-21578 dataset in terms of ER  X   X  ( n ) and ER M  X  the results of the same experiments in terms of ENER and ENER M  X  as a function of the chosen value of  X  are instead reported in Table 1. The initial error generated by the automatic classifier is E  X  1 = . 152 and E M 1 = . 383. The optimal value of  X  returned by the k -fold cross-validation phase is . 420.

The first insight we can draw from these results is that our utility-theoretic method outperforms the baseline in a very substantial way. For instance, for  X  = . 10 (corresponding to p = . 996) it obtains a relative improvement over the baseline of +30% in terms of ENER  X   X  and of +119% in terms of ENER M  X  . Improvements obtained for the two other tested values of  X  are qualitatively similar.

A second insight is that, surprisingly, our method hardly differs in terms of performance from Oracle1 . The two curves can be barely distinguished in Figure 1, and in terms of ENER  X  Oracle1 is even outperformed by our utility-theoretic method, albeit by a narrow margin (.221 vs. .217 for ENER and .233 vs. .224 for ENER M  X  , both for  X  = . 10; the other tested values for  X  give similar results). This shows that (at least judging from this experiment) Laplace smoothing is nearly optimal, and there is likely not much we can gain from applying alternative, more sophisticated smoothing meth-ods. This is sharply different from what happens in language modelling, where Laplace smoothing has been shown to be an underperformer [9]. The fact that our method slightly (and strangely) outperforms Oracle1 is probably due to ac-cidental,  X  X erendipitous X  interactions between the probabil-ity estimation component (Equation 7) and the contingency cell estimation component of Section 3.4.

Note that in Figure 1 the ER  X   X  curves (left) are smoother than the ER M  X  curves (right). This is due to the fact that E 1 is evaluated on a single, global contingency table, so that correcting an individual document always has a small effect on E  X  1 . By contrast, even correcting a single document may have a major effect on a class-specific value of E cially if the class is infrequent), and this may bring about a relatively major effect on E M 1 too. We have run a second batch of experiments in which we randomly split the Reuters-21578 test set in 10 equally-sized parts (about 330 documents each), we run each ranking method on each such part individually, and we average the results across the 10 parts. We call this scenario Reuters-21578/10 . The relative results in terms of ER  X   X  ER M  X  ( n ) are shown in Figure 2, while the results of the same experiments in terms of ENER  X   X  and ENER M  X  are reported in Table 2.

The rationale of these experiments is checking how the methods fare when ranking test sets much smaller that the Reuters-21578 test set. This is more challenging than ranking larger sets, since in this case Laplace smoothing (i) can seriously perturb the relative proportions among the cell counts, which can generate poor estimates of G ( FP and G ( FN j ), and (ii) is performed for more classes, since we smooth  X  X n demand X  only and since the likelihood that the  X   X  j are smaller than 1 is higher with small test sets.
Figure 2 confirms that our utility-theoretic method sub-stantially outperforms the baseline also in this context. Note that the E M 1 curves (left) are smoother than the analogous curves of the full Reuters-21578 . This is due to the fact that the curves in Figure 2 result from averages across 10 dif-ferent experiments, and the increase brought about at rank n is actually the average of the increases brought about at rank n in the 10 experiments.

That our utility-theoretic method substantially outper-forms the baseline also in this experiment can be seen also (0 . 20) E NER M  X  (0 . 05) E NER M  X  (0 . 10) E NER M  X  (0 . 20) (0 . 20) E NER M  X  (0 . 05) E NER M  X  (0 . 10) E NER M  X  (0 . 20) from Table 2. For  X  = . 10 (corresponding to p = . 969) the relative improvement over the baseline is +25% for ENER  X  and +116% for ENER M  X  . Similarly substantial improve-ments are obtained for the two other values of  X  tested.
Note also that in this case our method underperforms Ora-cle1 (.212 vs. .215 for ENER  X   X  , .210 vs. .220 for ENER and this points to a possible, small suboptimality of the smoothing method adopted. We leave the investigation of more sophisticated smoothing methods to a future paper. In further experiments that we have run, we have split the Reuters-21578 test set even further, i.e., into 100 equally-sized parts of about 33 documents each, so as to test the performance of Laplace smoothing methods in even more challenging conditions. The ENER  X   X  and ENER M  X  results for this Reuters-21578/100 scenario are reported in Figure 3; we do not report the detailed ER  X   X  ( n ) and ER M  X  ( n ) plots for reasons of space. Our utility-theoretic model still out-performs the baseline, with a relative improvement of +18% on ENER  X   X  and +48% on ENER M  X  with  X  = . 10, corre-sponding to p = . 696; qualitatively similar improvements are obtained with the other tested values of  X  .

However, we consider the Reuters-21578/100 scenario less interesting than the two previously discussed ones, since applying a ranking method to a set of 33 documents only is of debatable utility, given that a human annotator confronted with the task of inspecting just 33 documents can arguably check them all without any need for ranking.

Incidentally, note that the Reuters-21578/10 and Reuters-21578/100 experiments model an application scenario in which a set of automatically labelled documents is split (e.g., to achieve faster throughput) among k human annotators, each one entrusted with inspecting a part of the set. In this case, each annotator is presented with a ranking of her own document subset, and works exclusively on it. It is important to note that both the baseline and our utility-theoretic method are explicitly optimized for ENER M  X  , and not for ENER  X   X  . To see this, note that the U ( d tion of Equation 8 is based on an unweighted sum of the class-specific U j ( d i ) scores, i.e., it pays equal importance to all classes, irrespective of frequency considerations. This means that it is optimized for metrics that also pay equal attention to all classes, as all macroaveraged measures such as ENER M  X  do.

By contrast, ENER  X   X  pays attention to classes propor-tionally to their frequency. Therefore, a ranking method that optimizes for ENER  X   X  should instead do away with class-specific utility functions and use a utility function that (similarly to what happens for E  X  1 ) is directly computed on a single, global contingency table obtained by the cell-wise sum of the class-specific contingency tables. In such a method, G ( FP ) and G ( FN ) would be global to C , i.e., they would be the same for all c j  X  C . We leave the investiga-tion of ranking methods optimized for ENER  X   X  to a future paper.

All this shows that the measure according to which both the baseline and our utility-theoretic method should be eval-uated is ENER M  X  , and not ENER  X   X  , since it is ENER that these methods were designed for 7 . However, we have also reported results measured according to E  X  1 for complete-ness, and in order to show that, while our methods were not meant for use with E  X  1 as the yardstick, they still perform well even in this context.

It should thus come as no surprise that the improvements displayed by our method (and the oracles) over the baseline are always higher, or much higher, for ENER M  X  than for ENER  X   X  , as is apparent from all the figures and tables in this paper. Many researchers have tackled the problem of how to use automated TC technologies in application contexts in which the required accuracy levels are unattainable by the gener-ated automatic classifiers.

A standard response to this problem is to adopt active learning (AL  X  see e.g., [11, 22]), i.e., use algorithms that optimize the informativeness of additional training exam-ples provided by a human annotator. Still, providing ad-ditional training examples, no matter how carefully chosen, may be insufficient, since in many applicative contexts high
The baseline we have used is, as specified in Section 5.2.1, our utility-theoretic method with G ( FP j ) and G ( FN j to 1; it is thus also optimized for E M 1 . A baseline method optimized for E  X  1 would be the method outlined in the last paragraph with G ( FP ) and G ( FN ) set to 1. (0 . 20) E NER M  X  (0 . 05) E NER M  X  (0 . 10) E NER M  X  (0 . 20) enough accuracy levels cannot be attained, irrespectively of the quantity and quality of the available training data. Sim-ilar considerations apply when active learning is carried out at the term level, rather than at the document level [10, 17].
A related response to the same problem is to adopt train-ing data cleaning (TDC  X  see e.g., [7, 8]), i.e., use algorithms that optimize the human annotator X  X  efforts at correcting possible labelling mistakes in the training set. Similarly to the case of AL, in many applicative contexts high enough accuracy levels cannot be attained even at the price of care-fully inspecting the entire training set for labelling mistakes.
Both AL and TDC are different from the task we deal with, since we are not concerned with improving the quality of the training set. We are instead concerned with improving the quality of the automatically classified test set, typically after all attempts at injecting quality in the automatic clas-sifier have proved insufficient; in particular, no retraining / reclassification phase is involved in SATC.
 Active learning. As remarked above, SATC certainly bears strong relations with active learning. In both SATC and in the selective sampling  X  also known as pool-based  X  approach to AL [13, 14], the automatically classified objects are ranked and the human annotator is encouraged to cor-rect possible misclassifications by working down from the top of the ranked list. However, as remarked above, the goals of the two tasks are different. For instance, in ac-tive learning we are interested in top-ranking the unlabelled documents that, once manually labelled, would maximize the information fed back to the learning process, while in SATC we are interested in top-ranking the unlabelled doc-uments that, once manually inspected, would maximize the expected accuracy of the automatically classified document set. As a result, the optimal ranking strategies for the two tasks may be different too.
 Semi-automated TC. While AL (and, to a much lesser degree, TDC) have been investigated extensively in a TC context, semi-automated TC has been completely neglected by the research community. While a number of papers (e.g., [12, 20, 23]) have evoked the existence of this scenario, we are not aware of any published papers that either discuss rank-ing policies for supporting the human annotator X  X  effort, or that attempt to quantify the effort needed for reaching a desired level of accuracy. For instance, while discussing a system for the automatic assignment of ICD9 classes to pa-tients X  discharge summaries, Larkey and Croft [12] say  X  X e envision these classifiers being used in an interactive system which would display the 20 or so top ranking [classes] and their scores to an expert user. The user could choose among these candidates (...) X , but do not present experiments that quantify the accuracy that the inspecting activity brings about, or methods aimed at optimizing the cost-effectiveness of this activity. We have presented a method for ranking the documents la-belled by an automatic classifier. The documents are ranked in such a way as to maximize the expected reduction in clas-sification error brought about by a human annotator who inspects a subset of the ranked list and corrects the labels when appropriate. We have also proposed an evaluation measure for such ranking methods, based on the notion of expected normalized error reduction. Experiments carried out on a standard dataset show that our method substan-tially outperforms a state-of-the-art baseline method. To the best of our knowledge, this is the first paper in the lit-erature that addresses semi-automated text classification as a task in its own right, and which presents methods explic-itly devised for optimizing it; this is obviously the reason of the very substantive improvements obtained by our method with respect to the baseline.

It should be remarked that the very fact of using a util-ity function, i.e., a function in which different events are characterized by different gains, makes sense here since we have adopted an evaluation function, such as F 1 , in which correcting a false positive or a false negative indeed brings about different benefits to the final effectiveness score. If we instead adopted accuracy (i.e., the percentage of binary classification decisions that are correct) as the evaluation measure, utility would default to the probability of misclas-sification and our method would coincide with the baseline, since correcting a false positive or a false negative would bring about the same benefit. The method we have pre-sented is justified by the fact that F 1 is the standard eval-uation function for text classification, while accuracy is a deprecated measure in a text classification context since it is not robust to class imbalance. See e.g., [20, Section 7.1.2] for a discussion of this point.

The method we have proposed is obviously valid also when a different instantiation of the F  X  function (i.e., with  X  6 = 1) is used as the evaluation function. This may be the case, e.g., when classification is to be applied to a recall-oriented task (such as e-discovery [16]), in which case values  X  &gt; 1 are appropriate. In these cases our utility-theoretic method can be used once the appropriate instance of F  X  is used in Equations 5 and 6 in place of F 1 . The same trivially holds for any other evaluation function, even different from F and even multivariate and non-linear, provided it can be computed from a contingency table. We also remark that this technique is obviously not limited to text classification, but can be useful in any classification context in which class imbalance [2], or cost-sensitivity in general [4], suggest using a measure (such as F  X  ) that caters for these characteristics.
Note that, by using our method, it is also easy to pro-vide the human annotator with an estimate of how accurate the labels of the test set are as a result of her inspecting all the documents until rank n . In fact, if the contingency cell estimates  X  TP j ,  X  FP j , and  X  FN j (see Section 3.4) are up-dated (adding and subtracting 1 where appropriate) after each correction made by the human annotator, at any point in the inspection activity these are up-to-date estimates of how well the test set is now classified, and from these esti-mates F 1 (or other) can be computed as usual.

In the next future we plan to carry our more experiments, using additional datasets, learners, and (when the test sets are small) smoothing methods. We are also currently testing an improved ranking method that we have recently designed. Essentially, this  X  X ynamic X  method is based on the observa-tion that inspecting a document misclassified for c j brings about changes in at least one of TP j , FP j , and FN j (and in G ( FP j ) and/or G ( FN j ) and U j (  X  ) as a consequence). This dynamic version of our ranking strategy consists of updat-ing (after each correction has been performed)  X  TP j ,  X  and  X  FN j by adding and subtracting 1 where appropriate, re-estimating G j ( FP ), G j ( FN ) and U j (  X  ), and bringing to bear these new estimates when selecting the document that should be presented next to the human annotator. [1] P. Burman. Smoothing sparse contingency tables. The [2] N. V. Chawla, N. Japkowicz, and A. Kolcz. Editorial: [3] S. F. Chen and J. Goodman. An empirical study of [4] C. Elkan. The foundations of cost-sensitive learning. [5] A. Esuli, T. Fagni, and F. Sebastiani. MP-Boost: A [6] A. Esuli and F. Sebastiani. Active learning strategies [7] A. Esuli and F. Sebastiani. Training data cleaning for [8] F. Fukumoto and Y. Suzuki. Correcting category [9] W. Gale and K. Church. What X  X  wrong with adding [10] S. Godbole, A. Harpale, S. Sarawagi, and [11] S. C. Hoi, R. Jin, and M. R. Lyu. Large-scale text [12] L. S. Larkey and W. B. Croft. Combining classifiers in [13] D. D. Lewis and J. Catlett. Heterogeneous uncertainty [14] A. K. McCallum and K. Nigam. Employing EM in [15] A. Moffat and J. Zobel. Rank-biased precision for [16] D. W. Oard, J. R. Baron, B. Hedin, D. D. Lewis, and [17] H. Raghavan, O. Madani, and R. Jones. Active [18] S. E. Robertson. A new interpretation of average [19] R. E. Schapire and Y. Singer. Boostexter: A [20] F. Sebastiani. Machine learning in automated text [21] J. S. Simonoff. A penalty function approach to [22] S. Tong and D. Koller. Support vector machine active [23] Y. Yang and X. Liu. A re-examination of text [24] C. Zhai and J. Lafferty. A study of smoothing
