 form D of cally, the likelihood ratio function d P /d Q and the divergence functional D methods, but no theoretical results on its convergence rate were provided. behavior of our estimator and compare it to other methods via simulations. divergence, which we later exploit to develop an M -estimator. Lebesgue measure  X  , with positive densities p (KL) divergence between P and Q is given by D the choice  X  ( t ) =  X  log( t ) for t &gt; 0 and +  X  otherwise. we can write  X  ( u ) = sup  X  , it can be shown that the supremum will be achieved for functi ons f such that q where q for any x  X  X  . Thus, we have proved [15, 11]: Lemma 1. Letting F be any function class in X  X  R , there holds: with equality if F  X   X  X  ( q  X  1  X  log(  X  v ) if u &lt; 0 and +  X  otherwise. By Lemma 1,
D K ( P , Q ) = sup In addition, the supremum is attained at g = p Let X n divergence and the density ratio g First, let G be a function class of X  X  R where R d P If the supremum is attained at  X  g non-negative measure of complexity for g such that I ( g G as follows: where G The estimation procedure involves solving the following pr ogram: where  X  obtain an estimate of the KL divergence D For the KL divergence, the difference |  X  D estimating the density ratio, various metrics are possible . Viewing g For the analysis, several assumptions are in order. First, a ssume that g from above and below: Next, the uniform norm of G Finally, on the bracket entropy of G [21]: For some 0 &lt;  X  &lt; 2 , The following is our main theoretical result, whose proof is given in Section 5: Theorem 2. (a) Under assumptions (8) , (9) and (10) , and letting  X  then under P : (b) If, in addition to (8) , (9) and (10) , there holds inf G expressed as an inner product g ( x ) =  X  w,  X ( x )  X  , where k g k simulation is the Gaussian kernel: Let G := H , and let the complexity measure be I ( g ) = k g k where { x function is extended take value  X  X  X  for negative arguments. Lemma 3. min  X  min Proof. Let  X  So, min implies the lemma immediately.
 If  X  w = 1  X  so the penalty term  X  mator for the KL divergence: our estimator when the true density ratio is not very smooth. A derivation similar to the previous case yields the followi ng convex program: Lemma 4. If  X  g 1 4 Proof. Define d R log g d l ( g 0 , g )  X  By the definition (6) of our estimator, we have: if
Z  X  g n + g 0
Z Let us now proceed to part (a) of the theorem. Define f Since f Apply Lemma 5.14 of [20] using distance metric d under Q (and so true under P as well, since d P /d Q is bounded from above), In the same vein, we obtain that under P measure: By condition (9), we have: d Combining Lemma 4 and Eqs. (15), (14), we obtain the followin g: 1 4  X  which implies, respectively, either Both scenarios conclude the proof if we set  X   X  1 which implies, respectively, either Both scenarios conclude the proof if we set  X   X  1 which implies that  X  h  X  O O
P (  X  n G mension has only a mild effect on our method.

