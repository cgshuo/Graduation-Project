 eas of speech and language processing. It provides efficient s olutions to seemingly intractable inference over exponentially-large spaces by sharing overl apping subproblems. Well-known examples of DP in our field include Viterbi and Forward-Backwa rd Algorithms for finite-state models, CKY and Earley Algorithms for context-free par sing, and A* Algorithm for both. These algorithms are widely used to solve problems ran ging from sequence labeling to word alignment to machine translation decoding.
 of DP from both theoretical and practical perspectives. In t he theory part, we try to unify various DP algorithms under a generic algebraic framework, where the above mentioned examples are merely special cases, and we can easily analyze their correctness and complex-ities. However, exact DP algorithms are often infeasible in practice due to time and space constraints. So in the practice part, we will survey several widely used tricks to reduce the size of the search space, including beam search, histogram p runing, coarse-to-fine search, and cube pruning. We will discuss these methods within the co ntext of state-of-the-art large-scale NLP systems. This tutorial is intended for researchers with any level of familiarity with dynamic program-ming. A basic understanding of the CKY Algorithm is recommen ded, but not required. Liang Huang is a Research Scientist at Google Research (Moun tain View). He recently obtained his PhD in 2008 from the University of Pennsylvania under Aravind Joshi and Kevin Knight (USC/ISI). His research interests include alg orithms in parsing and transla-tion, generic dynamic programming, and syntax-based machin e translation. His work on  X  X orest-based algorithms X  received an Outstanding Paper Aw ard at ACL 2008, as well as Best Paper Nominations at ACL 2007 and EMNLP 2008. He also lov es teaching and was a recipient of the University Teaching Prize at Penn.
