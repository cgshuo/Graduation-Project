 Maximizing the efficiency of data collection is important in any experimental setting. In neuro-physiology experiments, minimizing the number of trials needed to characterize a neural system is essential for maintaining the viability of a preparation and ensuring robust results. As a result, various approaches have been developed to optimize neurophysiology experiments online in order to choose the  X  X est X  stimuli given prior knowledge of the system and the observed history of the cell X  X  responses. The  X  X est X  stimulus can be defined a number of different ways depending on the experimental objectives. One reasonable choice, if we are interested in finding a neuron X  X   X  X referred stimulus, X  is the stimulus which maximizes the firing rate of the neuron [1, 2, 3, 4]. Alternatively, when investigating the coding properties of sensory cells it makes sense to define the optimal stim-ulus in terms of the mutual information between the stimulus and response [5].
 Here we take a system identification approach: we define the optimal stimulus as the one which tells us the most about how a neural system responds to its inputs [6, 7]. We consider neural systems in vector of parameters ~  X  . Since we estimate these parameters from experimental trials, we want to choose our stimuli so as to minimize the number of trials needed to robustly estimate ~  X  . Two inconvenient facts make it difficult to realize this goal in a computationally efficient manner: 1) model complexity  X  we typically need a large number of parameters to accurately model a system X  X  responses to stimuli ~x t which are themselves very high-dimensional (e.g., spatiotemporal movies if we are dealing with visual neurons). In particular, it is computationally challenging to 1) update our and 2) find the optimal stimulus quickly enough to be useful in an online experimental context. In this work we present methods for solving these problems using generalized linear models (GLM) posterior distribution of the model parameters. Our emphasis is on finding solutions which scale well in high dimensions. We solve problem (1) by using efficient rank-one update methods to update the Gaussian approximation to the posterior, and problem (2) by a reduction to a highly tractable one-dimensional optimization problem. Simulation results show that the resulting algorithm produces a set of stimulus-response pairs which is much more informative than the set produced by random sampling. Moreover, the algorithm is efficient enough that it could feasibly run in real-time. Neural systems are highly adaptive and more generally nonstatic. A robust approach to opti-mal experimental design must be able to cope with changes in ~  X  . We emphasize that the model framework analyzed here can account for three key types of changes: stimulus adaptation, spike rate adaptation, and random non-systematic changes. Adaptation which is completely stimu-lus dependent can be accounted for by including enough stimulus history terms in the model p ( r t |{ ~x t , ..., ~x t  X  t history-dependent effects, are accounted for explicitly in the model (1) below. Finally, we con-sider slow, non-systematic changes which could potentially be due to changes in the health, arousal, or attentive state of the preparation.
 Methods We model a neuron as a point process whose conditional intensity function (instantaneous firing rate) is given as the output of a generalized linear model (GLM) [8, 9]. This model class has been discussed extensively elsewhere; briefly, this class is fairly natural from a physiological point of view [10], with close connections to biophysical models such as the integrate-and-fire cell [9], and has been applied in a wide variety of experimental settings [11, 12, 13, 14]. The model is summarized as: In the above summation the filter coefficients k i,t  X  l capture the dependence of the neuron X  X  instan-taneous firing rate  X  t on the i th component of the vector stimulus at time t  X  l , ~x t  X  l ; the model therefore allows for spatiotemporal receptive fields. For convenience, we arrange all the stimulus coefficients in a vector, ~ k , which allows for a uniform treatment of the spatial and temporal compo-nents of the receptive field. The coefficients a j model the dependence on the observed recent activity r at time t  X  j (these terms may reflect e.g. refractory effects, burstiness, firing-rate adaptation, etc., depending on the value of the vector ~a [9]). For convenience we denote the unknown parameter vector as ~  X  = { ~ k ; ~a } .
 The experimental objective is the estimation of the unknown filter coefficients, ~  X  , given knowledge of the stimuli, ~x t , and the resulting responses r t . We chose the nonlinear stage of the GLM, the link function f () , to be the exponential function for simplicity. This choice ensures that the log likelihood of the observed data is a concave function of ~  X  [9].
 Representing and updating the posterior. As emphasized above, our first key task is to efficiently this problem, we approximate this posterior as a Gaussian; this approximation may be justified by the fact that the posterior is the product of two smooth, log-concave terms, the GLM likelihood function and the prior (which we assume to be Gaussian, for simplicity). Furthermore, the main theorem of [7] indicates that a Gaussian approximation of the posterior will be asymptotically accurate. We use a Laplace approximation to construct the Gaussian approximation of the posterior, p ( ~  X  t | ~x t , r t ) : we set ~ X  t to the peak of the posterior (i.e. the maximum a posteriori (MAP) esti-mate of ~  X  ), and the covariance matrix C t to the negative inverse of the Hessian of the log posterior at ~ X  t . In general, computing these terms directly requires O ( td 2 + d 3 ) time (where d = dim( ~  X  ) ; the time-complexity increases with t because to compute the posterior we must form a product of t likelihood terms, and the d 3 term is due to the inverse of the Hessian matrix), which is unfortunately too slow when t or d becomes large.
 ters, we use Bayes to write out the posterior: Now, to update  X  t we only need to find the peak of a one-dimensional function (as opposed to a d -dimensional function); this follows by noting that that the likelihood only varies along a single time. Moreover, from the second derivative term above it is clear that computing C t requires just a rank-one matrix update of C t  X  1 , which can be evaluated in O ( d 2 ) time via the Woodbury matrix our simulations (data not shown) showed that, despite this improved efficiency, the loss in accuracy due to this approximation was minimal.
 Deriving the (approximately) optimal stimulus. To simplify the derivation of our maximization strategy, we start by considering models in which the firing rate does not depend on past spiking, so ~  X  = { ~ k } . To choose the optimal stimulus for trial t +1 , we want to maximize the conditional mutual information with respect to the stimulus ~x t +1 . The first term does not depend on ~x t +1 , so maximizing the information requires minimizing the conditional entropy H ( ~  X  | ~x t +1 , r t +1 ) =
X We do not average the entropy of p ( ~  X  | r t +1 , ~x t +1 ) over ~x t +1 because we are only interested in the conditional entropy for the particular ~x t +1 which will be presented next. The equality above is due to our Gaussian approximation of p ( ~  X  | ~x t +1 , r t +1 ) . Therefore, we need to minimize E sian of the log-posterior, we have: J obs is the observed Fisher information.
 Here we use the fact that for the GLM, the likelihood depends only on the dot product,  X  = ~x t t +1 ~  X  . We can use the Woodbury lemma to evaluate the inverse: where D ( r t +1 ,  X  ) =  X  2 log p ( r t +1 |  X  ) / X  X  2 . Using some basic matrix identities, Ignoring the higher order terms, we need to minimize E r Gaussian p ( ~  X  | ~x t , r t ) to evaluate this expectation. After some algebra, we find that to maximize I ( ~  X  ; r t +1 | ~x t +1 , ~x t , r t ) , we need to maximize Computing the optimal stimulus. For the GLM the most informative stimulus is undefined, since increasing the stimulus power || ~x t +1 || 2 increases the informativeness of any putatively  X  X ptimal X  stimulus. To obtain a well-posed problem, we optimize the stimulus under the usual power con-straint || ~x t +1 || 2  X  e &lt;  X  . We maximize Eqn. 11 under this constraint using Lagrange multipliers and an eigendecomposition to reduce our original d -dimensional optimization problem to a one-dimensional problem. Expressing Eqn. 11 in terms of the eigenvectors of C t yields: where u i and y i represent the projection of ~ X  t and ~x t +1 onto the i th eigenvector and c i is the cor-responding eigenvalue. To simplify notation we also introduce the functions g () and h () which are monotonically strictly increasing functions implicitly defined by Eqn. 12. We maximize F ( ~x t +1 ) by breaking the problem into an inner and outer problem by fixing the value of P i u i y i and max-imizing h () subject to that constraint. A single line search over all possible values of P i u i y i will then find the global maximum of F ( . ) . This approach is summarized by the equation: Since h () is increasing, to solve the inner problem we only need to solve: This last expression is a quadratic function with quadratic and linear constraints and we can solve it using the Lagrange method for constrained optimization. The result is an explicit system of equations for the optimal y i as a function of the Lagrange multiplier  X  1 . Thus to find the global optimum we simply vary  X  1 (this is equivalent to performing a search over b ), and compute the corresponding ~y (  X  1 ) . For each value of  X  1 we compute F ( ~y (  X  1 )) and choose the stimulus ~y (  X  1 ) which maximizes F () . It is possible to show (details omitted) that the maximum of F () must occur on the interval  X  1  X  c 0 , where c 0 is the largest eigenvalue. This restriction on the optimal  X  1 makes the implementation of the linesearch significantly faster and more stable. To summarize, updating the posterior and finding the optimal stimulus requires three steps: 1) a rank-one matrix update and one-dimensional search to compute  X  t and C t ; 2) an eigendecomposition of C ; 3) a one-dimensional search over  X  1  X  c 0 to compute the optimal stimulus. The most expensive step here is the eigendecomposition of C t ; in principle this step is O ( d 3 ) , while the other steps, as quite useful: recall that in this setting C t is just a rank-one modification of C t  X  1 , and there exist efficient algorithms for rank-one eigendecomposition updates [15]. While the worst-case running time of this rank-one modification of the eigendecomposition is still O ( d 3 ) , we found the average running time in our case to be O ( d 2 ) (Fig. 1(c)), due to deflation which reduces the cost of matrix multiplications associated with finding the eigenvectors of repeated eigenvalues. Therefore the total time complexity of our algorithm is empirically O ( d 2 ) on average.
 Spike history terms. The preceding derivation ignored the spike-history components of the GLM model; that is, we fixed ~a = 0 in equation (1). Incorporating spike history terms only affects the optimization step of our algorithm; updating the posterior of ~  X  = { ~ k ; ~a } proceeds exactly as before. The derivation of the optimization strategy proceeds in a similar fashion and leads to an analogous optimization strategy, albeit with a few slight differences in detail which we omit due to space constraints. The main difference is that instead of maximizing the quadratic expression in Eqn. 14 to find the maximum of h () , we need to maximize a quadratic expression which includes a linear term due to the correlation between the stimulus coefficients, ~ k , and the spike history coefficients, ~a . The results of our simulations with spike history terms are shown in Fig. 2.
 Dynamic ~  X  . In addition to fast changes due to adaptation and spike-history effects, animal prepara-tions often change slowly and nonsystematically over the course of an experiment [16]. We model these effects by letting ~  X  experience diffusion: Here w t is a normally distributed random variable with mean zero and known covariance matrix Q . This means that p ( ~  X  t +1 | ~x t , r t ) is Gaussian with mean ~ X  t and covariance C t + Q . To update the posterior and choose the optimal stimulus, we use the same procedure as described above 1 . Results Our first simulation considered the use of our algorithm for learning the receptive field of a visually sensitive neuron. We took the neuron X  X  receptive field to be a Gabor function, as a proxy model of a V1 simple cell. We generated synthetic responses by sampling Eqn. 1 with ~  X  set to a 25x33 Gabor function. We used this synthetic data to compare how well ~  X  could be estimated using information maximizing stimuli compared to using random stimuli. The stimuli were 2-d images which were rasterized in order to express ~x as a vector. The plots of the posterior means ~ X  t in Fig. 1 (recall these are equivalent to the MAP estimate of ~  X  ) show that the information maximizing strategy converges an order of magnitude more rapidly to the true ~  X  . These results are supported by the conclusion of [7] that the information maximization strategy is asymptotically never worse than using random stimuli and is in general more efficient.
 The running time for each step of the algorithm as a function of the dimensionality of ~  X  is plotted in Fig. 1(c). These results were obtained on a machine with a dual core Intel 2.80GHz XEON processor running Matlab. The solid lines indicate fitted polynomials of degree 1 for the 1d line search and degree 2 for the remaining curves; the total running time for each trial scaled as O ( d 2 ) , as predicted. When ~  X  was less than 200 dimensions, the total running time was roughly 50 ms (and for dim( ~  X  )  X  100 , the runtime was close to 15 ms), well within the range of tolerable latencies for many experiments.
 In Fig. 2 we apply our algorithm to characterize the receptive field of a neuron whose response depends on its past spiking. Here, the stimulus coefficients ~ k were chosen to follow a sine-wave; the spike history coefficients ~a were inhibitory and followed an exponential function. When choos-ing stimuli we updated the posterior for the full ~  X  = { ~ k ; ~a } simultaneously and maximized the information about both the stimulus coefficients and the spike history coefficients. The informa-tion maximizing strategy outperformed random sampling for estimating both the spike history and stimulus coefficients.
 Our final set of results, Fig. 3, considers a neuron whose receptive field drifts non-systematically with time. We take the receptive field to be a Gabor function whose center moves according to a random walk (we have in mind a slow random drift of eye position during a visual experiment). The results demonstrate the feasibility of the information-maximization strategy in the presence of non-stationary response properties ~  X  , and emphasize the superiority of adaptive methods in this context. Conclusion We have developed an efficient implementation of an algorithm for online optimization of neuro-physiology experiments based on information-theoretic criterion. Reasonable approximations based on a GLM framework allow the algorithm to run in near-real time even for high dimensional pa-rameter and stimulus spaces, and in the presence of spike-rate adaptation and time-varying neural response properties. Despite these approximations the algorithm consistently provides significant improvements over random sampling; indeed, the differences in efficiency are large enough that the information-optimization strategy may permit robust system identification in cases where it is simply not otherwise feasible to estimate the neuron X  X  parameters using random stimuli. Thus, in a sense, the proposed stimulus-optimization technique significantly extends the reach and power of classical neurophysiology methods.
 Acknowledgments JL is supported by the Computational Science Graduate Fellowship Program administered by the DOE under contract DE-FG02-97ER25308 and by the NSF IGERT Program in Hybrid Neural Mi-crosystems at Georgia Tech via grant number DGE-0333411. LP is supported by grant EY018003 from the NEI and by a Gatsby Foundation Pilot Grant. We thank P. Latham for helpful conversa-tions.
 [1] I. Nelken, et al. , Hearing Research 72 , 237 (1994). [2] P. Foldiak, Neurocomputing 38 X 40 , 1217 (2001). [3] K. Zhang, et al. , Proceedings (Computational and Systems Neuroscience Meeting, 2004). [4] R. C. deCharms, et al. , Science 280 , 1439 (1998). [5] C. Machens, et al. , Neuron 47 , 447 (2005). [6] A. Watson, et al. , Perception and Psychophysics 33 , 113 (1983). [7] L. Paninski, Neural Computation 17 , 1480 (2005). [8] P. McCullagh, et al. , Generalized linear models (Chapman and Hall, London, 1989). [9] L. Paninski, Network: Computation in Neural Systems 15 , 243 (2004). [10] E. Simoncelli, et al. , The Cognitive Neurosciences , M. Gazzaniga, ed. (MIT Press, 2004), third [11] P. Dayan, et al. , Theoretical Neuroscience (MIT Press, 2001). [12] E. Chichilnisky, Network: Computation in Neural Systems 12 , 199 (2001). [13] F. Theunissen, et al. , Network: Computation in Neural Systems 12 , 289 (2001). [14] L. Paninski, et al. , Journal of Neuroscience 24 , 8551 (2004). [15] M. Gu, et al. , SIAM Journal on Matrix Analysis and Applications 15 , 1266 (1994). [16] N. A. Lesica, et al. , IEEE Trans. On Neural Systems And Rehabilitation Engineering 13 , 194
