 TAESUN MOON and KATRIN ERK, The University of Texas at Austin Word Sense Disambiguation (WSD) is one of the oldest problems in computational linguistics [Weaver 1949] and still remains challenging today. State-of-the-art perfor-mance on WSD for WordNet senses is at only around 70 X 80% accuracy [Edmonds and Cotton 2001; Mihalcea et al. 2004a]. In WSD, polysemy is typically modeled through a list of dictionary senses thought to be mutually disjoint, such that each occurrence of a word is characterized through one best-fitting dictionary sense. However, the underly-ing assumption that each word has clear, disjoint senses has been drawn into question by linguists, lexicographers, and psychologists [Tuggy 1993; Cruse 1995; Kilgarriff 1997; Hanks 2000; Kintsch 2007]. The difficulty of doing WSD, together with these more fundamental issues, leads to the question of whether it may be useful to consider alternative computational models of word meaning that do not represent a word in-stance through a single best sense. Supporting evidence for this comes from a recent annotation study [Erk et al. 2009] in which annotators rated the applicability of each sense using a graded scale, rather than choosing only a single sense. The study found that in almost 80% of the sentences, annotators marked multiple senses as applying to some degree, and it also found that annotators made use of the intermediate points on the graded scale, rather than giving binary judgments. We take this to indicate that word meaning may be better represented through dynamic, context-dependent rep-resentations for each individual instance. This can be achieved straightforwardly by representing each instance through weighted features or through a distribution over features or senses.

There have recently been several models of word meaning in context that fit this description. We will call these models word usage models (where a usage is a word occurrence in a particular context). Most existing usage models compute individual representations for each word occurrence as points in vector space [Mitchell and Lapata 2008; Erk and Pad  X  o 2008, 2010; Thater et al. 2010]. In these models, the computation of usage vectors typically proceeds in two steps. The first step computes one context-independent vector per word, conflating all its usages. For example, the vector of charge.n would include contexts where charge.n means  X  X riminal charges X ,  X  X riticism X , and  X  X lectric charge X . In the second step, the context-independent vector for a word is combined with those of other words in the sentential context through an operation such as element-wise products or sums. The resulting vectors are generally different for each usage. Dinu and Lapata [2010] introduce a model that computes usage representations as distributions over latent senses rather than through a us-age vector. In contrast to WSD systems, all these usage representations do not refer to dictionary senses to characterize meaning. Thater et al. [2010] introduce the term contextualization for the process of computing a usage representation. We will adopt this term for this article as it is more appropriate here than the term  X  X isambiguation, X  which has an established interpretation of choosing a single best sense.

McCarthy and Navigli [2009] propose viewing word meaning in context as lists of weighted paraphrases (substitutes). Figure 1 shows two example sentences from their Lexical Substitution task at SemEval 2007, both for the target lemma charge.n . The right-hand column contains the gold substitutes, with the numbers indicating the num-ber of annotators that chose a particular substitute. So the gold weights for sentence 1812, for example, are 2 for accusation and allegation , and 1 for offence and indictment . The two instances of charge.n are clearly different, as is reflected in the lists of sub-stitutes: Sentence 1812 is talking about criminal charges, with paraphrases of offence and indictment , while 1813 is about criticism . On the other hand, the two instances are clearly related, as is shown by their overlap in the more highly weighted paraphrases accusation and allegation . So like the usage vector models, this account of word mean-ing 1 does not posit disjoint senses, as the paraphrase weights can be different for each individual usage. There is a close relation between the usage models described before and the McCarthy and Navigli proposal. Existing usage representations are usually not in themselves interpretable, so they are converted to weighted lists of paraphrases for evaluation, and evaluated against paraphrasing datasets like the Lexical Substitution dataset (Erk and Pad  X  o [2008] and subsequent approaches).
In this article we introduce a usage model that takes up and extends the McCarthy and Navigli proposal by representing usage meaning as a probability distribution over potential paraphrases. One advantage of this is that our model produces contextual-izations that are immediately interpretable, without the use of a landmark for inter-pretation. One possible disadvantage is that our model needs paraphrase candidates to be able to produce a representation; however, sets of distributionally similar words could in principle be used instead. We frame contextualization as an inference prob-lem, using undirected graphical models to infer the paraphrase distribution for every content word in a given sentence. Graphical models represent complex probability dis-tributions through a graph. In the graph, nodes stand for random variables, and edges stand for direct probabilistic interactions between them. The lack of edges between any two variables reflects independence assumptions. In our model, we represent each content word of the sentence through two adjacent nodes: the observed node represents the surface form of the word itself, and the hidden node represents its usage meaning. The distribution over values that we infer for the hidden node is a paraphrase dis-tribution for the observed word. To encode the fact that contextualizing information is exchanged between syntactic neighbors, the graph contains edges that mirror the dependency graph for the sentence.

Evaluated on a paraphrasing task, our model outperforms the current state-of-the-art usage vector model [Thater et al. 2010] on all parts of speech except verbs, where the previous model wins by a small margin.

Because it is based on undirected graphical models, we can easily generate variants of our model to explore questions about usage meaning computation. We ask whether the model benefits from the use of syntax, or whether it will work equally well when the edges reflect surface order rather than dependency relations. We find that using syntax instead of surface order leads to a considerable gain in performance for our model.
Another question that we investigate is whether the distribution of a hidden node is better inferred from observed neighbors or from neighboring hidden nodes. When inference uses neighboring hidden nodes, the meaning of a word in context is computed from the contextualized meanings of its neighbors. This is new, as all existing usage models compute contextualizations based on context-independent representations of neighbors. However, it is not necessarily the case that the use of contextualized neigh-bors will always be an advantage. There are cases where observed words should be better evidence than than their paraphrase distributions, namely when we want to capture collocational information. For example, in take long , the paraphrases of long should be irrelevant or maybe even harmful for computing a paraphrase distribution for take because take long is a collocation. We find that the use of collocational infor-mation (inference from observed neighbors) is crucial for model performance. At the same time, the use of observed words along with hidden nodes, combining collocational information with contextualized meaning of neighbors, achieves the best performance, if by a small margin.

Further knowledge sources X  X esides syntactic (or surface) neighbors X  X hat influence the hidden nodes can be added simply as further edges in the model. We test whether document topic as an additional knowledge source helps contextualization, implement-ing document topic through a topic model. In our evaluation, document topic does not improve contextualization performance, but this may be due to the particular lemmas in our test set.

The model combines information from multiple syntactic neighbors in a standard way by multiplying factors. Neighbors that furnish less evidence, in that they do not have strong preferences among the paraphrases of the target, have less influence on the target X  X  paraphrase distribution. Existing syntax-based usage models focus on only one neighbor at a time [Mitchell and Lapata 2010], find no improvement from using multiple neighbors [Erk and Pad  X  o 2009], or keep different argument positions in sepa-rate subspaces, such that they never interact [Thater et al. 2010]. In our model we find that the presence of multiple syntactic neighbors has a positive effect on performance for verbs and nouns, but not adjectives and adverbs, which makes sense, since the latter typically only have one syntactic neighbor.
Word usage models. Instead of assigning each usage a single best dictionary sense, word usage models compute representations that can be distinct for each usage. Most existing usage models do this by representing word usages as points in vector space. The simplest such model computes the meaning of a word w in a context c (which may consist of multiple words) by summing up the vectors for w and c [Landauer and Dumais 1997]. Kintsch [2001] computes a representation for a predicate w in the con-text of an argument a by determining the near neighbors of w that are most compatible with a and computing their centroid. Mitchell and Lapata [2008] (in the following), propose a general framework for semantic composition through vector combination that combines the vectors u , v for two constituents in a given syntactic relation and context. The models evaluated in the paper, however, disregard syntactic relation and context, and instantiate vector combination as either addition (yielding the Landauer and Dumais model) or component-wise multiplication. Mitchell and Lapata find su-perior performance for component-wise multiplication. Erk and Pad  X  o [2008] (in the following) propose a model in which a pair of syntactic neighbors mutually contextual-ize each other using selectional preference vectors. For example, in the phrase address the class , address.v and class.n mutually contextualize each other. The verb address.v is associated with a vector that describes typical direct objects (computed by summing over vectors of observed direct objects of address.v in a parsed corpus), and conversely class.n is associated with a vector that describes predicates that typically take class.n as an object. The usage vector for class.n is then computed by combining the context-independent vector for class.n with the direct object preference vector of address.v ,and conversely for the contextualization of address.v . Unfortunately the approach does not scale to the case of more than a single syntactic neighbor: Erk and Pad  X  o [2009] report that using multiple syntactic neighbors for contextualization does not improve perfor-mance of this model. Thater et al. [2009, 2010] (in what follows) also use selectional preferences for contextualization. They represent each word through two vectors. The first-order vector for a word w has dimensions REL , v for cooccurrence of w with v in syntactic relation REL . For example, address.v could have a dimension OBJ , problem.n showing cooccurrence of address.n with problem.n as direct object. The second-order vector for a word w consists of separate subvectors for each dependency relation REL . The subvector for REL is a combination of first-order vectors of REL -neighbors of w , similar to the selectional preference vectors of EP08. To compute a usage vector for address.v in the teacher addressed the class , the TFP10 model modifies the second-order vector of address.v by combining its SUBJ -subvector with the first-order vector for teacher.n ,and combining its OBJ -subvector with the first-order vector for class.n . This is the model if w is a verb or noun. For adjectives and adverbs, TFP10 use a better-performing variant which computes the usage vector for the headword of w in the dependency graph as the usage vector of w . This step improves performance, but having the meaning of a word be the meaning of its headword is hard to interpret.

In addition to these syntax-based usage models, there have been a number of usage models that represent context as a bag of words. Erk and Pad  X  o [2010] (in the following) compute usage vectors by picking most similar seen instances. Reisinger and Mooney [2010] induce word senses by clustering on bag-of-words contexts. They do not induce contextualized representations as such, but they compute distance to an occurrence as weighted average distance to the induced senses. Dinu and Lapata [2010] (in the following) use distributions over latent senses as contextualizations. Latent senses are computed using LDA or nonnegative matrix factorization over bag-of-words context.

Like existing usage models, our model computes an individual representation for each usage. We represent meaning in context not through usage vectors or latent senses, which are not in themselves interpretable. Rather we represent meaning in context directly as a distribution over paraphrases. Among the models discussed be-fore, the ones most closely related to our model are EP08 and TFP10, which both use selectional preferences for contextualization, as direct syntactic or surface neighbor-hood is the main source that our model uses for inference.

Paraphrases and inference rules. McCarthy and Navigli [2009] proposed repre-senting word usages through weighted paraphrases (see Figure 1). In the Lexical Substitution (in the following) dataset that they introduced for the 2007 SemEval task, each paraphrase is weighted by the number of annotators who proposed it. In the L EX S UB annotation, average pairwise inter-annotator agreement was 27.75%, and agreement with the most frequent paraphrase, when it existed (which was the case in 73.93% of the sentences), was at 50.67%. The LexSub dataset focuses on paraphrases for single words. In contrast, approaches to learning paraphrases from text usually consider both single-word and multiword paraphrases (e.g., Bannard and Callison-Burch [2005] and Barzilay and McKeown [2001]). Approaches to learning inference rules from text consider not only (single-and multiword) paraphrases but also other types of rules, such as enablement (fight  X  win) and happens-before (buy  X  own) [Lin and Pantel 2001; Chklovski and Pantel 2004; Szpektor and Dagan 2008; Berant et al. 2010]. A related task is to determine the applicability of an inference rule in a given sentential context [Pantel et al. 2007; Szpektor et al. 2008; Poon and Domingos 2009; Ritter et al. 2010]. Approaches to this problem use similarity in selectional preferences as well as similarity in sentence context to determine whether an inference rule applies in a given context.

In this article, we focus on the problem of deriving usage representations for in-dividual target words, so we do not address multiword paraphrases or more general inferences. However, employing usage representations to test inference rule applica-bility for both single-word and multiword expressions is an interesting problem that we plan to address in the future.

Vector space models for larger expressions. Approaches that derive vector space rep-resentations for whole phrases [Smolensky 1990; Mitchell and Lapata 2008, 2010; Baroni and Zamparelli 2010; Grefenstette and Sadrzadeh 2011] explore how to encode syntactic structure in a vector, and how to model phrase similarity. Vector space models for larger expressions have sometimes been used as usage vector models. For example, a vector for the phrase address class can also be used as a vector for address.v in the con-text of class.n . In fact, the Mitchell and Lapata [2008] model is a phrase model, but has been evaluated against word usage models. The model that we present in this article derives a separate representation for each word in context, rather than a joint represen-tation for a phrase. It is thus a word usage model, but not a model for larger expressions. All-words word sense disambiguation. Our approach has similarities to all-words WSD approaches, which typically disambiguate all words in a sentence at the same time and in relation to each other, usually with little or no training data [Snyder and Palmer 2004; Mihalcea et al. 2004b]. Similarly, our approach can be viewed as an all-words paraphrasing model. Among the all-words WSD approaches, the model of Nastase [2008] is most closely related to ours. In the model, words that are neighbors in a dependency graph mutually disambiguate each other using word sense relatedness scores determined through a heuristic. Preferred senses are computed in two passes through the dependency graph (one top-down, one bottom-up). The setting that we use allows us to use a more principled solution for inference using loopy belief propagation, in which information is passed through the graphical model until convergence. Note that we cannot use all-words WSD datasets for evaluation for our model, as they are labeled with a single best sense for each word, while our aim is to encode meaning as a distribution over sets of paraphrases.

Graphical models in computational linguistics. In computational linguistics, undi-rected graphical models have mainly been used in the shape of conditional random fields [Lafferty et al. 2001; Sutton et al. 2007] and Markov Logic Networks [Riedel and Meza-Ruiz 2008; Yoshikawa et al. 2009; Poon and Domingos 2009]. Such models have also occasionally been used for structural tasks such as morphology-based word generation [Dreyer and Eisner 2009], noun-phrase chunking [Sutton et al. 2007], and dependency parsing [Smith and Eisner 2008]. Directed graphical models have seen much more use in computational linguistics (e.g., topic models for semantics or HMMs for low-level syntax). In the context of modeling word meaning, Brody and Lapata [2009] use topic models for sense induction. They rely mainly on context word and word n-gram features, finding dependency features to be very sparse. Deschacht and Moens [2009] define a language model as a Hidden Markov Model in which observed words are generated by hidden variables ranging over the whole vocabulary. We can-not directly compare to either of those models: The Brody and Lapata model cannot be mapped to paraphrases in any straightforward way, and the Deschacht and Moens model does not constrain the hidden word to be a paraphrase. We will evaluate a variant of the Deschacht and Moens model that only considers paraphrases (in what follows called the sequential model). The model that we introduce is a usage model of word meaning, where each word representation is context dependent and inferred dynamically. We represent each usage as a distribution over potential paraphrases, and we define the task of computing a usage representation as a probabilistic inference task over graphs. We examine several probabilistic models to investigate how different knowledge sources and graph topologies affect predicted word meaning.

Though all the models we investigate have slightly different graphs, they share a common foundational node-node pair. As the building block for all our models, we con-struct two adjacent nodes for each content word of the sentence: one node (the observed node) represents the surface form of the word itself and the other node (the hidden node) represents its usage meaning. We call the distribution inferred over the hidden node given the evidence from its observed context the paraphrase distribution of the ob-served word. The observed context may include evidence from the word X  X  heads and dependencies, its left and right context, or the entire document. The integration of such knowledge sources is accomplished in a standard way by summing over all hid-den variables and multiplying the corresponding factors. This process of integration is inference, and contextualization is the inference of paraphrase distributions.
Directed graphical models express causality between nodes through the direction of the edges. We do not want to make such assumptions of causality. Instead we want the flow of information between nodes to go in all directions, so we use undirected graphical models instead. Figure 2(a) shows a simple undirected graphical model, with direct interactions between the nodes A , B ,and C . However, undirected graphical models (also known as Markov random fields) are ambiguous in the interactions that they describe: The model in Figure 2(a) could (among other things) describe a three-way interaction between A , B ,and C , or three two-way interactions between pairs of nodes. Figures 2(b) and 2(c) make those interpretations explicit in two factor graphs [Kschischang et al. 2001]. Briefly, a factor graph is a bipartite graph over two types of nodes, nodes that correspond to variables (circles) and nodes that correspond to factors (filled squares). A factor is a function whose arguments are the variable nodes adjacent to the factor node. The factor graph as a whole represents the product of all the factors in it. In what follows, we use factor graphs to define our models.

We now give an example of using factor graphs for inference when the context is a dependency parse. The left-hand side of Figure 3 shows the dependency parse for the sentence  X  X he girl caught the ball X , lemmatized and trimmed of function words. (We write subject cg to indicate that g = girl is the subject of c = catch, and analogously for object cb .) The right-hand side shows the factor graph to which we map the dependency graph. Each content word from the sentence is associated with a paraphrase node ,a hidden variable whose values range over the whole vocabulary. For example, the para-phrase node associated with catch is m c . Each content word is additionally associated with a unary factor, the word factor , representing the observed word. For catch this is f w c . This factor is linked to m c to model the influence of the observed word on the paraphrase distribution. The values of m c are restricted to the set of valid paraphrases for w c = catch, and which are derived from external knowledge sources. For each de-pendency edge we insert a binary factor, a selectional factor . For example, the subject cg edge becomes a factor f s cg linking m c and m g . It models the influence of selectional preferences on mutually contextualizing catch and girl : To what extent is each value of m c compatible with each value of m g ? Overall, for the random variables m c , m g , m b the graph in Figure 3 represents the following factorization of the global function F . In this article, we do not learn the parameters for factors like f w c , f s cg using EM. Instead, we determine parameters using a simple count-based approach described in Section 3.3, based on the assumption that interactions between hidden values (for example, the value get of m c and the value sphere of its neighbor m b ) follow the same parameters as interactions between observed words (here, get with direct object sphere ). Therefore, while there are many similarities that may be found between our model and popular graphical models with hidden state spaces such as HMMs, they differ in this respect. An important rationale for HMMs is to learn a set of parameters that maximize the likelihood and then find a state sequence that best explains some observation. Neither of these are our goals.

Marginal inference. The problem under study is an instance of the more general problem of marginal inference, which can be posed as follows. Given a set of random variables m and some measure of its information F s ( m ) in relation to another set of random variables s , what do we know about a specific random variable m i  X  m solely in relation to s ? The answer to this question is obtained by marginalizing out (i.e., summing over) the random variables that are not m i .
 The left-hand side of this equation is known as the marginal of m i and such functions as a whole in relation to F s ( m ) (called the global function) are referred to as marginals. In this most general form, this is an intractable problem, but if some random variables are independent of each other or are assumed to be independent, the ordering of the summation can be carefully arranged so as to make this pliable. This is a result of the very simple fact that multiplication distributes over addition [Aji and McEliece 2000]. Graphical representations of these independence assumptions such as factor graphs help formalize and visualize the models that derive from them. To conduct inference, we take advantage of the structural information in the sentence at hand. So we do not posit a bag-of-words usage model, but follow Mitchell and Lapata [2008] and Erk and Pad  X  o [2008] in assuming that the meaning of a word is influenced by the structure of the sentence that contains it. We consider two types of evidence from sentence structure, left-to-right sequential information and dependency parses. (Figure 3 illustrates the latter.)
In this subsection, we first discuss the different ways in which we may transform observations at the sentence level into probabilistic graphical models. We then expand our notion of evidence to include information at the document level. 3.1.1. Graph Transformations for Sentences. Here, we introduce four different ways of look-ing at a sentence. Each uses a different transformation of sentences which emphasize different aspects of how sentences are structured. The first emphasizes information flow between hidden units, the second uses collocation information only, and the third is a combination of both. The final is a simplified version that uses left-to-right local context. When the transformations are viewed as models, we call them model variants.
The same preprocessing is conducted for all model variants. All words are POS-tagged and lemmatized. All function words are removed, except for prepositions, which are transformed to edge labels. We leave out notation for POS-tags in the examples that follow to simplify presentation.

As a shared running example, we illustrate all models with the sentence:  X  X he girl caught the ball. X 
Paraphrase transformation (pt) . This transformation of a dependency parse gen-erates a graph like the one in Figure 3, where edges between content words are established only for the hidden nodes. This models the assumption that semantic information in a sentence flows through a dedicated layer that is not observed, but mirrors the structure of the observed dependency parse.

We now formalize this transformation. Let M be the set of all words. Let G be a dependency graph with nodes V G , directed edges E G , and set of edge labels R G . We map it to a factor graph as follows. For each i  X  V G , we create a paraphrase node m the fact that dependency relations are directed and asymmetric. Each paraphrase node m i is connected to the word factor f w i . Also, each selectional factor f r ij is connected to the paraphrase nodes m i and m j .

In sum, the paraphrase transformation of a dependency graph results in the following factorization of the global function , the global joint distribution over paraphrase nodes where G is some dependency parse and m is a set of paraphrase nodes. Eq. (1) is an instance of this global function for the graph in Figure 3.

Adjacency transformation ( at ) . In the pt model, the paraphrase distribution of a word is inferred from the paraphrase distributions of its neighbors. That is, we contextualize each word before using it as evidence for inference. However, in collocations like take long , the words themselves, rather than their paraphrases, may be the best predictors. To test this hypothesis, we use a transformation where the paraphrase distribution of a word is inferred from the neighboring observed words. Figure 4 shows an example, again for girl catch ball . The adjacency transformation converts dependency graphs into a set of disconnected component graphs with one paraphrase distribution in each component. Instead of the selectional factors of pt , at contains unary word selectional factors like f w w C = catch as subject.

The adjacency transformation of a dependency graph results in the following factor-ization of the global function where d i is the set of nodes that are dependents of node i in the dependency graph, and h i are nodes that are heads of node i .

Paraphrase + adjacency transformation (pat) . This is a combination of the paraphrase transformation and the adjacency transformation. It models both collocational strength of adjacent observations (i.e., at ) as well as generalized information from the entire sentence ( pt ). The example transformation of girl catch ball is given in Figure 5.
The paraphrase + adjacency transformation of a dependency graph results in the following factorization of the global function.

Sequential transformation ( seq ) . The three model variants given before posit depen-dency parsed syntax as the principal arbiter of meaning. However, a far simpler model can be posited which follows a minimal notion of syntax: a right branching grammar. Combined with the assumption that information is local, we propose a model variant called the sequential transformation where a paraphrase node only relies on the word to its left (should it exist), its own observation, and the word to its right (should it exist). The factorization of the global function is a simplified version of the one in the at variant given earlier. There is no paraphrase node with degree greater than three, and the sets of heads and dependents have at most one element. 3.1.2. Wider Document Context. We include evidence on wider document context through a topic model [Blei et al. 2003]. Given a document D , the topic model defines a document-specific distribution over topics  X  f D ( z ) and a distribution over words given atopic,  X  f T ( m , z ). By marginalizing over z , we characterize the likelihood of each para-phrase candidate given the document and thus define a unary factor for a paraphrase distribution. All preceding graph transformations can be augmented with this unary factor. For example, the nodes m p , m r , m b in the transformations of  X  X layer run ball X  can be linked to the additional document factor f D . Such a joint model incorporates lexical and syntactic evidence from the local sentence as well as topical evidence from the global document context. In graphs that are trees or polytrees, the sum-product algorithm can be used for in-ference. However, some dependency parsers (including the one that we use) generate graphs that are not polytrees, so we assume that the graphical models over which we conduct inference may contain loops. 2 Therefore, we use loopy belief propagation [Murphy et al. 1999] to approximate marginals. For graphs free of loops, loopy BP will converge to the correct marginal, and for graphs with loops the algorithm is known to perform well in practice [Weiss 2000].

Because it is not possible to perform exact inference of the marginal for m i (Eq. (2)) given a transformed dependency parse with loops, loopy BP instead approximates the marginal of m i at some iteration t + 1 based only on the values of the approximate marginals of its neighbors from the previous iteration t . In the sequence, we indicate this approximate, loopy marginal at iteration t by F ( t ) ( m i ), dropping the subscript from F
G for notational clarity. We will simply call this  X  X he marginal X  in what follows, but it should not be understood as an exact marginal.

Because of the variety of our models, we present the loopy BP update formula for the most specific model, pat + lda . The update equations for all other models can be derived from this by removing unnecessary terms from the formulas. The update equation for the marginal of m i for the pat + lda model variant at iteration t + 1 is given by where h i and d i are as before, and we define C ( m i ) is merely the product of unary factors that do not change values over iterations: the document factor f D , the word factor f w i , and the word selectional factors f w j , r ji i and dependent from their neighbors as would be the case for exact sum-product updates. Instead, they approximate this by having incorporated at iteration t the marginal values that their neighbors had at iteration t  X  1. These values are then marginalized over at iteration t + 1 for the node m i .
 Then loopy BP is run until convergence or until a fixed number of maximum iterations is reached. For the at and seq model, a truncated version of the previous loopy BP algorithm is the same as exact inference so it  X  X onverges X  in one iteration.
To ensure numerical stability, the paraphrase distributions were renormalized to sum to one at each iteration. The time complexity of loopy BP for the current batch of model variants can be very loosely bounded from above as follows. Let M be the maximum number of values possible for any paraphrase node m ,let D be the max-imum degree for any node in any dependency parse, let S be the order of a given dependency parse, and let T be the maximum number of iterations allowed before stopping. Then for any given sentence, the loopy BP update procedure given earlier is in O ( MDST ). In the vast majority of cases, the algorithm converges well before the allotted number of maximum iterations and the other variables are obviously smaller than their maximums on average. In practice, on a 64-bit Intel Xeon 2.5 GHz machine, the most complex pat + lda model variant takes a little less than two minutes on the approximately 2000 sentences in L EX S UB . The at variant is computed in seconds. As mentioned previously, we do not learn parameters for the word and selectional factors using EM in this article. Instead, we use methods from selectional preference and semantic similarity tasks to estimate them.

Before proceeding, we define surface Maximum Likelihood Estimates (sMLE) over all parameters that involve two nodes connected by a dependency relation. Because of the shape of the graphs in the model, such pairs only involve either observed/hidden or hidden/hidden node pairs. Say, for example, that an observed word  X  X atch X  and a paraphrase value  X  X phere X  are connected over the dependency relation  X  X bj X . We deal with this parameter through a heuristic. We state that the parameter P ( w i = catch , m j = sphere | r ij = obj) is just wish to define P ( m i ,w j | r ij ) (i.e., the head is hidden and the dependent is observed) or P ( m i , m j | r ij ) (i.e., both head and dependent are hidden). We call such mass functions P surface maximum likelihood estimates since the parameters are pulled heuristically from surface observations. The sMLEs will form the basis of the actual parameters that will be used in inference.

Selectional factors: interpolated count injected estimates ( int ) . We model the inter-action between two paraphrase distributions m i , m j with respect to a relation r ij as closely reflecting the surface count ratios for values of m i occurring in relation r ij to values of m j . For example, the selectional factor f s rp in Figure 5 maps the paraphrase values grab of m c and sphere of m b to an associative, nonnegative value that reflects the relative strength of seeing grab and sphere in a verb/object relation relative to other paraphrase values. To model this, we inject the sMLEs into these parameters. Because this estimate is likely to be sparse, we interpolate with similarly derived bigram and unigram sMLEs. We have where the weights  X  i sum to one.

Selectional factors: exponentiated PMI (epmi) . Raw frequency counts are known to adversely affect selectional preferences and are often transformed through pointwise mutual information, the log of two likelihood ratios. However, because factors/potential functions must be nonnegative, and because the log of two likelihood ratios can be negative, we take the exponential of pointwise mutual information and end up with the original ratio where the P s are sMLEs and  X  s is some small smoothing constant. 3
Word selectional factors: exponentiated PMI ( epmi ) and interpolated count injections ( int ). Word selectional factors model associativity between an observed word and a paraphrase node connected over some dependency relation. For example, given some observation w i connected to paraphrase node m j over relation r ij ,the epmi word selec-tional factor is defined as where the P s are sMLEs as before. int word selectional factors are defined in the same way as int selectional factors where we merely swap out the paraphrase node variables with observed node variables.
If it is the child w j which happens to be observed and the parent m i is the paraphrase node, then we make the appropriate changes to the previous factor definitions.
Word factors: using vector space similarity. The unary word factor f w ( m w )  X  [0 , 1] models semantic similarity between the observation w and the paraphrase random variable m w in the absence of context, but only for actual paraphrase candidates.
There are many ways of obtaining the parameters for word factors. In our case, we first derive a vector space representation for common words through the Dependen-cyVectors package 4 and normalize the vectors to unit length. We then define sets of paraphrases for observed words by combining the paraphrase sets in L EX S UB and the synsets in WordNet. Then for each paraphrase , observed word pairing, we calculate cosine similarity between the vector of a paraphrase and the vector of an observation. Because cosine is defined to lie in [  X  1 , 1], and because factors must be nonnegative, we monotonically transform these values by exponentiating the cosine. Finally, because some of the paraphrases lack representations in the vector space, 5 we assign such para-phrases v a smoothed constant value  X  and smooth all other paraphrase probabilities accordingly.

Formally, let C w be a set of known paraphrase c andidates for observed word w ,let v  X  C w be a value of the paraphrase random variable m w ,andlet M be the set of words and paraphrases which have a vector space representation. We denote the unit length vector representations of w and v as w and v , respectively. Then we define the factor, with smoothing constant  X  ,as where the normalization constant Z = w v  X  C
Note that any ranking based on cosine similarity between observations and para-phrases will be equivalent to the ranking based on the preceding word factors since exponentiation is a monotonic transformation on the reals. Test sets Word usage models are typically evaluated on a paraphrasing task, often using the L EX S UB dataset (illustrated in Figure 1). This is also the main dataset on which we evaluate our model. While the original Lexical Substitution task involved both the generation of paraphrase candidates and the computation of their weights for a given usage, EP08 and subsequent approaches focus on the second half of the task. They take the list of paraphrase candidates as given and weight them in a given context. In our experiments with the L EX S UB dataset, we follow EP08 in focusing on the second half of the task, paraphrase weighting, taking the list of paraphrase candidates as given. 6 L EX S UB consists of 2000 instances of 200 target words (verbs, nouns, adjectives, and adverbs) in sentential contexts, which were taken from the English Internet Corpus [Sharoff 2006]. To compile the list of potential paraphrases for a target, we proceed as follows: We first pool all paraphrases that L EX S UB annotators proposed for the target, and add all synonyms in all synsets of the target in WordNet 3.0. For address.v , the list of potential paraphrases contains, among others, speak.v , direct.v , call.v ,and handle.v . 7 For use with a topic model, we use the full documents containing the L EX S UB sentences. 8
In addition to the evaluation on L EX S UB , we briefly report results on two other datasets. The paraphrasing dataset by Mitchell and Lapata [2008] (hereafter) has hu-man ratings for paraphrase appropriateness (on a scale of 1 X 7) for verbs in the context of a subject noun. Given a target verb and subject noun, for example, discussion strayed , participants rated the goodness of a paraphrase for the verb in this context, for exam-ple, digress . The M/L dataset comprises a total of 3,600 human similarity judgments for 120 experimental items. Because participants gave ratings for the same paraphrase in different contexts, this dataset allows us to evaluate our model X  X  consistency across sentences. That is, it allows us to test whether the model can compare the applica-bility of a paraphrase p 1 in context s 1 to the applicability of a different paraphrase p 2 in a different context s 2 . Although for practical purposes the performance on the cross-sentence task is less central than the task of ranking the applicability of multiple paraphrases in the same context, it is an interesting question.
 The other additional dataset on which we test is by Biemann and Nygaard [2010]. They provide a dataset of paraphrases for nouns in context (hereafter TWSI ) collected on Amazon Mechanical Turk as a first step towards grouping usages into discrete senses. It contains paraphrases for the most frequent nouns of the English language, with sentence contexts taken from the English Wikipedia. The format of this dataset is similartoL EX S UB . We use version 1 of the data, 9 using the raw data with substitutions for all sentence contexts rather than only the contexts that were assigned to senses later. 10 This dataset comprises 7577 sentences with paraphrases for 392 nouns. We compile lists of paraphrase candidates in the same way as for L EX S UB . The TWSI dataset contains a high number of multiword expressions (about 20%) among paraphrase can-didates. Since our model currently cannot deal with multiword paraphrases, we omit them for now. We evaluate on the TWSI dataset, which to the best of our knowledge has not so far been used to evaluate paraphrase ranking on word usage models, because we consider it important to tap further datasets beyond L EX S UB . However, it is to be expected that TWSI is a very difficult dataset: It focuses on the most frequent nouns, which tend to be more ambiguous than medium-frequency lemmas.
 Parsing. We use the C&amp;C parser [Clark and Curran 2007] to parse the L EX S UB and TWSI datasets as well as the corpora from which we estimate probabilities. We transform prepositions from nodes to edge labels, and we retain only content words. All words are lemmatized.

Parameter estimation. For estimating selectional factor parameters, we use C&amp;C parses of two corpora X  X he British National Corpus ( BNC , 100 million words), and
W A C [Baroni et al. 2009] (2 billion words) X  X nd combine them ( U + B ). Additionally, we use a C&amp;C parse of the English Gigaword corpus (LDC2003T05, G IGA , 1 billion words) for a more direct comparison with the TFP10 model, which computes its vector space on that corpus. All words are lemmatized and paired with their part of speech. Word fac-tor parameters are estimated based on the paraphrase lists described in the previous paragraph. Vectors for these paraphrases are computed using the DependencyVectors package 11 with log-likelihood ratio transformation. To learn topic model parameters, we randomly take 26000 documents from UK W A C and combine them with the full L EX S UB documents. We then learn topic parameters with MALLET [McCallum 2002].
Testing convergence of inference. To test convergence, we examine whether all prob-ability values of the paraphrase distributions for all nodes change less than 1e-4 over a single iteration. If there is no convergence by 1000 iterations, we terminate inference and collect the values at the last iteration. In the majority of sentences, the algorithm converges in less than 20 iterations.

Smoothing constants. For smoothing constant  X  s , we take the smallest nonzero value of the respective unsmoothed factor and multiply that by 1e-4. We set  X  w to 0.1. The interpolated smoothing parameters are set to  X  1 = 0 . 9999 , X  2 = 9e  X  5 , X  3 = 5e  X  6 , X  4 = 5e  X  6.

Evaluation. For a target word w , we use as our model X  X  prediction the probabilities computed for the matching paraphrase distribution node, restricted to the paraphrase candidates for that target word in the dataset. For evaluating performance on the L EX S UB dataset, Thater et al. [2009] use Generalized Average Precision (GAP). Let A be a list of gold paraphrases for a given sentence, with gold weights a 1 ,..., a m . Let B = y 1 ,..., y n be the list of model predictions as ranked by the model, and let b if y i  X  A ). Let I ( b i ) = 1if y i  X  A , and zero otherwise. We write b i = 1 average gold weight of the first i model predictions, and analogously a i . Then We report macroaveraged GAP. 12
The SemEval task defined a  X  X recision out of ten X  (P oot ) measure for L EX S UB [McCarthy and Navigli 2009]. It uses the model X  X  ten top-ranked paraphrases as its prediction, and scores them by their gold weights. Let A and B be as earlier. Let B oot = y 1 ,..., y 10 be the model X  X  10 top predictions. Then We report macroaveraged P oot for comparison with previous approaches, but do not make P oot a main evaluation measure since it provides very similar information to GAP. Even though both measures carry the name  X  X recision X , they have more in common with recall, as they report the gold weight recovered by the model relative to the full gold weight. Also, they both take gold weights into account, but not model weights, using only the ranking predicted by the model.

In this article, we use as the second main evaluation measure (along with GAP) precision and recall, computed for the k highest-ranked paraphrases in the model X  X  prediction, for different k . For this evaluation, we consider the gold paraphrases as sets, ignoring gold paraphrase weights. One advantage of this evaluation is that it is easily interpretable. Another potential advantage, arguably, is that it ignores gold weights. L EX S UB annotators supplied paraphrases of their own choosing (rather than provide ratings for given paraphrases as in the M/L dataset). For that reason there is a certain random element to the gold paraphrases and the gold paraphrase weights, which we circumvent by not using the weights.
 DL10 use a nonparametric rank correlation measure, Kendall X  X   X  , to evaluate on the L
EX S UB dataset. This evaluation measure presupposes a gold dataset that allows for a comparison of paraphrase fit across sentences. While the M/L dataset is appropriate for such an evaluation, as mentioned earlier, we believe that this is not the case for L EX S UB and TWSI due to the method by which gold weights were derived in the annotation. We therefore do not use this measure to evaluate on the L EX S UB and TWSI datasets.
Like previous papers, we evaluate performance on the M/L dataset using Spearman X  X   X  , a nonparametric rank correlation measure. Like an evaluation with Kendall X  X   X  ,this requires a gold dataset that ranks paraphrases consistently across sentences. This is given by the design of the M/L dataset. In this section we discuss experimental results on the task of predicting paraphrase appropriateness. We refer to our group of models as PD for paraphrase distribution models.

Our main evaluation is on the L EX S UB dataset, on the task of ranking different paraphrase candidates for a target word in a common sentential context. We also report results on the new TWSI dataset on the same task, and on the M/L dataset on the more difficult task of ranking paraphrase appropriateness in a consistent manner across sentential contexts. Abbreviations used for models and datasets in this section are summarized in Table I.
 L EX S UB : Evaluation against benchmark and baseline models. We first compare the PD model to benchmark and baseline results. We test two versions of the PD model: using syntactic neighborhood as evidence ( pd:syn ) and using surface neighborhood ( pd:seq ). The only member of the pd:seq model class is the seq variant in Section 3.1.1 and similar to the Deschacht and Moens [2009] model. We list four benchmarks, the best-performing model in TFP10 [Thater et al. 2010], the best-performing model in EP10 [Erk and Pad  X  o 2010], the best-performing model in DL10 [Dinu and Lapata 2010], and a generalized version of the model in ML08 [Mitchell and Lapata 2008]. TFP10 reports better results than any previous syntax-based usage vector model, so it constitutes the current state-of-the-art. We list DL10 and EP10 because of their different modeling choices: Both are based on bag-of-words representations of sen-tences. EP10 is a vector space model, and DL10 use induced latent senses. We compare to component-wise multiplication (ML08) because it has proved to be a very strong simple model. Note that the ML08 paper uses component-wise multiplication on the M/L dataset, where only one single syntactic neighbor is available for each target. We extend the approach in a straightforward manner to the case of arbitrarily many syntactic neighbors by taking the component-wise multiplication of the target with all its neighbors. TFP10 and DL10 use G IGA as a basis, while EP10 uses BNC ,sothose approaches do not compare directly. We compare them to variants of pd:syn trained on G
IGA and BNC , respectively. For the ML08 model, we compute the vectors using the U + B corpus.

We list three baselines: singleton, frequency, and random. The singleton baseline ( singl ) assumes that the target paraphrase distribution is connected only to its obser-vation, that is, there is no contextual information. This is the same as the commonly used baseline that compares paraphrase vectors to the vector space representation of the target while ignoring the sentential context. For word factors, our model uses vector space similarity, so in the hidden node, the probability of each paraphrase is proportional to its vector space similarity to the vector of the target. The frequency baseline ( freq ) ranks paraphrases according to their frequency in UK W A C. The random baseline ( rand ) assigns random probabilities to the paraphrases.
 Tables II, IV, and V show GAP results for the PD model, benchmarks, and baselines. 15 Results in Table II are based on the U + B corpus, those in Table IV on G IGA ,andthose in Table V on the BNC . Boldface font shows the best results for each corpus. The pd:syn conditions shown are the model variant at + epmi .
 The pd:syn model based on U + B achieves the overall best results, with a GAP of 47.13. This is 3 points higher than the overall score of the pd:seq model on U + B , showing that the use of syntax makes a difference for this model. The ML08 model reaches an overall score that is only 1 point lower than pd:seq and 4 points lower than pd:syn , underscoring the strength of this benchmark model. The singl baseline is higher than the freq and rand baselines, but lower than any of the benchmark models. Comparing the baselines, it is interesting that similarity of paraphrase and target ( singl ) provides sizable performance gains over pure paraphrase frequency ( freq ). The contrast of BNC with G IGA and U + B indicates that the use of more data to estimate selectional factors has a considerable impact.

In an evaluation by target part of speech, TFP10 achieves the best performance for verbs. On all other parts of speech, the pd:syn variants with U + B parameters and G IGA parameters have the best performance, with an especially large advantage on adjec-tives. Compared to the BNC and G IGA conditions, the use of the larger U + B corpus leads to better results for nouns and verbs. The G IGA condition performs best for adjectives and adverbs.

One possible reason why TFP10 achieves the highest scores for verbs is that the vectors used by TFP10 have much higher dimensionality than the paraphrase state space of pd:syn . Perhaps having more dimensions is especially beneficial for verbs in modeling fine sense distinctions. It would be interesting to test whether modeling a larger state space that includes more than paraphrases improves pd:syn performance on verbs. Another possible explanation is that TFP10 uses the target X  X  headword as a stand-in for the target for adjectives and adverbs. This may be suboptimal for estimat-ing similarity to the target X  X  paraphrases.
 The GAP scores by part of speech follow a familiar pattern (for all approaches except TFP10) in that results for verbs are lower than for all other parts of speech. How-ever, there is another possible, general source of difficulty for verb contextualization. Examination of L EX S UB indicated that frequencies for L EX S UB verbs, as well as their paraphrases, are more skewed towards frequent lemmas than those for any other part of speech. As is well known, high-frequency lemmas tend to be more ambiguous, which makes them more difficult to contextualize. This is a problem that in principle can affect all approaches that evaluate on the L EX S UB data. For the pd:syn model we mea-sured this influence directly using a nonparametric correlation test, Spearman X  X   X  . We found a negative correlation (significant for many of the model variants) between lemma frequency or paraphrase frequency on the one hand and model performance on the other hand: Performance of the model goes down as lemma frequency rises, and also as paraphrase frequency rises. In this, our model resembles traditional WSD approaches, which typically struggle with high-frequency items as well.

As discussed before, we use as a second evaluation method along with GAP the mod-els X  precision and recall at different points in the model-generated rankings. Table III evaluates precision and recall at ranks 1 and 5 16 in the model-generated paraphrase lists. The relative performance of the models mirrors the GAP evaluation of the same models in Table II: The pd:syn model achieves the best performance at both rank cut-offs. At rank 5, it beats the pd:seq model by about 2 points F-score. (At rank 1, the difference is smaller. However, an evaluation on just a single highest-ranked para-phrase in a multiple paraphrase prediction task cannot be expected to yield a very stable measure. For a comparison of approaches, it is better to consult precision and recall at rank 5.) The ML08 benchmark model shows a performance that is lower than pd:syn and pd:seq , but higher than the baselines. Close to half (48.86%) of the pd:syn model X  X  highest-ranked suggestions are correct. Most L EX S UB instances are character-ized by multiple gold paraphrases, and with a single highest-ranked model paraphrase the pd:syn model only captures 13.87% of the gold paraphrases. At rank 5, it already captures more than half of the gold paraphrases. We can observe a trade-off of precision against recall, as at rank 5, the precision of pd:syn has declined to 36.39%. But recall rises much faster than precision drops, such that the pd:syn model X  X  F-score at rank 5 is 42.63, compared to an F-score of 21.61 at rank 1. This shows that pd:syn as well as other existing contextualization methods have promise, but also that further work is necessary to make them applicable in practice. In this, they resemble all-words WSD systems, which achieve accuracies of slightly over 50% on the benchmark All-Words datasets of Senseval-2 and Senseval-3 [Mihalcea et al. 2004a; Nastase 2008; Agirre and Soroa 2009].

For a more comprehensive analysis of precision and recall by model rank, the pre-cision/recall graphs in Figure 6 give performance for all ranks. They show the at and pat variants of the pd:syn model as well as the ML08 benchmark. The pd:syn model outperforms ML08 especially higher in the ranking, while at the last point of the rank-ing the two frameworks show the same performance. Looking at the evaluation by part of speech, the higher ranks of the pd:syn model show better performance than ML08 especially for verbs and nouns. Comparing at and pat , we see that there is only a small difference between the two. The pat variant shows slightly better performance on nouns, adjectives, and adverbs, while the at variant achieves slightly better results on verbs.

We also report P oot scores for completeness, but do not show them in further analyses, as they evaluate more or less the same properties of the models as GAP. The pd:syn variant with U + B parameters attained the highest overall score with 68.88, while the pd:seq model achieves 66.48. For the baselines, the scores are 62.54 (singl) and 59.61 (rand). TFP10 report a P oot of 75.43 for verbs, but do not give the score for other parts of speech. For comparison, the pd variant with G IGA achieves a P oot of 67.97 on verbs.
Model output examples. Figure 7 shows examples of the pd:syn model X  X  output for two L EX S UB sentences. The abbreviated sentence 1812 is shown in full in Figure 1. The model predictions are sorted in descending order of predicted probability. We are listing L EX S UB paraphrase candidates only, omitting paraphrases from WordNet. Gold paraphrases for each datapoint are boldfaced. Here and in general, the model produces highly skewed distributions with few high-probability items. For the first sentence, the three model variants produce similar paraphrase rankings, no matter whether the selectional information comes from observed words or hidden variables. The C&amp;C parse misses the dependency relation indicted -on -charges , but correctly identifies the modifier relation between drug and charge . Thus, words typically modified by drug (and its paraphrases) are ranked highly. In the second sentence (Figure 7(b)), take and risk form a pair with strong selectional preference. at and pat both rank verb paraphrases that bind strongly with risk in the top five X  X or example, assume , run  X , though assume and consider are not among the gold paraphrases. The ranked output of the two models is identical even though pat has inferred a more peaked distribution. The weakness of pt in this situation is evident where it has only two words in the top five that are in the gold and only three that can be considered to selectionally bind with risk : tolerate , consider , run .

Model parameters. Tables VI and VII compare different variants of the pd:syn model on the L EX S UB dataset. Table VI looks at two different ways of computing selectional fac-tors, with raw counts ( int ) or pointwise mutual information ( epmi ), and it also compares factors computed on the BNC to those computed on the much larger U + B . It only shows results for the at graph transformation; results for pt and pat are comparable. Com-puting selectional factors using pointwise mutual information rather than raw counts improves model performance in terms of GAP, and also in terms of both precision and recall, for all measures except precision and recall at rank 5. This shows that it is important to dampen frequency-related noise when using selectional factors, which is in keeping with findings by Thater et al. [2010]. The use of more data to train models X  using U + B rather than BNC  X  X onsistently improves models in all conditions. The use of a larger corpus for training leads to an even more pronounced improvement with epmi than when raw counts are used. Table VII compares the three graph transformation models in terms of GAP and in terms of precision, recall, and F-score computed at ranks 1 and 5. at and pat scores are consistently higher than those achieved by pt . So we can conclude that in the context of these experiments, at least, the use of collocation information is crucial for performance. One reason for this may be that the precision of the highest-ranked paraphrase in the hidden nodes is only at around 50%, with lower values for lower-ranked paraphrases. So it is remarkable that pat still outperforms at by a small margin, both in terms of GAP and in terms of precision and recall. This means that it is beneficial to have access not only to neighboring words, but also to their meaning in context. We expect that this effect will be more pronounced as models X  contextualization performance increases. Even though pat shows slightly better perfor-mance, we list at when comparing against benchmarks and baselines, and also when evaluating on the two additional datasets that follow, as it is the simpler model.
Document topic. We next test whether adding document topic information improves performance over a model that only uses syntactic neighbors as evidence. Table VIII compares results of pd:syn models with and without lda -derived factor nodes. It only shows the at condition; results for other conditions are comparable. The first line shows the result of a baseline lda experiment where each target node had a document-based lda factor, but no syntactic context. This model is similar to the singleton baseline in that it has no information from syntactic neighbors. We note that the result for the lda baseline model is even lower than the singleton baseline. Also, combining lda factors with the pd:syn model results in a slight deterioration of performance. 17
Overall, the contribution of LDA-derived factor nodes is disappointing, a marked difference to the usefulness of document context features in traditional WSD. One possible reason is that LDA topics may not provide much information for the words in our paraphrase distributions. LDA information will be most useful for words whose probability differs strongly across topics. We compute the entropy of a word across topics as a measure for its  X  X opicality X . Figure 8 shows density plots for the entropies of the paraphrases of L EX S UB lemmas, as well as for the top 30 words in all LDA topics that we used. The dotted line plots the entropy for the top LDA topic words, and the solid line plots the entropy of the L EX S UB paraphrase candidates. We see that paraphrases have two modes, one at high entropy, which indicates low  X  X opicality X , and another in the middle but still higher than a sizable portion of the highly ranked words. So using a topic model, though intuitively the most obvious approach to including document topic information, is not helpful for this task, but a different model of document context still may be. 18
Number of syntactic neighbors. Next we examine whether the model is able to successfully integrate contextualizing information from multiple syntactic neighbors. Erk and Pad  X  o [2009] found that use of multiple syntactic neighbors did not improve the EP08 model. We find that this is different in the pd:syn model. Figure 9 plots performance against the degree of the L EX S UB target node for different parts of speech, based on parameters from U + B with the pat + epmi condition. Other conditions show a similar picture. The x -axes indicate the degree of the target node in the dependency parse trimmed to include only content words. The y -axes show GAP. We can see that for verbs and nouns, there is a large increase in GAP for nodes with at least two neighbors. In contrast, the model performs best for adjectives and adverbs when there is exactly one neighbor, which makes sense, as they typically have fewer dependency neighbors. We conclude that for verbs and nouns, our model successfully integrates information from multiple syntactic neighbors.
 TWSI dataset. The TWSI dataset gives lexical substitutes for the most frequent nouns. So we can expect this dataset to be harder to model than L EX S UB , as the most frequent lemmas will in general be harder to contextualize than medium-frequency ones. How-ever, we consider it important to access additional datasets for the evaluation of usage models to avoid overfitting the L EX S UB data. Table IX shows GAP and precision/recall scores on TWSI for the pd:syn model (again we show the results for the simplest model, at ) as well as for benchmark and baseline systems. The scores confirm that this dataset is relatively hard to model. The random baseline is considerably lower than for L EX S UB , indicating that there is on average a greater number of paraphrase candidates for TWSI datapoints. The singleton baseline is exceptionally strong: The difference in GAP be-tween that baseline and the at model is only 0.03 points, and there is no difference in F-score. (The pat variant outperforms the singleton baseline by as little as 0.3 points, as Table X shows.) The singleton baseline is strong on TWSI not only in comparison to pd:syn : It outperforms the ML08 benchmark by 7 points GAP and by 2 points F-score. So on this dataset, context-aware models barely manage to rise above a model that ranks paraphrases just by similarity to the target without taking sentential context into account. Comparing pd:syn model variants in Table X, the results confirm the trend in the L EX S UB experiments where pat and at models show better performance than pt ,and pat outperforms at by a small margin.

M/L dataset. On the L EX S UB and TWSI datasets, we have evaluated the performance on the task of ranking the applicability of multiple paraphrases in a common sentence context. On the M/L dataset, we now evaluate our model X  X  performance on the task of ranking paraphrase applicability across sentences: How does the applicability of a paraphrase p 1 in sentence s 1 compare to the applicability of a different paraphrase p 2 in a different sentence s 2 ? Our model does not have a built-in mechanism for en-forcing cross-sentence consistency. On the contrary, the fact that the model weights are probabilities may reduce cross-sentence consistency, as the presence of one strong paraphrase reduces the probability mass available for another paraphrase, no matter how strong.

Table XI shows the results on the M/L dataset. Mitchell and Lapata estimate the ceiling (inter-rater agreement) at  X  = 0 . 4. Results on the BNC corpus are shown on the right, and results on U + B on the left. When using parameters estimated on the BNC ,the pd:syn model X  X  performance is lower than those of ML08 and EP08. But when using the U + B corpus as a basis, the correlation of the best pd:syn model condition, at , exceeds the best results reported on the BNC , and it also exceeds the results that the ML08 method achieves with the U + B corpus as a basis. Interestingly, while the pd:syn model X  X  results improve considerably when moving to a larger corpus, from  X  = 0 . 198 to  X  = 0 . 298, the performance of ML08 stays more or less the same. Also surprisingly, the pt model variant surpasses pat on this task, but only when trained on the larger corpus. The result lets us conclude that the pd:syn model has state-of-the art performance also on the task of providing consistent ratings of paraphrases across sentences. In this article we have introduced a usage model of word meaning that frames the task of contextualization as a probabilistic inference problem. It characterizes a word X  X  meaning as a distribution over potential paraphrases, which is inferred using an undi-rected graphical model.

Our main evaluation has been on the L EX S UB dataset on a paraphrasing task. We achieve state-of-the-art results with a GAP of 47.14. Precision of the highest-ranked paraphrase is at close to 50% with a recall of 14%. To achieve adequate recall, a single paraphrase per datapoint is not sufficient, as most datapoints have multiple gold paraphrases. When the five highest-ranked paraphrases are considered, precision is at 37%, while recall rises to 52%. Further findings on the L EX S UB dataset are as follows.  X  X he model achieves a considerable gain in performance from using syntactic infor-mation, rather than just surface neighborhood, as evidence.  X  X he use of collocational information ( at , pat model variant) is crucial to performance.
The pat model, which uses neighboring paraphrase distributions along with observed neighbors, narrowly outperforms at , which only uses observed neighbors. This is remarkable given the current precision of high-ranked paraphrases. If precision can be improved in future models, it can be expected that contextualizing words based on the context-dependent meaning of neighbors will lead to more substantial performance gains.  X  X e found no gain from using document topic as evidence, but this may have more to do with the informativeness of topic distributions for the L EX S UB targets than with the usefulness of document context in general.
  X  X or nouns and verbs, there is a clear gain in performance when the target has multiple neighbors, showing that the model successfully integrates information from multiple neighbors.  X  X s previously reported by Thater et al. [2010], transforming raw syntactic cooc-currence counts into pointwise mutual information makes a big difference in performance.
 We also evaluate on the new TWSI dataset, on the same task. The dataset proves very tough to model, in that the singleton baseline is competitive to the PD model. Both the singleton and PD comfortably outperform a benchmark model. Finally, we evaluate on the M/L dataset on the task of making consistent paraphrase applicability predictions across sentences. We find superior performance for the PD model when it is trained on a sufficiently large corpus, while a benchmark model fails to profit from the addition of further training material.

In future work, we plan to replace the rather simplistic method that we have used for estimating word and selectional factors. Instead, we will use the current estimates as a starting point for learning parameters using standard methods such as EM or Gibbs sampling. Second, we will extend our model to handle multiword paraphrases. Current approaches that determine the applicability of multiword paraphrases [Pantel et al. 2007; Szpektor et al. 2008; Poon and Domingos 2009; Ritter et al. 2010] rely centrally on the selectional constraints on argument slots, mostly treating the paraphrase as a black box. Analogously, we can incorporate multiword paraphrases into our model by treating them as  X  X ords with spaces X  and using them as hidden values. Another experiment we plan to do is to do a direct comparison of performance between a contextualization system and an all-words WSD system. In order to provide them with a level playing field, we will evaluate them on a paraphrasing task. A WSD system X  X  output can be converted to a paraphrase distribution by giving a uniform high weight to the words in the assigned synset, and a uniform weight of zero to the others.

