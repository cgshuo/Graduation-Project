 We propose a novel probabilistic retrieval model which weights terms according to their contexts in documents. The term weighting function of our model is similar to the language model and the binary independence model. The retrospective experiments (i.e., relevance information is present) illustrate the potential of our probabilistic context-based retrieval where the precision at the top 30 documents is about 43% for TREC-6 data and 52% for TREC-7 data. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Experimentation, Performance Context, Retrospective Experiment We propose a novel model that utilizes the context information to compute the term weight at each location in the document of the matched search term (or query term). The term weight is calculated by multiplying probabilities similar to the well-known probabilistic models (i.e., binary independence model [1]) and language model (e.g., [2]). This paper focuses on whether the use of context information can enhance retrieval effectiveness in retrospective experiments (that use the statistics of relevance information similar to the w4 term weight [1], the ratio of relevance odds and irrelevance odds). If there is a significant effectiveness enhancement, then the future research question (not addressed here) is how to obtain the effectiveness in predictive experiments that are close to our retrospective experiments. For valid comparisons, w4 weight [1] is used because it requires relevance information and its effectiveness is known to be good. There are research works (e.g., [3]) similar to ours in which the score of every location in the document of the search term contributes differently to the document similarity. By contrast, apart from incorporating the search term occurrences in the document for ranking, our score of every location in the document is determined by the terms located nearby the search term and by the relative location of these terms to the search term. Likewise, individual pieces. Unlike some passage retrieval, our model does not assign uniform scores to each search term occurrences in the passage. Finally, other studies (e.g., [5]) aggregate scores of the selected terms in the nearby context of the matched search term where the selected terms were found to have significant relations with the search term beforehand. By contrast, our model combines the scores of all terms in the context without identifying significant relations. There are a variety of formulae for calculating the term weights [6]. One of the most widely used term weights is known as the inverse document frequency (IDF) which was proposed in [7] and combines with the term frequency (TF) to form the well-known TF-IDF function. In order to investigate the contribution of context information in term weighting, we consider the occurrences of the location-specific terms when calculating term weights. Before further discussions, the meaning of a context should be clarified. Firstly, the position of each search term occurring in the document is located. Then the terms before and after the search term are considered to be the context of that search term. Hence, each context has a search term in the middle. This is different from passage retrieval in which the passages are defined by document structures or text blocks at regular intervals. The first reason for having the context is that our model is simulating a user making relevance judgment using keywords in context [8] as in [9]. The other reason is that a term with its context has a definite meaning and it becomes apparent whether the matched term has a similar meaning to the search term X  X  meaning. However, terms without contexts can be ambiguous. Each context c ( s i,k , n ) has a search term s i,k which is a query term and appears at the k -th location in the i -th document with context occur at the p -th location relative to s i,k in the i -th document where p  X  [ -0.5n, 0.5n ]. We define the probabilities of which a context c ( s i,k , n ) is relevant and irrelevant by the following equations: similar to the language model [2] where:  X   X  are relative frequency estimates of the probabilities of the term which occurs in the relevant documents and irrelevant documents, respectively. The term weight of each search term in the document depends on its context and is defined by the odds: constants, their ratio is a constant and hence this ratio can be discarded during ranking. Finally, the term weight w ( s search term s i,k is the log-odds ratio (similar to [1]): According to the disjunctive relevance decision principle [9], we define the similarity between query q and document d i the highest as the representative score: in order to avoid spurious matching of search terms. The experiment compares the context-based score mentioned above and the w4 weighting using the TREC-6 and -7 collections the BM11 weights of the 2-Poisson model [10] with blind feedback in a predictive experiment. The results in Table 1 are our baseline. We report the precision at top N documents (P@N), R-Precision and the mean average precision (MAP). Next, we modified the BM11 weight by substituting the w4 weighting into the IDF factor. For our model, the best context size should be empirically determined and it is set to 100 here as in [11] for simplicity. Table 2 shows the results of the retrospective experiment that compares the w4 weighting effectiveness and the context-based weighting effectiveness . When the relevance judgments are present, the performance is expected to be increased. As our proposed weighting scheme depends on location-specific information of search terms, every search term weight would be different for different contexts. The results showed that the precision of our weighting scheme is highly promising (c.f. [12]). The evidence suggests that context information is important for information retrieval. Our experiment results show that theoretically applying the context information could improve the performance for re-ranking documents. In this paper, experiments are done retrospectively and the results are promising. We would further investigate how to estimate the probabilities without relevance information for our model to operate in predictive experiments This work is supported by PolyU 5183/03E and partially supported by the CUHK strategic grant initiative ( account #4410001). The basic retrieval system was developed when Robert was on leave at UMASS. [1] S.E. Robertson and K. Sparck Jones, Relevance Weighting [2] J. F. Ponte and W. B. Croft, A Language Modeling [3] O. de Kretser and A. Moffat, Effective Document [4] M. Kaszkiel, J. Zobel and R. Sacks-Davis, Efficient Passage [5] P. Bruza and D. Song, A comparison of various approaches [6] J. Zobel and A. Moffat, Exploring the Similarity Space, ACM [7] K. Sparck Jones, A statistical interpretation of term [8] J. Kupiec, J. Pedersen, and F. Chen, A Trainable Document [9] Y. K. Kwong, R.W.P. Luk, W. Lam, K.S. Ho and F.L. [10] S. E. Robertson and S. Walker, Some Simple Effective [11] C.L.A. Clarke and E.L. Terra, Passage retrieval vs document [12] K. Sparck Jones, Summary Performance Comparisons 
T w4 ours w4 ours w4 ours w4 ours 
