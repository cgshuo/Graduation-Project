 When trained and evaluated on accurately labeled datasets, online email spam filters are remarkably effective, achieving error rates an order of magnitude better than classifiers in similar applications. But labels acquired from user feedback or third-party adjudication exhibit higher error rates than the best filters  X  even filters trained using the same source of labels. It is appropriate to use naturally occuring labels  X  in-cluding errors  X  as training data in evaluating spam filters. Erroneous labels are problematic, however, when used as ground truth to measure filter effectiveness. Any measure-ment of the filter X  X  error rate will be augmented and perhaps masked by the label error rate. Using two natural sources of labels, we demonstrate automatic and semi-automatic meth-ods that reduce the influence of labeling errors on evaluation, yielding substantially more precise measurements of true fil-tererrorrates.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]:information filtering General Terms: Experimentation, Measurement Keywords: spam, email, filtering, classification, noise
When trained and evaluated on accurately labeled datasets, online email spam filters achieve remarkably good perfor-mance. Gradient descent logistic regression [14], for exam-ple, yields an overall error rate of less than 0.5% when eval-uated on the trec05p-1 corpus [10] using the TREC 1 Spam Track methodology [11]. This result betters by an order of magnitude those reported for similar applications of classi-fiers (cf. [28]). But is it good enough? Can it be improved? Can it be demonstrated beyon d the laboratory? Imprecise knowledge of ground truth  X  the correct label for each fil-tered message  X  presents a substantial impediment to ad-dressing these questions. http://trec.nist.gov
The TREC methodology assumes a chronological sequence of accurately labeled email messages. The labels serve two distinct purposes in evaluation: as training examples for the filter, and as ground truth against which to measure filter error. Label errors therefore c ompromise both the filter X  X  learning and the accura cy of the measured error.
The TREC datasets were carefully labeled by the orga-nizers, adhering to a prescribed definition of spam: While there is no doubt that the TREC corpora contain some labeling errors, it is reasonable to assume that the er-ror rate is not much worse than that of the best reported filter results using the labels; i.e. about 0.5%. Labels ac-quired in real-world deployment may be expected to exhibit much higher error rates. When explicitly asked to classify messages, human subjects have been reported to exhibit er-ror rates of 3%-7% [30, 16]. Tacitly derived labels, such as those obtained from a  X  X eport spam X  button, where it is as-sumed that unreported messages are ham, may have even higher rates.

It would be more appropriate to use naturally occuring labels  X  including errors  X  as training data, in conjunction with more accurate labels fo r measurement. Were the nat-ural labels used for measurement, the estimated filter error rate would be augmented and perhaps masked by the la-bel error rate, especially when the filter X  X  true error rate is substantially lower than that of the labels. A filter with a true error rate of 0 . 5%, for example, might be estimated to have an error rate between 5 . 5% and 6 . 5% when evaluated using labels with an error rate of 6 . 0%, depending on the correlation between filter and label errors. A filter with a true error rate of 1 . 0% might show an error rate between 5 . 0% and 7 . 0%. These measurements would be insufficient to distinguish the two filters, and might even invert their relative performance.

An ideal filter evaluation would use a realistic sequence of messages, with a set of natural labels for training and a set of gold standard labels for evaluation. Accurately la-beled sequences of messages are rare and somewhat unre-alistic due to the costs and logistic challenges of acquiring and labeling them. The only public dataset for which both accurate and natural labels are available is trec05p-1 ,the TREC 2005 Public Spam corpus. The messages were ac-quired from Enron and released to the public in the course of a criminal investigation; accurate labels were created by the TREC coordinators [10]; natural labels were collected later from Web users through the SpamOrHam effort [16]. Although the typicality of the messages and labels may be questioned, the messages are real and the two labeling ef-forts are clearly independent. T he natural label error rate, at 6 . 7% [25], overwhelms that o f the accurate label error rate.

The cost and difficulty of acquiring both natural and accu-rate labels for a realistic email sequence may be prohibitive. The SpamOrHam labels were acquired with much effort on the part of Web users, and predicated on the ability to pub-lish the messages. It is unlikely that such an effort could be marshalled again, and, in any event, privacy considerations prevent almost any realistic email collection from being pub-lished. The gold standard labels were also acquired with much effort; privacy considerations impede similar efforts, as assessors are typically permitted to view messages only under restrictive and potentially unwieldy conditions.
Spam filtering as a research area is relatively new but nevertheless quite diverse. The TREC Spam Track is the largest and most realistic laboratory evaluation to date. In the three years it ran, ten test corpora with a total of 721,461 messages were used to test filters submitted by thirty-five participants. The results of more than one thousand experi-mental runs may be found in the summaries and appendices of the TREC proceedings [11, 6, 7].

ThecoretaskatTREC X  X he immediate feedback task  X  simulates the on-line deployment of a spam filter with idealized user feedback. The task, along with methods and evaluation measures embodied in an open-source toolkit [19], was developed by Cormack and Lynam [12].

This on-line evaluation strategy differs from traditional batch evaluation in that the messages are ordered and not explicitly partitioned into training and test examples. Every example is potentially available for training once the filter has rendered a verdict. The difference in methodology and results for state-of-the art filters has been studied by Cor-mack and Bratko [9]. The best results in these tests have been achieved by on-line support vector machines [26], logis-tic regression [14], and compression models [3], along with a number of open-source  X  X ayesian X  filters modeled after the work of Graham and Robinson [15, 24], notably Bogofil-ter [22] and OSBF-Lua [2]. The best-performing approach demonstrated using TREC methods and datasets is the fu-sion all filters submitted for evaluation at TREC [20].
Variants of the task, including delayed feedback, partial feedback, and active learning explore more realistic models of user feedback; in particular, the impact of tardy, incom-plete or restricted presentation of training examples after classification. The CEAS Live Challenge [1] integrates the TREC methodology with a live data stream, so as to be able to compare laboratory and in vivo filter deployment. Two new laboratory corpora were created; one captured in real time, and the other sampled from email delivered to clients of a large email service provider.

Sculley and Cormack [25] explore the effect of synthetic and natural label noise on filter training and conclude that both substantially compromise filter performance, natural noise more substantially so. Several approaches to mitigat-ing the effect are found to improve filter performance, but not to anywhere near that achieved with noise-free train-ing labels. A regularized version of an SVM classifier which performed well on synthetic noise yielded mediocre perfor-mance on natural noise. The effect of label noise on the measurement of filter error is not considered.

Labeling noise has been studied from the perspective of its impact on the accuracy machine learning algorithms, with the methodology of injecting artificial noise into a an ini-tially noise-free dataset. Brodley and Friedl [4] proposed a data cleaning methodology whereby instances misclassified with high confidence by a learner (or ones for which there is a significant disagreement within an ensemble of learners) are removed from the training collection. They showed that models created over such cleaned data tend to be more ac-curate and demonstrated that ensemble based models are more resilient to class noise, at least when the level of noise is moderate. In addition to cleaning methods relying on inconsistent instance removal, alternatives based on label correction [25] and instance weighting [23] have also been considered.

While there exists a substantial body of published worked dedicated to different filtering and feature extraction tech-niques, our work is not so much to improve on these tech-niques as to improve the evaluation methodology; in partic-ular, the ability to discriminate among and to measure the absolute performance of the best reported approaches for spam filtering, and for training label noise mitigation.
Lam amd Stork [18] discuss the problem of evaluating classifiers by means of test data with noisy labels. Given an estimate of the label noise and the assumption that label noise and filter error are uncorrelated, a precise estimate of filter error may be achieved, given enough examples. If, on the other hand, there may be correlation, wide bounds apply.

The issue of noisy and incomplete relevance judgements has been considered extensively within the context of the laboratory evaluation of information retrieval systems (cf. [5]). Although inter-adjudicator disagreement is large, sys-tem rankings are not particularly sensitive to it. Incom-pleteness is commonly resolved by the  X  X ooling method, X  in which the top-ranked documents from each system are ad-judicated as relevant or not, and all other documents are assumed to be irrelevant. More efficient methods select for adjudication more documents from top-performing systems, or documents most likely to discriminate among the filters under test; these methods are the subject of ongoing research interest. IR evaluation research is typically concerned only with ranking systems by relati ve performance on a particu-lar corpus, as opposed to a calibrated effectiveness measure. IR evaluation further differs from spam filter evaluation in that relevant documents are extremely rare, and error rates (e.g. 1-recall, 1-precision) are quite large  X  of the order of 50% in a typical evaluation.
The objective of this work is to determine the extent to which label error compromises filter performance measure-ment, and to validate a method of achieving better measure-ments without the intensive adjudication effort and access to messages normally associated with creating an accurate labeling. The automatic method requires no access what-soever to the messages, while the semi-automatic method requires adjudication of a small fraction. The automatic method is applicable in situations where results are gathered by instrumenting an email system, but the content of mes-Table 1: Agreement between nat and adj SpamOrHam label sets. Table 2: Agreement between nat and trec label sets. Table 3: Agreement between adj and trec label sets. sages are not available to evaluators. The semi-automatic method is applicable in situations where it is possible to adjudicate some messages, provided the number is not too onerous. For example, the original recipient may be asked to review occasional messages, or to spend an hour or two participating in a review process [21].

We assume that it is possible to apply several filters to exactly the same sequence of messages with the same train-ing labels and that each filter X  X  result (either a categorical decision or a score or both) is available, along with a nat-ural label, for each message in the sequence. The results of these filters are fused to form a set of labels which, we hypothesize, has a lower error r ate and yields more accurate results than the natural labels when used as ground truth for evaluation. The hypothesis is tested in several ways: We used email messages from two separate corpora: the TREC 2005 Public Spam Corpus (trec05p-1) [11], and the CEAS 2008 Live Challenge private corpus [1]. Four indepen-dent sets of labels were used in total; three for the TREC messages and one for the CEAS messages.

The TREC corpus includes a gold standard label for each message. In addition, we acquired the labels collected by the SpamOrHam project [16]. The messages labeled by
Tag Description bayes Naive Bayes, character 4-gram binary fea-nobs Naive Bayes, alphanumeric sequence  X  X ord X  bogo Bogofilter version 1.1.5, default parameters dmc Dynamic Markov Compression [3]. wat1 Gradient descent logistic regression, 4-gram osbf OSBF-Lua [2]. tft1 Relaxed Online Support Vector Machine [27].

Tag Description logbagf wat1 , modified to train only 10% of the fea-logbagm ensemble of wat1 filters, each modified to lrslow wat1 , with the learning rate parameter re-dmc Dynamic Markov Compression [3]. tft1-0.5 tft1 , with C parameter reduced from 100 to Table 5: Base filters altered to reduce sensitivity to label noise.
 SpamOrHam were selected at random with replacement, re-sulting in a variable number of labels per message. From the messages having two or more labels, we selected one la-bel at random to serve as the natural label, and a second (without replacement) to simulate the result of adjudica-tion. Messages having fewer than two SpamOrHam labels were eliminated from the evaluation. The TREC labels were used as a reference standard, and also to simulate more re-liable adjudication. The resulting dataset contains 80,476 messages, approximately 33,000 ham and 48,000 spam.
The CEAS corpus was collected from messages delivered to clients of a large service provider. The messages to be la-beled were selected at random from those delivered to a set of volunteer clients; the labels are the responses to specific adjudication requests to the recipients. The corpus con-tains 198,574 messages, of which 89,451 are labeled ham, and 109,123 spam. There is exactly one label per message.
The TREC methodology requires that filters return both a hard result (ham or spam) and a soft result (a  X  X paminess X  score) which may be compared after the fact to some thresh-old t . Hard results are evaluated as a pair of error rates ( fpr,fnr ) for ham and spam respectively. The labels fpr and fnr denote false positive and false negative rate from diagnostic test theory [13]. A classifier with lower fpr and fnr than another is superior. (Under the assumption that all messages have equal misclassification cost [17].) Whether a classifier with a lower fpr and higher fnr is superior or in-ferior depends on the user X  X  sen sitivity to each kind of error.
Tag Description bayesm 8 bayes ensemble, disjoint training examples. nobsm 8 nobs ensemble, disjoint training examples. bogom 8 bogo ensemble, disjoint training examples. dmcm 8 dmc ensemble, disjoint training examples. wat1m 8 wat1 ensemble, disjoint training examples. logbagfm 8 logbagf ensemble, disjoint training exam-osbfm 8 osbf ensemble, disjoint training examples. tft1m 8 tft1 ensemble, disjoint training examples. Table 6: Base filters in bagging configuration to re-duce sensitivity to label noise.
 A plethora of measures  X  including accuracy, weighted ac-curacy, total cost ratio, F-measure, and utility  X  attempt to quantify this sensitivity and to use this quantification to combine fpr and fnr, along with the corpus ham-to-spam ratio, into a one-dimensional measure.

The soft result may be characterized by the set of all dis-tinguishable ( fpr , fnr ) pairs for different values of t .Thisset of points defines a receiver operating characteristic (ROC) curve [29]; a filter whose ROC curve is strictly above that of another is superior in all deployment situations, while a filter whose ROC curve crosses that of another is superior for some threshold settings and inferior for others.
The area under the ROC curve ( AUC )providesanesti-mate of the effectiveness of a soft classifier over all threshold settings. AUC also has a probabilistic interpretation: it is the probability that the classifier will award a random spam message a higher score than a random ham message. In the spam filtering domain, typical AUC values are of the order of 0.999 or greater; following TREC, we report (1-AUC )% , the area above the ROC curve, as a percentage. So AUC =0.999 would be reported instead as (1-AUC )%=0.1.

While AUC provides an amenable score for ranking soft classifiers, the pair ( fpr,fnr ) does not serve this purpose for hard classifiers. It has been observed [13] that the di-agnostic tests, effectively invariant over a large number of threshold settings. Intuitively, a change in threshold set-ting that increases the odds of misclassifying ham by some multiplicative factor tends to decrease the odds of misclas-sifying spam by the same factor. Therefore dor is a useful summary measure largely uninfluenced by threshold setting. The same effect has been observed at TREC [11], giving rise to the measure logistic average misclassification rate, Note that the value LAM is necessarily between fpr and fnr ;when t is set to equalize error rates, we have fpr = fnr = LAM .

Under the assumption that dor is invariant, it is possi-bletoestimate( fpr ,fnr )from( fpr,fnr )bysolvingthe equation It is further possible to estimate AUC  X 
We chose to evaluate several base filters previously con-figured for the TREC Spam Filter Evaluation Toolkit. In addition, we included variants of these filters altered to mit-igate training label error.Table 4 provides an identifying tag and a short description for each base filter. Table 5 describes altered versions of some of these filters. Table 6 identifies ensemble filters that we created by running a particular fil-ter eight times, training each time on only one eighth of the examples, randomly partitioned. We were unable to com-plete some of the filter runs either because the filters failed or because they failed to complete in a reasonable amount of time. These runs were excluded from consideration; as a consequence 17 of the subject filters were used with the TREC data, and 11 on the CEAS data.
The overall objective of spam filter evaluation is to deter-mine which spam filters better approximate truth, so that they may better serve their intended purpose. If the true class of each message is known, filter performance may be quantified by an amenable measure of the distance between the filter X  X  result and truth. The TREC evaluations use re-ceiver operating characteristic ( ROC ) area under the curve (expressed as (1  X  AUC )(%), so that lower numbers are better) and logistic average misclassification rate (expressed as ( LAM )(%)) as threshold-insensitive measures of perfor-mance.

When a pair of filters exhibit similar or contradictory rel-ative performance according to these measures, pairwise dif-ferential comparison may provide a more sensitive indication of which is closer to truth. In a differential comparison, only the cases of disagreement between filter results are compared to truth; a simple sign test determines the better approxi-mation to truth. A tournament  X  in which each filter is differentially compared to each other filter  X  may be used to establish a ranking, but no quantitative measure of how close an approximation to truth is achieved by each filter.
Differential comparison, unlike the TREC measures, is very sensitive to the filters X  threshold settings, and also to the prevalence of spam in the evaluation dataset. It there-fore cannot reward, and is likely to penalize, a filter X  X  ability  X  as may well be desirable  X  to identify ham with a lower error rate than spam. In the TREC setting, where filters report a confidence score in addition to a categorical clas-sification, this shortcoming of differential comparison can be mitigated by threshold-adjusted differential comparison. Prior to comparison, each classifier X  X  threshold is adjusted to achieve equal apparent ham and spam error rates (i.e. fpr = fnr = LAM ). Threshold-adjusted differential com-parison affords a consistent threshold-independent approxi-mation, albeit one that fails to capture one aspect of filter performance.

Truth, at least with respect to spam filtering, is an ab-straction. It may be approximated but never realized; the aptness of an approximation can only be estimated. Infor-Table 7: Adjudicated differential comparison be-tween label sets l 1 and l 2 using a as adjudicator. rank trec nat adj power 0 . 82 0 . 90 0 . 88 Table 8: Adjudicated tournament ranking of subject filter performance, using trec, nat and adj labels as ground truth. mally, through a combination of qualitative observations and statistical inference, we argue that the TREC labels better approximate truth than either of the two SpamOrHam label sets, which are different but equally good approximations. Pairwise agreement and disagreement between the three la-bel sets (dubbed trec, nat, and adj) is quantified in tables 1 through 3. We see that nat and adj disagree on 10% of the messages, while each disagrees with trec on 6 . 9%. These agreement rates indicate that SpamOrHam judgements have 5 . 2% random error, and that the TREC judgements have considerably less. There may also be a systematic difference between the effective definition of spam applied by TREC and SpamOrHam assessors, or any number of other system-atic differences. The net effect is that each approximation differs from truth by two factors: random error (noise) and systematic error (bias).

For the purposes of this evaluation we define the true class to be the majority opinion of the hypothetical infinite popu-lation of users from which the SpamOrHam judgements are drawn. That is, we deem the SpamOrHam labels to have no bias, and to differ from truth by random error alone. A spam filter X  X  performance is therefore defined by how well it pre-dicts the majority opinion, notwithstanding any quibbling about the definition of spam or the competence of members of the population to apply the definition.

From the perspective of this definition, the TREC labels exhibit some bias. Evidence of this bias is apparent from the prevalence of spam labels in each set: nat contains 59 . 6% (95% c.l.: 59 . 2  X  59 . 9) spam, as does adj (59 . 2  X  59 . 9). On the other hand, trec contains 57 . 1% spam (57 . 2  X  57 . 8). nat and adj agree within the limits of chance (as we would ex-pect, given that they are independent samples from the same population) while trec disagrees by 2 . 5%, a significant sys-tematic error. While random error for the trec labels is dif-ficult to quantify, we may infer from this bias estimate and the disagreement rate that trec label noise is smaller than, and positively correlated with, SpamOrHam label noise.
Adjudicated differential comparison may be used to com-pare pairs of labelings. But the X  X orrect labels X  are unknown, so instead we use a third independent labeling, or a live ad-judicator, as a surrogate. Provided the third labeling is in-dependent and yields the correct label more often than not, we may conclude that, of the labelings being compared, the one that agrees with the third more often better approxi-mates truth. A sign test evaluates the overall significance of the comparison result. Adjudicated differential compari-son may be used to compare and rank labelings, but offers no quantitative estimate of the error rates of the labelings being compared or, for that matter, of the adjudication la-beling. Table 7 illustrates the result of adjudicated differ-ential comparison among the trec labelings; demonstrating formally our observation that the trec labels are more accu-rate, while the SpamOrHam labelings are statistically indis-tinguishable.
While the ultimate goal of this work is to accurately es-timate filter performance using standard measures, we first consider the problem of ranking filters. A ranking is consid-ered good to the extent that it orders filters consistently with the standard measures, without concern for quantitative es-timates. Kendall X  X   X  rank correlation is commonly used to compare rankings. If t isthetruerankingof filters according to some measure,  X  t 1 and  X  t 2 are approximations,  X  t 1 thetruerankingthan  X  t 2 if  X  ( t,  X  t 1 ) &lt; X  ( t,  X  t relation gives no indication whether the difference between t and  X  t 1 , or between t and  X  t 2 , or between  X  t 1 and sent chance or significant differences. To this end, instead of  X  ( t,  X  t ) we report the proportion of inversions between t and  X  t , as well as the proportion of significant inversions and the power of the estimate  X  t . An inversion between filters f f occurs if f 1 &gt;f 2 in t while f 1 &lt;f 2 in  X  t .Aninversionis significant if a statistical test determines that f 1 &lt;f ( p&lt; X  ), for some small  X  .Thepowerof  X  t is the fraction of pairs f 1 &lt;f 2 such that p&lt; X  . A good ranking would yield high power, and low significant inversions when compared to the true ranking. If the proportion of significant inver-sionsislessthan  X , we cannot reject the null hypothesis that the difference is due to chance. Unless otherwise stated, we assume  X  =0 . 05.

Table 8 shows the rankings of 17 subject filters (described in table 5), ranked by tournament using threshold-adjusted differential comparison using each of the three labels for ad-judication. Due to space limitations, we include statistical p-values for only one ranking, and then only for the differ-ences between adjacent filters in the ranking (e.g., in the adj column, osbfm &lt; wat1m ( p&lt; 0 . 03)). The power of the three rankings is given in the bottom row (e.g., the adj rank-ing has power 0 . 88 because 122 of the 136 pairings yield a significant difference). None of the inversions between any pair of rankings is significant ( p&lt; 0 . 05) according to either of the rankings.
We first consider the problem of measuring the filters X  threshold adjusted error rates that were used for ranking in the previous section. Later, we consider the TREC mea-sures. Table 9 estimates threshold adjusted error rates, us-ing adj and trec, respectively, as ground truth. We have not Table 9: Threshold adjusted error estimates using trec and adj labels as ground truth. formally computed the power of the rankings resulting from these estimates; however, we may conclude from the overlap-ping confidence intervals of all but two adjacent filters, the power is, as expected, much lower than that achieved using differential comparison. That said, there are no significant inversions between the rankings. The error rates are, as pre-dicted, substantially higher using the adj labels. For both labelings, the estimated filter error rates are comparable to known errors in the labels of 2 . 7% and 5 . 2%, respectively. The best we can determine from these results is that tft1-0.5, for example, likely has a true error rate between about 0 . 9% and 3 . 6%.

We can do better. A differential comparison between tft1-0.5 and the trec labels (adjudicated using adj) shows tft1-0.5 to be more accurate ( p&lt; 0 . 003). So the trec labeling serves better as an upper bound on the filter X  X  error rate than as ground truth. And if tft1-0.5 better approximates truth, why not use its results as ground truth? One possible objection is that results may be biased in favor of similar filters, and against dissimilar ones. One way to reduce bias is use a committee of filters to create the labels. Lynam and Cormack [20] have shown that filter fusion can be expected to outperform any single filter, even when the performance of the filters varies by several orders of magnitude (although we know in this case from evaluation using existing labels, that none of the filters is terrible). The best reported fu-sion method first adjusts the scores to estimate log-odds, and then combines the scores with on-line logistic regres-sion. The log-odds adjustment replaces the score s i for the i th message by The resulting scores, one per filter, comprise the input fea-ture vector for adaptive logistic regression. We applied this technique, and also performed threshold adjustment on the result so that fpr = fnr = LAM . The categorical results of this effort were used to form the pseudo-gold standard lmns. Adjudicated using adj, a differential comparison fails to Table 10: Logistic average misclassification esti-mates using lmns and trec labels as ground truth. show a significant difference between tft1-0.5 and lmns ( p&lt; 0 . 8). Adjudicated using trec, lmns is clearly superior ( p 0 . 000). The ranking yielded by lmns shows no significant in-versions from those reported above. Table 11 reports threshold-adjusted error using lmns as ground truth, and also reports (1-AUC)(%). We note that the error reported for tft1-0.5 is at the low end of the range we guessed based on mea-surements using the trec and adj labels. The AUC measures appear reasonable  X  the noise-tolerant methods do much bet-ter than the others, but still not as well as the others when they are trained and evaluated on the trec labels. In fact, the AUC measures are remarkably similar to those reported by Sculley and Cormack [25], using synthetic noise.
For comparison, AUC measures using trec and adj ground truth are presented in table 12. While the larger magnitude of the estimates is to be expected, the difference in score and ranking of tft1-0.5 is remarkable. Using trec or nat labels, its performance appears medio cre, whereas by every other measure it is the best by a substantial margin. LAM scores, on the other hand, are uniformly higher with respect to the trec labels, and there are no significant inversions (see table 13).

The same method was used to prepare new ground-truth labels for the CEAS dataset, albeit with fewer filters. Fig-ure 14 presents the AUC results, while table 15 presents the LAM results using as ground truth the original CEAS labels, as well as new labels constructed by fusion. The orig-inal values and the substantial improvement of all scores are consistent with high random error in the original labels  X  comparable to the level in the TREC dataset  X  which is abated in the new labels.
A fully automatic method fuses the results of candidate filters to yield a pseudo-gold labeling that is used as ground truth in evaluating email spam filters. The pseudo-gold la-bels exhibit a lower error rate than labels obtained from natural sources including user labels and exhaustive adjudi-cation by experts. Using the labeling as ground truth for Table 11: Threshold adjusted error and ROC area estimates using pseudo-gold labels as ground truth. evaluation results in lower estimated filter error rates across the board, according to standard measures. This effect is consistent with the hypothesis that the these new estimates are more precise in absolute terms. There is some possibility that the pseudo-gold labels to some extent represent  X  X roup think X  of the subject filters, which all rely exclusively on the text of the message for classification, albeit using different algorithms and feature engineering. This concern is simi-lar to that raised with respect to the pooling method for IR evaluation. This concern applies mainly to the magnitude of reported error rates, as differential comparison is insensitive to it.

Differential comparison, which requires some adjudica-tion, may be used to demonstrate that the pseudo-gold stan-dard has a lower error rate than other available labels. And, were a radical new filter to discover errors in the labels, this situation could be discoverable by differential comparison. It is important to note that differential comparison does not require that the adjudicator be more accurate than the la-bels, just that the adjudicator be more accurate than chance. Differential comparison may also be used to rank filters di-rectly; the rankings achieved by tournament ranking, even with noisy adjudication, are quite powerful, although they yield no quantitative estimate of filter error rates.
We call into question Sculley and Cormack X  X  claim [25] that spam filters perform more poorly with natural than with random training label noise. Our results are consistent with the hypothesis that the filters perform about as well on both, and that the reported results for the best-performing filter  X  SVM with a low C parameter  X  were substantially confounded by errors in the evaluation labels. Other ap-proaches designed to mitigate label noise, including slowed learning rates and bagging, generally improved performance as expected. Our overall conclusion is that noise-tolerant spam filters perform not quite as well as the best filters with clean data, but not nearly as poorly as previously reported. Table 12: ROC area estimates using trec and nat labels as ground truth. [1] The CEAS 2008 live spam challenge. [2] Assis, F. OSBF-Lua. http://osbf-lua.luaforge.net/. [3] Bratko, A., Cormack, G. V., Filipi  X  c, B., Lynam, [4] Brodley, C. E., and Friedl, M. A. Identifying [5] Buckley, C., and Voorhees, E. M. Retrieval [6] Cormack, G. V. TREC 2006 Spam Track Overview. [7] Cormack, G. V. TREC 2007 Spam Track Overview. [8] Cormack, G. V. University of waterloo participation [9] Cormack, G. V., and Bratko, A. Batch and [10] Cormack, G. V., and Lynam, T. R. Spam corpus [11] Cormack, G. V., and Lynam, T. R. TREC 2005 [12] Cormack, G. V., and Lynam, T. R. On-line Table 13: Logistic average misclassification esti-mates using lmns and trec labels as ground truth. Table 14: ROC area estimates using natural and pseudo-gold labels as ground truth, CEAS dataset. [13] Glas,A.S.,Lijmer,J.G.,Prins,M.H.,Bonsel, [14] Goodman, J., and tau Yih, W. Online [15] Graham, P. A plan for spam. [16] Graham-Cumming, J. SpamOrHam. Virus Bulletin [17] Kocz, A., and Alspector, J. SVM-based filtering of [18] Lam, C. P., and Stork, D. G. Evaluating classifiers [19] Lynam, T., and Cormack, G. Trec spam filter Table 15: Logistic average misclassification rate us-ing natural and pseudo-gold labels as ground truth, CEAS dataset. [20] Lynam, T. R., and Cormack, G. V. On-line spam [21] Mojdeh, M., and Cormack, G. V. A mail client [22] Raymond, E. S., Relson, D., Andree, M., and [23] Rebbapragada, U., and Brodley, C. E. Class [24] Robinson, G. A statistical approach to the spam [25] Sculley, D., and Cormack, G. V. Filtering spam [26] Sculley, D., and Wachman, G. M. Relaxed online [27] Sculley, D., and Wachman, G. M. Relaxed online [28] Sebastiani, F. Machine learning in automated text [29] Swets, J. A. Effectiveness of information retrieval [30] tau Yih, W., McCann, R., and Ko lcz, A.

