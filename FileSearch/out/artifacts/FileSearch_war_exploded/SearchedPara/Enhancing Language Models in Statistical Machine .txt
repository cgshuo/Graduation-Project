 Language model is one of the most important knowledge sources for statistical machine transla-tion (SMT) (Brown et al., 1993). The standard n -gram language model (Goodman, 2001) assigns probabilities to hypotheses in the target language conditioning on a context history of the preceding n  X  1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to ex-tend standard n -gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented.
In the first direction, more data is better. In or-der to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed man-ner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation.
The second direction digs deeply into monolin-gual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine transla-tion which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some ex-tent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies.

In this paper, we pursue the second direction with-out resorting to any linguistic resources such as a syntactic parser. With a belief that a language model that embraces a larger context provides better pre-diction ability, we learn additional information from training data to enhance conventional n -gram lan-guage models and extend their ability to capture richer contexts and long-distance dependencies. In particular, we integrate backward n -grams and mu-tual information (MI) triggers into language models in SMT.

In conventional n -gram language models, we look at the preceding n  X  1 words when calculating the probability of the current word. We henceforth call the previous n  X  1 words plus the current word as forward n -grams and a language model built on forward n -grams as forward n -gram language model. Similarly, backward n -grams refer to the succeeding n  X  1 words plus the current word. We train a backward n -gram language model on back-ward n -grams and integrate the forward and back-ward language models together into the decoder. In doing so, we attempt to capture both the preceding and succeeding contexts of the current word.
Different from the backward n -gram language model, the MI trigger model still looks at previous contexts, which however go beyond the scope of for-ward n -grams. If the current word is indexed as w i , the farthest word that the forward n -gram includes is w i  X  n +1 . However, the MI triggers are capable of detecting dependencies between w i and words from w 1 to w i  X  n . By these triggers ( k  X  i  X  n ), we can capture long-distance dependen-cies that are outside the scope of forward n -grams.
We integrate the proposed backward language model and the MI trigger model into a state-of-the-art phrase-based SMT system. We evaluate the effectiveness of both models on Chinese-to-English translation tasks with large-scale training data. Compared with the baseline which only uses the forward language model, our experimental re-sults show that the additional backward language model is able to gain about 0.5 BLEU points, while the MI trigger model gains about 0.4 BLEU points. When both models are integrated into the decoder, they collectively improve the performance by up to 1 BLEU point.

The paper is structured as follows. In Section 2, we will briefly introduce related work and show how our models differ from previous work. Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, de-scribe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an in-depth analysis. In the end, we conclude in Section 7. Previous work devoted to improving language mod-els in SMT mostly focus on two categories as we mentioned before 1 : large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based lan-guage models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philoso-phy is fundamentally different from them in that we build contextually-informed language models by us-ing backward n -grams and MI triggers, we discuss previous work that explore these two techniques (backward n -grams and MI triggers) in this section.
Since the context  X  X istory X  in the backward lan-guage model (BLM) is actually the future words to be generated, BLM is normally used in a post-processing where all words have already been gener-ated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confidence measure to detect wrongly recognized words in speech recog-nition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sen-tences are proceeded from the ending to the begin-ning. Our BLM is different from theirs in that we ac-cess the BLM during decoding (rather than after de-coding) where source sentences are still proceeded from the beginning to the ending.

Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected accord-ing to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n -gram lan-guage model and an MI trigger model. The latter model measures the mutual information of distance-dependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, es-pecially by Zhou X  X  MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one.

Raybaud et al. (2009) use MI triggers in their con-fidence measures to assess the quality of translation results after decoding. Our method is different from theirs in the MI calculation and trigger pair selec-tion. Mauser et al. (2009) propose bilingual triggers where two source words trigger one target word to improve lexical choice of target words. Our analysis (Section 6) show that our monolingual triggers can also help in the selection of target words. Given a sequence of words w m 1 = ( w 1 ...w m ) , a standard forward n -gram language model assigns a probability P f ( w m 1 ) to w m 1 as follows. where the approximation is based on the n th order Markov assumption. In other words, when we pre-dict the current word w i , we only consider the pre-ceding n  X  1 words w i  X  n +1 ...w i  X  1 instead of the whole context history w 1 ...w i  X  1 .

Different from the forward n -gram language model, the backward n -gram language model as-signs a probability P b ( w m 1 ) to w m 1 by looking at the succeeding context according to 3.1 Training For the convenience of training, we invert the or-der in each sentence in the training data, i.e., from the original order ( w 1 ...w m ) to the reverse order ( w m ...w 1 ) . In this way, we can use the same toolkit that we use to train a forward n -gram language model to train a backward n -gram language model without any other changes. To be consistent with training, we also need to reverse the order of trans-lation hypotheses when we access the trained back-ward language model 2 . Note that the Markov con-text history of Eq. (2) is w i + n  X  1 ...w i +1 instead of w i +1 ...w i + n  X  1 after we invert the order. The words are the same but the order is completely reversed. 3.2 Decoding In this section, we will present two algorithms to integrate the backward n -gram language model into two kinds of phrase-based decoders respec-tively: 1) a CKY-style decoder that adopts bracket-ing transduction grammar (BTG) (Wu, 1997; Xiong et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic program-ming algorithm to integrate a forward bigram lan-guage model with inversion transduction grammar. His algorithm is then adapted and extended for inte-grating forward n -gram language models into syn-chronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse 2. We adopt a different way to calculate language
Before we introduce the integration algorithms, we define three functions P , L , and R on strings (in a reverse order) over the English terminal alphabet T . The function P is defined as follows.

This function consists of two parts:  X  The first part ( a ) calculates incomplete n -gram  X  The second part ( b ) calculates complete n -The function is different from Chiang X  X  p func-tion in that his function p only calculates language model probabilities for the complete n -grams. Since we calculate backward language model probabilities during a beginning-to-ending (left-to-right) decod-ing process, the succeeding context for the current word is either yet to be generated or incomplete in terms of n -grams. The P function enables us to utilize incomplete succeeding contexts to approxi-mately predict words. Once the succeeding con-texts are complete, we can quickly update language model probabilities in an efficient way in our algo-rithms.

The other two functions L and R are defined as follows
The L and R function return the leftmost and right-most n  X  1 words from a string in a reverse order respectively.

Following Chiang (2007), we describe our algo-rithms in a deductive system. We firstly show the algorithm 3 that integrates the backward language model into a BTG-style decoder (Xiong et al., 2006) in Figure 1. The item [ A, i, j ; l | r ] indicates that a BTG node A has been constructed spanning from i to j on the source side with the leftmost | rightmost n  X  1 words l | r on the target side. As mentioned be-fore, all target strings assessed by the defined func-tions ( P , L , and R ) are in an inverted order (de-noted by e ). We only display the backward lan-guage model probability for each item, ignoring all other scores such as phrase translation probabilities. The Eq. (8) in Figure 1 shows how we calculate the backward language model probability for the ax-iom which applies a BTG lexicon rule to translate a source phrase c into a target phrase e . The Eq. (9) and (10) show how we update the backward lan-guage model probabilities for two inference rules which combine two neighboring blocks in a straight and inverted order respectively. The fundamental theories behind this update are
Whenever two strings e 1 and e 2 are concatenated in a straight or inverted order, we can reuse their P values ( P ( e programming. Only the probabilities of boundary words (e.g., R ( e 2 ) L ( e 1 ) in Eq. (6)) need to be re-calculated since they have complete n -grams after the concatenation. Table 1 shows values of P , L , and R in a 3-gram example which helps to verify Eq. (6). These two equations guarantee that our algorithm can correctly compute the backward lan-guage model probability of a sentence stepwise in a dynamic programming framework. 4
The theoretical time complexity of this algorithm is O ( m 3 | T | 4( n  X  1) ) because in the update parts in Eq. (6) and (7) both the numerator and denomina-tor have up to 2( n  X  1) terminal symbols. This is the same as the time complexity of Chiang X  X  language model integration (Chiang, 2007).

Figure 2 shows the algorithm that integrates the backward language model into a standard phrase-based SMT (Koehn et al., 2003). V denotes a cover-age vector which records source words translated so far. The Eq. (11) shows how we update the back-ward language model probability for a partial hy-pothesis when it is extended into a longer hypothesis by a target phrase translating an uncovered source c/e |R ( e )] : P ( e ) ( e ) [ A 2 , k + 1 , j ; L ( e 2 ) |R ( e 2 )] : P ( e 2 )
P ( e
P ( e ) [ A 2 , k + 1 , j ; L ( e 2 ) |R ( e 2 )] : P ( e 2 )
P ( e segment. This extension on the target side is simi-lar to the monotone combination of Eq. (9) in that a newly translated phrase is concatenated to an early translated sequence. It is well-known that long-distance dependencies be-tween words are very important for statistical lan-guage modeling. However, n -gram language models can only capture short-distance dependencies within an n -word window. In order to model long-distance dependencies, previous work such as (Rosenfeld et al., 1994) and (Zhou, 2004) exploit trigger pairs. A trigger pair is defined as an ordered 2-tuple ( x, y ) where word x occurs in the preceding context of word y . It can also be denoted in a more visual man-ner as x  X  y with x being the trigger and y the triggered word 5 .

We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y , which is defined as follows
Zhou (2004) proposes a new language model en-hanced with MI trigger pairs. In his model, the prob-ability of a given sentence w m 1 is approximated as
There are two components in his model. The first component is still the standard n -gram language model. The second one is the MI trigger model which multiples all exponential PMI values for trig-ger pairs where the current word is the triggered word and all preceding words outside the n -gram window of the current word are triggers. Note that his MI trigger model is distance-dependent since trigger pairs ( w k , w i ) are sensitive to their distance i  X  k  X  1 (zero distance for adjacent words). There-fore the distance between word x and word y should be taken into account when calculating their PMI.
In this paper, for simplicity, we adopt a distance-independent MI trigger model as follows
We integrate the MI trigger model into the log-linear model of machine translation as an additional knowledge source which complements the standard n -gram language model in capturing long-distance dependencies. By MERT (Och, 2003), we are even able to tune the weight of the MI trigger model against the weight of the standard n -gram language model while Zhou (2004) sets equal weights for both models. 4.1 Training We can use the maximum likelihood estimation method to calculate PMI for each trigger pair by tak-ing counts from training data. Let C ( x, y ) be the co-occurrence count of the trigger pair ( x, y ) in the training data. The joint probability of ( x, y ) is cal-culated as
The marginal probabilities of x and y can be de-duced from the joint probability as follows Since the number of distinct trigger pairs is O ( | T | 2 ) , the question is how to select valuable trig-ger pairs. We select trigger pairs according to the following three steps 1. The distance between x and y must not be less 2. C ( x, y ) &gt; c . In all our experiments we set c = 3. Finally, we only keep trigger pairs whose PMI 4.2 Decoding The MI trigger model of Eq. (14) can be directly integrated into the decoder. For the standard phrase-based decoder (Koehn et al., 2003), whenever a par-tial hypothesis is extended by a new target phrase, we can quickly retrieve the pre-computed PMI value for each trigger pair where the triggered word lo-cates in the newly translated target phrase and the trigger is outside the n -word window of the trig-gered word. It X  X  a little more complicated to in-tegrate the MI trigger model into the CKY-style phrase-based decoder. But we still can handle it by dynamic programming as follows where M I ( e 1  X  e 2 ) represents the PMI values in which a word in e 1 triggers a word in e 2 . It is defined as follows In this section, we conduct large-scale experiments on NIST Chinese-to-English translation tasks to evaluate the effectiveness of the proposed backward language model and MI trigger model in SMT. Our experiments focus on the following two issues: 1. How much improvements can we achieve by 2. Can we obtain a further improvement if we 5.1 System Overview Without loss of generality 6 , we evaluate our models in a phrase-based SMT system which adapts brack-eting transduction grammars to phrasal translation (Xiong et al., 2006). The log-linear model of this system can be formulated as where D denotes a derivation, r l 1 ..n lexicon rules which translate source phrases to tar-get phrases, and r m 1 ..n combine two neighboring blocks into a larger block in a straight or inverted order. The translation model M T consists of widely used phrase and lex-ical translation probabilities (Koehn et al., 2003). The reordering model M R predicts the merging or-der (straight or inverted) by using discriminative contextual features (Xiong et al., 2006). P f L is the standard forward n -gram language model.

If we simultaneously integrate both the backward language model P bL and the MI trigger model M I into the system, the new log-linear model will be formulated as 5.2 Experimental Setup Our training corpora 7 consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We used all corpora to train our translation model and smaller corpora without the United Na-tions corpus to build a maximum entropy based re-ordering model (Xiong et al., 2006).

To train our language models and MI trigger model, we used the Xinhua section of the En-glish Gigaword corpus (306 million words). Firstly, we built a forward 5-gram language model using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. Then we trained a back-ward 5-gram language model on the same monolin-gual corpus in the way described in Section 3.1. Fi-nally, we trained our MI trigger model still on this corpus according to the method in Section 4.1. The trained MI trigger model consists of 2.88M trigger pairs.

We used the NIST MT03 evaluation test data as the development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric, which uses the shortest reference sentence length for the brevity penalty. Statistical significance in BLEU differences is tested by paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results The experimental results on the two NIST test sets are shown in Table 2. When we combine the back-ward language model with the forward language model, we obtain 0.49 and 0.56 BLEU points over the baseline on the MT-04 and MT-05 test set respec-tively. Both improvements are statistically signifi-cant ( p &lt; 0 . 01 ). The MI trigger model also achieves statistically significant improvements of 0.33 and 0.44 BLEU points over the baseline on the MT-04 and MT-05 respectively.

When we integrate both the backward language model and the MI trigger model into our system, we obtain improvements of 1.09 and 0.71 BLEU points over the single forward language model on the MT-04 and MT-05 respectively. These improve-ments are larger than those achieved by using only one model (the backward language model or the MI trigger model). In this section, we will study more details of the two models by looking at the differences that they make on translation hypotheses. These differences will help us gain some insights into how the presented models improve translation quality.

Table 3 shows an example from our test set. The italic words in the hypothesis generated by using the backward language model (F+B) exactly match the reference. However, the italic words in the base-line hypothesis fail to match the reference due to the incorrect position of the word  X  X ecree X  (  X  X  X  ). We calculate the forward/backward language model score (the logarithm of language model probability) for the italic words in both the baseline and F+B hy-pothesis according to the trained language models. The difference in the forward language model score is only 1.58, which may be offset by differences in other features in the log-linear translation model. On the other hand, the difference in the backward lan-guage model score is 3.52. This larger difference may guarantee that the hypothesis generated by F+B
Source  X  X  X  X  X  X  X  X  X  X  ,  X  X  X  X  X  X  X  X  Baseline Beijing Youth Daily reported that F+B Beijing Youth Daily reported that Reference Beijing Youth Daily reported that is better enough to be selected as the best hypothe-sis by the decoder. This suggests that the backward language model is able to provide useful and dis-criminative information which is complementary to that given by the forward language model.

In Table 4, we present another example to show how the MI trigger model improves translation qual-ity. The major difference in hypotheses of this ex-ample is the word choice between  X  X s X  and  X  X as X . The new system enhanced with the MI trigger model (F+M) selects the former while the baseline selects the latter. The forward language model score for the baseline hypothesis is -26.41, which is higher than the score of the F+M hypothesis -26.67. This could be the reason why the baseline selects the word  X  X as X  instead of  X  X s X . As can be seen, there is an-other  X  X s X  in the preceding context of the word  X  X as X  in the baseline hypothesis. Unfortunately, this word  X  X s X  is located just outside the scope of the preceding 5-gram context of  X  X as X . The forward 5-gram lan-guage model is hence not able to take it into account when calculating the probability of  X  X as X . However, this is not a problem for the MI trigger model. Since  X  X s X  and  X  X as X  rarely co-occur in the same sentence, the PMI value of the trigger pair (is, was) 8 is -1.03
Source  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ,  X 
Baseline Self-Defense Force  X  X  trip is remark-
F+M Self-Defense Force  X  X  trip is remark-
Reference The Self-Defense Forces X  trip while the PMI value of the trigger pair (is, is) is as high as 0.32. Therefore our MI trigger model selects  X  X s X  rather than  X  X as X . 9 This example illustrates that the MI trigger model is capable of selecting correct words by using long-distance trigger pairs. We have presented two models to enhance the abil-ity of standard n -gram language models in captur-ing richer contexts and long-distance dependencies that go beyond the scope of forward n -gram win-dows. The two models have been integrated into the decoder and have shown to improve a state-of-the-art phrase-based SMT system. The first model is the backward language model which uses back-ward n -grams to predict the current word. We in-troduced algorithms that directly integrate the back-ward language model into a CKY-style and a stan-dard phrase-based decoder respectively. The sec-ond model is the MI trigger model that incorporates long-distance trigger pairs into language modeling.
Overall improvements are up to 1 BLEU point on the NIST Chinese-to-English translation tasks with large-scale training data. Further study of the two models indicates that backward n -grams and long-distance triggers provide useful information to im-prove translation quality.

In future work, we would like to integrate the backward language model into a syntax-based sys-tem in a way that is similar to the proposed algo-rithm shown in Figure 1. We are also interested in exploring more morphologically-or syntactically-informed triggers. For example, a verb in the past tense triggers another verb also in the past tense rather than the present tense.

