 Techniques for automatically summarising written text have been actively investigated in the field of natural language processing, and more recently new techniques have been developed for speech sum-marisation (Kikuchi et al., 2003). However it is still very hard to obtain good quality summaries. Moreover, recognition accuracy is still around 30% on spontaneous speech tasks, in contrast to speech read from text such as broadcast news. Spontaneous speech is characterised by disfluencies, repetitions, repairs, and fillers, all of which make recognition and consequently speech summarisation more diffi-cult (Zechner, 2002). In a previous study (Chatain et al., 2006), linguistic model (LiM) adaptation us-ing different types of word models has proved use-ful in order to improve summary quality. However sparsity of the data available for adaptation makes it difficult to obtain reliable estimates of word n-gram probabilities. In speech recognition, class models are often used in such cases to improve model ro-bustness. In this paper we extend the work previ-ously done on adapting the linguistic model of the speech summariser by investigating class models. We also use a wider variety of objective evaluation metrics to corroborate results. The summarisation system used in this paper is es-sentially the same as the one described in (Kikuchi et al., 2003), which involves a two step summarisa-tion process, consisting of sentence extraction and sentence compaction. Practically, only the sentence extraction part was used in this paper, as prelimi-nary experiments showed that compaction had little impact on results for the data used in this study.
Important sentences are first extracted accord-ing to the following score for each sentence W = w 1 , w 2 , ..., w n , obtained from the automatic speech recognition output: S ( W ) = where N is the number of words in the sentence W , and C ( w score, the significance score and the linguistic score of word w respective weighting factors of those scores, deter-mined experimentally.

For each word from the automatic speech recogni-tion transcription, a logarithmic value of its posterior probability, the ratio of a word hypothesis probabil-ity to that of all other hypotheses, is calculated using a word graph obtained from the speech recogniser and used as a confidence score.

For the significance score, the frequencies of oc-currence of 115k words were found using the WSJ and the Brown corpora.

In the experiments in this paper we modified the linguistic component to use combinations of dif-ferent linguistic models. The linguistic component gives the linguistic likelihood of word strings in the sentence. Starting with a baseline LiM (LiM we perform LiM adaptation by linearly interpolat-ing the baseline model with other component mod-els trained on different data. The probability of a given n-gram sequence then becomes: where P the probability assigned by model k .
 In the case of a two-sided class-based model, where P word w P a certain word class C ( w of word classes, C ( w
Different types of component LiM are built, com-ing from different sources of data, either as word or class models. The LiM are then combined for adaptation using linear inter-polation as in Equation (2). The linguistic score is then computed using this modified probability as in Equation (4): 3.1 Summarisation Accuracy To automatically evaluate the summarised speeches, correctly transcribed talks were manually sum-marised, and used as the correct targets for evalua-tion. Variations of manual summarisation results are merged into a word network, which is considered to approximately express all possible correct summari-sations covering subjective variations. The word ac-curacy of automatic summarisation is calculated as the summarisation accuracy (SumACCY) using the word network (Hori et al., 2003): Accuracy = ( Len  X  Sub  X  Ins  X  Del ) /Len  X  100[%] , where Sub is the number of substitution errors, Ins is the number of insertion errors, Del is the number of deletion errors, and Len is the number of words in the most similar word string in the network. 3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. ROUGE F-measure scores are given for ROUGE-2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4 (skip-bigram), using the model average (average score across all references) metric. Experiments were performed on spontaneous speech, using 9 talks taken from the Translanguage English Database (TED) corpus (Lamel et al., 1994; Wolfel and Burger, 2005), each transcribed and manually summarised by nine different humans for both 10% and 30% summarization ratios. Speech recognition transcriptions (ASR) were obtained for each talk, with an average word error rate of 33.3%.
A corpus consisting of around ten years of con-ference proceedings (17.8M words) on the subject of speech and signal processing is used to generate the LiM gorithm in (Ney et al., 1994).

Different types of component LiM are built and combined for adaptation as described in Section 2.
The first type of component linguistic models are built on the small corpus of hand-made summaries described above, made for the same summarisation ratio as the one we are generating. For each talk the hand-made summaries of the other eight talks (i.e. 72 summaries) were used as the LiM training corpus. This type of LiM is expected to help gener-ate automatic summaries in the same style as those made manually.
The second type of component linguistic models are built from the papers in the conference proceed-ings for the talk we want to summarise. This type of LiM, used for topic adaptation, is investigated be-cause key words and important sentences that appear in the associated paper are expected to have a high information value and should be selected during the summarisation process.

Three sets of experiments were made: in the first experiment (referred to as Word), LiM component models are word models, as introduced in (Chatain et al., 2006). For the second one (Class), both LiM models built using exactly the same data as the word models. For the third experiment (Mixed), the LiM is an interpolation of class and word models, while the component LiMs are class models.

To optimise use of the available data, a rotating form of cross-validation (Duda and Hart, 1973) is used: all talks but one are used for development, the remaining talk being used for testing. Summaries from the development talks are generated automati-cally by the system using different sets of parameters and the LiM the set of parameters which maximises the develop-ment score for the LiM ing talk. The purpose of the development phase is to choose the most effective combination of weights  X  talk using its set of optimised parameters is then evaluated using the same metric, which gives us our baseline for this talk. Using the same parameters as those that were selected for the baseline, we gener-ate summaries for the lectures in the development set for different LiM interpolation weights  X  between 0 and 1 in steps of 0.1, were investigated for the latter, and an optimal set of  X  Using these interpolation weights, as well as the set of parameters determined for the baseline, we gen-erate a summary of the test talk, which is evaluated using the same evaluation metric, giving us our fi-nal adapted result for this talk. Averaging those re-sults over the test set (i.e. all talks) gives us our final adapted result.

This process is repeated for all evaluation metrics, and all three experiments (Word, Class, and Mixed).
Lower bound results are given by random sum-marisation (Random) i.e. randomly extracting sen-tences and words, without use of the scores present in Equation (1) for appropriate summarisation ratios. 5.1 TRS Results Initial experiments were made on the human tran-scriptions (TRS), and results are given in Table 1. Experiments on word models (Word) show relative improvements in terms of SumACCY of 7.5% and 2.1% for the 10% and 30% summarisation ratios, re-spectively. ROUGE metrics, however, do not show any significant improvement.
 Using class models (Class and Mixed), for all ROUGE metrics, relative improvements range from 3.5% to 13.4% for the 10% summarisation ratio, and from 8.6% to 16.5% on the 30% summarisation ra-tio. For SumACCY, relative improvements between 11.5% to 12.9% are observed. 5.2 ASR Results ASR results for each experiment are given in Ta-ble 2 for appropriate summarisation ratios. As for the TRS, LiM adaptation showed improvements in terms of SumACCY, but ROUGE metrics do not cor-roborate those results for the 10% summarisation ra-tio. Using class models, for all ROUGE metrics, rel-ative improvements range from 6.0% to 22.2% and from 7.4% to 20.0% for the 10% and 30% summari-sation ratios, respectively. SumACCY relative im-provements range from 7.6% to 15.9%. Compared to previous experiments using only word models, improvements obtained using class models are larger and more significant for both ROUGE and SumACCY metrics. This can be explained by the fact that the data we are performing adaptation on is very sparse, and that the nine talks used in these experiments are quite different from each other, es-pecially since the speakers also vary in style. Class models are more robust to this spontaneous speech aspect than word models, since they generalise bet-ter to unseen word sequences.
 There is little difference between the Class and Mixed results, since the development phase assigned most weight to the class model component in the Mixed experiment, making the results quite similar to those of the Class experiment. In this paper we have investigated linguistic model adaptation using different sources of data for an au-tomatic speech summarisation system. Class mod-els have proved to be much more robust than word models for this process, and relative improvements ranging from 6.0% to 22.2% were obtained on a va-riety of evaluation metrics on summaries generated from automatic speech recogniser transcriptions.
Acknowledgements : The authors would like to thank M. W  X  olfel for the recogniser transcriptions and C. Hori for her work on two stage summarisa-tion and gathering the TED corpus data. This work is supported by the 21st Century COE Programme.
