 This paper presents a hierarchical generative model that captures the latent relation of cause and effect underlying user behavioral-originated data such as papers, twitter, and purchase history. Our proposal, the Latent Interest Topic model (LIT), introduces a latent variable into each docu-ment and each author layer in a coherent generative model. We call the former variable the document class, and the latter variable the author class, where these classes are in-dicator variables that allow the inclusion of different types of probability, and can be shared over documents with sim-ilar content and authors with similar interests, respectively. Significantly, unlike other works, LIT differentiates, respec-tively, document topics and user interests by using these classes. Consequently, LIT is superior to previous models in explaining the causal relationships behind the data by merging similar distributions; it also makes the computa-tion process easier. Experiments on a research paper corpus show that the proposed model can well capture document and author classes, and reduce the dimensionality of docu-ments to a low-dimensional author-document space, making it useful as a generative model.
 H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods Algorithms, experimentation Topic Modeling, Graphical Models, Latent Variable Model-ing, Information Extraction
Since the number of digital libraries and Web sourced data now exceed the capabilities of any individual, meth-ods that can automatically extract useful information from these data, have become an increasingly important research goal. In particular, extracting both the contents of docu-ments and their authors X  interests has received considerable attention in recent years [4]. However, most documents have not been assigned labels representing their contents. This has triggered the proposal of various topic models that can infer latent topics automatically, where each topic is a la-tent variable that has a probability distribution over words. These topic models represent documents as topic distribu-tions rather than bags of words. Such models provide useful descriptions for the generative process of various data and have been applied for information retrieval [16], social net-work analysis [9] and collaborative filtering [8].
Attention is being focused on how to capture user interest in several fields(e.g., Internet marketing, advertising sales and branding) [7, 10, 13]. If such as model were available, it would be possible to infer which topics each user prefers and measure the similarity between users in terms of their interests. Moreover, such models could incorporate these in-terests into knowledge/information services that offer higher quality. For example, they would yield information retrieval services that could match each user X  X  interests and person-alize the search results. Additionally, identifying each user X  X  interests helps us in extracting social networks by finding like-minded users and thus making more friends. Against this background, our aim is to capture, simultaneously, both the interests and topics that lie hidden in user behavior-originated data, and use these latent structures to explain the generative process.

In order to discover which topics are preferred by given authors, the previous topic models handle authors as an ob-served variable. For example, although the Author-Topic(AT) [12] model links authors to observed words in documents via latent topics [15], it groups all papers associated with a given author by using a single topic distribution associated with this author. Author-Persona-Topic(APT) [11] introduces a persona, which is also a latent variable, under a single given author. Thus, this model allows each author X  X  documents to be divided into one or more clusters, each with its own separate topic distribution specific to that persona. Since these models introduce a topic distribution for each author, a topic distribution is specific to one author, and thus is often too restrictive for modeling the author X  X  preference. That is to say, both these models estimate the topic distri-bution from just the documents associated with the author; inter-similarity of documents is not used, while they learn the word distribution from all documents with similar top-ics. Therefore, it is essential to create a coherent model that can learn the topic distribution, since the distinct top-ics may be accurately revealed by learning across the groups of documents and authors.

This paper presents the Latent Interest Topic(LIT) model; it extends both AT and APT to form a coherent model that captures how the structure replicates the authors X  interests and topics. A key advance is its introduction of the addi-tional layer of  X  X atent interest X . Here, we hypothesize that authors X  interests are represented as latent variables in the same way that documents are represented as topics in topic models. Following this hypothesis, we first introduce a la-tent variable with a probability distribution over topics, the document class, into each document. This class allows us to capture the topic co-occurrence patterns in the data and thus can be shared among documents with similar content. Second, we introduce another latent variable with a proba-bility distribution over these document classes, the author class, into each author. This class is also shared among authors with similar; it allows the same latent variable to represent authors who write documents that look to have similar content.

The contribution of this paper includes: 1. LIT is a generalization of author X  X  interest models such 2. LIT reduces calculation overhead. In learning an au-We demonstrate the efficacy of LIT through experiments on various data sets and show that this model can describe the generative process of a wide variety of user behavioral data.
Recent work on topic models has investigated richer struc-tures for the description of inter-topic correlations. An ex-ample is hierarchical LDA (hLDA) [3], which assumes a hi-erarchical structure among topics, where topics at higher levels are more general, such as stop words and topics at lower levels are organized into topics, such as more spe-cific words. Another closely related model is the correlated topic model (CTM) [2]; it represents each document as a mixture of topics, where the mixture proportion is sampled from a logistic normal distribution. The pachinko alloca-tion model (PAM) [6] discovers topics within hierarchies by SYMBOL DESCRIPTION A number of authors H number of author classes J number of document classes T number of topics D number of documents V number of unique words A d authors associated with document d D a number of documents written by author a
N d number of word tokens in document d a i author associated with i th token in document p d persona associated with document d s a author class associated with author a c d the document class associated with document z di the topic associated with the i th token in doc-w di the i th token in document d  X  a the multinomial distribution of persona spe- X  the multinomial distribution of author class  X  a the multinomial distribution of document  X  h the multinomial distribution of document  X  j the multinomial distribution of topics specific  X  t the multinomial distribution of words specific  X ,  X ,  X ,  X  the fixed parameters of symmetric Dirichlet using a directed acyclic graph structure to represent topic correlations.

Although documents are written by various authors, and thus the correlation varies with the author, all these models use only distributions over words for describing inter-topic correlations; they ignore the similarity of documents. Since these models and LIT are complementary, we can create hierarchical LIT.
This section details our related models AT and APT, and our AIT and LIT. Table 1 shows the notations used in this paper. Figure 1 shows the generative process with the graph-ical models.
 Here, we describe the concept of topic models by using Figure 1. In topic models, the latent variable represent-ing topic Z permits us to group wo rds that are commonly used across the documents associated with each topic, and models documents in abbreviated form. For example, terms such as  X  X robability X ,  X  X rror X ,  X  X pproximation X  and  X  X stima-tion X  appear frequently in the documents associated with the field of statistics, and can be represented as one topic copies of the document and associate one copy with each author. Feature AT APT AIT LIT
Number of possible in-terests given to each author a
Number of all possible interests
Number of parameters T  X  ( A + V ) ( T +1)  X  ( A + X  a | D  X 
V A  X  J + J  X  T + T  X  V H + H  X  J + J  X  T + T  X  V variable representing  X  X tatistics X . Namely, V kinds of words can be replaced by T ( T is generally fewer than V ) kinds of latent variable and each document can thus be represented by topic distributions rather than bags of words, where each topic is a latent variable that has a probability distribution over words. These models consider the generative process of documents as follows: A word is generated from each se-lected topic and this process continues until all tokens of each document have been! filled by words. Since topic mod-els are also based on the assumption that a bag of words can represent each document as an unordered collection of words, the grammar and even word order are disregarded. To extract information about authors automatically, Author-Topic(AT) model links authors to observed words in docu-ments via latent topics. In this model, each author a i is associated with a multinomial distribution over topics, rep-resented by  X  a , and each topic is associated with a multi-nomial distribution over words, represented by  X  .Adoc-ument with multiple authors has a distribution over topics that is a mixture of the distributions associated with the authors. When generating a document, this model samples author a uniformly from A d ,thentopic z from multinomial distribution  X  associated with author a ,andthenword w from multinomial topic distribution  X  associated with topic z . This sampling process is repeated N times to form doc-ument d .
Author-Persona-Topic(APT) is an extended model of AT, that is motivated by the observation that authors frequently write about several distinct subject areas and combinations thereof. The difference between APT and AT is that rather than grouping all papers by a given author under a single topic distribution, APT allow each author X  X  documents to be divided into one or more clusters, each with its own sep-arate topic distribution. These clusters represent different  X  X ersonas X  under which a single author writes, each repre-senting a different topical intersection of expertise. Since authors have varying numbers of personas, the same Dirich-let parameter cannot be used to draw all distributions over personas for every author. For example, an author with mul-tiple personas cannot draw a distribution over those these personas from the same prior distribution. Therefore, APT defines a separate Dirichlet parameter  X  a for every author; the number of dimensions equals the number of personas assigned to that author. First, we show the basic concept of our previous model, Author Interest Topic (AIT) [5], an extended model of APT. For modeling each author X  X  interest, AIT incorporates docu-ment class c d in each document layer; it provides an indica-tor variable that describes which mixture of topics each doc-ument, d takes, into d . Although both  X  a (AT) and  X  P a are associated with only authors and are only sampled for the current document, the document class can be shared among authors and is learned from all documents. Given the document class, the topic distribution of a document is generated by drawing words from a multinomial distribu-tion associated with this class instead of a document-specific mixing probability.
Second, we extend AIT into a fully generative model by adding other kinds of variables into this interests-topics rep-resentation. This model, Latent Interest Topic (LIT) adds the author class s a which provides an indicator variable that describes which mixture of document classes each author a takes, into each author a layer for modeling authors X  inter-ests. LIT represents authors associated with similar docu-ment classes by the same state variable s , while AIT gives them similar distributions  X  .

The introduction of author class turns LIT into a gen-erative model rather than a mere generalization of APT. Because LIT learns the probabilistic distribution of author classes  X  across all documents and authors, the learned  X  can give a probability over all possible author interests. Here, we explain the generative process by which an author writes a document in Figure 1 as follows. First, LIT assigns the author class s a to each author a in proportion to  X  . Second, this model samples the document class, c d ,ofeachdocu-ment d in proportion to  X  s associated with each author X  X  s and then allocates the mixture of topics in this document by using c d . Last, LIT generates words from each selected topic distribution over the vocabulary in the same way as the conventional topic models. On the other hand, both AT and APT learn author X  X  interest  X  a ,  X  P a only from his/her documents and so there is no way to infer overall interest in a given corpus and assign probability to a previously unseen document written by other authors.

LIT has an advantage over previous models in the cost of calculation. Table 2 compares the features of AT, APT, AIT and LIT with regard to inferring author X  X  interest. The dif-ference between LIT and the previous models lies in describ-ing each author X  X  interest by using author class in the same way that the topic models represent co-occurrence words as the same topic variable; this reduces the number of possible parameters without losing generality. That is, LIT repre-sents authors having similar interests as the same author class by merging these authors, which reduces the number of possible authors X  interests without losing generality. Like this class, document class allows LIT to represent documents having similar topics as the same document class by merg-ing parameters. Accordingly, as the size of training data is increased, relatively fewer parameters are needed. On the contrary, the parameters of the other models track the order of authors and so experience linear growth with the size of the training data.
Since LIT is a generalization of both AT and APT, LIT can be inferred by Gibbs sampling in the same way that is possible for both AT and APT without loss of generaliza-tion. The generative model for LIT can be described by the Bayesian hierarchical model. The Dirichlet hyper parameter  X  X  is the same as in AT and APT, except for the additional Dirichlet hyper parameter  X  , which denotes the document class distribution. A description of the generative process of LIT, explained in the previous subsection, for parameter estimation by Gibbs sampling is given below: 1. Draw multinomial  X  from Dirichlet prior  X  ; 2. Draw H multinomials  X  h from Dirichlet prior  X  ,one 3. Draw J multinomials  X  j from Dirichlet prior  X  ,onefor 4. Draw T multinomials  X  t from prior  X  ,oneforeach 5. For each author a : 6. For each document d a in author a : In this inference, we need to calculate the conditional dis-tribution. We begin with the joint distribution of the en-bution of the entire corpus is given by Since the multinomials  X ,  X  ,  X  and  X  can be adapted by the conjugate prior and then integrated out analytically, we need not sample  X ,  X  a ,  X  c and  X  z at all. Accordingly, we can integrate out these parameters in the above equation and obtain the joint distribution P ( w , s , c , z |  X ,  X ,  X ,  X  )isas follows: P ( w , s , c , z |  X ,  X ,  X ,  X  ) = =[  X   X  where n h represents the number of authors associated with author class h and n jt ( n hj , n tv ) represents the number of doc-ument class j (documents written by authors associated with class h ,word v ) assigned to topic t (document class j ,topic t ).
 Second, we need to calculate the conditional distributions P ( s a | s \ a , c , X , X  ), P ( c d | s a , c \ d , z , X , X  )and P ( z where s \ a represents the author class assignments for all to-kens except s a , c \ d represents the author class assignments for all tokens except z d ,and z \ di represents the topic as-signments for all tokens except w di .Foreachdocument, s assignment to h is sampled from P ( s a = h | s \ a , c , X , X  ). We use the chain rule and express it by P ( h | s \ a , c , X , X  ): where n j h \ a represents the number of documents written by authors who were placed in state h and assigned to j ,except a .

For each document, the predictive distribution of adding author class c d in the documents written by authors associ-ated with h to topic c d = j is given by where n hj \ d represents the number of documents written by authors associated with h and assigned to j ,except d ,and n \ di represents the total number of tokens assigned to topic t in the documents associated with document class j ,except token di .

For each token, the predictive distribution of adding word w di in document d written by a to topic z d = t is given by where n tv \ di represents the total number of tokens assigned to word v in topic t , except token di .
Here, we use CRP [1] to decide the number of author classes. In each author class sampling, the conditional dis-tribution P ( s a = h | s \ a , c , X , X  )isgivenby P ( h | c \ d ,a, z , X , X  )  X  instead of Eq 3.
We focus here on the extraction of interests from given documents, and present both qualitative and quantitative evaluations of the proposed models, and demonstrate the performance of both AIT and LIT as generative models. Here we use both document and topic distributions in the qualitative evaluation, and use both KL-Divergence and test-set perplexity for the quantitative evaluation.

The dataset used in our experiments consists of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW gathered over the last 8 years (2001-2008). We removed stop words, numbers, and the words that appeared less than five times in the corpus. Accordingly, we obtained a total set of 3078 documents and 20286 unique words from 2204 authors. Additionally, we applied both AT and APT to this data for training and comparisons.

We also used Netflix data; it consists of a set of rating records from Nov 1st, 1999 to Dec 31st, 2005. We first selected only those users who rated at least 20 movies and movies that were rated by at least 100 users.
 In our evaluation, the smoothing parameters  X  ,  X  ,  X  (APT, AIT, LIT) and  X  were set at 1/T, 1/V, 10(APT),1/J(AIT, LIT) and 1/H, respectively, -all weak symmetric priors along the previous works. We ran single Gibbs sampling chains for 10000 iterations. We performed experiments on machines with Dual Core 2.66 GHz Xeon processors.
In this section, we compare LIT to a post-hoc model in learning each distribution, and report the results. This post-hoc model first learns the topic distribution of each docu-ment by LDA, then estimates t he document class of each document by the maximum a posterior probability method over these topics, and finally estimates the author class of each author by the same method over document classes. Al-though LIT learns these distributions coherently, the post-hoc model learns them successively.

We generated a synthetic data matrix having 10 authors and 100 documents, where each author is associated with 10 documents independently and each document is represented as a vector consisting of words and their frequency. For more detail, this matrix is composed of a 10  X  2 author-author class matrix, a 2  X  10 author class-document class matrix, a 10  X  10 document class-topic matrix, and a 10  X  100 topic-word matrix. Each document vector is defined as n ( d i | a )= n d  X  p ( d i | a )= n d  X  = n d  X  where n d denotes the number of words in document d ,it was set to 100 in this experiment, and each probability on each row is defined. For example, p ( s h | a ) is the probability of author class s h given author a .Wesetthenumberof each latent variable to the number of corresponding latent variables in this dummy matrix, and trained each model over 500 iterations. Figure 2 shows the learned distribution over each variable. These results show two main points: cluster assignment and parameter estimation. We observe the following: (1)LIT learns more distinct distributions on each matrix and each distribution is more similar to that of the corresponding matrix; (2) The number of distributions learned by LIT equals that of the corresponding matrices. To conserve space, we do not show here the other cases ex-amined of more complicated and more voluminous synthetic data but they yielded similar results.
We present the document classes discovered by LIT and then demonstrate the ability of LIT to discover the mixture of the topics associated with the corresponding document class. Figure 2: (a)author-author class matrix, (b)author class-document class matrix, and (c) document class-topic matrix. (Top) Synthetic data, (Middle) LIT, and (Bottom) post-hoc. Light color indicates high probability. Since author class, document class and topic are latent variables, the order of these variables can vary among rows/columns. of topics and to filter documents by contents. Examples of these document classes from an LIT model with 100 topics, 200 document classes, and 30 author groups are shown in Ta-ble 3. This table shows 3 document classes together with la-bels, topics, and documents associated with each document class, for the 3 document classes of  X  X nformation retrieval X ,  X  X ata mining X  and  X  X ext model X . In the row of each docu-ment class c , 6 topics were identified by high P ( z | c )values; they are denoted by topic ID. For document class id 7, topic id 2 is the most likely topic, followed by topic id 8. In each topic column, 10 words are sorted in descending order of P ( w | z ). For topic id 2,  X  X cm X  is the most likely word. For each paper column, 10 papers were sampled from the docu-ments associated with each document class in proportional to the number of conference paper associated with this doc-ument class.

From this table, we observe the following: (1)Document classes can share several topic classes: Although there are more topic variables than there are document classes, each document class picks some topic variables and some of these topic variables are selected from some document classes with high probability. (2)Document class consists of specific top-ics and general topics: Specific topics are those that tend to appear in only a few kinds of document classes. Gen-eral topics tend to appear in more document classes than specific topics, and can appear with some specific topic in each document. In this table, z =2, z =8 and 28 are selected from some document classes with high probability, and are said to be general topics, while other topics(e.g., z =12, z =27 and z =33) are selected from only one document class and generate more specific words than the other topic variables. Consequently, LIT can differentiate whether each word is as-sociated with a specific or general topic, and the correlation of these topics.

Additionally, Table 4 presents the author class discovered by LIT. From this table, we observe the following: (1)The in-terpretation of Author class: Each author class is interpreted by the title or content of documents written by the author placed in this class. For example, in s = 0, authors tend to have written papers about classification, authors associ-ated with author class s = 3 have written papers about text modeling, and authors associated with author class s =16 have written papers about text data mining. (2)The effect of author class: Our hypothesis, that LIT can identify inter-pretable components with lower dimensionality than topic mixture while retaining the overall semantic and interest structure, is supported by this result. Clearly, since this classification is performed on two different classes such as topic mixture and user interest, it cannot be achieved with-out two classes.

Consequently, LIT can differentiate whether each word is associated with a specific or general topic, and the correla-tion of the topics.
To measure the ability of a model to act as a generative model, we computed test-set perplexity under estimated pa-rameters and compared the resulting values.

Perplexity, which is widely used in the language modeling community to assess the predictive power of a model, is alge-braically equivalent to the inverse of the geometric mean per-word likelihood (lower number s are better). Accordingly, perplexity monotonically decreases with log-likelihood, im-plying that lower perplexity is better since higher log-likelihood on training data means that the model fits the data better, and a higher log-likelihood on the test set implies that the model can explain the data better. That is, lower perplex-ity means that the words are less surprising to the model. For example, in purchase history data, a low perplexity on the test set means that the model captures the preference pattern for users such that the model-derived predicted pref-erence on test history data for a user would be quite close to his/her actual preferences; on the contrary, a high perplex-ity indicates that user X  X  preference on the test history would be quite different from the model X  X  prediction. A similar argument works for other data as well. The perplexity was computed for all algorithms using 100 samples from 100 dif-ferent chains using where W is the number of test words, G is the number of samples (from G different chains),  X  g t is the probability that t will be assigned by a model to document d in g and  X  g tv probability assigned by the model to word v conditioned on t in g . A lower score implies that word w d is less surprising to the model.

We computed the perplexity as follows. First, we ran-domly split each document into a test part (10%) and a learning part (remainder). For every document, the test part was held out to compute perplexity. Second, the learn-ing part was subjected to Gibbs sampling to estimate the Table 4: Author Classes from LIT on ACM papers: This table shows only the authors who were listed as the first author in this data set and who wrote more than 5 papers.
 Table 5: Perplexity of AT, APT and AIT/LIT: This difference between AIT and APT is significant ac-cording to one-tailed t-test with G = 100. Results that differ significantly from APT according to t-test p&lt; 0 . 01 , p&lt; 0 . 05 are marked with  X ** X ,  X * X  respec-tively. The number of topic variables T was fixed at 100, the number of document classes J was fixed at 200(AIT and LIT) and the number of author classes H was fixed at 30(LIT). The number of first row denotes the number of iteration in Gibbs sampling. The value of Avg means the average computing time for each iteration in Gibbs sampling. AT 1529 1488 1343 1339 1333 3.2s APT 1454 1304 1180 1059 1027 10.4s AIT 1322  X  X  X  1219  X  X  X  1106  X  X  X  990  X  X  X  965  X  X  X  11.7s
LIT 1258  X  X  X  1172  X  X  X  1010  X  X  X  925  X  X  X  920  X  X  X  12.2s parameters. Finally, a single set of topic counts was saved when a sample was taken; the log probability of test words that had never been seen before was computed in the same way as the perplexity computation of LDA.
 Table 5 shows the results of the perplexity comparison. Both AIT and LIT give significantly lower perplexity on the test set than either AT or APT, which shows that both AIT and LIT are better topic models. This is due to the ability of AIT and LIT to allow the document class to be shared across authors and to group documents under the various topic distributions rather than grouping documents by a given author or persona under a few topic distributions. This implies that the clustered documents contain less noise than otherwise. If the number of document classes is overly restricted, the difference between observed data and the data generated by the model under test increases which raises the perplexity.

To discuss the effect of the document class, we reran the perplexity comparison with different settings; the results are shown in Figure 3. From this figure, we observe the follow-ing: (1)Noise reduction: The decrease in perplexity slows in inverse proportion to the increase of number of topics, since an increase in the number of topics does not significantly af-fect topic assignment; thus assignment should be relatively invariant to an increase in topic number [14]. The document class allows AIT and LIT to group documents under the var-ious topic distributions rather than permitting various topic distributions on each document. Consequently, documents associated with the same document class have the same topic distribution. This implies that clustered documents contain less noise than otherwise, which reduces over all perplexity. (2)The number of document classes and author classes: In each document, about 80% of all tokens are occupied by a few kinds of topic class and many of these topics are reused in other documents. Consequently, we do not need many document classes, so | C | X  2 | Z | is enough. Since this is car-ried through to the number of author classes, | H | X  X  C | enough for tracking interests.
 Additionally, we compared the change in perplexity over Gibbs sampling; the results are shown in Figure 4. From this figure, we observe the following: (1)Abridgement: LIT Figure 3: Perplexity comparison of APT, AIT and LIT run with different settings of topic Z and docu-ment class C : In this figure, the x -axis denotes the Z value and the y -axis denotes the perplexity value. The number in parenthesis denotes the C value. Figure 4: Perplexity comparison of APT, AIT and LIT run on different settings of the iterations in Gibbs sampling: In this figure, the x -axis denotes the number of iterations in Gibbs sampling and the y -axis denotes the perplexity value. The number in parenthesis denotes the C value. Z was set to 100 for all models achieves low perplexity with fewer iterations than APT. Since the document class can abridge topics in each docu-ment using  X  c associated with document class, LIT handles topics in each document in a practical manner by using the document class. Consequently, the increase in the number of document classes impacts perplexity more significantly than the number of topics.
Unlike AT and APT, LIT can compute text perplexity for the overall interest, since the LIT model assigns the proba-bility of a previous unseen document. Accordingly, we mod-ified the above equation for measuring text perplexity based on the author class as follows: PPX =exp(  X  1 where  X  g t | j is the multinomial distribution of topics specific to class j of given document d . Since this equation allows us to infer which interest unseen authors will have in the corpus and assign probability to these documents, we can predict which interests will be preferred in future or the interest trends found in the latest document corpus. For this ex-periment, we randomly selecte d5%ofalldocumentsastest documents and learned  X  from the remaining documents. We then used ten-fold cross validation and computed test-set perplexity; the results of averaged perplexity are shown in Table 6.
 Table 6: Perplexity comparison of LIT: The number of latent variables is the same as given in Table 5
We measure average topic, word distribution separations between all pairs of different classes and discuss the effect of the document/author class in modeling author X  X  interest. We measure this distance by the average KL-Divergence. Table 7 shows the results of the distance comparison. From this table, we observe the following: Since both AT and APT infer authors X  interest in the document layer, these models learn each interest as a mixture of various topics in each document. This leads to a decrease in the KL divergence over topics and thus a decrease in the KL divergence over words. The author class allows LIT to infer this interest in each author instead of each document, and learn them as a topic co-occurrence patterns over each author. LIT then increases the KL divergence over topics and words by distinguishing their interest from topics.
Perplexity: Since both AT and APT learn the topic dis-tribution from only those documents associated with each author, they ignore other authors, even if there are some authors that have written documents associated with topics that look like those of the author X  X  document. Therefore, Table 7: Average KL divergence between topic, word distributions: All models are learned with the number of topics, Z , the number of document classes, C and the number of author class, S ,setat 100, 200 and 30, respectively. Results that differ sig-nificantly according to t-test p&lt; 0 . 01 , p&lt; 0 . 05 from APT are marked with  X ** X ,  X * X  respectively.
 these models require far more latent variables for learning the distinct topic distribution than LIT. For example, when comparing the word distribution estimates for each docu-ment and the estimate of the distributions across documents, it is clear that LIT merges similar distributions into one dis-tribution. Then, LIT can estimate word distributions with fewer latent variables, while each of these word distributions are more distinctive than those of AP and APT. Addition-ally, the models represent the same document as a varied topic distribution since these models include the difference in author X  X  interest in document layer, while LIT, uses the doc-ument class, represents each document as a different topic distribution. Moreover, the document class captures topic co-occurrence patterns in a give n corpus rather than distin-guishing frequency topics in each document. Consequently, LIT prevents the overfitting problem in the inference pro-cess and offers significantly lower perplexity under the same condition on the number of topics than the other models.
Second, we discuss the calculation overhead of LIT. In fact, LIT consumes more time the other models in each it-eration. Nevertheless, the addition of the latent variables reduces the number of parameters by merging similar pa-rameters, as shown in Table 2 . Therefore, LIT gains lower perplexity with fewer iterations than the other models, see Table 5 and Figure 4. In LIT, the number of parameters is constant based on the number of latent variables. Con-versely, the number in the other models is on the order of D or A, where both D and A are the number of documents and authors, respectively. Therefore, a linear growth in the number of parameters with the size of the training suggests that overfitting is likely to be a problem. Moreover, LIT doesn X  X  need to calculate the similarity after learning, be-cause it groups, simultaneously, authors and documents by assigning those classes. This calculation is inevitable for many applications and is also on the order of data set size. Since LIT represents documents as document class by using the mixture of topics, LIT can project new documents into a latent space without retraining the model.
In this paper, we show how topics can be expressed in text documents and interests of authors can be identified in these documents. We introduced a new algorithm that extracts both topics and interests. A novel feature of our model is the inclusion of both author and document class into a probabilistic topic model, in which documents are modeled as probabilistic distributions over topics and au-thors are modeled as probabilistic distributions over docu-ment classes. Future work includes extending the LIT model by taking other metadata such as time, references, and link structure into account, for tracking the dynamic behavior of interests and topics. [1] D. Aldous. Exchangeability and related topics ,volume [2] D. Blei and J. Lafferty. Correlated topic models. [3] D. Blei, T.Griffiths, M. Jordan, and J. Tenenbaum. [4] O. Chapelle and Y. Zhang. A dynamic bayesian [5] N. Kawamae. Author interest topic model. In SIGIR , [6] W. Li and A. McCallum. Pachinko allocation: [7] Y. Liu, R. Niculescu-mizil, and W. Gryc. Topic-link [8] B. Marlin. Modeling user rating profiles for [9] A. McCallum, A. Corrada-Emanuel, and X. Wang. [10] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. [11] D. Mimno and A. McCallum. Expertise modeling for [12] M.Steyvers,P.Smyth,M.Rosen-Zvi,andT.L.
 [13] I. Titov and R. McDonald. Modeling online reviews [14] H. Wallach, D. Mimno, and A. McCallum. Rethinking [15] X. Wang and A. McCallum. Topics over time: a [16] X. Wei and W. B. Croft. Lda-based document models
