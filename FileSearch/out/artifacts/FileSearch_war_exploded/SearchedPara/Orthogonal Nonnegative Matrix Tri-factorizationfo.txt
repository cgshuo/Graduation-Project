 Providing a meaningful cluster hierarchy to a document corpus has always been a major goal for the data mining community. Approaches to solve this problem have focused on document clustering algorithms, which are widely used in a number of different areas of text minin g and information retrieval. One of a latest presented approach for obtaining document cluster is Non-negative Matrix Factorization (NMF) [1], which aimed to provide a minimum error non-negative representation of the term-document ma trix. This technique can be considered as co-clustering [2], which aimed to c luster both the rows and columns of the original data simultaneously by making efficient use of the duality between data points (e.g. documents) and features (e.g. words). Put it another way, document clustering and word clustering are performed in a reinforcing manner.
However, traditional clustering algorithms fail to take advantage of knowledge from domain experts. Incorporating the additional information can greatly enhance has been made for clustering document cor pus in a semi-supervised way, aiming to cluster the document set under the guidance of some supervisory information.
Unfortunately, traditional approaches to semi-supervised document clustering inherently strongly depend on constraints within document themselves while ignore the useful semantic correlation information hidden within the words of the document corpus. We believe that adding word semantic information (such as word clusters indicating word semantics) as additional constraints can definitely improve document clustering performan ce. Thereafter how to effectively combine both document-level and word-level const raints to guide the process of document clustering is a problem that is definitely worthy of researching.

Based on the above considerations, in this paper, we propose a novel semi-supervised document co-clustering metho d via non-negative factorization of the term-document matrix for the given document corpus. We have extended the classical NMF approach by introducing bot h document-level and word-level con-straints based on some prior knowledge. Our clustering model encodes the user X  X  prior knowledge with a set of constraints to the objective function, and the document clustering task is carried out b y solving a constrained optimization problem. Specifically, we propose a semi-supervised co-clustering framework to cluster the words and documents simultaneously. Meanwhile, we derive iterative algorithms to perform orthogonal non-negative tri-factorization. The correctness and convergence of these algorithms are proved by showing that the solution satisfied the KKT optimality and these algorithms are guaranteed to converge. Experiments performed on various publicly available document datasets demon-strate the superior perform ance of the proposed work.

The basic outline of this paper is as follows: Section 2 introduces related works. Section 3 presents the semi-supervised orthogonal nonnegative matrix tri-factorization. The experiments and results are given in Section 4. Lastly, we conclude our paper in Section 5. This section briefly reviews related wor k about NMF and semi-supervised doc-ument clustering.
 The classical NMF algorithms [3] aim to find two matrix factors for a matrix X such that X  X  WH T ,where W m  X  k and H n  X  k are both nonnegative matrices. Ding et al.[4] made systematical analysis of NMF and introduced 3-factor NMF. They demonstrated that the orthogonality constraint leads to rigorous clustering interpretation. When 3-factor NMF is applied to the term-document matrix X , each column X j of X is an encoding of an original document and each entry x ij of vector X j is the significance of term i with respect to the semantics of X j , where i ranges across the terms in the dictionary. Thereafter, Orthogonal NMF factorizes X into three non-negative matrices where G is the cluster indicator matrix for clustering of documents of X and F is the word cluster indicator matrix for clustering of rows of X . The simultaneous row/column clustering can be solved by optimizing The Frobenius norm is often used to measure the error between the original matrix X and its low rank approximation FSG T . The rank of the approximation, k , is a parameter that must be set by users.

Several formulations of co-clustering problem are proposed in the past decade and they are superior to traditional one-side clustering. Dhillon [2] proposed a bi-partite spectral graph partitioning approach to co-cluster words and documents. Long et al.[5] presented a general principled model, called relation summary network to co-cluster the heterogeneous data on a k -partite graph. As for semi-supervised co-clustering algorithms, Ch en et al.[6] presented a semi-supervised document clustering model with simultaneous text representation and catego-rization. Fei et al.[7] proposed a semi-supervised clustering algorithm via matrix factorization. Li et al.[8] presented an interesting word-constrained clustering al-gorithm. The way of incorporating word constraints is very appealing and sets a good foundation for our model formulation. Even though these semi-supervised algorithms have shown to be superior to traditional clustering method, very little is known about the combination of constraints on both documents and words. One recent work came from Li et al.[9]. They have demonstrated a non-negative matrix tri-factorization approach to sentiment classification with prior knowl-edge about sentiment words in the lexi con and partial labels on documents. In this section, we first describe how we integrate two different constraints in our model in Sect. 3.1. We then derive the OSS-NMF model, prove the correctness and convergence of the algorithm in Sect. 3.2 and Sect. 3.3 respectively. 3.1 Incorporating Document-Level Constraints Our model treats the prior knowledge on the word side as categorization of words, represented by a complete specification F 0 for F . The prior knowledge on document-level is provided in the for m of two sets of pairwise constraints on documents: two documents are known to be highly related and must be grouped into the same document cluster; or two documents that are irrelevant and can not be grouped into the same cluster.

We make use of set A ml to denote that must-link document pairs ( d i 1 ,d j 1 ) are similar and must be clustered into the same document cluster: It is easy to demonstrate that the must-link constraints represent equivalence relation. Therefore, we can compute a collection of transitive closures from A ml . Each pair of documents in the same transitive closure must be in the same cluster in the clustering result. Meanwhile, cannot-link document pairs are collected into another set: where each pair of documents are considered dissimilar and ought not to be clustered into the same document cluster.

We then encode the must-link document pairs as a symmetric matrix A whose diagonal entries are all equal to one and the cannot-link document pairs as another matrix B .

Suppose each document in the corpus either completely belongs to a particular topic, or is more or less related to sever al topics. We can then regard these con-implies that the overlap g i 1 k g j 1 k &gt; 0 for some class k , and therefore ( GG T ) i 1 j 1 should be maximized. The must-lin k condition can be presented as the cannot-link constraints and minimize nonnegative, we write this condition as: 3.2 Algorithm Derivation Combining the above constraints together, we define the objective function of OSS-NMF as: J =min where  X  ,  X  and  X  are positive trade-off parameters that control the degree of enforcement of the user X  X  prior knowledge. The larger value the parameters take, the stronger enforcement of the users prior knowledge we will have; vise versa.
An iterative procedure to solve the optimization problem in Eq.(7) can be summarized as follows.
 Computation of S . Optimizing Eq.(7) with respect to S is equivalent to optimizing Setting  X  X  1  X  X  = 0 leads to the following updating formula: Computation of F . Optimizing Eq.(7) with respect to F is equivalent to optimizing We present an iterative multiplicative updating solution. After introducing the Lagrangian multiplier, the Lagrangian function is stated as This takes the exact form as Li demonstrated in [8], thereby we can update F as follows: Computation of G . Optimizing Eq.(7) with respect to G is equivalent to optimizing J Similar with the computation of F , we introduce the Lagrangian multiplier, thus the Lagrangian function is L ( G )= X  X  FSG T 2 + Tr (  X   X G T AG +  X G T BG )+ Tr [  X  2 ( G T G  X  I )] . (14) We show that G can be iterated as: The detailed analysis of computation of G is shown in the optimization section. When the iteration starts, we update one factor with others fixed. 3.3 Algorithm Correctness and Convergence To prove the correctness and convergence of our algorithm, we will make use of optimization theory, matrix inequalities and auxiliary functions that used in [3]. Correctness Theorem 1. Iftheupdateruleof S , F and G in Eq.(9), Eq.(12) and Eq.(15) converge, then the final solution satisfies the KKT optimality condition, i.e., the algorithm converges correctly to a local optima. Proof: Following the standard theory of constrained optimization, we intro-duce the Lagrangian multipliers  X  1 ,  X  2 and construct the following Lagrangian function: The correctness of updating rules for S in Eq.(9) and F in Eq.(12) have been proved in [8]. Therefore, we only need to proof the correctness of updating rules for G . Fixing F , S , we can get that the KKT complementary condition for the non-negativity of G We then obtain the Lagrangian multiplier, it is obvious that at convergence the solution satisfy We can see that this is identical to t he KKT condition. The above equation denotes that either the first factor equals to zero, or G ik is zero. If the first as well, vice versa. Thus, we have proved that if the iteration converges, the converged solution satisfies the KKT condi tion, i.e., it converges correctly to a local minima.
 Proof is completed.
 Convergence. We demonstrate that the above objective function decreased monotonically under these three updating rules. Before we proof the convergence of the algorithm, we need to construct the auxiliary function similar to that used in Lee and Seung [3]. We first introduce the definition of auxiliary function. Definition 1. Afunction Z ( H, H ) is called an auxiliary function of L ( H ) if it satisfies Lemma 1. If Z ( H, H ) is an auxiliary function, then L is non-increasing under the update L ( H ( t +1) ) is monotonic decreasing (non-increasing).
 Lemma 2. For any nonnegative matrices A  X  R n  X  n , B  X  R k  X  k , S  X  R n  X  k , S  X  R n  X  k , A , B are symmetric, the following inequality holds[10]: Theorem 2. The above iterative algorithms converge.
 Proof: To proof the algorithm converges, the key step is to find an appropriate auxiliary function Z ( G, G ) of L(G) in Eq.(14). We show that the following function Z ( G, G )= ik [  X  2 G ik (1 + log is its corresponding auxiliary function.

First, it is obvious that when G = G , the equality holds. Second, the inequal-ity holds Z ( G, G )  X  L ( G ). This is based on the following: a) The first term and third term in Z ( G, G ) are always smaller than the corresponding terms in L ( G ) because of the inequality z  X  1 + log( z )  X  z&gt; 0; b) The second and last term in Eq.(24) are always bigger than the corresponding terms in L ( G ), due to Lemma 2. Putting these together, we can guarantee that Z ( G, G )  X  L ( G ).
To find the minimum of Z ( G, G ), we take and the Hessian matrix of Z ( G, G ) is a diagonal matrix with positive diagonal elements.

Thus Z ( G, G ) is a convex function of G . Therefore, we can obtain the global minimum of Z . The minimum value is obtained by setting  X  X  ( G,G )  X  X  We can thereafter derive the updating rule of Eq.(16) Under this updating rule, L ( G ) decreases monotonically, where the Lagrangian multiplier k -by-k matrix  X  2 for enforcing the orthogonality and G T G = I is given by Proof is completed. This section provides empirical eviden ce to show the benefits of our model OSS-NMF. We compared our method with Constrained-Kmeans[11], Information-Theoretic Co-clustering, which is referred to as IT-Co-clustering[12], ONMF-W denoting Orthogonal NMF with word-level constraints[8], ONMF-D representing Orthogonal NMF with document-level constraints. Constrained K-means is the representative semi-supervised data clu stering method; Information-Theoretic Co-clustering is one of the most popular co-clustering method; ONMF-W and ONMF-D are two derived algorithms from our approach.

The requirement of word constraints is the specification of word catego-rization. Similar with Li [8], we took advantage of the ACM term taxonomy, which come naturally and strictly decide the taxonomy of computer society. The document-level constraints were gener ated by randomly selecting pairs of doc-uments. If the labels of this document pair are the same, then we generated a must link. In contrast, if the labels are different, a cannot link is generated. The amounts of constraints were determined by the size of input data. Incorporating dual constraints on our model, we believe that our approach should perform better given reasonable amount of labeled data. 4.1 Datasets Three different datasets widely used as b enchmark data sets in clustering liter-aturewereused.
 Citeseer dataset: Citeseer collection was made publicly available by Lise Getoor X  X  research group at University of Maryland. We end up with a sam-pling of Citeseer data containing 3312 documents. These data are classified into one of the following six classes: Agents, Artificial Intelligence, Data Base, Infor-mation Retrieval, Machine Learning, Human Computer Interaction.
 DBLP Dataset: This dataset is downloaded from DBLP Computer Science Bibliography spanning from 1999 to 2004. We extract the paper titles to form our dataset from 5 categories, which contains 2170 documents.
 URCS Technical Reports: This dataset is composed of abstracts of tech-nical reports published in the Department of Computer Science at Rochester University. There are altogether 512 reports abstracts grouped according to 4 categories.
 We pre-processed each document by tokenizing the text into bag-of-words. Then we applied stopwords removing and stemmed words. In particular, words that occur in less than three documents are removed. We used the weighted term-frequency vector to represent each document. 4.2 Evaluation Metrics We adopt the clustering accuracy and normalized mutual information as our per-formance measures. These performance measures are standard measures widely used for clustering. Clustering accuracy measures the cluster performance from the one-to-one relationship between clusters and classes point of view, which is defined as: where r i denotes the cluster label of a document and d i denotes the true class label, N is the total number of documents,  X  ( x, y ) is a function which equals one if x = y and equals zero otherwise, map ( r i ) is the permutation function which maps each cluster label to the corresponding label of the data set.

NMI measures how closely the clustering algorithm could reconstruct the underlying label distribution in the data. It is defined as: variables Z and Z , H ( Z ) is the Shannon entropy of Z ,and H ( Z | Z )isthe conditional entropy of Z given Z . In general, the larger the NMI value is, the better the clustering quality is. 4.3 Clustering Results Considering the document constraints are generated randomly, we run each al-gorithm 20 times for each dataset and took the average as statistical results. To give these algorithms some advantage, we set the number of clusters equal to the real number of all the document clusters and word clusters.
 Overall Evaluation. Table 1 shows the cluster accuracy and normalized mu-tual information of all the algorithms on all the data sets. From the experimental comparisons, we observe that our proposed method OO-SNMF effectively com-bined prior knowledge from the word side with constraints on the document side for improving clustering results. Moreover, our model outperforms most of the clustering methods on all the data sets. In summary, the experimental results match favorably with our hypotheses and encouraged us to further explore the reasons.

The superiority of our model arises in the following three aspects: (1) the mechanism of tri-factorization for term-document matrix allows setting different classes of terms and documents, which is in line with the real applications; (2) co-clustering the terms and documents with both constraints leads to improvement in the clustering of documents; (3) last but not least, the constraints on word-level are quite different from that of document-level, which means our model can incorporate distinguished semantic information on both sides for clustering. Effect of the Size of Words. In this section, we describe the effect of the size of words on clustering. These words can be used to represent the underlying  X  X oncept X  of the corresponding category cluster. We follow the term frequency criteria to select word. The performanc e results with different numbers of words on all of the datasets are demonstrated.
Both Accuracy and NMI show clear benefi ts of having more words: the perfor-mance increases as the amount of words grows, as shown in Fig.1. This indicates the addition of word semantic information can greatly help the clustering per-formance. It also shows a great variation with the increase of words. When the and suddenly drops and then becomes stable.
 Experiments on Pairwise Constraint of Documents. We conducted ex-periments for our framework by varying the number of pairwise constraints and size of words. Results from all these doc ument collections indicate that gener-ally as more and more constraints are added to the dataset being clustered, the performance of the clustering method becomes better, confirming previous dis-cussion on the effect of increase of more labeled data. Due to the limitation of this paper, we only present NMI and Clu ster Accuracy on Citeseer in Fig.2.
Our finding can be summarized as follows: (1) As long as the constraints are provided, our model always outperforms the traditional constrained methods. (2) The model performs much better with the increase of constraints. In this paper, we consider the problem of semi-supervised document co-clustering. We have presented a novel orthogonal semi-supervised nonnegative matrix tri-factorization model. We also have provided theoretical analysis of the correctness and convergence of the algorithm. The ability of our proposed algorithm to inte-grate double constraints makes it efficient for document co-clustering.
Our work leads to several questions. We incorporated the word prior knowl-edge as a specification of the initial word cluster. It would also be interesting to make use of pairwise constraints on the word side. In particular, a further in-teresting direction is to actively select in formative document pairs for obtaining user judgments so that the clustering performance can be improved with as few supervised data as possible.
 Acknowledgments. This work is supported by the National Science Foun-dation of China (No. 60933004, 60903141, 60775035), the National Basic Re-search Priorities Programme (No. 2007CB311004), 863 National High-Tech Pro-gram (No.2007AA-01Z132), and National Science and Technology Support Plan (No.2006BAC08B06).

