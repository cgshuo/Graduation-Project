 Deepti Pachauri  X  , Maxwell Collins  X  , Risi Kondor , Vikas Singh  X  X  { Matching one set of objects to another is a fundamen-tal problem in computer science. In computer vision it arises in the context of finding the correspondence between multiple images of the same scene taken from different viewpoints (Fig. 1). In bioinformatics one must align sequences of genes and amino acids. In machine learning one often needs to align examples before a meaningful similarity measure can be com-puted between them (Pachauri et al., 2011). Some of these problems reduce to a linear assignment problem where S n is the set of all permutations of { 1 , 2 ,...,n } , and Q ij captures the negative cost of matching ob-ject x i to object y j . This problem can be solved in O ( n 3 ) time using the well-known Kuhn X  X unkries (or Hungarian) algorithm.
 The limitation of (1) is that it does not take into ac-count the relationships of the x i  X  X  (and the y i  X  X ) to each other. For example, in matching feature points (or landmarks) between images, not only do we want landmarks in one image to be matched to similar land-marks in the other image, but we also want the dis-tances between landmarks in the first image to be sim-ilar to the distances between the corresponding land-marks in the second one. This leads to a more general optimization problem known as the quadratic assignment problem (QAP). One way to think of (2) is to view it as the problem of matching two weighted graphs with adja-cency matrices A and A 0 so as to maximize the overlap between them.
 Unfortunately, the QAP is NP X  X ard, and it is also no-toriously hard to approximate or solve heuristically. Combinatorial search methods such as branch and bound almost never manage to solve real world QAP instances in polynomial time, while convex optimiza-tion methods are thwarted by the fact that the feasible region (called the second order permutation polytope) has an exponential number of faces. From a purely empirical point of view the most successful approaches are ant-methods and the like, which are very heuristic algorithms with no optimality guarantees whatsoever. In a fraction of the cases they find a near-optimal so-lution very fast, while in many other cases they fail, and there is no way of knowing whether we are in the former domain or the latter.
 It is natural to ask whether one can use side infor-mation to make QAP easier. Here, we propose a new approach for doing this by learning a modified objec-tive function f  X  from a set of prior  X  X raining X  QAP instances. The two criteria that f  X  must satisfy are 1. arg max  X   X  S n f  X  (  X  ) should be close to the maxi-2. f  X  should be much easier to optimize than f . The vector  X  that parametrizes f  X  is determined by methods similar to Structural Risk Minimization. This form of risk minimization strategy (Finley &amp; Joachims, 2008), as in Structural SVMs, when applied to the training set, allows our parameter  X  to generalize to unseen examples well. Note that Structural SVMs are very well understood if the original inference problem (arg max f  X  ) is poly-time solvable (see (Tsochantaridis et al., 2006; Taskar et al., 2003)). Unfortunately, the original graph matching problem requires finding a  X  such that the minimizer of f  X  (  X  ) matches  X   X   X  this is itself a QAP. Existing theoretical guarantees for Struc-tural SVM are known to be far less satisfactory for such intractable problems (Tsochantaridis et al., 2006; Joachims et al., 2009).
 Fortunately, we may observe that the  X   X  X  of our in-terest are not arbitrary objects, rather constitute the symmetric group, and this opens the door to look at the properties of specific sub-classes of f  X  and/or  X  that may provide useful insights into efficient solution strategies. Specifically, this view allows us to lever-age an entire spectrum of tools from abstract alge-bra including non-commutative harmonic analysis and fast Fourier transforms on groups to develop efficient algorithms (Rudin, 1962; Diaconis, 1988). The real promise of this new approach lies in its generality: by setting matching problems in a broader algebraic framework it has the potential to serve as a basis for developing new matching algorithms that exploit the representation theory of groups and are better in-formed by the the characteristics of the underlying vi-sion or learning task.
 While the model developed here is applicable to the learning version of most of the above scenarios, to make our exposition concrete, we will restrict our attention to the problem of learning parameters for graph matching. The Learning Graph Matching prob-lem (Caetano et al., 2009) seeks to solve for parameters of compatibility functions so that the solution from an approximate solution matches the permutation matrix provided by the user as best as possible. Since the main objects of interest are permutation matrices, it makes this an ideal sandbox to develop and present our main ideas.
 Graph Matching and other Related Work.
 Graph Matching is the problem of finding correspon-dences between the nodes of two graphs to maximize alignment. One way to model it is to write it as a Quadratic Assignment Problem (QAP), which is among the most well-studied combinatorial optimiza-tion problems in the literature (Pardalos et al., 1994; Cela, 1998; Umeyama, 1988; Vishwanathan et al., 2007). Many alternative approaches for graph match-ing are also known (Leordeanu &amp; Hebert, 2005; Han-cock &amp; Wilson, 2009; Caelli &amp; Caetano, 2005). In the context of learning graph matching (Caetano et al., 2009; Leordeanu &amp; Hebert, 2009), one is interested in the following question: if the optimal correspondence between the nodes of a pair of graphs is known (and many such pairs are available), how should one use this knowledge to learn correspondences between another pair of graphs which were extracted under similar con-ditions . The notion of conditions reflects properties of the application under study  X  for instance, (Caetano et al., 2009) uses the example of image pairs acquired under similar illumination from an airport surveillance camera, where the matching task refers to aligning the  X  X eature points X  from such images. The to be deter-mined parameter  X  then corresponds to weights which appropriately adjust the joint feature map of node and edge compatibilities, so that the match found by the solver agrees with the user provided solution. Learning graph matching serves another very important need  X  by restricting our focus to a specific domain and tun-ing weights that best reflect practical considerations in that application, a less sophisticated approach may still be able to obtain good quality solutions in a fixed amount of time or memory (Xu et al., 2007). This has implications in most situations where training data is available. The algorithm in (Caetano et al., 2009) uses a nice structure learning formalization for this prob-lem. But finding the most violated constraint pre-cisely in the construction (Caetano et al., 2009) itself involves solving a QAP, and can only be approximately estimated (e.g., via its linear assignment relaxation). This Paper. We recast the learning problem above using ideas from the theory of non-commutative Fourier analysis on the symmetric group. Our core algorithm is defined on a certain tree which has the permutations (  X   X  S n ) at the leaves and each branch corresponds to a coset of S n ; the modulation of the edge and node compatibility functions then consists of solving a set of simple convex optimization mod-els defined in Fourier space. Broadly, the approach performs stochastic descent -like weight updates to in-duce a margin between the path that leads to the cor-rect user-provided  X   X  , relative to all other  X   X  X , for each training example. The contribution of this paper is the parameter learning framework for a class of combi-natorial problems where the solution is a candidate in the symmetric group S n . We show how the representa-tion theory of S n makes the procedure computationally tractable, and how Branch and Bound schemes can be modified to learn information relevant for problem in-stances coming from an application of interest. The key aspect of the QAP which our approach ex-ploits is that S n , the set of permutations that we need to optimize over, is a group , called the symmetric group over n letters. Recall that this means, with re-spect to the natural notion of multiplication, which is (  X  2  X  1 )( i ) =  X  2 (  X  1 ( i )), permutations in S n satisfy the following axioms: 1.  X  2  X  1  X  S n (closure); 2.  X  3 (  X  2  X  1 ) = (  X  3  X  2 )  X  1 (associativity); 3. there is an identity e  X  S n such that  X  1 e =  X  1 ; 4. every  X  has an inverse  X   X  1  X  S n such that  X   X  1  X  = Remarkably, these axioms are sufficient to define a meaningful notion of harmonic analysis on S n . How-ever, because S n is non-commutative (  X  2  X  1 6 =  X  1  X  the Fourier components will be matrices. In particular, the Fourier transform of a general function f : S n  X  R consists of the matrices where  X  = (  X  1 , X  2 ,..., X  k ) is the so-called integer par-tition of n and plays the role of  X  X requency X , while  X  resentation (irrep) of S n . For more details on these algebraic concepts, see this paper X  X  extended version. Kondor (2010) recently showed that the machinery of non-commutative Fourier transforms can be used to advantage in solving the QAP. One key observation is that if one defines so-called graph functions f A : S n  X  R , then the objective function of the QAP, can easily be expressed in Fourier space as where  X  stands for the conjugate transpose. Given that functions of the form (4) are band-limited to nents of b f A other than those indexed by these inte-ger partitions are identically zero), the Fourier trans-form of the objective function will also be restricted to b f (( n )) , b f (( n  X  1 , 1)) , b f (( n  X  2 , 2)) and b fact, this observation has already been made in (Rock-more et al., 2002a). We will use this idea for computing QAP bounds to be discussed shortly.
 Another key ingredient in our proposed approach is the technology of Fast Fourier Transforms (FFTs) on S , going back to the work of Clausen (1989). Re-call that given a subgroup H of S n and a permuta-tion  X   X  S n , the set  X H = {  X  X  |  X   X  H } is called a left H  X  X oset. Similarly, one can talk about right H  X  X osets such as H X  = {  X  X  |  X   X  H } , and two-sided cosets such as  X  1 H X  2 = {  X  1  X  X  2 |  X   X  H } . FFTs gen-erally work by first computing Fourier transforms of f restricted to small cosets, and then recursively assem-bling such small transforms into ever large ones until we reach the Fourier transform in (3) on the entire group. A QAP solver can use this structure to search S n by employing the Inverse Fast Fourier Transform (iFFT) to restrict f to various cosets and bound it. We illustrate a Coset tree in Figure 2 for n = 3. As briefly discussed above, the final component of Fourier space QAP solver are the bounds for f (re-stricted to various cosets) based on its Fourier compo-nents. The simplest such bound is where k k  X  denotes the nuclear norm. Combining these components leads to a Branch and Bound type optimization algorithm that runs in O ( n 3 ) time per branch visited and is competitive with more conven-tional exact QAP solvers (Kondor, 2010). Unfortu-nately, in many practical problems the resulting algo-rithm still takes an exponential amount of time to run, simply because it needs to visit so many branches. Observe that QAPs are hard because the objective function is relatively flat, and according to most rea-sonable metrics, the diameter of S n is small compared to its size. In practical problems, however, one can sometimes overcome such seemingly insurmountable barriers by making inventive use of side information. In particular, we take the approach of using similar QAP instances (derived from the application of inter-est) to learn a modified objective function that will more effectively drive our algorithm to the correct so-lution. Since solving a QAP from scratch for each modification of parameters is clearly an intractable op-tion, our main goal will be to adapt tools from non-commutative harmonic analysis to recast the problem in a way that sidesteps this burden. The framework described next formalizes this idea.
 We first write the QAP objective function for Graph matching. The function is defined on the adjacency matrices of a graph pair, and has a band-limited struc-ture observed in (Rockmore et al., 2002b). Next, we incorporate (i.e., parameterize) domain information within the objective. This will set us up to present the framework for learning these parameters given multi-ple instances of related graphs.
 Given two graphs G,G 0 of n vertices with adjacency matrices A,A 0 , the standard quadratic assignment problem finds the permutation which best aligns the graph. We choose  X  which optimizes In the unweighted case, f (  X  ) simply counts the number of edges which appear in both G and the permuted graph  X  ( G 0 ). The objective function of standard QAP is expressed as the graph correlation (Kondor, 2010) between the two graphs G and G 0 where f A and f A 0 are defined in section 2. One may then systematically search the coset tree (2) via the standard Branch and Bound to maximize (7). 3.1. Parameterizing the QAP objective QAP instances derived from Vision problems typically rely on some analytic form approximating the percep-tual similarity between two feature points. For in-stance, how similar are a pair of shape features (Be-longie et al., 2002), u and v , extracted from different images? If a node to node match which is percep-tually correct turns out to be only marginally better than many other incorrect matches, this necessarily suggests that the features are not very discriminative. One practical consequence is that a Branch and Bound type search will need to work much harder (i.e., explore many sub-trees) to find the global solution.
 The core strategy is to incorporate domain information in the QAP objective. To do this, we use the simple idea of composing various QAPs to write a base QAP objective which has certain desirable properties. Since features are available in most applications, it seems logical to use this additional information in the design of a function that can be used to inform the base QAP objective. While this function might not be perfect due to the noisy features, standard learning algorithms can be used to learn shared structure by observing multiple instances drawn from the specific domain. Using training data (examples from the application of interest) we will be able to learn the parameters such that learnt parameters will induce domain friendly QAP instances  X  biasing the search towards the more interesting matches first, while simultaneously sup-pressing the influence of misleading features. 3.2. Learning Graph Matching There are various ways of extracting features in Vi-sion. Each of them model the neighborhood relation-ship between the vertices of G based on similarity of features . The list may include edge features such as a Delaunay triangulation , Euclidean distance between interest points, Shape context features etc. Consider D such representations are at our disposal, where each encodes the corresponding graph neighborhood.
 We use each of these representations to generate D adjacency matrices for each graph. Each entry A d i,j is a squared distance between the feature values of vertex i and j of G according to representation d  X  D . Similarly, we define A 0 d for G 0 . We can write QAP objective (7) as before using ( A d ,A 0 d ) We want to find a match such that edge ( i,j ) in G should be assigned to an edge ( i 0 ,j 0 ) in G 0 that is of a similar length (or weight) simultaneously in all en-coded adjacency matrices. However, features are noisy and not very discriminative, and can lead to wrong as-signments. The strategy is to parameterize each f d (  X  ) and write a parameterized QAP objective for learning where the subscript  X  d represents parameterization, that is, we use the parameter vector  X   X  R D to mod-ulate the QAP function f  X  (  X  ).
 The learning algorithm then essentially amounts to ad-justing the  X  appearing in the parameterized QAP ob-jective using the true assignments  X   X  given for each training pair ( G m ,G 0 m ). Our general goal is to find for some loss function L (  X  ,  X  ) and regularizer  X , where  X   X  m (  X  ) is the optimal permutation for example m . Note that  X   X  m (  X  ) itself corresponds to solving a QAP objective given a QAP objective modulated by pa-rameter  X  . The goal is to  X  X earn X   X  by performing stochastic descent at each level of a tree of cosets . In our experiments, we implemented a 0 / 1 loss. We ob-serve that there may be many  X   X  X  that will yield the same 0 / 1 loss; the regularization  X  used here seeks the minimal difference from the  X  in the previous step, so that the path/node leading to  X   X  m is preferable to any other node at this level, by a small margin. The exact algorithm is discussed next. In the following, we explain our Fourier space bounds for learning. The Fourier space machinery discussed in  X  2 bounds the objective function for learning (in-troduced in  X  3). More details on FFT and irreducible representation of S n are in this paper X  X  longer version. 4.1. Fourier Space Upper Bounds for Learning When working in a so-called adapted system of repre-sentations, the Fourier matrices at level n of a general function f can be expressed in terms of the Fourier matrices at level n  X  1 as where L is the direct sum of Fourier matrices, and  X   X  n  X  1 is used to denote the integer partitions of the  X  X ncestor partitions X .
 The Fourier space QAP solver proceeds by searching the tree of cosets, which corresponds to assigning ver-tex n,n  X  1 ,... of G to some sequence of vertices of G 0 At level n  X  k in the coset tree, it decides which vertex of G 0 it should assign the vertex n  X  k to by comparing the Fourier space bounds. Kondor (2010) used the in-verse map of (10) to define the Fourier space bounds for standard QAP, where the inverse map at level n  X  1 is given by Fourier space bounds are defined as  X  f is replaced by  X  f  X  in (12) for learning correct param-eters. It turns out that for a fixed  X  , f  X  (  X  ) is easily computable from each f d  X  without having to perform a very costly full Fourier transform. The exact form in which  X  interacts with f presentation simple, we formulate our QAP objective as follows The Fourier transform of f  X  can be expressed as The fact that makes f  X  learnable is that b f verse map of b f d (  X  ) according to (11). The identity follows directly from linearity of Fourier transform. Ultimately, we may use the convex nature of the nu-clear norm that makes Jensen X  X  inequality handy to derive an easy-to-optimize set of bounds, With these concepts in hand, the only missing ingredi-ent is the actual procedure to calculate the parameters  X  that is presented next. 4.2. Stochastic gradient descent solver In general, our loss function L as in (9) is a hinge loss on the relative bounds between the correct nodes and their incorrect siblings. This takes the form which is summed over all examples, and along with the regularization represents the function we seek to minimize. And i  X  n  X  k denotes the correct node at level n  X  k for some  X   X  . Note that bounding the siblings of the correct nodes will bound f for all incorrect per-mutations.
 We employ a stochastic gradient descent approach sim-ilar to (Shalev-Shwartz et al., 2007) on upper bounds defined in (12). During the training phase, we know which node to node assignment we should make, that is, we know which of these bounds, say the one cor-responding to vertex i  X  n  X  k , we want to be the largest. A random training example and node i is selected and compared to its correct sibling i  X  n  X  k . We then reduce the objective for one term by taking a gradient step on  X  . When  X ( w ) =  X  2 k  X  k 2 2 and b f  X  i is replaced with the bounds in (15), each update takes the form  X  is an exponentially decaying step length parameter, M is the number of training examples, and the  X  M O ( n 2 ) term arises from splitting the regularizer over all nodes considered by the optimization.
 This is like structural SVM, where we try to find pa-rameters so that the model predicts i  X  n  X  k instead of i , with the bound for each correct node in the coset trees of the training set greater than its incorrect sib-lings by some margin. As per (15), learnt parameters are goodness measures of individual graph correlation function f d contributing in (13). To demonstrate proof of principle results of the pro-posed algorithm, we learnt parameters for compatibil-ity functions in the context of graph matching. This has numerous applications, but the setting below con-siders the task of aligning two 2D images using local features extracted from the image data including in-terest points, and shape context features.
 Edge X  X eatures: We performed Delaunay triangula-tion on interest points to generate unweighted edges. This provides unweighted adjacency matrix.
 Distance X  X eatures: We used 2D coordinates of in-terest points to calculate Euclidean distance between points. We extracted weighted adjacency matrices at various scales using 2D distances.
 Shape Context X  X eatures: As in (Caetano et al., 2009), we also included Shape context features (Be-longie et al., 2002). Briefly, the feature vector is a de-scriptor in Log-Polar space that describes the localized shape at each node. We generated weighted adjacency matrices by using normalized histogram differences of subsets of shape context features.
 Dataset and Setup: We performed an experimental evaluation on 3 datasets. We used a subset of the land-mark points provided. The experimental set for each dataset closely follow the setup described in (Caetano et al., 2009). Graph pair instances were generated such that the two graphs are separated by a varying base-line (referred to as  X  X ffset X  below). Our training data include multiple QAP X  X  for each pair of images. The algorithm learnt a suitable  X  . We performed standard 10 X  X old cross-validation on train/test data for each off-set. The number of pairs in the train/test splits varied based on the offset. We summarize our results on var-ious datasets below.
 Hotel/House Dataset: We considered the CMU house dataset, which contains 111 frames of a video sequence of a toy house. Landmark points were iden-tified and hand X  X abeled in each frame. Quantitative results corresponding to the matches found on the test set are shown in Fig 4 (left). (Red plots) present the accuracy of matches (on test sets) as the offset (separa-tion between frames) varies. We compared our results against a greedy ( X  X o Learning X ) matching on two feature settings (blue plots). As expected, no-learn-greedy approach perform poorly if not all features are informative (blue dashed line). For small offsets, no-learn-greedy takes advantage of the fact that the prob-lem instance is easy and the shape context features are useful, but its performance gradually deteriorates as the offset increases (blue bold line). A greedy as-signment using learnt weights still performs well. Note that the test phase does not perform any backtracking in Branch and Bound.
 We performed a similar experiment on CMU hotel dataset, which contains 101 frames of a video sequence of a toy hotel. Again, we see good overall agreement with the ground truth using the learnt parameters. Quantitative results are shown in Fig 4(center). Qual-itative results corresponding to the learnt matches on the test set shown in Fig 3 Silhouette Dataset: For our second experiment, we used the Silhouette database. We applied horizontal shear to twice its width and transformed the images synthetically. Results for this experiment are shown in Fig 4 (right). Introducing uninformative features makes matching problem more difficult (blue dashed line). While shape context features are quite useful, the learning setting still outperforms no-learning for all offset variations.
 Finally, we analysed learnt parameters for the setup that includes Delaunay and distance X  X ased adjacency matrices at 5 scales along with random graph pairs. Learning induced parameters corresponding to Delau-nay and distance. Fig 5 shows an example of aver-age weights produced in 10 X  X old cross-validation setup indicating reduced weights on uninformative features (e.g., non-informative distance scales).
 This paper shows that parameter learning (for set-ting up domain driven compatibility functions) for a general class of hard combinatorial optimization prob-lems can be performed efficiently if the solution to the primary objective function is a member of S n . We present a framework for performing weight updates on the nodes of a Coset tree. Observe that while the number of leaves of this tree is still n !, for the functions discussed here, the bandlimited nature of its Fourier transform makes the process tractable. Our algorithm is inspired by recent results (Kondor, 2008; Huang et al., 2009; Huang &amp; Guestrin, 2010) show-ing how concepts from harmonic analysis tie to top-ics in machine learning. Our procedure generalizes to other problems that can be cast as appropriate func-tions on S n , and provides a complementary approach to a number of problems typically tackled using Struc-ture Learning. We believe there is additional struc-ture that is not explicitly leveraged by our current model  X  for instance, we are evaluating the compu-tational benefits of performing weight updates on the frequency components instead of the features. Explor-ing these properties may provide strategies for other seemingly unrelated problems. For instance, very re-cently, some related ideas have been independently investigated for submodular set functions (Stobbe &amp; Krause, 2012). Our implementation will be available at http://pages.cs.wisc.edu/ ~ pachauri/ .
 Acknowledgments The authors thank Shamgar Gurevich, Nigel Boston, and Jerry Zhu for various sug-gestions. This work was supported in part by grants NIH AG034315, NSF RI 1116584, and UW-ICTR (via NIH CTSA award to UW). Collins is supported by the CIBM Training Program (NLM 5T15LM007359).

