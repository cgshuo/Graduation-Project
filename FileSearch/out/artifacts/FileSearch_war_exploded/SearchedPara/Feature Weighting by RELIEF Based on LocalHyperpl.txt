 Feature weighting plays an important step in the preprocessing of data, es-pecially in data classification. In general, the feature weights are obtained by assigning a continuous relevance value to each feature via a learning algorithm by stressing on the context or domain knowledge. The feature weighting pro-cedure is particularly useful for instance based learning models, which usually construct the distance metric by using all features. Moreover, feature weighting can reduce the risk of over-fitting by removing noisy features, thereby improve the predictive accuracy. Existing feature selection methods broadly falls into two categories, wrapper and filter methods. Wrapper methods use the predictive ac-curacy of predetermined classification algorithms (called base classifier), such as SVMs, as the criteria to determine the goodness of a subset of features [9,15]. Filter methods select features based on d iscriminant criteria that relies on the characteristics of data, independent of any classification algorithm [7,14,17]. The common discriminant criteria includes entropy measurement [18], Chi-squared measurement [21], Fisher ratio measurement [10], mutual information measure-ment [20,4,3], and RELIEF-based measurement [19,27,28].

Due to the emerging needs in biomedical a nd bioinformatics areas, researchers are particularly interested in algorit hms which can process of data with feature being of large (or huge) dimensions, such as, microarray scanning in cancer research. Therefore, filter methods are widely used due to its efficiency in com-putation. Among the existing filter methods in feature weighting, the RELIEF algorithm [19] is considered as one of the most successful ones due to its sim-plicity and effectiveness. The main idea behind RELIEF is to iteratively update feature weights by a distance margin to estimate the difference between neigh-boring patterns. It has been further gen eralized to average multiple, instead of just one, nearest neighbors when computing the sample margins, and was named as RELIEF-F [19]. The authors have shown that RELIEF-F can achieve significant improvement o n performance of the original RELIEF [19]. Sun sys-tematically proved that RELIEF is indeed an online algorithm for a convex optimization problem [27]. Through maximizing an averaged margin of nearest patterns in feature scaled space, RELIEF could estimate the feature weight in a straightforward and efficient manner. Based on the theoretical framework, one can impose outlier removal scheme called I-RELIEF since the margin averaging is sensitive to large variations [27]. To accomplish sparse feature weighting, the author introduced the l 1 penalty into optimization of I-RELIEF [28]. In this paper, we present a new feature we ighting algorithm to extend classical RELIEF model. The main contribution of the proposed algorithm is that the feature weights are estimated from local patterns other than global ones, as used in exiting methods [19,27,28]. Therefore, the proposed feature weighting scheme is particularly useful when combined with local pattern based classifiers, such as HKNN [30], ADAMENN [8] and discriminant adaptive nearest neighbor (DANN) [16]. Besides, local patterns are more robust to the noises and outliers. It is promising to be used in applications where data are severely contaminated by noises or rich of redundance.

This paper is organized as follows. Section 2 introduces the background of the classical RELIEF method and its variations, including F-RELIEF and I-RELIEF. The main result is reported in this section. Section 3 demonstrates the performance of the proposed model. Extensive experiments have been conducted to compare with the classical methods on benchmark data sets. Conclusion is presented in Section 4. 2.1 RELIEF The RELIEF algorithm has been successfully applied in feature weighing due to its simplicity and effectiveness [19,28]. The main idea of RELIEF is to iteratively adjust feature weights according to their ability to discriminate among neigh-boring patterns. Mathematically, suppose that x is a randomly selected sample of a binary class data. One can estimates its two nearest neighbors, wherein one is from its same class (called the nearest hit or NH) and the other is from a different class (called the nearest miss or NM). Then the weight w i for the i -th feature is updated by a heuristic estimation: Since there is no exhaustive or iterative search evolved in RELIEF updating, this scheme is very efficient for the processing of data with huge dimensions, thus it is particularly promising for large-scale problems such as analysis of microarray data [24,28,7]. The authors have gener alized the RELIEF model by averaging k , instead of just one in Eq. (1), nearest neighbors when computing the sample margins and was named as RELIEF-F model [19]. Experimental results have shown that RELIEF-F achieves superior p erformance over the original RELIEF. Its success is due to the robustness of mar gin estimation on multiple samples. However, the optimal number of nearest n eighbors needs to b e estimated empir-ically. Besides, RELIEF-F is also sensitive to noise degradation and the outliers. An benchmark achievement has been reported in [27], in which the author firstly proved that RELIEF is a convex optimization problem with a margin-based ob-jective function, x n for distance function d ( miss and hit for a sample x n , respectively.

To tackle the drawbacks of RELIEF, such as outlier detection and inaccurate updating, Sun reformulated the above problem as maximization of expected margin through scaling of features [27,28]: where NM = { i :1  X  i  X  N,y i = y n } ,NH = { i :1  X  i  X  N,y i = y n ,i = n } are the sets of the nearest miss and the nearest hit, respectively. P ( x = NM ( x n ) | W ) (or P ( x = NH ( x n ) | W )) are the probabilities of the sample x being in the set of NM ( x n )(or NH ( x n )) in the feature space scaled by weights w . Though the probability distributions are unknown in prior, they can be estimated via kernel density estimation [6]. Empirical study has shown that the I-RELIEF achieves significant improvements over the traditional models. Task of classification on feature scaled dataset achieves higher accuracy than standard techniques such as SVM [12,15,9,26] and NN model [25]. Task of feature weighting is also robust to noisy features. In applications with a huge dimension of features, economic feature weights are appreciated not only because of computational consideration, but also most features being irrelevant [14,17]. To obtain sparse and economic feature weighting, the author introduced the l 1 penalty into the optimization of I-RELIEF [28].

However, since the expectation in Eq. (3) is carried out on the set of nearest miss or hit, which consisted of the nearest neighbors of all observed samples, the feature weight estimation may be less inaccurate if the samples contain many outliers, or most of the features are being irrelevant. In both cases, the distance between the tested one and its nearest neighbors are in large value. It follows that large bias will be introduced in margin estimation via averaging operation. Although one can reduce the influence of the abnormal samples by introducing kernel distribution estimation [27,28], it will introduce additional free parameter estimation. Moreover, probability estimation via kernel approximation is sen-sitive to the sample size [6,13]. Therefore, it limits the empirical applications such as in analysis of microarray data, in which the data is notoriously known for that the dimension of sample observation is far less than that of the sample feature [11]. In this paper, we propose to use a local hyperplane to approximate the set of the nearest hit and miss and then estimate the feature weight through maximization of an expected margin defined by the hyperplane. The contribution of this approximation is that the hyperplane is more robust for noisy features degradation than averaging over all neighbors [19,27,28]. 2.2 Approximation by Local Hyperplane Given a sample x , it can be represented by a local hyperplane of class c by: where H is a I  X  n matrix composed by n NNs of the sample x : H = { h of class c . The parameter of  X  =(  X  1 ,..., X  n ) T is the weights of the proto-types { h i ,i =1 , 2 ,...,n } . It can be viewed as spanning coefficients of the subspace LH c ( x ). Therefore, the hyperplane can be represented as: { X  | H X  =  X  h between the sample x and its local hyperplane of LH c ( x ) within feature scaled space: where s =( s 1 ,s 2 ,...,s I )= H X   X  LH c ( x ). W is a diagonal matrix with diagonal elements w i being the weight of the i -th feature.
 We are proposing to use the hyper plane to represent the set of nearest miss NM ( x ) and nearest hit NH ( x ) for the given sample x . The beneficiary of the representation is to characterize the local sample patterns robustly. Then the distance between the sample to its NH (or NM ) set can be estimated from its local hyperplane other than averaging across over all samples within the set. Therefore, we redefine the margin for a sample x as  X  n = d ( x n  X  LH NM ( x n ))  X  d ( x tion of total margins: where H NM ( x n )and H NH ( x n ) are the nearest neighbors for the set of the nearest miss and hit of the sample x n .  X  n and  X  n are the coefficients for spanning the weight of the i -th feature, for i =1 , 2 ,...,I . To solve the minimization problem of Eq. (6), one should estimate the parameters of  X  n ,  X  n ,whichare dependent on the nearest neighborhoods. The main problem of the estimation, however, is that the nearest neighbors of a given sample are unknown before learning. In the presence of many thousands of irrelevant features, the nearest neighbors defined in the original space can be completely different from those in the induced space. Therefore, the nearest ne ighbors defined in the original feature space may not be true in the weighted feature space. To solve the difficulties, we have designed an iterative algorithm, similar to the EM algorithm and I-RELIEF [27], to achieve the goal.
 Step 1: In t -th iteration, for a given sample x ,weestimatetheparameterof  X  by constructing the local hyperplane of the nearest hit set within induced feature space. It is trivial to show that the minimization of Eq. (5) is equivalent to solving the following quadratic programming: where  X  H = H T W ( i ) H , f =  X  x T W ( i ) H ,and 1 is an unitary vector whose elements are all being 1. The matrix of W ( i ) is the t -th feature weight matrix, satisfying W ( i ) 1 = w . The parameter of  X  for nearest miss hyperplane is obtained similarly. Minimization of Eq. (5) is a constrained quadratic program problem and standard techniques can be used to obtain its solution. In particular, since the matrix of  X  H is symmetric and non-negative, the minimization could be solved efficiently through standard tec hniques, such as active set [23].
 Step 2: Estimation of the total margin with respect to w ( i ) . Step 3: Estimation of the weight W in ( i + 1)-th iteration. The above steps iterate alternatively unt il their convergence. The last two steps are similar to the one used in I-RELIEF [27], and we name our scheme as LH-RELIEF since it requires a local hyperplane approximation.
 The pseudo-code for the LH-RLIEF is summarized in Alg. (2.1) We shall demonstrate the performance of the proposed scheme through classifica-tion evaluation on both synthetic and empirical problems. In particular, we are interested in its: 1) performance of classification compared with other feature weighting scheme; 2) robustness when processing the samples with irrelevant features of large dimension. 3.1 Selection of Classifier In our experiments, we selected the hierarchical k -nearest neighbor (HKNN) algorithm to conduct the comparison on feature weighting [30]. HKNN could be viewed as a localized approximation of K -nearest neighbor model. In this model, each class is modeled as a smooth and low-dimensional manifold embedded in the high-dimensional data space by assuming that the manifolds are locally linear. There are two steps involved in classification by HKNN. In the first step, for each tested sample, it constructs local hy perplanes for each class. The label of the tested sample is assigned to the class whose local hyperplane to the tested sample is minimized. Empirical study has shown that the HKNN produced a comparable or even better performance of classification than standard techniques, including KNN and SVM [30,8,29]. One may note that the HKNN model shares the similar idea with our approach in that the sample information is inferred from local structure, which is the main reason for us to choose this particular classifier.
Since the HKNN model does not consider the influence of feature weights, the test data will be firstly scaled into feature space before the classification is carried out. The hyper-param eters used in training phase are estimated through ten-fold cross validation. 3.2 Fermat X  X  Spiral Problem In the first example, we shall test the performance of the proposed method on the well-known Fermat X  X  Spiral problem. The te st dataset consists of two classes with 200 samples for each class. The labels of t he Spiral are completely determined by its first two features. The shape of the Fermat X  X  Spiral distribution is shown in Fig. 1(a). Heuristically, the label of a sample will be inferred easily from its local neighbors. Classification based on local information will give more accurate as-signment than global measurement based prediction (or classification) does since the later one is sensitive to noise degradation. To tackle this drawback, Sun pro-posed to lower the influence of the samples nearby through modeling of their prob-ability distribution via kernel techniques [27]. This strategy is straightforward and successful. However, if the dominant (informative) features are buried by the ir-relevant (less informative) ones, estimation of the probability via distance will be less accurate since the irrelevant feature may introduce a large variation to dis-tance, for instance, the irrelevant feat ures are being in a huge dimension. In order to show this, irrelevant features following standard norm distribution are added to the Spiral for classification testing. The dimensions of irrelevant fea-Two feature weighting scheme, I-RELIEF and LH-RELIEF were firstly applied to quantify the importance of feature. Then the classification was performed on dataset scaled by the feature weights. F or each experiment, ten folds cross val-idation scheme is used to compute the accuracy of classification. To eliminate the statistical variations, we have conducted ten times experiments indepen-dently on each dataset and averaged classification error is recorded and, shown in Fig. 1(b). We observe that, the performance of the two methods are very similar when the dimension of the irrelevant features is small. However, if the di-mension of irrelevant features tends to be large, the performance of I-RELIEF is severely degraded by the noises. In comparison, the performance of LH-RELIEF is very stable and produces superior outcomes. 3.3 UCI Data Sets In the second experiment, we tested the proposed technique on ten medium sized datasets. The tested benchmark d ata sets were downloaded from the UCI Machine Learning Repository [1], and th ey have been widely tested by various classification benchmark models. The characteristics of the datasets are sum-marized in Table 1. We compare our algorithm with four other algorithms, in-cluding Iterative Search Margin Based Algorithm (Simba) [2], sparse Bayesian multinomial logistic regression (SBMLR) [5] and I-RELIEF [27]. Simba is a local learning based algorithm similar to RELIEF. SBMLR is a special kind of sparse multinomial logistic regression models with Bayesian regularization. Multinomial logistic regression algorithm has been su ccessfully used in text processing [31] and microarray classification [22]. The beneficiary of adding regularization pa-rameter into sparse multinomial logistic regression via a Laplace prior is that an analytical solution could be obtained. Besides, its performance is similar to us-ing cross-validation based model selection, thus greatly reducing computational expense.

For each dataset, the optimal parameters were estimated by ten-fold cross validation. The obtained feature weights under optimal parameters were used to scale the raw datasets. Twenty times e xperiments on each dataset were per-formed independently and classification errors were averaged to evaluate the performance of the feature weighting scheme. We will use the classification error to quantify the discrimination power of weighting scheme. Furthermore, statisti-cal testing is also useful to fully comprise the performance of feature weights [27]. We selected the Students paired two-tailed t -test to achieve the goal. The p -value of the t -test represents the probability that two sets of compared results come from distributions with an equal mean. In this experiment, a p -value of 0 . 05 is considered statisti cally significant.

The results are summarized in Table 2. We observe that that LH-RELIEF and I-RELIEF are statistically different from the tested ten datasets. The per-formance of classification after LH-R ELIEF is better than after I-RELIEF in 9 of 10 experiments. Among the four fea ture weighting schemes, LH-RELIEF outperforms others in 5 of 10 datasets, while almost is suboptimal in other five dataset.

In the last experiment, we are willing to test the performance of the algorithm on data in huge dimensions. More specifically, we are interested in the robustness of the algorithm on feature weighting with respect to the dimension of the irrel-evant features. We selected two test datasets: Bupa and Pima. For each dataset, irrelevant features are added to the raw dataset. The added irrelevant features are independently sampled from zero-mean and unit-variance Gaussian distribu-tion. Their dimensions are ranged from 0 to 1000. Including useless features is
Dataset LH-RELIEF I-RELIEF SBMLR Simba Bupa 69 . 7  X  66 . 7(0 . 00) 56 . 266 . 8 Teach 64 . 4  X  46 . 3(0 . 00) 34 . 462 . 3 Sonar 86 . 7  X  84 . 3(0 . 00) 82 . 785 . 7 Cancer 76 . 276 . 0(0 . 48) 76 . 9  X  76 . 4 Prokaryotic 90 . 5  X  89 . 8(0 . 00) 90 . 489 . 3 Eukaryotic 82 . 881 . 2(0 . 00) 83 . 5  X  81 . 3 Haberman 69 . 372 . 3(0 . 00) 69 . 968 . 7 Page Block 94 . 594 . 1(0 . 00) 95 . 7  X  89 . 8 Pima 74 . 070 . 3(0 . 00) 68 . 974 . 5  X 
Spambase 84 . 8  X  78 . 0(0 . 00) 79 . 339 . 4 less appreciated in applications where the acquisition of data is quite expensive. For example, it may complicate the pathway research if irrelevant genes are in-cluded in microarray data analysis [27]. We would welcome such complication in order to show the robustness of the algorithm.

The hyper-parameters, such as the kernel size  X  in I-RELIEF and the number of nearest neighbors k in LH-RELIEF are estimated through ten-fold cross vali-dation. To eliminate statistical variations, each algorithm is run for twenty times on each noisy dataset. In each run, a dataset is randomly partitioned into train-ing and testing. The averaged testing errors serve as the criterion to quantify the performance of the algorithm, and the results are drawn in Fig.2. For Bupa, the classification error of the classifier after LH-RELIEF is smaller than that after I-RELIEF in all dimensions, Fig. 2(a). This observation is coincided with the results in Table. 2, implying that the feature weights estimated by LH-RELIEF are more accurate and robust to the noises. For Pima, the performance of the two scheme is almost comparable when the dimension of the the irrelevant features is small, Fig. 2(b). However, the testing error after LH-RELIEF dramatically de-creased with respect to the dimension of the irreverent features. In comparison, the classification error after I-RELIEF tends to be greater. The experiment fur-ther demonstrates that the proposed feature weighting scheme is more immune to the noisy features by showing surprising high degree of robustness. In this paper, we proposed a new feature weight scheme to tackle the com-mon drawbacks of the RELIEF family. The nearest miss and hit subset are approximated by constructing a local hyperplane. Then the updating of feature weights is achieved by measuring the margin between the sample and its hy-perplane under general RELIEF framework. The main contribution of the new variation is that the margin is more robust to the noises and the outliers than ear-lier works do. Therefore, the feature weights can characterize the local structure more accurately. Experimental results on both synthetic and real-world datasets validate our findings. The proposed weighting scheme performs superior on most test data with respect to classification error. We also observed that the algorithm was convergent in most cases, though t heoretical justification is needed.
