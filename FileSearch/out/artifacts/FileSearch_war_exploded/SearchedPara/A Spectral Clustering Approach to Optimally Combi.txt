 We address the issue of clustering numerical vectors with a network. The problem setting is basically equivalent to con-strained clustering by Wagstaff and Cardie [20] and semi-supervised clustering by Basu et al. [2], but our focus is more on the optimal combination of two heterogeneous data sources. An application of this setting is web pages which can be numerically vectorized by their contents, e.g. term frequencies, and which are hyperlinked to each other, show-ing a network. Another typical application is genes whose behavior can be numerically measured and a gene network can be given from another data source. We first define a new graph clustering measure which we call normalized net-work modularity , by balancing the cluster size of the origi-nal modularity. We then propose a new clustering method which integrates the cost of cl ustering nume rical vectors with the cost of maximizing the normalized network modu-larity into a spectral relaxation problem. Our learning algo-rithm is based on spectral clustering which makes our issue an eigenvalue problem and uses k -means for final cluster as-signments. A significant advantage of our method is that we can optimize the weight parameter for balancing the two costs from the given data by choosing the minimum total cost. We evaluated the performance of our proposed method using a variety of datasets including synthetic data as well as real-world data from molecular biology. Experimental re-sults showed that our method is effective enough to have good results for clustering by numerical vectors and a net-work.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Algorithms and Experimentation Network modularity, spectral clustering, heterogeneous data sources, k -means, eigenvalue problem
Clustering, a major research subject in data mining, has been successfully applied to a wide variety of areas in the real world. In this paper, we address the issue of cluster-ing numerical vectors with a network. This is a general setting which can be found in a lot of applications and ba-sically equivalent to constrained clustering by Wagstaff and Cardie [20] and semi-supervised clustering by Basu et al. [2], but our focus is more on the optimal combination of two het-erogeneous data sources, numerical vectors and a network.
A typical application is web pages. This case, web pages will be clustered by their contents, say term frequencies, based on the assumption that if the contents of a page are very similar to those of another, these pages can be in the same cluster. In contrast, w eb pages are linked together, forming a network in which nod es and edges correspond to web pages and hyperlinks between them, respectively, and can be clustered based on the hyperlink connectivity. Clus-tering web pages based on hyperlinks is exactly graph par-titioning. Standard criteria for graph partitioning are ratio cut [8] and normalized cut [15]. Simply speaking, these cri-teria are to minimize the number of inter-cluster edges rel-evant to the size of a cluster. The number of inter-cluster edges which is called graph min-cut is to check the parti-tioning performance while the size of a cluster is to avoid a small cluster which might be generated by outliers. In other words, these criteria are obta ined by normalizing the graph min-cut by the cluster size. Given a network, minimizing normalized cut (or ratio cut) is a trace optimization problem which is NP-hard. Thus usually this problem is converted by spectral relaxation into an optimization problem with a constraint which can be solved by Lagrange multipliers, and the solution is given by an eigenvalue problem. This is called spectral graph clustering which is difficult to assign a cluster label to each node definitely. Thus usually k -means is finally applied to cluster assignments using the resultant eigenvectors.

In the problem setting of graph partitioning, a numerical weight can be attached to each edge of a given graph, and you may think that the similarity between two web pages can be a weight for the hyperlink between them. However, we assume that a given graph and a given set of numeri-cal vectors are independently observed. This assumption is natural. For example, the contents of web pages and their links are independently generated. More concretely, there is a case that two web pages are very similar to each other, even if there are no links between them. Thus we note that our problem cannot be solved directly by using an existing graph partitioning method only.

Another application is genes. Genes are expressed and function in a cell. Currently the quantitative expression of thousands of genes can be measured simultaneously by using the latest technology in genetic engineering, called cDNA microarray. We can have a numerical vector (gen-erally called a profile) for each gene by repeating the experi-ment of cDNA microarray. However cDNA microarray data is very noisy and unreliable. Thus naturally we often need another data source in clustering genes, since precise gene clustering is important in predicting gene function [16]. We can have more reliable information on genes as a gene net-work, although they are confined to relatively well-studied genes. For example, literature information provides us with the co-occurrence frequencies of genes in medical documents which can be turned into a network of genes by setting a cut-off value against the frequencies. Similarly, metabolic or gene regulatory networks which are generated from liter-ature are much more reliable than microarray data.
A potential approach for our problem setting would be to integrate the two data sources, numerical vectors and a network. A typical example of this direction is semi-supervised clustering based on a hidden Markov random field (HMRF) [2]. Semi-supervised clustering by HMRF is clustering numerical vectors by minimizing the objective function containing squared Euclidean distance as well as weighted network constraints. This method was extended to a more general framework in which the objective func-tion can be expressed as a trace optimization problem for which an efficient weighted kernel k -means algorithm was proposed [11]. This work is based on the idea that minimiz-ing the cost (or the objective function) of semi-supervised clustering by HMRF can be a trace optimization problem, which is true of minimizing the objective function of the weighted kernel k -means and more generally, minimizing a graph cut criterion such as normalized cut or ratio cut [3]. This work inspired us to combine numerical vectors with a network, but we note that ou r criterion for graph parti-tioning is clearly different from that in [11] and our focus is more on optimally combining the two data sources in terms of clustering.

Recent analysis on networks in the real-world data have revealed that they have some common global characteristics such as small-world phenomena [ 21], scale-free property [1], self-similarity [17] and hierarchical modularity [14]. We can expect that the performance of clustering in our problem set-ting might be improved by using some global network prop-erty than the vicinity information like the Markov property.
In light of the above, we propose a new method for cluster-ing numerical vectors with a network. We focus on network modularity , a global feature found in a lot of real-world net-workssuchasgenenetworks[14]andmustbeapowerful criterion for clustering by the graph connectivity. The orig-inal network modularity [13, 7, 6] is, intuitively, the number of intra-cluster edges minus the square of the number of inter-cluster edges. That is, this measure is given by using only the edges of a graph and is not balanced by the clus-ter size. Thus we first define normalized network modularity which is obtained by dividing the original network modu-larity by the cluster size. We then integrate the normalized network modularity with the cost of clustering numerical vectors into the framework of a trace optimization problem. Our clustering algorithm is based on spectral clustering by which our issue is relaxed into an eigenvalue problem and the final clusters are assigned by k -means clustering algo-rithm from the resultant eigen vectors. We stress that our work is an approach for clustering with not only numerical vectors but the network modularity. A significant merit of our method is that we can optimize the weight parameter for balancing the two data sources by choosing that which minimizes the total cost.

We evaluated the performance of our method using three types of datasets including both synthetic and real-world data. Our first dataset was synthetic both in numerical vectors and a network. Numerical vectors were generated randomly according to a mixture of von Mises-Fisher distri-butions, and a network was generated by selecting node pairs randomly. We confirmed the effectiveness of our method by checking normalized mutual information (NMI), a measure to check the performance of clustering methods, and the to-tal cost of clustering, with varying the value of the weight parameter for balancing the two data sources. In particular, we found that NMI was mostly maximized at the weight parameter value which minimized the total cost, meaning that our strategy of choosing the weight parameter value of the minimum total cost worked successfully. The second dataset was synthetic numerical vectors and a real metabolic network having the scale-free property and unbalanced clus-ter sizes. From the experimental results on this dataset, we showed that our method of optimally combining numerical vectors with a given network worked favorably against even a real scale-free network with unbalanced cluster sizes. The third dataset was real microarray expression profiles corre-sponding to numerical vectors and the real gene network used in the second dataset. We note that gene expression measured by microarray is heavily noisy and unreliable while the network we used is from a database which is manually curated and trustworthy. Interestingly the resultant weight parameter value was extremely biased to the network infor-mation, being consistent with the above reliability fact of the two input data sources.
We describe the notations that will be used throughout this paper.

Let N be the number of given numerical vectors (data points). Let E be the N  X  N matrix whose entries are all one. Let X := ( x 1 ,  X  X  X  , x N ) be given numerical vectors. Each x n has p entries, and let x n ( i )bethe i -th entry of x Let  X  X := (  X  x 1 ,  X  x 2 ,...,  X  x N )where  X  x n := x Y =  X  X T  X  X .Let G be a given network with N nodes and edges. Let W  X  N  X  N be a non-negative, symmet-ric matrix whose ( i, j )entry, w ij is a non-negative weight between nodes i and j . If there is no edge between nodes i and j , w ij is zero. We note that in our problem setting, W is an input having all information on a given graph G and is often called a weight matrix or an affinity matrix . Let  X  W be a matrix whose ( i, j )entry  X  w ij satisfies that  X  w trix whose ( i, i )entry d i satisfies that d i = D := d T d where d := ( d 1 ,  X  X  X  ,d N ). Let M be a matrix which satisfies M := D  X  1 d W .

Let K be the number of clusters which is an input. Let I K be the identity matrix of size K .Let Z := ( z 1 ,  X  X  X  , z an unsigned cluster assignment in which z T k =( z k, 1 , where z k,n (  X  X  0 , 1 } )is1if x n is in cluster k ,otherwisezero.  X  Z := Z  X  Z T Z .Let  X  := (  X  1 ,  X  X  X  ,  X  K )where  X  k be the rep-resentative (or the cluster center) of cluster k .Let Z k a set of nodes in a given graph (or numerical vectors) in cluster k ,andlet |Z k | be the number of all nodes in clus-ter k . Z :=  X  K k =1 Z k .Let V be a diagonal matrix whose ( k, k )entryis |Z k | . L ( Z k , Z k ):= L := L ( Z , Z ).

Let J be a cost of clustering numerical vectors X (or/and nodes in network G ). Let  X  be a numerical parameter which takes a value between zero and one, and balances the two data sources, i.e. numerical vectors X and network G .
We first briefly review the k -means clustering algorithm which is widely used in a lot of applications. The cost (or the objective function) of the k -means algorithm is given as follows: where Dist ( x i ,  X  k ) is a distance between numerical vector x i and cluster representative  X  k of cluster k .Wecanuse any distance such as Euclidean distance, cosine similarity and 1  X  Pearson correlation coefficient. In our experiments, we used cosine similarity which is used for clustering high-dimensional data such as text documents [25]:
The k -means algorithm minimizes the cost of Eq. (1) by repeating the following two steps alternately until conver-gence: 1) updating the cluster representative and 2) updat-ing cluster labels. Figure 1 shows the pseudocode of the k -means clustering algorithm.
We briefly review k -way graph partitioning which divides nodes of a given graph (network) into k clusters. The stan-dard criteria to be minimized in k -way graph partitioning are ratio cut and normalized cut which are given as follows:  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Input : X ,K, Z (0) ,  X  (0) Output : Z ,  X  ,J k-means ( X ,K, Z (0) ,  X  (0) ) 2:  X  X  X  X / 3: while J (  X  X , Z ;  X  ) is not converged do 7: end while  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  The numerator which is common to the above two cuts is the so-called graph min-cut . If we use the graph min-cut only, the clustering result is very sensitive to outliers. That is, a small cluster might be formed, if this cluster is relatively isolated from other nodes. So we need to normalize the graph min-cut by the number of nodes (ratio cut) or the total weight (normalized cut) in each cluster.

We can see that the above criteria can be rewritten as follows:
Finding a set of clusters which minimizes this criterion is an NP-hard problem, and we then solve this problem by relaxing the discrete cluster indicator matrix to a real valued one. We can then have the following optimization problem:
By using Lagrange multipliers, we can easily find that the solution of this optimization can be an eigenvalue problem. In a standard manner of spectral clustering, after solving the eigenvalue problem, we first select the resultant K  X  eigenvectors with the minimum eigenvalues. Then, since we cannot directly assign a cluster label to each numerical vec-tor (or each node in a given graph) by the resultant eigen-vectors, we apply the k -means clustering algorithm to the selected K  X  1 eigenvectors after their normalization. Network modularity is originally defined as follows [6]: where e k ( G ) is the number of edges in cluster k and g is the total sum of degrees over all nodes in cluster k .As shown in the above, the weight attached to each edge was not considered in the original definition of network modularity. Hereafter, we incorporate the edge weight into the network modularity, meaning that the binary definition on each edge is turned into a numerical one. We note that the property of the network modularity is totally kept in this extension. Eq.(2)canthenberewrittenbyusingonly L as follows: This measure is defined by the (weighted) number of edges only, meaning that the original modularity considers the number of edges only. More importantly, the original net-work modularity is not balanced by the cluster size, meaning that a cluster might become small when affected by outliers. Thus we define the new measure which we call normalized network modularity whose cost (or the objective function) is given as follows: J net ( W , Z )= This equation indicates that the larger negative value of this cost a clustered network has, the higher normalized network modularity this network has. The problem of finding the set of clusters which minimizes this cost is NP-hard. We then apply the spectral clustering approach to minimize the cost of normalized network modularity, J ( W , Z ). In the fol-lowing derivation, we partly borrow the idea of the spectral graph clustering by White and Smith which was developed for the original network modularity [22].

We can first modify the problem of minimizing J ( W , Z ) into the problem of minimizing the following trace: As shown in the spectral graph clustering using ratio (or nor-malized) cut, matrix Z must satisfy the following constraint since each node falls into one cluster only:
We can then replace Z with  X  Z and rewrite the trace op-timization problem in the following by relaxing the discrete cluster indicators into real-valued indicators:
The solution can be found via Lagrange multipliers in the following typical eigenvalue problem:  X  where  X  is a diagonal matrix of Lagrange multipliers. This can be further modified using  X  W , the normalized W ,into: When n  X  X  X  , the second term of Eq. (4) approaches zero faster than the first term. In addition, we can approximate  X  W by D  X  1 d W . We then approximate Eq. (4) by the follow-ing simple eigenvalue problem:  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Input : W ,K, Z (0) ,  X  (0) Output : Z 1: Compute D d from W . 3: Compute H of M . 4: Normalize H into  X  H . 5: [ Z ,  X  ,J ]  X  k-means (  X  H ,K, Z (0) ,  X  (0) )  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Figure 2: Pseudocode of spectral clustering for nor-malized network modularity.
 This modification allows M to be a very sparse matrix, meaning that we can reduce the computational cost of solv-ing the eigenvalue problem in Eq. (5).

After solving Eq. (5), we have the resultant K  X  1eigen-vectors with the minimum eigenvalues. That is, we can have N  X  ( K  X  1) matrix, H =( h 1 ,  X  X  X  , h K  X  1 )where h i be the i -th eigenvector of the selected K  X  1 eigenvectors. We then normalize this matrix into N  X  ( K  X  1) matrix,  X  H in which ( n, k )entry  X  h nk satisfies  X  h nk = h nk / is ( n, k )entryof H . This eigenmatrix  X  H can be an input of k -means. Figure 2 shows the pseudocode of this process.
We describe our proposed algorithm for combining two data sources: numerical vectors and a given weighted net-work.

We first set the cost (the objective function) of clustering numerical vectors as follows: where the cluster representative  X  k of cluster k is given as follows: The cost of k -means can be rewritten as follows:
J num (  X  X , Z )= 1 and it can then be a trace optimization problem:
We then combine the cost of clustering numerical vectors which is shown in Eq. (6) with the cost of the normalized network modularity which is given in Eq. (3), using  X  for balancing the two costs: Finding a set of clusters minimizing the integrated cost is also an NP-hard problem, and so we solve this problem by relaxing discrete cluster indicators into real-valued ones. By doing so, we can have the following optimization problem: minimize tr subject to  X  Z T  X  Z = I K (7)
This optimization problem can be also turned by Lagrange multipliers into the following eigenvalue problem for the to-tal cost: where
Once we have the above eigenvalue problem, we can use the same manner of clustering as done in spectral graph clus-tering. That is, given N  X  N matrix M , we can obtain the resultant K  X  1 eigenvectors with the minimum eigenvalues, to generate N  X  ( K  X  1) matrix S =( s 1 ,  X  X  X  , s K  X  1 )where s is the i -th eigenvector of the selected K  X  1eigenvectors. We then use the k -means clustering algorithm, after normal-izing S into  X  S which is the N  X  ( K  X  1) matrix in which ( n, k )entry  X  s nk satisfies  X  s nk = s nk / is ( n, k )entryof S .

As we saw in spectral graph clustering, the constraint given in Eq. (7) can be modified into another form. For example, we can use  X  Z T D d  X  Z = I K which was used for nor-malized cut in graph partitioning. This case, M  X  is given as follows: We used this constraint in our experiments, since normal-ized cut is more often used in graph partitioning than ratio cut. In addition, White and Smith [22] also used this mod-ification when they practically applied their spectral graph clustering with the original network modularity to the real world datasets.
The parameter  X  depends on the spectral space which is generated by combining the two data sources, meaning that the choice of  X  will heavily affect the clustering result. So we propose a method to estimate the optimal  X  value from given two data sources. In this method, varying  X  from zero to one, we choose the  X  which gives the minimum total cost, J total . Figure 3 shows the pseudocode of the whole procedure of our proposed algorithm.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Input : X , W ,K, Z (0) ,  X  (0) Output : Z 1:  X  X  X  X / 2: Y  X   X  X T  X  X 3: Compute D d and D from W 4: for  X  =0to1 do 5: M  X   X  D  X  6: Compute S of M  X  . 7: Normalize S into  X  S . 8: [ Z  X  ,  X   X  ,J  X  ]  X  k-means (  X  S ,K, Z (0) ,  X  (0) ) 9: end for  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X 
Figure 3: Pseudocode of the proposed algorithm. 1. Synthetic Numerical Vectors: We first assume that numerical vectors are randomly generated according to a mixture of von Mises-Fisher distributions [12] in which each component corresponds to a cluster: where  X  k ( k =1 ,  X  X  X  ,K ) are mixture propo rtions satisfying that follows: where I p/ 2  X  1 (  X  ) is the type 1 Bessel function of order p which is given as follows: Interested readers should refer [4] on the method for ran-domly generating numerical values according to this distri-bution. We used the following settings: K =4, p =3,  X  k =0 . 25 ( k =1 ,..., 4),  X  1 =(0 , 0 , 1) T ,  X  2 =(0 , 0 ,  X  3 =(  X  0 . 5 , other parameter,  X  which is called concentration parameter , behaves like the inverse of the variance of numerical vectors in a cluster. Thus numerical vectors which are generated with a larger  X  will be more concentrated on cluster repre-sentatives and be more easily clustered. Thus we changed  X  in our experiments to check the effect of  X  on clustering results. 2. Synthetic Random Network: To generate a net-work, we first assigned a cluster label to each node. We generated a network in which the number of nodes and the number of edges in each cluster are kept the same for all clusters. Thus the generated network was defined by three parameters: N v (the number of nodes in a cluster), N e (the Figure 4: Dataset 1: The distribution of eigenvectors s ( n =1 ,..., 400) which are shown by four different symbols (  X  ,  X  , + , ) corresponding to four different clusters at N  X  =0 . 3 and J =0 . 0538 and (c)  X  =1 and J =0 . 0809 . total number of edges) and N in (the number of edges in a cluster) which is equal to 1 2 L ( Z k , Z k )( k =1 ,  X  X  X  thenchoseavalueof N v to generate a set of nodes and randomly chose node pairs which are connected by edges to satisfy the values of N e and N in . In Dataset 1, we used N v = 100 (meaning that the total number of nodes is 400) and N e =1 , 600, while N in was changed to check how the clustering performance is affected by N in .
To evaluate our clustering results using true cluster labels, we used normalized mutual information (NMI) [18] which has been used in a lot of applications to measure the perfor-mance of clustering methods [24]. A larger NMI value in-dicates a better clustering result. Interested readers should see [18] for the detail of NMI.

We first checked the distribution of s n ( n =1 ,  X  X  X  , 400), i.e. the resultant eigenvectors of the eigenvalue problem in Eq. (8), when we fixed  X  =5and N in = 250. Figure 4 shows the three distributions of eigenvectors which are shown in four different symbols (  X  ,  X  , + , ) corresponding to four dif-ferent clusters, when  X  was at 0, 0.3 and 1. This figure shows that the distribution of eigenvectors changes with varying  X  . In particular, we can see that when  X  =0 . 3, the eigenvec-tors were separated most clearly among the three cases. In fact, when  X  =0, 0.3 and 1, J was 0.0932, 0.0538 and 0.0809, respectively, indicating that eigenvectors at  X  =0 . 3were the most concentrated on the cluster representatives among the three cases of  X  .

We then checked the effectiveness of our method of opti-mally combining two different data sources by using the cost J and NMI, when  X  is changed. We used each data set of all combinations of  X  =1 , 5 and 50 and N in = 250 , 280 and 300. Figure 5 shows J of all these cases, and Figure 6 shows NMI of all these cases. From these figures, first of all, we can see that with increasing N in ((a)  X  (b)  X  (c)), the modularity became higher, resulting with decreasing the cost ( J )and increasing NMI. This might be clearer if we focus on the  X  of one. On the other hand, we can see that with increasing  X  (1  X  5  X  50), numerical vectors were more concentrated on their cluster representatives, resulting with decreasing the cost ( J ) and increasing NMI. This might be also clearer if we focus on the  X  of zero.

In all 18 curves in Figures 5 and 6, the best value is ob-tained when 0 &lt; X &lt; 1, indicating that combining two data sources improved the cost J and NMI of  X  =0and  X  =1. In particular, we emphasize that the  X  value of the minimum J was mostly consistent with that of the maximum NMI. For example, when N in = 250 and  X  =1, J was minimized at  X  =0 . 5 and the maximum NMI was at  X  =0 . 4. Simi-larly, at  X  = 5, the minimum J was at  X  =0 . 4wherethe maximum NMI was obtained. This was true of N in = 280 and  X  =1where  X  =0 . 5 provided the best in both NMI and J . These results imply that our method of selecting  X  worked effectively for optimally combining numerical vec-tors with a given network. An interesting finding is that in a balanced case in which the cost (and NMI) of  X  =0is almost the same as that of  X  = 1, the curve became concave (and convex for NMI), indicating that the minimum cost (and the maximum NMI) can be easily found. On the other hand, in an unbalanced case such as  X  =1and N in = 310 in (c), the curve was not necessarily concave (and convex for NMI), meaning that only one data source (i.e. network information), is much more informative for clustering than the other (i.e. numerical vectors). This is natural, because numerical vectors at  X  = 1 were widely distributed, not concentrating on the cluster representatives, and it would bedifficulttodoclusteringbythem,comparingtothecase of N in = 310 where clustering could be relatively easy by using network information only. Thus in such a case, our method might choose  X  =1(or  X  = 0), but this would be the right selection, since the NMI at  X  =1(or  X  =0)must be the maximum or very close to the maximum in this case.
Using the same parameter setting as that in Figure 6, we finally checked NMI by repeating randomly generating datasets 400 times and averaging the performance over them. Figure 7 shows the averaged NMI obtained by this experi-ment. The curves in this figure were almost similar to those in Figure 6, implying that our results were very stable. To-tally we can say that our method optimally combined the two synthetic data sources. = (a) 250, (b) 280 and (c) 310. 1. Synthetic Numerical Vectors: Numerical values of a gene, such as gene expression, are usually measured exper-imentally, meaning that these values are noisy, comparing to the network which we can derive from a curated database in molecular biology 1 . Thusinthisexperiment,wegenerated synthetic numerical vectors to check the performance of our method of combining two data sources.

As we used a real metabolic network of 636 Saccharomyces cerevisiae genes (See below the way to generate this net-work.), we first assigned a cluster label to each node of the network in the following way: 1) We fixed the number of clusters and repeated running spectral graph clustering by White and Smith [22] 1,000 times on the real metabolic net-work, measuring the o riginal network modularity on the re-sultant clusters at each time. 2) Out of the 1,000 runs, we then chose the clusters with the highest network modular-ity 2 , and these clusters were used to assign a cluster label We show this fact more clearl y in the experiment using Dataset 3.
We note that the clusters with the highest modularity can-to each node. That is, these clusters were used as standard data for evaluation.

We then generated numerical vectors (corresponding to nodes in the metabolic network) in each cluster, assuming that they can be generated according to a component of the von Mises-Fisher distribution mixture as in Section 3.1.1. We used the following settings: K = 10, p =5,  X  k = 0 . 1( k =1 ,  X  X  X  , 10),  X  1 =(1 , 0 , 0 , 0 , 0) T ,  X  2  X  3 =(0 , 1 , 0 , 0 , 0) T ,  X  4 =(0 ,  X  1 , 0 , 0 , 0) T ,  X   X  6 =(0 , 0 ,  X  1 , 0 , 0) T ,  X  7 =(0 , 0 , 0 , 1 , 0) T ,  X   X  to check how clustering results are affected by  X  . 2. Real Metabolic Network: Metabolism is repre-sented by a directed graph, called a metabolic pathway which shows biochemical processes of synthesizing small molecules in a cell. Each node is labeled by a chemical compound. A directed edge from a node, say node A, to another, say node B, is a chemical reaction, meaning that the compound corre-sponding to node B is synthesized from that for node A, and each edge is labeled by a (enzyme) gene which catalyzes the corresponding chemical reaction. From a metabolic path-not be obtained so easily by our method of  X  = 1, since in spectral graph clustering, the final solution is obtained by k -means and is always an approximation. way which is stored in the KEGG (Kyoto Encyclopedia of Genes and Genomes) database [10], we generated a network by connecting two genes by an undirected edge, if they cat-alyze two neighboring chemical reactions in the metabolic pathway. The generated network which we call metabolic network had 636 nodes and 3,104 edges. Most importantly, this network has two important network features: the scale-free property [1] as well as hierarchical network modular-ity [14].
We used NMI again to validate the clustering results ob-tained by our method, with the standard data for evalu-ation. Figure 8 (a) shows the total cost J with changing  X  , and Figure 8 (b) shows NMI with changing  X  .There-sults we can derive from these figures were mostly consis-tent with those obtained by Dataset 1. For example, at  X  =10 2 and 10 3 where the distribution of numerical vectors was relatively concentrated on their cluster representatives, the curves could be concave for J (and convex for NMI), and the  X  value of the minimum cost and the  X  value of the maximum NMI were easily found. Furthermore they were mostly consistent with each other. In addition, at  X  =1 where numerical vectors were distributed broadly, the curve was not necessarily a strong concave for the cost J (and con-vex for NMI), implying that this case, the network informa-tion would be more useful for clustering than the numerical vectors. In fact, NMI at  X  = 1 reached around 0.9, a very high value, whereas that at  X  =0staysatlessthan0.6.
Figure 9 shows the clustering results obtained by our method at three different  X  values when  X  = 10. Ten different colors correspond to ten different clusters. Figure 10 shows the true cluster labels we used for both evaluation and gener-ating numerical vectors. From these figures, we can easily see that the clustering result at  X  =0 . 5 was the most sim-ilar to the true cluster labels among the three networks in Figure 9. For example, the center part of the true clusters were colored orange and dark blue, and this was consistent with the network at  X  =0 . 5 in Figure 9 (b). On the other hand, this center part has a lot of different colors at  X  =0 in Figure 9 (a) and is colored dark blue only at  X  =1in Figure 9 (c).

From these results, we can say that our method worked effectively for Dataset 2 which contains a real metabolic net-work with the scale-free property as well as unbalanced clus-ter sizes. 1. Real Numerical Vectors: Microarray Gene Ex-pression We used a microarray gene expression dataset [9] which has 300 expression profiles (numerical vectors) while around 200 missing values only. This dataset has been often used in the literature of microarray expression analysis [26, 23]. All missing values were interpolated by using the 10-nearest least square method by [19]. 2. Real Metabolic Network: We used the real metabolic network in Dataset 2.
In order to evaluate the clustering result by our method, we used ten categories in metabolism which were stored in the KEGG database. At least one of the ten categories could be assigned to each metabolic gene. We note that these ten categories are not defined directly from the metabolic pathways in the KEGG database, and so these ten clusters cannot be identified by using the metabolic network in our experiment only. We then used NMI again to validate the performance of our clustering result.

Figure 11 (a) shows the cost J of our method with chang-ing  X  . Figure 11 (b) shows NMI of our method with chang-ing  X  . The curves in these figures were not strong concave for J (and convex for NMI), meaning that the two data sources are heavily unbalanced as pointed out in the ex-perimental results of Dataset 1 and Dataset 2. In fact, by looking carefully, we can see that the minimum cost was at around  X  =0 . 8 to 0.9 where the best NMI was obtained, and the difference between the best NMI and NMI at  X  =1was insignificant. Also Figure 11 (b) shows the averaged result over 200 runs of each of the two spectral graph partitioning methods using normalized cut and ratio cut. We note that these methods used graph information only, and the results of them are shown as dotted straight lines in the figure. Our method significantly outperformed these two methods even at  X  = 1, implying that normalized network modularity is more effective for clustering than normalized cut and ratio cut.

We repeated the same experiment replacing the above mi-croarray dataset with datasets derived from a large microar-ray database [5], and found that the results were almost similar to the above case. From this result, we can say that the metabolic network derived from a curated database is more reliable in clustering metabolic genes than microarray expression. This result is consistent with existing under-standing on data reliability in molecular biology.
For Dataset-3, the  X  for the minimum cost took a value which was very close to one, around where NMI was max-imum or very close to the maximum. Thus we think that our method succeeded for this d ataset. As well in Dataset-2, our method worked favorably for optimally combining the real metabolic network with more reliable numerical vectors. Thus if we have more reliable numerical vectors on genes, the clustering result would be improved much more. Over all we can say that our method itself is very promising.
We have presented a new spectral approach to clustering numerical vectors with a net work. The focus of our method was on network modularity, a key network property in clus-tering, and we defined a new criterion, normalized network modularity, for combining the two different data sources in the framework of spectral clustering. A significant advan-tage of our method is that we can optimize the weight pa-rameter for balancing the two data sources, i.e. numerical vectors and a network. Also our algorithm is time-efficient, and practical computation time was less than one minute for any dataset in our experiment. Experimental results obtained by using three different types of datasets showed that our method worked favorably for optimally combining numerical vectors and a network.

In this paper, we have focused on two data sources, i.e. numerical vectors and a network, both in methodology and experiments. Our method can be easily extended to a more general framework for combining multiple heterogeneous data sources for clustering, and by doing so, the resultant clus-tering performance might be improved by adding another different data source. Thus interesting future work is to ap-ply the proposed method to a variety of real-world datasets to characterize more systematically under which the pro-posed method works well. And this would be performed by not only two different data sources but also more than two heterogeneous data sources, say numerical vectors, a net-work and another different type of dataset. It would also be interesting to develop, in the context of clustering, a general criterion which can cover a lot of distances for numerical val-ues as well as graph partitioning criteria such as normalized cut, ratio cut and network modularity.
This work is supported in part by Bioinformatics Edu-cation Program  X  X ducation and Research Organization for Genome Information Science X  and Kyoto University 21st Century COE Program  X  X nowledge Information Infrastruc-ture for Genome Science X  with support from MEXT, Japan. [1] A.-L. Barab  X  asi and A. Reka. Emergence of scaling in [2] S. Basu, M. Bilenko, and R. J. Mooney. A [3] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means, [4] I. S. Dhillon and S. Sra. Modeling data using [5] R. Edgar, M. Domrachev, and A. E. Lash. Gene [6] R. Guimera and L. A. Nunes Amaral. Functional [7] R. Guimera, M. Sales-Pardo, and L. A. N. Amaral. [8] L. Hagen and A. B. Kahng. New spectral methods for [9] T. R. Hughes et al. Functional discovery via a [10] M. Kanehisa et al. From genomics to chemical [11] B. Kulis, S. Basu, I. Dhillon, and R. J. Mooney. [12] K. V. Mardia and P. E. Jupp. Directional Statistics . [13] M. E. J. Newman and M. Girvan. Finding and [14] E. Ravasz et al. Hierarchical organization of [15] J. Shi and J. Malik. Normalized cuts and image [16] M. Shiga, I. Takigawa and H. Mamitsuka. Annotating [17] C. Song, S. Havlin, and H. A. Makse. Self-similarity of [18] A. Strehl and J. Ghosh. Relationship-based clustering [19] O. Troyanskaya et al. Missing value estimation [20] K. Wagstaff and C. Cardie. Clustering with [21] D. J. Watts and S. H. Strogatz. Collective dynamics of [22] S. White and P. Smyth. A spectral clustering [23] L. F. Wu et al. Large-scale prediction of [24] S. Zhong and J. Ghosh. A unified framework for [25] S. Zhong and J. Ghosh. Generative model-based [26] X. Zhou, M. C. Kao, and W. H. Wong. Transitive
