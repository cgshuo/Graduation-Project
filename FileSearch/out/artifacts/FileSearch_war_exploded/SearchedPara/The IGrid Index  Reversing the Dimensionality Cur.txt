 The similarit y searc h and indexing problem is w ell kno wn to b e a dicult one for high dimensional applications. Most indexing structures sho w a rapid degradation with increas-ing dimensionalit y whic h leads to an access of the en tire database for eac h query . F urthermore, recen t researc h re-sults sho w that in high dimensional space, ev en the concept of similarit y ma y not be v ery meaningful. In this pap er, w e prop ose the IGrid -index; a metho d for similarit y index-ing whic h uses a distance function whose meaningfulness is retained with increasing dimensionalit y . In addition, this tec hnique sho ws p erformance whic h is unique to all kno wn index structures; the p ercen tage of data accessed is in v ersely prop ortional to the o v erall data dimensionalit y . Th us, this tec hnique relies on the dimensionalit y to b e high in order to pro vide p erformance ecien t similarit y results. The IGrid -index can also supp ort a sp ecial kind of query whic h w e refer to as pro jected range queries; a query whic h is in-creasingly relev an t for v ery high dimensional data mining applications.
 H.2.8 [ Database Managemen t ]: Database Applications Dimensionalit y Curse, Indexing The similarit y searc h problem is de ned as follo ws: for a giv en target record in m ulti-dimensional space, nd the clos-est record to it based on some pre-de ned distance measure. This tec hnique nds applications in n umerous domains suc h as spatial databases, m ultimedia systems, data mining, and image retriev al [4, 5]. With the increasing a v ailabilit y of large rep ositories of v ery high dimensional data in n umer-ous application domains, it b ecomes increasingly imp ortan t to dev elop ecien t query pro cessing and similarit y indexing tec hniques for suc h data.
 An um b er of tec hniques suc h as KDB-T rees, kd-T rees, and Grid-Files are discussed in the classical database literature [26] for indexing m ultidimensional data. Man y of these tec hniques w ere initially prop osed in the con text of lo w-dimensional spatial applications. Starting with the seminal w ork of Guttman on R-T rees [18], considerable w ork has b een done on nding m ulti-dimensional index structures in the database arena. V arian ts of R-T rees suc has R -T rees, R + -T rees, and Hilb ert R-T rees [7, 21, 27], are able to re-solv ev arious kinds of queries more ecien tly ,b y using b etter pac king and split metho ds to build the tree structure. All of these metho ds partition the data in to ranges whic h are parallel to the original axis system. Subsequen tly , metho ds suc h as SS-T rees [28] w ere prop osed whic h do not necessar-ily partition the data using ranges from the original set of attributes. Other prominen t indexes and query pro cessing metho ds ma y b e found in [5, 20, 24, 28].
 The ab o v e tec hniques generally w ork w ell for lo w dimen-sional problems, though they degrade rapidly with increas-ing dimensionalit y , so that eac h query requires the access of almost all of the data; consequen tly sp ecialized tec hniques for high dimensional similarit y indexing suc h as X-T rees, SR-T rees and TV-T rees [11, 22, 23] ha v e b een prop osed. Because of the inheren t dicult y of exact nearest neigh-b or searc h for v ery high dimensional data, some in teresting metho ds ha v e also b een prop osed for appro ximate nearest neigh b ors in these cases [8, 17]. Other relev an t metho ds include the p yramid tec hnique [9], whic h ha v e b een pro-p osed for high dimensional range queries. Despite these in-dexing and query pro cessing metho ds, it has b een observ ed that with almost an y tec hnique, when the dimensionalit yis ab o v e 15 or 20, then all of the data is accessed. In fact, re-cen t results [6] sho w that a simple sequen tial scan p erforms b etter than an y of the space partitioning metho ds on uni-formly distributed data when the dimensionalit y is larger than 610. 1 This b eha vior has b een v alidated with empirical testing for signi can tly lo w er dimensionalities [15]. Con-sequen tly , a tec hnique called the V A-File [6] w as prop osed whic h assumes that a sequen tial scan is inevitable and tries to build an index b y compressing the time required for this sequen tial scan to b et w een 12 : 5% 25% of the data. This is ac hiev ed b y scanning a compressed represen tation of the data; some additional time o v erhead is required for resolv-ing the con icts due to the information lost in compression. This metho d sho ws more promising results than the b est 1 In practice this threshold is w ell b elo w 610, since the w orst case analysis is based on v ery crude estimations and b ounds.
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 space partitioning metho ds.
 Recen t researc h results sho w that in high dimensional space, ev en the concept of pro ximit yma y not b e v ery meaningful [12]. These results sho w that for certain classes of commonly used similarit y functions suc hasthe L p -norm, the nearest and furthest neigh b or are of the same relativ e distance to the query p oin t for large classes of data distributions. Sev-eral additional in teresting prop erties of the L p -norm ma ybe found in [1]. The lac k of relativ e con trast in terms of simi-larit y is somewhat undesirable, since it is not clear whether or not the nearest neigh bor is meaningful under suc h cir-cumstances. F or man y high dimensional data mining ap-plications, ev en the concept of similarit y is a heuristic one, and is not w ell de ned. There is not m uc h consensus and literature on nding distance functions whic h result in the most meaningful de nition of similarit y . The common use of the Euclidean distance metric for indexing structures arises from their initial applicabilit y to spatial databases (for whic h the L 2 -norm has sp ecial meaning). F or man y other high di-mensional application domains suc h as information retriev al (IR), categorical data and mark et bask et data, the imp or-tance of designing qualitativ ely e ectiv e and meaningful dis-tance functions using the aggregate statistical b eha vior of the data has b een w ell understo o d and appreciated [13, 16, 25]. These tec hniques ha v eho w ev er not b een applied to the m ultidimensional indexing problem in the quan titativ e do-main, where man y applications ha v e con tin ued to use the increasingly irrelev an t L p -norm.
 The lac k of de niteness in measuremen t of similarit y for high dimensional data has b een explored in recen tw ork, where the distance distributions of the data to the query p oin tin di eren t pro jections are examined in order to iden tify in ter-esting query-sp eci c features [19]. These iden ti ed features are used in order to determine the most similar ob jects. This tec hnique is called pr oje cte dne ar est neighb or se ar ch and can be v ery v aluable in understanding dimensional selectivit yin the neigh borhood of a query p oin t. Since it is a dicult problem to nd the b est com bination of dimensions in the neigh b orho o d of a query-p oin t, the tec hnique is slo w er than a sequen tial scan. Th us, this system is v ery useful for deal-ing with the qualit y issue of nearest neigh bor searc h and understanding the data b y pro viding (lo cally) in teresting com binations of dimensions whic h are discriminatory pro-jections. On the other hand, the tec hnique is not really fo cussed on the problem of rapid query resolution. In order to pro vide suc h capabilit y ,w e need to dev elop a meaningful similarit y function whic h can b e expressed in closed form, and can b e used in conjunction with an e ectiv e index. T o this e ect, w e dev elop a class of distance functions whic h are somewhat similar to the L p -norm in lo w dimensions, but b e-come increasingly di eren t with greater dimensionalit y . In addition, the dev elopmen t of a closed form distance function has implications for a wide v ariet y of other dicult high di-mensional data mining problems whic h rely on pro ximit y concepts.
 This pap er sho ws ho w the aggregate summary b eha vior of the data ma y b e used in order to design a qualitativ ely ef-fectiv e function whic h computes distances in a more exible w a y than the straigh tforw ard summation of distances o v er all dimensions. W e will pro vide a theoretical analysis of the meaningfulness of this distance function, whic h sho ws that it do es not su er from the lac k of discrimination caused b y high dimensional sparsit y as discussed in [12]. This theo-retical analysis is supplemen ted with qualitativ e empirical evidence whic h suggests that this metho d leads to more meaningful results. A k ey asp ect of this class of distance functions is that it turns out to b e index-friend ly ; w e will design a class of index structures ( IGrid and IGrid + ) based on these functions. These index structures sho w the sur-prising b eha vior of the rev ersal of the dimensionalit y curse in terms of p erformance; in other w ords the p ercen tage of data accessed is in v ersely prop ortional to the dimensional-it y . This is in con trast to the b eha vior of almost all kno wn indexing structures and algorithms.
 This pap er is organized as follo ws. The remainder of this section discusses the bac kground and de nition of the simi-larit y function. In section 2, w ein tro duce the IGrid index. A theoretical analysis is discussed in section 3. Section 4 dis-cusses ho win ter-attribute correlations ma y b e used in order to impro v e the similarit y calculations. In section 5, w e will discuss the application of the IGrid -index to a sp ecial kind of query whic hw e refer to as pro jected range queries. Em-pirical results are presen ted in section 6. Section 7 discusses an o v erview of the results of this pap er. This pap er discusses an indexing tec hnique for similarit y searc h whic h is b oth qualitativ ely e ectiv e and sho ws p er-formance ecien t b eha vior. T o this e ect, w e sho w that as the dimensionalit y increases it b ecomes more imp ortan t to compute similarit y functions in more exible w a ys than is the case with the L p -norm. The discrimination b eha vior of the similarit y function is further enhanced b y using the aggregate in ter-attribute correlation b eha vior. W e pro vide empirical evidence that this enhanced similarit y measure is more meaningful than the euclidean metric for real data sets in high dimensional data mining applications. A t the same time, this similarit y function can b e indexed e ectiv ely b y a metho d called the IGrid -index whic h sho ws the surpris-ingly pleasan t b eha vior of impro v ed p erformance with in-creasing dimensionalit y . This b eha vior is ro oted in the fact that the index and the distance measure exploit the greater statistical information a v ailable in high dimensional data in a more e ectiv ew a y . Th us, it solv es the problem of mean-ingful and p erformance ecien t similarit y indexing in one uni ed framew ork. W e also sho who w the IGrid -index can b e utilized for e ectiv e resolution of a sp ecial kind of query called the pro jected range query; a query whic h b ecomes more relev an t than the full dimensional query for v ery high dimensional applications. In this section, w e will illustrate some of the recen t results [12] whic h sho w that in high dimensional space, the ratio of the relativ e distances of the di eren t p oin ts to a giv en target con v erges to one. In order to do so, w e will establish certain notations and de nitions: d : Dimensionalit y of the data space N : Num b er of data p oin ts X d : Data p oin t from d -dimensional data distribution Q d dist d ( X d ) : Distance of X d from the origin (0 ;::: ; 0) D min d ; D max d : Nearest/F arthest distance of N p oin ts to origin E [ X ] ;var [ X ] : Exp ected v alue/v ariance of random v ariable X Y d ! p c : Y d con v erges in probabilit yto c as d )1
Theorem 1. Bey er et al. [12] If:
Pr oof. See [12] for pro of of a more general v ersion of this result.
 This result sho ws 2 that under certain conditions on the dis-tance functions and data distribution, the di erence b et w een the maxim um and minim um distances to a giv en target (the origin 3 in this case) do es not increase as fast as the nearest distance to an y p oin t in high dimensional space. This mak es a pro ximit y query meaningless and unstable b ecause there is p o or discrimination b et w een the nearest and furthest neigh-b or. Th us, in v ery high dimensional space a small relativ e p erturbation of the target p oin t in a direction a w a y from the nearest neigh b or could easily c hange the nearest neigh-bor in to the furthest neigh b or.
 These results are v aluable not just from the p ersp ectiv eof meaningfulness but also from the p erformance p ersp ectiv e of indexing. Most indexing metho ds w ork b y using some kind of partitioning (hierarc hical or at) of the data set. This partitioning is then used in order to p erform e ectiv e pruning of the data set. The k ey idea is that if it is already kno wn that some neigh bor X is close enough to the target, then one can prune a w a yanen tire partition b y sho wing that the optimistic distance b ound to that partition is no b etter than the distance to X [24]. The results in [12] sho w that in high dimensionalit y the nearest and farthest neigh bor ha v e v ery similar relativ e distances to the target. Consequen tly , the optimistic b ounds used b y most index structures are usually not sharp enough for an y kind of e ectiv e pruning in partition based metho ds. These results also sho w ho w appro ximate nearest neigh bor indexes [8, 17] are a ected b y increasing dimensionalit y; when the ratio of nearest to furthest distance is almost ev en, then the qualit yof an -appro ximate solution whic h is within a pre-sp eci ed ratio 1+ of the b est solution ma y be as bad as the furthest neigh b or. Th us, the excellen t -dep enden t p erformance re-sults of these tec hniques come at an increasing qualitativ e price with greater dimensionalit y and xed . In addition, these results exp ose the meaningfulness issues of a v ast n um-b er of problems suc h as clustering whic h rely on the concept of pro ximit y; some recen t results [2, 3] sho who w these prob-lems can b e understo o d more e ectiv ely b y using p oin ts and dimensions in a more exible w a y . 2 Other results [8] also sho w that under certain di eren t pre-conditions, the meaninglessness b eha vior is not quite as bad. Ho w ev er, some evidence of the degradation has b een demon-strated b y empirical testing for real distributions [12]. 3 W eha v e used the origin as the target in this case in order to simplify presen tation. See [12] for a more general statemen t of results. One of the reasons for the lac k of discrimination b et w een the nearest and furthest neigh b or is the fact that for ev ery pair of p oin ts there are dimensions with v arying distances to the corresp onding v alues in the target. The dominan t comp o-nen ts of distance functions suc h as the Euclidean metric are the dimensions on whic h the p oin ts are farthest apart; for the particular case of high dimensional data, this results in v ery p o or measuremen t of similarit y . This is b ecause when the dimensionalit y is high, ev en the most similar records are lik ely to ha v e a few feature v alues whic h are w ell separated b ecause of noise e ects and sparseness of the data; the ex-act degree of dissimilarit y on these few noisy dimensions will determine the order of distances to the target. In general, for a giv en feature, w e exp ect the v alues for t w o randomly pic k ed records to b e reasonably w ell separated (a v erage sep-aration along that range); there is no in teresting statisti-cal information in this fact. F or distance functions suc has the L p -norm, the results of [12] sho w that the a v eraging ef-fects of the di eren t dimensions (man y of whic h are noisy) start predominating with increasing dimensionalit y . A dif-feren t and complemen tary view of similarit yw ould b e one in whic h a prede ned pro ximit y threshold is de ned for eac h dimension, and the o v erall similarit y is de ned b oth b y the n um b er and qualit y of similarit y along the dimensions on whic h the t w o records are more pro ximate than this thresh-old. Th us, the similarit y function is directly a ected b y the n um b er of dimensions whic hha v e this in terestingly high lev el of pro ximit y , and b ey ond a certain qualit y threshold, the exact degree of dissimilarit yonagiv en dimension is not considered relev an t. Since the meaningfulness problem is sensitiv e to the data dimensionalit y , the criterion for pic k-ing this pro ximit y threshold is also dep enden t on the data dimensionalit y . Sp eci cally , it is deriv ed using a theoreti-cal analysis of meaningfulness; w e will revisit this issue in a later section.
 The analysis indicates that this de nition of similarit y con-tin ues to retain its meaningfulness for higher dimensionali-ties in terms of the relativ e con trasts in distances to a giv en target. In addition, w e will pro vide evidence using sev eral high dimensional real data sets that the qualit y of the near-est neigh b or returned b y suc h a tec hnique is as go o d or b et-ter than that pro vided b y the L p -norm. A t the same time, it is p ossible to design index structures whic h are able to prune a w a y a large part of the data while searc hing for a closest neigh b or to the target.
 W e will rst de ne a simple distance (similarit y) function in whic h higher n um b ers imply greater similarit y; later w e will sho who w to mo dify this distance function in order to use pro ximit y thresholds on the individual dimensions. Let X = ( x 1 ;::: x d ) and Y =( y 1 ;:: :y d )be t w o sets of co ordinates in d -dimensional space, so that ( x i ;y i ) 2 [ l i ;u i of lo w er and upp er b ounds l i and u i . Then, the distance (similarit y) function I D ist ( X; Y )bet w een X and Y is giv en b y: The presence of u i l i in the denominator studen tizes (or normalizes) the distances with resp ect to the di eren t ranges of the co ordinates.
 In order to incorp orate the concept of pro ximit y threshold-ing in the similarit y function, w e discretize the data in to sev eral ranges. Sp eci cally ,w e assume that eac h dimension is divided in to k d e qui-depth 4 ranges. Eac h of these is a con-tiguous range of v alues, suc h that a giv en range con tains a fraction 1 =k d of the total n um b er of records. Sp eci cally ,w e denote the j th range for dimension i b y R [ i; j ]. In order to emphasize the dep endency (to b e determined later) of k d the data dimensionalit y ,w eha v e used the dimensionalit y d in the subscript.
 Let X = ( x 1 ;:::x d ) and Y = ( y 1 ;:::y d ) be t w o records. Then the set of dimensions on whic h the t w o records are similar are those whic h share the same ranges. Th us, for dimension i , if b oth x i and y i b elong to the same range R [ i; j ], then the t w o records are said to b e in pr oximity on dimension i . The en tire set of dimensions on whic h the t w o records lie in the same range is referred to as the pr oximity set . Let S [ X; Y ; k d ] be the pro ximit y set for t w o records X and Y for a giv en lev el of discretization. F urthermore, for eac h dimension i 2 S [ X; Y ; k d ], let m i and n i upp er and lo w er b ounds for the corresp onding range in the dimension i in whic h the records X and Y are in pro ximit y to one another. Then, for a giv en pair of records X and Y and a lev el of discretization k d , the similarit ybet w een the records is giv en b y:
P I D ist ( X; Y ; k d )= Note that the v alue of the ab o v e expression will v ary b et w een 0 and jS [ X; Y ; k d ] j , since eac h individual expression in the summation lies b et w een 0 and 1.
 The ab o v e use of the similarit y function guaran tees a non-zero similarit y comp onen t only for those dimensions, in whic h the t w o records are pro ximate enough. The use of equi-depth partitions ensures that the probabilit y that t w o records ha v e a comp onen t in the same partitions giv en b y1 =k d . Th us, on the a v erage the ab o v e summation is lik ely to ha v e d=k comp onen ts. F or more similar records, the n um b er of suc h dimensions will b e greater, and eac h suc h individual comp o-nen t is also lik ely to con tribute more to the similarit yv alue. The ab o v e function leads to the ignoring of the exact degree of dissimilarit y on the distan t dimensions: w e will see from our empirical tests, that for the case of high dimensional data this creates a sparsit y/noise reduction whic h out w eighs the e ects of information loss. The use of equi-depth ranges as opp osed to equi-width ranges has considerable signi cance. In real applications, the data ma y be distributed in a v ery non-uniform w a y across the di eren t attributes. As a result, simple distance functions suc h as the L p -norm fail to tak e the aggregate b eha vior of the data in to accoun t while measuring similarit y . The use 4 In equi-depth ranges, eac h range con tains an equal n um-ber of records. In equiwidth ranges, eac h range con tains a similar length of v alues co v ered. The reason for pic king equi-depth ranges will b ecome clear so on. of equi-depth partitions ensures that when a particular re-gion is v ery dense, then the categorical range v alue is m uc h smaller. The idea here is that the pro ximit yof t w o records for a giv en feature v alue should not b e treated in a uniform w a y across the en tire range, but it should b e based on ho w close these features are with resp ect to the b eha vior of the entir e data set . Th us, the use of equi-depth ranges creates a an implicit normalization of the data b y taking in to accoun t the aggregate b eha vior in that range. This kind of normal-ization is similar in spirit to idf-normalization tec hniques [25] used in IR applications. Another adv an tage of the use of equi-depth ranges is the abilit y to predict and con trol the indexing b eha vior using the discretization parameter; an is-sue whic hw e will revisit in later sections. The use of ranges in the similarit y function pro vides consid-erable adv an tages in the use of an in v erted index in order to p erform the similarit y calculations. The IGrid -index (In-v erted Grid index) is based on the use of the in v erted index on a grid represen tation of the data. The division of the data in to ranges automatically creates a grid structure, in whic h eac h record b elongs to a particular cell. W e create an in v erted represen tation of the data as follo ws: (1) F or eac h range j for eac h dimension i w e main tain the lo w er and upp er b ounds of the range R [ i; j ]. (2) F or eac h range for eac h dimension, w e main tain a list of the record iden ti ers whic h lie in that range. Note that the use of k d equi-depth partitions along eac h dimension ensures that the length of eac hin v erted list will b e N=k d , where N is the total n um b er of records. (3) Along with eac hen try in the list of record iden ti ers, w e k eep the actual co ordinate for the corresp onding dimension in that record.
 Note that the size of the in v erted represen tation of the data is comparable to the size of the original database. Once w eha v e constructed this in v erted represen tation of the data from the grid structure, the calculation of similarit yis v ery simple. F or the target record, w e nd the appropriate range for eac h dimension. Then w e examine the corresp onding d lists. Since there is a total of d k d lists, the p ercen t-age of data accessed is small when the v alue of k d is large. The metho d of calculating similarit yisv ery similar to the metho d often used in information retriev al applications in using the in v erted represen tation for calculating the similar-it y based on w ord frequencies [25].
 Let T =( t 1 :::t d )beagiv en target record, and let m i b e the upp er and lo w er b ounds for the corresp onding range for dimension i . W esa y that a record is touche d b y the tar-get, if it lies on at least one of the d lists corresp onding to the ranges in the target record. The hash table main tains a record of the similarit yv alue of all the records whic h are touc hed b y the target. An en try is added to the hash table the rst time it is encoun tered during the examination of the data. While examining eac hen try on the lists with corre-sp onding co ordinate v alue x 0 ,w e calculate the con tribution de ned in Equation 2) to the similarit y function and k eep adding that v alue to the appropriate en try in the hash table. F or the purp ose of our results, w e used the v alue p =1. A t the end of the pro cess, the hash table en try with the largest similarit y v alue is the closest neigh b or. The metho d can easily b e generalized to nd the m ultiple nearest neigh b ors. An observ ation to b e k ept in mind is that the in v erted repre-sen tation alw a ys returns the IDs of the records as opp osed to the records themselv es. Ho w ev er, it is also assumed that an y reasonable query w ould ha v e an output whic h is signi can tly smaller than the original database (for example, for a simi-larit y query one ma y ask for the most similar record out of an enormous database of records). Th us, a small amoun tof constan t time is required in order to access the records from the database using the IDs of the returned records (using the natural index b y ID), an o v erhead whic h is indep enden tof database size and dep enden t only on the size of the output. If it ma y b e assumed that the users are in terested only in online queries whic hha v e resp onses that are small enough for in teractiv e searc h and exploration, then this o v erhead is (asymptotically) negligible for larger and larger collections of data. The p erformance of this tec hnique impro v es with increasing k ;av alue whic hw e will determine b y meaningfulness con-siderations. This is b ecause exactly d out of the d k d of the lists are accessed, and eac h list is of the same size. Th us, a fraction 1 =k d of the en tire in v erted index is accessed. Ho w-ev er, in order to mak e a fair comparison, w e w ould also need to compare the in v erted index size to that of the orig-inal database. Note that eac h Record Iden ti er o ccurs on exactly d lists; therefore the total n um ber of iden ti ers is N d . Along with eac h iden ti er in an in v erted list, w e store the actual v alue for the corresp onding feature of that record. This requires the storage of another N d v alues (in all). Th us, the total space requiremen t of the index is 2 N d , whereas the original database requires N d space. This cor-resp onds to a storage (and hence p erformance) o v erhead 5 of 100%. In this case, since the o v erhead is indep enden t of the v alue of k d , it follo ws that the larger the v alue of k the b etter the p erformance. In the next section, w e will dis-cuss ho w k d is determined b y meaningfulness considerations in a giv en dimensionalit y d . An imp ortan t issue in this tec hnique is to a v oid the hash ta-ble o v er o w in the pro cess of similarit y calculations. This is b ecause the p oten tial n um b er of hash table en tries is lik ely to be v ery large, and the hash table is alw a ys main tained in memory . In order to actually implemen t the system, the en tire database is not indexed as one en tit y . Instead, the database is divided in to c h unks. An in v erted index is built separately for eac h c h unk, and the most similar record is found one b y one for eac h c h unk. A t the end, the b est matc h among all c h unks is rep orted. Note that this division of database in to c h unks do es not c hange the p erformance b e-ha vior in terms of disk accesses from the in v erted index, but it eliminates the p ossibilit y of hash table o v er o ws. The size of eac hc h unk is determined b y k d times the total n um ber 5 W e do factor in these o v erheads in the p erformance results of the empirical section. W e will see that in spite of these o v erheads, the p erformance of the index is substan tially b et-ter than comp eting metho ds. of en tries in the hash table. This eliminates the p ossibil-it yof ano v er o w b ecause exactly 1 =k d of the en tries in the in v erted index are accessed without coun ting rep etitions. Note that the similarit y function is highly in uenced b y the n um b er of dimensions in whic h the record lies in the same range as the target. In general, w e exp ect that the nearest record will ha v e large cardinalit y of the pro ximit y set. It is insigh tful to lo ok at a crude appro ximation of the similarit y function in whic h only the cardinalit y of the pro ximit y set is used for the purp ose of measuring similarit y . The discrim-ination b eha vior of this function will pro vide considerable insigh tin to the case when the more re ned metho d of using the actual co ordinates are applied.
 In order to analyze the discrimination b eha vior, let us con-sider t w o records X and Y whic h are pic k ed from uniformly randomly distributed data. F rom the p ersp ectiv e of index-ing and meaningfulness degradation, uniformly distributed data is the most dicult case with increasing dimensional-it y . Then, on an y giv en dimension the ev en t that the t w o records lie in the same range is a b ernoulli random v ariable with success parameter 1 =k d . Sp eci cally , let us de ne the b ernoulli random v ariable M i as follo ws: M i =0 : x i ;y i not in same buc k et for dimension i M i =1 : x i ;y i in same buc k et for dimension i The random v ariable M i has a mean of 1 =k d and a v ari-ance of (1 =k d ) (1 1 =k d ). Let L b e the random v ariable, whic h is the sum of this b ernoulli v ariable o v er the d di-mensions. Th us, w e ha v e L = assume that X and Y are dra wn from uniform distribu-tions, then the v alues of M i will also be indep enden t and iden tically distributed. In suc h a case, when the dimen-sionalit y is high, the distribution of L approac hes a normal distribution whic h has a mean of ( d; k d ) = d=k d , and a standard deviation of ( d; k d )= pre-condition of Theorem 1 when in terpreted in this con-text w ould imply that the meaninglessness b eha vior is ex-hibited when lim d !1 ( d; k d ) = ( d; k d )=0. Consequen tly , for the purp ose of this analysis, w e will analyze the be-ha vior of the ratio ( d; k d ) = ( d; k d ) in order to measure the e ects of the discretization parameter on this result. The higher this ratio, the greater the lev el of discrimina-tion among the distances to the di eren t records. Using the ab o v e analyzed v alues of ( d; k d ) and ( d; k d ), w e obtain ( d; k d ) = ( d; k d )= the discretization parameter k d . Th us, an increase in k pro v es the meaningfulness b y ignoring the exact degree of dissimilarit y on the sparse dimensions; on the other hand, pic king k d to o large ma y result in loss of information along with noise reduction. Suc h a tradeo is handled b y pic king the minim um v alue of k d dep enden ton d so that the precon-dition of Theorem 1 is violated. In other w ords, w ew ould lik e: This implies that w e should pic k k d whic h is at least linearly dep enden ton d . Consequen tly , for the purp ose of this pap er, w e will use k d = d d e , where is some constan t. Note that pic king = 1 results in a distance function whic h is exactly similar to the L p -norm for 1-dimensional data, but b ecomes increasingly di eren t with increasing dimensionalit y . Simi-larly , pic king =0 : 5 and p = 1 creates a distance function whic h is similar to the L 1 -norm for 2-dimensions but it b e-comes di eren t in higher dimensionalities. Assuming that L -norm distance functions w ork w ell for lo w dimensional problems, these observ ations pro vide sucien t guidance to a good c hoice of to lie in the region of 0.5 or 1.
 F or this v alue of the discretization parameter the v alue of ( d; k d ) = ( d; k d ) remains almost constan t (and in fact in-creases sligh tly) with increasing dimensionalit y . As w e shall see later in the empirical section, this also has a direct e ect on the meaningfulness b eha vior of the similarit y function whic h do es not v ary m uc h with increasing dimensionalit y . Since w e sho w ed earlier that the p erformance of the IGrid -index impro v es with increasing k d ,ac hoice of k d = d d e results in an index structure for whic h the p erformance im-pro v es with increasing dimensionalit y . In high dimensional space, man y of the attributes are cor-related with one another. In ter-attribute correlations ha v e often b een used for designing distance functions in categor-ical domains where there is no natural ordering of attribute v alues. In suc h cases, the use of in ter-attribute summary in-formation pro vides the only p ossible insigh tin to the similar-it y of ob jects b y examining whether commonly co-o ccuring in ter-attribute v alues are presen t in the t w o ob jects [13, 16]. This insigh t is equally relev an t ev en for quan titativ e domains of data where a natural ordering of attribute v al-ues exists. The use of aggregate data b eha vior in order to measure similarit y b ecomes more imp ortan t for high dimen-sional data, where there ma y b e considerable redundancies, dep endencies, and relationships among the large n um ber of attributes [10]. Since a lot of the pro ximit y information ma y b e hidden in the aggregate summary b eha vior of the data, the use of the linearly separable L p -norm ma y be a poor represen tation of the similarit y , when considered in ligh tof the aggregate statistical b eha vior. W e note here that some data domains suc h as text factor the correlation b eha vior indirectly in to the distance function b y using data transfor-mation tec hniques suc h as Laten t Seman tic Indexing [14]. The rst step is to determine pairs of attribute ranges whic h are strongly correlated with one another. Let us de ne one new pseudo-attribute corresp onding to eac h range. Th us, there are a total of k d d pseudo-attributes, eac h of whic h corresp onds to a range from the original set of attributes. Let us denote these pseudo-attributes b y a 1 , a 2 , :: : , a pseudo-attribute is a 0-1 v alue indicating whether or not the corresp onding attribute in the record con tains that range. Exactly d of the pseudo-attributes tak e on the v alue of 1, whereas the others tak e on the v alue of 0. Th us, in terms of this new represen tation, a giv en record Z =( z 1 :::z d b e expressed as the set f i 1 ; :::i d g , where, for eac h r 2 (1 ;d ), a pseudo-attributes a i and a j ,w e calculate the corresp onding supp ort. The supp ort of a pair of pseudo-attributes is equal to the p ercen tage of records whic h tak e on the v alue of 1 for b oth of these pseudo-attributes. Th us, this v alue is sp eci c to the aggregate b eha vior of the en tire data set whic h is b eing indexed. Let us denote this supp ort b y s ij . The larger the v alue of this supp ort, the greater the lev el of correlation bet w een these pairs of pseudo-attributes. Th us, if w eha v e a pair of records X and Y , suc h that X con tains a i and Y con tains a j , and s ij is high, then this is eviden tial of the similarit ybet w een X and Y . Since there are a total of k pseudo-attributes, a total of k d 2 d computed. W e pic k a fraction f of the largest suc hv alues of the in ter-attribute correlation. These are the strongly connected pairs of comp onen ts.
 Let X = ( x 1 ;:: :x d ) and Y = ( y 1 ;::: y d ) be t w o sets of records. Then, let f xi 1 :::xid g and f yi 1 ::: yid g b e the in-dices of the corresp onding d pseudo-attributes whic h tak e on the v alue of 1. Then, the similarit ybet w een the records X and Y is equal to the n um b er of strongly connected pairs among the pseudo-attributes in X and Y . Th us, if the in ter-attribute correlation similarit y is denoted b y C I D ist ( ; ), then w eha v e: C I D ist ( X; Y ) = Cardinalit yof S where: S = f ( xip; y iq ): a xip and a yiq are strongly connected g The total similarit ybet w een t w o records is the sum of the pro ximit y-threshold based similarit y and correlation-based similarit y . Th us, w e ha v e D ist ( X; Y ) = P I D ist ( X; Y )+ C I D ist ( X; Y ). Eac h of the t w o records X and Y con tains exactly d pseudo-attributes. Th us, there are a total of d ( d 1) com binations of these pseudo-attributes, from records X and Y . Since only a fraction f of these p ossibilities are strongly connected on the a v erage, it means that if the data is uniformly distributed, the con tribution of C I D ist ( ; )to the similarit y function is lik ely to b e f d ( d 1). Recall that the con tribution of the P I D ist comp onen t of the sim-ilarit y function is of the same order of magnitude as the pro ximit y set, whic h in turn is again exp ected to b e d=k Consequen tly ,b yc ho osing f = c= (( d 1) k d ), w e obtain a similar order of magnitude for the in ter-attribute corre-lations as w ell. The exact v alue of the constan t c will de-termine the w eigh tage giv en to the correlation comp onen t in the similarit y measure, though the exact analysis of the pro cess for nding a v alue of c whic h results in the most meaningful notion of similarit yis bey ond the scop e of this pap er. F or the purp ose of our exp erimen ts, w e found c =1 to b e appropriate. 6 The in v erted represen tation also allo ws for an ecien t cal-culation of correlation-based similarit y .F or eac h of the k pseudo-attributes, w e main tain the lists of strongly con-nected pseudo-attributes. Note that since there are a total the size of the list for eac hofthe d k d pseudo-attributes is c on the a v erage. W e shall refer to these lists as the adja-cency lists for eac h pseudo-attribute. Since the size of eac h suc h list is so small (t ypically less than 5, when c is c ho-sen to b e 1), these lists can b e main tained in main memory . 6 Since the exp ected v alue of P I D ist ( X; Y ) is of the same order as the size of the pro ximit y set, a v alue of c = 1 balances P I D ist ( ; ) and C I D ist ( ; ) appro ximately . Th us, the memory requiremen t for main taining these adja-cency lists is of the order of c d k d = 2. The new expanded index with these lists will b e denoted as IGrid + . The same hash table metho d is used in order to p erform the similar-it y computations in this case also except that w e also need to access all the in v erted lists for all the pseudo-attributes whic h are strongly connected to the pseudo-attributes in the target, and add one to the corresp onding hash table en tries for those records. Th us, for eac h pseudo-attribute, w e ac-cess the adjacency lists of this pseudo-attribute, and nd all the other pseudo-attributes whic h are strongly connected. Then, the in v erted lists of these strongly connected pseudo-attributes are accessed. This will result in a p erformance o v erhead factor of at most (1 + c ), since for eac h pseudo-attribute in the target, an additional c in v erted lists ma y need to b e explored on the a v erage. Th us, when c is c hosen to b e 1, the total p erformance o v erhead is only a factor of 2. Since w e kno w that the disk access p ercen tage is in v ersely prop ortional to dimensionalit y , it follo ws that for v ery high dimensional data suc h a constan t factor w ould alw a ys be o set b y the asymptotic scalabilit y b eha vior with increasing dimensionalit y . The pro cess of discretizing in to ranges has considerable edge e ects b ecause t w o adjacen t in terv als will con tain v alues whic h are v ery close to one another. These edge e ects can b e eliminated b y using a second lev el of discretization, where eac h of the k d discretized in terv als are further sub divided in to l equi-depth ranges. The in v erted list for eac h of the l ranges is main tained separately . Th us, in this case there will b e a total of k d l in v erted lists, eac h of whic h con tain N= ( k d l ) Record Iden ti ers. F or eac h target record, rst the most re ned ranges corresp onding to them are found, and then the d ( l 1) = 2 e in v erted lists on either side of eac h of these re ned ranges are explored. The resulting metho d ensures that for eac h attribute in the target, the pro ximit y threshold explored on either side is symmetric. This elim-inates the edge e ects asso ciated with discretization. The larger the v alue of l pic k ed, the less the edge e ects; from practical considerations w e found the use of l = 3 sucien t. The amoun t of data accessed b y the tec hnique is not af-fected b y this further lev el of discretization; the n um ber of lists explored is m ultiplied b y a factor of l , whereas the size of eac h of those lists is reduced b y the same factor. The primary fo cus of this pap er is for dev eloping an index structure whic h sho ws e ectiv e p erformance and qualita-tiv e b eha vior for similarit y searc h in high dimensional data. Ho w ev er, the applicabilit y of this tec hnique extends to range queries in lo w dimensional pro jections; a kind of query whic h b ecomes increasingly relev an t for high dimensional data. The traditional metho d of p erforming range queries sp eci es full dimensional ranges (a lo w er and upp er b ound for eac h dimension), and uses these in order to nd the b est matc h. As the dimensionalit y increases, it b ecomes increasingly un-lik ely that all ranges are relev an t in a query . F or example, one ma y wish to sp ecify only the ranges for a small n um ber of dimensions (sa y 3 or 4), and for the remaining dimen-sions, it is assumed that the en tire range of v alues ough tto b e considered. F or suc h queries, traditional indexes suc has the p yramid tec hnique are no longer applicable, since they lead to the access of all the data.
 The grid structure and in v erted represen tation of the data giv es an easy tec hnique for resolving suc h queries as w ell. This is b ecause for eac h pro jected range sp eci ed b y the user, one needs to examine only those lists whose ranges ha v e a non-zero in tersection with the user-sp eci ed range. F or a giv en dimension, the union of all the records in the corresp onding equi-depth attribute grid ranges are relev an t. Then the in tersection of the records for the di eren t pro-jected dimensions sp eci ed b y the user results in the relev an t sets of records.
 In order to consider the adv an tages of suc h a tec hnique, let us consider the case of 1000-dimensional data, in whic h the lev el of discretization k d has also b een c hosen to b e 1000. Let us consider an example in whic h a user pic ks 4 of the dimensions, and for eac h dimension, pic ks a range whic h results in the access of a fraction q = 0 : 1 of the in v erted lists for that dimension. Th us, as a result the total fraction of the in v erted index accessed will b e equal to 0.1*4/1000. This is only equal to 0 : 04% of the data. In fact, if w e assume that for most practical applications, users are only lik ely to pic k a small n um ber of constan t dimensions for pro jected range queries irresp ectiv e of the data dimensionalit y , then the p erformance of the range query will also be in v ersely prop ortional to dimensionalit y . More sp eci cally , it is easy of v erify using the argumen ts similar to those ab o v e that the fraction of the index accessed is equal to r q=d , where r is the (small) n um b er of pro jected dimensions sp eci ed b y the user, q is the aver age fractional sp eci cit yofeac h range (p ercen tage of in v erted lists accessed for that dimension), and d is the total n um b er of dimensions. In this section, w e will discuss the empirical results whic h sho w that the IGrid -index is a meaningful and p erformance ecien t tec hnique for p erforming similarit y searc hin v ery high dimensional data. In particular, since our tec hnique c hanges the criterion for measuring similarit y ,itis v ery im-p ortan t to pro vide some understanding of the qualit y of the nearest neigh bor found. This presen ts considerable c hal-lenges b ecause of the inheren t dicult y in measuring qualit y and meaningfulness directly .
 In order to measure the qualitativ e p erformance, w e used a tec hnique whic h w e refer to as the class stripping te ch-nique . W e obtained some of the data from the UCI mac hine learning 7 rep ository . The data w as rst cleaned in order to tak e care of missing v alues, categorical attributes and non-con tin uous v alues. These data sets corresp ond to classi ca-tion problems consisting of a set of feature v ariables and a sp ecial v ariable whic h is designated as the class v ariable. W e stripp ed o the class v ariables from the data set and found the =5 nearest neigh b ors to eac hof the records in the data set using di eren t similarit y metho ds. In eac h case, w e tested the n um b er of records whic h matc hed with the class v ariable of the target record. If a similarit y metho d is p o or in discriminatory p o w er, then it is lik ely to matc h unrelated random records and the class v ariable matc hing is also lik ely 7 h ttp://www.cs.uci.edu/ ~ mlearn Figure 1: Meaningfulness Beha vior with increasing dimensionalit y( L 2 -norm) to b e p o or. This kind of b eha vior w ould b e exhibited b ya random (see rst column of T able 1) distance function in whic h the distance b et w een t w o records is a uniformly dis-tributed random n um b er. Ho w ev er, when the nearest neigh-b or is more meaningful in the feature space, it is also lik ely to matc h the class v ariable closely . W e concede that these results are eviden tial in nature, since the exact relationship bet w een the feature v ariables and class v ariable is unkno wn for these data sets. Ho w ev er, a consisten t impro v emen ton the class v ariable accuracy b y using the new measure for lo calit y in the feature space do es tend to b e strong evidence for meaningfulness of the nearest neigh b or, since the nearest neigh bor w as found without an y information ab out the class v ariable. The ab o v e results w ere obtained using = 1 and c =1.
 As w e can see from T able 1, our similarit y measures signi -can tly increase the nearest neigh b or accuracy; the addition of correlation based measures impro v es the o v erall b eha v-ior ev en further. Since the traditional indexes are designed with resp ect to the Euclidean distance metric; this sho ws that IGrid index will p erform qualitativ ely at least as w ell as other indices whic h rely on traditional distance norms. the meaningfulness measure de ned in [12] v aries with in-creasing dimensionalit y . In order to do so, w e generated uniform random distributions of N = 100 p oin ts in increas-ing dimensionalit y , and tested b oth our similarit y function and the euclidean distance metric for meaningfulness. The 8 Note that in the euclidean distance metric, lo w er n um b ers imply greater similarit y , whereas in our metric higher n um-Figure 2: Meaningfulness Beha vior with increasing dimensionalit y (PIDist) Figure 3: P erformance Scalabilit y (Increasing Di-mensionalit y) Figure 4: P erformance Scalabilit y (Increasing Di-mensionalit y) Figure 5: Pro jected Range Query P erformance (w.r.t. Data Dimensionalit y) Figure 6: Pro jected Range Query P erformance (w.r.t. User-sp eci ed dimensionalit y) similar to that in tro duced in Section 1.2, except that w e use the a v erage distance v alue in the denominator for the sak e of robustness. This ratio con v erges to 0 with increasing di-mensionalit y for large classes of data distributions [12]. An example of suc h a metric is the Euclidean distance metric for whic h the meaningfulness ratio is illustrated in Figure 1. Ho w ev er, the P I D ist function retains its meaningful-ness (see Figure 2) with increasing dimensionalit y b ecause it in ten tionally ignores the distan t and noisy dimensions to the target while measuring similarit y . As predicted b y our theoretical analysis, the meaningfulness actually impro v es sligh tly with increasing dimensionalit y . This is also guaran-teed b y our c hoice of discretization parameter whic h results in the violation of Bey er's pre-condition for meaningfulness. As eviden t from the results on the real data, this actually impro v es the qualit y of similarit yb y measuring the n um ber and qualit y of similarit y of the limited n um b er of dimensions on whic h the t w o records are v ery similar, rather than letting the sparse and distan t dimensions dominate the similarit y function. When the dimensionalit y is high, this turns out to b e a more robust measure of similarit y than traditional distance norms. In the previous subsection w e discussed the qualitativ ebe-ha vior of the the nearest neigh b or in terms of meaningful-ness. In this section, w e will discuss the p erformance of the nearest neigh b or metho d in terms of the p ercen tage of data accessed. An imp ortan t observ ation is regarding the space o v erhead in v olv ed in the in v erted represen tation of the data. The in v erted represen tation of the data requires the storing of the ID v alues on eac h list, as w ell as the exact co ordinate for the corresp onding dimension in that ID v alue. Since eac h record o ccurs on exactly d of the d k d lists, it follo ws that the total space required to store the in v erted represen tation is N d ; exactly the same as the original database represen-tation. Ho w ev er, the storage of b oth the co ordinate v alue and Record Iden ti er for eac hen try in the in v erted repre-sen tation requires an additional 100% o v erhead. This o v er-head is signi can t from the p erformance p oin t of view, since greater storage in data represen tation translates to greater access cost. Consequen tly , in all subsequen t p erformance c harts, the p erformance access results ha v e b een presen ted in terms of the size of the original data. Th us, if the en tire in v erted index is accessed then the corresp onding p erfor-mance co ordinate on the c hart w ould b e 200%. One of the in teresting observ ations ab out the IGrid -index is that the exact access fraction of the index can b e predicted analyti-cally (as 2 = d d e of the original database size) irresp ectiv eof the nature of the underlying data distribution; in the case of the IGrid + , the a v erage case b eha vior is within a fac-tor 1 + c of the p erformance of the IGrid -index. The exact curv e for the IGrid -index and the a v erage-case curv e for the IGrid + -index are presen ted in Figures 3 and 4 for di eren t data dimensionalities for the case when = 1 and c =1. Since w e presen t the results for v ery high dimensional data whic hisbey ond the range of traditional indexing tec hniques suc h as the X-T ree, w e compare the results of our metho d with those of the V A-File for similarit y queries. The V A-File b ers imply greater similarit y . Ho w ev er, since our in ten tion is only to c hec k the relativ e b eha vior of meaningfulness, w e can use the same function for b oth. [6] w orks on the assumption that for v ery high dimensionali-ties, space partitioning metho ds are outp erformed b y simple sequen tial scan metho ds. Consequen tly , the V A-File com-presses the data to ab out 12 : 5 25% of the original size, and then uses the sequen tial scan on the compressed data in conjunction with some o v erhead required for the con ict resolution arising from the information lost in compression. The relativ e time for con ict resolution increases with di-mensionalit y for the metho d discussed in [6]; therefore this tec hnique is not free of the dimensionalit y curse. In our ex-p erimen ts, w e compare the IGrid -index against the V A-File lower b ound , whic h ignores the time required for con ict res-olution, and compares the least p ossible p ercen tage of data accessed b y the V A-File v ersus the IGrid -index. (Th us, in realit y , our results are lik ely to outp erform the V A-File b y an ev en greater margin.) As in the case of the qualitativ e comparison with the Euclidean distance metric, w e ev alu-ate b oth the indices IGrid and IGrid + . The index IGrid + requires an additional amoun t of disk I/O, since it also ac-cesses the in v erted lists corresp onding to the strongly cor-related pseudo-attributes. Ho w ev er, when the dimension-alit y is high enough, the IGrid -index alw a ys requires m uc h smaller amoun t of I/O than the V A-File b ecause of its in-v ersion with dimensionalit y . In fact, for all dimensionalities 30 or ab o v e, b oth the IGrid and IGrid + indices outp erform the V A-File. An imp ortan t p oin t to b e noted is that b ecause of the e ect of increasing dimensionalit y on the data repre-sen tation and discretization, all of the p erformance results are completely indep enden t of the nature of the underly-ing distribution and v ary as 1 =d . Details are illustrated in Figures 3 and 4. Our analysis in earlier sections indicates that the discretiza-tion parameter k d = d d e should be linearly increasing with dimensionalit y . Note that a c hoice of equal to 0 : 5 or 1 creates a distance function whic h is quite similar to the L p -norm for 1-dimensional and 2-dimensional problems, but is increasingly di eren t for high dimensional problems. These lo w dimensional cases (in whic h the L p -norm w orks w ell) pro vided us the reference p oin t for pic king = 1 in all exp erimen ts of this pap er. It is useful to see ho w stable the nature of the similarit y function w as to small c hanges in the v alue of this parameter. W ew ould lik etoha v e a similarit y function whic h do es not c hange to o dramatically with small c hanges in . The accuracy v alues of the IGrid -index for the m usk data set for di eren tv alues of are illustrated in T able 2.The accuracy v alues do not c hange to o dramatically for v alues of bet w een 0.25 and 1, though the accuracy v alue of the Euclidean function is signi can tly di eren t. On ex-amining the actual p oin ts whic hw ere returned as the near-est neigh b ors, w e found that the set of nearest neigh b ors returned for these di eren tv alues of w ere highly o v erlap-ping (80 90%), whereas the set of neigh b ors returned b y the Euclidean function w as v ery di eren t.
 It is imp ortan t to understand that an index whic h is built for a particular v alue of = 0 can automatically resolv e 9 queries for all v alues of = 0 = for an yv alue of &gt; 1. This is ac hiev ed b y accessing the d e closest ranges to the curren t range for eac h dimension while p erforming the sim-ilarit y calculations. This means that it is desirable to con-struct the index for larger v alues of , whic h automatically pro vides the user the capabilit yto in teractiv ely c hange the similarit y function in order to examine di eren t kinds of so-lutions. Suc hin teractiv e abilit yma y pro vide the k ey to high dimensional similarit y searc hes whic h are often heuristically de ned. W e also study the b eha vior of the tec hnique with resp ect to pro jected range queries. F or the purp ose of pro jected range queries, w e pic k ed n = 4 dimensions at random, and sp eci ed a b ounding rectangle whose side w as dra wn from an exp onen tial distribution with a v erage side of 10% of the total. The results for uniformly distributed data are illus-trated in Figure 5. The use of equi-depth ranges ensures that the curv e can b e generalized to arbitrary distributions when the b ounding rectangle is dra wn in suc ha w a y that the depth (fraction of p oin ts enclosed in that range) of eac h side of the b ounding rectangle is dra wn from the same exp o-nen tial distribution. As w e can see, the p ercen tage of data accessed rapidly decreases with dimensionalit y and is in fact in v ersely prop ortional to it. Other index structures cannot handle suc h partial queries at all in high dimensional space, since the en tire range is relev an t for the other dimensions. This results in all the data b eing accessed in space partition-ing metho ds. The impro v emen t in b eha vior of the pro jected range query with dimensionalit y is particularly pleasing in ligh t of the increased relev ance of suc h queries in v ery high dimensional data.
 W e also tested ho w the p erformance v aried with increasing n um ber of pro jected dimensions pic k ed b y the user. The a v erage range sp eci cit y for eac h side w as again c hosen to b e 10% and the total n um b er of dimensions d w as 1000. As, w e can see from Figure 6, ev en for a relativ ely large n um ber of dimensions sp eci ed from the range query , the sp eci cit y of retriev al con tin ued to b e v ery high. In this pap er, w e discussed the IGrid -index, a metho d whose p erformance impro v es with increasing dimensionalit y of the data. These results are in con tradiction with all the p erfor-mance results for other indexes in high dimensional space. The k ey here is to understand that most indexing structures and algorithms ha v e b een dev elop ed in the past based on particular distance norms suc h as the L p -norm, whic hha v e natural ph ysical in terpretations in lo w dimensional space, but are poor represen tations of similarit y in high dimen-sional space b ecause of the noise e ects of sparsit y . F or example, ev en though the euclidean distance metric has a natural ph ysical in terpretation for spatial databases in 2-or 3-dimensions, the results of [12] sho w its meaninglessness in 9 This is only true for IGrid but not for IGrid + . high dimensional space b ecause of p o or con trast in the dis-tances to the di eren t p oin ts. This is also the reason that the high dimensional index structures and algorithms can-not prune a w a y large subsets of p oin ts easily while searc hing for the nearest neigh b or; there is no discrimination to b e-gin with. In high dimensional data mining applications, the notion of similarit y itself is heuristical to b egin with; there-fore it mak es sense to c ho ose measures whic h lead to greater con trast b et w een the di eren t p oin ts. The understanding of meaningfulness of the nearest neigh b or is critical in dev elop-ing distance measures whic h use only a small fraction of the least noisy information a v ailable in high dimensional data in order to measure similarit y . The use of suc h a strategy in order to measure similarit y has a surprisingly pleasan t side e ect; in high dimensional space one has greater exibilit y in pic king the information whic h pro vides b etter statistical evidence of similarit y rather than noise; therefore b y pic king a higher qualit y threshold for de ning the pro ximit y set, one is able to con tin ue to obtain meaningful nearest neigh b ors, while impro ving the indexing p erformance. [1] C. C. Aggarw al, A. Hinneburg, D. A. Keim. On The Surprising Beha vior of Distance Metrics in High
Dimensional Space. IBM R ese ar ch R ep ort, R C 21739 , 2000. [2] C. C. Aggarw al et al. F ast Algorithms for Pro jected
Clustering. A CM SIGMOD Confer enc ePr o c e e dings , pages 61{72, 1999. [3] C. C. Aggarw al, P .S.Y u. Finding Generalized Pro jected Clusters in High Dimensional Spaces. A CM
SIGMOD Confer enc ePr o c e e dings , pages 70{81, 2000. [4] C. C. Aggarw al, J. L. W olf, P .S.Y u. A New Metho d F or Similarit y Indexing of Mark et Bask et Data. A CM
SIGMOD Confer enc ePr o c e e dings , pages 407{418, 1999. [5] S. Ary a. Nearest Neigh b or Searc hing and Applications. Ph. D. Thesis, University of Maryland, Col le ge Park ,
MD, 1995. [6] R. W eb er, H.-J. Sc hec k, S. Blott. A Quan titativ e Analysis and P erformance Study for Similarit y Searc h Metho ds in High Dimensional Spaces. VLDB Confer enc e
Pr o c e e dings , pages 194{205, 1998. [7] N. Bec kman, H.-P . Kriegel, R. Sc hneider, B. Seeger. The R*-T ree: An Ecien t and Robust Metho d for P oin ts and Rectangles. A CM SIGMOD Confer enc e
Pr o c e e dings , pages 322{331, 1990. [8] K. P . Bennett, U. F a yy ad, D. Geiger. Densit y-Based Indexing for Appro ximate Nearest Neigh b or Queries.
A CM SIGKDD Confer enc ePr o c e e dings , pages 233{243, 1999. [9] S. Berc h told, C. B X  ohm, H.-P . Kriegel. The Pyramid T ec hnique: T o w ards Breaking the Curse of
Dimensionalit y . A CM SIGMOD Confer enc ePr o c e e dings , pages 142{153, 1998. [10] B.-U. P agel, F. Korn, C. F aloutsos. De ating the Dimensionalit y Curse Using Multiple F ractal
Dimensions. ICDE Confer enc ePr o c e e dings , pages 589{598, 2000. [11] S. Berc h told, D. Keim, H.-P . Kriegel. The X-T ree: An Index Structure for High Dimensional Data. VLDB
Confer enc ePr o c e e dings , pages 28{39, 1996. [12] K. Bey er et al. When is Nearest Neigh b ors
Meaningful? ICDT Confer enc ePr o c e e dings , pages 217{235, 1999. [13] G. Das, H. Mannila, P . Ronk ainen. Similarit yof A ttributes b y External Prob es. KDD Confer enc e
Pr o c e e dings , pages 16{22, 1998. [14] S. Deerw ester et al. Indexing b y Laten t Seman tic Analysis. Journal of the A meric an So ciety for
Information Scienc e , 41(6): pages 391{407, 1990. [15] U. Shaft, J. Goldstein, K. Bey er. Nearest Neigh bor Query P erformance for Unstable Distributions. T e chnic al
R ep ort TR 1388, University of Wisc onsin at Madison , 1998. [16] V. Gan ti, J. Gehrk e, R. Ramakrishnan. CA CTUS-Clustering Categorical Data Using Summaries. A CM
SIGKDD Confer enc ePr o c e e dings , pages 73{83, 1999. [17] A. Gionis, P . Indyk, R. Mot w ani. Similarit y Searc hin High Dimensions via Hashing. VLDB Confer enc e
Pr o c e e dings , pages 518{529, 1999. [18] A. Guttman. R-T rees: A Dynamic Index Structure for Spatial Searc hing. A CM SIGMOD Confer enc e
Pr o c e e dings , pages 47{57, 1984. [19] A. Hinneburg, C. C. Aggarw al, D. A. Keim. What is the Nearest Neigh b or in High Dimensional Spaces?
VLDB Confer enc ePr o c e e dings , 2000. [20] R. Jain, D. A. White. Similarit y Indexing: Algorithms and P erformance. SPIE Stor age and R etrieval for Image and Vide o Datab ases IV , 2670: pages 62{75, 1996. [21] I. Kamel, C. F aloutsos. Hilb ert R-T ree: An impro v ed
R-T ree Using F ractals. VLDB Confer enc ePr o c e e dings , pages 500{509, 1994. [22] N. Kata y ama, S. Satoh. The SR-T ree: An Index Structure for High Dimensional Nearest Neigh bor
Queries. A CM SIGMOD Confer enc ePr o c e e dings , pages 369{380, 1997. [23] K.-I. Lin, H. V. Jagadish, C. F aloutsos. The TV-tree: An Index Structure for High Dimensional Data. VLDB
Journal , 3(4): pages 517{542, 1994. [24] N. Roussop oulos, S. Kelley , F. Vincen t. Nearest Neigh b or Queries. A CM SIGMOD Confer enc e
Pr o c e e dings , pages 71{79, 1995. [25] G. Salton, M. J. McGill. In tro duction to Mo dern
Information Retriev al. Mc Gr aw Hil l , New Y ork. [26] H. Samet. Design and Analysis of Spatial Data
Structures. A ddison Wesley , 1989. [27] T. Sellis, N. Roussop oulos, C. F aloutsos. The R+ T ree: A Dynamic Index for Multidimensional Ob jects.
VLDB Confer enc ePr o c e e dings , pages 507{518, 1987. [28] D. A. White, R. Jain. Similarit y Indexing with the
SS-T ree. ICDE Confer enc ePr o c e e dings , pages 516{523, 1996.
