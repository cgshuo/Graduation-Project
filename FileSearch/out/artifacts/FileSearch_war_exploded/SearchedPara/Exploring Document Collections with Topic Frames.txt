 Topics automatically derived by topic models are not always easy and clearly interpretable by humans. The most proba-ble top words of a topic may leave room for ambiguous in-terpretations, especially when the top words are exclusively nouns. We demonstrate how part-of-speech (POS) tagging and co-location analysis of terms can be used to derive lin-guistic frames that yield more interpretable topic represen-tations. The so-called topic frames are demonstrated as fea-ture of the TopicExplorer system that allows to explore doc-ument collections using topic models, visualizations and key word search. Demo versions of TopicExplorer are available at http://topicexplorer.informatik.uni-halle.de/ .
 H.4 [ Information Systems Applications ]: Miscellaneous topic model; visualization; topic frame
Topic models offer a way to explore large document col-lections by presenting topics that show typical contents. As topic models are constructed by unsupervised learning, they can be applied without the need to manually annotate doc-uments. During learning, a topic model assigns all word tokens in the documents to topics pursuing two conflicting goals: (i) topics should be assigned to as few different words as possible and (ii) a document should exhibit as few topics as possible [2]. Learning algorithms find trade-off solutions in this scenario that correspond to local optima of a free energy function in case of variational methods or to some probable global states in case of Gibbs samplers. However, all known algorithms cannot provide a guarantee on how well the derived topics are interpretable by humans.
State-of-the-art is to represent topics X  X hich mathemati-cally are distributions over words X  X s list of most probable top words. Interpreting such word lists may pose a diffi-cult task for humans. Success depends on both background knowledge and familiarity with vocabulary. There are two possible problems that hinder an interpretation. First, the word list may mostly be composed of nouns whose interre-lation may be ambiguous. Imagine a list of country names, even if these countries are only from a particular region, there are many different possible interpretations that would fit to such a list of names. Thus, such a list of topic words is not well interpretable. Second, the list of top words ap-pears to be incoherent. That might be due to words that are unfamiliar to the analyst.

An open question is how to represent a topic in a way that is more clear and digestible to humans. Recent research on topic coherence shows: topics are more interpretable by humans when pairs of top words often appear together in documents [8]. Thus, representing topics as pairs of top words that are frequently co-located in documents could ease human understanding. However, that approach does not solve the list-of-nouns problem. A key observation is that verbs are often less prominent in topic distributions because they are more flexible used in different contexts. Thus, they appear less often in top word lists of topics.

Combinations of a noun and a verb could be seen as basic units for transmitting semantic content. Minski called such units frames in the early days of artificial intelligence [1, 10]. Thus, detecting a noun with a verb in close proximity in a document is a necessary condition for detecting a frame. A topic frame may be present when a topic model assigns both the noun and the verb to the same topic. Thus, a more interpretable representation of a topic is to show a list of frequent topic frames. We demonstrate that topic frames are a step towards interpretable representations of topics.
We implemented topic frames in our TopicExplorer sys-tem. The demo includes corpora in different languages. A suitable corpus for demonstration is one that is widely known. Thus, the user can verify the validity of topics and topic frames based on common background knowledge. Therefore, we demonstrate the system on a subset of arti-cles of the English Wikipedia, and a collection of well known German fairy tales. Furthermore, we show a real applica-tion use case of TopicExplorer: supporting research in social sciences to access a collection of Japanese blogs discussing the Fukushima disaster of 2011 and social responsibility.
The detection of topic frames requires additional data preprocessing steps that are not part of typical analysis pipelines for topic modeling. We describe the necessary steps and how they fit into the overall architecture of Topic-Explorer. We used three corpora covering English, German and Japanese to demonstrate the flexibility of TopicExplorer to handle different languages. The English corpus is a sub-set of the longest 10.000 Wikipedia articles. The German corpus covers German fairy tales collected by the Grimm brothers. The Japanese corpus originates from a pilot study of a project to analyze blog about the Fukushima disaster. Due to space restrictions, only results on English Wikipedia are presented in this articles. Demo versions with the other corpora are available at the TopicExplorer web site.
This kind of data preparation includes tokenization of doc-uments, part-of-speech (POS) tagging, tag aggregation, to-ken filtering and lemmatization. Tokenization breaks a doc-ument given as character string into a sequence of strings called tokens. For text data with Latin alphabets, white spaces indicate word boundaries. We used this kind of tok-enizer for German and English language. For languages like Chinese and Japanese, more sophisticated tools are needed that predict word boundaries based on training data. We used MeCab ( http://mecab.googlecode.com ) that tokenizes, POS tags and lemmatizes Japanese text. We store the ex-act start position of each token with respect to the current document. This is viable to determine whether two tokens assigned to the same topic are close within a document.
POS tagging annotates tokens as noun, verb or other grammatical word classes. Currently available POS taggers assign more detailed word classes to tokens than necessary for our analysis. Therefore, word classes have to be aggre-gated to the broad and crude classes noun and verb. Tree-Tagger [11] for example, which is used for German and En-glish, distinguishes between 5 different labels for verbs (VV, VVD, VVG, VVN, VVP, VVZ) 1 that we all treat as verbs. Beside using POS tags for detecting topic frames after topic modeling they can be used for filtering tokens that code less important content. In our demonstration, we filtered out all non-nouns and non-verbs including auxiliary verbs like forms of be, do and have.

Topic models compare words as strings. Therefore, re-ducing tokens to word lemmas helps to match words of same meaning. In summary, the output of linguistic preprocessing is a token sequence of nouns and verbs for each document, where each tokens is annotated with the aggregated POS tag, the lemmatized word as word type and exact start po-sition of the original token in the respective document.
We build LDA topic models [4] of different sizes for all three corpora using MALLET [9]. It is important to ob-tain the topic assignments of all tokens in all documents. MALLET has an option to output this kind of information. For efficient processing, it is nice to have the output of the topic assignments in the same order as the input sequence of the word tokens. This allows to efficiently match word tokens with topic assignments using a kind of merge join algorithm. Thus, the result after merging the output of topic modeling with the data from linguistic preprocesing
Description of labels: http://www.cis.uni-muenchen.de/ ~schmid/tools/TreeTagger/data/stts_guide.pdf is a large table with columns DocumentId , Token , Start-Position , WordType , POS and TopicId . From this table, we compute the probabilities P (word type | topic), P (topic | word type), P (document | topic) and P (topic | document).
These probabilities are used for different rankings in the web-based TopicExplorer user interface. The probabilities P (word type | topic) are used to compute top words for each topic. Further, those probabilities are used to form a vec-tor for each topic. We compute the full cosine similarity matrix between all topic vectors and use that matrix as in-put for hierarchical clustering. The hierarchical clustering in R outputs along with the clustering a serial ordering of the topics that are leaves in the clustering dendrogram. We use this ordering to assign colors to topics and arrange them horizontally at the bottom of the TopicExplorer user inter-face. Thus, similar topics are represented by similar color and closely located in horizontal ordering. Therefore, slowly moving the horizontal slider and reading the top word lists of the topics gives an uninterrupted impression of the contents in the document collection.

When clicking on the link above such a top word list a new tab is opened that shows a ranked list of documents. Doc-uments are ranked according to P (document | topic). In the screenshot, the most right list of top word with red back-ground indicates a topic about sports. This can be con-firmed by looking at the titles of the top ranking documents for that topics, which are shown above. For each docu-ment, a title and the first few lines are shown. Further, the four most important topics for that documents according to P (topic | document) are indicated as colored circles. When typing words into the keyword search field, autocompletion kicks in and suggests words together with respective impor-tant topic ranked according to P (topic | word type). An ear-lier version of TopicExplorer having this features had been demonstrated elsewhere [7].
The merged data after topic modeling X  X ontaining token positions, POS tags and topic assignments of tokens X  X llow to compute topic frames. A list of k top nouns and k top verb is computed for each topic with respect to the prob-abilities P (word type | topic). A co-location of two tokens that (i) are closer than a given threshold t in a document, (ii) have matching word types with a top noun and a top verb respectively, (iii) and are both assigned to the respec-tive topic constitute the occurrence of a topic frame. All occurrences of all possible topic frames are computed for each topic. For each topic frame, the number of occurrences and the number of documents containing such an occurrence are counted. For a topic, a list of topic frames in decreasing order of number of documents with the respective frame is an alternative representation of a topic.

Consider topics as shown in Figure 1. The topics are not clearly interpretable, especially the right topic starting with India shows just a list of country names. The topic could represent tourism or geography. The same is true for the middle topic that is something about America. Switching the topic representation to topic frames by clicking on the small icon in the upper left corner of the topic view in the bottom panel, the true meaning of the topics become more clear, Figure 2. The right topic starting with India is about foreign politics and the middle topic is about history of the United States. This demonstrates that topic frames consist-Figure 1: Topics with top word list that are not clearly interpretable.
 Figure 2: Topics represented by topic frames be-come more clearly interpretable. ing of noun verb combinations are effective to more clearly represent the content of a topic.

The meaning of a topic can be confirmed by looking at the title of top ranking documents for a topic. For the Amer-ica topic, the following Wikipedia articles are highly ranked: History of Missouri, African-American history, Abraham Lin-coln, Ulysses S. Grant, Seminole Wars. It is also possible to click on a particular topic frame, e.g. (American, settle). That frame appears in 20 documents that are listed in a new tab. Choosing one of the listed documents, e.g. Oregon trail opens another tab that display the particular docu-ment. Topic frames as well as key word assignments could be marked in the text, Figure 3.
Related software projects include the Topic Model Visual-ization Engine[5] ( http://code.google.com/p/tmve/ ) that transforms a topic model into a precomputed set of web pages. It shows important words for each topic as well as related documents and related topics. The Topical Guide[6] ( https://facwiki.cs.byu.edu/nlp/index.php/Topical_Guide ) shows similar information for each topic. Additionally, the top words for each topic could be presented with context in-formation. It also allows to filter by topics, different metrics, words and documents and can produce parallel coordinates plots that relate topics with other meta-data. Also simi-lar to our frame approach, n-grams related to a topic have been used to enhance the common top word visualization [3]. Another related project is the topic-based search inter-face Search-In-a-Box by Buntine ( http://cosco.hiit.fi/ search/sib.html ). It allows to search documents by key-words and filters optionally afterwards by topical text. The result list shows the important topics of the documents. Ad-ditionally to the search engine functionality, it offers a topic browser that shows important words and documents for each topic.
 We thank Mattes Angelus, Benjamin Schandera and Gert B  X  ohmer for their programming contributions to the code Figure 3: Topic frame (American, settle) in Wikipedia article Oregon trail. base of TopicExplorer. Further, we thank the Klaus Tschira Foundation for the support of the project. [1] K. Allan. Natural language semantics. 2001. [2] D. Blei. Topic modeling and digital humanities. [3] D. M. Blei and J. D. Lafferty. Visualizing topics with [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] A. Chaney and D. Blei. Visualizing topic models. In [6] M. J Gardner, J. Lutes, J. Lund, J. Hansen, D. [7] A. Hinneburg, R. Preiss, and R. Schr  X  oder.
 [8] J. H. Lau, D. Newman, and T. Baldwin. Machine [9] A. K. McCallum. Mallet: A machine learning for [10] M. Minsky. Frame-system theory. In P. N.
 application to german. In In Proc. of the ACL SIGDAT-Workshop , pages 47 X 50, 1995.
