 ORIGINAL PAPER Kui Hu  X  Zhi Tang  X  Liangcai Gao  X  Yadong Mu Abstract Standard JBIG2 algorithms for textual image compression focus on the features of alphabetic characters such as English, not considering the features of pictograph characters such as Chinese. In this work, an improved algorithm called MC-JBIG2 is developed, which aims at improving compression ratio for Chinese textual images. In the proposed method, first multiple features are extracted from the characters in the images. After that, a cascade of clusters is introduced to accomplish the pattern-matching task for the characters. Finally, to optimize the parameters used in the cascade of clusters, a Monte Carlo strategy is implemented to traverse the feasible space. Experimental results show MC-JBIG2 outperforms existing representative JBIG2 algorithms and systems on Chinese textual images. MC-JBIG2 can also improve compression ratio on Latin textual images, however, the improvement on Latin textual images is not as stable as the improvement on Chinese ones. Keywords Textual image compression  X  JBIG2  X  Pattern matching  X  Clustering  X  Monte Carlo method 1 Introduction Textual images can be regarded as binary document images that mainly consist of semantically meaningful charac-ters [ 1 , 2 ]. From the view of data compression, there are both bit-level and symbol-level redundancies in textual images. The latter is caused by the repetition of the identical sym-bols throughout the whole image [ 3 , 4 ]. Existing compres-sion standards for textual image include G3, G4, JBIG and JBIG2 [ 5 ]. Among these techniques, G3, G4 and JBIG are primarily based on bit-level compression, while JBIG2 fur-ther incorporates symbol-level compression by utilizing PM (pattern matching) techniques [ 6 ]. Besides these standards, DjVu that is based on MRC (Mixed Raster Content) model [ 7 ] can also be used to compress textual images. The basic idea behind DjVu is to separate images into foreground, back-ground and bi-level mask and to use different techniques to compress each of those components [ 8 , 9 ]. As for the bi-level mask, DjVu adopts a method called JB2 (a variation of JBIG2) to compress.

As stated above, JBIG2 involves two-level compres-sions, addressing the bit-level and symbol-level redundan-cies, respectively. First, for the symbol-level compression, the crucial step is to perform a pattern-matching procedure to construct template dictionaries. Symbols that are similar to each other under specific pre-defined metric tend to belong to identical template class i.e., cluster. With the obtained clus-ters, each symbol can be compactly represented by its cluster index and spatial position coordinates together. Then, for the bit-level compression, generally an arithmetic coding is per-formed to compress all stuffs, including the global dictionar-ies, indices and positions of each symbol.

In the symbol pattern-matching stage, seeking a good metric for measuring the similarity between a pair of symbols is a challenging task. Generally, two symbols are regarded as similar primarily based on the visual resemblance in the eyes of human viewers. However, the visual resemblance is not always consistent with the similarity scores calculated from bitmap difference of symbols. In the existence of noise, dith-ering or distortion, many visually similar symbols are nota-bly different in bit-level. The situation is more serious for Chinese textual images since Chinese character set is much larger than Latin one, and many Chinese characters have complex structure, and some characters are similar in profile to several other ones.

Due to the inconsistencies between the visual resemblance and bitmap similarity of symbols, existing pattern-match-ing algorithms used in JBIG2 face a dilemma in matching strategy [ 5 ]; with a strict matching condition, many visu-ally similar symbols will be classified to different template classes, thus deteriorating the compression performance; in contrast, to lower the matching, criterion tends to cause the substitution errors.
 In this work, we propose an improved algorithm called MC-JBIG2 to compress textual images with large symbol dictionaries, e.g., the Chinese textual images. The proposed MC-JBIG2 includes three key ingredients. First, based on the characteristics of Chinese textual images, we bring out the idea of extracting multiple features from the characters in the image. Second, we introduce a cascade of clusters to accomplish the PM procedure for the characters. Third, we implement a Monte Carlo strategy to optimize the parame-ters used in the cascade of clusters. We will describe these procedures in detail in subsequent sections.

MC-JBIG2 is compatible to JBIG2 standard and can be used to encode all bi-level images. Experimental results show MC-JBIG2 outperforms existing representative JBIG2 algorithms and systems on Chinese textual images. MC-JBIG2 can also improve compression performance on Latin textual images, however, the improvement on Latin textual images is not as stable as the improvement on Chinese ones.
The rest of this paper is organized as follows. In Sect. 2 , we review the literature in JBIG2 and discuss the motivation and the main idea of MC-JBIG2. In Sect. 3 , we introduce the framework of MC-JBIG2 and a cascade of clusters for pattern matching. In Sect. 4 , we elaborate on feature extraction from Chinese characters. In Sect. 5 , a Monte Carlo method to opti-mize the parameters in MC-JBIG2 is presented. Experimen-tal results are shown and discussed in Sect. 6 . Conclusion and future directions are given in Sect. 7 . 2 Related work JBIG2 is an international standard proposed by Joint Bi-level Image Expert Group (JBIG) for bi-level image compres-sion [ 6 ]. JBIG2 only defines the decoding specification and does not specify an encoding algorithm, leaving the encoder open to developers. In other words, researchers are encour-aged to develop concrete encoding algorithms for bi-level images.

JBIG2 techniques were initially developed by Howard et al. [ 10 ] who designed PM and S (Pattern Matching and Substitution) and SPM (Soft Pattern Matching) algorithms for lossy and lossless compression of bi-level images, respec-tively. PM and S achieves compression by substituting the matched symbols, and SPM encodes the new symbol using the matched entry in the dictionary as reference. Howard X  X  work was later accepted by ISO and ITU as a proposal in the JBIG2 standard draft.

Figure 1 demonstrates the work-flow of a typical JBIG2-based encoding system. It includes three basic components: symbol segmentation, pattern matching and entropy coding of bitmaps and integers. First of all, for symbol segmentation, a simple flood fill algorithm can be used [ 11 ]. To speedup this procedure, the encoder can use a run-based region fill algorithm to avoid visiting the same pixel repeatedly [ 5 , 12 ]. After that, the PM procedure will construct the symbol dic-tionary, meanwhile the page layout information will be kept. This procedure accomplishes symbol-level compression. At last, entropy coding can achieve further compression on the binary and integer data. In this part, generally an arith-metic coding can be performed.

Symbol segmentation is a crucial step in PM-based encod-ing system, though itself is not the main task of JBIG2 stan-dard. In some textual images such as Indian language [ 13 ], Farsi/Arabic script [ 14 ], Ottoman Turkish [ 15 ] and even Latin [ 1 ], there may exist undesired touching characters. On the one hand, these touching characters usually decrease redundancy in symbol level, which reduces the potential for the following PM procedure; on the other hand, when attaching some tiny strokes to the character, trunks may increase compression performance. For printed Chinese textual images, most Chinese characters are square and untouching in print-page, and Chinese character segmen-tation techniques are quite mature in the realm of charac-ter recognition [ 16 ]. So, in this work, we will focus on the pattern-matching procedure.

Ye and Cosman X  X  work on JBIG2 focused on symbol dictionary design, dynamic dictionary update, speedup tech-niques for pattern matching and quality control of recon-structed image in lossy compression etc [ 17  X  19 ]. In their work, various character features such as size, quadrant centroid distance and topological structures are extracted. However, the features they extracted are limited and are mainly for English characters. Their research was commer-cialized through technology transfer office of UCSD, which was later purchased by Apago Inc. and applied in the com-mercial products.

Yan et al. introduced morphological analysis in pattern matching for Chinese document images compression [ 20 ]. They proposed the concept of content-lossless compression, which indicates no substitution error, is permitted during the pattern-matching procedure. In this way, the reconstructed images are supposed to keep high readability in lossy com-pression mode. However, their research was not based on JBIG2 standard and the compression ratio failed to reach the design level of JBIG2.

Shang et al. [ 21 ] integrated JBIG2 with character recog-nition techniques. They constructed character image blocks and replaced similar ones with representative symbol images by utilizing the recognition results of characters and the con-fidence of these results. Their method was able to improve both lossy and lossless compression ratio in both Latin and Chinese textual images. For lossy compression, the algorithm led to much fewer substitution errors.

As an open compression standard, JBIG2 is compatible to all bi-level images, but its compression performance is content-dependent. So it is meaningful to develop algorithms for distinct textual images. Some researchers have developed compression algorithms for specific textual images, such as Indian language [ 13 ], Farsi/Arabic script [ 14 ] and Ottoman Turkish [ 15 ].

The motivation of our work is to improve JBIG2 compres-sion performance for Chinese textual images, and the main idea is to extract multilevel feature data from characters in the images and to accomplish the pattern-matching task for the characters effectively with the data. The extracted multilevel feature data offer us foundation to involve efficient match-ing and optimization algorithms in MC-JBIG2. In our work, as multilevel feature data are extracted, we can achieve further compression on the textual images. 3 Framework of MC-JBIG2 We name our algorithm as MC-JBIG2, where the MC has two indications: first, as we extract multilevel features and use a cascade of clusters to replace the pattern-matching pro-cedure of JBIG2, here MC is the abbreviation of Multilevel and Cascade; second, as a Monte Carlo method is used to optimize the parameters used in the cascade of clusters, here MC is the abbreviation of Monte Carlo. The above explana-tion also expresses the fundamental thoughts and principles of MC-JBIG2. 3.1 Characteristics of Chinese textual images As an international standard for bi-level images, JBIG2 focuses on features of alphabetic characters such as English, not considering the features of pictograph characters such as Chinese. Due to the characteristics of Chinese textual images, the compression performance of JBIG2 on Chinese textual images is typically inferior to that on English ones. Figure 2 is a typical Chinese textual image portion. From data com-pression perspective, we analyze and summarize the charac-teristics of Chinese textual images as follows: 1. Chinese character set is very large. Unlike the ASCII 2. Unlike alphabetic characters, many Chinese characters 3. A number of characters are similar in profile, such as 4. Last, as discussed in last section, most Chinese charac-
Based on above characteristics, we focus our work on the PM task of Chinese textual image compression. Though the proposed MC-JBIG2 is initially designed for Chinese textual images, we will also test its performance on English and other Latin textual images. Results will be shown in the experimental section. 3.2 MC-JBIG2 overview MC-JBIG2 consists of the following key ingredients: 1. Multiple feature extraction, which is crucial to the 2. Cascade of clusters, which is designed for pattern 3. Monte Carlo optimization, which aims to optimize the The framework of MC-JBIG2 algorithm is shown in Fig. 3 . In this section, we will introduce the details of the cascade of clusters. The feature extraction and Monte Carlo optimization parts will be introduced in later sections. 3.3 Cascade of clusters The PM procedure of JBIG2 can be regarded as a multi-class classification problem, so it is nature to involve appropriate machine learning algorithm to achieve the task. However, it is challenging to take existing machine learning algorithms to accomplish the job. The most common reason is that the class number in the system is too large. In real textual images, due to the factors of character size/font, noise, dithering or distortion, there are probably thousands of visually dif-ferent symbols that should be assigned to distinct classes. The problem is more critical for Chinese textual images as Chinese character set is much larger than Latin one.
Another issue comes from the precision aspect of classifi-cation. On the one hand, as substitution errors are not accept-able in a lossy compression system to keep content-lossless, the classifier X  X  false positive should be near zero; on the other hand, as the result of classification impacts the compression performance directly, the classifier X  X  false negative should be as low as possible.

Viola and Jones [ 22 ] developed a boosted cascade of clas-sifiers for visual object detection, which is capable of process-ing images extremely rapidly and achieving high detection rates. Motivated by Viola X  X ones algorithm, we first degrade the PM task to a clustering problem and then design a cas-cade of clusters to accomplish the work. The cascade of clus-ters can be regarded as a simplified variant of Viola-Jones algorithm without learning by Adaboost.

Each hierarchy of the cascade of clusters is imple-mented based on the minimal distance (threshold) clustering method [ 23 ] in data mining, as seen in Algorithm 1 .
Algorithm 1 : The minimal distance clustering algorithm
The minimal distance clustering is simple and highly effi-cient for data clustering. We extend it in the following aspects to implement the cascade of clusters: 1. Extend the operation of the above step (5) to a cascaded 2. Each hierarchy executes a decision operation on a pair 3. The decision operation of each hierarchy is relatively 4. The data in each hierarchy can be of different type: 5. Only the object getting positive result can be sent to next 6. Last, only when the object passes all hierarchies, it can
Flow chart of the cascade of clusters is shown in Fig. 4 . 3.4 Analysis of MC-JBIG2 Now let us analyze MC-JBIG2 in terms of accuracy and effi-ciency aspects.

First, the clustering of symbols in MC-JBIG2 is more accurate than the pattern matching in standard JBIG2. Tra-ditional pattern-matching algorithms utilize only limited information of symbol bitmaps, which deteriorates the per-formance. In MC-JBIG2, as multilevel feature data are extracted, we can design more effective and accurate match-ing strategies for symbols. In the cascade of clusters, each hierarchy is implemented with a subset of the feature data, which render certain characteristic of the symbols. The sym-bols that are sensitive on present data will be divided and only part of the symbols will pass the current hierarchy. Thus, when the symbols pass these hierarchies step by step the dif-ferent symbols will be divided gradually. This procedure is efficient in clustering symbols. We will demonstrate this in experimental section by observing the intermediate results of MC-JBIG2.
 Second, the algorithm is highly efficient in computation. First of all, denote the total number of input data objects as n , the number of object classes as m , and the number of hierarchies in the cascade of clusters structure as k , then the time complexity of the algorithm is O ( kmn ) . As a linear function of n , the computation is highly efficient. Besides that, the cascade of clusters also implement a early jump-out strategy: the hierarchies in the early stages of the cascaded structure generally take simple but effective rules to parti-tion the apparently different symbols, whereas at the end of the cascaded structure the hierarchies tend to take more complicated operation to distinguish the symbols that are close in profile. Thus, most symbols are abandoned in early stages and only a part of symbols go through all hierarchies, so by using cascaded structure the computation efficiency is improved.

Last, the algorithm is flexible and scalable. For example, every hierarchy of the cascaded structure is relatively independent, which indicates that we can focus on the local implementation of the algorithm to increase the matching efficiency of current hierarchy. In real system, by adding new hierarchy, optimizing or improving present hierarchy, or even changing the order of existing hierarchies may promote the compression result or performance. 4 Chinese character feature extraction For Chinese character feature extraction, plenty of work has been done by researchers in the field of character recogni-tion [ 16 ]. In our work, we totally extract eight types of fea-tures for Chinese characters, which are used in the cascade of clusters of MC-JBIG2.

The adopted features and related parameter (threshold) choice in the cascade of clusters are as follows:
Level 1: character size. This is the simplest feature of character in our system. We keep the width w and height h of characters and set the threshold T 1 = 2, which means if the difference of w or h exceeds 2 pixels the symbols will be assigned to different template classes. To avoid resolution sensitivity, we can also set T 1 be proportional to the charac-ter size.

Level 2: character density. The density is another basic feature, which is defined as the ratio of the numbers of black pixels to all pixels within the bounding box. Denote the threshold as T 2 . The threshold is only set an appropriate range and the ultimate value will be calculated via the Monte Carlo optimization in the next section. This principle will be adopted in following levels.

Level 3: character feature-points. As for the symbol bounding box, we first partition it evenly to 8  X  then at each grid point we take a 2  X  2 pixel-block as feature-point . Totally, we take 64 feature-points from the characters. Figure 5 shows the 8  X  8 grids and one of the 2  X  2 pixel-block. The threshold T 3 is determined according to the following rule: if one of the pixel-block is all white and the other pixel-block contains over 3 black pixels, then the two symbols are assigned to different template classes.

Level 4: character centroid. Centroid of black pixels shows the distribution trend of these pixels in four directions of top, bottom, left and right. Figure 6 shows the centroid point of a printed character. For centroid, we consider the offsets in horizon, vertical and their summation and denote the corre-sponding thresholds as T 4 . 1 , T 4 . 2 and T 4 . 3 . These values will also be evaluated via optimization.

Level 5: character four-edge coding. We take four-edge stripes, which are set a fixed width from the outer contour of symbol bounding box, and define the black pixel density of the edge stripes as four-edge coding of the character. Figure 7 shows the left edge stripe taken from a printed character. The threshold for each edge stripe is set T 5 . 1 , and the thresholds for summations of two, three and four-edge stripes are set T coding is on these four levels.

Level 6: character cross-edge coding. Similar to the above four-edge coding, we also take two-edge stripes horizontally and vertically from the middle of the symbol bounding box. Figure 8 shows the vertical edge stripes taken from a printed character. The thresholds for each edge stripe and summa-tions of two-edge stripes are T 6 . 1 and T 6 . 2 .
Level 7: character contour coding. Suppose we irradiate the characters with light from four directions of top, bottom, left and right. When the light reaches a black pixel for the first time, the region it has passed is defined as once-region . Then, after the light penetrates the black pixel(s) and reaches another black pixel the second time, accordingly we define the region it has passed as the twice-region .Asshownin Fig. 9 , the light irradiates from left to right. In the figure, the real line part is once-region, and the dashed line part is twice-region. The area ratio of once-region to the total bounding box is defined as once contour coding, it reflects the outer contour features of the character; the area ratio of twice-region to the total bounding box is defined as twice contour coding, it reflects the inner contour features of the charac-ter. Totally, from 4 directions we can get 8 contour coding. We set threshold T 7 . 1 for once contour coding, set threshold T 2 for twice contour coding, set threshold T 7 . 3 for summa-tion of once and twice contour coding and set threshold T for summation of all 8 contour coding of the character.
Level 8: character entropy. We divide the character into some square pixel-blocks. To adapt the size of the character, the edge length of each square is set as the square root of the length of shorter edge of the character X  X  bounding box. Sup-pose we divide the character into i squares, the black pixel density of each square is e 1 , e 2 ,..., e i , the black pixel den-sity of the whole character is e , and we define E = ((( e e +  X  X  X  + e 2 then E reflects the inequality degree of the character. We set threshold T 8 to compare characters on their entropies. Table 1 lists these character features we extract for MC-JBIG2 and their corresponding properties. Most of the above thresholds are not set fixed values. In MC-JBIG2, the values will be set an appropriate range and be optimized with the Monte Carlo method. We will introduce this in the next section. 5 Monte Carlo optimization An important task of MC-JBIG2 is to set the appropriate threshold values that are used in the cascade of clusters. This task is quite difficult as the values of these thresholds may affect each other. A candidate solution is to set these values artificially, observe the output results, to adjust the values according to the results continually. Then, from the results of different executions, we can manually choose the rela-tively best threshold values for the algorithm. However, this procedure is computationally expensive, and it is hard to get the optimal values. In this section, we will implement a Monte Carlo strategy to solve the optimization problem for the thresholds.
 5.1 Data labeling Before involving appropriate algorithm to resolve the opti-mization problem, data labeling became a prerequisite task. For computer machine, the so called symbols just means a set of pixel-blocks, and computer does not have any under-standing on them. In fact, elements such as figures, logos and even stains are all extracted as symbols from the images. So we should label these elements artificially, set them with the character-level values and then computer can  X  X dentify X  these elements.
 For involving solution to optimize the parameters in MC-JBIG2, we construct a labeled data set. In our work, we totally select 150 page textual images. Most of the images are in Chinese, and some images contain a few English charac-ters. We will introduce these textual images with more details in the experimental section of this paper. As for the extracted symbols from the images, we label all Chinese characters, English letters and Arabic numerals with their character-level values. The labeled images will be used as the training and test data set in a Monte Carlo method to get the optimized parameters. 5.2 Monte Carlo method Monte Carlo method is a class of computational algorithms that rely on repeated random sampling to compute the results [ 24 , 25 ]. The basic idea of Monte Carlo method is to traverse the feasible space to approach approximate optimal solution. Monte Carlo method is useful when it is unfeasible or impossible to compute an exact result with a deterministic algorithm. The computation process of Monte Carlo method is independent of problem X  X  dimension, and the program does not fall in local optimal solutions.

There is no single Monte Carlo method; instead, the term describes a large and widely used class of approaches. For numerical optimization problems, it generally follows such pattern or procedures: 1. Define a domain of possible inputs; 2. Generate inputs randomly from certain probability dis-3. Perform a deterministic computation using the inputs; 4. Repeat the above steps a finite times and choose the best 5.3 Implementation of Monte Carlo optimization In this subsection, we elaborate on the Monte Carlo method used for parameter optimization. First, all parameters (i.e., threshold values for multiple features) form a multi-dimensional vector. We assume each parameter lies in a bounded feasible region and is subject to the uniform distribution. It is possible to draw samples from these latent distributions by standard pseudo-random number functions.
Another issue is the measure function for the input. The target of MC-JBIG2 is to improve the efficiency of pattern-matching procedure. A good pattern-matching algorithm should pass the visually identical symbols as many as pos-sible and prohibit the visually different symbols to avoid substitution errors. So we set the measure function as this: minimize the number of classes generated in the cascade of clusters under the condition of not involving substitution errors.

In Table 1 , there are 15 thresholds that are not set fixed values, namely there are 15 parameters to be optimized. By setting them with the values generated from the above probability distribution, symbol classes can be constructed in the cascade of clusters. As the input data set has been labeled, the substitution errors can be caught when a pair of symbols with different labeled values are clustered into a distinct class.

In Monte Carlo method, the procedures will be repeated sufficiently many times, which is especially time-consuming. To speedup, it is better to early detect the operations that are impossible to outperform current best one. We adopt the following method: 1. First, we define an operation on vector U and V .Let U 2. Then, let the present best vector be V b . If candidate vec-3. Last in the program, we record vector V t , which is Figure 10 shows the flow chart of the Monte Carlo method. Besides the fundamental procedures of Monte Carlo method, the speedup scheme for computation is also included.
In this work, we totally label 150 page textual images, which are randomly split into training and test set. By exe-cuting the Monte Carlo procedures on the training set for 20,000 times, we select the best result as the threshold values for MC-JBIG2. Once the optimal threshold values are obtained, they will be used for the final textual images compression. 6 Experimental results In this section, we will present compression results of MC-JBIG2 on several data sets. Comparison with the exist-ing representative JBIG2 algorithms are given, under the con-tent-lossless experimental setting. 1 6.1 Data set for experiment At present, few data sets are built for Chinese textual image compression. To validate the effectiveness of MC-JBIG2, we collect a variety of textual images, which are mainly Chinese textual images. The data set we construct consists of the following parts: (1) the CMU-CDI (CMU Chinese Doc-ument Images) data set, which is originally used for Chinese character recognition, from Imaging System Lab of Carnegie Mellon University. 2 The CMU-CDI data set consists of 68-page scanned documents that contain 37,021 characters, and the documents are variant in terms of captured time and sources. (2) The other data set for textual image compres-sion is from a scanned book, built by the Institute of Com-puter Science and Technology at Peking University. We name the data set as PKU-SEB (PKU Scanned Electronic Book). It consists 82 pages and about 60,000 characters. (3) Besides CMU-CDI and PKU-SEB, we also select 10 scanned books of different types, which are named from Book-01 to Book-10. The image resolutions of above data set are all 300 dpi. Table 2 summarizes the above 12 data sets.
Although MC-JBIG2 is designed mainly for Chinese textual images, we also select some English and Spanish textual images (Book-08, Book-09 and Book-10) to test its performance on Latin textual images. 6.2 Evaluation criteria We choose three representative JBIG2 software for compari-son, i.e., (1) Open source software jbig2enc is a high perfor-mance JBIG2 algorithm that offers full source code. We name this software as Enc-JBIG2 hereafter. (2) As mentioned in Sect. 2 , Ye X  X  work was purchased by Apago Inc. and applied in the commercial products of the company. We can get the corresponding JBIG2 module from the products and name it as Apago-JBIG2. (3) Leadtools is an outstanding company in field of image processing and compression. We can also get the JBIG2 function from Leadtools products and name it as Lead-JBIG2.

As for the comparison criteria, we consider the following aspects. First, we keep the condition of content lossless. This means the compression is in lossy mode, while no substitu-tion errors are permitted and the reconstructed images can keep high readability. Then, we will look into the intermedi-ate results of the Monte Carlo optimization and the cascade of clusters. This will help us to understand the principles and procedures of MC-JBIG2. Last, we will compare the com-pressed file sizes of MC-JBIG2 with other JBIG2 algorithms. This is the most important criterion for evaluation.
Besides these aspects, execution time is another important factor. Because the above JBIG2 algorithms and software are all highly efficient in execution and can satisfy the require-ment of real application, we will not choose execution time as key comparison criterion. We will just discuss this aspect summarily. 6.3 Results and discussions First, let us show the results of Monte Carlo optimiza-tion for parameters. As shown in Table 1 , there are totally 15 thresholds that do not take fixed values. The threshold values will be regarded as parameters, which are 15-dimen-sion vectors. The parameters are set appropriate value range and are optimized with the Monte Carlo method. As intro-duced in Sect. 5 , for executing the Monte Carlo function, we need the labeled data set. In our work, we label data set of CMU-CDI and PKU-SEB, which totally include 150 page textual images. We choose 20 page images from each of the data set and take the 40 page images as training set for Monte Carlo function. The rest 110 page images in the labeled data set will be treated as test set. We execute the Monte Carlo function on the training set 20,000 times and record the optimized results in Table 3 .

The measure function of the Monte Carlo program is to let the number of clustered classes be minimal in the con-dition of not involving substitution errors. We can observe the descending trend of the class numbers visually in Fig. 11 . The numbers of clustered classes became stable with updates of Monte Carlo program.

The Monte Carlo program executes for more than a dozen hours on a normal PC, after that we get the optimized param-eters listed in the last row of Table 3 . We validate the param-eters on the test data set and have not found substitution errors on the set. So we can confidently choose the optimized parameters as threshold values for MC-JBIG2.

Then, after getting the optimized parameters, let us look into intermediate results of the cascade of clusters. We take 6 page images from CMU-CDI and PKU-SEB, respectively. We record original numbers of the character in the images and the numbers of the characters that pass each hierarchy of the cascade of clusters. As shown in Table 4 , column 0 lists the original character numbers and column j ( j = 1 , 2 ,..., records the numbers of characters that pass j hierarchies.
The above pass means the matched prototype can be found in the template dictionary for the character, so it can contrib-ute for compression by character substitution. In Table 4 , the numbers of passed characters decrease gradually along the hierarchies of the cascaded structure, this is because each hierarchy divides some visually different characters by specific feature data. As multilevel feature data are extracted, the cascade of clusters can separate the different characters gradually.

In Fig. 12 , we extract an image portion from the data set and record its intermediate results after every hierarchy of the cascade of clusters. We can see the contents become more accurate step by step.

After the pattern-matching procedure of JBIG2 or above cascade of clusters of MC-JBIG2, a template dictionary will be constructed. The dictionary contains all distinct sym-bols in the image for compression. Under the condition of not involving substitution errors, more effective match-ing algorithm will generate smaller dictionary. Generally, smaller dictionary means the symbols can be represented more compactly and will lead to further compression for the images. Table 5 records the dictionary sizes of 12 page images from data set CMU-CDI and PKU-SEB constructed by Enc-JBIG2 and MC-JBIG2. We can see the dictionary sizes of MC-JBIG2 are all smaller than Enc-JBIG2 ones, this shows MC-JBIG2 is more effective and creates more potential for compressing the images.
 Last, let us show the results of compression file sizes. We compress the 12 data set with Enc-JBIG2, Apago-JBIG2, Lead-JBIG2 and MC-JBIG2. Compression results are shown in Table 6 and Fig. 13 . We can see, com-pared with Enc-JBIG2, Apago-JBIG2 and Lead-JBIG2, compression file sizes of MC-JBIG2 on the 12 data set are all the smallest. Summating the 12 compression file sizes, we can see the results of MC-JBIG2 are improved 26.4, 36.0 and 46.1%, respectively, toward Enc-JBIG2, Apago-JBIG2 and Lead-JBIG2. MC-JBIG2 decreases com-pression file size apparently compared with these JBIG2 algorithms.

All above compression are in lossy mode and satisfies the condition of content lossless. For Enc-JBIG2, Apago-JBIG2 and Lead-JBIG2, we take the lossy mode and set the threshold values as recommended that do not cause substi-tution errors. For MC-JBIG2, we take the parameters opti-mized by the Monte Carlo method. The parameters have been validated on the test set and do not cause substitution errors. We also proofread the decompressed images, and no substi-tution error is found.

As shown in Table 2 , Book-08 is Chinese X  X nglish mixed textual image, Book-09 is English textual image, and Book-10 is Spanish textual image. From Table 6 and Fig. 13 , we can see MC-JBIG2 can also improve compression for Latin textual images. Figure 14 shows the improvements of MC-JBIG2 to Enc-JBIG2 on the 12 data set. We can see the improvement on Latin textual images is not as stable as the improvement on Chinese ones.

As for compression time, all above JBIG2 programs are executed on a common PC with CPU P4 3.0 GHz and memory 1.25 GB. To compress a single page image, it cost less than 1 second for each algorithm. MC-JBIG2 and Enc-JBIG2 cost only about 0.1 second to compress a single page image. As all algorithms are highly efficient in execution, we will not compare this more detailedly.

Finally, let us compare the results of MC-JBIG2 with the work of Yan et al. [ 20 ] and Shang et al. [ 21 ], respectively, which are mentioned in Sect. 2 . In their work, Yan and Shang collected several page scanned document images and offered compression results in the papers. Yan X  X  work was not based on JBIG2. They chose G3, G4 and JBIG as comparison targets. For Chinese document images, compression ratios of Yan X  X  algorithm were averagely 3.21 times to G3, 2.02 times to G4, and 1.56 times to JBIG. Based on the design level of JBIG2, the compression ratios are 3 X 8 times to G3 and 2 X 4 times to JBIG [ 20 ]. So Yan X  X  work did not reach the design level of JBIG2.

Shang X  X  work was based on JBIG2. Based on the fig-ures offered in the paper, the work achieved 14.05% over lossy PM and S in Latin textual images and 4.97% over lossy PM and S in Chinese textual images [ 21 ]. The algo-rithm led to few substitution errors in lossy PM and S mode and thus preserved acceptable decoded image quality. In our work, MC-JBIG2 is based on JBIG2 and the compression ratios are improved 26.4 X 46.1% to standard JBIG2 algo-rithms in the criterion of content lossless. Based on these facts, we may judge that the performance of MC-JBIG2 is better than Shang X  X  algorithm in the aspect mentioned above. 7 Summarization and further work In this work, we developed an improved algorithm MC-JBIG2 for Chinese textual image compression. The main idea of MC-JBIG2 is multiple feature data extraction. Based on the extracted multilevel feature data, we can improve the pattern-matching procedure of JBIG2, and this lead to further compression for textual images. To optimize the parameters used in MC-JBIG2, we implement a Monte Carlo method. Experimental results show MC-JBIG2 can improve compres-sion ratio on Chinese textual images. The compression results satisfy the condition of content lossless, namely no substitu-tion error happens during compression. Besides the improve-ment on compression ratio, MC-JBIG2 is also highly efficient in execution.
 As a framework for textual image compression, MC-JBIG2 is flexible and scalable. The further work on MC-JBIG2 includes following aspects. (1) The algorithm relies on the multilevel feature data, so if we can develop better feature extraction scheme, more compression on tex-tual images will be achieved. (2) As for the Monte Carlo optimization section, we believe some machine learning algorithms can be involved to resolve the task. However, because of the specific requirements, it may need big effort to work out the appropriate algorithm. (3) For Latin textual images, the improvement of MC-JBIG2 is still not stable. To improve the algorithm and make it adapt to all kinds of textual images is another meaningful research topic. References
