 The click-through information in web query logs has been widely used for web search tasks. However, it usually suf-fers from the data sparseness problem, known as the miss-ing/incomplete click problems, where large volume of pages receive few or no clicks. In this paper, we adapt two lan-guage modeling based approaches to address this issue in the context of using web query logs for web search. The first ap-proach discovers missing click-through query language fea-tures for web pages with no or few clicks from their simi-lar pages X  click-associated queries in the query logs, to help search. We further propose combining this content based ap-proach with the random walk approach on the click graph to further reduce click-through sparseness for search. The sec-ond approach follows the query expansion method and uti-lizes the queries and their clicked web pages in the query logs to reconstruct a structured variant of the relevance based language models for each user-input query for search. We design experiments with a publicly available query log ex-cerpt and two TREC web search tasks on the GOV2 and ClueWeb09 corpora to evaluate the search performance of different approaches. Our results show that using discovered semantic click-through query language features can statis-tically significantly improve search performance, compared with the baselines that do not use the discovered informa-tion. The combination approach that uses discovered click-through features from both random walk and the content based approach can further improve search performance. Categories and Subject Descriptors: H.3.3 [ Informa-tion Storage and Retrieval] : Information Search and Retrieval X  Search process,Retrieval models ;H.3.5 [ Informa-tion Storage and Retrieval] :Online Information Services X  Web-based services General Terms: Algorithms, Experimentation
Click-through data provide important user preference in-formation (both individual and collective) over the returned web search results and play important roles in designing and improving web search engines. For example, click-through information can be used to derive labeled training data for optimizing web ranking functions used by web search en-gines [14, 26]; user clicks can be directly used as relevance judgments of the clicked URLs to generate evaluation data for comparing different retrieval approaches[28, 3]; and col-lective click-through features can be extracted to enhance the ranking models of search engines[30, 1].

Unfortunately, click-through data usually suffer from a data sparseness problem where large volume of queries have few or no associated clicks [9, 11]. This problem may be caused by two related user click behaviors. One is that users may only click a very limited number of pages for a query so that the clicks are not complete; the other one is that users may just browse the returned snippets to fetch some use-ful information while not clicking any results even they are relevant. Gao et al. [11] referred to these two situations as the incomplete click problem and the missing click problem, respectively. These problems greatly limit the possibility and reliability of using click-through features for web search. For example, click-through features cannot be extracted for pages with no clicks. To overcome the click-through sparse-ness, Craswell and Szummer [9] built a query-URL bipartite click graph from a web query log and then proposed a ran-dom walk algorithm on the graph to discover missing clicks between query nodes and URL nodes. The intuition behind this approach is to use the transitions of the semantic re-lation between queries and their clicked URLs on the click graph to find plausible missing clicks.

However, the random walk approach can only partially al-leviate the click-through sparseness because it requires spe-cific link structures in the click graph to discover missing clicks. For example, URLs (web pages) that have not yet received any clicks in the search history can never be associ-ated with any previously issued queries in the query logs. To address this issue, Gao et al. [11] considered an al-ternative approach to compute click-through features from sparse click-through data. They introduced a Good-Turing estimator [12] based discounting method to smooth click-through features of web pages, so that pages with no clicks can have very small non-zero click-through features com-puted by discounting the average of the click-through fea-tures of all pages that receive exactly one click. Intuitively, their approach follows the smoothing approach of computing out-of-vocabulary (OOV) words X  probabilities in statistical language models to compute missing click-through features for web pages with no clicks.
Notice that although OOV words and missing clicks of web pages can be both viewed as events unseen in training data and thus handled in a similar way, there is an impor-tant difference between the two types of unseen events: we usually have little semantic information about OOV words while we normally have indexed the content of the web pages that have not received clicks yet. Gao et al. X  X  smooth-ing approach does not use any semantic information in the web page content, thus pages that have completely different content but no clicks will obtain the same smoothed click-through features . This is counter-intuitive and makes some smoothed features ineffective for ranking. Indeed, in the ex-periments they found that using the smoothed click-through features extracted from the content of a web page X  X  click-associated query strings (called query-dependent features by them) helped little for retrieval [11].

To overcome the weaknesses of the random walk approach [9] and the Good-Turing smoothing approach [11], we pro-pose to utilize the content similarity between web pages to address the click-through sparseness problem. Our content based approach is able to discover click-through query lan-guage model features that can properly convey semantic in-formation in the content of pages with no clicks to help re-trieval.

Specifically, we hypothesize that web pages that are simi-lar in content may be clicked by web searchers issuing simi-lar queries . Under this assumption, we introduce a language modeling based technique for discovering a target web page X  X  plausible missing click-associated queries, using the queries that led to the clicks on the target page X  X  similar pages. We then use the discovered query language features for search. We further present a way of combining the advantages of both the random walk approach and the content based ap-proach for helping search. In addition, we adapt an alterna-tive approach, based on the relevance-based language mod-els (RM)[18] to address the click-through sparseness issue for search. This approach, Structured Relevance Models, has been used to handle missing fields when searching semi-structured documents[19].

We evaluate the retrieval performance of different approaches using the Microsoft Live Search 2006 search query log ex-cerpt (MS-QLOG), which has been used in some query log studies [28, 3], and two different sets of ad hoc web search tasks: (1) the ones in the TREC 2004-2005 Terabyte Tracks [8, 7] and (2) the ones in the TREC 2009-2010 Web Tracks [6, 15] 1 . Our work has four major contributions: 1) To the best of our knowledge, we are the first to utilize web content similarity to discover missing click-through query language features for improving web search, although content similar-ity has been used in many other applications. 2) We adapt two language modeling based approaches to address click-through sparseness in the context of using query logs for web search; we also propose combining the content based approach and the random walk approach [9] for comput-ing effective ranking features from query logs that have the click-through sparseness issue. 3) We empirically show that using our approaches can statistically significantly improve web search performance. 4) We empirically compare differ-ent approaches of discovering missing click-through query language features for web search and do in-depth analysis on their advantages and weaknesses. http://plg.uwaterloo.ca/  X trecweb/2010.html
We begin by reviewing some related work. In  X  3, we de-scribe three approaches for discovering missing click-through query language features. In  X  4 we present how we use dif-ferent discovered information for search, then compare and analyze retrieval performance of different approaches using the TREC ad hoc web search tasks. Then we conclude in  X  5.
Previous research has encountered the data sparseness problem in click-through data, including the incomplete click problem and the missing click problem, when leveraging web query logs for helping different web search tasks [9, 1, 26]. However, there is relatively little of work directly han-dling the click-through sparseness. Craswell and Szummer [9] proposed a random walk approach on the query-URL click graph to discover plausible missing clicks. Gao et al. [11] have recently proposed a discounting method inspired by the Good-Turing estimator [12] to smooth click-through features for web pages that have received no clicks. Different from previous work, we propose using web content similar-ity to address the click-through sparseness. Recently, Seo et al. [27] proposed applying spectral graph analysis on the web content similarity graph to smooth click counts in the query logs for search. Our approach is similar to their ap-proach in terms of using web content similarity to address click-through sparseness; however, we specifically focus on discovering click-through query language features for search.
Our approach is closely related to other similarity-based techniques, such as clustering similar documents for smooth-ing document language models[16, 21], smoothing documents based on document-content similarity graph [22], and using web content similarity for missing anchor text discovery [31]; however, we focus on enriching web pages X  semantic click-through features for web search by using their similar pages X  click-associated queries. We further consider combining web content similarity and click graph information to improve discovered missing semantic click-through features for web search. We notice that Li et al. [20] considered combin-ing query language features and click graph information for mitigating the click-through sparseness in a different task of classifying the search intents of web queries in query logs. Different from them, we further consider using web content similarity for reducing the click-through sparseness and use the discovered query language features for retrieval.
There is significant research on using click-through data in the query log for enhancing web search performance: us-ing query-page click-through pairs to derive labeled training pairs for learning web page ranking functions [14, 26], ex-tracting click-through features and incorporating them into ranking models for web search [30, 1, 11]. The incom-plete/missing click problems present major challenges for both approaches of using click-through data for web search. Our research on discovering missing click-through features can benefit the latter research direction in particular.
We first describe two different approaches for discovering plausible missing click-through query language information for web pages with few or no clicks. Next, we present one way to combine the advantages of the two approaches. We emphasize that in our research we are particularly interested Figure 1: An illustration example of using random w alk approach to discover plausible missing clicks (denoted by dashed lines): (a) the original click graph; (b) the link-enriched click graph after ap-plying rank walk algorithm. in obtaining click-through query language features, which can convey some semantic information of the target web page , for search.
We start by reviewing the random walk approach that uses co-clicks in the click-through data to discover plausible missing clicks for web pages [9, 11]. We follow Gao et al. [11] and use the forward random walk [9] instead of the backward one for our task. This approach first builds a query-URL bipartite click graph from a web query log, by assigning the same query strings/URLs to the same query/URL nodes and linking them according to the click pairs in the query log; then it uses a random walk algorithm to discover plausible missing click edges. Intuitively, this approach assumes there exists close semantic relation among neighbor nodes in the click graph, and uses the transitions of the semantic relation on the graph for missing click edge discovery. For example, in Figure 1, q 1 and q 4 both lead to the clicks on u 1 , thus q and q 4 may be semantically related; therefore q 4 may also lead to the click on u 2 , which is q 1  X  X  clicked URL. Similarly, due to the co-clicks on u 3 by q 2 and q 3 , q 2 may also lead to the click on q 3  X  X  clicked URL u 4 .

Formally, assume a bipartite click graph G = &lt; Q, U, E &gt; is constructed from a set of query nodes Q = { q 1 . . .q set of web page URL nodes U = { u 1 . . .u n } and the edges E between the query nodes and the URL nodes. ( q i , u j )  X  E is an edge in G when q i leads to at least one click on u and w ( q i , u j ) represents the click count associated with the edge ( q i , u j ). We can normalize the w ( q i , u j ) to obtain the transition probability p ( u j  X  q i ) on the click graph between a query q i and each of its clicked web page u j by: and also the transition probability p ( q i  X  u j ) between a page u j and each of its click-associated queries q i by: Table 1: Some summary statistics of the click graph built from the MS-QLOG dataset and different en-riched graphs by the random walk approach with different noise filtering parameters.

We can use the above transition probabilities p ( u j  X  q i  X  { 1 . . .m } , j  X  { 1 . . .n } to compute the probability p of one query q i transiting to another one q j on the click graph in 2 t steps by the following iterative equations:
We can see that longer transition steps can discover tran-sitions to additional queries for a target query q i while the discovered semantic relation between them becomes weaker and noisier. Thus for effectiveness and efficiency, we follow Gao et al.[11] to set t = 1 in our experiments. In order to further filter some plausible noise, we follow their approach and require that the discovered transitions for the target ling parameter and tuned empirically on training data for different tasks.

After discovering similar queries for each query using the random walk approach, Gao et al.[11] expanded each web page X  X  click-associated queries with discovered similar queries. In this way, web pages are linked with more semantically related queries so that the incomplete click problem is par-tially mitigated. Then they used the enriched representation of the click-associated queries for each web page to extract click-through features to improve web search performance.
Table 1 shows some summary statistics of the original click graph and the enriched click graphs by the random walk ap-proach when we use the click pairs in MS-QLOG to build the graph. The first four rows in Table 1 show some summary statistics of the original click graph, indicating the click-through information is very sparse even for the pages that received some clicks  X  on average, each page only received 2.5 clicks and has about 1.4 unique click-associated queries. The last four rows show the number of click edges in each enriched graph by the random walk approach using different noise filtering parameters, indicating that incomplete click problem can be partially mitigated  X  on average, the num-ber of the unique click-associated queries of each web page has been raised to 6.5 when = 0 . 001.
Notice that the random walk approach needs specific click graph structures to discover plausible missing clicks, mean-ing it cannot handle web pages with no clicks. Therefore, we propose to adapt a content based approach, which was originally proposed by Yi and Allan for addressing the miss-ing anchor text issue in web search [31]. Intuitively, our approach assumes that web pages that are similar in con-tent may receive clicks from web searchers issuing similar queries. Under this assumption, we aim to discover a query language model for each page, in order to obtain effective missing semantic click-through features to help search.
We adapt Yi and Allan X  X  contextual translation approach of discovering missing anchor text [31] for our task. Briefly speaking, their approach first views the content of web pages as their anchor text X  X  descriptive context and utilizes the contextual translation approach [28] to measure the seman-tic relation between the anchor text associated with differ-ent pages. Given any page P i and a target page P 0 , the semantic relation between their associated anchor text A i and A 0 is measured by the contextual translation proba-bility t ( A i  X  A 0 ), computed from the Kullback-Leibler diver-gence (KL-div) between the document language models of P i and P 0 . Then they use t ( A i  X  A 0 ) to compute a relevant anchor text language model p ( w  X  A 0 ) for a target page P discover P 0  X  X  plausible missing anchor terms by: where A denotes the complete anchor text space of all pages and p ( w  X  A i ) is a multinomial distribution of anchor terms ( w ) over the vocabulary V A .

Similarly, we first view each page P i  X  X  content as the de-scriptive context of the page X  X  click-associated queries Q and use P i  X  X  document language model, p i = { p ( w  X  P Q  X  X  contextual language model, which is computed by ap-plying Dirichlet smoothing [17] on the original un-smoothed document language model: where p ML ( w  X  P i ) is the maximum likelihood (ML) estimate of observing a word w in the page, p ( w  X C ) is w  X  X  probability in the collection C , N P  X  is the length of P i  X  X  content and is the Dirichlet smoothing parameter.

Then given any page P i and a target page P 0 , we mea-sure the semantic relation between their click associated queries Q i and Q 0 by their contextual translation probabil-ity t ( Q i  X  Q 0 ), computed from the KL-div Div (  X  X  X  X  X  X  ) between their contextual models p 0 and p i : The end of Equation 6 is the likelihood of generating Q 0 context P 0 from the smoothed language model of Q i  X  X  con-text P i , being normalized by Q 0  X  X  context length.
After that, for each given target page P 0 , we calculate a relevant (click-associated) query language model ( RQLM ) p ( w  X  Q 0 ) to discover P 0  X  X  plausible click-associated query terms by: where Q i denotes all the queries that may lead to the clicks on P i but may be incomplete or missing , Q denotes the complete textual space of the click-associated queries of all pages, p ( w  X  Q i ) is a multinomial distribution of query terms ( w ) over the click-associated query language vocabulary V
To compute the RQLM p ( w  X  Q 0 ) in Equation 7, we use each page P i  X  X  click-associated queries originally observed in the query log to estimate a query language model p obs ( w  X  Q to approximate p ( w  X  Q i ), which should be estimated ideally from some unknown complete set of P i  X  X  all plausible click-associated queries in the query log 2 . In practice, for effec-tiveness and efficiency we compute the RQLM of the target page P 0 using the click-associated queries of P 0  X  X  top-k most similar pages in the query log. This choice is due to two rea-sons: (1) t ( Q i  X  Q 0 ) is very small for other pages thus has less impact on the RQLM; (2) increasing k can increase the num-ber of query samples for better estimating RQLM but also may introduce more noise to degrade the quality of the es-timated RQLM. We tune k  X  X  value on the training data for each different retrieval task.
We can take advantages of both the random walk ap-proach in  X  3.1 and our content based approach to further reduce the click-through sparseness and calculate better se-mantic click-through features for search. Here we present one language modeling based way to combine the advan-tages of two approaches.

We first employ the random walk approach to enrich the original bipartite click graph and discover more click-associated queries for each web page. Then we estimate a query lan-guage model p ( w  X  Q aug ) for each web page from the new added click-associated queries, which we call augmented queries, of the page. We also estimate a query language model p ( w  X  Q orig ) for each page from its click-associated queries originally observed in the query log that has not been en-riched by the random walk approach. Next, we employ the mixture model approach [23, 24] to combine two query lan-guage models p ( w  X  Q orig ) and p ( w  X  Q aug ), and compute a bet-ter smoothed query language model  X  p ( w  X  Q ) by: where is a meta-parameter to control the mixture weight (or prior probability) of each component and can be tuned on training data for different tasks. Then we use the up-dated query language model  X  p ( w  X  Q ) of each page to better approximate the p ( w  X  Q i ) in Equation 7 so that we can better estimate the RQLM p ( w  X  Q 0 ) of each page to help retrieval.
Next, we describe how we utilize missing click-through query language information discovered by these different ap-proaches to help improve search performance.
We have described several ways to infer click-through in-formation when in situations where a page or query has few clicks. In this section we consider how to use the inferred information and measure its utility. We first present how we utilize discovered missing click-through query language in-formation for retrieval in  X  4.1, following the language mod-eling based retrieval framework[25]. Then in  X  4.2 we con-sider an approach that expands the query with additional
We will use this fact in  X  3.3 to combine the random walk approach and the content based approach for discovering missing click-through features. terms, also inferred from the click-through log. For the con-venience of discussing different retrieval models and base-lines, we start by briefly describing the data and methodol-ogy we used for evaluating different approaches.

Mainly due to privacy and security concerns, there are very limited publicly available query log data for research. Here we use the MS-QLOG dataset which contains about 12m click-through events and also information of about 15m additional user-issued web queries that received no clicks, sampled from the query log of Microsoft X  X  web search engine during 05/01/2006 to 05/31/2006. We only use the click-through records in this dataset for our experiments.
We use the queries and the relevance judgments in two different sets of the TREC web search tasks to design re-trieval experiments. The first set consists of the ad hoc web search tasks in the TREC 2004-2005 Terabyte Tracks [8, 7], where the GOV2 collection (a TREC web collection crawled from government web sites during early 2004) was used for search; the second set consists of the ad hoc web search tasks in the TREC 2009 Web Track [6, 15] and the TREC 2010 Web Track 3 , where the search was originally performed on the ClueWeb-09 Dataset 4 (a larger TREC web collection re-cently crawled during 01/06/2009 to 02/27/2009 from all domains of the Web).

Because our approach depends on web page content simi-larity, we crawl the web pages of all the clicked URLs in MS-QLOG and use the crawled pages and their click-associated queries in MS-QLOG as the training data for extracting se-mantic click-through features. The GOV2 collection and the TREC category B subset of the ClueWeb09 web collec-tion (or ClueWeb09-T09B dataset), are used as the searched targets in our experiments. Each ClueWeb09 or GOV2 web page can be viewed as a page whose click information is com-pletely missing 5 , thus we need to handle the click-through sparseness problem in both the training pages and the searched collections.

More details about the data and methodology used for evaluating the retrieval performance of different approaches will be described in  X  4.3.1. Then we will discuss the experi-mental results in  X  4.3.2 and  X  4.3.3.
The first baseline is a query likelihood baseline following the typical language modeling based retrieval approach[25]. This baseline does not use any click-through features and ranks each web page P for a query Q by the likelihood of the page P  X  X  document language model p ( w  X  P ) generating the query Q : We use Dirichlet smoothing [17] to compute the document language model p ( w  X  P ) used in the above equation and de-note this query likelihood baseline QL here. We tune the Dirichlet parameter for QL to achieve the best retrieval performance for different tasks. Note that is fixed to 2500 when using Equation 5 to compute the document models of the crawled clicked pages for estimating RQLMs (relevant click-associated query language models described in  X  3.2). http://plg.uwaterloo.ca/  X trecweb/2010.html http://boston.lti.cs.cmu.edu/Data/clueweb09/
We follow the mixture model approach [23, 24] to use the discovered click-through query language model features to help search. After we estimate the RQLM p ( w  X  Q 0 ) for each page, we mix a web page P  X  X  document language model p ( w  X  P ) with the RQLM to obtain a better document lan-guage model  X  p ( w  X  P ) by: where p ( w  X  P ) is the original smoothed document model in the QL baseline and is the meta-parameter controlling the mixture weights of the component distributions. Then we can use the updated document language model  X  p ( w  X  P ) and Equation 9 for retrieval.

We have described three different approaches of discover-ing semantic missing click-through features in  X  3. We point out because the searched items here are ClueWeb09 or GOV2 web pages with no click information, only using the random walk approach cannot discover any click-associated queries for them . Therefore, we do not use the sparse click count (which is zero for almost all pages and not helpful for re-trieval) in our experiments, but use our content based ap-proach and the combination approach for improving search performance. In the combination approach, we first dis-cover plausible missing links in the click graph (built from MS-QLOG) by the random walk approach and then use the enriched click graph to estimate better RQLMs for the ClueWeb09 or GOV2 pages as described in  X  3.3. We de-note the retrieval baseline that employs our content based approach to update document models for search as RQLM , and the baseline that uses combination approach for search as RW+RQLM in later discussions.
Besides document smoothing approaches, we are also in-terested in exploring some query-side alternative approaches of handling missing click-through information for search. Here we adapt a structured variant of the relevance based language models [18], which was proposed by Lavrenko et al.[19] and called Structured Relevance Models (SRM), for discovering useful click-through query information to recon-struct queries. The SRM technique was originally developed to search semi-structured documents with incomplete/missing fields; thus, here we introduce field structure for the queries and pages, represent click-through information using this structure and then utilize the SRM approach for search.
Formally, we view each web page as a semi-structured document containing two fields: (1) the PageContent field (denoted by w p ) which contains the original page content and (2) the QueryContent field (denoted by w q ) which con-tains all the click-associated queries of the page in the web query log. Then for each unstructured query q , we gener-ate a semi-structured query q = { w p , w q } that has the same semi-structure as the web page document by duplicating the query string in both fields, i.e. w p = w q = q : intuitively, the query is searching for pages that match the query in con-tent and/or their click-associated queries . We assume that both fields are incomplete and then use the SRM approach to estimate plausible missing field values in q based on the observed { w p , w q } .
 We use our crawled pages of the clicked URLs in MS-QLOG and their click-associated queries in MS-QLOG to form the training semi-structured document collection W . We then use the training collection to calculate the SRM { R p (  X  ), R q (  X  ) } for q , where each relevance model R ifies how plausible it is the word w would occur in the field i ( i  X  { p, q } ) of q given the observed q = { w p , w q where w  X  w i denotes appending word w to the string w i and V i denotes the vocabulary of the field i . Using the training web page documents w  X   X  W and Equation 11, R i ( w ) can be further calculated by: To calculate the posterior probability P ( w  X   X  q ),we use the following equations: where P ( w  X  ) is assumed to be a uniform distribution, the meta-parameters p , q are used to control the impact of each field on the posterior probability and tuned with the training queries. When computing P ( w i  X  w  X  i ) , i  X  { p, q } in Equation 13, we fix the Dirichlet smoothing parameter p = 50 , q = 1 for the PageContent and QueryContent fields, respectively. 6
For efficiency and effectiveness we use q  X  X  top-k most simi-lar documents instead of all w  X   X  W to calculate R i ( w ). k is tuned with the training queries. Because the click informa-tion is completely missing in our two searched target collec-tions W  X  X  (ClueWeb09-T09B and GOV2), the QueryContent field is missing there. Therefore, we only use the relevance model R p ( w ) of the estimated SRM in the PageContent field to search each target collection. We interpolate it with the original query language model to obtain a better relevance model for retrieval: which is similar as in the Relevance Model 3 [10]. We use the parameter to control the impact of the original query language model on the updated relevance model and tune it with the training queries. Then the searched documents w  X  X   X  W  X  X  are ranked by their weighted cross-entropy [17] based similarity to R  X  p ( w ): We denote this query expansion retrieval baseline as SRM in our experiments.

For comparison, we also provide the typical highly effec-tive language modeling based query expansion baseline  X  Relevance Model [18]  X  in our experiments. We use the ver-sion of Relevance Model 3 [10] and denote it as RM . Note that different from SRM, here RM does not use any click-through information for search: it builds a relevance model from the top results of the QL baseline, which is obtained by running the original query directly against the search target (ClueWeb09-T09B or GOV2); then it mixes the built rele-vance model with the original query language model (similar as in Equation 14) and ranks the searched pages again using the updated model.

In addition, we consider an approach that combines the advantages of both the RM approach and the combination approach (RW+RQLM in  X  4.1) for further improving search performance. This approach first uses discovered click-through query information from RW+RQLM to get a better query-likelihood ranked list of pages for a given query, and then uses the top ranked pages to compute a plausibly better rel-evance model for query expansion and re-retrieval. This ap-proach is similar in spirit to previous research that combines document expansion techniques and RM for further improv-ing search [29]. We denote this approach as RW+RQLM +RM in the experiments.
As described at the beginning of  X  4, we consider two set of retrieval tasks. The first one is performed on GOV2 which contains about 25m U.S. government web pages; the second one is performed on ClueWeb09-T09B which contains about 50m English web pages. We use the Indri Search Engine 7 to index each collection by removing a standard list of 418 INQUERY [4] stopwords and applying Krovetz stemmer.
For the first retrieval task, we use 50 ad hoc queries (topic id:701-750,title-only) in the TREC 2004 Terabyte Track [7] for train and 50 ad hoc queries (topic id:751-800,title-only) in the TREC 2005 Terabyte Tracks[8] for test. For the sec-ond retrieval task, we use 50 ad hoc queries (title-only) in the TREC 2009 Web Track [6, 15] for train and 50 queries (title-only) in the TREC 2010 Web Track for test. More-over, instead of using the whole ClueWeb09 collection as the search target as in the TREC 2010 Web Track, we only use the ClueWeb09-T09B subset here; thus only relevant pages in this subset collection are used for evaluation. We crawled the web pages of the clicked URLs in the MS-QLOG during June 2010 and use the HTML pages down-loadable during that time period and their click-associated queries in the MS-QLOG as the training collection for our experiments. Originally there are about 5m unique clicked URLs in this query log, as shown in Table 1; we successfully crawled about 3m HTML pages of the clicked URLs and in-dexed them using the Indri Search Engine. We remove 418 INQUERY stopwords and apply Krovetz stemmer during the indexing and call the indexed collection as MS-QLOG-Web, which contains about 21.5 million unique words and 4.1 billion word postings. These training pages are then used to discover click-through query language features for the GOV2 or ClueWeb09 pages. We also preprocess the queries in the MS-QLOG using the same stopwords remov-ing and stemming procedure.

To evaluate the retrieval performance, we calculate typ-ical IR evaluation measurements including Mean Average Precision (MAP), Precision at position k (P@ k ), Normal-ized Discounted Cumulative Gain (NDCG) [13]. For the TREC 2009 Web Track queries, we report two additional measurements: statMAP and MPC(30) , which were used by the TREC community for that track [6] and computed http://www.lemurproject.org/indri/ by the TREC evaluation tool statAP MQ eval v3.pl 8 ; thus, we can compare our results with other researchers X  published results on the same query set. Intuitively, both statMAP and MPC(30) measurements are used for addressing the in-complete judgment issue [2]: the former one is a statistical version of the MAP measurement and the latter one is a statistical version of the measurement P@30.

In each retrieval task, we first tune the Dirichlet smooth-ing parameter in Equation 5 to obtain the best QL base-line that can achieve the highest MAP with training queries on each searched target collection (GOV2 or ClueWeb09-T09B). Then for both the RQLM baseline (using our con-tent based approach) and the RW+RQLM baseline (using the combination approach), we follow the reranking scheme, where we use the updated document language model by each approach to recompute the query likelihood scores of the top-1000 web pages returned by the QL baseline for each query and then rerank the pages. For the RQLM baseline, we tune these two parameters: the number ( k ) of the similar pages whose click-associated queries are used to compute the RQLM and the mixture weight in Equation 10. For the RW+RQLM baseline, we tune two additional parameters: the transition probability threshold (discussed in  X  3.1) and the query language model updating weight in Equation 8. For the SRM baseline, as described in  X  4.2, we tune the number of the similar pages ( k ) used to build SRM, the num-ber of terms ( N ) in each field of the built SRM, the meta-parameters in Equation 14 and p , q in Equation 13. For the RM baseline, we tune the number of top ranked pages ( k ), the number of terms ( N ) used to build a relevance model and the mixture weight  X  between the relevance model and the original query model. For the RW+RQLM+RM base-line, we first use the tuned RW+RQLM baseline to obtain a best query likelihood ranked list of pages, then we use this best ranked list to build a relevance model and tune the number of top ranked pages ( k  X  ), the number of terms ( N ) and the mixture weight  X  similarly as in the RM baseline.
In each retrieval task, we tune the parameters of each approach with the training queries, and then test their per-formance on the test queries.
Table 2 and 3 show the retrieval performance of different approaches with the training and testing queries, respec-tively, in the first retrieval task (TREC 2005 queries are widely known to perform well, explaining why they outper-form the training queries.) Table 4 and 5 show the retrieval performance of different approaches with the training and testing queries, respectively, in the second retrieval task. The  X  and  X  in these tables indicate statistically significant improvement over of the QL baseline based on one-sided t-test with p &lt; 0 . 05 and p &lt; 0 . 1,respectively. The statistically significant improvement over of the highly effec-tive RM baseline based on one-sided t-test with p &lt; 0 . 05. Table 2 and 4 also show the corresponding tuned parameters of each approach in the first and second retrieval task, re-spectively. These tables show using discovered click-through query information can help for improving web search perfor-mance. We have the following main observations: 1. Using click-through query language features from MS-QLOG benefit more for the web search tasks on the ClueWeb09
It is downloadable at: http://trec.nist.gov/data/ web09.html Table 2: Retrieval performance and tuned param-eters on the TREC 2004 Terabyte Track queries (train).
 Table 3: Retrieval performance on the TREC 2005 Terabyte Track queries (test). data than the ones on the GOV2 data. This is not surprising because the TREC ad hoc search tasks on the ClueWeb09 data are, in nature, more similar to real-world web search scenarios as those recorded in MS-QLOG: (1) the ClueWeb09 dataset were crawled from the general web while the GOV2 data was crawled only from government web sites; (2) the TREC Web Tracks queries were created to closely simulate the real-world web search scenarios, while the TREC Ter-abyte Track queries targeted government web pages in order to have some relevant pages in the GOV2 data. 2. On the test query sets in both retrieval tasks, (a) both RQLM and RW+RQLM performed statistically significantly better than QL in terms of MAP,P@10 and NDCG; (b) RW+RQLM+RM performed statistically significantly better than RM in terms of MAP and NDCG. This result demon-strates that using click-through query language model fea-tures discovered by our content based approach can help to improve the web search performance significantly, even compared with a highly effective typical query expansion baseline. This also indicates that our content based ap-proach can effectively alleviate the click-through sparseness problem. In addition, RW+RQLM performed slightly better than RQLM on the training query sets in both retrieval tasks and the test query set in the ClueWeb09 retrieval task, indi-cating that the combination of our content based approach and the click-graph based random walk approach can further reduce the click-through sparseness and refine the discovered missing click-through features for search. 3. The structured query expansion approach (SRM) achieved very good performance (3 rd on GOV2 and 2 nd on ClueWeb data) on the training query sets in both retrieval tasks. This shows when the model parameters are carefully tuned, SRM can use click-through information to discover miss-Table 4: Retrieval performance and tuned parame-ters on the TREC 2009 Web Track queries (train).
 Table 5: Retrieval performance on the TREC 2010 Web Track queries (test). ing query language information to improve the search ef-fectiveness. p = 0 . 99 , q = 0 . 01 in the first retrieval task implies that the reconstructed query field content mainly comes from the content of the MS-QLOG-Web pages that have the highest likelihoods of generating the original query. In contrast, p = 0 . 01 , q = 0 . 99 in the second retrieval task implies that the reconstructed query field content mainly de-termined by each query X  X  similar queries X  clicked pages, thus the query log information is more helpful for the search task on the ClueWeb09 data. However, we observe that on the test queries SRM achieved little improvement over the QL baseline. We will do more analysis on this issue in  X  4.3.3 to investigate some plausible causes, such as the sensitivity of the performance of SRM is to the change of its model parameters and irrelevant noise in the training data across different query sets. 4. The typical query expansion approach (RM) failed (i.e. performed worse than QL) on the TREC 2010 Web Track web search task (in Table 5), but leveraging the click-through query information discovered from random walk and our content based approach can make RM more resistent to ir-relevant noise in the searched collection and effectively re-duce the risk of topic-drifting . Indeed, on both training and test queries across different searched collections and retrieval tasks, RW+RQLM+RM approach performed robustly and very well: it achieved the best MAP on 3 of 4 query sets and the second best MAP on the remained query set.

It is worthwhile to compare our results with some very recently published results on the TREC Web Track ad hoc web search tasks on the ClueWeb09-T09B collection (our second retrieval task). Table 6 shows some results on the TREC 2009 Web Track ad hoc search task from some par-ticipants [15]. The 2nd-4th rows of the table show Koolen and Kamps X  results[15] on the same retrieval task when they Table 6: Retrieval performance of some published results on the TREC 2009 Web Track ad hoc queries. examined the potential of using existing anchor text in large scale web corpora for helping search. One major difference between their QL baseline and ours is that they used lin-ear smoothing approach while we used Dirichlet smooth-ing. The 5th-7th rows of Table 6 show the top3 best official TREC submissions for the same retrieval task from other participants. Comparing Table 6 with our results in Table 4, our retrieval approaches that use discovered click-through query language information from sparse click-through data achieved similar or better performance, compared to these top-performing TREC submissions.

To summarize, our content based approach can effectively discover missing click-through query language information to help improving retrieval performance. The content based approach can be combined with the random walk approach to further improve the quality of the discovered language model information from click-through data. The query-side approach of handling click-through sparseness performs very well on some query sets but not on other query sets. The discovered click-through query information can be combined with the typical relevance model to further improve web search performance.
We are concerned about how sensitive different approaches X  performance is to the change of their retrieval model param-eters. Specifically, for the content based approach (RQLM), we are interested in the number of similar pages needed to build RQLMs for each page to be able to perform rea-sonably well and how the change of this number will af-fect the retrieval performance; for the combination approach (RW+RQLM), we are further interested in the impact of the augmented queries discovered by the random walk approach on the performance. For the SRM based approach, we are concerned about the impact of different number of feedback pages used to build SRM and the mixture weight between the original query language and the built SRM on the re-trieval performance. As discussed in the previous section, our second retrieval task on the ClueWeb09-T09B collection better simulates the real-world web search scenarios; there-fore, here we use this task for our investigation.
Figure 2(a) and (b) depict the model parameters X  im-pact for RW+RQLM with training/testing queries in this retrieval task, respectively, where we fix = 0 . 01 , = 0 . 9 while varying and k . Figure 3(a) and (b) depict the model parameters X  impact for SRM with training/testing queries, respectively, where we fix p = 0 . 01 , q = 0 . 99 , N = 100 while varying and k .

From Figure 2, we have the following major observations: 1. Using click-associated queries from about 25  X  35 most similar pages to build RQLM for each page can achieve near optimal retrieval performance on both training/test query sets. Increasing k beyond 35 brings little additional ben-efit to (or even hurt) the retrieval performance, and only changes the performance very slowly. This property means that in real-world use, for efficiency we need only index click-through information from a small number of similar pages of each page, without sacrificing the retrieval effectiveness. 2. Using augmented queries discovered by the random walk approach from the click graph can slightly help the retrieval effectiveness. The mixture weight  X  X  value can be selected between 0 . 4  X  0 . 6 across different query sets and the change of this value among this range has little impact on the re-trieval performance. This also indicates that the augmented queries discovered by the random walk approach are at least as useful as the click-through query information discovered from the content based approach for search.

Figure 3 shows that the performance of SRM mainly de-pends on whether the training web pages (the MS-QLOG-Web collection here) contains useful content for helping to search the target web collection (the ClueWeb09-T09B here). For example, on the training queries, using only top-5 feed-back pages for re-constructing the PageContent query field can achieve very good performance; in contrast, on the test queries, the improvement is very little. Furthermore, the mixture weight also affects SRM X  X  retrieval performance significantly.

Figure 2 and 3 also reveal that for different query sets the optimal model parameters are very similar for RW+RQLM while different for SRM; therefore, if we exchange TREC 2009/2010 queries for training/test, RW+RQLM X  X  parame-ters will be nearly the same and so will achieve near optimal retrieval performance while SRM X  X  performance will signifi-cantly degrade.
In this paper, we adapt two language modeling based ap-proaches to address the click-through sparseness problem in the context of using web query logs for helping web search.
Our first approach stems from the contextual translation approaches [28, 31] and uses web content similarity for dis-covering missing click-through query language model infor-mation for web pages with no or few clicks, in order to help search. This approach computes a relevant (click-associated) query language model, called RQLM, for each web page from the click-associated queries of its similar pages in the web query log, and then uses the RQLM to smooth the original document language model of each page for achieving better retrieval performance. Compared with the random walk ap-proach [9], this content based approach does not need to use specific click graph structure to discover missing clicks thus can further mitigate the click-through sparseness. Further-more, we present a combination approach that takes advan-tage of both random walk and the content based approach to further improve search.

Our second approach follows the query expansion approach and utilizes the semantic relation between the queries and the content of their clicked URLs in the web query log to reconstruct a structured variant of the relevance based lan-guage models, called Structured Relevance Models (SRM)[18, 19], for each user-input query, to help search.

We then demonstrated the effectiveness and compared the performance of different approaches of handling the click-through sparseness problem for web search with two recent sets of TREC ad hoc web search tasks. The results showed that discovering missing click-through query language infor-mation from click-through data can statistically significantly improve search performance, compared with two retrieval baselines (QL and RM) that did not use the discovered in-formation. The document smoothing approaches (RQLM and RW+RQLM), performed well across different query sets while the query expansion approach (SRM) of using sparse click-through information was more sensitive to model pa-rameter selection and irrelevant noise in the click-through data. The random walk approach complemented the con-tent approach for addressing the click-through sparseness problem: the combination approach performed slightly bet-ter than the content-only approach in three of the four query sets in our experiments. In addition, the most complex ap-proach (RW+RQLM+RM) that combines to use the typical query expansion approach (RM) and the discovered click-through query language information from RW+RQLM can statistically significantly improve the highly effective RM baseline .

There are several interesting directions of future work. It is worthwhile to explore using the discovered missing click-through query language features beyond the language mod-eling based retrieval framework, for example, using the dis-covered features in the learning-to-rank retrieval approach [5], so that we can combine different approaches described here with the Good-Turing based smoothing approach [11] to see whether the retrieval performance can be further im-proved. Moreover, here we have only explored using the con-textual translation probability p ( P i  X  P 0 ) between web pages to discover useful missing click-through query language fea-tures; theoretically, we may also use this probability to com-pute an expected feature E ( f P 0 ) = P a page P 0 for any selected click-through feature from the same click-through features f P  X  of the page X  X  similar pages P . Intuitively, this approach aims to smooth the click-through features for web pages with no clicks as in Gao et al. X  X  approach, but leverages the web content similarity during the smoothing. We would like to explore the utility of these smoothed click-through features for web search.
This work was supported in part by the Center for In-telligent Information Retrieval, in part by NSF CLUE IIS-0844226, and in part by NSF grant #IIS-0910884. Any opin-ions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily re-flect those of the sponsors. (a) with the training queries; (b) with the test queries.
