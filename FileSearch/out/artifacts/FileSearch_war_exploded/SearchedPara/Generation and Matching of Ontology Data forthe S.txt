 Ontology has been realized to be an essent ial layer of the emerging semantic web [1]. According to description logics (DL) [2], an ontology as a knowledge base normally consists of a  X  X Box X  and a  X  X Box X . The TBox consists of concepts and their relations while the ABox consists of instances of concepts or individ-uals. For an ontology expressed in the web ontology language (OWL) [3], we use the term ontology schema and ontology data for the part corresponding to the notion of TBox and ABox respectivel y. Through the semantic web search engine Swoogle (http://swoogle.umbc.edu), we X  X e found that there are plenty of ontology schemas available over the web. But in contrast, ontology data does not seem to be very abundant.

The abundance of information and data in the current web has made the web an important place for people to seek information. This leads us to believe that the abundance of ontology data in the semantic web is very crucial. Therefore, besides the development of ontology schemas, the generation of ontology data also plays an important role for the semantic web.

Generation of ontology data can be achieved through different ways. This pa-per proposes a framework that supports the generation of ontology data through conversion and authoring. It is designed w ith the peer-to-peer architecture, which is more flexible and scalable in terms of ontology data management and distri-bution. Although several (ontology) data management or sharing frameworks based on peer-to-peer architecture have b een proposed (e.g. Edutella [4], Piazza [5],and etc), our framework has an advantage that differentiate it from them in that it deals with the issue of ontology data matching across peers. Normally, the ontology data for a certain doma in contributed by one peer may not be complete but may be implicitly related to those contributed by other peers as data overlapping often occurs. Our framework, designed with a matching pro-cess to discover the implicit relations, is able to help reduce the redundancy and increase the amount of richly inter-linked ontology data.
 The rest of the paper is organized as follo ws. Section 2 discusses related work. An overview of the framework is given in Section 3. Section 4 describes how the framework supports the generation of ontology data. Section 5 presents the function of ontology data matching. Experimental results about ontology data matching are shown in Section 6. Section 7 concludes the paper. We discuss relate work from two aspects: ontology data generation and data matching.

There are a few ways to generate ontology data. Some ontology development tools such as Pretege (http://protege.stanford.edu), SWOOP [6] can be used to create new ontology data as well as to develop ontology schemas. However, as these tools are designed with an emphasis on knowledge representation, they are often used by experienced domain experts familiar with ontology related tech-niques. Ordinary users may have difficulty using it, which hinders the generation of ontology data in a large scale. Annotation of existing data with ontologies is another way to generate ontology data (e.g., CREAM [7]). Ontology data can also be generated by automatically annotating web pages (e.g. [8]). Although a large amount of ontology data can be generated by this type of methods, some applications may not be able to use them due to its relatively poor quality.
On the other hand, data matching is mostly a research topic in the traditional database community. It involves creating s emantic matching between objects, in-stances, or records from different data sou rces (mainly different databases) [9]. Different techniques have been employed to perform data matching in differ-ent applications (e.g., [10,11]). Although we can directly use these methods to perform ontology data matching by treating instances in ontology data as data records in databases, this ignores the particular features that ontology data in-herently has, which may affect the performance of ontology data matching. We first give a brief overview of our framework that supports the generation and matching of ontology data. The framework uses the super peer topology [4] for the peer-to-peer architecture. Different fr om traditional client/server architec-ture, it shifts part of the tasks of the server (super peer) to the clients (ordinary peers). For example, when processin g a query, a super peer can simply tell the query issuer from which peers they can ge t potential query results instead of returning the complete qu ery results directly.

Accordingly, as shown in Fig. 1, our fr amework introduces two types of peers: super peer and ordinary peer (or just den oted as peer). The super peer acts as a coordinator for peers connected to it. It hosts the backbone ontology schema used for the generation of ontology data and provides related functions, one of which is ontology data matching (Section 5). On the other hand, peers offer functions that support ontology data generation ( ontology data publication )and query ( ontology data query ). 4.1 Data Conversion As many existing data are stored in databases or formatted in XML with no ontologies to interpret them, it is desirable to convert them into formats that can be explained by given ontology schemas and ready to be integrated. Mostly we use OWL to encode these converted data according to the backbone ontology. Here we only discuss the case of XML data due to limited space.

XQuery [12] is used to convert ordinary XML data into OWL format.Executed by a XQuery-compatible engine, a quer ywritteninXQuerytakesXMLfilesas input and generates results in an XML format defined by the query itself. Since OWL is also based on XML syntax, the problem is then transformed into how to design the query so that the output results are actually the desired OWL format. The advantage of using XQuery instead of developing our own programs for conversion is obvious. We don X  X  have to design any custom conversion rules and to maintain the program, which might be time-consuming and error-prone. As a W3C candidate recommendation, XQuer y is versatile enough to satisfy our needs and has several implemented query engines for us to choose. Therefore, to convert XML data to desired OWL format, we only focus on the design of the queries and use Nux (http://dsd.lbl.gov/nux/), a java toolkit capable of XQuery processing, to process them.
 4.2 Data Authoring While existing data is very useful for the ontology data generation, it is equally important to allow new data to be created and published in the framework.
Data authoring is the process during which peers create their own ontology data and publish it into the framework. Like the current Web, where data is directly contributed and published by various individuals and organizations, our framework should also enable different individuals or organizations to publish their new data via their corresponding p eers. Therefore, the data in the frame-work will be very dynamic, often reflecting its very recent status while still retaining reasonable semantics thanks to the backbone ontology schemas.
Users who want to author their concerned data through the peers should be familiar with the backbone ontology schemas. We develop web-based programs at each peer server so that users can get familiar with them easily and quickly. For example, Fig. 2 (a) shows the inter face that allows users to learn a back-bone ontology schema describing the university settings by browsing intuitively. Therefore, the users don X  X  have to study the original ontology schema encoded in OWL with more efforts. A professor, if he/she wants to publish some data about his/her recent publications, can choose the class that is most appropriate for the data. He/She then ca n use the selected class to c reate the ontology data, through a friendly interface as shown in Fig. 2 (b). With the data supplied by the user, the program at the peer server generates the corresponding ontology data in OWL format as shown in Fig. 2 (c). In summary, supported with these functionalities, users can create and publish ontology data with ease. Data matching process is designed to deal with the problem of implicit relations among the data from different peers. The necessity of it can be illustrated by the following example. Suppose a professor has a peer contributing data about his/her own details including contact information, research interests, supervised students, research groups, selected publications (without details such as abstracts and full texts), and etc. Meanwhile a publisher X  X  peer contributes information of a detailed publication list, which incl udes some publications (with full texts) of that professor. The publisher X  X  peer lets us know details of publications by that professor. All the data are published as instances of concepts of the given ontology. Because the instance describing the professor from the publisher X  X  peer is not explicitly related t o that from the professor X  s peer due to the decentralized environment, we only get a partial view of the professor X  X  information from these separate peers. It is impossible to issue an enquiry like getting some publication details of a professor whose research interests are of a given area.
Therefore, the task is to match the ins tances from different peers, making their implicit relations explicit. It is common to compute similarities between instances to determine if they are matched. Several similarity measurements from different aspects are used in the framework. A learning mechanism is employed to implicitly combine these measurements in a meaningful and adaptive way. This involves a learning phase to build the model and an matching phase to apply the model for data matching. 5.1 Learning Phase The learning phase involves the training of a binary classifier from matched and unmatched instances. Support vector machines [13] are chosen as the classifica-tion model in the proposed framework. First, a certain amount of initial data are gathered by super peer from different peers. This data set should contain a portion of matched instances. These matched instances are not discovered and specified initially, while training an SVM classifier requires both specified matched instances and unmatched instances as positive and negative samples. Therefore, these initial data should be checked and tagged manually for the training. An initial similarity checking and sorting process based on selected instances properties is performed to make the manual tagging easier. After all these initial data are tagged and the matched and unmatched instance pairs are created, it is ready to train the classifier.

Several similarity measurements are used to compute similarity/distance scores for instance pairs as different feature scores. string edit distance [14] (denoted as SED) and cosine similarity based on TF-IDF [15] (denoted as SIM) are used to measure the string similarity of instance properties at character level and at word level respectively. In addition to string-b ased similarities, ontology-based similar-ities are also used. We define the term o f  X  X oncept distance X  (denoted as CD), which can measure the distance of concep ts of two given instances according to the ontology. Instances belonging to the same concepts have the closest distances while those belonging to disjoint concepts have the largest distances. As ontol-ogy technology allows instances to be related by object properties [3], it is use-ful to check the  X  X ontext similarity X  (de noted as CS) for object properties of two instances. If the instances that are related to two target instances via the same object property are similar according to string-based similarities, the two target instances will have high context similarity. Details of these similarity measure-ments are presented in anoth er paper due to limited space.

Given the above different similarity me asurements, we create feature vector for each pair of instances from the initial tagged data set. For a pair of instance a and instance b , its feature vector is composed as follows: where d 1 ,  X  X  X  ,d m are data type properties [3] of the instances; and o 1 ,  X  X  X  ,o n are object properties. These feature vectors together with the tagged informa-tion (matched or unmatched) enable us to build the classification model for the matching. 5.2 Matching Phase During the Matching phase, the super peer matches instances from the peers when they contribute their own data. When a peer has some data published, the super peer will be notified. Instances in those data will be sent to the super peer for an initial check upon its request. The initial check searches potentially matched instances that are previously indexed for the new instances through an inverted index. If no instances are found for the new instances or the found instances have very low hits, these new instances will be ignored. This initial check screens off a number of instances. F or those instances with potentially matched instances found, instance pairs are created as the input of the classifier.
The trained SVM classifier is used to determine if the instance pairs are matched pairs with the following classification function: where K ( p , q ) is a kernel function used for mapping features into different spaces,  X  i is the Lagrangian coefficient of the i -th training instance pair, y i is the label of the training instance pair. In this function,  X  i , b are obtained during the learning phase and f ( q ) indicates the distance of q from the optimal hyperplane. So we can use this value to evaluate the confidence level of the pair being matched [11]. That is, if f ( r ) &gt;f ( q ), then r is more likely to be a matched pair than q . For a potentially matched instance pair q , we regard q as a matched pair if f ( q ) &gt; X  ,where  X  is obtained from experiments. This  X  allows the classifier to achieve the maximum F measure [16] in the cross validation.
After matched instance pairs are found through the above process, an index storing data relation information among peers is updated by adding information about these pairs. This index reveals the semantic relations of the data across the peers. It is therefore possible to query related information from more than one peer. Experiments are conducted to test the effectiveness of ontology data matching. Data related to the university setting (Professors, Publications, and etc) are col-lected from five different sources over the Web. As these existing data are in various formats, data conversion has been performed to make them aligned to a backbone ontology schema that describes the university setting. Totally there are 453 instances in the data set. After manua lly checking these instances, 136 in-stances are found to match with each other. Instance pairs including matched and unmatched pairs are then generated from these tagged instances. The SVM light package [17] is used in our experiments. 2 0 random experimental trials are con-ducted. For one trial we split the pair set into two folds randomly, one for train-ing, the other for testing and then rever se. Traditional measurements such as precision , recall and Fmeasure [16] are used for evaluation. We record the max-imum F measure achieved in each trial and it s corresponding recall and precision. The overall results, shown in table 1, are obtained by averaging all these trials. The first row indicates the different methods of similarity measurements (or their combinations) used in creating the feature vectors.  X  X NTO X  indicates the fea-tures related to ontology (CD, CS) are used. Overall, the method that explores the ontology features yields the best result.
 This paper proposes a framework that supports the generation and matching of ontology data in a peer-to-peer environment. It helps users generate ontology data in two ways. Besides data generation, the issue of ontology data matching in the peer-to-peer environment is also addressed. Experiments show that the proposed matching method which explores the ontology features outperforms other traditional methods. With a matching process that employs this method in the framework, ontology data across peers can be interrelated to offer better information services.

Future work includes the refinement of the data matching process and the design of particular query services based on interrelated ontology data after matching. Since the ontology data matching method can not completely guar-antee correct decisions, it is desirable t o incorporate peer in teraction to correct them. Given the interrelated data across d ifferent peers, particular query or rea-soning services will be designed to take advantage of them.

