 Joint pattern alignment attempts to remove from an ensemble of patterns the effect of nuisance transformations of a systematic nature. The aligned patterns have then a simpler structure and can be processed more easily. Joint pattern alignment is not the same problem as aligning a pattern to another; instead all the patterns are projected to a common  X  X eference X  (usually a subspace) which is unknown and needs to be discovered in the process.
 Joint pattern alignment is useful in many applications and has been addressed by several authors. Here we only review the methods that are most related the present work.
 Transform Component Analysis [7] (TCA) explicitly models the aligned ensemble as a Gaussian linear subspace of patterns. In fact, TCA is a direct extension of Probabilistic Principal Component Analysis (PPCA) [10]: Patterns are generated as in standard PPCA and additional hidden layers model the nuisance deformations. Expectation-maximization is used to learn the model from data which result in their alignment. Unfortunately the method requires the space of transformations to be quantized and it is not clear how well the approach could scale to complex scenarios. Image Congealing (IC) [9] takes a different perspective. The idea is that, as the nuisance deforma-tions should increase the complexity of the data, one should be able to identify and undo them by contrasting this effect. Thus IC transforms the data to minimize an appropriate measure of the  X  X om-plexity X  of the ensemble. With respect to TCA, IC results in a lighter formulation which enables addressing more complex transformations and makes fewer assumptions on the aligned ensemble. An issue with the standard formulation of IC is that it does not require the aligned data to be a faithful representation of the original data. Thus simplifying the data might not only remove the nuisance factors, but also the useful information carried by the patterns. For example, if entropy is used to measure complexity, a typical degenerate solution is obtained by mapping all the data to a constant, which results in minimum (null) entropy. Such solutions are avoided by explicitly regularizing the transformations, in ways that are however rather arbitrary [9].
 One should instead search for an optimal compromise between complexity of the simplified data and preservation of the useful information (Sect. 2). This approach is not only more direct, but also conceptually more straightforward as no ad hoc regularization needs to be introduced. We illus-trate some of its relationship with rate-distortion theory (Sect. 2.1) and information bottleneck [2] (Sect. 2.2) and we contrast it to IC (Sect. 2.4).
 In Sect. 3 we specialize our model to the problem of image alignment as done in [9]. For this case, we show that the new model has the same computational complexity of IC (Sect. 3.1). We also show that a Gauss-Newton based algorithm is possible, which is useful to converge quickly during the final stage of the optimization (Sect. 3.2; in a similar context a descent based algorithm was introduced in [1]). In Sect. 4 we illustrate the practical behavior of the algorithm, showing how the complexity-distortion compromise affects the final solution. In particular, our results compare favorably with the ones of [9], with the added simplicity and other benefits, such as noise suppression. We formulate joint pattern alignment as the problem of finding a deformed pattern ensemble which is simpler but faithful to the original data. This is similar to a lossy compression problem [5, 4, 3] and is in fact equivalent to it in some cases (Sect. 2.1).
 A pattern (or data) ensemble x  X  X  is a random variable with density p ( x ) . Similarly, an aligned ensemble or alignment y  X  X  of the ensemble x is another variable y that has conditional statistic p ( y | x ) . We seek for an alignment that is  X  X impler X  than x but  X  X aithful X  to x . The complexity R of the alignment y is measured by an operator R = H ( y ) such as, for example, the entropy of the random variable y (but we will see other options). The cost of representing x by y is expressed by a distortion function d ( x, y )  X  R + and the faithfulness of the alignment y is quantified as the expected distortion D = E [ d ( x, y )] .
 Consider a class W of deformations w : X X  X  acting on the patterns X . In order for the alignment y to factor out W we consider a distortion function which is invariant to the action of W ; in particular, given a base distortion d 0 ( x, y ) , we consider the deformation invariant distortion Thus an aligned pattern y is faithful to a deformed pattern x if it is possible to map y to x by a nuisance deformation w .
 Figuring out the best alignment y boils down in optimizing p ( y | x ) for complexity and distortion. However, this require trading off complexity and distortion and there is no unique way of doing so. The distortion-complexity function D ( R ) gives the best distortion D that can be achieved by alignments of complexity R . All such distortion-optimal alignments are equally good in principle, and it is the application that poses an upper bound on the acceptable distortion.
 D ( R ) can be computed by optimizing the distortion D w.r.t. p ( y | x ) while keeping constant the complexity R . However it is usually easier optimize the Lagrangian whose optimum is attained where the derivative of D ( R ) is equal to  X   X  . Then by varying  X  one spans the graph of D ( R ) and finds all the optimal alignments for given complexities. 2.1 Relation to rate-distortion and entropy constrained vector quantization If one chooses the mutual information I ( x, y ) as complexity measure H ( y ) in eq. (1), then (1) becomes a rate-distortion problem and the function D ( R ) a rate-distortion function [5]. The for-mulation is valid both for discrete and continuous spaces X , but yields to a mapping p ( y | x ) that is genuinely stochastic. Therefore the alignment y of a pattern x is in general not unique. This is because in rate-distortion y is an auxiliary variable used to derive a deterministic code for long sequences ( x 1 , . . . , x n ) of data, not for data x in isolation.
 In contrast, entropy constrained vector quantization [4, 3] assumes that y is finite (i.e. that it spans a finite subset of X ) and that it is functionally determined by x (i.e. y = y ( x ) ). Then it measures the complexity of y as the (discrete) entropy H ( y ) . This is analogous to a rate-distortion problem, except that one searches for a  X  X ingle letter X  optimal coding y of x rather than an optimal coding even if the ensemble x is continuous. 2.2 Relation to information bottleneck Information Bottleneck (IB) [2] is a special rate-distortion problem in which one compresses a variable x while preserving the information carried by x about another variable z , representing the task of interest. In this sense IB is similar to the idea proposed here. By designing an appropriate distribution p ( x, z ) it may also be possible to obtain an alignment effect similar to the one we seek here. For example, if W is a group of transformations, one may define z = z ( x ) = { w ( x ) : w  X  W} , for which z is indifferent exactly to the deformations w of x . 2.3 Alternative measures of complexity Instead of the entropy H ( y ) or the mutual information I ( x, y ) we can use alternative measures of complexity that yield to more convenient computations. An example is the averaged-per-pixel entropy introduced by IC [9] and discussed in Sect. 3. Generalizing this idea, we assume that the aligned data y depend functionally on the patterns x (i.e. y = y ( x ) ) and we express the complexity ensemble.
 Distortion and entropies are estimated empirically and non-parametrically. Concretely, given an ensemble x 1 , . . . , x K  X  X  of patterns, we recover transformations w 1 , . . . , w K  X  X  and aligned patterns y 1 , . . . , y K  X  X  that minimize ming (discrete case) or by a Parzen estimator [6] with Gaussian kernel g  X  ( y ) of variance  X  (contin-uous case 1 ), i.e. 2.4 Comparison to image congealing In IC [9], given data x 1 , . . . , x K  X  X  , one looks for transformations v : X X  X  , x 7 X  y such that the transformations enable to do so, one can minimize the entropy by mapping all the patterns to a constant; to avoid this one considers the regularized cost function where R ( v ) is a term penalizing unacceptable deformations. Compared to IC, in our formulation: I The distortion term E [ d ( x, y )] substitutes the arbitrary regularization R ( v ) . I The aligned patterns y are not obtained by deforming the patterns x ; instead y is obtained as a simplification of x within an acceptable level of distortion. This fact induces a noise-cancellation effect (Sect. 4).
 I The transformations w can be rather general, even non-invertible. IC can use complex trans-formations too, but most likely these would need to be heavily regularized as they would tend to annihilate the patterns. We apply our model to the problem of removing a family of geometric distortions from images. This is the same application for which IC [7] was proposed in the first place.
 x  X   X   X  R 2 and with range in [0 , 1] . The images may be affected by parametric transformations w (  X  ) = w (  X  ; q i ) : R 2  X  R 2 , so that eters of the transformation w i (for example, w i might be a 2-D affine transformation y = Lx + l and q i the vector q = [ L 11 L 21 L 12 L 22 l 1 l 2 ] ).
 bilinear interpolation and zero-padding are used. Therefore the symbol T i ( w i x ) really denotes the quantity where A ( x ; w i ) is a row vector of mixing coefficients determined by w i and and the interpolation We will also use the notation w i  X  T i = A ( w i ) T i where the left hand side is the stacking of the x  X   X  .
 T ( w i x )) 2 . The complexity of the aligned ensemble T ( y ) , y  X   X  is computed as in Sect. 2.3 by projecting on the image pixels and averaging their entropies (this is equivalent to assuming that the in Sect. 2.3). The complexity of a pixel is thus Finally the overall cost function is obtained by summing over all pixels and averaging over all images:
L ( w 1 , . . . , w K , T 1 , . . . , T K ) = 3.1 Basic search In this section we show how the optimization algorithm from [7] can be adapted to work with the new formulation. This algorithm is a simple coordinate maximization in the dimensions of the search space: This algorithm is appropriate if the dimensionality of the parameter vector q is reasonably small. Here we consider affine transformations for the sake of the illustration, so that q is six-dimensional. In (1.) and (2.) estimating the probabilities p ( T i ( y )) and the cost function final result will be refined by Gauss-Newton as explained in the next Section), we bypass this invertible 3 . Eventually all we do is substituting the regularization term P i R ( v i ) of [9] with the expected distortion Note that warping and un-warping the image I i is a lossy operation even if w i is bijective because the transformation, applied to digital images, introduces aliasing. Thus the new algorithm is simply avoiding those transformations w i that would introduce excessive loss of fidelity. 3.2 Gauss-Newton search With respect to IC, where only the transformations w 1 , . . . , w K are estimated, here we compute the templates T 1 , . . . , T k as well. While this might be not so important when a coarse approximation to the solution has to be found (for which the algorithm of Sect. 3.1 can be used), it must be taken into account to get refined results. This can be done (with a bit of numeric care) by Gauss-Newton (GN). Applying Gauss-Newton requires to take derivatives with respect to the pixel values T i ( y ) . We exploit the fact that the variables T ( y ) are continuous, as opposed to [9].
 We still process a single image per time, reiterating several times across the whole ensemble { I 1 ( x ) , . . . , I K ( x ) } . For a given image I i we update the warp parameters q i and the template T i simultaneously. We exploit the fact that, as the number K of images is usually big, the density p ( T ( y )) does not change significantly when only one of the templates T i is being changed. There-fore p ( T ( y )) can be assumed constant in the computation of the gradient and the Hessian of the cost function (3). The gradient is given by  X  X  in Sect. 3 and  X  y =  X  ( z  X  y ) is the 2-D discrete delta function centered on y , encoded as a vector. The approximated Hessian of the cost function (3) can be obtained as follows. First, we use the Gauss-Newton approximation for the derivative w.r.t. the transformation parameters q i We then have Figure 1: Toy example. Top left. We distort the patterns by applying translations drawn uniformly from the 8-shaped region (the center corresponds to the null translation). Top. We show the gradient based algorithm while it gradually aligns the patterns by reducing the complexity of the alignment y . Dark areas correspond to high values of the density of the alignment; we also superimpose the trajectory of one of the patterns. Unfortunately the gradient based algorithm, being a local tech-nique, gets trapped in two local modes (the modes can however be fused in a post-processing stage). Bottom. The basic algorithm completely eliminates the effect of the nuisance transformations doing a better job of avoiding local minima. Although for this simple problem the basic search is more effective, on more difficult scenarios the extra complexity of the Gauss-Newton search pays off (see Sect. 4). sion and D 2 the analogous operator for the second dimension. The second term of the last equation gives a very small contribution and can be dropped.
 The equations are all straightforward and result in la linear system template T ( y ) , y  X   X  . While this system is large, it is also extremely sparse an can be solved rather efficiently by standard methods [8]. The first experiment (Fig.1) is a toy problem illustrating our method. We collect K patterns x i , by drawing M i.i.d. samples from a 2-D Gaussian distribution and adding a random translation w i  X  R 2 to them. The distribution of the translations w i is generic (in the example w i is drawn uniformly from an 8-shaped region of the plane): This is not a problem as we do not need to make all the available samples { y ji , j = 1 , . . . , M, i = 1 , . . . , K } .
 In the second experiment (Fig. 2) we align hand-written digits extracted from the NIST Special Database 19. The results (Fig. 3) should be compared to the ones from [9]: They are of analogous quality, but they were achieved without regularizing the class of admissible transformations. Despite this, we did not observe any of the aligned patterns to collapse. In Fig. 4 we show the effect of choosing different values of the parameter  X  in the cost function (3). As  X  is increased, the alignment complexity is reduced and the fidelity of the alignment is degraded. By an appropriate choice of  X  , the alignment can be regarded as a  X  X estoration X  or  X  X anonization X  of the pattern which abstracts from details of the specific instance.
 Figure 2: Basic vs GN image alignment algorithms . Left. We show the results of applying the basic image alignment algorithm of Sect. 3.1. The patterns are zeroes from the NIST Special Database 19. We show in writing order: The expected value E [ T ( y )] ; the per-pixel entropy H ( T ( y )) (it can diagram as the algorithm minimizes the function D +  X R (in green we show some lines of constant The algorithm achieves a significantly better solution in term of the cost function (3). Moreover GN converges in only two sweeps of the dataset, while the basic algorithm after 10 sweeps is still slowly moving. This is due to the fact that GN selects both the best search direction and step size, resulting in a more efficient search strategy.
 Figure 3: Aligned patterns. Left. A few patterns from NIST Special Database 19. Middle. Basic algorithm: Results are very similar to [9], except that no regularization on the transformations is used. Right. GN algorithm: Patterns achieve a better alignment due to the more efficient search strategy; they also appear to be much more  X  X egular X  due to the noise cancellation effect discussed in Fig. 4. Bottom. More examples of patterns before and after GN alignment. IC is a useful algorithm for joint pattern alignment, both robust and flexible. In this paper we showed that the original formulation can be improved by realizing that alignment should result in a simplified representation of the useful information carried by the patterns rather than a simplification of the patterns. This results in a formulation that does not require inventing regularization terms in order to prevent degenerate solutions. We also showed that Gauss-Newton can be successfully applied to this problem for the case of image alignment and that this is in some regards more effective than the original IC algorithm. Figure 4: Distortion-complexity balance. We illustrate the effect of varying the parameter  X  in (3). (a) Estimated distortion-complexity function D ( R ) . The green (dashed) lines have slope equal to  X  and should be tangent to D ( R ) (Sect. 2). (b) We show the alignment T ( w i x ) of eight patterns (rows) as  X  is increased (columns). In order to reduce the entropy of the alignment, the algorithm  X  X orgets X  about specific details of each glyph. (c) The same as (b), but aligned.
 Acknowledgments We would like to acknowledge the support of AFOSR FA9550-06-1-0138 and ONR N00014-03-1-0850.
 [1] P. Ahammad, C. L. Harmon, A. Hammonds, S. S. Sastry, and G. M. Rubin. Joint nonparametric [2] K. Branson. The information bottleneck method. Lecture Slides, 2003. [3] J. Buhmann and H. K  X  uhnel. Vector quantization with complexity costs. IEEE Trans. on Infor-[4] P. A. Chou, T. Lookabaugh, and R. M. Gray. Entropy-constrained vector quantization. In 37, [5] T. M. Cover and J. A. Thomson. Elements of Information Theory . Wiley, 2006. [6] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification . Wiley Inerscience, 2001. [7] B. J. Frey and N. Jojic. Transformation-invariant clustering and dimensionality reduction using [8] G. H. Golub and C. F. Van Loan. Matrix Computations . The Johns Hopkins University Press, [9] E. G. Learned-Miller. Data driven image models through continuous joint alignment. PAMI , [10] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of The
