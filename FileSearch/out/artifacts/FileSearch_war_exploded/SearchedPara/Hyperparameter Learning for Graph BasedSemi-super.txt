 min-cut [3], harmonic energy minimization [11], and spectr al graphical transducer [8]. the spectral transformation of the graph Laplacian [6, 10].
 aim here is to get confident labeling of the data by the algorit hm. results show that learning the hyperparameters by minimizi ng the LOO loss is effective. Suppose we have a set of labeled data points { ( x only consider binary classification, i.e., y a set of unlabeled data points { x dimensionality of input feature vectors is m . 2.1 Graph Based Classification Algorithms mizes: where the nonnegative w y constraint f labels can be written neatly as: where f is the submatrix of D associated with unlabeled data. P , D  X  1 W . W are defined by: The solution (2) has a number of interesting properties poin ted out by [11]. All f data points. Define the probability of transferring from x normalization of w labeled point (absorbing boundary). Then f function and its minimizer is known to be harmonic: f weighted by p independent of w Finally, to translate the soft labels f f . P n the score of being negative. Suppose there are r labeled data, then we classify x 2.2 Basic Hyperparameter Learning Algorithms One of the simplest parametric form of w where x different  X  the hyperparameters  X  P with  X  P = U + (1  X  ) P , where  X  [0 , 1) , and U is the uniform matrix with U 3.1 Formulation and Efficient Calculation dation error. Suppose we hold out a labeled example x labeled and unlabeled examples. Making use of the result in ( 2), the soft label for x first component of f t Here, the value of f t If hope that f t where h To minimize Q , we use gradient-based optimization methods. The gradient is: using matrix property d X  X  1 =  X  X  X  1 ( d X ) X  X  1 . Denoting (  X  t ) &gt; , h 0 and noting  X  P =  X  U + (1  X   X  ) P, we have Since in both P t to where  X  can be U or L .  X  X  call it leave-one-out hyperparameter learning (LOOHL).
 Algorithm 1 na  X   X ve form of LOOHL terms in the cost by means of using matrix inversion lemma and careful pre-computation. One part of the cost, O ( lu 3 ) , stems from inverting I  X   X  P t (5). We note that for different t , I  X   X  P t two vectors  X ,  X   X  R u +1 such that I  X   X  P t 1 We only need to invert I  X   X  P t new total complexity related to matrix inversion is O u 3 + lu 2 . detail, we have: The crucial observation is the existence of  X  to gradient calculation.
 Algorithm 2 Efficient algorithm to gradient calculation Letting sw and  X  All  X  n 2  X  m + u 3 ) . The space cost is mild at O ( n 2 + n  X  m ) . graph learning process.
 Two degenerative graphs are shown in Figure 1. In example (a) , the points with the same x opposite class which has the same x push 1 / X  graph will effectively split into six disconnected sub-gra phs, each sharing the same x classified. One way to prevent such degenerate graphs is to pr event 1 / X  e.g., with a regularizer such as P be far smaller than 1 / X  In such a case, the regularizer P bandwidth P intentionally added to the datasets.
 and minimize the leave-one-out loss plus P different possible values. We run with different  X   X  and set all initial  X  by using a Gaussian prior with mean  X   X  1 and minimizing Q + C P to  X  d and  X  simultaneously. demonstration of LOOHL X  X  efficacy is not affected.
 L length 1. The initial common bandwidth and smoothing factor in MinEnt are selected by five fold cross
Loo loss N ormal , (2 r + )  X  1 X and there are r to the smallest objective function value for use in cross val idation testing. The final C points. The optimization solver we use is the Toolkit for Adv anced Optimization [2]. 1. LOOHL generally outperforms 5-CV and MinEnt. Both LOOHL+ Thrd and LOOHL+CMN outperform 5-CV and MinEnt (regardless of Thrd or CMN) on all datasets except thrombin and ionosphere, where either LOOHL+CMN or LOOHL+Thrd finally pe rforms best. be large. In other words, the difference between LOOHL+CMN a nd LOOHL+Thrd, compared with how well the graph is learned by LOOHL.
 racy and LOOHL+CMN does not offer much improvement then or ev en hurts performance due to any need to post-process by using class ratio information. MinEnt+CMN. Most of the time, MinEnt+CMN performs significa ntly better than MinEnt+Thrd, Figure 3: Accuracy of Ionosphere in percentage vs. number of labeled data.
 true for probe versions of the datasets. Figure 4 shows the co mparison. algorithms are designed and the empirical result is encoura ging. Acknowledgements tralian Research Council.

