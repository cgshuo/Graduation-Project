 1. Introduction
Recognizing Textual Entailment (RTE) is based on the task of deciding, given two text fragments, whether the meaning of ( Dagan &amp; Glickman, 2004 ). This area has relevance to a number of Natural Language Processing (NLP) based tasks such as Question Answering ( Harabagiu &amp; Hickl, 2006 ), Summarization ( Lloret, Ferr X ndez, Munoz, &amp; Palomar, 2008 ) and Machine ing in 2005, which ran initially as PASCAL RTE challenges but since RTE4 have become NIST challenges. Each RTE challenge provided an annotated training and test set which allowed for a common form of standard benchmarking by the participants.
The classical identification of entailment is considered as a form of logical entailment. However, the usual consideration is that textual entailment between a text (denoted by T and hypothesis (denoted by H ) may hold based on human judgement, even if this inference could not be shown as a logical entailment based on logical reasoning and computational semantics eral simple entailment rules whereby a rule describes a directional semantic inference between two predicates ( Bernant, Da-gan, Goldberger, &amp; Adler, 2012 ). Such rules can of course assist in solving the general problem of textual entailment.
There are many different approaches to this task and the use of various NLP components have included lexical similarity measures, anaphora resolution, paraphrasing, syntactic graph alignment, named entity recognition, semantic parsing and logical inference based on model theoretic approaches. Various RTE challenges have depended on one or more the aforemen- X  tioned processes. Resources have included WordNet ( Fellbaum, 1998 ), DIRT ( Lin &amp; Pantel, 2001 ) and other forms of world knowledge such as Wikipedia. There has been strong focus on considering the process of entailment as a Machine Learning (ML) classification task, and various approaches have been considered to find appropriate feature value vector representa-tions for the entailment pair, e.g. in RTE3, 13 out of 26 teams considered some form of Machine Learning (ML) classification.
Over the course of the challenges there has been a wide range of accuracies. The most successful approaches have reached impressive levels of accuracy. For example, Hickl and Bensley (2007) achieved 80% accuracy on the RTE3 test set, however this was achieved as a two stage potentially iterative process involving being able to select the best discourse commitments (a set of simple propositions that either the hypothesis or text show to be true) and applying entailment classification to the best choice of these aligned commitments. This process requires the use of numerous linguistic techniques and requires both the use of finite state transducers and machine learning.

In general, accuracies for the different RTE challenges have ranged from 50% to 60% on RTE1 (17 submissions), from 53% to 75% on RTE2 (23 submissions), from 49% to 80% on RTE3 (26 submissions) and from 45% to 74% on RTE4 (26 submissions) and from 54% to 73% for RTE5 (21 submissions) based on an evaluation of a test set ( Bentivogli, Dagan, Dang, Giampiccolo, &amp;
Magnini, 2009 ). However, when one considers a one pass linguistic process of feature extraction and machine learning based classification, the highest accuracy achieved was at most 64% for RTE2 and 67% for RTE3. achieved by more intricate processes which require more than one stage of linguistic processing or were not based on machine ticipants were provided with a training set. They were not constrained to only considering the training data provided by the given challenge and usually allowed for consideration of data from previous challenges, which was requisite in the case of
RTE4. The data sets were always independent of each other, the only similarity was data drawn from a number of natural lan-guage processing applications such as Question Answering (QA), Information Retrieval (IR), Information extraction (IE) and Doc-ument Summarization (SUM). There have been subsequent challenges after RTE5 but they have been more specialized concerning the specific task being evaluated with regards to a given application setting. The IBM Watson deep QA system pro-vided state of the art performance on the RTE6 challenge ( Ferrucci, 2012 ) which is part of an Update Summarization task. How-previous challenges involving classification. In addition, the latter system depends on the automatic construction of a semantic knowledge base, PRISMATIC from a large number (30 GB) of factual documents ( Fan, Kalyanpur, Gondek, &amp; Ferrucci, 2012 ). Ta-ble 1 shows examples of true and false entailments drawn from each of the NLP applications present in the RTE2 development data.

We focus on a one-stage process which does not require any computationally intensive steps, and which provides a sim-ple feature extraction to capture similarity/dissimilarity features between the entailment text and hypothesis. The linguistic processes to facilitate this require only tokenization, tagging and dependency parsing. The only knowledge resource that we consider is the use of WordNet. Based on such feature vectors, we wish to assess what is the best performing classifier or com-method provides the best generalization performance as seen by an evaluation over a range of data sets drawn from the var-ious RTE1 to RTE5 challenges.

The use of supervised classifiers in entailment has been prevalent but they generally focus on only one method of which decision trees or support vector machines are often a popular choice. One area that has received little attention is the appli-cation of ensemble learning which is the central focus of this paper. There has been some usage of ensemble based methods in previous challenges (for example Ferr X s and Rodr X guez (2007) applied AdaBoost whereas Kozareva and Montoyo (2006) considered Stacking and Voting). However, their utilization has been based on little or no justification and no thorough con-sideration has been made as to a choice of ensemble method and to their effectiveness. Ensemble learning refers to the pro-cess by which multiple learning machines are trained and their outputs combined, treating them as a committee of decision makers or predictors. The principle is that the committee decision, with individual predictions combined appropriately, should have better overall accuracy, on average, than any individual committee member. Numerous empirical and theoret-ical studies have demonstrated that ensemble models very often attain higher accuracy than single models ( Brown, 2010 ).
The generalization error of a classifier can be decomposed into a bias term, a variance term and an intrinsic noise term ( Koh-avi &amp; Wolpert, 1996 ). The bias measures the expectation of how close the learning algorithm X  X  average prediction matches a target whereas the variance measures how much the prediction varies per data set from the average over all possible data sets. Mechanisms for ensemble learning usually concentrate on reducing either the bias and/or variance in the generalization sufficient condition for an ensemble to be more accurate than any of its members is that it consists of diverse and accurate models ( Hansen &amp; Salamon, 1990 ). Two models are considered diverse if they are not correlated in their errors and accurate if the classification accuracy is better than random guessing ( Kuncheva, 2004 ). There has been numerous metrics proposed for the measure of diversity ( Brown, Wyatt, Harris, &amp; Yao, 2005b ). There are two general stratagems to ensemble learning methods: the first is concerned with generating base models where only one learning algorithm is employed ( homogeneous learning ) and the second is based on methods where different learning algorithms are applied to generate the base models ( heterogeneous learning ). In this paper, we consider both approaches and assess empirically which approach is best suited to the problem of entailment classification. The paper consists of the following sections, Feature Extraction, Ensemble Methods,
Experimental Evaluation and Analysis and Conclusions. 2. Feature extraction
The process of extracting features was influenced primarily by various lingustical features considered by Inkpen, Kipp, and Nastase (2006) for the RTE2 data challenge. Numerous other studies have focused on the extraction of similarity/dissim-features based on linguistic processing is immense, and consequently two different studies never focus on the exact same set of features. We chose this approach primarily as it allowed for a computationally lightweight extraction process with a rea-sonable level of accuracy as shown for RTE2 test data set of 58%. We extended this feature extraction process to allow for other features that we considered likely to be of relevance to the entailment classification process. Features that we chose required the application of linguistic processing mechanisms including tokenization, tagging, parsing and WordNet but did not require more intricate mechanisms such as graph alignment.

By utilizing such a method, we were able to evaluate a diverse range of single and ensemble based learning methods. Fea-ture extraction is based on the following NLP based pipeline utilizing entailment pairs shown in Fig. 1 . Features are either nominal or numeric (which are identified by the num prefix). Features are categorized into a number of broad categories: lexical, similarity, relational, and semantic. Table 2 summarizes the various features. Note that a number of ordinal features have a numeric counterpart. In the case that a feature is nominal, we indicate its potential choice of values as a set of values where  X  X  ...  X  X  indicates that the maximum value is dependent on the data set and as such is open-ended. However, as shown for the features extracted for the RTE2-dev in Table 3 , the range of possible values is not high. 2.1. Lexical features
A number of features are identified as counts in both absolute and normalized form (normalized relative to the hypoth-esis). These are the number of content words (words excluding stop words) in common, the number of stop words in com-mon ( stopWordOverlap , numStopWordOverlap ), the number of words in common ( wordOverlap , numWordOverlap ) the number of verbs in common ( verbOverlap , numVerbOverlap ) and nouns in common ( nounOverlap, numNounOverlap ). A num-ber of nominal features have values indicating whether both the text and hypothesis share a negation polarity context mod-ifier ( MacCartney, Grenager, de Marneffe, Cer, &amp; Manning, 2006 ). These include simple negation (not), downward-monotone quantifiers (no, few), restricting prepositions (without, except) and superlatives (e.g. tallest). 2.2. Similarity features
The features in this categorization are based on the usage of WordNet and in the case of one feature, distributional sim-ilarity and corpus statistics ( Corley and Mihalcea, 2005 ). The WordNet based features include synonym match, antonym match and meronym match and the use of verb entailment relations: verb cause and verb entailment. These features are expressed in both absolute and normalized form. Distributional similarity is based on utilizing pre-computed information content files from the British National Corpus provided by the WordNet::Similarity project ( Pedersen et al., 2004 ). The sim-ilarity between T and H is calculated based on Eq. (1.1) where maxSim is the maximum similarity between a word w e H and any word w in T and idf ( w ) is the inverse document frequency for the word. We calculated the inverse document frequency of terms based on an analysis of the NYTimes annotated corpus ( Sandhaus, 2008 ), which is a large corpus with just over 1.8 million documents.
If sim ( T , H )&gt; similarity_threshold , we set the feature similarity flag distSim = 1 (0 otherwise). 2.3. Relational features
The number of skip bigrams in common to the text and hypothesis and the number of skip bigrams consisting of noun or verbs only, both in normalized form, were identified. A skip bigram is any pair of words in their sentence order, allowing for arbitrary gaps between the pair.
 For example, given the entailment pair, with text, T1 and the hypothesis, H1 T1  X  X  X uerrillas killed a peasant in the city of Flores. X  X 
H1  X  X  X uerrillas killed a civilian. X  X  a X  X ,  X  X  X ill civilian X  X ,  X  X  X  civilian X  X  X .

Based on dependency parsing, we considered each grammatical dependency pair obtained based on the identified depen-dency relations in the text and hypothesis. The dependency relation is expressed by the tuple: relation head /POShead modifier /POSmodifier where POShead and POSmodifier denote the part of speech of the head and modifier element in the relation respectively.
In computing relation overlap, we use the tuple as it is, and also as a dependency pair (Head, Modifier), to cover the cases when the same lemma appear in different grammatical relations, possibly also with different parts of speech. An example of a full relation in H1 expressed as a tuple is: nsubj kill/VBD guerrilla/NNPS which has corresponding dependency pair (kill, guerilla).
 Tuples and dependency pairs are used to compute the following four features: Absolute number of overlapping pairs between the test and the hypothesis.

Normalized number of dependency pair overlap (absolute number divided by the number of tuples generated for the hypothesis).
 Absolute number of overlapping relations.
 Normalized number of overlapping relations.

A special feature numNegateVerbs based on dependency relations is determined by a normalized count of negated verbs that appear only in the hypothesis and not in the text. 2.4. Semantic features
Semantic features are derived based on two features: the first feature determines the number of named entities that are shared in common between the text and hypothesis. The second measure is based on the use of semantic role labeling. Based on an adapted approach derived from Wang, Li, Zhu, and Ding (2008) , we calculate a numeric semantic role similarity based on shared semantic roles, where a semantic role has form Arg0, Arg1, etc. (based on Propbank semantic roles). In our ap-proach, we only considered one predicate in the sentence, which is the verb below the root node in the dependency tree.
The verb predicate and its arguments with semantic role labels constitute the verb frame for this verb. Given text T and hypothesis H , let R h be the semantic roles in H (associated with the main predicate) and R ciated with the main predicate). Let { r 1 , r 2 , ... , r predicate. T t  X  r i  X  X f t t i 1 ; ... ; t t i j T where
The overall semantic role similarity is given by:
Two argument terms are considered related if based on the WordNet hierarchy, they share either a synonym, hypernym, hyp-onym, holonym or meronym relationship. For example, in the previous entailment pair example, there is one verb frame for
T1 associated with predicate kill and argument term sets T predicate kill and argument term sets T h ( Arg 0) = { guerilla }, T
As a result of rsim ( T t ( Arg 0), T h ( Arg 0)) = 1 and rsim ( T semantic role similarity. 3. Ensemble methods
We considered the following choice of single learning methods, which have been considered in a number of different evaluations for entailment classification in Table 4 . We give the WEKA class name and description of any options that are applicable. For brevity in the text we refer only to the WEKA class name and show option settings in brackets The WEKA realization of support vector machines was based on the sequential minimal optimization (SMO) algorithm, which uses a polynomial kernel by default. From a perspective of heterogeneous learning, the various learning methods have various dif-ferent learning biases, so they are also well suited for heterogeneous learning. As we did not expect any of the entailment 6 different base methods were considered. In the case of the other classifiers, the default settings were utilised. 3.1. Homogeneous learning mechanisms
We considered a range of ensemble homogenous approaches, many of which depend on some form of feature or instance data randomization. The devised mechanisms are generally well suited to high variance unstable learners, of which decision trees are a prime example of Nearest neighbours, Na X ve Bayes and Support Vector machines tend to be stable but biased independent bootstrap replicates to generate training sets. A classifier is constructed for each of these training sets and aggregates their classifications by a simple majority Vote in the final decision combination. In the random subspace method (RSM) ( Ho, 1998b ), classifiers are constructed in random subspaces of the data feature space. These classifiers are usually combined by simple majority Voting in the final decision combination. Skurichina and Duin (2002) investigated the method in comparison to Bagging for different training set sizes and levels of feature redundancy. They showed that the method is appropriate for small training sets with a high level of redundant features. Garc X a-Pedrajas and Ortiz-Boyer (2008) indicate that some of the feature subsets may lack the discriminatory ability to separate classes but this may be less of an issue in the case of a two class entailment problem.

In Random Forests ( Breiman, 2001 ), which is an ensemble technique specific to decision tree learning, a number of ran-dom trees are grown, where for each node split a random selection is made from a fixed number of attributes. Random tree outputs are also combined using majority Voting. AdaBoost ( Freund &amp; Schapire, 1996 ) has no random elements and grows an ensemble in an iterative fashion where base models are trained on successive reweightings of the training set where the cur-rent weights depend on the performance of the ensemble. The focus on the reweighting is to apply more emphasis on dif-by randomly splitting the feature space into subsets for each classifier and applying principal component analysis to each subset or group of features. The principal components from each subset are assembled to form a rotation matrix which is used to transform the training data for the given base classifier. The mechanism was shown to outperform Random Forest,
Bagging and AdaBoost. Table 5 provides details concerning the various homogeneous methods and the possible flag settings that we utilize. The option I indicating the number of base classifiers in ensemble is not shown, as it is always an option available for each method and in the experiments has the same setting for each method. The ensemble method, Decorate ( Melville &amp; Mooney, 2003 ), is an iterative mechanism based on enhancing diversity into an ensemble by adding base models where for each base model, artificial examples are added to the training set for the models and the model is added to the overall ensemble if the training error is not increased. The ensemble makes predictions in a similar fashion to Voting based on Averaging discussed in the next section. The setting for the E flag which indicates the desired ensemble size is the same setting as for the I flag. 3.2. Heterogeneous learning mechanisms
The most well known heterogeneous learning approach is the meta-modeling approach of Stacking ( Wolpert, 1992 ). The common approach to Stacking as devised by Ting and Witten (1999) in essence follows this process: during training, all base
J ; equal-sized folds, then uses J 1 folds for training and the remaining fold for testing. This process is repeated J times so that each fold is used for testing exactly once, thus generating one prediction for every example in the data set. Each clas-sifier X  X  output is therefore a class probability distribution for every example. The concatenated class probability distributions classifier. After training the meta classifier, the base classifiers are retrained on the complete training data. Ting and Witten (1999) proposed multi-response linear regression as the meta-learner which learns a linear regression model for each class.
Seewald (2002) derived an improved mechanism referred to as StackingC also based on a multi-response regression, with the difference that each linear model uses features based on only the relevant specific class distributions (whereas the ori-ginal approach by Ting and Witten uses all class probabilities for each linear model). We focused on this latter form of Stack-ing. A simpler approach to combining heterogeneous classifiers that we also considered is through Voting (where the WEKA class is referred to as Vote). The default form of Voting in WEKA is based on averaging of individual class probabilities for each base classifier and selecting the class with maximum probability. Due to the heterogeneous nature of the application of Stacking, there has been no detailed theoretical study outlining how and when it is effective, and studies tend to focus on the nature of the combiner and the ensemble set. It is thought though that its effectiveness lies in its ability to reduce bias ( Sharkey, 1996 ). The general consensus on how to form an ensemble suited for Stacking is to employ base learning methods with varying learning biases. 4. Experimental evaluation and analysis
This section describes how experiments were carried out, the data sets utilized, and provides an evaluation of single learning methods, homogeneous learning methods and heterogeneous learning methods. 4.1. Experimental setup
The process of Information extraction and entailment classification is based on the use of embedded GATE ( Cunningham, 2002 ). GATE embedded is an open source object-orientated framework developed in Java to provide embedded language processing functionality in diverse applications. It supports a number of processing resources such as sentence detection, tokenization, tagging and through a plugin, the Stanford dependency parser ( De Marneffe, MacCartney, &amp; Manning, 2006 ).
We utilised an OpenNLP plugin within GATE ( http://opennlp.apache.org/ ) to apply named entity recognition. The possible named entities were drawn from the following set of named entities: {PERSON,NAME,LOCATION,CURRENCY,DATE,TIME}. content, a GATE processing pipeline was constructed. The first pair part of the pipeline performs the appropriate feature extraction for each entailment pair and creates WEKA instance data for each pair. Once feature extraction is complete, the second part of the pipeline provides an evaluation framework which performs an evaluation of a number of single and ensemble based classification methods. Entailment classification is based on the GATE Learning plugin, which supports a small number of WEKA ( Witten &amp; Frank, 2005 ) based classifiers. We modified the latter plugin to allow for the use of any
WEKA classifier including a number of ensemble based methods. Also, where we considered an attribute to be numeric, we identified it in WEKA as a numeric attribute rather than considering all attributes to be nominal (which was the case in the original plugin). The evaluation performs a 1 10 fold cross-validation (CV) for each learning algorithm and calculates the macro-averaged F1-measure (F1 cv ), which is the F1-measure averaged over the 10 folds. For each fold, the recorded F1-measure is the harmonic mean of the precision and recall. The similarity_threshold for the distSim feature was kept at 0.7 for all experiments. Experiments were carried out on a Intel core i5 m480 2.67 GHz with 4G RAM. 4.2. Data sets
The data sets are drawn from the RTE1 to RTE5 challenges. For the purposes of evaluation we consider training and test sets as simply evaluation data sets so that in total we have 9 data sets. Each data set is independent of each other and there is no duplication of any of the entailment pairs across data sets. Table 6 provides details concerning the size of each of the data sets and average number of words in the text and the hypothesis per entailment pair. In the case of comparing single models, we also considered an extended set of datasets, based on 4 additional datasets (RTE1-random, RTE2-random, RTE3-random,
RTE5-random} where each data set consisted of 400 randomly selected instances drawn from a combination of the training and test data for the respective RTE challenge. The additional data sets allowed for a paired t-test with a higher number of degrees of freedom, as described in 4.3. 4.3. Single model comparison
This subsection concentrates on experiments based on single classification models only. Initially, we considered whether the inclusion of semantic features (neOverlap and numSemanticRoleMatch) improved the performance of single models. Our reason for the non-automatic inclusion of these features, was that the additional processing required to extract the features, was slow. This was due to the large level of memory overhead associated with the use of such features, as the requisite mod-els need to be loaded into memory. Table 7 presents the average F1 more features drawn from the set of semantic features { neOverlap , numSemanticRoleMatch }. The average was taken over the 6 base methods for each data set and this was repeated for each of the four different feature sets. We considered if any of the feature set comparisons showed significant improvement in accuracy as measured by comparing the average F1 based on 9 sample points in the case of 9 data sets and 13 for the extended set. This was not shown to be the case either when we considered only the original data sets or when we considered the extended set. As such, we excluded the use of semantic features from further evaluation as we wish to minimize the level of required linguistic processing.
For feature sets excluding semantic features, Table 8 shows the results of the individual based methods, and the average value for the given method, averaged across all data sets. In addition, the maximum value per data set and the index of the method returning the maximum is shown. The maximum accuracy varied quite considerably per data set from a high of 0.53 for RTE1-test and a high of 0.67 for RTE3-dev. Clearly based on averages, NaiveBayes outperforms all other methods. How-ever, it only returned the maximum accuracy in 5 out of 9 the original data sets.

The following table, Table 9 summarizes for how many datasets a given method denoted by a row entry outperforms an-other method under the column heading based on a comparison of the F1 in row b column a is less than or equal to the number of data sets in total (allowing for ties). In brackets is shown for how many data sets was the comparison shown to be statistically significant based on a paired t -test comparison of the F1-mea-sures per cross validation fold (significance level p &lt; 0.05) based on the 10 sample points.

It is clear that the number of data sets where the mean comparison was statistically significant was often far fewer than the number of data sets. This was a consequence of a large standard deviation in the observed F1-measure across folds for each method e.g. the standard deviation in F1-measure for NaiveBayes was 0.091 for the RTE2-dev data set. The large noted variation in standard deviation in F1 across folds for two individual methods means that a paired t -test will not show any statistical difference unless the mean values are very different. (A better comparison may have been based on using a 10 10 cross fold validation where we repeated a 1 10 fold validation 10 times for different stratifications of the data and com-pared the average per each validation. However, 10 10 cross fold validation would have been extremely time-consuming given the number of methods that we investigated). As a consequence, we are only able to draw any conclusions based on statistical tests by comparing the means of F1 cv across all datasets. For all subsequent experiments we focus on this latter measure, as a determiner as to whether one method outperformed another method. The variation in F1 data sets was also more sensitive to statistical comparison due to the much smaller standard deviation in the sample points in comparing means. In addition, it served our intention to measure the generalization capability of a method. This process was similar to the previous comparison involving the inclusion of semantic features in feature sets. This test showed that
NaiveBayes was the only method to statistically outperform all other methods. None of the other methods was shown to outperform any other at a statistically significant level. In addition, it returned the maximum accuracy for 5 out of 9 data sets. 4.4. Homogenous learning
We considered the approach of Random Forests, and the techniques of Rotation Forest, Bagging and AdaBoost as applied to the decision tree learner C4.5 and the hybrid decision tree learner NBTree. We also considered the method of RSM as ap-plied to nearest neighbours in addition to decision trees as this was shown to be beneficial by Ho (1998a) . For each ensemble method we set the flag I to 25 to indicate the presence of 25 base classifiers in the ensemble. There may be further gain in accuracy for a greater size of ensembles but if any gain is to be shown by a particular ensemble method, it should occur in the case of such an ensemble size ( Opitz &amp; Maclin, 1999 ).

An initial evaluation assessing the technique of RSM considered the performance based on the size of random subspace sizes at 20% of all features and 50% of all features (the number of features recommended by Ho (1998b)), as applied to IBk for K = 11 and K = 15, C4.5 and NBTree. Fig. 2 shows the performance of RSM as applied to IBk ( K 11), IBk ( K 15), C4.5 and
NBTree. The accuracy was slightly higher for IBk ( K 11), IBk ( K 15) and J48 for 20% of features than 50% of features (with only RTE1-dev being the exception to this general pattern in the data sets). The comparison for NBtree showed a more mixed level of performance, although the average for 50% of features for NBtree was higher than for 20% of features.
Table 10 shows the average F1 cv for each RSM method, and by way of comparison, the average F1
If the average was statistically significantly greater, the value is highlighted in bold. Table 10 highlights that random selec-tion of features was able to show improvements for both unstable learners such as J48 and NBTree and also for the stable learner IBk.

We evaluated how the other methods applicable to enhancing decision tree learning performed in comparison to random subspacing. Tables 11 and 12 provides a summary of this comparison for J48 and NBTree respectively. An average F1 was significantly higher than the baseline method is shown in bold, whereas an average that was significantly worse is shown in italics. In the case of J48, the average for Bagging and RotationForest was higher than the base method J48 and there were data sets for which either of two methods returned the highest F1-measure, however overall they did not match the performance of random subspacing.
 We surmise that a number of our features may be actually redundant as shown by the relative strength of RSM method.
Skurichina and Duin (2002) indicate that RSM may be beneficial in the presence of a high level of redundant features. The performance of Bagging reflects that the fact that Bagging does not increase the bias in error and may reduce the variance ( Breiman, 1996 ). However, it did not reach the performance level of RSM. Boosting may be particularly weak in the presence of a high number of redundant features although we have no evidence to support this conjecture (it was previously noted however that Boosting is often much weaker than Bagging in the presence of high classification noise where many instances are misclassified ( Dietterich, 2000 )). RotationForest did not improve upon the performance of Bagging and RandomForest returned a weaker performance than the base method. Decorate also improved upon the baseline method but not signifi-cantly. The results for RandomForest were lower than expected based on the previous studies which have shown an improvement in this technique over Bagging ( Breiman, 2001 ).

The variable parameter in RandomForest is the number of randomly chosen features ( K) that are allowed to be set for a split in construction of a decision tree. By default in WEKA this is set to log considered whether the impact of a different setting for the number of attributes had an influence on the performance of
RandomForest, as shown in Fig. 3 . Allowing for some variation per data set, there did not appear to be any major improve-ment in choosing a value different to the default, which is 6 in this study.

The result for RotationForest was also lower than expected. This is a more intricate method than random subspacing and how its performance is impacted by the presence of redundant features has not been considered, nor has a direct comparison to RSM. A key parameter was the size of subsets which by default in WEKA was set to 3. We considered whether allowing for different feature subsets size had a bearing on its performance. This is controlled by two flags G which set the minimum size of a group of features and  X  X  which set the maximum size of a group of features. Fig. 4 did not show a marked improve-ment using a different setting from the default setting of  X  X  G3 H 3 X  X , although for individual datasets there were some improvement in F1 cv .
 4.5. Heterogeneous learning
We considered the five learning methods (IBk ( K 15), NaiveBayes, J48, NBTree, SMO) as the base classifiers for hetero-geneous ensembles. We did not consider IB( K 11) as on average its performance was shown to be very similar to IBk( K 15). The heterogeneous combination method was either StackingC or Vote. The number of folds for cross-validation in Stack-ingC was set at 2. Given the noted improvement shown in the performance of the application of RSM to NBTree, IBk and J48, we also considered ensembles which replaced J48, IB( K 15) and/or NBTree by RSM utilizing the same base method. This led to 8 possible ensemble member sets as shown in Table 13 .

The usage of a particular ensemble member set is denoted by an identifier added as a subscript to the heterogeneous com-biner name. In such a case where RSM was introduced, we are applying a form of ensemble which had 3 levels, where one or more base learners is replaced by the homogeneous ensemble learning mechanism and the overall combination is based on one of the heterogeneous combiners. Tables 14 and 15 show the results for StackingC and Vote, with the ensemble set iden-tifiers denoting the ensemble set in usage. The baseline comparison is based on the ensemble set denoted by the subscript Results did not show any clear benefit in applying RSM to any of the base classifiers in the case of StackingC, as the average was no greater than StackingC a which did not utilize the RSM method. With the exception of StackingC reduced in comparison to StackingC a , but the value was only statistical lower in the case of StackingC
The highest performing approaches of StackingC a and StackingC shown to be statistically lower. StackingC a had a greater accuracy than Na X ve Bayes for three of the datasets and lower in the case of the other six. StackingC b returned a similar performance. In general, it would appear that StackingC could not compensate for the fact that NaiveBayes was significantly better than all the other base methods considered for ensemble formation. We speculate that the application of RSM to base models possibly lead to an overall increase in the bias of a
Stacked ensemble even if for the individual method the variance is reduced, which ultimately reduced the effectiveness of the Stacking method.

Table 15 shows the results of the combination based on Voting. In general, it was seen that a greater accuracy was achiev-able in combining base methods and the highest average accuracy was shown for Vote was higher than NaiveBayes by 0.008, although this difference was not statistically significant. In the latter case, the accuracy was also higher for 7 out of 9 of the data sets. So in the case of Voting, there was some benefit in combining ensembles, which have base methods that have been enhanced using the application of RSM. 5. Discussion
We chose to analyse in more detail the features that were present in one of the RTE data sets and their level of relevancy and redundancy where a feature is relevant if it has a functional dependency to a class and redundant if it is functionally dependent on another feature. We utilized feature selection methods to give an indicator of the levels of redundancy/accu-racy in our chosen feature sets. In general, a feature selection mechanism should select the subset of features which in com-bination, have the maximal dependency on the target class ( Peng, Long, &amp; Ding, 2005 ). In general, this joint distribution measure is hard to calculate accurately. Instead, attribute selection is based on a combination of heuristic search mechanism to generate a subset of features, and an evaluator to assess each attribute within a subset individually and providing an over-all score for the subset. One measure for determining the relevancy of an attribute is to measure its mutual information (or information gain) with the class attribute. Equally, the redundancy of one attribute with respect to another is measured by their mutual information. Attribute ranking can be calculated simply based on an information gain ranking (InfoGain). Info-Gain in WEKA requires a simple ranker search mechanism as attributes are simply ranked based on information gain scores.
Hall &amp; Holmes, (2003) developed a correlation feature selection (CFS) mechanism based on a selection of attributes, which have a high correlation with the class and a low inter-correlation. A different approach to filter based feature selection is based on the application of a classification algorithm to assess the performance of individual subsets of attributes, using cross validation. Such Wrapper mechanisms tend to be more computationally intensive, but in combination with a classifier tend to be more accurate than filter methods such as CFS.

Based on the WEKA Attribute-Relation File Format (ARFF) representation of the processed RTE2-dev data set, we utilized the WEKA explorer tool in order to apply the three described feature selection mechanisms. Search As Na X ve Bayes proved the most effective single learning algorithm, we utilized the latter classifier in the Wrapper based approach (even though this is only optimal for classification if the same method is utilized). Table 16 shows the WEKA settings for running attribute selection for each of the three methods as carried out within the explorer tool. The search mechanism that CFS and Wrapper and can generate all possible single feature expansions. The subset with the highest evaluation based on the evaluator, is chosen and expanded in the same manner by adding single features. If expanding a subset results in no improvement, the search drops back to the next best unexpanded subset and continues from there. In general, the number of possible anism and the possible parameter settings).

In each case the parameter settings are the default ones with the exception of using Na X ve Bayes for the Wrapper based method. Table 17 shows the feature sets selected, when each of the 3 methods was applied. In the case of InfoGain, all fea-tures are selected but are shown in rank order  X  it is possible to set a threshold on how many attributes should be selected based on the threshold but this was not considered. Clearly, both CFS and Wrapper reduce the number of features quite con-siderably indicating that a high number of features are judged either redundant or not relevant. This provides support for the reason that RSM performed well whereas other homogeneous methods were not able to address this issue to the same de-gree. We do not have an indicator from these methods whether an attribute should be considered irrelevant or redundant.
However, it does support our previous view that a number of features are redundant or irrelevant, as the InfoGain ranking showed that 13 attributes have an information gain of less 0.01 and could be deemed irrelevant and allow us to remove these attributes for further consideration.

The implication from this is that in the presence of a high number of such features, that Na X ve Bayes is better able to han-dle such feature sets than any of the other classification approaches. It was noted in a theoretical study by Rish (2001) that
Na X ve Bayes can perform well in the presence of a number of features which are functionally dependent. As mentioned, there has been numerous machine learning based entailment studies with different but possibly overlapping sets of similarity/dis-similarity features. Other RTE studies from past RTE challenges may not indicate the relative strength of Na X ve Bayes but our study is based on many data sets and in this respect is more comprehensive. Of course, the application of feature selection methods could also enhance the performance of the classifier, though it is not guaranteed to do so, and can degrade it, as feature selection is not based on the maximal dependency criteria.

We considered if we applied these methods to the RTE2-dev data set, whether there was any difference in the relative performance of the classifiers, based on the filtered data sets. Table 18 shows the effect on classification accuracy utilizing the WEKA Experimenter tool (note that the accuracies are not exactly the same as recorded previous for no feature selection, as the method for cross validation in the Experimenter tool is not exactly the same as in the GATE based pipeline). Noticeably the Wrapper method improved the performance of all the classifiers; however, the relative performance of the individual methods was not dramatically altered as no individual method statistically outperformed Na X ve Bayes.

The reason that ensemble methods proved ineffective lies with the fact that for ensembles to be effective the individual methods have to be both accurate and diverse ( Hansen &amp; Salamon, 1990 ), where a classifier is considered sufficiently accu-rate if its performance on test data is better than random guessing. As the latter situation generally held true, we assume that the level of diversity was not high in the ensemble. We considered by way of exposition, the case of heterogeneous ensem-bles, given that we expected that, as such ensembles contain very different learning methods, to have high levels of diversity and be more diverse than homogeneous ensembles.

There is a range of possible measures to assess the diversity of an ensemble. The WEKA framework we utilized, does not provide a means of assessing the diversity of ensembles. As an indicator of diversity, we focused on one measure proposed by
Dietterich (2000) which measures the diversity of a pair of classifiers based on their individual classifications over all test cases in the cross validation. Pairwise diversity involved calculating the kappa ( j ) statistic, which indicates the level of agreement between classifiers. A value of j = 0 indicates that the classifiers are statistically independent, whereas j = 1 indi-cates that the classifiers are perfectly correlated.

By considering the average j statistic of one classifier against all other classifiers, we are able to gain an indication of whether an ensemble composed of such classifiers is likely to be diverse. Table 19 shows the average j statistic for the five are very different in their learning biases, was surprising and does explain the lack of significant improvement in accuracy using ensemble methods. We surmise that due to a limited number of relevant and non-redundant features, the individual classifiers are unable to classify difficult entailment instances correctly and consequently show a high level of correlation in instances. If heterogeneous ensembles are not diverse, the implication is that homogeneous ensembles will be even less so, given the lack of relevant and non-redundant features. 6. Conclusions and future work
In this paper, we considered a simple framework for entailment classification based on the supervised learning of feature vectors based on the simple extraction of similarity/dissimilarity features between the text and hypothesis elements of an entailment pair. Based on this framework, we provided a detailed empirical study into what level of performance a user could expect from such a framework, by considering a range of standard single and ensemble based classifiers. We consid-ered five different learning algorithms in this study, which were chosen, based on their distinctness and the fact that previ-ous ensemble studies have shown them to be appropriate for improving their performance.

We showed that a number of homogeneous learning methods were able to enhance the performance of unstable learners and in this respect, RSM proved the most effective. RSM was also able to improve the performance of the stable learner near-est neighbours. The benefit of the RSM method is attributed to the likelihood that potentially a high number of features were not discriminatory in terms of entailment classification and in fact may be redundant. The high level of such features is indic-ative of why other ensemble methods which have shown to improve unstable learners such as decision trees and hybrid decision trees did not show a marked improvement in accuracy in our results.
 However, as measured by the performance across data sets, none of the homogeneous methods were more accurate than
Na X ve Bayes. As Na X ve Bayes is a stable form of learning, none of the homogeneous ensemble mechanisms was suited to enhancing its performance. We considered heterogeneous learners and found that a hybrid mechanism that combined het-erogeneous ensembles using Voting based on averaging as the combination mechanism with the RSM applied to certain unstable base learners, was able to show an improvement over Na X ve Bayes. However, the improvement was neither large nor was it significant. In Section 5 , we attribute the lack of improvement shown by heterogeneous ensembles to a lack of diversity.

Consequently, the study supports the view of Glickman et al. (2005) that  X  X  X he uncertain nature of textual entailment calls different entailment problem. In this respect, combining Na X ve Bayes with slightly weaker methods for entailment did not prove to be advantageous.

As described in Section 5 , the presence of high levels of redundant or irrelevant features limits the level of classification accuracy regardless of which single or combination model approach is adopted. This issue concerning the high level of redundant and irrelevant features was surprising, given that we chose a strong representation of various lexical, syntactic and similarity features, many of which are considered in other studies. This indicates that much larger feature sets need to be considered and an assessment given to the relevancy and redundancy of each feature.

A possible mechanism to improve ensemble performance is to consider much larger initial feature sets and utilize meth-ods that only retain sufficient accurate and diverse base classifiers, trained on feature sets which maximize relevancy and minimize redundancy. This could be achieved through integrating a feature selection method into the choice of base classi-fiers to retain in the ensemble. Given the relative strong performance of Na X ve Bayes, it would also be interesting to pursue an ensemble based mechanism which allowed for its usage, as most methods do not as it is a stable learner. One exception is the approach proposed by Rodr X guez and Kuncheva (2007) who were able to apply an ensemble approach based on the use of random oracle which proved effective with Na X ve Bayes, a mechanism we will consider in future work. A very different ap-proach to entailment focuses on the use of graph alignment mechanisms. We are also interested to see whether a semantic graph approach relying on heuristic functions to judge entailment, could, in combination with machine learning mecha-nisms, allow for a potentially more diverse set of classifiers and a better realization of ensemble combination. Acknowledgments SAP(AG).
 References
