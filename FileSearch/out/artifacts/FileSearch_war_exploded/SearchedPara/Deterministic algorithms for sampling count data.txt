 1. Introduction
Count data serve as the input for an important class of online analytical processing (OLAP) tasks, includ-for further processing. However, the volume of data has become so huge that mining and analysis algorithms that require several passes over the data are becoming prohibitively expensive. Sometimes, it is not even fea-cessed as a stream. For most OLAP tasks, exact counts are not required and an approximate representation is appropriate, motivating an approach called data reduction [4] . A similar trend was observed in traditional database management systems (DBMS) where exact results taking too long led to approximate query answer-ing as an alternative [11,16] .

A general data reduction approach that scales well with the data is sampling. The data stream community lyzing data, the use of random samples can lead to unsatisfactory results. For instance, samples may not accu-rately represent the entire data due to fluctuations in the random process. This difficulty is particularly apparent for small sample sizes and bypassing it requires further engineering.
 The main product of this research consists of two deterministic algorithms, named below Biased-L2 and
DRS, to find a sample S from a dataset D which optimizes the root mean square (RMS) error of the frequency vector of items over the sample (when compared to the original frequency vector of items in D ). Both algo-rithms are a clear improvement over SRS (simple random sample) and other more specialized deterministic sampling algorithms such as FAST [8] and EASE [5] . The samples our algorithms produce can be used as sur-rogate for the original data, for various purposes such as query optimization, approximate query answering represent all the values of all the attributes in the DBMS and one wants to maintain, for each table, a sample which is representative for every attribute simultaneously. We assume here categorical attributes X  X umerical attributes can be discretized, e.g., by using histograms and creating a category for each bucket. In Section 2 we talk about the previous work. Later, in Section 3 we present our sampling algorithms,
Biased-L2, and Deterministic Reservoir Sampling (DRS), for deterministically sampling count data. In Sec-tion 4 we evaluate our algorithms (Biased-L2 and DRS) on several real-world and synthetic datasets, with var-ious criteria and settings. Finally, in Section 5 we finish with the concluding remarks.

Our contributions  X  In this paper, we present two novel deterministic sampling algorithms: Biased-L2 and DRS, to sample count data (tabular or streaming).  X  Both of our algorithms generate samples with better accuracy and quality compared to the previous algo-rithms (EASE and SRS).  X  Our algorithms improve on previous algorithms both in run-time and memory footprint.  X  We perform extensive simulations with synthetic and real-world datasets under various settings, and dem-onstrate the superiority of our algorithms. 2. Related work The survey by Olken and Rotem [22] gives an overview of random sampling algorithms in databases. Sampling is discussed and compared against other data reduction methods in the NJ Data Reduction
Report [4] . In addition to sampling, a huge literature is available on histograms [15] and wavelet decom-positions as data reduction methods, and we do not attempt to survey it here. We note however that sam-pling provides a general-purpose reduction method which simultaneously applies to a wide range of applications. Moreover, the benefits of sampling vs. other data reduction methods are increased with multi-dimensional data: the larger the dimension, the more compact sampling becomes vs., e.g., multi-dimensional histograms or wavelet decompositions [4] . Also, sampling retains the relations and correlations between the dimensions, which may be lost by histograms or other reduction techniques. This latter point is important for data mining and analysis.

Zaki et al. [25] state that simple random sampling can reduce the I/O cost and computation time for asso-ciation rule mining. Toivonen [23] propose a sampling algorithm that generates candidate itemsets using a large enough random sample, and verifies these itemsets with another full database scan. Instead of a static sample, John and Langley [18] use a dynamic sample, where the size is selected by how much the sample rep-resents the data, based on the application. The FAST algorithm introduced by Chen et al. [8] creates a deter-ministic sample from a relatively large initial random sample by trimming or growing a sample according to a and creates a deterministic sample by performing consecutive halving rounds on the sample. EASE algorithm each level in order to be added to the sample. The penalties change based on the decision to accept or reject set. Multiple halving rounds per transaction and the penalty functions used for each round introduces addi-tional complexity to EASE compared to Biased-L2. The Biased-L2 algorithm we present in this paper uses ideas similar to EASE based on discrepancy theory [7] , but samples the dataset without introducing halving rounds and improves on the run-time and memory requirements, as well as the sample quality and accuracy.
Although in this paper we focus on count data that occur mostly in database settings, Biased-L2 algorithm is generic for any discretized data, and in [2] it is applied to sampling geometric point data for range counting applications.

The main difference between DRS and FAST is that DRS keeps a smaller sample in memory, examines each transaction only once, and it is suitable to handle streaming data. DRS algorithm uses a cost function based on RMS distance which is incrementally updated by changes to the sample. In this paper we only give the incremental formulas specific to our case, where the sample size does not change by updates. Additional incremental formulas for various distance functions are presented in [6] . As the sample size can be preset exactly, DRS does not have accuracy problems caused by the halving rounds of EASE. Johnson et al. [19] tice most stream sizes are unknown, this can be best done by allowing the algorithms to dynamically remove transactions from the sample, as in reservoir sampling [24] and DRS.

Gibbons and Matias [12] propose concise sampling and introduce algorithms to incrementally update a sample for any sequence of deletions and insertions. While concise sample dramatically reduces memory foot-print, it works for single attribute sampling, lacking the ability to give any correlation between attributes, which is desirable for multi-dimensional data.

Vitter [24] introduces reservoir sampling, which allows random sampling of streaming data. Reservoir sam-pling produces a sample of quality identical to SRS, but does not examine all the data. Whenever a new record selecting the worst record to evict. Gibbons and Matias [13] use reservoir sampling as a backing sample to keep the histograms up to date under insertions and future deletions.

In [19] different approximation algorithms are discussed including Reservoir Sampling [24] , Heavy Hitters algorithm [21] , Min-Hash Computation [9] and Subset X  X um sampling [10] . Among these we only compare our algorithms with Reservoir Sampling (random sampling in general), since the rest of the algorithms are tailored for specific applications. 3. Deterministic sampling algorithms
In this section we first describe the notation used throughout the paper, and then present our deterministic sampling algorithms: Biased-L2, and Deterministic Reservoir Sampling, respectively in Sections 3.2 and 3.3 . 3.1. Notation
Let D denote the database of interest, d = j D j the number of transactions, S a deterministic sample drawn
T avg denote the average number of items in a transaction, so that dT by a complete item per transaction enumeration).
 at least one transaction j D .A k -itemset is an itemset with k items, and their collection is denoted by I original items. Thus I  X  D  X  X [ k P 0 I k  X  D  X  . The itemsets over a sample S D are I  X  S  X  I  X  D  X  , and I defined similarly.

For a set T of transactions and an itemset A I , we let n ( A ; T ) be the number of transactions in T that contain A and j T j the total number of transactions in T . Then the support of A in T is given by an item is frequent in D (respectively in S ) if its support in D (respectively S ) is no less than t .
The distance between two sets D and S with respect to the 1-itemset frequencies can be computed via the discrepancy of D and S , defined as sample are via the L1-norm or the L2-norm (also called  X  X oot mean square X   X  RMS),
In order to measure the accuracy of the sample S for evaluating frequent itemset mining, as in [5,8] the fol-lowing measure is used: resents the number of itemsets exist in dataset but not in sample, and L ( S ) n L ( D ) the other way around. 3.2. Biased-L2
Biased-L2 algorithm examines each transaction in sequence, and builds up a sample with a fixed sampling rate of a . Each transaction is examined only once, in accordance with the streaming model, and either kept in the sample (accepted) or dropped out of the sample (rejected). The decision to keep or drop a transaction is deterministic, and based on the combined approximation properties for every item. Namely, the algorithm maintains a penalty function per item i based on the number n and the corresponding number r i for the selected sample. Each penalty function minimizes when the item fre-quency over the sample equals that over the data set, and increases sharply when the item is under-or over-as follows:
The first term in the equation is the L2-distance between the dataset and the sample. The second term in the equation is used to ensure that there is a choice of accepting or rejecting a transaction such that the penalty is not increased (without it, the penalty would always increase when r ing or rejecting a transaction). The total penalty for a transaction j is Q  X  or not to keep j is made by trying to minimize D Q . When a transaction is accepted, both r incremented but when it is rejected only n i is incremented. Therefore, the D Q function for a given item i becomes:
By choosing transactions such that D Q accept 6 D Q reject 2 r
 X  6 0.
The complete code of the Biased-L2 algorithm is presented in Fig. 1 . Since n whether the transaction is accepted or not, incrementing it ahead of time leads to the simplified acceptance condition given in line 9 of the pseudocode.
 where e  X  O Proof. The overall penalty is zero at first, and never increases during the sampling process. Thus, Rearranging the terms,
On the right hand side, we have the count of every item. Every term on the left is non-negative, which implies that every term on the left is less than or equal to the whole right hand side. Hence,
If we add a sentinel item to each transaction, and divide by a d , we get the final supports as 3.3. Better theoretical bounds
Biased-L2 uses a quadratic cost function. However, we can improve on the theoretical discrepancy bound if we use an exponential cost function, Q i = Q i ,1 + Q i ,2
We call the algorithm using the new cost function as Biased-EA. Based on this new cost function, we can prove the following theorem:
Theorem 2. The Biased-EA algorithm with sampling ratio a produces a sample of discrepancy e and size a d(1  X  e ), where e  X  O
Proof. In the beginning, r i =0, n i = 0, and Q i =2. Q (init) terms are positive. Therefore,
Rearranging the terms,
Since both terms are positive,
Taking logarithms and combining the inequalities, we get:
We want to minimize the error in terms of d i . Approximating log(1 d and taking the derivative, we find that a good choice for d
Substituting in Eq. (5) yields
If we add a sentinel item to each transaction, and divide by a d , we get the final supports as
Even though theoretically Biased-EA gives better discrepancy bounds than Biased-L2, the somewhat complex especially in streaming cases, where processing speed is important and data distribution is unknown. For these simplicity.

In this section, we presented Biased-L2 as the first of our deterministic sampling algorithms. Biased-L2 works in the streaming model, where each transaction is examined only once and a sample with a given rate a is created from the underlying stream. The algorithm is superior to EASE both in run-time and memory requirements, since EASE works on halvings where each transaction is examined O (log d ) times, and therefore
O ( m log d ) counters have to be kept. 3.4. Deterministic reservoir sampling (DRS)
In this section, we present deterministic reservoir sampling algorithm. The main idea is to maintain a is computed in order to keep a distance function as small as possible (here, we present the algorithm using
Dist 2 ). In particular, it differs from EASE and Biased-L2 by its ability to not only add new transactions to the sample but also remove undesired transactions. As we will show, this ability makes the sample more robust to changes of the distribution in the streaming or tabular data scenarios.

The algorithm maintains the worst transaction W in the sample, i.e., the one whose removal from s k is used to control the number of updates as follows: The algorithm scans the consecutive transactions in blocks of size k and for each block, computes the best transaction T for an update, i.e., such that Dis-t ( D ,( S n { W } [ { T })) is minimized. One important observation here is that even replacing W by the best T may not decrease the cost function in some situations. In this case, the sample is kept unchanged for this block.

The full algorithm is presented in Fig. 2 . This version of the algorithm works in a single pass and updates that the size of the dataset must be known in advance, in order to compute the Dist ing case, or other cases where we do not know the size of the dataset in advance, a slight modification to the algorithm is possible, that starts with an expected dataset size and zero frequencies, and gradually increases them during run-time.

Since at each update, only one transaction is added and another removed from the sample, limited number of items on average are affected from this change, allowing us to easily update the penalty function incremen-tally. The difference in penalty function after adding transaction T is Similarly, the difference after removing transaction W is size does not change:
Based on Eq. (8) the computation can be done in time O ( T one iteration of the loop is O ( T avg ) except if it triggers an update, in which case it becomes O ( sT each transaction in the sample needs to be re-examined to find the new worse transaction. In order to de-scribe the overall running time, we consider the choice of k . A choice of k = 1 means that we update in a totally greedy fashion (steepest descent), which might perform well in terms of error but might be very expensive in terms of run-time. A choice of k = d means no updates. In between these extremes, selecting a bigger value will decrease the number of updates on the sample and speed up the sampling process, but decrease the quality of the sample. Selecting a smaller value will slow down the process while increasing the quality of the sample. Ultimately, we should pick the smallest value of k which affords a reasonable run-ning time. Following the analogy with reservoir sampling [24] , we could hope that the number of updates borne out by the experiments. Instead, the actual bounds seems closer to the trivial upper bound of d / k .A good compromise seems k = s / c for some constant c &gt; 0, which implies a total number of updates which is ing the effect of k on the overall sample quality and accuracy.
 As for memory requirements, DRS needs to store the frequency counts of every item separately in D and S
DRS , as well as the sample, hence has a space complexity of O ( m + sT Biased-L2 (since finally s = a d ).

In Sections 3.2 and 3.3 , we presented our deterministic sampling algorithms, Biased-L2 and deterministic reservoir sampling. Among these two, Biased-L2 is our algorithm of choice if we need speed and simplicity, and DRS is preferred when we need exact sample sizes, or when the underlying dataset distribution changes and fast recovery is needed. 4. Experimental results
In this section, we compare the new algorithms (Biased-L2, DRS) with the previous ones on various data-sets, and show the superiority of the new algorithms in terms of sample quality and accuracy. We also high-light additional features of the DRS algorithm using tailored experiments.
 4.1. Datasets used The datasets used in our experiments are three synthetic datasets from IBM [17] (T5I3D100K,
T10I6D100K, T50I10D100K), and one real-world clickstream dataset (BMS-WebView-1 or BMS1) [20] . Both a transaction and of an itemset, in order to evaluate the dependency on these parameters and make sure the results donot differ too much. BMS1 acts as the real-world/typical control data set. More detailed information about the BMS1 dataset can be found in [20] . 4.2. Sampling count data
In this section, we compare the results of the simple random sample (SRS), EASE, Biased-L2, and DRS algorithms on the association rule datasets. The comparison is based on the quality and accuracy of the sam-ple, given the cost function in Eq. (3) .

Fig. 4 plots the RMS error, and Fig. 5 the accuracy results of SRS, EASE, Biased-L2, and DRS on all four equivalent of them, based on the size of the dataset. For synthetic datasets, the algorithms are run 50 times with a random shuffle of D , and the average is calculated. For the real-world dataset (BMS1), the original order transactions is kept, and deterministic algorithms (EASE, Biased-L2, and DRS) are run once, while random sampling algorithm (SRS) is again run 50 times.

Fig. 6 1 presents the ratio comparison of results in Fig. 4 relative to SRS for each dataset. From these results, we can say that the sample quality of DRS and Biased-L2 are superior compared to the results of
EASE and SRS. In terms of RMS error, on average, DRS is a factor of 14 times and Biased-L2 is a factor On average, the new algorithms are also a factor of six times better than SRS on all datasets.
In order to compare the accuracy of the samples, we use the Apriori [1] algorithm to generate association racy results of SRS, EASE, Biased-L2 and DRS on all datasets. In addition, Fig. 7 comparison of the accuracy results for each dataset, based on the SRS accuracy for each sampling rate. From the figures we can say that on average, DRS is a factor of 12, and Biased-L2 is a factor of eight times better than EASE on real-world dataset. Also on this real-world dataset, DRS is a factor of five times, and Biased-L2 slim, but both new algorithms are consistently more accurate than SRS. Looking at the accuracy results in
Fig. 5 , we see that in some datasets, up to 90% accuracy is obtained by using only 3% of the dataset. This result is especially important when mining huge amounts of data. For applications where 90% accuracy is suf-ficient, instead of running the mining algorithms on the whole data, which can take even days for some data-sets, a sample can be used, which is much smaller and easier to handle.

Although the EASE algorithm gives comparable accuracy bounds on synthetic datasets, it performs poorly dataset claim that most algorithms which work with synthetic datasets do not work well with this real-world want to highlight the fact that Biased-L2 and DRS work perfectly both on synthetic and real-world datasets.
On top of this, the major improvements of the present work over EASE are the running time and memory footprint of our new algorithms.
Finally, we compare CPU times for the algorithms we presented in this section. Fig. 8 presents the average time spent in milliseconds to process one transaction for each algorithm on a Pentium IV 3 GHz computer.
Biased-EA and Biased-L2 are up to five times faster compared to EASE. The main reason for this speed-up is the single pass structure of these algorithms compared to logarithmic halving steps in EASE. The
CPU time of DRS various with the sample size, as expected. More details about the running time of DRS is presented below in Section 4.3.1 . 4.3. Extensions to DRS algorithm
In the previous section we presented the accuracy and quality results of the samples generated by the DRS algorithm. In this section, we further present the additional properties of the DRS algorithm; using k param-eter to control the run-time performance and the fast-recovery property of the algorithm under distribution or sample size changes. 4.3.1. Changing the update rate
RMS errors of using different values of k on the BMS1 dataset. As the figure is plotted for different sample quality is similar to a random sample for bigger values of k (less updates), and quality increases for smaller values of k (frequent updates). Fig. 9 (center and right) plots the accuracy result and the CPU time of the Biased-L2 and DRS algorithms for various sampling rates. The plots clearly show that the k parameter in
DRS can be used effectively to control the trade-off between the running time of the algorithm and the qual-ity/accuracy of the sample. For example, for sampling rate of 0.062, we can achieve up to a factor of three times speed-up in run-time by selecting k = 25, without sacrificing much from the accuracy. The effect of k sonable choice, since larger k values do not significantly decrease the algorithm run-time any further. 4.3.2. Changing the sample size
In the DRS algorithm we can change the sample size at any time of the sampling process, quite easily. When the sample size is increased from s 1 to s 2 , s 1 &lt; s next transactions from the dataset are added to the sample without any evaluation (in the most basic case).
Similarly, when the sample size is decreased from s 1 to s ple by finding the worst transaction in the sample and removing it without replacement, enough times. When adding or removing transactions, the item counts of the sample are updated accordingly. After the sample reaches the desired size, it is used as the initial sample for DRS, and the sampling process resumes. Although period, the sample is as good as a deterministic sample again. In other words, once the DRS algorithm starts running again, the cost function decreases dramatically in a very short period of time.

The experiments in Fig. 10 are run on the real-world dataset BMS1, while processing the transactions in the original order to prevent introducing free randomness in the dataset. In Fig. 10 (left), the effect of increasing the sample size is presented. The lower line plots the trace of sampling the BMS1 dataset with a sample size of 500. The upper line plots the trace of sampling the same dataset with a sample size of 150 until the 30000th transaction, after which the sampling rate is changed to target a final sample size of 500, which causes the jump in the RMS error. After a number of transactions, the RMS error function converges to the value it would get for a sample size of 500. One important point to note here is that, when adding new transactions to the sample we did not use any evaluation criteria, just to demonstrate the effect on the RMS error value. One way to add more transactions is to make the whole process greedy, such that the peak caused by the sample size change would be lower, and the sample would converge gradually.
Fig. 10 (center) similarly shows the plot of decreasing the sample size from 500 to 400, and shows that the final RMS error value converges to the value it would get for a sample of size 400 (from the first trans-action). These results show us that the DRS sample size can be changed at any time during sampling, and the jump in the error function can be compensated for with the RMS error converging to its normal value for the new sampling rate, after examining only a small number of transactions (typically proportional to the size of the sample). Note that the convergence for Biased-L2 is much slower, which clearly shows the better recovery of DRS after a sudden change in sample size.

Another important outcome of the fast convergence of DRS is that we can take a random sample from the dataset at any time, use this sample as initial sample for DRS, and convert this random sample to a new sam-
The dataset BMS1 is sampled three times with sampling rates of 0.043, 0.0042, and 0.0019 (sample sizes of 2615, 253, and 116 respectively). Also, after examining 50000 transactions, a simple random sample is created for each case, having exactly the same sizes as 2615, 253, and 116. The plots after transaction 50000 show the results of using these random samples as initial samples for our algorithm. Clearly, after only examining a small number of new transactions, the RMS errors of the samples converge to the expected value. In the end, it makes little to no difference if we sample the whole dataset one transaction at a time, or if we get a random sample at any time and convert it using our DRS algorithm.
 To sum up the experiments in this section, in terms of sample accuracy and quality, both Biased-L2 and
DRS outperform EASE and SRS. Biased-L2 is our algorithm of choice if speed is important, and DRS is our choice if either the sample size must be chosen exactly, or if the sampling rate and/or data distribution changes frequently and fast convergence is needed. 5. Concluding remarks
In this paper, we have presented two novel deterministic sampling algorithms, Biased-L2 and DRS. Both algorithms are designed to sample count data, which is quite common for data mining applications, such as market basket data, web clickstream data and network data. Our new algorithms improve on previous algo-rithms both in run-time and memory footprint. Furthermore, by conducting extensive simulations on various synthetic and real-world datasets, we have shown that our algorithms generate samples with better accuracy and quality compared to the previous algorithms (SRS and EASE). Biased-L2 is computationally more effi-cient than DRS, and when the data is homogeneous or randomly shuffled, produces samples of comparable quality. For sudden changes in the distribution, however, DRS has the ability to remove under-or over-sam-pled transactions if a more suitable one is found during sampling. In the previous algorithms surveyed and in if the distribution of the dataset changes; DRS is not subject to this limitation.

References
