 1. Introduction
Ontologies are defined as formal, explicit specifications of a shared conceptualization [49] . They are an essential component in many knowledge-intensive areas like the Semantic Web [54] , knowledge management, and electronic commerce. The construction of domain ontologies relies on domain modellers and knowledge engineers, which are typically overwhelmed by the potential size, complexity and dynamicity of a specific domain. In consequence, the definition of exhaustive domain ontologies is a barrier that very few projects can overcome. Due to these reasons, nowadays, there is a need of methods that can tackle, or at least ease, the construction of domain ontologies. Automated Ontology Learning methods allow a reduction in the time and effort needed in the ontology development process [2] .

From a formal point of view, an ontology boils down to an object model represented by a set of concepts or classes C, which are taxonomically related by the transitive IS-A relation H C  X  C and non-taxonomically related by named object relations R * C  X  C  X  String. Even though many approaches for ontology learning have appeared during the last decade (see a survey in [2] ) most of them mostly focus on the automatic acqui-sition of C and H and often neglect the importance of non-taxonomic interlinkage between concepts. In fact, the discovery of non-taxonomic relations is considered as the least tackled problem within ontology learning [32] . It appears to be the most intricate task as, in general, it is not known how many and what type of concep-tual relationships should be modelled in a particular ontology. In general, two tasks have to be performed for learning non-taxonomic relationships. On the one hand, one has to detect which concepts are related. On the other hand, one has to figure out how these concepts are related; thus, a name for the relation has to be found.
Considering the state of the art in non-taxonomic learning (discussed in Section 2 ), this paper presents a novel contribution in this area, proposing an automatic methodology for acquiring non-taxonomic relation-ships, framed in the context of domain ontology learning.

As any other learning methodology, this data-driven knowledge acquisition process requires a source from which to extract relationships. In the past, this has been typically addressed by using domain texts, electronic dictionaries, semantic repositories (like WordNet [8] ), and structured and semi-structured information and data sources. Nowadays, with the enormous success of the Information Society, the Web has become an invaluable source of information for almost every possible domain of knowledge. This has motivated many researchers to start considering the Web as a valid repository for Information Retrieval and Knowledge Acqui-sition tasks. However, the Web suffers from many problems that are not typically observed in the previously mentioned classical information repositories. Those sources are often quite structured in a meaningful orga-nisation or selected by information engineers and, in consequence, one can assume the trustiness and validity of the information contained in them. In contrast, the Web presents a lack of structure, a high dynamicity of information, untrustworthiness of data sources and noise added by the visual representation, in addition to the ambiguity inherent to resources written in natural language. Despite all these shortcomings, the Web also presents characteristics that can be interesting for knowledge acquisition. Due to its huge size and heterogene-ity , it can be assumed that the Web approximates the real distribution of the information in humankind [46] .
From the learning point of view, this is a very interesting point. As will be discussed in Section 3 , this is one of our motivations for using the Web as the source for knowledge acquisition.
 Summarizing, this paper presents a novel approach for discovering non-taxonomic relationships from the
Web . The method is able to discover relevant verbs for a domain, which are used as the knowledge base to learn and label non-taxonomic relationships automatically and unsupervisedly. It uses the Web as the source of information from which to extract candidates and compute global scale statistics about information distri-bution. The only restriction is that resources should be written in English, as the method relies in English lan-guage regularities. The approach presented here has been designed as an extension of a previous work [11] that covers the learning of taxonomic relationships. The final goal of our research is the construction of domain ontologies from scratch.

The rest of the paper is organised as follows. Section 2 presents an overview of previous research performed in the non-taxonomic learning area. Section 3 describes the working environment (the Web) and the main knowledge acquisition techniques that configure the base of the present proposal. Section 4 briefly introduces the general approach for learning domain ontologies from the Web in which the present approach is framed.
The non-taxonomic learning stage is extensively described in Section 5 . Section 6 discusses some relevant issues about the learning, including bootstrapping techniques, dynamic adaptation of the analysed corpus and efficient analysis of web resources. Section 7 describes a novel automatic evaluation procedure (using
WordNet-based relatedness measures) and the results obtained for several well differentiated domains. The final section presents the conclusions and proposes some lines of future work. 2. Related work
There are several trends in learning non-taxonomic relationships from text depending on the degree of gen-erality of the extracted relations. Some authors have developed approaches for learning specific relationships such as Part-of [17] , Qualia [41] , Telic and Agentive [22] , Causation [47] or a combination of them [35] by using concrete linguistic patterns (e.g. X consists of Y , X is used for Y , X leads to Y ). Even though those approaches may have interest for developing or enriching general purpose semantic networks (such as WordNet), they are not able to retrieve domain-dependent relationships that are crucial for constructing domain ontologies.
There have been other domain-dependent approaches, developed primarily within the biomedical field as there are very large text collections available (e.g. PubMed). The goal of these works is to discover relation-ships between known concepts (e.g. symptoms, diseases ) by analysing large quantities of biomedical scientific articles [26,52] .

Another stream, more firmly grounded on ontology engineering, systematically seeks new unnamed rela-tions in text. Co-occurrence analysis between terms is used to infer relations, paying little attention to sentence structure. In those approaches the labelling problem is left upon the ontology designer ( Text-to-Onto [3] )ora small predefined set of WordNet-based labels is used ( OntoLearn [33] ). For example, the ASIUM system [10] hierarchically clusters nouns based on the verbs that they co-occur with; there is however no formal support for named relations.

The labelling problem is tackled in other approaches by relying on  X  X efault X  labels, under the assumption that, for example, the relation between a Company and a Product is always  X  produce  X  [30] . Byrd and Rabin [45] assign the label to a relation based on sentence patterns (e.g. location relation for the  X -based  X  construc-tion). They derive unnamed relations from concepts that co-occur by calculating the measure for mutual infor-mation [27] between terms. The Adaptiva system [7] allows the user to choose a relation from an ontology, which is used to interactively learn relation recognition patterns. Such a supervised approach does not pay off if the goal is to find new relations for a domain, as in the present paper.

Other approaches aim to learn more general relations exploiting the linguistic structure of text in a sim-ilar way as the present work. Relation extraction is therefore associated to the problem of acquiring selec-tion restrictions for verb arguments [5] . In this sense, Reinberger and Spyns [39] employ statistical methods based on frequency information over linguistic dependencies in order to establish relations between entities from a corpus of the biomedical domain. However, they are not concerned about label-ling the discovered relations, which results in an approach similar to those of Maedche and Staab [3] and
Kavalec et al. [32] . Kavalec and Sva  X  tek [31] address the labelling problem in a latter stage by using the verbs frequently occurring in the context of each concept pair association. Sabou [36] conducts her research on a corpus obtained from Web Service descriptions, that consists of simple sentence construc-tions from which ontology fragments can be easily extracted. Unfortunately, it needs a lot of manual interaction. More recently, Schutz and Buitelaar [4] developed a system ( RelExt ) that is capable of auto-matically identifying highly relevant triples (pairs of concepts connected by a relation) over concepts from an existing ontology. RelExt works by extracting relevant verbs and their grammatical arguments (i.e. terms) from a domain-specific text collection and constructing relations through a combination of linguis-tic and statistical processing.

The present approach also exploits the sentence structure for the English language (verb phrases and verb arguments). Concretely, it exploits verbs as the central point for discovering non-taxonomic relationships. On the contrary to the presented approaches, it starts from domain-related verbs that are learned automatically and unsupervisedly in a first stage. It considers specific verb phrases as domain-dependent semantic patterns that express non-taxonomic relations for a domain. In this manner, they are used as the seeds to retrieve domain-related concepts and label them accordingly. This is an important point as most of the previous works do not appropriately solve the labelling problem. As will be introduced in Section 3 , in order to develop an unsupervised solution, the proposed system employs co-occurrence analysis based on Web information distri-bution to select the most adequate domain-related concepts and relationships. 3. Knowledge acquisition premises and techniques
This section justifies why the Web can be a valid learning repository and which of its characteristics can be useful when performing knowledge acquisition tasks. After that, it summarizes some of the knowledge acqui-sition techniques employed in the present approach. 3.1. The Web as the source for learning
As mentioned in the introduction, due to the characteristics presented by the Web (i.e. lack of structure, dynamicity, untrustworthiness, noise, ambiguity, etc.), the application of classical knowledge acquisition methodologies is not very adequate. For that reason, this paper proposes a methodology that can fit better into the Web environment, taking into consideration the following facts:  X  The Web can be a valid knowledge learning repository thanks to the huge amount of available information (especially, English resources). In this sense, it has been argued that the number of resources available in the
Web is so vast and the amount of people generating web pages is so enormous, that the Web information distribution approximates the actual real distribution as used in society [46] . This is especially important if one intends to obtain results that are representative enough.  X  The redundancy of information in such a wide environment can represent a measure of its relevance and trustiness for a certain domain [15,19,40] . This characteristic can be useful when attempting to perform an unsupervised learning process. This can be a good approach to tackle the problem of untrustworthiness of the resources: we cannot trust the information contained in an individual website, but we can give more confidence to a fact that is enounced by a considerable amount of possibly independent sources. This fact is also related to the consensus that the extracted knowledge should present: implicit consensus can be achieved as concepts are selected among the terms that are frequently employed in documents produced by the virtual community of users [48] .  X  Keyword-based Web search engines (such as Google or Yahoo) are effective for indexing and retrieving web resources if the search queries are specific enough [18] . Their lack of semantic analysis makes them suit-able for any kind of possible domain of knowledge regardless of its generality. They can be considered as experts for corpus selection with the advantage that they are experts in all types of domains. In more detail, there are several aspects of web search engines that may result in a valuable aid in the knowledge acquisi-tion process:  X  The key point to obtain the maximum profit of keyword-based search engines is to construct the queries  X  Keyword-based search engines also provide statistics about information distribution in the whole Web. 3.2. Lightweight analysis
The use of complex text processing tools as a step towards accessing the knowledge within a huge repos-itory as the Web is impractical [33] . On the contrary, shallow analyses can miss some information contained in a certain web resource. However, if that information is relevant for the domain, sooner or later it will be contained in another resource. Thus, one can take profit of the amount of available resources and its high redundancy to apply lightweight analysis over large corpora. This simplistic approach may cause some problems regarding misinterpretation of text or natural language ambiguity. In order to minimize their effects, as will be described in Section 5 , we have introduced several unsupervised heuristics and restrictions designed over the premise that relevant domain knowledge will be redundantly retrieved in many different forms. In addition, bootstrapping techniques using the already acquired knowledge will be applied in order to contextualize queries and linguistic analyses as much as possible. This will minimize problems of drifting outside the explored domain. However, complex issues such as polysemy or synonymy are left as future lines of research.

In order to perform the analytic process efficiently, the amount of processed information for each resource should be reduced to the minimum. Concretely, at each moment, only the nearest context of the queried con-cepts will be evaluated. Those pieces of relevant information are known as  X  X  text nuggets  X  X  and their analysis allows to obtain relevant results without an exhaustive analysis of the whole text [34] . 3.3. Statistical analysis
When developing unsupervised learning approaches that do not rely on predefined knowledge, statistical analysis is an adequate technique to infer information relatedness [40] . However, statistical solutions suffer from the sparse data problem , i.e. they perform poorly when the words are relatively rare, due to the scarcity of data. Some authors [16] have demonstrated the convenience of using a large amount of texts to improve the quality of classical statistical methods. Concretely, the sparse data problem can be solved by using the hugest data source available: the Web [20,44] . However, the analysis of such an enormous repository is, in most cases, computationally unviable. Here is where the use the statistical information provided by web search engines can be adequate. Even though those statistics are referred only to the indexed resources, this Web subset is so vast (more than 10 billion webs in Google) that some authors [40,42,46] have demonstrated the convenience of using web search engines hit count to obtain robust statistics.

Concerning this last point, one of the most important precedents is Turney X  X  work [44] , where several heu-ristics for employing the statistics provided by web search engines ( X  X  X eb scale statistics X  X  [40] ) are presented.
Specifically, Turney uses a form of pointwise mutual information (PMI) [27] between words and phrases that is estimated from Web search engine hit counts for specifically formulated queries. The conclusion is that the degree of relationship between a pair of concepts can be measured through a combination of queries made to a Web search engine (involving those concepts and, optionally, their context). As an example, a typical score measure of co-occurrence between an initial word ( problem ) and a related candidate concept ( choice )is (1)
Statistics obtained directly from queries performed over a web search engine are particularly interesting as (i) they can be obtained in a very immediate and scalable way from publicly available web search engines, avoid-ing the necessity of large analyses of text, and (ii) they provide particularly robust measures about information distribution as they have been obtained from the whole Web (considering the characteristics introduced in Sec-tion 3.1 ). 3.4. Pattern-based knowledge acquisition
The use of linguistic patterns can be an effective technique to extract knowledge without expert X  X  supervision and without requiring predefined knowledge of the specific analysed domain (e.g. manual annotation, seman-tic repositories, ontologies, etc.) [38] . For the taxonomic case, for example, predefined domain independent linguistic patterns [21,38] are a very common manner of discovering hyponym relationships.

However, for the non-taxonomic case, aside from a reduced set of predefined relationships (e.g. meronymy, antonymy, synonymy, etc.), there do not exist domain independent patterns, as non-taxonomic relationships necting element between concepts is undeniable [43] . Verbs specify the interaction between the participants of some action or event by expressing relations between them. From an ontology engineering point of view, verbs express a relation between two classes that specify the domain and range of some action or event [4] . As presented in Section 2 , verbs have been extensively used in the past to define or retrieve non-taxonomic relationships.
Verbs are used in the present work as the domain patterns from which non-taxonomic relationships are discovered. In order to compile an adequate list of verbs for the domain, a previous stage for learning the more domain-related ones is introduced. 4. Domain ontology construction
The research presented in this paper is a part of a general one which main goal is to build domain ontol-ogies from scratch. In the past, we have developed techniques to learn taxonomies and discover named entities from the Web [11] , with the previously stated characteristics of non-supervision, automation and domain inde-pendence previously stated. Using those algorithms, one was able to retrieve taxonomic relationships and organise domain concepts in a hierarchical way. This learning stage was addressed by extensively querying web search engines and analysing web resources in order to extract and select relevant domain concepts.
The non-taxonomic learning approach described in this paper is designed as an extension of the previous tax-onomic learning stage, configuring a final method that covers the main aspects of the ontology learning pro-cess [50] .

As learning ontologies from scratch without semantic background is a difficult task, an incremental and iterative learning process, divided in several steps, has been designed. In this manner, the knowledge already acquired in one step can be used to constrain the analytic process in the following steps, for example, con-structing more specific Web queries. In addition, the retrieved concepts and relationships can be used as seeds for further analyses. At the end, through several iterations of the learning process, the system will incremen-tally construct the semantic network of concepts that will compose the domain ontology.

As shown in Fig. 1 , the full domain ontology learning process is divided in the following phases:  X  Taxonomic learning : it starts from a user specified keyword that indicates the domain for which the ontol-ogy should be constructed (e.g. cancer ). This term is used as a seed for the learning process. At this initial stage of the analysis, only general queries using domain independent patterns for hyponymy detection [21] are employed to retrieve corpus of documents by means of a Web search engine. Instead of performing complex analysis with a large amount of those resources, only subtle and lightweight linguistic analyses are applied over a reduced amount of resources. This allows detecting the most directly related knowledge and composing an initial taxonomy (i.e. basic types of cancers ). This process is described in detail in [11] .A procedure to detect named entities (e.g. healthcare organisations related to cancer ) and include them as instances of the taxonomy is also performed. The output of this process is a one-level taxonomy with gen-eral terms and a set of verbs that have appeared in the same context  X  sentence  X  as the searched domain keyword during the analysis.
  X  Non-taxonomic learning : the verb list compiled in the previous phase (e.g. is associated with, is caused by , etc.) and the initial keyword are used as the knowledge base for the non-taxonomic learning process. They are used as a bootstrap for constructing domain-related patterns for relation discovery (including them in the constructed web search queries). The result is that the system is able to obtain additional domain knowl-edge in the form of non-taxonomically related concepts (e.g. fat diet is associated with cancer ). The details of this process are fully developed in Section 5 .
  X  Recursive learning : the two previous learning stages are recursively executed for each obtained concept (taxonomically  X  e.g. skin cancer  X  and non-taxonomically  X  e.g. hepatitis  X  related). Each one becomes an individual seed for a particular set of further analyses. As the learning evolves, queries are longer, the search is more contextualized, web resources are more domain-related and, in consequence, the throughput of the methodologies and the quality of the results are potentially higher. The end of this recur-sive process is controlled by the algorithm itself considering, as will be described in Section 6.2 , the learning throughput of each iteration. The system X  X  output is a multi-level taxonomy in which each concept can be non-taxonomically related to other ones that, in their turn, can be the object of new taxonomic and non-taxonomic analyses. Illustrative examples (evaluated in Section 7 ) of a part of the structure that the system is able to obtain are presented in Figs. 2 and 3 .  X  Post-processing : the final structure is post-processed in order to obtain a more compact structure. Con-cretely, the system performs some analyses that try to detect redundancies (different morphological forms or the same concept, superfluous taxonomic relations), discover implicit semantic relationships (like multi-ple inheritance) and extract new knowledge (like class features in the form of attributes). 5. Learning non-taxonomic relationships
In this section, a novel automatic methodology for discovering non-taxonomic relationships from the Web is presented. From a general point of view, this task involves (i) the discovery and selection of verbs  X  non-tax-onomic labels  X  used for expressing non-taxonomic relationships in a specific domain and (ii) the discovery and selection of non-taxonomically related concepts (labelling the relationship according to the retrieved verbs).

Despite the unsupervised nature of the proposed method, the knowledge already acquired in the previous step (introduced in Section 4 ) is used as a bootstrap to contextualize the search process by adding acquired concepts to the web queries. In this case, as shown in Fig. 4 , apart from the initial domain keyword, this step receives a set of domain verb candidates compiled during the taxonomic analysis. All these data suppose a knowledge base from which to start the learning process. 5.1. Verb extraction and selection
The first step in the non-taxonomic learning methodology is the discovery of patterns that express non-taxonomic relationships . Those are typically composed by a verb relating a pair of concepts. In conse-quence, the verb is considered as the label of the relationship. Due to the large amount of verbs available in the English language, the system should find which of them are truly relevant for the particular domain.

The process starts during the taxonomic learning, in which the system compiles a set of verbs that are apparently related to the domain X  X  keyword (e.g. hypertension ). Concretely, it applies simple morphologic and syntactic analyses 1 over the keyword X  X  context  X  sentence  X  to retrieve hyponym candidates. Using the syntactically tagged sentence, the system can also extract the verb phrase in which subclass candidates are found. In many situations a conjugated verb with, optionally, a preposition that is referred to the domain X  X  keyword, is retrieved (e.g. is caused by, is associated with ). However, due to the variety of verbal forms (according to subject number, verbal tense, passive and conditional constructions, use of adverbs, etc.) and the unsupervised nature of the approach, the system cannot easily assess the right subject X  verb X  X bject semantics.

In order to avoid natural language related problems we have opted for a simplistic approach: as we only intend to extract labelled relationships, only those verbal forms that express them in an assertive way are retrieved. Concretely, verb phrases are extracted taking into consideration the following restrictions:  X  Only present tenses are permitted. No other forms such as futures, pasts or conditionals are allowed.  X  Verb phrases including modifiers in the form of adverbs of any kind are also rejected.  X  Verb phrases including a composition of verbs (e.g. tends to develop in ) are not accepted, as it is difficult to realize in which manner the main verb X  X  meaning is being modified. The only exception is the verb  X  X  X o be X  X , used to construct the passive form (very common in the English language).  X  Prepositions are allowed and attached to the verb.  X  Verbs expressing taxonomic relations are rejected (is/are, include, etc.) as it is preferred to treat the taxo-nomic case is treated independently.

Those verbs which fulfil the restrictions are finally extracted and classified in function of the role of the domain X  X  keyword: predecessors (e.g.  X  X  causes hypertension X  X ) or successors (e.g.  X  X  X ypertension is treated with  X  X ) of the domain X  X  keyword.

The next step consists on detecting which of those verbs are closely related to the searched domain. The objective is to select those verbs that are specific or commonly used in a particular domain. As they will be used to learn non-taxonomic relations, this selection stage helps to focus the analysis in the truly domain-specific relationships. In addition, thanks to the domain independent nature of the approach, any kind of concrete relationships for every possible domain of knowledge can be potentially retrieved.

To perform this process in an unsupervised fashion, the system uses a statistical analysis to measure the degree of relationship between the domain and each verb. As has been previously introduced, in order to obtain robust measures (that consider an amount of resources as large as possible), it uses web scale statistics that represent the distribution of a queried concept in the whole Web. Concretely, for each verb phrase candidate that has been extracted as a predecessor of the domain keyword, the system computes the following relatedness score (2) . The use of double quotes ( X  X  X  X ) in the search strings highly contextualizes the query, giving enough confidence to conclude that the verb phrase is really used to express a relationship in which the domain X  X  keyword is the object:
Alternatively, if the candidate has been extracted as a successor of the domain X  X  keyword, the relatedness (3) is computed in the following way:
In a similar manner, this last score states that the verb phrase is really used to express a relationship in which the domain X  X  keyword is the subject. There can be situations in which the same verb has been retrieved both as that case, both scores are computed and stored separately.

The obtained values are used to rank the list of verb phrase candidates. This allows to select those non-taxo-nomic labels that are more closely related to the analysed domain (see examples in Table 1 , for the hypertension domain) in order to use them as the base for learning non-taxonomically related concepts. The selection threshold controls the algorithm X  X  behaviour which typically results in a reduction among 60 X 70% of the total list of retrieved verbs for general concepts. The value should be tuned considering the reduced amount of hits poten-tially obtained by the score X  X  numerator (which involves several words with double quotes) in comparison with the high genericity of the denominator. The limit is set to distinguish those verbs that express domain dependant relationships from the general ones. Empirically a range of thresholds from 1E 3to1E 5 are suitable. 5.2. Retrieval and selection of related concepts
Once verb phrases have been selected, they are used to construct non-taxonomic patterns. Those express domain dependant relationships and can be employed to discover related concepts. In order to do this, the system queries a web search engine with the patterns  X  X  verb-phrase domain-keyword  X  X  or  X  X  domain-keyword verb-phrase  X  X  depending on the role of the domain X  X  keyword. As a result, it retrieves a corpus of resources contain-ing the specified query. The objective at this stage is to evaluate their content in order to obtain concepts that immediately precede (e.g.  X  X  high sodium diet is associated with hypertension X  X ) or succeed (e.g.  X  X  X ypertension is caused by hormonal problems  X  X ) the constructed pattern. Those new concepts become candidates for being non-taxonomically related with the initial keyword. The verb phrase is used as the label of the relation.
However, due to the same reasons introduced in the previous section, the quality of the candidate extraction process may be affected by the lack of semantic understanding of this approach. Extracting a piece of text  X  the particular pattern instance  X  from its context  X  the whole sentence  X  may result in weird relationships due to decontextualization problems. In order to avoid as many natural language derived problems as possible, only those sentences containing the pattern X  X  instance that match with a set of sim-plicity rules ( X  X  text nuggets  X  X  [33] introduced in Section 3.2 ) are evaluated. Concretely sentences must be of the form:
The domain X  X  keyword must appear in the subject or the object in function of its role within the particular verb phrase. Other noun phrases before the subject and after the object, or modifiers such as adverbs or subordi-nate constructions, are not allowed. Using those restrictions, ambiguity problems related to the lack of under-standing are avoided and only knowledge expressed in an assertive way is considered. In addition, in the noun phrase where the domain X  X  keyword appears, no additional modifiers (adjectives) are allowed at each iteration of the algorithm (e.g. a relationship defined for pulmonary hypertension and not for the general concept of hypertension ). As they probably specify a more specific concept they will be considered when evaluating more concrete taxonomic concepts (e.g. pulmonary hypertension ). Only meaningless words such as determiners are allowed in the extracted noun phrases. The new discovered concept (subject or object) can be composed by one or more words (e.g. diuretics , diuretic therapy ).

One may wonder if this approach is too restrictive, as this simplistic form is not the usual way of expressing knowledge in natural language. If the system were dealing with a limited repository this would be an impor-tant issue, as many complex but valid assertions might be omitted. However, when dealing with an enormous repository with a high redundancy such as the Web, it is much more probable to find the same knowledge expressed in many different forms (with different degrees of formal complexity). As stated in Section 3.2 , this simplistic approach has proved to be effective when dealing with big, heterogeneous, noisy, ambiguous envi-ronments like the Web [33] .

It is important to note that the fact of applying restrictive constraints over the text analysis in order to avoid natural language problems does not imply that the relationship expressed in the extracted nugget is valid. In consequence, once a set of new concepts has been extracted through the analysis of sentences, the next step is to decide which of them are related to the searched domain. In order to perform this selection process, web scale statistics about the co-occurrence of those two terms are used again. In this case, the relatedness score is computed with the following formula (4) :
In this case, the AND operator ensures that those two terms co-occur within the text but not necessarily in the same sentence. This is a more relaxed score than (3) because non-taxonomic relationships can be expressed in many different ways. Other score forms also considered, involving double quotes or adding the verb phrase, re-sulted in too restrictive queries returning very few results in many situations, compromising the robustness of the computed measures.

Those concepts whose relatedness with the initial keyword is higher than a specific threshold (see some examples in Table 2 , for the hypertension example) are selected and incorporated into the ontology. The score X  X  numerator is more general than for the verb selection case as the AND operator (and not double quotes) is used. In consequence, the threshold range should be higher than the previous to maintain a similar selection behaviour. We recommend a value among 1E 1 and 1E 2. As introduced previously, the relation is labelled
Note that the direction of the relation corresponds to the role that each concept plays in the sentences (subject or object). 6. Web content access, corpus selection and bootstrapping
Up to this point, an overview of the general ontology learning process and a detailed explanation on the non-taxonomic learning process have been offered. However, some questions regarding the specific access to the web resources, issues about finalisation (i.e. how to decide when the algorithm should continue the anal-ysis or stop the exploration) and the information used as a bootstrap should be considered. 6.1. Efficient access to the web content
Even though the main objective is to offer the best results and not the shortest response time, there are some ways to speed-up the process but maintaining the quality of the final ontology. Due to the particular nature of the approach, much of the time is employed in accessing the Word Wide Web (whenever the system is que-rying a web search engine or accessing a particular web site). As the Web X  X  response time is, in many situa-tions, orders of magnitude higher than the time required to process the web content, any improvement in this aspect can suppose a great difference from a temporal perspective.

The first improvement is related to the web search engine used to perform queries for obtaining web sites or web scale statistics. In order to avoid the saturation of one particular search engine, denegation of service or a low performance due to introduced courtesy waits, we have implemented several interfaces with different search engines such as Google, Yahoo and MSNSearch. In this manner, the system can alternate from one to another for several searches or even combine two of them for the same search. Analysing those search engines, it can be concluded that Google has the best Web coverage but its very limited access and extremely slow response times through the search API, introducing courtesy waits of several seconds for consecutive queries, really hamper its usefulness. On the other hand, MSNSearch offers a really good performance through the web interface without limitations (even performing thousands of consecutive queries) at the cost of a reduced coverage, especially for the most concrete queries. Yahoo remains at an intermediate point with slightly lower response time and better coverage than MSNSearch, but introducing access limitations (number of queries per day and IP). These behaviours are quite similar to those observed by an empirical study [24] . Considering this situation, the combined use of different search engines becomes almost mandatory.
Due to this search engine dependence, one can wonder about the influence of the particular web search engine used during the learning process in the final results. Concretely, each queried search engine provides its own ranked list of web resources and its own statistical measures about information distribution. From the analysis of the influence of these differences in the results, two main conclusions can be drawn. On the one hand, once a significant amount of web resources has been retrieved, the extracted knowledge using dif-ferent search engines tends to be the same, due to the high redundancy of information in the Web; on the other hand, although the absolute statistical values for a specific query may be quite different (due to the particular estimation algorithm employed by each web searcher), the final measures obtained during the selection about relatedness between concepts tend to be similar as they are always relative measures. The only observed dif-ference (apart from response times) is that Google, due to its high Web coverage, is able to return a signifi-cantly larger amount of resources than other search engines for constrained queries, allowing to acquire, potentially, more specific knowledge.

However, in order to avoid problems when mixing and evaluating results (e.g. statistics) from different searches, each engine will be used for a different purpose. Concretely, Google is used to retrieve web resources from which to extract candidates as it has the best coverage and MSNSearch is used to retrieve statistics from which to compute relative scores (requiring a large amount of queries) as it offers the best performance.
The second point that influences the performance is the way in which the content of web resources is accessed. More concretely, for a particular query that returns a set of web sites, the system typically accesses each particular web URL, downloads its content and starts working on it. This can suppose an important overhead depending on the Internet connection bandwidth, the size of the web site and the server X  X  response times. However, there are alternative ways of accessing partial web content, such as the snippets offered by web search engines (typically 2 or 3 lines of text covering the queried term). Considering the described learning process, those reduced views can be particularly useful because the verb phrase-based extraction of candidates only considers a short context (a sentence) for the constructed query. However, those snippets only cover one matching for the particular query and, if several instances can be found on the same web site, they will be omitted.

In order to decide the convenience of using the snippet-based analysis or the full web parsing, the following experiment has been conducted: for several domains, a web search engine has been queried using different domain-related verb phrases. Then, the first returned web sites were evaluated and the number of extractions of candidates that the system was able to obtain in each case was counted. In the test, it was able to extract between 8 and 13 candidates from the first 10 web sites, obtaining an extraction ratio between 0.8 and 1.3 with a maximum of 3 extractions of candidates per web site. Those low numbers were expected, due to the concrete nature of the patterns.

In consequence, it is quite convenient to use web search snippets that typically cover the maximum of 1 or 2 matchings (with a narrow context) per site. This speeds up things greatly as parsing one page of results is equivalent in terms of learning performance to access and parse up to 50 individual web sites. 6.2. Adaptive corpus size
During the explanation it has been mentioned the fact that a set of web resources is retrieved and analysed to extract candidates. However, how big should this set of web resources be in order to obtain a set of results with good recall?
In previous experiments [11] it has been observed in different domains that the growth of the number of discovered concepts (and in consequence the recall ) follows a logarithmic distribution in relation to the size of the search (from dozens to hundreds of web resources). This is caused in part by the redundancy of infor-mation [56] and the relevance-based sorting of web sites made by search engines [9] . Moreover, when the sys-tem has reached a point in which most of the domain concepts have been discovered, precision tends to decrease due to the growth of false candidates. As a consequence, analysing a large amount of web sites does not imply obtaining better results than with a more reduced but accurate corpus.

The ideal corpus size depends on many factors, like the domain X  X  generality, the quality of the web sources, the ranking policy of the search engine or the concreteness of the particular query. Due to the automatic, domain independent and dynamic nature of the present proposal, the corpus size cannot be set a priori. Thus, a mechanism that sets its size dynamically at execution time depending on how the learning is evolving is needed. It will allow to decide whether to continue evaluating more resources or not. We propose an incre-mental analytic methodology: the amount of web resources analysed during each learning step is increased (e.g. in sets of 50 web resources) until the system decides that most of the knowledge for a specific query has been already acquired.

More concretely, for a particular query (i.e. each verb phrase-based pattern), the system retrieves and anal-yses a reduced set of web resources (e.g. 50), extracting candidates and selecting related ones through the described statistical analyses. At the end of the process, if the percentage of selected terms from the list of extracted candidates ( learning rate ) is high (above a 50%), this indicates that the queried verb phrase is par-ticularly productive and a deeper analysis will potentially return more results. In this case, it queries again the search engine with an offset to obtain an additional set of web sites (e.g. the next 50 web sites) and repeat the learning stage. The process is iteratively executed until the global percentage of selected terms (computed from the accumulation of results of each iteration) falls below a certain threshold or no more knowledge has been acquired in the last iteration. This indicates that most of the knowledge related to the queried concept has been already acquired because most of the last retrieved terms have been rejected. The particular threshold can be specified in order to tune up the learning process (i.e. from general  X  high threshold  X  to exhaustive  X  low threshold  X  analysis) in a domain independent way. Typical thresholds used during our tests vary from 70% of selections (very constrained, small potential corpus) to 20% (very loose, wide potential corpus).
In order to illustrate this process, let us analyse the learning trace obtained when using the selected verb phrases as seeds for retrieving web resources. As an example, for the hypertension domain presented in Section 5 , and using a learning threshold of 60%, the following behaviour has been obtained (see Fig. 5 to follow the explanation):  X  The first verb phrase-based query is  X  X  suffer from hypertension  X  X . Analysing the first 50 results, it obtains 2 candidates but none of them is selected, giving us a learning rate of 0%. In consequence, no more results are analysed for this verb phrase.  X  Querying  X  X  hypertension is associated with  X  X  results, after evaluating the first 50 web previews, in 7 candidates and 5 selections. This provides a learning rate of 71,4%. So, the process continues by retrieving the next 50 results. Due to the high productiveness of this verb phrase, it iterates until 200 web resources. At that point, the number of candidates is 38 with 22 selections, resulting in a learning rate of 57,8%, below the specified 60% threshold.
  X  The query  X  X  hypertension is caused by  X  X  provides more than an 80% of selected candidates in the first itera-tion. However, at the third iteration, no more new candidates are retrieved and, in consequence, the process is stopped.  X  When all the verb phrases have been queried, the most productive ones have been  X  X  hypertension is associ-ated with  X  X ,  X  X  hypertension is caused by  X  X  and  X  X  is associated with hypertension  X  X  with 3 or more additional iterations.

Using the presented feedback mechanism through the full process it can be also ensured the correct final-isation of each learning step, with a dynamic adaptation of the effort put to analyse each concept. Moreover, the system is able to obtain results with a good coverage regardless of the generality or concreteness of the specific domain. From the temporal point of view, this approach provides a good learning/effort ratio as the algorithm decides to continue with the analysis only of the apparently productive concepts, discarding the unproductive ones. Some evaluations of the results obtained using this adaptive mechanism are offered in Section 7 . 6.3. Bootstrapping
Even though the ontology construction process starts from scratch, thanks to the incremental learning methodology presented, after each learning step, a partial set of results is available. Concretely, once the first one-level taxonomy for the domain has been obtained, that knowledge base can be used in further steps as a bootstrap (including additional terms in the web queries). Consequently, it is possible to improve future searches (i.e. deeper taxonomic analysis or non-taxonomic relationships) by creating more contextualized que-ries and retrieving more concrete resources.

In more detail, each acquired subclass for the initial domain X  X  concept can be used as a seed for further taxonomic and non-taxonomic learning steps. In this case, it can use the immediate superclass as a bootstrap.
Concretely, the system attaches that superclass to each web query (e.g.  X  X  leukaemia is related with X  X  AND  X  X  X an-cer  X  X ) performed in further analysis to retrieve web resources or computing statistics. One may see that the co-occurrence of the particular query and the immediate superclass is forced. Using this approach, it is possible to specify the context in which the particular concept should be analysed. This is especially useful when the ana-lysed subclass is polysemic or it is used in several domains, because the additional knowledge used in the learn-ing process can constrain and guide it to the corresponding  X  X  X ense X  X . As a consequence, the more knowledge is acquired, the more informed the learning process is.

Another knowledge that can be used as a bootstrap is the compiled and selected list of domain verbs related to a particular concept. As described in Section 5.1 , those verbs are extracted during the taxonomic analysis of a particular concept (e.g. cancer ) and filtered and used during the non-taxonomic stage. This process is repeated for each recursive execution so, for each new subclass (e.g. breast cancer ) an additional list of domain verbs is compiled. However, for all the concepts contained in the same taxonomy, the verbs retrieved for a particular superclass are, in general, adequate for any of its subclasses. In consequence, and in order to improve the throughput of the analysis, the domain verbs retrieved for a particular class are inherited and used during the non-taxonomic analysis by all of its subclasses. Two advantages are provided by using this mechanism (1) Considering the selected and rejected verbs for all the superclasses of a particular class can save the sys-(2) Due to the higher degree of specialisation of a subclass with respect to its superclass and considering the
In addition to all those aspects, once a multi-level taxonomy for the domain X  X  keyword and a set of non-taxonomically related concepts for each class have been recursively obtained, new domains of knowledge can be explored. Concretely, each new non-taxonomically related concept can be used as the seed of a new learning process, obtaining a multidimensional structure. In that case, in order to avoid drifting outside the domain, previously obtained concepts can also be attached to search queries to contextualize the search.
As an example, if the domain defined by the Cancer concept is explored, in addition to the multi-level tax-onomy that represents the different types of cancer, it is possible to find that a particular one  X  liver cancer  X  X s non-taxonomically related with the relation is caused by to the concept hepatitis (as shown in Fig. 2 ). Then, the new concept hepatitis can be the object of new recursive taxonomic and non-taxonomic analyses. However, the system attaches the concept  X  X  liver cancer  X  X  to each formulated query in order to maintain the context in which the analysis is focused. The process is recursively repeated adding the immediate anterior concept to the que-ries corresponding to the new one. The recursion finishes when no more new subclasses are retrieved and selected by the learning mechanism. Thanks to the constrained queries, the potential corpus will be narrower and the algorithm may decide to stop the analysis earlier. The objective is to control the correct finalisation of the process unsupervisedly and automatically, avoiding an excessive semantic distance between related con-cepts. In any case, a hard limit of 2 non-taxonomic links between concepts from the initial domain is estab-lished. The depth of each taxonomic structure is not constrained. 7. Evaluation
The evaluation is the final and mandatory step that should be performed in any ontology learning approach. It is especially important in unsupervised approaches as the present work, due to the lack of expert X  X  intervention during the learning process. However, the problem of evaluating non-taxonomically related concepts is quite complex [4] . Various proposals [31] have been made for comparing ontologies on the lexical as well as on the taxonomic level against a gold standard , but non-taxonomic relationships are rarely contained in a gold standard. In fact, an investigation of the structure of existing ontologies via the Swoogle ontology search engine [28] has shown that domain ontologies very occasionally model this kind of relationships.

Due to the problem of finding gold standards with good non-taxonomic coverage, and the amount of the obtained results, a manual evaluation is not reliable. So, we will centre the evaluation on objective and auto-matic evaluations. In this case, due to the lack of general purpose automatic evaluation procedures [2] , new mechanisms should be designed. We present an approach to evaluate ontological results in an automatic way against an electronic general domain repository: WordNet. 7.1. WordNet overview WordNet [8] is the most commonly used online lexical and semantic repository for the English language.
Many authors have been contributing to it [23,25] or have used it to perform many knowledge acquisition tasks. It offers a lexicon, a thesaurus and semantic linkage between the major part of English terms. It seeks to classify words into many categories and interrelate their meanings. It is organised in synonym sets ( synsets ): a set of words that are interchangeable in some context, because they share a commonly-agreed upon meaning with little or no variation. Every word in WordNet has a pointer to at least one synset. Each synset, in turn, must point to at least one word. At the next level there are lexical and semantic pointers. A semantic pointer is simply a directed edge in the graph whose nodes are synsets. Some interesting semantic pointers are: hyponym , meronym , attribute and similar to . Finally, each synset contains a description of its meaning, expressed in nat-ural language as a gloss. In some situations, example sentences of typical usage of that synset are also given.
All this information summarizes the meaning of a specific concept and models the knowledge available for a particular domain. Using it, one can compute the similarity and relatedness between concepts. There have been some initiatives for computing these measures, such as the WordNet::Similarity [55] Perl module. It offers an implementation of some standard measures that have been widely used by several authors to perform dif-ferent WordNet-based disambiguation tasks [1,29] .

More concretely, similarity measures use information found in an is-a hierarchy of concepts and quantify how much a concept A is like another concept B . WordNet is particularly well suited for computing similarity measures since it organises nouns and verbs into is-a hierarchies. Therefore, it can be adequate to evaluate taxonomic relationships. However, concepts can be related in many ways beyond being similar to each other (e.g. through the mentioned semantic pointers). This information, in conjunction with gloss descriptions, can be brought to bear when creating measures of relatedness. As a result, these measures tend to be more general and more appropriate for evaluating non-taxonomically related terms.

Among the different relatedness measures implemented by Word-Net::Similarity , gloss -vector has been cho-sen. This measure incorporates information from WordNet glosses as a unique representation for the under-lying concept, creating a co-occurrence matrix from a corpus made up of the WordNet glosses. Each content word used in a WordNet gloss has an associated context vector which represents first order co-occurrences (words that occur near each other in a corpus of text). Each gloss is represented by a gloss vector that is the average of all the context vectors of the words found in the gloss. Relatedness between concepts is mea-sured by finding the cosine between a pair of gloss vectors [55] . For a pair of terms, the bigger the measure is, the more related the terms are. In previous studies, it has been demonstrated that this measure returns the most reliable results [51] . Moreover, it does not depend on the degree of semantic interlinkage between the considered concepts (that is mainly limited to taxonomic relationships). This makes it adequate for evaluating non-taxonomic relationships. For those reasons, it is used as a measure of comparison for the obtained results in order to evaluate them. However, the main limitation is the high dependency on the words used to describe the glosses. This may introduce some problems about semantic coverage that will be shown in the following sections.

However, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modelled by semantic links and/or glosses in WordNet and, in consequence, in many situ-ations, truly related terms obtain a low score due to the relative WordNet X  X  poor coverage for specific domains [44] . However, these measures are some of the very few fully automatic general purpose ways of evaluating knowledge acquisition results. 7.2. Evaluation measures
When measuring the quality of an automatically created ontology, the typical measures used in Informa-tion Retrieval are Recall, Precision and F-Measure .

Recall (5) shows how much of the existing knowledge is extracted. To calculate the recall, the number of correctly selected items is divided by the overall number of items in a Gold Standard (whenever it is available).
Sometimes, especially if there does not exist a standard classification that can tell us the full set of expected domain terms, the Local _ Recall (6) can be computed. It is measured as the rate between the number of cor-rectly selected items against the full set of correct entities extracted from the analysed corpus. Despite its local-ity, this score can also give a measure of how well the selection procedure is accepting or rejecting candidates.
This metric is consistent with the recall used in TREC conferences [14] and has been used in the past [40] to evaluate automatically obtained knowledge.

Precision (7) specifies to which extent the knowledge is extracted correctly. In this case, the ratio between the correctly extracted items and the whole number of extracted ones is computed.

In addition to those individual measures, the F-Measure (8) provides the weighted harmonic mean of precision and recall, summarizing the global performance of the selection process. Similarly to the Recall ,a Local _ F-Measure (9) can be computed considering the Local _ Recall instead of the global one.
 7.3. Evaluation of non-taxonomic relationships
Considering that, in the present approach, the base to select a relationship between a pair of concepts are the statistical scores computed from the Web, it is possible to centre the evaluation in the comparison of those values with other relatedness measures between concepts. As discussed in Section 7.1 , among the different WordNet-based relatedness measures, gloss-vector [51] has been chosen.

The adequacy of the Web-based relatedness measure between two non-taxonomically related concepts is checked by comparing it against gloss-vector , whenever both terms are contained in WordNet. Unfortunately, this last requirement forces the omission in the evaluation process of a considerable amount of concrete tech-nological terms and concepts composed by the concatenation of several nouns and/or adjectives. One may also realize that the evaluation only considers pairs of related concepts and not the verb used to label the rela-tions. This is because on the one hand, WordNet-based relatedness measures are meant to be used for pairs of nouns and, on the other hand, verbs are not semantically linked to nouns. However, as final concepts are obtained by means of the discovered verb phrases, their quality (evaluated here) also depends on the quality of retrieved verbs.

Establishing selection thresholds for both measures, it is possible to evaluate the correctness of the candi-date selection procedure by computing correctly classified concepts (selected or discarded) and incorrectly classified ones (selected or discarded). In Tables 3 X 5 and Fig. 6 the evaluation results for several domains of knowledge with a well differentiated behaviour are presented.

Analysing these results, the following conclusions can be extracted:  X  Only a percentage of the full set of non-taxonomic relationships (40 X 74%) has been evaluated using Word-
Net. This is caused by the presence of concrete domain terms that are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures.  X  For the cancer case, high quality results have been obtained, as most of the extracted candidates represent correct relationships. WordNet has a particularly high coverage for this domain, containing even a repre-sentative amount of cancer types.  X  For the hypertension domain, quality is much lower. Analysing this last case in more detail, we have observed that the poor performance is caused in many situations by the way in which gloss-vector (and in general all WordNet-based relatedness measures) works. As has been introduced previously, those mea-sures completely depend on WordNet X  X  coverage for each specific concept (semantically expressed by inter-links or glosses); in consequence, when concepts are poorly covered in WordNet, those measures return a value that does not fully represent reality. For example, on the one hand, gloss-vector returns a low value of 0.04 for the relationship between atherosclerosis and hypertension , even though the first is a problem com-monly derived from the second. This is because, in WordNet, this fact is not mentioned in the atheroscle-rosis  X  gloss. On the other hand, for other general concepts such as family , the returned value with hypertension is 0.169. In contrast, the Web-based measure depends on the Web X  X  coverage for a particular term and, taking into consideration its size compared to WordNet, it can be seen why it is able to provide more consistent results over a wider set of concepts (returning, in the case of hypertension, a value of 0.47 for atherosclerosis and 0.0069 for family ).  X  The sensor case represents an intermediate situation but, due to the low percentage of evaluated relation-ships (40%) caused by the low coverage of WordNet for technological domains, measures are not as reliable as in the other two cases.
 The final conclusion is that the evaluation results based on relatedness measures highly depend on Word-
Net X  X  coverage for the particular domain. In contrast, the Web-based relatedness score hardly presents this handicap thanks to the high coverage offered by the Web for almost every possible domain of knowledge.
Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards con-taining non-taxonomic relationships, we have examined the domains from the expert X  X  point of view. This qualitative evaluation [37] can bring some interesting conclusions about the kind of results one can expect:  X  The retrieved concepts tend to be, in many situations, quite specific. This is caused by the score-based selec-tion that ranks higher those concepts that only co-occur in the specific domain. Considering that our goal is to compose a domain ontology, this is quite convenient as the discovery of very concrete concepts is nec-essary to construct a complete structure.  X  From the list of verbs compiled and selected for a certain domain, only a reduced set is really productive.
Most of the valid discovered relationships are associated to a reduced amount of verb phrases. So, at the end, the verb selection process is not very critical and mainly influences on the execution performance (less invalid verbs to evaluate).  X  Most of the invalid extracted relations are referred to incomplete phrase objects (e.g.  X  X  X ensor provides lin-ear X  X  ) which are caused by the narrow context (snippets) considered during the analysis. A higher precision is expected by performing the analysis over complete sentences, at the cost of a much higher runtime. 8. Conclusion and future work
The discovery of non-taxonomic relations is considered as the least tackled task within ontology learning [32] . Even though some approaches have been developed in the past for extracting related concepts (as intro-duced in Section 2 ), many of them are limited to a reduced set of predefined relations, specific domains of knowledge (such as biomedical domains), the use of semantic background (such as ontologies) and/or do not consider the labelling of the obtained relationships. This is why there is a need of novel and general pur-pose approaches covering the full process of learning relations.

The present proposal introduces a different approach: it starts by discovering domain-related verb phrases that can be considered as non-taxonomic labels. Then, they are used to retrieve related terms using the whole Web as the source of information. Thanks to this particular design, it is possible to tackle the problems of relation labelling (using the acquired verb phrases) and lack of understanding of an unsupervised approach (addressed by statistical analysis compiled from Web X  X  information distribution).

The most interesting features of the proposed methodology are (1) It is completely unsupervised during the Web analysis and the learning process. This is important due to (2) It operates automatically, allowing to perform easily executions to maintain the results updated. This (3) It is a domain independent solution, because no domain assumptions are formulated and no predefined (4) It uses an incremental learning approach with dynamic adaptation of the evaluated corpus as new
In summary, the approach uses several well known learning techniques (such as pattern-based linguistic analyses or statistical measures), combining and adapting them to the Web environment in order to present a novel way of extracting labelled non-taxonomic relationships. The main contributions of the developed meth-odology are (1) A method for selecting relevant domain-related verb phrases extracted during the taxonomic analysis. (2) A method to use those learned domain patterns to extract, select and label non-taxonomic relationships (3) An automatic evaluation procedure (described in Section 7 ) for checking the quality of the obtained
As future lines of research, some topics can be proposed  X  When dealing with natural language resources like web pages, problems related to semantic ambiguity may arise (mainly, polysemy and synonymy). For that reason, complementary methods to deal with polysemy [13] and synonymy [12] especially adapted to the working environment (web resources, search engines and lack of predefined knowledge) have been developed. We plan to integrate those techniques into the learning methodology in order to improve the quality of the results.  X  Concerning the discovered verb labelled relationships, in order to obtain a computer understandable knowledge base that allows inference, verb labels should be processed. One can try to classify them into predefined semantic classes, detect synonyms, inverses, etc. For example, the verb phrase  X  X  is included into  X  X  expresses a  X  X  part of  X  X  type relationship; start and begin labels express the same type of relationship). Verb clustering algorithms [53] and standard classifications of verbs [6] could be used for this purpose, adding additional information about the semantic content (e.g. senses, verb types, thematic roles, etc.) of verb labelled relationships.  X  The proposed evaluation methodology should be improved in order to tackle the limitations described in
Section 7 . Maintaining the automatic operation (WordNet-based), we plan to combine gloss-vector [51] with other relatedness measures [55] in order to improve the performance. In addition, expert X  X  opinions can be requested to evaluate the results more deeply, including multi-level hierarchies or verb labels.  X  The non-taxonomic learning process can be improved if the morphology of verb phrases is carefully con-sidered. Concretely, due to the diversity of ways of expressing a particular verb phrase (in function of the verbal tense or subject number), some candidates may be omitted due to the too restrictive matching policy implemented by keyword-based search engines. In this case, a procedure to properly conjugate verb phrases in common forms might increase the recall of extracted knowledge.
 Acknowledgement This work has been supported by the  X  X  Departament d X  X nnovacio  X  , Universitats i Empresa de la Generalitat de Catalunya i del Fons Social Europeu  X  X .

References
