 Ranking is a very important topic in information retrieval. While algorithms for learning ranking models have been intensively importance. The reality is that many feature selection methods used in classification are directly applied to ranking. We argue that because of the striking di fferences between ranking and classification, it is better to develop different feature selection methods for ranking. To this end, we propose a new feature selection method in this paper. Specifically, for each feature we use its value to rank the training instances, and define the ranking accuracy in terms of a performance measure or a loss function as the importance of the feature. We also define the correlation between the ranking results of tw o features as the similarity between them. Based on the defin itions, we formulate the feature selection issue as an optimization problem, for which it is to find the features with maximum total importance scores and minimum total similarity scores. We also demonstrate how to solve the optimization problem in an efficient way. We have tested the effectiveness of our feature sele ction method on two information retrieval datasets and with two ranking models. Experimental results show that our method can outperform traditional feature selection methods for the ranking task. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Selection process. Algorithms, Performance, Experimentation, Theory Information retrieval, learni ng to rank, feature selection Ranking is a central issue in information retrieval, in which given a set of objects (e.g., documents), a score for each of them is computed and the objects are sorted according to the scores. Depending on applications the sc ores may represent the degrees of relevance, preference, or importance. In this paper, without loss of generality, we take ranking in relevance search as example. Traditionally only a small number of strong features (e.g., BM25 [25] and language model [17][23]) were used to represent development of the supervised learning algorithms like Ranking SVM [10][13] and RankNet [4], it becomes possible to incorporate more features (strong or weak) into ranking models. In this situation, feature selection inevitably becomes an important issue, particularly from the following viewpoints. First, feature selection can help enhance accuracy in many machine learning problems, which strongly indicates that feature selection is also necessary for ranking. For example, although the generalization ability of Support Vector Machines (SVM) depends on margin which does not change with the addition of points, which can increase wh en the number of features [19][5][29] increases. Moreover, the probability of over-fitting also increases as the dimension of feature space increases, and feature selection is a powerful m eans to avoid over-fitting [22]. Second, feature selection can also help improve the efficiency of training. In information retrieval, especially in web search, usually computationally costly. For ex ample, when applying Ranking SVM to web search, it is easy to encounter a situation in which training cannot be completed in an acceptable time period (c.f., [12]). To cope with the problem, we can conduct feature selection before training, because the complexities of most learning algorithms are proportional to the number of features. Although feature selection is im portant, to our knowledge, there have been no methods of featur e selection dedicatedly proposed for ranking. Most of the methods used in ranking were developed which is named filter, feature selection is defined as a preprocessing step and can be independent from learning. A filter method computes a score for each f eature and then selects features according to the scores [20]. Yang et al [31] and Forman [7] conducted comparative studies on filter methods, and they found that information gain (IG) and chi-square (CHI) are among the most effective methods of feature selection for classification . The second category referred to as wrapper [15] utilizes the learning system as a black box to score subsets of features, and the third category called the embedded method [3] performs feature selection within the process of training. Among these three categories, the most comprehens ively-studied methods are the filter methods. Therefore, we also base our discussions on this category in this paper, and we will use  X  X eature selection X  and  X  X he filter methods for feature sel ection X  interchangeably. When applying the feature selec tion methods to ranking, several problems may arise. First, there is a significant gap between classification and ranking. In ranking, a number of ordered categories are used, representing the ranking relationship between in stances, while in classification the categories are  X  X lat X . Obviously, existing feature selection methods for classification are not suitable for ranking. Second, the evaluation measures (e.g. mean average precision (MAP) [32] and normalized discounted cumulative gain (NDCG) [11]) used in ranking problems ar e different from those measures used in classification: 1) in ranking usually precision is more important than recall [32] while in classification both precision and recall are important; 2) in ranking correctly ranking the top-n instances is more critical [11] while in classification making a correct classification decision is of equal significance for all instances. These differences indicate the necessity of developing new techniques for feature selection in ranking. In this paper, we propose a novel method for this purpose with the following properties. 1) The method makes use of ranking information, instead of 2) Inspired by the work in [1][14][27], it considers the 3) It models feature selection fo r ranking as a multi-objective 4) It provides a greedy search algorithm to solve the We believe that these properties are essential for feature selection in ranking. We have tested the performance of the proposed feature selection method on two datasets (OHSUMED [9] and .gov in TREC2004 [28]) and with two state-of-the-art ranking models (Ranking SVM [10] a nd RankNet [4]). Experimental results show that the proposed method can outperform traditional feature selection methods in the task of ranking for information retrieval. The rest of the paper is organize d as follows. Section 2 introduces our feature selection method. Sec tion 3 describes the experimental settings, and the experimental results are reported in Section 4. Section 5 summarizes the major fi ndings in this work, and lists potential future work. importance score of each feature v i , and define the similarity algorithm to maximize the total importance scores and minimize the total similarity scores of a set of features. We first assign an importance score to each feature. Specifically, we propose using an evaluation measure like MAP and NDCG (the definitions of them will be given in Section 3) or a loss function (e.g. pair-wise ranking errors [10][13]) to compute the importance score. In the former, we first rank instances using the feature, evaluate the performance in terms of the measure, and then take the evaluation result as the importance score. In the latter, we also rank instances using the feature, and then view a score inversely proportional to the corresponding loss as the importance score. Note that for some features larger values correspond to higher ranks while for other features smaller values correspond to higher ranks, when calculating MAP, NDCG or the loss of ranking models, we actually sort the instances for two larger score as the importance score of the feature. Inspired by the work in [1][14][27], we also consider removing redundancy in the selected features. This is particularly necessary in the cases in which we are required to only utilize a small number of features. In this work, we measure the sim ilarity between any two features on the basis of their ranking results. That is, we regard each feature as a ranking model, a nd the similarity between two features is represented by the similarity between the ranking results that they produce. Many methods have been proposed to measure the distance between tw o ranking results (ranking lists), such as Spearman X  X  footrule F , rank correlation R , and Kendall X  X  [16][18]. In principle all of them can be used here, and in this paper we choose Kendall X  X   X  as an example. The Kendall X  X  value of query q for any two features v i and v j can be calculated as follows, Where q D denotes the set of instance pairs (,) s t with respect to query q , #{ } represents the number of elements in a set, and instance s d by feature v i . For a set of queries, the Kendall X  X  lues of all the queries are averaged, and the result ( , ) as the final similarity score between features v i and v see that ( , ) ( , ) ij ji vv v v  X   X  = holds. As aforementioned, we want to select those features with largest total importance scores and smallest total similarity scores. Mathematically, this can be represented as follows: indicates that feature i v is selected (or not), importance score of feature i v , and , ij e denotes the similarity , (, ) evv  X  = , and obviously ,, ij ji ee = . In (1), there are two objectives: to maximize the sum of the importance scores of individual features, and to minimize the sum of similarity scores between any two features. Since multi-objective programming is not easy to solve, we take a common approach in optimization and convert multi-objective programming to single-objectiv e programming using linear combination. Here c is a parameter to balance the two objectives. The optimization in (2) is a ty pical 0-1 integer programming problem. As far as we know, there is no efficient solution to such kind of problem. One possible approach would be to perform too high to make it applicable in real applications. We need to look for more practical solutions. In this work, we propose a greedy search algorithm for tackling the issue, as in Fig.1. Algorithm GAS (Greedy search Algorithm of feature Selection) 1. Construct an undirected graph G 0 , in which each node 2. Construct a set S to contain the selected features. Initially S 3. For i = 1... t , 4. Output S t . The time complexity of the proposed algorithm is of order O(mt) , and thus the algorithm is efficient. Furthermore, as made clear in Theorem 1, the algorithm can help find the optimal solution under a condition, which is widely used in many additive models, such as Boosting. Theorem 1 : With the greedy search algorithm in Fig.1 one can find the optimal solution to pr oblem (2), provided that where t S denotes the selected feature set with |S|=t. Proof : feature, we do not change the already-selected t features. Denote 
Svi t = = , where th iteration. Then the task turns out to be that of finding the ( t +1)-th feature so that the following objective can be met. And since 1 tt SS +  X  and { | 1,..., } Note that the first part of the objective is a constant with respect to s , and thus the goal becomes to select the node maximizing the for the ( t +1)-th iteration, the current weight for each node (2 ) wc e weight is equivalent to selecting the feature that satisfies the optimization requirements in (2).  X  In our experiments, we used tw o benchmark datasets. The first dataset is the .gov data which was used in the topic distillation 1,053,110 documents and 75 queries with binary relevance judgments in the dataset. We fi rst used the BM25 model [25] to retrieve the top 1000 documents for each query, and then used the retrieved documents in our experi ments. We extracted 44 features for each document, including both conventional features like document length, term frequency, inverse document frequency, BM25, language model [17][23] features, PageRank, and HITS, and newly-proposed features, such as HostRank [30] and relevance propagation [24]. The second dataset is the OHSUMED data [9], which was used in many experiments in information retrieval [6][10], including the TREC-9 filtering track [26]. OHSUMED is a bi bliographical document collection, developed by Hersh et al at the Oregon Health Sciences University. It is a subset of the MEDLINE database. There are in total 16,140 query-document pairs upon which three levels of relevance judgments are made:  X  X efinitely relevant X ,  X  X ossibly relevant X , and  X  X ot relevant X . We extracted in total 26 features from each document in a similar way to that in [24] 1 . In our experiments, we divided each of the two datasets into three parts, for training (both feature selection and model training), validation, and testing. Therefore, for each dataset, we can create validation, and testing sets, and run six trials. The results we report in this paper are those averaged over six trials. We adopted two widely-used measures in evaluation of ranking methods for information retrieva l: MAP [32], and NDCG [4][11]. MAP is a measure on precision of ranking results. It is assumed that there are two types of documents: positive and negative (relevant and irrelevant). Precision at n measures the accuracy of top n results for a query. Average precision of a query is calculated based on precision at n : where n denotes position, N denotes number of documents retrieved, pos ( n ) denotes a binary function indicating whether the document at position n is positive. MAP is defined as AP averaged over all queries. In our experiments, the OHSUMED dataset has three types of labels. We define  X  X efinitely relevant X  as positive and the other two as negative when calculating MAP, as in [6]. NDCG is designed for measuring ra nking accuracies when there are multiple levels of relevanc e judgment. Given a query, NDCG at position n in is defined as where n denotes position, R ( j ) denotes score for rank j , and Z normalization factor to guarante e that a perfect ranking X  X  NDCG at position n equals 1. For queries for which the number of retrieved documents is less than n , NDCG is only calculated for the retrieved documents. In evaluation, NDCG is further averaged over all queries. Note that the above measures ar e not only used for evaluating feature selection methods, but also used within our method to compute the importance sc ores of features. Since feature selection is only a preprocessing step, its effectiveness should be evaluate d after combining with ranking models. In our experiments, two ranking models, Ranking SVM and RankNet, were used. It should be noted that althou gh the numbers of features in the .gov and the OHSUMED datasets used in our experiments are not particularly large, since the algorithm GAS is efficient, it can handle datasets with significantl y larger numbers of features. Many previous studies have shown that Ranking SVM [10][13] is an effective algorithm for ranking. Ranking SVM makes an extension of SVM to ranking; in contrast to traditional SVM which works on instances, Ranking SVM utilizes instance pairs and their preference labels in training. The optimization formulation of Ranking SVM is as follows: Similarly to Ranking SVM, RankNet [4] also uses instance pairs in training. RankNet employs a neural network as the ranking function and relative entropy as loss function. Let estimated posterior probability ( ) ij Pd d f and posterior probability, and let ,, ((, )) ((, )) qi j i j The loss for an instance pair in RankNet is defined as RankNet then employs gradient decent to minimize the total loss with respect to the training data. Since gradient decent may lead to local optimum, RankNet makes use of a validation set to select the best model. The effectiveness of RankNet especially on large-scale datasets has been verified [33]. Our experiments were conducted in the following way. First, we ran a feature selection method on the training set. Next, we used the selected features to train a ranking model with the training set, and tuned the parameters of the ranking model (e.g. the combination coefficient C in the objective function of Ranking SVM, and the number of epochs in RankNet) with the validation set. These two steps were repeated several times to tune the parameters in the feature selection methods (e.g. the parameter c in our method). Finally, we us ed the obtained ranking model to conduct ranking on the test set, a nd evaluated the results in terms of MAP and NDCG. Our proposed algorithm has two va riants. We list them in the following table. GAS-E GAS-L For comparison, we selected IG and CHI as the baselines. IG measures the reduction in uncertainty (entropy) in classification prediction when knowing the feature. CHI measures the degree of independence between the feature and the categories. Since the notion of category in ranking differs, in theory these two methods cannot be directly applied to ranking. As approximation, we categories, and treated  X  X efinitely relevant, X   X  X ossibly relevant, X  and  X  X ot relevant X  in the OHSUME D dataset as three categories. That is to say, the order info rmation among the  X  X ategories X  was ignored. Note that in practice IG and CHI are directly used as approximation is always made. In addition, we also used  X  X ith All Features (WAF) X  as another baseline, in order to show the benefit of conducting feature selection. Fig.2 shows the performances of the feature selection methods on the .gov dataset when they work as preprocessors of Ranking SVM. Fig.3 shows the performances when using RankNet as the ranking model. In the figures, the x-axis represents the number of selected features. Let us take Fig.2(a) as example. One can find that by using our algorithms (GAS-E and GAS-L), w ith only six features Ranking SVM can achieve the same or even better performances when compared with the baseline method WAF. With more features selected, the performances can be further enhanced. In particular, when the number of features is 18, the ranking performance becomes relatively 15% higher than that of WAF. 
Fig. 3 Ranking accuracy of RankNet with different feature When the number of selected features further increases, the performances do not improve, and in some cases, they even decrease. This validates the necessity of feature selection: the use of more features does not necessa rily lead to a higher ranking performance. The reason is that when more features are available, although the performance on the training set may get better, the performance on the test set may deteriorate, due to over-fitting. This is a phenomenon widely observed in other learning tasks such as classification [7]. Therefore, effective feature selection can improve both accuracy and efficiency (it is trivial) of learning for ranking. Experimental results indicate that in most cases GAS-L can outperform GAS-E, although not signi ficantly. Our explanation to this is as follows. Since feature selection is used as preprocessing of training, it is better to make the feature selection more coherent with the ranking model (i.e. GAS-L). The features selected by GAS-E may be good in terms of MAP or NDCG; however, they might not be good for training the model. Note that the difference between GAS-E and CAS-L is sm all, which does not prevent them from both outperforming othe r feature selection methods. Experimental results al so indicate that with GAS-L and GAS-E as feature selection methods the ranking performances of Ranking SVM are more stable than those with IG and CHI as feature selection methods. This is particularly true when the number of selected features is small. For example, from Fig.2(a) we can see that with four features, the MAP values of GAS-L and GAS-E are more than 0.3, while those of IG and CHI are only 0.28 and 0.25 respectively. Furthermore, IG and CHI cannot lead to clearly better performances than WAF. There may be two reasons: IG and CHI are not designed for ranking and the ordinal information between instances may lose when using them; there may be redundancy among features selected by IG and CHI. For NDCG@10 and for RankNet, we can observe similar tendencies and draw similar conclusions. Fig.4 shows the results of differ ent feature selection methods on the OHSUMED dataset when they work as preprocessors of Ranking SVM. It can be seen th at CHI performs the worst this time. When the number of features selected by CHI is smaller than 15, the ranking accuracy is significantly below that of WAF. By contrast, both IG and our algorithms can achieve good ranking accuracies with less than 5 features. With more features added, our algorithms gradually outperform IG. Let us take Fig.4(a) as example. With our algorithms the MAP of Ranking SVM increases when the number of selected features increases (from 5, selected. In most cases, our al gorithms outperform both IG and WAF by one or two percents. 
Fig. 5 Ranking accuracy of RankNet with different feature For NDCG@10 and for RankNet, we can observe similar tendencies and come to similar conclusions. In summary, our feature selection algorithms for ranking really outperform the feature selection methods proposed for classification, and also improve upon the baseline method without feature selection. From the results of the two datasets, we made the following observations: 1) Feature selection can improve the ranking performance more significantly for the .gov dataset than for the OHSUMED dataset. For example, some feature selection methods can lead to more than 10% relative improvement over WAF for the .gov dataset, while most feat ure selection methods can only result in 1~2% or even less improvement for the OHSUMED dataset. 2) Our proposed algorit hms outperform IG and CHI more significantly for the .gov dataset than for the OHSUMED dataset. For example, GAS-L is significantly better than IG and CHI for the .gov dataset; in contrast the improvement over IG is modest for the OHSUMED dataset. To figure out the reasons, we co nducted the following additional experiments. We first plotted the importance of each feature in the two datasets in Fig.6. The x-axis represents f eatures and the y-axis represents their MAP values when they are regarded as ranking models. The features are sorted according to their MAP values. From this figure we can see that the .gov da taset contains more ineffective features (or noisy features). There are more than 10 features whose MAP is smaller than 0.1. In this case, feature selection can help remove noisy features and thus improve the performance of final ranking. In contrast, most of the features in the OHSUMED dataset are equally effective. Therefore, the benefit of removing noisy features is not large. Furthermore, we plotted the similarity between any two features (in terms of Kendall X  X  ) in the two datasets in Fig.7. Here, both x-axis and y-axis represent featur es, and the level of darkness represents the strength of similarity (the darker, the more similar). From the figure we can see that the features in the .gov dataset are clustered into many blocks, with features in the same blocks our method also minimizes the total similarity scores between selected features, for each cluster, only representative features can be selected and thus we can redu ce the redundancy in the features. As a result, our method performs better than the other feature selection methods. For the OHSUMED dataset, there are only two large blocks, with most features similar to each other. In this case, the similarity punishment in our approach cannot work well. That is why the improvement of our method over the other methods is not so significant. Based on the discussions above, we c onclude that if the effects of features vary largely and there are redundant features, our method can work very well. When a pplying our method in practice, therefore, one can first test the two aspects. In this paper, we have proposed an optimization method for feature selection in ranking. To our knowledge, this is the first work dedicated to the topic. Th e contributions of this paper include the following points. 1) We have discussed the differe nces between classification and ranking, and made clear the limitations of the existing feature selection methods when applied to ranking. 2) We have proposed a novel method to select features for ranking, in which the problem is formalized as an optimization issue. In this method, we maximize the total importance scores of selected features, and at the same time minimize the total similarity scores between the features. We also give an efficient solution to the proposed optimization problem. 3) We have evaluated the proposed method using two public datasets, with two ranking models, and in terms of a number of evaluation measures. Experimental results have validated the effectiveness and efficiency of the proposed method. As discussed in this paper, feat ure selection for ranking is an important research topic, for which there are still many open problems that need to be addressed. 1) In this paper, we have used measures such as MAP and NDCG to compute the importance of a feature and used measures such as Kendall X  X  to compute the similarity between features. In principle, one could employ othe r measures for the same purpose. Furthermore, one could also choose to minimize redundancy among three or four features. 2) In this paper we have only given a greedy search algorithm for the optimization, which can guarantee to find the optimal solution of the integer programming problem under certain condition. It is meaningful to work out an efficient algorithm that solves the original optimization problem directly. With it, one can expect an improvement on ranking performance over those reported in this paper. 3) There are two objectives in ou r optimization method for feature selection. In this paper, we have combined them linearly for simplicity. In principle, one coul d employ other ways to represent the tradeoff between the two objectives. 4) We have demonstrated the effectiveness of our method with two datasets, and with a smal l number of manually extracted features. It is necessary to further conduct experiments on larger datasets and with more features.
 [1] R. Battiti. Using mutual information for selecting features in [2] P. Borlund. The concept of relevance in IR. Journal of the [3] L. Breiman, J. H. Friedman, R. A. Olshen, and C.J.Stone. [4] C. Burges, T. Shaked, E. Renshaw, A .Lazier, M. Deeds, N. [5] A. Blum and P. Langley. Select ion of relevant features and [6] Y. Cao, J. Xu, T. Y. Liu, H. Li, Y. Huang, H. W. Hon. [7] G. Forman . An extensive empirical study of feature selection [8] I. Guyon, A. Elisseeff. An introduction to variable and [9] W. Hersh, C. Buckley, T. J. Leone, and D. Hick-man. [10] R. Herbrich, T. Graepel, and K. Obermayer. Large margin [11] K. Jarvelin and J. Kekalainen. Cumulated gain-based [12] T. Joachims. Making large-sc ale SVM learning practical. [13] T. Joachims. Optimizing sear ch engines using clickthrough [14] N. Kwak, C. H. Choi. Input feature selection for [15] R. Kohavi, G. H. John. Wra ppers for feature selection. [16] M. Kendall. Rank correlation methods. Oxford University [17] J. Lafferty and C. Zhai. Document language models, query [18] A. M. Liebetrau. Measures of association, volume 32 of [19] W. Lior, S. Bileschi. Combining variable selection with [20] D. Mladenic and M. Grobelnik. Feature selection for [21] R. Nallapati. Discriminative mo dels for information retrieval. [22] A. Y. Ng. Feature selection, L1 vs. L2 regularization, and [23] J. Ponte and W. B. Croft. A language model approach to [24] T. Qin, T. Y. Liu, X. D. Zhang, Z. Chen, and W. Y. Ma. A [25] S. Robertson. Overview of th e okapi projects, Journal of [26] S.Robertson and D.A.Hull. Th e TREC-9 Filtering Track [27] S. Theodoridis, K. Koutroum bas. Pattern recognition. [28] E. M. Voorhees and D.K. Harman. TREC: experiment and [29] J. Weston, S. Mukherjee, O. Ch apelle, M. Pont il, T. Poggio [30] G. R. Xue, Q. Yang, H. J. Zeng, Y. Yu, and Z. Chen. [31] Y. Yang and Jan O. Pedersen. A comparative study on [32] R. B. Yates, B. R. Neto. Modern information retrieval, [33] MSN, http://www.msn.com. 
