 In this paper, we propose a novel approach for learning and evaluation in statistical ma-chine translation (SMT) that borrows ideas from response-based learning for grounded semantic parsing. In this framework, the meaning of a sen-tence is defined in the context of an extrinsic task. Successful communication of meaning is mea-sured by a successful interaction in this task, and feedback from this interaction is used for learning.
We suggest that in a similar way the preser-vation of meaning in machine translation should be defined in the context of an interaction in an extrinsic task. For example, in the context of a game, a description of a game rule is translated successfully if correct game moves can be per-formed based only on the translation. In the con-text of a question-answering scenario, a question is translated successfully if the correct answer is returned based only on the translation of the query.
We propose a framework of response-based learning that allows to extract supervision signals for structured learning from the response of an extrinsic task to a translation input. Here, learn-ing proceeds by  X  X rying out X  translation hypothe-ses, receiving a response from interacting in the task, and converting this response into a supervi-sion signal for updating model parameters. In case of positive feedback, the predicted translation can be treated as reference translation for a structured learning update. In case of negative feedback, a structural update can be performed against transla-tions that have been approved previously by pos-itive task feedback. This framework has several advantages:  X  The supervision signal in response-based  X  Response-based learning can repeatedly try  X  Task-specific response acts upon system
The proposed approach of response-based learning opens the doors for various extrinsic tasks in which SMT systems can be trained and evalu-ated. In this paper, we present a proof-of-concept experiment that uses feedback from a simulated world environment. Building on prior work in grounded semantic parsing, we generate transla-tions of queries, and receive feedback by execut-ing semantic parses of translated queries against the database. Successful response is defined as re-ceiving the same answer from the semantic parses for the translation and the original query. Our ex-perimental results show an improvement of about 6 points in F1-score for response-based learning over standard structured learning from reference translations. We show in an error analysis that this improvement can be attributed to using struc-tural and lexical variants of reference translations as positive examples in response-based learning. Furthermore, translations produced by response-based learning are found to be grammatical. This is due to the possibility to boost similarity to hu-man reference translations by the additional use of a cost function in our approach. The key idea of grounded language learning is to study natural language in the context of a non-linguistic environment, in which meaning is grounded in perception and/or action. This presents an analogy to human learning, where a learner tests her understanding in an actionable setting. Such a setting can be a simulated world environment in which the linguistic representa-tion can be directly executed by a computer sys-tem. For example, in semantic parsing, the learn-ing goal is to produce and successfully execute a meaning representation. Executable system ac-tions include access to databases such as the G EO -QUERY database on U.S. geography (Wong and Mooney (2006), inter alia ), the A TIS travel plan-ning database (Zettlemoyer and Collins (2009), inter alia ), robotic control in simulated naviga-tion tasks (Chen and Mooney (2011), inter alia ), databases of simulated card games (Goldwasser and Roth (2013), inter alia ), or the user-generated contents of F REEBASE (Cai and Yates (2013), in-ter alia ). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic envi-ronment. Rather, the semantic context for inter-pretation, as well as the success criterion in evalua-tion is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by Kwiatowski et al. (2013), Be-rant et al. (2013), or Goldwasser and Roth (2013). The algorithms presented in these works are vari-ants of structured prediction that take executability of semantic parses into account. Our work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT.

A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer as-sisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference transla-tion (Cesa-Bianchi et al., 2008), to human accep-tance or overriding of sentence completion pre-dictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all inter-action scenarios, it is important that the system learns dynamically from its errors in order to of-fer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has be-come the learning protocol of choice for CAT. On-line learning has been applied in generative SMT, e.g., using incremental versions of the EM algo-rithm (Ortiz-Mart  X   X nez et al., 2010; Hardt and Elm-ing, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Mart  X   X nez-G  X  omez et al., 2012; W  X  aschle et al., 2013; Denkowski et al., 2014). In a simi-lar way to deploying human feedback, extrinsic loss functions have been used to provide learn-ing signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an n -best list of translations or-der with respect to retrieval performance. In con-trast to our work, all mentioned approaches to in-teractive or adaptive learning in SMT rely on hu-man post-edits or human reference translations. Our work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task.

Interactive scenarios have been used for eval-uation purposes of translation systems for nearly 50 years, especially using human reading compre-hension testing (Pfafflin, 1965; Fuji, 1999; Jones et al., 2005), and more recently, using face-to-face conversation mediated via machine transla-tion (Sakamoto et al., 2013). However, despite of-fering direct and reliable prediction of translation quality, the cost and lack of reusability has con-fined task-based evaluations involving humans to testing scenarios, but prevented a use for interac-tive training of SMT systems as in our work.
Lastly, our work is related to cross-lingual nat-ural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation cam-proaches focus on improvements of the respective natural language processing task, our goal is to im-prove SMT by gathering feedback from the task. In this paper, we present a proof-of-concept of our ideas of embedding SMT into simulated world en-vironments as used in semantic parsing. We use the well-known G EOQUERY database on U.S. ge-ography for this purpose. Embedding SMT in a semantic parsing scenario means to define transla-tion quality by the ability of a semantic parser to construct a meaning representation from the trans-lated query, which returns the correct answer when executed against the database. If viewed as simu-lated gameplay, a valid game move in this scenario returns the correct answer to a translated query.
The diagram in Figure 1 gives a sketch of response-based learning from semantic parsing in the geographical domain. Given a manual Ger-man translation of the English query as source sen-tence, the SMT system produces an English target translation. This sentence is fed into a semantic parser that produces an executable parse represen-tation p h . Feedback is generated by executing the parse against the database of geographical facts. Positive feedback means that the correct answer is received, i.e., exec ( p g ) ? = exec ( p h ) indicates that the same answer is received from the gold standard parse p g and the parse for the hypothesis transla-tion p h ; negative feedback results in case a differ-ent or no answer is received.

The key advantage of response-based learning is the possibility to receive positive feedback even from predictions that differ from gold standard reference translations, but yet receive the cor-rect answer when parsed and matched against the database. Such structural and lexical variation broadens the learning capabilities in contrast to learning from fixed labeled data. For example, assume the following English query in the geo-graphical domain, and assume positive feedback from executing the corresponding semantic parse against the geographical database: The manual translation of the English original reads produces the result This translation will trigger negative task-based feedback: A comparison with the original allows the error to be traced back to the ambiguity of the German word Erhebung . Choosing a gen-eral domain translation instead of a translation ap-propriate for the geographical domain hinders the construction of a semantic parse that returns the correct answer from the database. An alternative translation might look as follows: Despite a large difference to the original En-glish string, key terms such as elevations and heights , or USA and US , can be mapped into the same predicate in the semantic parse, thus allow-ing to receive positive feedback from parse execu-tion against the geographical database. Recent approaches to machine learning for SMT formalize the task of discriminating good from bad translations as a structured prediction prob-lem. Assume a joint feature representation  X  ( x,y ) of input sentences x and output translations y  X  Y ( x ) , and a linear scoring function s ( x,y ; w ) for predicting a translation  X  y (where  X  X  ,  X  X  denotes the standard vector dot product) s.t.  X  y = arg max The structured perceptron algorithm (Collins, 2002) learns an optimal weight vector w by updat-the predicted translation  X  y is different from and This stochastic structural update aims to demote weights of features corresponding to incorrect de-cisions, and to promote weights of features for cor-rect decisions.

An application of structured prediction to SMT involves more than a straightforward replacement of labeled output structures by reference transla-tions. Firstly, update rules that require to com-pute a feature representation for the reference translation are suboptimal in SMT, because of-ten human-generated reference translations can-not be generated by the SMT system. Such  X  X n-reachable X  gold-standard translations need to be replaced by  X  X urrogate X  gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT sys-tem. Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU (Nakov et al. (2012), in-ter alia ) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith (2012).

An alternative approach to alleviate the depen-dency on labeled training data is response-based learning. Clarke et al. (2010) or Goldwasser and Roth (2013) describe a response-driven learning framework for the area of semantic parsing: Here a meaning representation is  X  X ried out X  by itera-tively generating system outputs, receiving feed-back from world interaction, and updating the model parameters. Applied to SMT, this means that we predict translations and use positive re-sponse from acting in the world to create  X  X urro-gate X  gold-standard translations. This decreases the dependency on a few (mostly only one) refer-ence translations and guides the learner to promote translations that perform well with respect to the extrinsic task.

In the following, we will present a framework that combines standard structured learning from given reference translations with response-based learning from task-approved references. We need to ensure that gold-standard translations lead to positive task-based feedback, that means they can be parsed and executed successfully against the database. In addition, we can use translation-specific cost functions based on sentence-level BLEU in order to boost similarity of translations to human reference translations.

We denote feedback by a binary execution func-tion e ( y )  X  { 1 , 0 } that tests whether executing the semantic parse for the prediction against the database receives the same answer as the parse for the gold standard reference. Our cost function sion of sentence-level BLEU Nakov et al. (2012). Define y + as a surrogate gold-standard translation that receives positive feedback, has a high model score, and a low cost of predicting y instead of y + = arg max The opposite of y + is the translation y  X  that leads to negative feedback, has a high model score, and a high cost. It is defined as follows: y  X  = arg max Update rules can be derived by minimization of the following ramp loss objective: min Minimization of this objective using stochastic (sub)gradient descent (McAllester and Keshet, 2011) yields the following update rule: The intuition behind this update rule is to discrim-inate the translation y + that leads to positive feed-back and best approximates (or is identical to) the reference within the means of the model from a translation y  X  which is favored by the model but does not execute and has high cost. This is done by putting all the weight on the former.

Algorithm 1 presents pseudo-code for our response-driven learning scenario. Upon predict-ing translation  X  y , in case of positive feedback from the task, we treat the prediction as surrogate refer-ence by setting y +  X   X  y , and by adding it to the set of reference translations for future use. Then we need to compute y  X  , and update by the differ-ence in feature representations of y + and y  X  , at a learning rate  X  . If the feedback is negative, we want to move the weights away from the predic-tion, thus we treat it as y  X  . To perform an update, we need to compute y + . If either y + or y  X  cannot be computed, the example is skipped.
 Algorithm 1 Response-based Online Learning repeat until Convergence
The sketched algorithm allows several varia-tions. In the form depicted above, it allows to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different ver-sions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing (Goldwasser and Roth, 2013; Kwiatowski et al., 2013; Berant et al., 2013). Lastly, regularization can be intro-duced by using update rules corresponding to pri-mal form optimization variants of support vector machines (Collobert and Bengio, 2004; Chapelle, 2007; Shalev-Shwartz et al., 2007). 5.1 Experimental Setup In our experiments, we use the G EOQUERY database on U.S. geography as provided by Jones over the respective method. questions and their logical forms. The English strings were manually translated into German by the authors of Jones et al. (2012)), and corrected for typos by the authors of this paper. We follow the provided split into 600 training examples and 280 test examples.

For response-based learning, we retrained the full 880 G EOQUERY examples in order to reach full parse coverage. This parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a lan-guage model trained on linearized logical forms. We used the hierarchical phrase-based variant of the parser. Note that we do not use G EOQUERY test data in SMT training. Parser training includes G
EOQUERY test data in order to be less depen-dent on parse and execution failures in the eval-uation: If a translation system, response-based or reference-based, translates the German input into the gold standard English query it should be re-warded by positive task feedback. To double-check whether including the 280 test examples in parser training gives an unfair advantage to response-based learning, we also present experi-mental results using the original parser of Andreas et al. (2013) that is trained only on the 600 G EO -QUERY training examples.

The bilingual SMT system used in our experi-ments is the state-of-the-art SCFG decoder CDEC (Dyer et al., 2010) 5 . We built grammars us-ing its implementation of the suffix array extrac-tion method described in Lopez (2007). For lan-guage modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the En-glish side of the training data. We trained the SMT system on the English-German parallel web data 2013) dataset. 5.2 Compared Systems Method 1 is the baseline system, consisting of the CDEC SMT system trained on the C OMMON C
RAWL data as described above. This system does not use any G EOQUERY data for training. Meth-ods 2-4 use the 600 training examples from G EO -QUERY for discriminative training only.

Variants of the response-based learning algo-rithm described above are implemented as a stand-alone tool that operates on CDEC n -best lists of 10,000 translations of the G EOQUERY training data. All variants use sparse features of CDEC as described in Simianer et al. (2012) that extract rule Table 3: Predicted translations by response-based learning (R gold standard references. shapes, rule identifiers, and bigrams in rule source and target directly from grammar rules. Method 4, named R EBOL , implements REsponse-Based Online Learning by instantiating y + and y  X  to the form described in Section 4: In addition to the model score s , it uses a cost function c based on sentence-level BLEU (Nakov et al., 2012) and tests translation hypotheses for task-based feed-back using a binary execution function e . This algorithm can convert predicted translations into references by task-feedback, and additionally use the given original English queries as references. Method 2, named E XEC , relies on task-execution by function e and searches for executable or non-executable translations with highest score s to dis-tinguish positive from negative training examples. It does not use a cost function and thus cannot make use of the original English queries.

We compare response-based learning with a standard structured prediction setup that omits the use of the execution function e in the definition of y + and y  X  . This algorithm can be seen as a stochastic (sub)gradient descent variant of R AM -PION (Gimpel and Smith, 2012). It does not make use of the semantic parser, but defines positive and negative examples based on score s and cost c with respect to human reference translations.

We report BLEU (Papineni et al., 2001) of translation system output measured against the original English queries. Furthermore, we report precision, recall, and F1-score for executing se-mantic parses built from translation system out-puts against the G EOQUERY database. Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of to-tal examples answered correctly; F1-score is the harmonic mean of both. Statistical significance is measured using Approximate Randomization (Noreen, 1989) where result differences with a p -value smaller than 0 . 05 are considered statistically significant.
 Methods 2-4 perform structured learning for SMT on the 600 G EOQUERY training examples and re-translate the 280 unseen G EOQUERY test data, following the data split of Jones et al. (2012). Training for R AMPION , R EBOL and E XEC was re-peated for 10 epochs. The learning rate  X  is set to a constant that is adjusted by cross-validation on the 600 training examples. 5.3 Empirical Results We present an experimental comparison of the four different systems according to BLEU and how many colorado rivers are there what are the populations of states which border texas what is the biggest capital city in the us which states border the state with the smallest area Table 4: Predicted translations by response-based learning (R F1, using an extended semantic parser (trained on 880 G EOQUERY examples) and the original parser (trained on 600 G EOQUERY training exam-ples). The extended parser reaches and F1-score of 99 . 64% on the 280 G EOQUERY test examples; the original parser yields an F1-score of 82 . 76% .
Table 1 reports results for the extended seman-tic parser. A system ranking according to F1-score shows about 6 points difference between the respective methods, ranking R EBOL over R AM -PION , E XEC and CDEC . The exploitation of task-feedback allows both E XEC and R EBOL to im-prove task-performance over the baseline. R E -BOL  X  X  combination of task feedback with a cost function achieves the best results since positively executable hypotheses and reference translations can both be exploited to guide the learning pro-cess. Since all English reference queries lead to positively executable parses in the setup that uses the extended semantic parser, R AMPION implic-itly also has access to task feedback. This allows R
AMPION to improve F1 over the baseline. All result differences are statistically significant.
In terms of BLEU score measured against the original English G EOQUERY queries, the best nominal result is obtained by R AMPION which uses them as reference translations. R EBOL per-forms worse since BLEU performance is opti-mized only implicitly in cases where original En-glish queries function as positive examples. How-ever, the result differences between these two systems do not score as statistically significant. Despite not optimizing for BLEU performance against references, the fact that positively exe-cutable translations include the references allows even E XEC to improve BLEU over CDEC which does not use G EOQUERY data at all in training. This result difference is statistically significant.
Table 2 compares the same systems using the original parser trained on 600 training examples. The system ranking according to F1-score shows the same ordering that is obtained when using an extended semantic parser. However, the respec-tive methods are separated only by 3 or less points in F1 score such that only the result difference of R
EBOL over the baseline CDEC and over E XEC is statistically significant. We conjecture that this is due to a higher number of empty parses on the test set which makes this comparison unstable.

In terms of BLEU measured against the original queries, the result differences between R EBOL and R
AMPION are not statistically significant, and nei-ther are the result differences between E XEC and CDEC . The result differences between systems of the former group and the systems of latter group are statistically significant. 5.4 Error Analysis For a better understanding of the differences be-tween the results produced by supervised and response-based learning, we conducted an er-how many states have a higher point than the highest point of the state with the largest capital city in the us what is the longest river that flows through a state that borders indiana what states does the mississippi river run through which is the highest peak not in alaska back versus translations by response-based learning (R
EBOL ) leading to negative feedback. ror analysis on the test examples. Table 3 shows examples where the translation predicted by response-based learning (R EBOL ) differs from the gold standard reference translation, but yet leads to positive feedback via a parse that returns the correct answer from the database. The examples show structural and lexical variation that leads to differences on the string level at equivalent posi-tive feedback from the extrinsic task. This can ex-plain the success of response-based learning: Lex-ical and structural variants of reference transla-tions can be used to boost model parameters to-wards translations with positive feedback, while the same translations might be considered as neg-ative examples in standard structured learning.
Table 4 shows examples where translations from R EBOL and R AMPION differ from the gold standard reference, and predictions by R EBOL lead to positive feedback, while predictions by R
AMPION lead to negative feedback. Table 5 shows examples where translations from R AM -PION outperform translations from R EBOL in terms of task feedback. We see that predictions from both systems are in general grammatical. This can be attributed to the use of sentence-level BLEU as cost function in R AMPION and R
EBOL . Translation errors of R AMPION can be traced back to mistranslations of key terms ( city border ). Translation errors of R EBOL more fre-quently show missing translations of terms. We presented a proposal for a new learning and evaluation framework for SMT. The central idea is to ground meaning transfer in successful in-teraction in an extrinsic task, and use task-based feedback for structured learning. We presented a proof-of-concept experiment that defines the ex-trinsic task as executing semantic parses of trans-lated queries against the G EOQUERY database. Our experiments show an improvement of about 6 points in F1-score for response-based learning over structured learning from reference transla-tions. Our error analysis shows that response-based learning generates grammatical translations which is due to the additional use of a cost func-tion that boosts similarity of translations to human reference translations.

In future work, we would like to extend our work on embedding SMT in virtual gameplay to larger and more diverse datasets, and involve hu-man feedback in the response-based learning loop.
