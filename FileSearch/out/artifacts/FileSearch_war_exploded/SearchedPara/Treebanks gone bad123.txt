 ORIGINAL PAPER Jennifer Foster Abstract This article describes how a treebank of ungram-matical sentences can be created from a treebank of well-formed sentences. The treebank creation procedure involves the automatic introduction of frequently occurring grammat-ical errors into the sentences in an existing treebank, and the minimal transformation of the original analyses in the tree-bank so that they describe the newly created ill-formed sen-tences. Such a treebank can be used to test how well a parser is able to ignore grammatical errors in texts (as people do), and can be used to induce a grammar capable of analysing such sentences. This article demonstrates these two applications using the Penn Treebank. In a robustness evaluation exper-iment, two state-of-the-art statistical parsers are evaluated on an ungrammatical version of Sect. 23 of the Wall Street Journal (WSJ) portion of the Penn treebank. This experiment shows that the performance of both parsers degrades with grammatical noise. A breakdown by error type is provided for both parsers. A second experiment retrains both parsers using an ungrammatical version of WSJ Sections 2 X 21. This experiment indicates that an ungrammatical treebank is a use-ful resource in improving parser robustness to grammatical errors, but that the correct combination of grammatical and ungrammatical training data has yet to be determined. Keywords Treebanks  X  Parser evaluation  X  Robust parsing  X  Ungrammatical language 1 Introduction If a parser is to play a useful role in a natural language pro-cessing application, it must be robust to noise in the form of grammatical errors. This robustness manifests itself at four increasingly informative levels: 1. Some Analysis : The parser returns an analysis (possibly 2. Correct Analysis : The parser returns a full analysis for 3. Correct Analysis + Grammaticality Judgement :The 4. Correct Analysis + Grammaticality Judgement + If the parser is to be employed in a grammar checking or Computer-Assisted Language Learning system, the third level of robustness is desirable, and the fourth level even more so. The input to such systems is the users X  own lan-guage, and they expect feedback on whether or not it is well formed. If, on the other hand, the parser is being used to pro-vide an analysis of a sentence that serves as a step on the way to capturing the sentence X  X  intended meaning, for example, in a machine translation or question answering system, the third and fourth levels of robustness are not a prerequisite. However, the first level of robustness is not guaranteed to be useful for such systems. It is, of course, better than a brittle response of no parse at all, but the second level of robust-ness is superior, because at this level, an analysis is returned which captures the ungrammatical sentence X  X  intended mean-ing. Assuming that the parser in question is reasonably accu-rate when faced with well-formed language, the second level of robustness is equivalent to Menzel X  X  definition of robust-ness [ 33 ] as a system X  X  reluctance to change its output when the input becomes increasingly ill-formed. Intuitively, this second level of robustness is close to the way people typi-cally react to common grammatical errors X  X hey attend to the sentence X  X  meaning as if the error did not exist. This article is concerned with the second level of robustness, in partic-ular, with examining to what extent this level of robustness can be achieved within a treebank-trained statistical parsing paradigm, using a treebank of ungrammatical sentences.
Traditional symbolic approaches to parsing employ gram-mars which aim to describe well-formed sentences and explicitly reject ill-formed ones. In order to analyse extra-grammatical input (including ungrammatical input) various robust parsing techniques have been proposed: constraint relaxation in which the parser X  X  grammar is made more lenient [ 15 , 21 , 42 ], error anticipation in which special mal-rules are employed to explicitly describe ungrammatical structures [ 2 , 38 ], minimum-edit-distance approaches in which the ungrammatical sentence is transformed until it can be parsed [ 29 , 43 ] and parse-fitting approaches in which par-tial parses are pieced together using heuristics [ 25 , 36 ]. The treebank-trained statistical parsers [ 5 , 8 , 9 , 12 ] of the last fif-teen years are inherently robust at the first level of robustness, since they will return an analysis for almost any sequence of words. The robustness of these parsers comes from their ability to overgenerate. Unlike traditional parsers, treebank-trained statistical parsers are generally agnostic to the con-cept of grammaticality but since they are usually trained on high-quality texts such as the Wall Street Journal (WSJ), it is not clear that they are able to provide an accurate anal-ysis for an ungrammatical sentence and thus achieve the second level of robustness. This article attempts to explore the extent to which a treebank-trained statistical parser can ignore grammatical noise using the idea of an  X  X ngrammat-ical treebank X . The exploration takes the following form: an ungrammatical version of the WSJ section of the Penn Tree-bank [ 31 , 32 ] is created, this ungrammatical version of the WSJ corpus is divided in the usual way into test and train-ing data, and two WSJ-trained parsers, Bikel X  X  implemen-tation of Collin X  X  Model 2 parser [ 5 , 12 ] and Charniak and Johnson X  X  reranking parser [ 9 ] are then evaluated against the test section of the ungrammatical WSJ to investigate these parsers X  resistance to grammatical noise. The usual Eng-lish training set for the parsers is then replaced/augmented with parses from the ungrammatical version of the WSJ, the parsers are retrained and then tested again to investi-gate whether the parsers X  resistance to grammatical noise can be improved without affecting their performance on well-formed sentences.

The article is organised as follows: the idea of an ungram-matical treebank is described in more detail in Sect. 2 . Sec-tion 3 contains a description of the data of interest, i.e. ungrammatical language. An attempt is made to give a def-inition of an ungrammatical sentence, and examples of sen-tences which fall under this definition and those which do not are provided. The process of creating an ungrammatical treebank is described in Sect. 4 . In Sect. 5 , the parser evalu-ation and retraining experiments involving the ungrammati-cal version of the WSJ treebank are presented and discussed. Finally, Sect. 6 summarises the main points in this article and proposes potentially worthwhile further work in this area. 2 An ungrammatical treebank: motivation and background A corpus of ungrammatical sentences is a useful resource, both as a source of evidence for the kind of ill-formed struc-tures that tend to occur in language, and as a source of test and training data for parsers which aim to accurately analyse sentences containing grammatical errors. Since people are able to comprehend text containing grammatical errors, it is reasonable to expect a parser to behave in the same way. A corpus of ungrammatical sentences can take the form of a learner corpus [ 16 , 22 ], i.e. a corpus of sentences produced by non-native learners of the language, or a more general form of error corpus, created by scanning texts for errors [ 1 , 18 ]. Learner corpora are particularly useful in the study of second language acquisition since they provide insight into the difficulties faced by native speakers of a particu-lar language when attempting to learn the corpus language. The more general form of error corpus is unconcerned with whether an error reflects linguistic competence or perfor-mance, it merely records that an error has occurred. Unfortu-nately, the compilation of both kinds of error corpus is a slow process, because it is not enough to merely collect a body of sentences, the grammaticality of each sentence must also be judged in order to determine whether an error has occurred. If an error has occurred, it then must be classified according to some error taxonomy.

A usefully large error corpus, in which every sentence is guaranteed to contain a grammatical error, can be quickly created by automatically introducing errors into a corpus of grammatical sentences. In order to ensure that this transfor-mation process is rooted in linguistic reality, it should, of course, be based on an analysis of naturally produced gram-matical errors. An interesting aspect of the automatically induced error corpus is its parallel nature, since the mean-ing of the ungrammatical sentence can be found by looking at its grammatical counterpart.
An even more useful resource for the devising and testing of robust parsers, is a treebank of ungrammatical sentences. In the same way that a treebank of grammatical sentences can be used as a source of test data to evaluate parser output, and as a source of training data to build a probability model, a treebank of ungrammatical sentences can be used to eval-uate and possibly improve upon a parser X  X  performance on ungrammatical language. In the absence of such a treebank, previous work by the author [ 17 ] attempts to evaluate parser output on ungrammatical data by evaluating it against its own output on a corrected, grammatical version of the same data. Thus, the parser provides its own gold standard. Bigert et al. [ 4 ] adopt a similar approach by introducing artificial spelling errors into error-free text and then evaluating parsers and a part-of-speech tagger on this text using their performance on the error-free text as a reference. Similarly, Lopresti [ 30 ] eval-uates the effect of OCR errors on various NLP tasks such as sentence boundary detection and part-of-speech tagging by calculating the minimum-edit-distance between the original document and the noisy OCRed document. A drawback of these approaches to robustness evaluation is that an applica-tion X  X  performance on noisy data is always evaluated against its performance on a well-formed version of the same data. It can happen that the application (parser, tagger, etc.) can produce an accurate analysis for the ill-formed input but not for the equivalent well-formed input (an example is provided by Foster [ 17 ]) and the evaluation metric must incorporate some kind of estimate of how often this is likely to occur. An ungrammatical treebank bypasses this problem because it serves as a stand-alone and accurate set of reference parses for ungrammatical sentences.

Of course, the creation of a treebank is a costly, laborious task. However, assuming the existence of a treebank of gram-matical sentences and a corpus of ungrammatical sentences derived automatically from the sentences in the grammatical treebank, it is possible to automatically create a treebank of ungrammatical sentences. This treebank can then be parti-tioned in the usual way, into a set of gold standard reference parses and a set of training parses for any data-driven prob-abilistic parser.

The idea of an automatically generated error corpus is not new. Bigert et al. [ 3 , 4 ], for example, automatically introduce spelling errors into texts. Okanohara and Tsujii [ 35 ] gener-ate ill-formed sentences (they use the term  X  X seudo-negative examples X ) using a n-gram language model and they then train a discriminative language model to tell the difference between these pseudo-negative examples and well-formed sentences. Smith and Eisner [ 39 , 40 ] automatically gener-ate ill-formed sentences by transposing or removing words within well-formed sentences. These ill-formed sentences are employed in a unsupervised learning technique called contrastive estimation which is used for part-of-speech tag-ging and dependency grammar induction. The idea of a tree-bank of ungrammatical sentences has been explored before by Kepser et al. [ 28 ], who are responsible for compiling SINBAD, a treebank of German sentences which have been judged to be grammatically deviant by linguists. The SIN-BAD treebank differs from the type of ungrammatical tree-bank which would be produced by the method described here because it is designed to be used more as an informational source for generative linguists rather than as a set of train-ing/test data for a robust parser. It is created manually rather than automatically, and is, thus, limited in size. 3 The data of interest: ungrammatical language It is difficult to provide a satisfactory definition of the term  X  X ngrammatical X : for the purposes of this research, a sen-tence is defined to be ungrammatical if all the words in the sentence are well-formed words of the language in question, but the sentence contains one or more error [ 18 ]. Although this definition simply defines ungrammaticality in terms of error, it is less circular than one in which an ungrammat-ical sentence is defined to be a sentence which cannot be generated by the grammar of the language. An error can take the form of a performance slip which can occur due to carelessness or tiredness, or a competence error which occurs due to a lack of knowledge of a particular construc-tion. This definition includes real-word or context-sensitive spelling errors and excludes non-word spelling errors. It also excludes the abbreviated informal language used in elec-tronic communication [ 10 , 13 , 14 ]. For example, given the well-formed sentence ( 1 ) and the above definition of ungrammatical, sentences ( 2 ) and ( 3 ) are ungrammatical, whereas sentences ( 4 ) and ( 5 ) are not. Sentence ( 2 ) is ungram-matical (according to the definition) because it contains a real-word spelling error and sentence ( 3 ) is ungrammatical because it violates a well-defined word order constraint of English. Sentence ( 4 ) contains an error (a non-word spelling error) but since not all the words in the sentence are well-formed words of the English language, it is not ungrammati-cal according to the above definition. Finally, sentence ( 5 )is not ungrammatical because it does not contain an error: the omission of the first person subject pronoun and the abbre-viation of be to b are well-formed according to the norms of SMS communication. (1) I will be in town soon (2) I will be it town soon (3) I will in town soon be (4) I will be in town soonn (5) Will b in town soon
Previous work by the author [ 18  X  20 ] involved the collec-tion of ungrammatical written sentences in the English of newspapers, academic papers, emails and website forums. The resulting 20,000 word corpus was analysed and the fol-lowing frequency ordering of the three word-level correction operators used to correct a grammatical error was found: substitute (48%) &gt; insert (24%) &gt; delete (17%) &gt; combination (11%) The same ordering of the substitution, deletion and insertion correction operators was found in a study of native speaker spoken language slips carried out by Stemberger [ 41 ]. Foster [ 18 ] found that among the grammatical errors which could be corrected by substituting one word for another (48% of total), the most common errors were real-word spelling errors such as ( 2 ) above (20%), agreement errors (9%) and errors in verb form (5%). In fact, 75% of all errors fall into one of the following five classes: (6) missing word errors: (7) extra word errors: (8) real-word spelling errors: (9) agreement errors: (10) verb form errors: A similar classification was adopted by Nicholls [ 34 ], having carried out an error analysis on a learner corpus. Different languages and text types will exhibit a different error den-sity and distribution. Hashemi [ 23 ], for example, finds that verb form errors are more common than agreement errors in a study of the written language of native Swedish speak-ing children, whereas agreement errors are more common than verb form errors in the English corpus upon which this research is based. Although not a truly representative sample, the corpus described by Foster is compiled from a sufficiently broad array of text types for us to conclude that the most common errors occurring within it are likely to occur elsewhere and for it to form the basis of the arti-ficial error creation procedure described in the next section. 4 Creating an ungrammatical treebank This section describes the procedure for creating an ungram-matical treebank. This procedure involves two steps: the first is the introduction of grammatical errors into the sentences in a treebank; the second is the transformation of the orig-inal gold standard analyses into gold standard analyses for the newly created ungrammatical sentences. The first step is described in Sect. 4.1 , and the second in Sect. 4.2 . 4.1 Automatic error creation The error creation procedure takes as input a part-of-speech tagged corpus of sentences which are assumed to be well-formed, and outputs a part-of-speech tagged corpus of ungrammatical sentences. The error creation procedure is inspired by the manually created error corpus created by Foster [ 18  X  20 ], and the automatically introduced errors take the form of the five most common error types found in this corpus and introduced in Sect. 3 , i.e. missing word errors, extra word errors, real-word spelling errors, agreement errors and verb form errors. The error creation procedure can be applied to its own output to yield sentences with more com-plex errors or with more than one of the above errors.
The top-level algorithm for creating the error corpus is shown in Fig. 1 . The distribution of the five error types is established by setting the values of the four variables miss-ing_freq , extra_freq , real_word_spell_freq and agree_freq . For this research, these relative frequency values are set to approximate the distribution found in the manually created error corpus (see Sect. 3 ).
 4.1.1 Missing word errors Missing word errors can be classified on the basis of the part of speech of the missing word. In the error corpus described by Foster [ 18 ], 98% of the missing word errors involve the omission of the following parts of speech (ordered by decreasing frequency) 1 det ( 28% ) &gt; verb ( 23% ) &gt; prep ( 21% ) &gt; pro ( 10% ) &gt; noun ( 7% ) &gt;  X  X o X  ( 7% ) &gt; conj ( 2% ) Missing word errors are introduced by searching a part-of-speech tagged sentence for all occurrences of words with the above part-of-speech tags and then deleting one from the sentence. The frequency ordering shown above is respected so that the resulting error corpus will contain, for exam-ple, more missing determiners than missing pronouns. In the unlikely event that a sentence contains none of the above parts of speech, no ungrammatical sentence is produced. Another case where no ungrammatical sentence is produced occurs when the input to the procedure is a one-word sentence such as Ye s .

The algorithm for creating missing word errors is detailed in Fig. 2 . The algorithm assumes the availability of miss-ing part-of-speech tag frequencies. As with the top-level algorithm, these are set according to the distribution found in the manually created corpus. 4.1.2 Extra word errors Extra word errors are introduced in the following three ways: 1. Random duplication of a token within a sentence: That X  X  2. Random duplication of a POS within a sentence: There 3. Random insertion of an arbitrary token into the sentence: The procedure considers each of these subclasses of extra word error equally likely, and attempts to insert one of them into a grammatical sentence. Adjectives (e.g. the great great man ) are not considered for duplication because, as with their omission, their repetition will not result in an ungrammat-icality. Apart from the case of duplicate tokens, the extra words are selected from a list of tagged words compiled from a random subset of the British National Corpus [ 7 ]. This random subset contains approximately 2,500 words. Again, the procedure for inserting an extra word is based on the analysis of extra word errors in the 20,000 word error corpus of Foster [ 18 ]. The algorithm is shown in Fig. 3 . Note that it is always possible to generate an extra word error within a sentence because it always possible to insert an arbitrary token at a random position. 4.1.3 Real-word spelling errors An error is classified as a real-word spelling error or context-sensitive spelling error if it can be corrected by a word similar to it in spelling. Two words are considered similar in spelling if the Levenshtein distance between them is one (e.g. to and too )([ 18 ]). Again following the error analysis carried out by Foster [ 18 ], a list of candidate English real-word spelling errors is compiled. The error creation procedure searches for all words in the input sentence which can be replaced by a word similar in spelling (subject to the pre-compiled list): one of these is then randomly selected and replaced. The list of real-word spelling errors contains 113 pairs and a sample of 15 involving function words related to the words is , it , in and if are shown in Table 1 . The list contains very com-mon English words such as a , the and he , and an ungram-matical sentence can be generated from most sentences. The algorithm for inserting real-word spelling errors is shown in Fig. 4 . 4.1.4 Agreement errors Subject X  X erb and determiner-noun number agreement errors are introduced into well-formed sentences by replacing a sin-gular determiner, noun or verb with its plural counterpart, or vice versa. For English, subject X  X erb agreement errors can only be introduced for present tense verbs, and determiner-noun agreement errors can only be introduced for determin-ers which are marked for number, e.g. demonstratives and the indefinite article. The procedure would be more productive if applied to a morphologically richer language. According to the error analysis carried out by Foster [ 18 ], the errone-ous word within an agreement error is more likely to be the rightmost word, i.e. the verb in a subject X  X erb agreement error or the noun in a determiner-noun agreement error. This is reflected in the algorithm for creating agreement errors which is shown in Fig. 5 . 4.1.5 Verb form error Verb form errors are introduced into well-formed sentences by changing the tense of a verb within the sentence, e.g. changing from the present participle verb form laughing to the infinitival form laugh . The procedure for inserting this type of error proceeds by identifying all verbs within a sentence, selecting one of these verbs at random, and replac-ing it with another verb form, also chosen at random. The algorithm is shown in Fig. 6 . Note that some transformations are not carried out, e.g. a present tense verb is not converted to its past form because the transformation will not result in an ill-formed sentence ( They laugh versus They laughed ) and a present tense verb is not converted to its plural or singular counterpart since this is already covered by the agreement error creation module. 4.1.6 Covert errors James [ 24 ]usestheterm covert error to describe a genuine language error which results in a sentence which is syntac-tically well-formed under some interpretation different from the intended one. The tendency of the error creation proce-dure to produce covert errors was estimated by carrying out the following small experiment: sentences were randomly extracted from the BNC and the error creation procedure applied to them. Five hundred of the resulting sentences (the first 100 for each error type) were then manually inspected to see if the sentence structures were grammatical. The percent-age of grammatical structures that are inadvertently produced for each error type and an example of each one are shown below: (11) Agreement Errors, 7% (12) Real-Word Spelling Errors, 10% (13) Missing Word Errors, 13% 2 (14) Extra Word Errors, 5% (15) Verb Form Errors, 6% The occurrence of these grammatical sentences in the artifi-cial error corpus can be reduced by fine-tuning the error cre-ation procedure or by using a finely grained part-of-speech tagset to tag the input corpus. For example, if the tagset could discriminate singular nouns like staff and company which can have a distributive reading from singular nouns such as car which do not, examples like ( 11 ) would not be produced. Similarly, if verb subcategorization frames were available to the error creation procedure, it would know that the verb steer can be used intransitively ( 13 ) or that the verb check can be used with the preposition in ( 14 ). It is unrealistic to assume, however, that covert errors can ever be completely elimi-nated. They are a natural linguistic phenomenon which occur in manually created error corpora containing real errors. One could argue, therefore, that they should not be eliminated. Ideally, a probabilistic parser should be sophisticated enough to favour an ill-formed structure with a plausible reading over a well-formed structure with an unlikely reading. For exam-ple, a parser that suspects that a verb form error has occurred in the right-hand sentence of Example ( 15 ) and interprets the turned as a present participle verb form and back as an adverb is more useful than a parser which interprets no turned back as a noun phrase. 4.1.7 Iteratively applying the error creation procedure The output of the error creation procedure is a tagged cor-pus of ungrammatical sentences (including the covert errors discussed in the previous section). This corpus can then be passed as input to the procedure to create a second corpus with even noisier data. This second corpus will contain sen-tences containing two separate errors such as ( 16 ), or sen-tences such as ( 17 )or( 18 ) which are correctable by applying a combination of the basic insert , delete , substitute correction operators. (16) This roadmap for the project has been derived.  X  This (17) I have problems to get the script to run.  X  I have (18) What does the thing do?  X  What does thing the do? Increasingly noisy corpora can be created by iteratively applying the error creation procedure to its own output. How-ever, if the data is too noisy it will become very difficult for a parser to accurately parse it, just as it becomes very difficult for people to understand extremely ungrammatical sentences such as ( 19 ). Is it reasonable to expect a computer parser to handle language that is problematic for the human parsing mechanism? (19) Hotkey Utility show the indicators on your display and 4.2 Gold standard transformation The gold standard transformation procedure takes an ungram-matical sentence and a gold standard syntactic analysis of the grammatical sentence from which the ungrammatical one has been generated, and outputs a gold standard syntactic analysis of the ungrammatical sentence. The transformation method is based on three assumptions, the third assumption following on from the first two: 1. At the heart of every ungrammatical sentence, there is a 2. The role of a parser is to produce an analysis for a sen-3. A parser which aims to be robust to errors should produce In keeping with these assumptions, the transformation proce-dure operates by changing as little as possible in the original grammatical sentence analysis to produce the analysis of the ungrammatical sentence. Ungrammatical treebanks can be automatically generated from any type of treebank, regard-less of the syntactic annotation scheme it employs. However, in this article, attention is restricted to context-free phrase structure trees. Examples are provided for the error types described in Sect. 4.1 . 4.2.1 Real-word spelling errors, agreement errors and verb Consider the grammatical sentence ( 20 ) and the ungrammat-ical sentence ( 21 ) which contains a real-word spelling error: (20) A romance is coming your way. (21) A romance in coming your way.

Figure 7 depicts a Penn-treebank-style gold standard parse tree 4 for the grammatical sentence ( 20 ) and, underneath it, the parse tree which will be produced by the transformation procedure for the ungrammatical sentence ( 21 ). This is con-sidered to be the gold standard parse for the ungrammatical sentence because it makes the crucial recognition that the word in is part of a verb phrase and contrasts in this way with another parse for the same sentence, shown in Fig. 8 , in which the sequence in coming your way is analysed as a prepositional phrase. A parser which produces the parse in Fig. 7 is robust to errors since it is able to see right through an ungrammatical sentence to the grammatical sentence at its heart, and produce a parse which reflects the meaning of the grammatical sentence. 5 The example sentence ( 21 ) con-tains a real-word spelling error but the same transformation would apply to any error correctable by a substitution, e.g. an agreement or a verb form error. This transformation is the substitution of the erroneous word onto the original word in the original analysis. 4.2.2 Missing word errors Consider the grammatical sentence ( 22 ) and its ungrammat-ical counterpart ( 23 ): (22) Prices are expected to drop . (23) Prices are expected drop .
 A gold standard parse tree for the grammatical ( 22 )isshown in Fig. 9 , with the gold standard parse tree which will be auto-matically generated for the ungrammatical ( 23 ) underneath. The bottom tree is produced by replacing the pre-terminal category (TO to) in the top tree in Fig. 9 with the trace (-NONE-0) . In contrast, Fig. 10 shows a less accurate parse tree for Sentence ( 23 ). 4.2.3 Extra word errors Consider the grammatical sentence ( 24 ) and the ungrammat-ical sentence ( 25 ) which contains an unnecessary extra word to : (24) Annotators parse the sentences. (25) Annotators parse to the sentences.
 Figure 11 shows the gold standard parse tree for the gram-matical ( 24 ), along with the two gold standard parse trees which will be generated automatically by the transformation procedure for the ungrammatical ( 25 ). In the ungrammat-ical gold standard trees, the superfluous to does not affect the constituent structure of the sentence (above the pre-terminal level). The only difference between the two trees is the level where the word to is attached. In both, to has not introduced any extra structure, which is a desirable result since the word does not contribute to the sentence X  X  mean-ing. Contrast this with the parse tree in Fig. 12 , in which the presence of the word to has caused a prepositional phrase to be introduced. 4.2.4 Tree transformation algorithm The tree transformation algorithm is shown in Fig. 13 .The top-level procedure takes as input a treebank tree, t , and a part-of-speech tagged sentence, s . The sentence s is the ungrammatical version of the sentence dominated by t .The procedure returns a list of ungrammatical trees dominating s . A list of trees rather than a single tree is returned because for extra word errors there can be more than one way of trans-forming the original tree to describe the ungrammatical sen-tence while preserving the intended meaning (see Fig. 11 ). If s contains a missing word error, a real-word spelling error, a verb form error or an agreement error, the list returned will always contain only one tree. Note that the algorithm assumes that the variables term_count and count are global variables available to all procedures. 4.2.5 Iteratively applying the gold standard transformation Just as the error creation procedure can be applied to its own output to create increasing levels of grammatical noise, the gold standard transformation algorithm can take as input a tree that it has already undergone transformation and produce another tree as output. Consider, for example, the ungram-matical sentence ( 21 ) repeated as ( 26 ) and sentences ( 27 ), ( 28 ) and ( 29 ) which are the result of introducing a second error into sentence ( 26 ): (26) A romance in coming your way. (27) A romances in coming your way. (28) A romance in coming way. (29) A the romance in coming your way. In order to produce gold standard parse trees for sentences ( 27 ), ( 28 ) and ( 29 ), the input to the tree transformation pro-cedure is the bottom tree in Fig. 7 . The output trees are shown in Fig. 14 .
 5 Experiments with an ungrammatical Penn treebank In this section, the usefulness of an automatically created ungrammatical treebank is demonstrated by describing a parser evaluation experiment and a parser retraining experi-ment which are carried out using an ungrammatical version of the WSJ portion of the Penn Treebank [ 31 , 32 ]. The eval-uation experiment is described in Sect. 5.1 and the retraining experiment in Sect. 5.2 . 5.1 Parser evaluation The aim of this experiment is to evaluate how well two lex-icalized, history-based, generative, statistical parsers cope with errors in text: a parser that copes well with errors pro-duces, for an ungrammatical sentence, an analysis which closely resembles the analysis it would produce for the sen-tence without the error.
Section 5.1.1 contains a description of how the experiment was carried out and Sect. 5.1.2 presents the results, which are then discussed in Sect. 5.1.3 . 5.1.1 Method The error creation procedure described in Sect. 4.1 is applied to the 2,416 sentences in Sect. 23 of the WSJ portion of the Penn Treebank [ 31 , 32 ], resulting in an error corpus of 2,416 sentences (133 sentences containing a verb form error, 234 sentences containing an agreement error, 511 sentences con-taining a real-word spelling error, 613 sentences containing an extra word and 925 sentences with a missing word). The gold standard transformation procedure described in Sect. 4.2 is then applied, resulting in an ungrammatical version of WSJ Section 23. A second noisier test set is created by applying the error creation and gold standard transformation proce-dures to the first ungrammatical version of WSJ Section 23 (see Sects. 4.1.7 , 4.2.5 ).

Two statistical parsers trained on the original grammat-ical WSJ Sections 2 X 21 are used to parse the ungrammati-cal sentences. The first parser is Bikel X  X  implementation of Collins X  generative head-driven probabilistic Model 2 [ 5 , 12 ]. The second parser is the June 2006 version of Charniak and Johnson X  X  two-stage parser [ 9 ]. The first-stage is a lexical-ized, generative, probabilistic parser [ 8 ] and the second stage is a maximum entropy reranker which exploits features of the entire parse tree to reorder the n-best parse trees produced by the first stage parser [ 9 , 11 , 27 ]. For this experiment, the input to both parsers is untagged.

The parses produced by both parsers are evaluated against the ungrammatical gold standard WSJ23 parses using the Parseval [ 6 ] labelled precision/recall metric. According to this metric, a constituent in a test parse tree is considered to be correct if it spans the same sequence of words and has the same label as a constituent in the corresponding gold stan-dard parse tree. Precision represents the number of correct constituents divided by the total number of constituents pro-duced by the parser. Recall represents the number of correct constituents divided by the total number of constituents in the gold standard set. The f-score is the harmonic mean of precision and recall. In the case of extra word errors, there is potentially more than one gold standard analysis for each sentence (see, for example, Fig. 11 ), and therefore the test sentence parse is evaluated against each of its gold standard parses, and the highest f-score is chosen. 5.1.2 Results Table 2 shows labelled precision, recall and f-score results calculated by evaluating the two parsers against the two ungrammatical versions of WSJ23 using the Parseval mea-sures.  X  X ngrammatical_1 X  is the test set produced by applying the error creation and gold standard transformation procedures to the original grammatical WSJ23.  X  X ngram-matical_2 X  is the test set produced by applying the error creation and gold standard transformation procedures to Ungrammatical_1. The first row in Table 2 indicates the scores received by the parsers on the original grammatical WSJ23 sentences. The first row figures represent an upper bound for the ungrammatical sentence results, because, as was illustrated in Sect. 4.2 , the grammatical and ungrammati-cal gold standard trees are isomorphic above the pre-terminal level and pre-terminal constituents or part-of-speech tags are ignored in calculation of constituent accuracy. Table 3 gives a breakdown of the Ungrammatical_1 results by error type. 5.1.3 Discussion As might be expected, the presence of a grammatical error in a sentence has an adverse effect on both parsers. The effect on both is quite similar, with an absolute f-score deterioration of 4.9% on Ungrammatical_1 for Bikel X  X  parser, and an abso-lute f-score deterioration of 4.8% for Charniak and Johnson X  X  parser. For both parsers, there is an even larger deterioration for Ungrammatical_2, 10% for Bikel X  X  parser and 9.4% for Charniak and Johnson X  X  parser. Again, this is unsurprising, since Ungrammatical_2 contains noisier sentences than the sentences in Ungrammatical_1: each has an edit distance of two from the original grammatical WSJ23 sentences.

The results in Tables 2 and 3 show that ungrammatical sentences containing agreement errors achieve scores which are the closest to the upper bound, suggesting that this type of error does not generally distract these parsers from finding the correct analysis. For both parsers, there is no signifi-cant difference between the results for subject X  X erb agree-ment errors and those for determiner-noun agreement errors. Real-word spelling errors are a problem for both parsers. It is not surprising that this error type performs badly, since the replacing word and the replaced word often have little in common grammatically. The worst performing error type for Bikel X  X  parser is the wrong verb form error. Interestingly, this is not the case for Charniak and Johnson X  X  parser. In par-ticular, Charniak and Johnson X  X  parser is more robust than Bikel X  X  parser to verb form errors involving the conversion of either an infinitival or present tense indicative verb form to a present participle form. Examples are shown in ( 30 ) and ( 31 ): (30) infinitival  X  present participle (31) present indicative  X  present participle
For both parsers, extra word errors achieve a higher recall score in comparison to their precision score which suggests that this kind of error tends to introduce unwanted structure into a parse. The two parsers X  scores for extra word errors are slightly lower when the extra word is a random token inserted at an arbitrary point in the sentence, as opposed to extra word errors involving adjacent duplicate tags or tokens.
For both parsers and in contrast to extra word errors, miss-ing word errors achieve a higher precision score in compar-ison to their recall score, suggesting that a lack of relevant structure is associated with this kind of error. This is what one might expect. Analysing the missing word errors by the part of speech that is omitted, it is clear that both parsers cope well with the omission of determiners and nouns, and less well with the omission of verbs, prepositions and conjunc-tions. Of all the error types, the missing word error is most associated with covert errors (see Sect. 4.1.6 ). A likely expla-nation for the better performance of errors involving missing nouns is that their omission is more inclined to result in a grammatical structure, e.g. in a noun-noun compound or as objects to verbs with both transitive and intransitive uses (see Example ( 13 )). An error involving a missing determiner is likely to be contained locally within a noun phrase and less likely to affect the parsing of other constituents in a sentence, as would appear to be the case for missing verbs, prepositions and conjunctions. 5.2 Parser retraining The aim of this experiment is to determine the effect of training the two parsers evaluated in Sect. 5.1 on an ungram-matical version of the WSJ. It is expected that this will have a positive effect on the parsers X  performance on the ungrammatical WSJ Section 23 sentences. Sect. 5.2.1 con-tains a description of how the experiment was carried out, Sect. 5.2.2 presents the results, and these results are discussed in Sect. 5.2.3 . 5.2.1 Method The error creation and tree transformation procedures described in Sect. 4 are applied to Sects. 2 X 21 of the WSJ portion of the Penn Treebank. Following the error analy-sis carried out by Foster [ 18 ], 4 of the 20 training sections (Sects. 2 X 5) contain sentences with more than one error or an error correctable by two applications of the insert , delete or substitute correction operators. This is achieved by apply-ing the error creation and tree transformation procedures to Sects. 2 X 5 and then applying the procedures again to their own output (see Sects. 4.1.7 , 4.2.5 ). The parsers evaluated in Sect. 5.1 are trained on this new training set. In a sec-ond experiment, the ungrammatical training set is combined with the original grammatical WSJ Sections 2 X 21, and the two parsers are trained on this combined set. 5.2.2 Results The overall results for the parser retraining experiments are shown in Table 4 . The first column of results represent the scores for both parsers on the three test sets before the retrain-ing and are a repeat of the results shown in Table 2 . The mid-dle column of results are those achieved after the parsers have been retrained on the ungrammatical WSJ2 X 21 only, and the third set of results are those achieved when the parsers are retrained on a combination of the ungrammatical WSJ2 X 21 and the original WSJ2 X 21. A breakdown for individual error types is shown in Table 5 . 5.2.3 Discussion As expected, both parsers achieve an improved f-score on the ungrammatical test data, when trained on ungrammatical data alone. For the Ungrammatical_1 test set, Bikel X  X  parser achieves an absolute improvement of 1.2%, and Charniak and Johnson X  X  parser achieves an absolute improvement of 1.4%. The improvement is statistically significant for both Bikel X  X  parser ( p &lt; 0 . 0001 for precision and p &lt; for recall) and Charniak and Johnson X  X  parser ( p &lt; 0 . for precision and recall). 6 For the noiser Ungrammatical_2 test set, the improvements are even greater, 3.9% for Bikel X  X  parser and 3.7% for Charniak and Johnson X  X  parser. Both improvements are statistically significant ( p &lt; 0 Unsurprisingly, when trained on ungrammatical data alone, the performance of both parsers on grammatical sentences is negatively affected. There is an absolute deterioration of 1.7% for Bikel X  X  parser and an absolute deterioration of 1.2% for Charniak and Johnson X  X  parser. The deterioration is sta-tistically significant for both parsers ( p &lt; 0 . 0001).
The situation improves (although not quite as much as hoped) when the parsers are trained on both the grammatical and ungrammatical versions of WSJ2-21. For Ungrammati-cal_1, Bikel X  X  parser achieves a statistically significant ( p 0 . 0001) improvement of 1.7% over the baseline of training on the original grammatical WSJ2-21. Charniak and Johnson X  X  parser achieves an improvement of 1.6%, also statistically significant ( p &lt; 0 . 0001). For Ungrammatical_2, there is an improvement of 4.3% for Bikel X  X  parser and 4.0% for Char-niak and Johnson X  X  parser, both improvements statistically significant ( p &lt; 0 . 0001). For the original WSJ23, the results for both parsers still, unfortunately, represent a decrease from the baseline f-score results:  X  0.8% for Bikel X  X  parser and  X  0.5% for Charniak and Johnson X  X  parser. Although these are small differences, they are statistically significant ( p 0 . 0001 for recall and p &lt; 0 . 03 for precision for Charniak and Johnson X  X  parser, p &lt; 0 . 0001 for precision and recall for Bikel X  X  parser). Training on a combination of grammati-cal and ungrammatical data seems to be better than training on ungrammatical data alone but it is clear from these results that a more sophisticated approach to combining both types of training data is required. An attempt was made to weight the training material in favour of the grammatical sentences by using ten copies of the original WSJ2-21 sentences, but this did not improve results.

The breakdown by error type shows very similar trends for both parsers. Comparing the first and third columns in Table 5 , we can see that the most pronounced improvement is shown for real-word spelling errors, with Bikel X  X  parser achieving an absolute f-score increase of 4.2% and Charniak and Johnson X  X  parser achieving an absolute f-score increase of 6.0%. The parsers have learnt new part-of-speech tags for certain words and this knowledge can lead to improved parse trees for ungrammatical input. ( 33 ) is an example: the knowl-edge that where can sometimes be confused with the verb were has allowed Charniak and Johnson X  X  parser to correctly posit a verb phrase, where previously it did not ( 32 ): (32) (S But (NP There) (WHADVP where ) (NP no buyers)) (33) (S But (NP There) (VP where (NP no buyers)))
If the parsers were initially robust to a particular error type, the inclusion of ungrammatical sentences and their trees in the training data does not help. In fact, it causes a deteri-oration, just as it causes a deterioration for the grammati-cal sentences. For both parsers, this happens for agreement errors and for certain types of missing word errors (miss-ing nouns and determiners). It happens to a certain extent for verb form errors with Charniak and Johnson X  X  reranking parser, although the net effect of combining grammatical and ungrammatical parse trees in the training material is positive. The parse trees ( 34 ) and ( 35 ) are examples of how the inclu-sion of ungrammatical material in the training set can con-fuse the parsers: Charniak and Johnson X  X  parser successfully ignores the agreement error in the sentence ( 34 ) when it is trained on grammatical data alone, but when it is trained on a mixture of grammatical sentences and various kinds of ungrammatical sentences, it finds the analysis ( 35 ) in which allocates is a noun and a verb has been omitted more probable than the one in which it is a singular or plural verb. (34) (S (NP The negotiations )(VP allocates (NP about (35) (S (NP The negotiations allocates ) (VP (NP about 6 Concluding remarks This article has introduced the concept of an automatically generated treebank of ungrammatical sentences. The purpose of such a treebank is to provide a source of ungrammati-cal test and training data for parsers. The treebank creation procedure involves the automatic introduction of common grammatical errors into the sentences in an existing treebank and the automatic transformation of the treebank analyses into analyses of the automatically generated ungrammati-cal sentences. The basic idea is that the original analysis is changed in as minimal a way as possible, so that the same semantic representation can be derived from an ungrammat-ical sentence parse as from its grammatical counterpart. The research described in this article has focused on five gram-matical error types which occur frequently in written text: missing word errors, extra word errors, real-word spelling errors, agreement errors and verb form errors. By applying the error creation and tree transformation to its own output, errors involving some combination of the above five error types can also be handled.

This article described two experiments which employed an ungrammatical version of the WSJ section of the Penn treebank. The first experiment was a parser evaluation exper-iment which tested the performance of two state-of-the-art WSJ-trained statistical parsers on sentences containing ungrammatical data. The Parseval metric was used to perform the evaluation, but the experiment could be repeated using another constituency-based metric such as the Leaf Ances-tor metric [ 37 ], or by transforming the constituent analyses into dependency analyses [ 26 ]. The evaluation experiment showed that both parsers are fairly robust to grammatical noise, in particular to sentences containing agreement errors. The second experiment tested these two parsers on the same ungrammatical data, after retraining them on noisy versions of WSJ2 X 21. When trained on ungrammatical parses alone, the parsers performed slightly better on ungrammatical data and slightly worse on grammatical data, compared to when they were trained on grammatical parses alone. Combin-ing both sets of training data yielded small but encouraging improvements for both the grammatical and ungrammatical test data.

An immediate future aim is to see if the retrained parsers behave in the same manner when faced with real errone-ous data. The problem here is the time it will take to annotate the erroneous data, a motivation for automatically creating an ungrammatical treebank in the first place. Nevertheless, some evaluation on real data is necessary to confirm the results presented here. Another challenge remains to improve per-formance on ungrammatical data without any adverse effect on grammatical data and a possible way forward is a two-stage parsing model, in which the grammar derived from the ungrammatical treebank is only employed when there is reason to suggest that the input sentence may contain an error. This is a matter for further research.
 References
