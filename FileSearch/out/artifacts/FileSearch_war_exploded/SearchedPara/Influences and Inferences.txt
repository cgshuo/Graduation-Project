 Information Sciences Institute University of Southern California
Marina del Rey, California 1. False and True Starts
I am deeply honored to receive the ACL X  X  Lifetime Achievement Award. I X  X  especially honored when I look back at the list of previous winners X  X huck Fillmore, Eugene they X  X e all my heroes.
 to take part in the conversation, and an award like this means that you X  X e taken part in the conversation.

For me, it all begins before I was born. My grandfather, a crusty old country lawyer in southern Indiana, told my father not to bother trying to go to law school.  X  X ou don X  X  know English grammar, X  he said.  X  X ou X  X l flunk out. X  My dad accepted the challenge, bought a book entitled English Grammar , by Smith, Magee, and Seward (1928), and mastered it. He went on to become a very successful lawyer.
 English classes looked to him more like social studies, and barely touched on grammar.
So he persuaded me X  X ctually, he probably bribed me, but I can X  X  remember what with X  X o master that same book, English Grammar by Smith, Magee, and Seward. This was a concession, because I was a math nerd, reading only textbooks on trigonometry and calculus, as my way of avoiding the humiliation of playing baseball. But I read the book, and I was amazed. English grammar was just like math! It had the same sorts of rules, the same kinds of abstractions, the same types of puzzles. It was actually fun!
Kuder Preference Test, which would help us decide what career to choose. I scored high in math and in language. So my high school counselor told me I should write math books. In fact, she got it exactly backwards. It wasn X  X  that I should do language about math. It was that I should do math about language.
 up not knowing whether they wanted to be a physicist or a poet. They just knew both sounded fascinating. Then they discovered our field.
 battery of aptitude tests to see what specialties we X  X  be best for. One of the tests was to see if we should be sent to the Monterey Language School. Looking back on it, I realize now it was testing how well you could understand formal language theory. They X  X  give you a bunch of rules for an artificial language, and you X  X  have to say whether different strings were in or not in the language. I X  X  never seen anything like it, but it was really fun to do. Later I met with a personnel specialist who went over my test scores. I got a 46 out of 50. He ignored that until I pointed it out to him. Then he said,  X  X hat X  X  a mistake. Nobody ever gets more than 6 or 7 points on that test. X  You X  X e going to South Vietnam. X  two years in South Carolina, and was glad to be there. How I managed that is a story for another occasion.
 uate school at New York University. In October I passed my oral exam in topics like algebraic topology and complex analysis, by one generous yes and two abstensions. In the subsequent months I discovered more and more facts about myself X  X or example, fascinating though recursion theory might be, I was never going to prove a theorem that Hartley Rogers would be compelled to include in his next edition. As I surveyed vaguely plausible fields, I realized I had no idea what the next problem to solve would be or even what makes a problem interesting.
 ered New York University X  X  best-kept secret: Naomi Sager X  X  Linguistic String Project. I think it is also computational linguistics X  best kept secret as well. She was motivated by the science, not by the performance, and her very impressive work is nowhere near as well-known as it should be. I think her Linguistic String Grammar (Sager 1981) ranks, as a computational specification of English syntax, with Pollard and Sag X  X  Head-driven
Phrase Structure Grammar (1994), for thoroughness, insight, and elegance. So, for exam-ple, in 1992 when we developed the FASTUS system for information extraction using cascaded finite-state transducers (Hobbs et al. 1997), it was straightforward to copy the rules for Noun Groups straight from her grammar. It X  X  no accident that in the late 1980s during the Strategic Computing Initiative and in the early 1990s in the Message Under-standing Conferences, three of the most important efforts were led by Linguistic String
Project alumni X  X alph Grishman X  X  group at New York University, Lynette Hirschman X  X  at Unisys, and my group at SRI International. I think the most important lesson I learned from Naomi Sager was to look closely at the data and to take it seriously. took a course in logic from him. I knew about his book on compilers and the classic
Dunford and Schwartz on functional analysis. But when I saw his book on mathematical economics and his book on the theory of relativity, I did some research to see if there was more than one Jack Schwartz. Among his writings was an unpublished Chapter 9 of his compilers book, on parsing natural language, which I of course read. apparent that the constraints on phrase structure rules had to be expressed and that one could do that with fairly simple operations on vectors of features, where among the features were what I called the  X  X ores X  of the constituents, since they bundled many of the relevant features. My  X  X ore X  was what linguists came to call  X  X ead. X  Years later,
I ran across Chapter 9 again and reread it, and realized that all the ideas in my thesis were there. So when in 1987 Schwartz told someone that I had anticipated head-driven phrase structure grammar, that was his way of saying he had anticipated head-driven phrase structure grammar. 782 that syntax was a solved problem X  X omething I still believe. But that left me adrift for problems to work on. I became discouraged, and found myself thinking again about driving that taxi. Then late one afternoon, just as I was about to go home, a graduate student named Fred Howard came into my office to ask a couple of questions. That triggered a discussion that lasted until 11 o X  X lock that evening. One of the wheels we (This was before Lakoff and Johnson [1980], but after similar observations by the 18th-century Italian philosopher Giambattista Vico (1968 [1744]) and the 20th-century
English literary critic I. A. Richards [1936].) But within a year, everything else of value that remained of the content of that discussion could be compressed into a long footnote in a technical report. In any case, this conversation lit a fire that fueled my research for the next 15 or 20 years.
 them. No doubt influenced by Chuck Rieger X  X  thesis (Rieger 1974), I asked what infer-ences we draw in the course of comprehension, and, an issue Rieger did not address, what inferences we do not draw. This culminated in 1976 in an unreadable (and unread) technical report (Hobbs 1976), microanalyzing one paragraph from Newsweek ,tryingto specify every bit of knowledge required for understanding the text and describing how every linguistic problem in the text invokes that knowledge to arrive at solutions. One could say that the rest of my career has been a matter of cleaning up and extending that technical report, in terms of representation, the process of inference and interpretation, and the specification of common-sense knowledge. 2. Representation
In 1977 I moved to SRI, where I fell under the influence of Nils Nilsson and Bob Moore, and of John McCarthy at nearby Stanford. They were campaigning to replace the ad hoc styles of representation of early AI with representations based on first-order logic. But the problem in a nutshell is this: When we are trying to represent an English sentence like Pat believes Chris is tall , we really want to write (1) believe(Pat, tall(Chris))
The difficulty is that tall is a predicate and tall(Chris) evaluates to true or false, so we are left with Pat believes a truth value, with not a hint of Chris X  X  tallness to be found. A common solution to this is to treat believe not as a predicate but as an opaque operator that blocks evaluation of its operands.
 about modal and temporal logics, Russell X  X  iota operator, functionals, lambda expres-sions, and so on, we might represent the sentence (2) Maybe the boy wanted to build a boat quickly. by the expression (3) (  X  x : BOY )[ PAST [ WANT ( x ,  X  z [(  X  y : BOAT ) Quick ( build )( z , y )]]]
This bothered me because it seemed like we were introducing a new operator with its own special logic every time we encountered a new word to define or characterize. For 20,000 words would we have to introduce 20,000 new operators? It seemed to me that we should rather stay within first-order logic, abiding by two principles: 1. All morphemes are created equal. 2. Every morpheme conveys a predication.
 we can represent Pat believes Chris is tall by (4) believe ( Pat , e )  X  tall ( e , Chris )
Sentence (2) is then represented (5) maybe ( e 5 )  X  the ( x , e 3 )  X  boy ( e 3 , x )  X  want ( e
There X  X  nothing exotic here (other than reification). It X  X  all first-order logic, predicates applied to arguments where the arguments are existentially quantified variables with widest possible scope, ranging over a universe of possible individuals.
 information is being conveyed by the word the . It is a relation between an entity x and a description e 3 , and it says the entity is uniquely mutually identifiable in context by means of the description. We can give this relation a name. We could call it something like uniquely-mutually-identifiable-in-context . But why not keep it simple, and name the predicate after the morpheme that conveys it  X  the ?  X  X avidsonian, X  after the philosopher Donald Davidson (1967), who proposed reifying would not have treated Chris X  X  tallness as a thing. By contrast, I adopted a position that, because I was young and wild, I called  X  X ntological promiscuity. X  Now that I X  X  older and more domesticated, I would probably call it something like  X  X ntological prosperity X  or  X  X ntological comfortable circumstances X  or maybe  X  X ntological glut. X  by the near solipsism that infected many researchers in the early days of AI. Our brains could be fooling us, just as we often fool computers to test our programs. Yes, there is probably a world out there that occasionally bites back. But the world is benevolent X  after all, we evolved in it. When we breathe, there is almost always oxygen there. That X  X  no accident. So it doesn X  X  matter very much what we believe. We can believe all sorts of crazy things and be completely ignorant of apparently real and pervasive phenomena.
Until the recent past we believed in the spirits of the dead, and we were entirely ignorant physical objects, sets, numbers, and possible worlds, what ontological scruples do you have anyway? So why should we give any credence at all to our intuitions about what exists and what doesn X  X ? Why not simply stipulate that everything that can be talked about exists in a Platonic universe of possible individuals, since that makes it so much easier to represent and reason about the content of natural language discourse? propositions, with one predication per morpheme. 784 (6) John is tall. would be represented (7) John ( e 1 , x )  X  tall ( e 3 , x ) whereas the sentence (8) John is not tall. would be represented (9) John ( e 1 , x )  X  not ( e 2 , e 3)  X  tall ( e 3 , x ) But P  X  Q  X  R implies P  X  R , so it would seem that John is not tall implies John is tall . eventuality of x  X  X  being tall. The eventuality e 3 may or may not exist in the real world, and if it does, that is one of its properties  X  Rexist ( e tence (6) is e 3 , the tall-ness, while the claim of sentence (8) is e tall-ness.
 as follows: Step 1: Identify the claim.
 Step 2: Propagate truth and falsity through implicatives.

For example, in (10) The lazy man did not manage to avoid attending the meeting.

Step 1 says the claim is the  X  X ot. X  Step 2 says that therefore  X  X anage X  is false,  X  X void X  true.
 of compositional semantics. In traditional approaches to compositional semantics, the meanings of constituents are lambda expressions, and composition happens by func-identifying variables with each other. This gives us a two-part account of compositional semantics. 1. The lexicon provides predicate X  X rgument relations. 2. Syntax identifies variables.

For the sentence (11) The man attended the meeting. ignoring the and tense, we get from the individual words the propositions (12) man ( e 1 , x 1 ), attend ( e 2 , x 2 , y 2 ), meeting ( e
When we recognize that attended the meeting is a verb phrase, this amounts to recogniz-ing that y 2 = y 3 . When we recognize the man attended the meeting as a clause, we have recognized that x 1 = x 2 . 3. Interpretation
In 1979 and 1980, I had the huge good fortune to participate in a biweekly discussion group on discourse, alternating between Stanford and Berkeley, consisting of some of the most illustrious scholars of language in the world, including Mike Agar, Dwight Bolinger, Eve and Herb Clark, Chuck Fillmore, Paul Kay, George Lakoff, Geoff Nunberg,
Ivan Sag, Dan Slobin, Elizabeth Traugott, and Tom Wasow. For me personally, the high point in these meetings, and one of the high points in my entire career, was when the sociologist Irving Goffman, visiting Berkeley at the time, used my paper  X  X onversation as Planned Behavior X  (Hobbs and Evans 1980) as a club to beat the sociolinguist John
Gumperz over the head with. Metaphorically speaking. We read and discussed mem-bers X  papers on interpreting nominal compounds, metonymy or deferred reference, de-nominalized nouns, metaphor, and other phenomena that came to be clustered by linguists under the name of  X  X adical Pragmatics X  (Cole 1981). (I thought a better name would be  X  X un-of-the-mill AI X .) set of inferences we draw as we understand a text. The answer that seemed most promising was that we need to draw those inferences required to resolve interpre-tation problems of the sort we were examining in the discussion group. But what systematicity was there to this set of problems? How would you know if your list was complete? cations, that is, a predicate applied to one or more arguments  X  p ( x ). This gives rise to three sorts of problems: 1. What is the predicate? What is p ? This question subsumes the problems of 2. What is the argument? What is x ? This question subsumes the problems of 3. In what way are the predicate and argument congruent? What about p and presented within the scope of single sentences, but they often require for their solution the entire discourse, the external context, and world knowledge. (My term never caught on probably because no one else saw this class of problems as a natural kind.) 786 course, in particular, that structure arising out of coherence relations between discourse segments. In this I was very much influenced by the work of the linguists Joseph Grimes (1975) and Robert Longacre (1976). I began collaborating with the anthropologist Mike Agar around this time, and we called this level of structure  X  X ocal coherence X  (Agar and Hobbs 1982).

Newman 1978) at BBN were doing very exciting work analyzing the structure of of planning from artificial intelligence. In work with David Evans and work with tion and to ethnographic interviews. Agar and I called this level of structure  X  X lobal coherence. X  ence, and global coherence X  X t was clear that a key role was played by the notions of implicature (Grice 1975), accommodation (Lewis 1979; Thomason 1985), and abduction (Peirce 1955). To solve even elementary problems like pronoun coreference, one had to make assumptions to get a good interpretation of the text, where the only justification for the assumptions was that they led to a good interpretation.
 ple, medical diagnosis (Pople 1973; Cox and Pietrzykowski 1986), and contemporary philosophers like Paul Thagard (1978), as well as work by Wilensky and Norvig at
Berkeley (Wilensky 1983; Norvig 1987) and Charniak and Goldman at Brown (Charniak and Goldman 1988) that seemed to be taking an approach similar to ours. Among the people in our group were Mark Stickel, Doug Edwards, and the pragmatics scholar
Steve Levinson, who was visiting Stanford at the time. We argued about what we were calling identity implicatures and referential implicatures, and about how to distinguish new from given information in discourse, and how to choose the best interpretation of a text.
 that he thought he had the answer to all our problems. He described his algorithm for weighted abduction. It struck me immediately as the double helix of computational linguistics, a feeling that has not entirely abandoned me today. First of all, it gave us a characterization of what constituted the interpretation of a stream of discourse. It gave us a clear criterion for what inferences to draw and not draw. The interpretation was the most economical explanation for what would make the text true, and an inference was appropriate if and only if it contributed to that explanation.
 few days, I saw how one would approach all the local pragmatics and local and global coherence problems in this framework. In discussions with Stu Shieber in the next few days it became apparent how one could integrate syntax smoothly into the framework. A big picture emerged (Hobbs et al. 1993).
 (quite obsolete now). It showed a man standing by the ocean, holding a camera, and by coming up with the best explanation for the observables (abduction). There was possible explanations. Maybe someone chopped the branch down, and maybe the boat was lifted into the tree with a crane. But this is not as good an interpretation because we have to assume two things (the chopping and the crane) rather than just one (the storm). The first interpretation is better because it is more economical. Less explains more.
 means there was an ad agency involved in posing the picture, and they very well could have done the chopping and used the crane, rather than wait for the rare event of a storm to arrange the picture for them.
 of the picture, thereby explicating the information conveyed by the picture. We could call the second explanation the  X  X ntentional X  one. It explains why the message occurs at all. Note that both interpretations need to be discovered if the advertisement is to be fully appreciated.
 machine, continuously trying to prove abductively (i.e., by making necessary assump-tions) that the observables in its environment constitute a coherent situation. (We can encompass action as well as perception by adding to what is proved the proposition that the owner of the brain will thrive in that situation.) 788 of words w . Generally the best explanation for an utterance is that it is an intentional act aimed at conveying information. We can capture this with the axiom (13) Segment ( w , e )  X  goal ( i , c )  X  cog ( c , u , e ) speaker i has the goal c that a hearer u adopt some cognitive stance toward e , then (defeasibly) i will utter to u the string of words w . The first conjunct in the antecedent is the entry point into the informational side of an interpretation: What is the content of the message? The second two conjuncts are the entry point into the intentional side: Why is the speaker conveying this content? in, or is a subgoal of, a larger plan the speaker is executing in the world. This is where that reasoning occurs. It encompasses what Agar and I called  X  X lobal coherence X  X  X ow does the utterance fit in with what else is going on in the world? into smaller segments, using the axiom (14) Segment ( w 1 , e 1 )  X  Segment ( w 2 , e 2 )  X  CoRel ( e
This axiom says that if w 1 is a segment describing situation e describing situation e 2 , and there is a relation between e is a segment describing a situation e somehow derivable from the relation. When we backchain on this axiom, we are explaining an interpretable segment of discourse by breaking it into parts, explaining the parts, and explaining the relation between them. between two states or events: causality, similarity, identity, a strong sort of temporal suc-cession I have called  X  X ccasion, X  the figure X  X round relation, and predicate X  X rgument relations. These are similar to other catalogues of discourse relations that others have come up with. However, the intent is to capture the information that can be conveyed by adjacency. By contrast, the relations of Rhetorical Structure Theory (Mann and
Thompson 1986) are a mixture of informational relations like similarity and intentional what the speaker is using adjacency to do. Often the coherence relation conveyed by adjacency is expressed redundantly (and with less ambiguity) in a conjunction ( so ), an problem, assuming the two do not conflict; discourse is rife with redundancy. bottoms out in individual clauses, and this is where syntax takes over. Adjacency in larger stretches of discourse can convey a variety of possible relations. As we saw at the end of Section 2, adjacency within clauses conveys predicate X  X rgument relations. Syntax relations with the rather crude device of concatenation. The best explanation of a clause is the decomposition given to us by compositional semantics. The best explanation for an individual morpheme is that it is intended to convey its corresponding predication. Thus, the syntactic analysis of a clause bottoms out in its logical form. the  X  X nterpretation as Abduction X  framework that the best abductive proof (i.e., the best explanation) of the logical form solved the local pragmatics problems as a side effect.
I won X  X  make an extended argument for that here, but one example should convey the basic idea.
 (15) The plane taxied to the terminal. terminal could be an airport terminal or a computer terminal, and taxiing could be a plane moving on the ground or a person riding in a cab.
 airplane ( x )  X  plane ( x ) wood-smoother ( x )  X  plane ( x ) airport-terminal ( y )  X  terminal ( y ) computer-terminal ( y )  X  terminal ( y ) move-on-ground ( x , y )  X  airplane ( x )  X  taxi ( x , y ) ride-in-cab ( x , y )  X  person ( x )  X  taxi ( x , y ) together with a rule that says airports have airplanes and airport terminals. airport ( z )  X  airplane ( x )  X  airport-terminal ( y ) disambiguated as a by-product by virtue of the axioms that are used in the explanation. The predicate airport-terminal plays a role; the predicate computer-terminal doesn X  X . why isn X  X  it more widely adopted? 1. Parsers were not accurate enough to produce good logical forms from 2. Algorithms for abduction were too inefficient. 3. There was a lack of an adequate knowledge base.
 are now highly accurate statistical parsers, and for several of these (e.g., Boxer; Bos 2008) a component for translating into a flat logical form has been implemented. tion as a problem in integer linear programming, building on earlier work by Charniak and Santos (Santos 1996). Our experience with this is that when we switched from a naive backchaining implementation to the ILP implementation, we got a speed-up of two orders of magnitude.
 natural language processing applications have had mixed success at best. But Schubert X  X  efforts (2002) to build a knowledge base by analyzing language use looks very promis-ing. Some applications have attempted to use OpenMind. WordNet hierarchies are used very widely and Harabagiu and Moldovan (2002) developed XWN, a conversion of 790
Logical Form:
Knowledge Base: airplane ( x )  X  plane ( x ) airport ( z )  X  airplane ( x )  X  airport-terminal ( y ) answering. FrameNet has been converted into logical axioms by Ovchinnikova et al. (2013), and she and her colleagues have shown that an abduction engine using a knowledge base derived from these sources is competitive with the best of the statistical systems in textual entailment and semantic role labeling.
 scribed in the next section. 4. Knowledge
We understand discourse so well because we know so much. Thus, one of the central problems in the study of language is how we use our knowledge of language and the world to interpret discourse. This breaks into two subproblems: 1. How do we encode the common-sense knowledge required for 2. How do we use this knowledge in the processing of discourse? thought the second of these is a solved problem. The answer is abduction. He agreed with me. We both agreed that the first problem was now the most important focus of research. But he said that he despaired of encoding that knowledge manually, and that X  X  reasons. I think the kind of knowledge that we want at the very core of a knowledge base for NLP can only be done manually by thoughtful people and cannot be done by any automatic methods currently imaginable. And I think there is systematicity that will make the task more tractable than we might believe at the outset.
 (16) The scores on the test ranged from 38 to 96.
 From the definition we would want to be able to answer the questions Did someone get a 96 on the test? Yes.
 Did someone get a 54 on the test? Maybe.
 Did someone get a 25 on the test? No.

In addition, we want to capture generalizations and we don X  X  want to multiply word senses needlessly. So we would like to have the same definition work for the sentences (17) The timber wolf ranges from northern Mexico to southern Alaska. (18) His behavior ranges from sullen to downright hostile. (19) The hepatitis cases range from moderate to severe.

We don X  X  want a  X  X pecies X  sense of range , and a  X  X ehavioral X  sense and an  X  X pidemio-logical X  sense.
 (20) (  X  x , y , z ) range ( x , y , z )  X  bottom is y and whose top is z , such that some member u 1 of x is at z , and every member u of x is at some point v in s subsequently.) ically rules of this complexity and at the same time rules of this level of abstractness.
I X  X  sure we X  X l be able to discover automatically facts such as  X  X ne has to be married before getting divorced, X  and  X  X ouses normally have thermostats. X  But facts like the definition of  X  X ange X  require human brains.
 the papers that struck a chord the most were the Generative Semanticists, like Jeffrey
Gruber, George Lakoff, Haj Ross, James McCawley, and others. They were analyzing the verb kill into cause to become not alive and the verb move ,asin x moves y from z to w ,into x causes a change from y being at z to y being at w . They also speculated on the abstract nature of the at relation as a source for many of the frozen spatial metaphors that pervade language.
 they were doing in tree transformations what they should have been doing in logic, 792 a mistake being repeated today by those working on so-called  X  X atural language infer-ence. X  Second, they lacked a notion of defeasibility, so that when they found examples of when X killed Y and Y didn X  X  end up not alive, they thought their theory was refuted.
 returned to it in the mid-1980s when Bill Croft, Doug Edwards, Ken Laws, and I worked on building up a knowledge base. Our goal, which we almost achieved, was to be able to prove as a theorem that wear on a component of an artifact can cause the artifact to fail, because wear is a loss of material and this causes a change of shape, and shape in artifacts is normally functional. At the time we were working on U.S. Navy texts dealing with worn-out air compressors.
 funding for, because its payoff in comparison to building special-purpose applications general knowledge in the next logical domain to attempt. For example, I was able to work with people like George Ferguson, Pat Hayes, and Drew McDermott on devel-oping the so-called  X  X WL-Time, X  a comprehensive ontology of time (Hobbs and Pan 2004), for DARPA X  X  DAML program on the Semantic Web, and ARDA X  X  AQUAINT program on question-answering provided the resources for my work with Feng Pan and Rutu Mulkar-Mehta on vague durations of events. Ram Nevatia X  X  ARDA-sponsored
MOVER project provided the opportunity to develop an ontology of event structure called VERL (Video Event Representation Language, Alexandre et al. 2005), and this led to work with Chris Welty, Mike Gruninger, and people at Cycorp on the ARDA-sponsored IKRIS project for developing an interlingua among several event and process ontologies. DARPA X  X  Machine Reading Program supported my student Rutu Mulkar-
Mehta X  X  work on granular or  X  X ow-to X  causality (Mulkar-Mehta, Hobbs, and Hovy 2011) and Niloofar Montazeri X  X  work defining or characterizing several hundred com-mon event-related words (Montazeri and Hobbs 2011). My work with Andrew Gordon on encoding common-sense psychology (Gordon and Hobbs 2004) has been funded by various agencies over the years, most recently by ONR. But some of the research has been  X  X tealth X  research X  X ork you don X  X  tell anyone about until it X  X  finished for fear your boss will find out and make you work on other stuff. My papers on causality and modality (Hobbs 2005) and on scales and half orders of magnitude (Hobbs 2000) were like this.
 2008). It is not enough to decompose  X  X ove X  into  X  X ause -change -at. X  It is not good enough to simply stipulate these as primitives. We need to explicate these concepts in core theories, a theory of causality, a theory of change of state, and a theory of composite entities and the figure X  X round relation. Lexical decompositions have to be anchored in such theories so we can not only decompose meanings but also be able to reason with the decomposed meanings.
 morphemes of the language. We have the underlying core theories. And we have axioms defining or characterizing the former in terms of the latter. Thus, in the  X  X ange X  example, range is the predicate corresponding to the morpheme. There is a core theory of scales rule that links the lexical predicate with the core theory.
 words for the textual entailment task. thing made of other things. This is intended to cover physical objects like a telephone, mixed objects like a book, abstract objects like a theory, and events like a concert. It is characterized by a set of components, a set of properties of the components, a set of relations among its components (the structure), and relations between the entity as a whole and its environment (including its function). The predicate at relates an external entity, the figure, to a component in a composite entity, the ground. Different figures and different grounds give us different meanings for at . (21) Spatial location: Pat is at the back of the store. (22) Location on a scale: Nuance closed at 58. (23) Membership in an organization: Pat is now at Google. (24) Location in a text: The table is at the end of the article. (25) Time of an event: At that moment, Pat stood up. (26) Event at event: Let X  X  discuss that at lunch. (27) At a predication: She was at ease in his company.

When at is specialized in this way, we tap into a whole vocabulary for talking about the domain, including concepts like move and range .
 e . Its principal properties are that e 1 and e 2 should have an entity in common X  X  change of state is a change of state of something . States e 1 and e an intermediate state. The predicate change is defeasibly transitive; in fact, backchaining on the transitivity axiom is one way to refine the granularity on processes. concept  X  X ause. X  A causal complex includes all the states and events that have to happen or hold in order for the effect to occur. We say that flipping a switch causes the light to go on. But many other conditions must be in the causal complex X  X he light bulb can X  X  on. The two key properties of a causal complex are that when everything in the causal complex happens or holds, so will the effect, and that everything that is in the causal complex is relevant in a sense that can be made precise.  X  X ausal complex X  is a rigorous or monotonic notion, but its utility in everyday life is limited because we almost never can specify everything in it. causal complex a particular eventuality that in a sense is the  X  X ctive X  part of the causal contexts, is the action that causes the light to come on. Causes are the focus of plan-ning, prediction, explanation, and interpreting discourse, but not diagnosis, because in diagnosis, something that normally happens or holds, doesn X  X .
 e happen means x does not cause e not to happen. (28) let ( x , e 1 )  X  not ( e 8 )  X  cause ( e 8 , x , e 9
In its most abstract sense go just means a change of state, as in Sometimes I go crazy . (29) go ( e 1 , y , e 2 , e 3 )  X  change ( e 1 , e 2 , e 3 794 constraints cause e 5 not to happen. (30) free ( e 3 , y , c , e 5 )  X  not ( e 3 , e 2 )  X  cause ( e
Something x holds y at z means that x causes y not to change from being at z . (31) hold ( e 2 , x , y , z )
Something x releases y from z means that there is a change from x  X  X  holding y at z . (32) release ( e 1 , x , y , z )  X  change ( e 1 , e 2 , e hypothesis follows from it or not. For example, from the text A Filippino hostage in Iraq was released we would like to be able conclude the hypothesis The captors let the hostage go free . Figure 3 illustrates the proof of this entailment relation, using the five axioms nothing causes it to not exist, and if there is a change from a state, that state no longer holds (Montazeri and Hobbs 2011).
 been doing with Andrew Gordon on axiomatizing common-sense psychology, or how we think we think. We have developed approaches to memory, belief, and mutual belief, envisioning causal chains in explanation and prediction, perception and control of the body, and goals and plans. I will focus on goals.
 have goals, we develop plans to achieve these goals, we execute the plans, we monitor the execution, and if things go wrong, we modify our plans and execute the new plans. e and believes e 1 causes e 2 , then defeasibly that will cause the agent to have e subgoal. The second property is a similar rule for enablement. These are the planning axioms; they generate hierarchical plans.
 distinguish three levels of helping. At the lowest level, inadvertant helping, you help someone when you do an action that is in a causal complex for one of their goals. In this sense, John McCain helped Barack Obama get elected by picking Sarah Palin as his running mate. A second level, intentional helping, is like the first with the addition that the helper performs the action in the service of the helpee X  X  goal. For example, if I take away a drunk friend X  X  car keys, I help him survive, a goal of his, but not driving is no part of his plan to survive. The third level, collaborative helping, happens when the helper and helpee engage in a shared plan together, as in helping someone carry a sofa. accomplished, and having it be a goal cause one to accomplish one of its subgoals. To succeed at doing E is to try to do E, and to have that trying cause E to be accomplished. To fail to do E is to try to do E and have E not happen.
 about the structure and function of the artifact. The theory of goals and planning gives achieve its functionality, a goal. For example, the function of a coffee cup is to move coffee. We achieve this by breaking it into two subgoals: having a cup contain the coffee and moving the cup. We achieve moving the cup by attaching a handle to the cup and moving the handle. Artifacts are plans made concrete (or in the case of a coffee cup, ceramic).
 much of it involves goals. In general, we can characterize emotions in terms of what causes them and what they cause. Emotions, like cognition more generally, mediate between perception and action. Thus, for a particular emotion, we specify an abstract emotions.
 sometimes merely when we anticipate that). That must mean that one X  X  beliefs in the relevant area are working, especially one X  X  beliefs about what causes what. So one effect of happiness is a higher level of activity X  X e plan to do more because our planning process is in good working order. A second effect of happiness is that we are not very open to a change of beliefs. If our beliefs are working, why should we change them? in cause and effects. Fear, anger, and disgust can be seen as various responses to a threat, given the properties of the threat, where a threat is something that will cause one X  X  goals to be defeated.
 automatically analyze texts in the manner I have described? When I wrote my technical report analyzing one paragraph of Newsweek in 1976, I thought the answer was that the goal was ten years away. When we began to implement a system based on weighted consistently optimistic. I think a concerted effort along these lines would yield some measure of success in about ten years. 796 References
