 1990s. Performance testing is needed by people including Grid system designers, de-velopers of Grid middleware, application developers and system users. Comparing to system performance is different because of its characteristics, for instance, heteroge-neity, dynamics and the special way for sharing resources. Accordingly, performance testing methods and tools especial for Grid system are necessary. 
The greatest motivation of our work is to provide a performance testing tool for Grid middleware developers, firstly for CROWN [1] middleware developers, to validate function and performance of their designing and implementing. To gain meaningful results, the Grid system, including physical resources, system architecture and workload, used in test should be close to the target system. Because testing we discuss and imple-ment here is real testing which is executed on real circumstance, physical resources and system architecture are exactly the same as the target system. Thus, what we are focusing on in our system is workload used in testing. 
Workload impacts Grid system performance greatly. Usually, there are three kinds of workloads can be used in testing to assess the performance of Grid systems: real Grid workload, synthetic Grid workload and benchmark workload. Synthetic workload, which is basing on workload model derived from real workload traces, is regarded to be a more appropriate candidate in our performance testing system. 
In this paper, we present CROWNBench, a system for Grid performance testing CROWNBench allows testers to customize their workload model, including applica-tion and job submission rules, basing on workload statistical model extracted from real workload trace. In this way, Grid middlew are developers can acquire comprehensive performance data of Grid system by running various kinds of workload. What is more, testing procedure can be controlled and replayed easily. Even though workload in CROWNBench system could be customized, synthetic workload should still be basing on workload model and its contained elements and parameters. So real workload traces of some certain Grid systems should be well workload. Workload models of kinds of computer systems have been well studied and researched [2] [3]. Even Grid system worklo ad model has been started to research on some Grid system testbeds to evaluate Gird system performance [4] [5]. Base general rules in these models are extracted. And for testing performance capacity of Grid sys-tem, amount and distribution of workload should also be tunable to simulate real workload in different time and on different Grid testbeds. 
GRASP [6] is a project for testing performance and validating dependency of Grid system by using probes, some low level benchmarks; NGB[7] is developed for Grid from NPB. It contains a suite of benchmark the structure of which is described by data flow graph; the benchmarks of GridBench [8] cover multiple levels benchmarks, in-cluding Micro X  X enchmark and Micro X  X ernel. GRENCHMARK [9] is a Grid per-formance testing framework with synthetic workload. But only four settled applications are supported to synthesize workload, and customizable workload is not supported. 
Comparing to projects discussed above, CROWNBench provides tools to analyze real workload trace and maintain testing en vironment, which are both not covered in these projects. Analyzing real workload trace could find out approximate workload statistical model, and testers could use it to customize the synthetic workload close to real workload and acquire meaningful result of Grid performance testing by running it. Maintaining an isolated testing environment for a Grid performance testing could eliminate the influence among different performance testing and minimize the influ-ence which is exerted on working Grid by performance testing. 3.1 Grid Performance When discussing performance of distributed systems, we are usually focusing on the quality and quantity of resources they providing. Grid is regarded as a virtual computer computer constituted by each layers of Grid system while it is running. 
Compared to traditional distributed system, performance of physical resources is not the only content of Grid system performance. Grid middleware performance is a very important part in the whole Grid system, esp ecially when we consider requirements of CROWNBench. Throughout development, the middleware must be developed itera-tively and incrementally. On each milestone, performance of middleware should be tested on real circumstance. After that, results of performance metrics, which defined before, are required to validate the design and implement of Grid middleware. 3.2 Grid Performance Testing System Usually, there are three ways of testing Grid performance, model analysis, emulating and real testing respectively. Comparing to real testing, former two have difficulties in emulating dynamic behaviors of the Grid system. Thus, we choose the way, imple-menting Grid performance testing on real Grid environment to gain performance results in CROWNBench. 
In order to fulfill our system goal, helping middleware developers to evaluate and compare performance of different middleware versions, Grid system performance testing should be implemented on same physical resources and Grid environment could be acquired to compare the performance of different middleware version, to testify whether the new one is better than the old ones. 
As a Grid performance testing system, CROWNBench faces challenges in three aspects, and now our contributions also locate in them: generating meaningful and comprehensive workload for Grid performance testing. They will be described in detail in 5.4. Generally, Grid workload includes all jobs submitted to Grid within a period of time. A essarily representative of workloads on other times and conditions. Thus, Gird work-load model should be derived from traces of history workloads, and then be studied as the basis of workload synthesizing. 
When we study Grid workload model from a higher view, there are two aspects Grid workload and customize synthetic workload. Secondly, job running time is also an aspect in Grid workload model as well as in traditional parallel and distributed system turnaround time. So it will not be settled down before testing. 5.1 System Architecture CROWNBench is a system for Grid performance testing with customizable synthetic workload, which has been used on Grid deployed with CROWN Grid middleware. Figure 1 shows its architecture. 
CROWNBench is built upon physical resources and Grid middleware. It is isolated from certain Grid middleware instance by Grid middleware agent factory . Layers in CROWNBench above isolating layer do not know which type of Grid middleware was deployed above physical resource layer in Grid environment. In running layer, there are synthetic workload automatically. Data transferrer takes the responsibility to transfer related data, including testing running files and results data, among nodes in Grid en-geneous Grid environment. In tools laye r, trace analyzer, workload composer and model of real workload by analyzing workload trace. Workload composer synthesizes workload for testing through definition of testers. E nvironment manager maintains a before testing starts and clearing it after testing finishes. Testers could use CROWN-Bench through system portal , including edit testing environment, view workload analyzing results and customize synthetic workload. 5.2 Grid Performance Metrics W stands for workload during a period of time. The number of applications included in workload is W . Job i J includes i J tasks, to ) 1 (  X  i J pre-tasks. It can run when these tasks finished. Similarly, i T could have 0 to ) 1 (  X  i J post tasks, which could run simultaneously only when i T finished. 
Because of characters of Grid system, a ratio of successful jobs to whole jobs can be looked as a metric of Grid system performan ce, especially in Grid system where testers look job correctness more important than processing speed. 
JS is the successful job rate for workload W . For an included job J , TS is the suc-cessful tasks rate: Average successful task rates, shown in (3), can be computed to evaluate whole Grid system performance: In Grid system which pays attention to quality of service, turn-around time TT of a job (time from submission to finishing) and each component processing time could be chosen as metrics. In a special scenario, workload contains only one kind of applica-tion. (4) is a metric of high level Grid system performance: Processing time of a certain component can be looked as a metric of that component X  X  performance. For example, (5) can be used for evaluating performance of scheduling component in Grid middleware. 5.3 CROWNBench Testing Process A typical testing process with CROWNBench is described below as Figure 2. 
As shown above, workload statistical model could be extracted after analyzing workload trace  X  ; Testers could define testing environment structure in topology description file  X  , and then testing environment will be built by environment man-ager; Basing on workload trace analyzing re sult, testers could edit Application De-scription File, which contains detailed tasks and working flow and dependency among them; The other file testers should edit is Submission Rule Description File, in which submission statistic rule is set  X  ; Workload Composer composes workload into Workload Description File for testing from Application Description File and Submission Rule Description File  X  ; Jobs Submitter parses workload and submits each job from appointed node and in appointed time  X  ; Testing Controller and script which describe the workflow of an application will control the running order of tasks  X  ; When testing is finished, output file was generated  X  ; After Data Transferrer transfers result files to the testing control node, result will be extracted and saved into database  X  ; After computing test results of metrics  X  ; Final results will be fed back to testers  X  . 5.4 Key Techniques of CROWNBench Analyzing Workload Trace. If testers could acquire some knowledge about statistical synthetic workload which will be more close to real workload in the Grid. In order to extract statistical model of real workload, CROWNBench analyzes data in logs. We assume Grid workload to be analyzed is pe riodical, stable and self similar [11] as the precondition of our analyzing, which prove our analyzing to be meaningful and feasible. Data in logs are processed in below steps. 2. Data filtering. Abnormal records should be filtered to improve the accuracy of Composing Workload. Testers define submission statistical rules for each job Submission Rule Description File (Figure 3), Workload Composer generate synthetic 4) in time order. This file will be parsed by Jobs Submitter. Maintaining Testing Environment. To eliminate the influence among different performance testing and minimize the influence which is exerted on Grid environment by performance testing, CROWNBench maintains a testing environment for a according to Testing Environment Topology File (Figure 5), and middleware and related files and database will be deleted after performance testing. 
Generally, these tasks are finished manually. In CROWNBench, we developed a tool, WSAnt, to implement automatically. Building and clearing of testing environment process are implemented in ANT script, and WSAnt controls its running in remote node by using web service technology. In this way, the whole process is implemented in a more automatic, accelerated and simple manner. Application X  X  Workflow Control. To control application workflow, we chose a flexible script tool, Ant [13], to avoid a large mount of developing works. Ant is a free open source tool of Apache Software Foundation. CROWNBench uses it to control workflow in applications for following reasons: at first, it is developed by JAVA, so it Grid services frame easily; Ant and its additional project Ant-contrib [14] provide tasks dependencies and workflow can easily mapping to relations between tasks of Ant. We can extend any Grid task to Ant task, and construct them to Grid application. Parameters of those tasks can be defined to change the amount of workload. In order to support complex workflow of Grid application, CROWNBench extended workflow control dictionary of CROWNBench. Figure 6 is a fraction of a sample workflow control script. 6.1 Experiment Environment Two suites performance testing experiments with CROWNBench are implemented on CROWN testbed, which is deployed on 30 tree-structured nodes in our research insti-tute. The first experiment is done to study the different influence on Grid system per-formance caused by different Grid workload model instances. We did the second one to find whether our system could gain meaningful performance results with synthetic workload comparing to real workload. 6.2 Experiment I In experiment we compose three applications (  X  X  X  X  ) with different structure by using three basic tasks, which have been provided by CROWNBench, including file transferring, float computing and I/O operation.  X  X  X  all contain sequential and parallel workflow,  X  is pure chain of tasks. In this term, CROWNBench is used to study in-fluence on Grid system performance given by job submission frequency. Three kind of workload described by matrix and testing results showed below. 
In daytime, besides testing workload, normal running workloads on Grid system are heavy. This situation will be eased during nighttime. Correspondingly, performance of experiments. We can find in both of two experiments that long lasting and continuous workload will give Grid system a more great impact. 
In this experiment, workload are same as ones used in first experiment, but they all contain only one application (  X  ) for simplifying testing workload model and com-puting metrics of Grid system Qos. The testing results are showed as Figure 10. 
From testing results listed above, we can make some probable conclusions. Jobs submission frequency and lasting time exert influence on both running correctness and Qos of Grid system. Those workloads with higher jobs submission frequency and longer lasting time will spend longer average turnaround time and scheduling time, and also generate greater performance jitter (variance). 6.3 Experiment II We had learned from jobs observing component in CROWN Grid middleware that the future, appears to be Poison distribution in a long term, and parameters of distribution are different among periods in a day. Such observing and analyzing processes are finished by component out of CROWNBench system, so it is beyond discussion of this processed. We used CROWNBench to compose synthetic workload using basic tasks of float computing and I/O operation and tuning parameters of Poison to emulate real AREM workload in each two hours period in a day. Then we chose a day to do per-formance testing every 2 hours. And we compared the average jobs rate of synthetic workload to real worklo ad to evaluate the effectiveness of our system. 
The Figure 11 show successful jobs rate of real workload and synthetic workload respectively. Its effect could be showed by results of this experiment. CROWNBench is an automatic Grid system performance testing system using cus-tomizable synthetic workload. It allows testers to define testing workload by con-structing workload and application with tasks which can be defined by them. Main contributions of CROWNBench are concluded as: 1. Testers can customize their Grid appli cations running for Grid performance 2. Testers can define jobs submission statistical laws to compose testing workload, 3. Running testing workload can be contro lled and replayed. Because user can de-4. The whole procedure of testing is automatic, because testers only need to take part 
