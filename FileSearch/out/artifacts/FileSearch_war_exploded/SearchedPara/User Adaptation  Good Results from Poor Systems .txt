 Several recent studies have found only a weak relationship between the performance of a retrieval system and the  X  X uccess X  achievable by human searchers. We hypothesize that searchers are successful precisely because they alter their behavior. To explore the possible causal relation between system performance and search behavior, we control system performance, hoping to elicit adaptive search behaviors. 36 subjects each completed 12 searches using either a standard system or one of two degraded systems. Using a general linear model, we isolate the main effect of system performance, by measuring and removing main effect s due to searcher variation, We find that searchers using our degraded systems are as successful as those using the standard syst em, but that, in achieving this success, they alter their behavior in ways that could be measured, in real time, by a suitably instrume nted system. Our findings suggest, quite generally, that some aspect s of behavioral dynamics may provide unobtrusive indicators of system performance. H.3.3 [ Information Search and Retrieval ]: Search process General Terms : Experimentation, Human Factors. Keywords : Experiment design, Anal ysis techniques, User modeling, Adaptive IR Systems Search systems  X  X earn about their us ers X  from direct inputs such as query terms and possibly from c ontextualizing or personalizing information. Models underlyi ng search systems use these parameters to seek a maximally re levant set of  X  X eturns X , however, the set returned is generally not ideal. Searchers have learned to overcome this difficulty, at least to the extent necessary to make systems useful. We ask what people are doing to maximize the performance of search systems. If we can learn what people do to overcome system failure, perhaps we can build systems that monitor user behavior for indicators of failure, so that adaptation can become a two-way street. We report a factorial experiment in which we intentionally manipulated a standard search system to produce degraded results, and studied how people solve th e problem of search failure. 36 subjects were recruited on the campus of a large mid-Atlantic university. Each was paid $15 to search on the same set of 12 topics, and to motivate search effort, were told that an additional $40 would be paid to the subject who  X  X ound the most good information sources, and the fewest bad s ources. X  Subjects were randomly assigned to experience one of three conditions during the middle 4 searches of their runs: a control condition or one of two experimental conditions. Subjects were told that they would search using the Google X  system, which did underlie the experimental system. Subjects provided de mographic information and information about prior search experience and attitudes in a pre-experiment questionnaire. As their experimental task, subject s were asked to find and identify  X  X ood information sources X  for an unspecified  X  X oss. X   X  X ood information source X  was defined as one  X  you could and would use to get information about the topi c. X  There was no time limit on searching. Subjects observed a dem onstration of the experimental system and completed a practice topic before beginning the experiment. Each topic search proceeded as follows: (1) the topic statement was displayed, (2) the subject completed a pre-search questionnaire, (3) the subject sear ched, and when done searching, (4) the subject completed a post-search questionnaire. Subjects searched as they normally would w ith the Google X  search interface, except that subjects used a checkbox to  X  X ag X  each  X  X ood Information Source X  ( GIS ) found for the topic. If a tagged source was displayed again during the topic search, the box was displayed as checked; subjects could uncheck the box. We scored the subject X  X  given. When done searching on a topic the subject could not return to the topic. Subjects were debriefed after completing the experiment. A 3x3 diagram-balanced factorial design was used. Topic order was controlled, with each subject assigned to one of 12 search orders, which balanced topic frequency across the three blocks (with the exception of two topics 1 ). One subject in each group searched in each of the 12 order assignments, for a total of 432 searches. Searches were conducted in three blocks of four topics each (see Figure 2). Block 1 was a pre-treatment control block, in which all three groups searched in the same standard condition. During Block 2, the treatment block , each group searched in a different condition. In Block 3 all subjects were retu rned to the standard condition. Subjects were not informed of th e blocking, and no break was given between blocks. Subjects in the control group searched using the standard system in all three bloc ks. Subjects in the experimental groups searched using the standard system in the first block, and then in their assigned experiment al condition during the treatment block. One experimental treatment, consistently -low-rankings (CLR), displayed results from positions 300-320 of lists retrieved by Google X . The other experimental treatment, inconsistently-low-ranking (ILR), displayed documents Due to an error, topic 8 was searched one extra time in the treatment block and one less time in the post-treatment block, and topic 6 was searched one less time in the treatment block and one more time in the post-treatment block. The design was intended to mimic a maladaptive mechanism, such as an automatic query expansion process that fails to converge correctly on a search topic. One monitor displayed the experi mental system. Web pages and documents opened by subjects were displayed in a second monitor. All subjects used the Firefox X  browser. The experimental system had two interfaces: (1) controlled the controlled searching with the topi c statement displayed in an upper frame, and a modified Google X  interface in a larger lower frame (see Figure 3). After the first search was comple ted, the upper frame displayed a  X  X eminder X  box, reporting total elapsed time since the start of the first search, the number of topics completed, and the number not yet finished. The box was updated at the start of each topic. Standard navigational links on the Google X  search interface were displayed but disabled. Every item in the results list was left-aligned and displayed using the text and fo rmatting obtained from Google X . A single checkbox to the left of each item enabled the searcher to tag as GIS each good source found.  X  X ext page X  links were visible but disabled, so that subjects could not view results on subsequent pages. When a list with fewer than twenty items was returned by Google X , only the returned items were displayed. For the two degraded systems, the standard Google X  results counts and timing text (e.g.  X  X esults 1 -20 of about xxx for xxxxx [query terms]. (0. xxx seconds) X ) were altered to indicate th at the list started at rank 1. Any  X  X id you mean... X  and  X  X int X  messa ges returned by Google X  were displayed. The  X  X id you mean... X  query suggestion link was  X  X ive X ; information sources. Measurem X  X t / description total item number of item displays during the search; includes repeated displays of the same item number of unique items displayed during the search; excludes repeated displays of the same item tagged item number of item displays for items tagged as GIS by the searcher; includes repeated displays of the same tagged item tagged unique items BTUI MTUI GTUI ATUI number of items tagged as GIS by the searcher; items are counted as tagged only once during the search; this count excludes repeated displays of tagged items total good items in the  X  X ollection X  for the topic total number of items tagged as GIS for the topic by any searcher during the experiment and judged to be a good source by the researcher Subject characteristics. Pre-experiment measures revealed no significant differences among the th ree subject groups with regard to prior experience with, and attitudes about, web searching and Google X . No significant differences were found in the demographic characteristics of each group (for all measures, ANOVA F (2,33) &lt; 1.284, p&gt;0.289). Subject attrition. Six subjects (3 control, 2 CLR, 1 ILR) quit the experiment before completing the third block, but after completing all the searches in the fi rst two blocks. Data from their completed searches was retained and data from incomplete searches excluded. 416 topic searches are used for analysis. The general characteristics of the 416 completed topic searches are presented in Table 4. ETTime 6.53 .158 1.53 22.73 Queries Entered 5.52 .218 1 30 ATUI 3.32 .127 0 13 GTUI 1.72 .103 0 12 MTUI .64 .046 0 6 BTUI .81 .063 0 11 We know from prior research [2] that three large effects are likely searchers have different search skills, styles, cognitive abilities and domain knowledge, and (3) searchers conducting a series of twelve searches in an experimental setting are likely to experience learning effects . Searchers are affected by the researcher X  X  demand that they search without a br eak until the task is complete. Searchers grow tired or bored with the task and early searches may be performed differently than later searches. The factorial design of our experiment enables us to isolate the effect of the system from these three confounding effects, using a general linear model. The model used was developed to evaluate collaborative searching systems [9] and has also been used in evaluation of Question Answering Systems [3, 6]. For our study, this model relates a user, u , using a system treatment s while engaged in searching on a topic t , at position p in the search order. The equation is: where the main effects are represented by the  X  parameters, and the term  X  represents random error. This model allows us to estimate the size of each confounding effect (User: U , Topic: T , and Position, P ) from the ensemble of measurements, for each search, and to subtract these effects. The resulting measure comprises the effect of the system plus random error. For example, the calculation for elapsed topic search time (ETTime) for user 2 (subject 2), working on topic 2, in position 1 (the first search in the experiment) is: ETTime u2,t2,p1 = 9.283 parameters are U model estimates 559 . 8 the difference between the measured value and the estimated value (9.283 -8.559 = 0.721). This value is the effect of the system used during the search, plus random e rror. In cases where the model  X  X vershoots X  the measured value, the saved value will be negative. The values are visualized in Figure 4, below. Data from all 416 completed searches were used in the computation of  X  + we discuss measurements from which topic, subject and position effects have been subtracted and report on the 288 searches completed during the first two bl ocks (36 subjects x 8 searches each). 0.0 0.0 Figure 4. Calculation of saved value for ETTime u2,t2,p2 Table 5. Contrasts: system performance. NOTE: Topic, subject and position effects have been removed from this data n=48 _ .. . dvSEM  X  _ .. . vSEM  X  X  Control 0.029 0.018  X  CLR 0.015 0.011  X  X  0.044 0.022 *  X  X 
ILR 0.016 0.012  X  X  0.045 0.016 *  X  X  Control 0.103 0.021  X  CLR 0.035 0.028  X  X  0.138 0.035 * *  X  X  ILR 0.068 0.033  X  X  0.171 0.043 * * *  X  X  Significant  X  _ v noted in last column: *  X  =.05, **  X  =.01, ***  X  =.001 ILR system produced lower GPrec in the treatment block and the decline is significantly different from the corresponding change p&lt;.05). Results are similar for GRec, with lower GRec in the treatment block for both the CLR group ( v = GRec ,  X  _ v CLR =-0.138, f=11.514, df 1, p=.001), and the ILR group ( v = GRec ,  X  _ v ILR =-0.171, f=17.630, df 1, p&lt;.001). We examined four basic measures of searcher performance: the number of good sources found (GTUI) during a topic search, the number of bad source found (BTUI), the number of marginal sources found (MTUI) and the tim e spent searching (ETTime) . For both treatment groups, relative to the control group, the block-to-block changes are not significantly different for any of these measures. We are interested how accurately our subjects identified good information sources. Good item ratio is the fraction of the items tagged GIS that were subsequen tly judged to be good sources. For both the CLR and ILR groups, relative to the control group, the block-to-block change in this ratio is not significantly different. We also examined the characteristics of the tagged items that were not good, using the marginal item ratio (the fraction of tagged items that were subsequently judge d to be  X  X arginal X ) and the bad item ratio (the fraction of tagged items that were judged  X  X ad X ). For the ILR group, the block-to-block changes for both ratios are subjects in the CLR group, a block-to-block decline in the marginal item ratio is not significantly different relative to the corresponding change for the control group, but the change in the bad item ratio is. Relative to the pre-treatment block, this ratio was higher in the treatment block, a significantly different change f=11.984, df 1, p=.001). We also considered two other measures of searcher performance: selectivity and sensitivity . Searcher selectivity is the fraction of the displayed items not tagged GIS by the searcher. For both treatment groups, relative to control, the block-to-block changes in selectivity are not significantly different. Searcher sensitivity is the fraction of the good source displays that the searcher tagged. For subjects in the CLR group, relative to the pre-treatment block, searcher sensitivity increased in the treatment block. The increase is significantly different Table 7. Contrasts: searcher behavior and system response. NOTE: Topic, subject and position effects have been removed from this data n=48 _ .. . dvSEM  X  _ .. . vSEM  X  X  control 0.112 0.050  X  X  CLR 0.156 0.082  X  0.268 0.096 * *  X 
ILR 0.044 0.058  X  X  0.068 0.101  X  control 0.527 0.279  X  CLR 0.644 0.430  X  X  1.171 0.513 *  X  X 
ILR 0.116 0.289  X  0.411 0.518  X  X  control 0.775 0.461  X  X  CLR 0.155 0.494  X  X  0.620 0.676  X 
ILR 0.930 0.396  X  1.705 0.634 *  X  control 0.063 0.021  X  CLR 0.021 0.018  X  X  0.084 0.027 * *  X  X  ILR 0.041 0.017  X  X  0.104 0.024 * * *  X  X  Significant  X  _ v noted in last column: *  X  =.05, **  X  =.01, ***  X  =.001 average list length, unique items displayed per query, and item display repetitions. Query rate is the average number of queries entered per minute. For the CLR group, query rate increased in the treatment block, a change that is significantly different from the block-to-block change for the control group (for which there was no statistically is not significantly different from the corresponding change for the control group. Average list length is the average number of items displayed in each list returned, including repeated displays of items. For the CLR group, the average list was shorter in the treatment block than in the pre-treatment block, a change that is significantly different from the corresponding change for the control group the ILR group, the change in average list length is not significantly different from the change for control. Unique items per query is the average number of unique items displayed for every query entered during a search. For the ILR group, unique items per query increased in the treatment block, a change that is significantly different from the corresponding change for the control group ( v = unique items per query ,  X  _ v ILR =1.705, f=5.957, df 1, p&lt;.05). For the CLR group, the block-to-block change in unique items per query is not significantly the ILR system shifted the starting rank of some lists (see Table 1, above). 85% of searches experienced at least one shift, while only 8% experienced the maximum 4 shif ts. Lists that were not shifted always started at the 300 th ranked item, the same rank used for all CLR lists. While unique items per query was highest for the ILR group, the block-to-block change is not significantly different Conclusions. We find that searchers using either of the degraded systems were as successful as those using a standard system. In achieving this success, they modified their behavior in ways that depend on the characteristics of the failure. Two adaptations appear, on average, to be indicative of the degraded performance: (1) an increase in the rate of query entry (a user behavior) and (2) a decrease in the occurrence of repeated item displays (a system response characteristic). Both of these can be observed, in real time, by a suitably instrumented system, at the server side. Limitations and future work. These results are encouraging, but further work is necessary to elim inate other possible effects. For example, the lower detection rate shown by users of the standard system may be due, at least in part , to other factors. It may reflect a kind of satisficing behavior on the part of subjects in the control group. Subjects facing a relative abundance of good sources may simply select a few that seem best among them, without trying to optimize by seeking and selecting all the available good sources. A similar effect, termed saturation , has been reported elsewhere [7]. It is also possible that our pre-and post-search instruments (not shown here for reasons of space) which asked subjects to predict the number of sources they would find, cause subjects to  X  X nchor X  on the expected number of sources.  X  X nchored X  subjects may stop searching when their expectations are met, even though additional sources are availabl e. Finally, subjects using the standard system, presented with relatively more  X  X ood X  results, may have been able to attend less to searching and more to differentiating the goodness of sources. This phenomenon would probably lower the rate of agreement in tagging, which can be explored by examining within-gr oup agreement levels in future analysis of the data we have collected. On the other hand, the design of the CLR system may have produced an exaggerated effect, as each list it returned started at the 300 th ranked item, and CLR users were more likely to receive empty or shorter lists (the reas ons for this are currently being investigated). These characteristics of the lists may have alerted subjects to the degraded performance, increasing their attentiveness. As reported, when faced with the CLR degraded system, searchers increased their pace of query entry. The increased pace may have caused more misspellings in query terms, exacerbating a cycle of shorter lists and even more rushed behavior, causing further misspellings. Since we recorded the frequency of Google X  X  spelling sugge stion messages, we are able to use them as a rough measure of misspellings (a spelling suggestion is unlikely for gross misspellings and typos). We examined misspelling-messages-pe r-query-entered (MMQE) and misspelling-messages-per-short-lis t (MMSL). For both measures we found no significant differences between CLR and the control group during the treatment bl ock. (ANOVA: MMQE, f=2.219, df 3, p=.087; MMSL, f=.492, df 3, p=.689). The possible effect of the length of returned lists will be examined elsewhere. Taking into account these limitati ons, however, this study finds significant and observable differences in averaged user behavior when the system is not serving them well. We note, finally, that we observe these differences in the mean, where accuracy is improved by combing results fo r several persons, and several searches. For these differences to be useful in system design, we must find that differences in user behavior (amplified here, by our experimental design) are large enough that a system can estimate 
