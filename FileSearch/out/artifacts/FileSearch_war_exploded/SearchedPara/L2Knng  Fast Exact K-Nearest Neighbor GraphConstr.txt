 The k -nearest neighbor graph is often used as a building block in information retrieval, clustering, online advertising, and recom-mender systems algorithms. The complexity of constructing the ex-act k -nearest neighbor graph is quadratic on the number of objects that are compared, and most existing methods solve the problem approximately. We present L2Knng , an efficient algorithm that finds the exact cosine similarity k -nearest neighbor graph for a set of sparse high-dimensional objects. Our algorithm quickly builds an approximate solution to the problem, identifying many of the most similar neighbors, and then uses theoretic bounds on the similar-ity of two vectors, based on the ` 2 -norm of part of the vectors, to find each object X  X  exact k -neighborhood. We perform an ex-tensive evaluation of our algorithm, comparing against both exact and approximate baselines, and demonstrate the efficiency of our method across a variety of real-world datasets and neighborhood sizes. Our approximate and exact L2Knng variants compute the k -nearest neighbor graph up to an order of magnitude faster than their respective baselines.
 k -nearest neighbor graph, similarity search, top-k , cosine similarity
Computing the k -nearest neighbor graph ( k -NNG) for a set of objects is a common task in fields such as information retrieval, clustering, recommender systems, and online advertising. For ex-ample, item-based nearest neighbor collaborative filtering algo-rithms recommend items (e.g., books or movies) to a user based on the k most similar items to each of the user X  X  preferred items [16].
The task of computing the k -NNG is computationally expensive, requiring O ( n 2 ) similarity comparisons, given a set of n objects. As a result, current methods that tackle the problem focus on ei-ther pruning the similarity search space, or solving the problem approximately. Exact methods either rely on efficient index struc-tures to limit computation to only those object pairs with non-zero similarities (e.g., IDX [21], BMM [9]) or on effective prun-ing techniques that enable efficient discovery of all object pairs with similarity above a threshold t , which is iteratively lowered  X  until all objects have at least k neighbors (e.g., SIM [21]). Ap-proximate methods employ heuristic strategies that aim to return a neighborhood that contains the majority of the true neighbors , i.e., those that would be found by an exhaustive search. These strate-gies include focusing on object pairs that share high-weight fea-tures (e.g., Greedy Filtering [21]) and iterative improvement of an initial random k -NNG by considering neighbors X  neighbors as po-tential neighbors (e.g., NN-Descent [11]).

Despite this extensive body of work, each of these two classes of methods have their limitations. Exact methods either do not rely on pruning, use computationally expensive pruning estimates, or re-peat many similarity estimations in the search for the true minimum neighborhood similarity threshold across all neighborhoods. Ap-proximate methods use candidate selection and comparison strate-gies that can lead to unnecessary similarity computations.
In this work, we introduce L2Knng , which solves the exact cosine similarity k -NNG construction problem efficiently by effectively pruning much of the similarity search space. We focus on input objects that are encoded by sparse high-dimensional non-negative vectors . Some examples include Web pages, and user/item profiles in a recommender system. In this context, cosine similarity has long been a standard comparison measure, especially in the fields of in-formation retrieval [19] and text mining [14]. To solve the k -NNG construction problem, L2Knng first obtains an initial approximate solution by considering, for each query object, a set of candidates that are likely to be part of the final k -NNG. Then, using the ap-proximate graph as a guide to prune the search space, L2Knng exe-cutes a neighbor search for each input object. We introduce several filtering methods specific to the problem at hand that successfully prune most objects that will not be part of the final k -NNG, result-ing in few objects having their similarity to a query object com-puted in full. We evaluate our methods experimentally on a variety of real-world datasets and neighborhood sizes, against both exact and approximate baselines. We find that L2Knng can provide over an order of magnitude performance improvement over baselines.
The remainder of the paper is organized as follows. Section 2 introduces the problem and notation used throughout the paper. Section 3 summarizes existing approaches to solving the k -NNG construction problem. Section 4 introduces our methods for con-structing the exact and approximate k -NNG. We describe our eval-uation methodology in Section 5 and analyze experimental results in Section 6, and Section 7 concludes the paper.
Let D = { d 1 ,d 2 ,...,d n } be a set of objects such that each object d i is a (sparse) vector in an m dimensional feature space. We will use d i to indicate the i th object, d i to indicate the feature vector associated with the i th object, and d i,j to indicate the value (or weight) of the j th feature of object d i .
We use the cosine function to measure vector similarity. To sim-plify the presentation of the algorithms, we assume that all vectors have been scaled to be of unit length ( || d i || = 1 ,  X  d that, the cosine between two vectors d i and d j is simply their dot-product, which we denote by dot ( d i , d j ) .

Given object d i , its k nearest neighbors in D , denoted by N the set of objects in D \{ d i } whose similarity with d i among all objects in D \{ d i } . The k -NNG of D is a directed graph G = ( V,E ) where vertices correspond to the objects and an edge ( v ,v j ) indicates that the j th object is among the k nearest neigh-bors of the i th object. An approximate k -NNG is one in which the k neighbors of each vertex do not necessarily correspond to the k most similar objects.

During its execution, our method keeps track of up to k neigh-bors in each object X  X  neighborhood. We denote by the minimum (neighborhood) similarity  X  d i the minimum similarity between ob-ject d i and one of its current k neighbors. We say that a neighbor-hood is improved when its minimum similarity  X  d i increases in value, and it is complete once all true neighbors that belong to a neighborhood have been added to it. Note that subsequently pro-cessed objects that have  X  d i similarity with the query object will not be added to the neighborhood as they do not improve it. An inverted index representation of D is a set of m lists, I = { I 1 ,I 2 ,...,I m } , one for each feature. List I j ( d ,d i,j ) , also called postings, where d i is an indexed object that has a non-zero value for feature j and d i,j is that value. Postings may store additional information, such as the position of the feature in the given document or other statistics.

Given a vector d i and a dimension p , we will denote by d vector  X  d i, 1 ,...,d i,p , 0 ,..., 0  X  , obtained by keeping the p leading dimensions in d i , which we call the prefix (vector) of d we refer to d &gt;p i =  X  0 ,..., 0 ,d i,p +1 ,...,d i,m  X  as the suffix of d obtained by setting the first p dimensions of d i to 0. One can then verify that Table 1 provides a summary of notation used in this work.
Relatively few k -NNG construction algorithms have been de-signed to address cosine similarity. Park et al. [21] describe Greedy Filtering , an approximate filtering-based approach which prioritizes computing similarities between objects with high weight features in common. After first reordering the dimensions of each vector based on their weight, in decreasing weight order, the algo-rithm builds a partial inverted index, which it uses to find candi-dates for each object. Candidates for an object d i are those objects in the inverted index lists associated with the leading dimensions in d , i.e., the prefix of d i . Greedy Filtering indexes enough of each vector X  X  prefix as to lead to at least  X  candidates for each object. After all prefixes are identified and the partial inverted index is con-structed, Greedy Filtering computes pairwise similarities of objects in each inverted index list, which can lead to much more than  X  sim-ilarity computations for each object, and repeated computations for pairs of objects with two or more common features in their prefixes.
In NN-Descent , Dong et al. [11] follow an iterative neighborhood improvement strategy based on the intuition that similar objects are likely to be found among the neighborhoods of objects in a query object X  X  neighborhood. Starting with a randomly chosen initial k -NNG, they iteratively improve the graph by computing, for each object d i , via a local join , pairwise similarities between d jects in its neighborhood, and those objects that contain d neighborhoods. The neighborhoods of both objects participating in a similarity computation are updated with the result. They avoid duplication of effort between iterations by only allowing an object to participate in the local join if it has been added to some neighbor-hood in the last update. Sampling and early termination parameters provide a way to control the compromise between algorithm run-time and recall. However, NN-Descent computes O ( n  X  k 2 similarities in its first iteration. Furthermore, the algorithm does not provide a way to filter out candidates that are unlikely to improve the query object X  X  neighborhood.

A number of k -NNG construction algorithms have been pro-posed for metric spaces , where we seek the k objects with the smallest metric distance from the query. Tree-based data structures are often used to facilitate partitioning the search space, allowing neighbor searches to be prioritized within grids close to the one the query object is in [5]. These types of methods have been shown effective in low dimensional spaces, but do not scale well as di-mensionality increases.

Top-k document retrieval is a related problem from information retrieval, which has had many proposed solutions over the years. Most methods in this class have been designed for very large doc-ument collections, focused on minimizing and/or parallelizing op-erations needed to quickly answer fairly short input queries. Result sets are in most cases inexact. Some recent works use an in-memory inverted index and pruning, called safe early termination , to return the same result set as an exhaustive search [6, 9, 10, 22, 23]. One could then solve the exact k -NNG problem by executing n top-k queries with one of these methods, one for each of the input ob-jects. In their Block-Max WAND ( BMW ) method [10], Ding and Suel use an augmented index structure, called a Block-Max index, which stores inverted lists as compressed blocks of postings, along with the maximum score that could be achieved given the values in the block postings. By using the block maximum scores for early termination, many blocks can be skipped, resulting in improved ex-ecution. Dimopoulos et al. [9] extended the work of Ding and Suel and designed several methods that take advantage of Block-Max type indexes. Among them, docID-oriented Block-Max Maxscore with variable block sizes ( BMM ) has been shown to outperform the others and several baselines (including BMW ) for long queries. The method partitions the postings in each inverted list into blocks of equally-sized ID ranges, allowing fast look-up for the block a document X  X  posting may be found in. Block sizes vary based on the number of postings in each list. For each block, BMM also keeps track of the maximum document ID and maximum score for any of the postings in the block. The Maxscore [24] algorithm described by Turtle and Flood is then adapted to use block maximum scores for early termination.

Locality Sensitive Hashing (LSH) [13, 15] uses families of func-tions that hash signatures of similar objects to the same bucket with high probability. The objects in the buckets that a query ob-ject hashes to can be considered its neighbors. The similarity with a neighbor can then either be estimated by comparing the object signatures or computed exactly. Created initially to solve the top-k retrieval problem, LSH has been shown effective at solving the nearest-neighbor problem (1-NNG), but suffers from low recall as the required neighborhood size increases [10]. In this work, we specifically focus on results with high recall (at least 95%). Some recent LSH variations have tackled the k -NNG problem specifi-cally (e.g., E2LSH [2] and DSH [12]), but focus on metric distance functions between objects, such as the Euclidean distance.
Another related problem is All-Pairs Similarity Search (APSS), or similarity join , which returns all object pairs in D with a sim-ilarity value of at least some threshold t . Bayardo et al. [4] pro-posed an initial algorithm to solve APSS which employed several strategies to prune the search space, based on a predefined object processing order. The majority of subsequently developed APSS methods [1, 3, 17] use the same framework as in their work. In the context of cosine similarity, Anastasiu and Karypis recently intro-duced L2AP [1], which has been shown to outperform all previous APSS methods by introducing tighter pruning bounds in each phase of the framework.

There have been a large number of top-k retrieval and approxi-mate k -NN search methods designed for distributed or parallel ar-chitectures, which can be used to search Web-scale datasets. The focus in these methods are query objects outside the input set. In contrast, L2Knng is an in-memory serial method designed for the efficient construction of a k -NNG from the input set of objects. We leave parallel and distributed extensions as future work.
The L2Knng algorithm consists of two distinct steps. In the first step, it uses a fast method that identifies, for each object, k similar objects that may not necessarily be the k nearest neighbors. In the second step, it scans over all the objects and progressively updates the k most similar objects of each object. Specifically, while pro-cessing an object d i , which we call the query , L2Knng updates the k nearest neighbors of all previously processed objects by taking into account their similarity to the query object. At the same time, it updates the k most similar objects of the query object by consid-ering its similarity to the preceding objects. Since the second step potentially considers all pairs of objects, the final set of the k most similar objects for each object are guaranteed to be their k nearest neighbors.

The key to L2Knng  X  X  efficiency stems from the following: (i) It uses an index data structure that enables it to quickly find potential neighbors, while pruning some that do not have enough features in common with the object being indexed. (ii) When searching for neighbors, it uses several vector similarity theoretic bounds to filter out many of the potential neighbors found by traversing the index. (iii) It uses a block processing strategy, which leads to efficient traversal of the inverted index lists and additionally improves the effectiveness of the pruning bounds. (iv) Finally, the initial approx-imate k -NNG built in its first step is instrumental towards effective indexing and pruning in L2Knng .
L2KnngApprox is the inexact k -NNG construction method used by L2Knng to build an initial approximate graph. It consists of two steps. First, it builds a set of initial neighborhoods, relying on the idea that high-weight features count heavily towards the similarity of two vectors [7,21]. Then, given that an object X  X  neighbor X  X  neigh-bor is also likely their neighbor [11, 20], it iteratively enhances the k -NNG by looking for new candidates in each neighbor X  X  neigh-borhood.

The first step is achieved as follows. For each object d L2KnngApprox builds a list of up to  X  (  X   X  k ) candidates, choos-ing among those objects that have features in common with d there are no more features to check or  X  candidates were found. It then computes the exact similarities of all candidates with d adds the objects with the top k values to d i  X  X  initial neighborhood.
The choice of candidate objects is crucial to obtaining an approx-imate graph that is close to the exact k -NNG. L2KnngApprox uses an inverted index to identify candidate objects with common fea-tures with d i . As a heuristic way to prioritize high-weight common features, it sorts the features in each vector in decreasing weight order, and sorts each of the lists in the inverted index in decreasing order of feature weights. L2KnngApprox then traverses two index lists at a time, in decreasing order of their associated weights in d . From the two lists, it chooses the candidate d c with the higher prefix dot product, which is more likely to be a true neighbor.
In the second step, L2KnngApprox executes up to  X  iterative neighborhood enhancement updates, in which, for each object d its k neighbors are updated by taking into account its similarity to some of the objects that are neighbors of its neighbors. This is done as follows. For each object d i , it traverses its neighborhood in decreasing order of d i  X  X  neighbor similarities. Given some neigh-bor d j , it then traverses its neighborhood, in decreasing order of d  X  X  neighbor similarities, to identify potential neighbors for d Avoiding objects that are already in d i  X  X  neighborhood or have d in their neighborhood, L2KnngApprox greedily chooses as candi-dates only those neighbor X  X  neighbors d k with a similarity value greater or equal than that between the query vector and its neighbor, sim ( d j , d k )  X  sim ( d i , d j ) , and limits the size of the candidate list to be  X  . L2KnngApprox then computes similarities between d and candidates, updating both relevant neighborhoods with the re-sults. We use  X  as an early termination parameter, stopping itera-tions early if less than  X   X  k  X | D | neighborhood changes occurred in an update.

Our strategy for choosing candidates in the first step improves upon the work of Park et al. [21] by limiting, for each object, the number of computed similarities. High quality candidates are greedily chosen from few inverted index lists. The neighborhood enhancement step improves upon the work of Dong et al. [11] in two ways. First, it ensures an upper bound on the number of sim-ilarity computations and prioritizes those candidates more likely to improve the neighborhood. Second, the enhancement steps will probably converge faster and to higher recall, as the input neigh-bors likely have higher similarity values than the randomly chosen neighbors in their method.
L2Knng uses information in the approximate k -NNG to prune some object pairs by indexing only a subset of the features in each object. In each iteration, L2Knng needs to identify among the previ-ously processed objects those whose neighborhoods can be updated by including the query object d i . In order to do this efficiently, it builds an inverted index incrementally, delaying the indexing of d until after its processing. However, future potential neighbors can only improve d i  X  X  neighborhood if their similarity with d than the minimum similarity in d i  X  X  neighborhood,  X  d i context, Chaudhuri et al. [8] noted that, given a predefined feature order, one can stop indexing features in d i as soon as they can en-sure d i will be found when processing future objects that have a greater similarity with d i than  X  d i . By indexing only the leading features of the query object, its prefix, those future objects with common features with d i only in its suffix, which will not be able to improve its neighborhood, will not be encountered when travers-ing the index and will thus be automatically pruned.

L2Knng indexes objects until their suffix ` 2 -norm falls below the minimum neighborhood similarity  X  d i . Given that all vectors in the dataset have unit length, based on the Cauchy-Schwarz inequality, the suffix ` 2 -norm of d i is an upper bound of the similarity of d suffix with any other object [1], including unprocessed objects in the set, Once the suffix norm || d &gt;j i || falls below  X  d i , if no common fea-tures were found between d i and some object d c within d dexed prefix, then d c cannot improve d i  X  X  neighborhood, since,
The query object d i must also be identified when processing fu-ture objects if d i can improve their neighborhoods. Let t minimum neighborhood similarity for object d i at the time of its in-dexing ( indexing threshold ), which later may be different than  X  Consider indexing an object d i at a threshold t d i and then process-ing an object d j with a smaller minimum neighborhood similarity. If  X  d j  X  sim ( d i , d j ) &lt; t d i , then d i is no longer guaranteed to be found when processing d j , and d j  X  X  neighborhood may be in-exact at the end of the algorithm execution. Therefore, to ensure correctness, objects must be indexed in a strictly non-decreasing indexing threshold order. L2Knng thus fixes the object processing order based on the minimum similarities in the initial approximate k -NNG it builds before processing objects.
 Algorithm 1 Indexing in L2Knng 2: b  X  1 6: end for
Algorithm 1 details the indexing procedure in L2Knng . The pre-fix of a vector d i is indexed while its suffix ` 2 in b , is above or equal to our threshold t d i (lines 3-6). The suffix ` -norm at each indexed feature (line 5) and the suffix ` 2 the un-indexed portion of d i ( suffix estimate , line 7) are also stored, to be used in other stages of the algorithm. We denote by d indexed prefix of object d i and by d &gt; i its un-indexed suffix.
L2Knng searches for neighbors of an object in two stages. During the candidate generation stage, L2Knng computes prefix similari-ties by traversing the index, using an accumulator [19] to keep track of partial dot-products between the query and encountered objects. An accumulator is a map based data structure that accumulates val-ues for given keys. Each object with non-zero accumulated value becomes a candidate and is added to a candidate list. Then, during candidate verification , L2Knng traverses the list of candidates and finalizes their similarity computations with the query, accumulating suffix similarities for each candidate. In the end, the accumulator will contain the exact similarity with each un-pruned candidate.
A computed similarity can only improve the neighborhood of a query object d i if it is above  X  d i . Furthermore, it can only improve neighborhoods of already processed objects if it is greater than the minimum of all neighborhood similarities of indexed objects. To keep track of this value, L2Knng could update a heap data structure each time the neighborhood of an indexed object is improved, but we have found this affects overall efficiency. Instead, L2Knng ap-proximates this value by the minimum indexing threshold among all indexed objects, denoted by it , which is strictly smaller than the current minimum of all indexed objects X  neighborhood similarities. Using a similar idea as during indexing, L2Knng only starts accu-mulating for an object d c while the query suffix ` 2 -norm is above the lower of these two bounds, min( it, X  d i ) . Once the suffix ` norm falls below this threshold, only index values for objects with non-zero accumulated partial dot-products are processed. Addition-ally, L2Knng uses the initial approximate k -NNG and the current version of the k -NNG to bypass already computed similarities.
During both the candidate generation and verification stages, there is a further opportunity for pruning when a common fea-ture j is encountered between the query and candidate vectors. To be useful, the final similarity value should improve the neighbor-hoods of either the query or candidate objects. The accumulator contains the exact similarity of the two prefix vectors, and the sim-ilarity of the suffix vectors can be estimated, based on the Cauchy-Schwarz inequality, as upper bounded by the product of their suffix ` -norms [1]. Thus, a candidate can be pruned if
L2Knng employs one additional pruning strategy during the can-didate verification stage. The se [ d c ] suffix estimate value that was stored when indexing the candidate d c estimates the dot-product between the un-indexed portion of d c and any other vector in the dataset, sim ( d &gt; c ,  X  ) . We use this value here as an estimate for the similarity between the query and candidate suffix, dot ( d If the sum of the accumulated score and the estimate falls below min(  X  d i , X  d c ) , the candidate is discarded.

Having presented the different pruning bounds used in L2Knng , note that their effectiveness would be greatly reduced without first computing the initial approximate k -NNG. First, indexing thresh-olds for unprocessed objects would be unknown, and L2Knng would have to index all object features, missing an important prun-ing opportunity. Similarly, during a search, the algorithm would have to consider all possible candidates with common features, as the minimum indexing threshold it would be 0. Finally, the min-imum neighborhood similarities of previously processed objects would likely be smaller, leading to less object pairs being pruned and more neighborhood updates. While L2Knng does not require the initial approximate graph to be computed by L2KnngApprox , an initial graph with high recall will lead to more effective pruning and higher efficiency in constructing the exact graph.
 Algorithm 2 delineates the procedure used to find neighbors in L2Knng . The variable r computes the suffix ` 2 -norm of the query vector, which is used to prevent accumulating similarity for objects that cannot improve neighborhoods (line 6) in the candidate gen-eration stage. At the end of the verification stage, the accumulator contains the exact similarity between the query and objects that survive pruning. These objects are added to the candidate or query neighborhoods if they can improve them.
Algorithm 3 gives an overview of L2Knng . The initial approx-imate graph k -NNG (  X  N , line 2) bootstraps the search framework, providing the necessary processing order for the main loop. As sug-gested by Bayardo et al. [4], we reorder dimensions in all vectors Algorithm 2 Searching for neighbors in L2Knng 10: end if 11: end if 12: end for 14: end for 20: next d c 21: end if 22: end for 25: end for Algorithm 3 The L2Knng Algorithm 3: Reorder dimensions in non-decreasing object frequency order 10: end for 11: return N in non-decreasing object frequency order as a heuristic way to min-imize the inverted index size.

The index keeps growing as more and more objects are pro-cessed. The minimum indexing threshold it defined by the initial k -NNG is likely very small, causing the majority of objects in the index to become candidates for each subsequent query. While many candidates will later be eliminated based on pruning bounds that take advantage of continuously updated neighborhood similarities, the delayed pruning can lead to slower execution. L2Knng improves the indexing threshold by periodically  X  X lushing X  the index. After completing the k -NNG construction for the already indexed ob-jects, the index can be discarded, speeding up future candidate gen-eration and providing an improved minimum indexing threshold it . Neighborhood construction can be finalized for a block of objects by executing FindNeighbors for all un-processed vectors, without indexing them. L2Knng then uses the updated minimum neighbor-hood similarities of unprocessed objects to define a new processing order. Given a number of blocks parameter  X  , L2Knng finalizes a block of indexed objects after processing every | D | / X  objects.
Many of the pruning schemes used by L2Knng are similar in na-ture with corresponding schemes that were developed to solve the all pairs similarity search (APSS) problem. However, there are a number of key differences between the solutions to the two prob-lems. First, APSS seeks to prune object pairs with a similarity be-low a threshold t , while L2Knng filters those pairs that cannot im-prove k -neighborhoods. These distinct goals lead to very different pruning bounds in the two methods. The threshold t is an input to the APSS problem. In our problem, t could be chosen to be the minimum neighborhood similarity  X  d i among all objects in the true k -NNG, which is unknown and would nonetheless be a sub-optimal choice for the k -NNG construction problem. Instead, we devise better thresholds, detailed in Sections 4.2 and 4.3, that can be used in each stage of the method to safely prune object pairs that cannot be a part of the true k -NNG. Second, the APSS solutions define an object processing order based on feature weights in each vector, which is not possible in our problem due to the absence of a common indexing threshold for all objects. L2Knng instead pro-cesses objects based on the minimum neighborhood similarities of an initial approximate graph, which ensures correctness and leads to higher pruning performance. Finally, we further improve per-formance by periodically finalizing a set of neighborhoods, which provides better pruning for unprocessed objects.
In this section, we describe the datasets, baseline algorithms, and performance measures used in our experiments.
We use six text-based datasets to evaluate each method. They represent some real-world and benchmark text corpora often used in text-categorization research. Their characteristics, including number of rows ( n ), columns ( m ), and non-zeros ( nnz ), and mean row/column length ( mrl / mcl ), are detailed in Table 2. Standard pre-processing, including tokenization, lemmatization, and tf-idf weighting, were used to encode text documents as vectors. We present additional details below.  X  RCV1 is a standard benchmark corpus containing over 800,000 newswire stories provided by Reuters, Ltd. for research pur-poses, made available by Lewis et al. [18].  X  RCV1-100k and RCV1-400k are random subsets of 100,000 and 400,000 documents, respectively, from RCV1.  X  WW500 contains documents with at least 500 distinct features, extracted from the October 2014 article dump of the English
Wikipedia 1 (Wiki dump).  X  WW200 contains documents from the Wiki dump with at least 200 distinct features.  X  WW200-250k is a random subset of size 250,000 from WW200.
We compare our methods against the following baselines.  X  kIdxJoin is a straight-forward baseline similar to IDX in [21] that first builds a full inverted index. Then, without performing any pruning, it uses the index to compute exactly, via accumula-tion, the similarity of each object with all other objects in the set, returning the top-k matches for each query object.  X  kL2AP solves the k -NNG problem by executing similarity searches using L2AP [1]. We modified L2AP to allow specify-ing a set of input query vectors. Then, as we iteratively reduce the search threshold t , we provide as input only those objects with incomplete neighborhoods.  X  BMM refers to the docID-oriented with variable block sizes ver-sion of the Block-Max Maxscore method by Dimopoulos et al. [9]. The method splits inverted lists into blocks and uses max-imum scores for postings in each block to prune the similarity search space. We adapted the method for cosine similarity rank-ing and chose the same block sizes as in their paper. Blocks were stored in compressed form, using PForDelta compression [25].  X  Maxscore is an in-memory implementation of the max_score in-formation retrieval algorithm [24], as described by Dimopoulos et al. in [9], adapted to rank based on cosine similarity.  X  Greedy Filtering is a state-of-the-art approach for solving the approximate k -NNG construction problem applied to sparse weighted vectors, proposed by Park et al. [21].  X  NN-Descent was designed by Dong et al. [11] to work with generic similarity measures and has been shown effective at solv-ing the approximate k -NNG construction problem in both sparse and dense datasets.
 While LSH has been a popular method for top-k search, it does not perform well in the k -NNG construction setting. Both Greedy Filtering and NN-Descent have been shown to outperform LSH when applied to this problem, for k typically  X  10 . Addition-ally, L2AP outperformed LSH in the related APSS problem. As we will show in Section 6, L2Knng significantly outperforms kL2AP , the k -NNG method based on L2AP , as well as Greedy Filtering and NN-Descent . As a result, we have chosen not to compare against LSH in this work.
When comparing approximate k -NNG construction methods, we use average recall to measure the accuracy of the returned result. We obtain the true k -NNG via a brute-force search, then compute the average recall as,
We follow others in using the number of full similarity com-putations as an architecture and programming language indepen-dent way to measure k -NNG construction cost [11, 21]. How-ever, we use a slightly different normalization constant, NC = | D | ( | D | X  1) , as our kIdxJoin baseline does not take advantage of symmetry in similarity computations, and thus may compute up to n  X  1 similarity values for each vector in the dataset. We report, for all algorithms, scan rate = # similarity evaluations /NC , and candidate rate = # candidates /NC.

An important characteristic in our experiments is CPU runtime, which is measured in seconds. Between a method A and a baseline method B , we report speedup as the ratio of B  X  X  execution time and that of A  X  X .
Our method and all baselines are single-threaded, serial pro-grams. A C++ based library implementing NN-Descent can be found at http://www.kgraph.org/ . A C based implementation of L2AP can be found at http://cs.umn.edu/~dragos/l2ap . We imple-mented 2 kIdxJoin , kL2AP , Greedy Filtering 3 , Maxscore , BMM , L2Knng , and L2KnngApprox in C and compiled our program us-ing gcc 4.4.7 with -O3 optimization. Each method was executed on its own node in a cluster of HP ProLiant BL280c G6 blade servers, each with 2.8 GHz Intel Xeon processors and 24 Gb RAM.

We executed each method for k  X  { 1 , 5 , 10 , 25 , 50 , 75 , 100 } and tuned parameters to achieve balanced high recall and efficient execution. For all L2Knng and L2KnngApprox experiments, we set the parameter  X  = 0 . 0001 . We tested kL2AP by decreasing the threshold t in steps of 0.1, 0.25, and 0.5, and report the best results among the step choices. For the NN-Descent library 4 , we set  X  = 1 , S = 20 , and indexing K =  X  (the candidate list size  X   X  k ). For all stochastic methods, we executed a minimum of 5 tries for each set of parameter values and we report averages of all tries. We now present our experiment results, along several directions. First, we test L2KnngApprox against approximate baselines. We then evaluate the effectiveness of our exact k -NNG building strate-gies. We measure the influence of the initial approximate graph quality on L2Knng  X  X  efficiency and the pruning effectiveness of dif-ferent stages in the L2Knng filtering framework. Finally, we eval-uate the runtime and memory scalability of L2Knng as the number of input objects increases, and its efficiency as opposed to exact baselines.
The efficiency of all the approximate methods under considera-tion are dependent on the number of candidates they are allowed to consider for each object,  X  . The larger the candidate pool is, the more likely the true neighborhood is found among the ob-jects in the pool. We compare the recall and execution time of L2KnngApprox with other approximate baselines, given the same candidate list and neighborhood size parameters,  X  and k . We tested each method, without changing any other parameters, given  X  = k, 2  X  k,..., 10  X  k , on the RCV1-400k and WW200-250k datasets. We tested L2KnngApprox with  X  = 0 ( L2KnngApprox which does not execute any iterative neighborhood updates, and with  X  = 3 ( L2KnngApprox 3 ).

Figure 1 plots recall versus execution time for our experiment results. For all methods, results for  X  = k are marked with a  X -" label, and those for  X  = 10  X  k with a  X + X  label. The best results are those points in the lower-right corner of each quadrant in the figure, achieving high recall in a short amount of time. Due to lack of space, we include only results for k  X  { 50 , 100 } . Results for other k values exhibit similar trends.

Methods generally exhibit higher recall and higher execu-tion time for larger  X  values. L2KnngApprox 0 takes much less time to execute than Greedy Filtering and, given large enough
Source code available at http://cs.umn.edu/~dragos/l2knng .
The authors of Greedy Filtering kindly provided a Java-based im-plementation of their algorithm for comparison. On average, our C implementation achieved 1.13x speedup over the Java one. We thank Wei Dong for his invaluable assistance with using the KGraph library and finding NN-Descent evaluation parameters. Figure 1: Recall and execution time of approximate methods given increasing candidate pool sizes.  X  , can achieve similar or higher recall. Both L2KnngApprox and Greedy Filtering require larger  X  values than NN-Descent to achieve high recall. Yet, NN-Descent does not improve much as  X  increases. L2KnngApprox 3 is able to outperform both competitors, with regards to both time and recall, for large enough  X  .
In this work, we focused on building the exact k -NNG. While approximate methods cannot easily achieve perfect recall, we com-pared their efficiency when seeking a close approximation of the true k -NNG. We executed each approximate method under a wide range of parameters and report the smallest time for which a mini-mum recall value of 0 . 95 was achieved. Figure 2 presents execution times for the approximate methods, for four of the datasets. Re-sults for the other datasets are similar and we omit them here due to lack of space. We also include the times for our exact variant, L2Knng , as comparison. Note that execution times are log-scaled. Lower values are preferred. The NN-Descent result is not included for the WW200 dataset, as we could not obtain high enough recall (the highest recall for k = 1 was 0.8854, even with  X  = 850 ).
L2KnngApprox was more efficient than Greedy Filtering in all cases and than NN-Descent in most cases, while achieving an order of magnitude improvement over our exact solution. For problems where perfect recall is not needed, L2KnngApprox can provide a close approximation in much less time. Even though NN-Descent had similar execution times as L2KnngApprox for some neighbor-hood sizes of the RCV1 datasets, it performed poorly on the WW datasets. This may be explained by the much higher dimension-ality and mean row length of the WW datasets as compared to the RCV1 datasets, which can lead to repeated inclusion of ob-jects in computationally expensive NN-Descent local joins. In con-trast, L2KnngApprox uses several strategies that limit the number of computed dot-products. It builds a higher quality initial graph than NN-Descent , prioritizes candidate inclusion, and sets a hard limit on the candidate list size in each iterative update.
While the final recall for an L2Knng execution is 1.0, our method uses an initial approximate graph  X  N as a guide in its k -NN search. A graph  X  N with high recall provides L2Knng with higher minimum neighborhood similarity values, which translate into tighter pruning bounds and leads to fewer full vector dot-products being computed (smaller scan rate) and faster runtime. We tested the influence of the initial graph quality in three scenarios on the RCV1-400k and WW200-250k datasets. In the first scenario ( random ), we gener-ated an initial graph by randomly picking k neighbors for each ob-ject from the set of objects with which they shared at least one feature in common. In the second scenario ( fast ), we chose param-eters that ensure fast execution, without guaranteeing high recall (  X  = k,  X  = 1 ). We executed the search using 10 completion blocks in both scenarios (  X  = 10 ). Finally, we include for compar-ison the best results we achieved after a parameter search ( best ).
Figure 3 presents our experiment results. The top of the figure shows, for each k value, the recall of the initial graph for the two tested datasets. The middle and bottom of the figure show, for each k value, the candidate rate and execution time after completing the exact k -NNG construction. The results emphasize the importance of the initial graph quality in L2Knng . The initial graph random test case had recall 0.018 and 0.009 on average across all k values for the RCV1-400k and WW200-250k datasets, respec-tively. The recall was 0.496 and 0.628 in the fast and 0.883 and 0.929 in the best test cases. These better initial graphs translate into both lower candidate rates and smaller execution times. The fast case performed similarly to the best case, showing that L2Knng can be used with reasonable parameter values and does not require ex-tensive parameter tuning.
The parameters  X  ,  X  , and  X  can influence the effectiveness and efficiency of our exact and approximate algorithms. Larger values for  X  increase the number of candidates considered for building the initial graph and will likely lead to increased recall for this stage. Similarly, higher  X  values translate to more iterations of ini-tial neighborhood enhancement, at the cost of more similarity com-putations. Increasing  X  values can lead to improved candidate gen-eration pruning and faster index traversal, at the cost of reading vectors in unprocessed blocks several times to find similarities with indexed vectors. There is a trade-off between the benefit of more efficacious pruning bounds and the time taken to achieve them.
We executed parameter sensitivity experiments on the RCV1-400k and WW200-250k datasets, for k  X  { 25 , 50 , 75 , 100 } . In each ex-periment, we fixed two of the parameters and varied the third. In the first experiment, given  X  = 0 , and  X  = 10 , we varied  X  be-tween 100 and 1000 . In the second experiment, given  X  = 300 , and  X  = 10 , we varied  X  between 0 (no initial neighborhood en-hancement) and 10 . Finally, to verify the sensitivity of the num-ber of blocks parameter,  X  , given  X  = 300 , and  X  = 1 , we var-ied  X  between 1 and 500 . Due to lack of space, we include here only a summary of the parameter study results. As expected, we found recall improves when either  X  or  X  are increased. While the rise is sharp at first, it levels off quickly as the parameter values get larger, showing that the most benefit is gained from checking a relatively small number of initial candidates and executing few neighborhood enhancement rounds. In general, the best results we obtained after parameter tuning were executed with 1  X   X   X  3 and 300  X   X   X  500 . The execution time was not greatly affected as we increased the values of  X  or  X  , showing that L2Knng is not very sensitive to these parameter choices. We found that increasing the number of blocks  X  initially leads to improved performance for all k values. While the improvement is more drastic at first,  X  values greater than 50 do not improve the results much, and can eventually lead to decreased efficiency.
L2Knng works by pruning the majority of the candidates that are not true neighbors. Candidates can be pruned while checking the suffix ` 2 -norm at a common feature during the candidate gen-eration stage ( cg ), during the candidate verification stage ( cv ), or after checking the suffix estimate score ( ses ). Since a partial dot-product is accumulated for each candidate before being pruned, it is important that candidates be pruned as early as possible. In an experiment in which we used fast defaults for all parameters (  X  = k,  X  = 1 ,  X  = 10 ), we counted the number of candi-dates that were pruned in each stage of the algorithm. Additionally, we display the number of candidates that survived all pruning and had full dot-products computed ( dps ). Figure 4 shows the results of this experiment for the RCV1-400k and WW200-250k datasets, as stacked bar charts showing the number of candidates for each category.
Results show that the majority of objects are pruned soon af-ter becoming candidates, in the candidate generation stage ( cg ). Of the remainder, most are pruned by the suffix estimate bound ( ses ), which is checked once, at the beginning of the candidate verification stage, and by additional pruning in the candidate ver-ification stage ( cv ). On average, across all k values, 0.15% and 0.02% of candidates survived all pruning for the RCV1-400k and WW200-250k datasets, respectively. A large number of objects never become candidates in L2Knng , as a result of either the ` norm based candidate acceptance bound in the candidate genera-tion stage of the algorithm, or due to the prefix-filtering based in-dex reduction. On average across all k values, only 38.17% and 88.66% of all potential candidates actually became candidates for the RCV1-400k and WW200-250k datasets.
As the dataset size increases, the exact k -NNG problem will take longer to solve, as each object has more potential neighbors that have to be vetted. As a way to verify scalability, we tested our methods on three subsets of the RCV1 and two subsets of the WW200 datasets. For each data subset, Table 3 reports, for k  X  { 25 , 50 , 75 , 100 } , the mean per-vector search time (top) and the maximum amount of memory used (bottom) for L2Knng and L2KnngApprox . The  X  sz column shows the relative dataset size increase. Parameters were tuned to achieve efficient execution, and, in the case of L2KnngApprox , high recall (95%).

The results show that the performance of both methods scales linearly compared to the dataset size. As the dataset size increases, our two methods perform better than they did for the smaller datasets (e.g., it takes much less than 8.04x the time of search-ing RCV1-100k to search RCV1 for 100 neighbors), while using memory directly proportional to the number of objects in the set.
The primary goal of this work is the efficient construction of the exact k -NNG. Figure 5 presents execution times for the exact meth-ods, for all six of the tested datasets. We also include the times for our approximate algorithm, L2KnngApprox , as comparison. Note that execution times are log-scaled, and lower values are preferred. The Maxscore and BMM experiments on the WW200 and WW500 datasets were terminated early, after executing for 5 days, which is more than twice the execution time of kIdxJoin for these datasets. Additionally, Table 4 shows the average speedup, across all k val-ues, of our algorithms against the best time achieved by competing approximate (left) and exact (right) methods.

L2Knng performed best among all exact methods, achieving over an order of magnitude improvement versus kIdxJoin for small values of k . The speedup is less pronounced as the value of k in-creases. This may be partially due to an increased number of neigh-total time (s), log-scaled Figure 5: Exact k -NNG construction efficiency comparison. Table 4: Average speedup of L2Knng and L2KnngApprox over best competing method. borhood updates during the search, which, in our implementation, incur the cost of heapifying neighborhood data structures.
While using a similar filtering framework as L2Knng , kL2AP performs poorly, at times taking longer to execute even than kIdxJoin , which is equivalent to a brute-force search. This may be due to repeated indexing in kL2AP and the size of its final in-dex. As t nears 0, even if we are only interested in finalizing a few neighborhoods, the inverted index lists will contain the majority of values in the dataset, and traversing it will produce many candi-dates. In contrast, L2Knng indexes each vector only once and uses block completion as an effective strategy to improve pruning.
Maxscore and BMM performed worst among all exact methods, which may be explained by the length of the query vectors used in solving the k -NNG problem. The methods were designed for short queries. They perform a sorting operation with each query and simultaneously traverse as many inverted lists as the number of features in the query vector, which can lead to loosing cache local-ity. In contrast, our method traverses one inverted list at a time and updates an accumulator in increasing index order, which is much more cache friendly for long queries.

Table 5 presents timing and scan rate results for the three top performing exact and approximate methods. As in Section 6.1.2, we report the smallest time for which a minimum recall value of 0 . 95 was achieved for all approximate methods. Due to lack of space, we only include results for the WW500 and RCV1 exper-iments and k  X  { 1 , 25 , 100 } . We use bold font to highlight the best result among approximate (top) and exact (bottom) methods, which are separated in the table by a dashed line. L2Knng and L2KnngApprox achieve most of the lowest scan rates among the competing methods, highlighting the ability of L2KnngApprox to find a quality approximate solution using few similarity compar-isons and the pruning ability of the L2Knng filtering framework. For the RCV1 datasets, NN-Descent achieves similar execution times as L2KnngApprox while having much higher scan rates. NN-Descent does not use accumulation and can take advantage of cache locality afforded by the short vector sizes in the RCV1 datasets. However, much longer vectors, combined with high scan rates, lead to poor NN-Descent performance on the WW datasets.
We presented L2Knng , our exact filtering-based solution to the cosine similarity k -NNG construction problem. L2Knng uses an initial approximate solution graph as a guide to find the desired true neighborhoods, through a modified similarity search frame-work. We introduced several new pruning bounds specific to this problem, which leverage the Cauchy-Schwarz inequality in partial vector dot-products at each stage in the framework to prevent full similarity computation for most object pairs. L2Knng achieves an order of magnitude improvement against exact baselines. Our in-exact k -NNG construction method, L2KnngApprox , achieves high recall in less time than competing approximate methods, and is an order of magnitude faster than our approximate baselines.
In this paper, we have focused on the cosine similarity function for building the k -NNG. It would be interesting to evaluate the ef-ficiency of ` 2 -norm filtering in the context of other similarity func-tions, such as the Dice and Tanimoto similarities. Another avenue of research involves scaling up the number of threads and proces-sors used to solve the problem, which in turn will scale the size of the problem that can be efficiently solved.
 This work was supported in part by NSF (IIS-0905220, OCI-1048018, CNS-1162405, IIS-1247632, IIP-1414153, IIS-1447788), Army Research Office (W911NF-14-1-0316), Intel Software and Services Group, and the Digital Technology Center at the Univer-sity of Minnesota. Access to research and computing facilities was provided by the Digital Technology Center and the Minnesota Su-percomputing Institute. We thank the reviewers for their helpful comments.
 Table 5: Execution time and scan rate for competing algo-rithms. Best results are emphasized in bold.
 [1] David C. Anastasiu and George Karypis. L2ap: Fast cosine [2] Alexandr Andoni and Piotr Indyk. Near-optimal hashing [3] Amit Awekar and Nagiza F. Samatova. Fast matching for all [4] Roberto J. Bayardo, Yiming Ma, and Ramakrishnan Srikant. [5] Alina Beygelzimer, Sham Kakade, and John Langford.
 [6] Andrei Z. Broder, David Carmel, Michael Herscovici, Aya [7] Chris Buckley and Alan F. Lewit. Optimization of inverted [8] Surajit Chaudhuri, Venkatesh Ganti, and Raghav Kaushik. A [9] Constantinos Dimopoulos, Sergey Nepomnyachiy, and [10] Shuai Ding and Torsten Suel. Faster top-k document retrieval [11] Wei Dong, Charikar Moses, and Kai Li. Efficient k-nearest [12] Jinyang Gao, Hosagrahar Visvesvaraya Jagadish, Wei Lu, [13] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. [14] Andreas Hotho, Andreas N X rnberger, and Gerhard Paa X . A [15] Piotr Indyk and Rajeev Motwani. Approximate nearest [16] George Karypis. Evaluation of item-based top-n [17] Dongjoo Lee, Jaehui Park, Junho Shim, and Sang-goo Lee. [18] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. [19] Christopher D. Manning, Prabhakar Raghavan, and Hinrich [20] Rodrigo Paredes, Edgar Ch X vez, Karina Figueroa, and [21] Youngki Park, Sungchan Park, Sang-goo Lee, and Woosung [22] Cristian Rossi, Edleno S. de Moura, Andre L. Carvalho, and [23] Trevor Strohman and W. Bruce Croft. Efficient document [24] Howard Turtle and James Flood. Query evaluation: [25] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter
