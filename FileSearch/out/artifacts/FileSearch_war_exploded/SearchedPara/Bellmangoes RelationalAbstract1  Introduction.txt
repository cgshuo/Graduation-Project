 Kristian Kersting 1 kersting@inf orma tik.uni-freibur g.de Martijn Van Otterlo 1 otterlo@cs.utwente.nl Luc De Raedt deraedt@inf orma tik.uni-freibur g.de There has been a lot of atten tion and progress in re-inforcemen t learning (RL) and Mark ov decision pro-cesses (MDPs) recen tly. Sev eral basic algorithms have been prop osed and their beha vior is relativ ely well un-dersto od today (Sutton &amp; Barto, 1998). This has led to an increased interest into the e X ects of generaliza-tion and to new challenges. One of them concerns the use of RL in relational domains (D X  X eroski et al., 2001). Even though a num ber of relational RL algorithms has been dev elop ed | essen tially through varying the un-derlying function appro ximators (Driessens &amp; Ramon, 2003; G X artner et al., 2003) | the problem of relational RL is still not well understo od and a theory of rela-tional RL is lacking.
 In traditional RL, the Bellman bac kup operator is one of the cen tral concepts. A particularly interesting ap-proac h is that of Dietteric h and Flann (1997), who sho wed that value bac kups in mo del-based RL can be upgraded to region-based bac kups, where multiple states are updated sim ultaneously using a bac kup op-erator that rev erses the action operators.
 Inspired by this work, the key con tribution of this pa-per is the introduction of a relational Bellman bac kup operator, called ReBel . ReBel is dev elop ed within a simple probabilistic STRIPS-lik e relational formal-ism that incorp orates sev eral elemen ts of relational and logical Mark ov Decision Programming (Kersting &amp; De Raedt, 2003; Van Otterlo, 2004) suc h as abstract states that are represen ted using relational queries. Using ReBel , we then dev elop a mo del-based rela-tional RL algorithm and demonstrate it on a num-ber of exp erimen ts. The approac h is also related to that by Boutilier et al. (2001) who emplo y a situation calculus based language. Although their work is cer-tainly elegan t and principled, due to the complexit y of the language, they neither rep ort on a complete im-plemen tation nor presen t automated exp erimen ts. In con trast, our approac h is simpler and therefore fully automated. It deals fully automatically with the same exp erimen tal example that Boutilier et al. rep ort on. Outline: Section 2 brie X  X  reviews relational logic and MDPs. After discussing value iteration (VI) for MDPs in Section 3, we introduce a language to compactly specify MDPs over relational domains in Section 4. In Section 5, we dev elop a relational VI algorithm based on ReBel . It is empirically validated in Section 6. Before concluding we discuss related work. Relational Logic , cf. (Nienh uys-Cheng &amp; de Wolf, 1997): An alphab et  X  is a set of relation sym bols p with arit y m  X  0, and a set of constan ts c . An atom p ( t 1 ; : : : t m ) is a relation sym bol p follo wed by a brac k-eted m -tuple of terms t i . A term is a variable X or a constan t c . A conjunction A is a set of atoms. The set of variables in a conjunction A is denoted as vars( A ). A substitution  X  is a set of assignmen ts of terms to all t i are terms. A term, atom or conjunction is called ground if it con tains no variables. Conjunctions are implicitly assumed to be existential ly quanti X e d . A conjunction A is said to be  X  -subsumed by a conjunc-tion B , denoted by A  X   X  B , if there exists a substi-tution  X  suc h that B X   X  A . The most gener al uni X er (mgu ) for atoms a and b is denoted by mgu ( a ; b ). A (Horn) clause H  X  B consists of a positiv e atom H and a conjunction B and can be read as H is true if B is true . The greatest lower bound (glb ) of two conjunc-tions A and B is the most general conjunction that is subsumed by both A and B . Both subsumption and glb are also de X ned for clauses. The Herbr and base of  X , HB  X  , is the set of all ground atoms whic h can be constructed with the predicate sym bols and constan ts of  X . An interpr etation is a subset of HB  X  . Our running example will be blocks world . Here, a blo ck X can be moved on top of another blo ck Y , de-noted as move ( X ; Y ). Valid relations are on ( X ; Y ), i.e. blo ck X is on Y , and cl ( Z ), i.e. blo ck Z is clear . To mo del the  X  X or, we follo w a common approac h. It is a set of blo cks whic h cannot be on top of other blo cks. Mark ov Decision Pro cesses , cf. (Sutton &amp; Barto, 1998): A Markov Decision Process (MDP) is a tuple M = h S; A; T; R i , where S is a set of states, A a set of actions, T : S  X  A  X  S ! [0 ; 1] a transition model and R : S  X  A  X  S ! [0 ; 1] a rewar d model . The set of actions applic able in a state s 2 S is denoted A ( s ). A transition from state i 2 S to j 2 S caused by some action a 2 A ( i ) occurs with probabilit y T ( i; a; j ) and a rew ard R ( i; a; j ) is receiv ed. T de X nes a prop er probabilit y distribution if for all states i 2 S and all actions a 2 A ( i ): P j 2 S T ( i; a; j ) = 1. A deterministic policy  X  : S ! A for M speci X es whic h action a 2 A ( s ) will be executed when the agen t is in some state s 2 S , i.e.  X  ( s ) = a . Giv en some MDP M = h S; A; T; R i , a policy  X  for M , and a disc ount factor  X  2 [0 ; 1], the state value func-tion V  X  : S ! R represen ts the value of being in a state follo wing policy  X  , w.r.t. exp ected rew ards. A similar state-action value function Q  X  : S  X  A ! R can be de X ned. A policy  X   X  is optimal if V  X   X  ( s )  X  V  X  0 8 s 2 S and 8  X  0 . Optimal value functions are de-noted V  X  and Q  X  . Bellman's (1957) optimality equa-tion states:
V  X  ( s ) = max From this equation, basically all metho ds for solving MDPs can be deriv ed. For example, the well-kno wn exact solution technique value iter ation (VI) is ob-tained from (1) by turning it into an update rule: V Based on Equation (2), the VI algorithm can be stated as follo ws: starting with a value function V 0 over all states, we iterativ ely update the value of eac h state according to (2) to get the next value functions V t ( t = 1 ; 2 ; 3 ; : : : ). VI is guaran teed to con verge in the limit towards V  X  , i.e. the Bellman optimalit y equation (1) holds for eac h state.
 Traditional VI as expressed by Equation (2) assumes that all states and values are represen ted explicitly in a table. This is impractical for all but the smallest state spaces. Furthermore, for relational domains, where the num ber of states can gro w very large (even in- X nitely large) this is infeasible. Therefore, metho ds that mak e abstract from speci X c states are needed. Suc h a metho d is dev elop ed in the next sections. Traditional MDPs are essen tially prop ositional in that eac h state can be represen ted using a separate prop osi-tion. In Markov decision programs these prop ositional sym bols are replaced by abstract states: De X nition 1 An abstr act state is a conjunction Z of logical atoms, i.e., a logical query.
 Abstract states represen t sets of states. More formally , a state is an interpretation, i.e. a set of grounds facts. Consider e.g. the state z = cl ( a ) ; cl ( b ) ; on ( a ; blo cks world. An abstract state Z is, e.g., cl ( X ). It represen ts all states that are subsumed by Z , i.e., all interpretations in whic h there exists something that is clear.
 We can now introduce the basic ingredien ts of Mark ov decision programs, namely , abstract actions , ab-stract rew ards , and integrit y constrain ts . An abstract action is de X ned as follo ws.
 De X nition 2 An action 2 is a  X nite set of action rules H and the arguments of the action and B is an abstr act state denoting the preconditions of A . H i is the i -th possible outc ome of A . It holds that P i p i = 1 . We assume that vars( A ) = ( vars ( H i ) [ vars ( B )). The seman tics of the action de X nition are: If the curr ent state b is subsume d by B , i.e., b  X   X  B , then taking action A will result in [ b n B X  ] [ H i  X  with probability p . So, if the preconditions are full X lled, all outcomes are possible. As an illustration, consider whic h moves blo ck X on Y with probabilit y 0 : 9. With probabilit y 0 : 1 the action fails, i.e., we do not change the state. Applied to the above state z the ac-tion tells us that move ( a ; b ; c ) will result in z 0  X  on probabilit y 0 : 1 we stay in z . This type of action de X -nition implemen ts a kind of probabilistic STRIPS op-erator.
 The mo del R of abstract rew ards speci X es the re-wards generated by entering abstract states. In our framew ork it coincides with our initial abstr act state value function V 0 .
 De X nition 3 An abstr act state value function V is a  X nite list of value rules of the form c  X  B wer e B is an abstr act state and c 2 R .
 To any abstract state Z , V assigns the maximal value c of all matc hing value rules c  X  B to Z as value. A rule matc hes if Z  X   X  B . Consider e.g. R = V 0 as
It assigns 0 to z but 10 to z 0 . Using true in the last value rule assures that all state are assigned a value. To dev elop ReBel , we will also emplo y abstr act action-state value functions, whic h are similar to ab-stract state value functions and of whic h an example can be found in Section 5.2.
 De X nition 4 An abstr act state action value function Q is a  X nite set of Q -rules of the form c : A  X  B wer e B is an abstr act state, A is an action and c 2 R . To any abstract state-action \pair" B and A , Q assigns the maximal value c of all abstract state action rules subsumed by A  X  B .
 Rew ards are speci X ed over queries, i.e., existen tially quan ti X ed goals. Although these are simple, they are expressiv e enough to specify man y interesting prob-lems studied by the (relational) RL comm unit y suc h as shortest-p ath problems . Here, the goal is to reac h certain (abstract) states. When a goal state is entered, the pro cess ends. In RL, episo dic tasks are enco ded us-ing absorbing states . We enco de it by arti X cial deter-ministic actions suc h as on ( a ; b ) 1 : 0: absorbing  X  X  X  X  X  X  X  X  X  whic h denotes that all states that are subsumed by on ( a ; b ) transition only to themselv es and generate only zero rew ards. For example, z is not absorbing but z 0 is.
 Finally , we need a way to cop e with the integrit y constrain ts imp osed by our domain. For instance, in the move de X nition above we emplo yed symmetry of 6 =. This can be mo deled by a set C of integrit y con-strain ts. Eac h integrit y constrain t is a Horn clause. For instance in the blo cks world, no blo ck can be free if there is a blo ck on top of it and no blo ck can be on itself: false  X  on ( X ; Y ) ; cl ( Y ) and X 6 = Y  X  on ( The completion of an abstract state Z is the least  X x-point of C [f Z g , i.e., all facts deducible from C [f Z g . For example, on ( a ; b ) does not enco de that a is not b . Using the rules above, this state is completed to on ( a ; b ) ; a 6 = b . Furthermore, if the completion in-cludes false , the state does not satisfy the constrain ts, i.e., it is an illegal state. To deal with integrit y con-strain ts, we also have to adapt our notations of action de X nitions and generalit y. Action de X nitions are now constrained so that they cannot lead to illegal states. For subsumption we emplo y the integrit y constrain ts as a bac kground theory and use Bun tine's gener alize d subsumption framew ork (Bun tine, 1988).
 Along the lines of (Kersting &amp; De Raedt, 2003; Van Otterlo, 2004), it can be pro ven that any Mark ov de-cision program induces a (possibly in X nite) MDP . We will now dev elop a value iteration algorithm for Mark ov decision programs, i.e., given an abstract re-ward mo del R , i.e., initial abstract state value function V , compute the next abstract state value functions V , t = 1 ; 2 ; : : : The main idea is to upgrade Bellman's traditional bac kup operator in Equation (2). Therefore, we it-erate over: 1) : Regress all preceding abstract states from V t . 2) : Compute Q t +1 over the regressed states. 3) : Compute V t +1 by maximizing over Q t +1 . We will now discuss eac h step in turn. 5.1. Regression Let V t be the curren t abstract state value function, say V 0 , and consider the abstract action move . For a single Bellman bac kup, all abstract states S whic h lead to a condition in V 0 when taking action move have to be computed. Thus, we have to reason from post-to preconditions. For example, the  X rst outcome of move ( a ; b ; c ) can lead from state S  X   X  cl ( a ) ; cl on ( a ; c ) ; on ( b ; d )  X  (inequalit y constrain ts omitted) to the abstract state S 0  X  on ( a ; b ). Thus, we have to compute the weak est preconditions for the outcomes of move and S .
 De X nition 5 All abstr act states which lead to S 0 when following some action rule H i p i : A  X  X  X  X  B constitute the so called weakest precondition wp i ( A; S 0 ) of the i -th outc ome of A .
 For example, S lies in the weak est precondition of S 0 , i.e., S 2 wp 1 ( move ( X ; Y ; Z ) ; S 0 ) but it does not lie in wp 2 ( move ( X ; Y ; Z ) ; S 0 ) To compute wp 1 ( move ( X ; Y ; Z ) ; S 0 ) we can assume that we \mo ved" from S to S 0 . Thus, 1) the preconditions of the action (rule) are full X lled in S , and 2) S 0 is partially caused by the  X rst outcome of the action. As an illustration of 2), consider on ( a ; b ) : move caused on ( a ; b ): We have been in abstract state S 1  X   X  cl ( a ) ; cl ( b ) ; on ( a ; Z ) ; a 6 = b and moved X = a on Y = b . move did not cause on ( a ; b ): We moved X on Y but not a on b . There-fore, we have been in abstract states T  X   X  cl ( X ) ; cl ( Y ) ; on ( X ; Z ) ; on ( a ; b ) ; X 6 = satisfying that we did not move a on b , i.e., on ( X ; Y ) 6 = on ( a ; b ), and that we did not move a from b away, i.e., on ( X ; Z ) 6 = on ( a ; b ). The con-strain ts guaran tee that applying move ( X ; Y ; Z ) in T preserv es on ( a ; b ). The de X nition of S simpli X es to S 2  X   X  T ^ X 6 = a  X  , S 3  X   X  T ^ X 6 = a ^ Z 6 = b  X  , S  X  T ^ Y 6 = b ^ X 6 = a  X  , and S 5  X   X  T ^ Y 6 = b ^ Z 6 = All S i are completed to the same state namely S and constan ts are mutually di X eren t.
 The abstract states S 1 ; S 6 together logically de X ne wp 1 ( move ( X ; Y ; Z ) ; S 0 )  X   X  S 1 _ S 6  X  . So far, we considered a single e X ect only , namely on ( a ; b ). In general, however, there can be multiple Pro cedure 1: WeakestPre returns the weak est pre-(com bined) e X ects that are or that are not caused by taking action move , cf. WeakestPre in Pro cedure 1. Consider for example S  X   X  on ( a ; b ) ; on ( c ; d )  X  . Mo ving a blo ck on some other blo ck can have caused either on ( a ; b ) or on ( c ; d ), or neither of them, cf. line 2. As-sume that no e X ect was caused. Then, S 00 is empt y and P = H 1 , cf. line 2. Therefore,  X  is the empt y substitu-tion and S  X   X  on ( a ; b ) ; on ( c ; d ) ; cl ( X ) ; cl (inequalit y constrain ts omitted) is a possible preim-age, cf. line 3. However, we kno w that move did not cause on ( a ; b ) ; on ( c ; d ). Therefore, it holds on ( X ; Z ) 6 = on ( a ; b ) ^ on ( X ; Z ) 6 = on 6. S can be simpli X ed for instance to S; X 6 = a ; X 6 = c whic h is a legal abstract state. The case that the ac-tion caused some e X ects is covered by the \mgu ( S 00 ; P ) exists " conditition in line 2. It is treated analogously . 5.2. Computing Abstract State Action Values Giv en the regressed abstract states and the curren t abstract state value function V t , we now compute an abstract state-action value function Q t +1 according to Pro cedure 2. To do so, (A) we treat eac h outcome of an action A as though it would be a single action and compute its abstract state action value, cf. line 4. Then, (B) we com bine the values of all outcomes to an abstract state action value for A , cf. lines 8{12. For the sak e of brevit y, we will not state constrain ts in the examples till the end of Section 5 : 3.
 For step (A) , consider again the  X rst outcome of move . The weak est precondition was wp 1 ( move ( X ; Y ; Z ) ; S 0 )  X  S _ S 6 . Because S 6 ; is absorbing, we assign an abstract state action value of 10 for taking action move , i.e., 10 : move ( X ; Y ; Z )  X  S 6 . The value of S 1 , however, is dep enden t on V t ( S 0 ), i.e. in our example V 0 . Assuming a discoun t factor of 0 : 9 this yields R ( S ) + p 1  X  0 : 9  X  V ( S 0 ) = 0+0 : 9  X  0 : 9  X  10 = 8 : 1 , i.e., 8 : 1 : move Pro cedure 2: QR ules returns the Q -rules of an action S 1 . Doing the same for all other rules in V 0 results in: For the second outcome of move , step (A) leads to: For step (B) , we note that eac h of these rules describ es situations suc h as if we are in a state then we can get some value for achieving the i -th outc ome of action A . This information has to be com bined to an abstract state action values for A . To do so, we select a rule from h a i X h c i , say h b i , and a rule from h d i X h f i , say h f i , and chec k whether we can be in both abstract states at the same time and whether we can apply the same action. In other words, we compute the greatest lower bound (glb) of the logical clauses underlying both value rules. If the glb (where the actions have to unify) exists and it is a legal state, then it is inserted as a new rule, cf. line 11. The value of the new rule is the sum of values of the com bined rules. For h b i and h f i this yields In con trast, h b i and h d i do not give a new rule. In our blo cks world example, QR ules yields the fol-lowing abstract state action value function when ap-plied on V 0 and move and absorbing : Note that we have sorted the Q -rules in descending order only for the sak e of readabilit y. 5.3. Computing Abstract State Values The set of Q -rules enables one to compute the next abstract state value function V t +1 . In con trast to the traditional case, Q -rules, i.e., values of abstract state action pairs, can overlap suc h as Q -rules h 1 i and h 2 i . To compute abstract state values we mak e use of the fact that V t +1 ( S ) = max A Q t +1 ( S; A ) due to Eq. (3). In general, any value-preserving transformation can be applied. In this pap er, we use a simple separate-and-conquer rule learning approac h where the rules to learn and the examples to learn from coincide, see VR ules in Pro cedure 3. We searc h for a Q -rule m having a maximal Q -value among Qrules , lines 3{4, separate the covered Q -rules, line 5, and recursiv ely conquer the remaining Q -rules by selecting more rules until no Q -rules remain, line 6. The main di X erence is that we select m and add it to V t +1 only if there is no other Q -rule left in Qrules with the same value whose body subsumes the body of m , cf. line 8. In our running example, we start with rule h 1 i . Because it is not subsumed by any other rule having the same value, we add 10  X  on ( a ; b ) to V 1 and, because it subsumes h 2 i , we remo ve h 2 i from Qrules . The remaining highest valued rule is h 3 i , and we iterate. After completing, this yields the new value function V 1 (constrain ts listed again): 5.4. Relational Bellman Bac kup Op erator To summarize, the general scheme of ReBel is: 1) Compute the weak est precondition of eac h action outcome for eac h abstract state in V t using Weakest-Pre . As done in QR ules , 2a) assign to eac h abstract state { action outcome pair computed in 1) a Q -value and 2b) com bine them based using the glb. 3) Maxi-mize the Q -rules to compute V t +1 using VR ules . Note that in 2b) , if there are n &gt; 1 man y outcomes of an action, then the Q -values of the n -th outcome are com bined with already com bined Q -values of the n  X  1 previous outcomes. Thus, there are n  X  1 man y com bi-nations per action. This migh t pro duces man y rules. To overcome this, one can adapt VR ules maximizing Q -rules to compress Q -rules: if we are in a state with di X eren t curren tly com bined values for compatible ac-tions, then we select only the higher one. This is safe because the higher valued Q -rule subsumes the lower valued one. Therefore, it would have been selected in any case later on.
 Formally , this Bellman bac kup requires an in X nite num ber of iterations to con verge to V  X  , cf. Section 6. In practice, we stop when the abstract value function changes by only a small amoun t. In this section we empirically validate ReBel . We im-plemen ted ReBel with compressing Qrules in the Pro-log system YAP version 4.4.4. and we used the supple-men ted constr aint hand ling rules library (Fr X  uhwirth, 1998). In all exp erimen ts we assume a discoun t factor of 0 : 9 and a goal rew ard of 10, i.e., in all other states we receiv e 0 rew ard. Only goal states are absorbing. Exp erimen ts were run on a 3 : 1 GHz Lin ux mac hine. The running times were estimated using YAP's build-in statistics ( runtime ;  X  ). We focused on standard examples kno wn from the relational RL literature. Blo cks World Exp erimen t I: We consider cl ( a ) as goal in our probabilistic blo cks world setting. The ex-perimen t sho ws that even on simple problems ReBel is not guaran teed to con verge on the structural level. Figure 1 sho ws the abstract state value function after 10 iterations. It took ReBel roughly 1 min ute to it-erate ten times. Figure 1 highligh ts that states that are one step further away from the goal get the same value. The value, however, is lower because of the ad-ditional blo ck on top of the stac k of a . Thus, because the num ber of blo cks is not restricted, value iteration will nev er stop.
 Prop osition: Abstraction does not guaran tee con ver-gence in in X nite domains because an in X nite num ber of abstract states can be required.
 This is interesting, because in X nite state spaces eas-ily arise when relational represen tations are used and relational abstraction was hop ed to be a solution. Nev-ertheless, relational value iteration can con verge even for in X nite domains as our third exp erimen t will sho w. Blo cks World Exp erimen t II: We consider the goal on ( a ; b ) in a deterministic blo cks world because it is rep orted to be a hard problem for mo del-free relational RL (RRL) approac hes (D X  X eroski et al., 2001; Driessens &amp; Ramon, 2003). For instance, Driessens and Ramon (2003) rep ort that on average the learned policies did not reac h optimal performance even for 5 blo cks. Using the same exp erimen tal set-up as in our  X rst exp erimen t but a deterministic move action, ReBel computed V 10 in less than 12 min utes. The abstract value function is partially sho wn in Figure 2. Because the move action is deterministic, V 10 is optimal for 10 blo cks (more than 58 million ground states). The opti-mal policy can directly be extracted by computing the maximizing Q -rules for eac h abstract state. In our ex-ample, this results in remo ving the top elemen ts from the stac ks on top of a and b . However, to compactly represen t this strategy , one needs to de X ne the predi-cate ontop . In the exp erimen ts Driessens and Ramon (2003) rep orted on, this was alw ays the case. The pol-icy based on Rebel is optimal no matter how man y blo cks there are.
 Load-Unload Exp erimen t: Our  X nal exp erimen t considers the logistics domain whic h Boutilier et al. (2001) solv ed semi-automatically . The domain con-sists of cities, truc ks and boxes. Boxes can be loaded onto and unloaded from truc ks, and truc ks can be driv en between cities. The predicate on ( B ; T ) denotes that a box B is on the truc k T , bin ( B ; C ) denotes that a box B is in some city C and tin ( T ; C ) denotes that a truc k T is in city C . The actions that can be performed are: load ( B ; T ) and unload ( B ; T ) specifying how a box B can be loaded onto or loaded from a truc k T and drive ( T ; C ) specifying that the truc k T is driv en to city C . The actions in this domain have probabilistic e X ects. The probabilit y of failing a load or unload action, i.e., staying in the curren t state, dep ends on whether it rains or not, denoted by rain . The action speci X cation is as follo ws (we omit the failing speci X -cations for the sak e of brevit y): where the probabilit y pr is 0 : 9 if R is rain and 0 : 7 if R is not rain . To correctly handle the explicit negation we used for rain , we pro vided false  X  rain ; not rain as constrain t. The goal in this domain is to get some box b in p where p stands for Paris , i.e., in bin ( b ; p ) we get a rew ard of 10.
 ReBel ran for less than 6 seconds to compute the re-sults summarized in Table 1. In con trast to the blo cks world examples, the solution con verges both at the value level and at the structur al level. E.g., tak e the situation in whic h a truc k is in a city di X eren t from Paris and the box is there too. Then, it will tak e three steps ( load  X  drive  X  unload ) to reac h the goal state and the state value in V 10 is 6 : 702 in case it rains. The abstract state value function applies no matter how man y truc ks, boxes and cities are presen t. In the past few years, there has been an increased and signi X can t interest in using rich relational represen ta-tions for mo deling and learning MDPs.
 In mo del-free relational RL, one has studied dif-feren t relational learners for function appro ximation (D X  X eroski et al., 2001; Leco euc he, 2001; Driessens &amp; Ramon, 2003; G X artner et al., 2003). Others have ap-plied Q -learning based on pre-sp eci X ed abstract state spaces: Kersting and De Raedt (2003) investigate pure Q -learning, Van Otterlo (2004) learns the Q -function via learning the underlying transition mo del. Fern et al. (2003) extended previous work on upgrading learned policies for small relational MDPS (RMDPs) with appr oximate d policy iter ation . Finally , Guestrin et al. (2003) recen tly rep orted on class-based, appro x-imate value functions for RMDPs.
 For mo del-based approac hes, there has been a surpris-ing lack of researc h on exact solution metho ds. From a general point of view, ReBel is closely related to deci-sion theoretic regression (DTR) (Boutilier et al., 1999) and, because of that, it is also related to regression planning in the same way as DTR is. Within DTR, most algorithms are designed to work with proposi-tional represen tations. Actually , the only exception the authors kno w of is that of Boutilier et al. (2001). ReBeL relates to this in that it also is a mo del-based exact solution metho d for RMDPs. One key di X er-ence is that Boutilier et al. emplo y situation calculus for represen ting RMDPs. Situation calculus is very expressiv e and as a consequence it is harder to sim-plify the logical descriptions of the abstract value func-tions states that are obtained. This may also explain why | to the best of the authors' kno wledge | that approac h has not been fully implemen ted and exp er-imen ted with. In con trast, because of the use of a simpler logical language, the simpli X cation in ReBel is computationally feasible. As sho wn in the exp eri-men ts, ReBel successfully and fully automatically im-plemen ts a relational value iteration.
 Finally , the work by Dietteric h and Flann (1997) is also concerned with generalizing Bellman bac kups but no relational represen tation is used. The key con tribution of this pap er is the introduc-tion of ReBel , a relational upgrade of the Bellman update operator. It has been used to implemen t a re-lational value iteration algorithm. It has been sho wn to be e X ectiv e in a num ber of simple though signi X -can t examples. This { in turn { has led to a num ber of novel insigh ts into relational MDPs. First, it has been sho wn that value-based metho ds for relational MDPs may not con verge because an in X nite num ber of abstract states has to represen ted. Second, we high-ligh ted that in suc h cases backgr ound know ledge may enable the learning of optimal policies. So, dep ending on the represen tation of the problem, one can or can-not learn the optimal policy . Therefore, using bac k-ground kno wledge in RMDPs is not only an interesting feature, but in some cases also a necessit y for success-ful learning. In this way, we have given an explanation for and con X rmed some of the exp erimen tal insigh ts of the early relational RL work (D X  X eroski et al., 2001). Further work could address com bining ReBel with other types of value-based metho ds, extending the rep-resen tation language, e X cien t data structures, com-plexit y analysis, and emplo ying other learning algo-rithms to compress value functions.
 The authors hop e that the theoretical insigh ts, as well as the algorithm dev elop ed in this pap er, will be help-ful in adv ancing the  X eld of relational RL as well as con tribute to an impro ved understanding of the prob-lems involved.
 The authors would like to thank the anon ymous re-view ers for their helpful commen ts. This researc h was supp orted by the Europ ean Union IST programme, con tract no. FP6-508861, APrIL II . Martijn Van Ot-terlo was supp orted by a Marie Curie fello wship at DAISY, HPMT-CT-2001-00251.

