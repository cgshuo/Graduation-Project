 Within a search engine, query auto-completion aims to pre-dict the final query the user wants to enter as they type, with the aim of reducing query entry time and potentially preparing the search results in advance of query submission. There are a large number of approaches to automatically rank candidate queries for the purposes of auto-completion. However, no study exists that compares these approaches on a single dataset. Hence, in this paper, we present a com-parison study between current approaches to rank candidate query completions for the user query as it is typed. Using a query-log and document corpus from a commercial medi-cal search engine, we study the performance of 11 candidate query ranking approaches from the literature and analyze where they are effective. We show that the most effective approaches to query auto-completion are largely dependent on the number of characters that the user has typed so far, with the most effective approach differing for short and long prefixes. Moreover, we show that if personalized information is available about the searcher, this additional information can be used to more effectively rank query candidate com-pletions, regardless of the prefix length.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: Query Completion, Information Retrieval
Query auto-completion is a relatively recent functional-ity offered by search engines, which, given a prefix already typed by a user, tries to guess the final query the user is going to type. Using this functionality, the user does not need to type the whole query and in some cases the auto-completion suggestions may help the user formulate a more precise query. From an information retrieval perspective, this can be seen as a ranking task, where the aim is to rank all possible candidate (complete) queries given the current prefix typed by the user. The top ranked candidate queries will then be displayed as the user types. Figure 1 illustrates c  X  Figure 1: Illustration of query auto-completion for the query  X  X nformation retrieval X . query auto-completion as implemented in a current search engine for the query  X  X nformation retrieval X .
 Importantly, query auto-completion is a challenging task. In particular, given only a few characters that can be quite ambiguous, there are potentially thousands of candidate queries that the user might type. To tackle this challenge, a vari-ety of approaches have been proposed [1, 7, 8, 11]. Early works examined probability-based ranking, where queries are ranked by their occurrence probability within a back-ground corpus [2]. Meanwhile, other studies have focused on leveraging evidence from query logs to rank the candi-date queries. For instance, Bar-Yossef and Kraus [1] ranked the candidate queries by textual similarity to a user profile, while Shokouhi [8] leveraged user history and demographic features within a learning-to-rank model to rank the candi-date queries. However, of important note is that these prior studies typically report performance using different propri-etary datasets (e.g. for privacy reasons) and often do not share any common baselines. Hence, it is difficult to com-pare the wealth of approaches to query auto-completion to see which is the most effective, and, as a result, it is unclear what the state-of-the-art in query autocompletion is.
As a step toward tackling this knowledge gap, in this pa-per, we perform a comparison of 11 different approaches to rank query auto-completions, with the aim of determin-ing those that are the most effective. In particular, using the same dataset comprised of medical articles and asso-ciated query/click logs from a commercial medical search engine, we perform a comparison of query auto-completion approaches taken from and inspired by the literature, cover-ing both query-log independent approaches and personalized approaches that are primarily driven by query logs. From analysis of the results, we show that in general, approaches that personalize to the specific user outperform approaches that either do not use query-log evidence at all, or only con-sider past queries across multiple users in aggregate. Fur-thermore, we show that there is a dependence between the number of characters that the user has typed and the per-formance of the approaches tested -with the most effective approach changing based on the prefix length.
A query auto-completion engine takes as input a prefix p of length l typed by the user in the search engine box and a set of candidate queries Q p to rank. A prefix p is a sequence of characters belonging to the query the user is about to submit. The candidate queries Q p are a set of pos-sible completions for the current prefix. These should be of high quality and represent the space of possible information needs that the user could have. The aim is to rank the can-didate queries Q p for a given prefix p , such that the query the user was typing appears within the top ranks.

Importantly, for the purposes of this paper, we assume that the set of candidate queries for each prefix are pre-provided. In this way, we can compare different query rank-ing strategies over the same set of candidate queries. We leave an examination of different approaches to generate these candidate queries to future work.
In this section, we summarize and formally define the 11 different candidate query ranking approaches that we com-pare in our subsequent experiments. In particular, we for-malize each approach as a scoring function score ( q ), where q  X  Q p . Note that we assume that we have access to a query-log containing past queries Q 0 and document clicks C 0 from which we can extract training evidence. We also naturally assume that we have access to the collection of documents D that the user is searching. Finally, for personalized ap-proaches, we refer to all queries previously entered by the current user as the user context .
 Most popular ranker (MP) is the ranking algorithm used as a baseline in the majority of the works present in the liter-ature. Query candidates are ranked according to their past popularity, calculated as the query X  X  frequency ( cf ) inside the query log. Usually, this frequency is normalized by the total sum of query frequencies. Notably, there are differ-ent variants of this approach. Shokouhi and Radinsky [9] replaced the query X  X  actual frequency with a predicted fre-quency, while Strizhevskaya et al. [10] model the query fre-quencies using a time-series. We use the actual query fre-quency within the query-log.
 Sentence occurrence ranker (SO) scores each query based upon the number of documents that match one or more of the query terms plus the number of documents that match all of the query terms. SO is inspired by [2].
 SO ( q ) =  X   X | exactMatch ( q,D ) | + | termMatch ( q,D ) |  X  &gt; 1 Time ranker (TR) is a simple ranking algorithm that scores each candidate completion q by the time difference between the current time and the most recent past occur-rence of it in the query log Q 0 , thereby promoting queries that have been issued recently. LRD is the largest time difference between q and any query in Q 0 .
 Most popular time ranker (MT) mixes most popular ranker and time ranker together, inspired by the hybrid ap-proach in [1]. In our work, we set  X  to 0.7.
 Term occurrence ranker (TO) ranks candidate queries based on the background popularity of those candidate X  X  terms. Term popularity is calculated as the mean of the frequency of the term inside the query log and the TF-IDF of the term within the corpus of documents being searched. The score for a candidate query is the mean of the scores for its terms.
 Near words (NW) takes into consideration the distance among the terms typed by the user in the candidate comple-tions. It promotes completions where the query terms are next to one another. The frequency difference (fd) among the completions having the highest and lowest frequency in Q p is calculated and divided by the number of terms present in the longest query, obtaining the term difference (td) . For each pair of user terms a value is assigned and summed to the overall score. That value is equal to the fd if one term directly follows the other and becomes lower as the distance increases. The td is subtracted from the fd for each term between the user terms pair. T u in Equation (6) is defined as the set of terms typed by the user.
 String similarity ranker (SS) scores a query based upon the similarity between that query and all previous queries issued by the user who submitted that query. The simi-larity measure used in this paper is the Jaro Winkler edit distance [13].
 WordNet similarity ranker (WR) uses WordNet [3] in order to capture semantic similarities between the query be-ing ranked and the previous queries entered by the same user. The similarity is calculated for every pair of terms present in the completion being ranked and the user con-text. Mean similarity over the user X  X  previous queries is used for scoring the completion. We use the Wu &amp; Palmer [12] measure that calculates the relatedness of two terms by considering the depths of the two related synsets in the WordNet taxonomies.
 N-Gram similarity ranker (NR) scores a candidate query based upon the syntactic relation between it and the user context. In particular, we use the N-Gram similarity algo-rithm presented in [4]. We chose to use the positional n-gram similarity as the n-gram matching strategy.
 Kernel similarity ranker (KR) is based on the work pre-sented in [5] and [6] that aims to estimate the similarity be-tween two small strings by taking into account their seman-tic similarities. In particular, collection enrichment using the document corpus is applied to each candidate query and the user context. A candidate query is scored based upon the similarity between its expanded form and the expanded form of the user context (using the same kernel similarity measure as in [6]).
 Clicked documents ranker (CR) models the user X  X  in-terests as the content of documents previously clicked ( D where this set is represented by the terms present inside the document titles. The candidate query is represented in the same way, but considering all of the documents previ-ously clicked for that candidate query in the query log ( D The cosine similarity measure is used to score the candidate query representation by its similarity to the representation of the user X  X  interests.

CR ( q ) = cosine ( q d ,c d ) q d = { t | t  X  d title ,d  X  D
The first three columns of Table 1 list the 11 approaches discussed above, highlighting whether each uses query-log information and/or the user context (e.g. queries previously entered by the user for personalization). Dataset: To evaluate query auto-completion, we use a sam-ple of 1,417,880 unique queries issued by 37,806 different users between November 2010 to April 2013, provided by the TRIPDatabase.com medical search engine. 1 This dataset additionally contains 1,418,996 medical articles. Note that TRIP has basic in-built query suggestion based only on the corpus statistics, similar to SO approach.
 Training/Testing Split: To facilitate experimentation, we divide the dataset into training and testing splits. In partic-ular, all queries issued before April 2013 are used for training and extracting the user context information (past queries and clicks). For testing, we use a sub-set of the query sam-ple, composed of 1405 queries by 968 users who had used TRIP for at least 3 months since March 2013 and had sub-mitted at least one query during the month of April 2013. Candidate Query Generation: To produce the candi-date query set for a prefix, we use the query log from the TRIP medical search engine. Given a prefix, we first use a weighted compressed ternary search tree ( WCT ) that re-turns the first k most likely completions for the last term in the current prefix. 2 For instance, the prefix  X  X iab X  might re-turn  X  X iabetes X  and  X  X iabetic X  as term completions. We then http://tripdatabase.com/
Often the prefix will be only a single incomplete term, e.g.  X  X eig X  but for longer prefixes it may be a second term that is incomplete, such as  X  X eight lo X . use these terms to retrieve all queries, from the aforemen-tioned search engine query-log, matching them and all the other terms already fully typed by the user.
 Evaluation Metrics: As a ranking task, we evaluate query auto-completion performance in terms of the mean recipro-cal rank (MRR) of the candidate query ranking produced by each approach over all of the test queries. To do so, we crop each query to the first k characters ( k  X  X  2 , 4 , 6 , 8 and 10 } ), representing the user at different stages of typing that query. Only the final query that the user issued is consid-ered the correct completion. We report MRR for all queries when concatenated to each of the target lengths.
In this section, we aim to answer the research question,  X  X Q: which query auto-completion ranking approaches are the most effective? X . To do so, we compare the performance of each query auto-completion ranking approach described in Section 3. In particular, Table 1 reports the MRR perfor-mance of each of these approaches when ranking candidate query (completions) for prefixes of lengths { 2,4,6,8,10 } .
From Table 1, we observe the following. First, as a sanity check, comparing the  X  X entence occurrence ranker X  (SO) that does not use any query-log evidence to the other approaches that leverage query logs, we see that the query-log-based approaches all outperform the  X  X entence occurrence ranker X  (SO) for all prefix lengths. This confirms our expectations that approaches that rely on corpus statistics [2] are outper-formed by query-log-based approaches [8].

Second, comparing the approaches that use query-log evi-dence without considering the current user in particular, we see that the most effective of these approaches when the user first starts typing (prefix length of 2), is the  X  X ost popular ranker X  (MP) approach. Recall that this approach simply chooses the most frequently occurring candidate query that starts with the target prefix. However, we also see that as the user continues to type (prefix lengths 4-10) the approach that combines query popularity with recency, i.e. the  X  X ost popular time ranker (MT) becomes incrementally more ef-fective. To explain this behaviour, it is first important to note that for all approaches, as the prefix length increases, the ranking performance of all approaches also increases. This is to be expected, since as the prefix becomes longer, there are less possible candidate queries that match the pre-fix. For a very short prefix, considering the candidate re-cency can be missleading, since there may be hundreds of candidate queries that were issued close to the current search time, but that are not likely to be good query candidates. Hence, the recency factor dilutes the more effective query frequency signal. However, as the candidate query set be-comes smaller (due to a longer prefix length), the recency signal becomes more useful, as it down-weights candidate queries that have not been issued for months or years, and hence are not likely candidates.

Third, comparing the approaches that use the query-log evidence without considering the current user (MP,TR,MT, TO, and NW) to those approaches that additionally employ the query log to personalize to the current user (WR,NR, KR, CR and SS), we see that, in general, personalized ap-proaches outperform unpersonalized approaches. For exam-ple, at prefix length 2, the approach that personalizes based on clicked documents ( X  X licked document ranker X  (CR)) out-Approaches MRR Ranking Query-log Personalized 2 4 6 8 10 Most Popular ranker (MP) Yes No 0.0964 0.2146 0.2851 0.3248 0.3641 Time Ranker (TR) Yes No 0.0324 H 0.1236 H 0.1995 H 0.2707 H 0.3281 H Near Words Ranker (NW) Yes No 0.0611 H 0.1576 H 0.2347 H 0.2972 H 0.3611 performs the query frequency-based ranker discussed earlier ( X  X ost popular ranker X  (MP)) by a large and statistically significant margin (0.0964 MAP to 0.1442 MAP). This indi-cates that personalization to each individual user is prefer-able. 3 Next, comparing approaches within the class of per-sonalized approaches, we see that for small prefix lengths (2-4) ranking the candidate queries via similarity to pre-viously clicked documents by the user ( X  X licked documents ranker X  (CR)) is the most effective. However, as the pre-fix length increases (6-10), the n-gram similarity approach that compares against the past queries issued by the user ( X  X -gram similarity ranker X  (NR)) becomes more effective. These results indicate that for short prefix lengths, where a user has likely issued multiple queries that are valid can-didates previously, using past queries is less effective as it is difficult to break ties between those candidates. On the other hand, as the prefix length increases, there is often only one previous candidate query that the user has entered that matches, which is likely to be the correct candidate (e.g. if the user is re-finding).

To answer our research question RQ, on our dataset there is no single approach that illustrates a top performance for all query lengths. Rather, the correct candidate query rank-ing approach is dependent on how many characters the user has typed so far. For short prefix lengths, simple past query frequency (MP ranker) is a good feature if no personaliza-tion information is available, while ranking based on the content of previously clicked documents is effective other-wise (CR ranker). For longer prefix lengths, mixing query candidate recency with its past usage frequency can result in performance gains (MT ranker) in the unpersonalized set-ting. Meanwhile. if personalized information is available to the search system, then ranking query suggestions by n-gram similarity to the users past queries is more effective (NR ranker).
In this paper, we presented a comparison study of current approaches to rank candidate query completions for the user query as they type. Using a query-log and document corpus from the TRIP medical search engine, we evaluated the per-formance of 11 candidate query ranking approaches from the
Note that this information may not be available if users are searching anonymously however. literature and analysed where they are effective. From this analysis, we conclude that the most effective approach to query auto-completion is largely dependent on the number of characters that the user has typed so far and that per-sonalized information can be used to more effectively rank the query candidate completions. This indicates selective approaches that apply different query-autocompletion mod-els depending on the prefix length and user context are a promising direction for future work.
We thank Jon Brassey from TRIPDatabase.com for pro-viding the data used in this research.
