 Let X =( x 1 ,x 2 , ...x n ) T be a sample of size n from an unknown distribution F with density function f , the problem of density estimation is to construct a estimator  X  f from X to approximate f as good as possible. Traditionally, the methods for density estimation are generally divided into two categories, para-metric methods and non-parametric ones. Finite mixture models [ 1 ] have been used for constructing parametric probabilistic models successfully, but they suf-fer from choosing the appropriate number of mixture components and are sen-sitive to initialization. The traditional nonparametric kernel density estimator (Parazen Window) [ 2 ] is guaranteed to converge to the underlying density under practical assumptions without worrying about the magic number k (every data-point is itself a component) [ 3 ]. However, it is not easy to choose an appropriate bandwidth parameter to achieve a good performance. A lot of researches have been made to find a data-driven criterion to search for a good value of band-width parameter [ 4 ], [ 5 ], [ 6 ]. In addition, the non-parametric methods and the bandwidth selection algorithms generally need to store all the training data in memory which is unfeasible in many cases.
 There have been several attempts to address the problems of parametric and nonparametric methods. RSDE [ 7 ] reduces the computational cost of full sample KDE through a sparsity induced optimization process. [ 8 ] presents an adaptive kernel density estimator based on linear diffusion process and achieves satisfac-tory performance. In [ 9 ], the author proposes a greedy algorithm for learning a gaussian mixture model, which starts with a single component and adds com-ponents sequentially until a maximum number k . Due to the global and local search procedure, this algorithm need to keep all training data around and is not suitable for online learning. [ 10 ] uses a user defined likelihood based thresh-old parameter to add new gaussian components for the purposed of incremental learning of gaussian mixture models. However, its learning strategy involves all the components in the current model for every input sample. That makes the model converges slowly and tends to over-smooth the density in the context of online learning. SOMN [ 11 ] adopts Self-Organizing Map as its structure and pro-poses a learning algorithm that minimizes the Kullback-Leibler distance between the estimator and the objective density function, the learning process is limited within a small number of nodes around the input data to accelerate the con-vergence of nodes. The problem of SOMN is the same as SOM, that is, it is difficult to specify a network topology in advance. oKDE [ 12 ] combines the mix-ture model and the KDE to realize online multivariate density estimation, it maintains and updates a mixture model of the observed data from which the KDE can be calculated, compression and revitalization procedures are executed regularly to balance the accuracy and model complexity. The final estimator is defined as a convolution of the sample distribution by a kernel. This convolution strategy makes oKDE easy to over-smooth the underlying density.
 we propose an incremental and local adaptive gaussian mixture which estimates object density function in an online way by maximizing the sample likelihood locally around each mixture component. Unlike the SOMN, LAIM need not to specify the network structure in advance. Using a similarity threshold based criterion, the method is able to allocate components incrementally to accommo-date novel data-points without destroying previously learned components. We also adopt a density based denoising algorithm that make the model more robust to noise. For density estimation, the LAIM is the same as traditional gaussian mix-ture model. Every gaussian component of LAIM could be summarized by three parameters: the mean vector  X  , covariance matrix  X  and n the effective number of data-points it possesses. We introduces n here for the purpose of extending the maximum likelihood estimation for single gaussian to a local adaptive learning strategy(see Section 2.2).
 where w i = n i and covariance matrix of the i th component.  X  ( x |  X ,  X  ) is the gaussian density function with mean  X  and covariance matrix  X  .
 The LAIM has three key steps: 1. Component Allocation . Construct a neighborhood set for input data-point and decide whether it is necessary to insert a new gaussian component into the current model. 2. Local Adaptive Learning . Update the parameters of the components in the neighborhood set based on maximum likelihood principle. 3. Denoising . Eliminate the components induced by the noisy data.
 We now give the details of each step. 2.1 Component Allocation Suppose we have built a mixture model for a series of data x where  X  ( x |  X  i ) is the gaussian density function, w i component i and  X  =(  X  i , X  2 ,..., X  K ) T is the parameter matrix for the mixture model. When the new data point x t is available, traditional EM-based algorithm would make a global adaption for all the components in the current model. This operation is guaranteed to increased the sample likelihood in the long run, however, it could also destroy the previously learned structures and trapped into local optimum if x t and following inputs are somewhat novel to the current mixture model. It is also possible that x t is just a noise that should not be learned.
 Therefore, to fit the novel data without destroying the old model, we need to decide when to allocate a new component. If we could keep all the historical samples at hand, it is possible to make choice based on the sample likelihood or some model selection criterion. In the context of online learning, we must rely on the current learned model and make choices locally. To measure the novelty of the new coming data point x t , we first evaluate its distance D ( x components around it weighted by its covariance matrix here i =1 , 2 ,...,K t , K t is the number of gaussian components at time t . Then we construct the neighbourhood set S t of x t S contains the set of components in the current model that should most respon-sible for x t , their parameters will be updated to fit x similarity threshold for component i , it controls the tendency of the correspond-ing component to absorb an input sample nearby, therefore the smaller the T the more local the algorithm will be. We know that D 2 ( x mixture model is indeed well fitted, we could just let T i where q is the confidence level, in practice we just set it with 0.9. However, we can X  X  make the assumption that the previously learned model is reliable due to the context of online learning, the initialized components need enough samples to converge to a state of well fitted. Therefore, we let T T , here  X  is a user defined parameter. To let the new components converge fast, we usually set it with value ranged 1.5 to 2.0 at insertion and decreases to 1 through the training procedure.
 deserves a new component to fit it. The initialization of the new component is as follows: where h is a user defined parameter, I is the identity matrix. h serves as a initial bandwidth for a new component, it should be relatively small compared to the actual standard deviation along each dimension in order to keep the locality sensitivity of LAIM. 2.2 Local Adaptive Learning Once the neighborhood set S t is determined for the input data at time t ,we limit the learning process within S t . The local learning strategy does not only accelerate the learning process, but also gives the LAIM the ability to fit new data without destroying the learned components far away from the current i nput. This property is essential for online learning due to the locality of the information, we could never have the global information about the whole training set, only the current model and current input are available. Doing things locally is a safe strategy so that we can handle the non-stationary input stream.
 a single gaussian density function [ 13 ] It is easy to see the learning step here is 1 /n , where n is the current number of samples. We want the learning step of each component could be different accord-ing to the current model and the learning process within the neighborhood region could be accelerate further since the members of the set are supposed to generate the current sample with high probability. Therefore, for every component i we make the following updates: r n  X 
 X  The quantity r i evaluates the responsibility of component i to the current data x , it distributed the effective number of observed samples to each component in S respectively weighted by their responsibilies. We also use it to adapt the updating stepsize for each component i  X  S t . Notice that when multiple com-ponents exist in set S t , this quantity would slow the process of the learning by shrinking the learning stepsize. In the case that S t has only one element, the above updating rules degenerate to the original maximum likelihood estimation for single gaussian component naturally. 2.3 Denoising For density estimation in practice, it is common that the training data are contam-inated by noise. With the assumption that the noisy data are mostly distributed over the regions where the objective density function f has low probability density and their distribution is sparse enough so that the main structure of f could still be discovered, we adopt a denoising scheme based on the effective numbers of each According to the insertion rule described by Section 2.1, those noisy data would lead to node insertion with high probability. However, the resulted components should non-noise components. Let be the mean effective number of the current mixture model, where K is the current number of components. We eliminates those components whose effective number is lower than some constant  X   X  [0 , 1] times M after every  X  input samples. Here  X  and  X  are user defined parameters, large value of  X  and small values of  X  should be set if the amount of noise is large. 2.4 Complete Algorithm As a summary, we give the complete algorithm of LAIM here.
 Algorithm 1.. Local Adaptive and Incremental Gaussian Mixture 3.1 Artificial Data-Sets The artificial density functions used here are the same to those in [ 8 ]. We first adopt the common used bimodal density to verify the effectiveness of our method sity estimator(kernel density estimation via diffusion, KDE-d for simple)[ 8 ]and gaussian mixture models with 2 components (the optimal choice) trained by batch EM algorithm. The parameters are set as follows: oKDE( D LAIM( h =0 . 5 , X  =1 . 5,  X  =  X  = inf). 3000 samples are drawn from (11) as training set, the resulted estimators are shown in fig. 1. We can see from fig. 1(a) and fig. 1(d) that our method and batch EM reconstruct the underlying density function almost perfectly. That is reasonable since the assumption of mixture of gaussian is perfect for (11), but LAIM doesn X  X  need to specify the number of components due to the incremental nature of the algorithm. From fig. 1(b) ,we can see that oKDE fits the right hand gaussian pretty well but over-smoothes the left hand gaussian component, that X  X  mainly because its estimation is based on a global convolution operation that lacks of locality sensitivity. fig. 1(c) also shows some under-smoothness on the right hand gaussian. This result shows that LAIM achieves the comparable performance to the batch EM algorithm in this simple bimodal situation.
 The parameters are set as follows: oKDE( D th =0 . 1), LAIM( h =0 . 5 , X  =1 . 5,  X  =  X  = inf), the number of components for GMM is identical to LAIM, which is 10 in this case. density function in each local region hence gives a reliable estimation. Batch EM algorithm fails to capture the whole structure of (12). Fig . 2(b) is also a result of over-smoothing the sample distribution constructed by oKDE.
 To quantify the approximation performance of our method, we did the numer-ical experiments on five artificial data sets (including (10), (11)). The criterion for the comparison is the numerical approximation to the following ratio, which was adopted by KDE-d [ 8 ]. Here  X  g is the estimator with which we want to compare,  X  f is our estimator and f is the underlying density function. (11) is the integrated squared error of the diffusion estimator to the integrated squared error of the alternative estimator. The results are shown in table . 1 . When compared to online method oKDE, our approach has lower integrated squared error, which means the corresponding estimator is more accurate. The proposed method outperformes the batch method KDE-d in case 1 and 3, that is reasonable since the density in those two cases are well-separated gaussians that are more suitable for the mixture models. In case 2 where the density function contains complex local structure, LAIM achieves comparable result to batch algorithm KDE-d. 3.2 Real Data-Sets We compared our method with oKDE, RSDE and KDE with bandwidth selected by cross validation(CV) on the real datasets obtained from the UCI Machine WineRed and WineWhite. For the density estimation, we estimated the density for each class separately. The data were randomly reordered, 75% of the data in each class were used for training, and the rest for testing. We conduct the same experiment twelve times and recorded the mean and standard deviation as the result. The oKDE were initialized by the first 10 samples and parame-ter D th was set to 0.1. Two parameters of LAIM are set as follows:  X  =2 . 2,  X  =0 . 05,  X  =0 . 1 N , where N is the total number of training samples. h is set according to the the scale of the data because it will affect the the complex-ity of the model: Iris(0.01), Pima(15), Wine(15), Winered(15), Winewhite(5). To measure the quality of estimation, we have computed the average negative likelihood(NLL) per test point, lower NLL generally suggests more accurate esti-mation.
 The results of the experiments after observing all the data-points are summa-rized in Table 2 . Compared to the online method oKDE, the proposed method achieves better results on all the data-sets. LAIM also ourperforms the batch methods on Iris, Pima and WineWhite.
 previous five data-sets. Although density estimator is not generally the most accurate classifier, the classification results based on simple Bayesian criterion still reflects the quality of density estimation. We have chosen a multiclass SVM with RBF kernel[ 17 ] as the baseline classifer and compares our method with oKDE, RSDE, and KDE with cross validation. The results of classification are summarized in Table 3 . From the table, we can see that the proposed method outperformes the online counterparts in most data-sets except for Wine and produces comparable results to the batch methods. Noticed that in the context of online learning, we don X  X  store any historical data but the current input and the learned model, therefore, the time complexity and space complexity of LAIM are much smaller than the batch methods. In this paper, the incremental and local adaptive gaussian mixture for online den-sity estimation(LAIM) is proposed. With the similarity threshold, the method could allocate components incrementally while training without specifying the number of gaussian components in advance. We proposes a local learning algo-rithm for updating the parameters of mixture model based on maximum like-lihood principle, this locality sensitivity enables the our model to discover the local density structure of the data samples in the context of online learning. A denoising scheme is used to eliminate the components initialized by noise. Experiments show that it outperforms the compared online density estimators and produces comparable results to the compared batch methods while keeping a lower model complexity.
 Acknowledgments. The authors are very grateful to Youlu Xing for many useful discussions and programming assistance. We also acknowledge Tao Zhu and Haoran Xu for helpful comments. This work is supported in part by the National Science Foundation of China under Grant Nos. (61375064, 61373001) and Jiangsu NSF grant (BK20131279).

