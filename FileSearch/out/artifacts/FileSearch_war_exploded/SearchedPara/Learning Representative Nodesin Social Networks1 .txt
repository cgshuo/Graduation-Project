
Ke Sun 1 , Donn Morrison 2 , Eric Bruno 3 , and St  X  ephane Marchand-Maillet 1 Inasocialnetwork,asmallsubsetof representative nodes can help establish a hierarchical messaging scheme: the correspondence with each individual node is through a nearby representative . Despite that the word  X  X epresentative X  can be interpreted in different ways in social analysis, here, the purpose of such a hier-archy is to broadcast information efficiently with constrained resources. Locally, these representatives should lie in hub positions so as to minimize the rout-ing cost to their nearby nodes. Globally, there should be as few representatives governing different regions so as to save resources.

From a machine learning perspective, a closely related problem is spectral clustering [1 X 3], where the network is partitioned into a fixed number of densely-connected sub-networks with sparser co nnections between them. This technique has been applied to social networks, e.g. , for community detection [4, 5] and spam nodes identification [6]. It is powerful in depicting complex clusters with simple implementations. It is computationally expensive for large datasets, especially when a proper number of clusters has to be searched over [4].

In the data mining community, the graph-based ranking algorithms [7 X 9] have a profound impact on the present World Wide Web and citation analysis systems. They rank graph nodes based on the general idea that the value of one node is positively related to the value of its neighbours. These approaches are further investigated by machine lea rning researchers using spectral graph theory [10, 11] and random walks [12]. In our task of selecting representatives, the highly ranked nodes by these algorithms usually have high neighbour overlap because of the mutual reinforcement between connected high degree nodes.

Motivated by seeking effective marketi ng strategies, efforts have been made to select a set of influential individuals [13 X 15] and to maximize their influence through information diffusion [16, 17]. Although the optimization problem is generally NP hard [13], reasonable assumptions lead to polynomial-time solv-able models [14] and efficient implementations with approximation bounds [15]. Targeting at similar objectives, methods from different perspectives are devel-oped with improved speed and performance [18, 19].

This work provides a novel approach to m easure the representativeness of graph nodes based on skeleton learning (SKE) [20]. It assigns each node a prob-ability of being a representative and minimizes the communication cost from a random node to its corresponding representative. This method is different from other approaches in two aspects. First, the learned distribution has low entropy with the representative nodes having large probability and the non-representative nodes having probability close to zero. Second, the representative nodes are mutually exclusive : if a node already has a nearby representative, it will penalize the representativeness of other nearby candidates. Such exclusiveness is not implemented as heuristics in a greedy manner [18], but fits in a minimiz-ing message length framework and allows global coordination in arranging the representatives.

The rest of this paper is outlined as follows. Section 2 introduces the skeleton learning. Section 3 presen ts the recent development of this approach on social network analysis. Section 4 and Section 5 show the experimental results on toy datasets and real social networks, resp ectively. Finally, Section 6 concludes. This section briefly reviews the recently p roposed skeleton learning [20]. Given a set of samples X = { x i } n i =1  X  D , this unsupervised method learns a probability  X  i for each on the  X  X keleton X  of the structures and diminishes on outliers.

The input samples are first encoded into a probability matrix P n  X  n =( p i | j ) as in Stochastic Neighbour Embedding [21], so that denotes the probability of node i receiving a message originated from node j with respect to space adjacency. In Eq.(1), || X || is 2-norm, and h j &gt; 0isakernel width parameter, which can be fixed so that the entropy of p  X | j equalstoapre-specified constant [21]. The latent distribution  X  =(  X  1 ,..., X  n ) corresponds to a discrete random variable, or the index j  X  X  1 , 2 ,...,n } of a random skeleton point x j  X  X  . By assumption, this random point sends out a message with respect to P n  X  n .Any x i  X  X  , upon receiving such a message, can infer the location of the skeleton point using Bayes X  rule as The objective is to optimally route from a random location in X to its skeleton point, which is implemented by minimizing with respect to  X  . Through such minimization, a compact set of skeleton po-sitions with large  X  i can be learned. As compared t o clustering methods, the skeleton model is a prior distribution defined on the observations, and the effec-tive number of skeleton points shrinks con tinuously during learning. Therefore no model selection is necessary to determine an appropriate number of clusters, and the learning process can be terminated at anytime to produce reasonable results. However, the effect of the kernel width parameter h j must be carefully investigated depending on application. In image denoising [20], it shows better performance in preserving the manifold structure as compared to a state-of-the-art denoising approach [22]. The gradient-based algorithm has a complexity of O ( n 2 ) at each step, which limits its scalability. The skeleton learning method introduced in Section 2 is performed on a set of coordinates for denoising and outlier detection. This section extends the idea to graph datasets and discusses related problems. Assume the input data is a graph G =( V , E ), where V = { 1 , 2 ,...,n } is the set of vertices and E = { ( i,j ) } is the set of edges. Throughout this paper, an undirected graph is treated as its directed version by replacing each edge i  X  j with two opposite arcs ( i,j )and ( j,i ). We aim to discover a random representative characterized by a discrete distribution  X  =(  X  1 ,..., X  n ) defined on V , so that any random node in V can communicate with it in the most e fficient and economical way.

We first construct a channel between two random nodes so that the communi-cation cost can be measured. The input graph G can be equivalently represented by its normalized adjacency matrix A =( a ij ) with where d i is the outdegree of the node i . If a node i has at least one outgoing link, the i  X  X h row of A defines a discrete distribution representing how likely i influences the other nodes according to the graph structure. To deal with nodes with no outgoing links or incoming links, we allow each node i to teleport to another random node with a small probability  X  . In social networks, such teleportation models i  X  X  influence through external ways not restricted by the network [7, 23]. Consider i sending a message to one unique receiver other than i at time 0. The probability that node j receives this message in one time step can be defined as p j | i =(1  X   X  ) a ij +  X / ( n  X  1) ( j = i ) . In matrix form it is equivalently where e =(1 ,..., 1) T and I is the identity matrix. If we allow this message to pass around in G for  X  times (  X  =1 , 2 ,... ) after time 0, the probability for each node j holding the message is given by the i  X  X h row of the matrix P  X  .It represents i  X  X  indirect influence over G through information spreading. In the extreme case when  X   X  X  X  ,allrowsof P  X  will tend to be the same, or the equilibrium distribution corresponding to the PageRank (PR) measure [7]. To distinguish the  X  X utgoing ability X  of different nodes,  X  should be a small value (e.g., 1 or 2) so that each node can only reach a local region around itself. Without loss of generality, we focus on the case  X  = 1 unless otherwise specified.
Assume a latent prior distribution  X  of each node being the information source. The sender i , upon any node j receiving a message, can be identified with Bayesian inference in Eq.(2) (with i and j interchanged). The total com-munication cost for every node j  X  V to reply to its information source is given by Eq.(3). By minimizing such a cost,  X  can be learned so that this communi-cation loop is established in the optimal way.

More intuitively, consider without loss of generality the graph G as a social network of n persons. A directed link ( i,j )  X  X  means that i could easily in-fluence j because of personal relationship, etc. One real-life example could be j  X  X ollows X  i on some microblogging website. In this context, the meaning of being a representative can be understood from Eq.(3). To minimize E (  X  ), on average  X  log p j | i should be small, which means the representative j can perceive news from its surrounding nodes easily. As another condition, q  X | i should have low entropy, which means each person i selects the candidate which influences i the most, and deselects other nearby candidates being its representative. Implementation The skeleton learning is implemented by gradient descent to minimize E (  X  ). The gradient of E (  X  ) has the form [20] Algorithm 1. Skeleton Learning on Graph Datasets Along  X   X  X / X  X  j , the candidate weight  X  j is adjusted at each step. Intuitively Eq.(6) says, for each node i within j  X  X  reachable range, j serves as a potential information source of i (the value q j | i, X  is significant enough based on Eq.(2)), and such i provides feedback to  X  j based on how efficiently it can reach back to then  X  X / X  X  j &lt; 0and  X  j increases, which means that i  X  X otes X  for j to become its representative. On the other hand, if the route i  X  j is too costly, i casts a negative vote for j . This type of gradient was discussed in a statistical machine learning framework [24] and further explored here in a non-parametric setting.
In general, each gradient descent step requires O ( |V| 2 ) computation [20] be-cause P is dense. However, the fact that most entries of the transition matrix P equal  X / ( n  X  1) can lead to more efficient implementations. On graph datasets, the gradient in Eq.(6) is further written as  X  X  In Algorithm 1, the simple gradient descent has a computational complexity of O ( |E| ) in each iteration. The stochastic gradient descent (SGD) [25] version reduces this computation time to O (max( d i )  X |S| ). Besides the learning rate, the algorithm has only one parameter  X  . By default we set  X  =0 . 2inthe following experiments. On real large social networks, the node degrees follow an exponential distribution, which may lead to trivial solutions if  X  is too large. For example, one node with significant number of links could become the sole representative over the whole network and communicate with the unconnected nodes through teleport. In this case we have to lower the value of  X  to penalize the teleport communication and to discover more representatives. Figure 1 presents several toy social networks. Table 1 shows the  X  i value and the PageRank value of each node. In Figure 1(a), only one person (node 1) is acquainted to all the others. SKE has successfully identified it as the sole representative. Figure 1(b) shows two groups of people, each with a central hub (node 1 and node 5), and a link from node 1 to node 5. The SKE values are very concentrated on these two centers, w ith node 5 having slightly larger weight due to the fact that no edge exists from node 5 to node 1. This type of penalty becomes clearer in the network shown in F igure 1(c). In this example, each node has exactly the same indegree. There is one node 3 which links to all the other nodes, while half of the linked nodes do not respond. Its  X  i is close to zero, meaning that it has been identified as a spam node. In general, the PageRank values are less concentrated and do not reveal such information.
 The proposed method is further tested on two  X  X rimary School Cumulative Networks X  [26] 1 , where the nodes represent st udents or teachers and the edges represent their face-to-face i nteractions. We only consider strong interactions , which are defined as all such edges ( A,B )if A and B has interacted for at least 2 minutes and on at least 2 occasions. As a result, there are 236 nodes and 1954 edges in network 1, and there are 238 nodes and 2176 edges in network 2.
Figure 2 shows the visualization of the network on day 1, where the SKE value  X  i is intuitively presented by the size of the corresponding circle and the node degree is presented by the color den sity. We see that the representatives (large circles) do not necessarily have hi gh degrees (dense color), and vice versa. This is further confirmed by looking at the accurate measurements given in Table 2. Among the top ranked nodes, some have small degrees, such as node  X 1843 X  in day 1, node  X 1521 X  and  X 1880 X  in day 2. We further look at the average degree of their neighbours ( D n ). All these three nodes have a relative small value of D n , which means some of their neighbours are poorly-connected. The connections with these non-so-popular nodes are highly valued in the SKE measurement. On the other hand, among the bottom ranked nodes, some have large degrees, such as node  X 1628 X  and node  X 1428 X  in day 1, node  X 1766 X  and  X 1778 X  in day 2. Generally they have a relative large D n value. Although they are well-connected, their relationships are mostly established with popular nodes, and thus have little value. We also see that the SKE measure has a  X  X harper X  distribution as compared to PageRank, where the tail nodes have very small values. The effective number of skeleton points (given by exp { X  i  X  i log  X  i } , or the number of uniformly distributed points with the same entropy as  X  )is 95.8 and 91.3 in network 1 and 2, respectively. We test the proposed approach as a seeding method for information diffusion in social networks [15], so that a small subset of seeds (corresponding to the representatives as discussed above) could influence as many nodes as possible. We use the collaboration networks in the Stanford Large Network Dataset Collection 2 [27]. ca-GrQc is a co-authorship network of physics publications, compiled from the General Relativity section of Arxiv. It has 5,242 nodes representing authors and 14,496 edges representing co-authorships. Similarly, ca-HepTh , ca-HepPh and ca-AstroPh are collaboration networks of different domains on Arxiv. Their sizes denoted by #nodes/#edges are 9,877/25,998 , 12,008/118,521 , 18,772/198,110 , respectively. All datasets are undirected and unweighted, which means the accurate number of times that two authors have collaborated is discarded . For each dataset, five diff erent seeding methods are applied, which select seeds based on descending degree, descending PageRank value, descending SKE value (  X  i ) computed by simple gradient descent and stochastic gradient descent, and the degree discount heuristic [18], respectively. The damping factor in PageRank is set to be 0.85 by convention. The teleport probability  X  in SKE is set to be 0.2.

To evaluate the seeding quality, we apply two different diffusion models, namely Independent Cascade Model (ICM) [17] and Linear Threshold Model (LTM) [16], once the selected seeds are marked as being activated .Bothofthese models expand the activated set of nodes in discrete steps with respect to the network structure. In ICM, an activated node v has a single chance to activate a neighbour w with the probability p v,w . By discarding the number of times that two authors have cooperated, diffus ion with ICM becomes harder because v has one link instead of several parallel links connecting with j .InLTM,anode v will be activated once the proportion of activated neighbours reaches a node-specific threshold  X  v . At convergence, the size of the activated set quantifies the influence of the initial seeds. To average the effect of random factors, the experiment for each seeding set is repeated for 100 times.

Figure 3(a-d) shows the size of the activated set varying with the number of seeds on ca-GrQc and ca-HepTh using ICM. Figure 3(g) condenses the re-sults on ca-AstroPh and ca-HepPh using 100 seeds with ICM. Generally SKE and DegreeDiscount outperform PageRa nk, which in turn outperforms Degree. Basically SKE tries to place the minimum number of seeds while guaranteeing the influence coverage over different regions. The good performance of DegreeD-iscount is due to a similar mechanism to penalize clustered seeds [18]. While the two approaches are comparable on ca-GrQc , seeding with SKE is obviously better on ca-HepTh (ICM probability=0.2), ca-AstroPh and ca-HepPh (ICM probability=0.1). Moreover, SKE can adapt to network changes through online learning, which is not straightforward for DegreeDiscount. Note, the SGD ver-sion of SKE has comparable performance with the simple implementation and is about ten times faster. Figure 3(h) shows the results with different values of  X  in the range [0 . 01 , 0 . 3]. We see that the influence coverage has a small variation and thus is not sensitive to this configuration.

As Figure 3(e-f) displays, the performance of SKE falls behind PageRank on the LTM experiments. Such results are expected. LTM, as well as the weighted independent cascade model [15], requires a certain proportion of v  X  X  neighbours to be activated in order for v to be activated. However, SKE places seeds so that they have less overlap and thus is not recommended in similar diffusion models, where more exposure to the spread increases the likelihood of activation. We have extended a recently proposed ske leton learning approach [20] to social network analysis. From an information diffusion perspective, the method aims to identify representative individuals that have greater potential influence over the network. In a minimizing communication cost framework, the gradient-based op-timization naturally allows nodes to cast negative votes to each other in order to derive a set of mutually exclusive candidates. Consequently, the resulting repre-sentatives lie in different regions which helps avoid overlap of neighbour sets. The computational complexity in each optimization step is improved from O ( |V| 2 ) to O ( |E| )( V :nodeset; E : edge set) and is further boosted with stochastic gradi-ent descent. As presented in our experime nts, this approach is able to discover important individuals who have fewer connections and are thus not considered by traditional methods such as PageRank. On real collaboration networks with the independent cascade model [17], the proposed method outperforms the tra-ditional ranking algorithms and the degree discount heuristic [18]. As for future work, we are interested in varying this technique for linear threshold model [16] and exploring other application scenarios such as community finding [5, 28]. Acknowledgements. This work is supported by Swiss National Science Foun-dation (SNSF) via the NCCR (IM)2, the European COST Action on Multilingual and Multifaceted Interactive Informa tion Access (MUMIA) via the Swiss State Secretariat for Education and Research (SER), and Irish CLIQUE Strategic Re-search Cluster (SFI grant no. 08/SRC/I1407).

