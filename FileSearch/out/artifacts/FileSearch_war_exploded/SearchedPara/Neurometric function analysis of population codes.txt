 The relative merits of different population coding schemes have mostly been studied (e.g. [1, 12], for a review see [2]) in the framework of stimulus reconstruction (figure 1a), where the performance of a code is judged on the basis of the mean squared error E [(  X   X   X   X  ) 2 ] . That is, if a stimulus  X  is encoded by a population of N neurons with tuning curves f i , we ask how well, on average, can an estimator reconstruct the true value of the presented stimulus based on the neural responses r , which were generated by the density p ( r |  X  ) . The average reconstruction error can be written as estimator  X   X  . For the sake of analytical tractability, most studies have employed Fisher Information (FI) (e.g. [1, 12]) to bound the conditional error variance Var  X   X  |  X  of an unbiased estimator from below according to the Cramer-Rao bound: For the comparison of different coding schemes, it is important that an estimator exists which can actually attain this lower bound. For short time windows and certain types of tuning functions, this may not always be the case [4]. In particular, it is unclear how different population coding schemes affect the fidelity with which a population of binary neurons can encode a stimulus variable. 1.1 A new approach for the analysis of population coding Here we view the population coding problem from a different perspective: We consider the case of stimulus discrimination in a two alternative forced choice paradigm (2AFC, figure 1b) with equally probable stimuli and compute two natural measures of coding accuracy: (1) the minimal discrimina-tion error E (  X  1 , X  2 ) of an ideal observer classifying a stimulus s based on the response distribution as either being  X  1 or  X  2 and (2) the Jensen-Shannon information I JS between the response distributions  X   X  = argmax s p ( s | r ) where s  X  X   X  1 , X  2 } and the prior distribution p ( s ) = 1 2 . It is given by and the Jensen-Shannon Information [13] is defined as where p ( r ) = P s  X   X  R accuracy since it directly measures the mutual information between the neural responses and the  X  X lass label X , i.e. the stimulus identity. By observing a population response pattern r , the uncertainty (in terms of entropy) about the stimulus is reduced by with prior distribution as above. In the following, we will restrict our analysis to the special case of shift-invariant population codes for angular variables and compute neurometric functions E ( X   X  ) and I JS ( X   X  ) (figure 1c) by setting  X  1 =  X  and  X  2 =  X  +  X   X  . In the limit of large populations, the dependence of these curves on  X  can be ignored. 1.2 Computing E and I JS While the integrals in equation (1) and (2) often cannot be solved, they are relatively easy to evaluate numerically using Monte-Carlo techniques [10]. For the minimal discrimination error, we use where r ( i ) is one of M samples, drawn from the mixture distribution p ( r ) = ( p ( r |  X  ) + p ( r |  X  +  X   X  )) . To approximate I JS , we evaluate each D KL term separately as where we draw samples r ( i ) from p ( r (i) |  X  ) . We use an analogous expression for D
KL [ p ( r |  X  +  X   X  ) k p ( r )] and plug these estimates into equation 2. This scheme provides consis-tent estimates of the desired quantities. For all simulations below we used M = 10 5 samples. In this section, we link the Fisher Information J  X  of a population code p ( r |  X  ) to the minimum dis-crimination error E ( X   X  ) and the Jensen-Shannon Information I JS ( X   X  ) in the 2AFC paradigm. First, we link Fisher Information to Jensen-Shannon information I JS . Second, we bound the minimum dis-crimination error in terms of the Jensen-Shannon information. 2.1 From Fisher Information to Jensen-Shannon Information In order to obtain a relationship between I JS and Fisher Information, we use an expression already derived in [7], where p ( r |  X  +  X   X  ) is expanded up to second order in  X   X  , which yields: Therefore, Fisher Information provides a good approximation of the Jensen-Shannon Information for sufficiently small  X   X  . 2.2 From Jensen-Shannon Information to Minimal Discrimination Error The minimal discrimination error E ( X   X  ) of an ideal observer is bounded from above and below in terms of I JS ( X   X  ) . An upper bound derived by [13] is given by Next, we derive a new lower bound on E , which is tighter than a bound derived by Lin [13]. To this end, we observe that from Fano X  X  inequality [8] it follows that where H [ E ] is the entropy of a Bernoulli distribution with p = E . The equality from first to second line follows as the number of stimuli or classes | s | = 2 . Since the entropy is monotonic in E on the interval [0 , 0 . 5] , we have the lower bound E  X E  X  , where E  X  is chosen such that equality holds. For an illustration, see figure 2a. The shape of both bounds, as well as Lin X  X  lower bound, are illustrated in figure 2b.
 In figure 2c we show the minimal discrimination error for a population code (red) together with the upper and lower bound (black) obtained by inserting I JS ( X   X  ) into equations 4 and 5. Both bounds follow nicely the neurometric function E ( X   X  ) . For comparison, we also show the upper and lower bound obtained by plugging Fisher Information into equation 3 and computing the bounds 4 and 5 based on this approximation of I JS ( X   X  ) (grey). Clearly, the approximation is valid for small  X   X  and becomes successively worse for large ones. 2.3 Previous work Only a small number of studies on neural population coding have used other measures than Fisher Information [18, 3, 6, 4]. Two approaches are most closely related to ours: Snippe and Koenderink [18] and Averbeck and Lee [3] used a measure analogous to the sensitivity index d 0 as a measure of coding accuracy. While Snippe and Koenderink have considered only the limit  X   X   X  0 , Averbeck and Lee evaluated equation 6 for finite  X   X  using  X  = 1 2 ( X   X  +  X   X  + X   X  ) and converted d 0 to a discrimination error E d 0 = 1  X  erf ( d 0 / 2) . This approximation is exact only if the class conditional distribution p ( r |  X  ) is Gaussian with fixed covariance  X   X  =  X  for all  X   X  . In that particular case, the entire neurometric function is fully determined by the Fisher Information [9]: J mean is the linear part of the Fisher Information (cf. equation 7). In the general case, it is not obvious what aspects of the quality of a population code are captured by the above measure. Therefore, both Fisher Information and the class-conditional second-order approximation used by Averbeck and Lee have shortcomings: The latter does not account for information originating from changes in the covariance matrix as is quantified by J cov (cf. equation 7). Fisher Information, on the other hand, can be quite uninformative about the coding accuracy of the population, especially when the tuning functions are highly nonlinear (see figure 3) or noise is large, as in these cases it is not certain whether the Cramer-Rao bound can actually be attained [4]. The examples studied in the next section demonstrate how these shortcomings can be overcome using the minimal discrimination error (equation 1). After describing the population model used in this study, we will illustrate in a simple example, how our proposed framework is more informative than previous approaches. Second, we will investigate how different noise correlations structures impact population coding on different timescales. 3.1 The population model In this section, we describe in detail the population model used in the remainder of the study. To facilitate comparability, we closely follow the model used in a recent study by Josic et al. [12] where applicable. We consider a population of N neurons tuned to orientation, where the firing rate of neuron i follows an average tuning profile f i (  X  ) with (a) a cosine-like shape with k = 1 in section 3.2 and k = 6 in section 3.3 and a (  X  ) = 1 2 (1 + cos(  X  )) or (b) a box-like shape Here,  X  i is the preferred orientation of neuron i and we use j = 12 . We consider two scenarios: Following Josic et al. [12], we model the stimulus-dependent covariance matrix as  X  ij (  X  ) =  X  relation coefficient. For long-term coding, we set v i (  X  ) = f i (  X  ) and for short-term coding, we tion s models the influence of the stimulus, while the function c models the spatial component desired mean level of correlation  X   X  , we use the method described in [12]. 3.2 Minimum discrimination error is more informative than Fisher Information As has been pointed out in [4], the shape of unimodal tuning functions can strongly influence the coding accuracy of population codes of angular variables. In particular, box-like tuning functions can be superior to cosine tuning functions. However, numerical evaluation of the minimum mean squared error for angular variables is much more difficult than the evaluation of the minimal dis-crimination error proposed here, and the above claim has only been verified up to N = 20 neurons. Here we compute the full neurometric functions for N = 10 , 50 , 250 binary neurons (figure 4). In this way, we show that the advantage of box-like tuning functions also holds for large numbers of neurons (compare red and black curves in figure 4 a-c). In addition, we note that Fisher Information does not provide an accurate account of the performance of box-like tuning functions: it fails as soon as the nonlinearity in the tuning functions becomes effective and overestimates the true minimal discrimination error E . Similarly, the approximate neurometric functions E d 0 ( X   X  ) obtained from equation 6 do not capture the shape of neurometric functions E ( X   X  ) but underestimate the minimal discrimination error. In contrast, the deviation between both curves stays rather small for cosine tuning functions. 3.3 Stimulus-dependent correlations have opposite effects for long-and short-term The shape of the noise covariance matrix  X   X  can strongly influence the coding fidelity of a neural population. In order to evaluate these effects it is important to take differences in the noise covariance for different stimuli into account. In this section, we will use our new framework to study different noise correlation structures for short-and long-term population coding.
 Previous studies so far have investigated the effect of noise correlations in the long-term case: Most N ( f (  X  ) ,  X (  X  )) (for detailed description of the population model see section 3.1). In this case, the FI of the population takes a particularly simple form. It can be decomposed into: where we omit the dependence on  X  for clarity and f 0 ,  X  0 are the derivatives of f and  X  with respect to  X  . J mean ,J cov are the Fisher information, when either only the mean or only the covariance are assumed to depend on  X  . For this case, various studies have investigated noise structures where correlations were either uniform across the population (figure 3c) or their magnitude decayed with difference in preferred orientations (figure 3d),  X  X imited range structure X  or  X  X patial decay X , see e.g. [1]). Only recently have stimulus-dependent correlations been analyzed in terms of Fisher informa-tion [12]. Josic et al. find that in the absence of limited range correlations, stimulus-dependent noise correlations (figure 3e) are beneficial for a population code, while in their presence (figure 3f), they are detrimental.
 We first compute the neurometric functions E ( X   X  ) and I JS ( X   X  ) for a population of 100 neurons in the case of long-term coding with a Gaussian noise model for the four possible noise correlation structures (figure 5a). We corroborate the results of Josic et al. in that we find that the lowest E or the highest I JS is achieved for a population with stimulus-dependent noise correlations and no limited range structure, while a population with stimulus-dependent noise correlations in the presence of spatial decay performs worst. Spatially uniform correlations (figure 3c) provide almost as good a code as the best coding scheme. Next, we directly compare long-and short-term population coding in a population of 15 neurons 1 . For short-term coding, we assume that the population activity is of binary nature, i.e. each neuron spikes at most once. Again, we compute neurometric functions E ( X   X  ) and I JS ( X   X  ) for all four possible correlation structures. The results for long-term coding do not differ between large and small populations (figure 5b), although relative differences between different coding schemes are less prominent. In contrast, we find that the beneficial impact of stimulus-dependent correlations in the absence of limited range structure reverses in short-term codes for large  X   X  (figure 5c). In this paper, we introduce the computation of neurometric functions as a new framework for study-ing the representational accuracy of neural population codes. Importantly, it allows for a rigorous treatment of nonlinear population codes (e.g. box-like tuning functions) and noise correlations for non-Gaussian noise models. This is particularly important for binary population codes on timescales where neurons fire at most one spike. Such codes are of special interest since psychophysical ex-periments have demonstrated that efficient computations can be performed in cortex on short time scales [19]. Previous studies have mostly focussed on long-term population codes, since in this case it is possible to study many question analytically using Fisher Information. Although the structure of neural population acitivity on short timescales has recently attracted much interest [16, 17, 15], population codes for binary population activity and, in particular, the impact of different noise corre-lation structures on such codes are not well understood. In contrast to previous work [14], neuromet-ric function analysis allows for a comprehensive treatment of both short-and long-term population codes in a single framework. In section 3.3, we have started to study population codes on short timescales and found important differences in the effect of noise correlations between short-and long-term population codes. In the future, we will extend these results to much larger populations adapting new techniques for approximate fitting of Ising models [15].
 The example discussed in section 3.2 demonstrates that neurometric functions can provide addi-tional information compared to Fisher Information: While Fisher Information is a single number for each potential population code, neurometric functions in terms of E or I JS assess the coding quality for each pair of stimuli. This also enables us to detect effects like the dependence of the relative performance of different population codes on  X   X  as shown in figure 5 c and f. We can furthermore easily extend the framework to take unequal prior probabilities into account. In equations 1 and 2 we have assumed equal prior probabilities p (  X  1 ) = p (  X  2 ) = 1 2 . Both E and I JS , however, are also well defined if this is not the case.
 The framework of stimulus discrimination in a 2AFC task has long been used in psychophysical and neurophysiological studies for measuring the accuracy of orientation coding in the visual system (e.g. [5, 21]). It is therefore appealing to use the same framework also in theoretical investigations on neural population coding since this facilitates the comparison with experimental data. Further-more, it allows studying population codes for categorial variables since, in contrast to Fisher Infor-mation, it does not require the variable of interest to be continuous. This is of advantage, as many neurophysiological studies investigate the encoding of categories, such as objects [11] or numbers [20].
 Acknowledgments We thank A. Tolias and J. Cotton for discussions. This work has been supported by the Bernstein award to MB (BMBF; FKZ: 01GQ0601) and a scholarship of the German National academic foun-dation to PB. [1] L. F. Abbott and Peter Dayan. The effect of correlated variability on the accuracy of a popula-[2] B. B. Averbeck, P. E. Latham, and A. Pouget. Neural correlations, population coding and [3] B. B. Averbeck and D. Lee. Effects of noise correlations on information encoding and decod-[4] M. Bethge, D. Rotermund, and K. Pawelzik. Optimal Short-Term population coding: When [5] A. Bradley, B. C. Skottun, I. Ohzawa, G. Sclar, and R. D. Freeman. Visual orientation and [6] N. Brunel and J. P. Nadal. Mutual information, fisher information, and population coding. [7] M. Casas, P. W. Lamberti, A. Plastino, and A. R. Plastino. Jensen-Shannon divergence, fisher [8] T. M. Cover and J. A. Thomas. Elements of Information Theory . Wiley-Interscience, 2006. [9] P. Dayan and L. F. Abbott. Theoretical neuroscience: Computational and mathematical mod-[10] J.R. Hershey and P.A. Olsen. Approximating the kullback leibler divergence between gaus-[11] C. P. Hung, G. Kreiman, T. Poggio, and J. J. DiCarlo. Fast readout of object identity from [12] K. Josic, E. Shea-Brown, B. Doiron, and J. de la Rocha. Stimulus-dependent correlations and [13] J. Lin. Divergence measures based on the shannon entropy. Information Theory, IEEE Trans-[14] S. Panzeri, A. Treves, S. Schultz, and E. T. Rolls. On decoding the responses of a population [15] Y. Roudi, J. Tyrcha, and J. Hertz. The ising model for neural data: Model quality and ap-[16] E. Schneidman, M. J. Berry, R. Segev, and W. Bialek. Weak pairwise correlations imply [17] J. Shlens, G. D. Field, J. L. Gauthier, M. Greschner, A. Sher, A. M. Litke, and E. J. [18] H. Snippe and J. Koenderink. Information in channel-coded systems: correlated receivers. [19] S. Thorpe, D. Fize, and C. Marlot. Speed of processing in the human visual system. Nature , [20] O. Tudusciuc and A. Nieder. Neuronal population coding of continuous and discrete quantity [21] P. Vazquez, M. Cano, and C. Acuna. Discrimination of line orientation in humans and mon-
