
Linear discriminant analysis (LDA) as a dimension re-duction method is widely used in data mining and machine learning. It however suffers from the small sample size (SSS) problem when data dimensionality is greater than the sample size. Many modified methods have been proposed to address some aspect of this difficulty from a particular view-point. A comprehensive framework that provides a complete solution to the SSS problem is still missing. In this paper, we provide a unified approach to LDA, and investigate the SSS problem in the framework of statistical learning the-ory. In such a unified approach, our analysis results in a deeper understanding of LDA. We demonstrate that LDA (and its nonlinear extension) belongs to the same frame-work where powerful classifiers such as support vector ma-chines (SVMs) are formulated. In addition, this approach allows us to establish an error bound for LDA. Finally our experiments validate our theoretical analysis results.
The purpose of this paper is to present a unified theo-retical framework for discriminant analysis. Discriminant analysis [6, 10] has been successfully used as a dimension-ality reduction technique for many classification problems. According to Fisher X  X  criterion, one has to find a projection matrix W that maximizes: where S b and S w are so-called between-class and within-class matrices, respectively. In practice, the  X  X mall sample size X  (SSS) problem is often encountered, where S w is sin-gular. Therefore, the maximization problem can be difficult to solve.

To address this issue, the term  X I is added, where  X  is a small positive number and I the identity matrix of proper size. This results in maximizing It can then be solved without any numerical problems.
In [9], Friedman discusses regularized discriminant anal-ysis with regard to the small sample size problem. Equation (2) is a special case of his regularized discriminant analysis in practice. However, Friedman leaves open the question of whether an optimal choice for the parameter  X  exists, and if so, whether this optimal choice is a unique one? Fur-ther, Friedman X  X  analysis is based on traditional parametric methods in statistics, and Gaussian distributions for under-lying random variables are often assumed. That is, his reg-ularization technique is one of improving the estimation of the covariance matrices that are possibly biased due to the small data size. The validity of this technique can be very limited. For example, the nonlinear case of discriminant analysis, called generalized discriminant analysis (GDA), becomes very popular in practice and shows promising po-tential in many applications. The idea is to map the data into a kernel induced feature space and then perform discrimi-nant analysis there. However, in a high and possible infinite dimensional feature space, GDA is not well justified from Friedman X  X  point of view. Thus, a rigorous justification for the regularization term  X I is still missing, especially when GDA is considered.

In [7], Evgeniou, Pontil and Poggio present a theoretical framework for the regularization functional (the so called regularization network) where || f || 2 K is a norm in a Reproducing Kernel Hilbert Space H defined by the positive definite function K , l is the number of data points or examples and  X  is a regularization parameter. In their paper they justify the above functional using statistical learning theory that is mostly developed by Vapnik [22].

In this paper, we will show that discriminant analysis can be cast in the framework of statistical learning theory. More specifically, Fisher X  X  criterion (1) is equivalent to the regularization network (3) without regularization, which is, min equivalent to the regularization network (3) with the param-eter  X  corresponding to the regularization term  X l . Thus the role of  X I is well justified by statistical learning theory. Fur-ther we show that the optimal choice of  X  exists, and that it is unique. Additionally we establish an error bound for the function that discriminant analysis tries to find. The relationship between least squares regression and Fisher X  X  linear discriminant has been well known for a long time. There is a good review in [6]. Actually in the sem-inal paper [8], Fisher already pointed out its connection to the regression solution. In [17], Mika also provides an ex-tensive review of LDA. Another widely used technique in statistics, canonical correlation analysis, is also closely re-lated to least squares regression [1].

The regularized least squares (RLS) methods have been studied for a long time, under different names. In statis-tics, ridge regression [12] has been very popular for solv-ing badly conditioned linear regression problems. After Tikhonov published his book [21], it was realized that ridge regression uses the regularization term in Tikhonov X  X  sense. In the 1980 X  X , weight decay was proposed to help prune unimportant neural network connections, and was soon rec-ognized [13] that weight decay is equivalent to ridge regres-sion. Recently this old method was found essential in the framework of statistical learning theory, labeled by different names (RLS [18], Regularized Network [7], least squares SVMs [19], and proximal SVMs [11]). It is demonstrated that this regression method is fully comparable to SVMs when used as a classifier [18, 24]. Most recently, an error bound for RLS given a finite sample data set was developed in [5].

Many elements discussed in this paper are scattered throughout the literature and may look familiar to some au-dience but not to others. The fact is that most are not com-prehensive enough for this topic and unclear issues pop up for many. For example, many researchers such as Mika [17] apply the regularization technique to address the SSS prob-lem but they still follow Friedman X  X  principle that is quite limited. It is our goal to adopt a simple yet unified approach to making this topic more comprehensive.
In this section, we first review LDA using Fisher X  X  cri-terion, and then go on to study it from a learning theory point of view, which we call discriminant learning analysis (DLA).
In LDA, within-class, between-class, and mixture scatter matrices are used to formulate the criteria of class separa-bility. Consider a J -class problem, where m 0 is the mean vector of all data, and m j is the mean vector of j -th class data. A within-class scatter matrix characterizes the scatter of samples around their respective class mean vectors, and it is expressed by S w = J j =1 l j i =1 ( x j i  X  m j )( x where l j is the size of the data in the j  X  th class. A between-class scatter matrix characterizes the scatter of the class means around the mixture mean m 0 . It is expressed by S ter matrix is the covariance matrix of all samples, regard-less of their class assignments, and it is given by S m = rion is used to find the projection matrix that maximizes (1). In order to determine the matrix W that maximizes J ( W ) , one can solve the generalized eigenvalue problem: S w i =  X  i S w w i . The eigenvectors corresponding to the largest eigenvalues form the columns of W . For a two class problem, it can be written in a simpler form: where m 1 and m 2 are the means of the two classes.
LDA is a supervised subspace learning problem where we are given a set z = { ( x i ,y i ) } l i =1 of l training exam-ples drawn i.i.d. from the probability space Z = X  X  Y . Here probability measure  X  is defined, and x i are the n -dimensional inputs. We first consider two class problems where we have y i  X  X  X  1 , 1 } as class labels. Without loss of generality, we assume x has zero mean, i.e., E ( x )=0 . Let us consider: This is a mean squared function estimation problem. Here we first consider the hypothesis space H L , the function class of linear projections. That is, H L = { f | f ( x )= w
T x, x  X  X } . Thus we will solve
It turns out that the solution of (6) is the same as the solution obtained by Fisher X  X  criterion.
 Lemma 1 The linear system derived by the least squares criterion (6) is equivalent to the one derived by Fisher X  X  criterion (1), up to a constant, in two class problems. Proof: We rewrite (6) as: We use the fact that Xy = l 1 m 1  X  l 2 m 2 . Taking the deriva-tive with respect to w and setting the result to 0 we obtain This is equivalent to Equation (4). When S w is full rank, three equations S m w =( l 1 m 1  X  l 2 m 2 ) , S m w = m 1 and S w w = m 1  X  m 2 have the same solution w up to a constant factor, given that the overall mean is 0. For details see Appendix A.
When both classes are gaussian distributed, the LDA can be shown optimal. However more often in reality they are not, then it is not clear how good the LDA is. The con-ventional view of LDA is limited here. The learning frame-work, on the other hand, is more general. There is no need to assume the distribution of the data to discuss the  X  X ood-ness X . Instead, the learning approach directly aims to find a good function mapping and the goodness is well defined (see Appendix C and ?? ).

In a general sense, LDA tries to find a transformation f such that the transformed data have a large class mean difference and small within-class scatters. The equivalence between (6) and (1) can be understood in another way: one minimizes the mean squared error with respect to each class mean while keeping the mean difference constant. We de-fine f  X  as the best function that minimizes the mean squared error. That is Given a set of data z , one tries to learn the function f is as close as possible to the optimal mapping f  X  . That is, f ity measure  X  is unknown and so is f  X  . Instead, we may consider (5) that is called the empirical error (risk) mini-mization. Because (6) is equivalent to Fisher X  X  criterion, it is clear that Fisher X  X  criterion leads to an empirical er-ror minimization problem. From the statistical learning the-ory point of view, however, without controlling the function norm, solving equation (5) often leads to overfitting data and the solution is not stable. And when the sample size is smaller than the dimensionality, it is an ill-posed problem and the solution is not unique. That is where diverse LDA techniques have been developed to solve the SSS problem.
Following [7], we instead minimize over a hypothesis space H the following regularized functional for a fixed positive parameter  X  : Again, we consider the linear projection function space H In this case, || f || 2 = w T w . Thus, l i =1 ( y i  X  w T x i ) 2 +  X lw T w = l  X  2( l 1 m l m 2 ) T w + w T ( S m +  X lI ) w . Taking the derivative with respect to w and setting the result to 0, we have In Appendix A, it is shown that (11) is equivalent to and therefore solving it is equivalent to maximizing (2). We call this approach discriminant learning analysis. Since for any given positive regularization term  X  , (9) always has a unique solution, we have Proposition 2 Discriminant learning analysis is a well-posed problem.
In the new framework of discriminant learning analysis, the nonlinear case is not an  X  X xtension X  of the linear case by the  X  X ernel X  trick. In the above analysis we have adopted the linear hypothesis space H L , whereby we have been looking for an optimum solution in that space. If we choose a general Reproducing Kernel Hilbert Space (RKHS) as our hypothesis space, we will obtain nonlinear function map-pings as the subspace. The derivation of DLA in a general function space is simply carried out in terms of function analysis, in a similar way as in linear space. Here we skip the details of the derivations. Briefly speaking, for a two class problem, the nonlinear DLA solution is found by fol-lowing a simple algorithm [18] where K is the Gram matrix of the training data, and K =[ k ( x vector of class labels. After solving c =[ c 1 , ..., c l given new data x will be projected onto the subspace by s = l i =1 c i k ( x i ,x ) .
By setting the discriminant learning analysis in the framework of statistical learning theory, we can provide an error bound for f DLA obtained by (9) in the following the-orem: Theorem 3 Let f  X  be defined by (8), with confidence 1  X   X  , the error bound for the solution f DLA of (9) is given by: where A (  X  ) and S (  X  ) are given by  X  1 / 2 L  X  1 4 k f minimizes S (  X  )+ A (  X  ) .

This decomposition is often called bias-variance trade-off. A (  X  ) is called the approximation error. Given a hy-pothesis space, it measures the  X  X rror X  between the optimal function in it and the true target. L  X  1 4 k in A (  X  ) is simply an operator. S (  X  ) bounds the sample error and is essentially derived by the big number law. M and C k are constants and v  X  ( l,  X  ) is the unique solution of an equation whose co-efficients contain sample size l and confidence parameter  X  . Due to its complex nature, we do not provide the details of the proof, which can be found in [5]. To the best of our knowledge, this is the first known error bound for LDA.
We have presented discriminant learning analysis as an alternative approach to LDA in two class problems. The data is projected onto only one dimension that is adequate for the two class problem. However, LDA is generally used to find a subspace with d dimensions for a multiple class problem. In this section we extend DLA to the multi-class case.

For a two class problem, the function f is a mapping from a vector x to a scaler y  X  R . If we want to find a subspace with d&gt; 1 dimensions, we actually consider a mapping from vector x to vector y  X  R d . This direct ap-proach has some difficulties. First, statistical learning the-ory is concerned with only the first kind of function map-ping f : R n  X  R , and cannot be directly extended to f : R n  X  R d . Second, subspace dimensionality d is not fixed (rather it is often provided by the user). These dif-ficulties make the attempt at directly casting DLA in the multiple class case as a least squares estimation problem a very difficult task.

Notice that LDA in a multiple class problem can be de-composed into l two class problems. In the i -th two class problem, it treats the i -th class as one class and all remain-ing classes as the second class. Each binary class problem is solved first, and after finding all subspaces, PCA is applied to find eigenvectors having the largest eigenvalues. These new eigenvectors are the solution of the original multi-class LDA problem (See Appendix B).

Thus, DLA can be naturally extended to the multi-class case. Simply, in the decomposition step, we replace two class LDA by two class DLA. In the linear case, it turns out that the DLA algorithm in the multi-class case simply solves This is equivalent to solving (2).
In this section, we will analyze the role of the regular-ization term  X  . We are especially interested in comparing the DLA algorithm (15) to various LDA algorithms, such as PCA+LDA [2, 20], scatter-LDA [15, 10], newLDA [3] and DLDA [23]. These techniques are mostly proposed for solving facial recognition problems where the SSS problem always occurs. We summarize them briefly: PCA+LDA: Apply PCA to remove the null space of S w Scatter-LDA: Same as PCA+LDA but maximizing newLDA: If S w is full rank then solve regular LDA; else in DLDA: Apply PCA to remove the null space of S b first,
It should be noted that PCA+LDA and Scatter-LDA can be equivalent when S w and S m span the same subspace. However, they are different when S b totally or partially spans the null space of S w , thus S w and S m span differ-ent subspaces. For facial images the latter case turns out to be more common. In [3], Chen et al. prove that the null space of S w contains discriminant information. They also show that Scatter-LDA is not  X  X ptimal X  in that it fails to dis-tinguish the most discriminant information in the null space of S w . Thus they proposed the newLDA method. However, newLDA fell short of making use of any information out-side of that null space. DLDA, on the other hand, discards the null space of S b . This is very problematic. First, it fails to distinguish the most discriminant information in the null space of S w , as in Scatter-LDA. Second, the null space of S is not completely useless. This can be seen in a two class case. DLDA simply chooses m as the subspace. However this can be biased because it ignores S w .

It should be noted that in [16] there is a misleading claim, where the authors mention that there is no significant differ-ence between newLDA and DLDA. This incorrect claim is also pointed by others [14]. In a word, all these techniques make  X  X ard X  decisions, either discarding a null space, or only working in a null space. In the following, we can see that DLA always works in the whole space, not making any  X  X ard X  decisions. Instead, it  X  X uses X  information from both subspaces. DLA can work in the whole space because it is well-posed.

We first consider how LDA is solved. For the moment let us assume that S w is full rank, and thus S  X  1 w exists. For symmetric matrix S w there is a matrix U such that U T U = I , S w = U  X  U T , and S T w = U  X   X  1 U T , where  X  is a diagonal matrix. U T is a transform matrix. Consider the following in a two class LDA The between vector m is transformed to a new basis by U T then multiplied by a constant 1  X  finally transformed again by U back to the origin basis.
To illustrate this process. We consider a simple case where the data are in two dimensions. x 1 and x 2 are the original axis bases, as shown in Figure 1. x 1 and x 2 are the orthogonal axis bases in which S w is diagonalized. We choose S w such that U T S w U = X =  X  and m 2 are the projection on x 1 and x 2 , respectively. By multiplying  X   X  1 , m  X  1 = 1 2 m 1 and m  X  2 =2 m 2 . The LDA solution, S  X  1 w m is shown in Figure 1-(a) and denoted by  X  X DA X .

Now consider DLA: ( S w +  X lI ) w = m . Notice that if matrix U can diagonalize S w then it can diagonal-
Figure 1. Illustration and comparison of solu-tions by LDA, DLA, DLDA and newLDA. ize ( S w +  X lI ) at the same time: U T ( S w +  X lI ) U = U T S w U +  X lU T U = X +  X lI . Thus, ( S w +  X lI )  X  1 = U ( X  +  X lI )  X  1 U T . To illustrate the effect of the regular-ization term, we choose  X l =1 and in Fig. 1,  X  i are the eigenvalues of S w +  X lI .Now  X   X  1 1 = 1 3 , and  X   X  1 2 The vector ( S w +  X lI )  X  1 m is shown in Fig. 1-(a) and de-noted by  X  X LA X .

We now consider the case where the SSS problem oc-curs. We keep  X  2 =1 . 5 the same but  X  1 =1 (that is, the original eigenvalue of S w is 0 along this dimension). This case is plotted in Fig. 1-(b). DLA takes advantage of infor-mation from both worlds. However, PCA+LDA will throw away the dimension x 1 and only keep the direction x 2 . Thus it ignores critical discriminant information. newLDA, on the other hand, will only keep the dimension x 1 and throw away any discriminant information along x 2 . DLDA simply chooses m .

Thus, when the SSS problem does not occur, DLA has a smoothing effect on the eigenvalues of S w . If the SSS prob-lem does occur, DLA fuses and keeps a balance between the information both in and out of the null space of S w . This balance is achieved by choosing proper  X  . We tested DLA algorithm against PCA+LDA, newLDA, DLDA and Scatter-LDA on FERET facial image data (http://www.itl.nist.gov/iad/humanid/feret/). We extracted 400 images for the experiment, where there are 50 indi-viduals with 8 images from each. The images have been preprocessed and normalized using standard methods from the FERET database. The size of each image is 150 x 130 pixels, with 256 grey levels per pixel.

We randomly choose five images per person for training, and the remaining three for testing. Thus the training set has 250 images while the testing set has 150. Subspaces are calculated from the training data, and the 1-NN classifier (3-NN failed for all the methods due to five images per per-son in the training data) is used to obtain the accuracy rates after projecting the data onto the subspace. To obtain aver-age performance, each method is repeated 10 times. Thus there are total 1,500 testing images for each method. The regularization term  X l is chosen by 10-fold cross-validation. The average accuracy rates are shown in Figure 2. The X -axis represents the dimensionality of the subspace. For each technique, the higher the dimension, the less discriminant the dimension. For most techniques, the accuracy rates in-crease quickly in the first 15 dimensions, and then increase slowly with additional dimensions.

DLA is uniformly better than any other algorithms, demonstrating its efficacy. It achieves the highest accuracy rate of 0.977. newLDA performs quite well in these ex-periments, again demonstrating that the most discriminant information is in the null space of S w , for the facial recogni-tion task. On the other hand, Scatter-LDA does not perform well at lower dimensional subspaces. But it eventually per-forms better than PCA+LDA, when dimensions are more than 42. All methods achieve their highest accuracy rate with a 49 dimensional subspace, which is not surprising, for this is a 50-class problem. It is noticed that the perfor-mance of newLDA and Scatter-LDA (its tail is not shown in the plot) drops quickly with unnecessary dimensions. Figure 2. Comparison of DLA, LDA, newLDA,
DLDA and Scatter-LDA on the FERET image data.
Table 1. Classification error rates in sub-spaces computed by GDA and NDLA, using 3-nn classifier, on 11 UCI data sets.
In these experiments, we compare nonlinear LDA (GDA) and nonlinear DLA (NDLA) in two class classifi-cation problems. We did not test other nonlinear techniques such as KDDA [16] due to their complex implementation. The implementation of kernel DLDA is very involved. We use 11 data sets from the UCI machine learning database. They are all two classification problems. For each data set, we randomly choose 60% as training and the remaining 40% as testing. We train GDA and NDLA on the training data and obtain projections. We then project both training and test data on the chosen subspace and use the 3-NN clas-sifier to obtain error rates. Note that for the two class case, one dimensional subspace is sufficient. We use the Gaus-producing Kernel Hilbert Space for both GDA and NDLA. The kernel parameter  X  and regularization term  X  were de-termined through 10-fold cross-validation. We repeat the experiments 10 times on each data set to obtain the average error rates.

The results are shown in Table 1. On 9 data sets out of 11, NDLA performs better than GDA. Especially on the breast cancer credit, heart cleve, heart Hungary and glass data, it is significantly better than GDA, with 95% confi-dence. This paper presents a learning approach, Discriminant Learning Analysis for effective dimension reduction. While discriminant analysis is formulated as a least square approx-imation problem, the DLA is a regularized one. Regard-less of the data distribution, the approach allows us to al-ways measure the goodness of mapping function that DLA computes: an error bound for it is established. The com-monly adopted technique: S w +  X I is well justified in the framework of statistical learning theory. The existence and uniqueness of the term  X  is obtained. The DLA is also com-pared to many other competing algorithms and it is demon-strated both theoretically and experimentally, that DLA is more robust than others.

In this appendix, we say Aw = c 1 v and Aw = c 2 m are equivalent in the sense that the solution w s are in the same direction, where A is a matrix, c i s are scalers and m is a vector. First we show that S m w = m and S w w = m have the same solution (same set of eigenvectors), where m = m 1  X  m 2 is the mean difference of the two class problem.
We know that solving S w w = m is equivalent to solving [6] where  X  and  X  are the eigenvector and eigenvalue matrices of S  X  1 w S b . Since we have S w = S m  X  S b , following [10] (pp. 454), (17) can be converted to This shows that  X  is also the eigenvector matrix of S  X  1 and its eigenvalue matrix is  X ( I + X )  X  1 . When the com-ponents of  X  and  X  i are arranged from the largest to the smallest as  X  1  X  ...  X   X  n , the corresponding components of  X ( I + X )  X  1 are also arranged as  X  1 1+  X  1  X  ...  X   X  That is, the t eigenvectors of S  X  1 m S b corresponding to the t largest eigenvalues are the same as the t eigenvectors of S w S b corresponding to the t largest eigenvalues. As a spe-cial case (two class problems), S m w = m and S w w = m share the same  X  X igenvector X  solution.

Now we show that S m w =( l 1 m 1  X  l 2 m 2 ) and S m w = m 1  X  m 2 share the same solution also. Consider that the overall mean m 0 is 0 . lm 0 = l 1 m 1 + l 2 m 2 =0 ,wehave m With a constant c , the solution of S m w = cm is still in the same direction along the mean difference m , and thus is equivalent to solving S  X  1 w S b  X = X  X  .

It is easy to show that ( S m +  X lI ) w =( l 1 m 1  X  l 2 m equivalent to ( S w +  X lI ) w = m 1  X  m 2 , simply replacing S and S m by S w ++  X lI and S m +  X lI in the above analysis. In Appendix A, we showed that for a two class problem, LDA and DLA correspond to solving (4) and (12), respec-tively. S m can be written as S m = U m  X  1 / 2 m  X  1 / 2 can be written as  X  1 / 2 m U T m w = X   X  1 / 2 m U T m m  X  v can be treated as a  X  X ata point. X  For a multi-class problem, Assuming S m is full rank, LDA corresponds to the system S b W = S m W  X  . It can be write as This is a simple eigenvalue problem. By solving V ,we can compute W by W = U m  X   X  1 / 2 m V. The eigenvec-tors in V with the largest eigenvalues have a one-to-one correspondence to the eigenvectors in W with the cor-responding largest eigenvalues in the original equation. Note that S b = J j =1 l j ( m j  X  m 0 )( m j  X  m 0 ) T = N b is the covariance of these J points. Equation (19) can also be viewed as a PCA problem on these J points. In the above analysis if we replace S m by S m +  X lI , we obtain that DLA corresponds to solving S b W =( S m +  X lI ) W  X  w Further The eigenvectors corresponding to the largest eigenvalues are chosen as the DLA subspace basis.

The first step in the development of learning theory is the assumption of existence of a probability measure  X  on the product space Z = X  X  Y , from which the data are drawn. One way to define the expected error of f is the least squares error:  X  ( f )= Z ( f ( x )  X  y ) 2 d X  for f : X  X  We try to minimize this error by  X  X earning. X  Define f  X  ( x )=
Y yd X  ( y measure on Y w.r.t. x . It has been shown [4] that the ex-pected error  X   X  of f  X  represents a lower bound on the error  X  ( f ) for any f . Hence at least in theory, the function f is the ideal one and so is often called the target function. However, since measure  X  is unknown, f  X  is unknown as well.

Starting from the training data z = { ( x i ,y i ) } l i , statisti-cal learning theory as developed by Vapnik [22] builds on the so-called empirical risk minimization (ERM) induction principle. Given a hypothesis space H , one attempts at min-imizing However, in general, without controlling the norm of ap-proximation functions, this problem is ill-posed. Generally speaking, a problem is called well-posed if its solution ex-ists, and is unique and stable (depends continuously on the data). A problem is ill-posed if it is not well-posed.
Regularization theory [21] is a device that was developed to turn an ill-posed problem into a well-posed one. The cru-cial step is to replace the functional in (22) by the following regularized functional where  X  is a regularization term, and || f || 2 K the norm of f in L 2  X  ( X ) . Now minimizing (23) makes it well-posed and can be solved by elementary linear algebra. Let f  X ,z be the minimizer of (23), the question then is: How good an approximation is f  X ,z to f  X  , or how small is  X  ( f  X ,z ( f  X ,z ( x )  X  f  X  ( x )) 2 d X  X ? Further, what is the best choice for  X  to minimize this error? In [5], the answers to these questions are given. They state: For each m  X  N and  X   X  [0 , 1) , there is a function E such that, for all  X &gt; 0 , X ( f  X ,z ( x )  X  f  X  ( x )) with confidence 1  X   X  . And there is a unique minimizer of E (  X  ) that is found by a simple algorithm to yield the  X  X est X  regularization parameter  X  =  X   X  .

