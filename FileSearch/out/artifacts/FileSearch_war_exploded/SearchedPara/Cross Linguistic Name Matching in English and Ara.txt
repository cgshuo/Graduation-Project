 technology that processes linguistic content, espe-cially in applications such as information retrieval, document clustering, entity extraction, and transla-tion. Name matching is not a trivial problem even within a language because names have more than one part, including titles, nicknames, and qualifiers such as Jr . or II . Across documents, instances of the name might not include the same name parts, and within documents, the second or third mention of a name will often have only one salient part. In multilingual applications, the problem is compli-cated by the fact that when a name is represented in a script different from its native script, there may be several alternative representations for each phoneme, leading to large number of potential variants for multi-part names. the current leader of Libya. In Arabic, there is only one way to write the consonants and long vowels of any person X  X  name, and the current leader of Libya X  X  name in un-vocalized Arabic text can only be written as  X  X  X  X  X  X  X   X  X  X  X  . In English, his name has many common representations. Ta-ble 1 documents the top five hits returned from a web search at www.google.com, using various English spellings of the name. English phoneme corresponding to the Standard Arabic phoneme /q/. The problem is further com-pounded by the fact that in many dialects spoken in the Arabic-speaking world, including Libya, this phoneme is pronounced as [g]. matches all versions of a particular name in lan-guage A to all possible versions of the same name in language B. Most solutions employ standard string similarity measures, which require the names to be represented in a common character set. The solution presented here exploits translit-eration conventions in normalization procedures and equivalence mappings for the standard Leven-shtein distance measure. methods that match strings based on similarity rather than identity. Common fuzzy matching techniques include edit distance, n-gram matching, and normalization procedures such as Soundex. This section surveys methods and tools currently used for fuzzy matching. 2.1 Soundex Soundex algorithm was designed to find spelling variations of names. Soundex represents classes of sounds that can be lumped together. The precise classes and algorithm are shown below in figures 1 and 2. 4. Return the first four characters of the resulting 5. Examples: Patrick = P362, Peter = P36, Peterson = many different names can appear to match each other when using the Soundex algorithm. 2.2 Levenshtein Edit Distance distance algorithm. A very comprehensive and accessible explanation of the Levenshtein algo-http://www.merriampark.com/ld.htm . distance where edit distance is defined as the num-ber of insertions, deletions or substitutions required to make the two strings match. A score of zero represents a perfect match. of size n , the algorithm has O( nm ) time and space complexity. A matrix is constructed with n rows and m columns. The function e(s i ,t j ) where s string t returns a 0 if the two characters are equal and a 1 otherwise. The algorithm can be repre-sented compactly with the recurrence relation shown in figure 3. ated by dividing the Levenshtein edit distance score by the length of the shortest (or longest) string, subtracting this number from one, and set-ting a threshold score that must be achieved in or-der for the strings to be considered a match. In this simple approach, longer pairs of strings are more likely to be matched than shorter pairs of strings with the same number of different characters. 2.3 Editex Dart (1996). It combines a Soundex style algo-rithm with Levenshtein by replacing the e(s i ,t j ) function of Levenshtein with a function r(s i ,t j ). identical, 1 if they belong to the same letter group and 2 otherwise. The full algorithm with the letter groups is shown in figures 4 and 5. The Editex algorithm neutralizes the h and w . This shows up same as r(s i ,t j ), with two exceptions. It compares letters of the same string rather than letters from the different strings. The other difference is that if s hancements to the Soundex and Levenshtein string matching algorithms. One enhancement is what they call  X  X apering. X  Tapering involves weighting mismatches at the beginning of the word with a higher score than mismatches towards the end of the word. The other enhancement is what they call phonometric methods , in which the input strings are mapped to pronunciation based phonemic rep-resentations. The edit distance algorithm is then applied to the phonemic representations of the strings. performed significantly better than alternatives they tested, including Soundex, Levenshtein edit distance, algorithms based on counting common n-gram sequences, and about ten permutations of tapering and phoneme base d enhancements to as-sorted combinations of Soundex, n-gram counting and Levenshtein. 2.4 SecondString and Fienberg (2003) is an open-source library of string-matching algorithms implemented in Java. http://secondstring.sourceforge.net . ment of string matching algorithms, both those based on the  X  X dit distance X  algorithm, and those based on other string matching algorithms. Sec-ondString also provides tools for combining matching algorithms to produce hybrid-matching algorithms, tools for training on string matching metrics and tools for matching on tokens within strings for multi-token strings. Arabic script were obtained from 106 Arabic texts and 105 English texts in a corpus of newswire arti-cles. We extracted 408 names from the English language articles and 255 names from the Arabic language articles. Manual cross-script matching identified 29 names common to both lists. list of names from the Arabic language texts against the entire list of English language names using algorithms from the SecondString toolkit. The Arabic names were transliterated using the computer program Artrans produced by Basis (2004). matching threshold was empirically set to a value that would return some matches, but minimized false matches. The Levenshtein  X  X dit-distance X  algorithm returns a simple integer indicating the number of edits required to make the two strings match. We normalized this number by using the formula  X   X  of strings with a fuzzy match score less than 0.875 was not considered to be a match. The intent of dividing by the length of both names is to mini-mize the weight of a mismatched character in longer strings. sion, we ignored all issues dealing with the fact that many English names correctly matched more than one Arabic name, and that many Arabic names correctly matched more than one English name. The number of correct matches is the num-ber of correct matches for each Arabic name, summed across all Arabic names having one or more matches. Recall R is defined as the number of correctly matched English names divided by the number of available correct English matches in the test set. Precision P is defined as the total number of correct names returned by the algorithm divided by the total number of names returned. The F-score is four algorithms that were tested. Smith-Waterman is based on Levenshtein edit-distance algorithm, with some parameterization of the gap score. SLIM is an iterative statistical learning algorithm based on a variety of estimation-maximization in which a Levenshtein edit-distance matrix is itera-tively processed to find the statistical probabilities of the overlap between two strings. Jaro is a type n-gram algorithm which measures the number and the order of the common characters between two strings. Needleman-Wunsch from Cohen et al. X  X  (2003) SecondString Java code library is the Java implementation referred to as  X  X evenshtein edit distance X  in this report. The Levenshtein algo-rithms clearly out performed the other metrics. name has more than one possible letter in its Eng-lish representation. For instance, the first letter of former Egyptian president Gamal Abd Al-Nasser X  X  first name is written with the Arabic letter  X  X  , which in most other dialects of Arabic is pro-nounced either as [  X  X  ] or [  X  ] , most closely resem-bling the English pronunciation of the letter  X  X  X . As previously noted,  X  X  has the received pronun-ciation of [q], but in many dialects it is pronounced as [g], just like the Egyptian pronunciation of Nas-ser X  X  first name Gamal. The conclusion is that there is no principled way to predict a single repre-sentation in English for an Arabic letter. names are not entirely predictable. Accented syl-lables will be given a long vowel, but in longer names, different writers will place the long vowels showing the accented syllables in different places. We observed six different ways to represent the name Milosevic in Arabic. edge of the craft for representing foreign names in Arabic and English is summarized in figure 6. These rules are based on first author Dr. Andrew Freeman X  X  1 experience with reading and translating Arabic language texts for more than 16 years. 5.1 Character Equivalence Classes (CEQ): six parts. We replaced the comparison for the character match in the Levenshtein algorithm with a function Ar( s i, t j ) that returns zero if the character from the English string is in the match set for the Arabic character s i ; , otherwise it returns a one. have the same character set, and we chose to use transliterated Arabic so that investigators who could not read Arabic script could still view and understand the results. The full set of transliterated Arabic equivalence classes is shown in Figure 8. The set was intentionally designed to handle Ara-bic text transliterated into either the Buckwalter transliteration (Buckwalte r, 2002) or the default setting of the transliteration software developed by Basis Technology (Basis, 2004). 5.2 Normalizing the Arabic string literation tool transforms certain Arabic letters into English digraphs with the appropriate two charac-ters from the following set: (kh, sh, th, dh). The Buckwalter transliteration method requires a one-to-one and recoverable mapping from the Arabic script to the transliterated script. We transformed these characters into the Basis representation with regular expressions. These regular expressions are shown in figure 9 as perl script. 5.3 Normalizing the English string making the English string more closely match the transliterated form of the Arabic string. These cor-respond to points 2 through 7 of the list in Figure 6. The perl code that implemented these transfor-mations is shown in figure 10. 5.4 Normalizing the vowel representations based on two observations that correspond to points 2 and 8 of Figure 6. Figure 11 shows some English names represented in Arabic transliterated using the Buckwalter transliteration method. Bill Clinton  X  X  X  X  X  X  X   X  X  X  byl klyntwn 
Colin Powell  X  X  X  X   X  X  X  X  X  kwlyn bAwl 
Richard Cheney  X  X  X  X  X   X  X  X  X  X  X  X  rytshArd Arabic as a long vowel or diphthong. This vowel or diphthong will appear in the transliterated un-vocalized text as either a  X  X , X   X  X  X  or  X  X . X  Unac-cented short vowels such as the  X  X  X  found in the second syllable of  X  X owell X  are not represented in Arabic. Contrast figure 11 with the data in figure 12. or  X  X  X  where there are lexically determined long vowels or diphthongs in Arabic. The English rep-resentation of these names must contain a vowel for every syllable. The edit-distance score for matching  X  X uhammad X  with  X  X Hmd X  will fail since only 4 out of 7 characters match. Lowering the match threshold will ra ise the recall score while lowering the precision score. Stripping all vowels from both strings will raise the precision on the matches for Arabic names in English, but will lower the precision for English names in Arabic. only those vowels that are represented in both strings. The algorithm is a variant of a sorted file merge. 5.5 Normalizing  X  X h X  representations with a name  X  X uchanan X  is represented in Arabic as  X  X y-wkAnAn X  and  X  X ichard X  is  X  X ytshArd. X  Thus, whichever choice the software makes for the cor-rect value of the English substring  X  X h, X  it will choose incorrectly some significant number of times. In one pass, every  X  X h X  in the English string gets mapped to  X  X sh. X  In a separate pass, every  X  X h X  in the English string is transformed into a  X  X . X  5.6 Light Stemming move the first letter of the transliterated Arabic name if it matched the prefixes  X  X , X   X  X  X  or  X  X  X  and run the algorithm another time if the match score was below the match threshold but above another lower threshold. The first two items are preposi-tions that attach to any noun. The third is a con-junction that attaches to any word. Full stemming for Arabic is a separate and non-trivial problem. plemented in perl and in Java. Figure 14 presents the results of the enhanced algorithm on the origi-nal baseline as compared with the baseline algo-rithm. The enhancements improved the F-score by 22%. 6.1 Results with a larger data set  X  X oy X  data sets with similar results, we used a more realistic data set, which I will call the TDT data set . This data set was composed of 577 Arabic names and 968 English names that had been manu-ally extracted from approximately 250 Arabic and English news articles on common topics in a NIST TDT corpus. There are 272 common names. The number of strings on the English side that correctly match an Arabic language string is 591. The actual number of matches in the set is 641, since many Arabic strings match to the same set of English names. For instance,  X  X dmond Pope X  has nine variants in English and six variants in Arabic. This gives 36 correct matches for the six Arabic spell-ings of Edmond Pope. combinations of the described enhancements. The plots of the F-score, precision and recall from these experiments using the TDT data set are shown in figures 15, 16, and 17. acter equivalency classes X  (CEQ) to the baseline algorithm boosts the F-score from around 48% to around 72%. Adding all othe r enhancements to the baseline algorithm, without adding CEQ only im-proves the f-score marginally. Combining these same enhancements with the CEQ raises the f-score by roughly 7% to almost 80%. performance with a threshold near 85%. When CEQ is not included, the algorithm has a peak per-formance when the match threshold is around 70%. The baseline algorithm will declare that the strings match at a cutoff of 70%. Because we are normal-izing by dividing by the lengths of both strings, this allows strings to match when half of their let-ters do not match. The CEQ forces a structure onto which characters are an allowable mismatch before the threshold is applied. This apparently leads to a reduction in the number allowable mis-matches when the match threshold is tested. Levenshtein algorithm is a function of the length of the two input strings, being |s| * |t|. This makes the time complexity (N 2 ) where N is the size of the average input string. The enhancements described here add to the time complexity. The increase is an average two or three extra compares per charac-ter and thus can be factored out of any equation. The new time complexity is K(|s|*|t|) where K &gt;= 3. taken by the Soundex and Editex algorithms. They try to reduce the complexity by collapsing groups of characters into a single super-class of characters. The algorithm here does some of that with the steps that normalize the strings. However, the largest boost in performance is with CEQ, which expands the number of allowable cross-language matches for many characters. number of matches would over-generate, raising the recall while lowering the precision. graphemes map to overlapping sets of characters in the English language strings. and one of the reflexes in English for Arabic  X  can be [g] as well. How do we differentiate the one from the other? Quite simp ly, the Arabic input is not random data. Those dialects that produce  X  as versa. The Arabic pronunciation of the string de-termines the correct alternation of the two charac-ters for us as it is written in English . On a string-by-string basis, it is very unlikely that the two rep-resentations will conflict. The numbers show that by adding CEQ, the baseline algorithm X  X  recall at threshold of 72.5%, goes from 57% to around 67% at a threshold of 85% for Arabic to English cross-linguistic name matching. Combining all of the enhancements raises the recall at a threshold of 85%, to 82%. As previously noted, augmenting the baseline algorithm with all enhancements ex-cept CEQ, does improve the performance dramati-cally. CEQ combines well with the other enhancements. ment with an f-score of 80%. However, anyone doing cross-linguistic name matches would proba-bly benefit by implementing some form of the character equivalence classes detailed here. 
