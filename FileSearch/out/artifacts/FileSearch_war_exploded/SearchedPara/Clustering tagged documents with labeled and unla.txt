 1. Introduction
In the past few years, there has been an exponential growth in the number of social networking sites and the popularity of these web sites can be attributed to the changes brought by Web 2.0 in the way users interact with the Web. One of the important features of Web 2.0 is tagging, which allows users to annotate resources with descriptive words. The widespread use of social networking sites and tags have given rise to many interesting and challenging problems to the research com-munity. One of the research topics is how to use tags to improve document analysis. Intuitively, social tags can provide more information than the words appearing in the content, since social tags are provided by users and are similar to the keywords of documents. Moreover, clustering can automatically group documents into meaningful categories, so clustering tagged documents has seen increasing attention recently ( Ramage, Heymann, Manning, &amp; Garcia-Molina, 2009; Zhou, Bian, Zheng, analysis tasks.

However, many researchers ( Brooks &amp; Montanez, 2005; Hayes, Avesani, &amp; Veeramachaneni, 2007; Ramage et al., 2009 ) showed that tags by themselves are weak at partitioning blog data, since there are no techniques for specifying  X  X  X eaning X  X , inducing a hierarchy or inferring relationships between tags. This study analyzes and presents how tags improve document clustering by employing various combinations of tags and content words. Document clustering plays an important role in providing better document retrieval, since a good document clustering method can help machines automatically organize a document corpus into a meaningful cluster hierarchy, which enables an efficient browsing and navigation of the corpus. For instance, clustering can automatically build an ontology like Yahoo!, 1 so the users can benefit from the structure  X  extractive text summarization ( Amini &amp; Gallinari, 2002 ) or automated essay grading ( Chen, Liu, Chang, &amp; Lee, 2010 ).
Clustering X  X  goal is to assign objects into groups so that objects from the same cluster are more similar to each other than objects from different clusters. Basically, clustering is an unsupervised learning approach without labeled data and clustering can generally be formalized as an optimization problem. For instance, K -means clustering partitions a data set by minimizing a sum-of-squares cost function. Although unsupervised learning approaches do not require labeled data to cluster docu-ments, proper seeding biases clustering toward a good region of the search space ( Basu, Banerjee, &amp; Mooney, 2002 ). Addi-tionally, it is very common that the experimenter possesses some background knowledge that could be useful in clustering the data. Basically, the background knowledge can be encoded as constraints of the clustering, and the constraints should be satisfied when the clustering process is completed. Consequently, semi-supervised clustering, learning from a combination of both labeled and unlabeled data, has become a topic of significant recent interest ( Basu et al., 2002, Basu, Bilenko, &amp; Mooney, 2004; Ji &amp; Xu, 2006; Wang et al., 2010; Wagstaff, Cardie, Rogers, &amp; Schr X dl, 2001 ).

This study employs our proposed algorithm called Constrained-PLSA to cluster tagged documents with a small amount of labeled documents. Constrained-PLSA algorithm extends the probabilistic latent semantic analysis (PLSA) ( Hofmann, 1999, 2001 ) by using the seeds to direct the clustering to toward a good region of the search space. As an unsupervised learning method, PLSA does not require labeled data. Additionally, PLSA is a generative model based on a mixture decomposition de-rived from a latent class model, and it has been successfully applied to a number of text analysis tasks such as topic mod-eling, document clustering, and collaborative filtering. Notably, two PLSA models are the aspect model and statistical clustering model ( Hofmann, 2001; Hofmann, Puzicha, &amp; Jordan, 1999 ). In a clustering model for documents, PLSA clustering model assumes that each document belongs to exactly one cluster. Conversely, the aspect model assumes that every occur-rence of a word in a document is associated with a unique state z k of the latent class variable ( Hofmann, 2001 ). The Con-strained-PLSA is based on statistical clustering model rather than aspect model.

This study conducted experiments on academic paper data sets collected from CiteULike, 2 which is a social bookmarking web site and is aimed to promote and develop the sharing of scientific references among researchers. Each post in CiteULike includes paper meta-data and the tags annotated by users. Paper full text information is unavailable in CiteULike, explaining why this study uses abstract information to represent paper content. Paper abstract is limited in size and it can be considered as summary of a paper. In addition to tag usage, this study also analyzes whether tagged document clustering can function properly when only summary of a paper is available. Two data collections are used in the experiments. The first data set is the document collection whose boundaries among document clusters are not clear; while the second one has clear boundaries among document clusters. The experimental results indicate that almost all of the methods can benefit from tags. However, unsupervised learning methods fail to function properly in the data set with noisy information, but Constrained-PLSA is much better than unsupervised learning methods even though only a small amount of labeled data is available. On the other hand, unsupervised learning methods can work well on the data set without noisy information and the Constrained-PLSA still outper-forms the other methods. The main contributions of this paper are the following:
This study focuses on tagged document clustering problem, where only abstracts and tags are available for analysis, and shows that our proposed semi-supervised clustering algorithm with a small amount of labeled data can effectively improve tagged document clustering performance.

This study analyzes how document analysis algorithms utilize tags in clustering tasks by using four combinations of tags and content words to represent a tagged document. Different weighting ratios are given in the experiments for further analysis.
 This study applies several state-of-the-art algorithms and our proposed semi-supervised clustering algorithm to CiteU-
Like data sets. The experimental results indicate that almost all of the methods can benefit from tags. Additionally, Con-strained-PLSA generally outperforms the other methods.

The rest of this work is organized as follows. Section 2 presents related surveys. Section 3 then introduces the Con-strained-PLSA algorithm. Section 4 presents feature representation with different combinations of tags and content words.
Next, Section 5 summarizes the results of several experiments. Conclusions are finally drawn in Section 6 . 2. Related surveys 2.1. Social tagging Tagging provides a mechanism to allow users to share, store, organize and retrieve the resources they are interested in.
Besides, the tags are annotated by users, so the tags can provide semantic information about the resources and they can help machines perform the classification or clustering tasks accurately. Chen, Wright, and Nejdl (2009) employed the data col-lected from Last.fm 3 to conduct experiments and their experimental results demonstrated the benefit of using tags for accurate music genre classification. Begelman, Keller, and Smadja (2006) showed that clustering techniques can improve the user expe-riences of current tagging services.
 Some researchers analyzed whether tags provide the necessary descriptive power to successfully group articles into sets. Brooks and Montanez (2005) selected 500 of the articles collected from Technorati as data set and employed the average pairwise cosine similarity as performance measurement. Their experimental results showed that tags are useful for grouping articles into broad categories, but less effective indicating the particular content of an article. Meanwhile, Ramage et al. (2009) analyzed how tags can be used as a complementary data source for improving automatic clustering of web pages. They presented that the inclusion of tagging data significantly improves the performance of clustering versus tags or words alone. This suggests convincingly that tags are a qualitatively different type of content than  X  X  X ust more words X  X  as has been suggested recently ( Berendt &amp; Hanser, 2007 ). 2.2. Semi-supervised learning
Semi-supervised learning methods can be further classified into semi-supervised classification and semi-supervised clus-tering methods. Semi-supervised classification employs the labeled data along with unlabeled data to construct a more accu-rate classifier; while semi-supervised clustering employs small amount of labeled data to bias the clustering of unlabeled objects into groups so that objects from the same cluster are more similar to each other than objects from different clusters. Thus, many clustering algorithms aim at the minimization of the cost function, which involves distortion measure between the objects and the cluster representatives. For instance, K -means locally minimizes the average squared distance between represent data and its objective function becomes a graph minimum cut problem.

Although unsupervised learning approaches do not require labeled data to cluster the documents, proper seeding biases sesses some background knowledge that could be useful in clustering the data. Basically, the background knowledge can be encoded as constrains of the clustering, and these constraints should be satisfied when the clustering process is completed. Wagstaff et al. (2001) proposed a semi-supervised variant of K -means called COP-KMeans to employ constraints to represent background knowledge. There are two types of constraints, must-link (two instances have to be together in the same cluster) tering that use initial labeled data for seeding. These two algorithms are Seeded-KMeans and Constrained-KMeans. In Seeded-strained-KMeans, the seeds are used to initialize centers and keep the grouping of labeled data unchanged throughout the clustering process. Their experimental results showed that Constrained-KMeans outperforms Seeded-KMeans. Many classi-fication or clustering algorithms rely on a distance measure between patterns to determine the pattern similarities, so defin-ing an appropriate distance measure between patterns is crucial to many machine learning algorithms. Wang, Chen, Zhang, and Li (2008) considered must-link and cannot-link constraints as supervisory information to learn a distance metric and their experimental results showed that K -means can benefit from the learned metric.

In addition to the above K -means variant approaches, there are many semi-supervised clustering approaches that are ex-tended from the other algorithms. In spectral clustering, Ji and Xu (2006) proposed to incorporate prior knowledge of cluster membership for document cluster analysis. The prior knowledge indicates pairs of documents that known to belong to the same cluster. Then, the prior knowledge is transformed into a set of constraints. The document clustering task is accom-plished by finding the best cuts of the graph under the constraints. Wang and Davidson (2010) proposed a framework for constrained spectral clustering algorithm, which preserves the original graph Laplacian and explicitly encodes the constraints. In non-negative matrix factorization (NMF) ( Lee &amp; Seung, 1999; Lee &amp; Seung, 2000 ), Wang, Li, and Zhang (2008) considered must-link and cannot-link as penalties to develop a penalized matrix factorization based approach for semi-supervised clustering. The Constrained-PLSA proposed in this study is an extension of PLSA clustering model. There are two main differences between our proposed approach and NMF-based semi-supervised learning approaches. First, Con-strained-PLSA uses maximum likelihood estimation (MLE) to estimate cluster memberships; while NMF-based semi-super-vised learning approaches rely on matrix factorization to obtain cluster memberships. Second, Constrained-PLSA fixes the cluster memberships of the data points in the seed set during expectation step and maximization step of the algorithm; while NMF-based semi-supervised learning approaches generally consider must-link and cannot-link constraints as regular-ization terms of optimization equations. 3. Constrained-PLSA 3.1. Notation The notations that will be used in the following sections are described in this section. Given a set of training documents focuses on semi-supervised learning approach, explaining why this study assumes that only small subset of the documents d the whole documents can be divided into two disjoint partitions, namely, D X D l [D u . Each document d i is considered to be an ordered list of word events, h w i ,1 , ... , w i , M i . This study uses w i , j to denote the word w j in the document d i
P ( w j z k ) represents the class conditional probability of a specific word conditioned on the unobserved class variable z k , and finally P ( z k j d i ) denotes a document specific probability distribution over the latent variable space. 3.2. PLSA aspect model
Learning from data is always a highly active field of research distributed over many disciplines ranging from statistics to machine learning, pattern recognition and natural language processing. Given a collection of data, the goal is to construct a model through learning process, then the model can discover the underlying structure of the observed data. Besides, the model can further recognize patterns in data, and then make predictions about new data based on what it has learned.
Hofmann et al. (1999) proposed an unsupervised learning framework from dyadic data. The dyadic data refers to a do-main with two sets of objects, X X f x 1 ; ... ; x N g and Y X f y 1 ; ... ; y M g , in which observations are made for ( x i , y j co-occurrence information. The dyadic data representation is commonly used in many application domains, such as text analysis, computer vision, and computational linguistics. In text analysis, X represents a document collection and Y repre-occurring in document x i .

Given the above observations, latent sematic analysis (LSA) is a theory and method for analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA applies singular value decomposition (SVD) to the document-term matrix and a low-rank approximation of the matrix could be used to determine patterns in the relationships between the terms and concepts contained in the text. LSA has been successfully applied to many different kinds of applications ( Deerwester, Dumais, Landauer, Furnas, &amp; Harshman, 1990; Liu, Hsaio, Lee, several important features. First, it is an unsupervised learning method, so it does not need labeled data. Second, PLSA is a generative model and it is based on a mixture decomposition derived from a latent class model. Third, the latent variable introduced by PLSA can infer more semantic information from the observations. For instance, PLSA can handle polysemy problem, namely, a word with many possible meanings.

Besides LSA, PLSA is highly related to NMF. The connection between PLSA and NMF was first presented in ( Hofmann, 2001 ). Gaussier and Goutte (2005) presented that PLSA provides a probabilistic interpretation of NMF. Ding, Li, and Peng (2008) further showed that PLSA and NMF (with the I-divergence objective function) optimize the same objective function. The probabilistic interpretation of NMF characteristic gives a foundation for document clustering using NMF. Notably, two PLSA models are the aspect model and statistical clustering model ( Hofmann, 2001; Hofmann et al., 1999 ). words, the observations can be thought of as being partitioned into K clusters. Besides, identical observation can be associ-ated with different latent class. A document can be associated with several latent topics. Each topic can generate terms according to its term distribution.

The standard procedure for maximum likelihood estimation in latent variable models is the Expectation Maximization are computed for the latent variable z based on the current estimates of the parameters.
 log likelihood. By standard calculation, one arrives at the following M-step re-estimation equations. As shown in Eq. (2) , the parameters are updated based on the posterior probabilities obtained in the previous E-step.
 3.3. Constrained-PLSA
In addition to aspect model, the other one is clustering model. In a clustering model for documents, PLSA clustering model assumes that each document belongs to exactly one cluster and it is only the finiteness of the number of observations per document that induces uncertainty about a document X  X  cluster membership. In contrast, the aspect model assumes that every occurrence of a word in a document is associated with a unique state z k of the latent class variable ( Hofmann, 2001 ). The Constrained-PLSA is an extension of PLSA clustering model and it is based on the same assumptions used by Nigam, McCallum, Thrun, and Mitchell (2000) : (1) the data is generated by a mixture model, and (2) there is a correspondence be-tween mixture components and classes. Liu, Hsaio, Lee, and Chen (2011) showed that the Constrained-PLSA can estimate maximum likelihood in latent variable models using the Expectation Maximization (EM) algorithm. Eq. (3) shows the E-step, which computes the posterior probability distribution of latent variable, denoted Q . The probability matrix Q is a N K ma-place P ( z k ) and H is a K M matrix, which represents topic-term distribution. Each row H k in H represents a topic and the entry value H kj represents the probability of topic k generating word w j .
 Then, lagrangian function can be obtained based on the expected complete log likelihood function and the probability con-dard calculation, one arrives at the following M-step re-estimation equations as shown in Eqs. (4) and (5) . Algorithm 1. Constrained-PLSA Algorithm.

Algorithm 1 shows the Constrained-PLSA algorithm. The inputs of the Constrained-PLSA include document term matrix H , topic k . The outputs of the Constrained-PLSA are the topic-term matrix H and the document-topic matrix Q . In initialization step, each row of H has to be normalized, so that the sum of each row is 1. The initial value of H k is determined by the doc-ument seeds belonging to topic k . The E-step and M-step are estimated according to Eqs. (3) X (5) , respectively. The topics of labeled documents have to be fixed. Each row of matrix Q has to be normalized, since it is a probability distribution vector. 4. Feature representation 4.1. Data corpora
CiteULike is a social bookmarking web site and is aimed to promote and develop the sharing of scientific references among researchers. Scientists can annotate their interested academic papers with tags and share the information with the other people. CiteUlike fuses together two separated categories of software: the new Web 2.0 breed of social bookmark-ing services and traditional bibliographic management software. While web bookmarks are simple URLs, citations are a bit more complex and include meta-data such as journal names, authors, and page numbers. However, meta-data information does not include paper category information, which is required for this study to evaluate system performances. This study assigns papers to communities according to their venues, using the classification system adopted by Microsoft X  X  academic actions on Graphics), and CGA (IEEE Computer Graphics and Applications). A paper published in the TOG would be classified as
Graphics field. Above paper classification mechanism is also used by Shi, Tseng, and Adamic (2009) . Obviously, some journals may belong to more than one field, explaining why this study only focuses on the fields that are highly unrelated. This study
Like, so this study employs the paper abstract and tags annotated by users as paper content. This corpus can be downloaded from http://islab.cis.nctu.edu.tw/download/. 4.2. Preprocess
In natural language processing (NLP) and information retrieval (IR), bag of words model tries to use an unordered collec-tion of words to represent a text, disregarding grammar and even word order. Restated, each word in the text contributes to a feature of the document. This study adopts the same approach to construct a feature vector of a document. Stop words are removed first, since stop words fail to provide adequate information for clustering tasks. Then, only English letters are used and they are converted to lower cases. 4.3. Feature representation
When the above preprocess is completed, each paper comprises tags and abstract. Tags are similar to the keywords of a paper; while abstract is similar to the summary of a paper. This study employs four combinations of tags and abstract words to represent papers, each of which uses the words appearing in tags and abstract to construct a feature vector V . This study ulary of tags is V t = h t 1 , ... , t j T j i .

Words Only: The feature vector only takes into account the words appearing in the abstract without using the tags. Thus, Meanwhile, V is normalized as a unit vector.

Tags Only: Similar to  X  X  X ords only X  X , only the tags are used to construct a feature vector. Thus, the feature vector can be vector.

Words + Tags : The words appearing in abstract are used to construct a word feature vector; while the tags are used to con-struct a tag feature vector. Furthermore, different weight ratios are given to tags and abstract words for further perfor-mance evaluation. Paper feature is the concatenation of word feature vector and tag feature vector. Thus, feature V is normalized as a unit vector.

Tags as Words: Analogous to words only, except the vocabulary is V w [ V t rather than V w . Different weightings are given to the term frequency of  X  X  X achine X  X  will be 1 + 2 n . Similarly, paper feature vector should be normalized as a unit vector. 5. Experiments
This study used two CiteULike data sets to evaluate system performances. Additionally, three unsupervised algorithms and one semi-supervised clustering algorithm called Constrained-KMeans ( Basu et al., 2002 ) are applied to the data sets to compare with our approach. The Constrained-PLSA and Constrained-KMeans are semi-supervised clustering methods, explaining why this study uses a small amount of labeled examples in the experiments. This study randomly selected exam-ples as the labeled ones and the rest of examples are unlabeled examples. Since the experiments focus on the issue whether a small amount of labeled examples can improve system performances, only 1% of the examples are labeled ones. To make experiments more objective, the labeled ones are excluded from system performance evaluations. Each evaluation runs ten times and the average of the results becomes its performance result. This kind of evaluation is a time-consuming process, but it can be more objective, since the selection of labeled examples is a random process and the seeds for unsupervised learning methods are also determined randomly. 5.1. Evaluation measurements
This study compared the generated clusters by using the F1 cluster evaluation measure ( Manning, Raghavan, &amp; Schtze, 2008 ). The F1 cluster evaluation measure considers both precision and recall, where precision and recall here are computed over pairs of documents for which two label assignments either agree or disagree. The F1 cluster evaluation measure is also used by Ramage et al. (2009) . Four evaluation metrics are necessary for the computation.

True Positives (TP): The clustering algorithm placed the two articles in a pair into the same cluster, and data corpus has them in the same class.

False Positives (FP): The clustering algorithm placed the two articles in a pair into the same cluster, but data corpus has them in differing classes.

True Negatives (TN): The clustering algorithm placed the two articles in a pair into differing clusters, and data corpus has them in differing classes.

False Negatives (FN): The clustering algorithm placed the two articles in a pair into differing clusters, but data corpus has them in the same class.
 Similar to traditional information retrieval definition, Eq. (6) shows the formulas of precision, recall and F1 evaluation. 5.2. Comparison methods
PLSA Clustering Model: The PLSA clustering model is evaluated in the experiments. Since the Constrained-PLSA is an exten-sion of PLSA clustering model, these experiments can be used to demonstrate how much performance can be improved using Constrained-PLSA with a small amount of labeled examples.

Constrained-PLSA: The Constrained-PLSA algorithm is mentioned above. The Constrained-PLSA uses labeled documents for two purposes. First, the labeled documents can provide a better initial guess for the algorithm to cluster the documents.
Second, these labeled examples can bias clustering toward a better searching space during the course of clustering ( Basu et al., 2002 ). Intuitively, the Constrained-PLSA can benefit from labeled documents to make the learning more fast and effective.
 crete data such as text corpora. Similar to PLSA, LDA considers each document as a mixture of topics, but LDA is a fully generative approach to language modeling which overcomes the inconsistent generative semantics of PLSA by treating the topic mixture weights as a K -parameter hidden random variable rather than a large set of individual parameters. Thus, the number of parameters in LDA does not grow with the size of the training corpus. Girolami and Kab X n (2003) have shown that PLSA is a maximum a posteriori estimated LDA model under a uniform Dirichlet prior. In LDA, the posterior distribution is intractable for exact inference. This study employs Gibbs sampling for the inference. Griffiths and Steyvers (2004) have shown that Gibbs sampling requires little memory and is competitive in speed and performance with exist-ing algorithms.

MM-LDA: The MM-LDA proposed by Ramage et al. (2009) extends LDA to jointly account for words and tags as distinct sets of observations. The MM-LDA generates a collection of tagged documents from K topics. The word generation process is equivalent to standard LDA. Meanwhile, the MM-LDA constructs distributions of tags per topic analogously to the con-struction of the word distributions per topic.

Constrained-KMeans: Basu et al. (2002) proposed two semi-supervised variants of KMeans clustering that use initial labeled data for seeding. These two algorithms are Seeded-KMeans and Constrained-KMeans. Their experimental results showed that Constrained-KMeans outperforms Seeded-KMeans. Meanwhile, Constrained-KMeans also outperforms COP-
KMeans ( Wagstaff et al., 2001 ). Thus, Constrained-KMeans is evaluated and compared with Constrained-PLSA in the experiments. 5.3. Tagged document clustering experiment
As mentioned above, this study collected the post information from CiteULike web site. Each post comprises paper meta-data and tags annotated by users. The category information of these posts are determined by Microsoft X  X  academic search service. Three fields are used for system evaluations, including database, graphics and programming language. Table 1 shows the journals under the three fields and statistical information for each field. The journal selection criterion is based on the number of papers tagged by CiteULike users, since some journals only have a few papers tagged by CiteULike users.
The first experiment focuses on  X  X  X ords only X  X  and  X  X  X ags only X  X  cases. Table 2 shows the experimental results. In this exper-iment, MM-LDA works just like LDA, since only word or tag information is available. Thus, LDA is used in this experiment. The second experiment focuses on  X  X  X ords + tags X  X  case. As described above, different weighting ratios are given to words and tags. Table 3 shows the experimental results, where nine combinations are used. The third experiment focuses on  X  X  X ags as words X  X  experiment. In this experiment, tags are thought of as the words appearing in the abstracts, but tags are given dif-ferent weights for the evaluation. Table 4 shows the experimental results, where tag weight is given from 1 to 20.
Moreover, this study further filtered some journals listed in Table 1 to create another data set for further analysis. The goal is to create a data set whose boundaries among clusters are clear. The filtering criterion is to remove the journals that are unrelated to the domain. Some journals are assigned to inappropriate categories by Microsoft academic search service. For instance, Mathematical Programming (MP) journal is under programming language, but it is a journal of the Mathemat-ical Optimization Society rather than programming language. Meanwhile, some journals X  focus may have been shifted. For instance, many papers in TKDE (IEEE Transactions on Knowledge and Data Engineering) focus on data mining or machine learning domains rather than database domain. Thus, database field only leaves the largest one, so the filtered data set can have clear boundaries among clusters. Table 5 shows the filtering result. The same experiments are conducted on the new data set. Tables 6 X 8 show the experimental results. 5.4. Discussion
Although the data sets used in the experiments are both coming from CiteULike, these two data sets possess different characteristics. The data set A employs the information obtained from Microsoft academic search service to assign paper cat-be considered as a data set with noisy information.

CiteULike Data Set A : In  X  X  X ords only X  X  and  X  X  X ags only X  X  experiments, Constrained-PLSA outperforms the other algorithms for most algorithms. Intuitively,  X  X  X ags only X  X  cannot provide enough information for the system to cluster the documents, since only a few tags are available for each paper. According to our analysis, one of the reasons is that CiteULike is designed specifically for researchers, who collect these papers for their own reference purposes. Thus, researchers can annotate these papers with exact tags. Moreover, noisy information exists in data set A and noisy information affects clus-tering performance. Unsupervised learning algorithms fail to function properly using  X  X  X ags only X  X  or  X  X  X ords only X  X  approaches, but Constrained-PLSA can improve clustering performance with a small amount of labeled examples.
In the second experiment, word and tag are given different weight ratios. Table 3 shows that Constrained-PLSA can gen-erally outperform the other algorithms. The clustering performances for MM-LDA and Constrained-PLSA can be the best when the weight ratio is  X  X 8:2 X  X .

In  X  X  X ags as words X  X  experiment, tags are the same as words, but given different weights. As shown in Table 4 , the Con-strained-PLSA outperforms the other algorithms. Moreover, the experimental results also indicate that Constrained-PLSA,
PLSA clustering model, and LDA can benefit from tags and their performances can be improved significantly as compared with  X  X  X ords only X  X  representation scheme.
 CiteULike Data Set B : In  X  X  X ords only X  X  and  X  X  X ags only X  X  experiments, Constrained-PLSA outperforms the other algorithms.
This data set is filtered manually, so it is easier to be clustered. All of the algorithms except Constrained-KMeans can func-tion properly in  X  X  X ords only X  X  experiment. In general, the number of words in an abstract is between 150 and 200. Stop word removal process removes many common words, so only a few words are left. The experimental results show that abstract information can provide enough information for the system to cluster documents, when the boundaries among clusters are clear. Meanwhile,  X  X  X ords only X  X  can work much better than  X  X  X ags only X  X  for all of the algorithms except Con-strained-KMeans. Although data set B is easier to be clustered, the experimental results present that  X  X  X ag only X  X  fails to provide adequate information for the system to cluster tagged documents. In other words, tags by themselves are weak at clustering paper data and this result conforms to many researchers X  experimental results ( Brooks &amp; Montanez, 2005;
Hayes et al., 2007; Ramage et al., 2009 ). The Constrained-KMeans fails to function properly in the experiments due to high dimensionality and sparsity problems. The Constrained-KMeans is an extension of K -means, which generally uses Euclid-ean distance as distance metric. It is apparent that each tagged document is in a high-dimensional and sparse space, but Euclidean metric may be inappropriate for determining similarity in a high-dimensional space ( Qi, Tang, Zha, Chua, &amp; Zhang, 2009 ).

In the second experiment, word and tag are given different weight ratios. Table 7 indicates that Constrained-PLSA can outperform the other algorithms. When the weight ratio is  X  X 9:1 X  X , PLSA clustering model, Constrained-PLSA and MM-
LDA can get the best performances. Although Constrained-PLSA is derived from PLSA clustering, the experimental results also indicate that the Constrained-PLSA with a small amount of labeled documents can significantly improve clustering performance. Besides, all of the algorithms can benefit from tags, but appropriate tag weight is required. When the tags dominate the clustering, the results will be similar to  X  X  X ags only X  X  cases.
 In the third experiment, tags are equivalent to words, but given different weights. As shown in Table 8 , the Constrained-
PLSA outperforms the other algorithms. The Constrained-PLSA can get the best performance when the weight of tag is given 3; while PLSA clustering and LDA can get the best performances when the weight of tag is given 1.

The experimental results indicate that PLSA, LDA and Constrained-PLSA can benefit from tags and their performances can be the best under  X  X  X ags as words X  X  representation scheme. Moreover, the performance of  X  X  X ags as words X  X  representation scheme is more stable than  X  X  X ords + tags X  X  representation scheme. The main difference between the two representation schemes is that the tags are represented as a separated tag vector in  X  X  X ords + tags X  X  representation scheme; while tags are equivalent to abstract words with different weights in  X  X  X ags as words X  X  representation scheme. In  X  X  X ords + tags X  X  repre-sentation scheme, the tag vector for each paper is a sparse vector, since the average number of tags for each paper is few. Thus, clustering performances of  X  X  X ords + tags X  X  representation scheme are unstable and may be worse than those of  X  X  X ords only X  X . On the other hand, abstract words can provide adequate information for tagged document clustering, and tags can highlight those important words in  X  X  X ags as words X  X  representation scheme. Cross validation scheme can be applied to the above two representations to determine the weight of tag. Practically, the data set may contain noisy information, the experimental results show that Constrained-PLSA can function properly and stably with a small amount of labeled examples. If the boundaries among clusters are very clear, unsupervised learning methods can work well, and
Constrained-PLSA still outperforms the other methods. 6. Conclusion
This study employs Constrained-PLSA to cluster tagged documents with a small amount of seeds. Additionally, the system discovers how to utilize tags in document analysis tasks. In feature representation, four combinations of tags and abstract words are used to represent possible situations. The data sets used in the experiment are academic paper information col-lected from CiteULike. Besides abstract, each paper in CiteULike comprises the tags annotated by users. Practically, tags are similar to the keywords of a paper; while abstract is similar to the summary of a paper. They can be considered as condensed information of a paper. To further analyze system performances on the data sets with and without noisy information, two data sets are used in the experiment. The experimental results show that almost all of the methods can benefit from tags.
However, unsupervised learning methods fail to function properly in the data set with noisy information, but Con-strained-PLSA function properly and stable even though only a small amount of labeled data is available. Conversely, unsu-pervised learning methods can work well on the data set without noisy information, and the Constrained-PLSA still outperforms the other methods. In many real applications, background knowledge is ready, making it appropriate to employ background knowledge to make the learning more fast and effective.
 Acknowledgment This work was supported in part by the National Science Council under the Grants NSC-100-2221-E-009-129 and NSC-100-2811-E-009-024.
 References
