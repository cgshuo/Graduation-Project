 Outsourcing the training of support vector machines (SVM) to external service providers benefits the data owner who is not familiar with the techniques of the SVM or has limited computing resources. In outsourcing, the data privacy is a critical issue for some legal or commercial reasons since there may be sensitive information contained in the data. Existing privacy-preserving SVM works are either not applicable to outsourcing or weak in security. In this paper, we propose a scheme for privacy-preserving outsourcing the training of the SVM without disclosing the actual content of the data to the service provider. In the proposed scheme, the data sent to the service provider is perturbed by a random trans-formation, and the service provider trains the SVM for the data owner from the perturbed data. The proposed scheme is stronger in security than existing techniques, and incurs very little redundant communication and computation cost. H.2.7 [ Database Management ]: Database Administra-tion X  Security, integrity, and protection ; H.2.8 [ Database Management ]: Database applications X  Data mining Algorithms, Experimentation, Security Privacy-preserving data mining, classification, outsourcing, support vector machines
Data mining is an important and widely used information technology. The organization which collects the data may not be expert in data mining techniques, or may not pos-sess sufficient computing resources to perform data mining since the data mining algorithms are usually computation-ally intensive. With only limited computing resources, per-forming data mining tasks for large-scale data may consume a lot of time. Outsourcing the data mining tasks to third-party service providers who are professional in this area is a reasonable choice for the data owner. Major IT compa-nies like Google and Microsoft are constructing infrastruc-tures to run Software as a Service (SaaS). This benefits small companies to run larger applications in the cloud-computing environment. Compared with performing data mining tasks in-house, outsourcing will save much hardware, software and personnel investments of the data owner, and then the data owner can focus more on its core business.

Data privacy is a concern in outsourcing the data mining tasks since the external service providers may be malicious or compromised. The data is a valuable asset of the data owner. The interest of the data owner will be hurt if the data are leaked to its commercial competitors. Leaking the data may even violate the laws if the personal information is contained. Therefore, for legal or commercial reasons, the content of the data needs to be protected when outsourcing the data mining tasks.

The support vector machine (SVM) [28] is a classification algorithm which yields state-of-the-art performance. There have been studies for outsourcing the SVMs by geometri-cally transforming the data with rotation and/or transla-tion [6,7], which transforms data to another vector space to perturb the content of instances but preserves the dot prod-uct or Euclidean distance relationships among all instances. Since the SVMs with common kernel functions depend only on the dot products or Euclidean distance among all pairs of instances, the SVMs can be derived from the rotated or translated data without hurting the performance. However, the preservation of the dot product or Euclidean distance relationships is a weakness in security. For n -dimensional data, if the attacker knows n or more linearly independent instances from some external information sources, it can re-build the mappings of the leaked instances and the trans-formed ones by distance inference, and then all transformed instances can be recovered by setting n equations. The work of [7] defended this weakness by adding Gaussian noise to degrade the preserved distance, but this contradicts the ob-jective of the rotational/translational transformation which aims to preserve the necessary utility of data for training the SVM.

Most existing privacy-preserving SVM works do not ad-dress the privacy issues for outsourcing the SVM training. The works of [13, 20, 21, 27, 31, 32] focused on how to train SVMs from the data partitioned among different parties without revealing each one X  X  own data to others. The work of [18] considered the privacy issues of releasing the built SVM classifiers. These privacy-preserving SVM works can-not be applied to protect the data privacy in outsourcing the SVM training to external service providers.

In this paper, we discuss the data privacy issues in out-sourcing the SVM training, and design a scheme for train-ing the SVM from the data perturbed by random transfor-mation. Unlike the geometric transformation, the random transformation transforms the data to a random vector space which does not preserve the dot product and Euclidean dis-tance relationships among instances, and hence is stronger in security . The proposed scheme enables the data owner to send the perturbed data to the service provider for outsourc-ing the SVM training without disclosing the actual content of the data, where the service provider trains SVMs from the perturbed data. Since the service provider may be un-trustworthy, the perturbation protects the data privacy by avoiding unauthorized accesses to the sensitive content. Not only the data privacy should be protected, the resulted clas-sifier is also the asset of the data owner. The classifier gen-erated from the perturbed data is also in perturbed form, which can only be recovered by the data owner. The service provider cannot use the perturbed classifier to do testings except testing the perturbed data sent from the data owner for outsourcing the testing. Figure 1(a) shows the applica-tion scenario of outsourcing the SVM training with privacy-preservation. The data owner sends perturbed training data to the service provider, and then the service provider trains the SVM from the perturbed training data for the data owner. The proposed scheme not only allows to solve an SVM problem from the perturbed training data, but the whole parameter search process by cross-validation [10, 23] for choosing an appropriate parameter combination to train the SVM. The testing can also be outsourced to the service provider. The data owner can send the perturbed testing instances to the service provider for privately outsourcing the testing as shown in Figure 1(b).

In privacy-preserving outsourcing the data mining, the additional computational cost imposed on the data owner should be minimized and the redundant communication cost should also not be too much, or the data owner rather per-forms data mining by itself than outsourcing. In the pro-posed scheme for privacy-preserving outsourcing the SVM training, the redundant communication cost is about 10% of the original data size. The data sent to the service provider is perturbed by a random linear transformation. Since the linear transformation can be executed efficiently, it incurs very little computational load to the data owner.
The rest of this paper is organized as follows: In Section 2, we survey related works and review the SVM for prelimi-naries. Section 3 describes our scheme of training the SVM from randomly transformed data for privacy-preserving out-sourcing the SVM training. Then in Section 4, we consider the outsourcing of the testing. Section 5 analyzes the com-plexity and security of the proposed scheme. Section 6 shows the experimental results. Finally, we conclude the paper in Section 7.
In this section, we survey the related works of privacy-preserving data mining, and review the SVM to give the preliminary of this work.
Privacy-preserving data mining techniques can be roughly classified into two major categories: releasing modified data for data mining [1, 3, 8], and cooperative data mining on distributed private data [12,26].

The purpose of the techniques of data mining on dis-tributed private data is for exchanging necessary information among parties to build data mining models on the whole data without revealing the actual content of each party X  X  own data to others. For example, the works of [12, 26] de-signed protocols for privacy-preserving association rule min-ing on the data partitioned among different parties, and the work of [19] considered for building decision trees. Most existing privacy-preserving SVM works [13,20,21,27,31,32] also belong to this category.

The techniques of releasing modified data for data mining is to let others build data mining models without revealing actual data. A popular approach for releasing modified data for data mining is perturbing the data by adding random noise. The data are individually perturbed by adding noise randomly drawn from a known distribution, and data mining algorithms are performed from the reconstructed aggregate distributions of the noisy data [3, 8]. The work of [3] ad-dressed learning decision trees, and the work of [8] addressed mining association rules. A condensation-based approach is proposed in [1], where data are first clustered into groups, and then pseudo-data are generated from those clustered groups. Data mining tasks are then done on the generated synthetic data instead of the original data. The anonymous data publishing technique like k -anonymity [22, 25], which makes the quasi-identifier values be able to indistinguish-ably map into at least k -records by generalizing or suppress-ing the values in quasi-identifier attributes, has also been successfully used in data mining. For example, the work of [11] studied the performance of the SVM built upon the anonymized data and the anonymized data with additional statistics of the generalized attributes.

The techniques of releasing modified data for data min-ing can partly fulfill the objective of privacy-preserving out-sourcing data mining tasks, which aims to let external ser-vice providers build data mining models for the data owner without revealing actual data. An issue of using such tech-niques for outsourcing is the data privacy since the modi-fied data still disclose the content in degraded precision or anonymized form. The other issue is that the built data mining models are also possessed by the service provider if such techniques are adopted. The main difference is that the privacy-preserving outsourcing data mining also requires protecting the access of the built data mining models. These issues of outsourcing the data mining are seldom addressed in the literature of privacy-preserving data mining.
The work of [29] designed a scheme for privacy-preserving outsourcing the association rule mining, in which the items in the database are substituted by ciphers. The cipher-substituted items protect the actual content of data but preserve the counts of itemsets for performing association rule mining. The association rules obtained by the service provider are also in ciphered form, which can only be utilized by the data owner itself.

The works of [6,7] considered the privacy-preserving out-sourcing of the SVM by utilizing geometric transformations to hide the actual content but preserve the dot product or Euclidean distance relationships of data for outsourcing the SVM. The built SVM classifiers from the transformed data are also in transformed form.

Except the works of geometric perturbation [6,7], existing privacy-preserving SVM works do not address the outsourc-ing of the SVM. The work of [18] considered the problem of releasing a built SVM classifier without revealing the sup-port vectors, which is a subset of the training data. The methods proposed in [13, 20, 21, 27, 31, 32] designed proto-cols to train the SVM on the training data separately held by different parties without revealing data to each other. In the works of [27, 31, 32], training data owners coopera-tively compute the Gram matrix to build the SVM problem without revealing their each own data to others by utilizing the secure multi-party integer sum. The work of [13] uti-lizes cryptographic techniques to design a private two-party kernel adatron algorithm for training the SVM. The works of [20,21] adopt the reduced SVM [15] with random reduced set for cooperatively building the kernel matrix among par-ties. The above-mentioned privacy-preserving SVM works are not applicable for a data owner to send the whole dataset to a single service provider to train the SVM with privacy-preservation.

To the best of our knowledge, currently only the geomet-ric transformation-based schemes of [6, 7] are applicable to privacy-preserving outsourcing the SVM. However, trans-forming the data by geometric transformation is weak in security as we have mentioned in Section 1.

Our scheme also utilizes the RSVM with random vectors as the reduced set, which has been used in [20, 21] for pri-vately computing the kernel matrix on the data partitioned among different parties. However, the way the RSVM is employed with random vectors in our work is different from [20,21]. In our privacy-preserving SVM outsourcing scheme, we capitalize on random vectors to prevent the inherent weakness in the full kernel matrix and enable the computa-tion of the kernel matrix from randomly transformed train-ing data.

There have been many works considering for outsourcing the database queries with privacy-preservation. For exam-ple, the works of [2,9] proposed schemes for performing com-parison operations and SQL queries on encrypted databases, and the work of [30] proposed a scheme for performing k -nearest neighbor queries on randomly transformed database. Outsourcing data mining with privacy-preservation is less discussed since there are usually complex operations involved in data mining algorithms.
The SVM is a statistically robust learning method based on the structural risk minimization of the statistical learning theory [28]. The objective of the SVM is to find an optimal separating hyperplane which maximizes the margin between two classes of data in the kernel induced feature space.
Without loss of generality, suppose there are m instances of training data. Each instance consists of a ( x i ; y i where x i  X  R n contains n attributes of the i -th instance and y  X  X  +1 ;  X  1 } is its class label. The SVM finds the optimal separating hyperplane w  X  x + b = 0 to obtain the decision function f ( x ) = w  X  x + b by solving the following quadratic programming optimization problem: subject to y i ( w  X  x i + b )  X  1  X  i , i  X  0, i = 1 ; :::; m Minimizing 1 2 || w || 2 in the objective function means maxi-mizing the margin between the two classes of data. The slack variable i denotes the extent of x i falling in the erro-neous region, for i = 1 ; : : : ; m , and C &gt; 0 is the parameter which denotes the cost of slacks. The objective function is a trade-off between maximizing the margin and minimizing the slacks. The resulted classifier is sgn( f ( x )) for deter-mining which side of the optimal separating hyperplane the testing instance x falls into.

The SVM X  X  optimization problem is usually solved in its dual form, which makes available the application of the ker-nel trick : where Q is called kernel matrix with Q i;j = y i y j k ( x form is f ( x ) = is called kernel function , which implicitly maps x and y into a high-dimensional space and computes their dot product there. By applying the kernel trick, the SVM maps data into the kernel induced high-dimensional space to find an optimal separating hyperplane. The original dot product is called linear kernel k ( x ; y ) = x  X  y . Commonly used kernel functions include Gaussian kernel k ( x ; y ) = exp(  X  g || y || 2 ) with g &gt; 0, polynomial kernel k ( x ; y ) = ( g x r ) d with g &gt; 0, and the neural network kernel k ( x ; y ) = tanh( g x  X  y + r ), where g , r , and d are kernel parameters.
Without appropriate choices on the cost and kernel pa-rameters, the SVM will not achieve good classification per-formance. A process of parameter search (also called model selection) is required to determine a suitable parameter com-bination of the cost and kernel parameters for training the SVM. Practitioners usually evaluate the performance of a parameter combination by measuring its average accuracy of cross-validation [10,23], and the search of parameter combi-nations is often done in a brute-force way. For example, with the Gaussian kernel, the guide of LIBSVM [5, 10] suggests performing a grid-search with exponential growth on the combinations of ( C; g ). The parameter combination which results in the highest average cross-validation accuracy will be selected to train an SVM classifier on the full training data.

The parameter search process can be very time-consuming since there are usually hundreds of parameter combinations to try, and for each parameter combination, with k -fold cross-validation, there are k SVMs to be trained on k  X  1 of the training data. For example, the default search range of LIBSVM X  X  parameter search tool for Gaussian kernel is C = 2  X  5 to 2 15 and g = 2  X  15 to 2 3 , both stepped in 2 where 5-fold cross-validation is used. There are 11  X  10 = 110 parameter combinations to be tested in its default setting, and hence there are totally 5  X  110 = 550 SVMs to be trained in the parameter search process.

If the training dataset is large, training an SVM is already costly, and the parameter search process even involves train-ing hundreds of SVMs along with hundreds of testings. Due to the heavy computational load, for a data owner who has only limited computation resource, it is reasonable to out-source the SVM training to an external service provider. It is noted that the SVM problems in the parameter search process are independent, and hence the parameter search process can be easily parallelized in cloud computing envi-ronment. Since the service provider may be untrusted or malicious, the actual content of the data needs appropriate protections to preserve the data privacy.
In this section, we introduce the scheme of training SVMs from random transformation-perturbed data for outsourcing the SVM training to a potentially malicious external ser-vice provider. Our work overcomes the security weakness of the geometric transformation by perturbing data with ran-dom transformation, which does not preserve the Euclidean distance and dot product relationships among the training instances, and hence provides stronger security in data pri-vacy. Note that the full kernel matrix Q of the conventional SVM problem (2) contains dot products or Euclidean dis-tance among all training instances for common kernel func-tions. If the service provider needs to solve SVM problems from the full kernel matrix, there will be similar security weakness like the rotationally/translationally transformed training data.

We tackle this problem by adopting the reduced SVM (RSVM) with random reduced set [14,15,20,21] for training SVMs from a secure kernel matrix. The RSVM is a SVM scaling up method, which utilizes a reduced kernel matrix . Each element of the reduced kernel matrix is computed from an instance in the training dataset and an instance in the reduced set . The number of instances in the reduced set is typically less than 10% of the training dataset [14, 15]. Hence the reduced kernel matrix is much smaller than a full kernel matrix and can easily fit into the main memory. Let x  X  R n , i = 1 ; : : : ; m stand for the instances of the training dataset, and y i  X  X  1 ;  X  1 } , i = 1 ; : : : ; m are their correspond-ing labels. Let R = { r j | r j  X  R n ; j = 1 ; : : : ;  X  m the reduced set, where  X  m &lt;&lt; m . The original RSVM pa-per adopted a subset of the training dataset as the reduced set [15]. The reduced kernel matrix K is an m  X   X  m matrix where An interesting property of the RSVM is that the reduced set R is not necessary to be a subset of the training dataset [14]; completely random vectors can act as the instances of the reduced set [20,21].

If random vectors are adopted, then an element in the reduced kernel matrix is the kernel evaluation between an instance in the training dataset and a random vector in the reduced set but not the kernel evaluation between the train-ing instances like the kernel matrix of a conventional SVM. Hence the reduced kernel matrix does not contain the dot product or distance relationships of the training instances. This forms the secure kernel matrix , which avoids the secu-rity weakness from disclosing the kernel matrix of a conven-tional SVM. As long as the random vectors are kept secret, even if part of training instances are leaked, other instances still cannot be derived from the secure kernel matrix.
If the service provider has the secure kernel matrix, it can solve the following RSVM problem [14,15]: where i , i = 1 ; : : : ; m are slack variables, and C is the cost parameter. The solutions v = ( v 1 ; : : : ; v m ) T , b , and the vectors of the reduced set constitute the decision function The optimization problem of the RSVM can be solved by a normal linear SVM solver [17] or the smooth SVM [16] used in the original RSVM paper [15]. Empirical studies of [14, 15, 17] showed that the RSVM can achieve similar classification performance to a conventional SVM.
Note that simply sharing the secure kernel matrix is not appropriate for outsourcing the SVM training since a re-duced kernel matrix is computed from a fixed kernel pa-rameter, while there are various SVMs with different kernel parameters to be trained in the parameter search process. The data owner will be imposed much computation load as well as much communication cost to build and send many se-cure kernel matrices with different kernel parameters to the service provider. Our goal is to outsource the SVM train-ing which must minimize as much load of the data owner as possible.
In the following, we show a random transformation-based data perturbation scheme which makes available the compu-tation of the secure kernel matrix from perturbed training data. The computational cost of the random transformation is similar to geometric perturbation methods [6,7], but the random transformation does not preserve the dot product or distance relationships between training instances and hence is stronger in security.
The data owner will send the perturbed training instances as well as the perturbed random vectors of the reduced set to the service provider for computing secure kernel matrices. A secure kernel matrix K is computed from training instances and random vectors of the reduced set by K i;j = k ( x i ; r i = 1 ; : : : ; m , j = 1 ; : : : ;  X  m . Our objective is to let the service provider compute the same K from perturbed train-ing instances and random vectors, while the perturbation scheme should not allow the security weakness of the geo-metric perturbation schemes, i.e., the dot product and Eu-clidean distance among training instances should not be pre-served. The perturbation scheme needs to preserve the ker-nel evaluations between a training instance and a random vector for computing secure kernel matrices. We utilize the random transformation perturbation for computing the dot product of two differently transformed instances [30]. Note that the attribute vectors x i  X  X  of training instances are usu-ally considered sensitive, but the class labels y i  X  X  are usually not.

Let M be a nonsingular n  X  n matrix composed of random values. We perturb the instances of the training dataset by a random linear transformation L : R n  X  R n , where the matrix M works as the random linear operator. All training instances are perturbed by the random transformation 1
Unlike the geometric perturbation, the random transfor-mation does not preserve the Euclidean distance and dot products between training instances since the vector space is randomly transformed. Hence the security weakness of the rotational or translational transformation does not exist in the data perturbed by random transformation. The random by another random linear transformation L  X  : R n  X  R n with ( M T )  X  1 as the random linear operator:
The perturbed training instances c i , i = 1 ; : : : ; m and per-turbed random vectors of the reduced set s j , j = 1 ; : : : ;  X  m are then sent to the service provider for building secure ker-nel matrices.

The dot product between an instance x i and a random vector r j can be equivalently computed from the dot product of c i and s j by c T i s j = ( M x i ) T ( M T )  X  1 r j = x T i I r j = x T i r j . Therefore, for the dot product-based ker-nel functions including the linear kernel k ( x i ; r j ) = x polynomial kernel k ( x i ; r j ) = ( g x i  X  r j + r ) d uations between an instance and a random vector can be
It is not necessary to put the whole matrix M in the main memory. The computation can be decomposed to M x = x
M : ; 1 +  X  X  X  + x n M : ;n , where M : ;i is the i -th column of M . equivalently derived from the perturbed training instances and random vectors.

For Gaussian kernel k ( x i ; r j ) = exp(  X  g || x i  X  r is based on the Euclidean distance, a slight modification is needed to add another two dimensions to the original in-stances x i  X  R n as x  X  i = ( x i; 1 ; x i; 2 ; : : : ; x before applying the transformation. The random vectors r  X  X  of the reduced set are also added by another two dimen-corresponded random matrix for random transformation is a nonsingular ( n +2)  X  ( n +2) matrix M . Similarly, the data are perturbed by c i = M x  X  i , and the random vectors are per-turbed by s j = ( M T )  X  1 r  X  j . The Euclidean distance between x i and r j in the Gaussian kernel can be equivalently com-puted from c i and s j by  X  2 c T i s j =  X  2 x  X  T i M T  X 
For common kernel functions, the kernel evaluations be-tween an instance in the training set and an instance in the reduced set can be equivalently computed from their per-turbed versions. Hence the service provider can derive the same secure kernel matrix K of (4) from the perturbed data c  X  X  and s j  X  X , i.e., the RSVM optimization problem derived from the perturbed data is the same to the one derived from original data. Therefore, the service provider can obtain the same solutions v j , j = 1 ; : : : ;  X  m and b of the decision function (5).
The parameter search by cross-validation involves training many SVMs on subsets of training instances with different parameter combinations. The service provider can perform this by building secure kernel matrices on subsets of the perturbed training instances with different kernel parame-ters, and then forming the RSVM problems with different parameter combinations to train intermediate classifiers for evaluating the performance by cross-validation.

An essential part in the cross-validation is testing for mea-suring the accuracy of classifying remaining part of the per-turbed training instances by the intermediate classifiers. Note that the elements of the reduced set r j , j = 1 ; : : : ;  X  m work as support vectors in the decision function (5). Since the ser-vice provider only has perturbed reduced set s j = ( M T ) j = 1 ; : : : ;  X  m , the classifier obtained by the service provider is also in a perturbed form:
To use this classifier for testing, the testing instances also need to be perturbed by the same random transformation of the training instances, which implies that the intermedi-ate classifiers can classify the perturbed training instances possessed by the service provider. For example, with the lin-f ( x ) classifies a perturbed training instance c i = M x i The Gaussian kernel, polynomial kernel, and neural network kernel can be tackled in similar ways like the step of deriv-ing secure kernel matrices from perturbed data. Classifying the perturbed training instance c i = M x i with the per-turbed decision function f  X  ( x ) is equivalent to classifying the original training instance x i by the unperturbed deci-sion function f ( x ). Hence the service provider can perform cross-validations to evaluate the performance of intermedi-ate classifiers by training and testing on perturbed training instances.

It is noted that the service provider cannot use the per-turbed decision function to classify the data other than the ones sent from the data owner. Since the perturbed deci-sion function f  X  ( x ) requires the testing instance being trans-formed to M x to annihilate the effect of ( M T )  X  1 multiplying to r j  X  X , if the testing instance is not perturbed by M , the perturbed decision function f  X  ( x ) will result in an unmean-ingful value. Hence for the service provider, the applicabil-ity of the perturbed decision function is restricted to classify only the data sent from the data owner for performing cross-validation. This protects the classifier built from the data of the data owner.

The following summarizes the whole scheme of privacy-preserving outsourcing the SVM training based on random transformations. The data owner generates random vectors as the reduced set for the RSVM, perturbs the training in-stances by a random matrix M , perturbs the random vectors by ( M T )  X  1 , and then sends the perturbed training instances and perturbed random vectors to the service provider. Upon receiving the perturbed data, the service provider performs the cross-validation-based parameter search process by com-puting secure kernel matrices from part of perturbed train-ing instances to train intermediate classifiers and using the intermediate classifiers to test on remaining perturbed train-ing instances for evaluating the performance of various pa-rameter combinations. The parameter combination which achieves the best average cross-validation accuracy will be adopted to form a final RSVM problem on complete per-turbed training instances, and then the solutions v j , j = 1 ; : : : ;  X  m and b of the final problem along with the adopted kernel parameter(s) will be returned to the data owner for composing a decision function with the original random vec-tors.

The random transformations are utilized to protect the data privacy in the initial stage before forming the SVM problem, and the RSVM with random vectors as the reduced set is utilized to prevent the security weakness in the inter-mediate computations. This two-layer protection overcomes the security weakness existing in the conventional SVM for-mulations and the geometric perturbation-based schemes [6, 7] for outsourcing the SVM training with privacy-preservation.
The perturbed decision function f  X  ( x ) in (8) also enables privacy-preserving outsourcing of the testing tasks. Since classifying the perturbed instance M x by the perturbed de-cision function f  X  ( x ) is equivalent to classifying the original instance x by the original decision function f ( x ), the data owner can perturb the testing instances by the random ma-trix M the same with the one to perturb training instances, and then send the perturbed testing instances to the service provider for outsourcing the testing tasks.

Outsourcing the testing tasks is not limited to outsourc-ing to the service provider who generates the SVM classi-fier from perturbed training data. The testing task can be outsourced to another party by sending to whom the per-turbed decision function, including the perturbed reduced set, and the perturbed testing instances for performing test-ings. When outsourcing the testing task, the random matrix for perturbation can be changed for higher security concerns. It is done by perturbing the testing instances by another ran-dom matrix M  X  and perturbing the random vectors of the reduced set by corresponded ( M  X  T )  X  1 , i.e., the random ma-trix M for perturbation in outsourcing the testing can be ar-bitrarily changed by providing that the testing instances and random vectors are perturbed by the pair M and ( M T )  X  1 respectively.

Although the decision function of the SVM is very sim-ple and can be computed fast, since there are slower ex-ponent computations involved in common kernel functions, randomly transforming the testing data for outsourcing the testing is still able to save some computational load of the data owner. We will validate this claim in the experiment.
Compared to simply outsourcing the SVM training with-out protecting the data, the additional computational cost imposed on the data owner for protecting data privacy in-cludes: (1) Generating an n  X  n random matrix for pertur-bation, which costs O ( n 2 ). (2) Perturbing m n -dimensional training instances by matrix multiplications, which costs O ( m  X  n 2 ). (3) Generating and perturbing  X  m random vectors, which costs O (  X  m  X  n 2 ). Hence the totally additional computa-tional cost imposed on the data owner is O (( m +  X  m +1) n O ( mn 2 ). For outsourcing the testing, the additional compu-tational cost is to perturb the testing instances, which costs O ( n 2 ) for each instance.

Compared to the geometric transformation-based scheme, applying the rotational transformation costs O ( mn 2 ), and applying the translational transformation vector costs O ( mn ). Hence the additional computational cost of using the geo-metric transformation is about O ( mn 2 ), which is similar to using our random transformation-based scheme.

The communication cost of sending the transformation-perturbed instances is same to sending the original instances, both are O ( mn ). The additional communication cost of us-ing our random transformation-based scheme is to send the perturbed random vectors, which incurs O (  X  mn ) redundant communication cost.

In the following, we consider the security issues. The dis-cussions consider the cases of dot product-based kernels. The Euclidean distance-based kernels can be applied in a similar way. Without loss of generality, consider the situa-tion that the service provider has known the content of m training instances from external information sources. (i) The service provider cannot recover the ran-(ii) The service provider cannot recover the con-(iii) The service provider cannot recover the remain-
The security of our scheme depends on the property of the random transformation-based perturbation, which does not preserve the dot product and Euclidean distance rela-tionships among the training instances and hence is resis-tant to the distance inference attack. For the geometric perturbation-based schemes [6,7], if n or more linearly inde-pendent instances are leaked, the correspondence between the original instances and the geometric-perturbed ones can be determined from the distance and dot product relation-ships, and then all other instances can be recovered by set-ting up equations.

The security of outsourcing the testing is similar. Since the service provider does not know the original content of the perturbed reduced set, it cannot recover the content of the perturbed testing instances from the dot product or Euclidean distance between the perturbed testing instances and the perturbed reduced set.
In the experiments, we first compare the the classification performance between a conventional SVM and the RSVM with random vectors as the reduced set to evaluate the effec-tiveness of the proposed scheme on classification. Then we measure the computational time imposed on the data owner of using our privacy-preserving outsourcing SVM scheme, and compare with the computational time of training the SVM locally to demonstrate the computational load saved from using the outsourcing scheme. Finally, we compare the classification performance with the SVM trained from the anonymized data since the anonymous data publishing technique [25] is suitable for revealing the datasets where only the identities of instances are concerned.

Since training SVMs on large datasets is very time con-suming, for the ease of experiments, we choose the datasets with moderate size for performing experiments. The dif-ference in the scale of consuming time between outsourcing and local training is clear to demonstrate the efficacy of our scheme. The datasets used in the experiments are avail-able at the UCI machine learning repository [4]. We select some medical datasets and bank credit datasets, which have stronger privacy concerns, to evaluate the effectiveness of the scheme. The medical datasets include Wisconsin breast cancer, Pima Indian diabetes, Liver disorder, Statlog heart disease, which contain medical records of patients. The bank credit datasets are Australian credit and German credit nu-meric version, in which personal information of bank cus-tomers is contained. The statistics of the datasets are shown in Table 1. The programs of random transformation and the RSVM are written in Matlab. The experimental platform is a PC featured with an Intel Core 2 Q6600 CPU and 4GB RAM, running Windows XP.

In this section, we compare the classification performance between a conventional SVM implementation, the LIBSVM [5], and the RSVM with random reduced set to show the effectiveness of our scheme on classification.

We solve the L2-norm RSVM problem (the (4) of Section 3.1) by the smooth SVM method [15,16,17]. Randomly gen-erated vectors are adopted as the reduced set for training the RSVM. The size of the reduced set is set to 10% of the size of the training dataset. The adopted kernel function in both the RSVM and the LIBSVM is the Gaussian kernel func-tion. The cost/kernel parameters for training the RSVM and LIBSVM are respectively determined by applying the grid search using cross-validation, where the search range is the default of LIBSVM X  X  parameter search tool [5,10].
Figure 2 shows the experimental results of comparing the classification performance. The reported accuracy is the 5-fold cross-validation average. It is seen that the classification accuracy of the RSVM with random vectors as the reduced set is similar to a conventional SVM, which validates that our scheme is effective for classification.
To demonstrate the benefits of outsourcing, we measure the computational overhead imposed on the data owner with Figure 2: Comparison of the classi cation accuracy between the RSVM with random reduced set and a conventional SVM. using the privacy-preserving outsourcing scheme, and com-pare it with the computational time of training the SVM locally by the data owner itself to show how much computa-tional cost can be saved from utilizing the privacy-preserving outsourcing scheme.

Table 2 shows the comparison of the required computing time of the data owner with and without utilizing the out-sourcing. The SVM training includes the parameter search process and training the final classifier by the selected pa-rameter combination. The search range adopted here is also the default of the LIBSVM X  X  parameter search tool [5, 10]. The training time of both the RSVM and the LIBSVM is listed for reference. Note that we do not aim to compare the training time between the two training methods since they are different implementations of the SVM. When using the outsourcing scheme for the dataset with m instances in n -attribute, to perturb the data, the data owner needs to generate an n  X  n random matrix for transformation, gener-ate  X  m= 10  X  random vectors in n -dimensional for the reduced set of the RSVM, and transform the m training instances and  X  m= 10  X  random vectors by matrix multiplication. It is seen that these computations can be executed very fast. On all of the datasets, they are done within 0.5 millisec-ond. However, locally training the SVM takes at least sev-eral seconds to complete, which costs the data owner more than 10,000 times of computing time than the outsourcing. The difference in the scale of computing time between out-sourcing and locally training is very large. This validates the claim that the proposed privacy-preserving outsourcing scheme incurs merely little computational overhead to the data owner. The computational load of the data owner can be significantly reduced, which clearly justify the efficacy of the privacy-preserving outsourcing scheme.
 Table 2: Time comparison of training SVMs with/without outsourcing Heart Disease 0.12 ms 2.9 s 6.5 s Breast cancer 0.14 ms 9.8 s 12.4 s
Liver disorder 0.07 ms 2.6 s 32.3 s
We measure the computing time of the perturbation with random transformation on large-scale synthetic datasets to evaluate the scalability. The number of the instances of the synthetic datasets ranges from 10000 to 50000, where the dimensionality of those datasets are in 500 and 1000-dimensional, respectively. The computing time of the ran-dom transformation is shown in Figure 3. It is seen that the random transformation scales well with the increase of instances. Randomly transforming 50000 instances in 1000-dimensional takes less than 5 seconds to complete. Figure 3: Computing time of perturbing data by random transformation.
The overhead imposed on the data owner for outsourcing the testing is randomly transforming the testing instances. We randomly generate 10000 testing instances for each dataset to compare the time of outsourcing testing and local test-ing, where classifiers of each dataset are the ones trained above. The results are reported in Table 3. It is seen that the outsourcing scheme can save tens to hundreds of times of computational load for the data owner. Since the SVM testing is already efficient, the difference between outsourc-ing the testing and the local testing is not as significant as the cases of training.
 Table 3: Time comparison of testing 10,000 in-stances with/without outsourcing Heart Disease 1.40 ms 43.29 ms 233.20 ms Breast cancer 1.23 ms 103.21 ms 113.17 ms
Liver disorder 0.39 ms 46.79 ms 430.28 ms
In this section, we compare the classification performance between the RSVM with random reduced set with the SVM classifiers trained from anonymous data anonymized by the k -anonymity technique [11,24]. If only the identities of data are concerned, the anonymous data publishing technique can be adopted for sending the anonymized data to service provider for outsourcing the SVM.
There are three of the above datasets containing quasi-identifier attributes: Statlog heart has { age, sex } , Pima Indian diabetes has { age, number of pregnant, body mass index } , and German credit has { purpose, credit amount, personal status and sex, present residence since, age, job Value generalization hierarchies are first built on the quasi-identifiers of each dataset, and then the Datafly algorithm [24] is performed to achieve k -anonymity. Since the SVM is a value-based algorithm, for numerical attributes, each generalized range is represented by the mean value, and for categorical data, the generalized category is represented by exhibiting all children categories [11]. The cost/kernel pa-rameters to train the SVMs from anonymized data are de-termined by grid-search using cross-validation.

The performance comparison between the RSVM with random reduced set and the SVMs trained from k -anonymized data with k = 32 and k = 128 is shown in Figure 4. The reported accuracy is 5-fold cross-validation average. On Ger-man credit dataset, the accuracy of applying the k -anonymity technique with k = 32 is similar to the RSVM with random reduced set, but it falls down when k = 128 due to the sev-erer distortion of the quasi-identifier values. On the Heart and Diabetes datasets, k = 32 is enough to significantly dis-tort their quasi-identifier values and thus results in lower accuracy. Figure 4: Classi cation performance comparison be-tween the RSVM with random reduced set and the SVMs trained from k -anonymized data.

It is seen that the distortion of quasi-identifiers to achieve k -anonymity will hurt the performance of the SVM, and the performance may get worse when a large k is applied for better identity protection. Compared with outsourcing the SVM by k -anonymity, our scheme hardly hurts the perfor-mance of the SVM, and provides better protection to the data privacy since all attributes are perturbed by the ran-dom transformation.
We propose a scheme for training the SVM from ran-dom transformation-perturbed data, which makes available the outsourcing of the SVM with privacy-preservation. The scheme overcomes the security weakness of existing works, and incurs very little overhead on the data owner. [1] C. C. Aggarwal and P. S. Yu. A condensation [2] R. Agrawal, J. Kiernan, R. Srikant, and Y. Xu. Order [3] R. Agrawal and R. Srikant. Privacy preserving data [4] A. Asuncion and D. Newman. UCI Machine Learning [5] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [6] K. Chen and L. Liu. Privacy preserving data [7] K. Chen, G. Sun, and L. Liu. Towards attack-resilient [8] A. Evfimievski, R. Srikant, R. Agrawal, and [9] H. Hac X g  X  um  X  u  X s, B. Iyer, C. Li, and S. Mehrotra. [10] C.-W. Hsu, C.-C. Chang, and C.-J. Lin. A practical [11] A. Inan, M. Kantarcioglu, and E. Bertino. Using [12] M. Kantarcioglu and C. Clifton. Privacy-preserving [13] S. Laur, H. Lipmaa, and T. Mielik  X  ainen.
 [14] Y.-J. Lee and S.-Y. Huang. Reduced support vector [15] Y.-J. Lee and O. L. Mangasarian. RSVM: Reduced [16] Y.-J. Lee and O. L. Mangasarian. SSVM: A smooth [17] K.-M. Lin and C.-J. Lin. A study on reduced support [18] K.-P. Lin and M.-S. Chen. Releasing the SVM [19] Y. Lindell and B. Pinkas. Privacy preserving data [20] O. L. Mangasarian, E. W. Wild, and G. M. Fung. [21] O. L. Mangasarian and T. Wild. Privacy-preserving [22] P. Samarati. Protecting respondents X  identities in [23] B. Sch  X  olkopf and A. J. Smola. Learning with Kernels: [24] L. Sweeney. Achieving k -anonymity privacy protection [25] L. Sweeney. k -anonymity: A model for protecting [26] J. Vaidya and C. Clifton. Privacy-preserving [27] J. Vaidya, H. Yu, and X. Jiang. Privacy-preserving [28] V. N. Vapnik. Statistical Learning Theory . John Wiley [29] W. K. Wong, D. W. Cheung, E. Hung, B. Kao, and [30] W. K. Wong, D. W. Cheung, B. Kao, and [31] H. Yu, X. Jiang, and J. Vaidya. Privacy-preserving [32] H. Yu, J. Vaidya, and X. Jiang. Privacy-preserving
