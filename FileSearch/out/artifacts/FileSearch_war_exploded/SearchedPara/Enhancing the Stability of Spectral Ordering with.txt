
Recent studies have demonstrated the prospects of data mining algorithms for addressing the task of seriation in pa-leontological data (i.e. the age-based ordering of the sites of excavation). A prominent approach is spectral ordering that computes a similarity measure between the sites and orders them such that similar sites become adjacent and dissimilar sites are placed far apart. In the paleontolog-ical domain, the similarity measure is based on the mam-mal genera whose remains are retrieved at each site of ex-cavation. Although spectral ordering achieves good per-formance in the seriation task, it ignores the background knowledge that is naturally present in the domain, as pa-leontologists can derive the ages of the sites of excavation within some accuracy. On the other hand, the age informa-tion is uncertain, so the best approach would be to combine the background knowledge with the information on mammal co-occurrences. Motivated by this kind of partial supervi-sion we propose a novel semi-supervised spectral ordering algorithm. Our algorithm modifies the Laplacian matrix used in spectral ordering, such that domain knowledge of the ordering is taken into account. Also, it performs feature selection (sparsification) by discarding features that con-tribute most to the unwanted variability of the data in boot-strap sampling. The theoretical properties of the proposed algorithm are thoroughly analyzed and it is demonstrated that the proposed framework enhances the stability of the spectral ordering output and induces computational gains.
In this paper we consider the task of ordering the obser-vations in the data, accompanied by partial supervision and sparsification 1 , aiming at a more stable ordering. Although it may initially seem surprising the we employ partial su-pervision and feature selection in a common framework, it is analytically demonstrated in the paper that each compo-nent addresses a di ff erent cause of instability of the results. In our context, stability refers to the variation of the end re-sult with respect to small changes in the data; in practice we will measure this by bootstrap sampling.

In distance based ordering, the task is to find a permuta-tion of objects such that similar objects become adjacent; in addition, the more dissimilar the objects are, the larger the order distance between them. The standard optimization problem formulation used for deriving the distance based ordering is known to be NP -hard [8], and spectral ordering [2, 4] presents a popular, algorithmically feasible approach for deriving approximate solutions. Despite the name  X  X r-dering X , the aim is not to rank the objects into any prefer-ence ranking, and the first and last object in the ordering are merely those that are maximally dissimilar to each other. Algorithmically, the order solution is derived by the eigen-vector corresponding to the second eigenvalue of the data Laplacian matrix.

Our main application area is paleontology: our obser-vations (objects, instances) are sites of excavation and our features (attributes, variables) are mammal genera whose remains are found at these sites. In addition, we have aux-iliary information on the estimated ages of the sites; this information is uncertain to some degree. Spectral order-ing of the sites of excavation can be based solely on the co-occurrences of the mammal genera, irrespective of the ages of the sites. It has been shown [6] that this kind of plain spectral ordering is a fast and standardized way of biochronological ordering of the sites. Albeit the favorable results in the biochronological ordering task, the spectral ordering does not take into account the background knowl-edge that naturally exists in the domain. The successful in-corporation of domain knowledge is expected to increase the quality of the results.
In the current study, we take advantage of the domain knowledge of the ages of the sites and combine that with the spectral ordering, ending up with a semi-supervised spec-tral ordering 2 . In addition, we consider feature selection. Towards this target the features that contribute most to the unwanted variation of the data (measured by bootstrap sam-pling) will be removed. These features correspond to mam-mals whose observations are noisy. The paleontological data is noisy in many respects [7]: the preservation, recov-ery and identification of fossils are all random to some ex-tent. These uncertainties are, however, hard to quantify, and a systematic way of characterizing the uncertainty would be most welcome  X  the behaviour of the features in bootstrap sampling is here chosen for this task.

The two components of the proposed framework, namely partial supervision and feature selection will make the re-sulting ordering more stable with respect to small variations in the data. As it is analyzed in detail in Section 6, each component of the framework addresses a di ff erent cause of instability of the spectral ordering results.

The theoretical analysis suggests and the experiments verify that the main advantages of the proposed framework as induced by the enhancement of stability are twofold: Firstly, results become more resilient to perturbations of the input, thus the reliability of the results is increased. Sec-ondly, the power method [17] computes the ordering result more e ffi ciently than in the original setting.
Given a set of n objects and a pairwise similarity mea-sure between them, the task of distance based ordering is to derive the order indexes of the objects such that similar objects are placed in adjacent orders while dissimilar ob-jects are placed far apart. More formally, distance sensitive ordering considers the following optimization problem: where w ij is the similarity between objects i and j and vec-tor r is the permutation of { 1 , 2 ,..., n } that optimizes the ob-jective function. The values of the elements r ( i ) of vector r reflect the ordering of object i .

It is known that the general optimization problem re-lated to distance based ordering is NP -hard [8], and thus approximate solutions should be considered. A popular ap-proach is spectral ordering [2, 4] that performs a contin-uous relaxation on the solution vector r , and reduces the optimization problem to a standard eigenvalue-eigenvector problem. In the context of this work we rely on a slight modification of the standard spectral ordering formulation as derived by [4], where the authors derive the ordering so-lution as the second eigenvector 3 of the normalized Lapla-cian matrix L = D  X  1 / 2 WD  X  1 / 2 . Here, W is the object-object similarity matrix W = X T X , D is the diagonal degree ma-trix containing the row sums of W , and the data matrix X contains the objects as its columns and the features as its rows. Other choices of W are also possible: W can essen-tially be any object-object similarity matrix. The use of the normalized Laplacian facilitates the theoretical analysis of the proposed semi-supervised spectral ordering framework and also presents theoretical advantages [16] over the un-normalized Laplacian that is commonly used for spectral ordering.

It should be noted that in the spectral graph theory litera-ture the normalized Laplacian matrix is commonly referred to as L = I  X  D  X  1 / 2 WD  X  1 / 2 , however in the context of this paper we will employ the aforementioned notation and con-sider the normalized Laplacian as L = D  X  1 / 2 WD  X  1 / 2 matrix is well studied in the context of spectral graph theory (e.g. [15] and references within) and it is known to have 1 as its largest eigenvalue. Moreover, by defining the object-similarity matrix W = X T X , L = D  X  1 / 2 WD  X  1 / 2 becomes positive semi-definite.
A common approach for measuring the stability of spec-tral algorithms requires the quantification, in the form of an error perturbation matrix E , of the uncertainty associated with the input matrix (eg. [11]). Based on matrix E the stability of spectral ordering is determined by the similarity of the ordering solution as derived by the original Lapla-cian matrix L versus the perturbed Laplacian matrix L + E . Further details on the computation of E in the domain of interest will be provided in Section 6.3.

Based on this formulation, the stability of the ordering solution can be derived by Matrix Perturbation Theory, and more precisely Stewart X  X  theorem on the perturbation of in-variant subspaces [14]. Based on Stewart X  X  theorem we can derive an upper bound on the di ff erence between the order-ing solution of L versus L + E . The upper bound applies when the smallest eigengap between the second eigenvalue of L and the rest is larger than four times the spectral norm of matrix E . In the case of spectral ordering the smallest eigengap is determined by the eigengap between the first and the second eigenvalue of the Laplacian matrix and the eigengap between the second and the third.
The upper bound gets smaller as the eigengap enlarges and the norm of the perturbation matrix E decreases. Thus, the stability depends on two factors: the size of the eigengap and the norm of the perturbation .

As we analyze further in the subsequent section, these eigengaps are not a mere theoretical artifact but are associ-ated with the data-structure as well as computational issues related to the derivation of the spectral ordering solution.
The eigengap between the first and the second eigenvalue of the Laplacian matrix is associated with the level of data connectivity. More precisely, if we consider the Laplacian D 1 / 2 WD  X  1 / 2 and the associated graph (i.e. a graph with edge weights W ( i , j )), then the size of the second eigen-value is associated with the cost of producing two separated clusters [5, 15]. In fact when the eigengap is 0, i.e. the al-gebraic multiplicity of first eigenvalue is larger than 1, then the graph is disconnected and the clusters can be produced with zero cost. The following theorem illustrates this rela-tion (note that we have appropriately changed the theorem statement from [15] to take into account that we consider the Laplacian D  X  1 / 2 WD  X  1 / 2 instead of I  X  D  X  1 / Theorem 4.1 (can be found in [15]) . Let G be an indirected graph with non-negative weights W. Then the multiplicity k of the eigenvalue 1 of matrix L = D  X  1 / 2 WD  X  1 / 2 equals the number of connected components in the graph. The eigenspace of 1 is spanned by the vectors D 1 / 2 e A components, where e A j that belong to the connected component A i .

Theorem 4.1 signifies that when the second eigenvalue is close to the first, a small amount of perturbation can make the graph disconnected, thus significantly a ff ecting the sec-ond eigenvector. Thus, spectral graph theory provides us with the necessary tools for understanding the source of in-stability when the eigengap between the first and the second eigengap is small.
In order to study the eigengap between the second and the third eigenvalue of the Laplacian matrix L , we assume that the data is adequately connected (i.e. the algebraic mul-tiplicity of the largest eigenvalue is 1) and consider the fol-lowing transformation: L = L  X  vv T , where v is the first degree matrix of the Laplacian L and e a unit vector, e ( i ) for all i ). With this definition the matrix L , apart from v , has exactly the same eigenvectors and eigenvalues as L . Thus, the second eigenvalue of L is the largest eigenvalue of L . This transformation is always possible and requires solely the computation of the degree matrix D .

The transformation of matrix L makes apparent the rel-evance of the power method [17] for computing the spec-tral ordering solution. Recall that the power method does not derive the full eigen-decomposition of a matrix and can compute solely the dominant eigenvalue and corresponding eigenvector. It starts with an initial vector b 0 , and then com-that is strictly larger than the rest and if the initial vector b has a non-zero component in the direction of the dominant eigenvector, then the rate of convergence of b k will be deter-value of A and  X  2 is the second in magnitude eigenvalue. The larger the eigengap between |  X  2 | and |  X  1 | the faster the convergence.

Based on L , the power method can be used to derive the ordering solution. The power method will converge with dominant eigenvalue of L ) and  X  3 is the third eigenvalue of L (and thus the second eigenvalue of L ).

This analysis illustrates that the convergence of the power method for computing the ordering solution depends on the eigengap between the second and the third eigenvalue of the Laplacian matrix. A method that successfully en-larges this eigengap will increase the e ffi ciency of the power method.
In order to study the behavior and the properties of the proposed spectral ordering framework, we need to recall certain elements of linear algebra. Firstly, we recall Weyl X  X  theorem on the perturbation of eigenvalues.
 Theorem 5.1 (Weyl, can be found in [14]) . Let A be a sym-metric matrix with eigenvalues  X  1  X   X  2  X  ...  X   X  n and E a symmetric perturbation with eigenvalues 1  X  2  X  ...  X  n . Then for i = 1 ,..., n the eigenvalues  X  i of A + E will lie in the interval [  X  i + n , X  i + 1 ] .

Another theorem we will employ is concerned with the a ff ect of rank-k updates to matrix eigenvalues.
 Theorem 5.2 (Wilkinson [17], can also be found in [12]) . Suppose B = A +  X   X  uu T where A  X  R n  X  n is symmetric, u  X  R n has unit Euclidean norm and  X   X  R . Then, there exist m 1 ,..., m n  X  0 , n i = 1 m i = 1 , such that Moreover, concerning rank-k updates B = A + k i = 1  X  i  X  there exist m ij  X  0 ,i = 1 ,..., n, j = 1 ,..., k with n such that
As we have mentioned in the introductory section, the proposed framework considers partial supervision and fea-ture selection with the general aim of stabilizing the spectral ordering results. In this section we will present each com-ponent of the framework and demonstrate their contribution to the stability of the results. Recall that in Section 3 we have stated that stability essentially depends on two factors, namely the size of the relevant eigengaps as well as the un-certainty associated with the Laplacian matrix estimates. In the subsequent sections it is analytically demonstrated that the semi-supervised component is associated with the en-largement of the eigengaps, while the feature selection is concerned with the reduction of uncertainty.
The semi-supervised component assumes that an initial ordering of the objects is provided and aims at adjusting the original object similarities such that the input order-ing is taken into account. Recall that the original object similarities are used for computing the Laplacian matrix D 1 / 2 WD  X  1 / 2 (i.e. W ( i , j ) is the similarity between object i and j ) that derives the ordering solution. The proposed method essentially aims at adjusting the values of the W matrix based on the input ordering.

In order to achieve this goal, we initially consider the definition of a Laplacian matrix that produces the initial in-put ordering, i.e. the second eigenvector derives the same results as the input order. If we consider the initial input or-{ 1 , 2 ,..., n } , and a degree matrix D , we can define the initial input Laplacian as: for all i ) and with d i being the i th diagonal element of the Degree matrix.
In order to understand the definition of the L input matrix, one should initially observe that vector v 0 is essentially the largest eigenvector of any Laplacian matrix with degree ma-trix D (if there are no disconnected components). Moreover, vector v 1 is by construction orthogonal to v 0 and produces exactly the same ordering as r . Based on the above we can write L input in the form of a Laplacian with degree matrix D , vectors v 0 and v 1 , with corresponding eigenvalues 1 and The W input matrix will contain the object similarities that generate the input ordering. Notice that this construction is possible for any degree matrix D .

It should also be noted that there exist di ff erent possi-ble definitions of the v 1 eigenvector that are orthogonal to v and also preserve the initial input order. However, the specific choice of v 1 imposes equal distances between the elements of the eigenvector v 1 and thus also on the  X  X ontin-uous X  ordering solution between the objects. In the absence of further knowledge on the initial input ordering it would not be reasonable to impose the additional bias of unequal distances between the objects.
 Based on the definition of L input we derive the final Laplacian as a linear combination of the original data Lapla-cian (thereafter referred to as L data ) and L input as: where 0  X  c  X  1 is a confidence factor associated with each component of the summation. The behavior of L semi can be understood if we write L semi as: which is possible since L input is defined with the same de-gree matrix as L data . This illustrates the main intuition of the semi-supervised framework that essentially adjusts the similarities of the original Laplacian such that the ordering is taken into account.

Intuitively one would expect that the use of supervision increases the reliability of the ordering results. This intu-ition is reflected in the eigengaps of L semi . As demonstrated in the subsequent analysis, they can be enlarged with the appropriate choice of the c parameter, as compared to L data
We will now analyze theoretically the behavior of the eigenvalues of L semi with respect to the parameter c ,the eigenstructure of L data as well as the ordering solutions of L data and L input . In most theorems we derive the required amount of supervision (i.e. required value for (1  X  c )or c ) such that the desired eigenvalue bounds, or eigengaps are achieved. We can summarize the theoretical results as fol-lows:  X  Theorem 6.1 demonstrates that parameter c can fully  X  Theorem 6.2 demonstrates that if the eigenvalues of  X  Theorems 6.3,6.4,6.5 demonstrate that the behavior of We will start with the dependence of the eigenvalues of L semi with respect to the parameter c . The following the-orem demonstrates that with the appropriate choice of pa-rameter c , large eigengaps can be achieved.
 Theorem 6.1. Let L data be an n  X  n normalized Graph Laplacian, c a real number such that 0  X  c  X  1 and L input the Laplacian as derived by an initial input ordering. Define the matrix L semi = cL data + (1  X  c ) L input . Its largest eigen-value will be  X  1 ( L semi ) = 1 , its second eigenvalue will reside in the interval 1 2  X  c 2 + c  X  n ( L data )  X   X  2 ( L semi  X  ( L data ) is the smallest eigenvalue of matrix L data , and its third eigenvalue will be smaller than c,  X  3 ( L semi )  X  Proof. In order to compute the appropriate bounds for the eigenvalues of L semi we can employ Weyl X  X  theorem on the and derive for the largest eigenvalue of L semi ,  X  1 ( L eigenvalue of L data is 1) and  X  1 ((1  X  c ) L input ) = (1 (since the largest eigenvalue of L input is 1) we can derive: Moreover for the first Laplacian eigenvector v 0 ,wehave c ) L semi v 0 = c  X  v 0 + (1  X  c )  X  v 0 = v 0 . Thus v 0 of L semi with corresponding eigenvalue 1. Thus Concerning the second eigenvalue of L semi we can employ Weyl X  X  theorem and state: It holds  X  2 ((1  X  c ) L input ) = (1  X  c ) 1 2 ,  X  n ( cL and  X  1 ( cL data ) = c . Thus, Concerning the third eigenvalue of L semi we can employ Weyl X  X  theorem and state: We have  X  3 ((1  X  c ) L input ) = (1  X  c )  X  0 = 0 (since L only two non-zero eigenvalues) and  X  1 ( cL data ) = c . Thus
The bounds derived in the theorem above depend solely on the parameter c and illustrate that with the appropriate choice of parameter c , large eigengaps can be achieved. However, if the eigengaps of matrix L data are already large, then little supervision (i.e. smaller values of (1  X  c )) is re-quired. The subsequent theorem illustrates this connection. Theorem 6.2. Let L data be an n  X  n normalized Graph Laplacian, and L input be the the Laplacian as derived by an initial input ordering. Define the matrix L semi = [ cL data (1  X  c ) L on the second eigenvalue of L semi ,  X  2 ( L semi )  X   X  2 the third eigenvalue of L semi ,  X  3 ( L semi )  X   X  3 , we must set c Proof. In order to apply Wilkinson X  X  theorem, we consider that matrix L semi is composed by a rank-2 update on matrix cL data . We can write for the three largest eigenvalues of L Since the largest eigenvalue of L semi is equal to 1, we have:  X  c + ( m Now for the second eigenvalue we can write:  X  Recall that we aim at determining the appropriate c such that the upper bound  X  2 is achieved. Thus we have: c  X  In order to derive the appropriate bound for the third eigenvalue we should initially observe that m 21 + m 22 2 Now for the third eigenvalue we can write:  X  Recall that we aim at determining the appropriate c such that the upper bound  X  3 is achieved. Thus we have: c  X 
The derived c for the second eigenvalue is meaning-ful when the desired upper bound  X  2 ( L semi )issmallerthan  X  ( L data ), and when both are larger that 1 2 , as this ensures that c  X  [0 , 1]. This is a natural setup because in order to achieve stability one should lower the second eigenvalue, as this will enlarge the eigengap between the first eigenvalue (which is always equal to 1) and the second. Concerning the derived c for the third eigenvalue, it is meaningful (i.e. c  X  [0 , 1]), when  X 
One would generally expect the behavior of the L semi = cL vectors of L data and L input and not solely on the eigenval-ues. It would be intuitive to consider that when the ordering solutions as derived by L data and L input conform to a high degree, then even with little supervision (i.e. small values of (1  X  c )), the reliability of the ordering results is rapidly increased. This is demonstrated in the following theorems. Theorem 6.3 (Best Case Scenario) . Let L data = v 0 v T 0  X  v 2 v T 2 + ... +  X  n v n v T n be the data Laplacian matrix and L rived by the second eigenvector of L data is equal to the provided supervision v 2 = v 1 , then the eigenvalues of ma- X  3 ,..., n. Moreover, the required supervision for achieving the eigengap  X  1 ( L semi )  X   X  2 ( L semi ) = gap, is c = 1 and the required supervision for achieving the eigengap  X  Proof. We have that the original data Laplacian is decom-v v T 0 + 1 2 v 2 v T 2 (since the two matrices induce the same order solution, i.e. v 2 = v 1 ). Thus: L c  X  Based on the above, we can derive the required c value as:  X  c  X 
On the other hand, when the initial input ordering solu-tion corresponds to the eigenvector of L data that is associ-ated with the smallest eigenvector, then more supervision (i.e. larger values of (1  X  c )) is required.
 Theorem 6.4 (Worst Case Scenario) . Let L data = v 0 v T 0  X  v 2 v T 2 + ... +  X  n v n v T n be the data Laplacian matrix and L to the last eigenvector of the Laplacian matrix v 1 = v n the eigenvalues of matrix L semi = cL data + (1  X  c ) L input will be  X  i ( L semi ) = c  X  i ( L data ) ,fori = 2 ,..., over, the required supervision for achieving the eigengap  X  Proof. We have that L semi = cL data + (1  X  c ) L input = c  X  Thus the eigengap between the second and the third eigen-value will steadily become smaller as supervision increases (i.e. (1  X  c ) increases), until the eigenvalue corresponding to the eigenvector v n gets larger than c  X  2 ( L data ). Based on the above, we can derive the required c value as:  X 
In general, we can express the initial input ordering so-lution (i.e. the second eigenvector of L input ) as a linear com-bination of the eigenvectors of L data . Based on this decom-position, it would be intuitive to expect that the eigenvec-tors that do not participate in the input ranking solutions are downgraded in importance. This is demonstrated in the subsequent theorem.
 Theorem 6.5. Let L data = v 0 v T 0 +  X  2 v 2 v T 2 + ... +  X  the data Laplacian matrix and L input = v 0 v T 0 + 1 2 v v as a linear combination of the eigenvectors of the data Laplacian matrix 4 ,v 1 = w 2 v 2 + w 3 v 3 + ... + w n v eigenvalues of L semi = cL data + (1  X  c ) L input are  X  1 and  X  i ( L semi ) = c  X  i ( L data ) for all i such that w w v 3 + ... + w n v n )( w 2 v T 2 + w 3 v T 3 + ... + w n v that for those v i such that w i = 0 we will have L input
This theorem signifies that the eigenvectors that do not participate in the input ranking solution will be quickly downgraded in importance (through the shrinkage of their eigenvalues), while the rest will finally converge to v 1 same e ff ect will take place concerning the eigenvectors that have small significance in the solution (i.e. w i  X  0).
As we have analyzed in section 3, an integral component of stability assessment is the quantification of uncertainty in the form of an error-perturbation matrix E . Since, we have already defined three matrices in the previous section ( L data , L input and L semi ), we will need to define an appro-priate perturbation matrix E for each. We will begin with L input that is associated with the initial input ordering. Sup-pose we can characterize the degree of reliability in the su-pervision by comparing two rankings produced by the do-main knowledge: if these are close to each other, then the domain knowledge is reliable. For both rankings we gener-ate a corresponding eigenvector as was described in Section 6.1, and the di ff erence between these vectors will be de-noted as v = u 1  X  u 2 where u 1 and u 2 are the two ranking eigenvectors. The element v ( i ) gives the uncertainty related to object i . The input perturbation matrix E input is a rank-1 matrix
We will define the error-perturbation matrix for the L data matrix in a way that will enable feature selection for uncer-tainty reduction. We initially observe that the order solution by D  X  1 / 2 X T u 2 , where u 2 is the second eigenvector of the  X  X eature Laplacian X  L feat = XD  X  1 X T . Notice that L data L feat have the same eigenvalues and if u is an eigenvector of L feat , then D  X  1 / 2 X T u is an eigenvector of L data the stability of the ordering solution can be derived by the stability of the L feat matrix. In order to quantify the uncer-tainty associated with the elements of L feat , we bootstrap the observations (here, excavation sites) and produce boot-strap confidence intervals for the elements of the L feat trix (pair-wise feature similarities). Consequently, we de-fine matrix E data such that E data ( i , j ) is the maximum di ence between L feat ( i , j ) and the endpoints of the respective confidence interval.

The error-perturbation matrix of L semi is derived by the norms of the matrices that take part in the summation. More precisely, we define
Having defined all the appropriate error-perturbation ma-trices, we can move on to evaluate the stability of the spec-tral ordering framework and explore possible approaches for uncertainty reduction.
Based on the definition of E data as the perturbation of a feature  X  feature matrix, we can consider feature selection for uncertainty reduction. The proposed framework is sim-ilar in spirit with [11], where the features that contribute maximally to the norm of E data matrix are sequentially re-moved. More precisely, at each step of the algorithm, the feature that corresponds to the column (or row) of matrix E data that has the highest norm is removed. Although, we employ feature selection in the same manner as in [11] we should stress that there are some important di ff erences. The main di ff erence is concerned with the fact that the new per-turbation matrix E data , as induced by the removal of a fea-ture, will not be a principal submatrix of E data . This is be-cause the removal of a feature will influence the values of the degree matrix D , thus a ff ecting the confidence intervals of all the feature-pairs. In order to address this issue, we re-compute the confidence intervals and E data matrix after each feature is removed. However, it should be noted that when there is a large number of features, we can expect that the degree matrix is not severally a ff ected and thus we can con-sider the principal submatrix of E data (after removing the row and column i that corresponds to the removed feature) as an accurate approximation of the new perturbation ma-trix E data . When this is the case, it is guaranteed that the the uncertainty, as expressed by the norm || E data || 2 will be reduced.
Concerning the semi-supervised component, our work is conceptually related to personalized Pagerank [9]. Per-sonalized Pagerank derives the stationary probability of the random walk based on a weighted linear combination of the transition matrix and a prior distribution, in the form of A = [ cP + (1  X  c ) S ] T , where P is the row-stochastic transition matrix and S = eu T , where u contains the prior distribution. Apart from the intuitive probabilistic interpretation of the A matrix, it has been shown that parameter c can control the eigengap between the largest and the second eigenvalue. Theorem 7.1 (Haveliwala and Kamvar [10]) . Let P be an n  X  n row-stochastic matrix. Let c be a real number such that 0  X  c  X  1 . Let S be the n  X  n rank-one row-stochastic matrix S = eu T , where e is the n-vector whose elements are all e i = 1 and u is an n-vector that represents a probability distribution. Define the matrix A = [ cP + (1  X  c ) S ] T second eigenvalue is |  X  2 | X  c.

Concerning the feature selection component our work is conceptually related to Stability based Sparse PCA [11]. In this work the authors consider the use of feature selec-tion for uncertainty reduction in the context of PCA, and demonstrate empirically that feature selection can stabilize the PCA results in several real-world UCI datasets.
We use results from matrix perturbation theory [14], stat-ing that the rank-k approximation of a matrix A is close to a rank-k approximation of A + E ,if E has weak spectral prop-erties compared to those of A . Somewhat similar properties have been used in a di ff erent setting, namely speeding up SVD and kernel PCA: Achlioptas [1] shows how to choose the perturbation E based on the elements of the A matrix, such that the matrix A + E is either a quantized or sampled version of A , making eigenvalue algorithms work faster.
The prospects of spectral ordering in the paleontologi-cal domain have been demonstrated by Fortelius et al [6]. In this work, plain spectral ordering of the sites, based on mammal co-occurrences and discarding the age information of the sites, was considered. In addition, Puolam  X  aki et al [13] present a full probabilistic model that again only con-siders the co-occurrences in the data.
In the experiments we aim at verifying that the proposed framework enhances the stability of spectral ordering and increases the relevant eigengaps in the paleontological data. Recall that this will increase the reliability of the ordering results and improve on the convergence rate of the power method. The experiments indeed verify the anticipated be-havior and increase the relevant eigengaps (demonstrated in Sections 8.2,8.3) and also the stability of the ordering re-sults (demonstrated in Section 8.3).
The paleontological data we are considering consists of findings of land mammal genera in Europe and Asia 25 to 2 million years ago. The data set is stored and coordinated in the University of Helsinki, Department of Geology [3]. Our version of the data set was downloaded on June 26, 2007.
The observations in our data are the sites of excavation and the features are mammal genera whose remains are found at these sites. In total we have 1887 observations and 823 features. The data matrix is 0-1 valued: an entry x otherwise. The data is very sparse: about 1 per cent of the entries are nonzero. We will also work with a small sub-set of data containing 1123 observations and 18 features; this subset is more dense, having 12 per cent of its entries nonzero. Thereafter we will refer to the sparse dataset as paleo sp and the dense dataset as paleo d .

In addition, we have auxiliary information on the esti-mated ages of the sites: an approximate age for each site, and also a more precise age for some sites; the methods available for estimating the ages vary from site to site, and thus at some sites the information is more certain than at others. The approximate ages will be used to construct an initial ranking r input of the sites, and this will be used as an input in the semi-supervised setting, results of which will be presented in Section 8.2. Both the precise and approx-imate ages will be needed when quantifying our belief in the initial ranking, that is, defining the perturbation matrix E input of L input as discussed in Section 6.3; empirical results on this will be shown in Section 8.3.

We will assume that the data is fully connected in that the algebraic multiplicity of the first eigenvalue of the data Laplacian is 1: if this is not the case, the removal of dis-connected observations will be a preprocessing step. In addition, we will preprocess the data such that almost-disconnected components are removed too: these corre-spond to objects that are very weakly connected to the rest of the objects. For such objects i ,thevalue r ( i ) in the order vector r (obtained by sorting the second eigenvector) is very large compared to other r ( j ). Let us first demonstrate that the eigengaps of the data Laplacian increase when domain knowledge is taken into account. These experiments were performed on the sparse and large paleo sp dataset, where the initial eigengaps are small. Recall that the stability of spectral ordering essen-tially depends on two factors, one of which are the eigen-gaps between the first and second eigenvalue and the second and third eigenvalue of the Laplacian. Figure 1 shows the behavior of the eigengaps of the semi-supervised Laplacian L sion. Choosing 1  X  c = 0 corresponds to no supervision, in which domain knowledge is not taken into account and the spectral ordering is done based on feature co-occurrences only; the eigengaps at c = 1 thus show the eigengaps of the data Laplacian. In contrast, 1  X  c = 1 corresponds to the trivial case of full supervision of the ranking, in which co-occurrences in the data are not taken into account but only the domain knowledge ranking is used. We observe that both eigengaps increase rapidly when the level of supervi-sion increases. Thus the spectral ordering becomes more stable as more emphasis is put on the domain knowledge.
We will then demonstrate that the stability of the spec-tral ordering increases as features are removed step by step. The removed features will be chosen based on their con-tribution on the variability of the feature-feature similarity matrix, measured as matrix E data discussed in Section 6.4. It should be noted that after each feature is removed, L semi is reevaluated based on L input and L data which are appropri-ately recomputed.

We will measure the stability of the spectral ordering by a  X  X tability factor X  sf that depends on the eigengaps and the
Figure 1. Eigengaps  X  1 ( L semi )  X   X  2 ( L semi ) ( + )and  X  vision. Horizontal axis: 1  X  c , confidence in domain knowledge. 1  X  c = 0 : no supervision; 1  X  c = 1 : full supervision. norm of the perturbation matrix: For the stability factor we will need appropriate values for L Let us first construct the semi-supervised Laplacian L choose the confidence factor c reflecting our belief in the observed data versus the initial ranking. We can either rely on a domain expert or better still, derive c from the body of domain knowledge: we will choose to define which naturally characterizes the confidence such that a large perturbation in the initial input ranking leads to a high confidence in the observed data, and vice versa. In the def-inition (5), the data perturbation E data will be obtained by bootstrap sampling as discussed in section 6.3. The input perturbation E input for paleontological data will be derived based on the availability of approximate or precise ages for each site: in addition to the initial ranking r input based on approximate ages of the sites, we construct another initial ranking r s using the precise ages available for some of the sites. (The sites for which a precise age is not available will get an average ranking in r s .) For both rankings r input we generate a corresponding eigenvector, v input and v s ,us-ing Equation (1). We then take the di ff erence between these eigenvectors as v = v input  X  v s and use that in place of v in Equation (2), to measure the di ff erence between the two orderings. This gives us the perturbation E input associated with the domain knowledge. Having now collected all the necessary components, we can construct the matrix L semi .
For the stability factor in Equation (4) we also need the value for || E semi || 2 . Based on the definition for c , the equa-tion (3) now simplifies ||
E
Having now defined all components of the stability fac-tor (4) let us then see how it behaves when features are it-eratively removed. Figure 2 shows the results. We have employed subset paleo d of 1123 observations and 18 fea-tures in this experiment. At each iteration, one feature is removed based on its contribution to the data perturbation. Simultaneously, a few observations typically get removed, as they have become disconnected with the other observa-tions due to the removal of the feature in question. The stability factor of the semi-supervised Laplacian increases during feature selection, showing that feature selection en-hances the stability of spectral ordering.
Figure 2. Stability factor during feature selec-tion. Horizontal axis: iteration. One feature is removed at each iteration.

In addition, recall that the eigengap between the second and third eigenvalue a ff ects the convergence of the power method, as discussed in Section 4.2. Figure 3 shows that this eigengap increases during feature selection, both in the original data and in the semi-supervised setting. Thus fea-ture selection enhances the behaviour of the power method in both the original spectral ordering framework and the semi-supervised framework.
In this paper we have shown how to increase the sta-bility of spectral ordering using two separate tools: par-
Figure 3. Eigengap between the 2nd and 3rd eigenvalue during feature selection. Semi-supervised spectral ordering (  X  ), original spectral ordering (  X  ). One feature is removed at each iteration (horizontal axis). tial supervision in the form of a (possibly uncertain) do-main knowledge ordering, and sparsification in the form of feature selection. We have presented a detailed theoretical analysis showing how the eigengaps between the first and second eigenvalue, and the second and third eigenvalue, of the Laplacian a ff ect the stability, and how partial supervi-sion will increase the eigengaps. Feature selection in turn will decrease the norm of the perturbation matrix E that quantifies the uncertainty associated with the observed data.
Our main application area is paleontology: we have con-sidered the ordering of the sites of excavation in paleonto-logical data, by complementing spectral ordering with do-main knowledge of the approximate ages of the sites. The paleontological data is noisy in that many observations are missing, and prone to small changes when the findings are more carefully examined. Also, we never have access to the exact ages of the sites. Thus when ordering the sites, the best we can aim at is an ordering that is as stable as possible with respect to small variations in the data. This motivates our task of optimizing the stability of spectral ordering. We have shown that in the paleontological data, the eigengaps quickly increase as semi-supervision is used. Also, feature selection, by removing the mammals that contribute most to the variation of the results in bootstrap sampling, is demon-strated to increase the stability of spectral ordering.
In future work we aim at exploring the potentials of our framework in di ff erent application domains, where partial supervision is naturally present. Moreover, we aim at ex-tending the proposed framework to spectral clustering. Acknowledgements. The authors wish to thank professor Mikael Fortelius for fruitful discussions regarding paleonto-logical data and professor Heikki Mannila for his insights in spectral ordering. E. Bingham was supported by Academy of Finland grant 118653 (ALGODAN).

