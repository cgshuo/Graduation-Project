 opinions or sentiments underlying user generated contents, such as online product reviews, blogs, discussion forums, etc. Previous studies that adopt machine learning algorithms mainly focus on designing effective features for this complex task. This paper presents our approach based on tree kernels for opinion mining of online product reviews. Tree kernels alleviate the complexity of feature selection and generate effective features to satisfy the special requirements in opinion mining. In this paper, we define several tree kernels for sentiment expression extraction and sentiment classification, which are subtasks of opinion mining. Our proposed tree kernels encode not only syntactic structure information, but also sentiment related information, such as sentiment boundary and sentiment polarity, which are important features to opinion mining. Experimental results on a benchmark data set indicate that tree kernels can significantly improve the performance of both sentiment expression extraction and sentiment classification. Besides, a linear combination of our proposed tree kernels and traditional feature vector kernel achieves the best performances using the benchmark data set. interested in sharing their personal opinions and reviews on the Web. This leads to a rapidly increasing scale of user generated contents, such as user feedback reviews, blogs, online forums, discussion groups, etc. Different from other kinds of online textual information, user generated contents are people-centric and contain a lot of subjective information. Extracting these subjective texts and mining user opinions in the text play significant roles in many applications, such as electronic commercial, search of customer opinions, opinion tracking for business intelligence and user behavior prediction for online targeted advertising. The need to find appropriate techniques to track opinions about products, persons and events raises some challenging problems in the field of text mining. Opinion mining is a technique to solve the challenges and serve the growing interest in text mining. This paper mainly focuses on opinion mining of online product reviews. categorized into two main types [1]: facts and opinions. Facts are objective expressions about entities, events and their properties. Opinions are usually subjective expressions that describe people X  X  sentiments or feelings towards the target objects. Technically, opinion mining, also known as sentiment analysis, can be regarded as a task of text classification [2, 3], where a phrase, a sentence or a document is labeled as a positive ( X  X humbs up X ) or negative ( X  X humbs down X ) opinion to a target object (i.e., product, film, person, etc.). Although a lot of existing work has been done for text classification, most of them aim at fact-based categorization without analyzing opinion or sentiment in the text. Different from traditional text classification, the main characteristics [4] of opinion mining are: 1) Sentiment is expressed in a more subtle manner, making it difficult to be identified by term information alone. We must consider the structured information and the semantic relation in a sentence or a document. 2) Sentiment orientation is quite context-sensitive and domain-dependent. This means that the same sentiment word may have different sentiment orientations in different sentences or domains. 3) The granularity of sentiment varies according to different applications. In fact, the task of opinion mining is not only to infer the positive or negative opinion from text, but also to identify the target object that the opinion is related to. This point is very important for practical applications of opinion mining. Hence, it requires a more in-depth analysis of sentiments in text. For example, in a sentence or a document, the author may have different opinions about many different target objects or different target features of an object, although the overall opinion may be positive or negative. Therefore, Liu [1] proposes the feature-based opinion mining. In this paper, we also focus on the opinion mining on the granularity of feature. kernels for opinion mining. Tree kernels have been applied to reduce the complexity in features design for natural language tasks, such as relation extraction, named entity recognition, semantic role labeling, etc. A key problem of tree kernels based approach is to design proper tree kernel spaces for the similarity evaluation. In our approach, we define four tree kernel spaces for comparative study: feature-sentiment tree (FST), generalized feature-sentiment tree (GFST), boundary marked GFST (GFST m ) and generalized feature-sentiment tree with polarity labels (GFST Moreover, this paper proposes a combined kernel by integrating the above four tree kernels with traditional feature vector kernel respectively. The latter kernel computes the similarity between two vectors using standard flat features extracted from sentiment expressions. In order to measure the association between target feature and sentiment words, as well as the contextual polarity of sentiment word, we introduce two new semantic features that generated via Pointwise Mutual Information (PMI) method. Experiment results show that our proposed tree kernels outperform the feature vector kernel using traditional flat features. The combination of our tree kernels and feature vector kernel can achieve the best performance using the benchmark data set. products reviews, our proposed approach is applicable to any text data with sentiments, such as news reviews, blogs, discussion forums, etc. In addition, the approach based on tree kernels can be applied to many related tasks, such as opinion retrieval, opinion question answering and opinion-based information extraction. Section II, we make a brief introduction of the related work in the field. The problem is defined in Section III. Section IV describes the basic approach to the task of opinion mining. The approach based on tree kernels is proposed in Section V. Section VI presents the combination between tree kernels and feature vector kernel. Experiments and result analysis are presented in Section VII. Finally we conclude the paper and discuss the future work in Section VIII. Natural Language Processing [1, 5]. The most common definition of the problem is a binary classification task to determine a phrase, a sentence or a document with either positive or negative polarity [3, 6]. Since traditional text categorization methods perform poorly on sentiment classification [3], Pang and Lee [6] propose a method based on minimum cuts in graphs as the supplement to traditional text classifiers. This method greatly facilitates incorporation of cross-sentence contextual constraints. Their experiments conducted on Movie Reviews show a significant improvement. Mullen and Collier [7] use Support Vector Machine (SVM) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives. Zhao et al. [8] propose an approach based on Conditional Random Field to the special characteristics in sentence level sentiment classification: contextual dependency and label redundancy. In their approach, sentiment classification is regarded as a sequence labeling problem using CRFs model to capture the relation between two adjacent sentences in the context. They employ the relations between the labels as  X  X edundant features X  for the sentiment classification. Zhang et al. [9] present a method for mining product reviews, in which they make use of features provided by shallow dependency parsing. TOKUHISA et al. [10] decompose the emotion classification task into two sub-steps: sentiment polarity classification (coarse-grained emotion classification), and emotion classification (fine-grained emotion classification). For each classification, they collect a large number of emotion provoking event instances as labeled examples. machine learning algorithms on annotated data set for the task of sentiment classification. Different from the machine learning approach, another approach is proposed based on sentiment orientation [2, 11-15]. It performs classification according to positive and negative sentiment words or phrases contained in each evaluation text. This approach does not require any prior training, and usually makes use of a pre-required sentiment lexicon. The classification result is determined via some functions based on the positive and negative indicators that defined in sentiment lexicon. In early work, Hatzivassiloglou et al. [14] present an approach to identify the word semantic orientation by the conjunctions that link two adjectives. Their approach actually makes use of the co-occur information in general text or dictionary glosses. Turney [2] presents a PMI-IR algorithm to estimate the semantic orientation of phrases. PMI-IR algorithm uses Pointwise Mutual Information and Information Retrieval to measure the sentiment similarity of pairs of words or phrases. Riloff and Wiebe [11] present a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. Hu and Liu [12] extract product features and opinion features for a product, followed by using a lexicon-based method to determine whether the opinion expressed on a product feature is positive or negative. A similar approach could be found in [16], in which three sentiment category functions are defined for the sentence sentiment classification. An improved method in [17] employs relaxation labeling classification technique in which the constraints are integrated as neighborhood features which influence the assignment of sentiment label. Ding et al. [13] improve the above methods and propose a holistic lexicon-based approach. This approach is able to handle sentiment words that are context dependent, which cause major difficulties for existing algorithms. is solved by language model approach which has shown its effectiveness and simplicity in information retrieval field. Mei and others [18] define a Topic Sentiment Mixture (TSM) model to capture the latent topical facets and their associated sentiments. TSM model can be built to extract the topic models in an unsupervised/semi-supervised way with an Expectation-Maximization (EM) algorithm. Similar methods can be found in [19-24]. The basic idea is to model the sentiment in text by a probabilistic model that estimates the probabilities of each sentiment categories. mining, such as sentiment analysis in multiple languages[25], cross-domain sentiment classification[26, 27], sentiment analysis for conditional sentences[28], and sentiment analysis for comparative sentences[29]. a product, a person, an organization, an event, etc. In this paper, we mainly focus on the opinion mining on product user reviews. Therefore, the target object that the opinion commented on should be a product. Users do not only express opinions on the product itself. Their opinions often focus on some product features of interest. Opinion mining must track the sentiment on the granularity of product features. Following [1, 13], we define target feature and sentiment word as follows: component (part) or property of a product, i.e. lens, screen, price, size, etc. word that expresses opinion, evaluation, emotion or attitude. Usually, there are three polarities for sentiment words: positive (e.g.,  X  X reat X  and  X  X njoy X ), negative (e.g.,  X  X oor X  and  X  X islike X ) and neutral (e.g.,  X  X ong X ,  X  X mall X  and  X  X eel X ). The neutral sentiment words often turn out to be positive or negative depending on the context that they appear. For example, let us consider the polarities of the neutral sentiment word  X  X mall X  in the following two sentences: express a negative opinion, while in the second one,  X  X mall X  is clearly being used to express a positive opinion. expression is a minimum sequence of text that contains an opinion toward a target feature. Sentiment expression usually contains a sentiment word. In this paper, we use the pair ( tf , s ) to represent the sentiment expression. For the sentiment expression without sentiment words, we use an alternative approach described in Section V. orientation (or sentiment polarity) indicates whether author X  X  opinion is positive or negative toward a target feature. follows: expressions ( tf , s ), and then determine the sentiment orientation (positive or negative) for each sentiment expression. mining is to identify all sentiment expressions and their sentiment orientations. We propose a two-stage approach to this task: 1) sentiment expression extraction, which is used to identify all correct sentiment expressions, 2) sentiment classification for sentiment expression. sentence may contain more than one sentiment word. Some of them may have opposite sentiment polarities. The situation will become more complicated when more than one target feature appears at the same time. These sentiment words and target features compose many candidate sentiment expressions. Fig. 1 (a) illustrates a parse tree of the sentence  X  I love the camera but lens cap is really annoying  X . There are four candidate expressions in this sentence: (camera, love), (lens cap, annoying), (camera, annoying) and (lens cap, love). The aim of sentiment expression extraction sentiment expression extraction can be considered as a task of classification that treats the correct pairs as positive examples and incorrect pairs as negative examples. At the stage of sentiment classification, we determine whether the sentiment expression ( tf , s ) expresses a positive or negative opinion on the target feature tf . We can construct a classifier for this task, such as SVM classifier using the features extracted from ( tf , s ). we give some related definitions as follows: s are the target feature and sentiment word that occur in t , whereby the target feature nodes and the sentiment nodes are defined as follows: nodes of tf are leaves belonging to all words of tf in t . nodes of s are leaves belonging to all words of s in t . Sentiment tree (FST) as follows: Sentiment Tree (FST) for a sentiment expression ( tf , s ) is a minimum subtree that spans the target feature nodes of tf and sentiment word nodes of s . following steps to train the classifiers for the sentiment expression extraction and sentiment classification:  X  Given a sentence, generate a full syntactic parse tree t  X  Let TF = { tf 1 , tf 2 , ..., tf m } and S = { s 1  X  For each pair ( tf i , s )  X  TF  X  S , FST i is a feature-camera but lens cap is really annoying  X  in Fig. 1 (a), there are two correct sentiment expressions: (camera, love) and (lens cap, annoying). Fig. 1 (b) shows FST 1 + and FST 2 + that are corresponding to (camera, love) and (lens cap, annoying). FST 1 + and FST 2 + are positive examples from E +. In Fig. 1 (c), FST 1 -and FST 2 -corresponding to two incorrect expressions (camera, annoying) and (lens cap, love) are negative examples from E -. We notice that examples within E + and E -are highly similar; however they are less similar across E + and E -. This means that we can design a function to measure the similarity between two FSTs, and classify FSTs according to the similarity function. E + and E -sets can be used to train a classifier for sentiment expression extraction. To train the sentiment classifier, the elements of E + can be reorganized as E + pos and E -according to the sentiment orientation of each pair of ( tf , s ). That is to say, the FST extracted from a sentiment expression with positive sentiment orientation will be added into E + pos , whereas the others will be added into E -shown in Fig. 1 (b), FST 1 + is an example from E + FST 2 + is an example from E -neg . In this paper, we adopt both structured and flat features to train the above two classifiers. The details will be provided in the following sections. classification problem using a machine learning algorithm. Most learning algorithms rely on feature-based representation strategy. This strategy represents each classification instance as a vector of n -dimensional predefined features. However, this representation model is not effective for the structured object such as syntactic parsing tree. For example, each sentiment expression ( tf , s ) in E + and E -will be decomposed into many subtree features. Since the number of different subtrees is exponential to the size of E + and E -, the subtree features space becomes huge. This leads to the problem of data sparsity. Therefore, it is unpractical to use the syntactic structure features directly. kernel-based methods [31-33] do not need to extract each individual feature from each parsing tree. The kernel-based methods simply evaluate the similarity between two trees via computing a kernel function between them. Given FST 1 and FST 2 corresponding to two sentiment expressions, the following kernel function is used to compute the similarity between FST 1 and FST 2 : where N 1 and N 2 are the sets of nodes in FST  X  ( n 1 , n 2 ) is the number of the common subtree rooted in scenarios as follows [32]:  X  If the productions at n 1 and n 2 are different, then  X  If the productions at n 1 and n 2 are the same, and n  X  If the productions at n 1 and n 2 are the same, but n where nc ( n 1 ) is the number of children of n 1 the weight of large tree structures. In our approach, to 0.4. The basic principle for tree kernel is to make the examples from the same category (or different categories) with high (or low) similarity. A. Tree Kernel Spaces for Sentiment Expression extraction, we need to construct a classifier using the features extracted from E + and E -. Tree kernel helps to construct the classifier by making use of the kernel function defined in Formula (1) without extracting each individual feature. The kernel function is used to measure the similarity by computing the common fragments between two FSTs. In order to make FSTs have more common fragments, we introduce the Generalized Feature-Sentiment Tree (GFST). GFST replaces all target feature nodes and sentiment nodes in FST with their labels (TF or S).  X $ X  symbol is added in front of  X  X F X  and  X  X  X  labels to apart from other terminals. sentence  X  The screen is large but quality is not good  X . There are two correct sentiment expressions, namely, (screen, large) and (quality, good). Fig. 2 (b) shows GFST GFST 1 -for the target feature  X  X creen X . The target feature node  X  X creen X  and sentiment word nodes  X  X arge X  and  X  X ood X  in GFST 1 + and GFST 1 -are replaced by  X $TF X  and  X $S X  respectively. GFST cannot be used due to the large number of common fragments shared by correct and incorrect sentiment expressions. Inspired by the boundary detection [30] in semantic role labeling, we mark the node that covers the sentiment word nodes with label  X  X  X  at the 2-th level of GFST to denote the boundary of sentiment. We refer to this structure as Marked GFST (GFST m ). Fig. 2(c) shows the GFST m 1 + and GFST m 1 -corresponding to the target feature  X  X creen X , and GFST m 2 + is a positive example corresponding to  X  X uality X . GFST 1 + and GFST 1 -in Fig. 2 (b). The total number of common fragments is 23. It indicates a high similarity between the positive and negative examples according to the Formula (1). This makes the classifier based on tree kernels less able to discriminate correct and incorrect tree structures. In contrast, there are only 9 common fragments between GFST m 1 + and GFST m 1 -in Fig. 3 (b). This indicates that more common tree fragments are shared between positive examples; whereas less common fragments are shared between positive and negative examples. verbs and adjectives that occur in the sentence as virtual sentiment words to generate all candidate sentiment expression pairs. B. Tree Kernel Spaces for Sentiment Classification sentiment expression ( tf , s ) expresses a positive or a negative opinion for the target feature tf . Intuitively, if two sentiment expressions have similar syntactic structures and contain similar words, they may have the same sentiment orientation to the target feature. kernel can help to construct a sentiment classifier by computing the similarity between input structures according to Formula (1). FST ( feature-sentiment tree ) and GFST ( generalized feature-sentiment tree ) can be selected for the computation of structured similarity. However, FST and GFST only encode the structured information and lexicon information in tree leaves. They do not encode any sentiment related information which is very important for sentiment classification. GFST with sentiment polarity labels of sentiment words. After labeling, there will be few common fragments between the tree structures corresponding to positive and negative sentiment expressions. In more detail, we mark all nodes that ascend from the sentiment words nodes to the root with the polarity label of sentiment words. We refer to this tree structure as a Generalized Feature-Sentiment Tree with Polarity labels (GFST p ). Thus, GFST p encodes not only the syntactic information but also the sentiment polarity information. As shown in Fig. 4, three clauses  X  excellent lens  X ,  X  nice screen  X  and  X  poor quality  X  have an exactly identical GFST. But (lens, excellent) and (screen, nice) have a different sentiment orientation from (quality, poor). This violates the principle of tree kernel. When we mark GFST with polarity labels (e.g.,  X  X J X  to  X  X J-p X  or  X  X J-n X ,  X  X N X  to  X  X N-p X  or  X  X N-n X ), the GFST will become two GFST p While GFST p + 1 and GFST p + 2 are still identical, GFST only has one common tree fragment  X  X N  X  $TF X  with GFST p -1 . Since the sentiment polarity of a word usually depends on target features, we use Pointwise Mutual Information (PMI) method to determine the polarity label in GFST p (Details will be described in Section VI.) above with traditional feature vector kernel for the sentiment expression extraction and sentiment classification. The features for feature vector kernel are categorized into three types: lexicon features, syntactic features and semantic features. and Negation word. Syntactic features contain syntactic path which linking target feature words and sentiment words (e.g., NN  X  NP  X  S  X  VP  X  JJ), path length, etc. Semantic features include the word sentiment polarity, semantic association features that generated via Pointwise Mutual Information (PMI) method. to use sentiment expression extraction to determine which sentiment expression is correct. Therefore, we need to measure the semantic association between tf and s . Similar to [2], we use the PMI to measure the semantic association. value as: where | C | is the total number of documents in corpus. A product review corpus crawled from Epinions 1 is used, and each review is split into sentences. We index these sentences of retrieved sentences which contain tf and s respectively. hits ( #uw 10( tf s )) is the count of retrieved sentences in which tf and s co-occur in an unordered window of 10 terms. avoid division by zero, 0.01 is added to the number of hits. The value of PMI reflects semantic association between tf and s . Therefore, we can use this value as a feature for the sentiment expression extraction. generated pros and cons, we index the sentences in pros and cons respectively. Thus, we can compute the PMI value of tf and s on pros corpus and cons corpus respectively. The difference of two PMI value can be used to determine the sentiment orientation of s for tf : pros corpus and cons corpus respectively. If PMI PMI cons &gt;0, the sentiment word s is positive to tf ; it is negative otherwise. for each sentiment expression, and use the polynomial kernel K to compute the similarity between two vectors. Finally, a linear combination of tree kernel K t and polynomial kernel K is formed as follows: 
K SE SE K FST FST K v v  X  X  =+ X  (5) where SE 1 and SE 2 are sentiment expressions. of SE 1 and SE 2 . K p ( v 1 , v 2 ) can be computed as follows: where c is a constant which is set to 1. d is the degree of the polynomial which is set to 2 according to the best performance in our experiment. operate with kernels by replacing the dot product with a kernel function, such as SVM, KNN and Perceptions. In this paper, we select SVM and employ the SVM-light-TK[30] to implement our proposed approach. In addition, we use a sentiment lexicon that contains 8821 words or phrases from OpinionFinder[34]. sentiment expression extraction and sentiment classification. The effect of our approach is then evaluated based on the performance comparison among tree kernels and their linear combination with polynomial kernel. Besides, we change the percentage of training data to demonstrate the learning curves of different kernels, followed by analyzing the parameter in Formula (5). Finally, we compare our approach with other related work. A. Data Set and Evaluation conduct experiments on a corpus that is annotated comprehensively. We choose two data sets to evaluate our approach. The first data set is Customer Review Data used in [12, 13] (Details are shown in Table 1). This corpus only annotates the target features and their sentiment polarities in each sentence. Thus, we re-annotate it by adding sentiment expression boundary labels to each review sentence. collection is much larger than the previous collection. Details are shown in Table 2. This data set is used to generate semantic features via PMI method. validation. The evaluation measures are standard text classification measures, such as accuracy (Acc), precision (P), recall (R) and F-measure (F). B. Overview of Experimental Results all the Kernels for sentiment expression extraction and sentiment classification tasks. Overall, the performance of tree kernels outperforms that of the baseline using polynomial kernel. A combination of tree kernels and polynomial kernel usually demonstrates an improvement over the performance of using them individually. This is because traditional feature vector kernel using standard flat features can provide some individual feature information that is useful for classification, especially when the training data set is relatively large. On the other hand, tree kernels only focus on selecting and composing the most effective features for structure, while ignoring the other features. Therefore, the tree kernels and the polynomial kernel can be considered as complements of each other. negative examples used in the sentiment expression extraction experiment. The performance of GFST is better than that of FST as shown in Table 3. This illustrates that generalizing target feature nodes in FST can improve the performance. Among all tree kernels, GFST m performs the best for all performance measures. This is because the positive and negative examples of GFST m s have fewer common fragments than GFSTs do. GFST m +Poly achieves the best performance as expected with an improvement of 14.51% over baseline. using FST kernel alone do not show enough improvement over the baseline as shown in Table 4. However, the performance improves a lot when FST is generalized into GFST. Besides, GFST p that encodes the polarity information achieves a better performance than GFST. It is worth mentioning that the performance of classification for positive sentiment expression is much better than that of negative sentiment expression. There are two possible reasons to explain this phenomenon. Firstly, negative opinions are usually expressed by customers in a more subtle and implicit way. This causes negative opinions more difficult to be identified than positive opinions. Secondly, in the training data set, the number of negative sentiment expressions is 829, accounting for 32.7% of all; while the number of positive sentiment expressions is 1707, accounting for 67.3% of all. The scale of negative sentiment expressions is significantly lower than that of positive sentiment an improvement of 15.79% over baseline. makes a significant impact on sentiment classification. In fact, sentiment expression extraction identifies the boundary of sentiment expression in a sentence. Hence some unrelated structured or flat features will be removed. The performance of sentiment classification on correctly classified instances of sentiment expression extraction is apparently better than that of incorrect ones. This illustrates the necessity of sentiment expression extraction for sentiment classification. C. Learning Curves performances by changing the percentage of training data from 0.1 to 1, with a step up size of 0.1. The learning curves show that although the performances of all kernels are low with very few training examples, the GFST, GFST m and GFST p kernel outperform the polynomial kernel and FST kernel significantly. This means that using generalized tree kernels can achieve a relatively better performance when small training data set is available. It is because generalized tree kernels generalize some lexicon features on target feature and sentiment word nodes. For example, when training data set is small, sentiment words or target feature terms appearing in test data may not appear in training data. This leads to more misclassified examples because the sentiment words and target feature terms are important indicators for classification. D. Analysis of Parameter experiments. According to Formula (5),  X  controls the proportion of tree kernel in the linear combination. Fig. 6 (a) and (b) show how accuracy varies according with  X  sentiment expression extraction and sentiment classification. The combined kernels actually degenerate into a polynomial kernel, when  X  = 0. We notice that the introduction of tree kernels can significantly improve the performance. But when  X  is approaching to 1, the performance deteriorates gradually. However it is still better than without using tree kernel. When  X  = 1, the combined kernels degenerate into a tree kernel. In more detail, for sentiment expression extraction, the best performance is reached when  X  = 0.4 for GFST+Poly and GFST m +Poly. And when  X  = 0.5, FST achieves the best performance. For sentiment classification, the best performance is reached when  X  = 0.5 for GFST+Poly and GFST p +Poly. And when  X  = 0.3, FST achieves the best performance. E. Results comparison with previous related work OPINE[17], Opinion Observer[13] and our work based on the same benchmark data set (reviews of 5 products selected from data set 1). The precision, recall and F-measure are evaluated based on positive examples. FBS, OPINE and Opinion Observer adopt the approach based on sentiment orientation. This approach determines the sentiment polarity for a target feature by some function based on the positive and negative indicators that defined in sentiment lexicon. We notice that FBS achieves the best result on precision, but the worst on recall. According to the best result (using based on tree kernels outperforms the others for the accuracy, recall and F-measure with a slight loss in precision. solve the task of opinion mining of online product reviews. We take a two-stage strategy which divides the task of opinion mining into two subtasks: sentiment expression extraction and sentiment classification. Then, we define four tree kernels that encoding different structured features for the above two subtasks. A comparative study is conducted among the effect of using each proposed tree kernel, the polynomial kernel and the linear combination of tree kernels and the polynomial kernel. Experimental results illustrate that using generalized tree kernels can achieve a relatively high performance when small training data set is available. The linear combined kernels that integrate traditional polynomial kernel with our proposed tree kernels achieve a noticeable improvement (of 14.51% and 15.79%) over the baseline for sentiment expression extraction and sentiment classification. mining for negative opinions is worse than that for positive opinions. However negative opinion may be more important than positive opinion in some applications. For example, customers would like to read negative feedbacks to make buying decisions, and companies prefer to track negative opinions for product improvement. There is still huge potential for further research to improve the performance for mining negative opinions. Another interesting future research issue is to apply tree kernels to other related tasks, such as opinion question answering and opinion-based information extraction. National Natural Science Foundation (No: 60705022) and Program for New Century Excellent Talent in University, China (No: NCET-06-0161) 
