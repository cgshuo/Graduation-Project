 Multi-label problems arise in various domains such as multi -topic document categorization and protein function predic -tion. One natural way to deal with such problems is to con-struct a binary classifier for each label, resulting in a set o f independent binary classification problems. Since the mult i-ple labels share the same input space, and the semantics con-veyed by different labels are usually correlated, it is essen tial to exploit the correlation information contained in differe nt labels. In this paper, we consider a general framework for extracting shared structures in multi-label classificatio n. In this framework, a common subspace is assumed to be shared among multiple labels. We show that the optimal solution to the proposed formulation can be obtained by solving a generalized eigenvalue problem, though the problem is non-convex. For high-dimensional problems, direct computatio n of the solution is expensive, and we develop an efficient al-gorithm for this case. One appealing feature of the proposed framework is that it includes several well-known algorithm s as special cases, thus elucidating their intrinsic relatio nships. We have conducted extensive experiments on eleven multi-topic web page categorization tasks, and results demonstra te the effectiveness of the proposed formulation in comparison with several representative algorithms.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms Multi-label classification, shared subspace, least square s loss
Learning and mining from objects annotated with mul-tiple labels is a frequently encountered and widely studied problem in many domains. For example, in web page cate-gorization [27, 19, 28], a web page can be assigned to mul-tiple topics. In gene and protein function prediction [6, 25 ], multiple functional labels are associated with each gene an d protein, since an individual gene or protein usually perfor ms multiple functions. In automated newswire categorization , multiple labels can be associated with a newswire story in-dicating its subject categories and the regional categorie s of reported events [34]. One common aspect of these prob-lems is that multiple labels are associated with a single ob-ject, and they are hence called multi-label problems. Such problems are more general than the traditional multi-class problems in which a single label is assigned to an object. Driven by various applications, such problems are receivin g increasing attention now [22, 16, 8, 33, 12, 18, 30].
One simple and popular approach for multi-label classi-fication is to construct a binary classifier for each label in which instances relevant to this label form the positive cla ss, and the rest form the negative class. This approach has been applied successfully to various applications [9, 31, 1 7, 9]. However, it fails to capture the correlation informatio n among different labels, which is critical for many applica-tions where the semantics conveyed by different labels are correlated. Indeed, it has been shown that the decoupling of multiple labels may compromise the performance signifi-cantly in certain applications [27]. For example, in modeli ng the topics and authorship of documents, it is evident that the topics and authors of documents are correlated, since a particular author may only write on certain topics. Hence, it is essential to model them in a coordinated fashion so that their intrinsic relationships can be captured.

In this paper, we propose a general framework for extract-ing shared structures (subspace) in multi-label classifica tion. In this framework, a binary classifier is constructed for eac h label to discriminate this label from the rest of them. How-ever, unlike the approach that build the binary classifier independently, a low-dimensional subspace is assumed to be shared among multiple labels. The predictive functions in our formulation consist of two parts: the first part is con-tributed from the representations in the original data spac e, and the second one is contributed from the embedding in the shared subspace. A similar formulation has been proposed in [3] for multi-task learning. We show that when the least squares loss is used in classification, the linear transform a-tion that characterizes the shared subspace can be computed by solving a generalized eigenvalue problem. In contrast, the formulation proposed in [3] is non-convex and needs to be solved iteratively. For high-dimensional problems, dir ect computation of the solution is computationally expensive, and we develop an efficient algorithm for this case. One appealing feature of the proposed framework is that it in-cludes several well-known algorithms as special cases, thu s elucidating their intrinsic relationships. We have conduc ted extensive experiments on eleven multi-topic web page cat-egorization tasks, and results demonstrate the effectivene ss of the proposed formulation. Experimental results also sho w that the proposed formulation based on least squares loss is comparable to other formulations based on hinge loss, while it is much more efficient.

The key contributions of this paper are highlighted as fol-lows:
The rest of this paper is organized as follows: We present the framework for extracting shared subspace in Section 2. The efficient algorithm for computing the solution is devel-oped in Section 3. We discuss its relationship with existing formulations in Section 4 and report experimental results i n Section 5. Then we conclude and discuss further research in Section 6.
 Notations : We use n , d , and m to denote the number of training instances, the data dimensionality, and the numbe r of labels, respectively. The data matrix and the label indi-cator matrix are denoted as X = [ x 1 , , x n ] T  X  IR n  X  d Y  X  IR n  X  m , where x i  X  IR d is the i th instance, and Y if the i th instance has the  X  th label, and  X  1 otherwise.
We are given a set of input data { x i } n i =1  X  R d and the class label indicator matrix Y  X  R n  X  m that encodes the la-bel information, where m and n are the number of labels and the number of instances, respectively. Following the tradi -tional supervised learning framework, we learn m functions { f  X  } m  X  =1 from the data that minimize the following regular-ized empirical risk: where y  X  i = Y i X  , L is a prescribed loss function,  X ( f ) is a regularization functional measuring the smoothness of f , and &gt; 0 is the regularization parameter.
We propose a multi-label learning framework, in which a low-dimensional subspace is shared by all labels. The pre-dictive functions in this framework consist of two parts: on e part is contributed from the original data space, and the other part is derived from the shared subspace as follows: where w  X   X  R d and v  X   X  R r are the weight vectors,  X   X  R r  X  d is the linear transformation used to parameterize the shared low-dimensional subspace, and r is the dimensional-ity of the shared subspace. The transformation  X  is common for all labels, and it has orthonormal rows, that is  X  X  T In this formulation, the input data are projected onto a low-dimensional subspace by  X , and this low-dimensional projec -tion is combined with the original representation to produc e the final prediction. Note that a similar formulation has been proposed in [3] to capture the shared predictive struc-tures in multi-task learning, and our formulation differs wi th it in important aspects (see Section 4.1 for a comparison).
Following the regularization formulation in Eq. (1), we propose to estimate the parameters { w  X  , v  X  } m  X  =1 and  X  by minimizing the following regularized empirical risk:
X ( 1 subject to the constraint that  X  X  T = I . Note that in the above formulation, the first regularization term || w  X  || trols the amount of information shared by all labels, while the second regularization term || w  X  +  X  T v  X  || 2 controls the complexity of the model. By a change of variable, this prob-lem can be reformulated equivalently as follows: min In this paper, we consider the least squares loss, i.e., It has been shown [11, 24] that the least squares loss func-tion is comparable to other loss functions such as the hinge loss employed in support vector machines (SVM) [26] when appropriate regularization is added. Hence, we get the fol-lowing optimization problem: min where X = [ x 1 , , x n ] T  X  R n  X  d is the data matrix, y [ y , , y  X  n ] T  X  R n . The formulation in Eq. (4) can be ex-pressed compactly as: where kk F denotes the Frobenius norm of matrix [13], U = [ u 1 , , u m ], and V = [ v 1 , , v m ].
We show that the optimal V  X  that solves the optimization problem in Eq. (5) can be expressed in terms of  X  and U , as summarized in the following lemma:
Lemma 2.1. Let U , V , and  X  be defined as above. Then the optimal V  X  that solves the optimization problem in Eq. (5) is given by V  X  =  X  U .

Proof. The only term in Eq. (5) that depends on V is || U  X   X  T V || 2 F , which can be expressed equivalently as: || U  X   X  T V || 2 F = tr( U T  X  V T  X )( U  X   X  T V ) (6) where tr( ) denote the trace of matrix, and we have used the property that for any matrix A . Taking the derivative of the expression in Eq. (6) with respect to V , and setting it to zero, we obtain where we have used the property that  X  X  T = I . This com-pletes the proof of the lemma. It follows from Lemma 2.1 that the objective function in Eq. (5) can be rewritten as: = 1 n k XU  X  Y k 2 F +  X  || U  X   X  T  X  U || 2 F +  X  || U || = 1 Hence, the optimization problem in Eq. (5) can be expressed equivalently as: min s. t.  X  X  T = I. (8) We show that the optimal U  X  can be expressed in terms of  X . This is summarized in the following lemma: Lemma 2.2. Let X , Y , U , and  X  be defined as above. Then the optimal U  X  that solves the optimization problem in Eq. (8) can be expressed as: where M is defined as: Proof. Taking the derivative of the objective function in Eq. (8) with respect to U , and setting it to zero, we obtain where M is defined in Eq. (10).
It follows from Lemma 2.2 that we can substitute the ex-pression for U  X  in Eq. (9) into Eq. (8) and obtain the fol-lowing optimization problem with respect to  X :
We show in the following theorem that the optimization problem in Eq. (12) can be simplified, and the optimal  X   X  can be obtained by solving a generalized eigenvalue problem . Theorem 2.1. Let X , Y , and  X  be defined as above. Then the optimal  X   X  that solves the optimization problem in Eq. (12) can be obtained by solving the following trace maximization problem: where S 1 and S 2 are defined as: and M is defined in Eq. (10).

Proof. We need the Sherman-Woodbury-Morrison for-mula [13] for computing matrix inverse: It follows from the formula in Eq. (16) that where the last equality follows since  X  X  T = I . By substitut-ing the expression in Eq. (17) into the optimization problem in Eq. (12), we obtain the following problem: max s. t.  X  X  T = I, (18) where we have omitted the term Y T XM  X  1 X T Y since it is independent of  X . By using the property that tr( AB ) = tr( BA ) for any two matrices A and B , and noticing the definitions of S 1 and S 2 in Eqs. (14) and (15), respectively, we prove this theorem.

Let Z = [ z 1 , , z r ] be the matrix consisting of the top r eigenvectors corresponding to the largest r nonzero eigen-values of the generalized eigenvalue problem: S 1 z =  X S 2 Let Z = Z q Z r be the QR decomposition of Z , where Z q has orthonormal columns and Z r is upper triangular. It is easy to verify [32] that the objective function in Eq. (13) is inva ri-ant of any nonsingular transformation, that is, Q and NQ achieve the same objective value for any nonsingular matrix N  X  IR r  X  r . It follows that the optimal Q  X  solving Eq. (13) is given by Q  X  = Z T q . Note that S 1 is positive definite (see Eq. (20) below), thus Z can also be obtained by computing the top eigenvectors of S  X  1 1 S 2 .
From the discussions in the last section, the optimal  X   X  is given by the eigenvectors of S  X  1 1 S 2  X  R d  X  d corresponding to the r largest eigenvalues. When the data dimensionality, i.e., d , is small, the eigenvectors of S  X  1 1 S 2 can be computed directly. However, when d is large, direct eigendecomposi-tion is computationally expensive. In this section, we show how we can compute the eigenvectors efficiently for this case.
It follows from the Sherman-Woodbury-Morrison formula in Eq. (16) that M Hence, we have I  X   X M  X  1 =  X  which is positive definite when  X  &gt; 0.

It follows from the definitions of M , S 1 , and S 2 in Eqs. (10), (14), and (15) that = ( M  X   X I )  X  1 X T Y Y T XM  X  1 (21) = 1 Let be the singular value decomposition (SVD) [13] of X , where U  X  IR n  X  n and V  X  IR d  X  d are orthogonal, is diagonal, and t = rank( X ). Let U = [ U 1 , U 2 ], where U IR and V 2  X  IR d  X  ( d  X  t ) , and  X  t consists of the first t rows and the first t columns of  X . Then we have + 1  X  V 2 I  X  1 V T 2 X T Y Y T X 1 n X T X + (  X  +  X  ) I = V 1 ( 1 = V 1 ( 1 = V 1 ( 1 The second and the third equalities follow since the columns of V 2 are in the null space of X , that is, Define three diagonal matrices D 1 , D 2 , and D as follows: Then we have where Denote C = Y T U 1  X  D  X  R m  X  t and let be the SVD of C where P 1  X  R m  X  m and P 2  X  R t  X  t are orthogonal, and  X   X  R m  X  t is diagonal. Then where  X   X  =  X  T  X   X  R t  X  t . It follows from Eq. (28) that the eigenvectors of S  X  1 1 responding to nonzero eigenvalues are given by the columns of V 1 DP 2 . The algorithm for computing the optimal  X   X  high-dimensional data is summarized as follows:
After obtaining  X   X  , we need to compute the optimal U  X  given by Eq. (9). Note that the matrix M  X  IR d  X  d is in-volved in Eq. (9), and hence it is expensive to compute U  X  directly for high-dimensional data. More specifically, we need to make use of the expressions in Eqs. (17), (19), and (20) so that explicit formations of the matrices M and M  X  1 are avoided.

The SVD of X in the first step takes O ( dn 2 ) time assum-ing d &gt; n . The size of C is m  X  t where m is the number of tasks and t = rank( X ). Hence the SVD of C in the third step takes O ( tm 2 ) time assuming t &gt; m . The QR decompo-sition in the fourth step takes O ( dt 2 ) time. Typically, m and t are both small. Thus, the cost of the proposed algorithm for computing  X   X  is dominated by the cost for computing the SVD of X . A summary of relevant matrices and their associated computational complexity are listed in Table 1. Table 1: Summary of relevant matrices. The size, computation required, and the associated complex-ity of each relevant matrix are listed.

In this section, we show that the proposed formulation includes several well-known algorithms as special cases. W e begin by discussing related work.
Dimensionality Reduction Canonical correlation anal-ysis (CCA) [15] and partial least squares (PLS) [29, 4] are classical techniques for modeling relations between sets o f observed variables. They both compute low-dimensional embedding of sets of variables simultaneously. Their main difference is that CCA maximizes the correlations between variables in the embedded space, while PLS maximizes their covariances. One popular use of CCA and PLS is for su-pervised learning, in which one set of variables are derived from the data and another set is derived from the class la-bels. In this setting, the data can be projected onto a lower-dimensional space directed by the label information. Such formulation is particularly appealing in the context of di-mensionality reduction for multi-label data. When applied to multi-class problems, CCA reduces to the well-known lin-ear discriminant analysis (LDA) formulation [10] in which a projection is obtained by maximizing the ratio of inter-cla ss distance to intra-class distance.

Multi-task Learning In [3], a similar formulation has been proposed for multi-task learning. In this formulation , the input data for different tasks can be different, and the following optimization problem is involved: min where x  X  i is the i th instance in the  X  th task and n  X  number of instances in the  X  th task. It is shown [3] that the resulting optimization problem is non-convex even for convex loss functions. Hence, an iterative procedure calle d the alternating structure optimization (ASO) algorithm is proposed to compute a locally optimal solution. A similar idea of sharing part of the model parameters among multiple tasks has been explored in the Bayesian framework [5].
Multi-class Learning Formulation for extracting shared structures in multi-class classification has been proposed re-cently [1]. In this formulation, a low-rank transformation is computed to uncover the shared structures in multi-class classification. The final prediction is solely based on the lo w-dimensional representations in the dimensionality-reduc ed space. Moreover, the low-rank constraint is non-convex, an d it is first relaxed to the convex trace norm constraint. The relaxed problem can be formulated as a semidefinite program which is expensive to solve. Hence, gradient-based optimiz a-tion technique is employed to solve the relaxed problem.
The formulation proposed in Section 2 includes several existing algorithm as special cases. In particular, by sett ing the regularization parameters  X  and  X  in Eq. (5) to different values, we obtain several well-known algorithms.
In this section, we evaluate the proposed formulation on multi-topic web page categorization tasks.
The multi-topic web page categorization data sets were described in [27, 19, 28], and they were compiled from 11 top-level categories in the  X  X ahoo.com X  domain. The web pages collected from each top-level category form a data set . The top-level categories are further divided into a number of second-level subcategories, and those subcategories fo rm the topics to be categorized in each data set. Note that the 11 multi-topic categorization problems are compiled an d solved independently as in [27]. We preprocess the data sets by removing topics with less than 100 web pages, words occurring less than 5 times, and web pages without topics. We use the TF-IDF encoding to represent web pages, and all web pages are normalized to unit length. The statistics of all data sets are summarized in Table 2.

We use area under the receiver operating characteristic (ROC) curve, called AUC, and F1 score as the performance measure. To measure the performance across multiple labels using F1 score, we use both the macro F1 and the micro F1 scores [21, 31]. The F1 score depends on the threshold values of the classification models, and the thresholds computed by models are usually not optimized for it. Indeed, all methods yield low F1 scores when the thresholds computed by models are used. It is shown recently [9] that tuning the threshold based on F1 score on the training data can significantly im-prove performance. Hence, we tune the threshold value of each model based on the training data.
We evaluate the proposed formulation on the 11 multi-topic web page categorization data sets. The experimental results on five relevant methods are also reported. Parame-ters of all the methods are tuned using 5-fold cross-validat ion based on F1 score. The setup is summarized as follows: Table 2: Statistics of the Yahoo data sets. m , d , and N denote the number of labels, the data dimensionality, and the total number of in-stance, respectively, in the data set after prepro-cessing.  X  X axNPI X / X  X inNPI X  denotes the max-imum/minimum number of positive instances for each topic (label).
 Data set m d N MaxNPI MinNPI Arts 19 17973 7441 1838 104 Business 17 16621 9968 8648 110 Computers 23 25259 12371 6559 108 Education 14 20782 11817 3738 127 Entertainment 14 27435 12691 3687 221 Health 14 18430 9109 4703 114 Recreation 18 25095 12797 2534 169 Reference 15 26397 7929 3782 156 Science 22 24002 6345 1548 102 Social 21 32492 11914 5148 104 Society 21 29189 14507 7193 113 The SVM problems are solved using the LIBSVM [7] soft-ware package. All the codes and data sets used for the ex-periments are available at the supplemental website 1 .
We randomly sample 1000 data points from each data set as training data (each label is guaranteed to appear in at least one data point), and the remaining data points are used as test data. This process is repeated five times to gen-erate five random training/test partitions, and the average d performance and standard deviations are reported. Tables 3 and 4 show the performance of the six methods in terms of AUC, macro F1, and micro F1. We can observe that the proposed formulation outperforms all other five compared methods in terms of macro F1 score on all of the 11 data sets. In terms of AUC, the proposed formulation achieves the highest AUC on 9 data sets, while SVM-based meth-ods achieve the highest AUC on the other two data sets. In terms of micro F1 score, the proposed formulation outper-forms other methods on 10 data sets. The low performance of ASO SVM may be due to the early termination of its it-erative procedure in parameter tuning, since it is computa-tionally very expensive. In general, tuning the parameter C in SVM with cross-validation yield a performance improve-ment of about 1% in most cases. On some of the data sets, the performance of CCA+SVM and SVM is different. This may be due to the numerical problems encountered when solving the eigenvalue problem related to CCA. The perfor-mance improvement achieved by the proposed formulation over other compared methods is consistent across data sets and performance measures.
We evaluate the scalability of the proposed multi-label formulation on the Health and Science data sets which con-tain the minimum and maximum number of labels among the 11 data sets. In particular, we increase the number of training samples on the Health and Science data sets grad-ually, and record the computation time of ML LS , SVM, and ASO SVM . The training time for a fixed parameter setting and the time for parameter tuning using cross-validation ar e plotted in Figure 1. We can observe that SVM is the fastest http://www.public.asu.edu/~sji03/multilabel/ Algorithm Arts Business Computer Education Entertainment Health Figure 1: Comparison of computation time for ML LS , SVM, and ASO and ASO SVM is the slowest among the three compared algo-rithms. Moreover, the difference between ML LS and SVM is small. The computational cost of the proposed formulation is dominated by the cost of SVD computation on the data matrix X , and it is independent of the number of labels. In contrast, the computational costs of SVM and ASO SVM de-pend on the number of labels. Hence, the difference between SVM and ML LS tends to be smaller on the Science data set, since the Science data set has a larger number of labels than the Health data set (14 and 22 labels, respectively). Note that in ML LS , the two regularization parameters  X  and  X  are tuned using double cross-validation. However, the SVD on X needs to be computed only once irrespective of the size of the candidate sets for  X  and  X  . This experiment also shows that the running time of ASO SVM may fluctuate as the number of training instances increases. This may be due to the fact that the convergence rate of the ASO SVM algorithm depends on the initialization.
We conduct experiments to evaluate the sensitivity of the proposed formulation to the values of the regularization pa -rameters  X  and  X  . We randomly sample 1000 data points from each of the three data sets Arts, Recreation, and Sci-ence, and the averaged macro F1 scores over 5-fold cross-validation for different values of  X  and  X  are depicted in Figure 2. We can observe that the highest performance on all three data sets is achieved at some intermediate values o f  X  and  X  . Moreover, this experiments show that the perfor-mance of the proposed multi-label formulation is sensitive to the values of the regularization parameters. Note that the parameter tuning time of the proposed formulation does not depend on the size of the candidate sets directly, since the computational cost is dominated by that of the SVD of X which needs to be performed only once. Hence, large candidate sets for  X  and  X  can be employed in practice.
We present a framework for extracting shared subspace in multi-label classification in this paper. In this frame-work, a subspace is assumed to be shared among multiple labels, and a linear transformation is computed to discover this subspace. We show that when the least squares loss is reported. The highest performance is highlighted in each ca se. panel) data sets. used in classification, the optimal solution for the propose d formulation can be computed via a generalized eigenvalue problem. For high-dimensional data, direct computation of this problem is computationally expensive, and we develop an efficient algorithm for this case. We show that the pro-posed formulation is a general framework that includes sev-eral well-known formulations as special cases. Experiment al results on eleven multi-topic web page categorization task s show that the proposed formulation outperforms competing methods in most cases.

Our results show that applying regularization on both parts of the predictor can potentially improve performance . We have attempted to compare the proposed formulation with an extension of the ASO algorithm in which both parts of the predictor are regularized. However, this extension of the ASO algorithm is computationally demanding when both regularization parameters are tuned using double cros s-validation. Hence, we are not able to include its results in this paper. We will explore ways to improve the efficiency of this algorithm in the future. The data matrices in many applications such as the ones used in this paper are sparse. Hence, techniques for computing the SVD of sparse matrices as proposed in [20] can be employed to expedite the compu-tation. We plan to apply such techniques in our algorithm in the future.
 This research is sponsored in part by the Arizona State Uni-versity and by the National Science Foundation Grant IIS-0612069. [1] Y. Amit, M. Fink, N. Srebro, and S. Ullman.
 [2] E. D. Andersen and K. D. Andersen. The MOSEK [3] R. K. Ando and T. Zhang. A framework for learning [4] J. Arenas-Garc  X  X a, K. B. Petersen, and L. K. Hansen. [5] B. Bakker and T. Heskes. Task clustering and gating [6] Z. Barutcuoglu, R. E. Schapire, and O. G.
 [7] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [8] A. Elisseeff and J. Weston. A kernel method for [9] R.-E. Fan and C.-J. Lin. A study on threshold [10] K. Fukunaga. Introduction to statistical pattern [11] G. M. Fung and O. L. Mangasarian. Multicategory [12] N. Ghamrawi and A. McCallum. Collective multi-label [13] G. H. Golub and C. F. Van Loan. Matrix [14] A. Hoerl and R. Kennard. Ridge regression: Biased [15] H. Hotelling. Relations between two sets of variates. [16] R. Jin and Z. Ghahramani. Learning with multiple [17] T. Joachims. Text categorization with support vector [18] F. Kang, R. Jin, and R. Sukthankar. Correlated label [19] H. Kazawa, T. Izumitani, H. Taira, and E. Maeda. [20] R. M. Larsen. Computing the SVD for large and [21] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [22] A. McCallum. Multi-label text classification with a [23] H. Park, M. Jeon, and J. B. Rosen. Lower dimensional [24] R. Rifkin and A. Klautau. In defense of one-vs-all [25] V. Roth and B. Fischer. Improved functional [26] S. Sch  X  olkopf and A. Smola. Learning with Kernels: [27] N. Ueda and K. Saito. Parametric mixture models for [28] N. Ueda and K. Saito. Single-shot detection of [29] H. Wold. Estimation of principal components and [30] R. Yan, J. Tesic, and J. R. Smith. Model-shared [31] Y. Yang and J. O. Pedersen. A comparative study on [32] J. Ye. Characterization of a family of algorithms for [33] K. Yu, S. Yu, and V. Tresp. Multi-label informed [34] J. Zhang, Z. Ghahramani, and Y. Yang. Learning
