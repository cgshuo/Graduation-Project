 Community Question Answering (C QA) services have evolved into a popular way of information seeking and providing. User-posted questions in CQA are generally organized into hierarchical categories. In this paper, we define and study a novel problem which is referred to as New Category Identification (NCI) in CQA question archives. New Category Identification is primarily concerned with detecting and characterizing new or emerging categories which are not included in the existing category hierarchy. We define this pr oblem formally, and propose both unsupervised and semi-supervised topic modeling methods to solve it. Experiments with a ground-truth set built from Yahoo! Answers show that our methods identify and interpret new categories effectively. H.3.5 [ Information Storage and Retrieval ]: Online Information Services  X  Web-based services. Algorithms, Performance, Experimentation. Community Question Answering, Topic Modeling, New Category. 
With the blooming of Web 2.0, user-generated contents (UGC) such as Wikipedia, YouTube and Flickr begin to flourish. One type of UGC sites is the Community Question Answering (CQA) services, which enable users to post or answer questions on various subjects. Among CQA webs ites, Yahoo! Answers is now becoming the most popular portal. Since its launch in 2005, Yahoo! Answers has attracted millions of users, and has stored a tremendous number of community que stions in its database. As the volume of these questions is growing to an intractably huge size, how to manage them efficiently and effectively has become an increasingly important research issue. 
In Yahoo! Answers, the community questions are organized in the form of hierarchical categories. However, the maintenance of this category hierarchy highly relies on human efforts, and usually its structures remain unchange d in a fairly long period. Consequently, the current categories are definitely unable to capture newly-arising topics whic h are attracting intensive public attention. Questions belonging to neither of the existing categories would be assigned by users into the pseudo  X  Other  X  category, e.g.,  X  X ther-Internet X  in the  X  X nternet X  domain. These accumulating Other questions bring difficulties and inconvenience to both users and CQA service providers. In this paper, we study extensively the problem of New Category Identification (NCI) in CQA, which aims to find potential categories not included currently. This problem subsumes inte resting applications in that the CQA category structures can be enriched and refined continuously. 
Recently, there has been a grow ing amount of research [2, 3, 4, 5, 8, 9, 10, 11] on Community Question Answering. However, there are yet no mechanisms with which we can find new categories in the CQA question archives. In this study, we formulate the novel NCI problem as a topic modeling issue. We first adapt PLSA, a basic algorithm in the context of topic modeling, to New Category Identif ication. Essentially, PLSA is an unsupervised method, i.e., we are unaware of the meaning of the generated categories, even after the model has been fully estimated. We develop semi-supe rvised topic modeling methods to solve this challenge. Specifi cally, we cast the prior knowledge about specific categories into PLSA in a probabilistic manner and fit the model to the question collection with Maximum A Posterior (MAP) estimation. With the model estimated, we can then naturally reach the categories to be identified. 
The rest of the paper is organized as follows. In Section 2, we formally define the problem of New Category Identification. After that, we present the methods in Section 3. Then, we show and discuss the experimental results in Section 4. Finally, we have the conclusion and future work in Section 5. 
In Yahoo! Answers, one commun ity question usually consists of three parts, i.e., the subject (a brief statement of the question), the content (additional detailed descriptions of the question) and the answers posted by other users. We define the associated text of a community question as the concatenation of its subject, content and answers. 
Formally, we let D denote the domain we are interested in, and 12 {, , } Qqq = L denote a collection of Other questions in  X 
D . When considering the associated text, each question qQ  X  can be described as a word vector as follows. where (,) h cw q is the number of occurrences of word associated text of question q , and V is the whole set of words in the collection Q . Our basic idea is to perform topic modeling on the collection Q and group Other questions into distinct topics. After topic modeling, the topical structures of the collection Q can be represented as where each question group i G corresponds to an underlying topic in Q . Then qualified question groups are selected out as categories that we have identified. We define three criteria for group selection. These criteria can be found in a more detailed version 1 of this paper. 
Probabilistic latent semantic analysis (PLSA) [1] has been applied to topic modeling with promising results [6, 7, 12]. For the NCI problem, our idea is to use a unigram language model (a multinomial word distribution) to model a group (topic). To be consistent with previous literature, we still define the k unigram language models as 12 {, , , } k  X   X  X   X = L which capture individual groups. Then each word h w in question q is generated from a two-stage process: first, a group j  X  is chosen conditionally for the question according to , qj  X  ; second, the word h w is generated from j  X  according to the conditional probability ( | ) From a statistical perspective, the question collection Q is the observed data, and its log-likelihood is described as where  X  represents the set of model parameters, , qj measures the conditional probability of choosing j  X  given q . 
We perform Maximum Likelihood Estimation using the EM algorithm to estimate the model. The latent variable defined as the group from which the word h w in question q is generated. During the estimation process, the model parameters are updated iteratively as follows. E-step: http://dm.thss.tsinghua.edu.cn: 8080/yajiem/cikm-detailed.pdf where , () question q is generated from the th j group. 
When the iterative estimation process converges, we assign question q into the group which has the largest group mapping function is 
In the prior-PLSA method, we adopt this group assignment strategy as well. 
In many application scenarios, users indeed know what potential categories they are interested in. For example, we want to know whether  X  X witter X  should be a new category in the  X  X nternet X  domain. In this case, it would be nice if we establish  X  X witter X  as a predefined facet and guide topic modeling with this prior knowledge. From PLSA, we s ee that the estimation results are language models whose elements are ( | ) hj pw  X  . Therefore, it is natural to also input prior knowledge as language models. Specifically, we may want to input j  X  as the prior topic model for topic j given by the user. For  X  X witter X , probability to words like  X  X weet X ,  X  X ollow X ,  X  X ser X ,  X  X essage X , distributions manually. In our deta iled version, we also formulate how to get prior knowledge automatically from Wikipedia. 
On the language model j  X  , we define a Dirichlet prior 
Dir p w  X  X   X  + X  using j  X  , where the factor j  X  indicates how strong our confidence is on the prior j  X  . This prior determines the probability of a specific setting of therefore is called a distribution of distribution . Dirichlet distribution is a conjugate prio r for multinomial distribution and we will see the advantage of this conjugacy in parameter estimation. Then the probability of j  X  can be formulated as where the beta function ({1 ( | )} ) normalizing factor which can be expressed into combination of gamma functions. In order to keep the form of the prior uncluttered, we omit this factor. Then the prior for the whole parameter set  X  is 
With the prior defined above, we turn to Bayesian inference to maximize the posterior probability of the parameters  X  after we have been give the observed data Q , rather than maximize the likelihood of Q . For parameter estimation, we need to find a set of parameters  X   X  as Then for the log-likelihood value, we have where (|) LQ  X  remains the same as Equation (3), ( ) L  X  is the log-likelihood for the prior ( ) p  X  , and const is a constant value. 
We can use the Maximum A Posterior (MAP) estimator to obtain the parameters. As revealed by Equation (7), ( ) L  X  is independent of the latent variable ,  X  . So the introduction of the prior only affects the estimation of ( | ) hj pw  X  . The overall MAP estimation is performed by rewriting the updating formula for ( | ) hj pw  X  . For completeness, we give the updating formulas as follows. E-step:
Due to usage of a conjugate prior, we can see that the updating formula for ( | ) hj pw  X  here has the similar form with that of PLSA. An instructive interpretati on of this formula is: for each topic j  X  , we observe an additional pseudo community question, whose size (number of words) is j  X  and whose word distribution follows the prior distribution (|) hj pw  X  . Therefore, the conjugacy of the Dirichlet prior allows for a tractable and interpretable solution to the MAP estimation. 
After the model is estimated, we can directly reach the categories that we expect to id entify, without any indication from knowledge about  X  X witter X . Then the question group corresponding to j  X  , is exactly on the topic  X  X witter X . For convenience, this semi-supe rvised method is called prior-PLSA . 
With the APIs 2 provided by Yahoo! De veloper Network, we create an inclusive dataset by downloading 6000 questions from  X  X ther-Internet X  in the  X  X nternet X  domain. These questions have http://developer.yahoo.com/answers/ been issued over a period from January to April 2010. We only focus on the resolved questions, meaning questions that have been given their best answers. Fo r preprocessing, we perform document frequency feature selection on the vocabulary: those words which appear in less than three questions are removed. 
We first run the unsupervised met hod on the dataset. In Table 1, we present the sample results of three groups that are generated by PLSA and filtered with the three criteria. The table shows the top 10 questions that are ranked according to , qj discover and interpret the three categories in a meaningful way. The first group is about Twitter and the second one is about eBay. The third group is talking about Lockerz, a website which was launched in early 2010. These three categories do not currently exist under the  X  X nternet X  domain. 
Although some questions are also noisy or misclassified, most of the top 10 questions are assigne d to the right categories, which is sufficient to help us recognize the underlying semantics of the categories. 
These three categories are taken as target categories in our experiments. In principle, we ar e able to identify any categories with semi-supervised methods , only if we can obtain the appropriate prior knowledge. However, in order for performance comparison, we also identify th e three target categories when running prior-PLSA. Since j  X  represents the size of a pseudo question, we heuristically set j  X  to the average size of the questions in Q , which is 179 in our dataset. 
In order to quantitatively evaluate the methods, we ask volun-teers (graduate students from our department) to annotate the results of the three target categories. In particular, annotators judge and then mark each community question as  X  X elevant X  or  X  X rrelevant X  to the category it belongs to. To ensure these standard outputs to be precise and consiste nt, we give rigorous annotation guidelines to the annotators. In cases when the questions cannot be determined directly, e.g.,  X  X ow long will it take? X , the annotators are required to browse these questions X  pages for final judgment. After obtaining the human annotated results, we evaluate the methods w ith two metrics, i.e., Hit Number and Weighted Precision . 
Hit Number aims to measure how many relevant questions are absorbed into each target categor y, which gives basic perspective into the inner structure of each category. Weighted Precision is the precision of each category when we consider the weight of each question. Formally, if we denote a target category as the relevant questions in it as i C % , then Weighted Precision is defined as 
The intuition behind Weighted Purity is that the important questions contribute more to the purity of the category, whereas the performance will be given penalty if irrelevant questions are given high importance. We compare the two methods on the Twitter, eBay and Lockerz target categories in Table 2. Note that the performance of topic modeling depends on initialization of parameters. For parameter values. The prior knowledge gives predefined descriptions of the target cate gories, with which prior-PLSA can adjust the estimation process to make the parameter values close to the prior distributions. This eventually enables prior-PLSA to absorb more relevant questions into each category, and the Hit Number of prior-PLSA is therefore generally larger than that of the basic PLSA. Also, prior-PLSA consistently outperforms PLSA on Weighted Precision. The introduced prior knowledge acts to  X  X hape X  the basic structures of the formed target categories. Due to the restriction and shaping of prior knowledge, the absorbed questions mostly conform to the overall features of the target categories. Therefore, prior-PLSA results in more high-quality categories and performs more e ffectively for the New Category Identification problem. In this paper, we study the novel problem of New Category Identification. We give the formal description of this problem, and propose both unsupervised and se mi-supervised topic modeling methods to solve it. The results show that our methods perform effectively in finding and interpreting potential categories in CQA. For future work, we will try to test our methods on other domains in Yahoo! Answers. Also, we consider extending this problem to other social media websites such as blogs and online forums. This work was supported by Nati onal Natural Science Funding of China under Grant No. 90718022 and National 863 Plans Project under Grant No. 2009AA01Z410. Also , Jie Tang was supported by National High-tech R&amp;D Program (No. 2009AA01Z138). [1] T. Hofmann. Probabilistic Latent Semantic Indexing. In [2] J. Jeon, W. B. Croft, J. H. Lee, and S. Park. A Framework to [3] P. Jurczyk, and E. Agichtein. Discovering Authorities in [4] Y. Liu, J. Bian, and E. Ag ichtein. Predicting Information [5] Y. Liu, N. Narasimhan, V. Va sudevan, and E. Agichtein. Is [6] Y. Lu, and C. Zhai. Opinion Integration Through Semi-[7] Y. Lu, C. Zhai, and N. Sundaresan. Rated Aspect [8] M. Qu, G. Qiu, X. He, C. Zhang, H. Wu, J. Bu, and C. Chen. [9] K. Wang, Z. Ming, and T. S. Chua. A Syntactic Tree Matching [10] X. J. Wang, X. Tu, D. Feng, and L. Zhang. Ranking [11] X. Xue, J. Jeon, and W. B. Croft. Retrieval Models for [12] C. Zhai, A. Velivelli, and B. Yu. A Cross-Collection Mixture 
