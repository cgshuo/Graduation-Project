 Information retrieval systems are evaluated against test collections of topics, documents, and assessments of which documents are rel-evant to which topics. Documents are chosen for relevance assess-ment by pooling runs from a set of existing systems. New sys-tems can return unassessed documents, leading to an evaluation bias against them. In this paper, we propose to estimate the de-gree of bias against an unpooled system, and to adjust the system X  X  score accordingly. Bias estimation can be done via leave-one-out experiments on the existing, pooled systems, but this requires the problematic assumption that the new system is similar to the ex-isting ones. Instead, we propose that all systems, new and pooled, be fully assessed against a common set of topics, and the bias ob-served against the new system on the common topics be used to adjust scores on the existing topics. We demonstrate using resam-pling experiments on TREC test sets that our method leads to a marked reduction in error, even with only a relatively small num-ber of common topics, and that the error decreases as the number of topics increases.
 H.3.4 [Information Storage and Retrieval]: Systems and software  X  performance evaluation .
 Retrieval experiment, evaluation, system measurement Measurement, performance, experimentation
Information Retrieval (IR) systems are evaluated using test col-lections, which contain a document corpus, a set of topics to run against that corpus, and judgments (called qrels) as to which docu-ments are relevant to which topics. A system returns a ranked list of documents or run for each query. The documents are marked Copyright 2009 ACM 978-1-60558-483-6/09/07 ... $ 5.00. for relevance using the qrels, and an evaluation metric is applied to the resulting vector of relevancies to calculate an effectiveness score for the run. The mean of the per-topic scores becomes the effectiveness score for the system against the test collection.
Relevance judgments are performed by human assessors, and are expensive to collect. It is not in general practical to assess every document in the corpus for relevance to every topic. Instead, the top documents from the runs of the systems participating in the test collection X  X  original experiment are pooled , and only these docu-ments are assessed. The assumption behind pooling is that, if a diverse enough range of good systems contribute to the pool, and if the systems are pooled to a sufficient depth, then the pool should contain  X  X lmost all X  the documents in the corpus relevant to each topic. If a new, unpooled system is run against the test collection, and it returns unpooled, unassessed documents, these documents can reasonably be assumed irrelevant . As a result, despite its in-completeness, the test collection is acceptably robust to reuse.
As document corpuses have grown in size, however, the assump-tion that pooling will retrieve nearly all relevant documents has be-come increasingly suspect. Unassessed documents returned by new systems may therefore in fact be relevant, and assuming them to be irrelevant will lead to an evaluation bias against new systems. In ad-dition, the bias can be systematic against systems that are different in nature from those which contributed to the pool. For instance, Buckley et al. [2007] suggest that recent large TREC collections have their pools flooded with documents rich in query keywords, and are biased against retrieval methods that attempt to go beyond keyword matching. Such systematic bias is not merely unfair to certain systems, but is an obstacle to an entire direction of potential retrieval improvement.

The pooling approach requires deep assessments of the pooled runs, in order to provide good coverage of the set of relevant doc-uments and make the collection reusable. Users, though, rarely look beyond the first page of results [Joachims et al., 2005], so deeper assessment is not necessary to capture the typical user ex-perience. Assessing a large number of queries to a shallow depth gives greater experimental power than assessing a small number of queries deeply [Webber et al., 2008]. One would rather assess 1 , 000 queries to depth 10 than 100 queries to depth 100 . Anecdotal evidence suggests that shallow, broad assessment is indeed the ap-proach taken by large search engines; see, for instance, the data set described in Najork and Craswell [2008]. Methods that allow for the reliable reuse of such shallowly-assessed topics are attractive.
Rather than assuming unassessed documents to be irrelevant, which is biased against new systems, unassessed documents can instead be ignored. This can be done either by using a special-purpose metric such as BPref [Buckley and Voorhees, 2004], or else by excising the unassessed documents from the run, shifting the remaining (assessed) documents up, and then evaluating the resulting condensed list with a standard evaluation metric [Sakai, 2007]. However, the fact that a document has not been returned by other systems is evidence for its not being relevant. Therefore, excising it from the run, and promoting in its place a document that has been returned by other runs and so is more likely to be relevant, leads to a bias in favor of unpooled systems [Sakai, 2008].
Assuming unassessed documents to be irrelevant, then, is biased against new systems, while condensing runs by excising unassessed documents is biased in favor of new systems. Nor is the degree of bias fixed. Rather, it depends on the comprehensiveness of the pool, and therefore on the number, quality, and variety of the pooled systems. Where a small number of similar systems are shallowly pooled, the bias of assumed irrelevance is strong, while that of con-densed lists is relatively weak. As the comprehensiveness of the pool increases, the bias of assumed irrelevance decreases, and the bias of condensed lists increases. Therefore, neither assumed ir-relevance nor condensed lists are appropriate and bias-free in all circumstances, nor will a static adjustment method work.
In this paper, we propose to address the problem of bias against unpooled systems by estimating that bias, and adjusting the un-pooled score accordingly. The estimation is made empirically, from the systems under evaluation, and rests only on sampling theory; it requires no prior information or model fitting. The bias estimate becomes an adjustment factor, and this factor is added to the score of the unpooled system, to derive the adjusted score.

The first method of adjustment we examine does not require the new system to be fully assessed for any topics. Instead, the bias estimate is derived from the existing, fully-pooled systems, using a leave-one-out experiment. This method is straightforward and can be applied with entirely static collections such as those produced by the TREC effort. However, the resampling technique applied assumes that the existing, fully-pooled systems and the new, un-pooled system have been randomly sampled from a common sys-tem population. If the new system is significantly better, or just significantly different, then the random sampling assumption is in-valid, and the adjustment method is unreliable and likely to under-state the system X  X  effectiveness.

A more robust method of bias estimation and adjustment can be applied if full assessment of the new system, along with the exist-ing ones, is available on a set of common topics. The error on the unpooled score of the new system can be directly observed on the common topics, and used as an estimate of unpooled bias for that system. The estimate is applied as an adjustment to the unpooled scores achieved by the new system on existing topics. The resulting adjusted scores are unbiased, and have markedly less mean error than the unadjusted ones. Lower mean error is achieved with only a few common topics, and the error decreases as the number of common topics increases. Estimating bias based on a set of fully-assessed topics is preferable in its assumptions to the leave-one-out estimation upon the fully-pooled systems because the inference is being made not from systems to systems, but from topics to topics. Therefore, the underlying assumption is not the dubious one that the systems have been randomly sampled, but the more reasonable one that the topics have. Using our method, the system evaluator can leverage a small number of common topics to reuse the assess-ment effort already spent on a large number of existing topics.
Buckley and Voorhees [2004] propose BPref as a special-purpose metric for handling incomplete relevance information. Yilmaz and Aslam [2006] calculate AP directly on lists from which unjudged documents have been excised, a method which they refer to as In-duced AP. Sakai [2007] suggests applying general-purpose metrics to runs with unjudged documents excised, and introduces the ex-pression condensed lists to describe such runs. Sakai also demon-strates that BPref is in fact a restricted form of AP on condensed lists, where evaluation is cut off once a certain number of non-relevant documents are seen in a run. All of these three papers perform experiments in which incomplete relevance information is formed by randomly sampling documents from the full qrels set. This construction method is inherently unbiased and therefore highly artificial; it does not simulate the effect of shallow pooling or of comparing unpooled against pooled systems. Sakai [2008] in-stead creates incomplete relevance information by partial pooling, and demonstrates that in this circumstance, condensed lists lead to bias in favor of new systems.

Unassessed documents pose such a thorny problem in part be-cause most existing evaluation metrics do not directly express the degree of uncertainty that arises from the presence of unassessed documents in a run. For many metrics, indeed, the uncertainty is difficult to quantify, particularly where the metric in normalized by the number of relevant documents. Moffat and Zobel [2008] propose a new metric, Rank-Biased Precision (RBP), which is un-normalized but naturally convergent, with the contribution of each rank having a fixed weight. A run X  X  RBP score is expressed not as a single value, but as a base value and residual. The residual exactly quantifies the uncertainty that results from incomplete as-sessment. If two systems have overlapping residuals, then it cannot be concluded for certain that one is superior to the other.
Yilmaz and Aslam [2006] propose that documents for assess-ment be chosen by uniform random sampling from the pool. The sampled documents are then used to estimate the true score. The estimator is unbiased, but has relatively high variance. They apply this sampling method to AP, referring to the resulting metric as In-ferred AP. Aslam et al. [2006] instead use a lower variance unequal sampling scheme, in which a document is sampled with probabil-ity proportional to its weight under the evaluation metric employed. These sampling methods cannot be applied in environments where incomplete assessments have been chosen by non-random means, such as pooling of a subset of systems. Aslam and Pavlu [2008] combine the pooling and random sampling using stratified sam-pling. Stratified sampling is applied by Yilmaz et al. [2008] to an environment which mixes pooling and random sampling. Their finding that the method is not subject to pooling bias is not con-firmed in application [Carterette et al., 2008], possibly because ag-gregating probability of inclusion across multiple runs by taking the mean of the per-run probabilities may not properly account for reinforcement by like systems.

Instead of pooling on the one hand, or random sampling on the other, a number of authors have proposed that documents for as-sessment should be chosen in an attempt to maximize some eval-uation goal; for instance, to boost the proportion of relevant docu-ments [Cormack et al., 1998], or to focus on the score accuracy of the best-performing systems [Moffat et al., 2007]. Carterette et al. [2006] select documents so as to maximize confidence that one sys-tem does or does not have a positive score delta with another, using a simple fixed probability of relevance model. A more complex model is developed in Carterette [2007]. Each system is treated as an expert, and in returning or failing to return a document, a sys-tem is asserting its judgment as to the probability that a document is relevant. The higher the document is returned in a ranking, the stronger the assertion of its probability of relevance. Then, as docu-ments are incrementally assessed, the reliability of each system can be progressively calibrated. Multiple logistic regressions are used to aggregate the evidence and formulate a probability of relevance. These probabilities of relevance can be used to directly estimate a score for a system. When employed in practice, estimated scores were consistently a third of actual scores [Carterette et al., 2008]; this suggests that a strong bias would occur if the method were used to compare pooled and unpooled systems, particularly if the number of pooled systems was small.
Our approach is to estimate the degree of bias that a system suf-fers from not being pooled based on a leave-one-out experiment. Estimation can be undertaken solely on the existing, pooled sys-tems, and then the result applied to the new, unpooled system; however, inference from systems to systems is problematic, as we demonstrate. Preferably, if a common set of topics for which the new system is also fully assessed is available or can be created, the bias against the new system can be directly measured on that subset. The resulting bias estimate is then applied as a score adjustment to the unpooled score. The unpooled score can be calculated either by assuming that all unassessed documents are irrelevant, or else by excising unpooled documents and evaluating the condensed lists.
This section begins by introducing the materials and methods employed. The degree of bias that unpooled systems suffer under assumed irrelevance, and enjoy under condensed lists, is then illus-trated on test set data.
Two test sets from the TREC effort are used in this paper. The first is from the 2004 Robust Track [Voorhees, 2004]. It consists of 110 systems submitted by 14 groups, run against 249 topics. The large number of topics makes this data set particularly attrac-tive for use in meta-evaluation studies such as this. Of the topics, 200 were drawn from earlier tracks of TREC, and the relevance as-sessments from these tracks reused, without new assessments being performed, meaning that not all documents returned by 2004 Ro-bust Track systems have assessments. Additionally, only a subset (albeit a plurality) of systems were pooled for the 49 new topics. To avoid confusion between documents unassessed in the origi-nal collection, and documents unassessed because of experimental withdrawal from the pool, the former are eliminated by expanding the original qrel set with non-relevant judgments for all unassessed documents. This affects only 3% of total returned documents for the old topics, and 1.5% for the new.

The second data set used is from the AdHoc track of TREC-8 [Voorhees and Harman, 1999], consisting of 129 systems submit-ted by 40 groups, run against 50 topics. The additional value con-tributed by this data set is the manual runs it contains. Manual runs allow for human involvement in query formulation and reformula-tion. They typically outperform automatic runs, and in particular find a much higher proportion of unique relevant documents. In TREC-8, the 13 manual runs find 24% of the relevant documents, while the 116 automatic runs between them only find 17% (the re-mainder are returned by both categories of runs). Similarly, the best 11 manual systems are also the best 11 systems over all, at least un-der some metrics. The Robust test set contains no manual runs. It will be observed later in this paper that methods of score estimation that appear to perform well on homogeneous system sets often per-form poorly on the more interesting case of heterogeneous sets. We test this by attempting to use information from automatic systems to estimate scores on manual ones.

An evaluation metric is a function that takes a vector of relevan-cies and produces a real-valued score that summarizes the vector, rewarding the return of relevant documents, and generally giving higher weight to higher rankings. Topics have differing degrees of difficulty. Many metrics attempt to compensate for this by nor-malizing scores based on the number of relevant documents for a topic. Also, metrics can be evaluated to a greater depth than runs are pooled, with assessments for documents beyond pool depth in one ranking being available if those documents were returned be-fore pool depth in another ranking. Both normalization and evalu-ation beyond pooling depth add to the complexity of score estima-tion, since finding relevant documents in one run affects the scores of other runs.

The metric employed in this paper is rank-biased precision at ten (RBP@10). Rank-biased precision assigns geometrically de-caying weights to each rank, with the score being the inner product of the relevance and weight vectors. Since the geometric sequence is convergent, RBP scores fit within the range of [0 , 1) . Also, be-cause each rank has a fixed weight (due in part to the exclusion of a normalization step), the degree of uncertainty arising from par-tial unassessment (either because evaluation stops at a certain rank, or because the relevance of some documents up to that rank is un-known) can be precisely stated as a residual value. For this paper, we take the base RBP value as our point score. Additionally, we cut off evaluation at depth 10, and adjust the rank weights accord-ingly, assigning no weights (or residual) to ranks 11 and beyond. The precision-at-ten metric was also employed in experiments, but since the outcomes obtained are very similar to RBP@10, the re-sults are not separately reported here.

As mentioned in the introduction, unassessed documents can be handled either by assuming them irrelevant or by condensing the lists. We implement condensing of lists by shuffling in documents from beyond evaluation (hence pooling) depth. If there are insuffi-cient assessed documents in the ranking, the trailing positions are filled with placeholder irrelevant documents.
In this section, we observe the empirical bias of excluding sys-tems from the pool. This can be derived from a leave-one-out ex-periment. We sample a set of fully-pooled systems from the test set, and randomly select one system s from the sampled set. The score of s under full assessment is calculated. Then, we form a pool consisting of the sampled systems, but excluding s , which is equivalent to marking all documents uniquely returned by s to pool depth as unassessed. The partial score of s is calculated, using ei-ther presumed irrelevance or list condensing. The bias is then the difference between the partial and true score. This is calculated for system s on every topic. The random sampling of system sets and unpooled system is repeated multiple times, to give a distribution of biases. The whole experiment is performed for different sizes of the fully-pooled set, allowing us to empirically relate bias to pooled system set size. Note that this is an observational study based on sub-sampling. The full set of systems being sub-sampled from is not itself randomly sampled from the universal population of sys-tems, so we cannot infer that the observed mean biases and bias distributions hold for all systems. This investigation supplements that undertaken in Sakai [2008]; however, our focus is on a much smaller number of pooled systems than are investigated there.
Figure 1 graphs the mean and quartile biases for rank-biased precision; the figure for precision at ten (not shown) is very sim-ilar. Using condensed lists is biased in favor of the unpooled sys-tem, while assuming unassessed documents to be irrelevant is bi-ased against it. For assumed irrelevance, the bias steadily decreases as the number of pooled systems or pool width increases, roughly halving when the pool width is doubled, as the number of unassessed Figure 1: Empirical RBP ( p = 0 . 8 ) bias for unpooled system for different numbers of pooled systems. Pooling is to depth 10 . The TREC 2004 Robust Track data set is used. Graphed is the mean and quartiles of the difference in mean system RBP score between the true score for the unpooled system and the score using either condensed lists or assumed irrelevance. Each data point represents 100 system set subsamplings. documents (not shown) steadily drops. The behavior of condensed lists is more complex. Fewer unassessed documents means that the condensed and true relevance vectors are more similar. At the same time, more pooled systems omitting a document strengthens the odds that the document is in fact irrelevant, and therefore strength-ens the bias resulting from excising it and replacing it with a pooled document, partially counteracting the effect of better accuracy. As a result, condensed lists have less bias than assumed irrelevance for small numbers of pooled systems, but assumed irrelevance has less bias for larger ones. We also observe that for wide pools, the distribution is quite skewed (mean is close to third quartile), as the frequency with which the unpooled system is largely  X  X overed X  by an almost-identical runset from the same family increases.
Two methods of estimating unpooled bias are discussed in this section. The first, bias inference from systems , requires no fully-assessed common topics, and instead takes its estimate from a leave-one-out experiment on the fully-pooled systems. The method is simple and requires no additional assessment effort. However, in inferring from systems to systems, it dubiously assumes that the systems have been randomly sampled from the same population. The second, bias inference from topics , requires that a subset of topics be fully assessed for all systems, and directly observes the bias against the otherwise unpooled system on those topics. While this requires extra assessment effort, the method makes the more reasonable assumption that the topics have been randomly sampled from the same population. A formal analysis of the common-topics method is provided, and experimental results are given.
Section 2.2 examined the empirical bias that results from exclud-ing a system from the pool, both under assumed irrelevance and us-ing condensed lists, for a particular test set. This observed bias, or perhaps an average across a few different test sets, could be direc tly used as a global score adjustment factor for unpooled systems. The problem is that we could not be certain that the results for one test set would be transferable to another. An alternative is to derive the estimate directly from the experimental set at hand.
 Algorithm 1 Adjust scores based on inference from systems T  X  set of topics S  X  set of (pooled) systems
Q  X  set of qrels on T derived from pool of S r  X  (unpooled) system for s  X  S do end for a  X  P s  X  S  X  s / | S |  X  adjustment factor u r  X  mean (unpooled) score of r evaluated against Q return u r + a Table 1: Bias inference from systems. Mean absolute er-ror (MAE) of leave-one-out score adjustment and unadjusted scores for RBP ( p = 0 . 8 ) under presumed irrelevance, for dif-ferent numbers of pooled systems. The left columns are for all systems from the TREC 2004 Robust Track. The right columns show estimation of unpooled manual system scores from pooled automatic systems on the TREC-8 AdHoc Track data set.

Adjustment factors for an unpooled system being compared to a set of pooled systems can be derived by a leave-one-out experi-ment. Say that an unpooled system r is being compared to S , a set of pooled systems. We remove each of the systems in S from the pool in turn, and calculate its unpooled score, either by assuming unassessed documents to be irrelevant or by condensing the run. The difference between mean unpooled and pooled scores of each system in S is the observed pooling error for that system. The av-erage across the observed errors of S provides an estimate of the pooling bias for the test set, and therefore of the adjustment factor a that should be added to the mean score of unpooled systems to correct for this bias. An important refinement to this method is that when each system s is withdrawn from the pool S for its unpooled score to be calculated, the new system r is added to the pool to re-place it. This has the effect of retaining any documents common to s and r but not found in S \ s . Otherwise, we would be estimating the penalty against an unpooled system in an n  X  1 pool, and our adjustment would be biased; specifically, it would tend to overes-timate the adjustment a . This leave-one-out adjustment method is described in Algorithm 1.

The effectiveness of leave-one-out score adjustment can be ex-perimentally assessed by resampling on an existing test set. For this purpose, we take the 2004 TREC Robust Track data set. A set of n + 1 systems are randomly sampled from the full system set, with uniform probability. One of these systems is selected to act as the unpooled system r , and the remaining n to form S , the set of pooled systems. Judgments from the original qrel set are reused. The mean score of r across all 249 topics is adjusted based on a leave-one-out experiment on S . The system sampling is repeated 100 times for each pool size. For this experiment, unassessed doc-uments are assumed irrelevant. The resulting mean absolute error (MAE) between the true score on the one hand, and the unpooled score (raw or adjusted) on the other, is then calculated. Let t true score for topic i , let s i be the unpooled score (raw or adjusted), and let N be the number of topics; then: The MAE figures for the experiment are reported in the left-hand columns of Table 1. Adjustment leads to much greater accuracy of scores, particularly with smaller pools. In addition, it is unbi-ased (as likely to over as to underestimate), whereas the unadjusted scores are all underestimates, meaning that bias or mean error (not separately reported in the table) is identical to mean absolute error.
The apparently good results obtained on the 2004 TREC Ro-bust Track data set are, however, misleading. The uniform ran-dom sampling employed is artificially beneficial to the adjustment method being examined. For instance, it is not surprising that the adjusted scores are unbiased, because for every randomly-selecte d set of systems that leads to a high estimate, there will be another randomly-selected set that leads to a compensating low one. This would not be a problem if the real-world evaluation environment in which this technique was used were indeed one in which systems were being randomly sampled for evaluation, but in general this will not be the case. Rather, the new system under the evaluation will be one that the developer has consciously tried to make better than the existing ones. What can happen when the leave-one-out score adjustment method is employed in such a situation is illus-trated by the right-hand columns of Table 1. Here, the TREC8 Ad-Hoc Track data set is employed. The new, unpooled system whose score is to be adjusted is randomly selected from the 11 best manual systems, while the pooled systems are sampled from the remainder of the system set. As described previously, the manual systems are significantly different from and better than the automatic ones, as shown by the large number of unique relevant documents they re-turn. Exclusion from the pool and the use of presumed irrelevance greatly underestimates the performance of these manual runs, and while leave-one-out adjustment helps, there is still a strong error, even with large pool sizes.
Section 3.1 has examined the derivation of adjustment factors from a leave-one-out experiment on the fully-pooled systems. The basic principle was inference from one set of systems to another system. As was pointed out, the more the inferred-to system differs from the inferred-from systems, the more tenuous this inference becomes. And when performing evaluation on a new system, that system is generally only interesting to the degree that it is different from, and better than, the existing ones.

A more robust inference can be performed if there exists, or can be created, a set of common topics for which both the existing sys-tems S and the new system r are fully assessed. In this case, the bias against r of being omitted from the pool can be directly ob-served on the common topics, since both true and unpooled scores are known. Then, this observed bias can be generalized as an ad-justment factor for that system X  X  scores on the topics for which it is genuinely unpooled. The process is described in Algorithm 2. Inference from common topics to unpooled topics is more robust than from pooled systems to unpooled systems because we are in-ferring from one set of topics to another, rather than from systems to systems, and it is more reasonable to assume that the topics are Algorithm 2 Adjust scores based on inference from topics T  X  set of topics S  X  set of (pooled) systems
Q T  X  qrels on T derived from pool of S r  X  (unpooled) system C  X  set of common topics
Q C  X  pool S  X  r on C and assess for relevance for c  X  C do end for a  X  P c  X  C  X  r,c / | C |  X  adjustment factor u r  X  mean (unpooled) score of r evaluated against Q T return u r + a randomly sampled from the same population than that systems are; indeed, in some experimental settings, random sampling of topics can be directly enforced.

We begin by offering a formal analysis of the common-topics ad-justment method as a form of sample-based ratio estimation. Then an experimental assessment is performed, which validates the for-mal analysis and demonstrates that common-topics adjustment leads to greatly improved accuracy over unadjusted scores, even if the new system is quite different from the existing ones.
 The proposed method is a form of ratio estimator [Thompson, 2002, Chapter 7]. Ratio estimators are of use where a cheap but inaccu-rate measure x is available for every element of a population, while the more costly true value y is only known for a sample. The mean ratio r between x and y is estimated from the sample, and applied to the x values across the population to estimate the true mean value of y ; that is,  X  y = r x . For us, the desired value is the mean of the true scores t (or simply ), and the cheap approximations are the unpooled scores u . We use arithmetic difference rather than ratio for the adjustment, since it can readily happen that the unpooled score u i for a topic i is 0 when the true score t i is greater than 0 , in which case the ratio t i /u i is undefined. Let N be the total number of topics, unpooled and common, and n be the number of common topics, that is, the topics for which all systems are fully assessed. Analytically, we will be treating the n common topics as randomly sampled from the full set of N topics. So the estimated adjustment a , derived from the n common topics, is: The estimate of the true mean score , using the adjusted estimator , for all N topics based on the unpooled scores u i is: where the quantity defined on the left of the equation should be understood as  X  X he a -based estimator of true mean  X , not  X  X he estimator of the mean of a  X . Of course, for n of these N topics we know the true score; however, since by derivation the adjustment a will be exactly correct for the mean of these n unpooled scores, we do not need to separately account for these n topics in the estimator. The adjustment a is an estimate of the true adjustment A that should be applied to the unpooled scores of all N topics in order to get the true mean score. As a is derived by sampling n deltas from the full N deltas whose mean is A , it follows that a is an unbiased estimator of A , and therefore that  X  a is an unbiased estimator of , the true mean score. The variance of this estimator is: The left-hand fraction here and subsequently adjusts for the small population; that is, for the fact that the exact values are known for n of the N elements in the population, and estimation is only be applied for the remaining N  X  n . The numerator of the right-hand fraction is: namely, the mean squared error of the per-topic adjusted scores against the true scores across all N topics (loosely speaking, the variance of the adjusted scores).

Instead of the adjusted estimator  X  a , we could take the mean true score  X  t from the n topics for which full assessment has been performed. This is also an unbiased estimator of , the true mean score. The variance of this sampled estimator  X  n is given by ele-mentary sampling theory, and is: where  X  2 t is the variance of the true scores across all N topics. Comparing Equations 3 and 5 shows that adjusted estimator is more accurate than the sampled estimator when  X  2 a , the MSE of the ad-justed scores, is less than  X  2 t , the variance of the true scores. And this is something which, due to the high inter-query variance of most metrics, is quite generally the case. That is, adjusted scores are in general closer to true scores than true scores are to their mean. Indeed, of the 500 different randomly-sampled system sets used in the experimental section that follows, in not one is the variance of true scores less than the MSE of adjusted scores, either with unassessed documents assumed irrelevant or condensed lists em-ployed.

The unadjusted scores could be used instead of the adjusted ones as a (generally biased) unadjusted estimator . The error on the unad-justed scores is A , of which a is an estimator. The error on the ad-justed scores is A  X  a ; that is, it is dependent on the degree to which a  X  X  estimation of A is incorrect. Therefore, the adjusted scores will be more accurate than the unadjusted scores if 0 &lt; ( a/A ) &lt; 2 ; that is, if the following two conditions are met: 1. the estimated adjustment a is the same sign as the true ad-2. the estimated adjustment a is no more than twice the true
Where the unadjusted scores always misestimate the true scores in the same direction (always underestimate them, or always over-estimate them), as occurs when unassessed documents are assumed irrelevant, Condition 1 is met. And since the expected value of a is A , Condition 2 will be met the vast majority of the time (exactly how often depends on the distribution of a ).

However, where the unadjusted scores can underestimate the true scores for some topics, and overestimate them for others, Condi-tion 1 is not guaranteed. Additionally, although a = A in expec-tation, its distribution may spread beyond 0 at one end, potentially violating Condition 1, and above 2 A at the other, potentially violat-ing Condition 2. The cumulative density beyond these limits gives Algorithm 3 Sample systems, topics to assess adjustment accuracy T  X  249 TREC Robust 2004 topics X  X  110 TREC Robust 2004 systems I  X  100  X  number of system sampling repeats
J  X  200  X  number of topic sampling repeats for w  X  X  2 , 4 , 10 , 20 , 40 } do  X  pool widths end for
E  X  E/ ( I  X  J )  X  Take mean error over I  X  J repeats return E the probability that the adjusted scores are less accurate than the unadjusted ones, and this probability depends on the distribution of a . If we assume a to be normally distributed under the central limit theorem (CLT), then to have 68% confidence that the adjusted scores are more accurate than the unadjusted ones requires that the standard deviation of the adjustment estimator be less than the true adjustment. That is, which is derived by taking the square root of Equation 3. For differ-ent degrees of confidence, different percentiles of the normal cum u-lative distribution function should be checked. For small n , where the assumption of normality is dubious, a bootstrap can be used instead.

Of course, in most evaluation settings, the true values of A and a are unknown, so Equation 6 cannot be directly calculated. The value of  X  2 a can be estimated as: that is, the observed MSE on the sampled topics. This would enable us to assess whether using the adjusted scores across all N topics rather than just the true scores on the n topics was likely to improve accuracy  X  which, as noted before, will usually be the case. As for the adjustment A , the estimator for it is a itself.
 The purpose of this section is to empirically assess the improve-ment in accuracy that score adjustment, based on bias inference from topics, provides over using the unadjusted, unpooled scores. Necessarily, the precise results achieved apply only to the particular test sets examined, but they are in accordance with the preceding formal analysis, and are indicative of what might be expected in general. Table 3: MAE of unadjusted, mixed, and adjusted scores, based on condensed lists. Metric is RBP ( p = 0 . 8 ). Other details are as for Table 2.

We begin by exploring the relative accuracy of unadjusted and adjusted scores, with unassessed documents assumed to be irrele-vant. The error on adjusted scores is observed using the experiment described in Algorithm 3. The mean absolute error (MAE) from the true score is taken as the measure of accuracy of the adjusted scores, with each MAE entry averaged from 20 , 000 subsamples. We also record the error on unadjusted scores for each system sam-ple. In addition, the true scores of the common topics are mixed with the unadjusted scores of the remaining topics and their error is calculated. This is necessary for a fair comparison, since in the adjusted scores the adjustment is precisely correct for the mean of the common topics.

The results of the experiment described in Algorithm 3 are given in Table 2. We first observe by following along the rows of the adjusted score results that the error of adjusted scores is propor-tional to p ( N  X  n ) / ( n  X  N ) , as the analysis predicts. (To be ex-act, the analysis makes this prediction of RMSE, but it holds true of MAE as well.) In contrast, the error of the unadjusted scores, with true scores mixed in, is proportional to ( N  X  n ) /n , and hence declines at a slower rate. For narrow pools, score adjustment offers a marked improvement over unadjusted scores, even for a hand-ful of topics in the common topic set. If the number of common topics is increased, quite high fidelity can be achieved even with only a couple of fully-pooled systems. As the pool width increases, the error of the unadjusted scores decreases, and while adjustment still improves accuracy, the relative benefit for the same number of common topics is decreased. On the other hand, as the number of unassessed documents decreases, the effort involved in taking a given number of topics for which r was unpooled and filling in the unassessed documents decreases, allowing a larger common topic set to be created for the same amount of overall effort.
Table 3 reports the same experiment as Table 2, but this time using condensed lists to handle unassessed documents in the un-Table 4: MAE of unadjusted and adjusted scores, on the TREC 8 manual dataset. Metric is RBP ( p = 0 . 8 ). Other details are as for Table 2. pooled system, both for the unadjusted scores and as the base for the adjusted scores. Only the mixed scores with 10 true scores mixed in have been reported; the remainder decline at the same rate as for Table 2. Condensed lists tend to be biased in favor of the unpooled system (see Figure 1), but the error is not uniform. Particularly in the case of very narrow pools, condensed scores on some topics are less than true scores. This means that variability is high relative to bias, which in turn leads to high variance in the adjustment estimator (see Equation 4). Equation 6 predicts that this will diminish the accuracy of the adjusted scores relative to the unadjusted, condensed ones, and this is indeed what we ob-serve in Table 3. For a pool width of 2 , the unadjusted scores are more accurate than the adjusted scores with 10 common top-ics, and are roughly as accurate for a pool width of 4 . As the pool width increases, though, the error becomes more consistently one-sided (that is, condensed scores are higher than true ones), an d the adjusted scores become more reliable. In any case, increasing the number of common topics improves the quality of the adjusted scores more rapidly than of the unadjusted ones.

Section 3.1 observed the problems caused for inferring the ad-justment from the pooled systems to the unpooled system when the latter is significantly different from the former. The point was demonstrated using the manual runs of the TREC 8 AdHoc track as the unpooled systems. In Table 4, the unpooled scores of these same manual runs are adjusted using the common-topic method in-stead. Since there are only 50 topics altogether in this test set, the number of topics that can experimentally be held as common is limited. Nevertheless, the utility of the common-topics adjustment method is clear. The error of the estimate with only 10 common topics and 2 pooled systems is well under half that of the unad-justed score with 40 pooled systems. Even if the unpooled system is distinctly different from the pooled ones, score adjustment from common topics provides serviceable accuracy.
In this paper, we have proposed a simple and robust sampling method for correcting the score bias suffered (or enjoyed) by un-pooled systems. The method proposed does require that a certain number of topics be fully assessed for the otherwise unpooled sys-tem, as well as for the systems in the pool. Additionally, the fully-assessed topics must be randomly sampled from the same popula-tion as the existing, partially-unpooled ones. However, we suggest that both of these conditions will often exist already in private lab work, and can in most cases be cheaply and reliably attained if they do not. In return, our method offers an adjustment method that is unbiased and much lower variance than either using the unadjusted scores, or making do with only the fully-assessed topics. In addi-tion, the degree of variance itself can be estimated from the topics used, and additional common topics added to reduce it if desired.
The unpooled score approximations dealt with in this paper have been the simple ones of, on the one hand, assuming unassessed documents to be irrelevant, or on the other hand, excising them and condensing the rankings. However, the method can be applied to any approach to approximating unpooled scores  X  or indeed any situation in which an expensive, exhaustive assessment on one small set of topics might be supplemented by a cheaper, approxi-mate assessment on a second, larger set. This might indeed include methods where the approximated evaluations are made with little or no human assessment at all.

Score adjustment could also be incorporated with more complex sampling and inferential schemes, as one form of evidence amongst many. Score adjustment has the particular advantage that, whereas pooling or other sampling bias is a problem for many schemes, ad-justment directly addresses and substantially solves the issue. Per-haps its best use in this field might be as a sort of sanity check for more complex methods.

However, the main attractions of the sampling method proposed here are the minimal inferential assumptions it makes, and its ro-bustness to pooling bias. This recommends it to evaluators working for the most part in a traditional pooled setup, and who are wary of more complex, potentially more fragile inferential methods. This work was supported by the Australian Research Council.
