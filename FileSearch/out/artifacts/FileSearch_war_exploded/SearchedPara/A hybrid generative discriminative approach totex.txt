 1. Introduction trary additional components as studied in ( Brochu &amp; Freitas, 2003; Lu &amp; Getoor, 2003 ). p ( x j j y ) for each component is individually modeled, where x of p ( x j j y ).
 good classification performance. In Lu and Getoor (2003) , a class posterior probability P ( y j x ponent is individually modeled, and then the simple product of P ( y j x x belongs.

Hybrid classifiers learn a class conditional probability model for each component, p ( x classifiers.
 cifically, we design individual component generative models p ( x utilize additional information effectively and thus improve classification performance. explore the formulation.
 individual component generative models. We train the NB models of components with a leave-one-out cross-validation of the training samples to improve their generalization abilities. and our conclusions are presented in Section 6 . 2. Conventional approaches { x 1 , ... , x j , ... , x J }. The classifier is trained on training sample set D  X f X  x basic formulas for the conventional approaches. 2.1. Generative approach eled as where h j k is a model parameter for the j th component in the k th class and h for continuous feature vectors.
 by
Here, p  X  h j k  X  is a prior over parameter h j k . Clearly, component model parameter h considering the other parameters.
 According to the Bayes rule, class posterior probabilities P ( y = k j x ; H ) can be derived as 2.2. Discriminative approach logistic regression (MLR) ( Hastie et al., 2001 ), class posterior probabilities are modeled as where W ={ w 1 , ... , w K } is a set of unknown model parameters. w x . W is estimated for maximizing the following penalized conditional log-likelihood: Here, p ( W ) is a prior over parameter W .

In Lu and Getoor (2003) , an individual MLR model P ( k j x that 2.3. Hybrid approach for binary classification
Hybrid classifiers learn a class conditional probability model for each component, p ( x to
Then, by introducing the weight parameters b j for the components and b posterior probability is extended as follows: mum class posterior likelihood as mentioned above. 3. Proposed method classifier to document and Web page classification. 3.1. Hybrid approach 3.1.1. Component generative models class conditional independence as described in Section 2.1 . Let p  X  x j j k ; h model in the k th class, where h j k denotes the model parameter. h h estimate is computed to maximize the objective function using training sample set D . 3.1.2. Discriminative class posterior design distribution by combining component generative models based on the maximum entropy (ME) principle ( Berger et al., 1996 ).
 P n  X  1 d  X  x x n ; k y n  X  = N of the training samples as X where ~ p  X  x  X  X  training data, such that By maximizing the conditional entropy H  X  R  X  X  obtain the target distribution: where K  X ff k j g J j  X  1 ; f l k g K k  X  1 g is a set of Lagrange multipliers. k we call this classifier  X  X  X ybrid X  X .
 class posterior P ( k j x ; H ) shown in Eq. (3) . Actually, if k the binary classifications shown in Eq. (8) .If K =2, k j R ( k j x ; H , B ).
 for R  X  k j x ; ^ H ; K  X  of training samples ( x n , y n ( x , y n ). The objective function of K then becomes upper convex function. We summarize the algorithm for estimating these model parameters in Fig. 1 . 3.1.3. Another class posterior by ME generative models and multinomial logistic regression as by using the constraint: instead of Eq. (9) . Here, K ={ { k jk } j , k ,{ l k } k
Hybrid shown in Eq. (11) , where the same combination weight k discriminatively determined as well as k j , we can regard R  X  k j x ; call this classifier  X  X  X ybrid-L X  X .
 in Section 4 . 3.2. Application to text classification representation, known as the Bag-of-Words (BOW) representation. Let x j  X  X  x the feature (word-frequency) vector of the j th component of a data sample, where x ponent. In the NB model, the probability distribution of x distribution: Here, h j ki &gt; 0 and data sample belonging to the k th class.
 for link and author information.
 obtain the class posterior distribution for Hybrid:
The class posterior distribution for Hybrid-L is expressed by using k
For MAP estimation of NB parameter h j k , as the prior p  X  h p  X  h j k  X / ture vectors of training samples that belong to the k th class. Then, the estimate of h
For Hybrid and Hybrid-L, we tune the hyperparameter n j k with a leave-one-out cross-validation of the training samples, because we confirmed this tuning was practically useful for classification. Here, mate of h j k  X f h j ki g i computed by using training samples other than ~ x the EM algorithm ( Dempster, Laird, &amp; Rubin, 1977 ). See Appendix A for the details. 4. Experiments 4.1. Test collections
An empirical evaluation was performed on four test collections: 20 newsgroups (20news), NIPS , ences created by Yann using optical character recognition. in components M and T in the dataset.
 words, respectively, in components M, T, A, and R in the dataset.
 of Web pages or files. We removed vocabulary words in the same way as for 20news and removed URLs 484 different URLs, respectively.

Cora contains more than 30,000 summaries of technical papers that belong to one of 70 groups. We AU, IL, and OL in the dataset.

Table 1 shows the properties of the components in the four datasets. j D Table 1 shows the percentage of data samples whose components are not empty. In 20news, NIPS, and
WebKB, j D c j / j D t j for M was close to 100%. On the other hand, j D for each component in Table 1 shows the average number of features contained by the component and implies the component size. In 20news, NIPS, and WebKB, j F j / j D components. 4.2. Experimental settings 4.2.1. Evaluation methods used for dealing with multiple components.
 4.2.2. Evaluation measure 4.3. Experiment 1 4.3.1. Compared classifiers with Hybrid and Hybrid-L. To construct the NB and NLR classifiers, component M was used for 20news, mance of the five components. 4.3.2. Results Hybrid is significant ( p &lt; 0.05) in the Wilcoxon test.
 uted toward improving the classification performance.

However, Hybrid-L did not always outperform the NB and MLR classifiers, when the number of training more overfitted with training samples than Hybrid. 4.4. Experiment 2 4.4.1. Compared classifiers types of media, we examined the SNB classification performance to evaluate Hybrid. 4.4.2. Results Table 3 shows the average classification accuracies obtained with Hybrid, PNB, PMLR, SNB, and SMLR. cation accuracies of the classifier and Hybrid is significant ( p &lt; 0.05) in the Wilcoxon test. generative approaches was similar to that of the pure discriminative approaches. 4.4.3. Analysis of combination weights approach. Each circle in Fig. 2 represents the average estimate of combination weight k indicates a j = k M j F j j / j F j M , where j F j M and k the number of features contained by the j th component, j F j means the ratio of the component size.
 obtained high average classification accuracies. For WebKB, the average estimate of k sizes were small but where the classification performance was good. (OL) were larger than for other components. We can suppose that our hybrid approach automatically performance. 4.5. Experiment 3 4.5.1. Compared classifiers combination, we considered the weighted average sifiers { P ( k j x j )} j , where the ratio of classification accuracies of { P ( k j x weighted average of component NB (MLR) classifiers  X  X  X ANB (WAMLR) X  X  and the weighted product of component NB (MLR) classifiers  X  X  X PNB (WPMLR) X  X . 4.5.2. Results Table 4 shows the average classification accuracies obtained with Hybrid, WANB, WAMLR, WPNB, and their classification accuracies. 5. Related work were proposed in ( Beyerlein, 1998; Glotin, Vergyri, Neti, Potamianos, &amp; Luettin, 2001 ). 6. Conclusion obtained by the ME principle, where a combination weight of a component is provided per class. of component generative models on the basis of the discriminative approach. data samples with and without class labels.
 Appendix A. Hyperparameter tuning procedure by where
Therefore, we can view ^ h j ;  X  m  X  ki as a linear interpolation between w training sample ~ x m , we can regard L  X  n j k  X  shown in Eq. (19) as a function of b : (A.3) , we obtain n j k to maximize L  X  n j k  X  shown in Eq. (19) .
 References
