 ingandmachinetranslation,usewordsasbasicunits.UnlikeEnglish,therearenowhite spacesbetweenwordsinmanyAsianlanguages.Therefore,identifyingwordboundaries is a fundamental task of processing these languages.
 and unsupervised methods. Supervised methods rely heavily on labeled data of a given language, thus they require much manual work. On the other hand, unsupervised meth-odshavebecomeincreasinglyimportantinresearchduetotheirindependenceofhuman efforts, as well as adaptability to any domains. In addition, the unsupervised learning process shows insights on how human beings acquire lexical knowledge.
 tics suchas the cohesion andthe separating degree ofresulting units [1][2]. The second categoryevaluatesprobabilityofasegmentationofagivenstringbasedonexplicitprob-abilisticmodelsvianonparametricBayesianinference[3 X 5].Bayesianmethodsbecome for Bayesian unsupervised word segmentation is how to model contextual dependen-Contextual dependencies include word-level dependencies and character-level depen-dencies. Several hierarchical Bayesian models are capable to capture continuous word-level dependencies [3 X 5]. Besides, [4] considered continuous character dependencies and [5] characterized a wider range of inter-word dependencies by adaptor grammars whichisthestate-of-the-artmodel.Butadaptorgrammarsforsegmentationisdependent on language. Different grammars need to be carefully designed for different languages. It is still expensive to apply adaptor grammar on natural text corpora due to high com-putational cost.
 dependencies from three perspectives. Firstly, we try to explore not only continuous charactergroupsbutalsogappycharacterpatternsamongdifferentwords.Forexample, we intend to learn the extremely meaningful gappy pattern  X  X ...t X  among words such Pattern X   X  ...  X   X  X ppearsinwordssuchas X   X   X   X  (calculator) X , X   X   X   X  (timer) X , X   X   X   X  (scoringindicator) X  X nd X   X   X   X  (taximeter) X .Whenwecometoaplausibleword ofthispattern,itmightbereasonabletoassignthiswordhighprobability.Secondly,We have great impacts on whether the character should be merged into left, right or as a tends to be a separate word, but the fourth letter  X  X  X  tends to be combined with other characters.Thirdly,weshowemphasisonthefertilityofacharacter.Fertilitymeansthat length which is believed to be an important factor for unsupervised word segmentation. cality and fertility parameters. [6] exploited monolingual word alignments to extract supervised dependency parsing under a monolingual alignment model. We are inspired totreatthewordsegmentationasaproblemofmonolingualcharacteralignment.Bytak-ing the source side and the target side as the same sequence of monolingual characters, we can produce an alignment inside a string. When we produce a character alignment, wesimultaneouslyobtainasegmentationthateachwordisconsistentwiththecharacter alignmentbyamappingalgorithm.AGibbssamplersampleseverycandidatealignment positionforeachcharacter.TheposteriordistributionisproductofexpertsofIBMMod-els 1-3 [8], hidden markov alignment model [9], as well as a hierarchical Pitman-Yor language model [10]. After several iterations, most frequent samples are selected to be final segmentation results.
 corpora [11], which outperforms the best model in [4] by more than 16.5% in F-score and approaches the state-of-art model [5]. On standard Chinese text datasets, we also improve the segmentation accuracy by 1.9 to 2.9 F-score points compared to [1]. ed works, we describe the joint model. Then we explain the Gibbs sampling algorithm. In the last two sections, we show the experimental results and draw conclusions. 2.1 Word Alignment to model the translation probability from e to f , a hidden alignment variable a is in-troduced, P r ( f j e ) = model 1 only considers lexical translation probability t ( f j j e a Hidden Markov alignment model, an alignment is dependent of the previous one. [13] and [14]. But all these works rely on bilingual information. 2.2 Introduction to Pitman-Yor Process distributions. It is governed by a discount parameter 0 d &lt; 1 , a strength param-eter a &gt; d and a base distribution G 0 . The generated distribution G is marked as G P Y P ( a, d, G 0 ) . The discount parameter d is responsible for probability smooth-ing while the strength parameter a controls the similarity between G 0 and G . Restaurant Process (CRP). CRP can be described using the analogy of a restaurant has tomer always sits at the first table. Suppose after a time, a restaurant already has n customers and m occupied tables. The next customer either selects an occupied table ( customer. Then the table choosing probability customer seat an occupied table, he can share the dish labeling that table with others. denotethelabeloftable i .Givenpreviouslabelandtableassignments,wecansumover all the tables labeled with h , customers who are served with the dish h in the previous table assignments. The hierarchical Pitman-Yor language model describes the n-gram language model in a way that the ( n 1) gram probability distribution is used as the base distribution as corpus vocabulary. [4] employs a nested hierarchical Pitman-Yor language model. The G c is a character-level n-gram language model and G c is generated in the same way so used for segmentation in [3][16]. Dirichlet Process is a special case of PYP with discount parameter equals to zero. Adaptor grammars also uses PYP to describe the probability distribution of a parsing rule [5]. 3.1 Monolingual Character Alignment (MCA) [0 ...n ] g is computed by equation (4). a  X  X ULL X . Monolingual character alignment (MCA) prevents each character is aligned to itself. That kind of alignments make no sense for segmentation. Figure 1 shows two MCA examples of string  X  X smartboy X .
 3.2 Generative Story and a word segmentation w by equation (5). is hyperparameter. We apply generative models to decompose p ( a, w j s , ) . generate a character alignment inside the given string. Then, we deduce a word seg-mentation result from the character alignment. The decomposing procedure is shown in equation (6). two-stage model has two disadvantages. First, the alignment a is very sparse. The seg-mentation model has little effect on the alignment model. The purpose of MCA is to in-fer a good segmentation, rather than to capture translation clues. We suppose character alignment and word segmentation benefit from each other. A good character alignment could lead to a good word segmentation and vice versa. Second, the computational cost of mapped to several segmentations according to different heuristic rules. The segmenta-tion selection procedure is relatively slow.
 step model produce word segmentation and character alignment simultaneously. The generating procedure is shown in equation (7). w a denotes the corresponding segmentation according to character alignment a . This modelgeneratesacharacteralignment a first.Atthesametime,itconverts a toaunique segmentation w a . The segmentation result is the side product of character alignment. One advantage of this model is that it makes a tighter connection between character alignment and word segmentation. When generating a character alignment, the proba-bilityofthecorrespondingsegmentationmustbeconsidered.Forexample,ifwewantto compare the two alignments in Figure 1, we need to consider the plausibility of related segmentations, X  X smartboy X  X nd X  X smartboy X .Anotheradvantageisthatitisefficien-t because it performs a unique mapping from character alignment to segmentation and only requires a single step computing. 3.3 Mapping Alignment to Segmentation character alignment a according to alignment consistency . Statistical machine transla-tion models often make use of alignment consistency to extract bilingual phrase pairs. a high plausible word. Considering length factors, we choose smallest non-crossed span is too long. Instead, we regard two smaller spanes  X  X  X  and  X  X mart X  as words. In mono-lingualcharacteralignment,aspanconsistentwithalignmentmaycoveranotherone.In span, therefore we choose the longer span  X  X oy X  as a word. The smallest non-crossed strategy try to control the length of a word as well as to bond connected characters as many as possible into a word. The mapping algorithm is shown in Algorithm 1. 1 -3 is initiation. f stands for the start point of the next smallest non-crossed span. P stands for the alignment boundary of the previous character. C stands for the alignment boundary of the current character. The alignment boundary of a character means the minimum value and maximum value of the set of its position, the position it aligned to and all positions aligned to it. For example, in figure 1(a), the alignment boundary of m relationship of P and C , word boundaries are determined. Line 6 -10 means if P and C are not intersected, then we find a target span [ f, i 1] and update the value of P and f . Line 11 -14 means if P and C are intersected, then we merge P and C . P C = [ to Algorithm 1, two character alignments and their corresponding segmentations are shown in Figure 1. 3.4 Product of Experts Models and Hidden Markov alignment model. Those models are used to exploit liter-word segmentation could have effect on each other in the one-step model, we use a product of experts (PoE) to combine them. PoE multiplies several probability distribu-tiontogetherandhasbiastowardsampleswhichhavehighprobabilityinallsub-models. under PoE is shown in Equation (8).
 m 1 , m 2 , m 3 and m h are adapted from alignment models IBM Model 1-3 and HMM model respectively. m s is a model for segmentation. Distributions over these five sub-models and their Pitman-Yor priors are shown in Table 1.
 bound together. IBM Model 1 is changed to model the character association probability in MCA.
 Table 1, j V j is the number of character types in the corpus.
 In Table 1, m i = a i i and n i = ( c i , l ) , P m thispaper.Thevalidvalueof m i isrestrictedto [ 5 , 5] .Wepreventtoolongalignments because it will cause the under-segmentation problem. When a i = 0 , we use a similar methodusedin [17]byviewingeverycharacterisprecededbyaNULLtoken.Itmeans behave differently in different positions.
 tion probability and fertility probability. NULL insertion can not be applied to MCA. Because source side and target side are the same in MCA, so we do not need to in-sert NULL tokens in target side. Also in MCA all characters are allowed to be aligned to NULL, Thus l  X  0 might be zero.  X  0 denotes the number of characters aligned to NULL. Instead, we directly handle the alignment probability from NULL by linked to NULL, p 0 = 1 p 1 . A character which prefers to distribution over greater value of  X  has a tendency of forming multi-character words with neighboring aligned characters. We use p 1 = 0 . 2 in this paper.
 Location Model, we reformulate the transition probability as: We make T = 5 in this paper. This model depicts the first order dependence of jump over characters. Jump distance is usually small inside a word but large between word boundaries. When a i = 0 , the same method mentioned in Location Model is used to calculate the distance.
 1, the spelling model G m s where k is the length of w . Different from [12], we use a method proposed in [4] to
Algorithm 1: Converting Alignment to Segmentation 3 P &lt; b [1] .l, b [1] .r &gt; 4 for i 2 ... j s j do 5 C &lt; b [1] .l, b [1] .r &gt; 6 if ! P \ C then 7 w w [ [ f, i 1] 12 P P C 15 w w [ [ f, j s j ] Therefore, we use Gibbs sampling to simulate the procedure of character alignment. Gibbs sampling is a special case of Monte Carlo Markov Chain method, and it is guar-anteed to converge to the true posterior distribution. The denominator in equation (8) is expensive to track, therefore we ignore the denominator. Assume before a sampling a position i conditioned on other values is:
P ( a i = j j A  X  i , S ; ) / P m s ( w  X  a sition, w a denotes the exclusion of current segmentation. w  X  mentation after setting a i = j .
 B isthenumberofburn-initerations.TheGibbssamplerfirstrandomlyinitializesword boundaries of a string and then randomly assigns an alignment connected to characters in the same word for each character. After initialization, the Gibbs sampler repeatedly samples a reasonable alignment for each character conditioned on all other alignments and segmentations. A blocked computing is performed by moving an alignment from one position to another since each movement might result in different segmentations. An example of counts change during one movement is shown in Table 3. After B burn-in iterations, we collect K segmentation samples for each string s . The most frequent sample will be the final result. As for the hyper-parameter sampling, we use a slice sampler [5] by putting a flat beta prior Beta (1 , 1) on the discount parameter d and a vague prior Gamma (10 , 0 . 1) on the strength parameter a . of corpus. One of them is the public SIGHAN Bakeoff 2005 dataset [18]. This dataset contains four kinds of data, i.e. CITYU, MSR, PKU and AS. CITYU and AS are tra-ditional Chinese text. MSR and PKU are simplified Chinese text. We only use the test set data for alignment. The other corpus consists of English phonetic scripts made by BrentoftheBernstein-Ratnercorpus [11]intheCHILDESdatabase [19].Alineofthis corpus is  X  X uwanttusiD6bUk X  and the corresponding English text is  X  X ou want to see the book X . We need to segment the phonetic script into  X  X u want tu si D6 bUk X . Details of all corpora are shown in Table 2.
 a single character,such as English letters and Arabic numerals. We make use of them to segment a long string into several shorter strings. This preprocessing is beneficial for string alignment to overcome a part of sparsity of sentence length. We compare our result with models that also encode the information of punctuation or word types. pler for 501 iterations, including 250 burn-in iterations. In order to speed up conver-gence, we use a simulated annealing procedure, which cools down the Gibbs sampler from a high temperature T 0 = 10 to a final temperature T f = 1 with geometric de-cline ( T f hyperparameters for 20 iterations.
 performance. For phonetic scripts, we also calculate the same metrics(LP, LR, LF) over induced lexicons. 5.1 English phonetic transcripts Process based model (HDP), Nested Hierarchical Pitman-Yor Langauge model (NPY) and Adaptor Grammar based model (AG). Table 4 shows the accuracy of segmenta-tion results. Word token accuracy of our model has surpassed both HDP and NPY. It is surprised that the result of MCA is so close to AG, even MCA has a weaker ability to identify word-level collocations compared to a three layers of collocation-syllable structure in AG. One significant difference between AG and MCA is that AG models the relationship between characters in terms of a hierarchical syllable structure while MCA applies an alignment structure. 5.2 Model Comparison multiple combinations are shown in Table 5. m 1 and m s are essential for basic segmen-and (4) to (6), we can find F value increase by 25.2, 16.9 and 18.6 points respective-segmentation result. Another interesting phenomenon we observed is that model m 3 al-ways helps improve the recall value. With analysis of setting (2) and (3) together with m 3 is capable of overcoming the under-segment problem to some extent. Although m h alsoisalocationfactor,itislesspowerfulthan m 2 .Butcombiningthesetwooverlapped factors still achieve a valuable improvement. 5.3 Chinese Word Segmentation hasreportedresultsunderthesameexperimentalconditions.NPYmodelusedadditional training data. So we use ESA as our baseline model. Results are shown in Table 6. comparedtoNPYmodel,weoutperformthemonMSRdatasetwith1.7pointsimprove-ment. However, MCA loses to NPY on CITYU corpus. We can infer from Table 2 that CITYU is a smaller corpus compared to another three Chinese corpora, and the AC val-ue is the smallest. It tends to have a positive correlation between AC and the accuracy improvements. The greater AC value is, the more times a word appears from a corpora. Characters inside general words will have stronger relationships. NPY has good perfor-mance on smaller corpus while MCA might show its potential on larger corpus. Some examples of segmentation of MSR corpus are X   X  X  X  X   X  X  X  X  X   X  X  X  X  X  X  X   X  X  X  X  X  D X   X  X   X  X  X  X   X  X  X  X  X  ,  X  X  X  X  X  X  X  X   X . From these results, we can see that our method can recognize some words with complex character struc-tures, such as  X   X  X  X  X  X   X  and  X   X  X  X  X  X   X . Some words are over-segmented.  X   X  X  X   X   X  X houldbemergedasawholeword. X   X   X  X anberegardedasaffixcharacters,itoften appearsintheboundaryofaword.Therefore,theyaremoreprobabletoaligntoNULL. This phenomenon can lead to some fine-grained segmentation results. But fine-grained segmentation results might be more suitable for SMT, we will evaluate them in future. chical bayesian models for unsupervised word segmentation. We adopt a joint model to produce monolingual character alignment and word segmentation at the same time. Through experiments, we show that this model plays a significant role in improving word segmentation accuracy on both phonetic scripts and Chinese natural text corpus. In the future, we will work out character to block alignment models instead of trans-forming character to character alignment models at hand.
 and CAS Action Plan for the Development of Western China (No. KGZD-EW-501).
