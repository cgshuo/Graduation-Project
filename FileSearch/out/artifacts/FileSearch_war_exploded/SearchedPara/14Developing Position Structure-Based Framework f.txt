 Relation extraction is a task to find semantic relations between two entities from the text. This task was recently promoted by the Automatic Content Extraction (ACE) Evaluation program. For instance, the sentence  X  X ill Gates is the chairman of Microsoft Corporation X  conveys the ACE-style relation  X  X RG-AFFILIATION X  between the two entities  X  X ill Gates (PER) X  and  X  X icrosoft Corporation (ORG) X , where PER and ORG are entity types, and ORG-AFFILIATION is a relation type.

The task of relation extraction has been extensively studied over the past years mainly for English. It is usually cast as a classification problem. Existing approaches include feature-based and kernel-based methods. Feature-based approaches [Jiang and Zhai 2007; Kambhatla 2004; Zhou et al. 2005, 2009a] transform the context of two entities into a linear vector of carefully selected linguistic features varying from entity semantic information to lexical and syntactic features of the context. Kernel-based approaches [Zhang et al. 2006; Zhou et al. 2007, 2010], on the other hand, design kernel functions on the relation context X  X  structured representation such as parse tree or dependency tree and then compute the similarity between two relation instances.
In contrast to the significant achievement concerning English and other Western languages, research progress in Chinese relation extraction is relatively limited. This might be due to the nature of Chinese language, for example, no word boundaries and lack of morphological variations, etc. The system-segmented words are already not error free, thus also affecting the quality of the generated parse trees. All these errors will undoubtedly propagate to a subsequent processing, such as the relation extraction. It is therefore reasonable to conclude that word-based features and kernel-based (especially tree-kernel-based) approaches are not suitable for Chinese, at least at the current stage. Huang et al. [2008] provided empirical evidence showing that in the ACE 2007 Chinese relation extraction task, a rather simple feature-based approach was able to outperform the best adopted parse tree kernel-based approach.

In this article, we present a novel feature-based Chinese relation extraction frame-work, in which all the features do not require the Chinese word segmentation or deep natural language processing. Particularly, this framework is based on a 9-position structure feature between two entities. The design of this feature is motivated by the fact that there are some obvious connections between relation types/subtypes and posi-tion structures of two entities. For example, in many  X  X art-Whole X  relation instances, one entity is often nested in the other entity, where nested is a position structure and Part-Whole is a relation type. In addition, compared with the 3-position structure im-plicitly or explicitly used in many feature-based methods, for example, those in Zhou et al. [2005], Che et al. [2005b], Chen et al. [2010], this 9-position structure is more dis-criminative since it is more effective in relieving the class imbalance problem. It is im-portant to deal with this problem since there are far more negative relation instances than positive ones [Kambhatla 2006] and consequently this problem often hurts the performance of standard classifiers [Chawla et al. 2004].

In our framework, instead of trying to explore every feature reported in the litera-ture [Che et al. 2005b; Chen et al. 2010; Jiang and Zhai 2007; Zhou and Zhang 2007; Zhou et al. 2005], our focus is to investigate the usefulness of our 9-positition struc-ture. Therefore, we only complement the position structure feature with some basic character-based features, such as entity context (both internal and external) character N -grams and four word lists extracted from a published Chinese dictionary. After the classification with standard classifiers, we also derive some correction and inference mechanisms in order to further improve the classified results. Specifically, at first we rectify the classified relation types/subtypes by certain constraints, which are derived from the possible relation types/subtypes between any two entity types. Second, based on the relation hierarchy, a consistency c heck is carried out to make sure the relation type and the corresponding relation subtype a re consistent. The aforesaid possible re-lations and relation hierarchy are available in the ACE task guideline. In addition to the above correction strategie s, the entity co-reference information and some linguistic indicators are introduced to infer more posi tive relation instances through their links to the classified positive ones. It should be noted that this process can further integrate our strategies into a unified framework. Specifically, the classified results of different position structures can be linked together through the inferring process.

Experiments on the ACE 2005 data set show that the 9-position structure can provide strong support for Chinese relation extraction. Meanwhile, it can be cap-tured with less effort than applying deep natural language processing. The entity co-reference does not help as much as we have expected. The lack of necessary anno-tations for the co-referenced entity mentions within a single document might be the main reason. By contrast, other strategies in our framework can further boost the extraction performance.

The remainder of this article is organized as follows. Section 2 briefly introduces the definition of the ACE relation extraction task and reviews the related work. Sec-tion 3 defines three types of features, namely position structure (including 9-position and 3-position), entity type, and character-based features. Our feature-based Chinese relation extraction framework is proposed in Section 4. Experimental studies on the ACE 2005 Chinese data set are presented in Section 5. Finally, Section 6 concludes the article. The research on relation extraction has been initiated and promoted by the Mes-sage Understanding Conferences (MUCs) (MUC, 1987 X 1998) and the NIST Automatic Content Extraction (ACE) program 1 (ACE, 2001 X 2008). Accord ing to the ACE 2005 program 2 , there are five primary ACE tasks, that is, the detection and recognition of entities, values, temporal expressions, relations, and events. In this article, we fo-cus on the ACE Relation Detection and Recognition (RDR) task and directly use the available entity information. An entity is an object or a set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities or entity mentions 3 . For example, the sentence  X  X eorge Bush traveled to France on Thursday for a summit X  conveys the ACE-style relation  X  Physical.Located X  between the entity mentions  X  X eorge Bush X  and  X  X rance X , where  X  X hysical X  and  X  X ocated X  are predefined relation type and subtype, respectively.  X  X eorge Bush X  is the Arg-1 and  X  X rance X  is the Arg-2. We can say that  X  X eorge Bush X  is  X  X ocated X  in  X  X rance X , but not vice versa.
The task of relation extraction can be regarded as the problem to classify the rela-tion type, relation subtype, and the argument order of each relation instance between any two entity mentions. Formally, let r =( s , em 1 , em 2 ) denote a relation instance, where s is a sentence, em 1 and em 2 are two entity mentions in s ,and em 1 either pre-cedes or embeds em 2 in the text. Given all relation instances { r i } , our goal is to learn a function that maps each relation instance r i to a type t  X  T and a subtype s t  X  ST ,and to identify the role (i.e., argument order Arg-1 or Arg-2) of the two entity mentions. Here, T denotes the set of predefined relation types plus the type None ,and ST is the set of predefined relation subtypes plus the None subtype. None means that there is no relation between two entity mentions, or the relation is not annotated. The classified relation is correct if and only if its type/s ubtype is correct and its two arguments are in the correct order. The research on relation extraction can be roughly divided into two directions, that is, feature-based and kernel-based. We first review the related work according to different directions and then review the work particularly for Chinese language.

Feature-based approaches transform the context of two entities into a linear vec-tor of carefully selected linguistic features based on different levels of text analysis, ranging from morphological analysis and part-of-speech (POS) tagging to full pars-ing and dependency parsing. Miller et al. [2000] augmented syntactic full parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees. Kambhatla [2004] employed maximum entropy (ME) models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention type, overlap, dependency, and parse tree. Besides these features, Zhou et al. [2005] further explored other features derived from the base phrase chunk-ing information, semi-automatically collected country name list and personal relative trigger word list; and then took into account all the features into the classification step, where Support Vector Machines (SVMs) [Joachims 1998] were selected as the classi-fiers. Jiang and Zhai [2007] then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree, and dependency parse tree. Their experiments showed that using only the basic unit features within each feature subspace can already achieve state-of-art performance, while over-inclusion of complex features might hurt the performance. The reason could be that if combining several feature subspaces into one subspace, dif-ferent original subspaces might have too much overlap [Zhou et al. 2009a]. To avoid such a feature overlapping problem, Zhou et al. [2009a] proposed a multi-view ap-proach to relation extraction.

On the other hand, kernel-based approaches design kernel functions on the relation context X  X  structured representation such as parse tree or dependency tree, and then compute the similarity between two relation instances. Zelenko et al. [2003] proposed a kernel over two parse trees which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen [2004] extended this work to estimate the similarity of augmented dependency trees. The above two X  X  work was further advanced by Bunescu and Mooney [2005] who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. These three tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending node from the roots to the current nodes, making their kernels with high precision but very low recall. Later, in order to incorporate the advantages of feature-based meth-ods, Zhang et al. [2006] developed a composite kernel that combined convolution parse tree kernel with an entity kernel, and showed its effectiveness in capturing various syntactic features. Zhou et al. [2007] experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans and applied a composite kernel to combine a convolution parse tree kernel and a state-of-art linear kernel for integrating both structured and flat features. Miyao et al. [2008] evaluated the use-fulness of different syntactic parsers for the relation extraction carried out by SVMs with tree-kernels. Zhou et al. [2010] further integrated more syntactic and semantic information into the above context-sensitive convolution kernel. Katrenko et al. [2010] introduced local alignment kernels and explored various possibilities of using them for the relation extraction.

Besides the above supervised methods, some unsupervised methods [Chen et al. 2006a; Nakov and Hearst 2008; Takaaki et al. 2004] and semi-supervised methods [Chen et al. 2006b; Zhang 2004; Zhou et al. 2009b] were also explored. Unsupervised methods could overcome some difficulties in supervised approaches, such as labor-intensive annotation efforts. However, they could hardly be directly applied in many NLP tasks since there is no relation type label attached to each instance in the cluster-ing results [Chen et al. 2006b]. Therefore, semi-supervised methods have drawn much attention recently [Chen et al. 2006b].

The aforementioned works are mainly focused on English relations. Although Chi-nese processing is of the same importance as English and other Western language processing, unfortunately less work has been published on Chinese relation extrac-tion. Che et al. [2005a] defined an improved edit distance kernel over the original Chinese string representation around part icular entities. They studied only one ACE-style relation type, that is, PERSON-AFFILIATION. Che et al. [2005b] explored sev-eral features and evaluated their performance on the ACE 2004 Chinese evaluation data. Huang et al. [2008] provided evidence showing that in ACE 2007 Chinese rela-tion extraction, a rather simple feature-based approach is able to achieve reasonable performance (i.e., 0.63 F-measure); however, the best reported results of parse tree kernel-based approaches is unexpectedly low (i.e., 0.35 F-measure only). More re-cently, Zhang et al. [2009] proposed a composite kernel-based approach for ACE 2005 Chinese RDR task. Chen et al. [2010] adopted Deep Belief Network (DBN) and showed its effectiveness.

The insufficient study in Chinese relation extraction drives us to investigate how to find an approach that is particularly appropriate for Chinese. In this article, we pro-pose a novel position structure based framework for Chinese relation extraction. The contributions are three-fold. First, we propo se a 9-position structure feature, which is used as the major component to form our framework. Second, we derive certain con-straints based on possible relations and relation hierarchies, in order to improve the correctness and consistency of the classified relation types, subtypes and argument or-ders. Third, the entity co-reference inform ation is used to infer more positive relation instances through their links to the classified positive ones. In this section, we describe the features used in our framework. In Table I, we first show the hierarchy of relation types and subtypes, as well as the frequencies of anno-tated (positive) relation instances on the ACE 2005 Chinese corpus.
 Recall that our task is to identify the relations between any two entity mentions. Therefore, all the features are related to the entity mention pairs and their con-texts. Specifically, for each pair of mentions, three kinds of features, namely posi-tion structure feature, entity type/subtype feature and character-based feature, are involved. For vector representations of features for the classification, please refer to Appendix B. Intuitively, the position structure of two entity mentions ( em 1 and em 2 )hassomeob-vious connections with the type/subtype of the relation they might be. This can be un-derstood from the following observations. In a lot of  X  X art-Whole X  relation instances, the position structure of em 1 and em 2 tends to be nested. For example, in the sentence  X  X he U.S. Congress decided to veto the ecology bill X , the two nested mentions, em 1 ( X  X he U.S. Congress X ) and em 2 ( X  X .S. X ) have a  X  X art-Whole.Subsidiary X  relation. In addition, for many  X  X hysical.Located X  relations, the position structure of em 1 and em 2 is more likely to be adjacent, that is, em 1 and em 2 are not nested and there is no entity mention in between them. For example, in a sentence  X  X housands of Palestinians rushed the Israeli checkpoint X , the relation of the two adjacent mentions, em 1 ( X  X housands of Palestinians X ) and em 2 ( X  X he Israeli checkpoint X ) is  X  X hysical.Located X . These obser-vations drive us to analyze the position structure of the two entity mentions in-depth. We define nine types of the position structure as illustrated in Figure 1. The formal definition for these 9-position structure types is given in Appendix A. Appendix C presents one Chinese example (selected from the ACE 2005 dataset for Chinese rela-tion extraction) for each position structure. Here, we briefly explain these nine position structures.

For the structure types (a), (b), and (c), em 2 is nested (i.e., included) in em 1 .In(a), there are no other entity mention that includes em 2 and is also nested in em 1 .In(b), there is at least one entity mention (not em 1 or em 2 ) that includes em 2 and is also nested in em 1 .In(c), em 1 includes em 2 ,and em 2 includes em 1 as well.

For the structure types (d), (e), and (f ), em 1 and em 2 are not nested and there are no other full entity mentions in between them, even though there could be some char-acters in between em 1 and em 2 . In (d), neither of the two entity mentions is nested in other entity mentions. In (e), em 1 or em 2 is nested in another entity mention. In (f ), both em 1 and em 2 are nested in other entity mentions.

For the structure types (g), (h), and (i), em 1 and em 2 are not nested and there is at least one full entity mention in between them. In (g), neither of the two entity mentions is nested in other entity mentions. In (h), em 1 or em 2 is nested in other entity mentions. In (i), both em 1 and em 2 are nested in other entity mentions.
On the other hand, we can merge structure types (a), (b), and (c) into one single structure type. Similarly, we can merge the structure types (d), (e), and (f ), as well as combine the types (g), (h), and (i). This means that one can combine structures of each row in Figure 1 into one logical structure with a logical  X  X r X . As a result, we can obtain three position structures, that i s, Nested+, Adjacent+, Separated+, each corresponding to one row in Figure 1. This 3-position structure feature has been ex-plicitly or implicitly adopted in several methods, for example, in [Che et al. 2005b; Chen et al. 2010; Jiang and Zhai 2007; Zhou et al. 2005]. Specifically, this 3-position structure feature is exactly the position structure feature in Chen et al. [2010]. Zhou et al. [2005] defined an Overlap category of features, which consider if one entity men-tion is included (or called nested) in the other entity mention, and if there are words or other entity mentions in between the two concerned entity mentions. Che et al. [2005b] adopted an Order feature, which also considers if one entity mention is included in the other one. We also think that in the parse tree feature spaces, for example, those in Jiang and Zhai [2007], the position structures of two entity mentions are implicitly considered. 3.1.1 Class Imbalance Problem. We analyze the difference between the 9-position structure feature and the 3-position one in terms of the effectiveness in solving the class imbalance problem. This problem typically occurs when there are far more in-stances of some classes than those of others. In such cases, standard classifiers tend to be overwhelmed by large classes and ignore the small ones and consequently cause a significant bottleneck in performance [Chawla et al. 2004]. The task of relation extrac-tion encounters the class imbalance problem [Culotta et al. 2006; Kambhatla 2006], that is, there are much more None (negative) class relation instances than ACE anno-tated (positive) class relation instances. For instance, in Tables II and III, the overall ratio of positive to negative class is 1:12.01 on ACE 2005 corpus. If we divide all the relation instances according to differe nt position structure types, we can observe that compared with the situation of the 3-positon structures, the class imbalance prob-lem with respect to the 9-positon structures is less serious for the majority ( &gt; 99%) of relation instances. Specifically, in 3-position structures, the ratios of positive to nega-tive relation instances for Nested+, Adjacent+ and Separated+ are 1:0.7273, 1:13.3629 and 1:85.1853, respectively. On the other hand, in 9-position structures, the ratios for Nested, Adjacent, and Separated are 1:0.37, 1:6.82, and 1:42.87, respectively. This relieves the class imbalance problem a lot. It should be noted that these main struc-tures, that is, Nested, Adjacent, and Separated, occupy most ( &gt; 99%) of the positive relation instances. Especially, the Nested structure is the most important one since it has approximately 68% of all the positive relation instances. We can see that the positive-to-negative ratio of Nested structure is much larger than the overall ratio. These two features are concerned with the entity type and subtype of both entity mentions (i.e., em 1 and em 2 ). Entity mentions inherit the attributes (i.e., entity type and subtype) from the corresponding entity. Figure 2 shows the dependency between entity and its mentions. For each mention pair, the combination of their entity types is for entity type feature and similarly their entity subtypes are for the entity subtype feature.

The ACE 2005 categorizes entities into seven types (see Table IV), including  X  X ER X ,  X  X RG X ,  X  X PE X ,  X  X OC X ,  X  X AC X ,  X  X EA X , and  X  X EH X . Each type is further divided into subtypes (see Table IV). Character-based features involve N  X  gram features and wordlist-based features. Before describing them, we extract three types of character sequences from the context where two entity mentions appear. Note that we use characters instead of words.  X  Internal Character Sequence
These character sequences are concerned about the extents and the heads 4 of both entity mentions, and can be categorized into four types of sequences as follows:  X  In-Between Character Sequence
If em 1 ( em 2 ) does not contain em 2 ( em 1 ), then all the characters between two entity mentions will be extracted as the in-between character sequence.  X  External Context Character Sequence
These character sequences are concerned with the characters around two entity mentions in a given window size w s , and can be classified as the following four types.
The extraction of external character sequ ences must comply with one rule, that is, the extracted character sequence cannot enter into or move across any entity mentions.  X  N -gram Features
All character sequences are then transformed into N -gram features. For example, c will be used to construct one Uni-gram feature as well as one Bi-gram feature.  X  Wordlist-Based Features
With insufficient training data, many dis criminative words for the relation extrac-tion might not be covered by N -gram features. Therefore, we build wordlist-based features which are extracted from a published Chinese dictionary. These wordlists in-clude Chinese preposition list (165 words), orientation list (105 words), auxiliary list (20 words), and conjunction list (25 words). It can be expected that some words in these wordlists can serve as strong indicators for some relation types or subtypes. For instance, if there is an orientation word  X  X outh X  in the context of two entity mentions, it is more likely that these two mentions have a  X  X hysical.Located X  relation. The in-between and external context character sequences are transformed to wordlist-based features. On the other hand, the internal character sequences are not involved since they are not likely to include those words related to preposition, orientation, auxil-iary or conjunction words. Each involved character sequence is used to construct one wordlist-based feature for every wordlist. Features with respect to different wordlist are different from each other. Our relation extraction framework is su mmarized in Model 1, which is based on the 9-position structure. In Step 1 we divide all the relation instances into nine parts according to the 9-position structures defined in Section 3.1 in order to solve the class imbalance problem. The detailed motivation of this divide strategy has been discussed in Section 3.3.1 and is also verified by the experiments in Section 5.

In Step 2, we initially perform the RDR task by a cascade strategy, that is, carrying out the relation detection and recognition separately. Specifically, we first classify every relation instance as positive or negative. Then, we classify each positive relation as one of the relation type/subtype. Both classifications are carried out by standard classifiers (i.e., SVMs). The cascade strategy is against the all-at-one strategy, that is, carrying out relation detection and re cognition at one time by classifiers. We do not adopt the all-at-once strategy because the number of positive relation instances in any one relation type/subtype is much smaller than the number of negative ones. On the other hand, the cascade strategy can relieve the class imbalance problem due to the fact that the number of all positive relation instances is much bigger than that of positive ones in any one relation type/subtype. Then, we will explain the strategies in other steps (i.e., Step 3 to Step 5). In many tasks of information extraction, such as entity extraction and relation extrac-tion, some prior knowledge is usually involved that can be useful to the tasks. In ACE 2005 guidelines, a table (e.g., Table V) of possible relation between Arg-1 and Arg-2 is provided. Given two entity mentions, the possible relation type and subtype can be obtained according to the two entity types (listed in Table IV). For instance, according to Table V, if both entity types of Arg-1 and Arg-2 are PER (person), the possible re-lation type can be Per-Social, and the relation subtypes can be Business or Family. If the entity type of Arg-1 is PER and that of Arg-2 is ORG (organization), the possible relation type can be Org-Aff (Org-affiliation).

This kind of prior knowledge has two important roles. First, we can rectify the relation type/subtype classified by SVMs. According to the entity types of two entity mentions, if the classified relation type/subtype is not possible then we will revise the type/subtype to None . Second, if the relation type/subtype is possible, we then adjust the argument order of the two entity mentions.

In many feature-based models [Kambhatla 2004; Wang and Li 2006; Zhou et al. 2005], they used a different approach to the argument order problem. Specifically, except for symmetric relations, the argument order is modeled by considering one re-lation subtype as two new relation subtypes with different orders. For example, the relation subtype Physical.Located is changed to two relation subtypes, namely em 1 -Physical.Located-em 2 and em 2 -Physical.Located-em 1 , where the former denotes that em 1 is the Arg-1, and the latter denotes that em 2 is the Arg-1. There are two draw-backs of their strategy. First, it could be more time-consuming since it involves almost twice the number of classifiers we need. Second, it may make the class imbalance problem more serious because the number of positive relation instances in each (new) relation subtype becomes smaller than the number of the positive instances in each (original) relation subtype. In our framework, the relation type and subtype are classified separately and they may not be consistent. We then try to make th em consistent according to the relation hierarchy (see Table I in Section 3). There are some existing strategies, such as strictly Bottom-Up [Kambhatla 2004; Zhou et al. 2005] and Guiding Top-Down, to deal with this problem. With regard to the strictly Bottom-Up strategy, the relation type should conform to the relation subtype. Once the sub type is recognized, the type is determined by the subtype, since a subtype belongs to one unique type. As for the Guiding Top-Down strategy, the upper level (relation type) guides the down-level (relation subtype). It assumes that the classification result of relation type is more precise. As a result, the subtype will be revised to None if it does not conform to the type.

However, we think that these two strategies lack necessary interaction between two levels, and hence do not make full use of both levels X  classification results. Therefore, we derive the following consistency check strategy.

Similarly, we can have the Subtype Selection based consistency check strategy, which selects cn most likely subtypes, and check them against the types. The relation extraction performances of d ifferent position structures have great disparity. Our experiments show that the performances of the Nested and Adjacent structures are much better than the results of the other seven structures. In fact, there are almost no positive relation instances classified for the other seven position structures. This phenomenon may have two reasons. First, the imbalance class problems in the other seven position structures are much more serious, as evidenced in Table III. Second, intuitively, Nested a nd Adjacent relation instances are more likely to be positive classes (or more likely to be annotated) and hence can be extracted easily.

There are some linguistic homogeneous characteristics (such as co-reference) that can be used to infer more positive relations through the classified positive ones. Specif-ically, after obtaining one classified positive relation with Nested or Adjacent struc-ture, we can assign its relation type/subtype to other relation instances with different position structure but sharing the same attributes. These attributes are related to co-reference information and pattern-based information, which will be described below. 4.3.1 Co-reference-Based Inference. Each entity mention belongs to only one entity and hence naturally inherits the type and subtype attributes from the corresponding entity (see Figure 2). Entity mentions are considered as co-referent when they belong to the same entity.

Once Nested and Adjacent relation instances are recognized as positive, the co-reference information can be adopted to carry out the relation inferring. Specifically, if a relation instance with different position structure has the same two entities as in the classified positive one, this relation instance will be classified as the same rela-tion type and subtype. For example, both  X  X e X  and  X  X ates X  may refer to  X  X ill Gates of Microsoft X . If a relation  X  X RG-AFFILIATION X  is held between  X  X ill Gates X  and  X  X i-crosoft X , it must be also held between  X  X e X  and  X  X icrosoft X . Formally, given two entities e infer more relations which may not be identified by classifiers.

When considering the co-reference information, we may find another type of incon-em 22 ) are different in their contexts or structures, and R denotes the classified re-lation type/subtype. The co-reference not only helps for inference but also provides a chance to check the consistency among entity mention pairs. As the classification results of SVM can be transformed to probability by a sigmoid function the relations of lower probability mention pairs can be revised according to the relation of highest probability mention pairs. In Equation (1), the left side denotes that the probability of relation type/subtype t for relation instance r and y t is the output value of the t by the classifiers. 4.3.2 Pattern-Based Inference. The classified positive relation instances of adjacent structure can infer more relation instances of separated structure if there are some linguistic indicators in the local context. For example, given a local context  X  X oth em 1 and em 2 are located in em 3  X , if em 2 and em 3 are classified as a positive relation in-stance, em 1 and em 2 will have the same relation type/subtype as that em 2 and em 3 hold. Currently, the indicators under consideration are  X  X nd X  and  X  X r X . However, more patterns can be included in the future. We evaluate our relation extraction fra mework on the training dataset for the ACE 2005 Chinese Relation Detection and Recognition (RDR) 7 task provided by the Lin-guistic Data Consortium (LDC). The 633 documents have been manually annotated with 9,299 instances of relations. Meanwhile, 6 relation types and 18 subtypes are predefined. More details are shown in Table I in Section 3. Because of no test data at hand, we randomly select 75% out of the 633 documents as the training data and the remaining documents are used for evaluation. All the reported performances in this article on the ACE RDR 2005 corpus are evaluated using 4-fold cross validation on the entire corpus. In this article, we only measure the performance of relation extraction model on  X  X rue X  mentions with  X  X rue X  chaining of co-reference (i.e., they are annotated by LDC annotators). The aim of our experiments is to evaluate the performance of the proposed features (especially the 9-position structure feature) in Section 3, as well as each step in our relation framework in Section 4. Two baseline methods are involved. Both of them carry out the RDR task as an all-at-once multi-class classification problem. In the first baseline method, the involved features are the 3-position structure feature in Section 3.1 and other features in Sections 3.2 and 3.3. Recall that (see Section 3.1) the 3-position feature is implicitly or explicitly used in many feature-based methods, for example, those in Zhou et al. [2005], Zhou and Zhang [2007], Che et al. [2005b], Chen et al. [2010]. In the second baseline method, the 9-position structure feature and other features are adopted. The first baseline (denoted as 3-Position Baseline) is used to test whether the 9-position feature is helpful, while the second baseline (denoted as 9-Position Baseline) is to test the performance of each step in our framework. When evaluating each step, its following steps will not be executed.

Besides the above main aims, we also evaluate the roles of different categories of features in Section 3 played in our framewo rk. In addition, we provide a performance comparison between our framework and the kernel based framework in Zhang et al. [2009], which also adopted the 9-position feature (slightly different from ours), and carried out the ACE 2005 Chinese RDR task as well. Finally, since the dimensions of the vector representations for all the features are very large, we would like to study the effectiveness of some feature selection methods such as Information Gain (IG) [Yang and Pedersen 1997] and Bi-norm Separation (BNS) [Forman 2003].

The SVMlight [Joachims 1998] with linear kernel and default configuration is adopted as the classification tool. In Steps 1 X 3 of our framework, for every entity mention pair ( em 1 ,em 2 ) in a sentence, we simply choose em 1 as Arg-1, and em 2 as Arg-2, where em 1 precedes or contains em 2 . The window size ( w s ) of character-based features is 4. The options count cn in the type and subtype selection based consistency check strategy (see Section 4.2) are all set to 2.
 As for the evaluation metrics, we adopt three primary metrics, that is, Precision, Recall, F-measure, which are also commonly used to evaluate other relation extrac-tion methods, for example, those in Zhou et al. [2005; 2007; 2010], Zhou and Zhang [2007], Jiang and Zhai [2007], Zhang et al. [2006], and Chen et al. [2010]. In addition, the Wilcoxon signed rank test is adopted as the measure of the statistical significance of the improvements over baseline methods. The improvements (at significance level 0.05) over the 3-Position Baseline and 9-Position Baseline are denoted as  X   X   X  X nd X   X   X , respectively, in the result table. In each table, both the performance of positive rela-tion types and those of positive relation subtypes will be reported. All results are the average ones over 4-fold experiments. Note that the results are slightly different from those in Li et al. [2008], which did not involve the 4-fold experiments. In this set of experiments, we will first compare our 9-position feature with the 3-position feature when all other features are involved and we do not divide the relation instances. Second, we evaluate our Divide strategy (Step 1) in our framework. Table VI summarizes the experimental results. We have the following conclusions. First, when we do not divide relation instances, the 9-Position Baseline significantly outperforms 3-Position Baseline, which shows the effectiveness of our 9-position feature. This is due to the fact that the class imbalance problem of the 3-positon is more serious than that of the 9-position (see Section 3.1 and Tables II and III). Second, the 9-Position-Divide significantly further improves the F -measure 18.15% and 23.15% over 9-Position Baseline in relation types and relation subtypes recognition, respectively. The aim is to investigate the effectiveness of the cascade strategy, that is, the Step 2 in our framework. In the detection stage, binary-class SVMlight is adopted, while in the recognition stage, multi-class SVMlight is adopted. Table VII presents the experimental results. We can see that the Cascade strategy outperforms the all-at-once strategy. As discussed in Section 4.1, the possible relation information between Arg-1 and Arg-2 has two important roles: one is to rectify the classification results; the other is to adjust the argument order. Table VIII shows the performance of this step. We can clearly see that it is contributing and improves the F -measure 2.82% and 5.26% in type and subtype recognition, respectively. This is to test Step 4, that is, the consistency check method in Section 4.2. Table IX shows the results, indicating that the strategies using subtypes to determine or select types (Type Selection) perform better than the Subtype Selection Strategy. This may be attributed to the fact that previous correction (in Step 3) for relation subtype is better than that of relation type. Overall, the type selection based consistency check strategy is the best one. We first present the results (after Step 4) of Nested and Adjacent structures in Table X. The results of other structures are not shown since they are almost zero. According to these reported results and our discussion in Section 4.3, intuitively we can follow the path of  X  X ested  X  Adjacent  X  Separated  X  Others X  to perform the inference. But soon we find that if two concerned entity mentions are nested, almost all the co-refereed mentions are nested. So basically inference works on the path  X  X djacent  X  Separated  X  Others X .

Then, through this inference path, we use both co-reference information and lin-guistic indicators to construct relation infe rring. The performance of relation inferring is summarized in Table XI. We can see that the inferring step does not help as much as we have expected. This might be due to that the lack of enough annotated relations for co-reference mentions and for those sharing the same patterns, that is, linguistic indicators. Then, we evaluate the contribution of every feature category for our framework. All the steps in our framework (see Section 4) are involved, but we will adopt the features in-crementally. Only entity type and subtype f eatures do not work. Therefore, Table XII shows the results when we incrementally add the 9-position structure, the external contexts and internal contexts, Uni-grams and Bi-grams, and at last the word lists on them. The observations are: first, the 9-position structure provides stronger sup-port than other individual features. Second, Uni-grams provide more discriminative information than Bi-grams. Third, external context seems more useful than internal context. At last, the wordlist feature slightly improves the performance.
 We provide a performance comparison between our framework and the kernel-based framework in Zhang et al. [2009], which was also evaluated on the ACE 2005 Chinese RDR task (for relation type only). Table XIII reports the results, which shows that our approach outperforms this kernel-based approach, although it uses features that are similar to those in our framework. Since the large dimension and serious sparseness of the vector representation, we would like to test whether the feature selection methods can be useful in our task. Two feature selection methods (IG and BNS) are investigated.

Information gain [Yang and Pedersen 1997] of a term measures the number of bits of information obtained for category prediction by the presence or absence of the term in a document. Let m be the number of classes. The information gain of a term t is defined as Forman [2003] presented an empirical comparison of 12 feature selection methods. Results revealed the surprising performa nce of a new feature selection metric,  X  X i-Normal Separation X  (BNS). Let tp ( t ) be true positives (number of positive cases con-taining term t ), fp ( t ) be false positives (number of negative cases containing term t ), pos denote the number of positive cases, neg be the number of negative cases, tpr ( t ) denote the sample true-positive rate ( tp ( t )/ pos )and fpr ( t ) be the sample false-positive rate ( fpr ( t )/ neg ). BNS can be defined as follows: For each method mentioned above, we implement feature selection in two ways. One is to construct feature selection on the whole feature space. The other is to implement feature selection on N -gram subfeatures, for example, left-4 context Uni-gram, while holding entity type/subtype and wordlist-based features unchanged. The latter strategy gain better performance according to the expe rimental results in Figures 3 and 4, where  X  X revious X  corresponds to the result without feature selection. Although fewer features can reduce the time cost of classifiers, the relation extraction results do not seem to be promising. This might be because that SVM itself has enough power to find the discriminative dimensions on the given data set. To continue this direction, we may want to use some other formal methods, such as the PLSI [Hofmann 1999], which has successful application in text processing. This remains as our future work. In this article, we propose a position structure-based framework for Chinese entity relation extraction. The main contributions can be concluded as follows. First, a 9-position structure feature, which is conceptually clear and computationally efficient, is devised to relieve the serious class imbalance problem. This feature is also used as a major component to form our divide-and-conquer relation extraction framework. Second, the possible relation-based constraints are used to verify the relation clas-sification results and adjust the argument orders of relations. Third, an interactive consistency checking strategy is proposed to check whether the classified type and subtype conform to the given relation hierarchy. Last but not the least, co-reference information and pattern-based features are used to infer the more positive relations through the classified positive ones. The effectiveness of them, especially the position structure feature, has been demonstrated in the experiments conducted on the ACE 2005 Chinese data set.

Although the inferring step has not received the convincing performance improve-ment, this direction could be interesting and fruitful. It is because that the inferring can be derived from a graph where a vertex represents an entity, and the initial edge is the classified relations of Nested and Adjacent structure. Then, the other relations of any structure could be represented by this graph. Moreover, this graph can represent the relations of entity pairs which are not in one sentence. We will investigate this direction in the future. Furthermore, as for the efficiency issue of the proposed framework, we would like to investigate the usefulness of l 1-norm SVM, which is efficient in dealing with large-scale data sets [Sra 2006]. We will also sys-tematically investigate its effectiveness in improving the performance of the relation extraction.
 Given an entity mention em ,let em . start and em . end denote the start and end positions em 2 ,where em 1  X  em 2 or em 1 precedes em 2 , the position structure of them can be grouped into nine categories in Table XIV.
 Once the features are obtained, the task of the Chinese Relation Extraction is modeled as a multi-class classification problem. Support Vector Machine (SVM) [Boser et al. 1992; Cortes and Vapnik 1995] is selected as the classification tool since it represents the state-of-the-art in the machine learning research. Given a training set of labeled instance pairs (x i , y i ) , i =1 ,..., l where x i  X  R n and y i  X  X  1 ,  X  1 } l ,SVMrequiresthe solution of the following optimization problem:
We use SVM in both the relation detection process and relation type and subtype recognition process. As described in Manevitz and Yousef [2001], there are four differ-ent text representations, that is, binary, frequency, tf-idf, and Hadamard. In this arti-cle, we use binary vector representation for the features obtained before, as explained in Table XV. We then combine the following vectors into a single feature vector to SVM.

 Relation extraction is a task to find semantic relations between two entities from the text. This task was recently promoted by the Automatic Content Extraction (ACE) Evaluation program. For instance, the sentence  X  X ill Gates is the chairman of Microsoft Corporation X  conveys the ACE-style relation  X  X RG-AFFILIATION X  between the two entities  X  X ill Gates (PER) X  and  X  X icrosoft Corporation (ORG) X , where PER and ORG are entity types, and ORG-AFFILIATION is a relation type.

The task of relation extraction has been extensively studied over the past years mainly for English. It is usually cast as a classification problem. Existing approaches include feature-based and kernel-based methods. Feature-based approaches [Jiang and Zhai 2007; Kambhatla 2004; Zhou et al. 2005, 2009a] transform the context of two entities into a linear vector of carefully selected linguistic features varying from entity semantic information to lexical and syntactic features of the context. Kernel-based approaches [Zhang et al. 2006; Zhou et al. 2007, 2010], on the other hand, design kernel functions on the relation context X  X  structured representation such as parse tree or dependency tree and then compute the similarity between two relation instances.
In contrast to the significant achievement concerning English and other Western languages, research progress in Chinese relation extraction is relatively limited. This might be due to the nature of Chinese language, for example, no word boundaries and lack of morphological variations, etc. The system-segmented words are already not error free, thus also affecting the quality of the generated parse trees. All these errors will undoubtedly propagate to a subsequent processing, such as the relation extraction. It is therefore reasonable to conclude that word-based features and kernel-based (especially tree-kernel-based) approaches are not suitable for Chinese, at least at the current stage. Huang et al. [2008] provided empirical evidence showing that in the ACE 2007 Chinese relation extraction task, a rather simple feature-based approach was able to outperform the best adopted parse tree kernel-based approach.

In this article, we present a novel feature-based Chinese relation extraction frame-work, in which all the features do not require the Chinese word segmentation or deep natural language processing. Particularly, this framework is based on a 9-position structure feature between two entities. The design of this feature is motivated by the fact that there are some obvious connections between relation types/subtypes and posi-tion structures of two entities. For example, in many  X  X art-Whole X  relation instances, one entity is often nested in the other entity, where nested is a position structure and Part-Whole is a relation type. In addition, compared with the 3-position structure im-plicitly or explicitly used in many feature-based methods, for example, those in Zhou et al. [2005], Che et al. [2005b], Chen et al. [2010], this 9-position structure is more dis-criminative since it is more effective in relieving the class imbalance problem. It is im-portant to deal with this problem since there are far more negative relation instances than positive ones [Kambhatla 2006] and consequently this problem often hurts the performance of standard classifiers [Chawla et al. 2004].

In our framework, instead of trying to explore every feature reported in the litera-ture [Che et al. 2005b; Chen et al. 2010; Jiang and Zhai 2007; Zhou and Zhang 2007; Zhou et al. 2005], our focus is to investigate the usefulness of our 9-positition struc-ture. Therefore, we only complement the position structure feature with some basic character-based features, such as entity context (both internal and external) character N -grams and four word lists extracted from a published Chinese dictionary. After the classification with standard classifiers, we also derive some correction and inference mechanisms in order to further improve the classified results. Specifically, at first we rectify the classified relation types/subtypes by certain constraints, which are derived from the possible relation types/subtypes between any two entity types. Second, based on the relation hierarchy, a consistency c heck is carried out to make sure the relation type and the corresponding relation subtype a re consistent. The aforesaid possible re-lations and relation hierarchy are available in the ACE task guideline. In addition to the above correction strategie s, the entity co-reference information and some linguistic indicators are introduced to infer more posi tive relation instances through their links to the classified positive ones. It should be noted that this process can further integrate our strategies into a unified framework. Specifically, the classified results of different position structures can be linked together through the inferring process.

Experiments on the ACE 2005 data set show that the 9-position structure can provide strong support for Chinese relation extraction. Meanwhile, it can be cap-tured with less effort than applying deep natural language processing. The entity co-reference does not help as much as we have expected. The lack of necessary anno-tations for the co-referenced entity mentions within a single document might be the main reason. By contrast, other strategies in our framework can further boost the extraction performance.

The remainder of this article is organized as follows. Section 2 briefly introduces the definition of the ACE relation extraction task and reviews the related work. Sec-tion 3 defines three types of features, namely position structure (including 9-position and 3-position), entity type, and character-based features. Our feature-based Chinese relation extraction framework is proposed in Section 4. Experimental studies on the ACE 2005 Chinese data set are presented in Section 5. Finally, Section 6 concludes the article. The research on relation extraction has been initiated and promoted by the Mes-sage Understanding Conferences (MUCs) (MUC, 1987 X 1998) and the NIST Automatic Content Extraction (ACE) program 1 (ACE, 2001 X 2008). Accord ing to the ACE 2005 program 2 , there are five primary ACE tasks, that is, the detection and recognition of entities, values, temporal expressions, relations, and events. In this article, we fo-cus on the ACE Relation Detection and Recognition (RDR) task and directly use the available entity information. An entity is an object or a set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities or entity mentions 3 . For example, the sentence  X  X eorge Bush traveled to France on Thursday for a summit X  conveys the ACE-style relation  X  Physical.Located X  between the entity mentions  X  X eorge Bush X  and  X  X rance X , where  X  X hysical X  and  X  X ocated X  are predefined relation type and subtype, respectively.  X  X eorge Bush X  is the Arg-1 and  X  X rance X  is the Arg-2. We can say that  X  X eorge Bush X  is  X  X ocated X  in  X  X rance X , but not vice versa.
The task of relation extraction can be regarded as the problem to classify the rela-tion type, relation subtype, and the argument order of each relation instance between any two entity mentions. Formally, let r =( s , em 1 , em 2 ) denote a relation instance, where s is a sentence, em 1 and em 2 are two entity mentions in s ,and em 1 either pre-cedes or embeds em 2 in the text. Given all relation instances { r i } , our goal is to learn a function that maps each relation instance r i to a type t  X  T and a subtype s t  X  ST ,and to identify the role (i.e., argument order Arg-1 or Arg-2) of the two entity mentions. Here, T denotes the set of predefined relation types plus the type None ,and ST is the set of predefined relation subtypes plus the None subtype. None means that there is no relation between two entity mentions, or the relation is not annotated. The classified relation is correct if and only if its type/s ubtype is correct and its two arguments are in the correct order. The research on relation extraction can be roughly divided into two directions, that is, feature-based and kernel-based. We first review the related work according to different directions and then review the work particularly for Chinese language.

Feature-based approaches transform the context of two entities into a linear vec-tor of carefully selected linguistic features based on different levels of text analysis, ranging from morphological analysis and part-of-speech (POS) tagging to full pars-ing and dependency parsing. Miller et al. [2000] augmented syntactic full parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees. Kambhatla [2004] employed maximum entropy (ME) models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention type, overlap, dependency, and parse tree. Besides these features, Zhou et al. [2005] further explored other features derived from the base phrase chunk-ing information, semi-automatically collected country name list and personal relative trigger word list; and then took into account all the features into the classification step, where Support Vector Machines (SVMs) [Joachims 1998] were selected as the classi-fiers. Jiang and Zhai [2007] then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree, and dependency parse tree. Their experiments showed that using only the basic unit features within each feature subspace can already achieve state-of-art performance, while over-inclusion of complex features might hurt the performance. The reason could be that if combining several feature subspaces into one subspace, dif-ferent original subspaces might have too much overlap [Zhou et al. 2009a]. To avoid such a feature overlapping problem, Zhou et al. [2009a] proposed a multi-view ap-proach to relation extraction.

On the other hand, kernel-based approaches design kernel functions on the relation context X  X  structured representation such as parse tree or dependency tree, and then compute the similarity between two relation instances. Zelenko et al. [2003] proposed a kernel over two parse trees which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen [2004] extended this work to estimate the similarity of augmented dependency trees. The above two X  X  work was further advanced by Bunescu and Mooney [2005] who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. These three tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending node from the roots to the current nodes, making their kernels with high precision but very low recall. Later, in order to incorporate the advantages of feature-based meth-ods, Zhang et al. [2006] developed a composite kernel that combined convolution parse tree kernel with an entity kernel, and showed its effectiveness in capturing various syntactic features. Zhou et al. [2007] experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans and applied a composite kernel to combine a convolution parse tree kernel and a state-of-art linear kernel for integrating both structured and flat features. Miyao et al. [2008] evaluated the use-fulness of different syntactic parsers for the relation extraction carried out by SVMs with tree-kernels. Zhou et al. [2010] further integrated more syntactic and semantic information into the above context-sensitive convolution kernel. Katrenko et al. [2010] introduced local alignment kernels and explored various possibilities of using them for the relation extraction.

Besides the above supervised methods, some unsupervised methods [Chen et al. 2006a; Nakov and Hearst 2008; Takaaki et al. 2004] and semi-supervised methods [Chen et al. 2006b; Zhang 2004; Zhou et al. 2009b] were also explored. Unsupervised methods could overcome some difficulties in supervised approaches, such as labor-intensive annotation efforts. However, they could hardly be directly applied in many NLP tasks since there is no relation type label attached to each instance in the cluster-ing results [Chen et al. 2006b]. Therefore, semi-supervised methods have drawn much attention recently [Chen et al. 2006b].

The aforementioned works are mainly focused on English relations. Although Chi-nese processing is of the same importance as English and other Western language processing, unfortunately less work has been published on Chinese relation extrac-tion. Che et al. [2005a] defined an improved edit distance kernel over the original Chinese string representation around part icular entities. They studied only one ACE-style relation type, that is, PERSON-AFFILIATION. Che et al. [2005b] explored sev-eral features and evaluated their performance on the ACE 2004 Chinese evaluation data. Huang et al. [2008] provided evidence showing that in ACE 2007 Chinese rela-tion extraction, a rather simple feature-based approach is able to achieve reasonable performance (i.e., 0.63 F-measure); however, the best reported results of parse tree kernel-based approaches is unexpectedly low (i.e., 0.35 F-measure only). More re-cently, Zhang et al. [2009] proposed a composite kernel-based approach for ACE 2005 Chinese RDR task. Chen et al. [2010] adopted Deep Belief Network (DBN) and showed its effectiveness.

The insufficient study in Chinese relation extraction drives us to investigate how to find an approach that is particularly appropriate for Chinese. In this article, we pro-pose a novel position structure based framework for Chinese relation extraction. The contributions are three-fold. First, we propo se a 9-position structure feature, which is used as the major component to form our framework. Second, we derive certain con-straints based on possible relations and relation hierarchies, in order to improve the correctness and consistency of the classified relation types, subtypes and argument or-ders. Third, the entity co-reference inform ation is used to infer more positive relation instances through their links to the classified positive ones. In this section, we describe the features used in our framework. In Table I, we first show the hierarchy of relation types and subtypes, as well as the frequencies of anno-tated (positive) relation instances on the ACE 2005 Chinese corpus.
 Recall that our task is to identify the relations between any two entity mentions. Therefore, all the features are related to the entity mention pairs and their con-texts. Specifically, for each pair of mentions, three kinds of features, namely posi-tion structure feature, entity type/subtype feature and character-based feature, are involved. For vector representations of features for the classification, please refer to Appendix B. Intuitively, the position structure of two entity mentions ( em 1 and em 2 )hassomeob-vious connections with the type/subtype of the relation they might be. This can be un-derstood from the following observations. In a lot of  X  X art-Whole X  relation instances, the position structure of em 1 and em 2 tends to be nested. For example, in the sentence  X  X he U.S. Congress decided to veto the ecology bill X , the two nested mentions, em 1 ( X  X he U.S. Congress X ) and em 2 ( X  X .S. X ) have a  X  X art-Whole.Subsidiary X  relation. In addition, for many  X  X hysical.Located X  relations, the position structure of em 1 and em 2 is more likely to be adjacent, that is, em 1 and em 2 are not nested and there is no entity mention in between them. For example, in a sentence  X  X housands of Palestinians rushed the Israeli checkpoint X , the relation of the two adjacent mentions, em 1 ( X  X housands of Palestinians X ) and em 2 ( X  X he Israeli checkpoint X ) is  X  X hysical.Located X . These obser-vations drive us to analyze the position structure of the two entity mentions in-depth. We define nine types of the position structure as illustrated in Figure 1. The formal definition for these 9-position structure types is given in Appendix A. Appendix C presents one Chinese example (selected from the ACE 2005 dataset for Chinese rela-tion extraction) for each position structure. Here, we briefly explain these nine position structures.

For the structure types (a), (b), and (c), em 2 is nested (i.e., included) in em 1 .In(a), there are no other entity mention that includes em 2 and is also nested in em 1 .In(b), there is at least one entity mention (not em 1 or em 2 ) that includes em 2 and is also nested in em 1 .In(c), em 1 includes em 2 ,and em 2 includes em 1 as well.

For the structure types (d), (e), and (f ), em 1 and em 2 are not nested and there are no other full entity mentions in between them, even though there could be some char-acters in between em 1 and em 2 . In (d), neither of the two entity mentions is nested in other entity mentions. In (e), em 1 or em 2 is nested in another entity mention. In (f ), both em 1 and em 2 are nested in other entity mentions.

For the structure types (g), (h), and (i), em 1 and em 2 are not nested and there is at least one full entity mention in between them. In (g), neither of the two entity mentions is nested in other entity mentions. In (h), em 1 or em 2 is nested in other entity mentions. In (i), both em 1 and em 2 are nested in other entity mentions.
On the other hand, we can merge structure types (a), (b), and (c) into one single structure type. Similarly, we can merge the structure types (d), (e), and (f ), as well as combine the types (g), (h), and (i). This means that one can combine structures of each row in Figure 1 into one logical structure with a logical  X  X r X . As a result, we can obtain three position structures, that i s, Nested+, Adjacent+, Separated+, each corresponding to one row in Figure 1. This 3-position structure feature has been ex-plicitly or implicitly adopted in several methods, for example, in [Che et al. 2005b; Chen et al. 2010; Jiang and Zhai 2007; Zhou et al. 2005]. Specifically, this 3-position structure feature is exactly the position structure feature in Chen et al. [2010]. Zhou et al. [2005] defined an Overlap category of features, which consider if one entity men-tion is included (or called nested) in the other entity mention, and if there are words or other entity mentions in between the two concerned entity mentions. Che et al. [2005b] adopted an Order feature, which also considers if one entity mention is included in the other one. We also think that in the parse tree feature spaces, for example, those in Jiang and Zhai [2007], the position structures of two entity mentions are implicitly considered. 3.1.1 Class Imbalance Problem. We analyze the difference between the 9-position structure feature and the 3-position one in terms of the effectiveness in solving the class imbalance problem. This problem typically occurs when there are far more in-stances of some classes than those of others. In such cases, standard classifiers tend to be overwhelmed by large classes and ignore the small ones and consequently cause a significant bottleneck in performance [Chawla et al. 2004]. The task of relation extrac-tion encounters the class imbalance problem [Culotta et al. 2006; Kambhatla 2006], that is, there are much more None (negative) class relation instances than ACE anno-tated (positive) class relation instances. For instance, in Tables II and III, the overall ratio of positive to negative class is 1:12.01 on ACE 2005 corpus. If we divide all the relation instances according to differe nt position structure types, we can observe that compared with the situation of the 3-positon structures, the class imbalance prob-lem with respect to the 9-positon structures is less serious for the majority ( &gt; 99%) of relation instances. Specifically, in 3-position structures, the ratios of positive to nega-tive relation instances for Nested+, Adjacent+ and Separated+ are 1:0.7273, 1:13.3629 and 1:85.1853, respectively. On the other hand, in 9-position structures, the ratios for Nested, Adjacent, and Separated are 1:0.37, 1:6.82, and 1:42.87, respectively. This relieves the class imbalance problem a lot. It should be noted that these main struc-tures, that is, Nested, Adjacent, and Separated, occupy most ( &gt; 99%) of the positive relation instances. Especially, the Nested structure is the most important one since it has approximately 68% of all the positive relation instances. We can see that the positive-to-negative ratio of Nested structure is much larger than the overall ratio. These two features are concerned with the entity type and subtype of both entity mentions (i.e., em 1 and em 2 ). Entity mentions inherit the attributes (i.e., entity type and subtype) from the corresponding entity. Figure 2 shows the dependency between entity and its mentions. For each mention pair, the combination of their entity types is for entity type feature and similarly their entity subtypes are for the entity subtype feature.

The ACE 2005 categorizes entities into seven types (see Table IV), including  X  X ER X ,  X  X RG X ,  X  X PE X ,  X  X OC X ,  X  X AC X ,  X  X EA X , and  X  X EH X . Each type is further divided into subtypes (see Table IV). Character-based features involve N  X  gram features and wordlist-based features. Before describing them, we extract three types of character sequences from the context where two entity mentions appear. Note that we use characters instead of words.  X  Internal Character Sequence
These character sequences are concerned about the extents and the heads 4 of both entity mentions, and can be categorized into four types of sequences as follows:  X  In-Between Character Sequence
If em 1 ( em 2 ) does not contain em 2 ( em 1 ), then all the characters between two entity mentions will be extracted as the in-between character sequence.  X  External Context Character Sequence
These character sequences are concerned with the characters around two entity mentions in a given window size w s , and can be classified as the following four types.
The extraction of external character sequ ences must comply with one rule, that is, the extracted character sequence cannot enter into or move across any entity mentions.  X  N -gram Features
All character sequences are then transformed into N -gram features. For example, c will be used to construct one Uni-gram feature as well as one Bi-gram feature.  X  Wordlist-Based Features
With insufficient training data, many dis criminative words for the relation extrac-tion might not be covered by N -gram features. Therefore, we build wordlist-based features which are extracted from a published Chinese dictionary. These wordlists in-clude Chinese preposition list (165 words), orientation list (105 words), auxiliary list (20 words), and conjunction list (25 words). It can be expected that some words in these wordlists can serve as strong indicators for some relation types or subtypes. For instance, if there is an orientation word  X  X outh X  in the context of two entity mentions, it is more likely that these two mentions have a  X  X hysical.Located X  relation. The in-between and external context character sequences are transformed to wordlist-based features. On the other hand, the internal character sequences are not involved since they are not likely to include those words related to preposition, orientation, auxil-iary or conjunction words. Each involved character sequence is used to construct one wordlist-based feature for every wordlist. Features with respect to different wordlist are different from each other. Our relation extraction framework is su mmarized in Model 1, which is based on the 9-position structure. In Step 1 we divide all the relation instances into nine parts according to the 9-position structures defined in Section 3.1 in order to solve the class imbalance problem. The detailed motivation of this divide strategy has been discussed in Section 3.3.1 and is also verified by the experiments in Section 5.

In Step 2, we initially perform the RDR task by a cascade strategy, that is, carrying out the relation detection and recognition separately. Specifically, we first classify every relation instance as positive or negative. Then, we classify each positive relation as one of the relation type/subtype. Both classifications are carried out by standard classifiers (i.e., SVMs). The cascade strategy is against the all-at-one strategy, that is, carrying out relation detection and re cognition at one time by classifiers. We do not adopt the all-at-once strategy because the number of positive relation instances in any one relation type/subtype is much smaller than the number of negative ones. On the other hand, the cascade strategy can relieve the class imbalance problem due to the fact that the number of all positive relation instances is much bigger than that of positive ones in any one relation type/subtype. Then, we will explain the strategies in other steps (i.e., Step 3 to Step 5). In many tasks of information extraction, such as entity extraction and relation extrac-tion, some prior knowledge is usually involved that can be useful to the tasks. In ACE 2005 guidelines, a table (e.g., Table V) of possible relation between Arg-1 and Arg-2 is provided. Given two entity mentions, the possible relation type and subtype can be obtained according to the two entity types (listed in Table IV). For instance, according to Table V, if both entity types of Arg-1 and Arg-2 are PER (person), the possible re-lation type can be Per-Social, and the relation subtypes can be Business or Family. If the entity type of Arg-1 is PER and that of Arg-2 is ORG (organization), the possible relation type can be Org-Aff (Org-affiliation).

This kind of prior knowledge has two important roles. First, we can rectify the relation type/subtype classified by SVMs. According to the entity types of two entity mentions, if the classified relation type/subtype is not possible then we will revise the type/subtype to None . Second, if the relation type/subtype is possible, we then adjust the argument order of the two entity mentions.

In many feature-based models [Kambhatla 2004; Wang and Li 2006; Zhou et al. 2005], they used a different approach to the argument order problem. Specifically, except for symmetric relations, the argument order is modeled by considering one re-lation subtype as two new relation subtypes with different orders. For example, the relation subtype Physical.Located is changed to two relation subtypes, namely em 1 -Physical.Located-em 2 and em 2 -Physical.Located-em 1 , where the former denotes that em 1 is the Arg-1, and the latter denotes that em 2 is the Arg-1. There are two draw-backs of their strategy. First, it could be more time-consuming since it involves almost twice the number of classifiers we need. Second, it may make the class imbalance problem more serious because the number of positive relation instances in each (new) relation subtype becomes smaller than the number of the positive instances in each (original) relation subtype. In our framework, the relation type and subtype are classified separately and they may not be consistent. We then try to make th em consistent according to the relation hierarchy (see Table I in Section 3). There are some existing strategies, such as strictly Bottom-Up [Kambhatla 2004; Zhou et al. 2005] and Guiding Top-Down, to deal with this problem. With regard to the strictly Bottom-Up strategy, the relation type should conform to the relation subtype. Once the sub type is recognized, the type is determined by the subtype, since a subtype belongs to one unique type. As for the Guiding Top-Down strategy, the upper level (relation type) guides the down-level (relation subtype). It assumes that the classification result of relation type is more precise. As a result, the subtype will be revised to None if it does not conform to the type.

However, we think that these two strategies lack necessary interaction between two levels, and hence do not make full use of both levels X  classification results. Therefore, we derive the following consistency check strategy.

Similarly, we can have the Subtype Selection based consistency check strategy, which selects cn most likely subtypes, and check them against the types. The relation extraction performances of d ifferent position structures have great disparity. Our experiments show that the performances of the Nested and Adjacent structures are much better than the results of the other seven structures. In fact, there are almost no positive relation instances classified for the other seven position structures. This phenomenon may have two reasons. First, the imbalance class problems in the other seven position structures are much more serious, as evidenced in Table III. Second, intuitively, Nested a nd Adjacent relation instances are more likely to be positive classes (or more likely to be annotated) and hence can be extracted easily.

There are some linguistic homogeneous characteristics (such as co-reference) that can be used to infer more positive relations through the classified positive ones. Specif-ically, after obtaining one classified positive relation with Nested or Adjacent struc-ture, we can assign its relation type/subtype to other relation instances with different position structure but sharing the same attributes. These attributes are related to co-reference information and pattern-based information, which will be described below. 4.3.1 Co-reference-Based Inference. Each entity mention belongs to only one entity and hence naturally inherits the type and subtype attributes from the corresponding entity (see Figure 2). Entity mentions are considered as co-referent when they belong to the same entity.

Once Nested and Adjacent relation instances are recognized as positive, the co-reference information can be adopted to carry out the relation inferring. Specifically, if a relation instance with different position structure has the same two entities as in the classified positive one, this relation instance will be classified as the same rela-tion type and subtype. For example, both  X  X e X  and  X  X ates X  may refer to  X  X ill Gates of Microsoft X . If a relation  X  X RG-AFFILIATION X  is held between  X  X ill Gates X  and  X  X i-crosoft X , it must be also held between  X  X e X  and  X  X icrosoft X . Formally, given two entities e infer more relations which may not be identified by classifiers.

When considering the co-reference information, we may find another type of incon-em 22 ) are different in their contexts or structures, and R denotes the classified re-lation type/subtype. The co-reference not only helps for inference but also provides a chance to check the consistency among entity mention pairs. As the classification results of SVM can be transformed to probability by a sigmoid function the relations of lower probability mention pairs can be revised according to the relation of highest probability mention pairs. In Equation (1), the left side denotes that the probability of relation type/subtype t for relation instance r and y t is the output value of the t by the classifiers. 4.3.2 Pattern-Based Inference. The classified positive relation instances of adjacent structure can infer more relation instances of separated structure if there are some linguistic indicators in the local context. For example, given a local context  X  X oth em 1 and em 2 are located in em 3  X , if em 2 and em 3 are classified as a positive relation in-stance, em 1 and em 2 will have the same relation type/subtype as that em 2 and em 3 hold. Currently, the indicators under consideration are  X  X nd X  and  X  X r X . However, more patterns can be included in the future. We evaluate our relation extraction fra mework on the training dataset for the ACE 2005 Chinese Relation Detection and Recognition (RDR) 7 task provided by the Lin-guistic Data Consortium (LDC). The 633 documents have been manually annotated with 9,299 instances of relations. Meanwhile, 6 relation types and 18 subtypes are predefined. More details are shown in Table I in Section 3. Because of no test data at hand, we randomly select 75% out of the 633 documents as the training data and the remaining documents are used for evaluation. All the reported performances in this article on the ACE RDR 2005 corpus are evaluated using 4-fold cross validation on the entire corpus. In this article, we only measure the performance of relation extraction model on  X  X rue X  mentions with  X  X rue X  chaining of co-reference (i.e., they are annotated by LDC annotators). The aim of our experiments is to evaluate the performance of the proposed features (especially the 9-position structure feature) in Section 3, as well as each step in our relation framework in Section 4. Two baseline methods are involved. Both of them carry out the RDR task as an all-at-once multi-class classification problem. In the first baseline method, the involved features are the 3-position structure feature in Section 3.1 and other features in Sections 3.2 and 3.3. Recall that (see Section 3.1) the 3-position feature is implicitly or explicitly used in many feature-based methods, for example, those in Zhou et al. [2005], Zhou and Zhang [2007], Che et al. [2005b], Chen et al. [2010]. In the second baseline method, the 9-position structure feature and other features are adopted. The first baseline (denoted as 3-Position Baseline) is used to test whether the 9-position feature is helpful, while the second baseline (denoted as 9-Position Baseline) is to test the performance of each step in our framework. When evaluating each step, its following steps will not be executed.

Besides the above main aims, we also evaluate the roles of different categories of features in Section 3 played in our framewo rk. In addition, we provide a performance comparison between our framework and the kernel based framework in Zhang et al. [2009], which also adopted the 9-position feature (slightly different from ours), and carried out the ACE 2005 Chinese RDR task as well. Finally, since the dimensions of the vector representations for all the features are very large, we would like to study the effectiveness of some feature selection methods such as Information Gain (IG) [Yang and Pedersen 1997] and Bi-norm Separation (BNS) [Forman 2003].

The SVMlight [Joachims 1998] with linear kernel and default configuration is adopted as the classification tool. In Steps 1 X 3 of our framework, for every entity mention pair ( em 1 ,em 2 ) in a sentence, we simply choose em 1 as Arg-1, and em 2 as Arg-2, where em 1 precedes or contains em 2 . The window size ( w s ) of character-based features is 4. The options count cn in the type and subtype selection based consistency check strategy (see Section 4.2) are all set to 2.
 As for the evaluation metrics, we adopt three primary metrics, that is, Precision, Recall, F-measure, which are also commonly used to evaluate other relation extrac-tion methods, for example, those in Zhou et al. [2005; 2007; 2010], Zhou and Zhang [2007], Jiang and Zhai [2007], Zhang et al. [2006], and Chen et al. [2010]. In addition, the Wilcoxon signed rank test is adopted as the measure of the statistical significance of the improvements over baseline methods. The improvements (at significance level 0.05) over the 3-Position Baseline and 9-Position Baseline are denoted as  X   X   X  X nd X   X   X , respectively, in the result table. In each table, both the performance of positive rela-tion types and those of positive relation subtypes will be reported. All results are the average ones over 4-fold experiments. Note that the results are slightly different from those in Li et al. [2008], which did not involve the 4-fold experiments. In this set of experiments, we will first compare our 9-position feature with the 3-position feature when all other features are involved and we do not divide the relation instances. Second, we evaluate our Divide strategy (Step 1) in our framework. Table VI summarizes the experimental results. We have the following conclusions. First, when we do not divide relation instances, the 9-Position Baseline significantly outperforms 3-Position Baseline, which shows the effectiveness of our 9-position feature. This is due to the fact that the class imbalance problem of the 3-positon is more serious than that of the 9-position (see Section 3.1 and Tables II and III). Second, the 9-Position-Divide significantly further improves the F -measure 18.15% and 23.15% over 9-Position Baseline in relation types and relation subtypes recognition, respectively. The aim is to investigate the effectiveness of the cascade strategy, that is, the Step 2 in our framework. In the detection stage, binary-class SVMlight is adopted, while in the recognition stage, multi-class SVMlight is adopted. Table VII presents the experimental results. We can see that the Cascade strategy outperforms the all-at-once strategy. As discussed in Section 4.1, the possible relation information between Arg-1 and Arg-2 has two important roles: one is to rectify the classification results; the other is to adjust the argument order. Table VIII shows the performance of this step. We can clearly see that it is contributing and improves the F -measure 2.82% and 5.26% in type and subtype recognition, respectively. This is to test Step 4, that is, the consistency check method in Section 4.2. Table IX shows the results, indicating that the strategies using subtypes to determine or select types (Type Selection) perform better than the Subtype Selection Strategy. This may be attributed to the fact that previous correction (in Step 3) for relation subtype is better than that of relation type. Overall, the type selection based consistency check strategy is the best one. We first present the results (after Step 4) of Nested and Adjacent structures in Table X. The results of other structures are not shown since they are almost zero. According to these reported results and our discussion in Section 4.3, intuitively we can follow the path of  X  X ested  X  Adjacent  X  Separated  X  Others X  to perform the inference. But soon we find that if two concerned entity mentions are nested, almost all the co-refereed mentions are nested. So basically inference works on the path  X  X djacent  X  Separated  X  Others X .

Then, through this inference path, we use both co-reference information and lin-guistic indicators to construct relation infe rring. The performance of relation inferring is summarized in Table XI. We can see that the inferring step does not help as much as we have expected. This might be due to that the lack of enough annotated relations for co-reference mentions and for those sharing the same patterns, that is, linguistic indicators. Then, we evaluate the contribution of every feature category for our framework. All the steps in our framework (see Section 4) are involved, but we will adopt the features in-crementally. Only entity type and subtype f eatures do not work. Therefore, Table XII shows the results when we incrementally add the 9-position structure, the external contexts and internal contexts, Uni-grams and Bi-grams, and at last the word lists on them. The observations are: first, the 9-position structure provides stronger sup-port than other individual features. Second, Uni-grams provide more discriminative information than Bi-grams. Third, external context seems more useful than internal context. At last, the wordlist feature slightly improves the performance.
 We provide a performance comparison between our framework and the kernel-based framework in Zhang et al. [2009], which was also evaluated on the ACE 2005 Chinese RDR task (for relation type only). Table XIII reports the results, which shows that our approach outperforms this kernel-based approach, although it uses features that are similar to those in our framework. Since the large dimension and serious sparseness of the vector representation, we would like to test whether the feature selection methods can be useful in our task. Two feature selection methods (IG and BNS) are investigated.

Information gain [Yang and Pedersen 1997] of a term measures the number of bits of information obtained for category prediction by the presence or absence of the term in a document. Let m be the number of classes. The information gain of a term t is defined as Forman [2003] presented an empirical comparison of 12 feature selection methods. Results revealed the surprising performa nce of a new feature selection metric,  X  X i-Normal Separation X  (BNS). Let tp ( t ) be true positives (number of positive cases con-taining term t ), fp ( t ) be false positives (number of negative cases containing term t ), pos denote the number of positive cases, neg be the number of negative cases, tpr ( t ) denote the sample true-positive rate ( tp ( t )/ pos )and fpr ( t ) be the sample false-positive rate ( fpr ( t )/ neg ). BNS can be defined as follows: For each method mentioned above, we implement feature selection in two ways. One is to construct feature selection on the whole feature space. The other is to implement feature selection on N -gram subfeatures, for example, left-4 context Uni-gram, while holding entity type/subtype and wordlist-based features unchanged. The latter strategy gain better performance according to the expe rimental results in Figures 3 and 4, where  X  X revious X  corresponds to the result without feature selection. Although fewer features can reduce the time cost of classifiers, the relation extraction results do not seem to be promising. This might be because that SVM itself has enough power to find the discriminative dimensions on the given data set. To continue this direction, we may want to use some other formal methods, such as the PLSI [Hofmann 1999], which has successful application in text processing. This remains as our future work. In this article, we propose a position structure-based framework for Chinese entity relation extraction. The main contributions can be concluded as follows. First, a 9-position structure feature, which is conceptually clear and computationally efficient, is devised to relieve the serious class imbalance problem. This feature is also used as a major component to form our divide-and-conquer relation extraction framework. Second, the possible relation-based constraints are used to verify the relation clas-sification results and adjust the argument orders of relations. Third, an interactive consistency checking strategy is proposed to check whether the classified type and subtype conform to the given relation hierarchy. Last but not the least, co-reference information and pattern-based features are used to infer the more positive relations through the classified positive ones. The effectiveness of them, especially the position structure feature, has been demonstrated in the experiments conducted on the ACE 2005 Chinese data set.

Although the inferring step has not received the convincing performance improve-ment, this direction could be interesting and fruitful. It is because that the inferring can be derived from a graph where a vertex represents an entity, and the initial edge is the classified relations of Nested and Adjacent structure. Then, the other relations of any structure could be represented by this graph. Moreover, this graph can represent the relations of entity pairs which are not in one sentence. We will investigate this direction in the future. Furthermore, as for the efficiency issue of the proposed framework, we would like to investigate the usefulness of l 1-norm SVM, which is efficient in dealing with large-scale data sets [Sra 2006]. We will also sys-tematically investigate its effectiveness in improving the performance of the relation extraction.
 Given an entity mention em ,let em . start and em . end denote the start and end positions em 2 ,where em 1  X  em 2 or em 1 precedes em 2 , the position structure of them can be grouped into nine categories in Table XIV.
 Once the features are obtained, the task of the Chinese Relation Extraction is modeled as a multi-class classification problem. Support Vector Machine (SVM) [Boser et al. 1992; Cortes and Vapnik 1995] is selected as the classification tool since it represents the state-of-the-art in the machine learning research. Given a training set of labeled instance pairs (x i , y i ) , i =1 ,..., l where x i  X  R n and y i  X  X  1 ,  X  1 } l ,SVMrequiresthe solution of the following optimization problem:
We use SVM in both the relation detection process and relation type and subtype recognition process. As described in Manevitz and Yousef [2001], there are four differ-ent text representations, that is, binary, frequency, tf-idf, and Hadamard. In this arti-cle, we use binary vector representation for the features obtained before, as explained in Table XV. We then combine the following vectors into a single feature vector to SVM.

