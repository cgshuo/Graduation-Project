 We investigate to what extent people making relevance judgements for a reusable IR test collection are exchangeable. We consider three classes of judge:  X  X old standard X  judges, who are topic origi-nators and are experts in a particular information seeking task;  X  X il-ver standard X  judges, who are task experts but did not create topics; and  X  X ronze standard X  judges, who are those who did not define topics and are not experts in the task.

Analysis shows low levels of agreement in relevance judgements between these three groups. We report on experiments to determine if this is sufficient to invalidate the use of a test collection for mea-suring system performance when relevance assessments have been created by silver standard or bronze standard judges. We find that both system scores and system rankings are subject to consistent but small differences across the three assessment sets. It appears that test collections are not completely robust to changes of judge when these judges vary widely in task and topic expertise. Bronze standard judges may not be able to substitute for topic and task experts, due to changes in the relative performance of assessed sys-tems, and gold standard judges are preferred.
 H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness) Performance, Experimentation, Measurement
Test collections for information retrieval (IR) typically consist of a corpus of documents, a set of topics, and a set of relevance judge-ments over a subset of documents from the corpus for each topic. Relevance judgements or assessments are made by people called  X  Work carried out at the CSIRO ICT Centre.
 relevance judges or assessors (we use the term judges). The rela-tive performance of IR systems can be compared by their ability to discover relevant documents over the set of topics using measures such as mean average precision (MAP) and normalised discounted cumulative gain (NDCG) [12]. In earlier times, it was possible to judge every document for every topic because the size of the corpus was so small. However, even by the mid 1970 X  X , this became prac-tically infeasible due to the increasing number of documents in the available corpora, and the pooling method was developed to select a likely subset of documents for review by the judges [13]. In the pooling method, unjudged documents are considered to be irrele-vant, although it is known that relevant judgements may continue to be discovered from this set [4, 9].

TREC Enterprise 2007 developed a new test collection [3], with documents from Australia X  X  Commonwealth Scientific and Indus-trial Research Organisation (CSIRO). Topics were created by the Organisation X  X  science communicators and documents were judged for relevance by a variety of people inside and outside CSIRO.
As noted by Harter [11] and others, substantial variations in rele-vance judgements arise among different people, reflecting a host of possible backgrounds and experiences. We used the development of this test collection as an opportunity to investigate two issues. First, whether differences in topic and task expertise affect rele-vance judgements in any systematic way. Second, whether differ-ent relevance judgements consequently affect performance scores and rank ordering assigned to IR systems.
A number of studies have investigated whether variations in rel-evance assessment exist and how they affect measures of retrieval performance. These are summarised in Table 1. (A related body of work, investigating IR system evaluation measures and their corre-spondence with user satisfaction, is not the focus of this survey.)
Lesk and Salton X  X  study [14] used eight people (a mixture of li-brarians and library science students), each authoring six topics. Each topic author judged their own six topics over the Ispra col-lection (1268 abstracts in the documentation and library science field), using a binary scale, and also judged six more topics from six different authors. All documents for all topics were judged by two people, one of whom was the topic author. Agreement levels averaged 0 . 31, using Jaccard similarity. The relevance assessments were used as input to three variations of the SMART system, which produced performance measures. Lesk and Salton found that de-spite the low level of agreement among judges, the recall-precision
Voorhees [22] + Cormack et al [9] 2 + 3 49  X  124 2 X 5 33%, Jaccard 74 MAP output measures remain  X  X asically invariant for the collection be-ing studied X . They explored four reasons why this may be so, and concluded that this is ultimately due to similarities in the docu-ments retrieved early in the ranked lists by different methods, and considered relevant by either judge, as the performance measures used favour finding early relevant documents. Lesk and Salton dis-cuss their work in terms of a strong and a weak hypothesis, both of which they found support for in their experiments. These can be readily understood in terms of changes to relevance judgements making changes to system scores (by some measure) or to relative orderings of systems (by their scores on some measure).
 Cleverdon X  X  work [7] reports on re-judging documents from the Cranfield-II investigation by three additional judges, and used 42 topics, 19 index languages, and a five-level relevance scale. Rather than rejudging the entire corpus exhaustively, 200 documents were selected for each topic, using a non-random sampling method to fo-cus on documents originally judged relevant, plus some additional documents from the corpus. Each judge re-judged all topics. The rankings of the normalised recall (NR) scores for the index lan-guages were then compared using Spearman X  X   X  for each of the three new judges in addition to the original Cranfield judges; cor-relations were found to be at least 0.92 for each combination of judges. Cleverdon concludes, similarly to Lesk and Salton, that performance measures and rank-orders remain similar despite dif-ferences in relevance assessments.
 Burgin [5] reports on a study with a corpus of 1239 papers from Medline. Four groups of judges were involved: three groups were subject experts (one a professor of pediatrics and the topic author; one a set of nine faculty colleagues; and one a group of postdoc-toral fellows in pediatrics) and one was a medical bibliographer. They carried out relevance judgements for 100 queries, using a three-level relevance scale. Mean inter-judge agreement levels us-ing overlap agreement as per Lesk and Salton ranged from 40% to 55%. Burgin re-tested some of the explanations in Lesk and Salton X  X  study for why variations in relevance judgements did not materially affect the recall and precision results, in the context of his own study. He found support for their hypotheses, as well as three of their explanations for why these results held, despite not-ing differences in relevance scales, instructions to the judges, col-lection material, and nature of queries.

Harter [11] explored the past literature of empirical studies on how variations in relevance assessments affect measures of retrieval effectiveness. He noted:  X  X ll find significant variations in rele-vance assessments among judges. And all conclude that these vari-ations have no appreciable effect on measures of retrieval effective-ness, that is, in the comparative ranking of different systems. X 
He concluded by calling for an evaluation approach that reflects real world users; specifically trying to address and control for fac-tors that lead to variations in relevance assessments. Our study pursues this approach with task and topic expertise.

In the TREC-5 overview [23], Voorhees and Harman report on inter-judge agreement experiments with a group of NIST-employed judges (retired information analysts), who also comprised the orig-inal topic authors. Assessment used a binary relevance scale, over 49 topics. The re-judged pools consisted of 400 documents per topic, with up to 200 judged relevant by the topic author and the balance made up of judged non-relevant documents. Each topic was re-judged by two judges. Harman and Voorhees found rela-tively high levels of three-way unanimous agreement (71 . pared to earlier studies. An observation was that judges agreed more on non-relevance than relevance. They also created sets of relevance judgements and compared system performance on MAP across the different judgement sets, reporting little apparent change in system rankings. The TREC-4 collection was many times the size of earlier collections, and also more diverse, but despite this the conclusions remained fundamentally the same.

This preliminary work was followed by a longer set of exper-iments by Voorhees comparing judgements from both the NIST judges for TREC-4, and NIST and University of Waterloo judges for a set of overlapped judgements on part of the TREC-6 dataset [22]. The Waterloo judges used an interactive searching and judg-ing process to find and assess documents, using a three-level rele-vance scale. NIST judges used the standard TREC pooling method and a binary relevance scale. Correlation using Kendall X  X  0 . 896 for 76 systems ranked by MAP over the relevance judge-ments produced by NIST and the University of Waterloo. One topic (of 50) had no overlapping documents from both sets. This thorough analysis concluded that the relevance assessments created rarely have major effects on the relative system ordering. Voorhees adds caveats regarding situations when the number of relevant doc-uments for topics is low, and when runs involve significant rele-vance feedback. Part of the explanation for this, she suggests, is instability in the measures used for performance in these circum-stances. She also remarks on the minimum number of topics re-quired to obtain acceptable correlation  X  her assessment was a minimum of 25 for this dataset. She concluded that the Cranfield approach and the TREC test collections were robust for their pri-mary purpose  X  analysing the performance of retrieval algorithm variations when building an IR system. A twin of this study is the work reported by Cormack et al. in [9], although their focus is more on how to create test collections without the expense of pooling.
Sormunen X  X  study reported re-judging work comparing TREC X  X  binary relevance scale with a four-level relevance scale [19], but system performance measures were not the concern here.

Most recently, Trotman et al. report on a series of experiments for multiple judging of 15 INEX 2006 topics using between three and five judges per topic [20, 21]. Rank correlation of the 64 sys-tems was &gt; 0 . 95 using Spearman X  X   X  , over a number of synthesised relevance judgement sets compared to the original baseline. Their conclusion was that exchanging judges makes little appreciable dif-ference in rank order of systems, and suggest this would support an effective way to partition judging workload among individuals.
On a related issue, Mizzaro provides some frameworks for mea-suring disagreement between judges [15], but does not carry out empirical studies.
The collection used in the TREC Enterprise 2007 track was cre-ated by crawling pages from CSIRO X  X  public websites. These were distributed as the corpus to track participants, together with 50 top-ics. The topics were created with the involvement of science com-municators from CSIRO. Their role is to communicate the science and business of CSIRO to the general public, including by publish-ing material on the public websites. The science communicators were asked to create topics for what was described as the  X  X issing overview page X  problem  X  where an overview page on a particular general topic was not currently available, and the science commu-nicator was interested in creating one. This is a real job carried out by science communicators. Topics consisted of a short query (as might be issued to a web search engine); a longer description of the kinds of subjects that ought to be covered in the topic; a few key URLs for documents that already existed; and a few key contacts (human experts within CSIRO).

The track X  X  document search task required each system to re-port a ranked list of documents relevant to the topic. At least one run from each participant had to be a query-only run without man-ual intervention of any kind. Individuals among the track partici-pants carried out judging of pooled document results for each topic. Pooling was to depth 75 from the two runs per participating sys-tem given assessment priority by the participant. Judging instruc-tions stated that the documents were to be classified on the basis of whether they would be good candidates to be included in a list of links in the new overview page. Documents were classified by each judge as  X  X ighly likely X  to be included in this list;  X  X ossibly X  included or useful; or  X  X ot X  useful. This type of task-specific rel-evance classification is common to a large body of test collection creation activities in the Cranfield tradition, although the number of categories used may vary. Multiple judgement levels are more common in Web-oriented retrieval activities than classic ad hoc in-formation retrieval evaluation.
 Per-topic measures reported were average precision (AP) and NDCG with a gain of 2 for  X  X ighly likely X  and 1 for  X  X ossibly X  documents. To compute AP, only  X  X ighly likely X  judgement classi-fications were considered relevant.

At a later time, a number of documents were re-judged by two sets of additional judges: as a  X  X old standard X  documents were re-judged by the CSIRO science communicators who originally pro-posed the topic, and as a  X  X ilver standard X  they were rejudged by science communicators from outside CSIRO. We consider  X  X old standard X  judges to be experts in both the task and the topic.  X  X il-ver standard X  judges are considered to have task expertise, but not topic expertise. Finally, the original  X  X ronze standard X  judges  X  TREC participants  X  have neither task nor topic expertise. Since the time of gold and silver standard judges was at a premium, we sampled documents from the pools to limit the number of judge-ments being carried out by these individuals.

Of the 33 topics for which we have complete overlap sample judgements (gold, silver, and bronze standard), a total of three gold standard judges, one silver standard judge, and nineteen bronze standard judges were involved. (More silver judges participated, but unfortunately without overlap on topics from gold judges.) A total of 3004 documents were judged by these three sets of judges (compared to 22500 for the full pools for corresponding topics). There was an average of 91 documents per topic, with a low of 53 and high of 176.
CSIRO science communicators were the assumed system users for TREC Enterprise 2007 and represent the best possible judges. However, evaluations using the Cranfield model often substitute sil-ver or bronze standard judges. Performance scores based on these lower-quality judgements do not measure performance for the as-sumed system user, and may diverge to the extent that silver or bronze standard judgements differ from the gold standard. Any such divergence could in turn lead to incorrect conclusions regard-ing a system X  X  suitability for the task.

The questions we asked in the experiments here were: 1. How much agreement in relevance judgements is there be-2. If there is a low level of agreement, what effect does this
Measures for sampling methods were originally established us-ing random selection of documents for judging [24]. This approach has recently been extended for non-random selection of documents [25]. Past work of Harman and Voorhees [23] suggested judges were more likely to agree that documents were irrelevant to a topic, than they were to agree on what constituted relevance. In situations where judging is limited or costly, it makes sense to focus judg-ing effort on those documents more likely to be potentially relevant than those which are uncontentiously irrelevant. Indeed the pooling method typically used in TREC is exactly one such approach; also highly ranked relevant documents have more effect on performance measures than lowly ranked ones. As collections grow bigger, there arise more concerns regarding the creation of bias in the relevance assessments using pooling [4]. Investigations of non-random sam-pling methods in this context have also been conducted [18], which look to ensure sufficient samples are carried out in highly ranked documents due to their effect on performance measures.

In this section, we introduce infNDCG, a measure which like infAP [25] estimates a well-known performance score from a small number of judgements. A detailed derivation and analysis can be found elsewhere [25].

With infNDCG shown to be accurate, we examine the level of inter-judge agreement (Section 4.3) and the effect this has on sys-tem performance measures and rankings (Section 4.4).
Gold and silver standard relevance judgements were not avail-able over the entire pool. We therefore use inferred measures to es-timate what would have been recorded over full judgements. InfAP [24] is an estimator for average precision; infNDCG, which we de-scribe here, estimates NDCG.
There are different versions of the NDCG metric depending on the discount function and the gains associated with relevance grades, etc. In this paper, we adopt the version of NDCG in trec_eval Let r ( 1 ) , ( 2 ) ,... r ( Z ) be the relevance values associated with the Z documents retrieved by a search engine in response to a query q . Then, the NDCG value of this search engine can be computed as N q is the normalisation constant for query q , chosen so that the NDCG value of a perfect list is 1.

Estimation of NDCG with incomplete judgements can be divided into two parts: (1) estimating N q and (2) estimating DCG. Then, NDCG can be computed as E [ DCG ] / E [ N q ] .
The normalisation constant N q for a query q can be defined as the maximum possible DCG value over that query. Hence, esti-mation of N q can be defined as a two-step process: (1) For each relevance grade r ( j ) &gt; 0, estimate the number of documents with that relevance grade. (2) Calculate the DCG value of an optimal list by assuming that in an optimal list the estimated number of documents would be sorted (in descending order) based on their relevance grades.

Suppose incomplete relevance judgements were created by di-viding the complete pool into disjoint sets (strata) and randomly picking (sampling) some documents from each stratum to be judged. The sampling within each stratum is independent of the other, hence, the sampling percentage could be different for each stratum.
For each stratum s , let nr s ( j ) be the number of sampled docu-ments with relevance grade r ( j ) and let n s be the total number of documents sampled from strata s and N s be the total number of documents that fall in strata s . Since the n s documents are sampled uniformly from strata s , the estimated number of documents with relevance grade r ( j ) within this strata can be computed as
Then, the expected number of documents with relevance grade r ( j ) within the complete pool can be computed as
Once these estimates are obtained, one can use these values to estimate the value of the normalisation constant.
Given Z documents retrieved by a search engine, let r ( i relevance grade of the document at rank i and log 2 ( i + discount factor associated with this rank. For each rank i , define a new variable x ( i ) , where x ( i )= Z  X  r ( i ) / log can be written as the output of the following random experiment: 1. Pick a document at random from the output of the search
Any sampling strategy could be thought of here. For example, an extreme case is where each document has a different probability of being sampled as discussed in Aslam et. al. [2]; each stratum can be thought of as containing only one document. 2. Output the value of x ( i ) .

It is easy to see that if we have the relevance judgements for all Z documents, the expected value of this random experiment is exactly equal to DCG.

Now consider estimating the outcome of this random experiment when relevance judgements are incomplete. Consider the first step of the random experiment, picking a document at random. Let Z the number of documents in the output of the search engine that fall in strata s . When picking a document at random, with probability Z / Z , we pick a document from strata s .

Therefore, the expected value of the above random experiment can be written as:
Now consider the second step of the random experiment, com-puting the expected value of x ( i ) given that the document at rank i falls in strata s . Let sampled s be the set of sampled documents from strata s and n s be the number of documents sampled from this strata. Since documents within strata S are uniformly sampled, the expected value of x ( i ) can be computed as:
Once E [ N q ] and E[DCG] are computed, infNDCG can then be computed as infNDCG = E [ DCG ] / E [ N q ] .

Note that this assumes that N q and DCG are independent of each other, which may not always be the case. Better estimates of NDCG can be obtained by considering this dependence. For the sake of simplicity, throughout this paper, we will assume that these terms are independent.
We were interested to investigate different methods for choosing documents for rejudging by gold and silver standard judges given at least one (bronze standard) labelling already. Two methods were used: topic-based sampling and effort-based sampling. These may be explained most easily with reference to an example.

Consider the judgements in Table 2 for two topics A and B. In topic-based sampling, we wish to apportion the sample size in ac-cordance with the original pool size of each topic; for example, we might choose a 10% sample size. Thus we would select 10 docu-ments from topic A and 20 documents from topic B. Placing em-phasis on documents originally judged highly or possibly relevant, we might choose to select 60% of each topic X  X  sample from the highly relevant labelled documents, 30% of the sample from the possibly relevant labelled documents, and 10% of the documents from the not relevant label. We would thus select documents in numbers as shown in the second part of the table. Note how this places re-judging emphasis on those documents originally labelled highly relevant  X  they form 60% of the sample to be re-judged, but constitute only 20% of the original pool. Note that the row sums for both topics are exactly 10% of their original size.

The effort-based method samples documents with some proba-bility from each pool of labelled documents, irrespective of topic. The third row (labelled  X  X otal X  in the table) is now the data we use for sampling. Using the same overall 60 : 30 : 10 ratio for the final 10% sample of 30 documents, we might end up with documents chosen from each topic as shown in the third part of the table. Note the column rows sum to the same numbers as for topic-based sam-pling, but the topic row totals do not.
In our experiments, we wished to support an investigation of which sampling method might prove better. With relatively little extra judging effort, this could be achieved by aggregating maxi-mum numbers of documents per topic, as shown in the final part of the table. We refer to this as full sampling.

These three sampling methods (topic, effort, and full) were com-pared to the full measures evaluated over the bronze standard judge-ments (the only ones available with entire pool judgements). Se-lection percentages for the topic sample method were 60 ( 30 ( possibly ) :10 ( not ) , with replacements to make up the per-topic sample size coming in order from the more relevant sub-pools. (So for example, if there was only 1 document labelled  X  X ighly likely X , but the sample size said there should be 5, then an additional 4 doc-uments would be chosen at random from the documents labelled  X  X ossibly X  for the topic.) For the effort sample method, the percent-ages were 42 : 28 : 3 from the respective bronze standard labelled pools of documents (over all 50 topics).

The original definition of infAP [24] assumes that incomplete relevance judgments are a random subset of complete judgments. Throughout this paper, when we refer to infAP, we refer to the new version of the measure that can incorporate non-random judg-ments [25].
 Calculations of infAP and infNDCG were compared to AP and NDCG (as computed by trec_eval ; NDCG calculation is pre-official release) across all 33 topics and across all 15 automatic query-only runs. This approach (per-topic comparison) is intrinsi-cally more likely to show differences, due to topic variation, than comparing scores averaged over all topics. Note that infAP and AP were calculated with  X  X ighly likely X  and  X  X ossibly X  judgements collapsed into a single  X  X elevant X  category for this purpose. (We also computed infAP and AP counting only the  X  X ighly likely X  doc-ument judgements as relevant, but little difference exists between the two approaches.) Results are shown in Table 3. Kendall X  X  both inferred measures is highly correlated with the corresponding full measures ( p &lt; 0 . 01) in all three methods. Error bars are calcu-lated over 10 subsamples drawn at random from the full sample for the effort and topic sample methods. The effort sample method has
Topic: AP NDCG topic 0.89  X  0.01 0.053  X  0.002 0.91  X  0.01 0.037  X  0.004 effort 0.86  X  0.01 0.062  X  0.008 0.80  X  0.02 0.063  X  0.007
Mean: MAP mean NDCG topic 0.99  X  0.01 0.027  X  0.003 0.97  X  0.02 0.005  X  0.002 effort 0.96  X  0.02 0.012  X  0.007 0.92  X  0.02 0.011  X  0.002 Table 3: Kendall X  X  tau correlation (  X  ) and Root Mean Squared error (RMSe) for original vs inferred measures. For topic and effort sampling, figures are mean  X  one standard error. the lowest correlation, possibly because the effort sample method was based on pools formed from the labels over all 50 topics, rather than the final 33.

Kendall X  X   X  provides us with an understanding of how well the different measures track the original, but there could still be a large difference in absolute scores. To assess how closely the scores are related, we use root mean square error (RMSe in the table). The worst RMS error value is 0 . 063  X  0 . 007 (using infNDCG and effort sampling), and other combinations are lower than this.

The measures of correlation and RMS errors originally reported [24] are based on scores averaged over all topics. We show the results for averaged scores in the lower half of Table 3. The corre-lation and error rates compare favourably with the earlier work.
These results demonstrate that the two inferred measures appear to be reliable estimators, and can be calculated with much less judg-ing effort. In the remainder of this paper, we report infAP and infNDCG over 33 topics as a measure of performance using the full sample (since, probably because it has more documents in the sample, it has the highest  X  and lowest RMSe).
InfNDCG and infAP are reliable predictors of NDCG and AP, but will differ as relevance judgements disagree. We therefore con-sidered the level of agreement between judges over the TREC En-terprise documents and topics.

Table 4 summarises the level of agreement between judges, as conditioned probability distributions. For example, of those doc-uments classified as not relevant by bronze standard judges, 5% were classified as highly relevant by gold standard judges; of those documents judged highly relevant by bronze standard judges, 36% were classified the same way by silver standard judges.

Several trends are apparent in this simple analysis. First, there is broad agreement between all groups of judges on irrelevant docu-ments, with between 81% and 89% agreement between each pair. There is also reasonable, although lower, agreement on which doc-uments are highly likely to be included in the putative overview page. The lowest agreement in this case has 36% of documents marked highly by bronze standard judges marked the same way by silver standard judges; these are the two sets of judges without topic expertise. There is most disagreement on documents marked as possible, although the agreement still appears better than chance alone.

Overall, silver standard judges rate fewer documents in the top two categories than do gold standard judges, which suggests scores Table 4: Agreement between judges (probability distribution of gold or silver standard judgements, given bronze or gold stan-dard judgements). See text for details. based on these judgements would be lower. Bronze standard judges are more willing than their gold standard counterparts to mark doc-uments as possibly or highly likely to be included: there is only 58% agreement with the gold standard in the  X  X ighly likely X  cate-gory, and 24% of the documents marked highly by bronze judges were considered irrelevant by the gold standard. We may expect higher scores from these judgements than from the gold standard.
Further analysis used Cohen X  X   X  [8], which is a chance-corrected measure of the agreement between two judges classifying objects into arbitrary categories. Negative values indicate less agreement than would be expected by chance alone, zero indicates agreement due only to chance, and higher values (up to 1) indicate agreement beyond that expected by chance. In the field of computational lin-guistics, Carletta made a case for  X  in inter-coder agreement [6], and it has become a standard measure there, albeit with ongoing discussions about the appropriate way to interpret what values sig-nify degrees of agreement [10]. A recent survey article by Artstein and Poesio [1] provides a thorough review of the many consider-ations in the use of the  X  measure, its assumptions, variants, and alternatives. Here we use a variant due to Siegel and Castellan [17], and following Rietveld and van Hout [16] consider  X   X  indicate substantial agreement.

Agreement between gold and silver standard judges, calculated over the labelling of all sampled documents for each topic, is il-lustrated in Figure 1(a). Although in 25 of the 33 topics agree-(  X   X  0 . 6). Similar results were observed for the gold and bronze standard judges (Figure 1(b)): significant but small agreement in 22 of 33 topics and significant and substantial agreement in only one. Regardless of the debate over the measure, these results fail to indicate much agreement exists between judges.

Note that due to the sampling method used (Section 4.2), most documents seen by gold or silver standard judges had already been marked as  X  X ighly likely X  or  X  X ossibly X  by a bronze standard judge. If all judges classified a uniform random sample drawn from the full set of documents in the corpus, we may expect higher levels of agreement reported by  X  , because the expectation of seeing a relevant document is low in such a sample, and it is easy for judges to detect obviously irrelevant material. In general, however, we are less interested in these  X  X asy X  cases where documents are clearly irrelevant; and disagreements about these documents are less likely to impact performance scores for a well-performing IR system.
This low level of agreement between judges, over a substan-tial majority of topics, suggests that measures including infAP and infNDCG may vary substantially if judges are exchanged. Further analysis investigated this possibility.
TREC runs vary in the amount of information they use and the way they process the query. In the following analyses we have considered the fifteen runs which were reported as fully automatic, using short query titles only, as they are most representative of cur-rent search systems.

This selection of runs avoids having many runs from the same system, which often vary in small and subtle ways due to algorith-mic experiments being carried out by participants. Past work has indicated that systems differ more between each other, than from variations in a single system. We thus chose not to include such variants since small differences in judgements may lead to per-turbations in system ordering (and falsely drawn observations that changes in ordering occur) that are not good indicators for more significant differences between systems.
A simple analysis considered the effect of variations in judge-ments on each system X  X  performance. Inferred scores, both infAP and infNCDG, were calculated for each selected run and topic for all three sets of relevance judgements. We take the score at each topic as an indication of each run X  X  performance, and use the Wil-coxon matched-pairs signed-ranks procedure to compare perfor-mance according to each set of judgements.

There was a consistent, but small, difference in infAP scores calculated from gold or silver standard relevance judgements (Fig-ure 2(a)). Of the fifteen runs considered, eleven performed signifi-cantly worse on silver standard than gold standard judgements, with absolute differences in mean infAP of 0.01 to 0.07 ( p &lt; formance appeared higher using bronze standard judgements than gold standard judgements; six of fifteen runs had absolute improve-ments of 0.08 to 0.10 ( p &lt; 0 . 05). This is consistent with our obser-vations in Section 4.3.

Differences were less consistent when systems were scored by infNDCG (Figure 2(b)). Only one run was significantly different with silver standard judgements (a drop of 0.02), and five runs were significantly better with bronze judgements (a gain of 0.06 to 0.08). Again, this is in agreement with earlier observations.

Overall, infAP is more likely to differ between judges than is infNDCG. Bronze standard judges also show more variation from the gold standard than do silver standard judges. The absolute dif-ferences in mean scores are small, however, even in those cases where they are statistically significant.
Ranked lists of runs, ordered by performance scores, are com-monly reported and can be used to identify the best-performing systems. If changes in relevance judgements, and hence on per-formance scores, were to perturb these ranked lists then medium-or low-performing systems may mistakenly be identified as high-performing; this could influence future development and possibly purchasing decisions. A final analysis therefore investigated the difference in these ranked lists as judges are changed.
When systems were ranked by mean infAP, there was little dif-ference in order whether gold or silver standard judgements were used (  X  = 0 . 96). The same was true for gold and silver standard judgements when systems were ordered by mean infNDCG (  X  = 0 . 94). Comparing orderings produced by gold and bronze standard judgements, however, showed larger differences:  X  = 0 . 73 and 0 for mean infAP and mean infNDCG respectively.

Some consistency in determining the top performing systems was observed, more noticeably with gold and silver judgements. The top four runs were ranked highest on both mean infAP and mean infNDCG (though not in the same order) by these judges. Bronze judgements agree in identifying the top three performing systems with the gold standard using infNDCG. All agree on the three lowest performing systems with both measures.

It seems therefore that the document-level disagreements ob-served between gold and silver standard judges do not have a mate-rial effect on system rankings. Rankings based on bronze standard judgements however show greater difference, and where judges of this type are used rankings should be treated as approximate only.
Our experiments have some limitations. First, although our 33 topics are more than the 25 recommended for stability by Voorhees [22], more topics would be better. Similarly, having more judges involved in each category would increase confidence that effects being seen were due to the task and topic expertise level of the judge. Second, the results we have found may be due to the partic-ular task and organisation considered here, and evaluation of differ-ent tasks or organisations may not show similarly consistent differ-ences. Third, it would have been interesting to control the sampled judging process with additional bronze judges doing re-judging. Fi-nally, we did not control for other factors which may affect judge-ments, beyond the grouping of judges into expertise categories and presentation of documents in random order.

Future work will increase the number of topics and judges in-volved. We also intend carrying out further analysis to investigate if there are additional systematic sources of variance that contribute to the differences we have observed.

There are several contributions in this paper. In Table 1, we provide a useful summary of several published empirical studies on document relevance judging by multiple judges. We believe we are the first to argue for using Cohen X  X   X  , which corrects for agreement by chance, to measure inter-judge agreement in IR rel-evance assessment. We demonstrate empirically using Kendall X  X   X  and RMS error that inferred measures (for non-uniform random sampled judging) correlate well with AP and NDCG both by topic and averaged over all topics. On the data available, topic-based sampling appears a better approach than effort-based sampling. Fi-nally, we have tried to control for degrees of task and topic expertise in our groups of judges to investigate their effect. The Cranfield (and TREC) methodology gives us a user model. In an experiment that is intended to model a real-world search sce-nario, such as the TREC Enterprise track, it is desirable that judg-ments in the model correspond to opinions of a real user. Past stud-ies have shown that different judges from the same population are exchangeable. Our judges are drawn from different populations.
Our conclusions arising from the experiments conducted are as follows. First, we have investigated whether task and/or topical expertise affect relevance judgements in a consistent manner  X  they do. Relatively low levels of agreement exist between types of judge, and the agreement is even less between gold and bronze standard than gold and silver.

Second, we have investigated whether such differences in rele-vance assessments affect performance scores  X  again, they do. On a per topic basis, these differences definitely affect the scores on the inferred measures. Per topic variation is well known to occur in the Cranfield method, leading to the reporting of averaged mea-sures over a sufficient numbers of topics. In our investigation, when averaged over the 33 topics that were judged by all three categories of judges, these differences affect the measures in a small but con-sistent way. Bronze standard judges appear to be less discerning than gold standard judges leading to higher overall scores. Con-versely, system scores from the silver standard judge X  X  judgements were slightly worse than those from the gold standard judges.
Overall we conclude, like earlier investigators, that the Cranfield method of evaluation is somewhat robust to variations in relevance judgements. Having controlled for task and topic expertise, system performance measures show statistically significant, but not large, differences. Similarly, system orderings allow us to identify  X  X ood X  and  X  X ad X  IR systems at a broad-brush level.

However, we find that bronze standard judges may not be a reli-able substitute for the gold standard task and topic experts. Neither silver standard nor bronze standard judges were the topic origina-tors, yet silver standard judges performed closer to gold standard than did bronze standard judges. This suggests that disagreement stems not just from judging by non-originators. It is possible that unfamiliarity with task and topic context plays a major role. When evaluating relative system performance for task-specific IR, it can be important to obtain judgements from either the gold standard judges or a close surrogate to accurately reflect user pref-erences. The new sampling methods and inferred measures dis-cussed here allow this to be done at lower cost and effort than be-fore. We would like to thank the CSIRO science communicators, TREC Enterprise 2007 participants, Simon Barry, Lish Fejer, Donna Har-man, David Hawking, Stephen Robertson, Wouter Weerkamp, and other colleagues and reviewers who provided advice and assistance. [1] R. Artstein and M. Poesio. Inter-coder agreement for [2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method [3] P. Bailey, N. Craswell, I. Soboroff, and A. P. de Vries. The [4] C. Buckley and E. M. Voorhees. Retrieval evaluation with [5] R. Burgin. Variations in relevance judgments and the [6] J. Carletta. Assessing agreement on classification tasks: The [7] C. W. Cleverdon. The effect of variations in relevance [8] J. Cohen. A coefficient of agreement for nominal scales. [9] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient [10] B. D. Eugenio and M. Glass. The kappa statistic: a second [11] S. P. Harter. Variations in relevance assessments and the [12] K. J X rvelin and J. Kek X l X inen. IR evaluation methods for [13] K. S. Jones and K. van Rijsbergen. Information retrieval test [14] M. E. Lesk and G. Salton. Relevance assessments and [15] S. Mizzaro. Measuring the agreement among relevance [16] R. Rietveld and R. van Hout. Statistical Techniques for the [17] S. Sigel and N. J. Castellan. Nonparametric Statistics for the [18] I. Soboroff. A comparison of pooled and sampled relevance [19] E. Sormunen. Liberal relevance criteria of TREC: counting [20] A. Trotman and D. Jenkinson. IR Evaluation Using Multiple [21] A. Trotman, N. Pharo, and D. Jenkinson. Can we at least [22] E. M. Voorhees. Variations in relevance judgments and the [23] E. M. Voorhees and D. Harman. Overview of the Fifth Text [24] E. Yilmaz and J. A. Aslam. Estimating average precision [25] E. Yilmaz, E. Kanoulas, and J. Aslam. A simple and efficient
