 1. Introduction
Information retrieval as a core technology has been widely used for the WWW search services and digital libraries. In recent years, an increasing number of researchers have been working in this area and many different techniques have been investigated to improve the effectiveness of retrieval. Quite a large number of retrieval models have been proposed and experimented with test document collections. For example, in the book  X  X  X odern Information Retrieval X  written by Bae-za-Yates and Ribeiro-Neto (1999) , 11 different retrieval models were discussed. In addition, some other aspects such as user relevance feedback, document representation, query representation, query expansion, phrase recognition, thesaurus, context no single information retrieval system is able to deal with all these aspects at the same time, let along deal with them in the same way as some other systems. Therefore, many information retrieval systems developed are close in effectiveness but different from each other in implementation. In such a situation, data fusion, which uses a group of information retrieval systems to search the same document collection, and then merges the results from these different systems, is an attractive option to improve retrieval effectiveness. Recently, meta-search becomes an application of the data fusion technique. If the document collections across different web services are more or less the same, then the data fusion methods can be applied directly; if the document collections are quite different, then some variations of the data fusion methods can be used for obtaining more effective results (Wu &amp; McClean, 2007 ).

Quite afew datafusionmethods suchas CombSum(Fox,Koushik, Shaw,Modlin,&amp; Rao, 1993;Fox &amp; Shaw, 1994 ), CombMNZ correlation method ( Wu &amp; McClean, 2005, 2006a ), Markov chain-based methods (Dwork, Kumar, Naor, &amp; Sivakumar, 2001; ten, 2007 ) have been proposed, and extensive experiments using TREC data have been reported to evaluate these methods.
Experimental results show that, in general, data fusion is an effective technique for improvement of effectiveness, and very of-ten the fused results are better than the best component results involved.

The linear combination data fusion method is a very flexible method since different weights can be assigned to different systems. In some related experiments, (e.g., Aslam &amp; Montague, 2001; Thompson, 1993; Wu &amp; Crestani, 2002; Wu &amp;
McClean, 2006a ), a simple weighting schema was used: for a system, its weight is set as its average performance over a group of training queries. This weighting schema is straightforward and can be calculated or updated easily, therefore it is espe-cially suitable in a very dynamic environment. However, it has not been investigated how good this weighting schema is.
In some previous researches, different numerical optimisation methods such as golden section search ( Vogt &amp; Cottrell, 1998, 1999 ) and conjugate gradient (Bartell et al., 1994 ) were used to search suitable weights for component systems.
One major drawback of using these optimisation methods is their low efficiency. In some situations, such as the WWW, and digital libraries, where documents are updated frequently, each component system X  X  performance may vary consider-ably from time to time. The weights for the systems should be updated accordingly. In such a situation, it may not be feasible to use those very time-consuming weighting methods to update weights frequently.

In this paper, we investigate the weighting issue through extensive experiments. The key point is to try to find the rela-tion between performances of component systems and their corresponding weights which can lead to good fusion perfor-mance. As we shall see later in this paper, a power function weighting schema with a power of between 2 and 8, is more effective than the simple weighting schema for data fusion. On the other hand, those power function weights can be obtained as efficiently as simple weights. In fact, the simple weighting schema can be regarded as a special case of a power function weighting schema with a power of 1.

The rest of this paper is organized as follows: in Section 2 we review some related work on data fusion, especially on the linear combination method. Section 3 describes the linear combination method and the weighting issue. Section 4 discusses an experiment to evaluate different performance weighting schemas. Further observations from the experiment are dis-cussed in Section 5. Section 6 provides a few conclusive remarks. 2. Previous work on data fusion
Usually relevance-related scores are provided by information retrieval systems for all retrieved documents. Some algo-assigned to documents in component retrieval results. Others, such as Borda fusion (Aslam &amp; Montague, 2001 ), Markov tiple criteria approach ( Farah &amp; Vanderpooten, 2007 ) make use of the rank that each document occupies in each component result, as the scores are not always available.

Relevance-related scores obtained from different information retrieval systems may be diverse. Usually it is impossible to compare them directly and some kind of score normalization is required. Linear score normalization methods have been dis-cussed in Lee (1997), Montague andAslam (2001)and Wu et al. (2006) , and non-linear score normalizationmethods have been discussed in Manmatha, Rath, and Feng (2001) and Nottelmann and Fuhr (2003) . Since different retrieval systems use different ways to score documents, different score normalization methods may be required for different retrieval systems to obtain bet-ter effectiveness. However, the linear score normalization method has been widely used before in data fusion experiments.
CombSum, CombMNZ and some other methods were investigated by Fox et al. (1993) and Fox and Shaw (1994) . They found that CombSum and CombMNZ outperformed the others. CombSum sets the score of each document in the fused result to the sum of the scores obtained by the component result, while in CombMNZ the score of each document is obtained by multiplying this sum by the number of results which have non-zero scores.

Lee (1997) conducted an experiment with six submitted results to TREC 3. He found that CombMNZ was slightly better than CombSum in his experiment. However, later experiments, for example, in Montague and Aslam (2001), Lillis et al. (2006), Wu, Crestani, and Bi (2006) and Wu and McClean (2006b) and others, found that Lee X  X  conclusion is not always true and the probability that CombSum and CombMNZ are better than each other is roughly the same.
In the following we review a few papers which are very relevant to the linear combination data fusion method. Thompson (1993) used the linear combination method to fuse results submitted to TREC 1. He found that the combined results, weighted by performance level, performed no better than a combination using uniform weights (CombSum). This perfor-mance level weighting schema has been referred to as the simple weighting schema in this paper.

Bartell et al. (1994) used a numerical optimisation method, conjugate gradient, to search good weights of different sys-tems. Because the method is time-consuming, only 2 X 3 component systems and top 15 documents returned from each sys-tem for a given query were considered in their investigation.

Vogt and Cottrell (1998, 1999) analysed the performance of the linear combination method by linear regression. In their experiments, they used all possible pairs of 61 systems submitted to the TREC 5 ad-hoc track. An optimisation method, gold-en section search, was used to search good system weights. Due to the nature of the golden section search, only two com-ponent systems can be considered for fusion.

Human relevance judgments are needed to evaluate the performances of component systems in order to decide the appropriate weights for them. Without any human relevance judgment involved, several methods (e.g., Nuray &amp; Can, 2006; Soboroff, Nicholas, &amp; Cahan, 2001; Wu &amp; Crestani, 2003 ) could automatically estimate each component result X  X  per-formance, then the linear combination method can be used for data fusion. However, performance estimation from these methods are usually not very accurate.

Unlike other studies (e.g., Aslam &amp; Montague, 2001; Thompson, 1993; Wu &amp; Crestani, 2002) which only concern perfor-mance, both system performance and dissimilarities between results were considered in Wu and McClean (2005, 2006a) .In this weighting schema, any system X  X  weight is a compound weight which is the product of its performance weight and its dissimilarity weight. Here the performance weight is also defined as the average performance of the system over a group of training queries, while the dissimilarity weight is defined as the average dissimilarity between the system in question and all other systems over a group of training queries. More recently, the combination of performance weights and dissimilarity weights was justified using statistical principles (Wu, 2009 ).

In general, the linear combination method is more effective than CombSum and CombMNZ in all above experiments con-ducted before except (Bartell et al., 1994 ). However, how to decide system weights is still an open question. Numerical opti-misation methods can find effective solutions but too costly for many applications. This is especially the case when we do not have a good understanding of the problem and have to search in a very large area to find possible good solutions. On the other hand, the simple weighting schema can be implemented efficiently, but its effectiveness has not been investigated.
In this paper, we would like to investigate this issue through extensive experiments. Besides the simple weighting schema, we explore more options. In this study, we only consider performance, but not dissimilarity among component results for two reasons. Firstly, compared with performance, the effect of dissimilarity on data fusion is smaller (Wu &amp; McClean, 2006b ). Secondly, it has been investigated in Wu and McClean (2005, 2006a) . Anyway, it should be true that considering re-sults dissimilarity can bring further (though small) improvement. 3. The linear combination method and weights assignment
Suppose we have n information retrieval systems ir 1 ; ir
Each r i includes a ranked list of documents. The documents are ranked according to their scores. If we have no knowledge of the systems, then an equal weight for every component system is a reasonable policy. If we have some knowledge of those retrieval systems (e.g., evaluating performance by using some training queries), then we are able to take a more profitable policy: good systems are assigned heavy weights and poor systems are assigned light weights. Thus the performance of the fused results can be improved by the linear combination method, which uses the following equation to calculate score:
Here s i  X  d ; q  X  1 is the normalized score of document d in result r score of d for q . All the documents can be ranked according to their calculated scores.

For each system ir i , suppose its average performance (e.g., measured by mean average precision (MAP) or recall-level pre-cision (RP)) over a group of training queries is p i , then p was used in previous research in data fusion (e.g., Aslam &amp; Montague, 2001; Thompson, 1993; Wu &amp; Crestani, 2002; Wu &amp;
McClean, 2006a ). In most cases (except in Thompson, 1993 ) the linear combination method with the simple weighting sche-ma outperformed both CombSum and CombMNZ. However, except the simple weighting schema, no other weighting sche-mas have been investigated.

Here we assume that the weight that each result possesses is only determined by its performance, or w
Note that the performance of a retrieval system is always in the range of [0,1] when using MAP or RP or other commonly used measures. For the above function, the multiplication function is useless, since both weights are enlarged or dwindled by the same times. We find that using power functions of performance p power values are used, we can associate weight with performance in many different ways.
Let us take an example to illustrate this. Suppose we have two systems ir respectively. If a power of 1 is used, then the weights w following table list the normalized weights of them when a power of 0 X 5 is used. All the scores have been normalized using the equation w i  X  w i w power 0 1 2 3 4 5 w 1 0.50 0.43 0.36 0.30 0.24 0.19 w 2 0.50 0.57 0.64 0.70 0.76 0.81
When a negative power is used, then the system in good performance is assigned a light weight, while the system in poor performance is assigned a heavy weight. In such a case, we cannot obtain very effective results by fusing them using the lin-ear combination method. Therefore, negative powers are not appropriate for our purpose and we only consider non-negative powers later in this paper.

When a power of greater than 0 is used, then the system in good performance is assigned a heavy weight, while the sys-tem in poor performance is assigned a light weight. Two special cases are 0 and 1. When 0 is used, then the same weight is assigned to all systems. Thus the linear combination method becomes CombSum. When 1 is used, then we obtain the simple weighting schema. If we use a very large power (much greater than 1) for the weighting schema, then the good system has a larger impact, and the poor system has a smaller impact on the fused result. If a large enough power is used, then the good system will be assigned a weight very close to 1, and the poor system will be assigned a weight very close to 0. Thus the fusion process will be dominated by the good component system and the fused result will be very much like the good com-words, we can obtain some good weights to make the fused results better than the best component result.

It is straightforward to expand the situation in which more than two component systems are included. 4. Empirical investigation of appropriate weights
The purpose of our empirical investigation is to evaluate the simple weighting schema and to try to find more effective schemas if possible. Four groups of TREC data were used for the experiment. Their information is summarized in Table 1 (see
Appendix for all the results involved and their performances measured by MAP). From Table 1 we can see that these four groups of submitted results (called runs in TREC) are different in many ways from track (Web, Robust, and Terabyte), the number of results submitted (97, 78, 110, and 58) and selected (32, 62, 77, and 41) in this experiment, ries used (50, 100, and 249), to the number of retrieved documents for each query in each submitted result (1000 and 10,000). They comprise a good combination for us to evaluate data fusion methods.

The zero-one linear normalization method was used for score normalization. It uses the following equation: normalized score that d should obtain. No matter what the initial range is, the normalized scores of any result are always in the range of [0,1].

For all the systems involved, we evaluated their average performance measured by mean average precision (MAP) over a group of queries. Then different values (0.5,1.0,1.5,2.0, ... ) were used as the power in the power function to calculate weights for the linear combination method. In a year group, we chose m ( m = 3, 4, 5, 6, 7, 8, 9, or 10) component results from all available results for fusion. For each setting of m , we randomly chose m component results from all available results for fusion. This process was repeated 200 times. Four metrics were used to evaluate the fused retrieval results. They are mean average precision (MAP), recall-level precision (RP), percentage of the fused results whose performance on MAP is better than the best component result (PMAP), and percentage of the fused results whose performance on RP is better than the best component result (PRP). Besides the linear combination method with different weighting schemas, CombSum and Comb-MNZ were also involved in the experiment.

Tables 2 X 5 show the experimental result, in which four different powers (0.5, 1.0, 1.5, and 2.0) are used for the linear combination method. Each data point in the tables is the average of 8 200 q num measured values. Here 8 is the different number (3,4, ... ,9,10) of component results used, 200 is the number of runs for a certain number of component results, and q num is the number of queries in each year group (see Table 1 ).
 All the data fusion methods involved have similar behaviours, though these methods are quite different in performance.
For all of them, TREC 2001 is the most successful group, TREC 2004 is the second most successful group, TRECs 2003 and 2005 are the least successful groups.

Among all the data fusion methods, CombMNZ has the worst performance. CombMNZ does well in two year groups TRECs 2001 and 2004, but poorly in two other year groups TRECs 2003 and 2005. In fact, CombMNZ beats the best component re-sult in TREC 2001 (+9.04% in MAP, +5.54% in RP, 79.44% in PMAP, and 73.37% in PRP) and TREC 2004 (+3.46% in MAP, +2.74% in RP, 81.69% in PMAP, and 80.81% in PRP), but worse than the best result in TREC 2003 ( 2.41% in MAP, 1.14% in RP, 28.16% in PMAP, and 49.5% in PRP) and TREC 2005 ( 4.79% in MAP, 4.51% in RP, 29.62% in PMAP, and 30.56% in PRP).
It is worthwhile to compare CombMNZ with CombSum. CombSum is better than CombMNZ in all four year groups. How-ever, just like CombMNZ, CombSum does not perform as well as the best component result in TRECs 2003 and 2005; while it performs better than the best component result in TRECs 2001 and 2004.
 With any of the four powers chosen, the linear combination method performs better than the best component result,
CombSum, and CombMNZ in all four year groups. However, the improvement rates of the linear combination method are different from 1 year group to another. Comparing with all different weighting schemas used, we can find that the larger the power is used for weighting calculation, the better the linear combination method performs. The differences are espe-cially bigger for PMAP and PRP. Considering the average of all year groups, the percentage is over 76 for both PMAP and
PRP when a power of 0.5 is used. When the power reaches 2.0, the percentage is above 86 for both PMAP and PRP. This dem-onstrates that the linear combination method is more reliable than CombSum (69.05% for PMAP, 68.77% for PRP) and Comb-MNZ (54.73% for PMAP, 58.56% for PRP), when a power of 2 is used for weight calculation.

From the above experimental results we can see that the linear combination method increases in performance with the power used for weight calculation. Since only four different values (0.5,1,1.5,2) have been tested, it is interesting to find how far this trend continues. Therefore, we use more values (3,4,5, ... ,20) as power for the linear combination method with the same setting as before. The experimental result is shown in Figs. 1 X 4 .

In Figs. 1 and 2 , the curves of TREC 2004 reach their maximum when a power of 4 or 5 is used. While for the three other groups, the curves are quite flat and they reach their maximum when a power of between 7 and 10 is used. It seems that, for obtaining the optimum fusion results, different powers may be needed for different sets of component results. This may seem a little strange, but one explanation for this is: data fusion is affected by many factors such as the number of compo-nent results involved, performances and performance differences of component results, dissimilarity among component re-just by any single factor, though performances of component results is probably the most important one among all the fac-tors. Anyway, if we only consider performance, then a power of 1, as the simple weighting schema does, is far from the optimum.

Two-tailed T tests were carried out to compare the performance of the data fusion methods involved. the differences between CombSum and the linear combination method (with any power 1 X 20) are always statistically sig-as good as CombSum when a power of 12 or more is used for MAP (or a power of 11 or more for RP). In all other cases, the linear combination method is better than CombSum. We also compared the linear combination method pairs with adjacent integer powers such as 1 and 2, 2 and 3, etc. In every year group, most of the pairs are different at a level of 0.000 with a few exceptions. In TREC 2001, when comparing the pair with powers 4 and 5, the significance value is 0.745 (MAP). In TREC 2003, the significance value is 0.024 (MAP) for the pair with powers 10 and 11. In TREC 2004, the significance value is 0.037 (RP) for the pair with powers 6 and 7; the significance values are 0.690 (MAP ) and 0.037 (RP) for the pair with powers 7 and 8. In
TREC 2005, the significance value is 0.072 (PP) for the pair with powers 7 and 8; the significance value is 0.035 (RP) for the pair with powers 8 and 9; the significance value is 0.002 (MAP) for the pair with powers 9 and 10. All such exceptions happen when the linear combination method is at the peak of performance.

In Figs. 3 and 4, PMAP and PRP increase very rapidly with the power at the beginning. Both of them reach their maximum almost at the same point as corresponding MAP and RP curves. After that, all the curves decrease gently with power. In two year groups TRECs 2001 and 2004, both PMAP and PRP are around 90 when a power of 1 is used; while in two other year groups TRECs 2003 and 2005, both PMAP and PRP are about 70% when a power of 1 is used.

If we consider all the metrics and all year groups, then the optimum points are not the same. However, we can observe that the performance always increases when the power increases from 1 to 4 for all years groups and all metrics. This sug-gests that using 2 or 3 or 4 as the power is very likely a better option than using 1, as the simple weighting schema does.
Compared with the simple weighting schema, an improvement rate of 1 X 2% is achieved on MAP and RP, and an improvement rate of 10 X 15% is achieved on PMAP and PRP by using a power of 4. TREC 2001 reaches its peak when a power of 4 is used.
However, the three other groups continue to increase for some time. For example, if we use a power of 7 or 8, and compare it with the simple weighting schema, then an improvement rate of 2.5% on MAP and RP, and an improvement rate of 15 X 20% on PMAP and PRP are achievable. 5. Further observations
In this section we further discuss some related issues about the linear combination data fusion method. The same data as in Section 3 will be used. First let us see the effect of the number of component results on the fused results. For each group of component results chosen randomly, we fused them using the linear combination method 20 times, each with a different weight (a power of 1,2, ... ,20 was used). Then we chose the best one from all 20 fusion results generated for consideration.
The experimental result is presented in Fig. 5 . As in Section 3, each data point is the average of 8 200 q num measured val-ues, where 8 is the number of different groups (including 3,4, ... ,10 component results), 200 is the number of runs with a specific number of component results, and q num is the number of queries in each year group. In this section, we only pres-ent results measured by MAP. Similar results were observed using RP.

We can see that all four curves almost increase linearly with the number of component results, demonstrating that the number of component results has a positive effect on the performance of the fused result using the linear combination method.

Next let us take a look at the relationship between the average performance of component results and the improvement rate of performance that the fused result can obtain over the average performance. For a group of component results, we chose the best fusion result as above. Then we divided the average performance on MAP into equal intervals 0 X 0.0999,0.1000 X 0.1999, ... , put all the runs into appropriate intervals, and calculated the improvement rate of performance of the fused results for each interval.

Fig. 6 shows the experimental result. Each data point is the average of all the runs in an interval and  X  X 0.1 X  on the hori-zontal axis denotes the interval of 0.1000 X 0.1999 and so on. In all year groups, the improvement rate of performance of the fused results decrease rapidly with the average performance of component results, with a few exceptions. In fact, in every year group, all the runs are roughly normally distributed (see Fig. 7 ). All the exceptions occur in those data points that in-clude very few runs. This demonstrates that the better the component results are in performance, the less benefit we can obtain from data fusion.

When using the linear combination method to fuse results, the contribution of a component result to the fused result is determined by the weight assigned to it. It is interesting to observe the contribution that each component result to the fused result when different weighting schemas are used. In order to achieve this, we carry out the following procedure: first, in a year group, a given number of component results are chosen and fused using different power weighting schemas; second, the best and the worst component results are identified; third, for each weighting schema, we calculate the Euclidean dis-tance between the worst and the best component results, between the worst component results and the fused results, and between the best component results and the fused results after normalising them using the zero-one normalization method. Using the same data as in Section 3, we carried out the experiment and hereby present the result of TREC 2001 in Fig. 8 .
The results for the three other year groups are analogous and the figures are omitted here. In Fig. 8 , each data point is the average of 8 200 50 fusion runs. Since the Euclidean distance between the worst and best component results is not affected by data fusion at all, it is a flat line in Fig. 8 . As expected, when the power increases, the distance between the worst com-ponent results and the fused results increases, while the distance between the best component results and the fused results decreases. However, quite surprisingly, even when the power is as high as 20, the distance between the best component re-sults and the fused results is about 1.5 (compared with about 6, the distance between the worst component results and the fused results), which is not close to 0 by any means. This demonstrates that even when a power as high as 20 is used, the fused results are not just affected by the best component results. Other component results (apart from the best ones) still have significant impact on the fused results.

Next we investigate how the power weighting schema can be used with optimisation methods to search suitable weights more effectively and efficiently. For a set of component systems over a group of queries, we would like to find better weights for all of them. It is the case that for every component system, the weights it should take to obtain optimum fusion result are different across queries. However, we would like to find a single weight for each system considering all the training queries as a whole. Such a weight obtained from training queries can be used directly in practice.

Since the optimisation methods are time-consuming, we experimented with 200 fusion runs, each of them including three component results in TREC 2001. As in (Bartell et al., 1994 ), we used the conjugate gradient method. The conjugate gradient method is an algorithm for finding the nearest local maximum of a function of n variables which presupposes that the gradient of the function can be computed. For each component result involved, we evaluated its average performance (MAP) over all 50 queries. Suppose that the three component results are r p ; p 2 , and p 3 , respectively. Then we used p i ( i = 1, 2, and 3) as r that there are multiple local maxima in the search space, since different starting weights usually lead to different maxima (local maxima). The conjugate gradient method, like many other optimisation methods, is always able to find a local max-imum, but not always a global maximum. Table 6 shows the experimental result.

We should note that the starting point can affect both effectiveness and efficiency of optimisation methods. Firstly, if the starting point chosen is very close to a local or global maximum, then it takes only a few steps for the optimisation method to approach it. Otherwise, it may take much longer time. Thus the efficiency of the optimisation method can be affected. Sec-local or global maximum will be found. Therefore, the effectiveness of the optimisation method can be affected by the start-ing point. In Table 6 , p 3 leads to the best results among all the options. Therefore, a proper power weighting schema can be used as a good starting point for the optimisation method to search more favourable weights. Compared the figures in col-umn 3 with the figures in column 2, some of them, with a  X  X   X  mark, are statistically significant ( p -value
The time required for each run of the conjugate gradient method is given in Table 6 as well. We used a reasonably good performance PC (Intel Dual_core CPUs and 1 GM of RAM) for this. For each group of three component results over 50 queries, the time required varies from 16 s to 1.5 min. However, in the case of 16 s for p 5 , no improvement has been made. Since the optimisation method cannot find any better points in the neighbourhood and stops the search process after only one or two steps. The more time the search process takes, the more improvement on fusion we can obtain (for the searched point to be compared with the start point). More time is needed if more component results or more queries are involved. Especially when more component results are involved, the time complexity of the search process increases very rapidly. On the other hand, it takes less than 2 s to calculate the weights of three component results using the power function schema. Most of the time required is to calculate the performances of all component results. When more results or more queries are used, the time required increases linearly.

One phenomenon in TREC may be worth investigating in this study. In each year group, any participant may submit more than one run to the same track. Those runs were more similar than usual since many of them were just obtained by using the same information retrieval system but different parameter settings/different query formats. In our previous experiments, We do not distinguish runs from the same participant or not. In each combination, a few component results may come from the same participants, then the fused results may be biased to them. In order to avoiding such things from happening, we divide submitted runs into groups and all the runs submitted by the same participant are put in the same group. We randomly se-lect runs from all the groups as before. But for any single combination, we choose at most one run from any particular group.
We refer this to be the non-same-participant restriction later in this paper. We used TRECs 2001 and 2003 in this experi-ment. As before, in a year group, we chose m ( m = 3, 4, 5, 6, 7, 8, 9, or 10) component results from all available results for fusion with the non-same-participant restriction. For each setting of m , we formed and tested 200 combinations. Fig. 9 shows the experimental result.

In Fig. 9 ,  X  X 2001 revised X  and  X  X 2003 revised X  denote the groups that observe the non-same-participant restriction, while  X  X 2001 original X  and  X  X 2003 original X  denote the groups which do not observe the non-same-participant restriction. curves of  X  X 2001 original X  and  X  X 2001 revised X  are not always very close, this is because different component results were in-volved in two corresponding groups, though it is the same case for  X  X 2003 original X  and  X  X 2003 revised X . We can see that the two corresponding curves for comparison are the same in shape and they reach the maximum points with more or less the same power values. Therefore, it demonstrates that the even dissimilarity among component results does not affect our con-clusion before.

In all above experiments, the same group of queries was used for obtaining performance weights and evaluation of data fusion methods. Now let us investigate a more practical scenario: different groups of queries were used for obtaining per-formance weights and evaluation of data fusion methods. In order to do this, we divide all the queries in every year group into two sub-groups: odd-numbered queries and even-numbered queries. We obtain two different performance weights from these two sub-groups. Then for those odd-numbered queries, we evaluate all data fusion methods using the weights obtained from even-numbered queries; and for those even-numbered queries, we evaluate all data fusion methods using the weights obtained from odd-numbered queries. Three year groups (TREC 2001, 2003, and 2004) were used in this exper-iment. All the combinations involved are the same as that in the experiment in Section 4. Thus we can carry out a fair com-parison of these two different settings.

Fig. 10 shows the result.  X  X 2001_diff X ,  X  X 2003_diff X , and  X  X 2004_diff X  are from the setting that uses different group of queries for performance weights training and fusion methods evaluation; while  X  X 2001_same X ,  X  X 2003_same X , and  X  X 2004_same X  are from the setting that uses the same group of queries for performance weights training and fusion methods evaluation.
From Fig. 10 , we can see that using the same group of queries can always lead to better fusion performance than using different groups of queries for performance weights training and data fusion methods evaluation. The difference between the two corresponding curves becomes larger when a larger power value is used for weight assignment. How-ever, the two corresponding curves in each year group bear the same shape. Therefore, our observations and conclusions obtained before by using the same group of queries for performance weights training and data fusion methods evaluation are also held for the situation that different queries are used for performance weights training and data fusion methods evaluation.
 In Fig. 10 , we can also find that, in each year group, the difference between the two curves are different. Among them,
TREC 2001 has the biggest difference, TREC 2004 has the smallest difference, while TREC 2003 is in the middle. This is due to the different accuracies of performance estimation for these three groups of component results. Table 7 shows the correlation of performances between the two sub-groups of queries in each year group. Among these three groups, the cor-relation between the two sub-groups in TREC 2004 is the strongest, which is followed by the two sub-groups in TREC 2003, and the correlation between the two sub-groups in TREC 2001 is the weakest. Further, we believe this is mainly because of the different numbers of queries used in each year group. In TREC 2001, there are only 50 queries, and each sub-group in-cludes 25 queries; in TREC 2003, there are 100 queries, and each sub-group includes 50 queries; and in TREC 2004, there are 249 queries, and each sub-group includes 125 or 124 queries. This experiment also suggests that a relatively large number of queries (at least 50) should be used for an accurate estimation of performance. 6. Conclusive remarks
In this paper we have presented our work about how to assign appropriate weights for the linear combination data fusion method. From the extensive experiments conducted with the TREC data, we conclude that a series of power functions (pow-ers between 2 and 8) are better than the simple weighting schema, in which a system is assigned a weight equal to its aver-age performance. The power function schema can be implemented as efficiently as the simple weighting schema.
We have also investigated some related issues about the linear combination method. First, we have found that the num-ber of component results has a positive effect on the performance of the fused result, while the average performance of com-ponent results has a negative effect on the performance of the fused result. Second, we have investigated the similarity between the fused results and component results when different powers are used in the linear combination method. A very informative observation is: even when a power as big as 20 is used, the difference between the fused results and the best component results is still very significant. In other words, other results than the best component results still have very sig-nificant impact on the fused results. Finally, we have demonstrated that the observation from this study is also very useful for the optimisation methods to achieve more effective weights more efficiently. Since the whole space is too large, any opti-misation method would only be possible to search in a very small area to find a local maximum. The observations from this study can tell the optimisation methods where is the right area for a search. In summary, we believe that our empirical inves-tigations and observations presented in this paper are very useful for us and other researchers to have a better understanding of the linear combination data fusion method and the data fusion technique in general.
 Acknowledgements
The authors thank the anonymous reviewers for their helpful suggestions and comments which have been taken to im-prove the quality and presentation of this paper. Xiaoqin Zeng and Lixin Han X  X  work is partially supported by the National Natural Science Foundation of China under grant numbers 60673186 and 60571048.
 Appendix In this appendix, all the systems used in the experiment are listed with their average performances (on MAP).
TREC 2001 (32 in total) apl10wc(0.1567), apl10wd(0.2035), flabxt(0.1719), flabxtd(0.2332), flabxtdn(0.1843), flabxtl(0.1705), fub01be2(0.2225), fub01idf(0.1900), fub01ne(0.1790), fub01ne2(0.1962), hum01tdlx(0.2201), iit01tde(0.1791), jscbtawtl1(0.1890), jscbtawtl2(0.1954), jscbtawtl3(0.2003), jscbtawtl4(0.2060), kuadhoc2001(0.2088), Merxtd(0.1729), msrcn2(0.1863), msrcn3(0.1779), msrcn4(0.1878), ok10wtnd0(0.2512), ok10wtnd1(0.2831), pir1Wa(0.1715), posnir01ptd(0.1877), ri-cAP(0.2077), ricMM(0.2096), ricMS(0.2068), ricST(0.1933), uncfslm(0.0780), uncvsmm(0.1269), UniNEn7d(0.2242).
TREC 2003 (62 in total) aplrob03a(0.2998), aplrob03b(0.2522), aplrob03c(0.2521), aplrob03d(0.2726), aplrob03e(0.2535), fub03IeOLKe3(0.2503), fub03InB2e3(0.2435), fub03IneOBu3(0.2329), fub03IneOLe3(0.2479), fub03InOLe3(0.2519), humR03d(0.2367), humR03de(0.2627), InexpC2(0.2249), InexpC2QE(0.2384), MU03rob01(0.1926), MU03rob02(0.2187), MU03rob04(0.2147),
MU03rob05(0.2029), oce03noXbm(0.2292), oce03noXbmD(0.1986), oce03noXpr(0.1853), oce03Xbm(0.2446), oce03Xpr(0.1836), pircRBa1(0.3100),pircRBa2(0.3111), pircRBd1(0.2774), pircRBd2(0.2900), pircRBd3(0.2816), rut-cor030(0.0482), rutcor03100(0.0582), rutcor0325(0.0590), rutcor0350(0.0683), rutcor0375(0.0767), SABIR03BASE(0.2021), SABIR03BF(0.2263), SABIR03MERGE(0.2254), Sel50(0.2190), Sel50QE(0.2387), Sel78QE(0.2432), THUIRr0301(0.2597), THU-IRr0302(0.2666), THUIRr0303(0.2571), THUIRr0305(0.2434), UAmsT03R(0.2324), UAmsT03RDesc(0.2065), UAmsT03RFb(0.2452), UAmsT03RSt(0.2450), UAmsT03RStFb(0.2373), UIUC03Rd1(0.2424), UIUC03Rd2(0.2408),
UIUC03Rd3(0.2502), UIUC03Rt1(0.2052), UIUC03Rtd1(0.2660), uwmtCR0(0.2763), uwmtCR1(0.2344), uwmtCR2(0.2692), uwmtCR4(0.2737), VTcdhgp1(0.2649), VTcdhgp3(0.2637), VTDokrcgp5(0.2563), VTgpdhgp2(0.2731), VTgpdhgp4(0.2696).
TREC 2004 (77 in total) apl04rsTDNfw(0.3172), apl04rsTDNw5(0.2828), fub04De(0.3062), fub04Dg(0.3088), fub04Dge(0.3237), fub04T2ge(0.2954), fub04TDNe(0.3391), fub04TDNg(0.3262), fub04TDNge(0.3405), fub04Te(0.2968), fub04Tg(0.2987), fub04Tge(0.3089), humR04d4e5(0.2756), humR04t5e1(0.2768), icl04pos2d(0.1746), icl04pos2f(0.2160), icl04-pos2td(0.1888), icl04pos48f(0.1825), icl04pos7f(0.2059), icl04pos7td(0.1783), JuruDes(0.2678), JuruDesAggr(0.2628), JuruDesLaMd(0.2686), JuruDesQE(0.2719), JuruDesSwQE(0.2714), JuruDesTrSl(0.2658), JuruTitDes(0.2803), NLPR04clus10(0.3059), NLPR04clus9(0.2915), NLPR04COMB(0.2819), NLPR04LcA(0.2829), NLPR04LMts(0.2438), NLPR04NcA(0.2829), NLPR04okall(0.2778), NLPR04OKapi(0.2617), NLPR04okdiv(0.2729), NLPR04oktwo(0.2808),
NLPR04SemLM(0.2761), pircRB04d2(0.3134), pircRB04d3(0.3319), pircRB04d4(0.3338), pircRB04d5(0.3319), pir-cRB04t2(0.2984), pircRB04t3(0.3331), pircRB04t4(0.3304), pircRB04td2(0.3586), pircRB04td3(0.3575), polyudp2(0.1948), polyudp4(0.1945), polyudp5(0.2455), polyudp6(0.2383), SABIR04BA(0.2944), SABIR04BD(0.2627), SABIR04BT(0.2533),
SABIR04FA(0.2840), SABIR04FD(0.2609), SABIR04FT(0.2508), uogRobDBase(0.2959), uogRobDWR10(0.3033), uo-gRobDWR5(0.3021), uogRobLBase(0.3056), uogRobLT(0.3128), uogRobLWR10(0.3201), uogRobLWR5(0.3161), uogRobS-
Base(0.2955), uogRobSWR10(0.3011), uogRobSWR5(0.2982), vtumdesc(0.2945), vtumlong252(0.3245), vtumlong254(0.3252), vtumlong344(0.3275), vtumlong348(0.3275), vtumlong432(0.3275), vtumlong436(0.3280), vtumti-tle(0.2822), wdo25qla1(0.2458), wdoqla1(0.2914).
 TREC 2005 (41 in total)
DCU05ABM25(0.2887), DCU05ACOMBO(0.2916), DCU05ADID(0.2886), DCU05AWTF(0.3021), humT05l(0.3154), humT05x5l(0.3322), humT05xl(0.3360), humT05xle(0.3655), indri05Adm(0.3505), indri05AdmfL(0.4041), indri05AdmfS(0.3886), indri05Aql(0.3252), JuruDF0(0.2843), JuruDF1(0.2855), juruFeRa(0.2752), juruFeSe(0.2692), MU05T-Ba1(0.3199), MU05TBa2(0.3218), MU05TBa3(0.3063), MU05TBa4(0.3092), NTUAH1(0.3023), NTUAH2(0.3233), NTU-
AH3(0.2425), NTUAH4(0.2364), QUT05DBEn(0.1645), QUT05TBEn(0.1894), QUT05TBMRel(0.1837), QUT05TSynEn(0.0881), sab05tball(0.2087), sab05tbas(0.2088), UAmsT05aTeLM(0.1685), UAmsT05aTeVS(0.1996), uogTB05LQEV(0.3650), uogTB05SQE(0.3755), uogTB05SQEH(0.3548), uogTB05SQES(0.3687), uwmtEwtaD00t(0.3173), uwmtEwtaD02t(0.2173), uwmtEwtaPt(0.3451), uwmtEwtaPtdn(0.3480), york05tAa1(0.1565).
 References
