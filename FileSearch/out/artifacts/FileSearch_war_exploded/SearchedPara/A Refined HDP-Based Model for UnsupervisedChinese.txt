 Chinese word segmentation is a very important component for almost all natu-ral language processing tasks. Although supervised segmentation systems have been widely used, they rely on manually segmented corpora, which are often specific to domain and various kinds of segmentation guidelines. As a result, su-pervised segmentation systems perform poorly on out-of-domain corpus such as Microblog corpus [2] which contains lots of new words and domain specific words. In order to tackle this problem, unsupervised word segmentation becomes a very important issue. Various kinds of models have been proposed for unsupervised word segmentation task. [3] compared serveral popular models for unsupervised word segmentaion with a unified framework. [4] presented a model based on the Variation of Branching Entropy. [5] proposed a iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids thresholds. refines the HDP-Based model [6]. This model gives a better estimation of the base measure in HDP by using a dictionary-based model. We also show that the initial segmentation state for HDP model plays a very important role in model performance. A better initial segmentation can lead to a better performance. We test our system on the PKU and MSRA benchmark datasets provided by Second Segmentation Bake-off (SIGHAN 2005) [1] and our method performed better than the state-of-the-art systems.
 overview of the HDP-based unsupervised word segmentation model. In section 3, we describe our models in detail. Section 4 shows our experiment results on the benchmark dataset. We then conclude the paper with section 5. The Dirichlet Process (DP) is a stochastic process used in Bayesian non-parametric models of data. Let H be a distribution called base measure. The DP is a prob-ability distribution, i.e. each draw from a DP is itself a distribution over distri-butions where H is basically the mean of the DP and  X  can be understood as an in-verse variance. We can see that Dirichlet Process can be viewed as an infinite dimensional generalization of Dirichlet distributions.
 nonparametric Bayesian approach to clustering grouped data. It uses a Dirichlet process for each group of data, with the Dirichlet processes for all groups sharing a base distribution which is itself drawn from a Dirichlet process. The process defines a set of random probability measure G j , one for each group, and a global random probability measure G 0 . The global measure G 0 is distributed as a DP with concentration parameter  X  and base measure H and the random measure G j are given by a DP with concentration parameter  X  1 and base measure G 0 HDP. They define a bigram model by assuming each word has a different distri-bution over the words that follow it, but all these distributions are linked: word l . G l is linked to other DPs by sharing a common base distribution G 0 . The generating process can be represented according to the Chinese Restaurant Franchise (CRF) [7] metaphor. The metaphor is as follows. We have a restaurant franchise with a shared menu G 0 and each restaurant has infinitely many tables. When the n + 1th customer enter the restaurant l , the customer either joins an already occupied table k with probability proportional to the number n lk of customers already sitting there and share the dish, or sits at a new table with probability proportional to  X  1 and order a dish from menu G 0 . Choosing dish from menu G 0 is a similar process, we can either choose an already ordered dish j with probability proportional to the number n j of dishes already been ordered by all restaurants, or choose a new dish from H with probability proportional to  X  . In this bigram model, each w i  X  1 corresponds to a restaurant and each w i is a dish. In practice, we can not observe G l and G 0 directly because it will be infinite dimensional distribution over possible words. However, we can integrate observed segmentation result: observed segment h and P ( w i | h ) is defined as: Here t w i is the total number of tables labeled with w i , t is the total number of tables and H ( w i ) is the prior knowledge of the probability of word w i . word segmentation by repeatly sampling the value of each possible word bound-ary location, conditioned on the current values of all other boundary loca-ther a word boundary or not. For example, let current segmentation result be  X c (Each c corresponds to a Chinese character). If the current sampling location i is a word boundary, the segmentation result would become  X w 1 w 2  X  where w main the same. Let h 1 the first hypotheses and h 2 be the second. The posterior possibility for Gibbs sampling would be: Here h  X  is the current values of all other boundary locations without current position i and w l ( w r ) is the first word to the left (right) of the current word w . After the Gibbs sampling converged, the segmentation result can be obtained according to the word boundary results. In this section, we present our model in detail and show how HDP-based model can be refined to impove the segmentation performance. 3.1 Improved Base Measure As we can see in equation (1) and (2), the effect of posterior possibility of HDP is in fact a kind of smoothing. The bigram probability is smoothed by backing off to the unigram model and the unigram is smoothed by the base measure H , namely the prior probabilities over words. If the lexicon is finite, we can use a lexicon will be countbaly infinite. So building an accurate H is very important for word segmentation. [6] used a unigram character-based language model. [9] used a uniform distribution over characters dependent on word length with a Poission distribution.
 tuition behind of our method is that given a large segmented corpus, a better estimation of the probability of a word can be obtained by using maximum like-lihood estimation which is much more accurate than a simple character-based unigram model. However, in an unsupervised word segmentation task, we do not have a segmented corpus for probability estimation. To get the segmented corpus in an unsupervised way, we can use other unsupervised word segmentation sys-tem to segment the corpus. Although this could be inaccurate, substrings that are recognized as words would tend to have a high probability in the segmented corpus. To obtain a better estimation, we use different unsupervised word seg-mentation models to segment the corpus and merge the results together. Because different models give a different view of what a word is. The substring which is a real word tends to be recognized by all the models thus having a high proba-bility. On the other hand, substring that is not a word tends to appear in none of the models.
 systems to segment the training corpus. Then the words whose frequency is bigger than a threshold are selected from all the segmentation results and we merge the results to form a dictionary, that is, the frequency of the same word from different results are added up. Given the dictionary of words with their frequency, the base measure H is defined as follows: Here, P ml ( w i ) is the maximum likelihood estimation of the word probability from the dictionary and P smooth ( w i ) is the base measure defined by [6]. As defined in the probability of the j th character c ij of word w i , which can be obtained using maximum likelihood estimation from training data. P ml ( w i ) and P smooth ( w i ) are interpolated by parameter  X  to make a trade-off between the two kinds of probability. As we can see in section 4, this better estimation of base measure H helps improve the model performance. 3.2 Initial State As we present in section 2, the HDP model iteratively samples the value each possible word boundary location using Gibbs sampling. The procedure of sam-pling can be viewed as a random search in space of possible segmentation states. [6] use a random segmentation as the initial segmentation state and show that However, we believe that a better initial state can lead to a better result and help converge much faster. In our method, we use a state-of-the-art unsupervised word segmentation system to segment the data first and use the segmentation result as a initial segmentation for the HDP model. As we can see in section 4, by using a better initial state, our method obtained a much better result than both the state-of-the-art system and the HDP model with random initial state. In this section we test our model on PKU and MSRA datasets released by the Second Segmentation Bake-off (SIGHAN 2005) [1] and make a comparision with previous work. 4.1 Prior Knowledge Used Concerning unsupervised Chinese segmentation, a problem needs to be clarified is to what extent prior knowledge could be injected into the model. To be an as strict unsupervised model as possible, no prior knowledge such as word length, punctuation information, encoding scheme could be used. However, information like punctuation can be easily used to improve the performance. The problem is that we could not know what kind of prior knowledge other models used. For example, one might use manually designed regular expression to deal with numbers and dates, but does not list the regular expressions in paper. This makes it difficult to re-implement other models and make a fair comparision. To compare our model with previous models under the same condition, only puncuation information is used in our experiments. Punctuation information can improve the performance, since such information usually unambiguously marks boundary of words. It is very reasonable to use them in unsupervised Chinese segmentation model. 4.2 Model Selection We randomly selected 2000 sentences from the training data as our development set for parameter tuning. We set  X  1 =100,  X  =10,  X  =0.8 , p s =0.5. We used two unsupervised word segmentation model to form the dictionary and give a initial segmentation result as described in section 3. The first model we use is nVBE [4]. It follows Harris X  X  hypothesis in Kempe [10] and Tanaka-Ishii X  X  [11] reformulation and base their work on the Variation of Branching Entropy. They improve on [12] by adding normalization and viterbi decoding. This model achieves state-of-the-art results on the Second Segmentation Bake-off (SIGHAN 2005) datasets. The second model we use is based on mutual information. Using mutual information is motivated by the observation of previous work by Hank and Church [13]. If threshold, we prefer to identify AB as a word over those having lower MI values. We computed the mutual information on the training data. During the segmen-tation, we separate two adjacent characters to form a word boundary if their MI value is lower than a threshold. The threshold is set to 2.5 in our experiment. Although this model is not the state-of-the-art model, it is easy to implement and do give a different view of what a word is compared with nVBE. We put the training and test data together for segmenting. The word frequency threshold is set to 10 and two segmentations are merged to form the final dictionary. 4.3 Experiment Result We test our model on the PKU and MSRA datasets released by the Second Segmentation Bake-off (SIGHAN 2005) [1]. We re-implement the nVBE model and the MI model and build our model based on these implementations. All the training data and test data are merged together for segmentation and only the test data are used for evaluation. The overall F-scores of different models are given in Table 1.
 measure, the HDP model (HDP + dict) achieves a better result although only by a small margin. By using the segmentation result of nVBE as the initial seg-mentation, the HDP model (HDP+nVBE) gets a much better result than both the original HDP model and the nVBE model. Compared with nVBE, the F-score increases by 1.3% on PKU corpora and 1.2% on MSRA corpora. The HDP model with initial segmentation by MI (HDP+MI) also obtained a better result but not as well as HDP+nVBE model. This shows that the initial segmentation do play an important role in the model performance. A better initial segmen-tation tends to lead to a better performance. What X  X  more, we find that with a better initial segmentation, the algorithm converges much faster than ordi-nary HDP. The HDP+nVBE converged after about 50 iterations while ordinary HDP needed 1000 iterations to converge. This saves a lot of time as sampling on a large dataset can be quite slow. The best model (HDP+nVBE+dict) is obtained by using the initial segmentation of nVBE and giving better estima-tion of base meausure with the dictionary-based model. Many errors are related to dates, Chinese numbers and English words. We believe that with a better preprocessing our model can achieve a much better result. In this paper, we proposed a refined HDP model for unsupervised Chinese word segmentation. The refined HDP model uses a better estimation of base mea-sure and replaces the random initial segmentation with a better one by exploit-ing other state-of-the-art unsupervised word segmentation systems. The refined HDP model achieves much better result than the state-of-the-art system on PKU and MSRA benchmark datasets. This work is supported by National Natural Science Foundation of China under Grant No. 61273318 and 60975054
