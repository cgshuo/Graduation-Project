 1. Introduction markets, to structure information and discover hidden knowledge.
 sion can be processed from a different source or media.
 methods are strongly similar. Thus, they can be more easily unified, for example, through a voting algorithm. the process leads to more accurate results. The aim of this paper is to present an extension of S laboration between the clustering methods and allows an improvement of the final results. To evaluate benefits of using the collaborative clustering method S benefit of adding such new information to the data sets.
 both cases: with and without the use of background knowledge during the collaboration.
Finally, conclusions and perspectives are drawn in Section 5. 2. Related works sequently, they consider that the initial provided partitions are the only ones necessary. partition, and thus induce a new bias.
 terns, link between patterns, the number of expected clusters, the expected size of the clusters, etc. 2.1. Ensemble clustering better grasp the underlying distribution of the data space.
 balanced-size clusters (i.e. all the clusters tend to have the same number of data objects). been presented in the literature and seem to outperform the graph based methods. clusterings as a new set of categorical features. The K MEANS combining partitions generated by weak clustering algorithms that use data projections and random data splits. to each member of the ensemble.
 others consensus clustering algorithms.
 map built from the set of base clusterings and apply an EM-like consensus clustering.
Todirectlyaddressthe correspondence problem(amongthe clustersofdifferentclusterings) incombiningmultiplecluster-ings,Zhangetal. [15]introducedanewframeworkbasedonsoftcorrespondence.Theproposedalgorithmprovidesaconsensus formation.
 proaches introduce a new bias, relative to the objective function chosen when merging the different clusterings. 2.2. Multi-objective clustering tives used.
 the solution without background knowledge.
 Faceli et al. [21] described the multi-objective method called grates the same objective function (maximization of compactness and connexity of the clusters) as over operator which uses ensemble clustering techniques is added: the purpose of the solutions which are a trade-off of the different objectives, while the mate the quality of the clusters.
 previous one on the data and the use of background knowledge improved the result of the method. ferent criteria are investigated to quantify this purity.
 mate the quality of the clusters. 2.3. Fuzzy collaborative clustering type of clustering is used to discover a structure of web information in the spaces of semantics and data. provided for synthetic and real-world data.
 the ensemble. 2.4. Clustering with background knowledge Many approaches have been investigated to use background knowledge to guide the clustering process. Wagstaff et al. [30] presented a constrained version of the K utility (i.e the potential interest) of a set of constraints [34,35] .

Kumar and Kummamuru [36] introduced another kind of knowledge through a clustering algorithm that uses supervision wise constraints ( must-link and cannot-link ) for supervision.
 K MEANS algorithms and generally requires less constraints to obtain the same accuracies. amount of supervision.
 samples to seed (i.e. to initialize) the clusters of the K The choice between these two approaches must be done according to the knowledge about noise in the dataset.
To tackle the problem of incorporating partial background knowledge into clustering, when the labeled samples have number of labeled samples.
 Basu et al. [41] also proposed a probabilistic model for semi-supervised clustering, based on Hidden Markov Random imental results on several text data sets demonstrate the advantages of this framework. fication algorithms.
 function using background knowledge. Experiments are carried out on collections of images composed of 2000 images. which limit its use.
 3. Knowledge-guided collaborative clustering clusters of the different results.
 sensus computation can help to obtain better results.
 of several clustering results.
 In this section, we first present the existing unsupervised collaborative clustering method called S ent how we integrate knowledge into this collaborative clustering process. 3.1. Collaborative process overview ment about their proposals.
 the clusters, and to apply a unifying technique, such as a voting method [47]. used in the system.
 information is available about the correspondence between different clusters of different clusterings.
Let R  X fR i g 1 6 i 6 m be the set of results given by the m different algorithms. Let C
The corresponding cluster CC C i k ; R j of C i k in the result R sion matrix X i ; j between two results R i and R j is a n
The intercluster similarity between two clusters C i k and C into account the distribution q (Eq. (4)) of the cluster C where
The entire collaborative clustering process is broken down in three main phases: (1) Initial clusterings  X  Each method computes its result independently. (3) Consensus computation  X  The refined results are unified using a voting algorithm. The entire algorithm of the method is detailed and explained in Algorithm 1.
Algorithm 1 . Collaborative clustering process 1: Let R  X fR i g 1 6 i 6 m be the initial set of clusterings 2: Let K  X  conflicts  X  R  X  be the set of conflicts in R as defined in Eq. (5) 3: Let R best :  X  R be the best temporary solution 4: Let K best :  X  K be the conflicts of the best temporary solution 5: while j K j P 0 do 7: R :  X  conflictResolution  X  R ; K i ; j k  X  Algorithm 2 8: if C  X  R  X  &gt; C  X  R best  X  then 9: R best :  X  R 10: K best :  X  K :  X  conflicts  X  R  X  11: bt :  X  0 13: K :  X  K nK i ; j k 14: else 15: bt :  X  bt  X  1 16: K :  X  K nK i ; j k 17: if bt &gt; j K j then 18: R :  X  R best 19: K :  X  K best nK i ; j k Algorithm ( continued )
Algorithm 1 . Collaborative clustering process 20: end if 21: end if 22: end while 23: consensus computation 3.1.1. Initial clusterings different clusters. According to the base method selected, different parameters need to be set. 3.1.2. Results refinement 3.1.2.1. Detection of the conflicts. The detection of the conflicts consists in seeking in R all the couples C
S C k ; CC  X C i k ; R j  X  &lt; 1, which means that the cluster C
Each conflict K i ; j k is identified by one cluster C i k cluster similarity: ing to the conflict importance coefficient (Eq. (6)).
 try to improve their similarity. The operators which can be applied to a result are the following: Merging of clusters: some clusters are merged together; such as S C i k ; C j l &gt; p cr , where 0 6 p cr 6 1 is given by the user. The p the intersection between the two clusters is considered as significant. For example, p less than 20% of the objects of C i k ; C j l is not considered as a significant representative of C have one cluster for each object. These two solutions are not relevant and must be avoided. solution as those presented before: where and p q and p s are given by the user  X  p q  X  p s  X  1  X  .
 on each of the four couples of results:  X R i ; R j  X  ;  X R the conflict.
 Algorithm 2 . Conflict resolution Require : R the ensemble of clusterings Require : K i ; j k the conflict to solve
Ensure : R H  X  conflictResolution  X  R ; K i ; j k  X  the new ensemble after the resolution follows: where Three cases can occur: (line 12). Then, the conflict is removed from the list and the algorithm iterates. half part of the conflicts list), all the results are reinitialized to the best temporary solution (line 17).
The process is iterated until some conflicts still remain in the conflicts list (line 5). 3.1.3. Consensus computation number of clusters.

The basic idea is that for each object to cluster, each result R ple, and for its corresponding cluster CC C i k ; R j in each other result R ter for the object, for example C j l . It means that this object should be in the cluster C of the methods.
 For each object p a voting matrix is computed as: where and The object p is then assigned to the cluster V , defined as: 3.2. Background knowledge integration the constraints provided on the objects.
 (Section 3.2.2) we will see how we integrated it into the S 3.2.1. Examples of knowledge integration to integrate link-constraints in the algorithm Pairwise Constrained K function: terms) reflecting the agreement according to the sets of available constraints ( M and C ). The functions  X  l return 1 if the constraint between the couple of objects  X  x ing to available samples. It is defined as: (and the cluster will be considered as impure). This impurity is evaluated thanks to the Gini index: where P kl is the number of objects belonging to the lth class in the cluster C knowledge.
 case of various number of clusters in the results and avoid results with very high or very low number of clusters: where the impurity measure is defined by: where P kl is the number of objects belonging to the lth class in the cluster C cmax is the most represented class in the cluster C i k
The penalty is defined as 3.2.2. Knowledge integration in the S AMARAH method ifications of a couple of results is relevant. This criterion includes a quality criterion d result thanks to external knowledge, such as an estimation of the number of clusters, some labeled samples or some constraints.
 integrating an evaluation of the agreement of the results with different kinds of constraints as follows:  X  q  X  X 2 X  0 ; 1  X  and p c is the relative importance given by the user to the c -th constraint  X  p scribed below some frequently encountered constraints that can be used. number of clusters is given below: result. The internal quality of the k th cluster is given as: where n i k is the cardinal of C i k ; g i k is the gravity center of C take into account the number of clusters n i , the criterion p i is defined as: where  X  n inf ; n sup is given by the user, as the range of expected number of clusters. agreement index [52]. Information theoretic measures could also be used [53]. The Rand index is a measure of the similarity between two data partitions defined by: The WG agreement index is defined by etc.). This index takes values in [0,1]: 1 indicates that all the objects in the clustering R class label of the objects in R j . The defined constraint is: can easily be computed as where n r is the number of constraints between the objects, l respects the constraint l j , 0 otherwise.
 ples of objects belonging to different clusters. 3.3. Example Houston. 1 this problem, the knowledge of the range of expected number of clusters can be used. outsite the range are strongly reduced.
 that this information improves substantially the shape of the search space. less local minimas, the optimal solution (nine clusters) being strongly highlighted. 4. Evaluation of the proposed method 4.1. Protocol of experiments ration, to produce even better results.

Two kinds of experiment have been carried out:
For all the experiments with S AMARAH , we used the K MEANS to find results with a number of clusters in [2; 10] (i.e. n 4.2. Data sets
Seven different datasets from the UCI machine learning repository [56] were used in the experiments: (1) Iris data set, which contains three classes of 50 instances, where each class refers to a type of iris plant; 4.3. Comparison using quality indexes We used six different quality indexes to evaluate the quality of each set: The Rand index [49].
 The Jaccard index [50].
 The Falks X  X allow index [51].
 The Wemmert X  X an X arski index [52].
 The F-Measure index [51].
 The Kappa index [48].
 Rand index for all the experiments and all the datasets.
 the sets provided by the collaboration process using background knowledge ( kcol ) gives even better results. 4.4. Comparison using cascade evaluation interest of adding such new information to the datasets.
 most interesting information.
 rate knowledge compared to the initial unrefined set of results.

D  X f o i g 1 6 i 6 l where the object o i is characterized by the m attributes A in the jth initial clustering result. Let R j  X  o i  X  be the cluster of the object o created to integrate knowledge provided from the different clusterings: attributes, and we made 100 runs of 10-fold cross validations, each on the three versions of each dataset. with the clustering D 1 , the one embedded with the refined clustering D adding the clustering information just added noise to the dataset.
 volved in strong conflict. 5. Conclusion
In many clustering problems, the user is able to provide some background knowledge which can be used to guide the has been defined yet.
 in their search for a better solution.
 rate results.

References
