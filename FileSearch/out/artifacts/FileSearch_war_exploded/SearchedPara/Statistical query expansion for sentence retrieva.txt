 David E. Losada Abstract The retrieval of sentences that are relevant to a given information need is a challenging passage retrieval task. In this context, the well-known vocabulary mismatch problem arises severely because of the fine granularity of the task. Short queries, which are usually the rule rather than the exception, aggravate the problem. Consequently, effective sentence retrieval methods tend to apply some form of query expansion, usually based on pseudo-relevance feedback. Nevertheless, there are no extensive studies comparing dif-ferent statistical expansion strategies for sentence retrieval. In this work we study thor-oughly the effect of distinct statistical expansion methods on sentence retrieval. We start from a set of retrieved documents in which relevant sentences have to be found. In our experiments different term selection strategies are evaluated and we provide empirical evidence to show that expansion before sentence retrieval yields competitive performance. This is particularly novel because expansion for sentence retrieval is often done after sentence retrieval (i.e. expansion terms are mined from a ranked set of sentences) and there are no comparative results available between both types of expansion. Furthermore, this comparison is particularly valuable because there are important implications in time efficiency. We also carefully analyze expansion on weak and strong queries and demon-strate clearly that expanding queries before sentence retrieval is not only more convenient for efficiency purposes, but also more effective when handling poor queries.
 Keywords Sentence retrieval Query expansion Information retrieval 1 Introduction Effective sentence retrieval methods are potentially beneficial to many IR systems. There are many tasks whose performance is affected by the effectiveness of a sentence retrieval module. In web IR, information access can be facilitated provided that a good ranking of sentences, ordered by estimated relevance to the user, is supplied (White et al. 2005 ). Current search engines force the user to assess the surrogates associated with each webpage in order to decide whether it is worth visiting the page and, next, analyze exhaustively the clicked documents to locate the relevant material, if any. It is recognized that this 2-step process has an associated cost in time, effort and even stress (Kirsh 2000 ). A good selection of relevant sentences from the top retrieved documents can help, especially for particular types of queries. Question answering systems usually require some form of passage retrieval to isolate the document pieces in which the answer is likely to be found. This step is often done at the sentence level (Murdock 2006 ). One of the main areas in text summarization is centered on building summaries by extracting important sentences from the target document(s). If the summaries are query-biased then effective techniques to measure query-sentence similarities are needed (Tombros and Sanderson 1998 ). Information Extraction methods often involve some sentence retrieval algorithm to support their processes (Nobata and Sekine 1999 ). In Novelty Detection (Harman 2002 ), systems operate on sentences because it is recognized that redundancy is better analyzed using small units of texts. Sentence retrieval mechanisms have also been found important in Machine Translation (Doi et al. 2005 ).

Given a set of documents, our work focuses on a retrieval task based on selecting sentences relevant to a given information need, which is expressed as a textual query. This sentence retrieval problem is defined to work with documents highly related to the query. This simulates a working environment in which an initial document retrieval process is run and, next, the top ranked documents are input to a sentence retrieval module that filters out the irrelevant sentences and supplies the user with a rank of sentences. As argued in White et al. ( 2005 ), a sentence retrieval interface of this kind would be very valuable, especially for searches in which the user does not have a clear idea about the topics involved and the sentences supplied can help her/him to clarify the purpose of the search. This specific form of passage retrieval was recognized as an important problem and included in the TREC Novelty tracks from 2002 to 2004 (Harman 2002 ; Soboroff and Harman 2003 ; Soboroff 2004 ). The TREC Novelty tracks ask the participants to retrieve relevant sentences and then to filter out redundancy. However, we are only concerned here with the sentence retrieval problem. In the context of these tracks, three different datasets were provided (including relevance judgments at the sentence level). These collections are suitable for the objectives pursued here and, in any case, there are not many other sentence retrieval benchmarks.
Query expansion strategies, which have played a major role in document retrieval, are not sufficiently tested for sentence retrieval problems. Although some works have reported improvements using classical expansion techniques via pseudo-relevance feedback (Larkey et al. 2002 ), there are no comparisons available extensively testing different term selection methods and studying the effect of the number of sentences and terms used for expansion. Expansion strategies developed for document retrieval might be ineffective for sentence retrieval because the number of matching terms is much smaller and thus per-formance might be harmed. Due to the importance of query expansion in sentence retrieval, we feel strongly that a complete study on this subject is required. The vocabulary mismatch problem is a severe obstacle to yield effective retrieval at the sentence level and the role of query expansion as an remediation measure needs to be carefully analyzed. Furthermore, there are no comparative results between expansion before sentence retrieval and expansion after sentence retrieval. Expansion before sentence retrieval has been par-ticularly neglected. Since we start from a set of top ranked documents, it makes sense to study blind feedback methods working directly with the initial ranking of documents and compare them with regular pseudo-relevance feedback applied after running a first sen-tence retrieval process. Note also that this has important implications for efficiency that should not be disregarded.
Our study will be primarily focused on two standard automatic expansion methods that have worked well in document retrieval problems: pseudo-relevance feedback (PRF) (Buckley et al. 1996 ) and local context analysis (LCA) (Xu and Croft 1996 , 2000 ). These techniques are general enough to be applied across different domains and collections. Although some works have managed to get effective expansion with linguistic resources (see Related Work), we are concerned here only with purely statistical methods, which are simpler and applicable under very distinct scenarios. Furthermore, we conduct a detailed analysis of query expansion for weak and strong queries and provide some guidance on how to expand them in order to achieve effective sentence retrieval performance.

The rest of the paper is organized as follows. Section 2 reviews some papers related to our research. Section 3 presents the sentence retrieval method and the expansion tech-niques tested. The experiments are reported and analyzed in Section 4 . The paper ends with some conclusions. 2 Related work Sentence retrieval is a challenging area. Many researchers have proposed different solu-tions based on a wide range of models and techniques such as query expansion (either via pseudo-relevance feedback or with the aid of a lexical resource), part-of-speech tagging, clustering of sentences, named entities, supervised learning, and language modeling. Despite the variety of approaches investigated, simple adaptations of regular tf/idf mea-sures (sometimes aided with some form of pseudo-relevance feedback) can be labeled as state of the art sentence retrieval methods (Allan et al. 2003 ; Li and Croft 2005 ).
Many studies have examined the use of expanded queries either via pseudo-relevance feedback (Collins-Thompson et al. 2002 ) or with the assistance of a terminological resource, such as Wordnet (Zhang et al. 2004 ). The effect of pseudo-relevance feedback is known to be very sensitive to the quality of the initial ranks. Motivated by this, some researchers have applied selective feedback (Abdul-Jaleel et al. 2004 ), which is more stable but requires training data.

Expansion with synonyms or related terms from a lexical resource is problematic because noisy terms can be easily introduced into the new query. Moreover, a large terminological resource, with good coverage, is not always available. As a matter of fact, lexical expansion is usually equal or inferior to purely statistical expansion methods in sentence retrieval (Harman 2002 ; Soboroff and Harman 2003 ; Soboroff 2004 ). A notable exception is the work conducted by Zhang et al. ( 2002 ) in which a combination of query expansion and sentence expansion using Wordnet yielded good sentence retrieval performance.

Expansion approaches based on co-occurrence data have been also proposed. For instance, in Zhang et al. ( 2004 ) the authors expand the query with terms that co-occur often with the query terms in the retrieved documents. Co-occurrence statistics from a back-ground corpus have been applied in Schiffman ( 2002 ). Nevertheless, there is not much evidence that these approaches can outperform the standard pseudo-feedback methods. 1
A remarkable contribution to the area of sentence retrieval was done by Murdock ( 2006 ) in her thesis, where several principled sentence retrieval models based on Language Modeling were proposed. In her work, some experiments investigating the effects of pseudo-relevance feedback on sentence retrieval were reported. Query expansion produced negative results but a single expansion technique, based on Relevance Models, was tested. Our paper is complementary to her work because we study here standard models based on tf/idf under different expansion scenarios (i.e. after and before sentence retrieval) and, furthermore, we pay special attention to the effect of expansion on weak and strong queries.

Rather than expanding queries with new terms, other studies have focused on improving the matching process by analyzing carefully the nature of the sentence components. For instance, in Li and Croft ( 2005 ), patterns such as phrases, combinations of query terms and named entities were identified into sentences and the sentence retrieval process was driven by such artifacts. Although this technique was very effective for detecting redundant sentences, it was not significantly better than a regular tf/idf baseline for finding relevant sentences.

In document retrieval, some papers have applied query expansion where terms were selected from passages surrounding query terms (Hawking et al. 1998 ). Expansion after sentence retrieval produces similar effects because top ranked sentences will be populated with query terms and, hence, the new terms tend to co-occur with the topic terms. Some other papers have applied query expansion for document retrieval using different forms of passages (Voorhees and Harman 2005 ). These papers focus on ad hoc document retrieval experiments whereas we are interested in sentence retrieval. Furthermore, we do not want our results to be biased by any passage segmentation module and the notion of passage is restricted here to document sentences.

In Losada and Ferna  X  ndez ( 2007 ), an effective sentence retrieval method, based on extracting highly frequent terms from top ranked documents, was designed. This method actually represents a way to exploit the information from top retrieved documents before sentence retrieval. It was successfully compared against query expansion using pseudo-relevance feedback from top retrieved sentences (i.e. expansion after sentence retrieval). Nevertheless, expansion before sentence retrieval (i.e. expanding directly from the top retrieved documents) was not properly tested. For instance, sophisticated expansion techniques, such as LCA, were not considered in the experimental design.

A distinctive aspect of our work is to evaluate expansion before sentence retrieval. In the literature on sentence retrieval, the peculiarities of the sentence retrieval task are often ignored. Most expansion studies do not make full use of the information available but simply apply expansion methods that worked well in document retrieval. We argue that the ranked set of documents contains valuable information on the importance of terms that should not be disregarded. In this respect, we believe that it is important to check the effectiveness of query expansion methods when applied before sentence retrieval (i.e. working directly on the top retrieved documents available). There are at least two reasons that support this claim. First, sentence retrieval is very sensitive to the quality of the query and, hence, we might be safer working on the initial set of documents rather than on a subsequent ranking of sentences. Second, it would avoid retrieving an initial ranking of sentences and therefore would bring about a benefit in terms of efficiency.

Summing up, there is the general feeling in the sentence retrieval community that some form of expansion is needed to achieve reasonably good performance. However, expansion methods have not been adequately compared and, actually, we find in the literature con-flicting outcomes depending on the collection, baseline method tested, etc. In this paper we aim to clarify the role of expansion strategies in sentence retrieval by testing some standard methods against three different datasets, applying a very competitive baseline and checking the effects on weak and strong queries. 3 Sentence retrieval method To study properly different query expansion strategies we need first to decide which sentence retrieval method is appropriate for our purposes. Since we want to evaluate the ability of expansion techniques to improve the state of the art in sentence retrieval, we have to set a competitive sentence retrieval technique. In Allan et al. ( 2003 ), the results of some sentence retrieval experiments are discussed. A simple vector space retrieval technique is shown to perform at least as well as any other method and, actually, its performance is the most robust. This method, which we will refer to as tf/isf, 2 applies a weighting scheme that is a variant of tf/idf applied at the sentence level. Although other effective methods, such as those based on clusters of sentences, can be found in the literature (Harman 2002 ; Soboroff and Harman 2003 ; Soboroff 2004 ), we skip them deliberately because the tf/isf method is simpler and we therefore avoid possible biases and complications coming from evolved approaches (e.g. the effect of the quality of the clusters). We believe strongly that the simplicity of this method is a good feature, making the results presented here potentially applicable in very different scenarios.

The relevance of a sentence s given a query q is estimated in Allan et al. ( 2003 ) as: where sf t is the number of sentences in which t appears, n is the number of sentences in the collection and tf t , q (tf t , s ) is the number of occurrences of t in q ( s ).
To further check that tf/isf was competitive we designed some preliminary experiments whose results are reported in Sect. 4.1 . This included experiments using alternative sen-tence retrieval methods (BM25 and Language Modeling with Kullback X  X iebler Distance 3 ), different combinations of the pre-processing strategies and even additional tests using idf sentence retrieval method whose performance is comparable or superior to the best per-formance attainable by other effective methods. 3.1 Expansion after sentence retrieval By query expansion after sentence retrieval (ASR) we refer to the regular pseudo-rele-vance feedback process adapted to the sentence retrieval case. First, the query is run against the sentences in the top retrieved documents and, next, the top retrieved sentences are used to mine expansion terms. Two main strategies are considered to select new terms: Pseudo-relevance feedback (PRF) and LCA. 4
Pseudo-relevance feedback (also called local or blind feedback) is a traditional concept in IR (Attar and Fraenkel 1977 ), which basically consists of selecting the terms with more (raw) counts in the top retrieved sentences. Although it did not work well with small (pre-TREC) collections, its merits for large-scale document retrieval have been apparent in many TREC experiments (Voorhees and Harman 2005 ). Nevertheless, the effects this method has on sentence retrieval have not been studied in detail. Actually, some papers have reported improvements with PRF-based expansion but other studies are sceptical about PRF improving sentence retrieval (Murdock 2006 ). We therefore expect that the experiments reported here help to shed light on this issue. Note also that there are some parameters needed for success, such as the number of top sentences and the number of expansion terms. Sentences are very small pieces of text and retrieval performance may be very sensitive to the parameter configuration.

Local context analysis is a successful expansion method proposed by Xu and Croft ( 1996 , 2000 ). It has been adopted by other research groups in several large-scale experi-ments in document retrieval (Voorhees and Harman 2005 ). Nevertheless, the effects of LCA on sentence retrieval are barely discussed in the literature and there are no experi-mental results available comparing LCA and local feedback. The main motivation to propose LCA was that local feedback fails if there is a large number of non-relevant items in the top ranked set. The LCA method tries to be less erratic and is designed to work on document passages. We take here an instance of the LCA proposal where passages are simply document sentences. The main hypothesis of LCA is that common terms from relevant documents (sentences, in our case) will tend to co-occur with query terms within the top-ranked documents (sentences). In this way, a term selection metric is defined, yielding an expansion method that is more robust than local feedback. Sentence retrieval is more difficult (and intrinsically different) than document retrieval and, hence, this hypothesis should be re-visited and evaluated empirically with sentence retrieval benchmarks.

Although LCA works for concept selection, where concepts can be single terms or phrases, we are only concerned here with selecting single terms for expansion. Let us consider a query q , whose query terms are qt 1 , ... , qt m , and a set of top ranked sentences S = { s 1 , s 2 ,..., s n }. The terms appearing in S are ranked according to the formula: where N is number of sentences in the collection, N t is number of sentences in the col-constant set to 0.1 to avoid zero value.
The co( t , qt i ) value counts the number of co-occurrences between t and qt i in S . co_degree( t , qt i ) is a co-occurrence metric that represents the degree of co-occurrence of t with qt i based on the following hypotheses: (a) good expansion terms will tend to co-occur with all query terms in the top-ranked sentences and, (b) good expansion terms should not just co-occur with a query term by chance (this is implemented by considering that the higher the term t  X  X  frequency in the whole collection, the more likely it is that it co-occurs with qt i by chance). f ( t , q ) is a combined score of the individual co-occurrences of t with each query term.
 This term ranking function is a variant of the regular tf/idf measure utilized popularly in IR. Most often preferred terms will be those rare terms (idf effect) that co-occur frequently with many query terms. More details and explanations about the LCA method can be found elsewhere (Xu and Croft 1996 , 2000 ).

Given this measure, the terms in the retrieved sentences can be ordered in decreasing order of f ( t , q ) and the top ranked terms are selected to expand the query.

For simplicity, we do not consider here any parameterized re-weighting strategy (e.g. based on Rocchio X  X  formula). With both methods (PRF and LCA), the selected terms are simply incorporated as new terms in the query. Note that this involves expansion (new terms that were not present in the original query) but also basic re-weighting (the query term frequency is increased for terms belonging to the old query that are also selected in the expansion phase). 3.2 Expansion before sentence retrieval One strategy that has not received much attention is to run query expansion before retrieving any sentence (BSR). This alternative was not explored in the past but it could become very valuable. First, for efficiency reasons: we can skip the initial sentence retrieval process (no sentence retrieval is required for doing expansion). Second, query expansion may be more robust if we work directly from the top ranked documents. Observe that poor queries will likely introduce a great deal of noise if we use them to retrieve some sentences to feed the term selection module. The initial ranking of docu-ments is arguably weak for such queries but, still, a second usage of the original topic for query expansion purposes might be not advisable. It is therefore interesting to evaluate empirically these issues and compare expansion BSR and expansion ASR.

Some experiments were designed to evaluate expansion BSR. The term selection methods were the same as those explained in the previous section but the sentences used to mine the expansion terms are taken directly from the initial ranking of documents available for the task. More specifically, the top X documents ( X is a parameter) are used for term mining. To maintain consistency with the ASR experiments, term selection works also at the sentence level. The BSR-version of LCA extracts new terms analyzing the co-occurrences in the sentences of the top X documents. Similarly, expansion BSR with pseudo-relevance feedback incorporates into the query the terms with more counts in the sentences of the top X documents. However, note that there is no sentence retrieval here (e.g. if X = 1 then all sentences from the top document are considered in the term selection process). 3.3 Complexity issues Expansion ASR introduces an important time penalty because it requires a sentence retrieval process for term selection. In contrast, expansion BSR works directly from the sentences in the top retrieved documents. This is a considerable saving.

Given a set of sentences X  X ither a set of sentences ranked in decreasing order of similarity to a given query (expansion ASR) or a set of sentences appearing in top ranked documents (expansion BSR), it is interesting to compare the steps needed to compute the ranks of terms with PRF or LCA. PRF simply requires traversing the sentences and accumulating the term counts in a proper data structure (e.g. a term-count structure ordered by count). LCA requires also visiting every sentence and accumulating the co( t , q i ) counts (for each q i ). The time complexity of this process across retrieved sentences is equivalent to the time complexity needed by PRF (although the space complexity is higher with LCA because we need to store independent statistics for each query term). LCA incorporates an additional time penalty to compute the final f ( t , q ) values (product across query terms). This cost, which is linear with respect to the number of query terms, could be assumed to be negligible, especially if queries are short. 3.4 Final remarks As argued above, we expect that expansion ASR suffers from poor performance when queries are poor. In such cases, expansion BSR might be more solid. On the other hand, expansion ASR could be very effective with good queries. A good query can potentially retrieve many on-topic sentences where good expansion terms are easily found. Expansion BSR might be less effective here, especially if the documents are long and multi-topic. Working directly on the ranked set of documents can lead to new queries containing terms that are off-topic but popular in the initial ranking of documents. In this respect, we expect that expansion BSR with LCA is more robust because it requires that the expansion terms co-occur frequently with the initial query terms.

Another expansion alternative would consist of combining both types of expansion (ASR and BSR). However, this type of combination is out of the scope of the present paper. 4 Experiments We designed a complete pool of experiments to test the expansion configurations. The experiments were run against three different collections of data [those supplied for the TREC-2002, TREC-2003 and TREC-2004 novelty tracks (Harman 2002 ; Soboroff and Harman 2003 ; Soboroff 2004 )]. There are no newer TREC collections suitable for our experiments because we need relevance judgments at the sentence level. This sort of judgment is only available in the novelty track, whose last edition took place in 2004. The novelty track data were constructed as follows. Every year there were 50 topics available. In TREC-2002, the topics were taken from TRECs 6, 7 and 8 5 and the documents were obtained from regular TREC ad hoc collections (Los Angeles Times, Federal Register, Foreign Broadcast Information Service, and Financial Times Limited). In 2003 and 2004, the topics were created by assessors designated specifically for the task (topics N1-N100) (Soboroff and Harman 2003 ; Soboroff 2004 ) and the document collection was a standard news collection (AQUAINT). For each topic, a rank of documents was obtained by NIST using an effective retrieval engine. In 2002 and 2003 the task aimed at finding relevant sentences in relevant documents and, therefore, the ranks included only relevant docu-ments (i.e. given a topic the set of relevant documents to the topic were collected and ranked using a document retrieval engine). By contrast, the TREC-2004 ranks contained also non-relevant documents (i.e. the initial search for documents was done against a regular document base, with relevant and non-relevant documents). Note that this means that the non-relevant documents are close matches to the relevant documents, and not random non-relevant documents (Soboroff 2004 ). In any case, the ranks of documents contained at most 25 relevant documents for each query.

Each document was also automatically split into sentences at NIST and sentences were assigned identifiers. The participants were given these ranks of sentence-tagged documents and they were asked to locate the relevant sentences. The relevance judgments in this task are complete because the assessors reviewed carefully the ranked documents and marked every sentence as relevant or non-relevant to the topic. In TREC-2002, very few sentences were judged as relevant (approximately 2% of the sentences in the documents). In TREC-2003 and TREC-2004 the average percentage of relevant sentences was much higher than in 2002 (approximately 40% in 2003 and 20% in 2004). Given the different characteristics of these datasets, they form an assorted set of testbeds to perform a thorough evaluation.
We consider here two different evaluation measures: the F measure, which was the official measure utilized in the TREC novelty tracks, and precision at ten sentences retrieved ( P @10). The F measure is the harmonic mean (evenly weighted) of sentence set recall and precision. This is a consistent performance ratio because it is meaningful even when the number of relevant sentences varies widely across topics (Harman 2002 ). The F values reported here are obtained by retrieving 5% of the sentences in TREC 2002, and 50% of the sentences in TREC 2003 and TREC 2004. These thresholds, which have been applied in the past, are reasonable given the number of relevant sentences in every col-lection. Additionally, P @10 ratios are included in our reports. P @10 is important in many applications, such as web sentence retrieval (White et al. 2005 ), which require a good distribution of relevant material in the top rank positions.

We focus our interest on short queries (constructed from the title tags of the TREC topics) because properly handling this type of query is challenging in sentence retrieval. These queries are good candidates for expansion because they are often ambiguous. 4.1 Evaluating the baseline To ensure that the baseline (tf/isf, Eq. 1 ) is capable of yielding state of the art performance, we ran some preliminary experiments comparing it against Okapi BM25 (Robertson et al. 1995 ) and a Language Modeling approach based on Kullback X  X eibler Divergence (KLD) as described in Larkey et al. ( 2002 ) (with Dirichlet smoothing). The performance of BM25 is influenced by some parameters: k 1 controls the term frequency effect, b controls a length-based correction and k 3 is related to query term frequency. We tested exhaustively different parameter configurations ( k 1 between 0 and 2 in steps of 0.2, b between 0 and 1 in steps of 0.1 and different values of k 3 between 1 and 1,000). Similarly, we experimented with the KLD model for different values of the l constant, which determines the amount of smoothing applied ( l = 10, 100, 500, 1,000, 3,000, 5,000). Results are reported in Table 1 . A run marked with an asterisk means that the difference in performance between significant difference between the tf/isf run and the best BM25 run. We also observed that BM25 was very sensitive to the parameter setting (many BM25 runs performed significantly worse than tf/isf). On the other hand, KLD was inferior to both tf/isf and BM25. These results reinforced previous findings about the robustness of the tf/isf method (Allan et al. 2003 ; Li and Croft 2005 ) and demonstrated that this method is a very solid baseline. Note also that tf/isf is parameter-free whereas the results reported for BM25 and KLD are the best ones obtained across the configurations tested.

We also experimented with different combinations of the standard preprocessing strategies (stopwords vs no stopwords, stemming vs no stemming). Although there was not much overall difference, the runs with stopword processing and no stemming were slightly more consistent.

The tf/isf method takes the isf statistics from the sentences in the documents available for the task (which is a small set of sentences). A term that is very common within the retrieved documents would therefore receive a low isf weight. This might be problematic because content-bearing terms that are frequent in a given set of documents are assigned small weights. We were therefore wondering whether better performance might be obtained using data from a larger collection. To check this, we indexed a large collection of documents (the collection used in the TREC-8 ad hoc experiments) and ran some exper-iments with regular idf statistics obtained from this index (i.e. in Eq. 1 sf t was replaced by n , which is the document frequency of t in TREC-8). The original tf/isf method computed at the sentence level over the small document base was slightly superior. It appears that the small index of sentences is good enough for sentence retrieval (at least for these short queries). We therefore set the basic sentence retrieval method to be the original tf/isf approach with stopword removal and no stemming.

Note that we use short queries, while the groups participating in the TREC novelty tracks were allowed to use the whole topic. This means that the results presented here are not comparable to any of the results reported in the novelty tracks. Actually, we expect that the results obtained here are worse than the ones achieved in TREC because of our experimental conditions. Nevertheless, short queries are the rule rather than the exception in many applications and it is therefore important to study in depth the sentence retrieval performance with such queries. Moreover, query expansion methods are especially important when the user supplies few search terms. 4.2 Evaluating query expansion strategies Let us now pay attention to the effects of query expansion on sentence retrieval perfor-mance. With expansion ASR, we first ran the tf/isf sentence retrieval method on the ranked set of documents associated with each query. This produced a ranked set of sentences from which some expansion terms were selected using either PRF or LCA. These new terms were included in the query and the tf/isf sentence retrieval model was run again with the expanded query. We tested different configurations of the number of expansion terms (5, 10, 20 and 50) and the number of top retrieved sentences in which terms are selected (5, 10, 25, 50 and 100). On the other hand, expansion BSR selects terms directly from the ranked set of documents. We planned experiments using the top 1, 5, 10, 15 or 25 documents with varying number of expansion terms.

The experimental results are reported in Table 2 (expansion ASR X  P @10), Table 3 (expansion BSR X  P @10), Table 4 (expansion ASR X  F measure) and Table 5 (expansion BSR X  F measure). The tables also include the performance of the baseline tf/isf with no expansion (underlined after the collection X  X  name). For each collection and type of expansion the best parameter configurations are marked in bold. Expansion runs whose improvement over the baseline is statistically significant are marked with an asterisk. To analyze graphically how performance changes with the number of expansion terms and the number of top sentences/documents, Figs. 1 , 2 , 3 and 4 present the surface plots of the performance of every expansion strategy given the parameter settings.

First of all, it is interesting to observe the effect of expansion on the TREC-2002 collection. There are some expansion configurations that show P @10 and F ratios that are higher than the baseline X  X  ratios. In any case, few of the improvements are statistically significant. Observe that this collection contains very few relevant sentences ( &amp; 2%) and therefore any expansion strategy is likely incorporating unrelated terms into the new queries. PRF is particularly problematic here because it often performs worse than the baseline (32 out of the 80 TREC-2002 PRF expansion runs perform worse than the baseline). In contrast, LCA does not improve significantly over the baseline but at least there are fewer LCA runs yielding performance that is worse than the baseline performance (14 runs out of 80).

On the other hand, most expansion methods produce statistical significant improve-ments in TREC-2003 and TREC-2004 (for both P @10 and F ). These results show that statistical query expansion is beneficial in sentence retrieval provided that the number of relevance sentences in the ranked set of documents is sufficient.

In a real application, one would have to decide whether or not to apply expansion for each query. This selected expansion could be based on estimations of the number of relevant documents (or passages) retrieved. In this respect, the recent advances on query performance prediction methods (Hauff et al. 2009 ) might be of help. 4.2.1 Number of expansion terms and number of top sentences/documents Next, we analyze the trends with respect to the number of expansion terms and the number of top sentences/documents.

Expansion ASR, P@10 (Table 2 ; Fig. 1 ) The standard PRF expansion tends to improve significantly over the baseline with few expansion terms ( B 10) and many sentences. A safe configuration would be 10 expansion terms selected from 100 sentences (this configura-tion yields statistical significant improvements in all collections). Other expansion configurations also significantly outperformed the baseline but the improvements are not consistent across collections. On the other hand, LCA tolerates expansions with many terms slightly better than PRF (12 LCA runs with 20 or more expansion terms produced significant improvements over the baseline while only 10 PRF runs with C 20 expansion terms improved over the baseline). Still, it is quite difficult to choose an optimal setting for LCA. Given the results obtained here, 20 expansion terms-50 sentences or 50 expansion terms-100 sentences look like reasonable choices.

Expansion BSR, P@10 (Table 3 ; Fig. 2 ) When expanding queries before sentence retrieval, both methods are rather erratic in terms of P @10. Many configurations do not lead to statistical significant improvements. Furthermore, the optimal configuration is not stable across collections.
 Expansion ASR, F-measure (Table 4 ; Fig. 3 ) With both expansion methods, PRF and LCA, the highest performance tends to be found when a large number of expansion terms are selected from a large number of top sentences. P @10 is a high precision measure but the F measure is influenced by both precision and recall. This explains why the optimal F measure performance is found with expansions involving many new terms, while P @10 does not show such a clear trend.

Expansion BSR, F-measure (Table 5 ; Fig. 4 ) With LCA, there is also a clear tendency to prefer many expansion terms extracted from many documents. However, with PRF, the optimal configuration varies significantly depending on the collection. PRF is clearly less reliable than LCA in this case.

Given this report, it seems that LCA performs the best with many expansion terms extracted either from many sentences (ASR) or from many documents (BSR). In terms of F measure, this is definitely the case. In terms of P @10, the trend is less obvious but still there exists some slight preference towards expansions with many terms mined from many sentences or documents. In contrast, PRF is much more erratic and its optimal expansion configuration is much more difficult to assess. 4.2.2 Best and average performance Let us now analyze the best and average performance attainable by each expansion method. For a more clear picture of the experimental outcome, these results are summa-rized in Table 6 . The difference between the best ASR and BSR runs has been tested for statistical significance and the BSR run is marked with the symbol y when the difference between the run and the respective ASR run is significant. In terms of P @10, there is no significant difference between the best runs. This means that any configuration (ASR/ BSR ? PRF/LCA) can lead to optimal performance provided that the parameters (number of expansion terms and number of top sentences/documents) are set adequately. Looking at the average P @10 values, we found some interesting trends. With expansion ASR, PRF is more solid than LCA. On the contrary, with expansion BSR, LCA tends to be more reliable (especially when the conditions are difficult as with TREC-2002 where few of the sen-tences are relevant). This makes sense because the sentences feeding the ASR term selection module are potentially closer to the query than the sentences feeding the BSR term selection module. Recall that expansion ASR runs an initial sentence retrieval from the query and the retrieved sentences are used for term selection purposes. In contrast, expansion BSR works directly with the initial ranked set of documents, where the on-topic sentences might be scattered across the documents. This means that a rough term selection metric (such as local feedback) is good enough with expansion ASR but it is less consistent when there is not an initial sentence retrieval process.
 In terms of the F measure, the results are basically the same as the ones found with P @10. PRF tends to work better with expansion ASR while LCA tends to be more solid with expansion BSR. In two collections the best run of PRF with expansion ASR performs significantly better than the best run of PRF with expansion BSR. It is interesting to note that the single collection where the difference is not significant is TREC-2002, where there are few relevant sentences. This makes sense because expansion ASR is very sensitive to the quality of the initial sentence retrieval process. If these ranked sentences contain many non-relevant items then expansion ASR show little improvement over expansion BSR.
In terms of effectiveness, expansion ASR with PRF and expansion BSR with LCA are the most robust expansion methods for sentence retrieval. Both approaches lead to good P @10 and F measure performance ratios. Since expansion BSR is less expensive than expansion ASR (because we do not need an initial sentence retrieval process), expansion BSR with LCA looks the most suitable choice. One can rightly argue that LCA is more costly than PRF but, as argued in Sect. 3.3 , the additional complexity requirements are acceptable. This means that we can achieve state of the art sentence retrieval performance with significant savings in terms of efficiency. This is a novel result because the studies conducted in the literature have been mostly focused on the standard expansion methods (ASR). Furthermore, if the aim of the retrieval application is to retrieve ten good sentences (i.e. recall is not a major issue) then expansion BSR with PRF is a good choice. As shown in Table 6 , this retrieval technique, which is the most efficient method, does not perform significantly worse than the other expansion methods (in terms of P @10). 4.3 Strongest and weakest queries The results shown above are valuable to propose query expansion configurations that work well on average. However, it is also interesting to conduct a more fine-grained analysis. Effectiveness varies widely between queries and system designers need to pay attention to the individual queries, and not just to the average performance in order to make further improvements in overall retrieval effectiveness.
 The experiments reported in the previous section showed that, in some cases (e.g. P @10), there was no significant difference between the best runs of every expansion method. Nevertheless, this does not mean that the expansion techniques behave equiva-lently with respect to the quality of the query. Actually, we expect that expansion BSR works better than expansion ASR for poor queries and, conversely, expansion ASR is expected to perform better than expansion BSR with good queries. This belief is based on the fact that a weak query might severely harm the performance of ASR because of the low quality of the ranking of sentences. In contrast, BSR does not require an initial ranking of sentences.
 In this section we try to verify whether or not our experimental data supports this claim. To this aim, we analyze the behavior of the expansion methods with respect to the quality of the queries.

For each collection, we compiled the set of 15 queries that produced the lowest P @10 values given the tf/isf method and the set of 15 queries that produced the highest P @10 values given the tf/isf method. These sets of queries are good representatives of weak and strong queries and a given expansion method might be more or less suitable for a particular set. In order to make the experiments reproducible, these sets of weakest and strongest queries and their average P @10 (tf/isf) are reported in Table 7 . Observe that by weak (strong) query we mean a query for which few (many) relevant sentences are retrieved. Therefore, a specific and well-formed query might be categorized as weak provided that it has very few relevant sentences in the corpus. Conversely, a badly specified query can be classified as strong just because there are many relevant sentences in the corpus. Our intention here is to compare the expansion strategies with queries that retrieve either many or few relevant sentences regardless of the reasons why these queries do so (e.g. query construction vs difficulty of the collection).
 To compare the performance of expansion ASR and expansion BSR with these queries, Table 8 presents the P @10 and F measures obtained with PRF and LCA for both types of expansions. The results reported here correspond with parameter settings associated to the best runs identified in Section 4 . Table 6 presented the performance measures averaged across all topics (rows labeled as  X  X  X est X  X ) and we present here the average across the 15 weakest topics and the average across the 15 strongest topics.
 Expansion ASR does not perform well with poor queries. In terms of P @10 expansion BSR is clearly better than expansion ASR. In particular, expansion BSR with PRF is the best choice. Observe that expansion BSR with PRF was inferior to expansion BSR with LCA when considering all topics (Table 6 ) but, interestingly, it is quite solid for the subset of queries that are weak. These results indicate that the most efficient and simple expansion method (expansion BSR with PRF) is the most suitable for weak queries. There is less distinction between the F measure results but, still, there is some tendency showing that expansion BSR is preferable to expansion ASR when queries do not retrieve much relevant material.

With strong queries, the situation is the opposite. The highest performance tends to be found with expansion ASR and PRF. Although the F measure results are not very con-clusive, P @10 shows a tendency in favor of this expansion method.

This analysis confirms our intuition with respect to the relative merits of expansion ASR and expansion BSR. Expansion ASR works consistently with good queries but its per-formance falls when the queries are weak. In such cases, expansion BSR is much more robust and, additionally, it is easier to compute.

Ideally, if an efficient method to estimate the quality of the query and the amount of relevant material in the top ranks was available then we could dynamically set the expansion approach to apply. To the best of our knowledge, the ability of current query difficulty measures (Cronen-Townsend et al. 2002 ; Macdonald et al. 2005 ; Hauff et al. 2009 ) for sentence retrieval purposes is unknown. We tested some of the query predictors proposed in Macdonald et al. ( 2005 ) (average inverse collection term frequency and query scope) but found no correlation between these measures and the final sentence retrieval performance. Nevertheless, our query prediction experiments were limited. A challenging line of future work is to study many other methods for predicting the performance of a sentence retrieval process. In this respect, the very thorough comparative study presented in Hauff et al. ( 2009 ) will be a key reference to understand which predictors are appro-priate for our purposes.
 5 Conclusions In this paper we have presented a thorough study on the effects of query expansion strategies for sentence retrieval. We have worked with a standard sentence retrieval method, proven that it is competitive against other robust techniques and supplied a complete study of query expansion under this framework.

Query expansion is of paramount importance in sentence retrieval problems but a complete comparison of different expansion techniques was not available so far. Standard expansion methods such as local feedback and local context analysis have worked well in document retrieval but their role in sentence retrieval problems was largely unknown. Moreover, query expansion for sentence retrieval has not been fully exploited in the past because expansion before sentence retrieval (i.e. expanding directly the query from the top ranked documents) has not been studied.

The results of our study can be summarized as follows. In terms of average effec-tiveness, expansion ASR with PRF and expansion BSR with LCA are the most robust expansion methods for sentence retrieval. Both approaches lead to good P @10 and F measure performance ratios. Since expansion BSR is less expensive than expansion ASR (because we do not need an initial sentence retrieval process), expansion BSR with LCA looks to be the most suitable choice. This means that we can achieve state of the art sentence retrieval performance with significant savings in terms of efficiency. This is a novel result because the studies conducted in the literature have been mostly focused on the standard expansion methods (ASR).
 In high precision applications, where recall is not a major issue, expansion BSR with PRF is a good choice. As shown in Table 6 , this retrieval technique, which is the most efficient method, does not perform significantly worse than the other expansion methods (in terms of P @10).

Regarding the number of expansion terms and the number of top documents/sentences from which terms are mined, we found that LCA shows a slight tendency to achieve its highest performance with expansions involving many terms while PRF is more erratic with respect to the ideal number of expansion terms. In general, PRF is very sensitive to the parameter setting. Although the top performance attainable by PRF tends to be similar to LCA X  X  top performance, the parameter settings are more problematic with PRF.

Besides the overall analysis across all queries, we conducted a detailed study of the behavior of the expansion strategies with particular types of queries. Expansion ASR works consistently with good queries but its performance falls when the queries are weak. In such cases, expansion BSR is much more solid and it is easier to compute. This is an important outcome that needs to be taken into account in future developments on query expansion for sentence retrieval.

Summing up, although some past studies have been skeptical on the role of query expansion for sentence retrieval, our report shows that it is a consistent technique to improve sentence retrieval performance provided that the retrieved documents contain a reasonable number of relevant sentences. The two methods tested, PRF and LCA, can produce significant benefits when parameters are set appropriately. Even with an extremely low population of relevant sentences (TREC-2002), a proper query expansion configura-tion (e.g. BSR ? LCA for high precision purposes and ASR ? PRF otherwise) yields often to improvements in performance. Our results showed also that sentence retrieval performance is highly dependent on the parameter settings. This is an expected outcome when applying query expansion to different retrieval tasks. However, we provided here some clues to set parameters properly.
The incorporation of query expansion leads to sentence retrieval methods that perform reasonably well. Therefore, it is natural to consider the application of these effective retrieval methods in different IR scenarios. In the near future, we plan to study the effects of these sentence retrieval methods in areas such as Question Answering, Text Summa-rization, and Structured Retrieval (e.g. XML retrieval). Additionally, the impact of our research in tasks related to sentence retrieval, such as focused and aggregate retrieval, will be further studied.
 References
