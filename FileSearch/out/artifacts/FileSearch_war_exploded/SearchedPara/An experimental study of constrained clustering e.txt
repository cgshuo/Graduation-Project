 1. Introduction Data analysis has a central role to play in several real-world domains, such as medicine, advertising or market analysis.
With the ever-growing size of data collections in possession of public institutions and private companies there is a need for accurate automatic data analysis tools in order to be able to process these great amounts of data in a timely manner.
Clustering ( Jain, Murty, &amp; Flynn, 1999 ) is the most common form of automatic unsupervised data analysis. Traditionally, clustering algorithms worked by trying to find relationships in the data forming groups (clusters) using only the information present in the data, aiming to fulfil two goals: maximise the similarity between the data points which are assigned to the same cluster and keep the data points assigned to different clusters as dissimilar as possible. On the other hand, in classifi-cation, the most popular supervised approach ( Sebastiani, 2002 ), the user knows exactly which groups are present in the data, and feeds the algorithm with examples from these groups. With these examples, the algorithm characterises the cat-egories of the data in order to be able to assign new data instances (i.e., data which has not been seen before) to the right group.

So, customarily, one of the main differences between clustering and classification has been the degree of control that the user has over the entire process. Whereas in classification the outcome of the process lies almost fully in the choice of the examples made by the user (and thus the great importance of the technique which has been used to select them), the  X  capability of the user to influence the result of the clustering algorithms was limited at best to selecting which metric was used to compare the data instances or tweaking some details of the innards of the algorithms, such as which Laplacian is used in spectral methods or using Complete, Single or Average link in hierarchical ones.

Lately, a new fashion of semi-supervised clustering algorithms, coined as constrained clustering ( Basu, Davidson, &amp; Wag-staff, 2008 ), has emerged. These new algorithms can incorporate some a priori domain knowledge, allowing the user to somehow guide the clustering process. This information is given to the algorithm as a set of pairwise constraints involving pairs of data points and expressing real hard restrictions or preferences about whether or not these pairs should be in the same cluster, as opposed to the extensive set of examples of each group which is needed in classification. Thus, and even though the user can influence the outcome, constrained clustering is still a clustering process, as it is the algorithm itself which determines which groups exist in the data, while in classification the goal is cataloguing previously unseen data points into groups which have been already defined taking into account the examples given by the user.

With these constrained clustering approaches, knowledge that was unused in traditional clustering algorithms is used to improve the grouping of data in real domains. An example would be collections where the data instances contain informa-tion that comes from multiple evidence sources, such as medical reports. If we want to cluster such a collection using the report text as the main source of evidence, it might be useful to guide the clustering process introducing constraints using domain knowledge about dates, geolocalisation, race, gender, etc. of the patients. Constrained clustering can also be useful in collections where data points have an obvious grouping and the traditional algorithms tend to be biased to that clustering, which is not interesting for the users. Using the same domain as in the previous example, a clustering algorithm could point out a well-known relation between a disease and the patients X  age, however, the medical doctors might want to acquire an alternative explanation of the data, relating it to other factors.

Constrained clustering has been studied in recent years by several authors ( Ares, Parapar, &amp; Barreiro, 2009; Bae &amp; Bailey, &amp; Schr X dl, 2001; Yang &amp; Callan, 2006 ), showing that, as expected, the introduction of domain knowledge in the clustering process indeed improves the quality of the clustering obtained.

However, in most of these studies the experiments were carried out drawing the constraints from the reference group-ing which was later used as ground truth in the assessment of the clustering results. Thus, the domain knowledge supplied to the algorithms in the vast majority of the experiments was totally accurate, as each of the pairwise constraints holds in the reference grouping. In this case, all of this information is precise and would actually help the clustering process to obtain an outcome closer to the one used as reference (thus helping the algorithm to achieve better results). Even though this approach enables us to measure the effectiveness of the introduced constraints, it does not take into account factors which would be present in most realistic settings, such as misjudgements made by the user or the inevitable errors that any automatic method used to create the constraints from the data would make. In a real world situation, the robustness of the constrained clustering algorithms to  X  X  X ad X  X  constraints is bound to play a significant role in the final effectiveness of the algorithm.

For instance, if we want to cluster web pages, it could be useful to use the tags assigned to them by the users of a social tagging service to guide the clustering, creating constraints between the pages. Ares, Parapar, and Barreiro (2011) proposed an approach to automatically extract these constraints from Delicious data, linking two documents with a positive constraint if they share a minimum number of tags. Even though they showed that using these constraints works well, greatly improv-ing the results of clustering pages using only the documents, only the tags or adding tags to documents X  contents, they also reported that the set of constraints contains a fair amount of noise, due to the different criteria that users have used to anno-tate the content with a particular tag, the polysemy of the words used as tags or even errors in the tagging process. The com-parison between the results using all constraints and using only the accurate ones indicated that this noise notably reduces the maximum effectiveness of the constraints. Furthermore, they point out the existence of noticeable differences in the sta-bility of the parameters of the constrained clustering algorithms used in the experiments.

To the best of our knowledge, the problem of the robustness of the constrained clustering algorithms has only been thor-oughly studied by Nelson and Cohen (2007) in the framework of probabilistic algorithms. As for the field of spectral cluster-ing, the only work we are aware of is Coleman et al. (2008) . However, the authors admit that their work is basically theoretical, focused in the problem of clustering into only two clusters and that more experimental work is needed in that area.

In this paper we will explore the behaviour of four constrained clustering algorithms, representing approaches from both spectral and flat partitioning algorithm families, simulating a realistic setting where some of the constraints which are fed to them are not accurate. We report the results of experiments conducted over text and numeric datasets using two different noise models of the constraints, one which injects noise randomly and an original one based on intuitions over where the erroneous constraints are more likely to appear in real clustering tasks. These results highlight the strengths and weaknesses of each method when working with positive and negative constraints, which we have used to conclude the scenarios in which each algorithm is more appropriate.

The remainder of the paper is organised as follows. In Section 2 we survey constrained clustering and the algorithms which have been used in this paper. Section 3 introduces the design and the methodology used in the experiments, while
Section 4 is devoted to the results of those experiments. Next, Section 5 deals with the related work, and finally Section 6 provides conclusions and points out possible extensions of this work. 2. Constrained clustering
Constrained clustering algorithms enable the introduction of some domain knowledge into the clustering process in the form of pairwise constraints over the data instances. These constraints bear the input of the user on whether the data points affected by them should (what we will call positive constraints) or should not (what we will call negative constraints) be cat-egorised in the same cluster by the constrained clustering algorithm, depending the degree in which they are honoured on the type of constraints and on the algorithm used.
 Next, we will survey four different approaches to constrained clustering: Constrained k-Means ( Wagstaff et al., 2001 ), Soft Constrained k-Means ( Ares et al., 2009 ), Constrained Normalised Cut ( Ji et al., 2006 ), and one based on the work of Kamvar,
Klein, and Manning (2003) . 2.1. Partitional algorithms Both Constrained k-Means (CKM) and Soft Constrained k-Means (SCKM) are based on the skeleton of k-Means (KM,
McQueen, 1967 ), an iterative clustering algorithm which is one of the most popular ones due to its simplicity and good per-formance, which enables its use in large datasets.

In the case of CKM, the criterion used by KM to assign data instances to clusters (each instance is assigned to the cluster with the most similar centroid) is altered to enforce absolute (i.e. all of them have to be honoured to have an acceptable clus-tering) positive and negative constraints (Must-link and Cannot-link, respectively). To enforce Must-links, a datapoint is di-rectly assigned to a cluster if in this iteration a data instance with which the datapoint is linked by one of them has been assigned to that cluster. Otherwise, and in order to enforce the Cannot-links, the data instance will be assigned to the cluster with the most similar centroid, excluding those clusters which contain a datapoint linked by a Cannot-link constraint with the one being assigned.

The absoluteness of the constraints, reflected in those changes in the assignment policy, and the way in which the con-straints are enforced (which entails a great sensitivity of the clustering results to the order in which the data instances are in-spected)canresultinproblemsintheclustering.Astheauthorswarnintheirpaper,amoderatenumberofnegativeconstraints can render a clustering impossible if a datapoint cannot be assigned to any cluster due to having a Cannot-link with data in-stances in each cluster. This stagnation of the algorithm might not have happened if the data points were inspected in a differ-ent order. Moreover, all the data points connected by Must-links will be assigned blindly to the cluster where the one which was first inspected was assigned, without taking into account their similarity with the centroids of the clusters. Albeit we are points could have ended up in a completely different cluster, which, on average, could be a better clustering for them.
In order to overcome these problems while maintaining a similar framework, Ares et al. proposed the SCKM algorithm, an extension to batch k-Means which, as well as including the Must-link and Cannot-link constraints introduced by Wagstaff et al. includes two new kinds of positive and negative constraints. These new constraints are May-link and May-not-link, meaning that two data instances are/are not likely to be in the same cluster. Unlike Must-link and Cannot-link, they are not absolute, that is, a certain clustering will be still acceptable if any of these constraints is not respected by it.
Hence, SCKM proceeds as follows when assigning a datapoint to a cluster. First, the eventual absolute constraints affecting the datapoint are accounted for. If the point is not assigned straightforwardly because of a Must-link, a score is calculated for all suitable clusters (i.e., those not containing a datapoint with which the present one is linked by a Cannot-link). This score is initialised with the similarity between the datapoint and the centroid of the cluster, and it will be modified depending on the soft constraints affecting the data instance. Namely, the score of a cluster is increased a certain amount w for each datapoint which was last assigned to that cluster (whether in this or in the previous iteration) and has a May-link with the data in-stance being assigned. If the constraint is a May-not-link, the same amount w is subtracted from the score. After this process, the data instance is assigned to the cluster with the highest score.

This strategy, along with the non-absoluteness of the constraints, greatly reduces the dependency of the results on the inspection order of the data points. In the case of the positive constraints, the datapoints linked by May-links are not directly assigned to the same cluster as the one of them firstly examined. Also, the algorithm takes into account the destination clus-ters of the entire set of linked datapoints when one of them is assigned, not only those assigned before it in the iteration, thereby easing the dependency on the inspection order. This also applies to negative constraints, where the possibility of a stagnation of the clustering process is also avoided regardless of what the number of May-not-links might be, as they are enforced reducing the score of the cluster, instead of vetoing them. 2.2. Spectral clustering algorithms
Spectral clustering algorithms ( Ding, 2004; von Luxburg, 2006 ) use graph spectral techniques to tackle the clustering problem transforming it into a graph cut problem. Thus, finding a good clustering of the data in k clusters (one with high similarity between data points assigned to the same cluster and consequently low similarity between points in different clusters) can be reformulated in terms of finding a good cut of a weighted graph where each vertex corresponds to a data point and the weight of an edge is proportional to the similarity between data points, that is, a partition of that graph in k connected components (corresponding each one of them to a cluster) such that the weights of the edges that join vertices in different connected components are minimised while maximising the weights of the edges between vertices in the same component. There are several functions which express this objective. One of the most popular is Normalised Cut (NCut, Shi &amp;
Malik, 2000 ), defined in a way such a cut of the graph with a low NCut value corresponds to a good (as defined above) clus-tering of the data. Hence, the Normalised Cut (NC) algorithm proceeds building the graph from the data and finding a cut of it with a small NCut value.

It can be shown that the minimisation of NC can be presented as a matrix trace minimisation problem ( Shi &amp; Malik, 2000; von Luxburg, 2006 ), which, if subject to some constraints, would yield the exact solution. Unfortunately, these same constraints make solving the problem NP-hard, and so they have to be relaxed in order to make the algorithm computa-tionally affordable. With this relaxation, the datapoints are projected in a reduced space ( R k , where k is the desired num-ber of clusters) using the smallest k eigenvectors of a Laplacian matrix of the graph. Given these projections, some technique (such as k-Means) is used to find a discrete segmentation of this space. Once this segmentation has been per-formed, we can backtrace each projected datapoint to the original one, obtaining the final outcome of the NC clustering algorithm.

In Ji et al. (2006) the authors proposed a Constrained Normalised Cut (CNC) algorithm which introduces non-absolute positive constraints in NC. In order to do so, they altered the function minimised in the NC algorithm to obtain a new one, such that the cut of the graph which minimises this function would convey a grouping which is still a good one but also tries to respect the constraints supplied by the user. To achieve this, they built a new matrix which encodes positive con-straints and introduced it in the core of the minimisation problem, controlling the degree of enforcement of the constraints with a parameter b , with higher values of this parameter meaning a tighter enforcement. Again, the results of the minimi-sation is a projection of the points in R k , and so a segmentation of the projected datapoints has to be performed in order to produce the final clustering of the data.

Unfortunately, the CNC algorithm proposed by Ji et al. can only accommodate positive constraints. As the negative infor-mation also plays an equally important role in real world problems (and can even be the only kind of information available in certain domains) a study of the robustness to noisy negative constraints of a spectral-based algorithm is very interesting. In order to do so, we will use in our experiments a schema analogous to the one proposed in Kamvar et al. (2003) , which lets the user introduce both positive and negative information. Following their approach, in the algorithm used in the experiments in this paper, which we have dubbed NCIC (Normalised Cut with Imposed Constraints), we alter the weights of the edges of the graph which join data instances involved in Must and Cannot-link constraints, setting them to the maximum and the min-imum possible value, respectively. Afterwards, the process follows the same steps of the basic Normalised Cut algorithm, but using these modified weights, thus enabling the use of positive and negative constraints in a Normalised-Cut-based algorithm. 2.3. Comparison of computational costs
In relation to costs, the partitional algorithms outperform spectral algorithms both in time and space. In terms of time, the cost of KM can be estimated as O ( ikn ), where n is the size of the dataset to cluster, k is the desired number of clusters and i is the number of iterations which the algorithm takes to converge. In constrained algorithms there is an obvious penalty in the form of searches in the constraints lists, and so the computational cost can be estimated to be O ( iknc ), where c expresses the difference in cost between only comparing a datapoint with a centroid and searching also which constraints affect the point.
However, the overhead introduced by this step is not dependent on the size of the collection of data instances, and is in fact in most cases negligible compared with the cost of comparing data points and centroids. Thus, the cost in time of KM, CKM and SCKM can be arguably assumed in total to be linear with the size of the dataset, while in NC, CNC and NCIC just the graph construction phase is O ( n 2 ) by itself, having to add to that estimation the cost of solving the eigenproblem.
Regarding memory needs, the spectral algorithms are also costlier than the partitional ones, as, while both families of algorithms need to have the representation of the datapoints in memory, the spectral algorithms need also to store the graph, namely a matrix containing the weights. In both cases, the addition of the constraints does not change this situation. Finally, Table 1 summarises the clustering methods that we have surveyed in this section.
 3. Experimental design and methodology
As previously stated, the behaviour of constrained clustering algorithms when some of the constraints are not accurate is of great importance to assess their suitability in realistic scenarios, where a considerable amount of the constraints would come from user X  X  input (which could be prone to errors) or from automatic constraint extraction techniques (which would tend to generalise rules over features of the data instances which may not be always valid).

In order to study this behaviour, in the experiments reported in this paper we have supplied our own implementations of the algorithms exposed in Section 2 with a synthetically created combination of accurate and inaccurate constraints, exam-ining the evolution of the quality of the clustering outcome as the ratio of the later is increased. To create the inaccurate constraints we have followed two different strategies, one in which the constraints were taken randomly and another in which they were chosen using a more realistic noise model. Also, in our experiments we have separated the results with positive and negative constraints in order to be able to compare the effect of spurious constraints over the two types of information. 3.1. Collections
In the experiments we have used five datasets: (i) WebKB Universities. This text dataset was created as a subset of WebKB X  X  Universities Dataset. The original WebKB (ii) WebKB Topics. The same dataset as (i), but this time distributed in five groups, corresponding to the topics  X  X  X ourse X  X , (iii) Vehicle Silhouettes. This numeric dataset was created in the Turing Institute (Glasgow, Scotland) extracting certain (iv) News 3 Related. This dataset is a sample of three categories of the 20 Newsgroups collection ( Asuncion &amp; Newman, (v) Letter AOZ. This dataset is a subset of the Letter Recognition dataset ( Asuncion &amp; Newman, 2007 ), taking the examples
This selection of datasets covers an interesting range of scenarios. As was pointed out by Basu et al. (2004) , the clustering of small datasets comprised by sparse high-dimensional data is notably difficult, as the clustering algorithms are more prone to fall in local minima. This is the case of our textual datasets, (i), (ii) and (iv), as they are composed by sparse datapoints with very high dimensionality (as is usually the case with data points representing text documents, where each dimension stands for a term of the collection) and contain a small number of data instances (compared to that high dimensionality). Datasets (iii) and (v) are numeric datasets, with dimensionalities much smaller than those of the textual collections. 3.2. Data representation
To represent the textual documents of the WebKB collection we have used Mutual Information (MI), as it has been shown ument d of a textual collection with m terms and n documents is represented by a vector where each mi ( d , t ) is the pointwise mutual information of term t and document d (Eq. (2) ). There, N  X  tf ( d , f ) is the frequency of the term t in the document d .
In the case of datasets (iii) and (v), as the data is already numerical, each datapoint was represented directly by vectors in respectively R 18 and R 16 , which were created concatenating the 18 features of the points in the case of (iii) and the 16 fea-tures for (v).

Both in the partitional (in the comparisons with the centroids) and in the spectral algorithms (to set the values of the weights of the graph and in the segmentation of the projected data points) the similarity measure used between the vectors representing the data points was the cosine distance. 3.3. Creation of the constraints
As previously mentioned, the objective of the experiments conducted in this paper is to study the behaviour of con-strained clustering algorithms under more realistic circumstances, where not all the domain knowledge supplied to them is accurate. To do so, sets of constraints with a growing ratio of false (non-accurate) constraints were created from the ref-erence grouping used as clustering ground truth. The initial set of truthful constraints was created by randomly selecting pairs of data instances which belonged to the same cluster (to create the Must-links) or to different clusters in the reference (to create the Cannot-links) in the reference grouping.
 To create the non-accurate constraints we have followed two different strategies.

In the first one (which we have identified by  X  X  X ND X  X  in the results) the spurious constraints were created randomly, in a process similar to the one carried out to create the accurate constraints, but this time reversed. That is, in this approach, the false Must-links were created by randomly selecting pairs of data points which belonged to different clusters in the reference grouping and the false Cannot-links were created by selecting pairs belonging to the same clusters.

Even though it can be used as a first approximation to model a real world constrained clustering problem, with this strat-egy the spurious constraints are evenly distributed among all the possible ones, a fact which can be argued to be unrealistic. Thus, with our second strategy we have tried to devise a more realistic approach.

In this second constraint generation strategy (which we have identified by  X  X  X IM X  X  in the results) the inaccurate con-straints were created using the similarity between data instances. What we are capturing with this approach is the intu-ition that the errors in the constraints are likely to happen between pairs of data points which do not seem to belong in the same cluster, in the case of erroneous negative constraints, or in different ones, in the case of erroneous positive constraints.

Concretely, we think that, when a human user (or an automatic constraint extraction system) is creating positive con-straints, the possible spurious ones will be concentrated between datapoints which, belonging in different clusters, are very similar. Thus, to create the spurious Must-links constraints we have followed two steps: first, all the similarities between pairs of data points belonging to different clusters in the reference grouping are calculated (see Section 3.2 ). Then, the pairs with the highest similarities are chosen and a Must-link constraint is created between them. These constraints will be spu-rious with respect to the reference grouping, as we are only considering pairs of points which are in different clusters in it.
In the case of the inaccurate negative constraints, the rationale is similar: they will be concentrated between datapoints which, belonging in the same cluster, are very dissimilar. Hence, the process to create them is analogous, but this time using the lowest similarities between datapoints which are in the same cluster in the reference grouping. As it stems from these steps, the spurious positive and negative constraints created using this second approach will be the same in all the initialisations of the constraints (but not in the case of the accurate constraints, which are generated in the way explained above).

The initial amount of true constraints used in the experiments depended on the kind of constraints used. For the Must-links it was 1% of the possible truthful positive constraints for each collection. For the Cannot-link constraints, due to their lesser informativeness, it was 5% of the possible negatives, in order to have in all algorithms a starting point which improved perceptibly the effectiveness of their non-constrained counterparts. As for the non-accurate constraints, ten different amounts of them were tested, starting with 10% of the truthful constraints and ending with the same amount of true and false constraints, with steps of 10%. Finally, the transitive closure of the constraints was only performed when they were fed to Constrained k-Means, as it is entailed by the absoluteness of the constraints. For the other clustering algorithms no transitive closure was used. 3.4. Metrics
In order to assess the effectiveness of the different clustering algorithms we have compared the outcomes of the algo-rithms ( X ={ x 1 , x 2 , ... , x k }) with the reference groupings  X  C  X f c three metrics show the same trends, only the results for Adjusted Rand Index (ARI) are presented in this paper. This metric measures the ratio of good decisions made by the algorithm over a collection of n data points on a pairwise basis (Eq. (3) ). It is based on Rand Index ( Rand, 1971 ), correcting certain deficiencies of that metric, namely, it is corrected for chance. To do so, the expected value of the index is subtracted from the unadjusted index and the result is divided by the maximum value of the index (from which the expected value has been subtracted as well). Higher values of Adjusted Rand Index indicate a greater similarity between the results and the reference. 3.5. Experiment settings
In the experiments we have considered that the number of clusters ( k ) in the grouping used as reference was known, and so the number of desired clusters was set to that amount in each of the tested clustering algorithms.

Also, in our experiments we have detected that the quality of the results of the spectral methods improved considerably (2005) reported a similar behaviour over other collections). Hence, in all the spectral clustering algorithms (constrained and unconstrained) we have used in each collection the number of eigenvectors which yielded the better results in Normalised Cut (see Table 2 ).

This situation can be attributed to a wide array of factors. For instance, in the textual collections the documents in each cluster are quite heterogeneous, and so using a small number of eigenvectors cannot capture all the information needed to decide to which cluster a document should be assigned. The comparison of the number of eigenvectors used for datasets (i) and (ii) offers some evidence supporting this hypothesis: even though in both cases the underlying documents are the same, when they are grouped by universities (and so each cluster is more heterogeneous, as it contains documents from all the five topics) the best number of eigenvectors is much bigger than the best number when using the grouping by topic. Further-more, other important factors which could help to explain this circumstance, both in textual and numerical collections, are the relatively small number of desired clusters (so that keeping only k eigenvectors would cause a loss of information) or even the process used to build the graph, in which we did not prune any edge (i.e., it was totally connected), even if the associated weight was small. However, it should also be noted that taking too many dimensions is in fact harmful to the effectiveness of the algorithms, as it only adds noise to the projected datapoints.

Thus, the only parameters left to be set are the strength of the constraints ( w ) in SCKM and the degree of enforcement of the constraints ( b ) in CNC. We have tested several values in order to show their effect over the performance of the algorithms in an environment affected by noise in the constraints.

Due to the dependency of the k-Means based algorithms on the initial set of seeds we tested five different initialisations of them, created taking data points randomly from the data to cluster. These same sets of seeds were used in the clustering of the projection of the data points in NC, CNC and NCIC, as we have used k-Means to carry out that process.

Moreover, five different sets of constraints were tested in order to have a better representation of the behaviour of the algorithms. Also, in each initialisation the order in which the data points were examined was randomised. Hence, each re-ported result is the average of twenty-five different groupings (five initialisations of the seeds and five initialisations of the constraints). 3.6. Statistical significance
Finally, we have assessed the statistical significance of the results of the experiments using the Sign Test ( Conover, 1971 ), a choice which was motivated by its reduced number of assumptions about the data in comparison with other tests such as
Wilcoxon X  X  or Student X  X  t . The results of each constrained method were compared with its baseline, that is, CNC and INC were compared with NC and SCKM with KM.

For a given amount of false constraints, five observations ( X the seeds, where X i is the ARI of the non-constrained method and Y the five initialisations of the constraints. Over these observations we performed a Lower-Tailed test, where the null hypoth-esis was H 0 : P (+) P P ( ), i.e., that the values X i were greater or equal to Y baseline was greater or comparable to that of the constrained method), and the alternative hypothesis was H 4. Results
Figs. 1 X 5 show the results obtained by the algorithms analysed in each dataset. In the case of CNC and SCKM, only a few runs (those corresponding to the most interesting values of each algorithm X  X  parameter) are shown. The results of the uncon-strained methods (KM and NC) are also plotted as dotted straight lines.

In each figure, the top graphs show the results with positive constraints and the bottom ones the results with negative constraints, while the graphs on the left show the results with the first constraint generation method (RND) and the ones on the right the results with the second method (SIM). In each graph the horizontal axis shows the ratio between the non-accu-rate and the accurate constraints, with the points in x = 0 indicating the values without any false constraints. The exact num-ber of accurate constraints used in each experiment is shown in the title of each subfigure. Table 3 shows the results achieved by the non-constrained versions of the algorithms analysed, while Tables 4 and 5 show the results of the algorithms analysed without inaccurate constraints (i.e., the initial point in the graphs) for the values of the parameter of each method shown in the figures.

As a preliminary note, our experiments showed the enormous problems faced by Constrained k-Means with a moderate number of constraints, which in most cases make clustering impossible. These problems were not limited to a stagnation of the algorithm when using negative constraints (see Section 2.1 ) but also appeared when using positive constraints: due to the effect of the Must-links, occasionally one or more clusters were empty after the datapoints were assigned, which made the recalculation of the centroids and the progress of the algorithm impossible. It should be noted that CKM is a deterministic algorithm, that is, clustering the same documents with the same constraints and the same seeds will always yield the same results, following the exact same process. Hence, running the algorithm again without altering any of those factors would be of no use to prevent these problems. As we have considered that altering the seeds or the constraints due to CKM X  X  behaviour would be unfair for the other clustering algorithms, both situations, with Must-links and Cannot-links, were treated as errors in the clustering process. As these errors affected many runs, we have chosen to discard the results of CKM and to not show them in the tables or graphs. Anyhow, we were able to extract some interesting insights from the runs which were con-ducted successfully (mainly with positive constraints). While the initial (i.e., without inaccurate constraints) quality values were in those cases high (comparable to Constrained Normalised Cut), they dropped very fast as false constraints were intro-duced. We think, in tune with what happens with the Chunklet Method in Nelson and Cohen (2007) , that this is due to the absoluteness of the constraints and the inherent transitiveness that they entail, which multiplies the effect of the inaccurate ones.
 Commenting on the results of the algorithms, one of the most remarkable among them is the improvement attained by Constrained Normalised Cut over the results of the non-constrained method in all datasets, which can be observed comparing the values of NC and CNC in Tables 3 and 4 . The decrease in quality as more inaccurate constraints are added is more marked for the higher values of b , with the runs made with the lower values of that parameter (which controls the degree of enforcement of the constraints) showing that with a less tight observance of them the algorithm is able to maintain good results (being better than Soft Constrained k-Means) until the noise reaches levels which could be deemed unrealistic for a lot of real world applications. In fact, using the similarity-based constraint generation strategy this decrease in the qual-ity values is so greatly softened that even for the highest ratios of bad constraints the baseline is still improved. Our intuition about the causes of this trend (which appears in the runs of all the algorithms, with both positive and negative constraints) is given at the end of this section.

As for the Soft Constrained k-Means, if we inspect Tables 3 X 5 we can see that, when using positive information, the ori-ginal improvement over the non-constrained algorithm (k-Means) is more modest than in the case of CNC. Even so, in the text datasets, (i), (ii) and (iv), this improvement over its own baseline is kept until the highest noise levels for the lowest value of w . Moreover, with the first constraint generation strategy, in several cases the results of SCKM also outperform unconstrained NC with moderate amounts of inaccurate constraints, a situation that with the SIM strategy lasts until the highest ratios. This resistance to noise can be attributed to how SCKM incorporates the constraints (see Section 2.1 and Ares et al., 2009 ). Without noise, when a data instance is assigned the constraints will boost the score of one cluster (the one con-taining most of the data points which should be in the same cluster as the point being assigned). But, as the ratio of inac-curate constraints grows, the differences between the modifiers applied to the clusters will be smaller, due to the noise in the constraints. Hence, the accurate and the inaccurate constraints will be cancelling each other, and the assignment would rely again mostly in the similarities between data points and centroids.

When SCKM is used with noisy Cannot-links created with the RND strategy the situation is slightly different: whereas the improvement over KM is greater (due to the larger number of constraints), the fall in the quality values is much sharper. This happens because, for a certain datapoint, the true Cannot-links affecting it will be evenly distributed between datapoints in the other clusters. However, its false constraints will be all with points in the same cluster, and so, a small number of them is enough to counteract the effect of the true ones. From that point on, adding more false Cannot-links significantly harms the quality of the clustering. Following the global trend, if the noisy constraints are generated with the SIM strategy the problem is greatly eased.

In our experiments, the approach which we have dubbed Normalised Cut with Imposed Constraints has shown an irreg-ular behaviour when using only good constraints. With positive constraints, in the case of datasets (i) and (iv) NCIC improves NC noticeably, obtaining lesser improvements in datasets (ii) and (v) and a non-existent one in dataset (iii), where using
NCIC to incorporate the constraints is actually harmful. Using true negative constraints, the situation is the opposite. NCIC being slightly inferior to those of NC. This situation is caused by how NCIC uses the constraints. This algorithm changes the weights of the edges joining vertices representing datapoints used in positive and negative constraints to respectively the maximum or minimum possible similarity values (namely 1 and 0, using the cosine distance). In the case of datasets (i) and (ii) (which are composed by the same datapoints, but distributed according to different criteria) and (iv) the similarities are very low, which amplifies the effect of the Must-links and reduces the effect of Cannot-links. In datasets (iii) and (v) the similarity values are very high, and so the impact of the Cannot-links is boosted and the effect of Must-links is lessened. As for why NCIC results with constraints are in some cases worse than without them, we think that this is caused by the change of weights induced by the constraints greatly altering the original similarity space and deeply affecting the relations between the datapoints such that the projection made by the algorithm is not able to represent faithfully neither the similarities be-tween datapoints nor the constraints. However, further experiments should be conducted in order to test this hypothesis. A similar situation (accurate constraints being not beneficial in a constrained clustering process) has been previously reported by Davidson, Wagstaff, and Basu (2006) .

As for the behaviour of NCIC under noisy constraints, it also follows the same irregular behaviour indicated in the previ-ous paragraph. Thus, as with the accurate constraints, when the difference between the similarity values and the value with which they are substituted is large (Must-links in (i), (ii) and (iv), Cannot-links in (iii) and (v)) the effect of the inaccurate of the noise are attenuated. This is confirmed by the differences found in the behaviour of the algorithm depending on the false constraint generation method used. Following the trend appreciable in the other analysed algorithms, the fall in NCIC X  X  performance is smaller when the inaccurate constraints are generated with the SIM method. This is because with this meth-od the pairs of data points with the highest values of similarity are used to build the false positive constraints (and the pairs with the smallest ones for the false negatives), and thus the actual changes caused by these inaccurate constraints over the weights of the graph are smaller in this case.

This last result offers also an explanation of why all the analysed algorithms are more robust to the false constraints cre-ated with SIM than to those created with RND. With this last approach the bad constraints will be evenly distributed, but with the first one, following our intuition about which errors a user or an automatic constraint creation scheme are more likely to make, they will be concentrated over pairs of data instances which belonging to different clusters are very similar and vice versa. Thus, we are introducing false constraints between pairs of data instances over which the algorithms them-selves were already prone to make errors. Consequently, the results point out that, in a real world situation, if the inaccurate constraints are a product of misjudgements caused by the similarity between datapoints (i.e., not because of systematic er-rors) their effect might be lessened (but apparent nonetheless). 4.1. Statistical significance
As for the statistical significance (see Section 3.6 ) Tables 6 and 7 show, starting from the point where all the constraints are accurate, the largest ratio between bad and good constraints for which the improvement of the constrained method over the non-constrained one is still significant, that is, the null hypothesis is rejected for a p -value that parameter was not tested for that method, while  X  X  X  X  X  means that none of the combinations between accurate and inac-curate constraints yielded a significant improvement over the baseline (not even using only accurate constraints).
Overall, this statistical significance data shows the same trends as the figures and tables commented upon before, as in most cases the last significant point is the one before the constrained method falls below its baseline, which supports the analysis that we have made in this section. However, there are still some peculiarities worth noting.

It is well known that the results of the k -Means algorithm and the algorithms based on it are quite sensitive to the seeds of the clustering. In this paper we have worked with random initialisations of the seeds, relying in the smoothing effect of averaging over several ones to have a faithful representation of the effectiveness of the algorithms. However, the Sign Test compares individually the results of each constrained method and its baseline for each initialisation, and hence the effects of the aforementioned dependency are in some cases revealed. This is readily apparent in the values of the results for dataset (iii), for which in several cases the accurate constraints are not able to attain a significant improvement. Another example is
SCKM with positive constraints, where the last significant improvement is usually a bit earlier than the point for which the results of this method are better in average than those of KM (for this algorithm the problem of the dependency on the seeds is also aggravated by its dependency on the order in which the data points are examined). Even so, it should be noted that the improvements are still significant until a considerable amount of false constraints are added. 5. Related work
As previously indicated, the vast majority of the literature in experimenting with constrained clustering (including the papers where the algorithms analysed in our work were introduced) uses constraints extracted directly from the reference grouping which was used in the evaluation. Thus, these constraints were all accurate, that is, they do not deal at all with the problem of the robustness of the algorithms to noise.

To the best of our knowledge, this problem has only been thoroughly addressed by Nelson and Cohen (2007) . Their work tests the performance with noisy constraints of three existing probabilistic constrained clustering algorithms ( Lange et al., 2005; Lu &amp; Leen, 2004; Shental et al., 2003 ) and another one introduced by them in the paper (the Sampled Chunklet Algo-rithm). These constraints are extracted from the reference grouping, taking randomly pairs of data points and mislabelling a portion of them (i.e., turning some true Must-links into Cannot-links and vice versa), an approach very similar to the one which we have labelled RND in our experiments. One interesting feature of Nelson and Cohen X  X  constraints is that the degree of confidence of the source of the constraints in each piece of advice is a probability in a strict mathematical sense. This prob-ability is used in the experiments to set the penalty associated to violating the constraints (i.e. their weights), directly in their algorithm and using a result derived by them in the paper in the other methods. Hence, the weights created from these prob-abilities have the great advantage over other approaches of having a clear interpretation. The results of the experiments underline the importance of taking into account the degree of noise in the constraints when adjusting the adherence of the algorithms to these pieces of advice. Moreover, and related to this, they also point out the huge problem posed by ap-proaches which use absolute constraints (and of the transitive closure entailed by them), as their effectiveness drops dramat-ically when the constraints are noisy. A similar consideration was made by Basu et al. (2004) , who if the constraints are known to be noisy recommend not using their transitive closure.

The two important insights described above are also apparent in the results of our experiments, which were conducted over completely different families of algorithms: they deal with probabilistic clustering algorithms, while our study is cen-tred in spectral and flat partition algorithms. Moreover, even though the weights of the constraints used in our experiments lack such a clear interpretation, we have used in our analysis a more realistic noise model. Finally, Nelson and Cohen use in their paper only small numeric datasets, while we carried out our experiments also over textual datasets, which are more challenging due to their nature, as they result in very sparse data points.

Another interesting paper is Coleman et al. (2008) , in which the authors study how to deal with inconsistent advice in the framework of spectral clustering. They devise three different methods, based of combining solving a 2-correlation clustering problem over the constraints and a spectral clustering problem (Normalised Cut) over the datapoints. The basic difference between the three approaches is the degree of freedom that they grant the algorithm to ignore the advice given by the con-straints. They test the methods over six numeric datasets, using two different kinds of synthetic inconsistent advice, one con-centrated over a small subset of the dataspace and the other spanning all the pairs of datapoints, agreeing each constraint independently with the actual classification with the same probability (again similar to the RND approach of the present paper). The results of their experiments hint again that, when the noise in the advice rises, more freedom has to be given to the algorithm to ignore it in order to achieve good results. However, it is worth noting that the authors admit that their work is basically theoretical, and that they focus on obtaining only two clusters from the data. Hence, they assert that more experimental work is needed in that area.

As it stems from the previous descriptions of the related papers, to the best of our knowledge this is the first time in which the problem of how to create realistic noise in constraints is addressed. In the field of classification, Esuli and Sebastiani (2009) proposed an approach to create realistic false positive and negative examples of a class using the confidence of the classificator on whether they belong or not to it, arguing that the examples which the classificator catalogues with lowest certainties are the ones that a human annotator is more likely to misclassify due to lack of experience or time. Among the results presented in that paper, Esuli and Sebastiani report that this realistic perturbation is less damaging to the effective-ness of the classification task than the random one, a result in consonance with the one reported in the constrained cluster-ing task in the present paper. 6. Conclusions and future work
In this paper we have tested the behaviour of four different constrained clustering algorithms when the information sup-plied to them is not entirely accurate. To do so we have carried out a series of experiments in which their effectiveness is measured as the ratio of inaccurate constraints (which were generated using two different approaches) increases. Whereas one of the algorithms, Constrained k-Means, showed its unsuitability with a high number of positive or negative constraints, we have found that, as expected, each one of the other methods showed different features which point out the scenarios where employing it might be the soundest election.

When using positive constraints (the only kind of constraints that it can use) Constrained Normalised Cut was the most effective constrained clustering algorithm under the initial conditions (i.e., without inaccurate constraints). For the smallest values of the parameter b when the false constraints were added the effectiveness was still good in most cases until reaching high ratios of inaccurate constraints. Hence, CNC would be the best option when the amount of false constraints is moderate to high and the computational cost (both in time and space) is not a crucial issue.

For its part, while Soft Constrained k-Means improves its baseline (k-Means), it usually performs worse than CNC when using positive constraints, and even in most cases its effectiveness with higher ratios of inaccurate constraints is worse than
Normalised Cut, which is a non-constrained method. These results suggest that, when using positive information, SCKM should be used only if the computational cost is critical. In the case of negative constraints, its effectiveness without false constraints was very good, reaching very high quality values. When bad negative constraints were added using the more realistic approach, the quality of the results was still very good until the highest noise levels, which shows that SCKM is an algorithm very suitable to incorporate negative information into clustering, even without restrictions on the spacial or temporal costs of the process.

The Normalised Cut with Imposed Constraints algorithm showed an irregular behaviour in the experiments, with initial results which, in the best case, do not outperform the other methods and in the worse case are actually below its own base-line. Even though the degradation of the results when adding inaccurate constraints was remarkably low, this circumstance causes NCIC to be outperformed by NC and SCKM both with positive and negative noisy constraints (except with the highest noise levels when the constraints are generated with the  X  X  X andom X  X  strategy). Moreover, this algorithm does not have any advantage in terms of cost over the other methods. The conjunction of these results recommends that using NCIC in real world clustering problems should, at the very least, be considered very carefully.

On a more general level, the results of our experiments reinforce the idea, pointed out by Nelson and Cohen (2007) , that the degree of truthfulness of the constraints should be taken into account when tuning the clustering algorithms, adjusting the observance of the constraints required to the algorithm according to the suspected ratio of inaccurate constraints in the information available. Moreover, the comparison between the results obtained with the two different false constraint gen-eration methods suggest that, when the errors in the constraints are a product of misjudgements induced by high or low similarity between datapoints, the effect of the inaccurate constraints is likely to be lessened.

Precisely, we think that the study of the noise models offers plenty of room in terms of future work, such as devising dif-ferent approaches to find the pairs of datapoints between which a spurious constraint is more likely to appear. Moreover, it would be very interesting to perform studies in real world scenarios to characterise the kind of errors a user or an automatic constraint extraction system is more prone to fall into, in order to be able to conduct more useful studies.
In conclusion, we believe that studying the behaviour of the constrained clustering algorithms in noisy scenarios is a very important issue, and that more in-depth studies should be conducted in order to have a more complete understanding. Acknowledgements This work was funded by Ministerio de Ciencia e Innovaci X n from the Spanish Government under Project TIN2008-06566-
C04-04. The first author wants also to acknowledge the support of Ministerio de Educaci X n from the Spanish Government un-der the FPU Grant AP2007-02476.
 References
