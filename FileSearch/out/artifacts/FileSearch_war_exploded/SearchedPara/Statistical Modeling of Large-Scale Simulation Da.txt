 I Simulation ] I Ad-Hoc 
In this paper, we describe and evaluate two statistical modeling techniques for AQSirtL The first model captures the "true" (unbiased) mean of systematic partitions of the data. We call this model the mean modeler. The mean modeler has two main advantages. First, it makes no assumptions about the distribution of the data. Second, it calculates its model parameters through 
Our second model captures the normality of systematic partitions of the data by utilizing the Anderson-Darling goodness-of-fit test [5]. This model is called the goodness-of-fit modeler. Similar to the mean modeler, the goodness-of-fit modeler is able to calculate its model parameters through one sweep of the data. However, error on this model is the Type I error associated with the goodness-of-fit test. Section 2 describes these two approaches in details. 
Despite their simplicity, these models have performed extremely well on our empirical studies of range queries (see Section 4). 
The answer to a query is judged by its weighted average precision associated with a query's answer by scaling the one-sided 
Chebyshev inequality with a metric representing the topology of the original mesh in that partition [6]. We chose to utilize the one-sided Chebyshev inequality since it does not make any assumptions about the data. Section 3 describes AQSim's query processor and our precision measure in details. 
Section 4 presents two case-studies, which illustrate the value of our approach. Sections 5 and 6 discuss some related and future work, respectively. Section 7 summarizes our work. paper. Figure 2. A Mesh Data Set Representing a Star with either 1 data point or a set of data points with standard deviation of zero. In the second stopping criterion, the partition user-defined threshold is a scaling factor for the standard deviation of variable v. For example, c = 1 means that the minimum and maximum values for each non-partitioning variable does not assume any distribution on the data points. For the mean modeler, standard deviation is the same as RMSE (root mean square error) since the true mean, which is an unbiased estimator, is used as the model. Thus, the RMSE is the error metric associated with the mean modeler. mean, ,u~, and standard deviation, oi. For the goodness-of-fit modeler, the partitioning step stops when the hypothesis test for normality is not rejected. We use the Anderson-Darling test for normality (which is considered to be the most powerful goodness-of-fit test for normality) for our goodness-of-fit test [5]. The Anderson-Darling test involves calculating the A 2 metric for variable vi ~ N(gi, oi), which is defined to be where n = number of data points for vi and zj = O( ). 0(.) is the standard normal distribution function. We reject tto if A 2 1 + 0.75n +-~-exceeds the critical value associated with the user-specified error threshold [5]. Otherwise, we accept Ho. For each variable v, the error on this model is defined to be words, the model error is equal to the Type I error. AQSim's query processor takes a user's query and the amount of running time is less than the user-defined time limit, the query processor searches the hierarchical partitions (which were made by the model generator) for those partitions that contain highly precise models for the given query. the answer that modelj of partitioni would produce for the query, Q, as a percentage of partitionfs mesh topology) Specifically, Precision(Q, modelj, partitioni) = Filled_Volume(partitioni) x P(Q, modelj, partitioni), where Filled_Volume returns the percentage of non-empty space in the given partition's spatial bounding box and is defined to be Filled_Volume(parentpartition) = # of chilflren child=l where Filled Volume(leaf_partition) = 1. P(Q, modelj, partitioni) is calculated by using the one-sided Chebyshev inequalities [6], which are defined to be X is a random variable with mean,u and variance o ~. The variable a is a real number. The advantage of using the Chebyshev inequalities is that no assumption is made on the distribution of the data in a partition. For example, suppose we are given the -0.5), the precision is equal to For more complicated queries, we make new random variables and calculate mean and standard deviation values for them based on the original means and standard deviations. We assume independence between the original variables when calculating the means and standard deviations of the new random variables. For example, suppose we are given the query, (temperature/pressure)_&lt; density. This query is equivalent to (temperature/pressure)-density _&lt; O. We create a new random variable, R, where R = (temperature~pressure)-density. So, our query is now R _&lt; 0. By calculating R's mean and standard deviation, we can use the aforementioned formula, namely Precision(Q, model1, partitioni) = Filled Volume(partitioni)  X  P(Q, modelj, partitioni), to calculate our query's precision. Mean of R, ,us, is equal to E[(temperature/pressure ) -density] = E[( temperature/pressure )] -E[density] = E[temperature].( 1 Vat[pressure/]) To get this formula, we assume that the random variables temperature and pressure are independent. Moreover, we use the formula E[g(X).h(Y)] = E[g(X)].E[h(Y)], where X and Y are two 3 The percentage of a partition's mesh topology corresponds to how well the partifion's bounding box matches the underlying mesh topology. 490 independent random variables. The functions, g and h, are over X and Y, respectively. Finally, we use a lemma from Ross [9], which states the following: "Let Z be a random variable having finite expectation i~ and variance a z. Let g(o) be a twice differentiable function. Then E[g(Z)I = g(Iz) + (g'(g)*0.5*o~). '' equation E[R 2] -(E[R]) 2 and assuming independence between temperature, pressure, and density. Due to space limitations, we have omitted the formula for R's standard deviation. 
To get a single number representing the overall precision of our answer, we compute the weighted average of the precisions (which were calculated on all the explored partitions). In # of explored partitions / \ where mj is set to a particular model (e.g., the mean modeler). 
Our first data set represents a wall crashing a can. 'It has 14 variables, 44 time steps, and 443,872 data points. The variables pressure, acceleration along each axis, velocity along each axis, its first time step when all the 440K points are plotted. 
Partition % of Total # of % of non-Avg. # of 1.00 4.2 425,075 19.4 80.6 1.3 1.50 33.0 297,566 13.4 86.6 1.7 1.75 40.1 265,939 12.5 87.5 1.9 2.00 ' 51.5 215,255 11.1 88.9 2.3 2.25 62.4 166,986 10,3 89.7 3.0 2.50 71.6 125,912 9.6 90.4 3.9 2.75 78.1 97,410 9.1 90.9 5.0 3.00 82.6 77,277 8.6 91.4 6.3 Figure 6. Can Data Set at its First Time Step with Partition modeler. Recall that the partition threshold for this modeler restricts the distance between minimum and maximum values of a variable and its mean value with respect to RMSE. 
For our mean modeler experiments, Figures 6 ti~ough 8 show the with no constraint on the query processor's execution time and expected, we get better compression as the partition threshold for the mean modeler gets larger (since we are allowing the range of Figure 7. Can Data Set at its First Time Step with Partition 
Table 2 lists the compression results on the can data for the goodness-of-fit modeler. The partition threshold in this table represents the confidence region of our normality test, which is equal to 100 x (1 -Type I error). Figure 8. Can Data Set at its First Time Step with Partition 
Table 2. Goodness-of-Fit Modeler's Compression Results % Partition % of Total # of % of leaf Threshold Compression partitions partitions a partition 50.0 39.6 272,583 12.6 87.4 1.9 80.0 57.3 189,533 10.1 89.9 2.6 85 60.9 173,766 9.7 90.3 2.8 90.0 65.8 151,818 9.3 90.7 3.2 95.0 73.7 116,948 8.8 91.2 4.2 99.99 91.4 38,344 7.3 92.7 12.5 
For our goodness-of-fit modeler experiments, Figures 9 through &gt; 0 is posed with no constraint on execution time and with partition thresholds of 99.99%, 95%, and 50% respectively. 
Again not surprisingly, we get better compression as the partition threshold for the goodness-of-fit modeler gets larger (since the confidence region shrinks). However, as you see in Figure 11 even with 91.4% compression, we are able to return a highly precise answer. The weighted average of all the precisions is 100% on the answer to the query, time &gt; O. Figure 9. Can Data Set at its First Time Step with Partition variables, 16 time steps, and 1,708,852 zones. The variables distance, grid vertex values, grid movement along the x and y axes, d(energy)/d(temperature), density, electron temperature, points are plotted. the mean modeler. Again, recall that the partition threshold for this modeler restricts the distance between minimum and maximum of a variable and its mean value with respect to RMSE. 
For our mean modeler experiments, Figure 14 shows the is posed with no constraint on execution time and with partition we get better compression as the partition threshold for the mean a variable to be larger). However, as you see even with 92.1% compression, we are able to return a highly precise answer. The weighted average of all the precisions is 100% on the answer to the query, time &gt; O. Figure 13. Astrophysics Data Set at its First Time Step Partition % of Total # of % of leaf 
Threshold Compression partitions leaf partitions data point iv 1.75 67.4 728,081 17.9 82.1 2.9 2.00 70.1 511,395 17.8 82.2 4.1 2.25 79.7 347,471 17.7 82.3 6.0 2.50 85.8 242,840 18.7 81.3 8.7 2.75 89.6 177,448 19.0 81.0 11.9 3.00 92.1 135,548 17.8 82.2 15.3 
Table 4 lists the compression results on the can data for the is equal to 100 x (1 -Type I error). Figure 14. Astrophysics Data Set at its First Time Step with Table 4. Goodness-of-Fit Modeler's Compression Results % Partition % of Total # of % of non-% of leaf Avg. # of 
Threshold Compression partitions 80.0 85 90.0 95.0 99.99 
For our goodness-of-fit modeler experiments, Figure 15 shows the is posed with no constraint on execution time and with partition thresholds of 99.99%. Again not surprisingly, we get better compression as the partition threshold for the goodness-of-fit modeler gets larger (since the confidence region shrinks). 
However, as you see in Figure 15 even with 94.3% compression, we are able to return a highly precise answer. The weighted Figure 15. Astrophysics Data Set at its First Time Step with Our experimental results illustrate the value of using simple statistical modeling techniques on scientific simulation data sets. Both of our approaches require only one sweep of the data and generate models that compress the data up to 94%. The goodness-of-fit modeler performed better than the mean surprising to us since our two data sets describe physical phenomena and the goodness-of-fit modeler is biased towards such normally distributed data sets. In general, we prefer the mean modeler since it makes no assumption on the data. Our work is similar to Freitag and Loy's work at Argonne National Laboratory [7]. Their system builds distributed octrees from large scientific data sets. They, however, reduce their data the tree at different resolutions. STING [10] is also similar to AQSim except that it assumes that small data sets containing only tens of thousands of data points. AQUA [2] uses cached summary data in an OLAP domain. Unfortunately, they use sampling and histogram techniques, which can remove outliers from data sets. In our experiences, outliers are very important to scientists. Moreover, histograms are computationally expensive on high-dimensional data sets. We are investigating other modeling techniques for AQSim's model generator. Specifically, we are constrained to models that range queries (see [3]) particular, we are investigating techniques which will minimize seek time. In addition, parallelizing AQSim's query processor is part of our future work. Finally, we are conducting experiments on other larger data sets. To help scientists gather knowledge from their large-scale simulation data, we are developing the ad-hoc query infrastructure, AQSim. Our system consists of two components: (i) the model generator and (ii) the query processor. The model generator reduces the data storage requirements by creating and resolutions. The query processor decreases the query access times modeling techniques for simulation data. Our mean modeler computes the unbiased mean of systematic partitions of the data. It makes no assumptions about the distribution of the data and Our goodness-of-fit modeler utilizes the Andersen-Darling Lozares, I., Musick, R., Tang, N.A., Lee, B., and Snapp, R. Approximate ad-hoc query engine for simulation data. In Proceedings of JCDL 2001 (Roanoke VA, June 2001), ACM Press, 255-:256. S. The Aqua approximate query answering system. In Proceedings of the 1999 ACM SIGMOD, ACM Press, 574-576. Resolution Modeling of Large Scale Scientific Simulation Data. LLNL Technical Report, 2002. Approximate query processing using wavelets, In Proceedings of VLDB 2000 (Cairo Egypt, September 2000), ACM Press, 111-122. Techniques, Marcel Dekker, Inc., 1986. the Sciences, 3 rd edition. Brooks/Cole Publishing Company, Pacific Grove, CA, 1991. visualization of large data sets using a distributed memory octree. In Proceedings of SC 1999 (Portland OR, November 1999), ACM Press, Article 60. large-scale computational science, In Proceedings of SIGMOD Record 1999, ACM Press, 28(4):49-57. Ross, S. A First Course in Probability, 4 th edition. Prentice Hall, Englewood Cliffs, NJ, 1994. information grid approach m spatial data mining. In Proceedings of the VLDB (Athens Greece, August 1997), Morgan Kaufmann Publishers, 186-195. 
