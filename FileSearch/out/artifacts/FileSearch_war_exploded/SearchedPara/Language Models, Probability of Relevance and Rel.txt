 This paper proposes a measure of relevance likelihood derived specifically for language models. Su ch a measure may be used to guide a user on how far to browse through the list of retrieved items or for pseudo-relevance feedb ack. To derive this measure, it is necessary to make the assumption that a user is seeking an ideal (usually non-existent) document and the actual relevant documents in the collection will c ontain fragments of this ideal document. Thus, in deriving this measure we propose a novel way of capturing relevance in Language Modelling. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Measurement, Theory. Relevance likelihood, Language Modelling, ranking function. Information Retrieval (IR) syst ems usually order documents according to some ranking function, which assesses relevance in an ordinal way. Here, we propos e that a ranking function derived from a probabilistic model, specifically Language Modelling [7], can be also used to provide a measure of relevance likelihood for term frequency models (as defined in secti on 2)  X  that is, it yields an actual probability rather than merely an ordinal measure based on probability. Such a measure might be used to decide how far a list of retrieved items to search, gauge query difficulty or be used in pseudo-relevance feedback. We concur with Turtle and Croft [12] that  X  an estimate of probability of relevance is more us eful than the ranking by itself  X  but accept that for many IR models this is as yet impossible. We therefore propose a measure of relevance likelihood which gives more information than the ranking by itself. This paper addresses two fundamental questions: (i) wh at does such a measure mean? and, (ii) how might it be calcu lated for language models? The arguments presented herein also s eek to contribute to the debate about the relationship between Language Modelling and relevance [5,10]. This research was funded by the EPSRC Project  X  Interactive Modus Operandi Vi sualisation (iMOV). The ideas presented here arose originally out of analysing text descriptions of crimes from a police digital archive but have much wider applications. The key feature which distinguishes probabilistic models from vector space models is that the latter are based on the notion of similarity between document and query, and the former are based on the probability that a document can be observed to satisfy some information need (however this might be defined). This implies that  X  X atisfying the need X  or relevance is a binary attribute. Here we assume that relevan ce means topicality and does not apply to the accuracy, timeliness or clarity of the text. The justification for using probabilitie s to rank relevant documents comes from the Probability Ranking Principle [9] which states that ordering documents by probability of relevance is the best obtainable ordering for the data available. However, an IR system that just retrieves documents in order of relevance does not require the actual probability. It is sufficient to use a ranking function which preserves the ordering imposed by the probabilities and for practical r easons this is what is done. The problem we face in attempting to create a measure of relevance likelihood from existing m odels is that these models rely on order-preserving transformations to eliminate inestimable variables from the calculation. For example, consider a (very simplified) model which relates the probability of relevance p with the terms in a document d and query q : where the LHS is the log odds and the RHS has two terms: a function, which can be calcula ted from information known about the document and query, and an unknown constant. We start with a ranking function We can make the following order-preserving transformation: If we could calculate probability. However, since k is unknown, we make a further order-preserving transformation: Although we cannot calculate transformation is lossy . This simplified example demonstrates why extracting an actual probability from the ranking function is problematic since probabilistic models are derived using lossy order-preserving transformations. It is important here to disti nguish between IR models which require only term frequencies and models which require either specific probability estimates ente red by the user or relevance judgements. A model will be considered a term frequency model if it requires only that documents, queries and the entire collection are indexed to produce a vector of term frequencies and no other information is required. We note that a special case will be Bernoulli models where only the presence or absence of a term is considered. However, we will specifically address multinomial models here. The advantage of term frequency models is that indexing can be conducted automatically. In the simplest case terms can be single words which may be stemmed and/or have stopwords removed as required. There are examples of models that require other information which can yield a probability of relevance. Fuhr [4] provides a series of models based on pr obabilistic indexing. A more comprehensive survey of such mode ls is to be found in [3]. This approach requires human estimates of probability in the indexing processes. Turtle and Croft [12] propose a model based on Inference Networks. Again this requires a user inputting probabilities. A further approach [2, ch. 3] assumes relevance judgements for a query have already been made for a subset of the collection. This is used to infe r relevance for all other documents in the collection. In all these cas es the additional information is fundamental to the model and thus these models are not applicable to the problem we wish to solve here. We now consider three approaches which require only term statistics. In each case the ranking function is derived from the assumption that term frequencies alone can be used to order documents by the probability of re levance. We show that in the first two we have no way deriving a measure of relevance likelihood from the ranking function and in the third we have to make an unreasonable assumption. This model [11] is an example of a classic probabilistic model (as opposed to Language Modelling). It seeks to use the probability that a document is relevant to a query as the basis for its ranking Relevance is assumed to be related to the co-occurrence of terms in document and query where some terms contribute to the relevance more than others. Ce rtain independence assumptions are made about the occurrence of terms in documents and the query. Derivation of the ranking function from the actual probabilities makes not one but several order-p reserving transformations some of which are lossy. An example is eliminating from the model the probability that an arbitrary documen t is relevant to an arbitrary query. This transformation is necessary since it is difficult to estimate this probability from vectors of terms alone, even if we knew all the possible queries in advance. Here we will consider multinomial language models where it is assumed we index documents a nd queries by the frequency of terms. A (unigram) language m odel is a random process which emits terms belonging to a vocabulary with differing probabilities. The Language Modelling approach assumes that for each document in the collection there is a probabilistic language model which created it. Each language model is estimated from that document although we assume a non-zero probability of any term in the collection being emitted to avoid the Zero Propagatability Problem [7] by some form of smoothing. Language models do not model rele vance directly [7] and indeed Spark Jones et al. [10] argue that for this reason the theoretical justification for Language Mode lling is a far-from-settled matter. Language Modelling uses as its ranking function, r, the probability that a language model which generated the document d would generate the query q . This is not the same as the probability that the document is relevant to the query. Laffety and Zhai [5] provide a theoretical justification for Language Modelling which avoids the troublesome assumption that the user is conducting a known ite m search  X  that is there is a unique relevant document in the collection. Again, they derive the ranking function from the starting point of the probability of a document being relevant to a que ry. Nevertheless there are many order-preserving transformations, some of which are lossy. As with the derivation of the OKAP I model, inestimable quantities are eliminated since it is argued they do not affect the ordering. For this reason we cannot extract the actual probability from the ranking function. There are some specific applications where we can assume that the user is seeking a single relevant document; an example is the use of Language Modelling for the prioritisation of suspects based on textual descriptions of crimes [1]. To extend this assumption to the classic IR problem of a user searching for documents to fulfil some information need is unrealistic. Nevertheless, if we are prepared to suspend belief temporarily, the following argument can be used. We assume that the user composed a query with the intention of finding the single required document. The probability that the language model of document d would generate a query q is taken to be the probability that q would be composed by the user to find document d . By using Bayes X  theorem, we can estimate that for a query q , probability that each document d is the one required by the user: where () d P is the prior probability of the document being the relevant one for an arbitrary query and () q P is the probability that such a query would be constructed. In the absence of other information, we assume that all documents are equally likely to be retrieved for an unknown query. Thus where D is the (constant) size of the collection. We also know q P is a constant since we are dealing with one query, so we can write: Since we know that all probabilities must add to 1: In other words the probability of relevance is simply a normalisation of the ranking function r . We would obviously expect the required document to have the highest probability. However, the probability assigned to the highest rank item is a measure of the likelihood we can assert that this is indeed the required documen t. Thus we can use this as a measure of relevance likelihood. So far, we have avoided the question of what we mean by relevance. Models which take in either probabilities or relevance judgements as inputs are consistent with more than one view of relevance. Whatever criteria are used to judge the inputs will define the meaning of the output. For the arguments presented in sections 2.2 and 2.3 relevance is assumed to be related to co-occurrence of terms but a precise de finition is not given. In 2.4 we are searching for. In the next section we base our arguments on the definition of relevance implied by the TREC evaluation initiative. The derivations of the ranking function for OKAPI and for Language Modelling, using Lafferty and Zhai X  X  [5] argument, can never be used to derive the actual probability of relevance. This is because term frequencies cannot be used to derive certain quantities that such an estimat e would require. Indeed it is difficult to imagine how the actual probability of relevance could be estimated without being able to estimate the density of relevant documents in the collection. Th is is precisely why the order-preserving transformations were used to eliminate them. We will now extend the argument of section 2.4 and generalise it to multiple relevant documents. In doing so, firstly we have to adopt a specific view of relevance. Secondly, we lose the notion of an actual probability but nevertheless maintain that we have a measure of likelihood meaning a higher value will imply that this document is more likely to be relevant, irrespective of where it is ranked against other documents. There are many different views of relevance [6] and no universal agreement that it is a binary attribute, which is a precondition for applying probabilistic models. We argue that the TREC procedure for assigning relevance has become a de facto standard for relevance for laboratory based IR evaluation. We are not asserting that it is only valid definition or even optimal, but since standard IR collections such as TREC are used for many empirical evaluations, this is the definiti on we focus on. From Voorhees description of the TREC relevance judgements [13], we can extract three key properties of relevance: A corollary of this last property is that adding irrelevant material (padding) to a relevant document will not alter its relevance status. We note that these prope rties are consistent with van Rijsbergen X  X  definition [8, ch. 7]:  X  X  document is relevant to an information need if and only if it contains at least one sentence whic h is relevant to that need. X  These definitions should be seen as pragmatic in that they sidestep many of the complex i ssues of relevance and make the process of assigning a relevan ce judgement more repeatable. It could be that none, some or all the documents in the collection are relevant to an information need so clearly we cannot assume a unique relevant document. We now attempt to circumvent this problem by postulating the existe nce of a hypothetical document, the ideal satisfying document . We define the Ideal Satisfying Document (ISD) as one which contains all the relevant material found in any document in the sentences within the collection. This is ideal in an artificial sense since it would not read well as a piece of prose. Since term frequency models take no account of word order we do not have to concern ourselves with the order in which the sentences were composed. A researcher in possession of the ISD could find all the information necessary to write any report since, by definition, any other document would be s uperfluous. The collection need not contain the ISD and it is very unlikely that would; this does not matter. We can postulate the existence of the ISD inside or outside the collection. Nevertheless the ISD is composed entirely from vocabulary found within the collection. We now write down the language model ranking function: where t is a term, () t p will emit term t and () q t c , is the number of occurrences of t in q . Since all language models with smoothing will emit any term in the vocabulary with a non-zero probability then there is a non-zero probability that any language model will generate the ISD. For a given topic, the relevant documents in the collection will by definition contain material found in the ISD. Relevant documents will have some overlap with the ISD (share common sentences) and non-relevant documents will have no overlap. Documents which overlap will clearly have common terms. Documents which do not overlap may also have so me common terms but we would expect far fewer of them. So, relevant documents will have a higher probability of their langua ge models generating the ISD than non-relevant documents. There are two special cases to consider here: Where there is a unique relevant document, it will contain the ISD (plus possibly some padding). Where there are no relevant documents, the ISD will be empty and the behaviour of the model is undefined. We propose that the probability of generating the ISD can be seen as a measure of relevance likelihood, using an argument analogous with the one in section 2.4. Clearly we would expect the relevant documents to be given the higher probability of generating the ISD. However, if we take one document and ignore its position in the ranking, the probability that it could generate the ISD is a measure of how likely it is to be relevant. This probability of generating the ISD cannot be estimated directly; we would need to know all the relevant parts of all the documents in the collection whic h would defeat the purpose of retrieval process in the first place. We propose instead that the query is a proxy for the ISD. Indeed we maintain that the query was composed by the user with th e intention of finding the ISD if it were indeed present in the collection. So relevance likelihood may be estimated as normalised ranking function (equation 10). We have argued that the nor malised ranking function from Language Modelling can be interpreted as a probability and this can be done without the assumption that there is a single relevant document. We therefore have a th eoretical justification for this probability being used as a m easure of relevance likelihood. An empirical validation has been carried out using three TREC collections, which, for reasons of space, cannot be detailed here. Using the measure of entire precision [1] we were able to compare the probability likelihood measure across many queries in the same collection. By comparing the actual values of the likelihood measure with rankings de rived from it we were able to provide empirical evidence that it does measure relevance likelihood. [1] Bache, R., Crestani F., Cant er D., Youngs D., Application of [2] R. Baeza-Yates, B Ribeiro-Neto, Modern Information [3] F. Crestani, M. Lalmas, C. J. van Rijsbergen, I. Campbell, [4] N. Fuhr, Models for Retrie val With Probabilistic Indexing, [5] J. Lafferty, C Zhai, Proba balistic Relevance Models Based [6] S. Mizzaro, Relevance: The whole history. In T. Bellardo [7] J.M. Ponte, W.B. Croft,  X  X  Language Modeling Approach to [8] C.J. van Rijsbergen, Information Retrieval , Buttereworths, [9] S.E. Robertson, The Probability Ranking Principle in IR, in K. [10] K. Spark-Jones, S. Roberts on, D. Hiemstra, H. Zaragoza, [11] K. Sparck Jones, S. Wa lker and S.E. Robertson, A [12] H. Turtle, W.B. Croft, Inference Networks for Document [13] E. Vorrhees, Overview of TREC 2003, 
