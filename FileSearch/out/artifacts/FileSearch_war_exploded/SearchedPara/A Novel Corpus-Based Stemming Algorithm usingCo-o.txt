 We present a stemming algorithm for text retrieval. The algorithm uses the statistics collected on the basis of cer-tain corpus analysis based on the co-occurrence between two word variants. We use a very simple co-occurrence mea-sure that reflects how often a pair of word variants occurs in a document as well as in the whole corpus. A graph is formed where the word variants are the nodes and two word variants form an edge if they co-occur. On the basis of the co-occurrence measure, a certain edge strength is de-fined for each of the edges. Finally, on the basis of the edge strengths, we propose a partition algorithm that groups the word variants based on their strongest neighbours, that is, the neighbours with largest strengths.

Our stemming algorithm has two static parameters and does not use any other information except the co-occurrence statistics from the corpus. The experiments on TREC, CLEF and FIRE data consisting of four European and two Asian languages show a significant improvement over no-stem strat-egy on all the languages. Also, the proposed algorithm sig-nificantly outperforms a number of strong stemmers includ-ing the rule-based ones on a number of languages. For highly inflectional languages, a relative improvement of about 50% is obtained compared to un-normalized words and a relative improvement ranging from 5% to 16% is obtained compared to the rule based stemmer for the concerned language. H.3.1 [ Information Systems ]: Content Analysis and In-dexing -linguistic processing, indexing; H.3.3 [ Information Systems ]: Information Search and Retrieval Algorithm, Experimentation Stemming, Corpus analysis
Vocabulary mismatch between queries and documents is a well known problem in an IR system. There are several forms of vocabulary mismatch, and the variation of words because of the morphological process (both derivational and inflectional) is such an instance. Stemming is a form of natural language processing that is commonly used in an IR system to address the aforementioned problem.

Stemming is a well studied technology and several stem-mers of varying spirit and flavor exist in the literature. The most commonly used stemmers encode a set of language spe-cific rules to group the morphologically related words. The potential danger of such stemmers is that they often produce aggressive conflation (for example, Porter stemmer conflates both execute and executive to the same term [22]).
The corpus based stemmers on the other hand do not use language specific rules and try to find groups from the am-bient corpus. Depending upon the nature of the algorithm, corpus based stemmers can be classified into two broad cat-egories. In one category, simply the lexicon (acquired from the corpus) is analyzed to find groups, and in other an ad-ditional evidence such as co-occurrence of word variants or the context of the words in which they appear are used. The former category of corpus based stemmers is found to give comparable performances with the rule based stemmers, whereas corpus analysis based stemmers with a background stemmers are much better alternatives, particularly in pre-cision oriented systems such as web search [18].

Therefore, a solution to this problem of aggressive stem-ming is the usage of co-occurrence data that carries an addi-tional evidence of their relatedness. Like Xu et. al [22], we hypothesize that, if two words co-occur in a document quite often then the likelihood of their proximity is high. We fur-ther advance our argument that words in a set are related if they co-occur as well as they form a community among themselves. Therefore, words which co-occur within a com-munity are much better members to be grouped in a class rather than words which do not. In doing so, we might miss out some words which are actually morphological variations. But we believe such cases is either infrequent or cause less damage.

Several corpus based approaches [12, 22] exist for stem-ming which perform efficiently. For example, YASS, a method based on a string distance measure and complete linkage clustering algorithm, performs fairly well on languages like English, French, Bengali etc., but it is relatively ineffective on highly inflectional languages like Marathi and Hungarian.
The method developed by Xu et. al. [22] carries certain weaknesses. First, the association measure depends on some parameter and therefore proper tuning is necessary. Second, two graph partition algorithms (connected component and optimal partition) used have different behaviors. The con-nected component algorithm creates chains [7] and the opti-mal partition algorithm is infeasible for complex languages where the morphologically related classes have large sizes. Obviously, the optimal partition algorithm can be used by first creating the classes using some background stemmer (as in case of English they used Porter and Krovetz stem-mer), but that requires an additional constraint and may be difficult to apply on a language which does not have a background stemmer.

One important criterion of clustering algorithms is the determination of parameters that is used to form the clus-ters. For example, in partitional clustering algorithms (like k -means), the number of clusters needs to be specified be-forehand. Also, in hierarchical clustering algorithms (like single linkage), a similarity threshold value is to be specified to determine the actual number of clusters. Both methods are crucially dependent on the parameter values. Parame-ters also depend on several factors like, the nature of the language, or the nature of the corpus, or the kind of similar-ity measure used etc. So a desirable property of a clustering algorithm is its insensitivity to parameters. Therefore, we take this argument into account in designing our algorithm and finally end up developing a clustering method which does not depend on any parameter.

Therefore, in the light of the above the purpose of our al-gorithm is three fold. The first aim is to see whether solely based on co-occurrence data we can design an effective lan-guage independent stemming method for mono-lingual re-trieval. The second aim is to see whether it can act as a better alternative to stemmers based on extensive lan-guage specific rules. Thirdly, we want to develop a method for which the necessity of parameter fine tuning is limited thereby improving the robustness in performance.

The overview of our method is the following. We first collect the co-occurrence of words from the given corpus. Then the words sharing a common prefix of a given length are mapped in a graph with their edge weights equal to the value given by the co-occurrence metric. These strengths are then used to redefine the co-occurrence strength between words using their common co-occurring neighbours (words). Finally, this strength works as the basis of our grouping algorithm which proceeds as follows. First, it identifies some specially designated edges called strong edges and deletes all other edges which are not strong. Once this has been done, the next step finds the clusters of words simply by applying the connected component algorithm.

The organization of our article is as follows. In the next section, we discuss about the background work on various stemming strategies. Section 3 presents the proposed ap-proach. The description about the data and the retrieval system is given in section 4. Experimental results are pre-sented in section 5. In section 6 we discuss about possible merits and demerits of the baseline stemmers and the pro-posed stemmer. We conclude in section 7.
Stemming is a long studied technology and several stem-mers of various principles and flavors exist in the literature. Broadly, they can be categorized into two classes, one cat-egory of stemmers relies on language specific rules and the other category is the corpus based approaches. The advan-tage of corpus based stemmers over the rule based methods is that they do not require knowledge of the specific languages. Additionally, corpus based stemmers reflect certain charac-teristics of the ambient corpus and therefore, are sometimes more effective compared to the rule based stemmers [22]. Corpus based stemmers can again be subdivided into mainly two types depending upon their principle. The one category simply analyzes the lexicon (words from a corpus) to un-derstand the morphology of the concerned language. One category of corpus based methods, uses the context of the words to be stemmed or their co-occurrences in the partic-ular corpus. For precision oriented systems, such analysis are reported to perform better than the blind rule based stemmers [22, 18]. Furthermore, corpus analysis can again be used to improve an already existing aggressive stemmer. Apart from the aforementioned methods, there are a num-ber of works reporting the effectiveness of character n -gram tokenization for addressing the problem for alphabetic lan-guages [14, 16]. In the rest of this section, we review some of these methods.
Perhaps the most widely used language specific stemmers used for English IR is the Porter [19] algorithm. A number of other language dependent methods exist such as Lovins [10] and Krovetz [8]. Lennon et. al. [9] compared the per-formance of Porter and Lovins stemmer on English queries and found little improvement. Harman [6] also experimented with English data using three stemmers without any positive outcomes. Nevertheless, subsequent studies [8, 22] demon-strated the usefulness of stemmers on simple language (like English) IR. The most notable advantage of these methods is their ease of use : rules once designed can be used on any corpus without further processing. Generally, these methods handle both derivational as well as inflectional morphology. But the obvious requirement to design such stemmers is the knowledge of the language.

A number of works have reported the effectiveness of light stemming approaches [20, 4] for a variety of languages both European and Asian. As opposed to the stemmers like Porter, light stemming methods focus on removing only in-flectional variants from the end of a word.
Majumder et al. [12] developed a language independent, clustering based unsupervised stemming algorithm (YASS) that is capable of handling a family of primarily suffixing languages. They defined a string distance measure between word pairs that reward long matching prefixes and penalize an early mismatch as follows.
 Let the length of two strings X and Y be n + 1 (if strings are of unequal length, null characters are added at the end of the shorter string to make them equal), and let m denote the position of the first mismatch between X and Y (i.e. x = y 0 , x 1 = y 1 , . . . , x m  X  1 = y m  X  1 , but x m 6 = y define D as follows: D ( X, Y ) = n  X  m + 1 First of all, such a string distance does not have the knowl-edge of whether one word is a morphological variation of the other. The string distance returns a high similarity simply because two words share a long prefix. Complete linkage clustering was used to discover groups of (presumably mor-phologically related) words. They concluded that in terms of retrieval effectiveness, this approach is comparable to rule based stemmers like Porter or Lovins for English. On Ben-gali and French test collections, YASS provides substantial performance improvement as compared to using no stem-ming.

Goldsmith [5] presented an unsupervised morphological analyzer based on minimum-description-length model. The frequency of stems and suffixes that result from every pos-sible breakpoint in each term of a collection is examined. A breakpoint for each token is optimal if every instance of a token must have the same breakpoint and which minimizes the number of bits necessary to encode the collection. The underlying intuition is that breakpoints should be chosen in such a way that each token can be segmented into relatively common stems and common suffixes.

Bacchin et al. [2] described a language independent prob-abilistic model for stemmer generation which makes use of the mutual reinforcement relationship between stems and suffixes. From a finite collection of words they first generate a set of substrings (prefixes and suffixes) by splitting each word at all possible positions, except for those which gen-erate empty substrings. Then they form a directed graph where a node implies a substring and a directed edge be-tween node x and y exists if there is a word z in the collection such that z = xy . Now they estimate the prefix score and suffix score using HITS algorithm with the assumption that the good prefixes point to good suffixes, and good suffixes are pointed to by good prefixes . Once the prefix and suffix scores are estimated, the algorithm determines the most probable split (into prefix and suffix pair) for each word in the collec-tion which maximizes the probability of the prefix and suffix pair.

Oard et al. [17] did suffix discovery statistically in a text collection and eliminated the word endings. They first counted the frequency of every one, two, three and four char-acter suffix that would result in a stem of three or more characters for the first 500,000 words of the collection. Then they subtracted the frequency of the most common subsum-ing suffix of the next longer length from each suffix (for ex-ample, frequency of  X -ing X  from the the frequency of  X -ng X ). The adjusted frequencies were then used to sort all n -gram suffixes in descending order. In case of English, the count versus rank plot was found to be convex and so the rank at which the second derivative was maximum was chosen as the cutoff limit for the number of suffixes for each length.
Xu and Croft [22] investigated the usefulness of corpus analysis in ad hoc retrieval. The basic assumption of their work is that word variants that should be conflated will occur in the same documents or, more specifically, in the same text window (they reported on 100 word text window). Based on this assumption, they devise a co-occurrence met-ric that measures the association of two words. The measure is given below. where n a , n b are the number of occurrences of words a and b in the corpus, and n ab is the number of times both a and b fall in the chosen window. Here k is a parameter and can be estimated from the corpus. Two graph partitioning al-gorithms (connected component, optimal partitioning) are then used to refine the initial equivalence classes of words that shares a number of characters in their common prefix or the classes generated by different rule based stemmers like Porter or Krovetz. This technique helps to identify the cor-pus dependent conflations and also minimizes the bad con-flations (universal and university to the same class) made by a linguistic rule based stemmers. The reported results show that corpus analysis indeed improved the performance signif-icantly on both English and Spanish corpora. Our method is similar in spirit with this approach, but varies in many re-spects. First, we use much simpler co-occurrence measure. Second, we introduce a re-weighting scheme that uses the initially calculated co-occurrence to respect their commu-nity affinity. Finally, the method for forming clusters here is different from connected component algorithm and is based on the nearest neighbour. Therefore, the nature of our pro-posed clustering algorithm provides us the liberty to ignore the tuning of parameters.

Recently Peng et. al [18] proposed a context sensitive stemming approach to web search. On the query side, they propose a statistical language modeling based approach to predict which word variants are better forms than the orig-inal word for search purpose and expanding the query with only those forms. In order to decide which morphological variants are better candidates to add to the original query, they use the bigrams to the left and right of a word as its context features by mining a large web corpus. At the same time on the document side, they propose a conservative con-text sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query. The method is found to perform effec-tively on web data. However, their method is not completely independent unlike others, since they determine the possible morphological variants using an already existing stemmer (such as porter).
Very recently, character n -gram tokenization [16] has been used to address the morphological variations in alphabetic languages. In contrast with the other methods presented in this section, n -gram method not only handles derivational and inflectional morphology, but also copes with the spelling variations as well as the word forms due to compounding. In particular, 4 and 5 grams provide substantial performance improvement on a wide variety of languages of different ori-gins. One major pitfall with n -grams is the increase in the size of the inverted index. A passage of k characters con-tains k  X  n + 1 n -grams of length n , but only approximately ( k + 1) / ( l + 1) words, where l is the average word length for the language [15]. As a consequence of index size expansion, n -gram based indexing consumes 6 times disk space as op-posed to plain word indexing and queries can take 8 times as long to execute.
The goal of the proposed method is to group the set of morphologically related words appearing in a corpus. The method takes a set of unique words and their co-occurrence information and returns a set of groups. We impose a nec-essary condition that the two words are morphologically re-lated if they share a common prefix of given length. We follow here the same argument given by Xu et. al [22] (that if we choose too long common prefix the algorithm will fail to group words such as cat and cats ) and consider only word pairs sharing first 3 characters (common prefix). This is the first static parameter (say, l 1 ).

The algorithm comprises of three main modules as follows. The words in each group are likely to be morphologically related. Certainly, such an algorithm does not guarantee that all words in a group are related or all morphological variations will be grouped together. However, we expect that such an approach will reduce the erroneous conflations.
The following explanation will help us to understand more clearly the usefulness of co-occurrence information. Let a and b be two words in a corpus and let df ( a ) denotes the number of documents in that corpus that contain a . Now if we group a and b together, the effective df (denoted as edf ) of each of the words will be the following. Here, df ( a  X  b ) denotes the number of documents in which both a and b occur. Clearly, df ( a  X  b )  X  min ( df ( a ) , df ( b )), and the values of edf ( a ) , edf ( b ) will go up if df ( a  X  b ) is low (means a and b co-occurs infrequently). So a ranking function that uses inverse document frequency as a measure of importance of a query term, may influence the ranking heavily because of such merging (putting a and b together). Furthermore, it is more dangerous to group an infrequent term with a frequently occurring term and if such a case hap-pens co-occurrence information controls the effect to some extent.
The basic assumption of our method is that a document conveying a topic very often uses a term which is related to the original terms of the topic [23]. For example, a document discussing about education most likely uses the term educate or its variants. Furthermore, in the case of a document dis-cussing about a person very frequently uses its inflectional variants. Since morphological variations are semantically closely related with the meaning of the original terms, their document level co-occurrence provides a very useful infor-mation which can be used to group them together.

We define the co-occurrence strength of two words a and b in a corpus C as follows.
 where d and tf p,q represents the document and the term frequency of term p in the document q respectively. In other words, the quantity CO ( a, b ) measures how often two words a , b occur in the same document of a given corpus C . Note that unlike the co-occurrence measure given by Xu et.al. [22], CO ( a, b ) does not contain any parameter and therefore is much robust when the corpus or the language is changed.
Once the co-occurrence strengths between words are com-puted, the words are then mapped in an undirected graph. A pair of words w 1 and w 2 is connected by an edge ( w 1 CO ( w 1 , w 2 ) &gt; 0 and they have a common prefix of a given length (we chose 3) along with the suffixes (after truncating the longest common prefix) which are the residues (the ends after removal of longest common prefix) of more than one pair of co-occurring words with long common prefix (length larger than 5 which is the second static parameter, say, l The second condition is used as a heuristic to predict a mor-phological variation. The edge ( w 1 , w 2 ) is assigned a weight CO ( w 1 , w 2 ). Re-calculation of co-occurrence strength then operates on this graph and is motivated by the following principle.
 Two co-occurring words will receive higher weight if both of them co-occur with other words.

With the document level co-occurrence strength in hand, we can re-weight the strengths by the following formula. Here we consider the word pairs ( a, b ) for which CO ( a, b ) &gt; 0. The notation N a,b denotes the set of common neighbours of a and b where a node w is a common neighbour of a and b if CO ( a, w ) &gt; 0 and CO ( b, w ) &gt; 0. Clearly, RCO ( a, b )  X  CO ( a, b ) and they are equal only if a and b have no common co-occurring words. The value of RCO ( a, b ) depends on two factors given below. 1. The number of common co-occurring words of a and 2. The strength in which they co-occur with the com-
Figure 1 shows an motivating example underlying the role of co-occurring neighbours. The vertices represent the words and the edge weights represent their co-occurrence strength as computed by equation 2. A set of words is a better candidate to form a semantically related group if their co-occurring words form a community. It is, however, unlikely that all the words in the group will co-occur with each other, but a word related with the theme of a group, is likely to co-occur with many members of the group. So the informa-tion embedded in the neighbourhood of two words are used to better infer the proximity of the word pairs.

As an example, consider graph given in Figure 1. Clearly, solely based on the co-occurrence strength it is hard to de-termine whether w 4 is to be grouped with w 1 or w 5 , because both w 1 and w 5 have equal co-occurrence strength with w But, since w 4 has more common neighbours with w 1 , the chance is high that w 4 is semantically more related with w than with w 5 . The re-weighted graph in Figure 2 clears out the grouping dilemma using the measure given in equation 3.
So far we have focused on measuring the co-occurrence strength between word pairs. Recall that our ultimate goal is to group morphologically related words. Here we present a simple algorithm based on the strongest neighbour of each word. The following principle is the key to our proposed algorithm.
 A word (say w 1 ) will be grouped with another word (say w if either w 1 is the strongest neighbour of w 2 or w 2 is the strongest neighbour of w 1 .
 The algorithm first identifies the strong edges from the given graph. A strong edge is an edge ( u, v ) if either of the follow-ing holds.
Upon doing this, the edges which are not strong are deleted from the graph. Finally, the clusters are formed by find-ing connected components. Each such cluster represents a morphologically related group. The complete stemming al-gorithm is given in Figure 3.

The clustering algorithm based on the above principle does not involve any parameter. The parameter free nature of the algorithm makes it more robust unlike other clustering algorithm such as single linkage algorithm, where a thresh-old is to be set to form the groups. The threshold selection is not a trivial task particularly in a multilingual set up where the parameters are very often language dependent as well as corpus dependent [12, 11].

Note that the proposed algorithm differs from the single linkage clustering algorithm in two respects. First parameter selection is not required and second, the algorithm does not create too many elongated clusters as elongation of clusters primarily depends on the value of the similarity threshold.
To better understand the difference between the proposed strongest neighbour based algorithm and single linkage clus-Input : A symmetric matrix A = { a ij } of order n , where a ij = CO ( w i , w j ) .
 Output : A set of clusters of words. 1: /* weights from neighbours */ 2: for all i, j (1  X  i &lt; j  X  n ) do 3: if a ij &gt; 0 then 4: let S = { m | a im &gt; 0 and a jm &gt; 0 } 5: b ij  X  P 6: end if 7: end for 8: /* adding the weights */ 9: for all i, j (1  X  i &lt; j  X  n ) do 10: if a ij &gt; 0 then 12: end if 13: end for 14: let E s = {  X  } /* the set of strong edges */ 15: /* clustering the vertices */ 16: for all i (1  X  i  X  n ) do 17: let p  X  argmax 18: E s = E s  X  ( i, p ) 19: end for 20: Find the connected components considering only edges 21: Make each connected component a cluster tering algorithm, consider the example graph of Figure 4. If single linkage clustering algorithm is used without any threshold the entire graph is returned as a cluster (Figure 5). On the contrary, the proposed strongest neighbour based algorithm automatically breaks the edge connecting w 3 and w 6 (Figure 6), since the nearest neighbours of w 3 and w 6 are w 2 and w 7 respectively (alternatively, the edge ( w is not a strong edge).
In what follows, we describe an array of experiments for evaluating our proposed co-occurrence based stemmer on a number of languages. As part of our experiments we com-pare the performance of our method with a number of other stemmers (given in section 4.3).

In further explorations, we perform a deeper analysis to understand the impact of the proposed method on individual queries. In addition, we study the goodness of our method in improving precision at the top of the ranked list. The details of our experimental test sets are as follows. We conducted our experiments on six languages of various origin and morphological complexity to evaluate our algo-rithm. Among the languages studied, Marathi and Hun-garian are highly inflectional and agglutinative in nature whereas English is the least inflectional. Table 1 summarizes the corpora and queries used for evaluation. For the English experiments we used the TREC 6-7-8 queries and the docu-ment collection comprises of FBIS, Financial Times and Los Angles Times. Both Marathi and Bengali documents and garian and Hungarian documents and topics are taken from CLEF 06 and CLEF 07 and the Czech test sets (queries and documents) are from CLEF 07. 2 topics from Hungarian and 12 topics from Marathi have no relevant documents in the recall base. Therefore, during experiments we removed those queries from our test set. The queries in each lan-guage have been formed using title and description fields of the experimental topics. Standard stopword lists are used to remove insignificant words from the documents and queries and no other preprocessing steps were applied.

We used MAP [13] as our primary evaluation metric and in addition we also report R-precision and precision at 10 in order to see the precision enhancing powers of the stemmers. In addition we also report the number of relevant document 1 http://w w w.isical.ac.in/  X  fire retrieved in each language. Statistical significance tests were performed using paired t -test based on the query wise MAP values at a 95% confidence interval. All experiments have been conducted using a probabilistic divergence from ran-domness based model called IFB2 [1]. We produce all our
We compare the proposed method with three baselines stemmers and also against no stemming. Some recently de-performance of our algorithm. The stemmer developed by Dolamic et. al. [4] is chosen as the rule based approach for Marathi and Bengali. The Hungarian stemmer is by Savoy [21] and the Czech stemmer is by Dolamic et. al [3]. Porter stemmer was used as English rule based stemmer. The stem-mer proposed by Xu et. al. [22] was used as a baseline cor-pus analysis based stemmer. The YASS was chosen as the other lexicon analysis based statistical stemmer. The exper-available by the authors. We have implemented the stem-mer proposed by Xu et. al. We did not implement the stem-mer based on optimal partition technique, since that will be very inefficient for the kind of languages we are studying. Instead, we have implemented the algorithm based on con-nected component and trigram matching (the words sharing first three character).

Note that we have chosen the baseline stemmers which performs very well and also varies in their principle, since one of our goal is to see if the proposed method can be used as a better alternative to the existing stemmers. Some parameter needed to set for XU and YASS. XU depends on two parameters (namely k in em formula and the em score cut off value to determine the connected components) and YASS depends on similarity threshold value for cluster determination. We have chosen k = 2 . 74  X  10  X  6 and . 01 as em cut off value for XU and for YASS we set the threshold to 1.5 as recommended by the respective authors.
In the tables we use the following abbreviations. NO means no stemming, RB for rule based stemmer, XU for the stemmer developed by [22] and PROP used for proposed method.
 R-Prec 0.2293 0.2930 0.2500 0.3161 0.3289 Rel.Ret 1611 2003 1790 2085 2101
The main experimental results are presented in Tables 2 to 7. Each table gives MAP, R-precision, precision at 10 and the number of relevant documents retrieved. From Table 2 it is clear that, on Bulgarian data all stemmers improved the MAP value. The MAP value achieved by the proposed 4 http://www.isical.ac.in/  X  clia/resources.html algorithm is much better than that achieved by either of rule based stemmer or XU. However, the difference is not very large compared to the performance achieved by YASS. R-precision improved consistently with the improvement of MAP.

R-Prec 0.2611 0.3456 0.2868 0.3295 0.3441 Rel.Ret 551 667 632 672 681
The trend is similar for the languages like Czech (Table 3), Hungar ian (Table 4) and (Marathi Table 5). Marathi and Hungarian are two highly inflectional languages and therefore, a significant benefit is achieved. Figure 7 clearly demonstrates the improvement graphically. The experimen-tal results once again validate the conclusions made by Savoy et. al. [21] and Majumder et. al. [11].
 R-Prec 0.2518 0.3117 0.3019 0.3152 0.3585 Rel.Ret 1367 1723 1803 1730 1964
The experimental results obtained by using the proposed approac h is noticeably better than XU on Czech (26.9%), Bulgarian (32.9%) and Hungarian (25.9%). Also, the perfor-mance differences between the proposed method and YASS are very large on both Marathi (24.6%) and Hungarian (17.3), whereas performance differences are relatively small for lan-guages such as Czech and Bulgarian. The comparison be-tween PROP and no stem reveals that PROP achieves more than 50% MAP on four languages (namely, Czech, Marathi, Bulgarian and Hungarian). On Bengali data this improve-ment is almost 20%. Although, English results improved through the application of PROP but not as much as other languages, which certainly confirms the previous reported results [12]. The relative percentage (MAP) improvement between the PROP and other methods is given in Table 8. R-Prec 0.3010 0.3114 0.4066 0.3472 0.4190 Rel.Ret 1463 1472 1621 1546 1653
The stemmer based on language dependent rules has suc-cessfully handled morphological variations from retrieval per-spective. Porter stemmer is the best among all the stemmers studied on TREC 6-7-8 queries. But when the proposed co-occurrence based stemmer is applied on more inflectional languages, all languages achieve consistently better average precision than rule based stemmers. A particularly notice-able outcome is the performance of rule based stemmer on Marathi data. When all the languages have been benefited through the rule based stemmers significantly, Marathi re-sults show very marginal improvement compared to no stem-ming. We suspect that there may be some error in the avail-R-Prec 0.3582 0.3923 0.3983 0.4024 0.4153 Rel.Ret 2173 2264 2279 2272 2275
Figure 8 shows the precision at 10 achieved by the chosen approac hes on all data sets. The observation that can be made from the figure is that once again the precision at top for more inflectional languages is noticeably high when the proposed method is applied. Comparison between PROP and XU shows that on Hungarian, Czech and Bulgarian PROP is noticeably better than XU. However, the differ-ences are relatively small on Marathi, Bengali and English. On English, Porter stemmer achieves the maximum p@10 which is slightly better than the others. Rule based stem-mer also does fairly better (compared to no stem) on Ben-gali, Czech, Hungarian and English, but poorly performs on Marathi and Bulgarian.

We further study the recall enhancing ability of the var-ious methods studied. In Figure 9 we show the number of relevant documents retrieved at 1000. Clearly, except Bul-garian XU retrieved almost an equal number of documents with PROP and even better in English. PROP retrieved maximum number of documents in Hungarian and also in Marathi.

Table 9 provides the percentage of query in which the pro-posed method achieves better MAP than the other meth-ods including the un-stemmed runs on all six languages. More than 80% of Marathi and Hungarian queries have been benefited if the words are normalized using the proposed co-occurrence based method compare to the un-normalized case. Substantial amount (close to 80%) of Bulgarian and Czech queries also achieves better average precision due to the use of the proposed stemmer. However, stemming hurts many Bengali and English queries. Particularly, almost one-third English queries have been hurt for using the proposed method. When, we compare the query-wise performance, we observed that in all other languages (except English), the proposed method outperformed the other stemmers. Only in English, Porter stemmer performed marginally better than the proposed method.

The outcomes of statistical significance test is summa-rized in Table 10. A star (or a cross) symbol in a cell ( x, y ) means the proposed method performs statistically signifi-cantly better (or statistically equally) than the method y on language x . Clearly, the choice of the proposed method over no stemming is always significantly beneficial to an IR sys-tem. YASS and the proposed method performed statistically equally on Bengali and English data. Furthermore, no sig-nificant difference is found between the proposed approach 5 http://mem b ers.unine.ch/jacques.savoy/clef/index.html R-Prec 0.2733 0.3008 0.2934 0.2913 0.3001 Rel.Ret 6812 7751 7751 7652 7638 and both the Porter and XU stemmer on English data. But on all other languages the performance differences of the proposed method are statistically significant than the other methods (only exception is Czech where the performance difference between RB and PROP is not significant).
In the previous section we have reported the experimen-tal results only. In this section we shed some light on the possible merits and demerits of each of the methods.
We have already pointed out some cases where the rule based stemmer falters for less morphologically complex lan-guages like English. But such errors may be more detrimen-tal for complex languages. Rule based stemmers generally remove some hand picked suffixes from word endings and in doing so it may commit serious mistake by removing a valid word ending treating it as a suffix. Sometimes, it also fails to conflate related terms. Therefore, co-occurrence analysis can be a choice to reduce such errors.

Now, consider the case of XU. Almost on all languages it performed quite well in terms of retrieving number of rele-vant documents. We believe this is because of the kind of graph partitioning technique used to group morphologically related words. The connected component algorithm used by XU, creates long classes and therefore, although it enhances recall but hurts the precision, which ultimately leads to the overall degradation of MAP values.

Turning to YASS, we have noticed that it performs very well on Bengali, Czech and Bulgarian but performs less ef-fectively on Hungarian and Marathi. It does not suffer from the problem of chaining in creating clusters, but has some other limitations. First, the string distance measure falters when suffix lengths are long, which is very common in ag-glutinative languages (private and privatization are kept in separate groups because of the long suffix ization at the end of privatization ). Secondly, the string distance measure re-turns the same similarity value for the word pairs like { cat , cats } and { fat , fate } , which clearly is not right. In addi-tion, complete linkage clustering produces different clusters of same objects if simply the objects are considered in dif-ferent order, this erratically splits a valid cluster. Possibly, these are the main reasons why YASS is less effective on Marathi and Hungarian.

One important module of the proposed algorithm is the re-calculation of the initial co-occurrence strength. The re-calculated strength more clearly discriminates a word as-sociation from the other co-occurring word. The nearest neighbour based algorithm, coupled with the re-calculated weight, more accurately forms the clusters. Table 11 demon-strates the usability of re-weighting approach. Clearly, on all languages the re-calculated weights improved the perfor-mance of the algorithm.

The proposed method has two parameters, namely, l 1 and l . We conducted our initial experiments with English and Bengali by setting l 1 = 3 and l 2 = 5 and then extended our experiments with other four languages under the same settings. A noticeable loss is observed for a larger value of l 1 , since a larger value of l 1 leads to fragmentation of some valid groups. On the other hand, l 2 = 6 , 7 , 8 led to marginal fluctuations in accuracy. For some languages, there was insignificant improvement while for the rest, there was insignificant loss. Finally, the choice of 0.5 in equation 3, was arbitrary and we did not experiment with any other value of this factor.
In this paper we have proposed a stemming algorithm based on co-occurrence of words in a corpus. One impor-tant characteristic of our algorithm is that its parameters are static. A set of experiments on several standard data sets shows that retrieval performance is significantly en-hanced through the use of the method and in more than half of the languages the proposed approach outperformed the stemmers based on the language specific knowledge. Also, the method performed better than two other corpus based strong stemmers on all the languages. Further analysis re-veals that the proposed stemmer outperforms the competing methods in large number of queries. Based on these results, we believe that the proposed method can be used as a better alternative to the rule based stemmers.
 We are very grateful to Mandar Mitra for valuable discus-sions. We thank SIGIR and Singhal Family (Donald B. Crouch Travel Grant) for the student travel grant to the first author. [1] G. Amati and C. J. Van Rijsbergen. Probabilistic [2] M. Bacchin, N. Ferro, and M. Melucci. A probabilistic [3] L. Dolamic and J. Savoy. Indexing and stemming [4] L. Dolamic and J. Savoy. Comparative study of [5] J. Goldsmith. Unsupervised learning of the [6] D. Harman. How effective is suffixing. Journal of the [7] A. K. Jain, M. N. Murty, and P. J. Flynn. Data [8] R. Krovetz. Viewing morphology as an inference [9] M. Lennon, D. S. Pierce, B. D. Tarry, and P. Willett. [10] J. Lovins. Development of a stemming algorithm. [11] P. Majumder, M. Mitra, and D. Pal. Bulgarian, [12] P. Majumder, M. Mitra, S. K. Parui, G. Kole, [13] C. D. Manning, P. Raghavan, and H. Schutze.
 [14] J. Mayfield and P. McNamee. Single n-gram [15] P. Mcnamee and J. Mayfield. Character n-gram [16] P. McNamee, C. K. Nicholas, and J. Mayfield. [17] D. W. Oard, G.-A. Levow, and C. I. Cabezas. Clef [18] F. Peng, N. Ahmed, X. Li, and Y. Lu. Context [19] M. F. Porter. An algorithm for suffix stripping. pages [20] J. Savoy. Light stemming approaches for the french, [21] J. Savoy. Searching strategies for the hungarian [22] J. Xu and W. B. Croft. Corpus-based stemming using [23] J. Xu and W. B. Croft. Improving the effectiveness of
