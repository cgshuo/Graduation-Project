 Inverted indexes using sequences of characters (n-grams) a s terms provide an error-resilient and language-independen t way to query for arbitrary substrings and perform approxi-mate matching in a text, but present a number of practical problems: they have a very large number of terms, they exhibit pathologically expensive worst-case query times o n certain natural inputs, and they cannot cope with very short query strings. In word-based indexes, static index pruning has been successful in reducing index size while maintain-ing precision, at the expense of recall. Taking advantage of the unique inclusion structure of n-gram terms of different lengths, we show that the lexicon size of an n-gram index can be reduced by 7 to 15 times without any loss of recall, and without any increase in either index size or query time. Because the lexicon is typically stored in main memory, this substantially reduces the memory required for queries. Si-multaneously, our construction is also the first overlappin g n-gram index to place tunable worst-case bounds on false positives and to permit efficient queries on strings of any length. Using this construction, we also demonstrate the first feasible n-gram index using words rather than charac-ters as units, and its applications to phrase searching. Categories and Subject Descriptors: H.3.1 [Content Analysis and Indexing]: Indexing methods; E.4 [Coding and Information Theory]: Data compaction and compression General Terms: Algorithms, Experimentation
Consider the following exact search problem: given a col-lection of documents, quickly locate all occurrences of som e arbitrary string in this collection. The collection may con -tain any type of data, structured in any manner.

Standard word-based inverted indexes are of little assis-tance for searching files that have no explicit word struc-ture, such as genome sequences and image data. Effectively searching highly-structured data, such as programming lan -guage source files or XML, typically requires specific knowl-Figure 1: Number of terms and overall size of a compressed n-gram index over a small TREC-1MB collection for various n-gram lengths, with and with-out posting lists. Number of terms (top) is identical for both. Posting lists are compressed by Moffat and Stuiver X  X  interpolative coding [19]; the lexicon is compressed by front coding and Huffman coding. edge of the grammar and an expensive parsing phase; a sim-ple word-based index would not be effective for searching a source code database for "%s" or a L A T E X database for x^2/4 . Word-based indexes are also language-dependent: their stopword lists and stemming algorithms depend specif -ically on the source language, and for texts written in Asian languages, even word-breaking requires complex language-specific techniques. For these reasons, word-based indexes are not well-suited to a system that needs to search hetero-geneous data in a uniform way.

Classical n-gram indexes, which store the location of every sequence of n characters in the input for some fixed n , over-come these limitations. They are language-independent and are readily generalizable to approximate matching, which i s important for bioinformatics applications as well as query -ing texts containing minor OCR errors or typos. The choice of n is a design choice based on what is most effective for a particular application: if n is too small, each term will have a long posting list and queries will require large numbers of term lookups, making queries too expensive; but as n in-creases the number of terms and the compressed index size increases exponentially, as shown in Figure 1.

An important design choice in an n-gram index is whether to store the offset lists of each n-gram in each document, or to store document lists only. If offset lists are stored, then achieving high precision and recall on query results is straightforward, by using relative offset information to al ign posting lists before intersection; the systems of Adams and Meltzer [1] and Kim et al [13] use this method.

On the other hand, as Figure 1 shows, discarding offset lists provides a simple way of producing much smaller in-dexes; Mayfield and McNamee X  X  HAIRCUT system [18] and our own system use this approach. Any unqualified refer-ences to classical n-gram indexes in the remainder of this paper will refer specifically to n-gram indexes without offse t lists.

For small values of n , the index size without offset lists is sublinear, but as n increases the number of terms increases exponentially with n and lexicon storage begins to dominate the index space. Because lexicons are typically stored in main memory, lexicon size directly impacts the load time and memory requirements of the query engine.

Additionally, discarding the offset lists introduces a new problem: queries become conservative overestimates. In th e language of query relevancy, they maintain perfect recall, but may have arbitrarily poor precision. For example, a query for  X  X here X  over a trigram (3-gram) index may return documents containing the words  X  X ere X ,  X  X hen X , and  X  X er X . These false positives, called retrieval noise by Ogawa and Matsuda [20], can be eliminated by a linear scan over the documents occurring in the query result, as first suggested by Adams and Meltzer [1]. This approach takes advantage of the assumption that most applications will, once they locate a term, want to load its document context for further processing; therefore scanning documents that contain the query term involves no additional cost. If precision is high , the overhead of scanning is small.

However, classical n-gram indexes without offset informa-tion are not designed to provide any worst-case guarantees on precision; there are natural inputs for which they yield pathologically expensive query times. For example, in one test case we divided the Canterbury Corpus King James Bible [3] into 1000 documents and constructed a classical 3-gram index on this collection. This index returned 81% of all documents when queried for  X  the man and his  X , a phrase that occurs in only one document. As another exam-ple, an n-gram index on a collection containing many long runs of a single character will tend to yield poor behavior on queries that contain long runs of that character (intu-itively,  X  X -grams can X  X  count X ). This unpredictable behav ior is a serious problem for interactive applications, where th e user would be baffled to encounter long processing delays on queries that produce small result sets.

Another issue with classical n-gram indexes is that queries on strings shorter than n characters are expensive and com-plex: they can only be found by retrieving all index terms containing the query and unioning them. Whereas the num-ber of terms retrieved increases linearly as query strings become longer, it increases exponentially as they become shorter, and a secondary index is required on the lexicon to locate these terms. The alternative of including posting li sts for all n -grams of length at most n greatly increases index size, typically doubling it.

In this paper we present TinyLex, an n-gram index de-signed to minimize the lexicon size and eliminate pathologi -cal worst-case behavior while keeping both recall and preci -sion competitive with classical n-gram indexes in the typic al case. It achieves this by selectively incorporating n-gram s of many lengths, effectively providing an arbitrarily large co n-text. Its construction also permits simple, efficient querie s on short strings.

Because TinyLex indexes use many lengths of n-gram, they are not parameterized by n-gram length; instead they are parameterized by a threshold value t placing a hard up-per bound on the number of false positives in query results. The index is constructed by a multipass procedure that ex-amines one length of n-gram at a time, beginning with the smallest, and computes the query size of every such n-gram. An n-gram is added to the lexicon if its actual posting list is significantly smaller than its query list. By tracking parti al query results and pruning the list of candidate n-grams from pass to pass, efficient index construction over a wide range of lengths is made practical. Further details are described in section 3.

Our results, detailed in section 6, show that TinyLex in-dexes achieve index sizes and median query times compara-ble to or better than those of classical n-gram indexes with a number of terms that is 7 to 16 times smaller for English text and 1.2 to 13 times smaller for genome sequences. In fact, our pruning is so effective that it renders n-gram in-dexes feasible on large alphabets such as English words, as described in section 6.4. We also demonstrate scalability of index construction, and exhibit worst-case query sets fo r which TinyLex query times are dramatically (2 X 30 times) faster than those of classical n-gram indexes.

The primary limitations of our approach compared to classical n-gram indexes, detailed in section 6.6, include a longer index construction time (about 9.8 times longer) and (with the standard parameter settings) slower query times on queries for which the classical n-gram index would return empty results. Also, as described in section 7, the problem of generalizing our approach to dynamic collections remain s open.

In section 2, we describe a number of related works that address the problem of precision in n-gram indexes or deal with similar index structures or applications. Section 3 de -scribes our basic index construction, with details about it s encoding in section 4 and query processing in section 5. Per-formance is evaluated in section 6. Future work and applica-tions are discussed in section 7 and conclusions in section 8 .
A number of elements of our motivation and construction have appeared in previous work.

Wu and Manber X  X  agrep [24] and GLIMPSE [16] were mo-tivated by desktop search applications and also store impre -cise location information that necessitates a linear scan t o filter out false positives; like n-gram indexes, they enable ap-proximate matching. However, only words are indexed, so they only apply to documents with a word concept. More-over, the posting lists of all words in the index that match the pattern are searched. This leads to poor precision on queries that span words or contain common words; by index-ing terms that span words, n-gram indexes overcome these limitations.

Exact string search is one of the applications targeted by suffix arrays, a compact data structure that lists all offsets into a collection, sorted by the order of the suffix strings occurring at those offsets. Although traditional construc-tion algorithms make poor use of the memory hierarchy and so fail to scale, sophisticated techniques such as those of Dementiev et al. [9] scale effectively to large collections. However, even compact suffix arrays such as those of M  X  aki-nen [15] still tend to be several times larger than our in-dexes ( X  X ess than twice the size of the text X  as opposed to 15 X 60% the size of the text), partly because the number of suffixes is large and partly because their overconstrained or -dering prevents them from taking advantage of delta-based list compression.

Because our pruning criteria are based on the document frequency of n-grams, and suffix arrays can be used to ef-ficiently determine the document frequency of all n-grams [25], we considered using them in our index construction procedure; however, for the collections we considered the cost and complexity of generating the suffix array in the first place exceeded that of the simple scanning-based procedure described herein. This approach may be more profitable for very large collections.

The n-gram/2L system of Kim et al. [13] is also designed to reduce the space and time overhead of classical n-gram indexes by using a two-level index. A small front-end index maps small m -grams to larger n -grams containing them, and the back-end index maps these larger n -grams to document offsets. The primary advantage of this approach is that only offsets equal to zero mod n  X  m + 1 need to be stored in the back-end index. However, their approach depends criticall y on the presence of offset information, which we eschew to reduce index size, and n-gram/2L exhibits poor worst-case behavior in cases where the front-end index yields a large set of n -grams to query.

Ogawa and Matsuda [20] constructed a classical n-gram index with offset lists for Japanese queried using a subset of nonoverlapping terms. They address the false positives (which they call  X  X etrieval noise X ) introduced by their re-duced query term set with a simple query planner that pre-dicts which set of nonoverlapping terms will produce the least noise, and then adds one additional term for further differentiation. Although like our approach this work at-tempts to address the issue of false positives, their index also depends on offset information and only attempts to ad-dress noise introduced by the query planning process.
Mayfield and McNamee X  X  HAIRCUT system [18] shares our design choice of eliminating offset lists to reduce in-dex size. Although HAIRCUT focused on ranked search, an application we do not consider in this paper, the trends exhibited by HAIRCUT X  X  lexicon size and index size over a range of n-gram lengths are consistent with our baseline measurements for classical n-gram indexes, and we expect this system to exhibit poor worst-case query times as de-scribed in section 1.

Static index pruning on word-based indexes [7][6] also fo-cuses on shrinking the lexicon, but because word terms don X  X  share the rich substring structure of n-gram terms, it is not able to maintain perfect recall or provide guarantees on pre -cision; instead, its goal is to maintain relevance in highly -ranked results. In exchange for this loss of information, th is type of pruning is more successful in decreasing overall ind ex size.

Mah and D X  X more pointed out the issue of retrieval noise in n-gram indexes and modelled it statistically via a simi-larity function and a multinomial distribution [14][8] How -ever, their goal was also retrieval of relevant documents, rather than complete enumeration of matching documents. Mah and D X  X more also augmented their classical n-gram indexes with larger n-grams in order to improve precision by adding n-grams which are extensions of existing frequent index terms. However, this ad hoc approach does not gen-eralize well to many lengths of n-grams and included many n-grams that are strictly unnecessary, such as unique exten -sions of frequent n-grams.

N-gram models in language modelling (with words rather than characters as units) routinely use variable-length mo d-els created by a process called pruning [22]. In this process, a large model is constructed using n-grams for a large value of n , then all n-grams that can be removed without signifi-cantly altering the model X  X  predictions are removed. In fac t, the most similar work to ours comes from language mod-elling, where Siivola and Pellom [21] describe a method for  X  X rowing X  an n-gram language model beginning with small n-grams and adding larger ones whenever this decreases the data coding length; they point out the advantage, shared by our method, that  X  X v]ery high order n-grams can be used X  because the complete set of high-order n-grams never needs to be stored.

One important application of our technique is efficient phrase searching using an auxiliary index of word n-grams, as discussed in section 6.4. Bahle et al. [4] discuss a simila r technique for phrase searching using an auxiliary  X  X extwor d index X  that is pruned by only including phrases where com-mon words appear first, similar to the approach of Mah and D X  X more but at the word n-gram level. For the same reason, it includes some strictly unnecessary terms; for example, i f a two-word term occurs very frequently, it may be added to the nextword index, even if the two terms never occur sep-arately. Our scheme excludes terms like this and includes important longer word n-grams of length 3 and 4; we show that overall space overhead is comparable. It is difficult to draw a conclusive comparison however, since we do not mea-sure query times in this work.

The tradeoff between size and precision exhibited by our index is a classical one that appeared as early as 1970 in Bloom filters [5], where the average number of false positive s per query increases as the table size is reduced. Zobel et al. [26] discuss the repercussions of this tradeoff in signature file indexes, which are also hash-based; our implementation , however, does not use hashing.
We begin by considering a simplified problem: suppose we are given a document collection and a threshold t , and we wish to ensure that, for any string of length between n min and n max occurring in the document collection, its query result has at most t false positives. In other words, the query result on strings not occurring in the collection is (f or the moment) irrelevant.
 Formally, suppose the alphabet is  X , that S :  X   X   X  P ( X   X  ) yields the set of all substrings of a given string, and the document collection is viewed as a collection of strings D i over  X   X  ( i  X  N ). Any index implies a query function Q :  X   X   X  P ( N ) producing the query result for a given string, and P ( s ) = { n : s  X  S ( D n ) } is the actual posting list for s . We seek an index yielding a Q satisfying As in a typical inverted index, we select a lexicon L  X  P ( X   X  ) and the index explicitly stores P ( s ) for all s  X  L . In a typical inverted index, Q is defined by: where T ( q ) is the term set of q , a set of potential index terms extracted from q . For word-based indexes these are the words of q (typically stemmed and with stopwords re-moved). For classical n-gram indexes these are the sub-strings of q of a fixed length.
 Since s  X  S ( q ) implies P ( s )  X  P ( q ), we have Q ( q )  X  P ( q ). Terms not in the lexicon are asserted implicitly to occur nowhere. However, in a variable-length index such as ours, T ( q ) = S ( q ), and any L such that S ( q )  X  L for all queries q returning results would be too large for an explicit representation. To avoid this issue, we instead assume that terms not in the lexicon implicitly occur everywhere, and define:
We aim to satisfy our requirement using a variable-length n-gram index containing terms of many lengths. To motivate the construction procedure, suppose the index contains the n-gram  X  X ugg X  and we wish to determine whether we should add  X  X uggl X  to the index. In many collections,  X  X uggl X  will have exactly the same posting list as X  X ugg X ; any query strin g containing  X  X uggl X  also contains  X  X ugg X , so adding  X  X uggl X  eliminates no additional documents, and it should not be to L will add little information.

To exploit this, we construct a multipass greedy algo-rithm, detailed in Figure 3. The procedure makes a pass over the collection for each value of n from n min to n max beginning with the smallest, and computes the posting list of every n-gram in the collection (lines 3 X 6). At the end of each pass, the partially constructed index is queried for ea ch n-gram encountered during that pass (line 8); any n-gram exhibiting t or more false positives is added to the index (lines 9 X 12).

The resulting index will trivially satisfy (1) because we have visited every string satisfying the condition n min  X  | s |  X  n max  X  | P ( s ) | &gt; 0 and added it to L if it failed to
Moreover, this set is minimal in the sense that the Q implied by any proper subset of L fails to satisfy (1): be-cause only terms of larger or equal size are added after a 1 function ConstructIndexSimple( D , n min , n max , t ): 2 for n from n min to n max : 3 for d from 1 to | D | : 4 for i from 1 to | D d | : 5 s  X  D d [ i..i + n  X  1] 6 Add d to P ( s ) 7 for each s where | s | = n and | P ( s ) | &gt; 0: 8 Q ( s )  X  T P ( s 0 ) over substrings s 0 of s in L 9 if | Q ( s ) | X  X  P ( s ) | X  t 10 Discard P ( s ) 11 else 12 L  X  L  X  X  s } 13 return { L, P } Figure 3: Pseudocode for simple index construction. D is the document collection and P contains the posting lists of n-grams that have been added to the index. given n-gram s is added, and no such term can be a sub-string of s , they do not affect | Q ( s ) | . Hence if s satisfied | Q ( s ) | X  X  P ( s ) | &gt; t when it was added, it will again satisfy it if s itself is removed.

On the other hand, this construction does not necessarily produce the smallest possible L satisfying (1). For example, suppose there are three documents 0, 1, 2 and five distinct n-grams of length between 1 and 2 with P ( a ) = { 0 , 1 } , P ( b ) = { 0 , 2 } , P ( aa ) = P ( ab ) = P ( ba ) = { 0 } . Setting t = 1, the construction produces L = { aa, ab, ba } , but L = { a, b } also satisfies (1). We do not attempt to minimize | L | globally.
The distribution of n-gram lengths in the lexicon produced by this algorithm is nontrivial and follows a predictable pa t-tern: the number of n-grams rapidly increases to some peak value, depending on the collection, then decreases to a tail of small values before falling to zero. Two example distribu -tions are shown for an English text collection and a genome sequence collection in Figure 2.
The index described so far operates only a limited set of queries, and construction rapidly becomes infeasible as n max increases. The reasons for this are twofold. The first is that it requires tracking the posting list of every n-gram in the collection; for a large value of n , most n-grams are unique and occur in one place, so storing these lists typical ly requires storage several times the size of the collection, e ven though almost all of them will not pass the condition for inclusion in L . The second is that as the size of a query string q grows, queries become more expensive because S ( q )  X  L grows.

To address these, we note that if | P ( s ) | &gt; 0, the condition | Q ( s ) | X  X  P ( s ) | X  t is implied by the stronger condition: We call this the heritable condition because it has the useful property that if it holds for s , it holds for all superstrings of s . We exploit it in two ways. First, while scanning the collection, we ignore any n-grams satisfying the heritable condition and do not track their posting lists. If they are encountered multiple times, they are queried each time. For large n , this makes sense, because most n-grams will be ex-cluded by the heritable condition and will also occur rarely .
Number of terms in lexicon (b) Distribution of n-gram lengths on chr14 collec-tion, a genome sequence, with 5% threshold. Omit-ted are 157 n-grams of length &gt; 12.

Second, while scanning the collection, if we encounter an n-gram that fails the heritable condition at offset i , we do not need to visit offsets i  X  1 or i in the next pass; by maintaining a list of offsets into each document and pruning it during the scan, each pass becomes successively shorter. This also pro -vides a convenient stopping condition: once the offset list becomes empty, we have enumerated all possible n-grams in the collection that might have t or more false positives, allowing efficient index construction for n max =  X  . If we also set n min = 1, the guarantee is provided over all sub-strings in the collection, and we can remove the condition n min  X | s | X  n max from (1): An important question is whether the condition | P ( s ) | &gt; 0 can also be removed. It X  X  natural for users to present querie s for strings absent from the collection, and the lexicon pro-duced by this construction can yield large numbers of false positives for such queries. The key lies in the contrapositi ve words, if we query for s and then scan t +1 documents from the query result without observing s , we can safely conclude that s does not occur in the remaining documents either. This allows us to provide a strong unconditional worst-case guarantee: where Q 0 ( s ) with P ( s )  X  Q 0 ( s )  X  Q ( s ) is the set of docu-ments read by the linear scan phase of query processing.
To further accelerate index construction, we note that for any s with | s | = n , Q ( s ) can be computed as the intersection of Q ( s 1 ) and Q ( s 2 ), where s 1 and s 2 are the two length n  X  1 substrings of s . Here, Q ( s i ) will either be P ( s i ), if s added to the lexicon, or else will be its query list generated in the previous pass. Thus we retain all query results from the previous pass; this does not require prohibitive memory , since we need only retain query results for strings passing the heritable condition.

Combining this optimization with the heritable condition optimizations described above, we obtain the construction procedure in Figure 4. In this procedure, each query re-1 function ConstructIndex( D , n min , n max , t ): 2 Q ( )  X  X  1 , 2 , ..., | D |} 3 O i  X  X  1 , 2 , ..., | D i |} for all 1  X  i  X | D | 4 for n from n min to n max (or until  X  i O i =  X  ): 5 for d from 1 to | D | : 6 for each i in O d : 7 s  X  D d [ i..i + n  X  1] 8 if Q ( s ) has not been assigned 9 Q ( s )  X  Q ( s [1 ..n  X  1])  X  Q ( s [2 ..n ]) 10 if Q ( s )  X  t + 1 11 Discard Q ( s ) 12 O i  X  O i  X  X  i  X  1 , i } 13 if Q ( s ) has been assigned 14 Add d to P ( s ) 15 for each s where | s | = n and | P ( s ) | &gt; 0: 16 if | Q ( s ) | X  X  P ( s ) | X  t 17 Discard P ( s ) 18 else 19 L  X  L  X  X  s } 20 Q ( s )  X  P ( s ) 21 Discard Q ( s ) for all s with | s | &lt; n 22 return { L, P } Figure 4: Optimized pseudocode for index construc-tion. D is the document collection, O maintains a list of offsets in each document, Q contains temporary query results, and P contains the posting lists of n-grams that have been added to the index. In prac-tical implementations, D , O , and P ( s ) for | s | &lt; n  X  1 are stored compressed in persistent storage. sult is constructed by intersecting two previously compute d query results (line 9); if an n-gram s was added to the in-dex, Q ( s ) will equal its posting list (line 20). N-grams failing the heritable condition are discarded and ignored (lines 10  X  11). The offsets list, initially containing all offsets in all documents (line 3) is pruned whenever an n-gram fails the heritable condition (line 12).

Finally, another useful trick to accelerate index construc -tion and limit memory usage is to separate each pass into two passes, the first counting the document frequencies of each term passing the heritable condition, using this informati on to prune the terms, then making another pass to build the Figure 5: The impact of subset encoding on the size of the compressed posting lists for various values of the threshold parameter. From top to bottom the curves show the index size with no subset encoding, with rarest-substring subset encoding, and with full subset encoding. Data is the TREC 10MB collec-tion. Whether or not subset compression is used, all posting lists are compressed by interpolative coding. posting lists of the remaining terms. This optimization is not shown in the pseudocode, but is straightforward to im-plement.
Although our method could be the basis for an in-memory index, for example for searching a document in a text edi-tor, our experiments in this work are limited to disk-based indexes; consequently, query time is dominated by disk ac-cesses, allowing us to emphasize space-efficient codes over ones that are fast to decode. All codes that we evaluated were built on Moffat and Stuiver X  X  interpolative coding[19] , rather than a faster-to-decode but more complex alternativ e such as the word-aligned codes of Anh and Moffat.[2]
In section 3 we noted that, typically, the closer | Q ( s ) | is to | P ( s ) | , the less information is conveyed by adding s to the index. Therefore, the bits that we invest in encoding is understood to be the query against the index omitting the n-gram s from the lexicon). A simple way to do this is using a subset encoding : having determined both Q ( s ) and P ( s ) we can compute the index set I ( P ( s ) , Q ( s )), which contains n  X  N if and only if the n th largest element of Q ( s ) is in P ( s ). This set is the same size as P ( s ) but has a smaller range and a smaller average gap, resulting in significantly better compression by standard list compression methods such as Golomb coding and interpolative coding. Figure 5 demonstrates the compression achieved by subset compres-sion on the TREC 10MB collection for various threshold values.

Unfortunately, although highly effective at compressing the index, subset encoding makes decoding expensive for long queries because a roughly quadratic number of posting lists must be retrieved to compute the result, and a quadrati c number of intersections must be performed in the dynamic programming algorithm that computes Q ( s ) for each sub-string s of the query. As a compromise, we experimented with a simplified method where each posting list was en-coded as a subset of the posting list of its rarest substring that was also in the index; we call this rarest-substring sub-set encoding . This achieves good compression and much more practical query times, but its query times are still not competitive with the independently-encoded posting lists of classical n-gram indexes.

In the end, we relied on independent encoding of each term X  X  posting list in order to keep query times competi-tive with classical n-gram indexes that also use independen t encoding. Subset encoding may still prove useful in scenar-ios like desktop search where small indexes and predictable query times are more important than average query times. Also, if there is no access to the original document collec-tion, and so no ability to perform a linear scan phase, subset encoding makes practical the extreme threshold setting of t = 0, where all query results are exact.
Although our construction algorithm is significantly dif-ferent from that of classical n-gram indexes, the result is still a standard inverted index, and query processing is per -formed in the same manner as on any other inverted index, with the noted exception that n-grams not occurring in the lexicon are assumed to occur in all documents.

Ogawa and Matsuda [20] highlighted the importance of query planning in n-gram indexes: the set of terms retrieved by the query affects both query time and precision. A sim-ple query planning strategy, used as early as 1993 by Adams and Meltzer [1], is to separate a query of length q into d q/n e non-overlapping segments and retrieve only these terms; tw o non-overlapping terms tend to have a smaller intersection than two overlapping terms, and in indexes with offset in-formation, this plan is sufficient to ensure perfect precisio n. Ogawa and Matsuda used more sophisticated query planning to select a subset of these nonoverlapping terms.
A simple but effective query planning mechanism is to store in the lexicon the document frequency of each term, then retrieve the terms relevant to a query in order by their document frequency. As each term is retrieved it is suc-cessively intersected with a running query result. If at any point the result becomes a single document, we terminate early. Since long queries tend to occur in a single document and contain rare substrings, only a fraction of all terms typ -ically needs to be retrieved. In our experiments this query planner was used for both types of indexes being compared.
Additionally, our queries do not retrieve terms that occur as proper substrings of terms already retrieved, because s 0  X  S ( s ) implies P ( s 0 )  X  P ( s ).
Previous sections used smaller TREC-1MB and TREC-10MB collections, based on non-normalized article bodies from the first document and the first 11 documents in the TREC WSJ (Wall Street Journal) collection, respectively. In this section we prefer larger collections: a TREC-100MB collection based on the first 101 documents from the TREC WSJ collection, and an 86MB chr14 collection created by dividing the human chromosome 14 genome sequence into 4000-byte chunks overlapping by 20 bytes; this was derived from NCBI Build 36.1 [11] as distributed by the UCSC Genome Browser project [12]. (a) Comparison of TinyLex and classical n-gram indexes on the TREC-100MB English text data set; percentage values are threshold parameters as a percentage of all documents.
Query strings are drawn uniformly at random from the collection. Lexicon size is improved by 7.4, 8.8, 14, and 16 times.
 7-grams 16 384 29 106 66
TinyLex 20% 13 164 29 88 57 8-grams 65 536 52 68 47 6% threshold 25 930 51 65 47 9-grams 262 037 75 60 31 3% threshold 45 269 73 61 32 10-grams 1 032 550 100 40 31 1% threshold 79 500 100 42 31 (b) Comparison of TinyLex and classical n-gram indexes on the chr14 genome data set; percentage values are threshold parameters as a percentage of all documents. Query strings are drawn uniformly at random from the collection. Lexicon size is improved by 1.2, 2.5, 5.8, and 13 times.

For each corpus, we constructed a series of classical n-gram indexes without offset lists; for TREC-100M, n ranged from 3 to 6, and for chr14 n ranged from 7 to 10. For each classi-cal n-gram index, we constructed a corresponding TinyLex index with a threshold value selected so that the final in-dex sizes would be close to that of the classical indexes (all threshold values are specified as a percentage of all docu-ments).

To eliminate the effects of caching, all reads of posting lists and documents were done against a 7200 RPM raw disk (no filesystem) with no buffering or caching other than an on-disk 8MB buffer. All experiments were performed on a single 2.0 GHz CPU core.

All query times include all phases of query processing: list retrieval, list processing, and false positive scanning. M edian and mean query times are both given because the distribu-tion of query times is not consistently symmetric and may contain outliers.
Our primary goal was to demonstrate that we can achieve index size and query time comparable to classical n-gram in-dexes using a much smaller lexicon. For each index and each query length, 30 seconds of queries were performed and the mean and median query time taken. Queries were drawn from the collection uniformly at random. To ensure that processing of documents containing the query did not dom-inate query time, the query times shown here are for large queries of length 30 X 50 characters.

As shown in Figures 6(a) and 6(b), TinyLex achieves a dramatically smaller lexicon with no penalty to median quer y time or overall index size. The index size is comparable over -all, rather than smaller, because smaller n-grams have larg er posting lists. On the TREC-100M set, the mean query times of the TinyLex index are significantly less than the classica l n-gram index because its query distribution is less skewed; that is, there are less outlying queries that take an unusu-ally long time. We suspect this is because n-grams in En-glish text tend to follow a form of Zipf X  X  law, as shown by Egghe [10], whereas n-grams in genome sequences do not. Although the times shown here are for long queries, the TinyLex indexes never exceeded 2.7 times the classical n-gram median query time for any query length.

Note that although in these tables we focus on TinyLex indexes with a few discrete threshold values, one of the im-portant advantages of our index is that is more finely tunable than the classical n-gram index: the index size varies grad-ually as the threshold parameter is varied, allowing it to be fine-tuned to meet specific resource constraints.
In section 1, we explained that classical n-gram indexes can exhibit pathologically expensive query behavior on cer -tain inputs, and that our construction avoids this. To demon -strate this, we generate a bad set of query strings that pro-duces large numbers of false positives in query results. The procedure for generating the strings starts with a short, frequently-occurring string and iteratively expands it wh ile keeping its query set large. These sets are intentionally ar -tificial and meant to illustrate worst-case performance.
Each bad set generated contains one string of every length, each formed by repeating a single generator string enough times to fill the query. For the TREC-100MB collection, the generator string was  X  the company said  X , and for the chr14 collection, the the generator string was  X  X  X . We mea-sured the query performance of each of the indexes used in the comparison in section 6.2. As shown in Figure 7, the dif-ference between the TinyLex and classical indexes on these queries is dramatic: even the smallest TinyLex indexes out-perform the largest classical n-gram indexes by at least 30% on long queries, despite being 7.2 and 3.5 times smaller. The best TinyLex indexes achieve query times close to zero, about 15 X 30 times faster than the classical n-gram index of comparable size. The failure of even highly precise classi-cal n-gram indexes to tackle natural queries like these is a marked disadvantage that our construction corrects.
As mentioned in section 2, it X  X  common in language mod-elling to create variable-length n-gram models using words rather than characters as units. The ubiquitous phrase search feature of search engines can be cast as a query for docu-ments containing a particular word n-gram; the simple word-(b) chr14. From top to bottom, classical indexes (thin lines) are 7-, 8-, 9-, 10-grams, TinyLex in-dexes (thick lines) are 30%, 6%, 3%, 1% threshold. Query: ( t )* (thick lines) n-gram indexes. based index is the special case of this with n-gram length equal to one.

All of the problems with n-gram indexes described in sec-tion 1 apply equally to this type of index: unless offset lists are stored for each index term, phrase queries can easily pro -duce large numbers of false positive documents that contain all the words in the phrase but not the phrase. For example, the words  X  X e X  and  X  X ith X  occur in every document of the TREC-10MB collection, but the phrase  X  X e with X  occurs in only one.

Storing offsets to perform phrase searches is expensive in storage cost and query processing time. The nextword in-dexes of Williams et al. [23] take a different approach, storing the set of all word bigrams and using standard compression techniques to reduce the index size; but these indexes are still very large, about 60% of the collection size, due to a large number of terms. For example, in the TREC-10MB collection with about 38,000 unique words, there are over 500,000 unique bigrams. A similar problem occurs at the character level in documents featuring large alphabets, su ch as Asian-language texts.

The hybrid indexes of Bahle et al. [4] reduce storage re-quirements by pruning the set of bigrams stored, and accel-erate queries by using a query planning technique based on the document frequencies of the constituent words. Their pruning criterion retains only pairs where the first word of the bigram is among the most k common words. By apply-ing the TinyLex construction to word n-grams, and using our query planning scheme that retrieves terms of low fre-quency first, we expect to achieve similar advantages, while performing better on queries where bigrams are insufficient, or where the second word (rather than the first) is the com-mon one.

We encoded each document in the TREC-10MB collec-tion as a sequence of word symbols, and then used our in-dex construction algorithm directly on these sequences. Th e portion of the index describing 1-grams is effectively a sim-ple word-based index, and was used as our base index size. As shown in Table 1, the additional space requirement of adding larger n-grams is comparable to the hybrid indexes of Bahle et al. in typical parameter ranges. Although query Table 1: Size of an auxiliary TinyLex phrase index over the TREC-10M collection, which has 38,000 distinct words, for a variety of threshold values. Size does not include unigrams. The Hybrid k schemes of Bahle et al. [4] retain only bigrams where the first word was among the most k frequent by docu-ment frequency. The final row shows the overhead of adding offset lists. performance remains to be determined in future work, this offers a promising alternative to existing methods of phrase searching.

An important advantage of TinyLex over nextword in-dexes is that it includes a small number of 3-grams and 4-grams that would cause a large number of false positives in a nextword index. An example from our experiments is  X  X f the first X , a phrase which occurs in only 1% of documents, even though each bigram in it occurs in at least 21% of doc-uments. Even an unpruned nextword index, which requires many times more storage, is susceptible to this limitation. A number of works such as Adams and Meltzer [1] and Mayfield and McNamee X  X  n-gram stemming [17] have used a two-phase approach to reduce index size where first words are extracted from the collection and then n-grams are ex-tracted from each word. This prevents queries that span words, but still permits word and partial word queries. We Figure 8: Scalability of TinyLex construction time at 1% and 5% threshold values over TREC collections of various sizes; scaling is roughly linear, tending up for the largest collection. can adapt our approach to use this technique by simply ex-cluding any n-grams that contain internal spaces; the resul t is an n-gram index that operates as a functional replace-ment for a word-based index with a much smaller lexicon size. For example, a 7%-threshold TinyLex index over the TREC-100MB collection yields a 15 MB index with only 13548 terms in its lexicon; a word-based index over the same collection has 100449 terms.

One interesting result we found is that for a fixed thresh-old percentage, the TinyLex index size is largely indepen-dent of the document size  X  as documents become smaller, the number of documents grows but the lexicon shrinks be-cause shorter n-grams are selected by the construction. Thi s is useful because it allows us to freely select a document siz e close to the size of a unit of transfer, such as a disk block or cache line, keeping the index as small as possible while stil l assigning unit cost to each document scan. It also demon-strates the ability of the index to tune itself to different ty pes of collections; in classical n-gram indexes approximating a similar invariance of index size over different document siz es requires the parameter n to be systematically varied. In this experiment we varied the size of the documents into which we divided the human chromosome 14 genome sequence used for the chr14 collection, maintaining an overlap of 20 bytes , and computed 5% threshold indexes on each. The results are shown in Table 2.

Finally, we demonstrated scalability of index constructio n by completing a 1% and a 5% threshold index on the TREC-1MB, TREC-10MB, and TREC-100MB collections, and a TREC-1GB collection drawing from the TREC AP (Asso-ciated Press), WSJ (Wall Street Journal), and FR (Federal Register) collections. Results are shown in Figure 8.
Our work has two primary limitations as compared to classical n-gram indexes.

First, because index construction is a multipass algorithm performing a large number of queries, index construction is more expensive in time and space than for classical n-gram indexes, which need only make a single pass over the collection followed by a sort. Using the index pairs from section 6.2 on the TREC-100MB collection, construction of Table 2: A TinyLex 5% index on the chr14 collec-tion with various document sizes. As the document size is varied, the compressed index size remains roughly constant. The peak term length is the most frequently-occurring length of n-gram in the lexicon, and increases with document size. the TinyLex index required 9.8 times longer on average than a classical n-gram index of comparable size.

Second, classical n-gram indexes are able to return empty query results for a large class of queries not existing in the document (namely, those containing a substring of length n that does not occur in the document); due to our modified query computation, we produce nonempty results for these queries, and may have to scan up to t + 1 documents to determine that the query does not occur. In applications where this is important, it may be preferable to set n min and to return empty query results for queries containing a string of length n min not in the lexicon. We chose n min in our experiments because it results in smaller lexicons, slightly smaller indexes, and enables queries on arbitrari ly short strings.

Finally, as detailed below, the problem of generalizing our approach to dynamic collections such as those used by web search engines remains open.
Techniques for incremental update on ordinary inverted indexes do not apply to TinyLex indexes in a straightforward manner, partly because some of the information needed for update is not available explicitly in the lexicon, and partl y because of strong dependencies between index terms. For example, suppose we delete or modify a document that hap-pened to be one of the false positive documents for the query on s , before s was added to the index. This may make | Q ( s ) | X  X  P ( s ) |  X  t where it was not before, necessitating the removal of s from L ; but not only is there no efficient way to identify all such s from the removed document con-tents, but removing s from L may force superstrings of s in other documents to be added to L (because their query size has increased), which may in turn require removal of other terms, and so on. Adding documents is even more problem-atic because the new document may be a new false positive for a term that was not previously included in the index, but should now be  X  there is no clear way to identify such terms.

Future evaluations include: evaluating phrase search quer y times for word n-gram indexes; evaluating the improvement our technique can yield for n-gram indexes that use off-set lists and nonoverlapping queries; evaluations on large -alphabet natural-language texts such as Asian texts; and evaluations on larger collections.
In this work we described a method for generating variable-length n-gram indexes that have query performance and size comparable to classical n-gram indexes, but with dramati-cally smaller lexicons, enabling query engines to load more quickly and use less memory. It provides strong worst-case guarantees on the precision of query results, leading to mor e predictable query times, and permits queries on strings of any length. We anticipate that this type of n-gram index will make new types of interactive queries available on memory-constrained devices and will open the door to future researc h exploiting small lexicons. I would like to thank Ken Church, Susan Dumais, Hugh Williams, and Michael Cameron for useful discussion and for recommending relevant resources, reviewers, and venue s; and Nick Lester, Galen Hunt, Orion Hodson, Ed Nightin-gale, and anonymous reviewers for their reviews, which were critical in effectively developing this paper. [1] E. S. Adams and A. C. Meltzer. Trigrams as index [2] V. N. Anh and A. Moffat. Inverted index compression [3] R. Arnold and T. Bell. A corpus for the evaluation of [4] D. Bahle, H. E. Williams, and J. Zobel. Efficient [5] B. H. Bloom. Space/time trade-offs in hash coding [6] S. B  X  uttcher and C. L. A. Clarke. A document-centric [7] D. Carmel, D. Cohen, R. Fagin, E. Farchi, [8] R. J. D X  X more and C. P. Mah. One-time complete [9] R. Dementiev, J. K  X  arkk  X  ainen, J. Mehnert, and [10] L. Egghe. The distribution of n-grams. Scientometrics , [11] International Human Genome Sequencing [12] W. Kent, C. W. Sugnet, T. S. Furey, K. Roskin, T. H. [13] M.-S. Kim, K.-Y. Whang, J.-G. Lee, and M.-J. Lee. [14] C. P. Mah and R. J. D X  X more. Complete statistical [15] V. M  X  akinen. Trade off between compression and [16] U. Manber and S. Wu. GLIMPSE: a tool to search [17] J. Mayfield and P. McNamee. Single n-gram [18] P. McNamee, J. Mayfield, and C. Piatko. Haircut: a [19] A. Moffat and L. Stuiver. Binary interpolative coding [20] Y. Ogawa and T. Matsuda. An efficient document [21] V. Siivola and B. Pellom. Growing an n-gram [22] A. Stolcke. Entropy-based pruning of backoff language [23] H. E. Williams, J. Zobel, and P. Anderson. What X  X  [24] S. Wu and U. Manber. Agrep: A fast approximate [25] M. Yamamoto and K. W. Church. Using suffix arrays [26] J. Zobel, A. Moffat, and K. Ramamohanarao. Inverted
