 Author disambiguation in digital libraries becomes increasingly d-i ffi cult as the number of publications and consequently the number of ambiguous author names keep growing. The fully automatic author disambiguation approach could not give satisfactory results due to the lack of signals in many cases. Furthermore, human judg-ment on the basis of automatic algorithms is also not suitable be-cause the automatically disambiguated results are often mixed and not understandable for humans. In this paper, we propose a Label-ing Oriented Author Disambiguation approach, called LOAD, to combine machine learning and human judgment together in author disambiguation. LOAD exploits a framework which consists of high precision clustering, high recall clustering, and top dissimilar clusters selection and ranking. In the framework, supervised learn-ing algorithms are used to train the similarity functions between publications and a clustering algorithm is further applied to gener-ate clusters. To validate the e ff ectiveness and e ffi ciency of the pro-posed LOAD approach, comprehensive experiments are conducted. Comparing to conventional author disambiguation algorithms, the LOAD yields much more accurate results to assist human label-ing. Further experiments show that the LOAD approach can save labeling time dramatically.
 H.3.3 [ Information Storage and Retrieval ]: Digital Libraries; D.2.8 [ Database Management ]: Database applications Algorithms, Experimentation Author Disambiguation, User Contribution, Human Judgment
This w ork was conducted at Microsoft Research Asia when the first author visited there.

In digital libraries, author disambiguation becomes increasingly di ffi cult as the number of publications continuously grows and so does the number of authors with ambiguous names. Previous re-search aimed to solve the problem in a fully automatic way. Unfor-tunately, in current digital libraries, the information about publica-tions is always insu ffi cient, and the rapid increase of digital content also makes the manual creation of metadata highly costly. Fully automatic author disambiguation could not give satisfactory results in this situation. How to leverage limited human e ff orts to solve the author disambiguation problem becomes more and more important. In this paper, we propose a Labeling Oriented Author Disambigua-tion approach to solve the problem in a novel and e ffi cient way.
We have been developing Microsoft Academic Search 1 to help scientists and students locate research publications. The publica-tions include papers, books, patents, etc. By integrating informa-tion about tens of millions of publications from publishers, Cross-Ref and the web, we face serious author disambiguation problem. For example, in our system, we find that there are 117 di thors (before 24th Octber, 2010) named  X  X ei Zhang X . Due to the incomplete and inconsistent nature of the data, it is impossible for fully automatic disambiguation algorithms to correctly assign the publications to all di ff erent  X  X ei Zhang X  X .

A user contribution feature has recently been developed to change the situation. The feature supports users to manually correct the re-sults output by fully automatic disambiguation algorithms. Howev-er, we find the manual labeling process is still very time-consuming and tedious. Given that in the large scale digital libraries the num-ber of ambiguous names is huge, it is really not suitable for humans to correct errors made by the automatic algorithm.

In this paper, we propose a novel Labeling Oriented Author Dis-ambiguation approach, called LOAD, to conquer the author disam-biguation challenges in construction of digital libraries (or academ-ic search engines) together with users. The key ideas are: 1) Find High Precision Clusters (HPCs) for each author to change the label-ing granularity from individual publications to clusters; 2) Cluster the found HPCs into High Recall Clusters (HRCs) to place all pub-lications of one author into the same cluster. Thus the labeling for one author can be limited within a narrow scope; 3) Select the best HPC in each HRC as the starting point to cover di ff erent impor-tant authors for labeling. Other similar HPCs can be re-ranked for further labeling.

To achieve the goals of the proposed LOAD approach, machine learning algorithms are used. First, we exploit a supervised learn-ing algorithm to get similarity between publications, i.e., judge whether two publications belong to the same author. More specifi-cally, two classifiers for HPC and HRC are trained respectively and various rich features are investigated. On the basis of learned sim-http: // academic.research.microsoft.com ilarities, a clustering algorithm is further applied to generate final clusters. Comprehensive experiments are conducted to empirical-ly verify the e ff ectiveness of our proposed approach. One public dataset contains about 7,000 publications from DBLP and two in-ternal datasets contains about 6,000 publications are used. Correct authors for all publications were labeled by manual. A typical con-ventional automatic author disambiguation algorithm is taken as the baseline. The results are obtained by applying the algorithm in the real-world academic search engine in which we do not know which names are ambiguous and which are unambiguous. Our algorithm achieves promising results on ambiguous names while keeps little harm to unambiguous names. For HPC, the experimental result-s show that it improves precision from 93.72% to more than 99% on average. For HRC, the recall is also improved from 75.43% to 91.85% on average. The results also show that the LOAD can im-prove the human labeling e ffi ciency by about 10 to 30 times com-pared to the baseline.

The rest of the paper is organized as follows. Section 2 intro-duces related work. Section 3 describes motivation of our work. Section 4 explains the proposed approach and Section 5 presents experimental results. Section 6 concludes the paper with remarks while proposing future work.
Automatic Author Disambiguation for academic publications has gained continuous attention from research community for years. In this section, we will introduce related works of problem formuliza-tion, various frameworks and features used in automatic author dis-ambiguation.

Most previous work formulized author disambiguation problem as a statistical learning problem. Han et al. formulated it as a clus-tering problem and solved it by a model-based k-means algorithm in [4] and by a K-way spectral clustering algorithm in [5]. They al-so viewed the problem as a classification problem in [6]. The main idea is to train a classifier to identify whether two publications be-long to the same author. A graph partition based approach using Quasi-Clique is proposed by [2]. Treeratpituk and Giles [10] also proposed a random forest algorithm.

The framework for author disambiguation has also been studied comprehensively. On et al. [2] proposed a two-step framework which first divides publications into small blocks to reduce com-putational cost. Several work such as [1] focused on the blocking approaches. Fan et al. [3] proposed the graph-based framework for author disambiguation. However, performance issue in the graph framework is a big problem. Wang et al. [11] lightened the di ty by changing clustering on papers to clustering on atomic clusters. But for digital libraries with millions of publications, graph based framework is still not the best choice. Huang et al. [7] proposed another two-step framework for large-scale author disambiguation. It learned the similarity function on pair of publications in the first step and performs clustering using the learned similarity in the sec-ond step. We adopt a similar framework and the di ff erence is that we improve the framework by splitting its first step into high preci-sion similarity learning and high recall similarity learning. Features used in author disambiguation are also widely studied. Metadata including automatic extracted data are taken as features in many work [5, 10]. Tan et al. [9] introduced a useful feature named Inverse Host Frequency (IHF) by using search engine for author disambiguation. Song et al. [8] employed PLSA and LDA to get research topics for authors and then disambiguate them. All of the above work did not investigate the features related to high precision classification or high recall classification which are proposed in this paper.
Fully automatic author disambiguation is infeasible due to the data quality problem and the data sparsity problem.

Data quality problem is very common in large scale digital li-braries which often integrate data from di ff erent sources such as CrossRef, pdf files, and etc. However, such data often contain er-rors, conflict with each other, or have many variations. The data quality problem could confuse the automatic algorithms a lot and harm the disambiguation accuracy greatly.

Data sparsity problem means many publications lack necessary metadata information. For example, it is proved that email and a ffi liation are very useful for finding correct author. However, in our system, only about 10% publications have the email metadata and 23% have a ffi liation information. Without necessary signals, author disambiguation cannot be fully solved automatically.
One possible solution for fully solving the author disambigua-tion problem is to leverage human judgment to correct the errors of automatic algorithms. To valid the possibility, we perform a case study on authors named  X  X ei Zhang X . The algorithm proposed in [6] is used before human labeling. 117 di ff erent authors are split into 141 clusters. Three human labelers were asked to correct the results. Finally they spent 7 hours on average to finish the task. Part (a) in Figure 1 illustrates the situation for human labeling. The hu-Figure 1: Comparison between Conventional Author Disam-biguation and LOAD man labeling process is very time-consuming for several reasons: First, publications in each cluster are not pure enough and human labelers still need to check them one by one. For example, the most diverse cluster contains publications from 27 di ff erent authors. This process is nearly equal to the fully manual labeling. Second, as one author X  X  publications could be assigned to several di ff erent clusters, human labelers need to go through many clusters to find all results. For the  X  X ei Zhang X  from PolyU (Hongkong Polytechnic Universi-ty), his publications are assigned to 8 di ff erent authors. So at least 8 clusters need to be checked one by one to finish the labeling.
From the discussions above, we can find that to fully solve the author disambiguation problem, we need to design an algorithm to assist human labeling e ffi ciently from these perspectives: 1) Most ambiguous cases need to be solved by automatic author disam-biguation before human labeling. 2) Human labeling need to be conducted on bigger granularity rather than on each individual pub-lication. 3) Human labeling need to be limited within a narrow range rather than the whole publication list. 4) Human labeling need to be started from some good starting points.
In this section, we propose an approach named LOAD for au-thor disambiguation. First we provide an overview of the LOAD and how we formulate it. Then we study a key problem, i.e., the pairwise classification with rich features in LOAD. We start from an example to show the framework in Figure 2. Step 0 is the initial state which shows several publications authored by  X  X ei Zhang X  at PolyU,  X  X ei Zhang X  at Microsoft, and  X  X ei Zhang X  at SJTU. In Step 1, all publications are clustered into 5 clusters. No-tice that each HPC only contains publications of one author here. In Step 2, HPCs are further clustered into several HRCs. Publica-tions of both  X  X ei Zhang X  at Microsoft and  X  X ei Zhang X  at SJTU are placed into the same cluster HRC 2 . In Step 3, HPC 1 Zhang X  at PolyU and HPC 4 of  X  X ei Zhang X  at Microsoft are select-ed as starting points for labeling. Human labeling starts from the results of Step 3. Once we labeled HPC 1 in HRC 1 as publications of  X  X ei Zhang X  at PolyU, we can easily find his rest publications from HPC 2 in the same HRC. Meanwhile, once we labeled HPC as publications of  X  X ei Zhang X  at Microsoft, HPC 3 is ranked high-er for labeling. When all his publications are labeled, we can find that the rest publications in HPC 5 belongs exactly to  X  X ei Zhang X  at SJTU. The output of Step 3 is also shown as Part (b) in Figure 1. Comparing to the confused cases shown as Part (a) in Figure 1, Part(b) is much easier for human labeling. The comparison shows the benefit of our proposed LOAD approach intuitively.

The three-step framework of LOAD is: (1) High Precision Clus-tering: It tries to place one authors X  publications into as fewer clus-ters as possible, while tries to keep all publications in each cluster written by the same author. Thus, human labeling can be conducted on clusters rather than on individual publications. (2) High Recall Clustering: It is conducted on the results of HPCs, it is di from HPC in two aspects: First, the goal of HRC is trying to put all publications of one author into the same cluster. Second, each HPC is totally contained in a HRC. (3) Top Dissimilar HPCs Selection and Ranking: after we get the HPCs and HRCs, we can select the best HPC in each HRC as the exemplar clusters for human labeling.
Now we formulize the concepts and goals of LOAD in this sec-tion. Given a set of publications X = f x 1 ; x 2 ; ; x n cation x i has p authors xa i = f xa i 1 ; xa i 2 ;:::; xa blocking method described in [7], publications can be first clustered into blocks according to author name string similarity. So in each name block we only need to consider the disambiguation problem for one author name from each paper. We use Y = f y 1 ; y indicate the concerned author of related publications in X respec-tively, y i 2 xa i ; i 2 [1 ; n ]. Each y i correspond to one of the real world authors A = f a 1 ; a 2 ;:::; a h g . Then each step in LOAD can be formulized as follows.
 High Precision Clustering
Assume n publications X = f x 1 ; x 2 ; ; x n g are clustered into l clusters C = f c 1 ; c 2 ; ; c l g . The goal is to maximize the overall similarity V HPC in Equation (1) while keeping the high precision of each cluster. The c p and c q here are HPC clusters. High Recall Clustering
The goal of step 2 is to place all publications of one author into only one cluster. It can be formulated in the similar way like the first step. We process high recall clustering by maximizing the sim-ilarity V HRC in Equation (2) while ensuring the diversity between each cluster. The c p and c q here are the HRC clusters. Top Dissimilar HPCs Selection and Ranking
For all HPCs in one HRC, we pick up the best HPC, i.e., HPC as an exemplar cluster for human labeling. We formally define the task in Equation (3). It means the HPCs with bigger size and higher overall inner similarity will have better chances of being selected. These exemplar clusters often corresponds to important authors for they often have more and correlated publications.
 Once people select one HPC, the rest HPCs in that HRC can be re-ranked according to the similarity with the labeled HPC. The similarity can be calculated in Equation (4).
 Through the reranking approach, users need only to check whether top ranked HPC can be merged with labeled HPCs, which further reduces the human labeling e ff orts.
The object function in Section 4.1.2 is a little di ffi cult to learn, as our goal is to apply the algorithm in the large scale digital library, we need to find a less optimization but more e ffi cient algorithm. In this section, we first analyze the formulations and simplify them in-to two sub-problems, the pairwise similarity learning problem and the clustering problem. Then we introduce the sub-problems re-spectively.
 Formulations Simplification
We take Equation (1) as example to analysis how to simplify the optimization process. The goal of (1) is to place publications of the same author into the same cluster. At the same time, avoid to place publications of di ff erent authors into the same cluster. We propose to simplify the formulation into two steps by combining the ap-proaches proposed by [2, 6, 7]. First, a similarity function between two publications is trained. Then, we can cluster the publications according to the similarity generated by the learned similarity func-tion. Both Equation (1) and (2) can be approximated with these two steps. As explained in Section 4.1.1, HPC and HRC should be trained separately. Section 4.2 will discuss the details. Pairwise Similarity Learning
Many previous work studied the problem of training a weighted similarity function [4, 5, 6, 7]. Our approach exploits the simi-lar idea. The key problem is to find an adequate function which can represent the similarity / dissimilarity between the publication pairs. In the proposed approach, we view the similarity calculation between publication pairs as a classification problem. First, infor-mation about a publication pair is represented in a feature vector ! x . Then a pairwise classification model is trained to judge whether the two publications are similar. We used the linear model showed in Equation (5) to compute the similarity for each paper pair. The weight light toolkit 2 . Clustering
How to cluster publications e ffi ciently in large scale digital li-braries is a big challenge. In some previous work [11], the agglom-erative clustering algorithm has been proved to be very e and e ffi cient. So we exploit it as our clustering algorithm. Yes we can also try more powerful clustering algorithms. However, to show the idea of our proposed approach, only this clustering algo-rithm is tried.
In this section, we discuss in detail about how we conduct the high precision clustering and high recall clustering with rich fea-tures respectively. As explained in Section 4.1.3, the clustering algorithm used in both steps are the same. So we only focus on how to learn two classification models e ff ectively here.
Pairwise classification for HPC is to judge whether two publica-tions are authored by the same person, so all features are defined via comparison of a pair of publications. In order to achieve high precision, we drill down features into di ff erent value scales. Two types of measures, i.e., SIF (Shared Item Frequency) and IPF (In-verse Publication Frequency) are also defined inspired by the idea of TF and IDF.

Suppose a publication has a list of metadata such as a list of coauthors and a list of references, we define SIF to represent the http: // svmlight.joachims.org / similarity of two lists I 1 and I 2 : From another perspective, IPF is defined to discount the importance of common features. Suppose A = I 1 ; I 2 ; ; I n is the set of all feature list, i is a feature that appears in one or several list, IPF defined as follows: The IPF to compare two lists is defined as the sum of shared IPF features.
 Referred to the above definitions, we adopted the strict comparison and IPF features of the metadata "name", "email", "a ffi liation" and "homepage" between two authors; also, the SIF and IPF features of "coauthor name", "coauthor email", "coauthor a ffi liation", "coau-thor homepage", "title bigram", "reference" and "download link" are extracted. Besides, a binary feature "self citation" which rep-resents whether one publication cited the other, or vice versa are extracted; the "publishing year" interval between two papers are also considered.
The high precision classifier aims at collecting high confidence signals while the high recall classifier aims at collecting as much signals as possible. The basic high recall features are in accordance with high precision features. Here, we only list the di ff
Email : We split the email into prefix and su ffi x by  X  X  X . It is because some authors might change the email su ffi xes but tend to reserve the same prefixes. Besides, the email su ffi x might reflect the author X  X  organization. When comparing, common email su ffi (e.g. @gmail.com, @yahoo.com) are removed.

A ffi liation : The a ffi liation information is segmented and parsed into three groups: university, department and group name. We will compare these three groups of information separately. Some heuristic rules are defined to segment and parse the variations of a ffi liation information.

Homepage : Authors from the same organization often share the same domain for their homepage. Thus,  X  X omepage X  is further split into domain and su ffi x for comparison.

Coauthor related feature : For the coauthor email, a ffi liation and homepage, similar parsing is performed to obtain more infor-mation.

Other features : for all other metadata, IPF features are removed to ensure a high recall.
Two internal data sets and one public data set are used in our experiments. Authors of publications were manually labeled by human experts.

CS data set: The CS(Case Study) data set is created when we conduct the case study for author disambiguation. Publications in this set are highly ambiguous. To avoid the case that the dis-ambiguation algorithm might harm unambiguous authors, we also added some unambiguous authors here.
UE data set: As mentions before, our system opens a feature for users to edit and correct author X  X  publication list. All such user edited data are collected as UE(User Edited) data set.
 DBLP data set: The DBLP (Digital Bibliography &amp; Library Project) dataset is an open source data set 3 . There are 9,160 la-beled publications in it. The publications without any metadata in our system are removed. After the cleaning, 7,434 publications are taken as the data set.

Basic statistics of three data sets are given in Table 1. As one au-thor can have several di ff erent name variations, number of authors can be less than the number of names. UE is just the case.
Statistics CS UE DBLP Total # of publications 1,322 4,624 7,434 13,380 # of names 54 1,199 739 1,992 # of authors 171 1,052 1,267 2,490
Avg / Max authors 3.96 / 117 1.01 / 6 1.71 / 17 2.23 / per name
Avg / Max names 1.25 / 4 1.15 / 5 1.00 / 2 1.13 / 4 per author
Avg / Max publica-7.73 / 323 4.40 / 131 5.89 / 129 6.01 tions per authors
As LOAD is to help users to edit the publication list, we evaluate it from several perspectives of user contribution.
 First, we evaluate the results by using accuracy.
 The evaluation measure is described in Equation (9).
 If one publication in the cluster is incorrect, the whole cluster will be taken as incorrect. For HPC cluster, if all publications in it be-long to the same author, the cluster is counted as  X  X orrect X , oth-erwise  X  X ncorrect X . For HRC cluster, if all authors in it have no publications in other clusters, this cluster is counted as  X  X orrect X , otherwise  X  X ncorrect X .
 and number of authors in HRCs.

For HPCs, to avoid the case that each HPC contains only one publication, we evaluate the number of publications in them. For HRCs, to avoid the case that all publications are placed into the same HRC, we evaluate the number of authors in them.
We take the human labeling after conventional automatic author disambiguation as the baseline. In baseline, we reduce the two step-s (clustering of HPCs and HRCs) in LOAD into one step, and then cluster the publications using the same algorithm in LOAD. More-over, the baseline uses both HPC related and HRC related features. Strictly speaking, the baseline is more powerful than conventional automatic author disambiguation algorithms.
We first evaluate the accuracy by taking the whole cluster as a base unit. Results are shown in Table 2 and Table 3. Notice that the baseline methods in two tables are actually the same while the http: // dblp.uni-trier.de / xml / results are di ff erent because: Table 2 lists the results of finding "whether all publications in the cluster belong to the same author" while Table 3 lists the results of finding "whether all publications of authors can be find in the same cluster".
 From the results we can see that the accuracy of both HPC and HRC are greatly improved comparing to the baseline. In three data sets, HPCs archive nearly 100% precision. HRCs also have much better accuracy than baseline.

The average publication number in each cluster for HPC, HRC, and baseline is shown in Table 4. The results of HRC are com-parable with those in the baseline. We also compared the average author number in each cluster for HPC, HRC, and baseline. The results are shown in Table 5. Average author number in HRC is 1.43. That is to say, labelers only need to label one or two authors in each HRC.
 Table 4: Comparison of Number of Publications in Clusters To investigate how much labeling time can be saved through LOAD, we compare it with two other methods, i.e., fully manual labeling and labeling after conventional automatic disambiguation (baseline). For simplicity, the comparison is performed in a pub-lication list with the same author name. We take the total number of publication as n , number of authors as m , reading time for each publication as r , comparing time for each publication as c , d discount factor in the i th method.

In the first approach, human labelers need to go through all the publications. For each publication, they first need to read the basic information, and then compare the current publication X  X  authorship with previous labeled authors. The reading time can be estimated as O ( r n ) and comparing time as O ( c n m d 1 ) here.
In the second approach, as one cluster may contain several au-thors X  publications while an author X  X  publications may be assigned to several di ff erent clusters, labelers still need to conduct labeling on individual publications in all clusters. The reading time is still O ( r n ) and the comparing time is O ( c n m d 2 ).

In the third approach (LOAD), labelers can view the whole HPC as one unit. Let average size of HPC be n 1 , the estimated read-ing time will be O ( r n 1 n average HPC number in each HRC becomes n 2 n ison can be limited within the HRC, the number of comparison beled HPC need only to be compared with the previous labeled one. The number of comparison can be further approximated as O ( c n n
Table 6 lists the estimated time for three methods. It also list the real labeling time for  X  X ei Zhang X . Compared to the baseline, LOAD improves the label e ffi ciency by about 11 times.
Approaches Estimated T ime Labeling Time Manual O ( r n + c n m d 1 ) 102,000 Baseline O ( r n + c n m d 2 ) 26,400
LOAD O (( r n 1 n
We also estimate the parameters during the case study to CS data set: r = 15, c = 10, d 1 = 0 : 3, d 2 = 0 : 1, and d 3 = 0 n = 9. According to the estimated time, LOAD improves the label e ffi ciency about 30 times compared to the baseline method for  X  X ei Zhang X . The gap between estimated time and labeling time is due to that LOAD cannot get 100% HRC accuracy so extra e ff orts are needed. Anyway, roughly speaking, LOAD can improve the human labeling e ffi ciency about 10 to 30 times.
Pairwise classification is very important for LOAD. We conduct further experiments to validate the e ff ectiveness of the rich features for building the high precision and the high recall classifiers.
The pairwise classification is to judge whether two publication-s are written by the same author. Thus, publication pairs written by the same author are taken as positive instances, otherwise pairs with the same names but belong to di ff erent authors as negative in-stances. We combine both CS and UE data sets described in Section 5.1 as the new pairwise data set. About 2 / 3 instances are randomly selected for training and the rest for testing. The total positive pairs are 37,629, negative pairs are 241,178.
 The experimental results for pairwise classification are shown in Table 7.

Both pairwise classifiers for HPC and for HRC achieve quite high precision and recall. Although the the error will be amplified with the increase of cluster size, the very accurate results of pair-wise classification can help to get the high accuracy for final results for labeling.
This paper proposes a novel approach named LOAD to solve the author disambiguation problem together with users. The proposed approach follows a three-step framework which is very flexible and scalable for large scale author disambiguation in digital libraries. Comparing to conventional automatic disambiguation algorithms, LOAD can improve disambiguation accuracy greatly and save a lot of labeling time for human labelers.

There are several possible directions of future work. First, it would be interesting to design an iterative process for author dis-ambiguation. Semi-supervised learning algorithms can then be im-ported to improve the accuracy. Second, we need to enlarge the feature sources to improve the automatic disambiguation accuracy. Also, more directly optimization algorithms can be applied to get better results.
We thank Microsoft Academic Search team for their encourage-ments and supports. We thank Hang Li and the anonymous review-ers for their comments to this paper. [1] M. Bilenko, R. J. Mooney, W. W. Cohen, P. D. Ravikumar, [2] O. Byung-won, D. Lee, J. Kang, and P. Mitra. Comparative [3] X. Fan, J. Wang, L. Bing, L. Zhou, and W. Hu. Ghost: an [4] H. Han, H. Zha, and C. L. Giles. A model-based k-means [5] H. Han, H. Zha, and C. L. Giles. Name disambiguation in [6] H. Han, H. Zha, C. Li, K. Tsioutsiouliklis, and C. L. GILES. [7] J. Huang, S. Ertekin, and C. L. Giles. E ffi cient name [8] Y. Song, J. Huang, I. G. Councill, J. Li, and C. L. Giles. [9] Y. F. Tan, M. yen Kan, and D. Lee. Search engine driven [10] P. Treeratpituk and C. L. Giles. Disambiguating authors in [11] F. Wang, J. Li, J. Tang, J. Zhang, and K. Wang. Name
