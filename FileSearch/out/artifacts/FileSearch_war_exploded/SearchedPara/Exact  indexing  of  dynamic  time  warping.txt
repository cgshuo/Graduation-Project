
The indexing of very large time series databases has attracted the attention of the database community in recen t years. The vast majority of work in this area has focused on indexing under the Euclidean distance metric (Agrawal et al. 1995; Chan et al. 2003; Das et al. 1998; Debregeas and Hebrail 1998; Faloutsos et al. 1994;
Keogh et al. 2000, 2001; Korn et al. 1997; Yi and Faloutsos 2000). However, there is an increasing awareness that the Euclidean distance is a very brittle distance measure (Aach and Church 2001; Bar-Joseph et al. 2002; Berndt and Clifford 1994; Chu et al. 2002; Diez and Gonzalez 2000; Kadous 1999; Keogh and Pazzani 2000; Kollios allows an elastic shifting of the time axis, to accommodate sequences that are similar but out of phase, as shown in Fig. 1. Just such a technique, based on dynamic programming, has long been known to the s peech-processing community (Itakura 1975; Kruskall and Liberman 1983; Myers et al. 1980; Rabiner and Juang 1993; Rabiner et al. 1978; Sakoe and Chiba 1978; Tappert and Das 1978). Berndt and
Clifford introduced the technique, dynamic time warping (DTW), to the database community (Berndt and Clifford 1994). Although they demonstrate the utility of the approach, they acknowledge that its resistance to indexing is a problem and that  X  ... performance on very large databases may be a limitation . X  Despite this shortcoming of DTW, it is still widely used in various fields. In bioinformatics, Aach and Church successfu lly applied DTW to RNA expression data (Aach and
Church 2001). In chemical engineering, it has been used for the synchronization and monitoring of batch processes in polymerization (Gollmer and Posten 1995). DTW has been successfully used to align biometric data, such as gait (Gavrila and Davis 1995), signatures (Munich and Perona 1999) and even fingerprints (Kovacs-
Vajna 2000). Many researchers, including Caiani et al. (1998), have demonstrated the utility of DTW for ECG pattern matching. Rath and Manmatha have applied DTW to the problem of indexing repositories of handwritten historical documents (Rath and Manmatha 2002) (although handwriting i s two-dimensional, it can trivially be rerepresented as a one-dimensional time series). Finally, in robotics, Schmill et al. demonstrated a technique that utilizes DTW to cluster an agent X  X  sensory outputs (Schmill et al. 1999).
 mon is that they need to find the best match to a query time series, from a (possibly very large) pool of candidates. This can trivially be achieved by sequential scanning, comparing each and every candidate to the query, in an arbitrary order. The problem with sequential scanning is that it is simply too slow for most applications. What we really need is a technique to index the data, that is, to find the best match with-out having to examine every candidate (Roussopoulos et al. 1995; Seidl and Kriegel 1998). Indexing techniques can be exact , guaranteeing to return the same result as sequential scanning (Faloutsos et al. 1994; Keogh et al. 2000, 2001), or approxi-mate , returning good matches but not necessa rily the best matches (Faloutsos and Lin 1995; Park et al. 2001; Yi et al. 1998).
 the Euclidean distance (Chan et al. 2003; Faloutsos et al. 1994; Keogh et al. 2000; Yi and Faloutsos 2000) (see Keogh et al. (2001) for a more comprehensive listing).
In addition, several researchers have shown techniques to approximately index DTW (Park, personal communication) or intr oduced methods to reduce its demanding CPU time (Berndt and Clifford 1994; Chu et al. 2002). However, only two researchers have claimed to introduce an exact indexing technique for DTW (Kim et al. 2001; Park et al. 1999). In the case of Park et al. (1999), the claim of no false dismissals was later retracted (Park et al. 1999, 2001). We have carefully implemented the only technique to correctly claim the ability to exactly index DTW and the only other lower bound-ing approximation of DTW for detailed comparison with our proposed approach.
In contrast with the approaches above, we will prove the no-false-dismissal prop-erty of our approach and demonstrate its superiority with the most comprehensive set of time series indexing experiments ever undertaken. In particular, in terms of number and diversity of datasets, size of datasets, range of query lengths and index-ing parameters, our experiments are one to two orders of magnitude more than all previous papers combined.

The rest of the paper is organized as follows. In Sect. 2, we will consider the util-ity of time series similarity search, revie w the DTW algorithm, and consider related work. In Sect. 3, we will introduce a novel lower bounding technique that tightly approximates the true DTW distance. Section 4 introduces a method that allows the exact indexing using our lower bounding function. In Sect. 5, we conduct an ex-haustive empirical comparison of our method with competing techniques. Finally, in
Sect. 6, we offer conclusions and suggestions for extensions. exploration of very large databases; it is also a subroutine in many data mining appli-cations, including rule discovery (Das et al. 1998), clustering (Debregeas and Hebrail 1998) and classification (Diez and Gonzalez 2000; Kadous 1999). The superiority of DTW over Euclidean distance for these ta sks has been demonstrated by several authors (Aach and Church 2001; Bar-Joseph et al. 2002; Caiani et al. 1998; Chu et al. 2002; Keogh and Pazzani 2000; Yi et al. 1998). However, for completeness, we include a simple experiment to illustrate the point.

The most studied time series classification/clustering problem is the cylinder X  X ell X  funnel dataset (Diez and Gonzalez 2000; Kadous 1999); it is a deceptively simple-looking three-class problem. All classes are of length 128. The cylinder class consists of a short flat section, followed by a sudden jump to an elevated plateau, and a return of the plateau itself are controlled by random variables. The bell class replaces the plateau with a ramp and the funnel class is simply the mirror image of the bell class.
Finally, all instances are corrupted by Gaussian noise. More details about the dataset and a data generator can be downloaded from the UCR Time Series Data Mining
Archive (http://www.cs.ucr.edu/  X eamonn/ TSDMA). Figure 2 shows typical examples of each class.

The problem has been attacked with sophisti cated techniques, including rule-base learners (Diez and Gonzalez 2000; Kadous 1999), boosting, Bayesian techniques, and decision trees. We performed a simple classification experiment on this dataset, using the one-nearest neighbor algorithm. Our dataset consists of ten instances of each class and the classifier was evaluated using the leaving-one-out strategy. Because we had the luxury of unlimited data, we averaged the results over 1,000 runs. The mean error rate for the Euclidean distance metric on the problem was 0.2734, but for DTW, it was only 0.0269, an order of magnitude lower. This off-the-shelf result is competitive with the highly tuned, sophisticated tec hniques enumerated above. The lower error rate came with a cost, however; classification with DTW took approximately 230 times longer than that with Euclidean distance.
 it. Suppose we have two time series, Q and C, of length n and m , respectively, where ( i th , j th ) element of the matrix contains the distance d q i and c j (i.e. d the alignment between the points q i and c j . This is illustrated in Fig. 3. A warping path W is a contiguous (in the sense stated below) set of matrix elements that defines a mapping between Q and C .The k th element of W is defined as we have The warping path is typically subject to several constraints.
  X 
Boundary conditions: w 1 = ( 1 , 1 ) and w K = ( m , n ) path to start and finish in diagonally opposite corner cells of the matrix.  X 
Continuity: Given w k = ( a , b ) ,then w k  X  1 = ( a , b b  X  b  X  1. This restricts the allowable steps in the warping path to adjacent cells (including diagonally adjacent cells).  X 
Monotonicity: Given w k = ( a , b ) ,then w k  X  1 = ( a , b  X  b  X  0. This forces the points in W to be monotonically spaced in time. There are exponentially many warping pat hs that satisfy the above conditions. However, we are only interested in the path that minimizes the warping cost:
This path can be found using dynamic programming to evaluate the following Re-currence, which defines the cumulative distance  X ( i , j in the current cell and the minimum of th e cumulative distances of the adjacent elements: The Euclidean distance between two se quences can be seen as a special case of
DTW where the k th element of W is constrained such that
Note that it is only defined in the special case where the two sequences have the same length. The time and space complexity of DTW is O( nm ).
 kall and Liberman (1983) and Rabiner and Juang (1993) for a more detailed treat-ment.
While there has been much work on indexing time series under the Euclidean metric (Chan et al. 2003; Faloutsos et al. 1994; Keogh et al. 2000, 2001; Yi and Faloutsos 2000), there has been much less progress on indexing under DTW.
 utilizes their FastMap technique (Falout sos and Lin 1995). The idea is to embed the sequences into Euclid ean space such that the dista nces between them are ap-proximately preserved, then classic multidimensional index structures can be utilized (Guttman 1984; Seidl and Kriegel 1988). In addition, they introduced a lower bound-ing function (described in more detail in Sect. 3.2) that can be used to prune some of the inevitable false hits their met hod will introduce. The method does produce an observed (maximum) speedup of 7.8 over sequential scanning. However, this does have some limitations. First, it does allow false dismissals. Second, while the time to build the index is linear in M (the size of the database), it is actually O( Mn which quickly becomes intractable for very large databases and/or long sequences. (Kim et al. 2001). The method extracts four features from the sequences and orga-nizes them in a multidimensional index structure. They introduced a lower bounding function (described in more detail in Sect. 3.2) that is defined on the four features and thus guarantees no false dismissals. Although the work introduced the first tech-nique for exact indexing under DTW, it suffers from several limitations. First, the method only allows the extraction of exactly four features and thus cannot take ad-vantage of multidimensional index structure s. In addition, although four features are extracted, only one of them (determined a t query time) is actually used in the lower bounding function; thus, the lower bound is very loose and many false alarms are generated, each of which will require evaluation with the quadratic-time DTW al-gorithm.
 based on a piecewise linear representation of the data. They prove that this method can guarantee no false dismissals. Unfortunately, the no-false-dismissals claim is in-correct. A candidate sequence in the data base can differ from the query sequence by an arbitrarily small epsilon and still not be retrieved (Park, personal communica-tion). A later version of the paper did carry a disclaimer stating,  X  that a subsequence similar to a query in terms of the original time warping distance may not be included in the answer set in our approach  X  (Park et al. 2001). However, this qualification understates the problem. Having tested the approach with 39,200 experiments on 32 different datasets, we found that the approach only returned the true best match to a one-nearest neighbor query 613 times. This result does not significantly differ from random chance. We therefore exclude this approach from further consideration.
 the method is interesting, we do not incl ude it in our empirical comparisons because the index size is one to two orders of magnitude larger than the data itself. Such enormous space overhead is simply untenable for very large databases. In any case, the claimed speedup is rather modest.
There has also been some work in which attempts at indexing and/or lower bounding are abandoned, and instead, efforts are concentrated on fast approximation of the DTW distance using a lower resolution approximation of the data. The idea was introduced by Keogh and Pazzani (2000), who use a piecewise linear approxi-mation of the data. The method shows significant speedup with few false dismissals.
A similar idea was suggested by Chan et al. (2003). Here, the authors obtain the lower resolution of the data approximation with wavelets and use their approximate distance measure instead of that of Yi et al., i.e. the lower bounding measure, within the FastMap framework. The method improves the speedup of the work of Yi et al. work at the expense of introducing more false dismissals.

Finally, there has been some work on obtaining warping alignments by methods other than DTW (Bar-Joseph et al. 2002; Kwong et al. 1996). For example, Kwong et al. consider a genetic algorithm-based approach (Kwong et al. 1996), and recent work by Bar-Joseph et al. considers a technique based on linear transformations of spline-based approximations (Bar-Joseph et al. 2002). However, both methods are stochastic and require multiple runs (possibly with parameter changes) to achieve an acceptable alignment. In addition, bot h methods are clearly nonindexable. How-ever, both works do reiterate the superiority of warping over nonwarping for pattern matching.
In this section, we explain the importance of lower bounding and introduce our new lower bounding distance measure.
Time series similarity search under the Euclidean metric is heavily I/O bound; how-ever, similarity search under DTW is also very demanding in terms of CPU time. One way to address this problem is to use a fast lower bounding function to help prune sequences that could not possibly be the best match. Table 2 gives the pseudo-code for such an algorithm.
  X  It must be fast to compute. Clearly, a measure that takes as long to compute  X  It must be a relatively tight lower bound. A function can achieve a trivial lower have been studied extensively (Kruskall and Liberman 1983), there has been far less work on DTW, which is very similar in spirit to its discrete cousins. Below, we will consider the existing DTW lower bounding techniques.
To the best of our knowledge, there are only two existing lower bounding functions available for DTW (not including Park et al. (1999), which incorrectly claims to be lower bounding, or Park et al. (2000), which has a time complexity equal to the full algorithm). While referring the interested reader to the original papers for detailed explanations, below, we give a visual intuition and brief explanation of each. to as LB_Kim), works by extracting a f our-tuple feature vector from each sequence.
The features are the first and last elements of the sequence, together with the max-imum and minimum values. The maximum squared differences of corresponding features are reported as the lower bound. Figure 4 illustrates the idea. to as LB_Yi) takes advantage of the observation that all the points in one sequence that are larger (smaller) than the maximum (minimum) of the other sequence must contribute at least the squared difference of their value and the maximum (mini-mum) value of the other sequence to the final DTW distance. Figure 5 illustrates the idea.
Before introducing our lower bounding tec hnique, we must review additional details of the DTW algorithm that we deliberately omitted until now.
In addition to the constraints on the warping path enumerated in Sect. 2.1, virtually all practitioners using DTW also constrain the warping path in a global sense by limiting how far it may stray from the diagonal (Berndt and Clifford 1994; Chu et al. 2002; Gollmer and Posten 1995; Itakura 1975; Keogh and Pazzani 2000; Myers et al. 1980; Sakoe and Chiba 1978; Tappert and Das 1978). The subset of the matrix that the warping path is allowed to visit is called the warping window. Figure 6 illustrates two of the most frequently used global constraints, the Sakoe-Chiba band (Sakoe and Chiba 1978) and the Itakura parallelogram (Itakura 1975).

There are several reasons for using global constraints, one of which is that they slightly speed up the DTW distance calculation. However, the most important rea-son is to prevent pathological warpings, where a relatively small section of one se-quence maps onto a relatively large section of another. The importance of global constraints was documented by the originators of the DTW algorithm, who were exclusively interested in aligning speech patterns (Sakoe and Chiba 1978). However, it has been empirically confirmed in other settings, including finance, medicine, bio-metrics, chemistry, astronomy, robotics, and industry.
In addition to the global constraints listed above, there has been active research on local constraints (Itakura 1975; Myers et al. 1980; Rabiner and Juang 1993; Sakoe and Chiba 1978; Tappert and Das 1978) for several decades. The basic idea is to limit the permissible warping paths, by providing local restrictions on the set of alternative steps considered. For exampl e, we can visualize Eq. (5) as a diagram of warping path may take at each stage. We could replace Eq. (5) with min {  X ( i  X  1 , j  X  1 ),  X ( i  X  1 , j  X  2 ),  X ( i  X  2 , j pattern shown in Fig. 7c. Using this equation, the warping path is forced to move one diagonal step for each step parallel to an axis. The effective intensity of the slope constraint can be measured by P = n / m . Figures 7.a to 7.d illustrate the four original constraints suggested by Sako e and Chiba (1978); in addition, many others have been suggested, including asymmetric ones. The constraint might be designed based on domain knowledge, or from experience learned through trial and error.
Rabiner and Juang X  X  classic paper contains an extensive review (Rabiner and Juang 1993). The important implication of local constraints for our work is the fact that they can be reinterpreted as global constraints. Figure 7.e shows an example.
Create a shadow matrix, which is the same size as the DTW matrix. Initialize all the elements of the shadow matrix as unr eachable. Call the DTW function, using the relevant constraint; every time the recurrence visits a new cell
DTW matrix, the cell ( i , j ) of the shadow matrix can be labeled as reachable. The convex hull of all the reachable cells form s a band, which can be interpreted as a global constraint. Note that we only have to do this once and we can then store the resulting constraint for future use. While this reinterpretation of local constraints to our knowledge. Finally, we note that global and local constraints can be used together; the interpretation being that, where they conflict, the most restrictive con-straint (i.e. the one that forces the path closest to the diagonal line) is used (Kruskall and Liberman 1983).
We can view a global or local constraint as constraining the indices of the warping path w k = ( i , j ) k such that j  X  r  X  i  X  j + r ,where r is a term defining the reach , or allowed range of warping, for a given point in a sequence. In the case of the Sakoe-Chiba band, r is independent of i ; for the Itakura parallelogram, r is a function of i .
 We will use the term r to define two new sequences, U and L:
U and L stand for Upper and Lower , respectively; we can see why if we plot them together with the original sequence Q as in Fig. 8. They form a bounding envelope that encloses Q from above and below. Note that, although the Sakoe-Chiba band is of constant width, the corresponding envelope generally is not of uniform thickness.
In particular, the envelope is wider when the underlying query sequence is changing rapidly, and narrower when the query sequence plateaus.

An obvious but important property of U and L is the following: for DTW.
 part of the candidate matching sequence not falling within the envelope and the nearest (orthogonal) corresponding section o f the envelope. Figure 9 illustrates the idea.
 ogram provides a tighter bound than the Sakoe-Chiba band does, and both appear tighter than LB_Kim or LB_Yi in Figs. 4 and 5, respectively.

Proposition 1. For any two sequences Q and C of the same length n , for any global constraint on the warping path of the form j  X  r  X  i  X  j + holds: LB_Keogh(Q,C)  X  DTW(Q,C)
Proof. Wewishtoprove Our strategy will be to assume the opposite and show that it leads to a contradiction.
Assume
Because the terms under the radicals are positive, we can square both sides:
From Eq. (3), we know that n  X  K (with 0  X  K  X  n  X  every term on the left-hand side (LHS), with a unique term on the right-hand side (RHS), leaving K  X  n terms unmatched.
We will map the i th term on the LHS with one of the i a single i ; so to enforce the desired one-to-one mapping, we will map to the one with the lowest value for j . All the other w k  X  X  are placed in the unmatched summation.
For the moment, let us ignore the unmatched terms and see what relationship exists between just the matched terms and th e LHS. There are three cases to consider; let us consider the case when c i &gt; U i :
Because we have n = m (recall LB_Keogh is only defined when j  X  r  X  i  X  j + r ,  X  i  X  r  X  j  X  i + r , so we can rewrite the RHS as
If we remove all terms except q j from the RHS, we are left with
The case when c i &lt; L i yields to a similar argument. The third case yields
But if all the matched terms in on the LHS, then the only hope of our assumption being correct is if is a negative number, but the sum of squared terms can never be negative! LB_Keogh(Q,C)  X  DTW(Q,C).

Virtually all approaches to indexing time series under the Euclidean distance that guarantee no false dismissals use the GEMINI framework of Faloutsos et al. (Chan et al. 2003; Faloutsos et al. 1994; Keogh et al. 2000, 2001; Korn et al. 1997; Yi and Faloutsos 2000). Using the GEMINI framework, all one has to do is to choose a high level representation of the data and define a lower bounding measure on it (Faloutsos et al. 1994). Many such representations have been suggested, including
Fourier transforms (Faloutsos et al. 1994), Wavelets (Chan et al. 2003), singular value decomposition (Korn et al. 1997), adaptive piecewise aggregate approximation (Keogh et al. 2001), and a simple technique independently introduced by two authors called piecewise aggregate approximation (PAA) (Keogh et al. 2000; Yi and Falout-sos 2000). This technique is attractive because it is simple, intuitive, and competitive with the other more complex approaches. In this section, we will show that PAA can be adapted to allow indexing under DTW. We begin with a brief review of PAA.
We have previously denoted a time series as C = c quence in our database is n units long. Let N be the dimensionality of the space we wish to index (1  X  N  X  n ). For convenience, we assume that N is a factor of n . While this is not a requirement of our approach, it does simplify the notation.
Simply stated, to reduce the time series from n dimensions to N dimensions, the data is divided into N equal-sized frames. The mean value of the data falling within a frame is calculated, and a vector of th ese values becomes the data-reduced rep-resentation. The complicated subscripting in Eq. (10) just insures that the original sequence is divided into the correct number and size of frames. The representation can best be visualized as an attempt to model the original time series with a linear combination of box basis functions as shown in Fig. 10.
Given two original sequences Q and C, we can transform them into using Eq. (10), and approximate their Euclidean distance by:
A proof that DR (  X  Q ,  X  C ) lower bounds the true Euclidean distance is in Keogh et al. (2000) (a different proof appears in Yi and Faloutsos (2000)).
In Sect. 3, we introduced the lowering bounding function LB_Keogh; However, cal-culating this function requires n values. Because n may be in the order of hun-dreds to thousands and multidimensional inde x structures begin to degrade rapidly somewhere above 16 dimensions (Hellerstein et al. 1997; Seidl and Kriegel 1988), we need a way to create a lower, N -dimension version of the function, where N is a number that can be reasonably handled by a multidimensional index structure (Guttman 1984). We also need this lower dimension version of the function to lower bound LB_Keogh (and therefore, by transitivity, DTW).

We begin by creating special piecewise aggregate approximations of U and L, which we will denote as  X  U and  X  L . Although they are piecewise aggregate approxi-mations, the definitions of  X  U and  X  L differ from those we have seen in Eq. (10); in particular, we have
We can visualize  X  U and  X  L as the piecewise constant functions that bound, without intersecting, U and L, respectively. Figure 11 illustrates this intuition. we denote as LB_PAA. Given a candidate sequence C, transformed to and a query sequence Q, with its companion PAA functions  X  function lower bounds LB_Keogh:
The proof that LB_PAA(Q,  X  C )  X  LB_Keogh(Q,C) is a straightforward but long ex-tension of Proposition 1; we omit it for brevity.
 that returns a lower bounding measure of the distance between a query Q and R, where R is a minimum bounding rectangle (MBR).
 associated with U ,whereL ={ l 1 , l 2 ,..., l N } and H ={ and higher endpoints of the major diagona l of R. By definition, R is the smallest rectangle that spatially contains each PAA point  X  C =  X  the above, MINDIST(Q,R) is defined as This function is visualized in Fig. 12.
 the K-nearest neighbor search (K-NN) algorithm. The basic algorithm is shown in
Table 3. It is an optimization on the GEMINI K-NN algorithm (Faloutsos et al. 1994) as suggested by Seidl and Kriegel (1988) and is a modification of the algorithm used for indexing time series under the Euclidean metric in Keogh et al. (2001).
A query KNNSearch(Q,K) with query sequence Q and desired number of neigh-bors K retrieves a set C of K time series such that, for any two Sequences, C
E /  X 
C , and DTW(Q,C)  X  DTW(Q,E). Like the classic K-NN algorithm (Roussopou-los et al. 1995), the algorithm in Table 3 uses a priority queue to visit nodes/objects in the index in the increasing order of their distances from Q in the indexed (i.e. PAA) space. The distance of an object (i.e. PAA point) C from Q is defined by
LB_PAA(Q,  X  C ) (cf. Sect. 4.2, Eq. (14)) while the distance of a node U from Q is defined by the minimum distance MINDIST(Q,R) of the minimum bounding rect-angle (MBR) R associated with U from Q.

We begin by pushing the root node of the index into the queue (line 1). The algorithm navigates the index by popping out the item from the top of the queue at each step (line 8). If the popped item is a PAA point C ,wegotodisktoretrieve the original time series C, and we compute its exact distance DWT(Q,C) from the query and then insert it into a temporary list temp (lines 9 X 11). If, on the other hand, the popped item is a node of the index structure, we compute the distance of each of its children from Q and push them into queue (Lines 12 X 17).

We only move a sequence C from temp to result when we are sure that it is one of the K-NN of Q. That is to say, there exists no object E
DTW(Q,E) &lt; DTW(Q,C) and | result | &lt; K. This second condition is guaranteed by the exit condition in line 7. The first condition can be guaranteed as follows.
Let I be the set of PAA points retrieved thus far using the index (i.e. I result). If we can guarantee that  X  C  X  I ,  X  E /  X  I , LB_PAA(Q, then the condition  X  X TW(Q,C)  X  top.dist X  in line 4 will ensure that there exists no unexplored sequence E such that DTW(Q,E) &lt; DTW(Q,C).
 creasing order of their distances DTW(Q, C) (by keeping temp sorted by DTW(Q,C)), we ensure that there exists no explored object E such that DTW(Q,E) DTW(Q,C).
 are also needed for answering range queries using a multidimensional index struc-ture. We can use a classic R-tree-styl e recursive search algorithm. Because both
MINDIST(Q,R) and LB_PAA(Q,  X  C ) lower bound DTW(Q,C), the algorithm shown in Table 4 is correct (Faloutsos and Lin 1995).

In this section, we test our proposed approach with a comprehensive set of experi-ments.
Previous experience in reimplementing and testing more than a dozen different Eu-clidean time series indexing techniques (Keogh et al. 2000, 2001), suggests that many published results do not generalize to real-world datasets and conditions. We therefore conducted the experiments in this paper with the explicit goal of conduct-ing the most comprehensive and detailed set of time series indexing experiments ever attempted. In particular, we have taken the following steps to insure the most meaningful and gener alizable results.  X  Instead of testing on just one or two datasets, as is typical (Agrawal et al. 1995;
Berndt and Clifford 1994; Chan et al. 2003; Faloutsos et al. 1994; Kim et al. 2001; Park et al. 1999, 2000, 2001), we tested all algorithms on 32 datasets.
These datasets cover the complete spectrum of stationary/nonstationary, noisy/ smooth, cyclical/noncyclical, symmetric/asymmetric, etc. The data also represents the many areas in which DTW is used, including finance, medicine, biometrics, chemistry, astronomy, robotics, networking, and industry.  X 
We designed our experiments to be completely reproducible. We saved every random number, every setting and all data, and have made them available on a free CD-ROM.  X 
To ensure true randomness where required, we used random numbers created by a quantum mechanical process (Walker 2001).  X 
Although we also present results of an implemented system, we present com-prehensive results that are completely independent of implementation details (i.e. page size, cache size, etc). This is to guard against implementation bias (Heller-stein et al. 1997; Keogh et al. 2001) and to allow and encourage independent replication of our results.

For simplicity and brevity, we only show results for nearest neighbor queries; however, we obtained very similar resu lts for range queries. Because of the large volume of experiments conducted, in this section, we will present graphics to sum-marize our findings and we will reproduc e the actual numbers in Appendix A.
Unless otherwise stated, we used the Sakoe-Chiba band with a width of 10% of n , because this appears to be the most comm only used constraint in the literature (Rabiner et al. 1978; Sakoe and Chiba 1978). We note that, had we used the Itakura parallelogram instead, our results would be even better (depending on the dataset, by an approximate factor of 4 X 12).
We begin our experiments with a compa rison of the tightness of the lower bounds for the three functions LB_Yi, LB_Kim, and LB_Keogh. We define T as the ratio of the estimated distance between two se quences over the true distance between the same two sequences.

T is in the range [ 0 , 1 ] , with the larger the better. To estimate T for each of the 32 datasets, we did the following: We randomly extracted 50 sequences of length 256.
We compared each sequence to the 49 others , using the true DTW distance, and the three lower bounding functions. For each dataset, we report T as the average ratio from the 1,225 (50*49/2) comparisons made.
 produces tighter bounds than LB_Kim, and its average value is approximately 1.38 times larger. The most obvious result from the experiment, however, is the dominance of LB_Keogh. It wins on every dataset, and its average value is approximately 3.11 times larger than its nearest rival. Because the efficiency of indexing has a (much) greater than linear dependence on the tightness of the lower bounding function, these results augur well for our approach.
 midrange of queries reported in the litera ture (Chan et al. 2003; Chu et al 2002; Park et al. 1999; Yi et al. 1998). However, we also experimented with queries in the range of 32-1,024. This range was chosen to include the longest and shortest reported in the literature (Chan et al. 2003; Park, personal c ommunication). All techniques perform better for short queries; however, while both LB_Kim and LB_Yi degrade rapidly for longer queries, LB_Keogh stays almost constant for longer queries. This effect was observed on all datasets. For brevity, we just present results for the random walk dataset in Fig. 14.
To compare the pruning power of the three techniques under consideration, we meas-ure P , the fraction of the database that does not require full computation of DTW while still allowing us to guarantee that we have found the nearest match to a 1-NN query.
 extract 50 sequences of length 256. For each of the 50 sequences, we separate out the sequence from the other 49 sequences. We then find the nearest match to our withheld sequence among the remaining 49 sequences using the sequential scan al-gorithm of Table 2. We measure the number of times we can use the linear-time lower bounding functions to prune away the quadratic-time computation of the full
DTW algorithm. For fairness, we visit t he 49 sequences in the same order for each approach. The value P reported is averaged over all 50 runs.

Note the value of P depends only on the data and is completely independent of any implementation choices, includi ng spatial access method, buffer size, com-puter language, or hardware platform. A similar idea for evaluating indexing schemes appears in Hellerstein et al. (1997).

The results are summarized in Fig. 15. On 25 out of 32 datasets, LB_Yi is more efficient at pruning than LB_Kim. On average, it was able to prune 1.53 times as many items. Once again, however, the most obvious result is the dominance of
LB_Keogh. It wins on every dataset and was able to prune 3.95 times as many items as LB_Yi and 6.06 times as many items as LB_Kim.

Note that, while these results are powerful implementation-independent predic-tors of indexing performance, they may actually be pessimistic. There are two related its the items and calculates the DTW meas ures (where necessary) in a predefined order. A more efficient implementation would sort and then visit the sequences, in ascending order of the lower bounding distance. This, of course, is essentially what spatial indexing does.
 performance is the relatively small size of the datasets. We should expect the frac-tion of pruned sequences to increase on lar ger datasets. The reason is because the larger the dataset, the greater the chance there is of a good match being found, and a good match allows us to extract the maximum benefit from the pruning conditional
LB_dist &lt; best_so_far in line 4 of the algorithm in Table 2. To demonstrate this effect, we ran the same experiment ab ove on increasingly larger subsets of the random walk dataset. The results are shown in Fig. 16.

The 32 datasets used in the previous experiments illustrate the dominance of the pro-posed approach on a wide variety of datasets. However, most are not large enough by themselves to warrant the title of large datab ase. We therefore pooled all 32 datasets into a single dataset that we call mixed bag (MB). In addition to this ultraheteroge-neous data, we created a very large database of random walk data (RW II) because this is the most studied dataset for indexing comparisons (Chan et al. 2003; Chu et by contrast with the above, a very homogeneous dataset. Details of these datasets appear in Appendix A.
 physical memory and 57.2 GB of secondary storage. The spatial access method used was the R-tree (Gollmer and Posten 1995).
 CPU cost.

Definition. The Normalized CPU cost : The ratio of average CPU time to execute a query using the index to the average CPU time required performing a linear (se-quential) scan. The normalized cost of a linear scan is 1.0.
 disk access, whereas any indexing techni que must make random disk accesses. It is generally understood that random access is about ten times slower than sequential ac-cess (Hellerstein et al. 1997; Roussopoulos et al. 1995; Seidl and Kriegel 1988). For fairness, we allowed linear scan to utilize the lower bounding function LB_Keogh. clude it in this experiment. We originally included LB_Kim in the experiments, but found that it never beat the linear scan, and we therefore decided to exclude it from graphic presentation.

We tested over a range of query lengths and dimensionalities, but show just one typical result for brevity. Figure 17 shows the normalized CPU cost of linear scan and
LB_Keogh, for queries of length 256, with a 16-dimensional index, for increasingly large databases.

The experiments above convincingly demonstrate the superiority of the proposed lower bounding technique for different data sets, different query lengths, different database sizes, etc. However, all the experiments use a Sakoe-Chiba band with a width of 10% of the query length because, as noted above, this seems to be the most com-mon constraint used in practice (Rabiner et al. 1978; Sakoe and Chiba 1978). How-ever, it is natural to ask how sensitive the r esults are to this parameter. To find out, we repeated the experiment in Sect. 5.2, this time also testing a warping window twice as wide and half as wide as the original e xperiments. Because showing the results and last datasets from the list in Appendix A. The results are shown in Fig. 18.
The results are excellent. Even with an extremely wide warping window, we still convincingly beat the two competing approaches. Furthermore, tightening the warping window to a still realistic value of 5% of the query length produces an extraordinary tight lower bound.
In Sect. 3.3.1, we justified using warping windows by noting that researchers who use DTW to solve real-world problems have documented their utility (Aach and Church 2001; Caiani et al. 1998; Gavrila and Davis 1995; Gollmer and Posten 1995;
Itakura 1975; Kovacs-Vajna 2000; Munich and Perona 1999; Rath and Manmatha 2002). However, because warping windows are the cornerstone of our lower bound-ing technique, we will conduct experiments to explicitly justify their use. As we are interested in indexing data, an appropriate way to do this might be to measure pre-cision/recall under varying warping windo ws. However, to our knowledge, there are no large time series datasets that have been annotated as relevant/irrelevant for given queries. Instead, we will consider the effect of warping windows on classification of time series because accuracy in classificatio n is a close analogue of precision/recall in information retrieval.
 peared in the literature several times and are publicly available (Kadous 1999; Diez and Gonzalez 2000). In particular, we teste d the following datasets, which are illus-trated in Fig. 19A.  X  Transient Classification Benchmark (TCB): A synthetic dataset designed to sim- X  Australian Sign Language (ASL): This dataset consists of the X-axis motion of width of the Sakoe-Chiba band from 1 to n . Each classifier was evaluated using the leaving-one-out cross validation. The results are shown in Fig. 19B.
 confirmation of the superiority of DTW ove r Euclidean distance (recall that warping window width = 1, is equivalent to Euclidean distance). A more interesting obser-vation is that, while some DTW helps, there are diminishing returns. In the case of ASL, too much freedom in warping act ually hurts the accuracy. These results strongly support the idea that constraining DTW with warping windows is a good idea, independent of our ability to exploit i t for speedup. The results also suggest an interesting research direction; can one automatically learn the best warping window for a particular dataset and query? We are actively exploring this question.
In one of the most referenced papers on time series similarity ever published (Agrawal et al. 1995), the authors explicitly state,  X  X ynamic time warping speeded up by indexing. X  This sentiment has since been echoed in several dozen other papers (Chan et al. 2003; Yi et al. 1998). How then have we achieved the seemingly impossible? First, we have only considered the case where the two se-quences are of the same length. This is not really a limitation b ecause the user can always reinterpolate the query to any desired length in O( n only index sequences if we assume the warping path is constrained. Once again, are aware of reiterates the absolute necessity of using constraints (Aach and Church 2001; Berndt and Clifford 1994; Caiani et al. 1998; Chu et al. 2002; Gollmer and
Posten 1995; Itakura 1975; Rabiner et al. 1978; Sakoe and Chiba 1978; Schmill et al. 1999; Strik and Boves 1988; Tappert and Das 1978).

Our approach is particularly a ttractive because, as a special case ( r is set to zero), it degenerates to Euclidean indexing using PAA, an approach that has been shown by two independent groups of researchers to be state of the art in terms of efficiency and flexibility (Chu et al. 2002; Keogh et al. 2000; Yi et al. 1998).
There are several directions in which this work may be extended. For example, we note that some algorithms for matching two and three-dimensional shapes are very close analogues of the DTW algorithm and thus may benefit from a similar lower bounding function. In addition, Rath and Manmatha (Rath and Manmatha 2002) have informed us that they have generalized LB_Keogh to multidimensional time series and intend to use the resulting algorithm for indexing massive repositories of handwritten historical documents (personal communication).

The raw numbers obtained from the experiments discussed in Sects. 5.2 and 5.3 are shown in Table 5. These numbers may be visualized in Figs. 13 and 15, respectively.
