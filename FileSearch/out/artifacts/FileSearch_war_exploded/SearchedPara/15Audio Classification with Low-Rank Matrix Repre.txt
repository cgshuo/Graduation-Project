 ZIQIANG SHI, JIQING HAN, and TIERAN ZHENG , Harbin Institute of Technology Audio feature extraction and classification methods have been studied by many re-searchers over the years [Lu et al. 2002; Cai et al. 2003; Shen et al. 2006; Song and Zhang 2008; Atrey et al. 2006; Umapathy et al. 2007; Fu et al. 2011]. In general, au-dio classification can be performed in two steps, which involves reducing the audio sound to a small set of parameters using various feature extraction techniques and classifying or categorizing over these parameters. Audio signals have been tradition-ally characterized by Mel-frequency cepstral coefficients (MFCCs) or some other time-frequency representations such as the short-time Fourier transform and the wavelet transform [Shen et al. 2006; Umapathy et al. 2007; Zhuang et al. 2008; Fu et al. 2011]. Many of those features are common to audio signal processing and speech recognition, and have many successful performances in various applications. However almost all these features are based on short time duration and in vector form (it is easy to handle but sometimes not proper), although it is believed that long time duration (seconds) help a lot in decision making [Ellis and Lee 2004; Atrey et al. 2006; Zhuang et al. 2008]. In this work we will build robust features on a long time duration in matrix form which is a more natural way using long time audio information.

In order to map or smooth the audio signal into a robust matrix space, we introduce the trace norm regularization technique to audio signal processing. The trace norm regularization is a principled approach to learn low-rank matrices through convex op-timization problems [Fazel et al. 2001]. These similar problems arise in many machine learning tasks such as matrix completion [Srebro et al. 2005], multi-task learning [Argyriou et al. 2008], robust principle component antilysis (robust PCA) [Wright et al. 2009; Lin et al. 2010], and matrix classification [Tomioka and Aihara 2007]. In this article, robust PCA is used to extract matrix representation features for audio signals. Unlike traditional frame based vector features, these matrix features are extracted based on sequences of audio frames. It is believed that in a short duration the signals are contributed by a few factors. Thus it is natural to approximate the frame sequence by low-rank features using robust PCA which assumes that the observed matrices are combinations of some low-rank matrices and some corruption noise matrices.

Having extracted descriptive features, various machine learning methods are used to provide a final classification of the audio signal such as rule-based approaches, Gaus-sian mixture models, support vector machines, Bayesian networks, and etc. [Umapathy et al. 2007; Zhuang et al. 2008; Cai et al. 2003; Song and Zhang 2008; Fu et al. 2011]. In most previous work, these two steps for audio classification are always separate and independent. In this work, we can learn the classifiers in solving similar optimiza-tion problems using trace norm regularization. After extraction of the robust low-rank matrix feature, the regularization framework based matrix classification approach pro-posed by Tomioka and Aihara [2007] is used to predict the label.

The problem of matrix classification (MC) with spectral regularization was first proposed by Tomioka and Aihara [2007]. The goal of the problem is to infer the weight matrix and bias under low trace norm constraints and low deviation of the empirical statistics from their predictions. The trace norm was use to measure the complexity of the weight matrix of the linear classifier for matrix classifications. This kind of inference task belongs to the more general problem of learning low-rank matrix through convex optimization. For the matrix rank minimization is NP-hard in general due to the combinatorial nature of the rank function, a commonly-used convex relaxation of the rank function is the trace norm (nuclear norm) [Fazel et al. 2001], defined as the sum of the singular values of the matrix.

Recent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [Toh and Yun 2010; Ji and Ye 2009; Liu et al. 2009]. These general algorithm can be adapted to matrix classification suitably. In these methods, most are iterative batch procedures [Toh and Yun 2010; Ji and Ye 2009; Liu et al. 2009], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm. This kind of learning procedure cannot deal with huge size training set for the data probably cannot be loaded into memory simultaneously. Furthermore these batch methods cannot be started until the training data are prepared, hence cannot effectively deal with training data appear in sequence, such as audio and video processing [Shi et al. 2011a].

To address these problems in matrix classification learning, we transform the general batch-mode accelerated proximal gradient (APG) [Toh and Yun 2010; Ji and Ye 2009] method for trace norm minimization is transformed into an online learning framework. In this framework, the approaches work by processing the matrix training samples, one at a time, or in mini-batches to learn the weight matrix and the bias for matrix classification. In addition, we derived the closed-form of the Lipschitz constant. Thus the step size estimation [Toh and Yun 2010; Ji and Ye 2009] of the general APG method was omitted in our approach. Even more, for the mini-bath case, in order to further speed up the learning process with large scale training data, we implement several modules in the online algorithm in paralle, including past information update parallel, calculation of the GridTr(  X  ,  X  ) in our algorithms, and so on. Experiments show that all the efforts get nearly 10-fold speed-ups with inexact algorithms achieving same performance.

The remainder of the article is structured as follows: Section 2 presents the low-rank matrix representation features with robust PCA for audio classification. The matrix classification problem solving via the APG method and the proposed audio classification method are presented in Section 3. The proposed online methods for weight and bias learning are introduced in Section 5.1. The experimental results on laugh/non-laugh and applause/non-applause audio signal classification are presented in Section 5. Finally we give some conclusions in Section 6. In the last few years, a variety of features have been proposed for audio and speech processing [Cai et al. 2003; Atrey et al. 2006; Zhuang et al. 2008]. However, presence of noise and distortion, especially large corruptions always affects these traditional features and hence the performance of the systems based on these features. The degra-dation of the performance based on these traditional features depends on the property of existing distortions, which vary from added small noise to large corruption of the input data. For example, the audio signal may be corrupted by large noise such as sud-den movement of chair, door close, and so on. Some approaches have been developed to cope with such sparse large corruption. One successful method is the robust Principal Component Analysis (robust PCA) [Wright et al. 2009], which. In this article, robust PCA is adapted to extract robust audio feature.

Frame based features are first stacked into matrixes, and then robust PCA is used to transform these matrixes into low-rank approximations. Assume that we have a set of input observations, D  X  R m  X  n , where m is the number of frames in each stacked matrix signal and n is the dimensions of the input data. Given an observed matrix data D  X  R m  X  n , where m is the number of frames and n represents the number of samples in a frame. It is assumed that it can be decomposed as D = A + E , where A is low-rank and E is sparse. Recently, Wright et al. [2009] showed that the A can be exactly recovered by solving where  X   X  denotes the trace norm of a matrix which is defined as the sum of the singular values,  X  1 denotes the sum of the absolute values of matrix elements, and  X  is a positive regularization parameter.

In order to solve Equation (1), several algorithms have been proposed, among which the augmented Lagrange multiplier method is the most efficient and accurate at present [Lin et al. 2010]. In order to apply the augmented Lagrange multiplier (ALM) to the robust PCA problem, Lin et. al. [2010] identify the problem as and the Lagrangian function becomes as the sum of the products between corresponding items in the two matrices. Two ALM algorithms to solve the above formulation are proposed in Lin et al. [2010]. Considering a balance between processing speed and accuracy, the robust PCA via the inexact ALM method is chosen in our work. Thus the matrix representation feature extraction process based on this approach is summarized in Algorithm 1. In Algorithm 1, J ( D )is defined as the larger one of D 2 and  X   X  1 D  X  , where  X   X  is the maximum absolute value of the matrix elements. The S  X  [  X  ] is the soft-thresholding operator introduced in [Lin et al. 2010].

Figure 1 shows exemplar audio signals under two kinds of distortions, white noises and large errors (LE), with sample distortion levels and the corresponding recovered low-rank matrices. It can be seen that robust PCA can successfully recover the low-rank component from the original or corrupted signals. Figure 2 shows the spectrograms of the signals in Figure 1, respectively. We can see that the spectrograms of the low-rank components vary not much compare to the spectrograms of the corrupted signals. Having extracted robust matrix representation features, the linear matrix classification approach based on trace norm regularization framework proposed in Tomioka and Aihara [2007] is used to classify them. The motivation for trace norm regularization framework is two fold: a) trace norm considers the interactive information among the frames in the matrix while the simple approach that treat the matrix as a long vector would lose the information; b) trace norm is a suitable quantity that measures the complexity of the linear classifier. Generally, the problem for trace norm regularization based matrix classification is formulated as where W  X  R m  X  n is the unknown weight matrix , b  X  R is the bias ,  X   X  denotes the trace norm defined as the sum of the singular values, and  X  is the regularization parameter. f ( W , b ) = s i = 1 ( y i , Tr( W T X i ) + b ) is the empirical loss function induced by some In many audio event detection or audio scene recognition tasks, many problems are always abstracted into binary problems, and then multiple event or scene classifications are built based on these binary detectors or classifiers. Thus in this work, the labels are assumed belong to { X  1 , 1 } . In this work, the standard squared loss function is used. Recently Toh and Yun [2010], Ji and Ye [2009], and Liu et al. [2009] independently proposed similar algorithms that converge as O ( 1 where k is the iteration counter. The precondition of using APG algorithm is that the loss function should be smooth, convex, and the gradient should satisfy Lipschitz condition. Since f s ( W , b ) in this work is a composition of smooth convex function with an affine mapping, hence it is convex and smooth [Boyd and Vandenberghe 2004]. For Lipschitz continuous, it is shown in the appendix that the gradient of f s ( W , b ), denoted as is Lipschitz continuous with constant L = 2 mn s i = 1 X i 2 F . The derivation of the con-stant (see the Appendix) is very similar to the tensor case that introduced in Shi et al. [2011b]. Thus the APG method can be used to solve matrix classification problem. In order to solve the unconstrained convex optimization problem (4), APG approximate f ( W , b ) locally as a quadratic function with bias fixed and solve which is assumed to be easy, to update the solution W . Based on the the work of Nesterov [1983, 2005], Toh and Yun [2010], Ji and Ye [2009], and Liu et al. [2009] showed that setting Z k = W k + t k  X  1  X  1 t results in a convergence rate of O ( 1 k general APG [Toh and Yun 2010; Ji and Ye 2009; Liu et al. 2009] is omitted, for we have explicit Lipschitz constant. The APG approach for batch-mode weight matrix learning is described Algorithm 2. The S  X  [  X  ] in Algorithm 2 is the soft-thresholding operator introduced in [Lin et al. 2010]: where x  X  R and  X &gt; 0. For vectors and matrices, this operator is extended by applying element-wise.

The general APG [Toh and Yun 2010; Ji and Ye 2009; Liu et al. 2009] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules. In order to update the bias b , fixes the weight matrix W k and solve the following problem which results in the bias updating rule This results in the line 6 of Algorithm 2. For the stopping criteria of the iterations, we take the following relative error conditions:
After the weight matrix W and bias b are found, the observed MFCCs matrix X i can be classified via
The APG based batch-mode weight learning method is effective for small training set, but with large training sets, this classical optimization technique may become impractical in terms of memory requirements. Furthermore, this method cannot effi-ciently deal with dynamic training data of time sequences, such as audio and video processing. To tackle the insufficiency, we propose an online learning framework in the following section. We present in this section the basic components of our online learning algorithm for ma-trix classification, as well as a few minor variants which speed up our implementation in practice.

Our procedure is summarized in Algorithm 3, where GridTr( Z k , t , B t ) denotes an block of B t . The  X  operator in the algorithm denotes the Kronecker product. For two blocks of sizes m 2  X  n 2 equal to A [ i , j ] B .

The inner loop of the algorithm draws one training sample ( X t , y t )atatimefrom a training set which is composed of i.i.d. samples of a distribution p ( X , y ). Then we sample. Then the Algorithm 2 is applied to update the parameters of the classifier with W  X  1 obtained at the previous iteration as the warm start. The APG is used in Algorithm 3 to update the parameters for every observed sample by solving the subproblem with fixed bias b by solving exactly which cause computational load for large scale training set. Here due to the closeness of consecutive parameters, we do not need precise solution. Rather, updating W  X  1 once when solving this sub-problem is sufficient in practice. This resulting in an online MC learning method based on inexact APG, described in Algorithm 4. Further more, we may update the past information in mini-batch manner. Let us denote lines 5 and 9 of Algorithms 3 and 4 by But in real applications, this batch method may not improve the convergence speed on the whole since the batch past information computation (Equation (13)) would occupy much of the time. The updating of B t needs to do Kronecher product which spend much of the computing resource. If the computation cost of Equation (13) can be ignored or largely decreased, for example by parallel computing, the batch method would increase the convergence speed by a factor of  X  . In real applications, using this batch method may not improve the convergence speed on the whole since the batch past information computation (Equation (13)) would occupy much of the time. The updating of B t needs to do Kronecker product which spends much of the computing resource. If the computation cost of Equation (13) can be ignored or largely decreased, for example, by parallel computing, the batch method would increase the convergence speed by the numbers of computing cores. In this article we implement two parts of all the algorithms in parallel: 1) The past information update; 2) The computation of GridTr(  X  ,  X  ). The training data sampled from the distribution p ( X , y ) are sent to the slave nodes. For each slave node, local past information is computed and sent to the master node, which works out the global past information for parameter updating. For calculation of GridTr(  X  ,  X  ), it is almost the same procedure as past information update, which is shown in Figure 3. The parallel GridTr(  X  ,  X  ) is denoted as ParaGridTr(  X  ,  X  ). The final parallel online MC learning based on APG with mini-batch past information update is shown in Algorithm 5. For the evaluation of the algorithms, a collected database, which is the one used in our previous work [Shi et al. 2011a], is used. we have implemented three main sets of experiments to test the methods proposed in this work, include online learning of matrix feature classification, robustness of low-rank matrix representation feature, and parallel speedup of learning algorithms.
 In order to evaluate the performance of the online learning for matrix classification problem, we compared the following five algorithms: 1) the traditional batch algorithm with exact APG algorithm (APG); 2) the online learning algorithm with exact APG (OL APG); 3) the online learning algorithm with inexact APG (OL IAPG); 4) the on-line learning algorithm with exact APG and update Equation (13) (OL APG Batch); 5) the online learning algorithm with inexact APG and update Equation (13) (OL IAPG Batch). All algorithms are run in Matlab on a personal computer with an Intel 3.40GHz dual-core central processing unit (CPU) and 2GB memory.

For this experiment, the single channel (mono) audio streams were first resampled to 8kHz, and then a short-time Fourier magnitude spectrum is calculated over short-term frames (20ms long) with nonoverlap. Our fundamental frame-level feature is 13 dimen-sional MFCCs including energy, and adjacent 50 frames (one second) of MFCCs form the MFCCs matrix feature. Our goal is to classify the matrices according to their labels. Two learning tasks are used to evaluate the performance of the online learning method, which are laugh/non-laugh signal classifier learning and applause/non-applause signal classifier learning. For OL APG and OL APG Batch algorithms, the parameters in the stopping criteria (10) are set  X  1 = 10  X  8 and  X  2 = 10  X  8 or smaller. These paremeters are determined by empirical evidence that larger values would make the algorithms diverge. For the regularization constant  X  , in this work we use  X  = 1 throughout.
Figure 4 compares the five online algorithms. The proposed online algorithm draws samples from the entire training set. We use a logarithmic scale for the computation time. Figure 4(a) shows the values of the target functions as functions of time. It can be seen that the online learning methods without batch or with small batch past infor-mation updating converge faster than the methods with large batch past information updating and reason for this has been explained in the last paragraph of Section 5.1. After online methods and batch methods converge, the two methods result in almost equal performance. Figure 4(b)(d) shows the classification rates for different algorithms respectively. In accordance with the values of the target functions, the classification ac-curacies of online methods without or with small batch updating become stable quickly than that of methods with batch updating. Although the inexact algorithms process samples much fast with less resources than that with exact APGs, they converge slowly. In order to evaluate the effectiveness of the robust PCA extracted low-rank matrix features, three kinds of features are compared: 1) original MFCCs Matrix features; 2) MFCCs Matrix features corrupted with 5dB, 0dB and  X  5dB white Gaussian noise; 3) MFCCs Matrix features corrupted with 10%, 30%, 50% random large errors (LE); 4) robust PCA extracted MFCCs Matrix features; 5) robust PCA extracted features of 2); 6) robust PCA extracted features of 3). In the comparisons, the parameters in the stopping criteria (10) are set  X  1 = 10  X  6 and  X  2 = 10  X  6 , which are determined by the same method as in Section 5.1. The regularization constant  X  (different from the regularization constant  X  in Equation (12)) is set 1 / which is a classical normalization factor.

We then evaluated the performance of all these features in terms of the classification precision of the one second audio clips. Figure 5 shows the results of all the robustness with different matrix features under different noise conditions as the functions of the training time used in Algorithm 2. It can seen that robust PCA extracted low-rank features perform better under all the noise conditions, especially with random large errors. Unlike original MFCCs matrix features that have a decrease of approximately 20% in audio signals classification accuracy of this category, when 10% of the elements of feature elements are corrupted with random large errors, robust PCA extracted low-rank features were almost able to keep an equivalence performance with noiseless situation. By using robust PCA extracted low-rank features, we were able to achieve an averaged decrease of 10%, compared to the 20% decrease of original features under WGN noise. These confirm our believe that low-rank component is insensitive to noise, and robust PCA extracted low-rank features are more robust than traditional MFCC features. Using similar experiment settings as in online experiment, we perform the parallel speed-up using the schemes proposed in Section 5.3. Four algorithms are compared to show the efficiency of the speed-up: 1) the parallel online learning algorithm with exact APG (Para OL APG); 2) the online learning algorithm with exact APG (OL APG); 3) the parallel online learning algorithm with inexact APG (Para OL IAPG); 4) the online learning algorithm with inexact APG (OL IAPG). We obtain about 10-fold speed-ups in achieving the same performance for our algorithms with inexact APGs using parallel schemes. Figure 6 shows that the algorithms using parallel schemes converge quickly than using only single-thread algorithms. It is showed Also, for algorithms with inexact APG process much more samples than those with exact APG in the same unit of time, thus that generally Para OL IAPG and OL IAPG converges fast than Para OL APG and OL APG for here the data may be more useful than methods. Speed-up schemes with inexact APG provide a more flexible and effective way to do matrix classification learning. For completeness, the results of the whole proposed features and classifiers are also compared with the state-of-the-art SVM classifier using 650 dimension long vector feature obtained by vectorizing the matrix. The results are summarized in Table I and Table II for applause/non-applause and laugh/non-laugh classification respectively. Here the standard classifier with exact APG using low-rank features are used. The results show that the SVM become useless under noise condition, while our features and methods still works. In this work, we present a novel framework based on trace norm minimization for audio signal classification. The novel method unified feature extraction and pattern classi-fication into the same framework. In this framework, robust PCA extracted low-rank component of original signal is more robust to corrupted noise and errors, especially to random large errors. We also introduced online learning algorithms for matrices classification tasks. We obtain the closed-form updating rules of the weight matrix and the bias. We derive the explicit form of the Lipschitz constant, which saves the compu-tation burden in searching step size. Experiments show that even the percent of the original feature elements corrupted with random large errors is up to 50%, the perfor-mance of the robust PCA extracted features almost have no decrease. In the domain of matrix data classification, we almost solve the three problems proposed in [Boyd et al. 2011] for  X  X ig Data. X  Our parallel schemes enable about 10-fold speed-ups when achieving the same performance. In future work, we plan to test this robust feature in other audio or speech processing related applications and extend robust PCA, even trace norm minimization related methods from matrices to the more general multi-way arrays (tensors). Some work related to learning methods are also worth considering, such that the alternating between minimization with respect to weight matrix and bias may results in fluctuation of target value (even in batch mode), thus optimization algorithm that minimization jointly on weight matrix and bias are required; for multi-classification problems with more classes, some hierarchy methods may be introduced to improve the classification accuracy.
 Applying Equation (5) with U , V , we obtain where in the last inequality, the easily verified fact that Tr( A T B )  X  A 1 B 1  X  mn A F B F for  X  A , B  X  R m  X  n is used. Here  X  1 denotes the 1 norm which is the sum of the absolute values of the matrix elements.
 Thus  X  W f s (  X  , b ) is Lipschitz continuous with constant L = 2 mn s i = 1 X i 2 F .
