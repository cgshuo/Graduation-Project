 Web documents are typically associated with many text streams, including the body, the title and the URL that are determined by the authors, and the anchor text or search queries used by others to refer to the documents. Through a sy stematic large sc ale analysis on their cross entropy, we show that these text streams appear to be composed in different languag e styles, and hence warrant re-spective language models to prope rly describe their properties. We propose a language modeling approach to Web document retrieval in which each document is characterized by a mixture model with components corresponding to the various text streams associated with the document. Immediate issues for such a mix-ture model arise as all the text streams are not always present for the documents, and they do not share the same lexicon, making it challenging to properly combine the statistics from the mixture components. To address these issues, we introduce an  X  X pen-vocabulary X  smoothing technique so that all the component lan-guage models have the same cardina lity and their scores can simp-ly be linearly combined. To ensure that the approach can cope with Web scale applications, th e model training algorithm is de-signed to require no labeled data and can be fully automated with few heuristics and no em pirical parameter tunings. The evaluation on Web document ranking tasks shows that the component lan-guage models indeed have varyi ng degrees of capabilities as pre-dicted by the cross-entropy analysis, and the combined mixture model outperforms the state-of-t he-art BM25F based system.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Theory, Experimentation. Information Retrieval, Mixtur e Language Models, Smoothing, Parameter Estimation, Probabilistic Relevance Model. Inspired by the success in speech recognition, Ponte and Croft [23] introduced the language modeling (LM) techniques to infor-mation retrieval (IR) that have since become an important re-search area. The motivation is very simple: just as we would like a speech recognition system to transcribe speech into the most like-ly uttered texts, we w ould like an IR system to retrieve documents that have high proba bilities of meeting the information needs encoded in the query. Over a decade of studies on this topic, it has been now widely understood that LM is a principled realization of the statistical approach envision ed by Maron and Kuhns at the dawn of IR [19], and that it s underlying statistical framework provides mathematically sound explanations to why many proven heuristics, such as TF/IDF we ightings and document length nor-malization, have been working so well [1][6][9][18][31]. As is in the case of speech recognition, LM for IR can be formulated as a Bayesian risk minimization prob lem [16], for which the optimal performance can be achieved by following the maximum a post-eriori decision rule that was first shown in [7] and reiterated for IR by Zhai and Lafferty [33]. Specifically, given a query Q , a minimum risk retrieval system should rank the document D based on product of the likelihood of the query under the document language model, P D ( Q ), and the prior of the document P ( D ): An enthusing question still in the center of the LM for IR research is what the document language model is and how it could be ob-tained [6][18][31]. While it is intuitive to use the text body to train the document language model as in the majority of the work [18][31], it has been widely r ecognized that queries are often composed in a different language style than the document body, and a poor query likelihood can thus occur for relevant documents proposed a hidden Markov model in which an additional latent stage is included to model the que ry generation process. Lafferty et al. have argued for an explicit model of the query language itself [16], and proposed the mach ine translation techniques to bridge the gap between the body and the query [1]. Jin et al. [14], for example, used the title and th e body of a document as the tar-get and the source languages, respectively, and demonstrated that the  X  X ranslated X  title LM can be a viable choice as the P The two-stage LM by Zhai and Lafferty [33] proposed yet another idea of using smoothing techniques. There, the document LM is first created by smoothing the document body with a body back-ground model, which is then followed by a second stage of smoothing ideally with a query background model. In practice, documents often have more fields than just the title and the body. This is particularly true in the Web environment where, in addition to the textual contents created by the document authors, Web documents are also annotated with inbound anchor text by other document authors, as well as the user queries leading to clicks on the documents. Traditional IR has viewed the mul-tiple-field document retrieval as a structured document retrieval problem, and some established re trieval models, such as BM25, have been generalized to multi-field document retrieval [26]. A straightforward generalization for LM is to view the document as being described by multiple text streams. As shown in Sec. 2, a quantitative analysis on the documents indexed by a commercial Web search engine does confirm that all these text streams seem to be written in their own langua ge styles and have varying de-grees of mismatch with the query, justifying the idea to model their linguistic characteristics separately. Empirical studies on applying the LMs for different ta sk domains have also confirmed that mixing textual sources with different language styles can significantly degrade the qua lity of LMs (e.g., [2]). When a probability space is divide d into disjoint partitions, the probability of an event can be evaluated as the sum of the condi-tional probability of the event occurring in each partition, weighted by the prior of that partition. Apply this principle to the document modeling and let D i denote the i th stream of D and P the corresponding component LM for the stream, we have Such a mixture distribution has been widely used for LM in speech and language processing [11] as well as in IR (e.g., [22]). However, beneath the simple linear interpolation form of (2) lies the serious question of the cond itions under which the component LMs can be combined properly. Since the foundation of mixture modeling is derived from the probability space partitioning, all the mixture components should ther efore be modeling the same un-derlying probability space. It is widely known [11] that LMs hav-ing different sets of vocabular y should be viewed as modeling different domains and therefore th eir scores cannot be directly compared, let alone combined into a mixture distribution. When applying LM for IR, for example, it is critical to ensure that all document LMs have the same vo cabulary so that the document LMs do not selectively treat differ ent portion of the query as out of vocabulary (OOV) te rms. The common appr oach to smooth the document LMs with a shared background model effectively makes all documents use the same vocabulary of the background model. Still, running into OOVs is quite common. For IR using non-mixture LM, encountering OOVs in a query is not a severe problem because the impact in com puting the ranking scores (1) is the same for all the documents. This is not true for mixture LM described by (2) since OOVs of one mixture component are not necessarily OOVs for others, maki ng how to properly compute the combined probability of the query a critical question. To address this problem, we in this work undertake a so-called  X  X pen-vocabulary X  LM approach that is prevalent in the language processing community (e.g., [11] ) but has not been extensively studied for IR. At the core of the open-vocabulary LM is a for-midable challenge to assess th e probability mass for OOVs. LMs that can yield non-trivial probabilities for OOVs can be viewed as modeling a language with infi nite vocabulary. All open-vocabulary LMs thus at least have the same cardinality for their vocabulary, and their probabilis tic scores are on a more solid ground to be comparable and combined. As surveyed in Sec. 3, the open vocabulary LM is far from a solved research problem because it inevitably requires one to  X  X uess X  the unseen. All the techniques proposed in the past five decades have all involved some kinds of heuristics or para meter tunings that make it chal-lenging to deploy the model outside of research labs. This is be-cause the application domains usua lly have different environmen-incongruent to the properties of the tuning data. The scale of the Web typically amplifies the difficu lty of these issues, as demon-strated in the machine learning results reported in [28] that show the retrieval performance can be highly volatile depending on how the parameters in BM25F are acquired. In this paper, we propose an in formation theoretically motivated method towards open vocabulary L Ms. The emphasis here is to obtain an analytically tractable and fully automated system that alleviates the problems arising fr om heuristic parameter tunings. Typically, such an approach can only yield  X  X tatistically optimal X  outcome and cannot guarantee th e performance be better than fine-tuned systems in all cases. We apply this spirit to both the smoothing of the mixture component LM P Di and the estimation of the mixture weights in (2). In Sec. 4, we present the detailed mathematical derivation that shows how the smoothing parame-ters can be obtained by comput ing how N-gram is predicted by (N-1)-gram. In particular, th e OOV probability mass, which is equivalent to unigram discount, can therefore be estimated by inspecting how the unigram is predicted by the zero-gram. In Sec. 5 we describe the methods to com pute mixture coefficients, and in Sec. 6 we describe th e experimental results. The contributions of the paper are as follows: First, we provide a large scale quantitative analysis to verify how the query language is different in style from document body. We confirm and gene-ralize the prevalent informal observations that, on the Web scale, various fields associated with the documents do have significantly different properties. From a mode ling perspective, the analytical outcomes suggest these text source s are better modeled separately. Based on the analysis, we propose a mixture LM approach to IR. The mixture model has to address two immediate and formidable challenges. First, it requires an open-vocabulary LM that has no known solution without heuristics until this work. We propose a mathematically tractable close form solution to realize open-vocabulary LMs. Secondly, the mi xture model increases the num-ber of parameters, and we show IR results are very sensitive to tuning. We show that our propose d analytical method can achieve high quality performance w ithout empirical tuning. The observations that the query language is different in styles from document body and may be closer to titles are intuitive. To formalize the analysis, we conduct a large scale analysis on a June 2009 snapshot of the Web documents in the EN-US market. We examine the language usages in the document text body, the title, the anchor text, as well as the queries against a commercial search engine at the same time. To quan tify the language usages in these streams, we first build a statistical N-gram LM for each of them and study the complexity of the language using information theo-retic measurements such as entropy or cross-entropy. The LMs used in this section, with the ex ception of query LMs, are all pub-licly accessible through [20]. Form ally, the cross-entropy between model P A and P B is Since the logarithmic function is convex, it can be easily shown that the cross entropy is smallest when the two models are iden-tical. The cross entropy function can therefore be viewed as a measurement that quantifies how different the two models are. The entropy of a model P A , H ( P A ) = H ( P confusion on the base of the logarithm, we further convert the entropy into the corresponding linear scale perplexity measure-ment, namely, Previously, it has been estimated that the trigram perplexity of general English has an upper bound of 247 words per position based on a 1 million word corpus of American English of varying topics and genres [2]. In contrast, in the Web snapshot the vocabulary size is at least 1.2 billion for the document body, and 60 million, 150 million, and 252 million for the title, anchor te xt and the user query streams, respectively. As our main objective is to investigate how these language sources can be used to model user queries, we study the cross-entropy between the query LM to others, the results of which are shown in Figure 1. As can be seen, when the LM grows more powerful with increas-ing order, the query language perplexity keeps dropping, from 1754 for unigram down to 180 for trigram and 168 for 4-gram. It thus appears that the query langua ge falls within the previously estimated upper bound for perplexity , 247, for general English. The cross-stream perplexities give hints on the efficacy of using various streams to model the que ry language. The document body has shown consistently the largest mismatch with the queries, while anchor text seems the best choice among the three to model the queries with powerful enough LM (i.e., N &gt; 1). We note that, starting at bigram, both title and an chor text models have a small-er perplexity than the unigram model of query itself. The study lends some support to the hypothesis that document title is a better source than the body to build a LM for IR. Up to trigram, the heightened modeling power with an increasing order uniformly improves the perplexities of all streams for que-ries, although this increased capab ility can also enhance the style mismatch that eventually leads to the perplexity increase at higher order. For the document body and title, the payoff of using more powerful LMs seems to taper off at bigram, whereas trigram may still be worthwhile for the anchor text. As are in many applications of LMs, the perplexity measure is not the ultimate metric for applications, in other words, models with lower perplexities do not necessarily lead to a better performance. However, the perplexity analysis is still informative in that higher perplexity models can seldom outperform the lower perplexity ones. For any unigram LM P C with vocabulary V , the probabilities of all the in-vocabulary and OOV tokens sum up to 1. An open-vocabulary LM is a model that reserves non-zero probability mass for OOVs: When an open vocabulary model is us ed to evaluate a text corpus and encounter additional k distinct OOV tokens, the maximum entropy principle [13] is ofte n applied to evenly distribute pUnk from V and assign it for pUnk . The  X  X iscount X  strategy, as is often called, remains an unsolved research problem since Shannon in-vented N-gram as part of the information theory. Since its publication in 1953, the Good-Turing formula is still a widely used or served as the foundation for many discounting strategies [11]. It states that, if there are n r tokens that appear ex-actly r times in a corpus, then for the purpose of calculating prob-ability we should  X  X retend  X  these tokens appear r * times where Accordingly, the probability of en countering such a token is given by where | T | denotes the total number of tokens in the corpus. By applying the Good-Turing formula for r = 0, we have the total probability mass that should be re served for all the unseen tokens is Namely, the probability mass for the unseen is equal to that of the single occurrence tokens. Obviou sly, how good this discounting strategy is depends heavily on how accurate the Good-Turing formula characterizes the statistical properties of the application in question. Although the Good-Turing has been shown to be useful and superior to many other heuris tics for a wide range of applica-tions, the formula is still seen as enigmatic and finding an intuitive explanation to its underlying heur istics remains an active research question [23]. In this paper, we adopt a more analytically tractable approach to open-vocabulary discount. The key element in our method is a model adaptation algorithm called CALM first proposed by Wang and Li [29]. A close examination of the original presentation re-veals that the adaptation framework in CALM can be explained in an alternative manner using the widely known vector space para-digm. As the original CALM was developed for N-gram, we try to keep the discussion in this section general even though we only report experimental data for unigr am (N = 1) in this paper. 
Figure 1: Cross-stream perplexities against queries for First, we note that a LM can be thought of as a vector from an underlying functional space in wh ich all the admissible language models for a given lexicon V form a simplex of the space, namely, trivial binary lexicon can be represented by a point within the line segment enclosed by (1, 0) and (0, 1) on a two dimensional Eucli-dean space as illustrated in Figure 2. Let P B denote the back-ground LM and P O the statistics of a set of newly observed data we would like to adapt the back ground LM to, respectively. The goal of adaptation is to find a target ,  X   X  T P , P P P the background LM, is minimized . Because the resultant model P has to reside on the simplex, one cannot simply use the Euclidean norm to compute distances and determine the adapted LM without constraints. However, it can be easily verified that such a con-straint can be met if we choose the adjustment vector  X  P along the direction of the difference vector of B O ly, we want to pick a non-negative  X  O so as to point the adjust-ment towards the right direction, and to choose  X  avoid overshoot. Putting it together, we have LM adaptation can therefore be achieved by linear interpolation, assuming the same mathematical form as smoothing. A significant contribution of CALM is to derive how the adapta-tion coefficient can be calculated mathematically when the under-lying LM is based on N-gram assuming a multinomial distribu-tion. Following the work of [29], O  X   X  1 can be interpreted as the prior probability of P B being the correct model and whose closed form formulation can be obtaine d using Stirling X  X  approximation. In the Appendix we show that the adaptation coefficient for a given set of observations O can be computed as where L O and n ( t ) denote the document length and the term fre-quency of the term t , respectively. In short, the adaptation coeffi-cient has a closed form relationship to the Kullback-Leibler (KL) divergence between the background model and the ML estimation of the LM of the document P O ( t ) = n ( t ) / L coefficient  X  O is 0 indeed. The more the background model disa-grees with the observation, the more negative the right hand side of (4) will become, which leads  X  O to approach 1. The CALM interpolation formula of (3) indicates that in the target LM P T , only  X  O portion of the probability comes from the obser-vation. In other words, the observation is  X  X iscounted X  because a probability mass of O  X   X  1 in the target LM is set aside for sources external to the observation. One can therefore use (4) to compute the discount factor of an N-gram by choosing the corresponding (N-1)-gram as the background. For N &gt; 1, (4) coincides with the formulation of the well-known Stol cke heuristics [27] that has been widely used in the N-gram LM pruning: N-grams that can be reasonably predicted by (N-1)-gram can be pruned out of the model. For the purpose of this work, we further extend the idea down to N = 1, where the observation P O and the background P are the unigram and zero-gram LMs, respectively. Conventional-ly, the zero-gram LM refers to the least informative LM that treats every token as OOV, namely, its probability mass is exclusively allocated for OOV. Given an observation with a vocabulary size | V |, a zero-gram LM would just equally distribute its probability mass equally among the vocabulary, leading (4) into As in Sec. 2, H ( P O ) here denotes the (empirical) entropy of the observation LM P O . We can further convert (5) from the loga-rithmic into the linear scale and express the discount factor in terms of perplexity and vocabulary size: The interpretation of this outcome is quite intuitive. As well un-derstood, perplexity is the expected number of alternatives when a language model is used to generate a token each time. The ratio of the perplexity to the vocabulary size characterizes how equivocal the language model is. The result of (6) suggests that the higher the ratio, the less certain the language model is and hence the larger the discount should be. At the extreme case when the per-plexity equals the vocabulary size, the language model is basically generating tokens in the random pattern as the zero-gram, and hence the discount factor becomes 1. In this paper, we compose the smoothed stream component LM P with (3), using an open-vocabulary LM trained from the stream collection as the background model. To be more specific, we first for each document D and each stream i create a closed-vocabulary maximum likelihood model as the observation P The vocabulary for the stream V O,Ci and the closed-vocabulary stream collection model is thus obtained as The discount factor is computed with (6) and is used to attenuate the in-vocabulary probability as The P T,Ci is the open-vocabulary stream collection model. Finally, the stream collection model is used as the background to obtain the smoothed document stream model through linear interpolation Figure 2: A 2-dimesional illu stration of model adaptation Here, the smoothing with the st ream collection model ensures each document LM has the same number of mixture components even though the document does not have some stream observa-tions. This smoothing alleviates the dilemma that some streams are sporadic and sparse for many Web documents. Although the interpolation coefficient  X  Di in (7) can in practice be kept as a free parameter to be empirically tuned (e.g., [32]), a major objective of this work is to explore alternatives that are tuning-free and thus more desirable when an IR system leaves a lab environment. In addition to th e methods described in the next section, we note that the interpolation coefficient in (7) can also be computed using (4) in a document dependent yet query inde-pendent fashion. Several observatio ns can be made from this ap-proach. First, the adaptation coefficient of (4) is document de-pendent as desired. Unlike the Di richlet smoothing used in [32] that can also yield documen t dependent estimation of  X  Di achieves this without having to make a strong assumption that the family of the prior distributions is conjugate to the multinomial distribution. The estimation is fully automatable in that it does not leave us with a free parameter that can vary and has to be empiri-cally determined from applications to applications. CALM can therefore be implemented at the index time not only in a batch mode but also in an online fash ion that model adaptation takes place as soon as the document ente rs the collection (e.g. crawled from the Web). Secondly, since CALM uses a linear interpolation method, the  X  X DF effect X  pointed ou t by Zhai and Lafferty [32] to explain why LM performs well for IR as the traditional TF/IDF approach also applies to CALM. Third, we note that the computa-tion of (4) is light weight. Its co mplexity grows only linearly with the unique terms in the observation. The mixture weights for the component LMs play a central role in the multi-style LM approach. As is in the previous section, we note that we can apply CALM adaptation formula to compute the weights of the multi-component mixture (2) by first re-arranging the distribution as two-component mixture: As (4) can be applied to obtain w 0 , the process can be recursively repeated to obtain other coefficients. Since the goal of the docu-ment LM is to evaluate queries, one would like the model to be close to the query language. Accord ingly, it seems appropriate to choose the query stream as D 0 in (8) so that the CALM formula functions as adapting other mixture components to the query. This method leads to document-dependen t mixture weights, leaves no parameter to tune, and is enticing in terms of engineering Web scale IR because the mixture coefficients can be pre-computed when the documents are being indexed. The query independent nature of th e mixture weights, however, is not as intellectually satisfying as the query dependent ones. While the perplexity studies suggest the average closeness of web docu-ment streams to the queries, we observe that the styles of individ-ual queries vary dramatically: As some queries can benefit from large weights on the anchor text or the user query streams, it is not the case for others, especially those whose target documents are new and yet to be widely linked to or sought after with search engines. Indeed, our pilot studies suggest that query dependent weights outperform query independ ent ones and thus the latter results are omitted in this paper. The query dependent portion of the ranking function is the query likelihood  X   X   X  X  X  X  in (2). The objective of choosing the optimal mixture weights is to maximize this likelihood. As shown in (7), each mixture component itself is a two-component mixture that has parameters to be determined. We can obtain the re-estimation formula under Expectation Ma ximization (EM) algorithm as and To assess the effectiveness of the proposed tuning-free methods for the web document retrieval, we conduct the experiments on the same query set generating the Web test collection previously described by Svore and Burges [28]. The collection consists of 11,845 distinct queries and a retr ieval base of more than 1.2 mil-lion documents with 5-scale relevance judgments that can be used to compute NDCG as the metric for the retrieval function. The data set has the following five streams associated with each web document: document text body (B), title (T), URL (U), anchor text (A), and the user queries (C) that have one or more clicks on the document recorded in the search engine logs. The percentages of documents with n on-empty streams are as shown in Table 1. Table 1: Portions of documents in the test collection with non-empty text body (B), title (T), URL (U), anchor text (A), and Documents that contain only graphic contents, for example, will be regarded as having empty text body. Since the user query stream is sparsely populated, we exclude it from the study in this paper. The test collection, having been studied by multiple institutions, comes with a few well-established retrieval results. In the follow-ing, we report two pertinent expe rimental data as baselines for comparison. The first is based on Okapi BM25 [25] and its multi-field extension [26] (referred to as BM25F below), both of which parameters are taken from the pub lished results in [28]. We note that neither set of the Okapi experiment takes into account the document prior, which we have found to be critical in downplay-ing the roles of undesirable contents such as spam. Similar obser-vations on the importance of document prior have been made for other applications [15]. As such, we adopt the machine learning technique described in [28] and train a neural network ranker based system that uses NDCG@10 as the objective function to combine BM25/F with the document prior, and its results are reported based on leave-one-out cross validation on the test col-lection below. The results, shown in the  X  X ean% (standard devia-tion %) X  format, are labeled as  X  X racle 1 X  since the machine learning is conducted on the test collection with all the relevance judgments. There is no reason to believe the results reported here cannot be reproduced elsewhere, such as the recent TREC Web Track data set [4], provided that the document prior can be computed with methods that effectively confront the prevalent spamming activi-ties on the Web. Specifically, the test collection used in this paper includes a technique described in [30] that identifies spammers based on the HTTP redirection pa tterns. We have found such crawling time features critical and can augment other link graph analysis and content based methods and lead to an effective prior estimation that makes the IR metrics more meaningful. We first conduct a series of sing le style LM experiments to under-stand the merits of the adaptive LM (Sec. 4) against the well-known Dirichlet smoothing based LM. To be precise, the single style LM here means that the document is represented by a single stream. The rationale for the experimental design is as follows. Aside from the open-vocabulary, which does not play a role for single style LM in the IR tasks, the  X  X uning-free X  method uses the same form, i.e., linear interpolation, to smooth the LM. The novelty here is in the manner of how the interpolation coefficients, which can be interpreted as the prior of the distributions to be interpo-lated (Sec. 4.1), are chosen. The Dirichlet approach makes the assumption of conjugate prior, whic h is only contingent upon the distribution family of the LM an d not on the empirical observa-tions. Accordingly, the Dirichlet smoothing leaves a free parame-ter that has to be empirically tuned based on the application data. In contrast, the adaptive LM makes no assumption on the distribu-tion family of the prior. Rather, it capitalizes on the data observed in the document to derive an analytical yet data-driven estimate of the prior, thereby achieving the objective of no free parameters. Table 2 shows the experimental re sults with an emphasis to un-derstand the parameter tuning effects. The experimental condition labeled  X  X ALM X  implements (7) for smoothing, whereas the ex-periments labeled  X  X M X  utilize the EM algorithm to find the in-terpolation coefficient that maximizes the query likelihood for each individual query. As previously described, CALM is an ap-proach where all the parameters can be computed at the document index time, while EM has to be carried out in retrieval time. Even though parameters maximizing query likelihood do not necessari-ly improve NDCG scores, it appears to be the case between the CALM and the EM cases. We note that, even though the CALM method does not further utilize query specific information for smoothing, its performance has already come close to the  X  X M X  method. Both LM approaches record higher NDCG scores than with significance level of 0.05), and come to the high-end perfor-mance of the Oracle 1 that utilizes more data to train the parame-ters. The closeness to the Oracle 1 result is surprisingly encourag-ing because all the LM methods op timize only the indirect meas-ures of query likelihood, whereas in all Oracle 1 cases NDCG@10 is directly optimized on the test collection. To further understand the parameter tuning, we run a grid search on the free parameter in Dirichle t smoothing (from 50 to 500 with a step size 50) and tabulate the corresponding retrieval results in Table 2 labeled as  X  X racle 2 X  in the  X  X ean% (standard devia-tion%) X  format. The results confirm that the choices of free para-meters can introduce significant variances in NDCG, and that the EM method can produce reason able results without tuning. In all cases, the retrieval experiments lend support to the analysis choice for IR tasks, and their relative efficacy seems to track the perplexity prediction well. For example, anchor text is consistent-ly outperforms the title and the body streams across all experi-mental conditions. Table 3 summarizes that experiments that test to what extent mul-tiple streams can be combined to improve retrieval performance. The  X  X ALM + EM X  condition uses the interpolation coefficients for individual streams determined at the index time in the same manner as described in Sec. 6.1, and uses the EM algorithm to compute the mixture weights at the retrieval time when the query is received. In comparison, the  X  X  oint EM X  condition uses the EM algorithm to jointly determine the mixture weights and the inter-manner described in (10) of Sec. 5. As is in the case for the single style LM, the total retrieval time approach  X  X oint EM X  seems to offer consistent better performance than the partially index time method CALM+EM. Both LM methods produce reasonable per-formance, even though they do not utilize any judgment data and only are indirectly optimized for query likelihood rather directly on NDCG. Regardless the modeling techniques, all experimental conditions consistently show that better retrieval performance can be achieved when more streams are included in the retrieval mod-el. The motivation behind the emphasis on  X  X uning free X  is based on our empirical observation that many retrieval methods typically yield dramatically unstable performance, the root cause of which can be traced to their sensitivity to the free parameters in the mod-els. As mixture models increase the number of model parameters, the robustness issue is inevitably exacerbated. We demonstrate the sensitivity issues by including an experimental condition  X  X racle 0 X  in which we retrain the neural net on the test collection to ob-tain the optimal BM25F parameters. As can be seen, the NDCG metrics change dramatically from the baseline where the BM25F parameters were trained on a separa te dataset that is created using the same pooling methodology and judgments guidelines but with the collection harvested from the Web 2 months earlier. More troublingly, such a dramatic swin g in performance metric cannot be discovered through cross validation, as the standard deviations in both Oracle 0 and Oracle 1 appear small. Our investigation confirms that single style stream experiments do not exhibit such a big gap in BM25 performance. This leads to our working hypo-thesis that the combinations of multiple text streams introduce the new performance robustness chal lenges, a topic that warrants more research in the future. The key question of using LM for IR is how to create a LM for each document that best models the queries used to retrieve the document. Studying the textual re sources with the documents, we first present convincing and quan titative evidence that different language styles are used for com posing the document body, title, anchor text, and queries. As such, these different styles are better separately modeled and then combined to form the document language model. The immediate question is how LMs with different vocabulary sets can be combined in a princi pled way. Previous attempts to this so-called open-vocabulary LM problem resorts to heuristics many of which are hard to verify. The most famous and widely used, the Good-Turing formula, is recognized as enigmatic and unintuitive. We propose an altern ative based on rigorous mathe-matical derivations with few assumptions. The same mathematical framework, based on LM adaptation, also suggests that once the open-vocabulary issue is resolved the model combination can be achieved by simple linear interpolation. Such a simple form al-lows us to employ the EM algorithm to dynamically compute the query-document matching scores without tuning free parameters. Our experiments show that the proposed approach can produce retrieval performance close to the high-end oracle results. The authors would like to thank Chris Thrasher, Paul Hsu, Eve-lyne Viegas, Fritz Behr, and Zijian Zheng for the collaboration. The linear interpolation of (3) indicates the adapted distribution P is a mixture of the ML estimation of the observation data P the background model P B . Note that the probability of an event E is the mixture sum of the event taking place under various condi-tions C i weighted by the respective priors P ( C i ): We can view the mixture coefficient in (1) as the prior probability of the respective mixture component being the  X  X eal X  distribution in describing the probabilistic events whose statistical property is characterized by P T . In the case of adaptation, the probability of the background being the real distribution can be estimated by computing how effective the background model predicts the ob-servation where token t occurs n ( t ) times among a total of L kens, namely, With the assumption that the background model P B being a multi-nomial distribution, the probabili ty of the observation evaluated against P B is Equivalently, The factorial terms in the above equation can be approximated by the well-known Stirling formula Accordingly, we have Note that the mixture weight is the per-token probability whereas P ( O ) above is evaluated over a total of L O tokens. With the sta-tistical independent assumptions of the tokens in the LM, we have which leads to (4). [1] Berger, A. and Lafferty, J. 1999. Information retrieval as [2] Brown, P., della Pietra, S. A., della Pietra, V. J., Lai, J., [3] Bulyko, I., Ostendorff, M., Siu, M., Ng, T., Stolcke, A., and [4] Clark, C. L. A., and Craswell, N. 2009. Report on the TREC [5] Collins-Thompson, K. and Callan, J. 2005. Query expansion [6] Croft, W. B., Metzler, D., and Strohman, T. 2009. Search [7] Duda, R. O., Hart, P. E. 1973. Pattern Classification and [8] Fang, H., Tao, T., Zhai, C. 2004. A formal study of informa-[9] Gao, J., Nie, J., Wu, G., Ca o, G. 2004. Dependence language [10] Hiemstra, D. and Kraaij, W. 2005. 21 language models at [11] Huang, X. D., Acero, A., and Hon, H.-W. 2001. Spoken Lan-[12] Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and Behr, F. [13] Jaynes, E. T. 1957. Information theory and statistical me-[14] Jin, R., Hauptmann, and Zhai, C. 2002. Title language model [15] Kraaij, W., Westerveld, T., and Hiemstra, D., 2002. The [16] Lafferty, J. and Zhai, C. 2001. Document language models, [17] Lavrenko, V., and Croft, W. B. 2001. Relevance-based lan-[18] Manning, C., Raghavan, P. , and Schutze, H. 2008. Introduc-[19] Maron, M, and Kuhns, J. 1960. On relevance, probabilistic [20] Microsoft web n-gram services. [21] Miller, D., Leek, T., Schwartz , R. M. 1999. A hidden Mar-[22] Ogilvie, P. and Callan, J. 2003. Combining document repre-[23] Orlitsky, A., Santhanam, N. P., and Zhang, J. 2003. Always [24] Ponte, J., and W. B. Croft. 1998. A language model approach [25] Robertson, S. E., Walker, S., Sparck-Jones, K. S., Hancock-[26] Robertson, S. E., Zaragoza, H. , and Taylor, M. 2004. Simple [27] Stolcke, A. 1998. Entropy-based pruning of backoff language [28] Svore, K. M. and Burges, C. J. C. 2009. A machine learning [29] Wang, K. and Li, X. 2009. Efficacy of a constantly adaptive [30] Wang, Y.-M., Ma, M., Niu, Y. , and Chen, H. 2007. Spam [31] Zhai, C. 2008. Statistical langu age models for information [32] Zhai, C. and Lafferty, J. 2001. A study of smoothing me-[33] Zhai, C., and Lafferty, J. 2002. Two-stage language models 
