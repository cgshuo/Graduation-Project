 The object name and visual description are two most common metadata available for the concrete objects. The conversion between th ese two attributes is effective in various situations, as indicated below.
 Searching: Today X  X  web search engines have high reliability in finding web pages when the object name is specified. For example, if the product X  X  name was given, its web site comes into top entries in most cases. However, the name of the entity is not always available. The existing search engines do not give reliable results when only the product X  X  description was given. In case where the name of the object is not known by the user, it is often difficult to reach the product X  X  page only by descriptions.
 Navigation: In car or walk navigation systems, the names of the building are not helpful if the user is not well acquainted with the area. The visual descriptions are preferred in these situ-ations. Since most existing navigational systems store the building data by their names, a conversion system between the object X  X  name and visual description is necessary to meet this requirement.

The conversion module between object names and visual descriptions can be applied to various fields. However, such general conversion engine does not exist yet. The goal of this research is to create a system for converting object names with visual descrip-tions, and vice versa, by applying knowledge mining to large text sources, such as the Web and encyclopedias, as indicated in Figure 1.

The rest of the paper is organized as follo ws. Section 2 is the related work. Section 3 discusses the searching ad navigation based on visual descriptions. Section 4 discusses extraction of the name and the visual description pairs from large size text data. Section 5 is the conclusion. The use of visual descriptions has been discussed for various applications. Jaimes and Chang described a conceptual framework for indexing different aspects of vi-sual information[1]. Mark et al. modeled visual descriptions specific to geographic objects[2]. The M-OntoMat-Annotizer is a tool for semantic annotation on images and videos [3][4]. It has the Visual Descriptor Extraction (VDE) tool as a plug-in that links RDF domain ontology and MPEG-7 visual descriptors. However, in this model MPEG-7 visual descriptors must be registered manually beforehand.

There have been researches on image based s earching. Julia discussed the real-space search environment[6]. Watanabe et al. im plemented a system where the characters contained in a picture is used as a search query[7]. Finding similar images based on color and texture is a popular mechanism. In these systems, images are used as search queries[8][9]. However, searching based on the shapes of the objects is still difficult, especially for three dimensional objects[10][11]. Therefore, at present moment, our method of searching based on visu al descriptions is effective.

Nakaoka et al. discussed search engines for children[12]. They pointed out that chil-dren often have difficulties in giving names of objects they want to search for. The chil-dren X  X  search engine must provide means to understand children X  X  intentions without asking for specific object names.

The use of visually significant objects such as landmarks in car navigation has been discussed for example by Michon and Denis, Elias and Brenner[13][14][16]. These researches suggest that landmarks provide effective mean of route navigation. However, visual descriptions of landmarks are more effective than the landmark names, especially for the first time visitors. As the domain of the search engines continue to expand, the search engines must evolve also. In the real-space mobile or ubiquitous environment, object names are not always available, and the visual descriptions are the only queries that the user can provide. At present moment, object identification based on RFID (radio frequency identification) tags have not been wide spread yet, and the image recognition technology is still under development. The automatic identification of the object being unsatisfactory in many situations, our model of converting visual descriptions to object names is effective mean of enhancing the search engines.
 The application examples are as indicated below.
 Tourism: The user is unable to give the name of the building he/she sees. He/she can only give the present location, and describe the building. This is also true when he/she encountered any kind of unfamiliar objects while traveling.
 Unfamiliar tools: In case of rarely-used tools such as the home workshop tools, the user can not remember their names, and only give their descriptions.
 Web search queries by children: In the web search engines for children, visual de-scriptions play important role, since children often can not give the exact name of what he/she wants to search. Instead, they can describe the objects using visual impressions.
In general, the visual-description based search engines from the user side consist of the following steps. 1. The user send visual description as the query. 2. The system stems the description. 3. The system find the object name that best matches the stemmed terms. 4. The search engine finds the corresponding web page.

On the other hand, spatial description can be used for navigation tasks also. Most of the car or walk navigation systems today have visualization mechanism by either the map or 3D graphics interface. 3D graphics is said to require too much user attention, and therefore not preferred while driving. Describing routes by machine voice is often safer for the drivers. In such case, the visual descriptions of the buildings are effective.
The visual description is often more us eful than the object X  X  name in car or walk navigation. For example, if the name  X  X okyo Station X  was provided, the user who is not well acquainted with the area has no idea what kind of building he/she must look for. In such case, providing the visual description  X  X he old red brick building X  is helpful. The visual description mining means obtaining the pairs of the object names and visual descriptions from data sources.

The dependency analyzer, a commonly used tool in natural language analysis, is used to extract noun-to-describer relationships.

From a preliminary experiment, we have ext racted actual terms or phrases that in-dicate visual descriptions. Some of these indicators found on the extracted surrounding texts are listed in Table 1. The frequencies of these terms in the surrounding texts are used for distinguishing visual descriptions from general descriptions.

Components of the VD-Miner, our visual description mining engine, are as follows. 1. The parser obtains surrounding texts from the source texts. 2. The stemmer converts terms into stems. 3. The dependency analyzer extracts terms describing the target object. 4. The aggregator integrates the terms to quantize the descriptions.

The scripts were written using Perl. Googl e Web API was used to collect the source web pages[17]. The dependency analysis i s performed using a Japanese dependency analyzer KNP[18]. The result is stored as a rel ational database built on the PostgreSQL database management system.
 Figure 2 is the overall structure of the VD-Miner.

Although our present system uses the Web as the source of data, visual descriptions can also be extracted from encyclopedias in the same way. The data mining is easier and the result is more trustworthy in this case, since encyclopedias have structured data. This paper discussed the conversion system between the name and the visual description of objects. We have discussed enhanced search interface and navigational systems as the application examples. We gave the overview on the mining of visual description based on web resources and encyclopedia data. The experiments were performed for significant geographic objects, and the frequencies of visual descriptions contained in the surrounding texts were measured. We have also extracted indicators that signify visual description phrases from other descriptions, based on the obtained text.
Future works include refinement of the indicators of visual descriptions, the mech-anism for the automatic extraction of visual descriptions from encyclopedia, and also the merging of the retrieved information into the knowledge base.

