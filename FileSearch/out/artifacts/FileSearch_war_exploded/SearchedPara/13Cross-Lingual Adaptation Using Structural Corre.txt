 Over the past two decades supervised machine learning methods have been success-fully applied to many problems in natural language processing (e.g., named entity recognition, relation extraction, sentiment analysis) and information retrieval (e.g., text classification, information filtering). These methods, however, rely on large, an-notated training corpora whose acquisition is time consuming, costly, and inherently language-specific. As a consequence most of the available training corpora are in English only. Since an ever-increasing fraction of the textual content available in dig-ital form is written in languages other than English 1 , this fact limits the widespread application of state-of-the-art techniques from Natural Language Processing (NLP) and Information Retrieval (IR). Technology for cross-lingual adaptation aims to over-come this problem by transferring the knowledge encoded within annotated (= labeled) data written in a source language to create a classifier for a different target language. Cross-lingual adaptation can thus be viewed as a special case of domain adaptation where each language acts as a separate domain.

In contrast to  X  X lassical X  domain adaptation, cross-lingual adaptation is character-ized by the fact that the two domains, that is, the languages, have nonoverlapping feature spaces, which has both theoretical and practical implications for domain adap-tation. In classical domain adaptation, as well as in related problems such as covariate shift, overlapping feature spaces are implicitly presumed by the following or similar assumptions: (1) the existence of generalizable features, that is, features which behave similarly in both domains [Blitzer et al. 2006; Daume 2007; Jiang and Zhai 2007], and, (2) the support of the test data distribution is contained in the support of the train-ing data distribution [Bickel et al. 2009]. If, on the other hand, the feature sets are nonoverlapping, one needs external knowledge to link features of the source domain and the target domain [Dai et al. 2008].

This article presents an approach for cross-lingual adaptation in the context of text classification: Cross-Language Structural Correspondence Learning (CL-SCL). CL-SCL uses unlabeled data from both languages along with external domain knowledge in the form of a word translation oracle to i nduce cross-lingual word correspondences. The approach is based on Structural Corresp ondence Learning (SCL), a recently pro-posed algorithm for domain adaptation in natural language processing [Blitzer et al. 2006, 2007].

Similar to SCL, CL-SCL induces correspondences among the words from both do-mains, that is, languages, using a small number of so-called pivots . In CL-SCL, a pivot is a pair of words, { w S ,w T } , from the source language S and the target language T , which possess similar semantics. Testing the occurrence of w S or w T in a set of un-labeled documents from S and T yields two classes across these languages: one class contains the documents where either w S or w T occur, the other class contains the doc-uments where neither w S nor w T occur. Ideally, a pivot splits the set of unlabeled documents with respect to the semantics that is associated with { w S ,w T } .Thecor-relation between w S or w T and other words w , w  X  X  w S ,w T } is modeled by a linear classifier which then is used as a language-independent predictor for the two classes. A small number of pivots can capture a sufficiently large part of the correspondences between S and T in order to (1) construct a cross-lingual representation and (2) learn a classifier that operates on this representation. Several advantages follow from this approach.  X  Task specificity . The approach induces task-specific word correspondences since it considers X  X uring the pivot selection step X  task-specific characteristics of language use.  X  Efficiency in terms of linguistic resources . The approach uses unlabeled docu-ments from both languages along with a small budget of calls (100 X 500) to a word translation oracle, instead of employing a parallel corpus or an extensive bilingual dictionary.  X  Efficiency in terms of computing resources . The approach solves the classification problem directly, instead of resorting to a more general and potentially much harder problem such as machine translation.

This article is an extended version of [Prettenhofer and Stein 2010]. It includes a more detailed discussion of related work, computational considerations, and new experiments. In particular, we propose a novel strategy to obtain a sparse parameter matrix in the third step of CL-SCL, which leads to a significant improvement upon our previously reported results. This contribution has implications beyond CL-SCL, in particular for SCL [Blitzer et al. 2006] and related techniques such as Alternating Structural Optimization [Ando and Zhang 2005a].

The article is organized as follows: Section 2 discusses cross-lingual adaptation in the context of related work including domain adaptation and dataset shift. Section 3 introduces the problem of cross-language te xt classification, a special case of cross-lingual adaptation. Section 4 describes Cro ss-Language Structural Correspondence Learning, including computational considerations. Section 5 reports on the design and the results of experiments in the area of cross-language sentiment classification and topic classification. Section 6 concludes our work. The idea to transfer knowledge from a source learning setting S to a different target learning setting T is an active field of research [Pan and Yang 2009], and Figure 1 organizes well-known problems of this field within a taxonomy. The taxonomy starts with the two most important determinants in a learning setting, namely, the domain and the task . A domain is defined by: (1) a set of features M , (2) a space of possible feature vector realizations x , which typically is the R | M | , and (3) a probability distri-bution P ( x ) over the space of possible feature vector realizations. 2 Ataskspecifiesa set of labels corresponding to classes, typically { +1 ,  X  1 } , along with a conditional dis-tribution P ( y | x ), with y  X  X  +1 ,  X  1 } . Alternatively, a task can be specified by a sample { ( x , y ) | x  X  R | M | , y  X  X  +1 ,  X  1 }} . In Figure 1 the domain adaptation branch is unfolded since it is the focus of this article. The upper subbranch addresses problems where the feature sets are unchanged; without loss of generality P S ( x ) = P T ( x ) can also be presumed for problems in the lower subbranch  X  X ifferent feature sets. X  Domain adaptation refers to the problem of adapting a statistical classifier trained on data from one or more source domains to a different target domain. In the basic domain adaptation setting we are given labeled data from a source domain S and unlabeled domain. Beyond this setting one can further distinguish whether a small amount of labeled data from the target domain is available [Daume 2007; Finkel and Manning 2009] or not [Blitzer et al. 2006; Jiang and Zhai 2007]. The latter setting is referred to as unsupervised domain adaptation.

Blitzer et al. [2006] propose an effective algorithm for unsupervised domain adapta-tion, called Structural Correspondence Learning. In a first step, SCL selects features that generalize across domains, which the authors call pivots. SCL then models the correlation between the pivots and all other features by training linear classifiers on the unlabeled data from both domains. This information is used to induce correspon-dences among features from the different domains and to learn a shared representa-tion  X  that is meaningful across both domains. This shared representation is appended to the original feature space, that is, each instance is represented as x = x  X  x ,acon-catenation of the original and the shared representation. Concatenating the two fea-ture spaces enables the classifier to exploit both generalizable features in the original representation as well as feature correspondences in the shared representation.
The major difference between CL-SCL and SCL refers to pivot selection and, in particular, the notion of a pivot. In SCL, pivots are features that generalize across domains. In cross-lingual adaptation the two domains have nonoverlapping feature spaces, and hence one needs external knowledge to link the two feature spaces. In CL-SCL this external knowledge is represented by a feature translation oracle and pivots are pairs of features, one from each domain, that behave similarly in both domains. Furthermore, the original feature space c annot be used in cross-lingual adaptation, again due to nonoverlapping feature spaces. 3
SCL is related to the structural learning paradigm introduced by [Ando and Zhang 2005a]. The basic idea of structural learning is to constrain the hypothesis space of a learning task by considering multiple different but related tasks on the same input space. Based on this paradigm Ando and Zhang [2005b] present a semisupervised learning method, called Alternating Structural Optimization (ASO), which automati-cally generates related tasks from unlabeled data. These auxiliary tasks correspond to the pivots in SCL. In ASO, however, their purpose is to leverage information in un-labeled data in order to create a better feature space which improves the learning of the target task, especially in the presence of little labeled training data. The authors show that ASO delivers state-of-the-art performance for a variety of natural language processing tasks including named entity recognition and syntactic chunking. Quattoni et al. [2007] apply ASO to image classification in settings where little labeled data is given. Traditional machine learning assumes that both training examples and test examples are drawn from identical distributions. This assumption is often violated in prac-tice, for instance due to the irreproducibility of the test conditions during the training phase. Dataset shift refers to the general problem when the joint distribution of in-puts (= feature vectors) and outputs (= labels) differs between training phase and test phase. The difference between dataset shift and domain adaptation is subtle; in fact, both refer to the same underlying problem bu t emerge from the viewpoints of different research communities. Dataset shift is coined by the machine learning community and builds on prior work in statistics, especially the work on covariate shift [Shimodaira 2000] and sample selection bias [Cortes et al. 2008]. In contrast, domain adaptation originates from the natural language processing community. Most of the early work on domain adaptation focuses on the question of how to leverage  X  X ut-domain data X  (= data associated with S ) effectively to learn a classifier when only little or no la-beled  X  X n-domain data X  (= data associated with T ) is available. The latter emphasizes the relationship to semisupervised learning, with the crucial difference that labeled data and unlabeled data stem from different distributions. Covariate shift can be considered as a special case of dataset shift that is closely related to unsupervised do-main adaptation. Covariate shift is characterized by the fact that the underlying class conditional distribution between training phase and test phase is identical, that is, P
S ( y | x )= P T ( y | x ), while the marginal distribution of the inputs (= covariates) dif-fers, that is, P S ( x ) = P T ( x ). An in-depth discussion of dataset shift is beyond the scope of this article, and the interested reader is referred to Quionero-Candela et al. [2009]. Similar to domain adaptation, cross-lingual adaptation refers to the problem of adapt-ing a statistical classifier trained on data from a source language S to a different target language T . Examples include the adaptation of a named entity recognizer, a syntactic parser, or a relation extractor. The major characteristic of cross-lingual adaptation is the fact that the two  X  X omains X  have nonoverlapping features sets, that is, M S = M T . While cross-lingual adaptation has not received much attention in the natural language processing community, 4 a special case of cross-lingual adaptation re-cently gained considerable interest: cross-language text classification, which is also the focus of this article.

Bel et al. [2003] belong to the first who explicitly considered the problem of cross-language text classification. Their research, however, is predated by work in Cross-Language Information Retrieval (CLIR) wh ere similar problems are addressed [Oard 1998]. Traditional approaches to cross-language text classification and CLIR use lin-guistic resources such as bilingual dictionaries or parallel corpora to induce correspon-dences between two languages [Lavrenko et al. 2002; Olsson et al. 2005]. Dumais et al. [1997] is considered as seminal work in CLIR: they propose a method which induces se-mantic correspondences between two languages by performing Latent Semantic Anal-ysis (LSA) on a parallel corpus. Li and Taylor [2007] improved upon this method by employing kernel Canonical Correlation Analysis (CCA) instead of LSA. The major limitations of these approaches are their computational complexity and their depen-dence on parallel corpora, which are hard to obtain, especially for less resource-rich languages. Gliozzo and Strapparava [2005] circumvent the dependence on a parallel corpus by using so-called multilingual domain models, which can be acquired from comparable corpora in an unsupervised manner. In Gliozzo and Strapparava [2006] they show for particular tasks that their approach can achieve a performance close to that of monolingual text classification.

Recent work in cross-language text classification focuses on the use of automatic machine translation technology. Most of these methods involve two steps: (1) trans-lation of the documents into the source or the target language, and (2) dimension-ality reduction or semisupervised learning to reduce the noise introduced by the machine translation. Methods which follow this two-step approach include the EM-based approach by Rigutini et al. [2005], the CCA approach by Fortuna and Shawe-Taylor [2005], the information bottleneck approach by Ling et al. [2008], and the cotraining approach by Wan [2009]. The work of Wei and Pal [2010] and Margolis et al. [2010] is closely related to our work. Unlike our approach they also rely on automatic machine translation in a first step and then apply SCL to reduce the noise introduced by the machine translation. In standard text classification, a document d is represented under the bag-of-words model as | V | -dimensional feature vector x  X  X ,where V , the vocabulary, denotes an ordered set of words, x i  X  x denotes the normalized frequency of word i in d ,and X is an inner product space. D S denotes the training set and comprises tuples of the form ( x , y ), which associate a feature vector x  X  X with a class label y  X  Y . For simplicity but without loss of generality we assume binary classification problems, Y = { +1, -1 } . The goal is to find a classifier f : X  X  Y that predicts the labels of new, previously unseen documents. In the following, we restrict ourselves to linear classifiers. where w is a weight vector that parameterizes the classifier and [  X  ] T denotes the ma-trix transpose. The computation of w from D S is referred to as model estimation or training. A common choice for w is given by a vector w  X  that minimizes the regularized training error. L is a loss function that measures the effectiveness (= classification performance) of the classifier, R is a regularization term that penalizes model complexity, and  X  is a nonnegative hyperparameter that models the trade-off between classification per-formance and model complexity. A common choice for R is L2-regularization, which imposes an L2-norm penalty on w , R ( w )= 1 2 w 2 2 = 1 2 w T w . Different choices for L entail different classifier types; for example, when choosing the hinge loss function one obtains the popular Support Vector Machine classifier [Zhang 2004].

Standard text classification distinguishes between labeled (training) documents and unlabeled (test) documents. Cross-language text classification poses an extra con-straint in that training documents and test documents are written in different lan-guages. Here, the language of the training documents is referred to as source language S , and the language of the test documents is referred to as target language T .The vocabulary V divides into V S and V T , called vocabulary of the source language and vocabulary of the target language, with V S  X  V T =  X  . In other words, documents from the training set and the test set map onto nonoverlapping regions of the feature space. Thus, a linear classifier trained on D S associates nonzero weights only with words from V S , which in turn means that it cannot be used to classify documents written in T .

One way to overcome this  X  X eature barrier X  is to find a cross-lingual representation for documents written in S and T that transfers classification knowledge between the two languages. Intuitively, one can understand such a cross-lingual representation as a concept space that underlies both languages. In the following, we will use  X  to denote a map that associates the original | V | -dimensional representation of a document d written in S or T with its k -dimensional cross-lingual representation,  X  : R | V |  X  R k . Once such a mapping is found the cross-language text classification problem becomes a standard classification problem in the cross-lingual feature space. Note that the existing methods for cross-language text classification can be characterized by the way  X  is constructed. For instance, cross-language latent semantic indexing [Dumais et al. 1997] and cross-language explicit semantic analysis [Potthast et al. 2008] estimate  X  using a parallel corpus. Other methods use linguistic resources such as a bilingual dictionary to obtain  X  [Bel et al. 2003; Olsson et al. 2005; Wu et al. 2008]. We now present a method for learning a map  X  by exploiting relations from unlabeled documents written in S and T . The proposed method, which we call Cross-Language Structural Correspondence Learning, CL-SCL, addresses the following learning setup (see also Figure 2): (1) Given a set of labeled training documents D S writteninlanguage S ,thegoalisto (2) In addition to the labeled training documents D S we have access to unlabeled doc-(3) Finally, we are given a budget of calls to a word translation oracle (e.g., a domain
CL-SCL comprises three steps: In the first step, CL-SCL selects word pairs { w
S ,w T } , called pivots, where w S  X  V S and w T  X  V T . Pivotshavetosatisfythe following conditions.
 Confidence. Both words, w S and w T , are predictive for the target task.
 Support. Both words, w S and w T , occur frequently in D S , u and D T , u respectively.
The confidence condition ensures that in the second step of CL-SCL only those corre-lations are modeled that are useful for discriminative learning. The support condition, on the other hand, ensures that these correlations can be estimated accurately. Con-sidering our sentiment classification example, the word pair { excellent S , exzellent T } satisfies both conditions: (1) the words are strong indicators of positive sentiment, and (2) the words occur frequently in book reviews from both languages. Note that the support of w S and w T can be determined from the unlabeled data D u . The confidence, however, can only be determined for w S since the setting gives us access to labeled data from S only.

We use the following heuristic to form an ordered set P of pivots: First, we choose asubset V P from the source vocabulary V S , | V P | | V S | , which contains those words with the highest mutual information with respect to the class label of the target task in D S . Second, for each word w S  X  V P we find its translation in the target vocabulary V
T by querying the translation oracle; we refer to the resulting set of word pairs as the candidate pivots P .
 If the translation oracle fails to find a translation w T  X  V T , w S is discarded.
We enforce the support condition by eliminating in P all candidate pivots { w S ,w T } where the document frequency of w S in D S , u or of w T in D T , u is smaller than some threshold  X  .
 Let m denote | P | , the number of pivots.

In the second step, CL-SCL models the correlations between each pivot { w S ,w T } X  P and all other words w  X  V \{ w S ,w T } . This is done by training classifiers that predict whether or not w S or w T occur in a document, based on the other words. For this purpose a training set D l is created for each pivot p l  X  P .
 MASK ( x , p l ) is a function that returns a copy of x where the components associated with the two words in p l are set to zero, which is equivalent to removing these words from the feature space. IN ( x , p l ) returns +1 if one of the components of x associated with the words in p l is nonzero and  X  1 otherwise. The support condition merely ensures that there is a sufficient number of positive training examples to accurately estimate the classifiers, that is, to avoid cases of extreme class imbalance. For each D l a linear classifier, characterized by the parameter vector w l , is trained by minimizing Eq. (2) on D l . Recall that each training set D l contains documents from both languages. Thus, for a pivot p l = { w S ,w T } the vector w l captures both the correlation between w S and V S \{ w S } and the correlation between w T and V T \{ w T } .

In the third step, CL-SCL identifies correlations across pivots by computing the singular value decomposition of the | V | X  m -dimensional parameter matrix W , W = w 1 ... w m .
 W encodes the correlation structure between pivot and non-pivot words in the form of multiple linear classifiers. In other words, the columns of U identify common substruc-tures among these classifiers. Choosing the columns of U associated with the largest singular values yields the substructures that capture most of the correlation in W .We define  X  as those columns of U that are associated with the k largest singular values. Algorithm 1 summarizes the three steps of CL-SCL. At training and test time, we apply the projection  X  to each input instance x .Thevector v  X  that minimizes the regularized training error for D S in the projected space is defined as follows.
The resulting classifier, which will operate in the cross-lingual setting, is defined as follows. Although the second step of CL-SCL involves the training of a fairly large number of linear classifiers, these classifiers can be learned efficiently due to: (1) efficient learn-ing algorithms for linear classifiers [Shalev-Shwartz et al. 2007] and (2) the fact that learning the pivot classifiers is an embarrassingly parallel problem. The computa-tional bottleneck of the CL-SCL procedure is the SVD of the dense parameter matrix W . [Ando and Zhang 2005a] as well as [Blitzer et al. 2007] propose to set negative entries in W to zero, in order to obtain a sparse matrix for which the SVD can be computed more efficiently [Berry 1992]. As a rationale for this step [Ando and Zhang 2005a] claim that  X  X he positive weights of a linear classifier are usually directly related to the target concept, while the negative components often yield much less specific information. X 
We propose a different strategy to obtain a sparse parameter matrix W ,namely to enforce sparse pivot classifiers w l by employing a proper regularization term R in the second step of CL-SCL. A straightforward solution is to use L1 regularization [Tibshirani 1996], which imposes an L1-norm penalty on w , R ( w )= w 1 = | V | j =1 | w j | . This strategy recently gained much attent ion in the natural language processing com-munity; [Gao et al. 2007] show that L1 regularized models have similar predictive power to L2 regularized models while being much smaller at the same time, that is, less parameters are nonzero.
 L1 regularization, however, has properties which are inadequate in the context of SCL, in particular its handling of highly correlated features. Zou and Hastie [2005] show that if there is a subset of features among which the pairwise correlations are high, L1 regularization tends to select only one feature while pushing the other feature weights towards zero. This is certainly not desirable for SCL since it relies on the proper modeling of correlations in order to induce correspondences among features. L2 regularization, by contrast, exhibits a  X  X rouping behavior, X  resulting in equal weights for correlated features. The SVD exploits this effect to find linear dependencies in the parameter matrix W . The Elastic Net combines both properties, the sparsity property of L1 regularization and the grouping behavior of L2 regularization [Zou and Hastie 2005]. It is given by the convex combination of both norms where  X   X  [0; 1] models the trade-off between grouping and sparsity. The Elastic Net is widely used in bioinformatics, in particular in the study of gene expression. An alternative view of cross-language struct ural correspondence learning is provided by the framework of structural learning [Ando and Zhang 2005a]. The basic idea of structural learning is to constrain the hypothesis space, that is, the space of possible weight vectors w  X  R | V | of the target task, by considering multiple different but re-lated auxiliary prediction tasks. Here, these auxiliary tasks are represented by the pivot classifiers, that is, the columns of W . Each column vector w l can be considered as a linear classifier which performs well in both languages.  X  T defines the principal com-ponents of these bilingual classifiers; it characterizes what good bilingual classifiers are like. These principal components span a subspace of the parameter space which we regard as an approximation to the subspace of bilingual classifiers 6 . The basic idea is to find a classifier in this subspace which does well on D S ,because X  X f  X  T is a good approximation to the subspace of bilingual classifiers X  X uch a classifier will do equally well on the target data. Following Ando and Zhang [2005a] and Quattoni et al. [2007], we restrict the weight vector w  X  for the target task to lie in the subspace defined by  X  , w  X  =  X  T v  X  ,where v  X  is defined as follows. Because of (  X  T v ) T = v T  X  this view of CL-SCL corresponds to the induction of a new fea-ture space as given by Eq. (3). Figure 3 illustrates the idea of the subspace constraint for | V | =3, m =3,and k =2. We evaluate CL-SCL for the tasks of cross-language sentiment classification and topic classification, using English as source language and German, French, and Japanese as target languages. We describe the experiment design, give implementation details, and present the evaluation results. Moreover, we give detailed analyses with respect to the nature of the induced cross-lingual correspondences, the use of unlabeled data, and important hyperparameters including th e effect of different regularization terms for the pivot classifiers. We use the cross-lingual sentiment dataset provided by [Prettenhofer and Stein 2010]. 7 The dataset contains Amazon product rev iews for the three product categories books, dvd, and music in the languages English, German, French, and Japanese. Each document is labeled according to its sentiment polarity as either positive or negative. The documents in the dataset are organized by language and product category. There are three balanced disjoint sets of training, test, and unlabeled documents for each language-category pair; the respective set sizes are 2,000, 2,000, and 9,000 X 50,000. Similar to [Prettenhofer and Stein 2010], each document d is represented as a normal-ized (unit length) feature vector x under a unigram bag-of-words model. Based on this dataset we create two tasks (see Table I for a summary statistics):
Sentiment Classification Task. For the task of cross-language sentiment classification we use the original partitioning of the cross-lingual sentiment dataset. Analogous to [Prettenhofer and Stein 2010] we use English as source language and German, French, and Japanese as target languages. For each of the nine target-language-category-combinations a sentiment classification task is created by taking the training set and the unlabeled set of the product category from S and the test set and the unlabeled set of the product category from T .

Topic Classification Task. For the task of cross-language topic classification we discard the original sentiment labels and use the p roduct category, that is, books, dvd, and music as document label. Again we use English as source language and German, French, and Japanese as target languages. In contrast to the sentiment classification tasks, the classification of reviews accordin g to product categories is a multiclass prob-lem with mutually exclusive classes. Hence for each of the three target languages one cross-language topic classification task is created with the training set and the unla-beled set of all product categories from S and the test set and the unlabeled set of all product category from T . For each of the three tasks we have 6,000 training and 6,000 test documents, containing a balanced number of examples. We set the number of unlabeled documents to 20,000 from each language-category pair. All experiments employ linear classifiers, which are trained by minimizing Eq. (2) us-ing Stochastic Gradient Descent (SGD). We use the plain SGD algorithm as described by [Zhang 2004] while adopting the learning rate schedule from PEGASOS [Shalev-Shwartz et al. 2007]. Similar to [Blitzer et al. 2007] and [Ando and Zhang 2005a], the modified Huber loss [Zhang 2004], a smoothed version of the hinge loss, is used as loss function L .

SGD and related methods based on stochastic approximation have been successfully applied to solve large-scale linear prediction problems in natural language processing and information retrieval [Shalev-Shwartz et al. 2007; Zhang 2004]. Their major ad-vantages are efficiency and ease of implementation. SGD, however, cannot be applied directly in connection with L1 regularization (and thus the Elastic Net) due to the fluctuations of the approximated gradients. To overcome this problem different solu-tions have been proposed, such as methods based on truncated gradients [Langford et al. 2009; Tsuruoka et al. 2009] and projected gradients [Duchi et al. 2008]. In our experiments we resort to the truncated stochastic gradient algorithm proposed by [Tsuruoka et al. 2009], which uses the cumulative L1 penalty to smooth out fluctu-ations in the approximated gradients. Note that Elastic Net regularization is applied for the pivot classifiers only; all other classifiers are trained using L2 regularization.
SGD receives two hyperparameters as input: the number of iterations T ,andthe regularization parameter  X  . In our experiments T is always set to 10 6 , which is about the number of iterations required for SGD to converge. For the target task,  X  is deter-mined by 3-fold cross-validation, testing for  X  all values 10  X  i , i  X  [0; 6]. For the pivot prediction task,  X  is set to the small value of 10  X  5 , in order to favor model accuracy over generalizability.

Since SGD is sensitive to feature scaling the projection  X  x is postprocessed as fol-lows: (1) Each feature of the cross-lingual representation is standardized to zero mean and unit variance, where mean and variance are estimated on D S  X  D u . (2) The cross-lingual document representations are scaled by a constant  X  such that | D
For multiclass classification the one-against-all strategy is applied. For multiclass problems, the subset V P from the source vocabulary V S is chosen as follows: (1) rank for each class the words according to mutual information with respect to all other classes, and (2) select the top ranked words from each ranking.

We follow Prettenhofer and Stein [2010] and use Google Translate as translation oracle, 8 which returns a single translation for each query word. In order to ensure the reproducibility of our results the cached translations provided by [Prettenhofer and Stein 2010] are used whenever possible. Note that the word translation oracle operates context-free, which is clearly suboptimal, and we refrain from sanitizing the translations in order to demonstrate the robustness of CL-SCL with respect to trans-lation noise.

For performance reasons, we implemented the SGD learning routine in C, the rest of code is written in Python. 9 The training of a single pivot classifier using Elastic Net regularization on 100.000 examples takes less than 3 seconds on a standard PC with an Intel Core2 Quad CPU with 2.83 GHz, exclusive the time to create the training examples. We train the pivot classifiers in parallel using a Hadoop cluster with 12 nodes of the aforesaid configuration. The training time for 450 pivot classifiers is less than two minutes, inclusive the time to create the training examples. For the SVD computation the Lanczos algorithm provided by SVDLIBC is employed. 10 The runtime of the SVD depends on the size of the vocabulary and the sparsity pattern of W ;inour experiments it is usually less than one minute. To get an upper bound on the performance of a cross-language method we first consider the monolingual setting. For each task a linear classifier is learned on the training set of the target language and tested on the test set. The resulting accuracy scores are referred to as upper bound. This bound info rms us about the expected performance on the target task if training data in the target language were available.

We choose two baselines to compare CL-SCL to other cross-language methods. The first baseline, CL-MT, is based on machine translation; it is a straightforward approach to cross-language text classification and has been used in a number of cross-language sentiment classification studies [Bautin et al. 2008; Hiroshi et al. 2004; Wan 2009]. CL-MT is determined as follows: (1) learn a linear classifier on the training data, (2) translate the test documents into the source language, and (3) predict the class label of the translated test documents. The translations of the test documents into the source language via Google Translate are provided by [Prettenhofer and Stein 2010]. Note that CL-MT does not make use of unlabeled documents.

The second baseline, CL-Dict, uses the pivots obtained from the first step of CL-SCL as a bilingual dictionary to construct the mapping  X  . This baseline is similar to the terminology translation approach in Bel et al. [2003]. The difference between CL-Dict and CL-SCL informs us about the extent to which CL-SCL leverages information about word correlations from unlabe led data. In general we expect CL-Dict to be inferior to CL-MT since words are translated without t aking their context into account. Note that CL-Dict uses unlabeled data only for pivot selection purposes. Table II contrasts the classification performance of CL-SCL with the upper bound and the baselines. Due to the inherent randomness of the training algorithm the accuracy scores are reported as mean  X  and standard deviation  X  of ten repetitions of SGD. McNemar X  X  test is used to analyze whether or not the results of CL-SCL and CL-MT are statistically significant [Dietterich 1998]. Again, due to the randomness of the training algorithm, statistical significance is analyzed for each of the ten repetitions, whereas significance at a specific level is reported only if it applies to all repetitions.
Observe that the upper bound does not exhibit high variability across the three languages. For sentiment classification the average accuracy is about 82%, which is consistent with prior work on monolingual sentiment analysis [Blitzer et al. 2007; Pang et al. 2002]. For product category classification the average accuracy is in the low 90 X  X , which is also consistent with prior work on monolingual product category classification [Crammer et al. 2009].

The performance of CL-MT differs considerably between the two European lan-guages and Japanese: for Japanese, the averaged differences between the upper bound and CL-MT (9.5% for sentiment and 7.3% for topic) are much larger than for German and French (5.3% for sentiment and 1.7% for topic). This can be explained by the fact that machine translation works better for European than for Asian languages such as Japanese.

CL-SCL receives four hyperparameters as input: the number of pivots m ,thedi-mensionality of the cross-lingual representation k , the minimum support  X  of a pivot in D S , u and D T , u , and the Elastic Net coefficient  X  . For cross-language sentiment classification we use fixed values of m = 450, k = 100,  X  = 30, and  X  =0 . 85. For cross-language topic classification we found that smaller values of m and k work sig-nificantly better. The results for topic classification are obtained by using fixed values of m = 250, k = 50,  X  = 50, and  X  =0 . 85. The parameter settings have been optimized using the German book review task (sentiment) and the German topic task.
 The results show that CL-SCL either outperforms or is at least competitive with CL-MT across all tasks. For German and Japanese sentiment classification we observe significant differences at a 0.05 and a 0.001 co nfidence level. For product category clas-sification we observe significant differences only for Japanese (0.001 confidence level). Interestingly, for German music reviews, the accuracy of CL-SCL even surpasses the upper bound, which may be interpreted as a semisupervised learning effect that stems from the massive use of unlabeled data.

Table III shows the classification performance relative to the upper bound, that is, the loss due to cross-lingual adaptation. The rightmost column of Table III shows the relative reduction in error due to cross-lingual adaptation of CL-SCL over CL-MT; a relative reduction of 50% means that CL-SCL cuts the adaptation loss of CL-MT by 50%. CL-SCL reduces the relative error by an average of 59% (sentiment clas-sification) and 30% (topic classification) over CL-MT. These results are a significant improvement upon our previously reported results in Prettenhofer and Stein [2010], which is attributed to the use of a different regularization term for the pivot classifiers.
By contrasting the results of CL-SCL and CL-Dict one can also see that CL-SCL suc-cessfully exploits word correlations in the unlabeled data in order to create an effective cross-lingual representation. The poor performance of CL-Dict on certain tasks such as Japanese book reviews suggests that there is considerable noise in the translation oracle, and that CL-SCL is to a considerable extent robust to that noise.
 This subsection analyzes the sensitivity of each of the four hyperparameters of CL-SCL in isolation while keeping the others fixed. If not specified otherwise, we use the same setting of the hyperparameters as in Table II.

Unlabeled Data. The first row of Figure 4 shows the performance of CL-SCL as a function of the ratio of labeled and unlabeled documents for sentiment classification 25 corresponds to the setting of Table II. As expected, an increase in the number of unlabeled documents results in an improved performance. However, a saturation at a ratio of 10 can be observed across most tasks.

Number of Pivots. The second row shows the influence of the number of pivots m on the performance of CL-SCL. Compared to the size of the vocabularies V S and V T , which is in 10 5 order of magnitude, the number of pivots is very small. The plots show that even a small number (100) of pivots captures a significant amount of the correspondence between S and T .

Dimensionality of the Cross-Lingual Representation. The third row shows the influence of the dimensionality of the cross-lingual representation k on the performance of CL-SCL. Taking the top k left singular vectors can be rega rded as a form of noise reduction, which has been introduced by: (1) suboptimal pivot selection and (2) estimation errors of w l . Obviously the SVD is crucial to the success of CL-SCL if m is sufficiently large. Observe that the value of k is task-insensitive: a value of 50 &lt; k &lt; 150 works equally well across all tasks.

Effect of Regularization. Table IV compares the effect of different strategies to obtain a sparse parameter matrix W on the performance of CL-SCL. The third column, L2 + , refers to the strategy in Blitzer et al. [2006] and Prettenhofer and Stein [2010] with L2 regularization and negative weights set to zero. [Blitzer et al. 2006] claim that this strategy does not only reduce runtime and memory consumption but also improves the performance; however, we do not observe such a performance improvement over L2 in our experiments. The fourth column shows the performance of L1 regulariza-tion, which reduces the number of nonzero features compared to L2 + from 16% to 2% on average. In Section 4.1 we argue that L1 regularization is not adequate due to its inadequate handling of highly correlated features and propose the Elastic Net penalty as an alternative. The empirical evidence supports this claim: Elastic Net regulariza-tion consistently outperforms both L2 + and L1 regularization and is competitive with L2 while keeping the number of nonzero features low (15% on average). Elastic Net regularization adds an additional hyperparameter  X  that models the relative impor-tance of L2 and L1 regularization. In the preceding experiments  X  is chosen such that the obtained density roughly equals the density of L2 + . A convenient property of the Elastic Net is that it encompasses L2 and L1 regularization as special cases. Thus, if m and | V | are sufficiently small and a dense SVD is computationally feasible  X  =1is optimal; otherwise, the optimal choice of  X  is governed by the computing resource. The promising results of CL-SCL raise the question why CL-SCL is able to create more effective word correspondences than a state-of-the-art machine translation system. We argue that primarily responsible for the effectiveness of CL-SCL is its ability to induce task-specific word correspondences. Due to the use of task-specific, unlabeled data, relevant characteristics of task-specific language use are captured by the pivot clas-sifiers. Table V exemplifies this claim with two pivots for German and English book reviews. Thetableshowshighly correlated words for the pivots { beautiful S ,sch  X  on T } and { boring S , langweilig T } , which are taken from the 50 highest positive entries in cor-responding weight vectors. One can distinguish between: (1) correlations that reflect similar meaning, such as  X  X mazing, X   X  X ovely, X  or  X  X lain, X  and (2) correlations that reflect the pivot pragmatics with respect to the task, such as  X  X icture, X   X  X oetry, X  or  X  X ages. X  Note in this respect that the authors of book reviews tend to use the word  X  X eautiful X  to refer to illustrations or to poetry, and that they use the word  X  X ages X  to indicate lengthy or boring books. We argue that while the first type of word correlations can be obtained by methods that operate on para llel corpora, the second correlation type requires an understanding of task-specific language use, which in general cannot be obtained from parallel corpora such as Europarl.

In order to gain further inside into the nature of the induced cross-lingual word cor-respondences Figure 5 shows two significant rows of  X  , again for German and English book reviews. 11 Each row in  X  projects all words in the vocabulary onto the real line. Words above the dashed line are from V S , words below the dashed line are from V T . Positive and negative values under these projections imply positive and negative sen-timent, respectively. The words in this illustration are selected as follows: (1) choose those two rows from  X  which receive the highest (positive) weight from the final clas-sifier, and (2) choose representative words from the 100 largest positive and negative entriesineachrow.

The first example clearly discriminates between well-written books which are well developed, organized, and researched, and books which are not, for example, books with a questionable or far-fetched content. The second row, on the other hand, indi-cates whether or not a book had life-changing effects on the author of the book reviews. The word correspondences encoded within  X  are indeed highly relevant to the task at hand, namely discriminating between positive and negative book reviews. This is a major advantage of CL-SCL over existing concept-based approaches to cross-language text classification [Dumais et al. 1997; Gliozzo and Strapparava 2006; Li and Taylor 2007; Potthast et al. 2008] since they induce cross-lingual word correspondences with-out considering the target task. This article presents Cross-Language Stru ctural Correspondence Learning, CL-SCL, as an effective technology for cross-lingual adaptation in the context of text classifica-tion. CL-SCL builds on Structural Correspondence Learning, a recently proposed algo-rithm for domain adaptation in natural language processing. CL-SCL uses unlabeled documents along with a word translation oracle to automatically induce task-specific, cross-lingual word correspondences.

We evaluated the approach for cross-language text classification, a special case of cross-lingual adaptation. The analysis covers performance and sensitivity issues in the context of sentiment and topic classification with English as source language and German, French, and Japanese as target languages. The results show a significant im-provement of our approach over a machine translation baseline, reducing the relative error due to cross-lingual adaptation by an average of 59% (sentiment classification) and 30% (topic classification). Furthermore, the Elastic Net is proposed as an effec-tive means to obtain a sparse parameter matrix, leading to a significant improvement upon our previously reported results [Prettenhofer and Stein 2010]. This technique has implications beyond CL-SCL, in particu lar for Structural Correspondence Learn-ing [Blitzer et al. 2006] and Alternating Structural Optimization [Ando and Zhang 2005a].
 Future work will concentrate on several open problems: while we introduced CL-SCL in the context of cross-language text classification, the method can be applied to any feature-based classifier if an appropriate feature translation oracle can be speci-fied. As important target tasks we will investigate named entity recognition and rela-tion extraction. These tasks require much richer feature representations than simple bag-of-words models. However, they also rely on word presence features which can be used to implement a simple word translation oracle analogous to the approach pre-sented here. Furthermore, our approach makes no assumption about how the trans-lation oracle is implemented: it could be a domain expert but also a heuristic based on lexical or phonetic similarity. Heuristics of this kind appear promising for techni-cal domains such as patent databases or biomedical databases where similar technical expressions are used in both languages. This aspect opens a range of opportunities to further reduce the resource requirements of CL-SCL.

