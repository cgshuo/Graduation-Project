 A large amount of research has focused on faster methods for finding top-k results in large document collections, one of the main scalability challenges for web search engines. In this paper, we propose a method for accelerating such top-k queries that builds on and generalizes methods recently pro-posed by several groups of researchers based on Block-Max Indexes [15, 10, 13]. In particular, we describe a system that uses a new filtering mechanism, based on a combina-tion of block maxima and bitmaps, that radically reduces the number of documents that have to be further evalu-ated. Our filtering mechanism exploits the SIMD processing capabilities of current microprocessors, and it is optimized through caching policies that select and store suitable filter structures based on properties of the query load. Our ex-perimental evaluation shows that the mechanism results in very significant speed-ups for disjunctive top-k queries un-der several state-of-the-art algorithms, including a speed-up of more than a factor of 2 over the fastest previously known methods.
 H.3.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval Algorithms, Performance, Experimentation top-k query processing, early termination, block-max in-verted index, docID-oriented block-max index, candidate fil-tering mechanism, posting bitset, live area computation
One of the major problems for large search engines is to keep up with the tremendous growth in the size of the web and the number of queries submitted by users. As discussed in [12], each of these measures increased by several orders of magnitude over the course of a decade. This creates per-formance challenges in many parts of the search engine ar-chitecture, including data acquisition, data analysis, and in-dex maintenance. However, these challenges are particularly acute in the query processing component, whose workload grows with both data size and query load. In fact, one might naively expect query processing costs to increase with the product of these measures, though in practice various tech-niques are used to limit this increase. In this paper, we focus on such techniques for improving query processing efficiency.
Query processing in search engines is a fairly complex process, typically involving hundreds of features used for ranking, multiple phases that identify promising documents based on subsets of the features, and a distributed archi-tecture that routes queries and results within and between clusters of thousands of machines. However, most systems appear to process a query by first evaluating on each ma-chine a fairly simple ranking function over an inverted index. This results in an initial set of say a few thousand results that is then further processed to identify the best ten or more results to return to the user [26]. We focus on im-proving this initial step, which is responsible for a significant fraction of the overall work. We assume that this initial step is a ranking function of the form r ( q,D ) = P t  X  q s ( t,D ), i.e., it is the sum of the scores of all the terms occurring in the query. This is a fairly common assumption, and accommo-dates popular functions such as BM25 as well as approaches based on automatically learning term scores that approxi-mate more complex ranking functions [1].

A straightforward way to execute such a simple disjunctive ranking function is to completely traverse the index struc-tures of all the query terms and compute or retrieve the scores of all the postings. Under this exhaustive method, the cost of each query increases linearly with the number of documents, making it very expensive for large collections. To overcome this problem, many researchers have proposed so-called early-termination techniques, i.e., techniques for finding the top results without computing or retrieving all posting scores [25, 9, 6, 23, 15, 10]. We focus on safe early-termination techniques, which must return the same top re-sults as the exhaustive method [22].

Content of this Paper: Recent work by several groups of researchers [10, 15, 19, 13] has shown significant improve-ments in performance based on a Block-Max Index , first pro-posed in [10, 15]. This is an index structure that stores maximum scores for groups of postings, thus allowing quick skipping, without full evaluation, of sets of postings with low scores. In this paper, we build on this approach to derive new algorithms with additional very significant performance gains. In particular, our contributions are as follows: (1) We design a general mechanism that can improve the (2) We present a space efficient caching policy for storing (3) We propose an optimized implementation of the mech-(4) We provide an extensive experimental evaluation that The remainder of this paper is organized as follows. In Section 2 we outline the background and related work. Sec-tion 3 describes the proposed filtering mechanism that uses Block-Max Indexes. Next, in Section 4 we present how to augment the mechanism with posting information. Section 5 explains our proposed solutions for the space overhead of the auxiliary structures and Section 6 presents preliminary results on our proposed methods. In Section 7 we show the performance of the filtering mechanisms on several scenar-ios, whereas in Section 8 we discuss caching policies for our mechanism. Finally, Section 9 presents additional results and Section 10 concludes and discusses future work.
In this section, we provide some background on inverted indexes, query processing and early termination techniques, Block-Max Indexes, and describe previous work.
Inverted Indexes: An inverted index [27, 30] is a simple and efficient data structure widely used by large search en-gines for query processing. This structure allows finding the documents where specific terms occur and can be formally defined as follows. Given a collection of N documents, we assume each document is identified by a unique document ID (docID) between 0 and N -1. An inverted index contains an inverted list L w for each distinct term w in the collec-tion. Each L w is a list of postings describing all documents where term w appears in the collection. More specifically, each posting contains the ID of the document (docID) that contains the term w , the number of occurrences of w in the document (the frequency), and potentially additional information such as the exact positions of these occurrences of term w in the document (positions) or other data. Un-less stated otherwise, we store docIDs and frequencies such that each posting is of the form ( d i ,f i ). There are several ways to layout the inverted lists, but in this work we focus on document-sorted indexes, where postings in each list are sorted by their docIDs d i ; see [18, 16, 6, 8, 23] for work using other layouts.

Index Compression: The inverted lists of frequent terms may contain millions or even billions of postings. To de-crease space requirements, inverted lists are usually kept in compressed form. A common approach for docID-sorted in-dexes is to store the differences between consecutive docIDs in a list in suitably compressed form [30]. Index compression is crucial for search engine performance, and many compres-sion methods have been proposed; see [29, 28, 21] for some recent work. For fast access, inverted lists are usually com-pressed in blocks of, say, 64 or 128 postings, such that each block can be accessed and decompressed individually. This is done by storing the uncompressed last docIDs and sizes of all blocks in a separate array that can then be used to lo-cate and decompress individual blocks. Here, we compress the inverted lists into blocks of 64 docIDs followed by the corresponding 64 frequencies.

Index Quantization: We usually store each posting as a pair ( d i ,f i ). However, it may sometimes be preferable to store postings as ( d i ,s i ) where s i is a score of d respect to w . There are two main motivations for this: (1) to save the cost of computing scores from frequency values and other statistics such as document size and global term frequency, and (2) for scores that cannot be easily computed on the fly as they are derived from complex ranking functions with hundreds of features, using techniques similar to those in [1]. However, storing each s i as a 32-or 64-bit number would result in very large index sizes. To reduce the size, so-called index quantization techniques are used that basically round the floating point score values to one out of a fixed set of, say, 256 distinct values. This may lead to a slight loss in precision during ranking, but guarantees that each score can be stored in only 8 bits. There are several quantization techniques in the literature; see [2, 3, 5, 4]. In this paper, we use the Global-by-Value technique proposed in [5], and also used in [11], using 8 bits for each score. All necessary score accumulations can be done directly in integer space, as in [2]. We refer to such an index structure as a quantized index , in contrast to a standard index storing frequencies values.
Index access: Document-sorted indexes allow fast index traversal during query execution based on a Document-At-A-Time (DAAT) approach. In DAAT index traversal, every list has a pointer to a current posting, and all pointers move forward in a synchronized manner as a query is processed. Thus, all postings (and thus documents) to the left of the pointers have already been processed, while postings to the right are yet to be considered. In DAAT, we typical have functions for opening and closing inverted lists for reading, and a function nextGEQ that, given a docID d and an open inverted list L , moves the pointer in L forward to the first posting with docID greater or equal to d . In addition, there is a function for retrieving other data (such as frequency or score) associated with the current posting. All decompres-sion operations in the inverted lists are hidden within these functions. In this work, we focus on algorithms that perform DAAT traversal on document-sorted indexes.
The simplest way to query an inverted index is to ask for all documents containing some (disjunctive) or all (conjunc-tive) of a set of query terms. Of course, this would return too many results on large document collections, and thus IR systems supporting ranked queries return the highest scor-ing documents among those that satisfy the condition. Many simple ranking functions have been proposed for this task, including BM25 and the Cosine measure [7]. These functions are typically of the form r ( q,D ) = P t  X  q s ( t,D ), meaning that the score of a document with respect to a query is the sum (or some other simple combination) of the query term scores of the document. In this paper, as in almost all pre-vious work, we assume disjunctive queries with a ranking function of this form, that is, our goal is to return the high-est scoring among all documents containing at least one of the query terms.

We note that such disjunctive queries tend to be more expensive than conjunctive queries that only need to score documents containing all query terms. For this reason, many search systems try to use conjunctive queries whenever pos-sible. However disjunctive queries are known to return bet-ter results in many situations, and thus it is important for search engines to also support these more expensive types of queries.

Cascade Ranking: The above simple ranking on top of a disjunctive or conjunctive filter is only the first phase of query processing in current search engines. These engines will typically take a limited number k of top-scoring results from the simple ranking, say a few thousand overall, or as little as tens per node on a large cluster of machines, and evaluate a second, more complicated but more precise, rank-ing function on these results. This process may be repeated with an even more evolved ranking function applied to the top results from this ranking. Thus, we expect our top-k algorithms to be the first phase of such an architecture, and will look at the effect of varying k from 10 to a few thousand.
Early Termination: The main performance bottleneck that we have to deal with is the length of the inverted lists, which can grow to many MBs or even GBs on large col-lections. As mentioned, a simple algorithm for this prob-lem would compute the score of any document containing at least one of the query terms; we call such an algorithm ex-haustive. To avoid scoring many of these documents, early-termination (ET) algorithms have been proposed. We say that an ET algorithm is safe if it outputs exactly the same top k results as an exhaustive algorithm [22]. (In the case of a quantized index, safeness is of course defined with respect to an exhaustive algorithm on the full quantized index.)
There are many safe and unsafe ET algorithms in the lit-erature; see, e.g., [25, 9, 6, 23, 15, 10] for a small sample. Some algorithms keep inverted lists sorted by docID, while others reorganize the lists by score s ( q,D ), so that all high-scoring postings are encountered early during query process-ing. In this paper, we focus on safe early termination. All our algorithms keep the inverted lists sorted by docID and use DAAT for index traversal  X  but our running times also outperform all safe methods with other index layouts.
In this part, we first describe a simple exhaustive algo-rithm and then outline two basic safe ET algorithms that use DAAT traversal on document-sorted indexes, the WAND al-gorithm in [9] and the Maxscore algorithm in [25]. Both al-gorithms store for each inverted list the highest impact score of any posting, called the maxscore of the list. Recall that in DAAT traversal, we maintain a pointer to the current posting of each list. Also, we use  X  to refer to the current threshold, which is the smallest score that can still make it into the top k results during the execution of the algorithm.
Exhaustive: The simplest query processing algorithm first picks the smallest current docID across all query terms, called cID . This can be done using either a loop or a heap, and cID is then fully evaluated. Afterwards, all pointers are moved to at least the next posting after docID cID. We also maintain a heap of the current top-k results, and add cID to this heap if it has a score larger than the current k th highest score.

WAND: This algorithm, proposed in [9], consists of three phases: pivot selection, alignment check, and evaluation. In every iteration, a pivot term is selected by summing up the maxscores of the participating lists in order of increasing current docID, until the sum becomes larger or equal to  X  . Next, WAND tries to align the current docIDs of the pre-ceding lists in the ordering with the pivot docID, by moving the pointers in these lists forward to the pivot docID. If this succeeds, and all pointers up to the pivot point to the same docID, then this docID is evaluated, if necessary inserted into the top-k heap, and the current pointer in the pivot list is moved forward. Then we go to the next iteration.
Maxscore: This algorithm was first described in [25]. We focus here on the DAAT versions of the algorithm in [24, 17, 13]. Maxscore splits the query terms into essential and non-essential terms as follows. We sum up the maxscores of the lists from lowest to highest while the sum remains less than  X  . The terms that contribute to this sum form the non-essential terms, and the others are the essential terms. The basic idea is that any document that can make it into the top k results must contain at least one essential term. Now we select as candidate, the smallest current docID of any essential list, and evaluate it. Thus, we basically run the exhaustive algorithm on the essential lists only. Of course, as the threshold  X  for making it into the top k increases during execution of the algorithm, we may move additional terms from the essential to the non-essential set.
The idea of storing and exploiting the maximum impact score of each inverted list was recently and independently extended by two research groups [10, 15], who proposed an augmented index structure called Block-Max Index (BM). This structure stores the highest impact score for each block in the inverted lists, where the blocks are defined by the index compression method. Recently in [13], the blocks of the Block-Max structures were decoupled from the index compression and defined on docID space rather than posting distribution. We will refer to Block-Max structures whose blocks are defined by postings as posting-oriented , while the ones defined on docID space as docID-oriented .
In this paper, we build on the work of [15, 10, 13], where we generalize and extend these ideas by proposing a filtering mechanism that uses Block-Max Indexes to further improve the query processing for disjunctive queries.
 More specifically, [10, 15] proposed the posting-oriented BM structure to approximate the score of a block in posting space. The Block-Max Indexes provide score approxima-tions in posting block resolution that enables skipping of compressed blocks. The approach in [15] proposed an en-hancement of the WAND [9] algorithm that uses the Block-Max index, called BMW , and operates as follows. After the sorting and pivot selection step as in [9], there is an addi-tional block-max check that tests whether the pivot docID can make it into the top-k results. In case the block-max test fails, BMW can safely skip until the end of the blocks. The work in [10] suggested an algorithm that is built on top of the Maxscore and also uses posting-oriented BM structures. In particular, given a query, the algorithm reads the stored BM information, generates aligned intervals in the docID domain, and computes upper bound scores for each interval. During query processing, each interval X  X  upper bound score is checked whether is larger or equal to the k th highest scor-ing document seen so far and if the test fails, the algorithm can skip blocks and safely save computations.

Both previous studies show experimentally that these aux-iliary indexes provide fast query response times with small space requirements. However, the skipping power offered by posting-oriented BM indexes is limited because the blocks are defined by the index compression method. More re-cently, a new layout of the BM was proposed [13], where the blocks are defined on the docID domain. The blocks of the BM structure are decoupled from the index compression and their block size can be assigned per-term using the fixed, the expected or the variable block size selection method. The docID-oriented BM indexes provide very fast lookups, be-cause the block-max access can be performed with bit shift operations. The authors in [13] use the variable size selec-tion scheme to deal with the space overhead of the structure and show that several algorithms perform faster using the proposed BM index. We refer to [15, 10, 13] for more de-tails on Block-Max indexes and query processing algorithms using such structures.

In this work, we extend the idea of constructing aligned in-tervals in docID space and computing interval upper bounds as in [10] by generalizing it on the docID-oriented BM struc-ture [13] with fixed block size. The result is aligned BM blocks for all lists, allowing fast block-maxscore accesses and aggregation of  X  X ligned block X  upper bounds. We build on this scheme and propose a filtering mechanism that utilizes docID-oriented BM indexes to discover the  X  X ive X  blocks in consecutive windows of docID space, which leads to signifi-cant performance gains over a series of DAAT algorithms for disjunctive queries. For the remainder of the paper we as-sume docID-oriented BM using the fixed block size selection method.
In this section we describe our proposed candidate filtering method that exploits the Block-Max index.

Live Area Computation: We can model DAAT query processing algorithms as a sequence of rounds, where in each one we examine whether a specific candidate docID can make it into the top-k results (by evaluating it or safely early terminating). Since our goal is to retrieve the top-k results fast, we can reduce the cost of query processing by minimizing the number of candidate docIDs. In previous works [10, 15, 13], the BM structure is utilized for candidate filtering in an online fashion, meaning that in each round there is a block-maxscore test checking whether the candi-date has a chance to be added in the result heap. However, in our approach the aggregation of the upper bound block-maxscores across query terms for consecutive aligned blocks is performed in batch mode and then exploited to aggres-sively skip blocks. Due to the docID-oriented BM layout, there is no need for the alignment step as needed in [10].
In typical DAAT query processing algorithms, at the end of each round, the candidate docID for the next round is se-lected. The selection of this document is always preceded by invocation of the nextGEQ() on some lists. Since nextGEQ() usually demands expensive block decompression, we would prefer to make this decision based on the available block-max information. Therefore, given a query with m terms and a threshold  X  , we define a block as live if the sum of block-max scores across m terms is greater or equal to  X  . Thus, we can safely skip the block and avoid redundant de-compressions and score calculations when a block is  X  X ead X . Remark that in [13] a similar optimization was suggested by the BMM-NLB algorithm, but in their setting there is no enforced block-max alignment.

We extend this idea by computing the live blocks of a window of docID space, where window refers to a contigu-ous docID region. Thus, we calculate block-maxscore sums for all blocks of a window. By definition, only blocks that summed up to more or equal than  X  (of the specific window) are live blocks. We will refer to this procedure as Live Area computation (LA) of a window. This per-window liveness information is stored in a structure called Liveness Bitset (LB) , where each bit corresponds to a block of the window. LB is used for candidate filtering by skipping  X  X ead X  blocks. In particular, our mechanism first obtains the next live block from the LB and then provides the first docID of this block to the nextGEQ(). The live area computation is performed prior to query processing to provide efficient filtering oracle for candidate selection. Note that at the end of query pro-cessing of each window, we re-use the memory occupied by the LB structure to avoid the costly memory initializations.
Windows: The window size under this setting is crucial since the number of live blocks depends on the most updated  X  . When the size of the window is too small, we pay for the construction overhead (LA). On the other hand, if the win-dow size is too large, the effectiveness of the LB would drop due to outdated thresholds. In practice we observed that the appropriate window size should be a function of L1 or L2 cache size. As you advance in docID space (and  X  grows), LB becomes more sparse, and hence, more effective. In pre-liminary experiments we observed that the convergence rate of  X  is quite fast.

SIMD: The computation of live areas is performed during query processing (online) per window, thus it must be fast. This process includes the summation of the aligned block-maxscores across terms, the comparison of this sum with  X  , and the encoding of the liveness information in LB. Since all inverted lists block-maxes are aligned in docID space, the LA calculation is vectorizable and SIMD capabilities of mod-ern CPUs can be exploited to accelerate this computation. Our calculation of LA is coded for the SSE instruction set and performs the following operations: (i) load 4 consecutive block-maxscores from each query term, (ii) sum the block-maxscores vectors, (iii) compare the results with  X  , and (iv) set the corresponding bits in LB. In a nutshell, LA X  X  calcula-tion overhead is negligible because of the natural speed-up of vectorized operations (SSE) and the limitation of LB to L1 cache. Note that SSE also applies to the integer operations used in the quantized index scenario.

In Figure 1, we see an example of LA computation for the 2-term query  X  X at squirrel X  of 4 consecutive block-maxscores. In this example, the BM scores for both terms are shown in grey, whereas their sum is colored green if the block is live, otherwise it is red. Under this scenario, only the third bit in LB is set. Since we are using SSE for the computation, every step is performed in one instruction. Thus, for any 2-term query we only need 5 instructions instead of 20 to compute the liveness of 4 consecutive blocks. The obtained informa-tion can filter all blocks in the window that could not make it into the top-k results and therefore, save decompressions and evaluations. Figure 1: LA computation using terms X  BM structures.
Varying Block Size and Threshold: The window-based LA calculation is computed using the most updated  X  . The early knowledge of a good threshold makes the fil-tering mechanism very effective since less blocks will be live. In Figure 2, we show the percentage of live blocks for var-ious block sizes and for different fixed thresholds (  X  is not updated). We observe that the number of live blocks de-creases when (i) decreasing the size of the block and (ii) providing higher  X  . Since larger blocks provide worse block-score approximations, with higher threshold they have bet-ter chances of being live. These observations provide a strong evidence about the effectiveness of the proposed filter-ing mechanism. In practice, when  X  is updated per window, the decrease in liveness is more significant (not linear). Figure 2: Live block percentage varying block size and
Filtering Mechanism Interface: Our proposed window-based candidate filtering mechanism provides a simple inter-face that can be easily used by any DAAT query processing algorithm. We now describe the various low-level compo-nents of our framework so its adaptation to existing or new algorithms becomes trivial.

In this part, we outline the computation of LB structure and how to utilize it during query processing. First, the LA computation is performed whenever the termination condi-tion of DAAT algorithms occurs. More specifically, these algorithms terminate when the docID of the candidate doc-ument is greater than the largest possible docID in the col-lection. At this point, the algorithms using our mechanism should be adapted to operate in the boundaries of the cur-rent window. Thus, whenever the docID of the candidate document exceeds the current window boundaries, the LA computation should be performed with the current  X  . This consists of block-maxscores summation, comparison to  X  for all blocks of the window, and setting the appropriate bits in the LB. Note that if the BM scores are not available, our mechanism needs direct access to the index to com-pute them. The liveness information of the LB can be ac-cessed by a core function called nextPotentialLiveDid , which given a docID returns the first docID of the current or the next live block. The returned docID and a specific list are then provided to the nextGEQ(). We hide the complex-ity of this procedure by introducing a new function called newNextGEQ , which encapsulates both nextLivePotential-Did() and nextGEQ(). Note that the original nextGEQ() performed on the  X  X ive X  candidate can still return a docID in a  X  X ead X  block. Thus, instead of calling the nextGEQ() the algorithms using our filtering mechanism could just use the new function newNextGEQ.

To sum up, algorithms could use our filtering mechanism by first changing the halting condition to be windows aware, then performing the described LA computation whenever the candidate docID is out of the current window bound-ary, and finally, by calling the newNextGEQ() instead of nextGEQ(). Hence, our mechanism offers a simple interface to query processing algorithms, by hiding all the complexity behind few functions.

Architecture of the Filtering Mechanism: We now describe how our technique communicates with the available data during query processing. Figure 3 shows our query processing pipeline, which consists of the Index, the Query Processor, the Filter Mechanism and the Filter Cache. The Filter Mechanism has direct bulk access to the Index so that it can generate the augmented structures on-the-fly for lists out of the Filter Cache, as we will describe in Section 5. Hence, the Filter Cache contains augmented structures (such as the BM Index and the LB) that are accessed and ex-ploited by the Filter Mechanism for effective skipping during query processing. The Query Processor uses the core func-tion nextGEQ() to access the Index and move the pointers of the inverted lists. It may also use the Filter Mechanism to request the liveness bit of a docID or the maxscore of a block. In that case, the information is served by the Fil-ter Cache and returned to the Query Processor. Remark that the communication between the Query Processor and the Filter Mechanism can in fact be encapsulated within the newNextGEQ() operator.

The LB provides a smart exploitation of the BM struc-ture with performance gains as we will experimentally show in the next section. Its effectiveness to filter candidate do-cIDs relies solely on the approximation quality of the block-maxscores. The early termination methods of algorithms using BM structures are mainly based on the score approx-imation. However, we could also filter candidate docIDs us-ing additional information about the postings existence in a block. For example, a block without postings does not con-tribute to the block-maxscore aggregation in LA. In fact, a block-maxscore is non-zero if and only if the block contains postings. In order to exploit the additional posting exis-tence information of a term we encode it as follows: for a contiguous docID region called sub-block , we store a bit de-scribing whether the term contains any posting in it. Thus, for each term we store a Posting Bitset (PB), where each bit represents the posting liveness of a specific sub-block. This encoding offers extremely fast accesses to the PB structure (because sub-blocks are docID-oriented).

The PB can be seen as a per-term filtering method that query processing algorithms can benefit from. In particular, this filtering mechanism consists of a basic function called nextLivePotentialPosting that given a docID and a list, re-turns a docID belonging to the current or next populated sub-block of the given list. Moreover, this function can be used to answer whether a term has any postings in a specific region, thus saving redundant computations such as decom-pressions of blocks and score calculations. Similar to the LB mechanism, the PB filtering technique also requires direct access to the index for construction. The PB structure can be easily combined with the BM index during LA compu-tation to provide finer granularity candidate filtering as we will shortly see.
In the previous section, we proposed the PB structure that provides information about the posting liveness. This structure can be utilized during the LA computation to pro-vide finer granularity filtering. In order to obtain a useful combination of the BM index and the PB, we need to mean-ingfully select the block size of the sub-blocks. Note that having a sub-block larger than the BM block does not in-troduce new information. We experimentally observed that a good size for the sub-blocks is 2 m  X  3 , where 2 m is the size of the BM block. For the rest of the paper, we assume that the sub-block size is 2 m  X  3 .

Our assumption about the sub-block size implies that ev-ery BM block is split into 8 sub-blocks. Therefore, for each BM block we have additional information about the post-ing liveness of its sub-blocks. Utilizing the PB structure in LA computation requires the following steps: first ev-ery block-maxscore is duplicated 8 times and masked with 8 sub-block bits, then the sub-block level scores are summed across terms, and finally, we compare the sums with the current most updated  X  and set the appropriate bit in the liveness bitset. Note that in our case, the temporary PB structure is 8 times larger than the corresponding structure we used for LB. We will refer to this structure that provides an effective candidate filtering mechanism for the query pro-cessing algorithms, as LB-PB . Similarly to LB, during query processing the algorithm can provide a docID to the LB-PB to obtain the next live sub-block in the window, avoiding redundant computations such as decompressions and score calculations. Given a docID, the mechanism returns a do-cID in the current or the next live sub-block. Remark that this filtering technique can exploit both nextLivePotential-Did and nextLivePotentialPosting functions, that now oper-ate on sub-block level, and its complexity can be hidden as well behind the newNextGEQ().

In Figure 4, we describe the live area computation of the query  X  X at squirrel X  for 4 BM blocks using the LB-PB mech-anism. The BM scores of both terms are colored in grey, and only the PB of the third block is shown. The non-empty sub-blocks are shaded. The available PB information enables finer block-maxscore granularity on sub-block level. Focusing on the third block, the LA-PB computation in-cludes the multiplication of the posting bitset of the third block of each term with the corresponding BM score. The result is depicted as grey sub-blocks, whereas the summa-tion of the aligned sub-blocks across terms is shown as red or green sub-blocks sums. We can alternatively think this pro-cedure as an alignment step in sub-block resolution. After the LB-PB filtering, only the sixth sub-block of third block was found to be live, while the original LB mechanism would label the entire third block (8 sub-blocks) as live. This ex-ample shows the effectiveness of the LB-PB mechanism and its superiority over the simple LB filtering technique. Figure 4: Example of the LB-PB filtering mechanism.
SIMD: Similarly with the LA computation, the calcu-lations of LA when combined with the PB structure can exploit the SIMD opportunities of modern CPUs. All steps of the LA computation of the LB-PB mechanism are vector-izable and therefore, the SSE instruction set is used in the implementation to speed up the calculations.

Varying Block Size and Threshold: Figure 5 presents the percentages of the live sub-blocks for various block sizes and different fixed thresholds. As in Figure X  X  2 observations, smaller block sizes and higher starting threshold result in less live sub-blocks, as expected. Comparing the percentages of Figure 2 and 5, we observe that the combination of LB and PB leads to less live blocks. These statistics provide evidence that the proposed candidate filtering mechanisms could give significant performance gains.
In this section we discuss how to deal with the space over-head of the auxiliary structures. So far we have not mentioned the space overhead of the BM index needed during the LA computation and we as-sumed BM resides in memory. In practice, the BM structure Figure 5: Live sub-block percentage varying block size with fixed block size is too large to be memory-resident, so we address the space overhead using the approach in [13], and we will refer to this as Length-based . Under this method, the BM for terms with few postings are not stored at all, but generated on-the-fly (OTF), while the BM scores for the rest are undergoing linear quantization (BMQ), such that every block-maxscore is 1-byte (instead of 4). The difference in our setting is that the OTF is now performed per window. The window-based OTF computation involves (i) the decompres-sion of the blocks (docIDs and frequencies), (ii) the score calculation of these docIDs, and (iii) the Block-Max Gener-ation (BMG). We further extend the length-based approach by compressing the quantized Block-Max values (BMQC) of the medium-sized lists. More specifically, we first create a skiplist for non-zero values and then we compress them us-ing a version of PForDelta [29]. This requires decompression of the compressed quantized values during query processing, but its overhead is minimal compared to its space gains.
The size of the sub-block plays a crucial role on the ef-fectiveness of the posting bitset structure. Very large sub-blocks provide poor posting existence information, whereas too small sub-blocks add significant space overhead. Overall, we address the space overhead of the posting bitset structure by following a length-based approach as in the case of the BM structure. A first observation is that for long lists we ex-pect most sub-blocks to be non-empty, hence a PB with all bits set is a decent approximation for it. Instead of storing the actual PBs for such lists, we  X  X ake X  them by pointing to a static array of 1 X  X , thus saving this space. We experimentally observed that this approximation of PB does not impair fil-tering abilities and does not cause a slow down. Moreover, since we already decompress the short lists in OTF BMG, we can use the docIDs to construct the PBs on-the-fly as well. The PB structures are stored in main-memory only for the medium-sized terms. Note that the construction of PB should be also considered under the window-based sce-nario as described in the previous section.

We will revisit the length-based approach for both struc-tures in Section 8 to achieve a better space/time tradeoff.
In this section, we evaluate and compare the performance of the simplest query processing algorithm, the exhaustive, when our filtering mechanisms are applied, with the best previously reported numbers to our knowledge [13] under the scenario of a standard index.
For our experiments, we use the TREC GOV2 dataset that contains 25 . 2 million web pages. We build inverted in-dex structures with 64 docIDs and frequencies per block, using a version of the PForDelta compression method [31] described in [29], but our ideas also apply to other compres-sion methods. The size of the uncompressed size of the data set is 426GB, whereas its compressed standard index size is 8 . 75GB and its compressed quantized index 11 . 97GB. We evaluate our methods using 1000 queries randomly selected from the TREC 2006 and 2005 Efficiency track query set.
We implemented all the algorithms in C++ using BM25 as our ranking function, and return top-10 results unless stated otherwise. For the rest of the paper, we assume docID-oriented BM using the fixed block size selection method with 64 docIDs per block. The experiments were conducted on a single core of an Intel Xeon server with 2.27Ghz and all data structures reside in memory.
Setup: We begin with a performance comparison of the exhaustive algorithm and its versions that use the LB and the LB-PB mechanisms, called EX , EX-LB and EX-LB-PB respectively, on the standard index. For this experiment we use the following length-based policies for the BM: (i) OTF for lists with length less than 95042, (ii) BMQC for lists between 95042 and 2 18 , and (iii) BMQ for the remaining ones. Similarly, for the PB structure we use the subsequent length-based rules: (i) OTF for lists with length less than 179758, (ii) maintaining in main memory the posting live-ness information for lists with length between 179758 and 2 22 , and (iii) faking of bitsets for lists larger than 2 22 space requirement for the described policy for the GOV2 data set is 2 GBs , less than 25% of the compressed index, while the use of PB increases the space to 4 GBs . We also compare the performance of the exhaustive algorithm when the filtering methods are applied with the fastest previous re-sults to our knowledge [13]. More specifically, the approach in [13] uses docID-oriented BM structures, variable block size selection methods and both OTF BMG (up to lists of size 2 15 ) and BMQ techniques (on the remaining lists), with 2 . 76 GBs space overhead. The authors of [13] were willing to share the code for their fastest algorithm, called BM-OPT, which selects based on the number of query terms the fastest algorithm between BMM, BMM-NLB and BMW.

Exhaustive speed-up: In Table 1, we present the query response times for the BM-OPT algorithm and all versions of the exhaustive algorithm for the TREC06 query trace, when the number of query terms varies. We can see that the LB filtering mechanism makes the exhaustive algorithm faster by a factor of 10, whereas the availability of PB boosts the exhaustive 16 times! The exhaustive algorithm is signif-icantly accelerated by the availability of the LB mechanism achieving average query response time of 13 . 47 ms . When the LB-PB is used it further reduces the average time to 8 . 59 ms , 16 times faster over EX . The performance boost of the mechanisms is consistent across varying number of query terms. Table 2 shows the radical decrease in term score evaluations and calls to nextGEQ() when the proposed mechanisms are applied. The reason for the significant re-duction in query response times, evaluations and nextGEQ() is the following: at the end of each iteration of any query processing algorithm, the pointers of some or all lists must be moved using the expensive nextGEQ() (which usually in-cludes decompression). Our mechanism checks the LB (or the LB-PB) structure in order to obtain the next live block (or sub-block) and then provides to the nextGEQ() the first docID of this block (or sub-block). Therefore, this test be-fore the call of nextGEQ() moves the pointers much further in docID space and results in much less redundant expensive decompressions, which is the main reason for the significant performance speed-up of our mechanisms.

Comparison with BM-OPT: Furthermore, we see that the performance of both EX-LB and EX-LB-PB outper-forms that of BM-OPT. Concerning the space requirements of the algorithms, as mentioned, the EX-LB occupies 2 GBs , less than the 2 . 76 GBs needed by the best previous approach. Thus, our filtering method uses less space to achieve faster times compared to the state-of-the-art algorithm, BM-OPT. When the PB structure is also available the space increases to 4 GBs and EX-LB-PB achieves much faster query re-sponse times compared to BM-OPT. Remark that our filter-ing mechanisms accelerate significantly the simplest query processing algorithm, the exhaustive, which outperforms the state-of-the-art BM-OPT algorithm.
 Table 1: Performance of the exhaustive algorithm on Table 2: Number of term score evaluations and calls to
In this section, we present a performance comparison of widely used DAAT query processing algorithms when our filtering techniques are applied. These algorithms either use (BMM-NLB, BMW) or not (WAND, Maxscore) the BM structure for early termination during query processing.
In particular, the WAND algorithm was implemented based on [9], Maxscore and BMM-NLB were coded according to [13], while the BMW algorithm based on [15]. All algorithms were modified in order to use the filtering mechanisms in-terface as previously described.
 The performance comparison of these algorithms for the TREC06 and TREC05 query trace when we vary the number of query terms is presented in Table 3 and 4. The significant speed-up of our candidate filtering mechanism over several query processing algorithms shows the effectiveness of the filtering. Table 3 shows that Maxscore outperforms WAND when any filtering method is applied. On algorithms that use the BM index to early terminate documents, BMW is consistently faster from BMM-NLB. We observe that the performance of Maxscore is similar to BMM-NLB and the reason is that BMM-NLB spends too much time during its several cascading filtering steps, which are not very useful when our mechanisms are used. BMW outperforms all al-gorithms with 6 . 45 ms using the LB and 5 . 74 ms when PB is also available. A similar performance behavior is observed for the TREC05 query trace in Table 4. Enforcing candi-date selection with LB (and LB-PB) in all algorithms con-sistently accelerates them. However, this was apparent from preliminary experiments, where the simplest algorithm out-performed the state-of-the-art BM-OPT. Due to space lim-itation, for the remainder of the paper we omit any results for the TREC05.
 Table 3: Performance comparison of several DAAT Table 4: Performance comparison of several DAAT
In this section, we discuss how we can further optimize both space and time constraints by moving from simple length-based caching policies to cost-based.
In the previous experiments, the performance of our meth-ods and their space requirements were based on decisions governed by the length of the lists, called Length-based poli-cies. These policies include (i) OTF computation of BM and PB for short lists, (ii) storing compressed quantized BM for medium lists (BMQC), (iii) faking of PB for long lists, and (iv) storing quantized BM and PB for all remaining lists.
For the previous experiments, the length-based parame-ters were fixed to specific values using acceptable space re-quirements to show the effectiveness of our mechanism. Al-though the length-based caching provides a good space/time tradeoff, it is solely depends on the length of the list. More specifically, it forces short lists X  augmented structures (BM and PB) to be computed OTF, whereas it always stores in the Filter Cache these structures for large-sized lists. This is not always optimal, since it is likely that some short terms will occur frequently in the query trace and thus, it may be more beneficial to store their augmented structures rather than computing them OTF. Therefore, we would like to move to policies that take into account the frequency of terms in the query workload and offer better space/time tradeoffs.
It is common that large web search engines use caching mechanisms so their performance adapts and optimizes to the query workload. Such mechanisms maintain statistics about the terms appearances in the query logs and based on them, decide whether to cache the additional information. The main idea behind such approach is to achieve perfor-mance speed-up by using less space smartly. Therefore, we try to maximize the obtained performance gains by following optimal policies for each term according to query statistics and under specific space requirements. We will refer to the proposed policies that are based on a benefit function that takes into account the frequency of the terms in the training query trace, as Cost-based policies. In particular, if a short term occurs very frequently in the query log, it would be reasonable to maintain its augmented structures in memory. On the other hand, if a long term does not appear too often in the queries, we refrain from storing it in memory.
The proposed cost-based caching policy follows the next steps: first, for each term we compute a rank under various policies for the BM index include the length-based policies (i) OTF, (ii) BMQC and (iii) BMQ. Similarly, the policies for the PB structure consist of (i) OTF, (ii) PB, and (iii) faking of PB. Then, given a space budget, we select the terms (from highest benefit rank to lowest) and store their structures in the Filter Cache. This selection procedure ends when the space requirements are met and thus, for all terms that were not selected by the caching policy, we use OTF (regardless of their length). We obtained the frequency of terms in the query log using as our training set the 99 k queries from the TREC 2006 Efficiency track query trace and then, applied our method to the remaining 1 k queries.
In Figure 6, we compare the space/time tradeoff of both caching methods on WAND, BMM-NLB and BMW, when only the LB filtering mechanism is available for the stan-dard index. As expected, given a specific space budget, the cost-based policy always achieves faster times for all the al-gorithms compared to the length-based approach. Our pro-posed method is very effective even in scenarios with very limited space constraints. In particular, for space budget of 0 . 5 GBs , BMW achieves 15 . 75 ms using the length-based pol-icy, whereas 10 . 22 ms using the cost-based. For the remain-der of the paper we will use the cost-based caching approach. Figure 7 shows that when more space is available, using the PB structure accelerates the query processing time. More-over, we observe that the quantized index can further reduce the query response times, for example Q-BMW-LB-PB for 2 GBs achieves 4 . 36 ms .
In this section, we provide some additional results. More specifically, we look the performance of our methods for re-odered indexes and the impact of increasing top-k on the performance of our techniques.
 Figure 6: Space/Time tradeoff of Length-based and Figure 7: Space/Time tradeoff of Cost-based caching on
DocID Reordering: There has been a huge amount of work to reduce the size of the compressed inverted indexes by reordering the docIDs. Under docID reordering, the doc-uments are assigned docIDs using different techniques in or-der to minimize the index size. The main idea is to assign closeby docIDs to documents sharing lots of terms. A sim-ple and effective method proposed by [20], which first sorts the URLs alphabetically and then assigns docIDs based on this ordering. More sophisticated reordering techniques were proposed in [14], whereas [28] showed that reordering speeds up the conjunctive query processing. Recent work in [15, 13] showed that docID reordering also accelerates the query re-sponse times of disjunctive queries. In Table 5 we show the performance of BMW using the cost-based caching policy under the simple reordering method of [20] when varying the number of query terms. For the remainder of the pa-per, the space requirement of the algorithms using the LB structure is 2 GBs , and 4 GBs when the PB is also used. As expected reordering gives a speed-up for both filtering mechanisms. In particular, BMW-LB achieves 3 . 02 ms av-erage times, whereas BMW-LB-PB performs similarly with 3 . 04 ms . The reason the performance of BMW-LB-PB is not better is that the reordering changes the distribution of postings in docID space.
 Table 5: Impact of docID reordering on BMW on the
Varying k in Top-k : In this experiment, we show the performance of our methods when k is increased. In Table 6 we see the performance of the filtering mechanisms on BMW as we increase k and vary the number of query terms for the standard and the quantized index. We observe that both mechanisms scale very well, but LB-PB performs better as k increases in both indexes. BMW-LB returns the top-100 results in 13 . 5 ms . In the quantized index scenario, the query times are further reduced since calculations operate in the integer domain. In particular, Q-BMW-LB-PB returns the top-10 results in 3 . 83 ms , while the top-1000 in 24 . 8 ms . Table 6: Impact of top-k on BMW performance on the
In this paper, we proposed a general filtering mechanism based on docID-oriented Block-Max indexes that significantly improves the performance of several widely used algorithms for safe early termination in disjunctive queries. We present a simple interface that can be easily used by several query processing algorithms. Moreover, we augment the mech-anism with an auxiliary structure that maintains posting liveness information for each term and show how it can be exploited to achieve faster query times. We also propose caching techniques that address the space/time tradeoff of our mechanisms and overall, we achieve more than a factor of 2 speed-up over the fastest previous methods.

The future work will introduce additional query process-ing algorithms that exploit the advanced parallel instruction sets available in modern CPUs (AVX) and faster compres-sion methods that use SIMD operations of current CPUs. This research was supported by NSF Grant IIS-1117829  X  X f-ficient Query Processing in Large Search Engines X , and by a grant from Google. Sergey Nepomnyachiy was supported by NSF Grant ITR-0904246  X  X he Role of Order in Search X .
