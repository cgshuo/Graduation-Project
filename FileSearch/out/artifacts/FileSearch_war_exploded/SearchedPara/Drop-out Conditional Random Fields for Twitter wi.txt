 Nowadays, people are generating tremendous amount of information on social websites. For ex-ample, more than 200 million tweets are generated everyday on Twitter (Ritter et al., 2011). Twitter has become a key news source, in addition to standard news channels. As such, social scientists are start-ing to pay attention to it in recent years (Bollen et al., 2011; Chung and Mustafaraj, 2011; Xu et al., 2014; Calvin et al., 2015; Baldwin et al., 2015; Bell-more et al., 2015). The traditional machine learned modeling approaches trained with small and clean general text, such as news articles, perform poorly when applied to tweets, because tweets are struc-turally very different from general text. Thus, it is necessary to build new models for Twitter. One could label a reasonable size of tweets to train a model for a natural language processing (NLP) ap-plication. The problem is that it is very expensive to refresh the annotated data to keep the model up-to-date, because users generate tweets in a unprece-dented rate (Hachman, 2011).

An obvious solution to the problem is to de-velop methods of utilizing a large amount of un-labeled data. One way is to induce word embed-dings in a real-valued vector space from a large num-ber of tweets (Kim et al., 2015a; Mikolov et al., 2013; Pennington et al., 2014). It is shown that the task-specific embeddings induced on tweets pro-vide more powerful than those created from out-of-domain texts (Owoputi et al., 2012; Anastasakos et al., 2014).

Another method is to build the task-specific gazetteers. Task-specific gazetteers make the mod-els more general and increase their coverage for un-seen events. They have been proven to be useful on a number of tasks (Smith and Osborne, 2006; Li et al., 2009; Liu and Sarikaya, 2014; Kim et al., 2015b; Kim et al., 2015c). Since gazetteers can improve modeling performance, here we more focus on how to use gazetteer more effectively. To build gazetteers with sufficient coverage for our task, we first expand gazetteers from knowledge graph and phrase embed-dings.

However, since the expanded gazetteers cover sig-nificant proportions of the entities in the training data, the weight of gazetteers features are easily in-flated and thus the model tends to rely too much on lexical features extracted from the gazetteers fea-tures to assign a tag rather than the contextual fea-tures such as n -gram, a phenomenon called  X  X eature under-training X . As a result, we often observe no-ticeable performance degradation at test time when the entity value does not exist in the training set or the entity dictionary.

To solve this problem, we introduce a model bination model proposed by Smith and Osborne (2006). In our experiments, we show that the pro-posed method significantly improves the F1 score from 65.54% to 69.38%, compared to the baseline. For the named entity recognition (NER) task, the in-put is a sentence consisting of a sequence of words, x = ( x 1 ...x n ) and the output is a sequence of corresponding named entity tags y = ( y 1 ...y n ) . We model the conditional probability p ( y | x ;  X  ) us-ing linear-chain CRFs (Lafferty et al., 2001): where  X  is a set of model parameters. Y con-tains all possible label sequences of x , and  X  maps ( x,y ) into a feature vector that is a linear combination of local feature vectors:  X ( x,y ) = P is to find  X  that maximizes the log likelihood of the training data under the model with l 2 -regularization:
CRFs have benefited from having a rich set of gazetteers as features in the model (Smith and Os-borne, 2006; Liu and Sarikaya, 2014; Hillard et al., 2011; Kim et al., 2014; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2015d). Smith and Osborne (2006) point out that common gazetteer features fire often enough to overwhelm other features during in-ference. They address this problem by building a combination of two models: one without gazetteers and another with gazetteers. Instead of combining two models, we propose a simple model by having a new penalty term to the equation (1): where G is a set of gazetteers and freq ( g ) counts how many times words appear in gazetteer g from training data. In our experiments, we tuned both penalty weights for local features and for gazetteer features based on a small held-out validation set. The  X  g is a member of model parameter  X  and each gazetteer has its own parameter  X  g . The in-troduced penalty decreases common gazetteers X  in-fluence on model X  X  decisions. By this term, we call our model dropout CRFs . The original dropout tech-nique removes features randomly -for each training instance, only a random subset of the features will be activated (Hinton et al., 2012; Xu and Sarikaya, 2014). While it can be perceived as a general treat-ment to the under-training problem, it is not specifi-cally directed at the problem we are facing in named entity recognition (NER) task. In NER, the under-training problem is more specific -the contextual features may not get large enough weights due to the strong influence of the gazetteer features. The negative impact of such under-training is also more measurable -if a named entity is unseen, the chance of a detection error becomes much higher. There-fore, we focus on decreasing influence of specific features. For specific features, we reduce the cover-age of dropout from all features to gazetteer feature through feature dependent regularization . Also, the objective function of dropout CRFs, given in equa-tion (2), is still convex because the equation (1) is convex and the new penalty term is linear with re-spect to  X  . Therefore, a standard optimization algo-rithm finds optimal  X  without sacrificing any abili-ties, which original CRFs have. In this section, we detail the feature templates used for our experiments. Besides basic features, we also employ part-of-speech (POS) tags, chunks, word representations and gazetteers. We run task-specific POS-tagger and chunker, which are trained on tweets annotated with Twitter-specific tags (Rit-ter et al., 2011) as well as standard Penn Treebank tags, of Owoputi et al. (2012) to produce POS tags and chunks. We explain the word representations and gazetteer features in the following subsections. 3.1 Basic Features The model of Ritter et al. (2011) employs the fea-tures described in this subsection. They are com-posed of the following features: (1) n -grams: uni-grams and bigrams, (2) capitalization, (3) three char-acter suffix and prefix presence, (4) binary features that indicate presence of hyphen, punctuation mark, single-digit and double-digit, (5) gazetteers (6) top-ics inferred by LabeledLDA (Ramage et al., 2009), and (7) brown cluster (Brown et al., 1992) produced by Ritter et al. (2011).

To alleviate the problem of word sparsity, we also use task-specific latent continuous word repre-sentations, induced on 65 million unlabeled tweets with 1.3 billion tokens. We create three sets of word representations: CCA (Dhillon et al., 2012; Kim et al., 2015a) based on matrix factorization, word2vec (Mikolov et al., 2013) and glove (Pen-nington et al., 2014), which are gradient based. All word representation algorithms produce 50 -dimensional word vectors for all words occurring at least 40 times in the data. We use left and right word of the target word as context for learning the word representations.

We also use compounding embeddings as an addi-tional feature. Combining multiple sets of features has been proven to be effective (Koo et al., 2008; Kim and Snyder, 2013; Yu et al., 2013). We ex-plore four different ways of combining the word rep-resentations: element-wise averaging, element-wise multiplication, concatenation and hierarchical clus-tering. We empirically determined that the element-wise averaging achieves better performance than single embeddings and other combination methods. We do not describe the results for embedding com-binations in detail here. NER models degrades when they encounter un-seen words during training. To make the problem worse, tweets contain many rare words and it is pro-hibitively expensive to create a training set with suf-ficient lexical coverage. To alleviate the problem, we extend the original gazetteers with two methods: gathering data from knowledge graph and construct-ing task-specific gazetteer with phrase embeddings. 4.1 Expansion from Knowledge Graph To expand gazetteers from knowledge graph, we ap-ply the following processing steps. We first extract the seed words from training data. With seed words, we then collect the relevant lexicons from knowl-edge graph such as Freebase, Wikipedia and Yelp. For example,  X  X ior X  is related to company and prod-uct from knowledge graph. We collect all lexicons associated with seed words. In addition, we post-process gazetteers for variance: i) organization: it is composed with full name with abbreviation, such as  X  X ndigenous Land Corporation (ILC) X . We also gen-erate variants of full names ( X  X ndigenous Land Cor-poration X ) and abbreviation ( X  X LC X ), respectively, ii) facility: because the term elementary indicates a school, we add a lexicon removing the word school of  X  X edder elementary school X . At the end of the pro-cessing, we end up with 2.7 millions lexicon items. 4.2 Constructing Gazetteers with Phrase We now describe how to construct task-specific gazetteer with phrase embeddings. We use canonical correlation analysis (CCA) (Hotelling, 1936) to in-duce vector representations for phrase embeddings. To extract candidate phrases from unlabeled Twit-ter data, we first count the frequency of the context words set for each token. The size of context words set ranges from 1 to 3 . The context words set oc-curring more than 100 are used as a rule to extract candidate phrases.

Let n be the number of candidate phrases ex-tracted by rules. Let x 1 ...x n be the original representations of the candidate phrases itself and y ...y n be the original representations of two words to the left and right of the candidate phrases.
We use the following definition for the original representations. Let d be the number of distinct can-didate phrases and d 0 be the number of distinct con-text words set.  X  x l  X  R d is a zero vector, in which the entry  X  y l  X  R d 0 is a zero vector, in which the entries
Using CCA, we obtain phrase embeddings U with k -dimensional space. To train a classifier, we man-ually construct a training data with 5 positive and 5 negative samples, for each gazetteer. With this data, we learn a binary classifier with the phrase embed-dings as a feature. Using this classifier, we predict whether the phrases fit to the gazetteers; we refer the readers to Neelakantan and Collins (2014) for details. To demonstrate the effectiveness of the dropout CRFs, we run experiments on named entity recog-nition task on the Twitter dataset of Baldwin et al. (2015). We refer the readers to Baldwin et al. (2015) for the details of the dataset. We split the data into 70% for training, 10% for tuning, and 20% for test-ing. For all the experiments presented in this section, both CRFs and dropout CRFs are trained using the L-BFGS (Liu and Nocedal, 1989). 5.1 Effectiveness of the Gazetteers One of our contributions is to augment the size of gazetteers with knowledge graph and phrase em-beddings. Table 1 represents the performance of a model with original gazetteers, which are collected by Ritter et al. (2011) from freebase (Base Gazet) and with gazetteers we extended (Our Gazet). The size of Base Gazet is 2.9 million and the size of Our Gazet is 6.6 million, which has an additional 3.7 million entries compared to the Base Gazet . The model trained Our Gazet improves the F1 score from 62.76% to 64.67%, compared to the baseline. As shown in Table 1, we believe that larger gazetteers can mitigate the  X  X nseen words X  problem by in-creasing the coverage of the gazetteers.
 5.2 Effectiveness of the Dropout CRFs We conducted additional experiments with the CRF model that uses Our Gazet . Table 2 shows the overall results for models with and without dropout. We compare three models: the vanilla CRFs (CRFs vanilla ), the combination model as de-scribed in Smith and Osborne (2006) (CRFs LOP ) and our dropout model (CRFs dropout ). To avoid model parameters for gazetteer features getting over-regularized, Smith and Osborne (2006) pro-pose to train separate models with and without gazetteers. They combine predictions from the two models by using logarithmic opinion pool (LOP). We refer the reader to Smith et al. (2005) for further details.
 The CRFs vanila yields 64.03% F1 score and the CRFs LOP improves the performance to 65.54%. The CRFs dropout , which reduces the influence of gazetteer features, improves the F1 score to 69.38%, which corresponds to a 13% decrease in error rela-tive to vanilla CRFs.
 5.3 Analysis While previous NER tasks mostly focus on report-ing numbers on the original data set (Baldwin et al., 2015; Yang and Kim, 2015; Kim et al., 2015c), we further investigate how the tagging performance may change, if entities are unseen at test time. To enable such analysis, we create additional test set based on the original test set by replacing each word in person and company entities with a special to-ken, XXXXX , indicating unseen words. This new test set represents an extreme case, where none of the words contained in the gazetteers are observed in the training data.

Table 3 represents the comparison of vanilla CRF model and dropout model for unseen test. Gazetteer is helpful to resolve  X  X nseen words X  problem. Un-fortunately, frequent appearance of gazetteer makes a model learn weak context feature and strong gazetteer feature. By forcing a weight of gazetteer feature low, the dropout model allows the weak con-text features to become strong and the large weight of gazetteer feature to become smaller. Conse-quently, CRF dropout shows the significant improve-ment compared to CRF vanilla .

To see a change of feature weight when we apply dropout technique, we show the feature weights for the word  X  X ahill X  of vanilla CRFs and dropout CRFs in Table 4. In vanilla CRFs, gazetteers have a strong weight compared to the context features. However, our dropout CRFs decrease the weight of gazetteer features, while making the context features larger, to steer the models X  decision in the right direction. In this paper, we showed how to improve the CRF based NER model for Twitter by exploiting a large number of gazetteers. Using gazetteers in model-ing helps the coverage and generalization but sim-ply incorporating gazetteers of all of large sizes into the model may lead to  X  X nder-training X  of parame-ters corresponding to the context features. We ad-dressed this problem by adding the dropout penalty term in the CRF training, which improves better pa-rameter. The proposed technique results in signifi-cant improvements over the baseline.
One of the future directions of research is to ex-tend the same idea to various sequence learning problems: part-of-speech tagging and slot tagging. We thank Do Kook Choe, Puyang Xu, Alan Rit-ter and Karl Startos for helpful discussion and feed-back.

