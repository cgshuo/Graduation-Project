 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.5.3 [ Pattern Recognition ]: Clustering X  algorithms Algorithms Multi-Clustering, Bi-Clustering, Formal Concept Analysis
Clustering is a central problem in data mining and ma-chine learning. Conventional clustering attempts to group similar objects from a single collection together according to some similarity or dissimilarity measure. Typically, data is organized in a two-dimensional data table consisting of ob-jects and attributes, and clustering is performed along one dimension of the table. The output of traditional cluster-ing algorithms is a partitioning of the objects into distinct sets. Bi-clustering (subspace clustering, co-clustering) si-multaneously clusters along both dimensions of a data table [13]. In other words the output of a bi-clustering algorithm is a set of objects along with the subspace of attributes in which this cluster appears in. In high-dimensional data tra-ditional clustering techniques that only cluster along one dimension of the table have proven inadequate due to the curse of dimensionality and noisy features. 3-clustering goes one step beyond bi-clustering and aims to concurrently clus-ter two data tables (databases) that share a common set of row labels, but whose column labels are distinct. Such clusters reveal the underlying connections between the el-ements of all three sets of labels. For example, consider two databases that contain gene-disease and gene-ontology data as shown in figure 1. Through 3-clustering we wish &lt; { a } , { 1 , 2 , 3 , 4 } &gt; &lt; { a, b } , { 1 , 3 , 4 } &gt; &lt; { a, w, b } , { 1 , 3 } &gt; &lt; { a, y } , { 1 , 2 } &gt; &lt; { a, x } , { 2 , 4 } &gt; &lt; { a, w, b, c, y, z } , { 1 } &gt; &lt; { a, b, d, x } , { 4 } &gt; (c) Clusters in the join of D 1 and D 2 to find a correspondence between the bi-clusters of the in-dividual data-sets in order to unveil underlying associations between attributes of the individual dastasets. Figure 1(d) shows the 3-clusters that connect both datasets, while figure 1(c) shows clusters corresponding to the join of D 1 and D The 3-clusters unveil the underlying interactions between the three distinct sets of diseases, genes, and gene ontology terms. Furthermore these clusters reveal the associations be-tween diseases and gene-ontology in terms of genes. Simply merging D 1 and D 2 then searching for bi-clusters would not unravel the underlying relationship between clusters con-taining gene-ontology terms and clusters containing diseases. Moreover, the computational cost of joining both tables and then searching for bi-clusters that reveal associations is pro-hibitive, as will be demonstrated in our experiments. For example consider the clusters listed in figure 1(c). The first two clusters &lt; { a } , { 1 , 2 , 3 , 4 } &gt; and &lt; only correspond to bi-clusters found within D 1 and thus do not reveal any association between gene-ontology terms and diseases. The next three clusters &lt; { a, w, b } , &lt; ciations; however, if we partition each cluster, the parti-tions do not correspond to local bi-clusters in the individual datasets. In other words consider &lt; { a, y } , { 1 , 2 partition &lt; { a } , { 1 , 2 } &gt; is not a bi-cluster in D is enclosed within the larger pattern &lt; { a } , { 1 , 2 , 3 , 4 Even in this small example we see that only two of the seven mined bi-clusters in the join of D 1 and D 2 revealed associ-ations between the clusters of D 1 and D 2 .Moreover,con-sider the 3-cluster &lt; { a, b, c } , { w, y, z } , { 1 } not only points out an association among diseases a, b, c and gene ontology terms w, y, z through the gene 1; but also an association among gene 1 and diseases w, y, z through gene-ontology terms a, b, c . Thus 3-clusters not only reveal as-sociations among attributes of individual data-sets through objects; but also unveil associations between local bi-clusters with respect to attributes in other data-sets.

Other sample applications where 3-clustering would prove useful are: 1. Social Networking: Discovering associations between 2. Web Mining: Consider two data tables which share In section 2 we present a formal definition of a 3-cluster. Our work below builds upon the ideas of lattice-based in-formation retrieval and concept analysis. As is shown in [9] the formal concepts located within a context correspond exactly to the bi-clusters. We take advantage of the con-ceptual hierarchy that is inherent within the data, in order to increase the usefulness of the clusters to the user. The problem with concept-lattice-based approaches is that the number of concepts in a database grows exponentially, while maintaining and traversing the lattice is often too expensive in both time and space. We overcome these problems by not explicitly maintaining a lattice data structure, while still taking advantage of the hierarchy of concepts located within the databases.
Previous works in text mining and biomedical informatics [13, 10, 8] have mainly focused on bi-clustering (bi-cliques,co-clustering, subspace clustering). The problem of bi-clustering is NP-Hard [7], however several algorithms have been pro-posed in this field. In binary data a bi-cluster corresponds to a maximal-edge bi-clique, which in turn corresponds to exactly to a closed itemset [21]. The idea of approximate bi-clusters has also been studied in several domains. Fur-thermore to the best of our knowledge no other 3-clustering methodology has been developed. In [4], Zhao and Zaki proposed TriCluster which mines clusters in 3D microarray data. TriCluster considers a two dimensional data table in which each data point is projected into a third dimension, such as time or space; resulting in a three dimensional data matrix. Tri-Clusters are then defined as 3-d sub-matrices in the three dimensional data matrix that satisfy certain ho-mogeneity conditions. Our approach discovers axis-aligned clusters across two-dimensional data-tables, rather than 3-d sub spaces in a 3-d data matrix. A general multi-way cluster-ing scheme was proposed in [20]. In that work cluster formu-lation is driven by  X  X he objective of capturing the maximal information X  in the original relation graph. The authors also focus on discovering  X  X ata cubes X  or tensors across multi-relational data. The proposed algorithm MRGC ,however performs a hard clustering and requires the number of clus-ters as input. Our work focuses on arbitrarily positioned, overlapping axis-aligned clusters in binary data, and does not require the number of clusters as input.

Distributed data mining (DDM) has recently received much attention with the advances in communication technology. Distributed clustering algorithms in vertically partitioned data have focused on traditional clustering algorithms in a distributed environment. [14, 15, 16] focus on the k-means algorithm, and detecting k-outliers in peer-to-peer environ-ments and across distributed databases. [17] modified the density based clustering algorithm DBSCAN to accommo-date distributed data sites. [18] implements the k -windows algorithm in a distributed sense. In [19] the authors present a privacy-preserving collabora tive filtering method in hori-zontally partitioned data, which utilizes bi-clustering.
In this paper we focus on discovering 3-clusters across ver-tically distributed binary data. While we only consider the case of two vertically partitioned datasets, our methodology can easily be extended to include n databases.
Definition 1. A database D =( O, I, R ) consists of two sets O and I and a relation R between O and I .Theele-ments in I are referred to as items and the elements in O are referred to as objects .

Let D 1 =( O, I 1 ,R 1 )and D 2 =( O, I 2 ,R 2 )denotetwo databases such that I 1  X  I 2 =  X  . Furthermore we assume that D 1 and D 2 share the same global set of objects with identical record id. We refer to D 1 and D 2 as vertically partitioned databases. D 1 or D 2 is referred to as the learner if it initiates a knowledge discovery task. For simplicity D 1 is always assigned the task of learner, and D 2 theroleas client .Any set X i  X  I i is called an itemset in dataset i ,andaset Y is called an objectset . Given a database D i =( O, I i ,R i ), let X be any arbitrary itemset and Y be an arbitrary objecset.  X  ( X ) is defined to be { z  X  O | xR i z for all x  X  X } ,thatis  X  ( X ) returns the objectset Z which has all items in X in D i . Similarly  X  i ( Y ) is defined to be { z  X  I i | zR i y for all y that is  X  i ( Y ) returns the itemset Z common to the objects in Y in D i .The support of an object o in database D i is defined as  X  i ( o )= |  X  i ( o ) | .

Definition 2. A pattern of the database D i is a pair &lt;X,Y &gt; with X  X  I i and Y  X  O such that  X  i ( Y )= X .
Definition 3. A formal concept or bi-cluster of the database D i is a pair &lt;X,Y &gt; with X  X  I i and Y  X  O such that  X  i ( X )= Y and X =  X  i ( Y ) Definition 4. A 3-cluster T ij of the databases learner, is a triple &lt; X,Y,Z &gt; with X  X  I i , Y  X  I j and Z  X  O such that &lt;X,Z&gt; is a bi-cluster in D i and &lt;Y,Z&gt; is a pattern in D j .
 The intuition behind this definition of a 3-cluster is that we wish to match concepts from the learner with patterns found in D 2 . This definition is flexible in the sense that we do not require the matching patterns in D 2 to be bi-clusters. How-ever, since bi-clusters are also patterns, matching bi-clusters will also be discovered. Consider the 3-clusters in figure 1(d). Under our definition &lt; { a, b, c } , { w, y, z } , { 4 even though &lt; { x } , { 4 } &gt; is not a bi-cluster in D the other hand consider &lt; { a, b, c } , { w, y, z } , { &lt; { a, b, c } , { 1 } &gt; is a bi-cluster in D 1 and &lt; { is also a bi-cluster in D 2 . The flexibility of our definition facilitates the discovery of arbitrarily positioned and over-lapping clusters across the two datasets. The number of 3-clusters across two databases may grow exponentially in the worst case, making it intractable to enumerate the entire set of 3-clusters. We allow the learner to specify the values mi 1 , mi 2 , mo which correspond to the minimum number of items from D 1 , D 2 and the minimum number of objects that each 3-cluster must contain. The tremendous number of 3-clusters that exist within the data also motivates the key question which 3-clusters are most informative to the user .
Intuitively, we would like our result to maximize the num-ber of one X  X  in a 3-cluster, while also maximizing the num-ber objects and items. However, we recognize the trade-off between the number of objects and the number of items in clusters. The more objects a cluster contains the fewer items it is bound to include, and vice-versa. Thus, the objective of maximizing the number of objects and items in 3-clusters are at odds. Building on the ideas of [3], we first develop a quality measure for local bi-clusters, and then use the same intuition to construct a quality measure for 3-clusters.
Local bi-clusters may be thought of as rectangular sub matrices of the original data table. The number of objects in the bi-cluster correspond directly to the height of the rect-angle, while the number of items correspond directly to its width. Given a bi-cluster C = &lt;X,Y &gt; let h ( C )denotethe height of C and w ( C ) denote the width of C .Thenumber of 1 X  X  in C then corresponds direct ly to the area enclosed by C , which we denote as  X  ( C )= w ( C )  X  h ( C )= | X Simply utilizing  X  as a quality measure for local bi-clusters has two major pitfalls. First, it does not distinguish between the individual contribution of width and height to the total number of 1 X  X  in a pattern. Second  X  does not take into account the fact that as height increases, the width of a bi-cluster must decrease. This raises the key question is how many items are we willing to trade for each additional ob-ject? . In order to overcome these pitfalls, we introduce a parameter  X  which represents a trade-off value for the per-centage of items we are willing to drop for each additional object added. Equivalently  X  represents how many units of width we are willing to drop for each additional unit of height. We now construct a quality measure,  X  centered around  X  and  X  ( C ). Formally, given a bi-cluster C in D i quality of C in D i .Then X  i should:
The above equations imply that the function  X  ( a, b )must satisfy the following two conditions [3]: 1.  X  ( a, b ) should be monotonically increasing in both a 2.  X  ( a, b ) should be a  X  -balanced. Mathematically this One such function that satisfies conditions 1 and 2 is:
This results in the following definition for the quality of a bi-cluster.
 Definition 5. Given  X  and a bi-cluster C = &lt;X,Y &gt; in D i its quality  X  i ( A )isgivenby Definition 5 follows directly from equation 2 and the intu-ition discussed above.
We now develop a quality measure for 3-clusters utilizing the same intuition that was used for local bi-clusters. 3-clusters may also be thought of as rectangular sub-matrices across two data-tables. The number of objects in a 3-cluster correspond directly to the height of the rectangle, while the combined number of items from D 1 and D 2 correspond to the width. Therefore, given a 3-cluster C 12 = &lt; X,Y,Z &gt; As the height of a 3-cluster increases, its width also must decrease, just as was the case with local bi-clusters. Utilizing this fact, and properties 1 and 2 from the previous section we may now derive  X  12 : Equation 8 is clearly  X  -balanced, and can be easily computed since it is the sum of the quality of local bi-clusters.
Definition 6. Given  X  and a 3-cluster C 12 = &lt;X,Y,Z&gt; across D i and D j its quality  X  ij ( C )isgivenby For simplicity we will denote  X  12 simply as  X .
The value of  X  will have a great effect on the nature of 3-clusters discovered. Higher values of  X  will favor 3-clusters containing more items from either D 1 or D 2 and fewer ob-jects. As mentioned earlier,  X  represents the trade-off be-tween how many items a user is willing to give up in order to include n more objects. In domains where users wish to discover associations between a large number of items from both I 1 and I 2 , but don X  X  necessarily care about the number of objects that they are associated with, then large values of  X  should be used. If the opposite is true, then smaller values of  X  should be used. For example, suppose O is a set of movies, while I 1 is attributes of male customers and I is attributes of female customers. A couples movie rec-ommender system would be more interested in discovering a few movies that encompass as many attributes from both I 1 and I 2 . On the other hand, match making software would prefer to find a smaller number of attributes from I 1 and I that are covered by several movies.
The search for 3-clusters must first identify the bi-clusters in D 1 . We follow a search strategy similar to [1]. However we introduce more pruning techniques, utilizing a simultaneous search and prune strategy over both the objectspace of D 1 and D 2 . Additionally the properties of  X  allow for even further condensation of the search space. Our algorithm consists of three main steps: 1. Enumerate a local bi-cluster C = &lt;X,Y &gt; in D 1 such 2. Form a 3-cluster C 12 by matching C with a pattern in 3. Assert that C 12 is of high quality.
D 1 and D 2 each construct a list of objectsets OS and a list itemsets IS by associating each item with its corre-sponding objectset, and every object with its correspond-ing itemset. Utilizing this approach the need for complex data structures is cut-down, and we can simply resort to set operations such as intersection and difference for computa-tion. Computing  X  i and  X  i only requires computing inter-sections among elements of IS and OS respectively. Given a OS and IS . We maintain these lists on disk, indexed by an index file; unless D i is sparse, and they can fit into main memory.
The search for candidate bi-clusters takes place over the objectset search tree shown in figure 2. The idea of prefix-based equivalence classes is used to break the original search space into independent sub-problems as is done in [6]. A pre-fix tree is an ordered data structure used for sorting strings. Two objectsets are in the same prefix class if they share a common k -length prefix P . The prefix class of P is denoted as [ P ]. Every node of the objectset search tree represents a pattern &lt;Y,PX i &gt; in D 1 with an object-prefix P , while the patterns contained in [ P ] constitute the sub-search space of P . For example in figure 2 the nodes &lt; { a } , { 2 , 3 &lt; class [2] = { 3 , 1 , 4 } ,sincetheyallsharetheprefix { 2
Given a pattern &lt;Y,PX i &gt; with tersection of  X  i ( PX i )with  X  i ( X j ), and the union of PX i with PX j to obtain the descendant node &lt;Y ,PX i X j &gt; and new prefix class [ PX i ] containing elements X k ,where k&gt;i . For example consider the node &lt; { a } , { 2 , 3 ure 2, which is contained in [2] = { 3 , 1 , 4 } . The descendant node &lt; { a } , { 2 , 3 , 1 } &gt; is obtained by  X  1 ( { a } X  X  a, b, c } = { a } and { 2 , 3 } X  X  2 , 1 } = { 2 , 3 , 1 formed prefix class is [23] = { 1 , 4 } .

Since every node maintains  X  i ( PX i )inmemory,then computing  X  i ( PX i )  X   X  i ( X j ) is extremely efficient as it only involves one intersection operation.

We sort the objects in D 1 in ascending order of the car-dinality of their associated itemsets and use this ordering to form our prefix classes. The ascending order has been adopted in many frequent itemset mining algorithms and has proved very effective in pruning the search space [1]. The basic idea behind this is to allow objects with low sup-port to from the longest prefix classes, since they are likely to be pruned early on in the search. Next, the learner re-quests the support of each object from the client and stores it in an array  X . Finally a depth first search of objectset search tree with extensive pruning over every prefix class is performed to discover potential candidate bi-clusters in D that can be used to form 3-clusters.
As the search proceeds down each level of the objectset tree, the next set of descendant nodes for the current node &lt;Y,PX i &gt; is computed by adding the j th object from [ P ], X j , and performing an intersection operation between  X  ( PX i )and  X  1 ( X j ). Let  X  be the ordered vector of  X  for all X k in [ P ]with k  X  j ; this is easily obtained from  X . We check the maximum possible support of the objects in [ P ], utilizing  X . If | PX i | &lt;mo ,thenlet x = mo  X  X  otherwise let x =1. x represents the minimum number of objects that need to be added in order to form a candi-date bi-cluster. Then the maximum client support defined as  X  x ( X ) is computed as follows:  X  x computes the maximum possible support for the ob-jects of the current child node in D 2 . For example con-sider the datasets D 1 and D 2 in figure 1 with mo = mi 1 mi 2 = 2. Consider the node &lt; { a } , { 2 } &gt; ,where[2]= { while  X  = (1 , 3 , 1) since  X  2 ( { 3 } )=1 , X  2 ( { 1  X  ( { 4 } )=1. Thusthe maximum client support is computed as  X  1 ((1 , 3 , 1)) = 1.

Proposition 1. Given a prefix node &lt;Y,PX i &gt; ,its n child &lt;Y ,PX i X n &gt; , x and  X , if | Y | &lt;mi 1 or  X  mi 2 then &lt;Y ,PX i X n &gt; and all its descendants cannot form candidate nodes.

Proof. Let  X  =  X  i ( PX i )  X   X  i ( X n ). Since |  X  | &lt;mi given any set , |  X   X  | &lt; = |  X  | &lt;mi 1 . Thus the first half of the  X  X r X  statement follows directly from the properties of set intersection. Since all 3-clusters require at least mo objects, then the maximum number of items in both the learner and client is limited by the support of the x th largest object; this again follows directly from the properties of set intersection. Since  X  x ( X ) corresponds exactly to the support of the x largest object in D 2 ,and  X  x ( X ) &lt;mi 2 ; then the support of all descendants in D 2 will also be less than mi 2 . Utilizing proposition 1, non-promising nodes are pruned from the search space.
After pruning, the newly formed descendant nodes are sorted again in ascending order of the cardinality of their itemsets. Given a descendant node C 1 = &lt;U 1 ,V 1 &gt; we exploit the fact that bi-clusters in a context form a complete lattice [2] and introduce the  X  X oncept jump X  operation  X  .
Definition 7. Given a pattern C i = &lt;U i ,V i &gt; , For example consider the pattern &lt; { a } , { 2 } &gt; in figure 2. We show how the concept jump operator exploits the clo-sure of bi-clusters, leading to more efficient searching of the objectset space.

Lemma 1. Given a pattern C i = &lt; X  i ( V i ) ,V i &gt; and  X  ( C i )= &lt;U i ,V i &gt; then V i  X  V i .

Proof. If g  X  V i then gRm for all m  X   X  i ( V i ), which implies that g  X   X  i (  X  i ( V i )), thus Lemma 1 ensures that after applying  X  to any node C = &lt; U i ,V i &gt; the resulting node C = &lt;U i ,V i &gt; will always contain at least as many objects as C . Further, utilizing this lemma we also show that  X  i ( U i )= V i , which entails that C is a bi-cluster.

Proposition 2. Given a pattern C i = &lt;U i ,V i &gt; and C i = &lt;U i ,V i &gt; then C i is a bi-cluster in D i Proof.
 By lemma 1 V i  X  V i . Thisimpliesthat: However we also have that Thus by equation 11 and 12 and definition 3, C i is a bi-cluster.
 Instead of traversing all prefix nodes between V 1 and V 1 our algorithm  X  X umps X  ahead to V 1 skipping all nodes on the path, since they cannot form bi-clusters. Any bi-cluster, C , formed in this manner we refer to as a candidate formed from the child pattern C 1 . Propositions 1 and 2 guarantee that all candidate bi-clusters C 1 in D 1 satisfy the first con-dition of forming 3-clusters. After forming a candidate bi-cluster C 1 = &lt;U 1 ,V 1 &gt; the next step is to match this bi-cluster to a pattern in D 2 . Following definition 3, if a pattern C U ,V 1 &gt; exisits in D 2 such that | U 2 | X  mi 2 then we may form a 3-cluster.

Proposition 3. Given a candidate bi-cluster C i = &lt;U i ,V i &gt; U
Proof. Assume that V j  X  V i . Thisimpliesthat  X  j ( V j )  X  ( V i ). By definition of a pattern  X  j ( V j )= U j  X   X  Let U j =  X  j ( V i ). Then by definition &lt;U i ,U j ,V i &gt; is a 3-cluster.
 Making use of proposition 3, in order for the candidate bi-cluster C 1 = &lt;U 1 ,V 1 &gt; to form a 3-cluster, D 1 requests that D 2 computes  X  2 ( V 1 ) and sends it to D 1 .If |  X  2 ( V 1 then the 3-cluster C 12 = &lt;U 1 , X  2 ( V 1 ) ,V 1 &gt; is formed and stored in a hashtable H . The search then recurses using V as the new prefix and [ V 1 ] as the new prefix class. Otherwise if  X  2 ( V 1 ) &lt;mi 2 then by proposition 1 we may prune the sub-search space of V 1 .
While D 2 does not maintain an explicit search tree, it stores the last objectset V received, and its associated item-set  X  2 ( V ). Therefore given any new objectset V such that V  X  V , then computing  X  2 ( V ) consists only of | V | X  X  V set intersections. However if V V then  X  2 ( V ) entails |
V | X  1 set intersections. A key question remains, given 3-cluster C 12 = &lt;U 1 , X  2 ( V 1 ) ,V 1 &gt; , is it possible to form a higher quality 3-cluster D 12 = &lt; X,Y,Z &gt; in the subspace of V If it is not possible to form such a 3-cluster, then the sub search space of V 1 should be pruned. The properties of  X  allows us to perform such pruning.

Proposition 4. Given 3-cluster C ij = &lt; T,U,V &gt; and any 3-cluster D ij = &lt; X,Y,Z &gt; such that V  X  Z ,let
Proof. =  X  : Assume that  X ( D ij )  X   X ( C ij ). Then by the  X  -balanced property of  X  we have Where x represents the least number of objects that must be added in order to form a 3-cluster of higher quality than C ij .Solvingfor x we get: Equation 14 and the  X  -balanced property of  X  imply that |
Z  X  =: Assume that | Z | X  X  V | X  log( r ) log(  X  ) .Thenwehave: By equation 15 and the  X  -balanced property of  X ,  X ( D 12  X ( C 12 ).
 Proposition 4 allows us to check the potential quality of 3-clusters in the sub-search space of V 1 . Making use of this proposition we store the most recently discovered 3-cluster C 12 .Ifa3-cluster D 12 = &lt;X,Y,Z&gt; is discovered in the log(  X  ) . y represents the least number of objects that must be added in order to form a 3-cluster of higher quality than C 12 . Two different cases arise: 1. | Z | X  X  V | X  y : Inthiscasewedelete C 12 since it is of 2. | Z | X  X  V | &lt;y : This entails that C 12 is of higher quality Moreover making use of y we can also apply proposition 1 to check if  X  y ( X )  X  mi 2 . In this case  X  would be the ordered vector of support in D 2 of the objects in [ Z ]. If  X  y ( X ) &lt;mi then sub-search space may also be pruned since not enough items are available in the sub-search space of Z to form 3-clusters. We refer to these pruning methods as  X  -pruning .
Algorithm 1 displays the pseudocode of our algorithm 3Clu for discovering 3-clusters. 3Clu is initialized with P = L 12 = H =  X  ,and[ P ]= O . The first for-loop (lines 2-6) prunes the children nodes utilizing proposition 1. Following this is the candidate formation stage. Line 7 is a simple check to make sure enough objects are present to proceed. The sort function on 9, sorts the prefix class of the current prefix in ascending order with respect to the cardinality of their itemsets. On line 11 descendant patterns are formed, while line 13 is another check to make sure enough objects are available to continue. We apply the concept jump oper-ation to each child pattern formed in line 14. The if state-ment on line 15 checks if the candidate bi-cluster C 1 has been previously generated. In line 16 D 1 requests  X  2 ( V from D 2 . Following this lines 17-20 form the 3-cluster C and check if all size constraints are met. Line 17 applies proposition 1 to prune the sub-search space of prefixes that do not contain enough items in D 2 . Line 18 checks if enough objects are present to form a 3-cluster. If this is not the case then we plunge deeper into the search (lines 26 -27). How-ever if all constraints are met then we add the discovered 3-cluster C 12 to the hashtable H . Finally we check if the sub-search space of V 1 can be pruned utilizing the function  X  -prune whose pseudocode appears in algorithm 2. 3Clu recurses using V 1 and [ V 1 ] as the prefix class. The last dis-covered 3-cluster, either C 12 or L 12 (lines 22 -25) is also kept in memory in order to perform  X  -pruning. Through-out the computation D 2 does not need to maintain a search tree of its own. Instead D 2 only has to compute  X  2 ( V Algorithm 2 implements  X  -pruning. Lines 5-7 correspond to case 1,lines 8-10 correspond to case 2a, and finally line 11 implements case 2b.
An execution trace of 3Clu with mo = mi 1 = mi 2 =1and  X  =0 . 5 on the example datasets D 1 and D 2 is presented in figure 3. As can be seen in the execution trace, much of the search space is pruned even though we are using the minimal parameters. Also no  X  -pruning took place, due to the fact that only two 3-clusters were discovered, and their objectsets were not supersets of each other.
The correctness of 3Clu is guaranteed by propositions 1,2,3, and 4.

Theorem 1. Given two datasets D 1 , D 2 and parameters mo, mi 1 ,mi 2 , X  , Algorithm 1 generates all high quality 3-clusters with respect to mo, mi 1 ,mi 2 , and  X  from D 1 and D , and only the high quality 3-clusters of D 1 and D 2 are generated.
We will analyze the time and memory complexity in terms of D 1 since the main search takes place at D 1 .Thecom-munication cost of the algorithm will also be considered, in terms of the number of messages passed between D 1 and D .

Computing the objectsets and itemsets is done in one lin-ear scan of the database. Depending on the dataset this is done in either O ( | O | )or O ( | I i | ). For the remaining of the complexity analysis we use the following notation: N is the total number of concepts in D 1 , i is the maximum size of an itemset and o is the maximum size of an objectset in D 1 . In the worst case there are N 3-clusters across both D 1 and D . Due to our  X  X ump X  operation the algorithm is called only for each concept in D 1 . The cost of the jump operation is bounded by o  X  i . When pruning the children nodes this step is bounded by o  X  i . The cost of sorting the nodes is never more than | O | X  log ( | O | ). Therefore the worst-case time complexity of Algorithm 4.1 is O ( | O | X  i  X  N ). During the en-tire process the objectsets and itemsets are kept in memory. The overhead for storing these are O ( | O | + | I 1 | ). Explor-ing the search space is done in d epth-first order, therefore only the itemsets on the current path needs to be kept in memory ( note that no explicit prefix tree is ever kept in memory). The maximal depth of a path is bound by o and the largest itemset is i . Consequently the space complexity of algorithm 4.1 is O ( | O | + | I 1 | + o  X  i ). The communication between D 1 and D 2 is limited to passing itemsets and ob-jectsets for every concept jump. Therefore in the worst-case the communication cost is O ( N ).
Here we present a brief discussion on extending 3Clu to mine more general n -clusters. Proposition 2 guarantees that the procedure of generating candidate bi-clusters remains the same for the general case of n -clusters. To form an n -cluster from candidate bi-clusters we simply modify line 16 of 3Clu to compute &lt; X  i ( V 1 ) ,V 1 &gt; for i =2 ,...,n .In addition  X  can be extended to contain the least support of each object in every database; this will facilitate simultane-ous pruning over all datasets . Finally, the monotonic and  X  -balanced properties of  X  insure that proposition 4 can be extended to the general case to perform  X  -pruning.
We ran a wide variety of experiments utilizing several real world datasets. All experiments were performed on Linux SUSE on a Pentium D 2.8 GHz, 3.68 GB of RAM, and all implementations were programmed in C++.
We choose two benchmark datasets and three datasets from the bio-informatics domain that are publicly available at www.cs.uc.edu/  X  alqadaf. The GO, Pheno, and Micro datasets all share the same row labels (genes), but distinct column la-bels corresponding to gene ontology terms, phenotypes and microRna respectively. For the two benchmark datasets we randomly partitioned each dataset in order simulate a ver-tical partitioning. Table 1 summarizes the characterstics of each dataset.

We ran several experiments in a simulated distributed set-ting, comparing the computation cost of 3Clu to the two state-of-the-art closed itemset mining algorithms CHARM and LCM . On the biological data we performed three performance tests. On test 1 D 1 = GOTerms and D 2 = PhenoTypes, test 2 D 1 =Mircoand D 2 = GO, while test 3 D 1 = Pheno and D 2 =Micro. Oneachtestweset mo = 1 and varied mi 1 ( mi 1 wasalwayssetequalto mi 2 ), which essentially told 3Clu to search for closed itemsets. To ensure a fair com-parison we ran CHARM and LCM only on D 1 and then ran a post-processing step that filtered out the closed item-sets that did not form 3-clusters with respect to D 2 .Moreover we disabled  X  -pruning on 3Clu . Figure 4 contains the results of these performance tests. All three datasets were sparse so lower values of support were of main interest. At higher val-ues of mi (support) all three algorithms perform equally on all datasets. As mi lowers we notice that 3Clu outperforms both CHARM and LCM on GO-Pheno and Pheno-Micro. Both CHARM and LCM were unable to terminate in under an hour on GO-Pheno with mi 1 &lt; 10(0 . 5%), while 3Clu finished in less than 1000 seconds. Even though we had disabled  X  -pruning, simultaneous pruning of the search space resulted in great boost of performance. Similar performance on the Micro-GO dataset can be attributed to the fact that Micro contained realtivley short objectsets compared to both GO and Pheno. We note that for the biological datasets the bulk of computation time (over 90 %) for CHARM and LCM was in enumerating the local bi-clusters of the learner, as opposed to the filtering stage.

Chess and Connect are much denser than the biological datasets. As can be seen from figure 4 3Clu outperforms the other two algorithms by over three orders of magnitude for most values of mi 1 . The prohibitive computational cost of simply mining bi-clusters in the learner and then searching for matches in D 2 is most apparent in these two datasets. On Chess, starting at mi 1 = 500(31 . 3%) both CHARM and LCM take over two hours to enumerate all 3-clusters while 3Clu terminates in under 27 seconds! Similarly on Connect we see this tend start at mi 1 =20 , 000(59%). We note that on these two datasets that for every value of mi 1 over 90 % of the computation time of both LCM and CHARM took place in the post-processing step of filtering the closed itemsets. The massive difference in performance is once again attributed to the simultaneous pruning strategy employed by 3Clu .For all values of mi 1 less than 10 % of the closed itemsets mined by CHARM and LCM resulted in 3-clusters. 3Clu completely avoids mining most of these closed itemsets and searching for matches, resulting in massive computational savings.
We applied 3Clu to Mushroom data set, which is part of the UCI Machine Learning Repository (http://www.ics.uci.edu/mlearn). This dataset contains 8124 objects (mushrooms), 22 categorical attributes and a label designating if each object is poisonous or edible. We em-ployed this dataset to verify the validity of the 3-clusters that our algorithm discovered by comparing the class la-bels of objects found within each 3-cluster. We created two random partitions of the dataset, where each partition con-tained with 11 categorical attributes (not including the class label) and shared the global set of objects (mushrooms). Each partition was then binarized. All the databases were relatively sparse, so we ran the algorithm with mo = 200, and mi 1 = mi 2 = 7. Furthermore we disabled  X  -pruning so as to enumerate all 3-clusters across the two datasets. After mining the 3-clusters in each experiment we added a post-processing step, that assigned each object only to the first cluster it appearers in. This was done due to the fact that many of 3-clusters were overlapping due to the spar-sity of the dataset. Next we compared the objects in each cluster to their class label, and report these results in figure 5. The rows of each result matrix represent the classes of mushrooms p (poisonous) and e (edible), while the columns represent the 3-clusters that each object was assigned to. The cells represent the percentages of total objects in the cluster. For example in figure 5(a) the first column, TC contains two cells, one corresponding to the class P and the other to the class E . The first cell indicates that cluster TC 1 contained 14.5 percent of all mushrooms and they are all poisonous. While the second cell specifies that no edible mushrooms are found in this cluster.
 We present the results of two experiments we conducted. Each experiment was conducted with a different random partitioning of the Mushrooms dataset. In the first exper-iment 3Clu mined 32 3-clusters, after the post-processing step these were reduced to 4 3-clusters (figure 5(a)). The second experiment mined 5 3-clusters. In both experiments not all objects were clustered since not all objects qualified to form 3-clusters. As can be seen from figure 5 all 3-clusters that were mined are pure with respect to the class labels. Every cluster only contains objects belonging to one class. Furthermore both classes are represented almost equally by the 3-clusters. This result indicates that the 3-clusters con-stitute valid clusters in each of D 1 , D 2 and in the global dataset D 1  X  D 2 . Moreover the result also shows that the 3-clusters discovered not only reveal associations between concepts in D 1 and patterns in D 2 ; but also form valid and informative clusters with respect to the global set of items.
We presented an effective algorithm to mine 3-clusters across two vertically partitioned binary datasets. Our for-mulation of 3-clusters is very general and allows for a variety of overlapping, yet maximal clusters to be found across the two datasets. A mathematical formulation for measuring the quality of 3-clusters was also introduced. We demon-strated empirically with real-wold datasets that our algo-rithm outperforms naive methods of discovering 3-clusters (simply enumerating bi-clusters in one dataset using meth-ods such as CHARM or LCM and then filtering) by orders of mag-nitude. Furthermore we illustrated both theoretically and empirically that our method mines useful quality 3-clusters from the data.
