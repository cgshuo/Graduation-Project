 Mining cluster evolution from multiple correlated time-varying text corpora is important in exploratory text analytics. In this paper, we propose an approach called evolutionary hierarchical Dirichlet processes (EvoHDP) to discover interesting cluster evolution pat-terns from such text data. We formulate the EvoHDP as a series of hierarchical Dirichlet processes (HDP) by adding time depen-dencies to the adjacent epochs, and propose a cascaded Gibbs sam-pling scheme to infer the model. This approach can discover di ent evolving patterns of clusters, including emergence, disappear-ance, evolution within a corpus and across di ff erent corpora. Ex-periments over synthetic and real-world multiple correlated time-varying data sets illustrate the e ff ectiveness of EvoHDP on discov-ering cluster evolution patterns.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.3 [ Pattern Recogni-tion ]: Clustering; G.3 [ Probability and Statistics ]: Nonparamet-ric statistics ; H.2.8 [ Database Management ]: Database applica-tions X  Data mining Algorithms; Experimentation Multiple correlated time-varying corpora, clustering, mixture mod-els, Bayesian nonparametric methods, Dirichlet processes
Nowadays, we are surrounded by overwhelming quantities of textual materials from various heterogenous corpora (e.g., news, blogs) everyday. The themes in these corpora are usually similar, however, diversity also exists. For example, news typically has more discussions on society, politics, and economics than blogs which might focus more on personal life. Even within a corpus, the Figure 1: The cluster  X  X inancial crisis X  discovered by EvoHDP in 103,986 articles crawled from three types of web corpora: blogs , news and message boards . Each corpus is represented by a colored stripe. The height of a stripe is the proportion of the cluster in corresponding corpus. For each corpus, the top keywords of this cluster in each month are placed on the stripe, where the size of a keyword encodes its frequency in the cluster. The popularity of this cluster in each corpus varied over time. In addition, the cluster was first active in blogs and then became popular in news and message boards. popularity of themes may also vary over time, and some of them may first appear in blogs, and then spread to news and message boards. Fig. 1 shows a real example of an evolving document clus-ter about  X  X inancial crisis X  in three types of web corpora, including blogs, news and message boards.

To better understand the complex data, users not only want to examine the document clusters, but also want to discern the cluster evolution patterns over time and across corpora. Specifically, from multiple correlated time-varying corpora, it is desirable to discover the following patterns: (1) clusters within each corpus at each time epoch, (2) shared clusters among di ff erent corpora at each epoch, (3) evolving patterns of clusters within a corpus, and (4) evolving patterns of clusters across corpora over time.

However, it is challenging to learn cluster evolution patterns from such complex data. The first challenge is how to model the clus-ters both across di ff erent corpora and over time. On the one hand, we need a single integrated model for all corpora to set up a global bookkeeper of clusters, otherwise we can not easily discern the evo-lution of a cluster across corpora over time. On the other hand, di ff erent corpora may share some clusters while also having their distinctive clusters. Hence the commonality and diversity should be both reflected in the single integrated model. The second challenge is how to model the time dependencies in the multiple corpora set-ting. It is very common the themes of a corpus evolves slowly along time, and thus the clustering patterns of adjacent time epochs usually exhibit strong correlations. Incorporating these correlations into the model in the multiple corpora setting is nontrivial. The last challenge is how to determine the cluster numbers. In time vary-ing text data, a cluster may emerge and disappear. Consequently, the cluster numbers may change through time. It is awkward to require users to specify a cluster number at each time epoch for each corpus. Therefore a mechanism is preferred to automatically determine all the numbers of clusters.

The traditional clustering approaches deal with a single static data corpus. Hence the direct use of a general global clustering model on all data may fail to represent the diversity both over time and across di ff erent corpora. Beyond the classical clustering ap-proaches, the most recent e ff orts only focus on tackling two sub problems. One is learning from multiple correlated text corpora , which aims to discover the related content across di ff erent text cor-pora as well as the distinctive information in each corpus [22, 26, 17]. Another is learning from a time-varying data corpus , which aims to discover the evolving patterns in the corpus as well as the snapshot clusters at each time epoch [3, 8, 9, 1, 16]. Both types of approaches are not su ffi cient to tackle the above challenges.
To deal with above challenges, in this paper, we propose an evo-lutionary hierarchical Dirichlet process (EvoHDP) model, which extends the hierarchical Dirichlet process (HDP) [22] to a time evolving scenario. In EvoHDP, each HDP is built for multiple cor-pora at each time epoch, and the time dependencies are incorpo-rated into adjacent epochs under the Markovian assumption. Specif-ically, the dependency is formulated by mixing two distinct Dirich-let processes (DPs). One is the DP model for the previous epoch, and the other is an updating DP model. To infer the EvoHDP model, we also propose a cascaded Gibbs sampling scheme. The proposed EvoHDP model can e ff ectively discover cluster evolution patterns over time and across corpora. Moreover, the cluster num-bers can automatically be determined due to the infinity property of DP.
In this section, we briefly introduce three categories of work re-lated to this paper, including learning from multiple correlated data corpora, learning evolution patterns from a time-varying corpus, and some initial e ff orts involving multiple dynamic data. In the research of learning from multiple correlated data corpora, HDP is a milestone [22]. It extended DP [2, 23] to model multiple correlated data corpora. In HDP, each data corpus is modeled by an infinite DP mixture model, and the infinite set of mixing clusters is shared among all corpora. Later works [16, 6] relax the assump-tion of HDP and incorporate more correlations between di ff corpora. Besides HDP, other e ff orts [4, 19, 26, 17] on this research topic are devoted to the extensions of the Latent Dirichlet Alloca-tion (LDA) topic model [5]. However, none of them consider the problem of automatically determining the cluster / topic numbers. In fact, as pointed out in [22], HDP can be used as an LDA-based topic model, where the number of clusters can be automatically inferred from data. Therefore, HDP is more practical when users have little knowledge about the content to be analyzed.

In the research of learning evolutionary clusters from a time-varying corpus, evolutionary clustering [8, 9, 32, 1, 30, 31] is a new research topic. Evolutionary clustering aims to preserve the smoothness of clustering results over time, while fitting the data of each epoch. Among the above works, the approaches in [1, 30, 31] utilized DP to automatically determine the cluster numbers. In fact, incorporating time dependencies into DP mixture models is a hot topic in the research of Bayesian nonparametric [20, 10, 7, 16, 15, 14]. Moreover, some works have focused on extending LDA to dy-namic topic models [3, 27, 25]. We noted that even though the title of [16] is similar to this paper, it actually presents an evolutionary DP mixture model for a single dynamic corpus.

To the best of our knowledge, there are four works that seem-ingly involve multiple time-varying data but actually handle dif-ferent problems [28, 29, 11, 21]. Wang et al. [28] focused on de-tecting the simultaneous busting of some topics in multiple text streams. They did not concentrate on the evolving patterns but only the busting behavior of topics. Wang et al. [29] extended [28] to ex-tract common topics from multiple text streams. They regarded that the underlying models of all streams are the same except that time delays exist between di ff erent streams. Hence they adjusted the timestamps of of all documents to synchronize multiple streams and then learned a common topic model. Their assumption pays more attention to aligning topics of di ff erent streams and thus de-generates the topic diversity among di ff erent streams. Leskovec et al. [11] focused on tracking the spreading behaviors of short phrases (or  X  X emes X  ) across the web to represent news cycles. Although memes act as signatures of clusters / topics, they are not enough to represent clusters / topics as the context information is lost. Tang et al. [21] worked on the dynamic multi-mode network with several types of actors. They aimed to provide a partition for each type of actors at each time epoch. There are two typical fea-tures in their problem: (1) the data to be handled is relational data, i.e., linkages between actors are required; (2) the partitions over time should be on a same set of actors while relationships among them vary over time. None of the above four works attempted to discover the cluster evolution over time and across corpora. On the contrary, this is exactly the focus of this paper. In addition, none of them handled the problem of automatically determining the clus-ter / topic numbers, which is one of the major considerations of our work. In this section, we briefly introduce DP and HDP.

A DP can be considered as a distribution of a random proba-bility measure 1 G , and we write G  X  DP(  X  0 , G 0 ), where positive concentration parameter, and G 0 is a base measure . Sethu-raman [18] showed that a measure G drawn from a DP is discrete, by the following stick-breaking construction : The discrete set of atoms {  X  k }  X  k = 1 are drawn from the base mea-sure, and GEM (  X  0 ) refers to such a process:  X   X  k  X  Beta (1  X  =  X   X  the vector, which will be followed in the rest of the paper. a probability measure concentrated at  X  k . After observing draws  X  ,..., X  This posterior preserves the possibility of drawing a new distinct value from G 0 but puts more concentration on observed values. HDP uses multiple DPs to model multiple correlated corpora. In HDP, a global measure G 0 is drawn from a DP (  X , H ) , with concen-tration parameter  X  and base measure H . Then, a set of measures {
G the corpus j . Such a process is summarized as Given the global measure G 0 , G j  X  X  are conditionally dependent.
In general, we can regard the measure as a distribution. Figure 2: Graphical representation for HDP: circles denote random variables, oval nodes denote parameters, shaded nodes denote observed variables, and plates indicate replication. (a) HDP. (b) The stick-breaking construction of HDP. the following mixture model where F ( x |  X  ji ) is a distribution parameterized by  X  e.g., that of an exponential family. Eqs. (3) and (4) together define the hierarchical Dirichlet process mixture model 2 . The graphical representation for an HDP is described in Fig. 2(a).

Moreover, according to Eq. (1), G 0 has the form G 0 = where  X  k i . i . d .  X  H ,  X  GEM(  X  ) . Then it is shown in [22] that G can be constructed as which means that di ff erent corpora share the same set of distinct atoms [22]. This is the stick-breaking construction of HDP, and the corresponding graphical model is shown in Fig. 2(b).
In this section, we begin with the introduction of the EvoHDP model, and then show how to infer the model using a proposed Gibbs sampling technique.

We first introduce the data settings and some notations which are useful for subsequent discussions. There are J corpora varying over T time epochs. Considering the possibility that no data observed for some corpora at an epoch, we denote the number of corpora at epoch t is J t . At each epoch t , there are n t j data samples in corpus j , and we denote a data sample (e.g., a document 3 ) as x assume the underlying model to generate x t ji for corpus j at epoch t is an infinite mixture model where G t j = We call the density parameterized by a distinct atom  X  k as a mixing component, which describes a cluster. 4
We model the multiple correlated time-varying corpora as a se-ries of HDPs with time dependencies, as shown in the graphical presentation of Fig. 3(a). Specifically, at each time epoch t , we use an HDP to model the multiple correlated corpora at that epoch and then put time dependencies between adjacent epochs based on the Markovian assumption. To build an overall bookkeeping of compo-nents for all epochs, we let these HDPs share an identical discrete base measure G , and G is drawn from DP(  X , H ) with H as the base measure. We call G the overall measure . Moreover, for an HDP to We just call  X  X ierarchical Dirichlet process mixture model X  as HDP for short in this paper.
Generally a data sample can be quite di ff erent. For example, when using HDP to formulate LDA topic model, a data sample is a word.
When x represents a word, and F ( x |  X  ) is a multinomial distribution over a finite word vocabulary, the distribution F is often regarded as a  X  X opic X  [5].
 Figure 3: The graphical representation for the EvoHDP model. (a) The original representation. (b) The stick-breaking con-struction. model the corpora at epoch t , we use G t 0 to denote the global mea-sure at that epoch, and call it the snapshot global measure . Then, the local measure G t j for the j -th corpus at time epoch t is called the snapshot local measure . In this way, an EvoHDP has one more layer than the original HDP [22].

The key issue of EvoHDP is how to incorporate time depen-dencies between adjacent epochs. We introduce two types of de-pendencies into di ff erent layers in EvoHDP to model the di evolving manner. In the following we will interpret this model step-by-step to explain why we use this scheme and how it would be useful.

The first is the dependency of snapshot global measure G t G 0 . Since G of the global components in all corpora. We call this global time dependency .

The second is the time dependency within a corpus, i.e., the de-pendency of the snapshot local measure G t j on G t  X  1 j the measure for the components within corpus j at time epoch t , ponents within the corpus j . Then we call this intra-corpus time dependency.

In Fig. 3(a), we use dashed lines to represent the second type of dependencies, since in some cases (e.g. in HDP based LDA), there are no intra-corpus dependencies.

The generation process of EvoHDP is as follows. 1. Draw an overall measure G  X  DP(  X , H ). G plays a role of the overall component bookkeeping for all corpora at all epochs. 2. For each epoch t : overall measure G and the previous snapshot global measure G corpus j at epoch t is drawn according to the snapshot global mea-sure G t 0 and the previous snapshot local measure G t  X  1 component densities and generate the data samples: where F ( x |  X  t ji ) is a distribution parameterized by an exponential family.

Compared to the original HDP model, two levels of time depen-dencies are incorporated in by Eq. (6) and Eq. (7). When we set all w and v t j to zero, the EvoHDP is a three-layer HDP. Figure 4: The generation mechanism of G t + 1 j . (a) A part insight into the generation process of restaurant j at epoch t + 1 . (b) The generation of tables of restaurant j at epoch t + 1 .
Following the convention of DP related models [22, 24, 16], we will also provide other two perspectives and a restaurant-metaphor for the EvoHDP. They help us better understand the model and lay the foundations of the inference scheme introduced in Sec. 4.4.
According to the stick-breaking construction (Eq. (1)) of DP, we can write the explicit form of G : Consequently, according to Eqs. (6) and (5), G 0 has the form Similarly, we can also write the form of G t j as In this way, we obtain the stick-breaking construction for Evo-HDP, which is shown in Fig. 3(b), where z t ji is the index of the component emitting x t ji .

According to this perspective, the EvoHDP provides a prior in share the same infinite set of mixing components {  X  k }  X  ferences among these snapshot models lie in the mixing weights.
Based on the stick-breaking construction for a DP, if we continue to use the stick-breaking constructions to represent t j and from the two DPs in Eqs. (8) and (9), we obtain the hierarchical in-finite mixture model of EvoHDP. This perspective clearly interprets the generation mechanism of EvoHDP.

We begin with the metaphor following the Chinese restaurant franchise (CRF) for HDP [22]. A corpus j is called a restaurant , and a global atom k is called a dish . We use a day to refer to a time epoch. We focus on the generation of component indicator z which is drawn from F ( x |  X  z t
We first show the generation mechanism of each snapshot lo-cal measure G t + 1 j = in day t + 1. Since t + 1 j is drawn from DP(  X  t + 1 0 , Eq. (9), we can represent this DP using the stick-breaking construc-tion as which is illustrated in Fig. 4(a). We call  X  a table , then u Figure 5: The generation mechanism of G t + 1 0 . (a) A part insight into the generation process of the snapshot global measure. (b) The generation of meta-tables of the franchise at epoch t distribution on tables. We can explain Eq. (10) as a peculiar way of dish serving in a restaurant. First, waiters place infinite number table-dish menu  X  t + 1 j by waiters. Then, when a customer i enters menu u t + 1 j (with probability u t + 1 table. Consequently, the component indicator z t + 1 ji = over, t + 1 j plays the role of the local customer-dish menu , which indicates the customers X  preference on dishes in day t + 1 . We then explain how a dish is placed on a table by the waiter. From Eq. (10), we see that { k t + 1 j  X  }  X  i . i . d .  X  from the local table-dish menu  X  t + 1 j . Notice that  X  t v dish menu in this restaurant yesterday and t + 1 is the global fran-chise menu of current day recommended by the franchise manager. cal customer-dish menu t j with probability v t + 1 j while from current day X  X  global franchise menu t + 1 with probability 1  X  v t means in the franchise, a restaurant designs its localized menu by considering both yesterday X  X  local taste and current day X  X  franchise menu.

Obviously, it is possible that multiple tables have a same dish. In restaurant j in day t + 1, we denote the number of tables with dish k as T t + 1 jk . Then the total number of tables in restaurant j in day t + 1 is denoted as T t + 1 T jk tables, there are T yesterday X  X  local customer-dish menu t j , and T 0  X  t + 1 dishes are selected from current day X  X  global franchise menu shown in Fig. 4(b).
Then we show the generation mechanism of the snapshot global measure G t + 1 0 = mends the global franchise menu t + 1 to all restaurants. The proce-dure is explained in Fig. 5(a). Since t + 1 is drawn from DP( as shown in Eq. (8), t + 1 can also be represented using the stick-breaking construction as We call each m a metatable , then u t + 1 is a distribution on metat-ables. The manager has infinite number of empty metatables be-forehand and he selects a dish k t + 1 m for each metatable m from the metatable-dish menu  X  t + 1 of day t + 1 . Remind that when a waiter in restaurant j place a table  X  , with probability 1  X  v t dish k t + 1 j  X  according to the global franchise menu t need to select a metatable m t + 1 j  X  according to the waiter-metatable menu u t + 1 , and then the dish k t + 1 should select. Hence the global franchise menu t + 1 also plays the role of the waiter-dish menu , which indicates how a waiter selects dishes for tables.

The dishes on the metatables of the franchise in day t + 1 also come from two menus, i.e., the yesterday X  X  global franchise menu and an overall menu . The overall menu reflects common taste of the franchise. It is also possible that di ff erent metatables have a same dish and we denote the number of metatables with dish k as M k . Among the M whose dishes are selected from yesterdays X  franchise menu, and M k metatables whose dishes are selected from the overall menu
The hierarchical infinite mixture model and the restaurant fran-chise metaphor actually define a Gibbs sampling scheme for Evo-HDP. We can derive a cascaded Gibbs sampling procedure by se-quentially sampling following variables.
According to the generation of metatables introduced in Fig. 5(b), what are drawn from the overall measure G = dishes on the metatables of all days designed by the franchise man-ager. We denote the number of all these metatables with dish k as M G as M  X  = the count variables {M k } k , according to the property Eq. (2), the posterior of G is also a DP: where K is the number of distinct dishes on all metatables. Accord-ing to Sec. 5.2 of [22], G can be represented as This augmented representation reformulates original infinite vector to an equivalent finite one with length K + 1. Then is sampled from the Dirichlet distribution of Eq. (13).

Notice that in the following, G t 0 and G t j are also represented using above augmented representation as G Then t and t j are both represented as finite vectors 4.4.2 Sampling t and t According to Fig. 5(b), what are drawn from t include two parts. One part is the T 0  X  t  X  X  = restaurants of day t . The second part is the M t  X  t + 1 M  X  metatables of next day t + 1 . We call all these tables and metatables drawn from t as pseudo-tables . We denote the number of pseudo-tables with the same dish k as T t k , then T t M k . As count variables {T t k } k , the posterior of t is also a DP. Similar to Eq. (13), t can also be sampled from a Dirichlet distribution where  X   X  t =  X  t + T t  X  , and
The sampling of t j is similar to that of t : ( where  X   X  t 0 =  X  t 0 + N t j  X  , and
Given t j , sampling z t ji is straightforward: where k  X  { 1 ,..., K , u } . u refers to the index for the new compo-nent as introduced in Eq. (12). When k = u is sampled, we add a new component into the component bookkeeping. In Eq. (20), the first item is a prior p likelihood where X k  X  t , ji is the set of all the samples having been assigned to component k , other than x t ji .
As described in Sec. 4.4.1 and Sec. 4.4.2, the posterior of , and t j depend on the count variables T t jk and M t k .
Remind that the count variable T t j  X  = all tables in restaurant j at epoch t . These tables are occupied by dish k must have sat at a table with dish k . Hence  X  k , the n in [22], given N t jk , the table number T t jk can be sampled from a Chinese Restaurant Process (CRP) with dish k , it can be counted from the component assignments. To know N t jk , we also need to know T t  X  t + 1 jk , which depends on T turn. Thus the variable T t jk can be sampled from following recursive procedure: ( where p = v
Similarly, we obtain the recursive sampling procedure for M where q = w t
The concentration parameters of DPs, i.e.,  X  , {  X  t } and also be sampled by putting a vague gamma prior on them The sampling method is the same as that in [22]. Moreover, the time dependency parameters w t and v t j can be taken as controlling parameters or also sampled using the method in [16] by putting a Beta prior for them and sampling from the posterior.

According to the sampling method for groups of variables de-scribed above, there are recursive dependencies along hierarchies and time epochs. We follow the dependencies of di ff erent sets of variables and design a cascaded Gibbs sample scheme. The proce-dure is summarized in Algorithm 1.
 Algorithm 1 A cascaded Gibbs sampling scheme (one iteration) 1: for t = T to 1 do 5: end for 7: end for 8: Sampling concentration parameters  X , X  t and  X  t 0 . 9: Sampling according to Sec. 4.4.1. 10: for t = 1 to T do 14: end for 15: end for
We not only need the component assignments Z , but also the component parameters {  X  k } . As introduced in Eq. (21), been integrated out in the sampling process. Having assignments Z , we can obtain the posterior of a  X  k : This distribution is a  X  X lobal X  one conditioned on data of all cor-pora from all epochs. In textual data, it denotes a global component k in the entire textual collection. If we limite the data in a corpus j at epoch t , we also obtain the posterior of  X  k as a local component This is the component k as a local one in corpus j at epoch t .
In this section, we use a synthetic data set from mixtures of multi-nomial distributions to test our approach.
The data set consists of three time-evolving corpora covering 5 time epochs. Hence  X  t , J t , the number of corpora at epoch t , is 3. There are totally K = 8 components (dishes) involved in all corpora. Each dish k is a 2-dimensional multinomial distribution F ( x |  X  k ) = Multinomial( x ;  X  k ), with density where x is a D -dimensional nonnegative integer vector, and a D -dimensional nonnegative real vector with D = 2 and we set Each corpus j at epoch t is a uniform mixture of 3 tables, and each table  X  is associated with a dish indexed by k t j  X  . We denote the dish associated with table  X  as k t j  X  . Hence this local model can be represented by Then n t j samples are drawn from this mixture model, which com-pose the corpus j at epoch t . More details of the data set are shown in Tab. 1. In the conjunction area of  X  X ables ( k t j 1 , at row t and column j are the three dish indices to compose the lo-cal mixture model. In such a data set, di ff erent corpora overlap on some components, and the underlying models evolve over time.
We introduce several numerical criteria to evaluate two types of performances. The first type is the static performance on fitting training data and predicting held-out data. The second type is the temporal performance on preserving correlation between epochs, including the correlation within a corpus and that across di corpora.
Two criteria are used to evaluate the static performance, the nor-malized mutual information (NMI) and perplexity .

NMI measures coherence between the clustering assignments and the true category labels. A higher value on NMI indicates a bet-ter clustering result. For each corpus j at each epoch t , having com-ponent assignments for all data samples, we can compute the value NMI t j for the corpus. Then we use average value as the final result on the criterion of NMI.

Perplexity is a standard metric in information retrieval. We de-note the training set as X train and the held-out test set as X the per-sample perplexity of a model is defined based on the likeli-hood of  X  X enerating the test set given the training set X : where x t ji , test the i -th data sample in corpus j at time epoch t and n test is the size of test data set. In text modeling, to eliminate the fluctuation caused by the di ff erent lengths of documents, the perword-perplexity is often used instead, i.e., n test is the number of all the tokens int the test document collection. In this paper, we use log( perword-perplexity ) and call it LogPerp . A lower value on LogPerp indicates better prediction performance.
We define three types of temporal criteria to evaluate the time dependency and model smoothness between time epochs.

Intra-corpus temporal correlation (IntraCorr) is defined to describe the average correlation between adjacent epochs within a corpus:
Inter-corpora temporal correlation (InterCorr) is defined to describe the average correlation between di ff erent corpora of adja-cent epochs:
Global temporal correlation (GCorr) is defined to describe the correlation between global distributions G t 0 of adjacent epochs: Above three criteria measure the time dependencies from the as-Table 2: Results on the synthetic data: local components and sizes.
 pect of correlation. Higher values on them are favored when static performances are similar.

We also define another three types of criteria from the aspect of divergence between adjacent epochs X  distributions. On the con-trary, lower values on them are favored when static performances are similar. They are defined as follows.
 Intra-corpus temporal KL divergence (IntraKL) Inter-corpora temporal KL divergence (InterKL) Global temporal KL divergence (GKL) where KL(  X || X  ) is the KL-divergence between two distributions.
In all the temporal criteria defined above, the expectations are calculated by MCMC samples obtained during the cascaded Gibbs sampling process.
Except for EvoHDP, we also ran a three-layer HDP, which is just a special case of EvoHDP when all the time dependencies are re-moved. All the settings for EvoHDP and HDP are the same. The component model F ( x |  X  k ) was set to a multinomial distribution, and the base measure H was set to the conjugate prior for F , a symmetric Dirichlet distribution with parameter 0 . 5 . The vague gamma priors in Eq. (28) for the concentration parameters of Evo-HDP and HDP were set to be Ga (10 . 0 , 1 . 0). An identical set of randomly generated component assignments was used to initialize both EvoHDP and HDP. In addition, to study the impact of time dependency parameters w t and v t j , we also set w t = v controlling parameter and swept  X  in { 0 . 1 , 0 . 3 , 0 . call the EvoHDP model with w t = v t j =  X  as  X  X voHDP X , and call the EvoHDP model with w t and v t j also sampled during the infer-ence as  X  X voHDP-spl X .

For the Gibbs sampling procedure of both models, we ran 20 chains, and set the burn-in time as 1000 for each chain. After the burn-in, from each chain another 50 MCMC samples were pre-served, then we obtained 1000 MCMC samples to calculate the evaluation criteria. The models were evaluated via 10-fold cross validation, and all criteria results were averaged on the 10 rounds. Figure 6: Results on the synthetic data set: static performances, averaged on 10-fold cross validation. Figure 7: Results on the synthetic data set: temporal correla-tions, averaged on 10-fold cross validation. Figure 8: Results on the synthetic data set: temporal diver-gences, averaged on 10-fold cross validation.

For the legends in Figs. 6 X 9, the the red dashed lines with er-rorbars are the results of HDP, and the purple lines with circles are the results of EvoHDP-spl, and the black lines with down-triangles are the results of HDP. In addition, the blue dotted lines with up-triangles are true values.
 The static performances with di ff erent  X   X  X  are illustrated in Fig. 6 . In a wide range of  X  , EvoHDP achieves better results than HDP on NMI and LogPerp. The estimated component numbers are shown in Fig. 6(c). We see that HDP tends to fit data by splitting into more components, and EvoHDP seems to eliminate this phenomenon.
The temporal performances are illustrated in Figs. 7 and 8. The  X  X rue X  values on the correlation and divergence criteria are calcu-lated via the ground truth labels. EvoHDP achieves higher cor-relations and lower divergences than HDP, where larger  X  gives stronger dependency between adjacent epochs. However, when becomes larger than a threshold (e.g. larger than 0.7), it seems it hurts the correlation criteria between epochs.
 Tab. 3 and Tab. 2 further illustrate details of the clustering results. The two tables list a typical clustering result in one trial with 0 . 8 . HDP produces much more components, and lots of them are actually similar and should be merged together.
In this section, we report the experiments on a real online doc-ument collection. This data set consists of 103,986 text articles queried from a search engine, Boardreader 5 , in which the time stamps of the articlles are ranged from July 2008 to December 2008, using 20 financial companies X  names, e.g.,  X  X IG insurance X ,  X  X ank of America X ,  X  X tate Farm X , etc. All these articles come from three types of public websites, i.e., news, blogs and message boards. We used the Mallet [13] package to pre-process the data set. We removed the stop words and rare words (appearing less than 10 times in the whole collection). After that, the vocabulary size of this text data set was W = 77 , 999. Term frequencies were extracted to represent each article. We organized the data set into http: // boardreader.com / (a) LogPerp (b) InterKL (c) GKL (d) K Figure 9: Results on real bank data set, averaged on 10 rounds. Figure 10: Overview of the overall clusters in all text corpora of all epochs. Each colored stripe represents a cluster, whose height is the number of articles assigned to the cluster. For each cluster, the top keywords of this cluster in each month are placed on the stripe, where the size of a keyword is proportional to its frequency in the cluster. 6 epochs by months, and each epoch was set as a month Hence we obtained a data set including 3 corpora along the 6 epochs. We regarded each article (represented by a vector of term frequencies) as a draw from a multinomial distribution with dimension W , i.e., F ( x |  X  k ) is a multinomial distribution. The base measure H was set to the conjugate Dirichlet prior with symmetric parameter 0.5.
To compare EvoHDP with HDP using numerical criteria, we ran-domly sampled a smaller subset, which consisted of 5,353 articles with 19,420 distinct words as the vocabulary. Experimental set-tings and parameters were the same as those in Sec. 5.3. All the results were also averaged on 10-fold cross validation. The nu-merical results are shown in Fig. 9. Since there is no ground truth labels for the data set, NMI can not be calculated and only the re-sults of LogPerp are given. Limited to space, only two temporal criteria are provided here, i.e., InterKL, and GKL. Consistent with the results on toy data, EvoHDP achieves lower LogPerp values and higher correlations (low divergence values). The phenomenon is also observed here that HDP tends to split up into more compo-nents (Fig. 9(d)) to fit data and overlook the correlations.
To have a clear insight into the evolution patterns in the multiple correlated corpora discovered by EvoHDP , we further leveraged the time-based topic visualization proposed by Liu et al. [12] to visually illustrate the analysis results on all the 103,986 text articles in several di ff erent views.

First, we present the overview of all text corpora including news, blogs and message boards, which is shown in Fig. 10. We can see five active clusters about  X  X inancial crisis X ,  X  X lection X ,  X  X rade mar-ket information X ,  X  X arclays premier league X  and  X  X ank related X . These five clusters are all finance related. For example, the  X  X lec-tion X  cluster tells about Obama and Maccain X  X  debate on bailout agreement, and the  X  X inancial crisis X  cluster tells about the bankrupt events of largest companies such as Lehman and AIG.

Then, we present the clusters of di ff erent corpora, i.e. news, blogs and message boards respectively. In Figs. 11 (a),(b), and (c), we present the absolute values of document numbers in each cluster at each epoch. In Figs. 11 (d),(e), and (f), we present the mixing proportions of clusters in a corpus at each epoch.

From these figures we find several interesting patterns. (1) The three corpora are similar but diversity exists . All of the three corpora are interested in  X  X inancial crisis X . However, blogs and message boards focus more on  X  X lection X  than news; news and message boards focus more on  X  X arclays premier league X ; and news focuses more on general  X  X ank related information X  and  X  X rade market information X . This may be because news tends to report more time-sensitive events, while blogs and message boards like to have a deep discussion on a particular event or the progress of an a ff air. (2) Correlations over time and across di ff erent corpora . For each corpus, the clusters change smoothly along time. Since we added the time dependencies between adjacent epochs, the cluster content does not change too much. Moreover, there are some clus-ters shared across di ff erent corpora. (3) Cluster emerging and disappearing . The cluster  X  X inancial crisis X  emerged in news and message boards in September due to the bankrupt of two financial companies, Lahman and AIG. The cluster  X  X lection X  emerged in blogs and message boards in September 2008 due to the televised presidential debate between Obama and McCain. We also note that there emerged a strange cluster ( X  X ds. / noise X ) in blogs from Octo-ber, with keywords like  X  X ovies X ,  X  X ideos X ,  X  X ex X  etc. . When we digged into the content, we found that the articles were full of noisy advertisement information. This may be because the blogs became very hot after September 2008, and then more and more automatic robots came to the site and presented nonsense information. In ad-dition, we also observe that the cluster  X  X arclays premier league X  disappeared in December, after Barclays determined to renew its sponsorship of the premier league and a new league season started.
To better compare the evolving behaviors of di ff erent clusters, we select three clusters from di ff erent corpora in the same view. The three clusters  X  X arclays premier league X ,  X  X lection X , and  X  X i-nancial crisis X  are shown in Fig. 12(b), Fig. 12(a) and Fig. 1, re-spectively. From the three figures, besides clearer witness of the previous three patterns, another two patterns are also discovered. (4) Cluster evolution within a corpus . First, the strength of a clus-ter in corresponding corpus varies overtime, which can be clearly observed in all the three figures. Second, the most frequent key-words of a cluster also vary over time, reflecting the evolving of the content of a cluster. Taking the cluster  X  X lection X  (Fig. 12(a)) as an example, in blogs, in August, the keywords  X  X bama X ,  X  X ush X ,  X  X residential X ,  X  X emocratic X  indicated the normal features before an election. However, in September,  X  X cCain X  appeared as the hottest keywords as Republicans nominated John McCain for pres-ident in September 4th. Besides, crisis related keywords such as  X  X risis X ,  X  X ig X ,  X (wall) street X  became hot due to the break-out of financial crisis in this month. More such features can also be ob-served in later months and other clusters. (5) Cluster evolution across di ff erent corpora . In Fig. 1 and Fig. 12(a), it is clear that both  X  X risis X  and  X  X lection X  clusters were first active in blogs, and then became popular in news and message boards. row, the width is the number of articles assigned to the cluster, i.e., n , while in the second row, it is the proportion of the cluster at that epoch, i.e.,  X  t jk .
Figure 12: Comparison of a cluster in di ff erent corpora. We propose an evolutionary hierarchical Dirichlet process (Evo-HDP) model to mine cluster evolution from multiple correlated time-varying corpora. EvoHDP extends original HDP by incorpo-rating time dependencies into a series of HDPs. A cascaded Gibbs sampling scheme is proposed to infer the model. Our approach can discover cluster emergence, disappearance, and evolution within a corpus and across di ff erent corpora. In addition, the cluster num-bers for all corpora at all epochs are inferred from data rather than specified.

Experiments on a synthetic data set and a real-world financial related web data set validated the e ff ectiveness of our approach. Compared to the original HDP, EvoHDP exhibits better predicting ability and stronger correlations across corpora over time on both data sets. In addition, on the real financial related web data, we observed that the cluster evolution patterns, emergence, disappear-ance, evolution within a corpus and across corpora, can be e tively discovered by EvoHDP.
The authors Jianwen Zhang and Changshui Zhang were sup-ported by National Natural Science Foundation of China (NSFC, Grant No. 60835002). We would like to thank Weihong Qian and Furu Wei for their help on preparing the visualization results. We also thank all the reviewers for the suggestions to improve the pa-per.
