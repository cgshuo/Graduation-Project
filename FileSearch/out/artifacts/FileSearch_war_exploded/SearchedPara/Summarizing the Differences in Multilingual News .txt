 There usually exist many news articles written in different languages about a hot news event. The news articles in different languages are written in different ways to reflect different standpoints. For example, the Chinese news agencies and the Western news agencies have published many articles to report the same news of  X  X iu Xiaobo X  X  Nobe l Prize X  in Chinese and English languages, respectively. The Chines e news articles and the English news articles share something about the news fact in common, but of multilingual news summarization for the purpose of finding and summarizing the major differences between the news articles about the same event in the Chinese and English languages. We propose a novel constrained co-ranking (C-CoRank) method for addressing this special task. The C-CoRank method adds the constraints between the difference score and the common score of each sentence to the co-ranking process. Evaluation results on the manually labeled test set with 15 news topics show the effectiveness of our proposed method, and the constrained co-ranking method can outperform a few baselines and the typical co-ranking method. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  abstracting methods ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  text analysis Algorithms, Experimentation, Design. Multi-document summarization, multilingual summarization, constrained co-ranking. With the advent of a hot news event, many news articles are written and published by different news agencies in different languages. The news articles in different languages are usually written in different ways. The conten ts in these news articles are overlapping with each other, but th ey usually cover a few different aspects about the event (or they have different focuses), because the agencies write the article s with different backgrounds, standpoints and purposes. For example, given the news of  X  X iu Xiaobo X  X  Nobel Prize X , in addition to the common content of event introduction, most Chinese news articles aim to reflect Chinese policies and standpoints about the event, but most English news articles aim to reflect the western countries X  understanding language will be biased. The summa ry of the diffe rences between help people understand the event in a more comprehensive and unbiased way. We focus on summarizing the major differences in multilingual news (Chinese vs. English) in this study. Given a Chinese document set and an English docume nt set about the same event, a Chinese summary is required to be created to reflect the major differential aspects in the Chinese news articles, and an English summary is required to be created to reflect the major differential aspects in the English news articles. In other words, the content in the Chinese summary is not the focus of the English documents, and the content in the English summary is not the focus of the Chinese documents. Our task can be considered as an extension of the update summarization task [7] in the following two ways: 1) our task aims to simultaneously create two summaries for two document sets to reflect the major differences in the two sets, while the update summarization task aims to create a summary for one document set after reading the other document set; 2) our task summarization task is defined in a monolingual setting. In this study, we present two graph-based ranking methods for addressing this special summarizat ion task. The first method is named CoRank, which can simu ltaneously extract a Chinese summary and an English summary from the two document sets in a unified graph-based ranking process. The second method is named Constrained CoRank (C-CoRank), which improves the CoRank method by introducing a new factor (common score) for each sentence and adding important constraints into the graph-based ranking process. We ma nually labeled the Chinese and English difference summaries for 15 news topics, and the experimental results on the data set show the effectiveness of our proposed method. The C-CoRank method can significantly outperform the baselines and the CoRank method. The rest of this paper is organized as follows: Section 2 reviews related work. Our proposed method is presented in Section 3. Sections 4 and 5 show the experiments and results. Lastly, we conclude this paper in Section 6. Document summarization methods can be extraction-based, abstraction-based or hybrid methods. We focus on extraction-based methods in this study, and the methods directly extract summary sentences from a document or document set by ranking the sentences in the document or document set. In the task of single document summarization, various features have been investigated for ra nking sentences in a document, including term frequency, senten ce position, cue words, stigma words, and topic signature [16, 21]. Machine learning techniques have been used for sentence ra nking [2, 13]. Litvak et al. [20] summarization based on the lin ear optimization of several sentence ranking measures using a genetic algorithm. In recent years, graph-based methods have been proposed for sentence ranking [9, 23]. Other methods include mutual reinforcement principle [33, 38]. In the task of multi-document summarization, the centroid-based method [28] ranks the sentences in a document set based on such features as cluster centroids, position and TFIDF. NeATS [17] makes use of new features such as topic signature to select important sentences. Machine Learning techniques have also been used for feature combining [37]. Themes (or topics, clusters) discovery in documents has been us ed for sentence selection [11]. Nenkova and Louis [26] investigate the influences of input difficulty on summarization performance. Celikyilmaz and Hakkani-Tur [6] formulate extrac tive summarization as a two-step learning problem by building a generative model for pattern discovery and a regression model fo r inference. Aker et al. [1] summary up to a given length, and they propose a discriminative training algorithm for directly ma ximizing the quality of the best summary. Graph-based methods have also been used to rank sentences for multi-document summarization [24, 30, 32, 34]. Update summarization is an emer ging new summarization task in the very recent years. Piloted in DUC2007, the task aims to create a short summary of a set of news articles, under the assumption that the user has already read a given set of earlier articles [7]. The update summary is expected to be relevant to the given query or topic, and contain salient and new content in the current articles. Many existing multi-document summarization methods can be adapted for this task by considering the information in the earlier documents. For example, Fisher and Roark [10] present a supervised sentence ranking approach for extractive update summarization. Nastase et al. [25] present an update summarization technique based on query expansion with encyclopedic knowledge and activation spreading in a large document graph. Boudin et al. [3] revise the MMR criteria for update summarization. Li et al. [15] propose a novel graph-based sentence ranking algorithm named PNR 2 for update summarization, and it models both the positive and negative mutual reinforcement between sentences in the ranking process. Du et al. [8] propose a novel approach based on manifold ranking with sink points for update summa rization. Wang and Li [2010] propose a new summarization met hod based on an incremental hierarchical clustering framework to update summaries when a new document arrives. In [36], the task is a little bit different from the above update summarization task, and it aims to address the problem of incremental summarization of a periodically updated document set in a timely way. As mentioned earlier, the update summarization task is closely the update summarization task by requiring creating difference summaries for two document sets in a multilingual setting. Though there is no common definition for contrastive comparative summary for two comparable document sets. Most previous work focuses on generating comparative summaries for opinionated texts, because the opin ionated texts usually contain a few explicit comparable aspects. For example, Kim and Zhai [12] formulate the problem of contrastive opinion summarization as an optimization problem and pres ent two general methods for generating a comparative summary using the framework. Paul et al. [27] present a two-stage approach with an unsupervised probabilistic model and Comparative LexRank to summarizing multiple contrastive viewpoints in opinionated text. The most closely related work is [22, 35] . Mani and Bloedorn [22] propose an approach based on term graph to highlight the similarities and differences in related documents. They first find the common and distinct terms and then pinpoint the similarities and differences based on the terms and align text extracts across documents. Wang et al. [35] study the problem of summarizing the differences between document groups in a monolingual setting. They propose a discriminative senten ce selection method to extract the most discriminative sentences from e ach document group. Other related work includes correlated summarization [39], which aims to summarize the correlation or similarity for a pair of news articles. Multilingual summarization [19, 29] aims to create a summary in a specific language from multiple documents in multiple languages, which is very different from our task of summarizing the differences in multilingual documents. Cross-lingual document summarization [5, 14, 31] aims to produce a summary in a different target language for a set of documents in a source language, which is also very different from our task. One popular method first translates the source documents into the corresponding documents in the target language, and then extracts summary sentences based on the information on the target side. The other popular method first extracts summary sentences from the source documents based on the information on the source side, and then translates the summary sentences into the corresponding summary sentences in the target language. The task of summarizing the differences in multilingual news is defined as follows: Given a set of Chinese news articles and a set of English news articles about the same news topic, the task aims to extract a few sentences from the Chinese news articles to reflect the important but differential aspects about the topic, and also extract a few sentences from the English news articles to reflect the important but differ ential aspects about the topic. The content of the Chinese summary is not the focus of the English news articles, and the content of the English summary is not the focus of the Chinese news articles. In order to address the above summarization task, we present a co-ranking method (CoRank) and a constrained co-ranking method (C-CoRank) within the graph-based ranking framework. The CoRank method is inspired by the recent graph-based ranking methods for document summarization [15, 30, 33], and it can extract the difference summaries from the two document sets in a unified ranking process. The C-CoRank method is an improvement of the CoRank method by incorporating a new factor into the graph model, and adding important constraints in the ranking process. In this method, the Chinese sentences and the English sentences are simultaneously ranked in a unified graph-based algorithm. We assign each sentence a difference score to indicate how much the sentence contains important but differential information. The difference score of each Chinese sentence relies not only on the Chinese sentences in the Chinese document set, but also on the English sentences in the English document set. Similarly, the difference score of each English sentence relies not only on the English sentences in the Englis h document set, but also on the Chinese sentences in the Chinese document set. In particular, the CoRank method is based on the following four assumptions: Assumption 1 : The difference score of a Chinese sentence would be high if the sentence is heavily correlated with other Chinese sentences with high difference scor es in the Chinese document set. Assumption 2 : The difference score of a Chinese sentence would be high if the sentence is very unrelated to the English sentences with high difference scores in the English document set. Assumption 3 : The difference score of an English sentence would be high if the sentence is heavily correlated with other English sentences with high difference scor es in the English document set. Assumption 4 : The difference score of an English sentence would be high if the sentence is very unrelated to the Chinese sentences with high difference scores in the Chinese document set. Formally, given an English document set D en and a Chinese document set D cn , let G =( V en , V cn , E en , E cn graph for the sentences in the two document sets. V 1  X  i  X  m } is the set of English sentences. V cn ={ s cn of Chinese sentences. m , n are the sentence numbers in the two document sets, respectively. Each sentence is represented by a term vector in the VSM model 1 . The term weight is computed by English sentences in the English document set. E cn to reflect the relationships between the Chinese sentences in the relationships between the Engl ish sentences and the Chinese sentences. Based on the graph representation, we compute the following matrices to reflect the three kinds of sentence relationships: M relationships between the English sentences. Each entry in the matrix corresponds to the cosine similarity between the two English sentences. For Chinese sentences, a term means a Chinese word after applying Chinese word segmentation. Then M en is normalized to en M equal to 1. M relationships between the Chinese sentences. Each entry in the matrix corresponds to the cosine similarity between the two Chinese sentences. Then M cn is normalized to cn M equal to 1. M relationships between the Engl ish sentences and the Chinese sentences. Each entry M encn ij in the matrix corresponds to the the sentences in different languages. In this study, the distance value is computed by fusing the following two distance values: the normalized Euclidean distance between the English sentence s and the translated English sentence s transen j normalized Euclidean distance between the translated Chinese sentence s transcn i for s en i and the Chinese sentence s geometric mean of the two values as the difference weight. The normalized Euclidean distance between two sentences ( s s ) is computed by normalizing the Euclidean distance between the two term vectors ( i s of the vectors. Then M encn is normalized to encn M ~ to make the sum of each row equal to 1. M make the sum of each row equal to 1. We use two column vectors u =[ u ( s cn i )] n  X 1 and v =[ v ( s denote the difference scores of the Chinese sentences and the English sentences, respectively. The above four assumptions can be rendered as follows: We use the Google Translate service for automatic Chinese-to-English and English-to-C hinese sentence translation. 
The normalized Euclidean performs better than the cosine-based distance in our study. After fusing the above equations, we can obtain the following iterative forms: And the matrix form is: where  X  and  X  specify the relative contributions to the final difference scores from the information in the same language and the information in the other language and we have  X  +  X  =1. For numerical computation of the scores, we can iteratively run the two equations until convergen ce. Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any sentences falls below a given threshold. In order to guarantee the convergence of the iterative form, u and v are normalized after each iteration. After we obtain the difference scores u and v for the Chinese and English sentences, we apply the same greedy algorithm [34] for redundancy removing in each la nguage. Finally, a few highly ranked Chinese sentences are selected as the difference summary for the Chinese document set, and a few highly ranked English sentences are selected as the difference summary for the English document set. This method improves the CoRank method by introducing a new factor for each sentence and adding strict constraints in the ranking process. In addition to the difference score, we introduce a common score for each sentence to indicate how much a sentence contains important and common information in the two document sets. Similarly, the common scores of the sentences can be computed based on the following four assumptions: Assumption 5 : The common score of a Chinese sentence would be high if the sentence is heavily correlated with other Chinese sentences with high common scores in the Chinese document set. Assumption 6 : The common score of an English sentence would be high if the sentence is heavily correlated with other English sentences with high common scores in the English document set. Assumption 7 : The common score of a Chinese sentence would be high if the sentence is heavily correlated with the English sentences with high common scores in the English document set. Assumption 8 : The common score of an English sentence would be high if the sentence is heavily correlated with the Chinese sentences with high common scores in the Chinese document set. In addition to the above assumptions, we assume that the difference score and the common score of each sentence is mutually exclusive. Assumption 9 : The sum of the difference score and the common score of each sentence is fixed to a particular value. The above assumption is reasonable because if a sentence is a good sentence to be selected into the difference summary, it is not suitable to be selected into the common summary, and vice versa. This assumption can be used to ad just the inappropriately assigned difference scores for some sentences. In order to compute the common scores for the sentences, we need to compute the following new matrices: W relationships between the English sentences and the Chinese sentences. Each entry W encn ij in the matrix corresponds to the similarity value between the English sentence s en i and the Chinese sentence s cn j . Similarly, the similarity value is computed by fusing the following two values: the cosine similarity value between the English sentence s en i and the translated English sentence s s , and the cosine similarity value between the translated Chinese sentence s transcn i for s en i and the Chinese sentence s geometric mean of the two values as the affinity weight. Then W encn is normalized to encn W ~ to make the sum of each row equal to 1. W make the sum of each row equal to 1. We use two column vectors y =[ y ( s cn i )] n  X 1 and z =[ z ( s denote the common scores of the Chinese sentences and the English sentences, respectively. Based on the assumptions 5-8, we can similarly obtain the following iterative forms: And the matrix form is: In order to simplify the parameter tuning, we use the same parameters of  X  and  X  as in the CoRank algorithm, and we have  X  +  X  =1. Till now the common scores and the difference scores are computed by using the CoRank algorithm separately. Based on assumption 9, we can add the following constraints at each iteration: equally important in the document sets. For example, the constraint value for a trivial sentence should be smaller than that for a salient sentence. In this study, we use the generic saliency score of each sentence as the constraint value for the sentence. The generic saliency score of a Chinese sentence is computed by using Chinese document set. It can be formulated in a recursive form as in the PageRank algorithm: where  X  is the damping factor usually set to 0.85, as in the PageRank algorithm. The generic saliency score of an English sentence is computed based on en M ~ in a similar way. In order to add the constraints to the ranking process, all the difference and common scores are initialized to 1, and the following steps are iteratively performed: 1) Perform the following two equations to compute the difference scores of the sentences: 2) Perform the following two equations to compute the common scores of the sentences: 3) Normalize the difference score and the common score of each sentence to meet the constraints (  X  ,  X  are temporary vectors): Note that the difference scores and the common scores are linearly normalized in the third step. For numerical computation of the scores, we can iteratively run the above steps until convergence or the iteration count reaches a preset number. and English sentences, and we apply the same greedy algorithm [34] for redundancy removing in each language. The difference summaries in the two languages are produced by selecting the highly ranked sentences, respectively. There is no benchmark dataset for difference summarization of multilingual news, so we built the evaluation dataset as follows: We first selected 15 hot news topics (e.g.  X  X 20 summit in 2010 X ,  X  X orth Korea X  X  attack on Sout h Korea X ,  X  X iu Xiaobo's Nobel Prize X , etc.), and then collected a few news articles about each topic in the Chinese and English languages by using both the Chinese and English versions of Google News Search topic, we selected the articles in the two languages within the same period. We collected the news articles published by major Chinese and Western news agencies . The statistics of the corpus are summarized in Table 1. The articles were first split into sentences. The Chinese sentences were automatically translated into English sentences by using Google Translate 4 , and the English sentences were automatically translated into Chinese sentences by using Google Translate . All the Chinese sentences (including the translated Chinese sentences) were segmented into words by using the Stanford Chinese Word Segmenter 5 . Two graduate students were employed to manually label the reference summaries for the 15 topics. Each student was asked to read both the Chinese and English documents for each topic, and then selected five Chinese senten ces and five English sentences as reference difference summaries for the two document sets. The labeling process lasted two weeks. Finally, there were totally 30 Chinese reference summaries and 30 English reference summaries. The Chinese reference summaries were segmented into words by using the same word segmentation tool. The English Google News is http://news.google.com/ The Chinese Google News is http://news.google.com.hk/ http://transl ate.google.com/ http://nlp.stanford.edu/ software/segmenter.shtml In the experiments, the summary length was fixed to five sentences in both languages. We used the ROUGE-1.5.5 [18] toolkit for comparing the system summaries with the reference summaries, which has been wide ly adopted by DUC and TAC for automatic summarization evaluation. It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. We showed the most popular two ROUGE F-measure scores in the experimental resu lts: ROUGE-2 (bigram-based) and ROUGE-SU4 (based on skip bigram with a maximum skip distance of 4). For Chinese summaries, we reported two types of ROUGE scores: character X  X ased and word-based. Character-based ROUGE scores were computed based on Chinese characters, and word-based ROUGE scores were computed based on Chinese words after using word segmentation. Character-based evaluation will not be influenced by the word segmentation performance, but Chinese character is less mean ingful than Chinese word. The CoRank and C-CoRank methods are compared with the following three baselines: Centroid : This baseline uses the centroid-based method [28] to compute the saliency scores of the sentences in each language separately. The saliency score of each sentence is computed by linearly combining the following three features: the cosine similarity between the sentence and the whole document set, the cosine similarity be tween the sentence a nd the first sentence within the same document, an d the position-based weight. The highly ranked sentences are selected into the difference summary after removing redundancy. Note that this baseline does not make any use of the cross-lingual information between the two document sets. Centroid++ : This baseline is an improvement of the centroid-based method by integrating the feature of the similarity between each sentence and the sentences in the other language. The difference score of each sentence is the subtraction of the centroid-based weight and the cross-language similarity value. MMR++ : This baseline uses the upda te summarization method in [3]. It revises the MMR criteria [4] by incorporating the cross-language sentence similarity. Table 2 shows the comparison re sults for Chinese difference summaries, and Table 3 shows the comparison results for English difference summaries. The parameter value of  X  for the CoRank and C-CoRank methods is simply set to 0.5 without tuning. We can see that both the CoRank and C-CoRank methods can significantly outperform the base lines methods over all metrics in both languages. The C-CoRank method can significantly outperform the CoRank method ove r most metrics. The results demonstrate the good effectiveness of the proposed C-CoRank method. The reason is that the introduction of the new factor of common score and the constraints can make the computation of the difference scores more reliable. Centroid 0.01589 0.02676 0.08808 0.08372 Centroid++ 0.01393 0.02943 0.07990 0.07823 MMR++ 0.00959 0.02215 0.06364 0.07604 (* indicates that the improvement over the three baselines are all improvement over the CoRank me thod is statistically significant by using t-test.) performance of our proposed method, we present the performance curves of the CoRank and C-CoRank method over different metrics when  X  ranges from 0.1 to 0.9. Figures 1-4 show the performance curves for Chinese difference summaries. Figures 5-6 show the performance curves for English difference summaries. Seen from the figures, the performance values of the CoRank method first increase with  X  , and then reach the peak values when almost do not change any more. The results demonstrate that the CoRank method relies more on the monolingual sentence links and the cross-language sentence links do not contribute much as expected. By analysis of the results, the CoRank method has the potential to select some sentences containing much common information into the summaries, which is not very appropriate for difference summarization. However, the performance values of the C-CoRank method first The C-CoRank method can well incorporate both the monolingual information and cross-language information for the summarization task. Overall speaking, the C-CoRank method can outperform the CoRank method over almost all the metrics when  X  is set within a wide range. The results further de monstrate the robustness of the C-CoRank method. The use of the constraints in the C-CoRank method makes the difference score and the common score for each sentence mutually exclusive, an d thus the difference score for a sentence containing much common information will not be high, when the common score for the sentence is high. 
Fig.2. Word-based ROUGE-SU4(F-measure) vs.  X  for Chinese 
Fig.3. Character-based ROUGE-2(F-measure) vs.  X  for Chinese 
Fig.6. ROUGE-SU4(F-measure) vs.  X  for English difference distance/similarity weights for Chinese difference summaries distance/similarity weights for English difference summaries In the above experiments, the cross-language sentence distance weight in matrix M encn and the cross-language sentence similarity weight in matrix W encn in the C-CoRank method are computed based on the geometric mean of the Chinese-side value and the English-side value. In order to investigate whether the two values really contribute to the final performance, we compare the original weight computation method ( ENCN ) with the following two methods using only one-side value: The first method ( EN ) uses only the English-side value, and the weights in the two matrices are computed as follows: The second method ( CN ) uses only the Chinese-side value, and the weights in the two matrices are computed as follows: The comparison results for the Ch inese summaries and the English summaries are shown in Figures 7 and 8, respectively. We can see that the original weight computation method (ENCN) can outperform the two methods (E N and CN) using only one-side value, and both the Chinese-side information and the English-side information contribute to the final performance. We also observe that the English-side value is more reliable than the Chinese-side value for computing the cross-language sentence similarity or distance, and the reason lies in that the step of Chinese word segmentation may introduce some noise for the distance/similarity computation. Finally, we show one running example for the topic of  X  X iu Xiaobo's Nobel Prize X . The differ ence summaries produced by the C-CoRank method are given below. The manual English translation of the Chinese summary is also given. We can see that the Chinese summary focuses on the opposition and accuse of Liu Xiaobo's Nobel Prize, while the English summary focuses on the support of Liu Xiaobo's Nobel Pri ze and China X  X  arrest of Liu Xiaobo. The difference summaries reflect the different standpoints of the Chinese news agencies an d the Western news agencies. The Chinese difference summary is as follows:  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  .  X  X  X  X  X  X  X  X  8  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  (Nobel Peace Prize is a political  X  X eward X  to Liu Xiaobo by the 
West. Norwegian Nobel Committee awarded the Nobel Peace Prize to Liu Xiaobo in this year, which once again exposed the double standards of Western countries. Nobel Committee awarded this year's Nobel Peace Prize to the Chinese "dissident" -Liu Xiaobo on day 8. To date, the Nobel Peace Prize has been awarded to two Chinese persons: one is the Dalai Lama and the other is Liu 
Xiaobo. Norwegian Nobel Committee made the decision for ideological bias and political needs, and the Nobel Peace Prize will become a political tool of some Western forces, and it seriously undermined the credibility of the Nobel Peace Prize, but also tarnished the honor of Mr. Nobel.) The English difference summary is as follows: 
As the West applauds Liu Xiao bo's Nobel, China sees another attempt to impose Western values on it. Norway's Nobel Peace 
Prize committee has done the right thing in awarding this year's prize to Chinese dissident Liu Xiaobo. Liu was represented at 
Friday's Nobel ceremony by an empty chair because China would not release him from prison -only the fifth time in the 109-year history of the prize that the winner was not in attendance. It tried to bully the Nobel committee into not awarding Mr. Liu this year's 
Nobel Peace Prize. China has prohibited Liu and his family members from leaving China to attend Friday's ceremony in Oslo. 
Two days after the prize was ann ounced, Mr Liu's wife, Liu Xia, met with him at the prison in northeastern China where he is serving his sentence, but she was escorted back to Beijing and placed under house arrest, a human rights group said. In this paper we address a novel summarization problem of summarizing the differences in multilingual news. We present the CoRank and the C-CoRank methods. In addition to the difference score of each sentence, the C-CoRank method incorporates the common score of each sentence into the ranking process by adding strict constraints. Evaluation resu lts on the manually labeled test set demonstrate the effectiveness and robustness of the C-CoRank method. In future work, we will adapt the proposed C-CoRank method for the update summarization task to further investigate its robustness. We will also summarize the differences between the news articles written by different news agencies in a monolingual setting. In addition to two languages, we will extend the summarization framework to summarize the differences in three or more languages. This work was supported by NSFC (60873155), Beijing Nova Program (2008B03) and NCET (NCET-08-0006). We thank the anonymous reviewers for their helpful comments. [1] A. Aker, T. Cohn, and R. Gaizauskas. Multi-document [2] M. R. Amini, P. Gallinari. The Use of Unlabeled Data to [3] F. Boudin, M. El-B  X  ze, J.-M. Torres-Moreno. The LIA [4] J. Carbonell and J. Goldstein. The use of MMR, [5] G. de Chalendar, R. Besan X on, O. Ferret, G. Grefenstette, and [6] A. Celikyilmaz and D. Hakkani-Tur. A hybrid hierarchical [7] H. T. Dang and K. Owczarzak. Overview of the TAC 2008 [8] P. Du, J. Guo, J. Zhang, X. Cheng. Manifold ranking with [9] G. ErKan, D. R. Radev. LexPageRank. Prestige in [10] S. Fisher and B. Roark. Query-focused supervised sentence [11] S. Harabagiu and F. Lacatusu. Topic themes for [12] H. D. Kim and C. Zhai. Generating comparative summaries [13] J. Kupiec, J. Pedersen, F. Chen. A.Trainable Document [14] A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. Och, E. [15] W. Li, F. Wei Q. Lu and Y. He. PNR 2 : Ranking sentences [16] C. Y. Lin, E. Hovy. The Automated Acquisition of Topic [17] C..-Y. Lin and E.. H. Hovy. From Single to Multi-document [18] C.-Y. Lin and E.H. Hovy. Automatic Evaluation of [19] C.-Y. Lin, L. Zhou, and E. Hovy. Multilingual summarization [20] M. Litvak, M. Last, and M. Friedman. A new approach to [21] H. P. Luhn. The Automatic Crea tion of literature Abstracts. [22] I. Mani and E. Bloedorn. Summarizing similarities and [23] R. Mihalcea, P. Tarau. TextRank : Bringing Order into Texts. [24] R. Mihalcea and P. Tarau. A language independent algorithm [25] V. Nastase, K. Filippova, S. P. Ponzetto. Generating update [26] A. Nenkova and A. Louis. Can you summarize this? [27] M. J. Paul, C. Zhai, and R. Girju. Summarizing contrastive [28] D. R. Radev, H. Y. Jing, M. St ys and D. Tam. Centroid-based [29] A. Siddharthan and K. McKeown. Improving multilingual [30] X. Wan. Towards a unified approach to simultaneous [31] X. Wan, H. Li and J. Xi ao. Cross-language document [32] X. Wan and J. Yang. Multi-document summarization using [33] X. Wan, J. Yang and J. Xiao. Towards an Iterative [34] X. Wan, J. Yang and J. Xiao. Manifold-ranking based [35] D. Wang, S. Zhu, T. Li, and Y. Gong. Comparative document [36] D. Wang, T. Li. Document update summarization using [37] K.-F. Wong, M. Wu and W. Li. Extractive summarization [38] H. Y. Zha. Generic Summariza tion and Keyphrase Extraction [39] Y. Zhang, X. Ji, C.-H. Chu, and H. Zha. Correlating 
