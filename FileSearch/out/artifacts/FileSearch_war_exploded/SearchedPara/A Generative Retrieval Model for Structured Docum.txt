
Structured documents contain elem ents defined by the author(s) and annotations assigned by other pe ople or processes. Structured documents pose challenges for proba bilistic retrieval models when there are mismatches between the structured query and the actual structure in a relevant document or erroneous structure introduced by an annotator. This paper make s three contributions. First, a new generative retrieval model is proposed to deal with the mismatch problem. This new m odel extends the basic keyword language model by treating structur e as hidden variable during the generation process. Second, varia tions of the model are compared. Third, term-level and structur e-level smoothing strategies are studied. Evaluation was conducted with INEX XML retrieval and question-answering retrieval tasks. Experimental results indicate that the optimal structured retrieval model is task dependent, two-level Dirichlet smoothing signi ficantly outperforms two-level Jelinek-Mercer smoothing, and with accurate structured queries, the proposed structured retrieval model outperforms keyword retrieval significantly, on both QA and INEX datasets. H.3.3 [ Information Search and Retrieval ]: Retrieval Models, Query formulation Algorithms, Experimentation, Theory Structured Retrieval, Generativ e Model, Indri Query Language, Language Model, XML Retrie val, Question Answering From the earliest days of information retrieval, search systems have used field-based retrieval to confine a search to just a portion of a document X  X  text or metadata, for example, the title, author, source, or publication date. In recent years, the widespread use of HTML and XML, and the increasing use of text annotations such parts-of-speech, named-entities, and semantic role labels for some applications have prompted renewed interest in document structure. Two well-known examples are retrieval of texts or text elements expressed in XML [1], and retrieval of texts or text elements for question answering [3, 8]. These two examples are not necessarily distinct, but typically XML retrieval focuses on structure introduced by the docum ent author(s), whereas question answering focuses on structure introduced by annotators. These two types of structure typically have different characteristics, for example, differing size, scope, and reliability. One very important difference is that the elements of an XML document must be nested strictly, whereas annotations have no such restriction, thus we frame the problem in terms of fields , not XML elements. 
Document fields are specified by embedded and/or offset ( X  X tandoff X ) annotations. A field is defined by a unique id, a field type, a starting position, and an e nding position. Fields may have hierarchical relationships. The document is considered a field  X  the outermost field. The search process ranks and returns field types specified by a query, for example, documents, titles, sections, sentences, or targets (a ty pe of semantic role). A query can consider evidence from a contained (inner) or descendent field when evaluating a containing (outer) or ancestor field [1, 8]. 
The traditional approach to field-based retrieval considers only the text within specified field types (an exact-match constraint). However, approximate matching is often preferred because users may not have a complete understandi ng of how structure is used in documents ( X  X mprecise queries X ), or the document structure may not be correct in all documents. The former type of mismatch annotator-generated fields (e.g., se mantic role labels). The latter type of mismatch applies primarily to automatically-generated fields. Both types of mismatch are common. Some retrieval models for XML documents use smoothing to obtain approximate matching (e.g., [14]), but this approach is less common. 
When a retrieval model allows evidence from a contained (inner) field to be used when evaluating a containing (outer) field, it must specify how to combine evidence when multiple contained fields match; this is particularly true if approximate matching is permitted. For example, when ranking sentences for a question answering application, a long sentence may contain two spans annotated as subject . If approximate matching is permitted, both match to some degree. The query or retrieval model must specify how this evidence is combined to produce a score for the containing sentence. 
This paper provides a principled solution to the problems of approximate matching and combin ation of evidence described above. We start with the Indri s earch engine X  X  basic term-based language model [9], and extend it to provide a generative structured retrieval model. We investigate the related problem of hierarchical smoothing methods th at incorporate document level evidence, and smoothing methods for combining evidence from fields of different lengths. Our work provides a principled method of doing approximate matching, and an empirical study of methods for combining evidence from inner fields. 
We begin with an introduction to the language modeling retrieval framework and the current field retrieval and structured retrieval models of the Indri search engine [8, 9]. Then, we focus on extending the field retrieval model into a new approximate structured retrieval model. A connection between the new structured retrieval model and th e traditional term based language model is revealed. Sample structured queries are used to demonstrate the effectiveness of the new model. In the experiments, two applications , question answering and XML element retrieval, are used to demonstrate the performance of the new model with new smoothing met hods. Finally related work is discussed, and conclusions follow. We start by introducing the la nguage modeling framework for information retrieval, and the Indr i search engine X  X  current field retrieval and structured retrieval capabilities [13, 8]. These lay the foundation for the remainder of the paper. In its very basic form, the la nguage modeling approach to IR involves computing the generation probability of a query given a document language model [5]. Qu ery terms are assumed to be independent of each other. The document model is smoothed with the collection model, for example using Jelinek-Mercer or Dirichlet smoothing methods [6]. Equations 2.1 and 2.2 show th e basic language model with Dirichlet smoothing. P s ( q i | D ) is the smoothed probability of document D generating query term q i and  X  is the scale parameter for Dirichlet smoothing. Indri uses language models and an inference network model for retrieval and scoring [12, 13]. Inference networks typically im plement query operators that dynamically produce new index terms (e.g., #1(wipe out) #syn(destroy #1(wipe out)) ) or combine scores produced by Indri X  X  default field retrieval model directly extends the traditional term-based language model by using the probability that the desired field  X  instead of the whole document  X  generated the query, and returning the queried fi elds, instead of documents, as results. Equation 2.3 is used instead, where q is the query, F is the desired field type, D is a document containing F , and P is the smoothed probability for the field to generate query term q
The major difference between document retrieval and field retrieval is that the surrounding context of a query term shrinks from a document to a field. Thus, term frequency becomes occurrences of the term within the field, and for each query term, the background smoothing probability comes from not only the collection but also the document containing the field. Including the document level smoothing is s hown to be effective in INEX XML retrieval [14]. Two level smoothing is achieved by interpolating the field F j model with both the document and the collection models. Equation 2.4 shows two level Jelinek-Mercer smoothing, in which P( q i | D ) is the unsmoothed document model. P|, where the document model comes from all F fields in D : 
This two level smoothing model is a general case of and can be backed off to the single level document smoothing by equating  X  in Equation 2.4 to 0. Two level Dirichlet smoothing is not present as this two level formulation is not general enough to be applied to Dirichlet smoothing. We return to this point in Section 3. 
In the Indri query language, a field query is expressed as #combine[F]( q 1 q 2 ... q n ) , for example #combine[sentence]( take measures ) , where [sentence] restricts the target scoring field to be sentence query terms are take and measures . The (probabilistic AND) operator multiplies the generation probabilities from take and measures to produce the final generation probability for a sentence. For a graphical model representation of this ex ample query, see Table 1. In Indri, XML elements and text annotations are treated uniformly as extents  X  spans of text in a document. An extent restriction on a query operator constrains the result extents to be of a specific type, for example, the query #combine[F]( q ) , where F defines the field type and q is a sub-query, evaluates #combine( q ) F fields of a document. Here, q can be a complex query which may also contain its own extent restrictions, for example, usability guidelines ) user interface ) . By embedding the section extent re striction inside the document extent restriction, this query retrieves documents, but uses relevance scores from sections as evidence, and combines them to yield the final score for the document. This kind of evidence-combination queries is frequently s een in the INEX datasets [15]. 
When executing the above example query from INEX 2006, for each document, #combine[section] will return a list of scores for each section that appears in the document; the scores depend #combine( design usability guidelines ) . Then, the list of section probabilities together w ith the relevance scores from the query terms user and interface are combined (probabilistic AND, because of the #combine node at the [document] level) to form the final score for the document. 
In the above example, fields are nested in the query, which means that the text span of the inner field is a substring of the document text. Indri also suppor ts fields that are related hierarchically (parent-child), but that do not satisfy a text span containment constraint. Such fi elds are convenient for semantic role labels, where the agent (usually arg0) and patient (usually arg1) are both linked to the target verb as their parent, even though the arguments are not contained within the target X  X  text span [8]. Relationships among fields are specified using  X  ./ child-parent relationship between an extent restriction and its immediate outer extent restriction in the query. For example, returns documents based on evidence from target fields. The  X  operator ensures that the matched arg0 and arg1 fields are actually children of the containing field in the query, i.e. the target field. One central problem for structured retrieval is the ability to do approximate matching. Typically , when constructing the query, the user does not know for sure what the structures in a relevant result will be like. Studies also s hows that the user will very likely guess wrong [16], thus soft matching is necessary. We show how it is done currently through smoothing. 
Term level smoothing in language m odels allows a soft match of fields with query terms. For example, #combine[section] ( Symphony #combine[title]( Music ) ) returns sections containing either a title field with the term Music or a term Symphony 1 . Partial match cases are ranked lower than the full match case where the section cont ains a title field that contains the term Music and the section also matches Symphony. Because of the two level smoothing in effect, the final score also depends on the document language model (and also document length if using Dirichlet smoothing). 
Besides term level smoothing, field level smoothing is also necessary when combining evidence from different types of fields. For example for the query #combine[section]( Symphony #combine[title] ( Music ) ) , if a section does not have a title field, without field level smoothing, using strict field matching, this section will end up having 0 probability. The field level smoothing in the version 4.5 of Indri adds an empty and 0-length extent for a field only when the field is missing. This feature allows the missing field match to back off to the background smoothing probabilities through the empty extent. 
This way of doing approximate stru ctured retrieval, instead of using the fields as hard constraint s, treats the fields in a document only as evidence for relevance. An example is,  X  X ind me sections that are likely to have title fields within the section and the title is likely to be relevant to Music X . As defined in INEX evaluations starting from 2003, the query structure is intended to be a hint, not necessarily a strict requirement [2]. Indri X  X  current field and struct ured retrieval model have two important problems that motivate our research. Length-biased term smoothing is used at the term level, for terms that appear in fields of varying lengths. For example, in the query #combine[title]( Music ) ) , Symphony has a context of a section, which is typically much longer than a title, in which the query term Music appears. Thus, when using Jelinek-Mercer smoothing, the difference between having and not having the word in a short extent is much larger than that in a longer extent. Equation 2.5 illustrates the problem , which follows Equation 2.4. The section must match at least one query term to be returned by the search engine. This strategy follows the convention of most experimental IR systems, wher e a document is matched if and only if it contains at least one of the query terms. P( | , ,tf( , ) 1) length( )
P ( | , , tf ( , ) 0) P( | ) (1 )P( | )
This ratio used to be the IDF (inverse document frequency) part of the retrieval model, but now em phasizes the length of the field, and prefers shorter fields proportional to the inverse of the extent length. This unexpected length bi as causes the retrieval model to greatly prefer matching of the #combine[title]( Music ) part of the query over matching of Symphony in a section. 
For field level smoothing, as outlined in the previous subsection, the empty extent method works reasonably well for the question answering task, but lacks theoretical justification: why not introduce the empty extent even when the field exists, why not more empty extents, or why not empty extents with a non-zero length? Section 3 provides a generative framework for doing approximate structural matching, in which more intuitions will be developed to guide the use of field level smoothing. Another problem that happens as we go structural is merging evidence from fields. In Indri vers ion 4.5, the extent restriction (e.g. the [f] in the query #combine[f]( q )) returns a list of probabilities P( #combine(q) | f i ) from matched fields, but does #combine( #combine[section]( Pop Music ) ) 2 , if a document contains two s ections, this query first returns the list of section extents, each section associated with its generation probability of the query terms (Pop Music). Then the final score for the document will be the multiplication of the two section probabilities, because of the outer #combine operator. Since language model probabilities are almost always smaller than 1, matching more fields yields more multiplications, and thus a smaller relevance score. This effect violates the intended query semantics; we call it the evidence merging problem . 
To overcome this problem, the user needs to consider carefully how to combine the two section probabilities in the query. One possibility is to set the document score based on the best matching Music ) ) ) . This approach avoids penalizing a document for having many relevant sections, because only the maximum scoring section contributes its score to the document score. Although #max works well for some tasks, e. g., question answering, there is no clear justification for it. It also does not reward a document for having multiple matching sections. Our goal is a coherent probabilistic model in which the problems described above are more easily analyzed and approached. This section develops improved re trieval models and smoothing methods to achieve that goal. Consider a simple canonical structured query with embedded field retrieval and using fields as evidence, as shown below. Since Indri defaults the outer-mos t extent to be document, we use the query #combine( #combine [section]( ... ) ) . In this discussion it is simplest to think of q 1 terms, but the model supports sets of terms and query operators. 
The generative story is that the document needs to generate the f field models, and then each f 1 model will generate the query term q and the structural term #combine[f 2 ]( q 2 ) . Next, each f field model in each f 1 model will generate the query term q hidden variable, and summed (or rather averaged) over all the f models in the document. The same is true for f 2 in each f (3.1) 
In the above derivation, tf( f , D ) is the number of times a field of type f occurs in document D , and each f i is a specific instance of f in the document model D . The output probability is now P which is obtained through mergi ng (averaging over) the list of probabilities P s ( t | f i , D ) for i =1 to tf( f , D )+1. 
Table 1 presents more queries a nd interpretations of them in their graphical models. Because of the generative interpretation, the structured retrieval model is ve ry different from models for flat #combine(Music Symphony)) , the query can be seen as a distribution of words, and the re trieval model calculates the KL divergence between the query and document distributions. In the structural case, the query does not specify a distribution of structural components. Instead, the whole query is a generative procedure that describes how th e document should generate the terms in the query. Thus, to cons truct a structured query, instead of using an example structure or  X  X magining X  how the terms should appear in different fields, chances are better off if the query takes into account how different relevant structures in the documents will follow the generation process specified in the query. 
The structured retrieval model ranks fields by how well they satisfy the queried generation pro cess. When the goal is to rank documents, the model combines th e probabilities from the fields in the document to produce a single probability for the document. 
The main contributions in this section are i) a generative framework for field-based retrie val, and ii) a specific way of merging the probabilities of multiple matching fields, through probabilistic interpretation. Next, based on this generative model, we show the smoothing methods th at are used to estimate the document and field language models. This constitutes the approximate matching ability of the retrieval model. 
Smoothing plays a central role fo r the approximate matching in field retrieval, and is effectiv e for both containment and parent-child relations between fields. When queries become more complex and fields smaller, mismatches occur more often. In language modeling terms, smaller fi elds are more sparse, and thus Smoothing improves the estimates. Because the generative structured retrieval model ge nerates both terms and fields, smoothing can happen at both the term level and the field level. 
At the term level we propose a new two level smoothing scheme which applies to Dirichlet smoothing. In order to solve the length bias problem for the Jelinek-Mercer smoothing (Section 2.5.1), the sm oothing method must take into account the length of the context that a query term appears in. Dirichlet smoothing [6] has this property. However, simply smoothed with the collection model, Dirichlet is no better than the document + collection level Je linek-Mercer smoothing [8]. 
We extend the two level smoot hing scheme to Dirichlet smoothing by generalizing the tw o level smoothing procedure as follows  X  apply the smoothing tw ice, recursively, by first smoothing the document with th e collection model and then smoothing the field model with the already smoothed document model. P( | , ) (1 ) P( | ) (1 )P( | ) qFD qD qC
Equations 3.2 and 3.3 are th e two level smoothing methods corresponding to Jelinek-Mercer and Dirichlet smoothing in their general form. Both can be  X  X acked off X  to the original one level smoothing described in [6] by equating  X  c in Equation 3.2 to 0, and  X  in Equation 3.3 to +infinity. Equation 3.2 is equivalent to Equation 2.4. 
Now that we have the Dirichlet smoothing working in the two level smoothing setting, it is easy to show that two level Dirichlet smoothing does not have the le ngth bias problem, i.e. the difference between having a term match and not having a term match does not depend on the cont ext length. For field level matching, the difference does not depend on field length: 
P( | , ,tf( , ) 0) 1P(|,) P(|,) 1P(|,) = X  =
At the document level, if there is field level term match (tf( q &gt; 0), field level match will dominate, as background smoothing probabilities from the document or collection are typically very small. If there is no field level term matching, the result is the same, i.e., the difference does not depend on the document length: = X  =
In practice, we need to tune the optimal parameter for Dirichlet smoothing. Its optimal parame ter value is dependent on the average length of the returned unit in order to satisfy the term discrimination constraint [7]. Fo r sentence retrieval with question answering queries that use semantic role label structures, we find 5 where typical queried fields are article, section, and figure, the and query-dependent smoothing to be a best Dirichlet smoothing. At the field level, for smoothing th e queried fields in a document, Dirichlet smoothing suggests the use of empty fields of the same type being added to the document model. Referring back to Equation 3.1, assume, given document model D , the probability of generating any one instance f i of the field typed f is just the probability of randomly picking f i out of all { f where the document model D also includes the empty fields for field level smoothing. Since it is empty, only the background smoothing scores will be filled in for all query terms for that field. This way, even without a queried field present, a document can be retrieved and ranked as long as there is a term match. If the number of empty fields added is 1, this field level smoothing is a Laplacian smoothing, i.e. Diri chlet smoothing with the scale parameter of the Dirichlet prior equal to 1. 
Because of the effect of smoothing, a partially matched document will have a relevance score lower than the documents where field and term both match. 
There are two parameters to tune for field-level smoothing. First is the Dirichlet scale parameter  X  how many empty fields to add; having more empty fields typically brings the field closer to the document language model. Sec ond is the length of the empty fields; using a non-zero length will down-weight the document and collection background probabilities. We find that adding one empty field is usually enough, a nd the length of the empty field does not affect performance much, thus we leave it as 0. In order for merging evidence from fields and retrieving fields to work at the same time, each extent restriction constraint must be aware of its context. In one case, the outer most extent restriction constraint (for example, the [section] in the 2 nd 1) means field retrieval, which does not need to return empty fields for smoothing, and a need to return a score for each matched field. In the other case, extent restric tion constraints on any other belief operators (e.g., #combine , #max ) mean using these fields as evidence, and require empty fields be added and scores merged to return one single probability for the node (e.g. the [title] 2 query of Table 1). 
Being context aware can also help increase retrieval speed. This is because in field retrieval, the outermost fields in the query that do not match a query term need not be scored at all, which typically cuts computational time in half or less. For example, in the 2 nd query of Table 1, unmatched [section] extents of matched documents can be skipped, whereas every title of a matched section should be scored, and the scores merged to form the relevance score for the section . Under the language modeling framewo rk for text retrieval, the structured retrieval model proposed above can be seen as a natural generalization of the classical term retrieval model, where there is a special type of field, let X  X  call it the  X  X erm X  field, surrounding each term in the document. This  X  X erm X  field is special in the sense they are all of length 1 always, and they cover every term in every document. 
This section shows that the stru ctured retrieval model bears a close relation with the traditional term language model. This is shown in Equation 3.4 by casting th e term retrieval model into the structured retrieval framework. For illustrative purposes, we calculate the maximum likelihood To cast it into the structured retrieval model, we assume every term in the document is surrounded by a  X  X erm X  field of length 1, covering exactly the term itself, and the document must generate the field  X  X erm X  first before generating term t . Which  X  X erm X  field generates the query term t is hidden, and t hus must be summed over, i.e. the probability of generating t is the sum of generating t from each  X  X erm X  field in the document. Since the number of  X  X erm X  fields in a document equa ls the length of the document, generating any one  X  X erm X  field given D has probability 1/length( D ). For simplicity we also assume no smoothing, i.e. a  X  X erm X  field containing the term t generates t with probability 1, otherwise, 0. The following full derivation shows how structured retrieval, through combining evidence from  X  X erm X  fields, yields exactly the term retrieval model: 
From Equation 3.4, it is easy to see that the classical term language model is just a special cas e. The empty  X  X erm X  fields for smoothing do not appear in the above derivation because smoothing is a separate issue. Fo r example, term level Dirichlet smoothing with scale parameter  X  is equivalent to adding any background smoothing  X  X erm X  fields to the document and setting  X  to +inf; Jelinek-Mercer is ju st linear interpolation with the smoothing field. In Indri, Equa tion 3.4 corresponds to the query #combine( #combine[term]( t ) ) , which is equivalent to the term retrieval query #combine( t ) . For the case of two query terms such as #combine( u v ) , the corresponding structured query is #combine( #combine[term]( u ) #combine[term]( v ) ) , where the outside #combine operator multiplies the generation probabilities of u and v . For more complex structures, such as: the document generates the target fields, which in turn generates the children arg0 and arg1 fields, which at last generates the words. The graphical model di agram in Table 1 describes the model corresponding to the query. See [8] for more motivated examples of queries that use semantic roles. During the evaluation of this query, the relevance scores are calculated and propagated from the leaf nodes to the top level recursively. Within each document, the scores for the target fields are accumulated, within each target field, the children arg0 and children arg1 scores are accumulated, and within each children field, the scores from the query terms are accumulated. Suppose the document model D has k [target] extents; the i [target] extent model contains l i [arg0] extents and m extents. Let Q t be the [target] extent restricted query:  X  #combine[./arg1]( Mary ) )  X , Q a0 =  X  #combine( John )  X , and Q a1 =  X  #combine( Mary )  X . The above query Q yields the following score when evaluated on D : where (3.6) The task of the models described here is to combine scores from multiple fields to generate a single score for the outer containing field, e.g. in Table 2, collapsing the scores from [./arg1] X  X  for each [target], and collapsing scores from the [target] fields for the containing sentence. In the dege nerate term retrieval model in Equation 3.4, collapsing is done by the average of relevance scores from the [term] fields. 
The generative model in Equation 3.4 generally just takes in a list of probabilities from the matching fields and outputs a single score  X  the average  X  for the list. We term this process belief combination . There are other ways of collapsing the list of probabilities in generating this single score. We consider three cases: i) average (AVG), ii) the maximum of the list (MAX), and iii) Probabilistic OR (OR). It should be noted that only AVG has a direct connection to the term based language model (Section  X   X  X  X  which equals approximately the sum of all probabilities (by ignoring higher order terms). Thus, the main di fference between the Probabilistic OR and the AVG model is the normalization by the total number of matching fields. These three combination met hods all exhibit three common properties: i) monotonicity  X  adding a relevant field will not decrease the final score, ii) probability preserving mapping  X  the final score is still in the [0, 1] ra nge, thus could still be interpreted as a probability and iii) submodular  X   X  X iminishing return X , as the number of relevant fields in creases, the benefit of adding additional relevant fields decreases. The most salient difference among the three is that MAX and OR care mostly about the highest scoring field, disregarding other fields (e.g. mismatched fields), with the output of OR even increasing with more irrelevant fields. For example, when combining probabilities 0.1 and 0. 9, MAX will output 0.9 and OR will output 0.91, while AVG yields 0.5. Here, AVG is the only measure that penalizes distracti ng fields, which, following Section 3.3.1, is equivalent to the le ngth normalization in term-based language models, where distrac ting terms increase the document length and thus decrease the probability of generating the query terms. AVG ensures that the mo re focused a document is about the query terms, the higher the ranki ng. This makes much sense in keyword queries, where users may expect the whole document to be relevant to the query instead of just a small part of the document being relevant to the que ry. However, in cases other than keyword document retrieval, for example, sentence level question answering, since the unit of retrieval is much smaller and the users X  effort to create structured queries is much larger, the user might ignore the effort of examining the whole result sentence Thus, a sentence with just a small relevant portion would be considered highly relevant, as long as the relevant portion shares a close match with the query. 
In short, more often than not, the users or the different application scenarios should determ ine which model to use. More detailed examples and results are shown in Table 2 and Table 4. Table 2 shows the relevance scor es for an example query to illustrate the three different methods in action. 
OR prefers longer sentences with many target verbs, even though most of the target words do not match, as shown in Table 2, sentence 3. This is because of the incremental values of the multiple smoothing probabilities of the unmatched target fields. 
AVG prefers short sentences with concentrated occurrences of matching fields. This behavior is also present in the case of term-based language models. Generally this is the same bias toward short sentences that is present in the traditional language model retrieval model with Dirichlet smoothing. 
MAX is the only one of the three combination methods that does not care about sentence length or the number of queried fields in a sentence; it only cares about the generation probability of the best field in the sentence. When th e field language model is smoothed with the document model, MAX pr efers matching fields appearing in highly matched documents. This is actually why the scores of sentences 1 and 4 in Table 2 are different for the MAX combination method, even though bot h contain perfect matches. Experiments were done with two datasets, the  X  X NEX X  and  X  X A X  datasets that reflect the two usage scenarios described above. 
The INEX dataset consisted of the Wikipedia collection of XML documents, using INEX 2006 a nd 2007 topics, which contain structural hints (Content And Stru cture queries). INEX queries can retrieve XML elements of an y type however our goal was to investigate the use of evidence from contained (inner) fields in ranking containing (outer) fields. To better match our goals, topics and relevance assessments were transformed in three ways: i) element retrieval queries were surrounded by a #combine operator so that they would use evidence from contained fields to rank and return whole documents, ii) a docum ent was considered relevant if it contained at least one relevant element, and iii) for the NDCG metric, a document X  X  degree of relevance was determined by the proportion of its text contained in relevant elements. 
The QA dataset consisted of the AQUAINT corpus, annotated with ASSERT semantic role labels , as described in [8]. This corpus has a matching set of TREC topics. Structured queries were created from pre-identified relevant sentences, i.e. the structured queries are best-case queries with the same set of keywords as the original queries. As in prior research [8], all of the relevant sentences are used to generate structured queries for each topic. However, a difference from [8] is that each constructed structured query in tu rn is used to retrieve all the relevant sentences for the original topic. Thus, the evaluation not only measures how accurate the retrieval model is, but also the model X  X  generalization ability, i.e. how well it handles the structural mismatches between the query and the rest of the relevant sentences that are not used to generate the query. In all the experiments, a training set was used to learn the optimal smoothing parameters. A grid para meter sweep was used to learn the parameters. The optimal values are listed below. 
Datasets Queries  X  Since term level smoothing is the basis for all the retrieval models described in this paper, we start by finding the most effective term types show the relevant semantic role labels in the sentence. smoothing method. Fixing the two collections and their corresponding sets of queries, we compared two-level Jelinek-Mercer with two-level Dirichlet. For each query, the top 1000 results were returned, if possible. For structured queries, the best performing field evidence combination method was used  X  MAX for QA and AVG for INEX. For keyword queries, since the INEX queries are for document retrieva l, two-level smoothing is the same as collection-level sm oothing, while for QA sentence retrieval queries, they are distinct. 
Results as summarized in Table 3 show two-level Dirichlet to be consistently better than two-leve l Jelinek-Mercer on all evaluation metrics and both structured retrieval tasks. Comparison with traditional single level (collection level) smoothing is omitted as previous study has already show n document level smoothing to be helpful [8], and our experiments also confirm that on both tasks. 
The next experiment fixed th e term smoothing and compared different structured retrieval models as described in Section 3.5. Results are presented in Table 4 and 5. From Table 4, first two rows, OR combination method works comparably with MAX and AVG, and the differences are not distinguishable. We hypothesize that the reason OR combination method is better than AVG is because of the way the test is conducted. When converting the INEX element level judgements into document level, a document is considered relevant if any one of its element is relevant. This way the evaluation does not penalize long documents with only a small relevant part. 
In order to take into account the lengths of non-relevant parts of result documents, a slightly cu stomized version of the NDCG measure [17] is adopted, with tw o simple modifications. In the INEX datasets, the sizes of the relevant elements (in bytes) for a relevant document are given, and the length of a document can be easily measured by extracting the content text of the XML documents excluding XML tags. Thus , first, the percentage of the relevant text (in bytes) of a document is used to quantize the relevance of a document, and second, when calculating the discounting factor, instead of using the rank of the document, the sum of lengths of the preceding documents in the rank list is used. 
As presented in Table 5, under NDCG, the AVG combination is significantly better than OR, almost always. From the average lengths of the returned top ranke d documents, it can be seen that OR returns documents that are much longer than AVG. In this case, most of the large documents will only have a small portion relevant, lending to small NDCG. 0.7818 0.5065 0.3094 0.7552 0.4583 0.3006 0.6452* 0.2373 0.1501 0.5138*** 0.1752*** 0.1617 
In terms of length biases, the Je linek-Mercer smoothing is biased toward shorter documents as compar ed to Dirichlet smoothing. The OR combination method is biased toward longer documents as compared to AVG. The optimal performance in NDCG is from AVG combination method + two-level Dirichlet smoothing. 
The optimal structured retrieval algorithm is on par with the best keyword performance. On INEX 06 dataset, keyword is slightly better, but not significant, while on INEX 07, structure is slightly better. The performance of the stru ctured queries depends largely on how well the queries are constructed to match the structures in the relevant documents. We did an error analysis of the structured queries on the INEX06 collection and the result suggests that in many cases, the queries are not well constructed. For example, i) the topic asks for figures, but the query doesn X  X  use evidence from [figure] extents, ii) the topic asks for  X  X oordinates X  or  X  X opulations X  of  X  X uropean capital cities X , while the query uses  X  X oordinates X  and  X  X opulations X , iii) using  X  X rigin of universe X  as an exact phrase match which is overly restricti ng, instead, an unordered window operator should be used. Thus we corrected these structured queries whose semantic interpretation is obviously different from the topic description. We also changed th e queries to use the document model instead of the article field language model, which is a minor change that can be made automatically. 
Manually correcting the structured queries only requires an understanding of the topics themselv es, and for the test topics, no results were referenced in any way. 
With the proofread INEX07 structur ed queries, structured retrieval outperforms keyword retrieval significantly, with degree of freedom significant improvement over keywor d retrieval is observed on the INEX datasets. 
With accurate structured que ries, the AVG model outperforms keyword retrieval significantly. The QA queries demonstrate this effect more (Table 4), as they are more accurately constructed. Several points can be made about the results on the QA dataset: i) well constructed structured queries perform better than keyword in top precision. Since the queries co me from true relevant sentences, they are more accurate than the IN EX human conjectured queries. ii) The model also generalizes from the structured query to the other relevant sentences reasonabl y well. P@10 and MAP were comparable or better than keyword. iii) The MAX combination method outperforms OR or AVG, probably because the sentence The difference between two-level Dirichlet and two-level Jelinek-Mercer is larger than that on the INEX data. The reason might be that because the optimal parameter values of Dirichlet smoothing depends on the average length of the queried extents [7], and since INEX queries contain extents of very different lengths (section, the queries. In this case, a fi eld dependent smoothing is more desirable, and we leave it to future work. There is considerable prior res earch about combining evidence from the different fields of a document (e.g. combining title, in-link anchor and body text of HTML). The evidence combination in this paper is multiple times in a result, e.g. combining evidence from all paragraphs of a document, or from targets in a sentence. [11] shows a use of mixture langua ge models for XML retrieval, which essentially flattens the XML element structure and mixes the language models. This flat st ructure among elements makes the model more robust to inaccurate queri es where the user is not sure whether a word should appear in a child field, but writes the query in that way anyway. For the same r eason, there is not much sensitivity to the structured query when it is in fact accurate. Table 6. Performance on the corrected queries of INEX 07 
NDCG Keyword Structure Change Significance @10 0.5063 0.5530 +20.2% p &lt; 0.006 @20 0.5797 0.6408 +20.4% p &lt; 0.0004 @30 0.6388 0.6854 +18.1% p &lt; 0.002 
Using element length as a prior for preferring longer XML elements has been found useful in the past [10]. However, since the experiments described in our work are document or sentence level retrieval, where the unit of the results is of more-or-less uniform lengths, length priors are not used. [14] uses a hierarchical language modeling approach for retrieval of XML elements. In that model, elements are smoothed with the containing parent and ancestor elements. That model targets keyword queries with structured documents, while this work explores query structures. In terms of smoothing, the two-level smoothing methods (extents smoothe d with document and collection models) used in this work can be seen as a special case of the hierarchical smoothing used in [14]. The two-level Dirichlet smoothing is new in this work and has not been applied to the structural retrieval problem in the literature, to the best knowledge of the authors. This paper proposed a generative retr ieval model for using structured queries to retrieve structured texts. The model is investigated in the context of the open-source Indri search engine, although it could be applied in other search engines, as well. The model uses smoothing to provide approximate matching at both the field level and term level. Field level smoothing is a generalization of the term level smoothing in traditional term-based language modeling approaches. For term level smoothing, our expe rimental results show that two-level (document + collection) Dirichlet smoothing significantly outperforms two-level Jelinek-Mer cer smoothing on two datasets. 
This paper also identifies evidence merging, in which the retrieval model combines evidence from individual occurrences of the same investigate the use of Average (AVG), Maximum (MAX), and probabilistic OR combination strategi es. Example queries show that AVG is biased towards shorter fields , MAX is not as biased, and OR prefers long fields. Experiments show that MAX is best on the Question Answering task, in which fields tend to be very short and irrelevant portions within a relevant sentence is not much of a factor. However, in XML document retrieval, where fields tend to be longer and redundancy within a document more indicative of relevance, the three combination methods are about equal when measured by MAP, MRR, and P@10, while AVG is best when measured by NDCG. 
Overall, the performance of the best structured retrieval models is not worse and sometimes better th an the best performing keyword retrieval methods. When the structured queries are reasonably accurate, i.e. for the QA dataset and the corrected INEX queries, structured retrieval significantly outperforms keyword. The results of this paper show that our genera tive structured retrieval model is robust under less accurate queries and outperforms keyword with high quality structured queries. The authors thank Paul Ogilvie and Matthew Bilotti for their work on the Indri search engine, on INEX and TREC QA datasets, and for helpful suggestions and discussions. 
This work is supported by National Science Foundation grant IIS-0534345, and AQUAINT program grant N61339-06-C-0132. The views and conclusions contained in this document are those of the authors X  and do not necessarily reflect those of the sponsor. [1] Norbert G X vert and Gabriella K azai. Overview of the INitiative [2] B X rkur Sigurbj X rnsson and Andrew Trotman. Queries: INEX [3] A. Echihabi and D. Marcu. A Noisy-Channel Approach to [4] J. Prager, J. Chu-Carroll, E. W. Brown and K. Czuba. Question [5] Jay M. Ponte and W. Bruce Croft. A Language Modeling [6] Chengxiang Zhai and John Lafferty. A study of smoothing [7] Hui Fang, Tao Tao and Chengxiang Zhai. A formal study of [8] M. W. Bilotti, P. Ogilvie, J. Callan and E. Nyberg. Structured [9] INDRI -Language modeling meets inference networks. [10] Jaap Kamps, Maarten de Rijke and B X rkur Sigurbj X rnsson. [11] Djoerd Hiemstra. Statistical Language Models for Intelligent [12] Don Metzler and Bruce Croft. Combining the Language Model [13] T. Strohman, D. Metzler, H. Turtle, and B. Croft. Indri: A [14] P. Ogilvie and J. Callan. Hi erarchical Language Models for [15] Saadia Malik, Andrew Trotman, Mounia Lalmas, Norbert Fuhr. [16] Andrew Trotman and Mounia Lalmas. Why Structural Hints in [17] K. J X rvelin and J. Kek X l X inen . IR evaluation methods for 
