 Wikis are web-based, collaborative content author-ing systems (Leuf and Cunningham, 2001). As they offer fast and simple means for adding and editing content, they are used for various purposes such as creating encyclopedias (e.g. Wikipedia 2 ), construct-ing dictionaries (e.g. Wiktionary 3 ), or hosting online communities (e.g. ACLWiki 4 ). However, as wikis do not enforce their users to structure pages or add com-plementary metadata, wikis often end up as a mass of unmanageable pages with meaningless page titles and no usable link structure (Buffa, 2006).

To solve this issue, we present the Wikulu sys-tem which uses natural language processing to sup-port wiki users with their typical tasks of adding, organizing, and finding content. For example, Wikulu supports users with reading longer texts by highlighting keyphrases using keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004). Support integrated in Wikulu also includes text segmentation to segment long pages, text simi-larity for detecting potential duplicates, or text sum-marization to facilitate reading of lengthy pages. Generally, Wikulu allows to integrate any NLP com-ponent which conforms to the standards of Apache UIMA (Ferrucci and Lally, 2004).

Wikulu is designed to integrate seamlessly with any wiki. Our system is implemented as an HTTP proxy server which intercepts the communication between the web browser and the underlying wiki engine. No further modifications to the original wiki installation are necessary. Currently, our system pro-totype contains adaptors for two widely used wiki engines: MediaWiki 5 and TWiki 6 . Adaptors for other wiki engines can be added with minimal effort. Gen-erally, Wikulu could also be applied to any web-based system other than wikis with only slight mod-ifications to its architecture.

In Figure 1, we show the integration of Wikulu with Wikipedia. 7 The additional user interface com-ponents are integrated into the default toolbar (high-lighted by a red box in the screenshot). In this ex-ample, the user has requested keyphrase highlight-ing in order to quickly get an idea about the main content of the wiki article. Wikulu then invokes the corresponding NLP component, and highlights the returned keyphrases in the article. In the next sec-tion, we give a more detailed overview of the differ-ent types of support provided by Wikulu. In this section, we present the different types of NLP-enabled support provided by Wikulu.
 Detecting Duplicates Whenever users add new content to a wiki there is the danger of duplicating already contained information. In order to avoid du-plication, users would need comprehensive knowl-edge of what content is already present in the wiki, which is almost impossible for large wikis like Wikipedia. Wikulu helps to detect potential du-plicates by computing the text similarity between newly added content and each existing wiki page. If a potential duplicate is detected, the user is noti-fied and may decide to augment the duplicate page instead of adding a new one. Wikulu integrates text similarity measures such as Explicit Semantic Anal-ysis (Gabrilovich and Markovitch, 2007) and Latent Semantic Analysis (Landauer et al., 1998).
 Suggesting Links While many wiki users read-ily add textual contents to wikis, they often re-strain from also adding links to related pages. How-ever, links in wikis are crucial as they allow users to quickly navigate from one page to another, or browse through the wiki. Therefore, it may be rea-sonable to augment a page about the topic sentiment analysis by a link to a page providing related in-formation such as evaluation datasets. Wikulu sup-ports users in this tedious task by automatically sug-gesting links. Link suggestion thereby is a two-step process: (a) first, suitable text phrases are extracted which might be worth to place a link on (see Fig-ure 2), and (b) for each phrase, related pages are ranked by comparing their relevance to the current page, and then presented to the user. The user may thus decide whether she wants to use a detected phrase as a link or not, and if so, which other wiki page to link this phrase to. Wikulu currently inte-grates link suggestion algorithms by Geva (2007) and Itakura and Clarke (2007).
 Semantic Searching The capabilities of a wiki X  X  built-in search engine are typically rather limited as it traditionally performs e.g. keyword-based re-trieval. If that keyword is not found in the wiki, the query returns an empty result set. However, a page might exist which is semantically related to the key-word, and should thus yield a match.

As the search engine is typically a core part of the wiki system, it is rather difficult to modify its be-havior. However, by leveraging Wikulu X  X  architec-ture, we can replace the default search mechanisms by algorithms which allow for semantic search to al-leviate the vocabulary mismatch problem (Gurevych et al., 2007).
 Segmenting Long Pages Due to the open edit-ing policy of wikis, pages tend to grow rather fast. For users, it is thus a major challenge to keep an overview of what content is present on a certain page. Wikulu therefore supports users by analyzing long pages through employing text segmentation al-gorithms which detect topically coherent segments of text. It then suggests segment boundaries which the user may or may not accept for inserting a sub-heading which makes pages easier to read and better to navigate. As shown in Figure 3, users are also en-couraged to set a title for each segment. 8 When ac-cepting one or more of these suggested boundaries, Wikulu stores them persistently in the wiki. Wikulu currently integrates text segmentation methods such as TextTiling (Hearst, 1997) or C99 (Choi, 2000). Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more acces-sible by generating an extractive summary. While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Fig-ure 4). Wikulu integrates extractive text summariza-tion methods such as LexRank (Erkan and Radev, 2004).
 Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight important keyphrases (see Figure 1). As Tucker and Whittaker (2009) have shown, highlighting important phrases assists users with reading longer texts and yields faster under-standing. Wikulu thus improves readability by em-ploying automatic keyphrase extraction algorithms. Additionally, Wikulu allows to dynamically adjust the number of keyphrases shown by presenting a slider to the user. We integrated keyphrase extrac-tion methods such as TextRank (Mihalcea and Tarau, 2004) and KEA (Witten et al., 1999). Further use cases for supporting wiki users include (i) visually analyzing the results of NLP algorithms, (ii) educational purposes, and (iii) enabling semantic wikis.
 Visually Analyzing the Results of NLP Algo-rithms Wikulu facilitates analyzing the results of NLP algorithms by using wiki pages as input doc-uments and visualizing the results directly on that page. Consider an NLP algorithm which performs sentiment analysis. Typically, we were to put our analysis sentences in a text file, launch the NLP ap-plication, process the file, and would read the output from either a built-in console or a separate output file. This procedure suffers from two major draw-backs: (a) it is inconvenient to copy existing data into a custom input format which can be fed into the NLP system, and (b) the textual output does not al-low presenting the results in a visually rich manner.
Wikulu tackles both challenges by using wiki pages as input/output documents. For instance, by running the sentiment analysis component right from within the wiki, its output can be written back to the originating wiki page, resulting in visually rich, possibly interactive presentations.
 Educational Purposes Wikulu is a handy tool for educational purposes as it allows to (a) rapidly create test data in a collaborative manner (see Section 2), and (b) visualize the results of NLP algorithms, as described above. Students can gather hands-on ex-perience by experimenting with NLP components in an easy-to-use wiki system. They can both collab-oratively edit input documents, and explore possi-ble results of e.g. different configurations of NLP components. In our system prototype, we integrated highlighting parts-of-speech which have been deter-mined by a POS tagger.
 Enabling Semantic Wikis Semantic wikis such as the Semantic MediaWiki (Kr  X  otzsch et al., 2006) augment standard wikis with machine-readable se-mantic annotations of pages and links. As those annotations have to be entered manually, this step is often skipped by users which severely limits the usefulness of semantic wikis. Wikulu could support users e.g. by automatically suggesting the type of a link by means of relation detection or the type of a page by means of text categorization. Thus, Wikulu could constitute an important step towards the se-mantification of the content contained in wikis. In this section, we detail our system architecture and describe what is necessary to make NLP algorithms available through our system. We also give a walk-through of Wikulu X  X  information flow. 4.1 Core Components Wikulu builds upon a modular architecture, as de-picted in Figure 5. It acts as an HTTP proxy server which intercepts the communication between the web browser and the target wiki engine, while it al-lows to run any Apache UIMA -compliant NLP com-ponent using an extensible plugin mechanism.
In the remainder of this section, we introduce each module: (a) the proxy server which allows to add Wikulu to any target wiki engine, (b) the JavaScript injection that bridges the gap between the client-and server-side code, (c) the plugin manager which gives access to any Apache UIMA -based NLP component, and (d) the wiki abstraction layer which offers a high-level interface to typical wiki operations such as reading and writing the wiki content.
 Proxy Server Wikulu is designed to work with any underlying wiki engine such as MediaWiki or TWiki . Consequently, we implemented it as an HTTP proxy server which allows it to be enabled at any time by changing the proxy settings of a user X  X  web browser. 9 The proxy server intercepts all re-quests between the user who interacts with her web browser, and the underlying wiki engine. For ex-ample, Wikulu passes certain requests to its lan-guage processing components, or augments the de-fault wiki toolbar by additional commands. We elab-orate on the latter in the following paragraph. JavaScript Injection Wikulu modifies the re-quests between web browser and target wiki by in-jecting custom client-side JavaScript code. Wikulu is thus capable of altering the default behavior of the wiki engine, e.g. replacing a keyword-based re-trieval by enhanced search methods (cf. Section 2), adding novel behavior such as additional toolbar buttons or advanced input fields, or augmenting the originating web page after a certain request has been processed, e.g. an NLP algorithm has been run. Plugin Manager Wikulu does not perform lan-guage processing itself. It relies on Apache UIMA -compliant NLP components which use wiki pages (or parts thereof) as input texts. Wikulu offers a so-phisticated plugin manager which takes care of dy-namically loading those NLP components. The plu-gin loader is designed to run plugins either every time a wiki page loads, or manually by picking them from the augmented wiki toolbar.
 The NLP components are available as server-side Java classes. Via direct web remoting 10 , those com-ponents are made accessible through a JavaScript proxy object. Wikulu offers a generic language pro-cessing plugin which takes the current page contents as input text, runs an NLP component, and writes its output back to the wiki. To run a custom Apache UIMA -compliant NLP component with Wikulu, one just needs to plug that particular NLP component into the generic plugin. No further adaptations to the generic plugin are necessary. However, more ad-vanced users may create fully customized plugins. Wiki Abstraction Layer Wikulu communicates with the underlying wiki engine via an abstraction layer. That layer provides a generic interface for accessing and manipulating the underlying wiki en-gine. Thereby, Wikulu can both be tightly coupled to a certain wiki instance such as MediaWiki or TWiki , while being flexible at the same time to adapt to a changing environment. New adaptors for other tar-get wiki engines such as Confluence 11 can be added with minimal effort. 4.2 Walk-Through Example Let X  X  assume that a user encounters a wiki page which is rather lengthy. She realizes that Wikulu X  X  keyphrase extraction component might help her to better grasp the idea of this page at a glance, so she activates Wikulu by setting her web browser to pass all requests through the proxy server. After applying the settings, the JavaScript injection mod-ule adds additional links to the wiki X  X  toolbar on the originating wiki page. Having decided to ap-ply keyphrase extraction, she then invokes that NLP component by clicking the corresponding link (see Figure 6). Before the request is passed to that com-ponent, Wikulu extracts the wiki page contents us-ing the high-level wiki abstraction layer. Thereafter, the request is passed via direct web remoting to the NLP component which has been loaded by Wikulu X  X  plugin mechanism. After processing the request, the extracted keyphrases are returned to Wikulu X  X  cus-tom JavaScript handlers and finally highlighted in the originating wiki page. Supporting wiki users with NLP techniques has not attracted a lot of research attention yet. A no-table exception is the work by Witte and Gitzinger (2007). They propose an architecture to connect wikis to services providing NLP functionality which are based on the General Architecture for Text En-gineering (Cunningham et al., 2002). Contrary to Wikulu, though, their system does not integrate transparently with an underlying wiki engine, but rather uses a separate application to apply NLP tech-niques. Thereby, wiki users can leverage the power of NLP algorithms, but need to interrupt their cur-rent workflow to switch to a different application. Moreover, their system is only loosely coupled with the underlying wiki engine. While it allows to read and write existing pages, it does not allow further modifications such as adding user interface controls.
A lot of work in the wiki community is done in the context of Wikipedia. For example, the FastestFox 12 plug-in for Wikipedia is able to suggest links to re-lated articles. However, unlike Wikulu, FastestFox is tailored towards Wikipedia and cannot be used with any other wiki platform. We presented Wikulu, an extensible system which integrates natural language processing techniques with wikis. Wikulu addresses the major challenge of supporting wiki users with their everyday tasks. Be-sides that, we demonstrated how Wikulu serves as a flexible environment for (a) visually analyzing the results of NLP algorithms, (b) educational purposes, and (c) enabling semantic wikis. By its modular and flexible architecture, we envision that Wikulu can support wiki users both in small focused environ-ments as well as in large-scale communities such as Wikipedia.

