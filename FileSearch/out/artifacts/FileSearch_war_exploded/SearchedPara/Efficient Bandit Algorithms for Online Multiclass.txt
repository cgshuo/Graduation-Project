 Sham M. Kakade sham@tti-c.org Shai Shalev-Shwartz shai@tti-c.org Ambuj Tewari tewari@tti-c.org In the conventional supervised learning paradigm, the learner has access to a data set in which the true labels of the inputs are provided. While attendant learning algorithms in this paradigm are enjoying wide ranging success, their effective application to a number of do-mains, including many web based applications, hinges on being able to learn in settings where the true la-bels are not fully disclosed, but rather the learning algorithm only receives some partial feedback. Impor-tant domains include both the (financially important) sponsored advertising on webpages and recommender systems. The typical setting is: first, a user queries the system; then using the query and other poten-tially rich knowledge the system has about the user (e.g. past purchases, their browsing history, etc.) the system makes a suggestion (e.g. it presents the user with a few ads they might click on or songs they might buy); finally, the user either positively or negatively re-sponds to the suggestion. Crucially, the system does not learn what would have happened had other sug-gestions been presented.
 We view such problems as naturally being online,  X  X andit X  versions of multiclass prediction problems, and, in this paper, we formalize such a model. In essence, this multiclass bandit problem is as follows: at each round, the learner receives an input x (say the users query, profile, and other high dimensional infor-mation); the learner predicts some class label  X  y (the suggestion); then the learner receives the limited feed-back of only whether the chosen label was correct or not. In the conventional,  X  X ull information X  supervised learning model, a true label y (possibly more than one or none at all) is revealed to the learner at each round  X  clearly unrealistic in the aforementioned applica-tions. In both cases, the learner desires to make as few mistakes as possible. The bandit version of this prob-lem is clearly more challenging, since, in addition to the issues ones faces for supervised learning (e.g. learn-ing a mapping from a high dimensional input space to the label space), one also faces balancing exploration and exploitation.
 This paper considers the workhorse of hypothesis spaces, namely linear predictors, in the bandit setting. Somewhat surprisingly, while there has been a stag-gering number of results on (margin based) linear pre-dictors and much recent work on bandit models, the intersection of these two settings is novel and opens a number of interesting (both theoretical and practical) questions, which we consider. In particular, we pay close attention to the important case where the data are linearly separable, where, in the full information setting, the (efficient) Perceptron algorithm makes a number of mistakes that is asymptotically bounded (so the actual error rate will rapidly converge to 0). There are a number of related results in the bandit literature. The Exp4 algorithm (for the  X  X xperts X  set-ting) of Auer et al. [1998] and the contextual bandit setting of Langford and Zhang [2007] are both bandit settings where the learner has side information (e.g. the input  X  X  X ) when making a decision  X  in fact, our setting can be thought of as a special case of the con-textual bandit setting 1 . However, these settings con-sider abstract hypothesis spaces and do not explicitly consider efficient algorithms. Technically related are the bandit algorithms for online convex optimization of Flaxman et al. [2005], Kleinberg [2004], which at-tempt to estimate a gradient (for optimization) with only partial feedback. However, these algorithms do not apply due to the subtleties of using the non-convex classification loss, which we discuss at the end of Sec-tion 2.
 This paper provides an efficient bandit algorithm, the Banditron, for multiclass prediction using linear hypothesis spaces, which enjoys a favorable mistake bound. We provide empirical results showing our al-gorithm is quite practical. For the case where the data is linearly separable, our mistake bound is O ( rounds. We also provide results toward characterizing the optimal achievable mistake bound for the linearly separable case (ignoring efficiency issues here) and in-troduce some important open questions regarding this issue. In the Extensions section, we also discuss up-date rules which generalize the Winnow algorithm (for L1 margins) and margin-mistake based algorithms to the bandit setting. We also discuss how our algorithm can be extended to ranking and settings where more than one prediction  X  y can be presented to the user (e.g. an advertising setting where multiple ads may be presented). We now formally define the problem of online multi-class prediction in the bandit setting. Online learning is performed in a sequence of consecutive rounds. On round t , the learner is given an instance vector x t  X  R and is required to predict a label out of a set of k pre-defined labels which we denote by [ k ] = { 1 ,...,k } . We denote the predicted label by  X  y t . In the full in-formation case, after predicting the label, the learner receives the correct label associated with x t , which we denote by y t  X  [ k ]. We consider a bandit set-ting, in which the feedback received by the learner is 1 [ X  y t 6 = y t ], where 1 [  X  ] is 1 if predicate  X  holds and 0 otherwise. That is, the learner knows if it predicted an incorrect label, but it does not know the identity of the correct label. The learner X  X  ultimate goal is to mini-mize the number of prediction mistakes, M , it makes along its run, where: To make M small, the learner may update its pre-diction mechanism after each round so as to be more accurate in later rounds.
 The prediction of the algorithm at round t is deter-mined by a hypothesis, h t : R d  X  [ k ], where h t is taken from a class of hypotheses H . In this paper we focus on the class of margin based linear hypotheses. Formally, each h  X  X  is parameterized by a matrix of weights W  X  R k  X  d and is defined to be: where ( W x ) j is the j th element of the vector obtained by multiplying the matrix W with the vector x . Since each hypothesis is parameterized by a weight matrix, we refer to a matrix W also as a hypothesis  X  by that we mean that the prediction is defined as given in Eq. (1). To evaluate the performance of a weight matrix W on an example ( x ,y ) we check whether W makes a prediction mistake, namely determine if arg max j ( W x ) j 6 = y .
 The class of margin based linear hypotheses for mul-ticlass learning has been extensively studied in the full information case [Duda and Hart, 1973, Vapnik, 1998, Weston and Watkins, 1999, Elisseeff and We-ston, 2001, Crammer and Singer, 2003]. Our start-ing point is a simple adaptation of the Perceptron algorithm [Rosenblatt, 1958] for multiclass prediction in the full information case (this adaptation is called Kesler X  X  construction in [Duda and Hart, 1973, Cram-mer and Singer, 2003]). Despite its age and simplicity, the Perceptron has proven to be quite effective in prac-tical problems, even when compared to state-of-the-art large margin algorithms [Freund and Schapire, 1999]. We denote by W t the weight matrix used by the Per-ceptron at round t . The Perceptron starts with the all zero matrix W 1 = 0 and updates it as follows where U t  X  R k  X  d is the matrix defined by In other words, if there is no prediction mistake (i.e. y t =  X  y t ), then there is no update (i.e. W t +1 = W t ), and if there is a prediction mistake, then x t is added to the y t th row of the weight matrix and subtracted from the  X  y t th row of the matrix.
 A relative mistake bound can be proven for the mul-ticlass Perceptron algorithm. The difficulty with pro-viding mistake bounds for any (efficient) algorithm in this setting stems from the fact that the classification loss is non-convex. Hence, performance bounds are commonly evaluated using the multiclass hinge-loss  X  what might be thought of as a convex relaxation of the classification loss. In particular, the hinge-loss of W on ( x ,y ) is defined as follows: ` ( W ; ( x ,y )) = max where [ a ] + = max { a, 0 } is the hinge function. The hinge-loss will be zero only if ( W x ) y  X  ( W x ) r  X  1 for all r 6 = y . The difference ( W x ) y  X  ( W x ) r is a gener-alization of the notion of margin from binary classifi-cation. Let  X  y = argmax r ( W x ) r be the prediction of W . Note that if  X  y 6 = y then ` ( W ; ( x ,y ))  X  1. Thus, the hinge-loss is a convex upper bound on the zero-one loss function, ` ( w ; ( x ,y ))  X  1 [ X  y 6 = y ]. The Perceptron mistake bound holds for any sequence of examples and compares the number of mistakes made by the Perceptron with the cumulative hinge-loss of any fixed weight matrix W ? , even one defined with prior knowledge of the sequence. Formally, let ( x 1 ,y 1 ) ,..., ( x T ,y T ) be a sequence of examples and assume for simplicity that k x t k  X  1 for all t . Let W ? be any fixed weight matrix. We denote by the cumulative hinge-loss of W ? over the sequence of examples and by the complexity of W ? . Here k X k 2 F denotes the Frobe-nius norm. Then the number of prediction mistakes of the multiclass Perceptron is at most, Algorithm 1 The Banditron Parameters:  X   X  (0 , 0 . 5)
Initialize W 1 = 0  X  R k  X  d for t = 1 , 2 ,...,T do end for A proof of the above mistake bound can be found for example in Fink et al. [2006]. The mistake bound in Eq. (7) consists of three terms: the loss of W ? , the complexity of W ? , and a sub-linear term which is often negligible. In particular, when the data is separable (i.e. L = 0), the number of mistakes is bounded by D . Unfortunately, the Perceptron X  X  update cannot be im-plemented in the bandit setting as we do not know the identity of y t . One direction is to work directly with the hinge-loss (which is convex) and try to use the ban-dit algorithms for online convex optimization of Flax-man et al. [2005], Kleinberg [2004]. In this work, they attempt to find an unbiased estimate of the gradient using only bandit feedback (i.e. using only the loss re-ceived as feedback). However, since the only feedback the learner receives is 1 [ X  y t 6 = y t ], one does not neces-sarily even know the hinge-loss for the chosen decision,  X  y , due to dependence of the hinge loss on the true la-bel y t . Hence, the results of Flaxman et al. [2005], Kleinberg [2004] are not directly applicable. We now present the Banditron in Algorithm 1, which is an adaptation of the multiclass Perceptron for the bandit case.
 Similar to the Perceptron, at each round we let  X  y t be the best label according to the current weight matrix W t , i.e.  X  y t = argmax Banditron exploits the quality of the current weight matrix by predicting the label  X  y t . Unlike the Percep-tron, if  X  y t 6 = y t , then we can not make an update since we are blind to the identity of y t . Roughly speaking, it is difficult to learn when we exploit using W t . For this reason, on some of the rounds we let the algo-rithm explore (with probability 1  X   X  ) and uniformly predict a random label from [ k ]. We denote by  X  y t the predicted label. On rounds in which we explore, (so  X  y 6 =  X  y t ), if we additionally receive a positive feedback, i.e.  X  y t = y t , then we indirectly obtain the full informa-tion regarding the identity of y t , and we can therefore update our weight matrix using this positive instance. The parameter  X  controls the exploration-exploitation tradeoff.
 The above intuitive argument is formalized by defining the update matrix  X  U t to be a function of the random-ized prediction  X  y t . We emphasize that  X  U t accesses the correct label y t only through the indicator 1 [ y t =  X  y and is thus adequate for the Bandit setting. As we show later in Lemma 4, the expected value of the Banditron X  X  update matrix  X  U t is exactly the Percep-tron X  X  update matrix U t . While there a number of other variants which also perform unbiased updates, we have found this one provides the most favorable performance (empirically speaking).
 The following theorem provides a bound on the ex-pected number of mistakes the Banditron makes.
 Theorem 1. (Mistake Bound). Assume that for the any matrix, let L be the cumulative hinge-loss of W ? as defined in Eq. (5), and let D be the complexity of W ? (i.e. D = 2 || W ? || 2 F ). Then the number of mistakes M made by the Banditron satisfies E [ M ]  X  L +  X  T + 3 max n kD  X  , p D X  T o + q kDL  X  . where expectation is taken with respect to the random-ness of the algorithm.
 Before turning to the proof of Thm. 1 let us first opti-mize the exploration-exploitation parameter  X  in dif-ferent scenarios. First, assume that the data is sepa-rable, that is L = 0. In this case, we can obtain a mis-take bound of O ( shows that an O ( the cumulative hinge-loss of W ? is small enough. Corollary 2 (Low noise) . Assume that the conditions stated in Thm. 1 hold and that there exists W ? with fixed complexity D and loss L  X  O ( by setting  X  = p k D/T we obtain the bound E [ M ]  X  O (  X  Next, let us consider the case where we have a constant (average) noise level of  X  , i.e. there exists  X   X  (0 , 1) such that L  X   X T . In this case, Corollary 3 (High noise) . Assume that the conditions stated in Thm. 1 hold and that there exists W ? with fixed complexity D and loss L  X   X T for a constant  X   X  (0 , 1) . Then, by setting  X  =  X  ( k D/T ) 1 / 3 we obtain the bound E [ M ]  X   X T (1+ ) where = O (( k D ) 1 / 3 T  X  1 / 3 We note that the bound in the above corollary can be also written in an additive form as: E [ M ]  X  L  X  O ( T 2 / 3 ). However, since we are not giving proper re-gret bounds as we compare mistakes to hinge-loss we prefer to directly bound E [ M ].
 Analysis: To prove Thm. 1 we first show that the random matrix  X  U t is an unbiased estimator of the up-date matrix U t used by the Perceptron. Formally, let E t [  X  U t ] be the expected value of  X  U t conditioned on  X  y ,...,  X  y t  X  1 . Then: Lemma 4. Let  X  U t be as defined in Algorithm 1 and let U t be as defined in Eq. (3). Then, E t [  X  U t ] = U Proof. For each r  X  [ k ] and j  X  [ d ] we have
E which completes the proof.
 Next, we bound the expected squared norm of  X  U t . Lemma 5. Let  X  U t be as defined in Algorithm 1. Then,
E t [ k Proof. We first observe that k  X  U t k 2 F = Therefore, if y t 6 =  X  y t then and if y t =  X  y t then Combining the two cases concludes our proof.
 Equipped with the above two lemmas, we are ready to prove Thm. 1.
 Proof of Thm. 1. Throughout the proof we use the prove the theorem by bounding E [  X  W ? ,W T +1  X  ] from above and from below starting with a lower bound. We can first use the fact that W 1 = 0 to rewrite E Expanding the definition of W t +1 and using Lemma 4 we obtain that for all t ,  X  t = E [  X  W ? ,  X  E [  X  W ? ,U t  X  ] . Next, we note that the definition of the hinge-loss given in Eq. (4) implies that the following holds regardless of the value of  X  y t Therefore  X  t  X  E [ 1 [ X  y t 6 = y t ] ]  X  ` ( W ? , ( x t ming over t we obtain the lower bound where  X  M := P T t =1 1 [ X  y t 6 = y t ] and L is as de-fined in Eq. (5). Next, we show an upper bound on E [  X  W ? ,W T +1  X  ]. Using Cauchy-Schwartz inequal-ease our notation, we use the shorthand k X k for de-noting the Frobenius norm. Using the definition of D given in Eq. (6), the concavity of the sqrt function, and Jensen X  X  inequality we obtain that We therefore need to upper bound the expected value of k W T +1 k 2 . Expanding the definition of W T +1 get that
E [ k W T +1 k 2 ] = E [ k W T k 2 +  X  W T ,  X  U T  X  + k Using Lemma 4 we obtain that E [  X  W t ,  X  U t  X  E [  X  W t ,U t  X  ]  X  0, where the second inequality follows from the definition of U t and  X  y t . Combining this with Lemma 5 and with the assumption k x t k  X  1 for all t we obtain that E Plugging the above into Eq. (9) and using the inequal-ity Comparing the above upper bound with the lower bound given in Eq. (8) and rearranging terms yield Standard algebraic manipulations give the bound
E [  X  M ]  X  L + Finally, our proof is concluded by noting that in ex-pectation we are exploring no more than  X T of the rounds and thus E [ M ]  X  E [  X  M ] +  X T . In this section we present results towards characteriz-ing the optimal achievable rate for the case where the data is separable. Here, in the full-information set-ting, the mistake bound of the Perceptron algorithm is finite and bounded by D . We now present an (inef-ficient) algorithm showing that the achievable mistake bound in the bandit setting is also finite  X  thus the Banditron X  X  mistake bound of O ( cant room for improvement (though the algorithm is quite simple and has reasonable performance, which we demonstrate in the next section).
 First, as a technical tool, we make the interesting ob-servation that the halving algorithm (generalized to the multiclass setting) is also applicable to the bandit setting. The algorithm is as follows: Let H 0 be the cur-rent set of  X  X ctive X  experts, which is initialized to the full set, i.e. H = H 0 at t = 1. At each round t , we pre-dict using the majority prediction r (i.e. the prediction r  X  [ k ] which the most hypotheses in H 0 predict). If we are correct, we make no update. If we are incorrect, we remove from the active set, H 0 , those h  X  X  0 which predicted the incorrect label r . Crucially, this (gener-alized) halving algorithm is implementable with only the bandit feedback that we receive. This algorithm enjoys the following mistake bound.
 Lemma 6. (Halving Algorithm). The halving algo-rithm (in the bandit setting) makes at most k ln |H| mistakes on any sequence in which there exists some hypothesis in H which makes no mistakes.
 Proof. Whenever the algorithm makes a mistake, the size of active set is reduced by at least a 1  X  1 /k fraction, since majority prediction uses a fraction of hypothesis (from the active set) that is at least 1 /k . Since the algorithm never removes a perfect hypothesis from the active set, the maximal number of mistakes, M , that can occur until H 0 consists of only perfect hypotheses satisfies (1  X  1 /k ) M |H| X  1. Using the in-equality (1  X  1 /k )  X  e  X  1 /k and solving for M leads to the claim.
 Using this, the following theorem shows that the achievable bound for the number of mistakes is asymp-totically finite. Unfortunately, the result has a dimen-sionality dependence on d . The algorithm essentially uses the margin condition to construct an appropri-ately sized cover for H , the set of all linear hypotheses, and runs the halving algorithm on this cover.
 Theorem 7. There exists a deterministic algorithm (in the bandit setting), taking D as input, which makes at most O ( k 2 d ln( Dd )) mistakes on any se-quence (where k x t k  X  1 ) that is linearly separable at margin 1 by some W ? , with 2 || W ? || 2 F  X  D . Proof. (sketch) Since the margin is 1, it is straightfor-ward to show that if W is a perturbation of W ? which satisfies || W ?  X  W ||  X   X  O ( 1  X  linearly separable under W . By noting that each co-ordinate in W ? is (rather crudely) bounded by there exists a discretized grid of H of size O ( which contains a linear separator. The algorithm sim-ply runs the halving algorithm on this cover.
 This result is in stark contrast to the Perceptron mis-take bound which has no dependence on the dimen-sion d . We now provide a mistake bound with no de-pendence on the dimension. Unfortunately, it is not asymptotically finite, as it is has a rather mild de-pendence on the time  X  it is O ( D ln T ) (ignoring k and higher order terms), while the Perceptron mistake bound is O ( D ).
 Theorem 8. There exists a randomized algorithm (in the bandit setting), taking as inputs D , T and  X  &gt; 0 , such that with probability greater than 1  X   X  the algo-rithm makes at most O ( k 2 D ln T + k  X  (ln D + ln ln T + k mistakes on any T length sequence (where k x t k  X  1 ) that is linearly separable at margin 1 by some W ? , with 2 || W ? || 2 F  X  D .
 The algorithm first constructs a random projection op-erator which projects any x into a space of dimension d gorithm in this lower dimensional space. The proof essentially consists of using the results in Arriaga and Vempala [2006] to argue that the (multiclass) margin is preserved under this random projection.
 Proof. (sketch) It is simpler to rescale W ? such that || W ? || F = 1 and the margin is 1 / T + k points x 1 to x T and the (row) vectors W ? 1 ,...W whose norms are all bounded by 1. Let P be a matrix of dimension d 0  X  d , where each entry of P is inde-pendently sampled from U (  X  1 , 1). Define the projec-tion operator  X ( v ) = 1  X  and Vempala [2006] (essentially a result from the JL lemma) shows that if d 0 = O ( D ln T + k  X  ) then this pro-jection additively preserves the inner products of these points up to 1 It follows that, after the projection, the data is lin-early separable with margin at least 1  X ( W ? ) denote the matrix where each row of W ? has been projected, then, also by the JL lemma, the norm ||  X ( W ? ) || F will (rather crudely) be bounded by 2 (re-call || W ? || F = 1). Hence, the projected data is linearly separable at margin 1 / (3 has norm O (1), which is identical to being separable at margin 1 with weight vector of complexity O ( D ). The algorithm is to first create a random projection matrix (which can be constructed without knowledge of the sequence) and then we can run the previous algorithm on the lower dimensional space d 0 . Since we have shown that the margin is preserved (up to a constant) in the lower dimensional space, the result follows from the previous Theorem 7, with d 0 as the dimensionality.
 We discuss open questions in the Extensions section. In this section, we report some experimental results for the Banditron algorithm on synthetic and real world data sets. For each data set, we ran Banditron for a wide range of values of the exploration parameter  X  . For each value of  X  , we report the average error rate, where the averaging is over 10 independent runs of Banditron.
 The results are shown on Fig. 1. Each column corre-sponds to one data set. The top figures plot the error rates of Banditron (for the best value of  X  ) and Per-ceptron as a function of the number of examples seen. We show these on a log-log scale to get a better visual indication of the asymptotics of these algorithms. The bottom figures plot the final error rates on the com-plete data set as a function of  X  . As expected, setting  X  too low or too high leads to higher error rates. The first data set, denoted by SynSep , is a 9-class, 400-dimensional synthetic data set of size 10 6 . The idea is to have a simple simulation of generating a text document. The coordinates represent different words in a small vocabulary of size 400. See the caption of Figure 1 for details. We ensure, by construction, that SynSep is linearly separable. The left plots in Figure 1 show the results for this data set. Since this is a sep-arable data set, Perceptron makes a bounded number of mistakes and its error rate plot falls at a rate of 1 /T yielding a slope of  X  1 on a log-log plot. Corollary 2 predicts that error rate for Banditron should decay faster than 1 / imately  X  0 . 55 on the log-log plot. The second data set, denoted by SynNonSep , is constructed in the same way as SynSep except that we introduce 5% la-bel noise. This makes the data set non-separable. The middle plots in Fig. 1 show the results for SynNon-Sep . The Perceptron error rate decays till it drops to 10% and then becomes constant. Banditron does not decay appreciably till 10 4 examples after which it falls rapidly to its final value of 10  X  0 . 89 = 13%. We construct our third data set Reuters4 from the Reuters RCV1 collection. Documents in the Reuters data set can have more than one label. We restrict ourselves to those documents that have exactly one label from the following set of labels: { ccat, ecat, gcat, mcat } . This gives us a 4-class data set of size 673 , 768 which includes about 84% of the documents in the original Reuters data set. We do this because the model considered in this paper assumes that ev-ery instance has a single true label. See the Extensions section for a discussion about dealing with multiple la-bels. We represent each document using bag-of-words, which leads to 346 , 810 dimensions. The right plots in Fig. 1 show the results for Reuters4 . The final er-ror rates for Perceptron and Banditron (  X  = 0 . 05) are 5 . 3% and 16 . 3% respectively. However, it is clear from the top plot that as the number of examples grows, the error rate of Banditron is dropping at a rate com-parable to that of Perceptron. We now discuss a few extensions of the Banditron algo-rithm and some open problems. These extensions may possibly improve the performance of the algorithm and also broaden the set of applications that can be tack-led by our approach. Due space constraints, we confine ourselves to a rather high level overview.
 Label Ranking: So far we assumed that each in-stance vector is associated with a single correct label and we must correctly predict this particular label. In many applications this binary dichotomy is inadequate as each label is associated with a degree of relevance, which reflects to what extent it is relevant to the in-stance vector in hand. Furthermore, it is sometime natural to predict a subset of the labels rather than a single label. For example, consider again the prob-lem of sponsored advertising on webpages described in the Introduction. Here, the system presents the user with a few ads. If the user positively responds to one of the suggestions (say by a  X  X lick X ), this implies that the user prefers this suggestion over the other sugges-tions, but it does not necessarily mean that the other suggestions are completely wrong.
 We now briefly discuss a possible extension of the Ban-ditron algorithm for this case (using techniques from Crammer et al. [2006]). On each round, we first find the r top ranked labels (where ranking is according to  X  w r , x t  X  ). With probability 1  X   X  we exploit and pre-dict these labels. With probability  X  we explore and randomly change one of the top ranked labels with another label which is ranked lower by our model. If we are exploring and the user chooses the replaced la-bel, then we obtain a feedback that can be used for improving our model. The Banditron analysis can be generalized to this case, leading to bounds on the num-ber of rounds in which the user negatively responds to our advertisement system.
 Multiplicative Updates and Margin-Based Up-dates: While deriving the Banditron algorithm, our starting point was the Perceptron, which is an ex-tremely simple online learning algorithm for the full information case. Over the years, many improvements of the Perceptron were suggested (see for example Shalev-Shwartz and Singer [2007] and the references therein). It is therefore interesting to study which al-gorithms can be adapted to the Bandit setting. We conjecture that it is relatively straightforward to adapt the multiplicative update scheme [Littlestone, 1988, Kivinen and Warmuth, 1997] to the bandit setting while achieving mistake bounds similar to the mistake bounds we derived for the Banditron. It is also possi-ble to adapt margin-based updates (i.e. updating also when there is no prediction mistake but only a mar-gin violation) to the bandit setting. Here, however, it seems that the resulting mistake bounds for the low noise case are inferior to the bound we obtained for the Banditron.
 Achievable Rates and Open Problems: The im-mediate question is how to improve our rate of O ( T 2 / 3 to O ( gorithm. We conjecture this is at least possible by some (possibly inefficient) algorithm. Important open questions in the separable case are: What is the opti-mal mistake bound? In particular, does there exist a finite mistake bound which has no dimensionality de-pendence? Furthermore, are there efficient algorithms which achieve the mistake bound of O ( D ln T ), pro-vided in Theorem 8 (or better)? Practically speaking, this last question is of the most importance, as then we would have an algorithm that actually achieves a very small mistake bound in certain cases.

