 Information di usion, viral mark eting, and collectiv e classi-cation all attempt to mo del and exploit the relationships in a net work to mak e inferences about the lab els of nodes. A variet y of techniques have been introduced and metho ds that com bine attribute information and neigh boring lab el information have been sho wn to be e ectiv e for collectiv e lab eling of the nodes in a net work. However, in part because of the correlation between node lab els that the techniques exploit, it is easy to nd cases in whic h, once a misclassi ca-tion is made, incorrect information propagates throughout the net work. This problem can be mitigated if the system is allo wed to judiciously acquire the lab els for a small num ber of nodes. Unfortunately , under relativ ely general assump-tions, determining the optimal set of lab els to acquire is intractable. Here we prop ose an acquisition metho d that learns the cases when a given collectiv e classi cation algo-rithm mak es mistak es, and suggests acquisitions to correct those mistak es. We empirically sho w on both real and syn-thetic datasets that this metho d signi can tly outp erforms a greedy appro ximate inference approac h, a viral mark et-ing approac h, and approac hes based on net work structural measures suc h as node degree and net work clustering. In ad-dition to signi can tly impro ving accuracy with just a small amoun t of lab eled data, our metho d is tractable on large net works.
 I.5.2 [ Pattern Recognition ]: Design Metho dology| Clas-si er design and evaluation Algorithms activ e inference, lab el acquisition, collectiv e classi cation
Information di usion, viral mark eting, and collectiv e clas-si cation all attempt to exploit relationships in a net work to reason and mak e inferences about the nodes in the net-work. The common intuition is that kno wing (or inferring) something about the lab el of a particular node can tell us something useful about the other nodes in the net work. For instance, the lab els of the link ed nodes tend to be corre-lated (not necessarily a positiv e correlation) for man y do-mains; hence, nding the correct lab el of a node is useful for not only that particular node, but it also has an im-pact on the predictions that are made about the rest of the net work. Thus, it has been sho wn that metho ds suc h as col-lectiv e classi cation, i.e., classifying the nodes of a net work sim ultaneously , can signi can tly outp erform the traditional indep enden t lab eling approac hes [2, 7, 13, 15, 24, 26].
However, sometimes, the adv antage of exploiting the rela-tionships can become a disadv antage. An incorrect predic-tion about a particular node (due to an appro ximate infer-ence pro cedure, noise in the data, mo del limitations, etc.) can propagate in the net work and lead to incorrect predic-tions about other nodes. For example, consider a simple binary classi cation problem where an island of nodes that should be lab eled with the positiv e lab el are surrounded with a sea of negativ ely lab eled nodes. The island may be ooded with the lab els of the neigh boring nodes in the sea; this can happ en for example when the mo del prefers intra-class in-teractions over inter-class interactions and achiev es this ob-jectiv e by ooding the island nodes with the lab els of the nodes in the sea.

This problem can be alleviated if the collectiv e inference algorithm is allo wed to judiciously acquire lab els for a small set of nodes in the net work. Dep ending on the applica-tion, lab els can be acquired by asking users to rate speci c items, a compan y can pro vide free samples to a small set of customers and customers' viral net working or purc hasing beha vior can be observ ed, or lab oratory exp erimen ts can be performed to determine protein functions, etc. However, as we sho w later, determining the optimal set of lab els to acquire is intractable under relativ ely general assumptions. Therefore, we are forced to resort to appro ximate and heuris-tic techniques to get practical solutions.

In this pap er, we describ e three polynomial-time lab el ac-quisition strategies. The rst and most obvious approac h is based on appro ximating the objectiv e function (whic h we de ne formally in Section 2) and greedily acquiring the la-bel that pro vides the highest impro vemen t in the objectiv e value. The second approac h is a direct application of a viral mark eting mo del. The third approac h is a simple yet e ec-tive acquisition metho d that learns the cases when a given collectiv e classi cation mo del mak es mistak es, nds islands of nodes that the collectiv e mo del is likely to misclassify , and suggests acquisitions to correct these poten tial mistak es.
We compare these three metho ds to one another and also to acquisition strategies that are based on net work struc-tural measures suc h as node degree and net work clustering. We empirically sho w that the third metho d we prop ose sig-ni can tly outp erforms all of the other metho ds on both real and syn thetic datasets.

The lab el acquisition problem has receiv ed ample atten-tion within the con text of activ e learning [3, 16, 27]. There are two main di erences between the scenario we address and the activ e learning scenario. First, activ e learning has traditionally been concerned with at data; here, we are interested in net work data. The second and the biggest dif-ference is that we assume that we have available an already trained mo del of the domain, and thus the learning has been done oine, but we have the option to acquire lab els to seed the classi cation during inference. This is the setting Rattigan et al. [22] introduced and referred to as \activ e in-ference." They looked at the relational net work classi er, introduced by Macsk assy and Pro vost [14] in whic h there are no node attributes; only lab els are propagated. Here, we build on this, and look at net works in whic h the nodes have attribute information and compare to the structural strategy that they introduced.
 Our con tributions in this pap er are:
We next form ulate the lab el acquisition problem and state the objectiv e function in Section 2. Then, we explain the three approac hes in Sections 3, 4, and 5. We then sho w ex-perimen tal results on both syn thetic and real datasets (Sec-tion 6). We nally discuss related work (Section 7) and future work (Section 8) and then conclude (Section 9).
In this section, we review the collectiv e classi cation prob-lem and de ne the objectiv e function for lab el acquisition for collectiv e classi cation. In this problem, we assume that our data is represen ted as a graph with nodes and edges, G = ( V ; E ). Eac h node V i 2 V is describ ed by an attribute vector X i and a class lab el Y i pair, V i = h X i ; Y i is a vector of individual attributes h X i 1 ; X i 2 ; : : : ; X the domain of Y i is f y 1 ; y 2 ; : : : ; y m g . Eac h edge E describ es some sort of relationship between its endp oints, E ij = h V i ; V j i .

Examples include: 1) social net works, where the nodes are people, the attributes include demographic information suc h as age and income and the edges are friendships: we may be interested in lab eling the people that are likely to par-tak e in some beha vior (e.g., smoking, IV drug use) or have some disease (e.g., a sexually transmitted disease, obesit y), 2) citation net works, where the nodes are publications, the attributes include some con ten t information and the edges are the citations, and we may be interested in nding sem-inal pap ers or categorizing the topics of the pap ers, and 3) biological net works, where the nodes are proteins, attributes include annotations, edges represen t interactions, and we may be interested in inferring protein function.
Often in graph data, the lab els of nodes are correlated (though not necessarily positiv ely correlated). For exam-ple, friends tend to have similar smoking beha viors, pap ers are likely to share the same topic of the pap ers that they cite, proteins are likely to have complemen tary functions. Col lective classi c ation is the term used for sim ultaneously predicting the lab els Y of V in the graph G , where Y denotes the set of lab els of all of the nodes, Y = f Y 1 ; Y 2 ; : : : ; Y
In general, the lab el of a node can be in uenced by its own attributes and the lab els and attributes of other nodes in the graph. There are man y collectiv e classi cation mo d-els prop osed to date that mak e di eren t mo deling assump-tions about these dep endencies. For instance, Neville and Jensen [19], Lu and Geto or [13], Macsk assy and Pro vost [15], and McDo well et al. [18] mak e use of local mo dels, suc h as Naiv e Bayes, Logistic Regression, etc., as a function of the local attributes X i and aggregation of the neigh bor lab els. Chakrabarti et al. [2] considered using the local attributes of the neigh boring nodes and sho wed that it in fact hurts the overall performance. Task ar et al. [26] t a Mark ov ran-dom eld, where the lab els and attributes are the random variables. For the purp oses of this pap er, we tak e this latter approac h and describ e it formally .

Let N i denote the lab els of the neigh boring nodes of V N i = f Y j jh V i ; V j i 2 Eg . We mak e the common Mark ovian assumption that Y i is directly in uenced only by X i and N . Giv en the values of N i , Y i is indep enden t of YnN i is indep enden t of X nf X i g , where X denotes the set of all attribute vectors in the graph, X = f X 1 ; X 2 ; : : : ; X join t probabilit y of P ( YjX ) is then given by: where are compatibilit y functions, and Z is the partition function. Note that these compatibilit y functions, as we de ned them, can be 3 or more dimensional. One simplifying assumption is to assume a pairwise MRF [26]. Then,
P ( YjX ) = We do not discuss the details of de ning the poten tial func-tions. The interested reader can nd more details in [26].
We also assume that we are given a training graph G tr ( V where all of the lab els are kno wn. We train our collectiv e mo del, CM , using this training graph. Giv en the test graph G , a trained mo del CM , and assuming the values of the at-tribute vectors X are kno wn, our goal is to correctly predict Y . We assume we are given a cost for misclassifying a node; when we classify a node as y k whereas the correct assignmen t is y l , we incur a cost of c kl . The exp ected misclassi cation cost ( EM C ) for one node is then given by: EM C ( Y i jX = x; CM ) = min We are trying to nd the join t assignmen t to the Y that minimizes the total exp ected misclassi cation cost,
As men tioned in the introduction, we are interested in settings where we are able to ask for the lab els for some of the nodes. More formally , we consider the case where we can acquire the values for a subset of the lab els AY . The total misclassi cation cost then changes as follo ws:
However, we do not kno w the values of A before we acquire them. Thus, we tak e an exp ectation over possible the values.
Of course, acquiring the value of a lab el is costly . Let the cost of acquiring value of the lab el Y i be C i . Extending it to sets, C ( A ) = P and thus the objectiv e function is the sum of the exp ected misclassi cation cost and the acquisition cost: L ( A ) = P
Giv en a spending budget B , the lab el acquisition problem is then to nd the optimal subset minimizing the sum of exp ected misclassi cation cost and acquisition cost.

The computational complexit y of nding the optimal A dep ends on two main factors. The rst is the computational complexit y of computing and updating the conditional prob-abilities P ( Y i jX = x; A = a ) for the exp ected misclassi ca-tion computations. The second factor is how the searc h space of A is factored when a speci c Y i is acquired. Un-less very restrictiv e assumptions about the structure of the underlying collectiv e mo del are made, suc h as linear dep en-dence on the neigh borho od and attributes, the problem is at least NP PP -complete [10], and this is the case for our mo del CM .

Since nding the optimal solution to the lab el acquisition problem is intractable in a general setting, we must resort to appro ximate techniques. In the next sections, we describ e three suc h techniques. The rst is a greedy strategy based on appro ximate inference. The second is based on an anal-ogy to viral mark eting. The third is a simple yet e ectiv e and intuitiv e approac h based on learning how to correct the mistak es. We also compare to simple strategies based on structural prop erties of the net work.
There are two reasons why nding the optimal solution is intractable: 1) exact probabilit y computation is intractable in general, and 2) unless the probabilit y space for A can be factored, one needs to consider all possible acquisition subsets AY , whic h is exp onen tial in the size of Y .
The rst technique we prop ose, appro ximate inference and greedy acquisition (AIGA), is the most obvious approac h. Instead of computing the exact probabilities when calcu-lating the exp ected misclassi cation costs, we appro ximate them. For instance, when the underlying mo del is an undi-rected graphical mo del, suc h as Mark ov random eld, there are man y appro ximate inference techniques we can mak e use of, suc h as loopy belief propagation [28], variational meth-ods [8], Gibbs sampling [5], etc. If the underlying mo del is a collection of local conditional mo dels, then one can use iterativ e approac hes to compute the probabilities [19, 13]. Moreo ver, instead of considering all possible subsets of Y , we tak e a greedy selection approac h, where we consider eac h lab el indep enden tly and greedily acquire the lab el that minimizes the objectiv e L ( A ).

Let the value aiga ( Y i ) be de ned as the reduction in the objectiv e function when the value of Y i is acquired:
The overall lab el acquisition problem is then solv ed by us-ing appro ximate inference to nd the lab el Y i that has the maxim um value ( Y i ) and whose inclusion in the acquisition set is not going to cause us to exceed the budget, acquire the value for it, and rep eat the pro cess until the budget is exhausted (Algorithm 1). Note that the value calculation at step 7 is essen tially an expected value of information cal-culation [6] and it requires running the inference pro cedure for eac h possible lab el.
 Algorithm 1 : Appro ximate inference and greedy acqui-sition (AIGA) algorithm.

Input : G { the test graph, CM { the learned collectiv e Output : A { the set of acquisitions
A ; while C ( A ) &lt; B do The success of this metho d dep ends on a num ber of things. First, the accuracy of this metho d dep ends hea vily on the precision of the estimated probabilit y values. If the probabil-ity estimates are not well-calibrated, then the exp ected mis-classi cation costs will be incorrect [29], making the value calculations meaningless. Second, the time this acquisition metho d tak es to run dep ends on the time complexit y of the appro ximate inference technique; we need to calculate the value aiga of eac h lab el, whic h requires running the inference algorithm once for eac h node and once for eac h possible value of the lab el. Thus, when the num ber of nodes and the num-ber of possible lab els are high, and if the inference technique is exp ensiv e, this acquisition metho d can be very slow.
Another, simpler, approac h to lab el acquisition is based on an analogy to viral mark eting [9, 23]. In the viral mar-keting setting, we have customers that are poten tial buy-ers of a pro duct and the customers have relationships be-tween eac h other, suc h as family , friendship, co-w ork er, etc. When a customer buys a pro duct, the customer adv ertises it (by word of mouth) to his or her neigh bors in the net work. Through mark eting, we can (hop efully) increase the chance that a customer will buy a pro duct by mark eting to the righ t set of customers. Thus, similar to the lab el acquisition prob-lem, there is then a question of to whic h subset of customers we should mark et, in the hop e that these customers will like the pro duct, buy it, and recommend it to their neigh bors, who will hop efully buy and recommend it in turn.

The analogous mapping to lab el acquisition for collectiv e classi cation is as follo ws. There are nodes (customers) that we need to classify and we have the choice to acquire the la-bels for (mark et to) some of them. Our task is to choose an initial set of lab els to acquire so that the num ber of correctly classi ed nodes (the customers who buy the pro duct) in the end is maximized. This implicitly assumes that the misclas-si cation costs c ij are symmetric and equal; i.e., c ij = c for all i; j .

There are man y viral mark eting approac hes that di er in the form ulation of the problem, the assumptions that they mak e, and the solutions that they o er [9, 23]. Reviewing these work and discussing the di erences is beyond the scop e of this pap er. Our viral mark eting form ulation is based on one of the recen t form ulations, the form ulation of Ric hard-son &amp; Domingos [23], that has an exact solution. We next describ e the details of the form ulation and the mapping.
For the viral mark eting form ulation, for eac h node V i we introduce a new indicator variable T i , whic h indicates whether Y i is predicted correctly . Whether a prediction is correct dep ends on the informativ eness of the attributes X whether its neigh bors N i are correctly classi ed, and whic h lab els are acquired, A . Follo wing [23] we mak e the assump-tion that this probabilit y is a linear com bination of a local probabilit y and a relational probabilit y as follo ws:
P ( T i jN i ; X i ; A ) , i P l ( T i j X i ; A ) + (1 i where i denotes how much an instance dep ends on its local attributes versus its neigh bors, where the local probabilit y P is de ned as: and the relational probabilit y P r is a linear com bination of the statuses of the neigh bors: The probabilit y P ( Y i = y k j X i ) can be computed by learning a classi er on the train graph G tr .

The objectiv e now is to mak e acquisitions so as to maxi-mize the total probabilit y of correctly classifying the nodes in the net work. To nd out whic h lab els will be the most valuable ones, we calculate two intuitiv e measures. The rst one measures how much a unit change in P l ( T i j X i ; A ) will a ect the net work: The second one measures how much an instance's probabil-ity of correct classi cation is increased when we acquire the lab el for it: Then, the e ect that acquiring a lab el Y i will have in the net work, i.e., the value of a lab el is just a pro duct of the two. We omit some of the details about how to deriv e these equa-tions. The interested reader can refer to [23].

The acquisition strategy is then as follo ws. First compute the value vma of eac h lab el, and then iterativ ely acquire the lab el with the highest value until the budget is exhausted (Algorithm 2). Note that because this particular viral mar-keting mo del has an exact solution, the values for the lab els need not be recomputed at eac h step.
 Algorithm 2 : Viral mark eting based acquisition (VMA) algorithm. Assumes uniform costs for the lab els, and assumes the misclassi cation costs are symmetric.

Input : G { the test graph, C i { the acquisition costs Output : A { the set of acquisitions
A ; while C ( A ) &lt; B do
With these assumptions, our form ulation is same as that of [23] with only one subtle di erence. In the viral mark et-ing domain, when a person is mark eted a pro duct, there is still a non-zero probabilit y for that person not buying the pro duct. In lab el acquisition, however, we assume that we can acquire lab els with perfect information; that is, there is no uncertain ty about a node's lab el after we acquire it.
The next metho d that we introduce is based on a simple intuition: The set of instances that the collectiv e classi -cation mo del misclassi es tend to be clustered together be-cause misclassifying one instance mak es it very likely that its neigh bors will be misclassi ed as well (propagation of incor-rect information). Thus, there are islands (or peninsulas) of misclassi cation in the graph { sets of connected nodes that are misclassi ed. If we can nd these islands of misclassi -cation, then we can poten tially trigger correct classi cation of those islands by acquiring lab els for a few of the nodes in these islands. The question is then how to nd the islands of misclassi cation.

We rst focus on nding out when a prediction for a par-ticular node is correct. We again asso ciate a random variable T with eac h V i 2V , denoting whether the prediction for Y was indeed correct. But, this time, instead of using the mo d-eling we did in the viral mark eting approac h, we construct some features that are possible indicators of whether a node is misclassi ed, and we learn a classi er on these features to mo del the dep endence of T i on the constructed features. To perform the learning phase, we use the lab el information of the training graph G tr , and predictions of the collectiv e mo del CM on the training graph. We next describ e the features we constructed for this task.

We construct three simple features, one local, one rela-tional, and one global. Intuitiv ely, the local feature captures how much the attributes disagree with the classi cation de-cisions of the collectiv e mo del CM . The relational feature captures how likely it is that the neigh bors of an instance are also misclassi ed. Lastly , the global feature captures how di eren t the posterior distribution of the classes is from the exp ected prior distribution. We next explain these features in detail and pro vide mathematical de nitions for them.
The local featur e measures how far the prediction of CM is from the truth according to the attributes. Assume that we predict Y i = y j using CM . Then, the local feature for node V i is de ned as: Again, we can compute P ( Y i = y j j X i ) by learning a local classi er on the nodes of the train graph G tr . The intuition behind the local feature lf is that if the attributes of a node disagree with the prediction based on CM , then it is a signal for a possible misclassi cation. The local feature is a mea-sure of the strength of the disagreemen t between the local classi er and the collectiv e mo del.

The relational featur e captures how likely that a node's neigh bors are also misclassi ed. The intuition is that if a node's neigh bors are misclassi ed, then the node itself is probably misclassi ed as well (because the mo del is a collec-tive mo del). There are di eren t possibilities for de ning the relational feature; for instance, it can be de ned as a recur-sive function of T i , and then it can be computed iterativ ely. We tak e the simplest approac h and de ne it as the average of the local features, lf j , of the neigh bors of the node V
Lastly , the glob al featur e captures the di erence between our prior belief about the class distributions and the poste-rior distribution that we get based on the predictions. For example, based on our prior belief, if we exp ect to classify 20% of the nodes with lab el y j , but CM predicts 60% of the nodes as lab el y j , then some of the nodes that are classi ed as y j are probably misclassi ed.
 Let the prior distribution of the class y j be denoted by P rior ( y j ) and let the posterior distribution based on the predictions of CM be denoted by P oster ior ( y j ). The P rior ( y can be estimated from the training graph G tr . Then, we de-ne the global feature for the node V i that is predicted as y as follo ws:
Having constructed these three features, we learn a clas-si er comes from the training graph G tr and the predictions of CM on this training graph. We used logistic regression but any vector based classi er will work. Next, we de ne the value of a particular acquisition. value rac ( Y i ) = ( P ( T i = 0 j lf i ; rf i ; gf i ) &gt; 0 : 5)+ where ( predicate ) = 1 if the predicate is true, 0 other-wise. The value of acquiring the lab el Y i is the num ber of nodes that are misclassi ed in the neigh borho od of this lab el, including itself, that can poten tially be corrected by this acquisition. Our acquisition metho d is then to greedily acquire the lab els that have the highest value, until the bud-get is exhausted (Algorithm 3). We again assume symmetric misclassi cation costs and uniform acquisition costs. Algorithm 3 : Re ect and Correct (RA C) based acqui-sition algorithm. Assumes uniform costs for the lab els, and assumes the misclassi cation costs are symmetric.
Input : G { the test graph, C i { the acquisition costs Output : A { the set of acquisitions
A ; while C ( A ) &lt; B do
We next describ e the exp erimen tal setup and results on some syn thetic and real datasets.
We compared the three metho ds, AIGA, VMA, and RA C, two metho ds that are based on net work structural mea-sures, a random (RND) acquisition metho d, no acquisition (NONE), and perfect information P I , and we rep ort accu-racies on both real and syn thetic datasets.
The rst of the acquisition metho ds based on net work structure is degree acquisition , (DEG); it rst ranks the nodes according to their degree in the graph and then ac-quires lab els until the budget is exhausted. The second metho d, K-Me diods clustering (KM), rst clusters the net-work into a presp eci ed num ber of clusters and then acquires the lab els for the cen ters of the clusters. Rattigan et al. [22] sho wed that KM outp erformed other structural metho ds, suc h as betweenness, degree, closeness, etc.

The accuracies corresp onding to no acquisition, NONE, were useful in two ways. When this metho d performs poorly , it is a good indicator for possible \ o ods" in a graph. We are also able to see how much the acquisitions help ed. The per-fect information P I accuracies, on the other hand, are useful to compare how close the acquisition metho ds are to the op-timal solution. Recall that nding the optimal solution is intractable; thus, we computed P I accuracies by letting the collectiv e classi er look at the lab els of the neigh bors of a node when it is making a decision about the node. Note that P I accuracy can still be sub optimal because P I is evaluated on Y whereas the acquisition metho ds are evaluated on YnA . A can poten tially include the noisy lab els on whic h even the Bayes Optimal classi er can mak e mistak es.

To be able to compare all these metho ds, we assumed uni-form acquisition costs and symmetric misclassi cation costs. Thus, the budget B determined how man y lab els we can ac-quire and we used accuracies to compare the metho ds with eac h other. For both syn thetic and real datasets, we used a pairwise Mark ov random eld [26] as our collectiv e mo del CM and used loopy belief propagation [28] for the inference. As a local mo del, LM , of the attributes, we used Naiv e Bayes, and we used Logistic Regression for RA C. We used Naiv e Bayes for the local attributes because that matc hed the generativ e mo del for the syn thetic data. Naiv e Bayes also performed comparably well with logistic regression on the real data. We used logistic regression for RA C because the features were numeric and logistic regression was able to handle them better than Naiv e Bayes with Gaussians.
We generated syn thetic data using the forest-re graph generation mo del [12]. The forest re mo del is sho wn to exhibit man y real-w orld phenomenon suc h as power law de-gree distribution, small world e ect, and shrinking diame-ters. However, the forest-re metho d, like most random net-work generators, does not generate lab els and attributes for the nodes. In order to lab el the nodes, we used the metho d that Rattigan et al. [22] describ ed, and after generating the lab els for the nodes, we generated attributes for eac h node using a Naiv e Bayes mo del.

For our evaluation, for eac h training graph, we learned our collectiv e, local, and RA C mo dels, and then generated ve test graphs and compared the acquisition metho ds on the test graphs, varying the num ber of lab els acquired. We rep eated this pro cedure ve times. We rep ort average accu-racies over the 25 test graphs.
Follo wing Rattigan et al. [22], we used a forw ard burning probabilit y of 0.4 and a bac kward burning probabilit y of 0.2. We lab eled eac h node with one of 5 possible lab els and generated 20 binary attributes using a simple Naiv e Bayes generation mo del; 4 attributes -indexed by the class -were Figure 1: Accuracy comparison for graphs of 100 nodes. All metho ds signi can tly outp erform the random acquisition. AIGA does worse than other acquisition metho ds. There are not signi can t dif-ferences between VMA, DEG, and KM. RA C out-performs all metho ds, and the di erences are statis-tically signi can t starting with 20% acquisition. generated with a probabilit y of success of 0.65 for the correct lab el and 16 attributes were generated with a probabilit y of success of 0.4 for the other lab els. We varied the num ber of nodes for di eren t exp erimen ts and we rep ort those num bers in the resp ectiv e results sections. We used = 0 : 5 for the VMA approac h, follo wing [23].
Even though the AIGA metho d is a polynomial-time al-gorithm, eac h single acquisition decision requires running inference for eac h node and for eac h possible value of its la-bel. Thus, it is impractical to run AIGA on large graphs. We begin by comparing all the metho ds including AIGA on small graphs, graphs of 100 nodes, and then compare the remaining metho ds on larger graphs of 2000 nodes.
For eac h exp erimen t, we rst rep ort the average degree, assortativit y [20], how well the local mo del LM does on average, and the average perfect information P I accuracies.
The rst set of graphs of 100 nodes had an average degree of 3 : 36 and an assortativit y of 0 : 62. LM had an average accuracy of 0 : 62, NONE had an average accuracy of 0.28, and average P I accuracy was 0 : 80. These large di erences between NONE and LM and NONE and P I are a good in-dication of \ o oding." The big di erence between LM and P I also sho ws that collectiv e classi cation has the poten tial to impro ve dramatically over at classi cation. We varied the percen tage of lab els acquired from 5% to 30% with 5% incremen ts. We sho w the accuracy comparisons for the ac-quisition metho ds in Figure 1.
 There are four imp ortan t results to observ e from Figure 1. The rst one is that lab el acquisition can alleviate ooding and that the choice of whic h nodes to lab el does matter because all informed metho ds outp erformed the random ac-quisition signi can tly at all levels of acquisition.
Second, all other informed acquisition metho ds outp er-Figure 2: Accuracy comparison for graphs of 2000 nodes and high assortativit y. RA C signi can tly out-performs all other metho ds at all percen tages. The di erences between RA C and the closest runner-up is sometimes 10% i.e., 200 nodes. The di erences between VMA, DEG, and KM are not statistically signi can t. formed the AIGA metho d signi can tly. This is surprising at rst, because one would exp ect the AIGA metho d to per-form the best. However, recall that the inference technique is loopy belief propagation, whic h is an appro ximate metho d of probabilit y computation, and it is kno wn to pro duce sub op-timal results when there are man y short cycles in the graph. Giv en the assortativit y of the data, we see that beliefs about the nodes' lab els reinforce eac h other iterativ ely, and thus most of the probabilit y distributions for the nodes' lab els are spik e distributions that spik e at value 1 for one lab el value and at 0 for the other values. Because the probabili-ties are extreme, the value aiga computations for lab els were not very discriminativ e.

The third observ ation is that KM did not perform better than the DEG metho d. This is in con trast to the results that Rattigan et al. [22] observ ed. There are at least three possible explanations for DEG performing equally well, or sometimes better than KM. The rst reason is that we use attributes in our setup whereas in their setup only node lab els were used. The second reason is our collectiv e mo del is a pairwise MRF whereas they used a relational neigh bor classi er [14]. And a third reason may be that the DEG heuristic is breaking cycles and thus allo wing the loopy belief propagation to con verge to the correct distribution.
The nal observ ation is that the RA C metho d outp er-formed all the other metho ds at all levels of acquisition. The di erences between RA C and AIGA and RA C and KM are statistically signi can t at all percen tages. The di er-ence between RA C and VMA and RA C and DEG became statistically signi can t at percen tage 20% and remained sig-ni can t thereafter.

We next compare the acquisition metho ds except AIGA on much larger graphs, graphs of 2000 nodes. We varied the level of assortativit y to compare the metho ds at di eren t settings. We rst discuss results for high assortativit y. Figure 3: Accuracy comparison for graphs with low assortativit y. We observ e similar trends with the previous exp erimen ts. The only di erence is that the accuracy gaps are not as big anymore, because nodes have a lesser impact on their neigh bors due to lower assortativit y.

The average degree for highly assortativ e graphs was 3 : 83 and the mean assortativit y was 0 : 80. LM had an average accuracy of 0 : 51 and P I accuracy as 0 : 93. We varied the percen tage of lab els acquired from 1% to 5% with 1% in-cremen ts, and we also sho w comparison at 10% to sho w the longer term trends. We sho w the accuracy comparisons for the acquisition metho ds in Figure 2.

We observ e similar trends with the exp erimen ts on the larger graphs, only at a ner detail. Flo oding is a problem that needs to be dealt with and RA C outp erforms all other metho ds signi can tly at all percen tages. The di erence be-tween RA C and the closest runner-ups are sometimes as high as 10% (i.e., 200 more nodes are lab eled correctly by RA C), and the di erence between RA C and random acquisition is sometimes as high as 20%. As for VMA and DEG, there is not a clear winner. Both approac hes outp erform KM ini-tially , but KM catc hes up, and even outp erforms them when we acquire 10% of the lab els, though the di erences are not statistically signi can t at p = 0 : 05.

It is imp ortan t to note that the forest re mo del generates graphs with power-la w degree distributions. That is, while there are some high degree nodes, there are not that man y high degree nodes in graph. Thus, after a certain num ber of lab els are acquired by the DEG metho d, the lab els corre-sponding to the high degree nodes will already be acquired, and DEG will be choosing any regular node. When that happ ens, KM becomes a more intelligen t metho d.

We nally presen t results on graphs with lower assortativ-ity. The average degree was 3 : 78 and the mean assortativit y was 0 : 61. LM had an average accuracy of 0 : 51 and P I accu-racy as 0 : 85. We sho w the accuracy comparisons in Figure 3.
We again observ e similar trends; the only di erence is that the impro vemen t over random acquisition is not quite as pro-nounced because acquiring a lab el for a node does not a ect its neigh bors as much anymore due to lower assortativit y. Figure 4: Accuracy comparison for the Cora dataset. The RA C metho d performed signi can tly better the other metho ds, a di erence of up to 18% compared to the closest runner-up. The di erences between the other metho ds were not statistically signi can t.
We exp erimen ted on two real publication datasets that are publicly available, the Cora dataset [17] and the Cite-Seer dataset [4]. The Cora dataset con tains a 2708 mac hine learning pap ers that are divided into 7 classes, while Cite-Seer dataset has 3312 documen ts that are divided into 6 classes.

Our evaluation metho dology is sligh tly di eren t from the general practice. In real life scenarios, we usually have much smaller lab eled data compared to the unlab eled data. This di erence in the prop ortion mak es the interactions between the unlab eled nodes more common than the interactions be-tween the lab eled and unlab eled nodes. To mimic these two observ ations, we adopted the follo wing evaluation strategy .
We divided eac h dataset into three disjoin t splits and re-peatedly trained on one split and tested on the remaining two (in con trast to training on two splits and testing on the other). Additionally , we did not mak e use of the edges between the lab eled nodes and the unlab eled ones during inference.

Because of these changes in the evaluation strategy , whic h we believ e results in a more realistic evaluation, the accura-cies corresp onding to NONE are very low compared to the num bers rep orted in the literature. The primary reason is that the test graph is more amenable to ooding now, be-cause it is large and there are no interactions between the test graph and the training graph. However, the P I accu-racies are close to the previously rep orted num bers [24].
We rst presen t results on the Cora dataset. The Cora splits had an average degree of 3 : 32 and mean assortativit y of 0 : 79, whic h is relativ ely high. This lead to a noticeable dif-ference between the LM performance and P I performance, whic h were 0 : 61 and 0 : 77 resp ectiv ely. The accuracy com-parisons for the Cora dataset are presen ted in Figure 4.
We observ ed similar trends in Cora to the ones we ob-serv ed in the syn thetic datasets. RA C performed signi -Figure 5: Accuracy comparison for the CiteSeer dataset. RA C outp erformed all other metho ds sig-ni can tly. The di erences between the other meth-ods were not statistically signi can t. can tly better than other metho ds, achieving up to 18% ac-curacy di erence over the closest runner-up, whic h is the DEG metho d. The DEG metho d outp erformed both VMA and KM but the di erences were not statistically signi can t. Interestingly , VMA started with a low accuracy compared to others but impro ved more steeply .

We nally presen t results on the CiteSeer dataset. The splits had an average degree of 2 : 71 and a mean assortativ-ity of 0 : 68, whic h was lower compared to that of Cora, thus the di erence between LM and P I was not as large; LM performance was 0 : 57 and P I performance was 0 : 61. How-ever, even with lower assortativit y, we observ ed the \island e ect," the accuracies without any acquisition was consider-ably low (Figure 5). For this dataset, we observ ed exactly the same trends we had for the Cora dataset.

We ran some preliminary exp erimen ts using Iterativ e Clas-si cation Algorithm [13] instead of a pairwise MRF as the underlying collectiv e mo del and observ ed similar results. We omit the results due to space limitations.
Substan tial researc h has been done on the area of activ e learning [3, 27, 16]. While activ e learning is related to lab el acquisition during inference, the aim for activ e learning is to acquire lab els to learn a good mo del. We are instead acquiring lab els during inference.

Another related area is viral (or targeted) mark eting [9, 11, 21, 23], where a subset of customers need to be selected for targeted adv ertisemen t so as to maximize the pro duct sales. We sho wed how viral mark eting is related to lab el acquisition and used Ric hardson &amp; Domingos's mo del [23] to compare against. Other mo dels could very well be used and compared against; one of the reasons we chose [23] is the fact that the exact solution was tractable.

The work in feature-v alue acquisition during testing [1, 25] is very related to the lab el acquisition problem; however, the focus has been on acquiring feature values, not lab els, and also they considered acquisition for non-graph data.
The most related work is that of Rattigan et al. [22]. As far as we kno w, they are the rst to publish directly on lab el acquisition during inference. They compared di eren t acqui-sition metho ds based on net work structural measures, suc h as degree and betweenness, and they suggested a metho d based on clustering the net work and empirically sho wed that the clustering metho d performed the best. They assumed that the nodes did not have any attributes, thus their metho d did not require any training data. We made di eren t as-sumptions about the data, that is, the nodes had attributes and we had some training data available. We implemen ted their metho d and compared with it.

Finally , cautious inference [18, 19] can be used to alleviate the problem of propagation of incorrect information. We explored using cautious inference, and it help ed when the islands of misclassi cations were small, but it did not solv e the problem when the ma jorit y of the net work was ooded with a small num ber of lab els.
One of the limitations of the RA C metho d is that it is based on the assumptions that the misclassi cation costs are symmetric and the acquisition costs are uniform. The second assumption can be lifted by making use of the probabilities that the RA C classi er pro duces about whether a node is misclassi ed. However, lifting the rst assumption requires further researc h.

The RA C metho d can very well be applied to the viral mark eting domain. RA C can be trained on some lab eled data and can be used to nd out \islands of non-buy ers," as we used it to nd islands of misclassi cation. Then, targeted adv ertisemen t can be done to those islands.
We form ulated the problem of lab el acquisition during in-ference and discussed why it is an imp ortan t and hard prob-lem. We describ ed three informed metho ds and compared them to two metho ds that are based on net work structural measures. The rst metho d that we describ ed is the most straigh tforw ard one and is based on appro ximate inference and greedy acquisition. We sho wed that it does not perform well in practice. We describ ed another metho d that is a di-rect application of viral mark eting to lab el acquisition. This metho d performed equally well with the net work structural metho ds. Finally , we prop osed a metho d that is based on learning when a collectiv e mo del mak es mistak es and sug-gests acquisitions to correct those mistak es. We empirically sho wed that this metho d signi can tly outp erformed all other metho ds on both real and syn thetic datasets.
 Ackno wledgmen ts: This work was supp orted by the National Science Foundation, NSF #0423845 and NSF #0438866, with additional supp ort from AR O #W911NF0710428.
