 exploitation of the apparently best ones.
 k is pulled, its mean-reward  X  Moreover, given the mean-reward  X  new arm is -optimal is of order  X  for small , i.e. P (  X  write f ( ) =  X ( g ( )) for  X  0 when  X  c discovered in order to gain information about its actual mea n-reward). before (discovery).
 Let us write k R n = n X   X   X  P the rewards we would have obtained by drawing an optimal arm ( an arm having a mean-reward arm-pulling strategy such as to minimize this regret.
 Overview of our results: We write v for all n  X  n whether  X   X  = 1 or  X   X  &lt; 1 . For  X   X  = 1 , our algorithms are such that E R we derive the lower-bound: for any  X  &gt; 0 ,  X   X   X  1 , any algorithm satisfies E R arm-increasing rule.
 in cases where there are many arms with small variance.
 as learning strategy have a regret E R strategy: E R that is, the upper bounds on the expected regret E R mentioned above, where the outcomes may take several possib le values.  X  and  X  are random variables. Our assumptions are the following: (C) there is a function V : [0 , 1]  X  R such that P {  X  2  X  V (  X   X   X   X  ) } = 1 . (B) implies that there exists positive constants c this happens when  X   X  = 1 ).
 Let X variables with common expected value denoted  X  arm k . Let T Bound) algorithm as introduced in [2]. Let ( E explores. For any arm k and nonnegative integers s, t , introduce with the convention 1 / 0 = +  X  . Define the UCB-V (for Variance estimate) policy: From [2, Theorem 1], the main property of B for any s  X  [0 , t ] we have  X  quantity at time t which upper bounds  X  quence ( E decreasing probability with time. 2.1 UCB revisited for the infinitely many-armed bandit (to have B we want that the best of the K arms has an expected reward close to the best possible arm. quence (e.g. such as E E t = 2 log t or discover new arms.
 We will start our analysis by considering the following UCB-V(  X  ) algorithm:
UCB-V(  X  ) algorithm : Given parameters K and the exploration sequence ( E Theorem 1 If the exploration sequence satisfies 2 log(10 log t )  X  E K  X  2 the expected regret of the UCB-V(  X  ) algorithm satisfies: arm from the pool, and where C is a positive constant depending only on c Proof: The UCB-V(  X  ) algorithm has two steps: randomly choose K arms and run a UCB sub-of the algorithm. In particular we consider in the following that  X  equality (obtained using Wald X  X  theorem): with  X  near-optimal arms, it is very unlikely that suboptimal arms are often drawn. Lemma 1 For any real number  X  and any positive integer u , we have where the expectations and probabilities are conditioned o n the set of selected arms. Proof: We have T that there exists k 0 6 = k such that for any s 0  X  [0 , t ] and any s  X  [ u, t ] , B from different arms, we obtain Lemma 1.
 Now we use Inequality (6) with  X  =  X   X  +  X  k larger than 32  X  2 k of (6) are small. Precisely, for any s  X  u and t  X  n , we have where in the last step we used Bernstein X  X  inequality twice. Summing up we obtain probabilities in (6). Since  X  =  X   X   X   X   X  the bounds of the different terms of (6) leads to with N T ( n )  X  n , the previous inequality can be simplified into from the first step of UCB-V(  X  ). The quantities  X  satisfying almost surely  X  2  X   X   X   X / 2 for a given  X  . Conditioning on  X  1 =  X  , the quantity N  X  with parameters K  X  1 and p , hence E (2  X  N  X  1 |  X  with  X  ( u ) = u (1  X  c 1)  X  ) u  X  so that  X  ( u )  X   X  ( u 0 ) with u 0 = 1 Let us take u to ensure u constant C depending on c indicates the difference between the UCB subroutine X  X  perf ormance and the best drawn arm. 2.2 Strategy for fixed play number Inequality (4). This leads to the following UCB-F (for Fixed horizon) algorithm.
UCB-F (fixed horizon) : given total number of plays n , and parameters  X   X  and  X  of (1) Theorem 2 For any n  X  2 , the expected regret of the UCB-F algorithm satisfies with C a constant depending only on c Proof: The result comes from Theorem 1 by bounding the expectation E = E V ( X ) theorem and Inequality (2), we have Putting these bounds in Theorem 1, we get with C a constant only depending on c taken of the order of the minimizer of these bounds up to a loga rithmic factor. E subroutine has difficulties in achieving low regret.
 is improvable. This remains an open problem. small enough constant c depending on c on c regret, it leads to a regret larger than C 00 c  X   X  n  X / (1+  X  ) with C 00 depending on c 2.3 Strategy for unknown play number To apply the UCB-F algorithm we need to know the total number o f plays n and we choose the to time into the set of sampled arms. Let K set K
UCB-AIR (Arm-Increasing Rule): given parameters  X   X  and  X  of (1), the same properties as the UCB-F algorithm (proof omitted fr om this extended abstract). with C a constant depending only on c x  X  (the number of maxima is assumed to be finite), i.e. [6] provides upper-and lower-bounds on the regret R actually does not satisfy our local assumption (14).
 E R n =  X  O ( improved rate E R a  X  X niform X  discretization of the domain.
 sampled at areas where  X  is high). For any dimension d , they obtain E R where d 0  X  d is their  X  X ooming dimension X . Under assumptions (14) we ded uce d 0 =  X   X  1 the Euclidean distance as metric, thus their regret is E R than algorithms specifically designed for continuum armed b andits.
