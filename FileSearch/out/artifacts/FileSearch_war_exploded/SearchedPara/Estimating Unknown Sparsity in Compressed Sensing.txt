 Miles E. Lopes mlopes@stat.berkeley.edu The central problem of compressed sensing (CS) is to estimate an unknown signal x  X  R p from n linear mea-surements y = ( y 1 ,...,y n ) given by where A  X  R n  X  p is a user-specified measurement ma-trix,  X  R n is a random noise vector, and n is much smaller than the signal dimension p . During the last several years, the theory of CS has drawn widespread attention to the fact that this seemingly ill-posed prob-lem can be solved reliably when x is sparse  X  in the sense that the parameter k x k 0 := card { j : x j 6 = 0 } is much less than p . For instance, if n is approxi-be achieved with high probability when A is drawn from a Gaussian ensemble (Donoho, 2006; Cand`es et al., 2006). Along these lines, the value of the pa-rameter k x k 0 is commonly assumed to be known in the analysis of recovery algorithms  X  even though it is typically unknown in practice. Due to the funda-mental role that sparsity plays in CS, this issue has been recognized as a significant gap between theory and practice by several authors (Ward, 2009; Eldar, 2009; Malioutov et al., 2008). Nevertheless, the liter-ature has been relatively quiet about the problems of estimating this parameter and quantifying its uncer-tainty. 1.1. Motivations and the role of sparsity At a conceptual level, the problem of estimating k x k 0 is quite different from the more well-studied prob-lems of estimating the full signal x or its support set S := { j : x j 6 = 0 } . The difference arises from sparsity assumptions. On one hand, a procedure for estimating k x k 0 should make very few assumptions about sparsity (if any). On the other hand, methods for estimating x or S often assume that a sparsity level is given, and then impose this value on the solution sequently, a simple plug-in estimate of k x k 0 , such as k b x k 0 or card( b S ), may fail when the sparsity assump-tions underlying To emphasize that there are many aspects of CS that depend on knowing k x k 0 , we provide several examples below. Our main point here is that a method for esti-mating k x k 0 is valuable because it can help to address a broad range of issues.
  X  Modeling assumptions. One of the core mod- X  The number of measurements. If the choice  X  The measurement matrix. Two of the most  X  Recovery algorithms. When recovery al-1.2. An alternative measure of sparsity Despite the important theoretical role of the param-eter k x k 0 in many aspects of CS, it has the practical drawback of being a highly unstable function of x . In particular, for real signals x  X  R p whose entries are not exactly equal to 0, the value k x k 0 = p is not a useful description of the effective number of coordinates. In order to estimate sparsity in a way that accounts the ` 0 norm with a  X  X oft X  version. More precisely, we would like to identify a function of x that can be interpreted like k x k 0 , but remains stable under small perturbations of x . A natural quantity that serves this purpose is the numerical sparsity which always satisfies 1  X  s ( x )  X  p for any non-zero Rickard, 2009; Hoyer, 2004; Lopes et al., 2011), it does not seem to be well known as a sparsity measure in CS. A key property of s ( x ) is that it is a sharp lower bound on k x k 0 for all non-zero x , which follows from applying the Cauchy-Schwarz in-equality to the relation k x k 1 =  X  x, sgn( x )  X  . (Equality in (3) is attained iff the non-zero coordinates of x are equal in magnitude.) We also note that this inequal-ity is invariant to scaling of x , since s ( x ) and k x k individually scale invariant. In the opposite direction, it is easy to see that the only continuous upper bound on k x k 0 is the trivial one: If a continuous function f satisfies k x k 0  X  f ( x )  X  p for all x in some open subset of
R p , then f must be identically equal to p . (There is a dense set of points where k x k 0 = p .) Therefore, we must be content with a continuous lower bound.
 The fact that s ( x ) is a sensible measure of sparsity for non-idealized signals is illustrated in Figure 1. In essence, if x has k large coordinates and p  X  k small coordinates, then s ( x )  X  k , whereas k x k 0 = p . In the left panel, the sorted coordinates of three different vectors in R 100 are plotted. The value of s ( x ) for each vector is marked with a triangle on the x-axis, which shows that s ( x ) adapts well to the decay profile. This idea can be seen in a more geometric way in the mid-dle and right panels, which plot the the sub-level sets S c := { x  X  R p : s ( x )  X  c } with c  X  [1 ,p ]. When c  X  1, the vectors in S c are closely aligned with the coordi-nate axes, and hence contain one effective coordinate. As c  X  p , the set S c includes more dense vectors until S 1.3. Related work.
 Some of the challenges described in Section 1.1 can be approached with the general tools of cross-validation (CV) and empirical risk minimization (ERM). This approach has been used to select various parameters, such as the number of measurements n (Malioutov et al., 2008; Ward, 2009), the number of OMP it-erations k (Ward, 2009), or the Lasso regularization parameter  X  (Eldar, 2009). At a high level, these methods consider a collection of (say m ) solutions b x of some tuning parameter of interest. For each so-lution, an empirical error estimate puted, and the value  X  j  X  corresponding to the smallest c err( Although methods based on CV/ERM share common motivations with our work here, these methods dif-fer from our approach in several ways. In particular, the problem of estimating a soft measure of sparsity, such as s ( x ), has not been considered from that angle. Also, the cited methods do not give any theoretical guarantees to ensure that the estimated sparsity level is close to the true one. Note that even if an estimate b x has small error k to be close to k x k 0 . This point is especially relevant when one is interested in identifying a set of important variables or interpreting features.
 From a computational point view, the CV/ERM ap-proaches can also be costly  X  since computed from a separate optimization problem for for each choice of the tuning parameter. By contrast, our method for estimating s ( x ) requires no optimiza-tion, and can be computed easily from just a small set of preliminary measurements. 1.4. Our contributions.
 The primary contribution of this paper is our treat-ment of unknown sparsity as a parameter estimation problem . Specifically, we identify a stable measure of sparsity that is relevant to CS, and propose an efficient estimator with provable guarantees. Secondly, we are not aware of any other papers that have demonstrated a distinction between random and deterministic mea-surements with regard to unknown sparsity (as in Sec-tion 4).
 The remainder of the paper is organized as follows. In Section 2, we show that a principled choice of n can be made if s ( x ) is known. This is accomplished by formulating a recovery condition for the Basis Pursuit algorithm directly in terms of s ( x ). Next, in Section 3, we propose an estimator free confidence interval for s ( x ). The procedure is also shown to extend to the problem of estimating a soft measure of rank for matrix-valued signals. In Section 4 we show that the use of randomized measurements is essential to estimating s ( x ). Finally, we present simu-lations in Section 5 to validate the consequences of our theoretical results. Due to space constraints, we defer all of our proofs to the supplement.
 0 and x  X  R p , which only corresponds to a genuine norm for q  X  1. For sequences of numbers a n and b n , we write a n . b n or a n = O ( b n ) if there is an absolute constant c &gt; 0 such that a n  X  cb n for all large n . If a /b n  X  0, we write a n = o ( b n ). For a matrix M , we define the Frobenius norm k M k F = q P i,j M 2 ij , the matrix ` 1 -norm k M k 1 = P i,j | M ij | . Finally, for two matrices A and B of the same size, we define the inner product  X  A,B  X  := tr( A &gt; B ). The purpose of this section is to present a simple proposition that links s ( x ) with recovery conditions for the Basis Pursuit algorithm (BP). This is an im-portant motivation for studying s ( x ), since it implies that if s ( x ) can be estimated well, then n can be cho-sen appropriately. In other words, we offer an adaptive choice of n .
 In order to explain the connection between s ( x ) and recovery, we first recall a standard result (Cand`es et al., 2006) that describes the ` 2 error rate of the BP algorithm. Informally, the result assumes that the noise is bounded as k k 2  X  0 for some con-stant 0 &gt; 0, the matrix A  X  R n  X  p is drawn from a suitable ensemble, and n satisfies n &amp; T log( pe/T ) for some T  X  { 1 ,...,p } with e = exp(1). The conclusion is that with high probability, the solution b x  X  argmin {k v k 1 : k Av  X  y k 2  X  0 ,v  X  R p } satisfies where x T  X  R p is the best T -term approximation 1 to x , and c 1 ,c 2 &gt; 0 are constants. This bound is a fun-damental point of reference, since it matches the mini-max optimal rate under certain conditions (Cand`es, 2006), and applies to all signals x  X  R p (rather than just k -sparse signals). Additional details may be found in (Cai et al., 2010) [Theorem 3.3], (Vershynin, 2010) [Theorem 5.65].
 We now aim to answer the question,  X  X f s ( x ) were known, how large should n be in order for to x ? X  Since the bound (4) assumes n &amp; T log( pe/T ), our question amounts to choosing T . For this purpose, it is natural to consider the relative ` 2 error so that the T -term approximation error 1  X  does not depend on the scale of x (i.e. invariant under x 7 X  cx with c 6 = 0).
 Proposition 1 below shows how knowledge of s ( x ) al-lows us to control the approximation error. Specifi-cally, the result shows that the condition T &amp; s ( x ) is necessary for the approximation error to be small, and the condition T &amp; s ( x ) log( p ) is sufficient. Proposition 1. Let x  X  R p \{ 0 } , and T  X  X  1 ,...,p } . The following statements hold for any c 0 , X  &gt; 0 . (i) If the T -term approximation error satisfies Remarks. A notable feature of these bounds is that they hold for all non-zero signals. In our sim-ulations in Section 5, we show that choosing n = 2 d b s ( x ) e log( p/ d to accurate reconstruction across many sparsity levels. In this section, we give a simple procedure to estimate s ( x ) for any x  X  R p \{ 0 } . The procedure uses a small number of measurements, makes no sparsity assump-tions, and requires very little computation. The mea-surements we prescribe may also be re-used to recover the full signal after s ( x ) has been estimated. The results in this section are based on the measure-ment model (1), written in scalar notation as We assume only that the noise variables i are inde-pendent, and bounded by | i | X   X  0 , for some constant  X  0 &gt; 0. No additional structure on the noise is needed. 3.1. Sketching with stable laws Our estimation procedure derives from a technique known as sketching in the streaming computation lit-erature (Indyk, 2006). Although this area deals with problems that have mathematical connections to CS, the use of sketching techniques in CS does not seem to be well known.
 For any q  X  (0 , 2], the sketching technique offers a way to estimate k x k q from a set of randomized lin-ear measurements. In our approach, we estimate s ( x ) = k x k 2 1 / k x k 2 2 by estimating k x k 1 and k x k separate sets of measurements. The core idea is to generate the measurement vectors a i  X  R p using sta-ble laws (Zolotarev, 1986).
 Definition 1. A random variable V has a symmet-ric stable distribution if its characteristic function is of the form E [exp( q  X  (0 , 2] and some  X  &gt; 0. We denote the distribu-tion by V  X  S q (  X  ), and  X  is referred to as the scale parameter.
 The most well-known examples of symmetric stable laws are the cases of q = 2 , 1, namely the Gaussian distribution N (0 , X  2 ) = S 2 (  X  ), and the Cauchy dis-drawn from S q (  X  ), we write a 1  X  S q (  X  )  X  p . The con-nection with ` q norms hinges on the following property of stable distributions (Zolotarev, 1986).
 Lemma 1. Suppose x  X  R p , and a 1  X  S q (  X  )  X  p with parameters q  X  (0 , 2] and  X  &gt; 0 . Then, the random variable  X  x,a 1  X  is distributed according to S q (  X  k x k a ,...,a n from S q (  X  )  X  p and let y i =  X  a i ,x  X  , then y ,...,y n is an i.i.d. sample from S q (  X  k x k q ). Hence, in the special case of noiseless linear measurements, the task of estimating k x k q is equivalent to a well-studied univariate problem: estimating the scale parameter of a stable law from an i.i.d. sample .
 When the y i are corrupted with noise, our analysis shows that standard estimators for scale parameters are only moderately affected. The impact of the noise can also be reduced via the choice of  X  when generating a i  X  S q (  X  )  X  p . The  X  parameter controls the  X  X nergy level X  of the measurement vectors a i . (Note that in the Gaussian case, if a 1  X  S 2 (  X  )  X  p , then E k a 1 k 2 2 In our results, we leave  X  as a free parameter to show how the effect of noise is reduced as  X  is increased. 3.2. Estimation procedure for s ( x ) Two sets of measurements are used to estimate s ( x ), and we write the total number as n = n 1 + n 2 . The first set is obtained by generating i.i.d. measurement vectors from a Cauchy distribution, The corresponding values y i are then used to estimate k x k 1 via the statistic which is a standard estimator of the scale parameter of the Cauchy distribution (Fama &amp; Roll, 1971; Li et al., 2007). Next, a second set of i.i.d. measurement vectors are generated from a Gaussian distribution In this case, the associated y i values are used to com-pute an estimate of k x k 2 2 given by which is a natural estimator of the variance of a Gaus-sian distribution. Combining these two statistics, our estimate of s ( x ) = k x k 2 1 / k x k 2 2 is defined as 3.3. Confidence interval.
 The following theorem describes the relative error Our result is stated in terms of the noise-to-signal ratio and the standard Gaussian quantile z 1  X   X  , which satis-fies  X ( z 1  X   X  ) = 1  X   X  for any coverage level  X   X  (0 , 1). In this notation, the following parameters govern the width of the confidence interval, and we write these simply as  X  n and  X  n . As is standard in high-dimensional statistics, we allow all of the model parameters p,x, X  0 and  X  to vary implicitly as func-ity, we choose to take measurement sets of equal sizes, n 1 = n 2 = n/ 2, and we place a mild constraint on  X  , namely  X  n (  X , X  ) &lt; 1. (Note that standard algorithms such as Basis Pursuit are not expected to perform well unless  X  1, as is clear from the bound (5).) Lastly, we make no restriction on the growth of p/n , which makes Theorem 1. Let  X   X  (0 , 1 / 2) and x  X  R p \{ 0 } . As-sume that model (6) holds. Suppose also that n 1 = n 2 = n/ 2 and  X  (  X , X  ) &lt; 1 for all n . Then as ( n,p )  X  X  X  , we have Remarks. The most important feature of this result is that the width of the confidence interval does not depend on the dimension or sparsity of the unknown signal. Concretely, this means that the number of mea-surements needed to estimate s ( x ) to a fixed precision is only O (1) with respect to the size of the problem. Our simulations in Section 5 also show that the relative error of of x . Lastly, when  X  n and  X  n are small, we note that the relative error | ( n  X  1 / 2 +  X  ) with high probability, which follows from 3.4. Estimating rank and sparsity of matrices The framework of CS naturally extends to the problem of recovering an unknown matrix X  X  R p 1  X  p 2 on the basis of the measurement model where y  X  R n , A is a user-specified linear operator from R p 1  X  p 2 to R n , and  X  R n is a vector of noise variables. In recent years, many researchers have ex-plored the recovery of X when it is assumed to have sparse or low rank structure. We refer to the pa-pers (Cand`es &amp; Plan, 2011; Chandrasekaran et al., 2010) for descriptions of numerous applications. In analogy with the previous section, the parameters rank( X ) or k X k 0 play important theoretical roles, but are very sensitive to perturbations of X . Likewise, it is of basic interest to estimate stable measures of rank and sparsity for matrices. Since the sparsity analogue forward extension of Section 3.2, we restrict our atten-tion to the more distinct problem of rank estimation. 3.4.1. The rank of semidefinite matrices In the context of recovering a low-rank positive semidefinite matrix X  X  S p  X  p + \{ 0 } , the quantity sparse vector. If we let  X  ( X )  X  R p + denote the vec-tor of ordered eigenvalues of X , the connection can be made explicit by writing rank( X ) = k  X  ( X ) k 0 . As in Section 3.2, our approach is to consider a robust alternative to the rank. Motivated by the quantity as our measure of the effective rank for non-zero X , which always satisfies 1  X  r ( X )  X  p . The quan-tity r ( X ) has appeared elsewhere as a measure of rank (Lopes et al., 2011; Tang &amp; Nehorai, 2010), but is less well known than other rank relaxations, such as the numerical rank k X k 2 F k X k 2 op (Rudelson &amp; Ver-shynin, 2007). The relationship between r ( X ) and rank( X ) is completely analogous to s ( x ) and k x k Namely, we have a sharp, scale-invariant inequality The quantity r ( X ) is more stable than rank( X ) in the sense that if X has k large eigenvalues, and p  X  k small eigenvalues, then r ( X )  X  k , whereas rank( X ) = p . Our procedure for estimating r ( X ) is based on esti-mating tr( X ) and k X k 2 F from separate sets of measure-ments. The semidefinite condition is exploited through the basic relation  X  I p  X  p ,X  X  = tr( X ) = k  X  ( X ) k estimate tr( X ), we use n 1 linear measurements of the form and compute the estimator  X  T 1 := 1  X  &gt; 0 is again the measurement energy parameter. Next, to estimate k X k 2 F , we note that if Z  X  R p  X  p if we collect n 2 additional measurements of the form y i =  X   X Z i ,X  X  + i , i = n 1 + 1 ,...,n 1 + n 2 , (15) where the Z i  X  R p  X  p are independent random matrices with i.i.d. N (0 , 1) entries, then a suitable estimator of k X k 2 F is  X  T 2 2 := 1 statistics, we propose as our estimate of r ( X ). In principle, this procedure can be refined by using the measurements (14) to esti-mate the noise distribution, but we omit these details. Also, we retain the assumptions of the previous sec-tion, and assume only that the i are independent and bounded by | i |  X   X  0 . The next theorem shows that the estimator with  X  being replaced by % :=  X  0 (  X  k X k F ), and with  X  n being replaced by  X  n =  X  n ( %, X  ) := z 1  X   X  / Theorem 2. Let  X   X  (0 , 1 / 2) and X  X  S p  X  p + \{ 0 } . Assume that model (13) holds. Suppose also that n 1 = n 2 = n/ 2 and  X  n (  X , X  ) &lt; 1 for all n . Then as ( n,p )  X   X  , we have Remarks. In parallel with Theorem 1, this confi-dence interval has the valuable property that its width does not depend on the rank or dimension of X , but merely on the noise-to-signal ratio % =  X  0 (  X  k X k F ). The relative error | ( n  X  1 / 2 + % ) with high probability when  X  n is small. The problem of constructing deterministic matrices A with good recovery properties (e.g. RIP-k or NSP-k ) has been a longstanding topic within CS. Since our procedure in Section 3.2 selects A at random, it is natural to ask if randomization is essential to the esti-mation of unknown sparsity. In this section, we show that estimating s ( x ) with a deterministic matrix A leads to results that are inherently different from our randomized procedure.
 At an informal level, the difference between random and deterministic matrices makes sense if we think of the estimation problem as a game between nature and a statistician. Namely, the statistician first chooses a matrix A  X  R n  X  p and an estimation rule  X  : R n  X  R . (The function  X  takes y  X  R n as input and returns nal x  X  R p \ { 0 } , with the goal of maximizing the statistician X  X  error. When the statistician chooses A deterministically, nature has the freedom to adversar-ially select an x that is ill-suited to the fixed matrix A . By contrast, if the statistician draws A at random, then nature does not know what value A will take, and therefore has less knowledge to choose a  X  X ad X  signal. In the case of noiseless random measurements, Theorem 1 implies that our particular estimation rule | b any non-zero x . (cf. Remarks for Theorem 1.) Our aim is now to show that for noiseless deterministic measurements, all estimation rules  X  have a worst-case than n  X  1 / 2 . In other words, there is always a choice of x that can defeat a deterministic procedure, whereas b s ( x ) is likely to succeed under any choice of x . In stating the following result, we note that it involves no randomness whatsoever  X  since we assume that the observed measurements y = Ax are noiseless and obtained from a deterministic matrix A .
 Theorem 3. The minimax relative error for estimat-ing s ( x ) from noiseless deterministic measurements y = Ax satisfies inf Remarks. Under the typical high-dimensional sce-nario where there is some  X   X  (0 ,  X  ) for which p/n  X   X  as ( n,p )  X   X  , we have the lower bound | Relative error of quences of Theorem 1, we study how the relative error | b s ( x ) /s ( x )  X  1 | depends on the parameters p ,  X  , and s ( x ). We generated measurements y = Ax + under a broad range of parameter settings, with x  X  R 10 4 in most cases. Note that although p = 10 4 is a very large dimension, it is not at all extreme from the viewpoint of applications (e.g. a megapixel image with p = 10 6 ). Details regarding parameter settings are given below. As anticipated by Theorem 1, the left and right panels in Figure 2 show that the relative error has no notice-able dependence on p or s ( x ). The middle panel shows that for fixed n 1 + n 2 , the relative error grows moder-ately with  X  =  X  0 on | the case of low noise (  X  = 10  X  2 ).
 Reconstruction of x based on problem of choosing n , we considered the choice of b n := 2 d b n adapts to the structure of the true signal, and is also sufficiently large for accurate reconstruction. First, to compute drew initial measurement sets of Cauchy and Gaus-sian vectors with n 1 = n 2 = 500 and  X  = 1. If it happened to be the case that 500  X  construction was performed using only the initial 500 Gaussian measurements. Alternatively, if then ( were drawn from N (0 , Further details are given below. Figure 3 illustrates the results for three power-law signals in R 10 4 with x coordinates of x are plotted in black, and those of are plotted in red. Clearly, there is good qualitative agreement in all cases. From left to right, the value of b n = 2 d Settings for relative error of each parameter setting labeled in the figures, we let n 1 = n 2 and averaged | b s ( x ) /s ( x )  X  1 | over 200 prob-lem instances of y = Ax + . In all cases, the matrix A was chosen according to (7) and (9) with  X  = 1 and  X  Uniform[  X   X  0 , X  0 ]. We always chose the normal-ization k x k 2 = 1, and  X  = 1 so that  X  =  X  0 . (In the left and right panels,  X  = 10  X  2 .) For the left and mid-dle panels, all signals have the decay profile x [ i ]  X  i For the right panel, the values s ( x ) = 2 , 58 , 4028, and 9878 were obtained using decay profiles x i  X  i  X   X  with chose p = 10 4 for all curves. A theoretical bound on | b s ( x ) /s ( x )  X  1 | in black was computed from Theorem 1 with  X  = 1 2  X  1 ity at least 1/2, and hence may be reasonably plotted against the average of | Settings for reconstruction (Figure 3). We com-puted reconstructions using the SPGL1 solver (van den Berg &amp; Friedlander, 2007; 2008) for the BP prob-lem the choice 0 =  X  0  X  Uniform[  X   X  0 , X  0 ] and  X  0 = 0 . 001. When re-using the first 500 Gaussian measurements, we re-scaled the vectors a i and the values y i by 1 / erated 25 problem instances and plotted the vector corresponding to the median of k runs (so that the plots reflect typical performance). MEL thanks the reviewers for constructive comments, as well as valuable references that substantially im-proved the paper. Peter Bickel is thanked for helpful discussions, and the DOE CSGF fellowship is grate-fully acknowledged for support under grant DE-FG02-97ER25308.
