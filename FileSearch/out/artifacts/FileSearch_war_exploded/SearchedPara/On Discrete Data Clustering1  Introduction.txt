 Discrete data appear in many machine learning and data mining applications. In this work, we are motivated by the need to construct powerful statistical ap-proaches to model, analyze and cluster th is type of data. Different statistical models have been proposed and were gener ally dedicated to text classification and language processing. It is well-known that the multinomial distribution per-forms well in the case of discrete data modeling. However, recent researches have shown that even this distribution has some drawbacks such as considering that the events to model are independent [1,2,3]. Different smoothing techniques have been proposed to overcome these probl ems. The most successful approach is the use of the Dirichlet distribution as a prior to the multinomial which results in a completely formal statistical model [1,3]. This is due to the fact that the Dirich-let is a conjugate prior to the multinomial. Despite this conjugacy property, Dirichlet has a very restrictive negative covariance structure which makes its use as a prior in the case of positively correlated data inappropriate [4,5].
In this paper, we present a discrete finite mixture model based on both a gen-eralization of the Dirichlet distribution and the multinomial. The choice of the generalized Dirichlet is motivated by the excellent results obtained when we have used it as a parent distribution in differen t pattern recognition and computer vi-sion tasks [5]. The estimation of the parameters of our mixture model is based on the maximum likelihood estimation by invoking the expectation maximization (EM) approach. The proposed mixture model is applied to an important prob-lem in computer vision which is the introduction of spatial constraints in color histograms. Indeed, we propo se a generative model for t his task. Our generative model is used for image databases categorization.
 Let X =( X 1 ,...,X D ) be a discrete vector which means that each element X l , l =1 ,...,D in X is discrete and takes on values 1 , 2 ,...,V . Then, the joint probability of X is given by where  X  ( X d = v ) is an indicator function, P =( P 1 ,...,P V ) is the parameter the samples will be used to set the probabilities, obtaining which gives poor estimate [1]. Then, the majority of the researchers assign a sin-gle Dirichlet or a Dirichlet mixture prior to the parameter vector of multinomial distribution to moderate the extreme estimates given by Eq. 2 [1]. The Dirichlet distribution with V parameters  X  =(  X  1 ,..., X  V ) is defined by The Dirichlet distribution depends on V parameters  X  1 ,..., X  V ,whichareall real and positive. In spite of its flexibility and the fact that it is conjugate to the multinomial, the Dirichlet has a very restrictive covariance matrix [4]. Another restriction of the Dirichlet distribution is that the variables with the same mean must have the same variance as shown in [6]. All these disadvantages can be handled by using the generalized Dirichlet distribution. The generalized Dirichlet pdf is defined by  X  let as a special case. Comparing to the Dirichlet, the generalized Dirichlet has V  X  2 extra parameters which is a very important advantage. Indeed, as the Dirichlet has V parameters, when constructing a Dirichlet prior and if the mean probabilities of the variables have been fixed, it remains only one degree of free-dom (by fixing the value of V v =1  X  v ) to adjust the distribution [7]. For the generalized Dirichlet, however, it remains V  X  1 degrees of freedom which makes it more flexible for several applications [5]. The mean of the generalized Dirichlet distribution is given by [8] In addition to this property, the generalized Dirichlet is conjugate to the multi-nomial distribution and we can easily show that the joint distribution of X and P is where  X  v =  X  v + f v and  X  v =  X  v + f v +1 + ... + f V for v =1 ,...,V  X  1,  X  over P , we obtain the marginal distribution of X We call this density the multinomial generalized Dirichlet distribution (MGD). Then, the posterior is given by by taking the generalized Dirichlet as a prior to the multinomial and according to Eq.5 and Eq.7, we obtain verify that this equation is reduced to where  X  V =  X  V  X  1 , which represents the expectation when we consider a Dirichlet distribution, with parameters (  X  1 ,..., X  V ), as a prior.

Suppose now that we select a generalized Dirichlet mixture as a prior to the multinomial. A generalized Dirichlet mixture with M components is defined as the entire set of parameters to be estimated. With a mixture prior, the marginal distribution of X is p ( X |  X  )= We call this density the multinomial generalized Dirichlet mixture (MGDM). And we can easily show that Given a set of independent vectors X = { X 1 ,..., X N } , the log-likelihood cor-responding to an M -component MGDM is given by The maximization defining the ML estimates is subject to the constraints 0 &lt; p  X  1and M we have used the EM algorithm [9]. In EM, the  X  X omplete X  data are considered to be Y i = { X i , Z i } ,where Z i =( Z i 1 ,...,Z iM )with constituting the  X  X issing X  data. The EM algorithm is based on the Q-function (the conditional expectation) of the complete-data log-likelihood where  X  ( t ) is the value of  X  at iteration t and The first term in Eq. 13 can be maximized by updating p j as following The maximization of the second term, h owever, does not yield to a closed form solution. Thus, we have used Newton-Raphson method which is based on the computation of the first and second derivatives. The iterative scheme of the Newton-Raphson method is given by the following equation: where H (  X  ( t  X  1) j ) is the hessian matrix. Color histograms are widely used as features vectors for images summarization and retrieval [10] and are used in diffe rent systems. This can be explained by the fact that histograms provide a stable object recognition in the presence of occlusions and over views change [10]. However, histograms do not include any spatial information which is an important issue in human visual perception. Different approaches have been proposed to integrate spatial information with color histograms [11,12]. In the following, we propose a statistical model based on the MGDM to introduce the spatial information into color histograms. The proposed model is then applied to images databases summarization.

Suppose that we have N labeled images I i , i =1 ,...,N classified in R classes and that the number of labeled images in each class r is equal to n r ( R r =1 n r = N ). By associating a distribution and a weight to each class in the training set, we can suppose that each image I i is generated by a mixture of R distributions L  X  K image I the spatial information, the probability of a pixel should be conditioned on its neighborhood. By taking the neighborhood consisting of the pixels at a distance d  X  D = { d the classic image histogram, if we suppose that each pixel X i lk is independent of its neighborhood, which is actually the standard naive Bayes assumption. According to Eq. 17 the par ameters of an individual mixture component are a multinomial distribution over the C  X  C possible color pairs and can be written as  X  c c Eq. 17 could be written as follows p ( I i |  X  r )= D d =1 C c Card {} refers to the number of elements of a set. Learning our model con-sists of estimating the parameters  X  c class r . By noting that we can associate a C 2 -dimensional vector of frequencies f distance d , the parameters are estimated using Eq. 11.

For our experiments, we used a database containing 45100 images. This data-base contains 10 homogeneous classes (see Figure 1). We divided the database on two sets. A data set containing 22550 images used for training. The remaining images were used for testing. We considered the RGB space with color quanti-Besides, we have considered only probabilities of pixels having same colors in order to reduce zero frequencies, which is a common approach and used, for instance, in the case of the autocorrelogram proposed by Huang et al. [12]. The accuracy classification p roduced by our classifier was measured by counting the number of misclassified images, yielding a confusion matrix. In this confusion matrix, the cell ( i, j ) represents the number of images from category i which are classified as category j . The number of images misclassified when we used MGDM, was 2189, which represents an accuracy of 90.29 percent (See Table 1). Table 2 represents the confusion matrix when we use Multinomial Dirichlet mix-tures (MDM) (3711 misclassified images which represents an accuracy of 83.54 percent). Table 3 shows the confusion matrix when we use multinomial mix-tures. In this case, the accuracy was 80.35 percent (4430 misclassified images). Note that the improvement achieved by the MGDM, comparing to MDM and multinomial mixtures, is highly statistically significant. We have proposed, discussed and evaluated a novel finite mixture to model discrete data. This mixture model is based on both the generalized Dirichlet and the multinomial distri butions. The recently propose d multinomial Dirichlet mixture has turned out to be a special case. The proposed model is powerful and flexible enough to be adapted to a broad variety of applications where discrete data play an important role such as information retrieval and filtering, natural language processing and bioinformatics.
 The completion of this research was made possible thanks to the Natural Sciences and Engineering Research Council of Canada (NSERC), a NATEQ Nouveaux Chercheurs Grant, and a start-up grant from Concordia University.

