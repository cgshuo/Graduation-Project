 1. Introduction
How to explore and utilize the huge amount of text documents is a major question in the areas of information retrieval and text mining. Document clustering (also referred to as text clustering) is one of the most important text mining methods that are developed to help users effectively navigate, summarize, and organize text documents. By organizing a large amount of documents into a number of meaningful clusters, document clustering can be used to browse a collection of documents or ment [3]. The problem of document clustering is generally defined as follows: given a set of documents, we would like to partition them into a predetermined or an automatically derived number of clusters, such that the documents assigned to in one cluster share the same topic, and the documents in different clusters represent different topics.
There are two general categories of clustering methods: agglomerative hierarchical and partitional methods. In previous research, both methods were applied to document clustering. Agglomerative hierarchical clustering (AHC) algorithms ini-tained. Comparing with the bottom-up method of AHC algorithms, the family of k -means algorithms [6,23,24,29] , which be-is repeated until an optimal set of k clusters are obtained based on a criterion function.

For document clustering, Unweighted Pair Group Method with Arithmetic Mean (UPGMA) [11] is reported to be the most tering of large text databases due to their relatively low computational requirement and high quality. widely used in document clustering algorithms and is reported performing very well [34]. The cosine function can be used in maximize the intra-cluster similarity.

Since the cosine function measures the similarity of two documents, only the pairwise similarity is considered when we determine whether a document is assigned to a cluster or not. However, when the clusters are not well separated, partition-ing them just based on the pairwise similarity is not good enough because some documents in different clusters may be sim-ilar to each other. To avoid this problem, we applied the concepts of neighbors and link , introduced in [14], to document clustering.
 of their common neighbors [14]. For example, link  X  p i ; p ter clusters than traditional algorithms.

Each text document can be viewed as a tuple with boolean attribute values, where each attribute corresponds to a unique neighbors and link could provide valuable information about the documents in the clustering process. We believe that the intra-cluster similarity better be measured not only based on the distance between the documents and the centroid, but also based on their neighbors. The link function can be used to enhance the evaluation of the closeness between documents be-cause it takes the information of surrounding documents into consideration.

In this paper, we propose to use the neighbors and link along with the cosine function in different aspects of the k -means the optimization process which adjusts the partitions by repeatedly calculating the new cluster centroids based on the doc-uments assigned to them and reassigning documents.
 three values: the pairwise similarity value calculated by the cosine function, the link function value, and the number of neighbors of documents in the data set. This combination helps us find a group of initial centroids with high quality.
Second, we propose a new similarity measure to determine the closest cluster centroid for each document during the cluster refinement phase. This similarity measure is composed of the cosine and link functions. We believe that, besides the pairwise similarity, involving the documents in the neighborhood can improve the accuracy of the closeness measure-ment between a document and a cluster centroid.
 of each cluster.
 We evaluated the performance of our proposed clustering algorithms on various real-life data sets extracted from the
Reuters-21578 Distribution 1.0 [30], the Classic text database [5], and a corpus of the Text Retrieval Conference (TREC) [16]. Our clustering algorithms demonstrated very significant improvement in the clustering accuracy.
The rest of this paper is organized as follows. In Section 2, we review the vector space model of documents, the cosine tion 4, experimental results of our clustering algorithms are compared with those of original algorithms in terms of the clustering accuracy. Section 5 reviews related work, and Section 6 contains some conclusions and future work. 2. Background 2.1. Vector space model of text documents
For most existing document clustering algorithms, documents are represented by using the vector space model [31].In this model, each document d is considered as a vector in the term-space and represented by the term frequency (TF) vector: there are several preprocessing steps, including the removal of stop words and the stemming on the documents. A widely used refinement to this model is to weight each term based on its inverse document frequency (IDF) in the document collec-tion. The idea is that the terms appearing frequently in many documents have limited discrimination power, so they need to be deemphasized [31]. This is commonly done by multiplying the frequency of each term i by log  X  n = df number of documents in the collection, and df i is the number of documents that contain term i (i.e., document frequency).
Thus, the tf X  X df representation of the document d is:
To account for the documents of different lengths, each document vector is normalized to a unit vector (i.e., k d In the rest of this paper, we assume that this vector space model is used to represent documents during the clustering.
Given a set C j of documents and their corresponding vector representations, the centroid vector c where each d i is the document vector in the set C j , and j C though each document vector d i is of unit length, the centroid vector c 2.2. Cosine similarity measure
For document clustering, there are different similarity measures available. The most commonly used is the cosine func-tion [31]. For two documents d i and d j , the similarity between them can be calculated as:
Since the document vectors are of unit length, the above equation is simplified to:
The cosine value is 1 when two documents are identical, and 0 if there is nothing in common between them (i.e., their document vectors are orthogonal to each other). 2.3. Neighbors and link
The neighbors of a document d in a data set are those documents that are considered similar to it [14]. Let sim  X  d similarity function capturing the pairwise similarity between two documents, d with a larger value indicating higher similarity. For a given threshold h , d
Here h is a user-defined threshold to control how similar a pair of documents should be in order to be considered as neigh-the user can choose an appropriate value for h .

The information about the neighbors of every document in the data set can be represented by a neighbor matrix . A neigh-bor matrix for a data set of n documents is an n n adjacency matrix M , in which an entry M  X  i ; j is 1 or 0 depending on whether documents d i and d j are neighbors or not [14]. The number of neighbors of a document d by N  X  d i  X  , and it is the number of entries whose values are 1 in the i th row of the matrix M .
The value of the link function link  X  d i ; d j  X  is defined as the number of common neighbors between d be obtained by multiplying the i th row of the neighbor matrix M with its j th column:
Thus, if link  X  d i ; d j  X  is large, then it is more probable that d
Since the cosine measures only the similarity between two documents, using it alone can be considered as a local ap-the knowledge of neighbor documents in evaluating the relationship between two documents. Thus, the link function also is a good candidate for measuring the closeness of two documents. 2.4. k-Means and bisecting k-means algorithms for document clustering d function is either minimized or maximized, depending on the definition of sim  X  d j  X  1 ; ... ; k , and sim  X  d i ; c j  X  evaluates the similarity between a document d is used to resent the documents and the cosine is used for sim  X  d vector is more similar to the document than those of other clusters, and the global criterion function is maximized in that case. This optimization process is known as an NP-complete problem [12], and the k -means algorithm was proposed to pro-vide an approximate solution [17]. The steps of k -means are as follows: 1. Select k initial cluster centroids, each of which represents a cluster. 2. For each document in the whole data set, compute the similarity with each cluster centroid, and assign the document to the closest (i.e., most similar) centroid. (assignment step) 3. Recalculate k centroids based on the documents assigned to them. 4. Repeat steps 2 and 3 until convergence.
 follows: 1. Select a cluster C j to split based on a heuristic function. 2. Find 2 subclusters of C j using the k -means algorithm: (bisecting step) (a) Select 2 initial cluster centroids. (b) For each document of C j , compute the similarity with the 2 cluster centroids, and assign the document to the closer (c) Recalculate 2 centroids based on the documents assigned to them. (d) Repeat steps 2b and 2c until convergence. 4. Repeat steps 1, 2 and 3 until k clusters are obtained.

I denotes the number of iterations for each bisecting step, and usually it is specified in advance. 3. Applications of the neighbors and link in the k -means and bisecting k -means algorithms 3.1. Selection of initial cluster centroids based on the ranks
The family of k -means algorithms start with initial cluster centroids, and documents are assigned to the clusters itera-based on this kind of iterative process are computationally efficient but often converge to local minima or maxima of the is one way to overcome this problem. random algorithm randomly chooses k documents from the data set as the initial centroids [11]. The buckshot algorithm picks ets of the same size, and the documents within each bucket are clustered. Then these clusters are treated as if they are individual documents, and the whole procedure is repeated until k clusters are obtained. The centroids of the resulting k clusters become the initial centroids.

In this paper, we propose a new method of selecting initial centroids based on the concepts of neighbors and link in addi-tion to the cosine. The documents in one cluster are supposed to be more similar to each other than the documents in dif-document in the data set could be used to evaluate how many documents are close enough to the document. Since both the cosine and link functions can measure the similarity of two documents, here we use them together to evaluate the dissim-ilarity of two documents which are initial centroid candidates.

First, by checking the neighbor matrix of the data set, we list the documents in descending order of their numbers of the top m documents are selected from this list. This set of m initial centroid candidates is denoted by S the most neighbors in the data set, we assume they are more likely the centers of clusters.

For example, let X  X  consider a data set S containing 6 documents, f d in Fig. 1 . When h  X  0 : 3 ; k  X  3 and n plus  X  1 ; S m has four documents: S
Next, we obtain the cosine and link values between every pair of documents in S ascending order of their cosine and link values, respectively. For a pair of documents d rank based on the cosine value, rank link  X  d i ; d j  X  be its rank based on the link value, and rank
Initial centroids better be well separated from each other in order to represent the whole data set. Thus, the document pairs with high ranks could be considered as good initial centroid candidates. For the selection of k initial centroids out of m candidates, there are m C k possible combinations. Each combination is a k -subset of S of each combination com k as:
That means, the rank value of a combination is the sum of the rank values of the a group of documents, so they can serve as the initial centroids of the k -means algorithm.

The effectiveness of this proposed method depends on the selection of n
Section 4.3.1, we will discuss how to select an appropriate n clusters, and some of them may be within a large cluster. Our experimental results showed that our proposed similarity mea-sure described in the following Section 3.2 could be adopted to improve the clustering results of those data sets. 3.2. Similarity measure based on the cosine and link functions
For document clustering, the cosine function is a very popular similarity measure. It measures the similarity between two documents as the correlation between the document vectors representing them. This correlation is quantified as the cosine value of the angle between the two vectors, and a larger cosine value indicates that the two documents share more terms and are more similar. When the cosine is adopted in the family of k -means algorithms, the correlation between each pair of a document and a centroid is evaluated during the assignment step.

However, the similarity measure based on the cosine may not work well for some document collection. Usually, the num-ber of unique terms in a document collection is very large while the average number of unique terms in a document is much smaller. In addition, documents that cover the same topic and belong to a single cluster may contain a small subset of the rest covers other branches of the family tree. Thus, those documents do not contain all the relevant terms listed above.
Another example is regarding the usage of synonyms. Different terms are used in different documents even if they cover the same topic. The documents in a cluster about the automobile industry may not use the same word to describe the car. pair of documents in a cluster have few terms in common, but have connections with other documents in the same cluster as those documents have many common terms with each of the two documents. In this case, the concept of link may help us identify the closeness of two documents by checking their neighbors. When a document d neighbors, and a document d j shares another group of terms with many neighbors of d similar by the cosine function, their common neighbors show how close they are.
 Another fact is the number of unique terms may be quite different for different topics as their vocabularies are different.
In a cluster involving a large vocabulary, since document vectors are spread over a larger number of terms, most document
The cluster refinement phase of the k -means algorithm is the process of maximizing the global criterion function when the not desirable because the documents in those clusters may be strongly related to each other.

On the other hand, if the global criterion function is based on the concept of link, which captures the information about regardless of its vocabulary size.

However, there is a case the link function may not perform well as the similarity measure by itself. In the cluster refine-ment phase, if a document is assigned to the cluster whose centroid shares the largest number of neighbors with this doc-cluster. For a fixed similarity threshold h , the centroid of a large cluster, say c small cluster, say c j . Thus, for a document d i , it is quite probable that link  X  d scenario, the global criterion function is maximized when most of the documents are assigned to one cluster while all the other clusters are almost empty.

Based on these discussions, we propose a new similarity measure for the family of k -means algorithms by combining the cosine and link functions as follows: where L max is the largest possible value of link  X  d i ; c all the documents in the data set are involved in the whole clustering process, the largest possible value of link  X  d number of documents in the data set  X  n  X  , which means all the documents in the data set are neighbors of both d the bisecting k -means algorithm, only the documents in the selected cluster are involved in each bisecting step. Thus, the ing k -means, the smallest possible value of link  X  d i ; c
We use L max to normalize the link values so that the value of link  X  d 0 6 of the cosine and link functions to evaluate the closeness of two documents, and a larger value of f  X  d ments in different aspects, our new similarity measure is more comprehensive. During the clustering process, iteratively each document is assigned to the cluster whose centroid is most similar to the document, so that the global criterion func-tion is maximized.
 trix, denoted by M 0 , in which an entry M 0  X  i ; n  X  j is 1 or 0 depending on whether a document d or not. The expanded neighbor matrix for the example data set S is shown in Fig. 2 . The value of link  X  d multiplying the i th row of M 0 with its  X  n  X  j  X  th column as: 3.3. Selection of a cluster to split based on the neighbors of the centroids means its documents are not closely related to each other, and the bonds between them are weak. Therefore, our selection of the size of the cluster, or the combination of both. But they found the difference between those different measurements is necessarily a good measurement of its compactness.
 size is smaller than that of the second one. The concept of neighbors, which is based on the similarity of two documents, tion which compares the neighbors of the centroids of remaining clusters as described below. Our experimental results show that the performance of bisecting k -means is improved, compared to the case of splitting the largest cluster.
Since we want to measure the compactness of a cluster, only the local neighbors of the centroid are counted. In other words, we just count those documents that are similar to the centroid and existing in that cluster. For a cluster C ber of local neighbors of the centroid c j s denoted by N  X  c whose value is 1 for d i 2 C j .

For the same cluster size and the same similarity threshold h , the centroid of a compact cluster should have more neigh-centroid by the size of the cluster to get a normalized value, denoted by V  X  c
When we choose a cluster to split, we choose the one with the smallest V value. 4. Experimental results
In order to show that our proposed methods can improve the performance of k -means and bisecting k -means in docu-results were compared with those of original k -means and bisecting k -means. The time and space complexities of the mod-ified algorithms are discussed, and we also compared the cosine and the Jaccard index as a similarity measure. We imple-mented all the algorithms in C++ on a SuSE Linux workstation with a 500 MHz processor and 384 MB memory. 4.1. Data sets
We used 13 test data sets extracted from three different types of text databases, which have been widely used by the and MED1, were extracted from the CISI, CACM and MEDLINE abstracts, respectively, which are included in the Classic text database [5].
 The second group of four test data sets, denoted by EXC1, ORG1, PEO1 and TOP1, were extracted from the EXCHANGES, ORGS, PEOPLE and TOPICS category sets of the Reuters-21578 Distribution 1.0 [30].

The third group of test data sets were prepared by ourselves. We tried to simulate the case of using a search engine to retrieve the desired documents from a database, and we adopted the Lemur Toolkit [25] as the search engine. The English newswire corpus of the HARD track of the Text Retrieval Conference (TREC) [16] was used as the database. This corpus in-cludes about 652,309 documents (in 1575 MB) from eight different sources, and there are 29 test queries. Among the 29 que-ries, HARD-306, HARD-309 and HARD-314 queries were sent to the search engine, and the top 200 results of these queries chose only top 200 documents is that usually users do not read more than 200 documents for a single query.
Each document in the test data set has been already pre-classified into one unique class. But, this information was hidden experiments, the removal of stop words and the stemming were performed as preprocessing steps on the data sets. Table 3 summarizes the characteristics of all the test data sets used for our experiments. The last column shows the average simi-larity of all the pairs of documents in each data set, and the cosine function is used for the measurement. 4.2. Evaluation methods of document clustering
We used the F-measure and purity values to evaluate the accuracy of our clustering algorithms. The F -measure is a har-described above, each cluster obtained can be considered as the result of a query, whereas each pre-classified set of docu-R  X  i ; j  X  of each cluster j for each class i .

If n i is the number of the members of class i , n j is the number of the members of cluster j , and n members of class i in cluster j , then P  X  i ; j  X  and R  X  i ; j  X  can be defined as: The corresponding F -measure F  X  i ; j  X  is defined as: Then, the F -measure of the whole clustering result is defined as: result is [34].

The purity of a cluster represents the fraction of the cluster corresponding to the largest class of documents assigned to that cluster, thus the purity of cluster j is defined as: The purity of the whole clustering result is a weighted sum of the cluster purities:
In general, the larger the purity value is, the better the clustering result is [36]. 4.3. Clustering results
Figs. 3 X 6 show the F -measure values of the clustering results of all the algorithms on 13 data sets, and Tables 4 and 5 show the purity values of the clustering results. In the original k -means (KM) and bisecting k -means (BKM) algorithms, neighbors of the centroids. We ran each algorithm 10 times to obtain the average F -measure and purity values. The exper-imental results demonstrate that our proposed methods of using the neighbors and link on KM and BKM can improve the clustering accuracy significantly. 4.3.1. Results of the selection of initial centroids based on the ranks
From the experimental results, we can see that the selection of initial centroids by using the ranks of documents performs much better than the random selection in terms of the clustering accuracy.
Since our rank-based method selects k centroids from k  X  n bers of neighbors, which indicates that they are close to a large number of documents, they may not be distributed evenly
The test results show that for KM, within the range of  X  0 ; k , the larger n decided to select k initial centroids from 2 k candidates. For BKM, the optimal range of n because only two initial centroids are needed at each bisecting step of BKM. Our rank-based method involves several steps, and the time complexity of each step is analyzed in detail as follows: Step 1: Creation of the neighbor matrix.
 Step 2: Obtaining the top m documents with most neighbors.
 Step 3: Ranking the document pairs in S m based on the cosine and link values.
 Step 4: Finding the best k -subset out of S m .
 And the total time required for the selection of k initial centroids is: remove the exponential component in the time complexity.

When k is large, instead of checking all the possible k -subsets of the documents in S k -subset, S 0 , incrementally. After step 3, first the document pair with the highest rank are inserted into S  X  k 2  X  selections; and at each selection, the best document out of k randomly selected documents from S goodness of each candidate document d i is evaluated by the rank words, for each candidate document d i , we compute the rank d rithm are 0.5535 and 0.5936, respectively. They are slightly lower than the case of original proposed selection method, but much better than the case of randomly selected initial centroids, where the average F -measure and purity values are 0.451 and 0.474, respectively.

For the bisecting k -means algorithm, since only two clusters are created at each bisecting step, the time complexity of selecting initial centroids is always O  X  n 2  X  no matter how large k is. 4.3.2. Results of the similarity measure based on the cosine and link functions The first step of our similarity measure based on the cosine and link functions is to find the neighbors of each document. set to 0.1 to obtain other experimental results reported in this paper.

In our new similarity measure, we use the linear combination of the cosine and link functions to measure the closeness weight of the cosine in the linear combination should be much smaller than that of the link.
The time complexity of our new similarity measure is determined by the computation of the cosine and link functions. For as:
The computation of the link function contains three parts: creating the neighbor matrix, expanding the neighbor matrix of the loop.

The time complexity of creating the neighbor matrix is derived in Section 4.3.1 as: T neighbor matrix is created just once before the first iteration of the loop; and at each iteration, it is expanded into an those k columns:
The calculation of the link value for every document with each of k centroids can be done by multiplying the vectors in the expanded neighbor matrix as described in Section 3.2, and its time complexity could be represented as: Thus, the time complexity for the new similarity measure is: our new similarity measure is O  X  n 2  X  for a data set containing n documents, and it is quite acceptable.
By adopting the new similarity measure to KM and BKM, both algorithms outperform the original ones on all 13 test data sets. We can thus conclude that the new similarity measure provides a more accurate measurement of the closeness be-tween a document and a centroid. 4.3.3. Results of the selection of a cluster to split based on the neighbors of the centroids
From the experimental results of the BKM with NB, shown in Figs. 5 and 6 and Table 5 , we can find that the new method F -measure, while the purity values of their clustering results are almost the same.

At each bisecting step, the original BKM splits the largest cluster. Our new cluster selection method is based on the com-pactness of clusters measured by the number of local neighbors of the centroids. Since SET1, SET2 and SET3 data sets are simulated search results, there are more terms shared between the documents in these data sets. In the other words, the search-result data sets are more compact than those in traditional data sets. This characteristic leads to the experimental results showing that there is no big difference between the two methods of selecting a cluster to split.
The time complexity of the BKM with NB is not much different from that of BKM, because the cost of selecting a cluster to split based on the number of local neighbors of the centroids is very small.

The experimental results proved that our measurement of the compactness of clusters by using the neighbors of the cen-troids is more accurate than just using the cluster size. For the data sets whose clusters are not compact, our BKM with NB performs much better than BKM. 4.3.4. Results of the combinations of proposed methods
We combined the three proposed methods, utilizing the neighbors and link, and ran the modified algorithms on all the
Since all of our proposed methods are utilizing the same neighbor matrix, adopting them into one algorithm is computation-ally meritorious. The average execution times per loop of different k -means algorithms on EXC1 and SET2 data sets are shown in Fig. 11 . We can see that the k -means using both ranks and CL does not require much extra time.
For most data sets, we found that the best clustering result obtained is close to the result of using the ranks alone. It adopting our new similarity measure based on the cosine and link functions. An example case is CISI1 data set containing 163 documents. Its maximum class size is 102 documents, and the minimum class size is 4 documents. Fig. 3 shows that, sure (CL), the clustering result is improved as expected ( F -measure value is 0.556), and the combination of these two methods achieves a much better clustering result ( F -measure value is 0.5953). 4.3.5. Comparison between the cosine and the Jaccard index
In our proposed methods, we used the cosine as the similarity measure between documents. The Jaccard index, also known as Jaccard similarity coefficient, is another similarity measure and, for document clustering, it can be defined as the ratio between the number of common terms in two documents and the number of terms in the union of two documents. So, for two documents d i and d j , their Jaccard index is:
For the comparison between the cosine and the Jaccard index, we used the Jaccard index in the place of the cosine as fol-lows. First, when we build the neighbor matrix, we used the Jaccard index to determine whether two documents are neigh-bors of each other. Second, we also measured the similarity between a document and a cluster centroid by replacing the cosine with the Jaccard index in Eq. (10) as:
Then, we performed the k -means with the Jaccard index and the k -means with f 0  X  d sets. Their results are compared with the cases of using the cosine in terms of the F -measure, as shown in Table 6 .
In Table 6 , first we can see that the cosine performs better than the Jaccard index for document clustering when they are formance of k -means in most cases. Third, the cosine works better than the Jaccard index when each of them is combined with the link function.
 last column of Table 6 , and we can see that the cosine still performs better than the Jaccard index. 5. Related Work
The general concepts of neighbors and link have been used in other clustering algorithms [9,10,13,22] with different def-initions for neighbors and link. In the clustering algorithm proposed in [22], for each data point, k nearest neighbors are found. Then, two data points are placed in the same cluster if they are nearest neighbors of each other and also have more than a certain number of shared nearest neighbors.
 data point, a pair of data points with a high mutual neighborhood value would be clustered together. between two data points is defined as the number of shared nearest neighbors, and if the strength is higher than a certain the number of strong links of each data point.

In [10], a density-based clustering algorithm was proposed, where the definition of a cluster is based on the notion of rounded by more than a certain number of data points. In that case, we can consider that p and q are in the same cluster.
However, unlike our proposed methods, these previous clustering algorithms do not use the concepts of neighbors and to measure the similarity between a data point a centroid; and do not use the neighbors of each centroid to measure the compactness of the corresponding cluster.

Recently, attention has been given to exploring the semantic information, like synonyms, polysemy and semantic hierarchy, for document clustering. Two techniques have been reported in many literature. One is ontology-based document clustering [20,21,27] . Ontology represents the semantic relationship between the terms, and it can be used to refine the vector space model by weighting, replacing or expanding the terms. Another technique is Latent
Semantic Analysis (LSA), which is also called Latent Semantic Index (LSI) [4,7,32,33,35] . LSA takes the term-document matrix as an input and uses the Singular Value Decomposition (SVD) to project the original high-dimensional term-document vector space to a low-dimensional concept vector space, in which the dimensions are orthogonal; i.e., statistically uncorrelated. One problem of LSA is the high computation cost of the SVD process for a large term-doc-ument matrix. For both techniques, after a new vector space is obtained, conventional document clustering algo-rithms can be used. It has been reported that both techniques can improve the clustering accuracy significantly [20,21,27,32,33,35] . 6. Conclusions
In this paper, we proposed three different methods of using the neighbors and link in the k -means and bisecting k -means algorithms for document clustering. Comparing with the local information given by the cosine function, the link function provides the global view in evaluating the closeness between two documents by using the neighbor documents.

We enhanced the k -means and bisecting k -means algorithms by using the ranks of documents for the selection of initial centroids, by using the linear combination of the cosine and link functions as a new similarity measure between a document with the original k -means and bisecting k -means on real-life data sets.

Our experimental results showed that the clustering accuracy of k -means and bisecting k -means is improved by adopting racy. Second, the test results showed that our new method of measuring the closeness between a document and a centroid based on the combination of the pairwise similarity and their common neighbors performs better than using the pairwise similarity alone. Third, the compactness of a cluster could be measured accurately by the neighbors of the centroid. Thus, for bisecting k -means, a cluster whose centroid has the smallest number of local neighbors can be split. Moreover, since without increasing the execution time much.
 documents, computing the link between documents by using the neighbor matrix, selecting the most similar cluster centroid for each document based on the new similarity measure, and computing the compactness of each cluster based on the num-ber of local neighbors of its centroid.

Ontology and Latent Semantic Analysis (LSA) are known to be useful for document clustering, so we plan to investigate how they can be integrated with the concepts of neighbors and link to improve the clustering accuracy.
References
