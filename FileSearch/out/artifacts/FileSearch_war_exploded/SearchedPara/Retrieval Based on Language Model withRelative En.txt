 The language modeling approach to information retrieval has recently been pro-posed as a new alternative to traditional vector space models and other proba-bilistic models owing to its sound theoretical basis and good empirical success. Although the language modeling approach has performed well empirically, a sig-nificant amount of performance increase is often due to feedback [5]. Several recent papers[3][4] have presented techniques for improving language modeling techniques using relevance or pseudo relevance feedback. In the most of existing work, feedback has so far only been deal with heuristically within the language modeling approach, and it has been incorporated in an unnatural way: by ex-panding a query with a set of terms. Such an expansion-based feedback strategy is generally not very compatible with the essence of the language modeling ap-proach, which is model estimation. As a result, the expanded query usually has to be interpreted differently than the original query. This is in contrast to the natural way of performing feedback in the classical relevance-based probabilistic model, such as the binary independence model [9].
 entropy and feedback. Different from the traditional language method in which the query is assumed being generated from the document language model, in our method we assume that the query and the document are respectively generated from the query language model and the document language model. We rank the documents according to the relative entropies of the estimated document language models with respect to the estimated query language model. We think that our assumption that the query is generated from the query model is more reasonable than the assumption that the query is generated from the document language model, because the query and the document are not completely same. In order to better capture the important processes behind relevance feedback and query expansion, we believe it is important to view the query as a sample from a model of the information need. So, when estimating a query model, we develop a natural approach to perform feedback, in which we assume the feedback documents are generated by a combined model in which one component is the feedback document language model and the other is the collection language model. The general idea of using language model for information retrieval is to build a language model M d for each document d , and rank the documents according to how likely the query q can be generated from each of these document models, i.e. are two typical methods for doing it. For example, Ponte and Croft [6] treat the query as a set of unique terms, and use the product of two probabilities  X  the probability of producing the query terms and the probability of not producing other terms  X  to approximate p ( q | M d ). The formula of this method is into account possibly multiple occurrences of the same term. Thus the query probability can be obtained by multiplying the individual term probabilities, and the formula can be written as where w i is the i th term in the query. ent from the above approaches in which both the query and the document are assumed to be generated from the document language model. We assume that the query is generated from the query language model while the document is generated from the document language model. The relative entropy is a measure of the distance between two distributions. In statistics, it arises as an expected logarithm of the likelihood ratio. The relative is q when the true distribution is p .
 Definition 1. The relative entropy between two probability mass functions p ( x ) and q ( x ) is defined as D ( p || q )= ments) that 0 log 0 0 =0and p log p 0 =  X  .
 However, it is not a true distance between distributions since it is not symmetric and does not satisfy the triangle inequality. Nonetheless, it is often useful to think of relative entropy as a  X  X istance X  between distributions [1]. 4.1 Ranking Model with Relative Entropy Let X  X  suppose that a query q is generated by a generative model p ( q | M q ) with M q denoting the parameters of the query unigram language model. Similarly, assume that a document d is generated by a generative model p ( d | M d ) with M d denoting the parameters of the document unigram language model. Let and  X  M d be the estimated query language model and document language model respectively, so, the relevance value of d with respect to q can be measured by the following function: specifically, the entropy of the query model M q . It can be ignored for the purpose of ranking documents, so we have a ranking formula such as 4.2 Estimating the Document Model In the language model, the simplest method to estimate p ( w | M d ) , which is a unigram language model based on the given document d , is the maximum likelihood estimator, simply given by relative counts where tf ( w, d ) is the number of times the word w occurs in the document d , is essentially the length of the document d .
 timate p ( w | M d ) is the problem of zero probability [9]. From the formula (8), we can see that if a word is unseen in the document d , we will get a zero probability according the maximum likelihood estimator. But a zero value of p ( w | M d )is not permitted to appear in the formula (6). To the problem, we use smoothing methods in the estimating of p ( w | M d ).
 method, and Absolute discounting etc., have been proposed, mostly in the con-text of speech recognition tasks. In general, all smoothing methods are trying to discount the probabilities of the words seen in the document, and then to assign the extra probability mass to the unseen words according to some  X  X allback X  model [7]. For information retrieval, it makes much sense, and is very common, to exploit the collection language model as the fallback model. Because a re-trieval task typically requires efficient computations over a large collection of documents, our study is constrained by the efficiency of the smoothing method. In this paper, we select the Jelinek-Mercer method and Absolute discounting method which are popular and relatively efficient to implement.
 likelihood model with the collection model, using a coefficient  X  to control the influence of each model [8].The method is given by of seen words by subtracting a constant from their counts. It is similar to the Jelinek-Mercer method, but differs in that it discounts the seen word probability by subtracting a constant instead of multiplying it by 1  X   X  . The model is given by sum to one. Here, | d | u is the number of unique terms in the document d ,and | d | = 4.3 Estimating the Query Model The simplest way to estimate p ( w | M q ) is also the maximum likelihood estimator, which gives us plore a new method to exploit feedback documents when estimating the query language model, which is different from the methods in [3] and [4]. In the new method, we assume the feedback documents are generated by a combined model in which one component is the feedback document language model and the other is the collection language model.
 model, q be the updated query, and p ( w | M q ) be the updated query language model. We assume that F =( f 1 ,f 2 , ..., f n ) is the set of feedback documents which are judged to be relevant by a user, or which are the top documents from an initial retrieval, and p ( w | M F ) is the language model of the set F . We employ a linear interpolation strategy for combining the language model of the feedback documents set with the language model of the original query. Then, the updated query model p ( w | M q )is where  X  controls the influence of the feedback documents set model to p ( w | M q ). We will describe how to estimate p ( w | M F ) as follows.
 ated by a probabilistic model p ( F | M F ). Specifically, assume that each word in F is generated independently according to M F by a generative model which is a unigram language model. That is, where tf ( w, f i ) is the number of times word w occurs in the document f i . to the query. Some information may be  X  X ackground noise X . The  X  X ackground noise X  is not considered in the above model, so, it is not very reasonable. A more reasonable model would be a combined model that generates a feedback document by combining the feedback document model p ( w | M F ) with a collection language model p ( w | C ). For most of the information contained in the collection is irrelevant, it is reasonable to use the collection language model as the model of the  X  X ackground noise X  in a feedback document. Under the simple combined model, p ( F | M F ) is described as where  X  is a parameter that indicates the amount of  X  X ackground noise X  in the feedback documents, and that needs to be set empirically.
 language model p ( w | C ) , we can use EM(Expectation Maximum) algorithm to compute the maximum likelihood estimate of M F . The estimated M F is as dating formulas as a substitute for p ( w | M F ) of the formula (14). Then we can obtain the value of p ( w | M q ) by using the formula (14). Our goal is to study the performance of our method presented in this paper by comparing it with other retrieval methods. For the convenience of describing the experiments, we call our method using the Jelinek-Mercer smoothing technique the REJM method, and call our method using the Absolute discounting smooth-ing technique the READ method, while we call the method (with feedback) in [4] the BLM method. We use precision-recall plot and average precision as two performance measures to evaluate the above three methods. 5.1 Data Sets We experiment over three data sets taken from TREC. They are the Associate Press Newswire (AP) 1988-90 with queries 51-150, the Financial Times (FT) 1991-94 with queries 301-400, and the Federal Register (FR) 1988-89 with queries 51-100. The first two data sets are news corpora and they are homogenous. In contrast, FR is a heterogeneous collection consisting of long documents than can span different subject areas. Queries are taken from the title field of TREC topics. Relevance judgments are taken from the judged pool of top retrieved documents by various participating retrieval system from previous [2]. Detail information of the data sets is given in Table 1.
 5.2 Experimental Setup Two sets of experiments are performed in this paper. The first set of exper-iments is to compare the performances of the READ method and the REMJ method with the performance of the BLM method. We get different results with different parameter setting in the READ method and the REJM method, but we use the best results of them to compare their performances. The second set of experiments investigates whether the performances of the READ method and the REMJ method are sensitive to the parameters which are  X  ,  X  ,  X  ,and  X  . different values of the two interpolation coefficients  X  and  X  , in the experiment, we assign fixed values (which make the methods have best retrieval performance empirically) to a coefficient while we change the values of the other coefficient. 5.3 Experimental Results Results of the first set of experiments are shown in Fig.1 and Table 2. From the precision-recall plots in Fig.1, we can observe that the performances of the REJM method and the READ method are better than that of the BLM method on the three data sets. In Table 2, it is shown that the average precision of the READ and the REJM on the three data sets are all better than the BLM method. We think that the improvements are mainly attributed to that the methods of performing feedback in REJM and READ are more compatible with the essence of the language modeling approach than that in BLM . And, we also note that the REJM method performs better in performance than the READ method. We think it is a result of that the Jelinek-Mercer smoothing method performs better than the Absolute discounting smoothing method.
 of part (a) in Fig.2 show the average precision of the REJM method on the three data sets according to different values of  X  , while  X  =0.7 and  X  =0.6. We can observe that the average precision of the REJM method is quite sensitive to the setting of  X  , and that the average precision is better when the value of  X  approximates to 0.6.
 on the three data sets for different settings of the parameter  X  , while the parame-ters  X  and  X  are set to be 0.7 and 0.6 respectively. Similarly, we can also observe that the average precision of the READ method is sensitive to the setting of  X  . and the READ method for different values of  X  , while  X  =0.6,  X  =0.7, and  X  =0.55. We find that the average precision of the REJM method and the READ method is significantly sensitive to the setting of  X  on both AP data set and FT data set, but it is relatively sensitive on FT data set. We think that the main reason may be the AP data set and the FT data set are homogenous, but the FR data set is heterogeneous.
 and the READ method on the three data sets for different values of  X  , while  X  =0.6,  X  =0.7, and  X  =0.7. It is obvious that the average precision of the two methods is relatively sensitive to the setting of  X  on three data sets, and the average precision of the two methods decreases while the value of  X  is larger than 0.55 on the FR data set. The phenomenon may be also relevant to that the FR data set is heterogeneous. We have presented a new method for information retrieval based on language model with relative entropy and feedback. Experimental results show that the method performs better than the method in [4]. Analysis of the results indicates that the performance is sensitive to the smoothing parameters used to estimate the value of the document language model. And the analysis also indicates that the performance is not always very sensitive to the interpolation coefficients which are used to estimate the value of the query language model. It sometimes is only relatively sensitive on different data sets.

