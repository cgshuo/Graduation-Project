 Ranking is the central problem for many applications in information retrieval (IR), such as document retrieval, web search, question answering, advertisement, and multimedia retrieval [6]. In the standard setting of ranking, a query is as-sociated with a set of objects to be ran ked, and a ranking model is employed to rank these objects in the descending order of their relevance to the query.
In this paper, we study a new task of ranking, called  X  X upplementary data assisted ranking X , or  X  X upplementary ranking X  for short. Different from the stan-dard setting of ranking, in the new task, two sets of objects are associated with a given query. One set contains the target objects to be ranked, and the other set contains some supplementary objects related to the query, whose orders are however not of our interest. For ease of reference, we call these two sets the target set and the supplementary set respectively.
 Many real ranking problems are in nature supplementary ranking problems. For example, in multi-lingual search [3], when a user issues an English query, a set of English documents containing the query terms compose the target set, while documents that are relevant to the query but written in other languages compose the supplementary set. In Web image search, the images compose the target set, while the web pages containing these images compose the supplementary set. As can be seen, in some applications, the target objects and the supplementary objects are of the same type (e.g., web pages), while in some other applications, they can be heterogeneous (e.g., images and web pages).

To solve the supplementary ranking task, one can directly apply existing meth-ods developed for standard ranking. Ho wever, due to the differences between the two tasks, such a direct application may not be a good choice. For example, while supervised learning to rank techniques have been shown very effective for stan-dard ranking, they completely ignore the supplementary data. Semi-supervised learning to rank methods leverage supplementary data in training, however, the supplementary data is still ignored in test. It is expected that if one can make good use of the supplementary data in both training and test, he/she can do a better job than existing approaches. This is exactly our proposal, and we refer to it as  X  X upplementary learning to rank X .

In our proposed approach, the ranking score of a target object is determined by two sub ranking models. The first sub model is similar to the ranking model in previous work, which measures the m atching between the target object and the query. The second sub model considers the relationship (e.g., similarity, preference, and parent-child relationsh ip) between the target object and those supplementary objects. Then we learn the two sub models by minimizing a certain loss function (which evaluates whether the ranking of the target objects is correct) on the training data. In the test phase, the learned sub models are applied to rank the target objects asso ciated with a new query, based on the information contained in both the target and supplementary sets. This new ranking mechanism has following advantages. First, the information contained in the supplementary set can be leveraged in a more comprehensive manner (i.e., in both training and test) than supervised and semi-supervised learning to rank. Second, from the machine learning point of view, the training and test processes become more consistent wi th each other, which ensures the learned model to generalize better than in semi-supervised learning to rank.

As a showcase of the proposed approach, we develop two Boosting-style sup-plementary learning to rank algorithms. These algorithms address two cases of supplementary ranking, one with homogeneous target and supplementary ob-jects, and the other with heterogeneous objects. In both algorithms, we leverage the supplementary objects in the construction of weak rankers for the second sub ranking model, and specify the relationship between target and supplementary objects as pairwise preference. In this way, the resultant algorithms regard the supplementary data as a reference, and ensure that a target object is ranked high if it is highly relevant to the query by itself, and it is more relevant than many supplementary objects. Experimental results on both public and large-scale com-mercial datasets show that the proposed algorithms (and thus the  X  X upplemen-tary learning to rank X  approach) can make effective use of the supplementary data and outperform previous methods.

The rest of this paper is organized as follows. Section 2 introduces the new task of supplementary ranking and a general approach to tackle the task. In Section 3, we present two Boosting-style algorithms for supplementary ranking. Experimental results are reported in Section 4. Conclusions and future work are given in the last section. 2.1 Supplementary Data Assisted Ranking In the standard setting of ranking, for each given query, the task is to rank a set of target objects according to their relevance to the query. While this is a general abstraction for ranking tasks, it ignores the fact that in many real applications, one can straightforwardly collect a set of supplementary objects related to the query, in addition to the t arget objects (for ease of reference, we call the sets containing the target objects and the supplementary objects the target set and the supplementary set respectively). The ranking of the objects in the supplementary set is usually not of our interest, however, these objects can be used to improve the ranking of the target objects.

Here we list several real applications in which supplementary objects naturally exist. Note that this is just an incomplete list, and one can find many other similar applications.  X  Multi-lingual Search . In the scenario of multilingual search [9], when a user  X  Document Re-ranking . In the scenario of re-ranking [5], we are given a initial  X  Web Image Retrieval . Because of the semantic gap between textual queries Given the wide availability of supplementary data as mentioned above, if we can well utilize them to assist the ranking of the target objects, we should be able to achieve better ranking performance in many applications. This is, however, not sufficiently studied in the literature of information retrieval. To emphasize its importance, we formally define the task of  X  X upplementary data assisted ranking X  (or  X  X upplementary ranking X  for short) in this paper. In the task, it is required that one leverages the supplementary objects to improve the ranking performance of the target objects. 2.2 Supplementary Learning to Rank In this subsection, we discuss how to solve the problem of supplementary ranking.
One na  X   X ve choice is to directly apply existing methods developed for standard ranking, e.g., the learning to rank methods which have been widely used in the literature [6]. However, due to th e differences between the two tasks, this might not be a good choice. For example, in most existing learning to rank methods (either supervised or semi-sup ervised), the ranking score of an object is only determined by the matching bet ween the query and the object. If we use such ranking models, the supplementary data can at most be considered in the training process (e.g., to refine the loss functions as in semi-supervised learning to rank). In the test phase, however, no supplementary data can be leveraged. To tackle the problem, one needs to re-define the ranking model to explicitly incorporate the supplementary data. This is exactly our proposal.

For ease of description, we give some notations in Table 1. If without confusion, we will omit the superscript q in the following discussions. With these notations, we can describe our proposed new ranking model as follows. where f Z ( x ) indicates that the ranking score of an object depends not only on itself x but also on the supplementary set Z .
 As can be seen, the proposed ranking model consists of two sub models. The first sub model f 0 ( x ) is the same as the ranking model used in traditional learning to rank methods, which is determined by x itself. We refer to it as  X  X ndividual sub model X . The second one f 1 ( x ; Z ) is, in contrast, determined by the relationship between x and the supplementary objects in Z . We refer to this sub model as  X  X upplementary sub model X . Note that, in some applications like web image retrieval, the target objects and the supplementary objects may be of different types (e.g., one is visual image and the other is textual document). In such a case, x i and z i may locate in different feature spaces, and we need to carefully design the supplementary sub model to handle the situation (we will make more discussions on this in Section 3).

Suppose both sub models contain unknown parameters. Then one can use a training set to learn the parameters. We call the approach that automatically learns the parameters in the sub models  X  X upplementary learning to rank X . In principle, any loss function can be used for this purpose, such as the hinge loss, the exponential loss, and the cross entropy loss, as long as it can measure whether the target objects in the training se t are correctly ranked by the model.
After we learn the sub models f 0 and f 1 , we can apply them to rank the target objects associated with a new query. Note that although the ranking model considers the supplementary objects, we only need to compute the ranking scores for the target objects. The supplementary objects contribute to this process, but it is unnecessary to compute their ranking scores, since this is not of our interest.
As compared with supervised learning to rank, supplementary learning to rank can leverage more information contained in the supplementary data. Since the ben-efit of using supplementary data has been verified by many previous works [3,11], supplementary learning to rank can be expected to achieve better ranking perfor-mance than supervised learning to rank. As compared with semi-supervised learn-ing to rank, supplementary learning to rank makes the training and test processes more consistent with each other. According to statistical learning theory [10], the trained model can be expected to generalize better on the test set. There are two steps to design a supplementary learning to rank algorithm: (1) defining the individual sub model and supplementary sub model, and (2) learning the parameters of the two sub models from the training data.

As mentioned in the previous section, any ranking loss can be used. In this section, we take the exponential loss as example and derive two Boosting-style algorithms for supplementary ranking. The first algorithm, RankBoost-Same, assumes that the target objects and the supplementary objects are of the same type. The second algorithm, RankBoost-Heter, can handle the case where the two sets contain heterogeneous objects.
 3.1 RankBoost-Same In this subsection, we consider a simple case, in which the target set and the supplementary set contain the same type of objects. Here we make discussions on the two sub models separately.

First, following the basic idea of Boosting, it is natural to define the individual sub model as a linear combination of a set of weak rankers. Assume there exist a set of weak rankers H = { h ( x ) } and each weak ranker is a function of x .Then the individual sub model f 0 ( x ) can be defined as follows, where  X  t is the combination weight of h t ( x ).

Second, we discuss the formulation of the supplementary sub model. We derive a new set of weak rankers from H using supplementary objects. Specifically, for each weaker ranker h  X  X  , we can define a new weak ranker as below, where | X | is the number of elements in a set, and I (  X  ) is an indicator function.
From Eqn. (3), one can see that here we specify the relationship between target and supplementary objects as pairwise preference. That is, we use h to count how many supplementary objects the target object x can beat in terms of the weak ranker h . The more supplementary objects a target object beats, the larger score it will get from h . In this way, the supplementary objects actually serve as a reference, and a target object is ranked high if it is more relevant than many supplementary objects.

Note that all the h functions compose a new set H . Again, following the idea of Boosting, we can define the supplementary sub model as the linear combina-tion of the weak rankers in H ,
After defining the set of the weak rankers, it is natural to use Boosting techniques to train the sub models. In the trainin g process, we follow two common practices in Boosting-style algorithms: (i) we take each feature of a query-document pair as aweakranker h in H ; (ii) in order to select the best weak ranker, we go through all the weak rankers in the candidate set, compute theirs losses, and choose the one with the minimum loss. The detailed algorithm is shown in Table 2. We refer to the algorithm as RankBoost-Same. 3.2 RankBoost-Heter In this sub section, we consider anothe r case, in which the target objects and the supplementary objects are of different types. In other words, x i and z i have different feature representations. Again, we make discussions on the two sub models separately.
First, for the individual sub model, the discussions can be very similar to those for RankBoost-Same. That is, w e assume that there exists a set H = { h ( x ) } of weak rankers based on the feature representation of the target objects, and define the individual sub model f 0 as Eqn. (2).

Second, for the supplementary sub model, the situation is a little more com-plex, since the target objects and the supplementary objects do not share the same feature representation. As a result, weak rankers in H cannot be applied to the supplementary objects. To tackle the problem, we assume that there is another set of weak rankers G = { g ( z ) } defined on the supplementary objects. Then for each weaker ranker h  X  X  and each weaker ranker g  X  X  , we define a new weak ranker h  X  X  as follows,
The underlying assumption in the above definition is that although the features for x i and z i are heterogeneous, the outputs of the weak rankers based on these features can be comparable with each other since they all measure the relevance of an object to the query.

After defining the new weak rankers, the supplementary sub model can be defined as their linear combination in Eqn. (4).

Then we can use Boosting techniques to trainthesubmodels.Wecallthecor-responding algorithm RankBoost-Heter. The relationship between RankBoost-Heter and RankBoost-Same can be summarized as follows.
  X  RankBoost-Heter can also be used to deal with supplementary ranking with  X  For the implementation, RankBoost-Same and RankBoost-Heter can be very 4.1 Settings Two datasets were used in our experime nts: one is from the LETOR benchmark collection [7,8], and the other is from a c ommercial web search engine. We used the MQ2007-semi dataset in LETOR 4.0 in our experiments, because it contains both labeled and unlabeled data. There are about 1700 queries in this dataset. On average, each query is associated with about 40 labeled documents and about 1000 unlabeled documents. There are three levels of relevance labels. To evaluate the proposed algorithms in the real scenario of web search, we also used a dataset obtained from a commercial search engine. We refer to it as the ComSE dataset. There are 6,600 queries in the dataset. On average, each query is associated with about 20 labeled documents and more th an 140 unlabeled documents. There are five levels of relevance labels.

The multi-fold cross validation strategy was used on both datasets. All the results reported in this section are the average results over multiple folds. We used NDCG [4] as the evaluation measure in our experiments, which is a widely-used IR measure for multi-level relevance judgments. 4.2 Supplementary Ranking on Homogeneous Data Note that in both MQ2007-semi and ComSE, the labeled and unlabeled docu-ments are of the same type, and represent ed in the same feature space. Therefore it is straightforward to use them to study the supplementary ranking with ho-mogeneous data. In particular, we regard the labeled documents as the target objects and the unlabeled documents as the supplementary objects.
 Baselines In addition to RankBoost-Same, we implemented three baselines.  X  X ankBoost. RankBoost [2] is a supervised learning to rank algorithm  X  RankBoost-All. In this method, we treat unlabeled documents as irrele- X  X ankBoost-Prop. This is the semi-supervised learning to rank method For all these baselines and RankBoost-Same, we set the maximal number of selected weak rankers to 600. All the algorithms have converged within 600 iterations in our experiments. Compared with RankBoost, the computational complexity of RankBoost-Same is increased due to the usage of supplementary data. However, according to [2], the computational complexity of RankBoost-Same is only about twice that of RankBoost since the number of weak rankers is doubled. In our experiments RankBoost-Same took less than two seconds per iteration while RankBoost took one second.
 Results. The experimental results on the MQ2007-semi and ComSE dataset are listed in Table 3. Take the results on the MQ2007-semi for example, we have the following observations.

First, RankBoost-All does not perform well. Its performance is even worse than RankBoost. This indicates that improper use of the supplementary data (i.e., simply treating those supplementary objects as irrelevant) may hurt the ranking performance.

Second, RankBoost-Prop slightly outperforms RankBoost. This is also in ac-cordance with the results reported in [1]. This result indicates that appropriately leveraging the supplementary data in the training process can lead to perfor-mance gain. However, the improvement of RankBoost-Prop over RankBoost is not statistically significant.

Third, RankBoost-Same outperforms all the baseline algorithms, including both supervised learning to rank methods and semi-supervised learning to rank algorithms. Furthermore, the improvement of RankBoost-Same over RankBoost is statistically significant in terms of NDCG@3, 5, and 10 (we use  X  to indicate statistical significance in the table). This indicates that further considering the supplementary data in the test process can lead to more performance gain. This is in accordance with our discussions throughout the paper.
 Similar conclusions can be drawn from the results on the ComSE dataset. 4.3 Supplementary Ranking on Heterogeneous Data Note that there is no publicly available heterogeneous dataset that can be used to test our proposed methods. In our experiments, we alternatively simulated such datasets based on MQ2007-semi and ComSE. Specifically, we randomly split the features in each dataset into two subsets A and B ,eachwithhalfof the original features. We used the features in subset A as the representation of the target objects, and used the features in subset B as the representation of the supplementary objects. In this way, we can guarantee that the target objects and the supplementary objects locate in different feature spaces. For ease of reference, we call the new datasets generated in this way MQ2007-heter and ComSE-heter.

Note that not all the baselines used in Section 4.2 can still work on MQ2007-heter and ComSE-heter, mainly because they assume that the target objects and the supplementary objects share the same feature representation. For example, RankBoost-Prop needs to compute similarity between a target object and a supplementary object. When the target objects and the supplementary objects do not use the same feature representation, the similarity cannot be calculated. As a result, RankBoost becomes the only meaningful baseline on this task. Again, we set the maximal number of selected weak rankers to 600 for RankBoost and RankBoost-Heter.

Table 4 shows the results of RankBoost and RankBoost-Heter on the two het-erogeneous datasets. From the table we can see that RankBoost-Heter performs much better than RankBoost. The improvements are statistically significant in terms of NDCG@1, NDCG@3 and NDCG@10 on the MQ2007-heter dataset, and in terms of all the four measures on the ComSE-heter dataset.

To sum up, the experimental results show that our proposed two algorithms perform better than several supervised learning to rank and semi-supervised learning to rank methods. This verifies our claim that by using the supplementary data in both training and test, one can do a better job in supplementary ranking. 4.4 Discussions In this section, we conduct some further study on our proposed algorithms, in order to understand how supplementary learning to rank leads to performance gains.

As can be seen from Section 3, the supplementary data is mainly used to derive new weak rankers. Therefore, we hypothesize that the good performance of our proposed algorithms should come from these new weak rankers. That is, the new weak rankers h are on average more effective than the original weak rankers h . To test this hypothesis, we have conducted the following analysis.
First, we looked at the models learned by the proposed algorithms. Note that there is a weight for each selected weak ranker in the models. The larger the weight is, the more important the corresponding weak ranker is. Here we take the MQ2007-semi and MQ2007-heter datasets for example. Given the models learned by RankBoost-Same and RankBoost-Heter from these datasets, we cal-culated the absolute sum of the weights (denoted as Sum Of Weight for ease of reference) for the original weak rankers and that for the new weak rankers, and list them in Table 5. From the table, we can see that for both RankBoost-Same and RankBoost-Heter, the new weak rankers earn much larger weights than the original weak rankers over all the five folds. This clearly shows that the new weak rankers play a major role in the learned models.

Second, we investigated the goodness of each individual weak ranker. Using the exponential loss of RankBoost, we can compute the ranking loss of the original rankers and new rankers as follows.

The smaller the loss is, the better the weak ranker is. For the MQ2007-semi dataset, we found that 12 of the top 20 best rankers belong to H ,andthe other8belongto H . For the MQ2007-heter dataset, we found that 14 of the top 20 best rankers belong to H and the other 6 belong to H .Inotherwords, H contains more good rankers than H . This verifies our hypothesis: on average, the supplementary data can help enhance the effectiveness of a weak ranker.
However, we would also like to point out that although on average the new weak rankers are more effective, the original weak rankers also play an important role in the models. They occupy a signific ant part in the top rankers, and their sum of weight is also not negligible. T his indicates the necessity of using our proposed two sub models simultaneously.
 In this paper, we have proposed a new task of ranking, named supplementary data assisted ranking, which can cover many important applications. We have proposed a general approach to the task, named supplementary learning to rank, and developed two Boosting-style algorithms. Experimental results have shown that by use of supplementary learning to rank, significantly better ranking per-formance can be achieved, than using supervised and unsupervised learning to rank techniques.

For future work, we plan to investigate the following issues. (1) We will study other ways of using the supplementary data in the ranking model. (2) We will develop supplementary learning to rank algorithms based on support vector ma-chines and neural networks. (3) We will apply the proposed approach to more real applications. (4) We will study the theoretical properties of supplementary learning to rank, e.g., its generalization ability and statistical consistency. We would like to thank Tie-Yan Liu and Hang Li for their valuable comments and suggestions on this work.

