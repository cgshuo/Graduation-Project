 Semantic analysis tries to solve problems arising from poly-semy and synonymy that are abundant in natural languages. Recently, Gabrilovich and Markovitch propose the Explicit Semantic Analysis (ESA) technique, which complements the well-known Latent Semantic Analysis (LSA) technique. In this paper, we show that the two techniques are not as dis-tinct as their names suggest; instead, we find that ES Ais equivalent to a LS Avariant, and this equivalence generalizes to all kernel methods using kernels arising from the canoni-cal dot product. Effectively, this result guarantees that ESA would not outperform the peak efficacy of LS Afor any appli-cations using the above kernel methods. In short, this paper for the first time establishes the connections between ESA and LSA, quantifies their relative efficacy, and generalizes the result to a big category of kernel methods.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  Linguistic processing Algorithms, Experimentation, Theory Latent semantic analysis, explicit semantic analysis, kernel methods
Latent semantic analysis (LSA) [4] is arguably the best known semantic analysis technique. It distills the con-cepts through a Singular Value Decomposition (SVD) of the document-by-term matrix constructed from the background corpus. Specifically, it findsa set of orthonormal bases in the term space, each of which roughly corresponds to a set of co-occurring terms. It is called latent because its concepts are not readily comprehensible by human beings. Despite the lack of comprehensibility, LS Ahas been successfully used in numerous applications and has inspired many advanced methods, e.g. , the probabilistic LS A(pLS A) and the Latent Dirichlet Allocation (LDA). But even with these advanced models, the concepts remain latent as they do not corre-spond to meaningful entities that human beings can readily understand.

In order to improve the comprehensibility, Gabrilovich and Markovitch propose the Explicit Semantic Analysis (ESA) technique, which takes each Wikipedia article as a concept. Since Wikipedia articles are readily comprehensi-ble by human beings, the concepts are X  X xplicit. X  Similar to LSA, text segments are projected onto the semantic space spanned by the concepts. The authors have shown the effi-cacy of the generated semantic features in an array of appli-cations, including computing semantic relatedness [8], text categorization [6] and information retrieval [5]. Encouraged by the effectiveness of ESA, other researchers also extend ES Ato text clustering [10, 11], word sense disambiguation [13], and multilingual retrieval [16], among many others.
While applauding the success of ESA, some researchers start probing why ES Aworks. Intuitively, there are two possible explanations. The first attributes the efficacy to the use of Wikipedia as the background corpus, which effec-tively leverages the largest repository of human knowledge base. However, Anderka and Stein report that the use of Wikipedia as the background corpus is not essential to the efficacy of ES A[2]. They show that other corpus ( e.g. ,the Reuters) could lead to even better results. As an extreme case, they show that using a random Gaussian matrix in place of the document-by-term matrix only marginally de-grades the efficacy. The other explanation gives credits to ESA X  X  unique way of projection, i.e. , using the document-by-term matrix as the project matrix directly. Nevertheless, any semantic analysis techniques could adopt Wikipedia as thebackgroundcorpus. Byfocusing ontheprojection aspect of ESA, Wong et al. reveal the connection between ESA and the Generalized Vector Space Model (GVSM) [18], and later show that CL-ES A[16], a cross-lingual retrieval method us-ing ESA, is equivalent to that using GVSM [1].

The connection between ES Aand GVSM, as well as the latent semantic kernel work [3], inspires us to examine by Microsoft Research at Redmond. the connection between ES Aand LS A. Intuitively, the two methods leverage the same document-by-term matrix, and they should not be as dichotomous as their names suggest. We formally study ES Aand LS Ain the GVSM framework, and find that ES Aand an LS Avariant induce the same semantic kernel, and hence ES Aand the LS Avariant are equivalent as retrieval models.

Building on top of this connection, we further reveal that what is more fundamental than the equivalence as retrieval models is the fact that ES Aand the LS Avariant are equiv-alent wrt. dot product. Because dot product defines a dot product space, and any algorithms that only depend on dot products ( i.e. , rotationally invariant) can be kernelized (p. 35 [17]), we claim that the equivalence is wrt. all kernel methods using linear kernel. In the end, we further gen-eralize the equivalence to all kernel methods with kernels arising from the dot product ( e.g. , Gaussian kernel, polyno-mial kernel and Sigmoid kernel). Thanks to the development of kernel methods [17], the set of kernel methods to which ES Aand the LS Avariant are equivalent include many top performing algorithms for classification, regression, cluster-ing, metric learning, novelty discovery, etc. . An immediate consequence of this proved equivalence is that ES Ais guar-anteed not to surpass the peak performance of LS Afor all the learning tasks using kernel methods where kernels arise from the dot product.

The rest of this paper is organized as follows. Section 2 discusses related work in details. We unify ES Aand LS A using SVD in Section 3, and reveal their connections and im-plications in Section 4. We present the experimental results in Section 5, and conclude this paper in Section 6.
The pursuit of semantic understanding of natural lan-guages dates back at least to 1970s when researchers try to build text semantics through logical formulae ( e.g. , [15]). However, the tremendous ambiguities inherent in natural languages hinder the development of comprehensible gram-mars for semantic understanding. Similar to speech recogni-tion, statistical methods gradually become the mainstream for natural language processing ( e.g. , see [12]).
Latent SemanticAnalysis (LSA),arguably thebest known semantic analysis method, was proposed by Deerwester et al. in 1990 [4]. LS Aextracts and represents the contextual-usage meaning of words by performing an SVD on the document-by-term matrix. It captures the co-occurrence in-formation among terms, and constructs a set of concepts based on the co-occurrences. Because thus constructed con-cepts do not correspond to human comprehensible entities, it is latent . Despite its incomprehensibility, LS Ahas been successfully used in many applications.

To improve interpretability, Gabrilovich and Markovitch recently propose the Explicit Semantic Analysis (ESA) tech-nique [8, 9], and report on successful results in computing semantic relatedness [8], information retrieval [5], and text categorization [6, 7]. ES Ais explicit in that each concept is a Wikipedia article that is readily understood by human beings. Because of its advantages in efficacy and implemen-tation, ES Ais embraced by the research community for var-ious tasks.

This wave of excitement about ES Aunintentionally fos-ters a general impression that ES Ais superior to LS A: the literature either explicitly reports a superior result of ESA to LS Afor certain tasks ( e.g. , Tables 4 of [8] on semantic relatedness, and its reproductions as Tables 2 in [9] and as Table 3 in [14]), or fails to report the result of using LSA ( e.g. , [6, 7] for text categorization, [11] for text clustering, and [16] for multilingual retrieval). Therefore, the relative efficacy between ES Aand LS Aremains unclear. In this paper, we show that ES Aand LS Aare tightly connected, and in principle, ES Ais not able to outperform the peak performance of LSA, which provides a clear answer to their relative efficacy.

In this paper, we further generalize the connection be-tween ES Aand LS Ato all the kernel methods with kernels arising from dot product. Some related work along this line includes [2] and [3]. Cristianini et al. examine the Gener-alized Vector Space Model (GSVM) from a kernel point of view and show that the application of LS Acould be kernel-ized. But the authors do not realize that an LS Avariant actually leads to the same semantic kernel as using the en-tire document-by-term matrix for projection, a result that we will establish in Section 4.1. Similarly, [2] reveals the con-nection between ES Aand GVSM, but does not relate ES A and LS Ain the kernel point of view. To the best of our knowledge, this is the first paper discussing the implications of ES Aand LS Afor general kernel methods. We unify ES Aand LS Athrough SVD in this section. ground corpus, where  X  and  X  are the corpus size and the vocabulary size, respectively. The value of  X  (  X , X  )isabout the appearance of term  X  in document  X  , for example, the TF-IDF value. Because this paper is concerned about the connection between ES Aand LS A, the particular values in-side  X  are not critical. Given a text segment  X   X   X   X  in BOW representation, we now examine how LS Aand ES A performs semantic analysis on  X  .

LS Afirst decomposes  X  using SVD. Let  X  =  X  X  X  X  X  X  (  X  ) (  X   X   X  X  X  X  {  X , X  } ), this gives where  X   X   X   X   X   X  , X   X   X   X  =  X  ,  X   X   X   X   X   X  , X   X   X   X  =  X  ,and  X = diag (  X  1 , X  2 ,  X  X  X  X  , X   X  )with  X  1  X   X  2  X  X  X  X  X  X  X   X   X  &gt; 0.
In canonical LSA, the column space of  X  is taken as the semantic space, and the projection image of  X  in the seman-tic space is hence  X   X  =  X   X   X  . Dependingon the applications, similarities between text segments are calculated based on  X   X  X . Because only  X  is used, we denote canonical LS Aby LSA V .
 Now let us take a look at ESA. In its original description, ES Afirst builds an inverted index from each term to the Wikipedia articles that contain the term. Then for a given text segment  X  , it retrieves the inverted lists for each term present in  X  , and returns the centroid of these inverted lists using the weights in  X  . To formalize, the projection image of  X  by ES Ais simply  X   X  =  X  X  X  , which is also recognized by [2].

We therefore could unify ES Aand LS Ausing SVD, as illustrated by Figure 1. Figure 1 plots the schema of SVD, and depicts how  X  is transformed into  X  X  X  when  X  is decom-posed. In Figure 1, we recognize that the ES Aprojection image and the LS Aprojection image at the two ends of the transformation chain, which hints on their connections.
Specifically,  X   X   X  is the projection image of LSA V .Inthe next step, the component of the image is scaled by the corre-sponding singular values, giving  X   X   X   X  . Finally, multiplying  X  on the left instantiates the image into  X   X  , resulting in the image  X   X   X   X   X  ,whichisexactly  X  X  X  , the projection image of ESA.

In addition to marking out the ES Aimage and LS Aim-age, Figure 1 also suggests  X   X   X   X  be a legitimate projection image. We denote this LS Avariant by LSA SV , which puts bigger weights to concepts corresponding to bigger singular values.

In the general case, we define the LS Afamily as any methods that take  X   X   X   X  as the projection image, where  X  control how the image of LSA V is scaled. In princi-ple,  X  could be automatically learnt to maximize the effi-cacy of LS Afor a given application. We will leave the full exploration of this direction to the future work, but con-sider one more LS Avariant, named LSA  X = experiment, LSA off between ES Aand canonical LS A. Finally, we note that aside from numerous variants encompassed by the LS Afam-ily, LS Acould also exploit low-rank approximation in SVD for the best efficacy. This section establishes the equivalence between ES Aand LSA SV , first as retrieval models in Section 4.1, and second wrt. dot product and metrics arising from the dot product in Section 4.2. In addition, we show that the equivalence holds no matter whether semantic features are used alone or together with original BOW features.
Wong et al. present the Generalized Vector Space Model (GVSM) in 1985 [18]. Basically, given two text segments  X  1 and  X  2 in BOW term vector space, the authors suggest a term-by-term matrix, encoding the relatedness between terms. For example, if  X  (  X , X  ) &gt; 0, the  X  th term and the  X  th term are collapsed together, and are hence considered a match.
 Cristinani et al. show in [3] that the canonical LS A( i.e. , LSA V in this paper) defines a latent semantic kernel  X  =  X   X   X   X  because the matching of LSA V images is calculated by where  X  ,  X  denotes the canonical dot product.
 Therefore, LSA V represents a retrieval model in the GVSM framework. We note that the semantic kernel  X   X  is hard to interpret. Let  X   X  denote the  X  th row of  X  ,then  X   X  (  X , X  )=  X   X   X   X   X   X  does not bare too much meaning.
Now let us look at the semantic kernels induced by ESA and LSA SV . Simple algebra as below reveals that they ac-tually induce the same semantic kernel: This states that ES Aand LSA SV are equivalent retrieval models from the GVSM point of view.

Although  X   X  X  X  =  X   X  2  X   X  appears hard to interpret, the above algebra equation states that  X   X  X  X  (  X , X  ) &gt; 0ifandonly if the  X  th term and the  X  th term co-occur in at least one document in the background corpus. Furthermore, since  X   X  differs from  X   X  X  X  by the  X  2 in the middle,  X   X  can be interpreted as kernels considering fuzzy co-occurrences. In general, we hope that the equivalence between ES Aand LSA SV would help us gain insights into the interpretability of LSA.
In this section, we show that ES Aand LSA SV are equiv-alent wrt. the dot product. Although the proof is based on the same algebraic equation as above, the result has a profound implication than the equivalence as retrieval mod-els. For convenience, let us define the ES Aprojection as  X  (  X  )=  X  X  X  ,andthatfor LSA SV by  X  (  X  )= X   X   X   X  .The equivalence is formally stated by the following theorem. Theorem 1 (Equivalence wrt. Dot Product).
 Given any  X  1  X   X   X  ,and  X  2  X   X   X  ,  X  X  X  (  X  1 ) ,  X  (  X  2  X  X  X  (  X  1 ) ,  X  (  X  2 )  X  .
 Proof: Theorem 1 states that although  X  (  X  )  X  X  X  X  (  X  )(infact,  X  (  X  )  X   X   X  and  X  (  X  )  X   X   X  ), the dot product is actually the same for any given two text segments in the semantic spaces defined by ES Aand LSA SV , respectively. There-fore, we could formally claim  X  and  X  induce the same dot product space. Furthermore, because a dot product defines anorm,  X   X   X  = same metric space. Intuitively, this guarantees that all met-rics, e.g. , length, angle, and distance, are always the same, which is formally stated by the following corollary.
Corollary 1.  X   X  1 , X  2  X   X   X  ,  X  X  X  X  (  X  (  X  1 ) ,  X  (  X  2  X  X  X  X  (  X  (  X  1 ) ,  X  (  X  2 )) ,  X  X  X  (  X  )  X  =  X  X  X  (  X  )  X  and  X  (  X  (  X   X  (  X  (  X  1 ) ,  X  (  X  2 )) ,where  X  .  X  denotes the conventional L-2  X  (  X  1 , X  2) =  X   X  1  X   X  2  X  is the Euclidean distance. Proof  X  X  X  X  (  X  (  X  1 ) ,  X  (  X  2 )) =  X   X  X  X  X  X  X  (  X  1 ) ,  X  (  X  2 )  X   X  (  X  (  X  1 ) ,  X  (  X  2 )) 2 =  X  X  X  (  X  1 )  X  X  X  (  X  2 )  X  2
In addition to the basic metrics as listed in the above corollary, we further show that ESA  X  LSA SV with respect to some X  X dvanced X  X etrics that are extensively used in ma-chine learning for measuring similarity. In particular, they are usually called kernel functions. The canonical dot prod-uct is simply the linear kernel whereas the three most com-monly used nonlinear kernels are listed below It is easy to see all the three kernels are functions of the Euclidean distance and the dot product. Based on Corol-lary 1, when these kernels are used, the similarity stays the same no matter whether the images are obtained from ESA or LSA SV .

Finally, we note that the equivalence in the same metric space remains valid even when the original BOW features are used together with the generated semantic features. To see this, we define then it is easy to see  X   X  X  X   X   X  wrt. dot product as well. Fol-lowing the same argument as described above, all the claims regarding  X  X  X  X  would apply to  X   X  X  X   X   X  . In other words, the equivalence between ES Aand LSA SV is agnostic to the original BOW features.
Now that we have proved the equivalence between ESA and LSA SV ,letus considertheimplications todownstream learning algorithms. This is important because all the pre-vious efforts in Section 4 would be in vain if the downstream algorithms do not respect the equivalence. Intuitively, since ES Aand LSA SV induce the same metric space, any algo-rithms manipulating data points in the metric space should yield the same results regardless ES Aor LSA SV .
Because what is fundamental to metric space is the dot product, we claim that any algorithms that can be exclu-sively expressed by the dot product would respect the equiv-alence. As it turns out, the set of such algorithms is simply the set of kernel methods [17]. We therefore claim that ESA is equivalent to LSA SV wrt. any kernel methods using lin-ear kernel. Furthermore, because of kernel tricks, the equiv-alence claim is further extended to kernel methods using any kernels arising from dot product, which includes linear ker-nel and the three nonlinear kernels as listed in Section 4.2.
In this section, we report on experimental results that support the proved equivalence between ES Aand LSA SV . In addition to the equivalence, we also compare the efficacy of ES Ato the LS Avariants when the dimensionality of  X  is varied.

The Nov 2005 Wikipedia dump is used as the background corpus in this paper. The dump was first used and cleaned up byGabrilovich. Terms are stemmed, and articles contain-ing fewer than 100 distinct terms are discarded. In order to show that relative efficacy between ES Aand the LS Avari-ants is insensitive to the size of the background corpus, two corpora are sampled from the cleaned Wikipedia dump: The standard TF-IDF technique is applied to both corpora to instantiate the document-by-term matrix.
 Following the common practice [19, 8], the WordSimilarity-353 collection is used as the testbed for semantic relatedness. The set contains 353 noun pairs representing various degrees of similarity. Each pair is rated by 13-16 human judges with university degrees having either mother-tongue-level or otherwise very fluent command of the English language. Word pairs are rated on the scale from 0 (totally unrelated words) to 10 (very much related or identical words). Ratings collected for each word pair are averaged to produce a single relatedness score. The goal is to assign a relatedness score to each pair such that the assigned score will maximally correlate with the human ratings.

Given a pair of terms from the Similarity-353 data set, we apply ES Aand LS Avariants to project them onto the corresponding semantic spaces, and calculate the related-ness score using different metrics. Because we would like to see the assigned score monotonically increases with in-creasingly higher human ratings, the Spearman X  X  rank-order correlation coefficient is used as the evaluation metrics for the goodness of semantic analysis, and a higher correlation coefficient means a better assignment.

Figure 2 plots the efficacy comparison between ES Aand three LS Avariants when  X  is varied from 100 up to 1000 on Wikipedia1k . In the first place, LSA SV always collides with ES Ausing  X  = 1000, no matter whether the relatedness is computed using the cosine similarity, dot product or the Euclidean distance. This is exactly as stated by Theorem 1 and Corollary 1.

Secondly, cosine similarity is the most effective among the three. This is as expected since neither dot product nor the Euclidean distance is normalized. Thirdly, focusing on Fig 2(a), we see that the three LS Avariants could all out-perform ES Awhen  X  is properly chosen. For example, the peak performance of LSA V and LSA which is roughly 18.4% better than ES A(0.38). This agrees with our claim that ES Acannot outperform the peak per-formance of LSA.

Finally,wenotethat LSA between LSA V and LSA SV , as discussed in Section 3. On the one hand, because of its similarity to LSA V ,it achieves quite similar peak performance as LSA V .Onthe other hand, since it is also close to LSA SV , which equates to ES Ain the full rank case, LSA much from ES Awhen  X  becomes bigger than the optimal value. In contrast, LSA V appears to be quite sensitive to  X  : it deteriorates very quickly and becomes much worse than ES Awhen  X  becomes very large. This result suggests that LSA when the search space of  X  is huge. All the above findings are cross validated using the Wikipedia10k corpus.
While we show here, both theoretically and through ex-periments, that ES Adoes not outperform the peak perfor-mance of LSA V , existing literatures claims ES Adramati-cally surpasses LS A( e.g. , [8, 9, 14]). Some possible reasons for the inconsistency could be (1) the authors did not use Wikipedia as the background corpus, (2) the authors used an LS Avariant that is not considered here, and (3) the au-thors used an LS Avariant considered here, but failed to locate the appropriate  X  . Unfortunately, we could not nail down the root cause for each of the above work here. This paper uncovered the equivalence between ES Aand LSA SV as retrieval models, and further revealed that they induce the same metric space. This discovery immediately established the equivalence between ES Aand LSA SV for all kernel methods using kernels arising from the dot prod-uct, and conveys a clear message about the relative efficacy between ES Aand LS A. We hope this work could shed light on the practice as well for real-world practitioners. [1] M. Anderka, N. Lipka, and B. Stein. Evaluating [2] M. Anderka and B. Stein. The esa retrieval model [3] N. Cristianini, J. Shawe-Taylor, and H. Lodhi. Latent [4] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [5] O. Egozi, E. Gabrilovich, and S. Markovitch. [6] E. Gabrilovich and S. Markovitch. Feature generation [7] E. Gabrilovich and S. Markovitch. Overcoming the [8] E. Gabrilovich and S. Markovitch. Computing [9] E. Gabrilovich and S. Markovitch. Wikipedia-based [10] J. Hu, L. Fang, Y. Cao, H.-J. Zeng, H. Li, Q. Yang, [11] X. Hu, X. Zhang, C. Lu, E. K. Park, and X. Zhou. [12] C. D. Manning and H. Schuetze. Foundations of [13] R. Mihalcea. Using Wikipedia for Automatic Word [14] D. Milne and I. H. Witten. An effective, low-cost [15] R. Montague. The Proper Treatment of Quantification [16] M. Potthast, B. Stein, and M. Anderka. A [17] B. Scholkopf and A. J. Smola. Learning with Kernels: [18] S. K. M. Wong, W. Ziarko, and P. C. N. Wong. [19] T. Zesch and I. Gurevych. Wisdom of crowds versus
