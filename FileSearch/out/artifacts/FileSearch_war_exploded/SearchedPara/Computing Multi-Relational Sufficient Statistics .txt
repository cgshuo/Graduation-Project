 Databases contain information about which relationships do and do not hold among entities. To make this infor-mation accessible for statistical analysis requires computing sufficient statistics that combine information from different database tables. Such statistics may involve any number of positive and negative relationships. With a naive enumer-ation approach, computing sufficient statistics for negative relationships is feasible only for small databases. We solve this problem with a new dynamic programming algorithm that performs a virtual join, where the requisite counts are computed without materializing join tables. Contingency table algebra is a new extension of relational algebra, that facilitates the efficient implementation of this M  X  obius vir-tual join operation. The M  X  obius Join scales to large datasets (over 1M tuples) with complex schemas. Empirical evalu-ation with seven benchmark datasets showed that informa-tion about the presence and absence of links can be exploited in feature selection, association rule mining, and Bayesian network learning.
 H.2.8 [ Database Applications ]: Data mining; H.2.4 [ Systems ]: Relational databases sufficient statistics; multi-relational databases; virtual join; relational algebra
Relational databases contain information about attributes of entities, and which relationships do and do not hold among entities. To make this information accessible for knowledge discovery requires requires computing sufficient statistics . For discrete data, these sufficient statistics are instantia-tion counts for conjunctive queries. For relational statis-tical analysis to discover cross-table correlations, sufficient lationships. In other words, the number of table operations is nearly linear in the size of the required output.
Evaluation. We evaluate the M  X  obius Join algorithm by computing contingency tables for seven real-world databases. The observed computation times exhibit the near-linear growth predicted by our theoretical analysis. They range from two seconds on the simpler database schemas to just over two hours for the most complex schema with over 1 million tu-ples from the IMDB database.

Given that computing sufficient statistics for negative re-lationships is feasible , the remainder of our experiments eval-uate their usefulness . These sufficient statistics allow statis-tical analysis to utilize the absence or presence of a relation-ship as a feature. Our benchmark datasets provide evidence that the positive and negative relationship features enhance different types of statistical analysis, as follows. (1) Fea-ture selection: When provided with sufficient statistics for negative and positive relationships, a standard feature selec-tion method selects relationship features for classification, (2) Association Rule Mining: A standard association rule learning method includes many association rules with rela-tionship conditions in its top 20 list. (3) Bayesian network learning. A Bayesian network provides a graphical sum-mary of the probabilistic dependencies among relationships and attributes in a database. On the two databases with the most complex schemas, enhanced sufficient statistics lead to a clearly superior model (better data fit with fewer parame-ters). This includes a database that is an order of magnitude larger than the databases for which graphical models have been learned previously [11].

Contributions. Our main contributions are as follows. 1. A dynamic program to compute a joint contingency 2. An extension of relational algebra for contingency ta-We contribute open-source code that implements the M  X  obius Join. All code and datasets are available on-line[3]. Our im-plementation makes extensive use of RDBMS capabilities. Like the BayesStore system [15], our system treats statistical components as first-class citizens in the database. Contin-gency tables are stored as database tables in addition to the original data tables. We use SQL queries to construct ini-tial contingency tables and to implement contingency table algebra operations.

Paper Organization. We review background for relational databases and statistical concepts. The main part of the pa-per describes the dynamic programming algorithm for com-puting a joint contingency table for all random variables. We define the contingency table algebra. A complexity analysis establishes feasible upper bounds on the number of contin-gency table operations required by the M  X  obius Join algo-rithm. We also investigate the scalability of the algorithm empirically. The final set of experiments examines how the cached sufficient statistics support the analysis of cross-table dependencies for different learning and data mining tasks. Table 1: Translation from ER Diagram to Random Variables. ships, and foreign key constraints to type constraints on the arguments of relationship predicates. Table 1 illustrates this translation, distinguishing attributes of entities ( 1Atts ) and attributes of relationships ( 2Atts ).
Sufficient statistics can be represented in contingency ta-bles as follows [5]. Consider a fixed list of random variables. A query is a set of ( variable = value ) pairs where each value is of a valid type for the random variable. The result set of a query in a database D is the set of instantiations of the first-order variables such that the query evaluates as true in D . For example, in the database of Figure 2 the result set for the query ( intelligence ( S ) = 2 , rank ( S ) = 1 , popularity ( P ) = 3 , teachingability ( P ) = 1 , RA ( P , S ) = T ) is the singleton { X  kim , oliver  X  X  . The count of a query is the cardinality of its result set.

For every set of variables V = { V 1 , ...,V n } there is a con-tingency table ct ( V ). This is a table with a row for each of the possible assignments of values to the variables in V , and a special integer column called count . The value of the count column in a row corresponding to V 1 = v 1 ,...,V n = v n records the count of the corresponding query. Figure 3 shows the contingency table for the university database. The value of a relationship attribute is undefined for entities that are not related. Following [9], we indicate this by writing capability ( P , S ) = n / a for a reserved constant n / a . The as-sertion capability ( P , S ) = n/a is therefore equivalent to the assertion that RA ( P , S ) = F. A conditional contingency Figure 3: Excerpt from the joint contingency table for the university database of Figure 2. table , written is the contingency table whose column headers are V 1 ,...,V k and whose rows comprise the subset that match the condi-tions to the right of the | symbol. We assume that contin-gency tables omit rows with count 0.
In this notation, the variables in the ct -table ct R are de-noted as R  X  Atts ( R ). The goal of the M  X  obius Join algorithm is to compute a contingency table for each chain R . In the example of Figure 4, the algorithm computes 10 contingency tables. The ct -table for the top element of the lattice is the joint ct -table for the entire database.

If a conjunctive query involves only positive relationships, then it can be computed using SQL X  X  count aggregate func-tion applied to a table join. To illustrate, we show the SQL for computing the positive relationship part of the ct -table for the RA ( P , S ) chain.
 Even more efficient than SQL count queries is the Tuple ID propagation method, a M  X  obius Join method for computing query counts with positive relationships only [16]. In the next section we assume that contingency tables for positive relationships only have been computed already, and consider how such tables can be extended to full contingency tables with both positive and negative relationships.
We describe a Virtual Join algorithm that computes the required sufficient statistics without materializing a cross product of entity sets. First, we introduce an extension of relational algebra that we term contingency table alge-bra . The purpose of this extension is to show that query counts using k + 1 negative relationships can be computed from two query counts that each involve at most k relation-ships. Second, a dynamic programming algorithm applies the algebraic identify repeatedly to build up a complete con-tingency table from partial tables.
We introduce relational algebra style operations defined on contingency tables. Selection  X   X  ct selects a subset of the rows in the ct -table Projection  X  V 1 ,...,V k ct selects a subset of the columns in
This section describes a method for computing the con-tingency tables level-wise in the relationship chain lattice. We start with a contingency table algebra equivalence that allows us to compute counts for rows with negative rela-tionships from rows with positive relations. Following [5], we use a  X  X on X  X  care X  value  X  to indicate that a query does not specify the value of a node. For instance, the query R 1 = T ,R 2 =  X  is equivalent to the query R 1 = T.
 Proposition 1. Let R be a relationship variable and let R be a set of relationship variables. Let Vars be a set of variables that does not contain R nor any of the 2Atts of R . Let X 1 ,..., X l be the first-order variables that appear in R but not in Vars , where l is possibly zero. Then we have ct ( Vars  X  1Atts ( R ) | R = T , R = F ) = (1) ct ( Vars | R = T , R =  X  )  X  ct ( X 1 )  X  X  X  X  X  ct ( X l )  X  ct ( Vars  X  1Atts ( R ) | R = T , R = T ) .
 If l = 0 , the equation holds without the cross-product term.
The proof is available at [8]. Figure 5 illustrates Equa-tion (1). The construction of the ct F table in Algorithm 1 provides pseudo-code for applying Equation (1) to compute a complete ct -table, given a partial table where a specified relationship variable R is true, and another partial table that does not contain the relationship variable. We refer to R as the pivot variable. For extra generality, Algorithm 1 applies Equation (1) with a condition that lists a set of re-lationship variables fixed to be true. Figure 5 illustrates the Pivot computation for the case of only one relationship. Algorithm 2 shows how the Pivot operation can be applied repeatedly to find all contingency tables in the relationship lattice. Figures 4 and 6 provide illustration. The outline of this computation is as follows.

Initialization. Compute ct -tables for entity tables. Com-pute ct -tables for each single relationship variable R , con-ditional on R = T. If R =  X  , then no link is specified between the first-order variables involved in the relation R . Therefore the individual counts for each first-order variable are independent of each other and the joint counts can be obtained by the cross product operation. Apply the Pivot function to construct the complete ct -table for relationship variable R .

Lattice Computation. The goal is to compute ct -tables for all relationship chains of length &gt; 1. For each relation-ship chain, order the relationship variables in the chain ar-bitrarily. Make each relationship variable in order the Pivot variable R i . For the current Pivot variable R i , find the conditional ct -table where R i is unspecified, and the sub-sequent relations R j with j &gt; i are true. This ct -table can be computed from a ct -table for a shorter chain that has been constructed already. The conditional ct -table has been constructed already, where R i is true, and the sub-sequent relations are true (see loop invariant). Apply the Pivot function to construct the complete ct -table, for any Pivot variable R i , conditional on the subsequent relations being true.
The key point about the M  X  obius Join (MJ ) algorithm is that it avoids materializing the cross product of entity tuples. The algorithm accesses only existing tuples, never constructs nonexisting tuples. The number of ct -table oper-ation is therefore independent of the number of data records in the original database. We bound the total number of ct -algebra operations performed by the M  X  obius Join algorithm in terms of the size of its output: the number of sufficient statistics that involve negative relationships.

Proposition 2. The number of ct -table operations per-formed by the M  X  obius Join algorithm is bounded as where r is the number of sufficient statistics that involve negative relationships.

The proof is available at [8]. Since the time cost of any algorithm must be at least as great as the time for writing the output, which is as least as great as r , the M  X  obius Join algorithm adds at most a logarithmic factor to this lower bound. This means that if the number r of sufficient statis-tics is a feasible bound on computational time and space, then computing the sufficient statistics is feasible. In our benchmark datasets, the number of sufficient statistics was feasible, as we report below. In Section 8 below we discuss options in case the number of sufficient statistics grows too large.
 Input : A relational database D ; a set of variables 10: for all Rchains R = R 1 ,...,R ` do 12: for i = 1 to ` do 13: if i equals 1 then 15: else 16: 1Atts  X  i := 1Atts ( R 1 ,..., R i  X  1 , R i + 1 ,..., R ` ). 17: 2Atts  X  i := 2Atts ( R 1 ,..., R i  X  1 , R i + 1 ,..., R ` ). 19: end if 20: Current ct := Pivot ( Current ct , ct  X  ). 23: end for Table 3: Constructing the contingency table for each dataset. M = million. N.T. = non-termination. Compress Ratio = CP-#tuples/#Statistics. gency tables confirmed the correctness of our implementa-tion. Table 3 compares the time and space costs of the MJ vs. the CP approach. The cross product was materialized using an SQL query. The ratio of the cross product size to the number of statistics in the ct -table measures how much compression the ct -table provides compared to enumerating the cross product. It shows that cross product materializa-tion requires an infeasible amount of space resources. The ct -table provides a substantial compression of the statistical information in the database, by a factor of over 4,500 for the largest database IMDB.

Computation Time. The numbers shown are the complete computation time for all statistics. For faster processing, both methods used a B+tree index built on each column in the original dataset. The MJ method also utilized B+ in-dexes on the ct -tables. We include the cost of building these
In this section we compare the time and space costs of computing both positive and negative relationships, vs. pos-itive relationships only. We use the following terminology. Link Analysis On refers to using a contingency table with sufficient statistics for both positive and negative relation-ships. An example is table ct in Figure 5. Link Analysis Off refers to using a contingency table with sufficient statis-tics for positive relationships only. An example is table ct + T in Figure 5. Table 4 shows the number of sufficient statis-tics required for link analysis on vs. off. The difference between the link analysis on statistics and the link analysis off statistics is the number of Extra Statistics. The Extra Time column shows how much time the MJ algorithm re-quires to compute the Extra Statistics after the contingency tables for positive relationships are constructed using SQL joins. As Figure 7 illustrates, the Extra Time stands in a nearly linear relationship to the number of Extra Statistics, which confirms the analysis of Section 4.3. Figure 8 shows that most of the MJ run time is spent on the Pivot compo-nent (Algorithm 1) rather than the main loop (Algorithm 2). In terms of ct -table operations, most time is spent on sub-traction/union rather than cross product.

Figure 8: Breakdown of MJ Total Running Time
We evaluate using link analysis on three different types of cross-table statistical analysis: feature selection, association rule mining, and learning a Bayesian network.
For each database, we selected a target for classification, then used Weka X  X  CFS feature subset selection method (Ver-sion 3.6.7) to select features for classification [2], given a contingency table. The idea is that if the existence of rela-tionships is relevant to classification, then there should be Table 6: Number of top 20 Association Rules that utilize relationship variables.
 Table 7: Model Structure Learning Time in seconds.
Our most challenging application is constructing a Bayesian network (BN) for a relational database. For single-table data, Bayesian network learning has been considered as a benchmark application for precomputing sufficient statistics [5, 4]. A Bayesian network structure is a directly acyclic graph whose nodes are random variables. Given an assign-ment of values to its parameters, a Bayesian network rep-resents a joint distribution over both attributes and rela-tionships in a relational database. Several researchers have noted the usefulness of constructing a graphical statistical model for a relational database [13, 15]. For data ex-ploration, a Bayes net model provides a succinct graphical representation of complex statistical-relational correlations. The model also supports probabilistic reasoning for answer-ing  X  X hat-if X  queries about the probabilities of uncertain outcomes.

We used the previously existing learn-and-join method (LAJ), which is the state of the art for Bayes net learn-ing in relational databases [11]. The LAJ method takes as input a contingency table for the entire database, so we can apply it with both link analysis on and link analysis off to obtain two different BN structures for each database. Our experiment is the first evaluation of the LAJ method with link analysis on. We used the LAJ implementation provided by its creators. We score all learned graph structures us-ing the same full contingency table with link analysis on, so that the scores are comparable. The idea is that turn-ing link analysis on should lead to a different structure that represents correlations, involving relationship variables, that exist in the data.
Table 7 provides the model search time for structure learn-ing with link analysis on and off. Structure learning is fast, even for the largest contingency table IMDB (less than 10 minutes run-time). With link analysis on, structure learn-ing takes more time as it processes more information. In both modes, the run-time for building the contingency ta-bles (Table 3) dominates the structure learning cost. For the Mondial database, there is no case where all relation-ship variables are simultaneously true, so with link analysis off the contingency table is empty.
We report two model metrics, the log-likelihood score, and the model complexity as measured by the number of pa-ADtrees [5]. An ADtree provides a memory-efficient data structure for storing and retrieving sufficient statistics once they have been computed. In this paper, we focus on the problem of computing the sufficient statistics, especially for the case where the relevant rows have not been materialized. Thus ADtrees and contingency tables are complementary representations for different purposes: contingency tables support a computationally efficient block access to sufficient statistics, whereas ADtrees provide a memory efficient com-pression of the sufficient statistics. An interesting direction for future work is to build an ADtree for the contingency table once it has been computed.

Relational Sufficient Statistics. Schulte et al. review pre-vious methods for computing statistics with negative rela-tionships [12]. They show that the fast M  X  obius transform can be used in the case of multiple negative relationships. Their evaluation considered only Bayes net parameter learn-ing with only one relationship. We examined computing joint sufficient statistics over the entire database. Other novel aspects are the ct -table operations and using the rela-tionship chain lattice to facilitate dynamic programming.
Utilizing the information in a relational database for sta-tistical modelling and pattern mining requires fast access to multi-relational sufficient statistics, that combine infor-mation across database tables. We presented an efficient dynamic program that computes sufficient statistics for any combination of positive and negative relationships, starting with a set of statistics for positive relationships only. Our dynamic program performs a virtual join operation, that counts the number of statistics in a table join without actu-ally constructing the join. We showed that the run time of the algorithm is O ( r log r ), where r is the number of suffi-cient statistics to be computed. The computed statistics are stored in contingency tables. We introduced contin-gency table algebra, an extension of relational algebra, to el-egantly describe and efficiently implement the dynamic pro-gram. Empirical evaluation on seven benchmark databases demonstrated the scalability of our algorithm; we compute sufficient statistics with positive and negative relationships in databases with over 1 million data records. Our experi-ments illustrated how access to sufficient statistics for both positive and negative relationships enhances feature selec-tion, rule mining, and Bayesian network learning.

Limitations and Future Work. Our dynamic program scales well with the number of rows, but not with the num-ber of columns and relationships in the database. This lim-itation stems from the fact that the contingency table size grows exponentially with the number of random variables in the table. In this paper, we applied the algorithm to con-struct a large table for all variables in the database. We em-phasize that this is only one way to apply the algorithm. The M  X  obius Join algorithm efficiently finds cross-table statistics for any set of variables, not only for the complete set of all variables in the database. An alternative is to apply the vir-tual join only up to a prespecified relatively small relation-ship chain length. Another possibility is to use postcount-ing [4]: Rather than precompute a large contingency table prior to learning, compute many small contingency tables for small subsets of variables on demand during learning.
In sum, our M  X  obius Virtual Join algorithm efficiently com-putes query counts which may involve any number of pos-
