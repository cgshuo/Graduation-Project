 Nearest neighbor search is one of the most fundamental problem in many applications including pattern recognition, image retrieval, object recognition, clustering, etc. For each application, given an object, a set of pr oblem-dependent features is extracted and is used to represent that object. The retrieva l process consists in comparing the features vector from the query object and that of the database X  X  objects to find the nearest neigh-bors or simply near enough neighbors. With today X  X  explosion of digital content the size of the database can be very large. A Na X ve comparison cannot be used in practice.
To quickly find nearest neighbors from a very large dataset, there are generally two approaches namely the space partitioning technique and the hashing technique. The first approach aims at organizing the feature space or the dataset, using additional data struc-ture. The simplest organization consists in dividing data into groups or clusters. Then the retrieval process is done in two steps that are the search for the closest cluster and the comparison between the query and each object present in this cluster. Tree structure, e.g. [7,14,5], can be used to speed up the search for closest cluster. However, for dataset which does not have a well-defined cluster structure, one may fail to identify neighbors of query point that falls near the cluster X  X  boundary. To cope with this problem, it is necessary to consider not only the closest cluster but its neighbor clusters as well. As the number of dimension increases, this overhead cost of verifying neighbor clusters increases as well.

To speed-up this search, [2,13] propose to assign a priority to each cluster based on the distance to the query point. This allows accelerating the search but can return only an approximate solution. To further speed-up the search, multiple kd-trees are consid-ered in [19]. Indeed, the authors propose to c onstruct multiple kd-trees with different parameters such that the nodes inside dif ferent trees are independent from each other, thus reduce the redundancy in the verificatio n process. Other speed-up method consists in using a more advance tree structure like the cover tree [3] that allow fast rejection of clusters not containing the query point. The lower-bound tree [4] uses information of data in each clusters to estimate the lower bound of the distance between the query point and each point in this cluster in order to quickly discard the clusters candidate while traversing the tree. For ring-cover tree [11], the recursive walk down each node or cluster of the tree depends on the structure of the data in this node, weather it has a clear separable structure (ring node) or it need a subset of elements to cover the whole data (cover node).
 Another interesting indexing and search technique called the redundant bits vector or RBV [9] does not use a tree structure for organizing the dataset. Indeed, RBV partitions the feature space into bins , each one being identified by a feature and an interval of values. Given an object, a set of corresponding bins can be identified by analyzing its feature X  X  value. For each bin, RBV keeps an index , that is a set of objects whose corresponding feature X  X  value falls into the interval of this bin. Given a query object, an intersection between all indexes of the corresponding bins is returned as set of nearest neighbors. A binary string is used to encode index in each bin for fast comparison. Experimentally, we have found that RBV works well if query is indeed in the database or if a nearly duplicated elements exist in the database.

The second approach of fast nearest neighbor search is the hashing technique [12,16,21,20]. The idea is to find a mapping function that transforms each object into a binary code such the Hamming distance between the resulting codes reflects the dis-tance in the original feature space, e.g. Euclidean distance. Working with binary code has two obvious advantages. Firstly, it is an efficient method to store million or more data in memory. It is then more scalable for coping with large or huge dataset or even with web-scale dataset. Secondly, the calculation of the distance between two binary codes requires only bit-wise operations and the summation over integer values. These operations are much faster than the floati ng-point operations. As a consequence the Hamming distance is much cheaper than the other distance. However, finding good bi-nary code that reflects the distance in the original space is a difficult task.
Locality sensitive hashing or LSH [11,8] is one of the most important step in hashing-based technique for fast nearest neighbor search. Indeed, LSH idea is to find a function that yields, with high probability, the same va lue for neighbor objects a nd different val-ues for dissimilar objects. In the original paper [11], the authors show that a random bit hashing function can be used to preserve the Hamming distance. Later, it is proven [1], using concept of stable distribution, that one may construct a hashing function ran-domly while preserving the Euclidean distance. Hamming embedding or HE [12] also relies on a random projection to construct a bi nary code. HE also consider additional information from cluster structure of the dataset while comparing the binary codes.
In practice, the binary code obtained from the random construction, LSH or HE, is not efficient , that is large number of hash functions are needed to approximate the Eu-clidean distance or other distances in the o riginal space. A learning machine can be used to carefully train a compact an d efficient binary code as demonstrated in [16]. Followed the same idea, [20] has adopted the boosting similarity sensitive coding technique [18] and the stacked restricted Boltzmann machin es [15] to learn binary codes for a dataset of 12.9 million web images. Even for this very large-scale dataset, the retrieval process using binary code can be done in a fraction of a second [20]. In [21] another approach of find a good binary code has been proposed. Indeed, the authors formally formulate an optimization problem describing the problem of finding a good binary code. The resulting algorithm called spectral hashing is simple but produces an efficient code that outperforms prior techniques. This work analyzes the spectral hashing, its possible shortcomings and solutions. 2.1 Formulation and Algorithm Let x =[ x 1 ; ... ; x d ] T  X  R d be a feature vectors in d dimensions. Spectral hashing, shorten as SH hereafter, searches for a binary mapping function f minimizing the aver-age distance between the resulting codes with respect to the Euclidean distance in the original space. Formally, this is done by solving the following problem: where W ( x , x )=exp(  X  x  X  x 2 / 2 ) is the weight function, p ( x ) is the distribution of x . The second constraint f ( x ) p ( x ) d x =0 requires that each bit has equal chance of being 1 or 0, while the third constraint f ( x ) f ( x ) T p ( x ) d x = I requires that the bits are uncorrelated. By relaxing the first constraint f ( x )  X  X  X  1 , 1 } m , several analyt-ical solutions are possible. These solutions are eigenfunctions of the weighted Laplace-Beltrami operator defined on the manifold [21]. More explicitly, let L p be the weighted Laplacian that maps a function f to g = L p f with g ( x ) /f ( x )= D ( x ) f ( x ) p ( x )  X  lem are any functions f satisfying L p f =  X f for some real value  X  ,inotherword f is an eigenfunction of L p .

To obtain the solution of the above problem, the original spectral hashing makes two assumptions; 1) p ( x ) is a separable distribution a nd 2) each feature is uniformly distributed. The first assumption implies that we may construct an eigenfunction of d -dimensional data by a product of one-dimensional eigenfunctions corresponding to each feature. The second assumption allows us picking the following eigenfunctions as one-dimensional eigenfunctions: where x is a real number uniformly distributed in range [ a, b ] ,  X  k the corresponding eigenvalues with k a parameter of these eigenfunctions. The multi-dimensional eigen-function is then constructed implicitly from these one-dimensional eigenfunction.
The above discussion leads to a two steps algorithm; The first step is the principal component analysis (PCA) step and the second one is the eigenfunction selection step. In fact, an example of a separable distribution is a multidimensional Gaussian, once rotated so that the axes are aligned. That is the reason why PCA is used in the first step. Note that for non-vectorial data, kernel PCA [17] can also be used in this step instead. After this step, each object can be represented as a vector in the principal subspace.
The second step consists in evaluating the above eigenvalues (equation (3)) for all possible k =1 , ..., K and for all features i =1 , ..., l with l the dimension of the princi-pal subspace. This result in a list of lK eigenvalues which are sorted in increasing order. Then ignoring the smallest eigenfunction (which corresponds to eigenvalue 0), the next m smallest eigenfunctions are selected to encode object into m bits code. Finally the output from each selected eigenfunction is t hresholded to obtai n the binary value.
It should be noted that the range of data on each principal axis varies proportional to the variance of the data projected on this axis. It is known that the variance of the i th PCA feature is given by the eigenvalue of this projection axis. As a consequence, the second step of spectral hashing can then be done by sorting eigenfunctions in decreas-ing order of  X  i /k 2 with  X  1 , ...,  X  l the eigenvalues obtained from the PCA step, then selecting the top m eigenfunctions.
 Using the same relation, the eigenfunction ( i, k ) can be defined on R d as follows: with v i the i th eigenvector,  X  i its corresponding eigenvalue. The binary code is then obtained from SIGN (  X  i,k ( x )) . 2.2 Discussion The spectral hashing relies on the sign of an eigenfunction to encode the projection of data on each principal axis into a binary c ode. It should be noted that the considered eigenfunction is indeed a sinus function. This function partitions the whole range of data into intervals of equal length and assign the binary code 1 and 0 to these intervals alternately. Uniform distr ibution implies that the same amount of data falls into each interval. In the following, we shall refer to the amount of data in each interval as its size . Thus, uniform distribution assures that the whole range of data is partitioned into intervals not only of the same length but also of the same size. Two solutions are con-sidered in this work. The first one is based on the probability integral transform theorem [6]. The second one relies on manual partition of the data into equal size intervals, then assign the binary code 1 and 0 to these intervals alternately.

Apart from the problem with uniform distribution, the binary encoding with eigen-function cannot guarantee that the resulting c ode has equal chance of being 1 or 0. In fact, the parameter k of the eigenfunction partitions the data into k +1 intervals. The first in-terval is labeled with binary code 1, then the code 0 and 1 are assigned to next intervals alternately. As a consequence when k is even, there will be more intervals of 1 than that of 0. Thus there will be more 1 than 0 for this bit, even if the data is uniformly distributed.
The above problem can be overcome by first sort the data in increasing order. Then carefully select k thresholds values for partition the whole data into k +1 intervals such than the odd (respectively the even) intervals are of the same size and such that the total size of all odd intervals is equal to that of even intervals. While this procedure solves the problem due to the data distribution and equalize the amount of 1 and 0 for the resulting bit, it is inconsistent with the eigenfunction selection procedure. Indeed, the eigenfunction selection criterion is based on the ratio  X  i /k 2 which corresponds roughly to the squared of the length of the interval along the eigen axis v i partitioned into k +1 equal portions. Having this criterion in mind, the selection of eigenfunction should be done as function of the squared of the average length of the intervals. These different extensions of SH will be studied in next section. This section discusses two extensions of SH to handle non-uniform distribution. The first extension (Sect. 3.1) relies on a probabilistic model to transform feature into uni-form distribution. The second extension relies manual partition does not require any additional assumption on the data distribution. This technique called generalized SH , will be presented in Sect. 3.2. 3.1 SH with Probability Transform To address the issue of uniform distribution, we propose to further transform the PCA-based feature using the probability integr al transform theorem. Indeed, if a random variable Y has continuous accumulated d istribution function F , then the probability integral transform theorem states that the new random variable Z = F ( Y ) will fol-low uniform distribution in range [0 , 1] [6]. Using this fact, one may try to estimate a probabilistic model for each PCA-based feat ure, then use the corresponding cumula-tive distribution function (cdf.) to transform this feature into a new feature following uniform distribution.

For instance, let X  X  consider the Gaussian distribution. If y follows Gaussian distribu-tion with mean 0 and variance  X  2 , then its cdf. is given by: an eigenfunction ( i, k ) , if we assume that the projection of data along the i th principal axis follows Gaussian distribution with 0 means and  X  i variance, then the following eigenfunction can be used instead of  X  i,k (equation (4)): In the following, we shall refer to the SH using this eigenfunction as the SH with Gaus-sian transform .
 3.2 Generalized SH The figures 1 (a), (b), and (c) show the projection of MNIST data along the first three principal axes. It is clear from this figure that the data distribution is not Gaussian. A better solution would be the use of empirical cdf. similar to the histogram equalization procedure in the image processing [10]. The latter requires, however, a user-selected number of bins for the histogram calculation. A more general solution for partitioning the data into intervals of equal size can be done by first sorting the data in increasing order, then carefully chosen the thresholds based on this sorted list.

This is done by first applying the PCA as in normal SH. Then the projection of data on each principal axis i =1 , ..., l is computed and is sorted in increasing order. Let z z , ..., z i n by The average length of the resulting intervals is then computed as follows: The m eigenfunctions having largest average length of intervals len i,k are selected for encoding.

For an eigenfunction ( i, k ) with v i the corresponding eigenvector and t i,k 1 , ..., t i,k k the corresponding thresholds, the output binary code for a new vector x is computed by 1. Compute z the projection of x on the axis v i ,i.e. z = ! v i , x " 2. If z&lt;t i,k 1 return 1 3. If t i,k k  X  z return 1 if k is even and 0 if k is odd The final m bits code of the vector x is the concatenation of the binary code from all m selected eigenfunctions. In the following, this new encoding scheme will be called generalized SH or GSH . 3.3 Toward a More Efficient Code For GSH, for each eigenfunction ( i, k ) the equality between the number of 1 and 0 can be guarantee by carefully chosen the thresholds t i,k j ,j =1 , ..., k . Indeed, if the data is partitioned into equal size intervals and labeled with binary code 1 and 0 alternately, then when k is even there will be more interval labeled with 1 than that with 0. There-fore, there will be more 1 than 0 for the binary code resulting from this eigenfunction. One may adjust these k thresholds such that 1. the size of all odd intervals are equal to n o 2. the size of all even intervals are equal to n e 3. the total size of all odd intervals is equal to the total size of even intervals. These three requirements can be satisfied by letting Then the k thresholds are selected by 2. For j =1 , ..., k do with z i 1 , ..., z i n be this sorted list of data projected on the axis v i . In this section, we describe the results of several experiments on the nearest neighbor search using binary codes from SH and its extensions. 4.1 Datasets and Evaluation Measures The evaluation is performed on the MNIST dataset 1 . This is a standard dataset for hand-written digit recognition task. It is compos ed of 60,000 training examples. Each exam-ple, is a bitmap of 28x28 pixels. We simply converted this bitmap into a vector of 784 dimensions. This is a medium size dataset with high dimensional data. Two thousands examples were randomly chosen as test queries. To evaluate different methods, for each test query two sorted lists were computed. The first one was the index of data in the dataset sorted in increasing order of Hamming distance between the databases binary codes and the binary code of the test query. The second one was sorted in increasing order of the Euclidean distance to the test query X  X  features vector. The first 1% in this second so rted list were considered as the true nearest neighbors. By comparing these two lists, a precision/recall graph can be drawn. 4.2 Experimental Results In this experiment, four methods were compared namely: 1. SH 2. SH with Gaussian transform 3. GSH-1: GSH where all intervals are of the same size 4. GSH-2: GSH where each bit has equal c hance of being 1 and 0 (Sect. 3.3). The first method, the spectral hashing or SH, relies on the uniform data distribution assumption. The second method, SH with Gaussian transform, tries to cope with this problem by using Gaussian assumption. Even if this assumption is also made arbitrary, its parameters are obtained from the actual d ata. This could lead to an improvement over the normal SH. The next two methods, GSH-1 and GSH-2, try to generalize the idea of SH for any data distribution. GSH-1 relies on the equal-size partition while GSH-2 tries to equalize the number of time each bit is 1 and 0.

Figure 2 shows precision/recall graphs of three methods on MNIST dataset with 8 bits (a), 16 bits (b), 32 bits (c), 64 bits (d), 128 bits (e), and 256 bits (f). From this figure, one may see that the vanilla SH performs surprisingly well compared to other extension, even if this data is clearly not uniformly distributed (see Figure 1). Moreover, one may see that the two extension of SH clearly outperform the normal SH when small number of bits is used (8, 16, or 32 bits). When large number of bits is used (64, 128, and 256 bits), these extensions yield higher precision result while losing the recall.
In fact, different SH extensions tend to produce search result with higher precision but with less recall. This can be seen clearer on the figures 3 (a) and (b). These figures plot precision and recall as function of the th resholds on the Hamming distance to the query using binary code of 32 bits and 256 bits respectively. These results indicate that the precision of the res ulting binary code increases as we rectify the underlying assumption about the data distribution. Thes e results also indicate that if we accept all data having Hamming distance to the query object less than a redefined threshold as a neighbor of the query, then the result become more reliable as we move from SH to GSH. For some tasks where precision is more important that recall, e.g. k-NN classifier, GSH represents an interesting technique. 4.3 Computational Cost Note that using 32-bits code, one will need less than 240 kilobytes to index 60,000 training examples of MNIST dataset. Using this code, on an Intel Core2Duo 1.83Ghz machine the nearest neighbors search is done in roughly 2.3 milliseconds for all meth-ods. This is due to the fact that these methods produces the same type of binary code. The difference between these methods lies in the derivation of the eigenfunction. To compute GSH X  X  eigenfunction, the sorted lis t of data projection on each principal axis is required. As a consequence, it may not be suited for very large dataset. Indeed, we believe that in this case, empirical cdf should be prefered to speed up the calculation.
The retrieval speed can be further improved by pre-computing the Hamming dis-tance. In fact, each binary code is kept in the computer X  X  memory as a sequence of chars . For example, a 64-bits code can be represented by 8 chars. Each char has 256 possible values. It is then possible to pre-compute the Hamming distance between any two chars and store them in a look-up table. The Hamming distance between two codes is then the summation of the Hamming distance between the chars representing these codes. Using this pre-computed distance and look-up table, the search time for 32-bits code reduces to 1.8 milliseconds.

It is worthy note that the Hamming distance assumes that all bits are of the same importance. In practice, some bit may convey more information that the others; Thus we may assign different weight for each bit i n the code. The idea of the pre-computed distance and the look-up table allows computing this weighted Hamming distance with-out increasing the computational cost. In our preliminary study of this work, we have tried to retrieved nearest neighbors based on this idea with weights selected heuristi-cally. The results were interesting. We are currently working on an automatic selection of these weights. In this work, we have analyzed the spectral hashing technique that has been designed to efficiently encode object into a binary string for fast nearest neighbor search. Two extensions of the spectral hashing have been considered. The first one focuses on the uniform distribution assumption. We have shown that by assuming that the data on each principal axis is Gaussian distributed a nd by using the Gaussian cdf. to further transform this feature, a more efficient code could be obtained. A more general solu-tion called generalized SH is also proposed. These extensions yield search result with more precision than that obtained from the normal SH. Evaluation of these different techniques on larger dataset is currently under investigation.

