 The quantity of personal data gathered by service providers via our daily activities continues to grow at a rapid pace. The sharing, and the subsequent analysis of, such data can support a wide range of activities, but concerns around pri-vacy often prompt an organization to transform the data to meet certain protection models (e.g., k -anonymity or  X  -differential privacy). These models, however, are based on simplistic adversarial frameworks, which can lead to both under-and over-protection. For instance, such models often assume that an adversary attacks a protected record ex-actly once. We introduce a principled approach to explicitly model the attack process as a series of steps. Specifically, we engineer a factored Markov decision process (FMDP) to optimally plan an attack from the adversary X  X  perspec-tive and assess the privacy risk accordingly. The FMDP captures the uncertainty in the adversary X  X  belief (e.g., the number of identified individuals that match the de-identified data) and enables the analysis of various real world deter-rence mechanisms beyond a traditional protection model, such as a penalty for committing an attack. We present an algorithm to solve the FMDP and illustrate its efficiency by simulating an attack on publicly accessible U.S. census records against a real identified resource of over 500,000 in-dividuals in a voter registry. Our results demonstrate that while traditional privacy models commonly expect an adver-sary to attack exactly once per record, an optimal attack in our model may involve exploiting none, one, or more indi-viduals in the pool of candidates, depending on context. K.4.1 [ Computing Milieux ]: Computers and Society X  Privacy ; K.6.0 [ Computing Milieux ]: General X  Economics Algorithms; Security; Theory c  X  Privacy; Re-identification; Planning
The quantity, quality, and diversity of personal data we shed through our daily activities continues to grow at a rapid pace. This data is collected by a wide range of organizations to assist in the optimization and refinement of the services they provide [1, 13]. At the same time, it is increasingly recognized that personal data has substantial worth beyond its initial use, such that it can be repurposed for a variety of endeavors, ranging from transparency in operations to basic research [23]. Despite the recognized value of personal data, organizations worry about how best to protect the rights of their constituents while maximizing the benefits [29].
One such right that our society tends to focus on is per-sonal privacy. While privacy is an overloaded term that takes on many different forms [25], one quantitative, broadly concerned definition centers on the notion of anonymity. There are various regulations that encourage organizations to suppress identifying information from personal data prior to its dissemination. Several examples of regulations with explicit identity protections include the Privacy Rule of the Health Insurance Portability and Accountability Act of 1996 (HIPAA) in the United States and the Data Protection Di-rective in the European Union. However, there is growing evidence that the resulting data is susceptible to intrusions [19]. One major type of intrusion is the re-identification at-tack, which happens when an adversary combines de-identified records with external resources to determine the identity of the corresponding individuals (e.g., [12, 18, 26, 27]).
While such attacks are possible, it is unclear if they are probable. This is important because laws and regulations do not require perfect protection, but rather that data be shared in a manner that makes it difficult to ascertain an individual X  X  identity. Organizations are thus afforded an opportunity to achieve data protection using risk manage-ment techniques, but they are hampered from accomplishing this goal for several reasons. First, there is little historical data on re-identification attacks [6]. This may be due to the rareness of such events or that they transpired behind closed doors. Second, prior investigations assume privacy risks can only be managed by perturbing data according to a formal model (e.g., k -anonymity [24] or -differential privacy [4]). Yet, there are many other elements that contribute to pri-vacy risks, such as deterrence mechanisms (e.g., data user agreements, the time and effort to gather the external in-formation necessary to compromise the data, or penalties Figure 1: An example of a re-identification scenario. for misusing data), that influence an adversary. Third, the typical adversarial model invoked in this setting assumes an attack is perpetrated in a pre-defined manner. For instance, under a k -anonymity framework it is assumed the adversary exploits an individual at random, with an expected success rate of 1 /k . Yet, an adversary could attempt to exploit all of the individuals in this group in the hopes that one will be the correct corresponding person. Under the  X  -differential privacy framework, the publisher limits the number of times an adversary can query a statistical database to satisfy the given privacy budget.

To overcome these limitations, we introduce a principled approach to assess re-identification risk using a process frame-work that incorporates data-and penalty-based disincen-tives. This approach is based on a stochastic model, yielding a quantitative evaluation of the re-identification risk.
For context, Figure 1 depicts an example of the type of attack our re-identification risk framework is designed to handle. Here, Bob corresponds to the de-identified data. Now, there is a probability he is in the external dataset that contains identifying information and a set of attributes in common with the de-identified data. In the event the ad-versary accesses the external dataset, he will find that the record with a diagnosis of tuberculosis might correspond to Alice, Bob or neither (because the corresponding individual may not be in the dataset). The adversary will be suc-cessful if he exploits Bob; however, it is possible the ad-versary is not sufficiently motivated to start the attack and the adversary may choose to terminate his attack at any point in the process (because of insufficient expected pay-offs). Our framework explicitly models the decisions the adversary must make and computes the probability that the adversary will reach a successful re-identification.
The specific contributions of this paper include: 1. Re-identification Risk Model. This research pro-poses a novel re-identification risk framework that formalizes incentive and deterrence mechanisms (e.g., potential penal-ties and information uncertainty) that are present in the real world environments where a de-identified dataset is made available. This framework explicitly models the adversary as an optimal planning agent using a factored Markov de-cision process (FMDP). Given that the state space of the FMDP grows rapidly, we introduce a two-level linear pro-gramming algorithm to efficiently solve it. 2. Case Study. We evaluate risk in the scenario where an adversary attempts to leverage a public voter registry in a specific U.S. State to attack de-identified Census records. Under the traditional adversarial model, the adversary is assumed to randomly choose an individual from the group of indistinguishable individuals in the external dataset that matches the targeted record under attack. However, our findings illustrate that the adversary X  X  behavior is highly dependent on the deterrence mechanism set in place. In particular, if the adversary X  X  expectation of the probabil-ity of being detected for each exploitation of an individual remains constant across attempts and he will always be pe-nalized if detected, he will choose to either attack 1) all the individuals in the correspond group or 2) none of the indi-viduals. This result illustrates how traditional beliefs about risk can either underprotect or overprotect the data. 3. Sensitivity Analysis. We conduct a detailed sen-sitivity analysis on the parameters of the model to illus-trate how the environment and policy decisions can influ-ence the adversary X  X  behavior. Specifically, we demonstrate how changes in the costs in each stage of the attack, penalty-based deterrence mechanism, and the probability of detec-tion influences when the adversary will cease their attack. Our result demonstrates that the adversary X  X  threshold is highly dependent on the deterrence mechanisms that are in place. This result is notable because it suggests that ad-versaries may be sufficiently deterred with a small amount of data manipulation, provided appropriate detection and penalization policies are instantiated.

We stress that our framework focuses on the application scenarios where record-level data needs to be shared. For ex-ample, in health care research setting, data sets containing patient information may need to be shared. In such scenar-ios, generalizing the data may be acceptable (e.g., instead of reporting exact birthday of individuals, we can only disclose the birth year) but adding noise may not be acceptable due to the semantic errors that may be introduced. Therefore, our proposed approach can be seen as complementary to re-cent developments in -differential privacy where synthetic data sets could be generated for preserving privacy and it is applicable for scenarios where differential privacy based approaches are not appropriate.
 The remainder of this paper is organized as follows. In Section 2, we review related work on adversarial modeling and re-identification risk assessment approaches. In Sec-tion 3, we introduce the re-identification risk quantification framework. In Section 4 and 5, we present the FMDP and al-gorithms to solve it and compute the re-identification prob-ability from the Markov process efficiently. In Section 6 and 7, we present an empirical analysis. In Section 8, we discuss the limitations of this work and provide future directions.
To provide context for our study, we discuss research in privacy preserving data publishing and adversarial modeling areas with a focus on where our work diverges.
There are many different views on what constitutes a privacy violation when considering data publishing. These views argue that privacy can be compromised when a record is linked to the individual from whom it was derived (often referred to as identity disclosure ) [10], the inference of a sensitive value associated with the corresponding individual (often referred to as attribute disclosure ) [16], the ability to detect if someone is a member of a dataset (often referred to as the presence/absence problem) [9, 20], or the degree to which viewing an individual X  X  contribution to a dataset permits an adversary to gain knowledge about them (the basis of models like  X  -differential privacy) [2, 5, 8].
In this research, we focus on the identity disclosure prob-lem because this is the primary focus of current regulation. We note, however, that our framework is applicable to any of the above scenarios because we are not introducing a new method for manipulating data. Rather, we are showing how to reason about pressures (e.g., penalties for misuse) that are beyond the scope of what such data manipulation tech-niques offer.
It has been suggested that assessing disclosure risk re-quires a holistic modeling of different types of adversaries [7]. Such models should account for the motivation, means, opportunity cost, consequence of attempt, and likelihood of success. In this vein, a recent study [17] presented the con-cept of a data environment that is composed of data, agents and infrastructure. However, such investigations have not provided a formal approach to risk quantification that ac-counts for the elements in the data environment. Rather, existing disclosure risk measures mainly focus on the unique-ness of records in the dataset and in the population. For instance, three popular disclosure risk metrics (prosecutor, journalist and marketer) [14] assume that the adversary is al-ways motivated to attack and the extra information required for re-identification is always available. As a consequence, the risk level is only dependent on the data itself.
Our disclosure risk measure explicitly formalizes the three elements of the data environment around the adversary X  X  de-cision making. We note that there have been several other investigations in applying game theoretic frameworks to an-alyze the adversary X  X  best course of action and the corre-sponding disclosure risk [28, 31]. For instance, the adversary in Wan et al. [31] is formalized as an opponent of the data publisher in a Stackelberg game. To maximize payout, the adversary decides if they should attack by comparing the po-tential gain against the cost of committing an attack. Yet, this model oversimplifies the adversary X  X  decision process of gathering, linking, and exploiting data. Moreover, in their formalization, there were no explicitly modeled penalties for detecting the misuse of the data.

To mitigate the disclosure risk, the publisher can adopt various protection methods (e.g., randomization and gener-alization) according to given formal protection models. To apply these formal protection models in practice, a protec-tion threshold (e.g., the k value for k anonymity [24]) needs to be selected and supported by risk and utility analysis. A method based on Pareto-optimality has been proposed to find the solutions with an optimal utility at different lev-els of k [3]. However, the risk metric used in such methods to determine the threshold is based solely on the protected data. By contrast, our framework can assist in determining what the threshold should be based on both the data set and other elements in the data environment. To the best of our knowledge, MDPs have not been used to model adversaries in the privacy preserving data publication setting. Yet it has been proven to be a useful tool in model-ing adversary X  X  optimal planning in security problems. This is because the MDP representation captures an adversary X  X  uncertainty on the outcome of a security related action [15]. Similarly, the adversary focused on privacy also exhibits un-certainty. For example, when the adversary chooses to ex-ploit an individual, he is uncertain about whether or not he will be detected and pulished. Also, before the adver-sary chooses to access the external dataset, he may not be certain about number of identified individuals to which the de-identified record may be related. An important difference between our adversarial model and the one in Letchford and Vorobeychik [15] is that in the security scenario, the adver-sary terminates once he is caught, whereas in our model, the adversary may only pay a fine and continue to attack.
Our framework quantifies the re-identification risk of pub-lishing each record in a de-identified dataset. We assume the dataset is composed of person-level records in a relational form. We define re-identification risk as the composite of the probability that an adversary re-identifies a record and the harm it causes: where P reid is the re-identification probability and L reid the associated publisher loss. We assume L reid is a prede-fined input, and focus on P reid .

The re-identification probability is derived from the ad-versary X  X  sequential decision process, outlined in Figure 2. The adversary begins with a de-identified record r . The ad-versary X  X  first decision is to access an external table D not. His second decision is whether to conduct a linkage at-tack, which yields an equivalence group of records G e . This corresponds to the set of individuals with the same value as the target X  X  published quasi-identifier (QI). At this point, each individual  X   X  G e has a probability that they actually correspond to the targeted record r . This translates into a probability that an attack (e.g., confirmation of the pa-tient X  X  identity) on  X  will be successful. If the attack fails, the adversary can choose to exploit another individual from G . This process can repeat until the adversary decides to stop or he exhausts all of the records in G e .
 There are several notable aspects of this attack process. First, it should be recognized that this is a stochastic pro-cess. For example, the adversary may not know if the in-dividual to whom the target record corresponds is in D Therefore, the outcome of accessing the external dataset is Figure 3: A general architecture of the re-identification risk quantification framework. uncertain. Furthermore, the result of exploiting an indi-vidual is stochastic, with outcomes ranging from success to failure to being detected and punished. A second notable aspect of the attack process is that there is a cost and a reward associated with each action, for instance, the reward for a success, the cost of accessing D e and the penalty if an attack is detected all determine the adversary X  X  utility.
More precisely, we model the adversary as a planner us-ing a factored Markov decision process (FMDP) [11]. In a FMDP, a state of the world is characterized by a collection of random variables (or factors). The adversary is modeled as a rational agent computing an optimal policy; i.e., an optimal action to choose in each state of the FMDP. Given such a policy, we can compute risk according to Equation 1.
In Figure 3, we show the general architecture of the re-identification risk quantification framework. The framework is composed of three modules (the black rectangles in Figure 3): 1) the FMDP formalization of the adversary X  X  decision process, 2) the FMDP solver, and 3) the re-identification risk computation module. The FMDP formalization mod-ule takes four inputs: i) the attack decision process, ii) the de-identified data, iii) the external dataset profile, and iv) the adversary X  X  profile. The factored MDP model is then solved by the FMDP solver module to determine the adver-sary X  X  optimal policy. Finally, the risk computation module computes the quantified risk value given optimal attack pol-icy and associated probability of successful re-identification attack. In the following sections, we dive into the details of each of the three modules.
The FMDP model is a 4-tuple ( X,A,R,P ), where X = { X 0 ,...,X m } is a finite set of random variables, each with a finite domain. In this model, A is a finite set of actions; R is the reward function R ( x,a ), representing the reward for each action a taken in state X = x ; and P is a Markovian transition function P ( X 0 i | X parent i ,a ), which represents the probability distribution of the state variable X 0 i in the next state given the value of a subset of state variables X parent and action a ( X parent i is the set of variables that X 0 pendent on given the action is a ). We denote the value of a state variable X i in state x as x [ X i ]. We assume that the FMDP has an infinite horizon, and time is exponentially discounted with a discount factor  X  .
As summarized in Table 1, the FMDP model is based on eight state variables. Here, we take a moment to provide intuition into each of these variables. First, X t is a binary variable that represents the termination of an attack. When X t = T (true), the corresponding state is absorbing, ef-Table 1: The state variables of the FMDP model.
 fectively ending the decision process. Next, we assume the existence of an attack detection mechanism, and the state of detection is indicated by a binary variable X d . The following variable, X s , indicates whether the exploit is successful (in which case the adversary obtains a positive reward). The next two variables are associated with data manipulation. X a is a binary indicator of whether the external dataset D has been accessed, while X l is a binary indicator of whether it has been linked to the published target record r . X p tains the number of times the exploitation has been detected and penalized. The final two variables, X g and X r track of the size of the equivalence group and the remaining unexploited individuals in the group. Thus, as the adversary attempts (unsuccessful) attacks on matched records, X r de-creases while X g remains constant. This is because the orig-inal group size associated with linking is fixed. To keep our presentation compact, we represent each state x as a vector [ x ,...,x m ] in the FMDP model, where x i denotes the value of the i th variable in the list [ X t ,X d ,X p ,X s ,X a
There are four classes of actions in our system, which are summarized in Table 2. The adversary has the option of aborting the attack at any time by choosing the terminate action. The other three actions represent the adversary X  X  op-eration in three different phases of the attack. The access action represents the accessing of the external dataset D The link action represents the linking of the de-identified record r to the external dataset D e . The exploit action rep-resents a potentially harmful exploitation of an individual that is deemed to be related to the record r under attack. The particular type of exploitation may differ under vari-ous circumstances. For example, if the adversary X  X  goal is to demonstrate the vulnerability of the system, the exploit may be to prove they can contact the individual and con-firm the record is really associated with them [30]. Or, the adversary X  X  goal may be to conduct direct marketing to the individual based on the sensitive information in the record (e.g., for a particular pharmaceutical). Regardless, an ex-ploit is assumed to be successful if it is conducted against the individual to whom the record corresponds. Figure 4: The dynamic Bayesian network (DBN) for each action of our FMDP model.
Reward functions are determined by several factors: the cost of taking an action, the loss to the adversary from detec-tion (both negative rewards), and the gain from a successful attack. We formally define the reward function as:
R ( x,a ) = R g ( x [ X s ] ,x [ X t ]) + R p ( x [ X d ] ,x [ X where C a is the cost of action a (denoted by C d , C c , and C for access , link , and exploit actions, respectively). C for the terminate action. R g ( x [ X s ] ,x [ X t ]) represents the gain from a succussful exploitation. R g ( x [ X s ] ,x [ X if X s = T and X t = F and 0 otherwise.

We assume there is a maximum number of times, n f , that the adversary will be subject to a penalty (e.g., a fine for law or contract violation) if he is detected. Note that this permits an analysis on the special case of n f = 1, where the adversary is only penalized once. This is notable be-cause it represents the real scenario where a data user is penalized for violating a contract, but is not prevented from continuing to exploit the data they have already received. We denote the cost related to the fine as R p ( x [ X d ] ,x [ X R ( x [ X d ] ,x [ X p ]) =  X  C p , if X d = T  X  X p &lt; n wise.
We use a dynamic Bayesian network (DBN)  X  a =  X  G a ,P a  X  for each action a (except action terminate ), as shown in Fig-ure 4, to represent the transition function P ( X i | X parent We denote the current state and the next state as x and x respectively. If the action is to terminate , x 0 [ X t ] = T .
If the action is to access, as the DBN shows in Figure 4(a), there are 3 state variables that may change in the following step: X a , X g and X r . We highlight that if the external dataset D e has not yet been accessed (i.e., x [ X a the adversary X  X  belief of the equivalence group size in the next state x 0 [ X g ] is a probability distribution over a set of values, represented as P ( G r,D e ). Our experiments simulate P ( G r,D e ) under different levels of certainty and its influence on the adversary X  X  behavior and re-identification risk.
In Figure 4(b), the link action sets x 0 [ X l ] = T when x [ X a ] = T (i.e., external dataset is available for linkage).
The prerequisite condition for the exploit action is x [ X T , x [ X s ] = F , and x [ X r ] &gt; 0. In other words, we can only exploit a record if 1) the equivalence group is non-empty, 2) the dataset has been linked to the record, and 3) the record has not already been re-identified. In this case, the number of remaining candidates in the equivalence group is decremented ( x 0 [ X r ] = x [ X r ]  X  1).

Moreover, the probability that the exploited individual is associated with the record is the probability of select-ing an individual at random from the set of individuals in the population (with the same quasi-identifier) who have not been exploited. The number of individuals with the same quasi-identifier in the population who have not been exploited is the sum of the number of individuals outside dataset d e . Thus, the success probability of an exploitation can be formally represented as: where prior r,D e is the probability that the individual corre-sponding to the data is in the external dataset D e .
P ( x 0 [ X d ] = T ) (i.e., the probability of being caught) de-noted as P det can be modeled in a number of ways. Since the probability an exploit is detected is very likely to in-crease with repeated attempts due to various factors (e.g., increased vigilance), we model the detection probability us-ing a sigmoid function: where x [ X g ]  X  x [ X r ] corresponds to the number of exploit attempts the adversary has committed against records in the equivalence group. Note that this formulation allows us to model the special case, where the probability of detection does not increase over time by setting h 1 = 0.
 Finally, regardless of the action, the transition of variable X t is determined as follows (see Figure 4(d)): x 0 [ X t ] = T if x [ X t ] = T  X  x [ X s ] = T  X  ( x [ X l ] = T  X  x [ X r
Solving an infinite-horizon discounted MDP amounts to computing an optimal policy,  X  ( x ), which prescribes an op-timal action to take in each state [22]. Equivalently, it suf-fices to compute a value function, V ( x ), which is the optimal discounted sum of rewards of an optimal policy.

A number of methods exist for solving an MDP. Linear programming (LP) is one such method, which computes the value function, V ( x ), for every state x . An important limi-tation of the standard methods, including LP, is scalability. In particular, if we do not take advantage of problem struc-ture, the runtime is polynomial in the number of states, which itself grows exponentially in the number of state vari-ables. Approaches exist that leverage the structure of the factored MDP, but they are approximate, and require the pre-specification of a fixed set of basis functions over the state space. Next, we present a special-purpose method, which we call Two-Level LP, that takes advantage of our problem structure (including the factored state) and reports an exact answer.
We designed the Two-level LP algorithm under the princi-ple of removing all the  X  X ell-known X  parts from the FMDP structure to save space and runtime. The algorithm con-structs a two-level structure from the state space. The states in the FMDP model form a sink cluster sub-structure, which satisfies the following properties: a) there is no outbound and b) there is only one inbound state (i.e., x start has only one inbound edge). Based on the property of the FMDP, each sink cluster can be solved independently. The bottom-level of the Two-Level LP algorithm solves a LP and stores the value of the state x start for each sink cluster. The top-level algorithm then constructs and solves a LP of the entire state space by replacing each sink state with its correspond-ing x start and assigns the pre-computed V ( x start ) to it.
Specifically, each sink cluster contains the descendant states of a state x start in which the adversary has taken the ac-tion of access and link, but has not yet started exploitation, i.e., x start = [ F,F, 0 ,F,T,T,s i ,s i ], s i  X  (0 , max( G Given two different group sizes s 1 and s 2 , the two sink clusters with x start = [ F,F, 0 ,F,T,T,s 1 ,s 1 ] and x [ F,F, 0 ,F,T,T,s 2 ,s 2 ] do not overlap because the value X remains constant when X a = T and X l = T . The resulting values of all the x start states are used in the top-level LP to solve the values for the remaining states, such as the state in which the adversary is attempting to access the external dataset (i.e., x = [ F,F, 0 ,F,F,F, 0 , 0)]).
 We make two performance improvements for Two-Level LP. First, we introduce a pruning strategy which leverages the fact that the value of the starting states for each cluster (i.e., V ( x start = [ F,F, 0 ,F,T,T,s,s ])) decreases as the size of the equivalence group X g = s increases. We omit the proof of this property due to brevity.

Thus, we sort the sink cluster by the value of x start [ X in ascending order. Specifically, if V ( x start ) = 0 given x start [ X g ] = s , then all of the sink clusters with x start s will be pruned. Second, we use a result caching strategy. In doing so, the result from the bottom-level LP is cached and reused with multiple records. This happens when there is an overlap in the adversary X  X  belief of the probability dis-tribution interval of the equivalence group size X g .
The re-identification probability P reid is the sum of the probability of reaching each of the states with x [ X and x [ X t ] = F in 1 to t max time steps. Formally, P reid computed as: In equation 5, x 0 = [ F,F, 0 ,F,F,F, 0 , 0] represents the state in which the adversary has not accessed the external dataset yet, x suc is the set of states with x [ X s ] = T and x [ X x is an arbitrary state. M is the state transition N  X  N matrix of a Markov chain, where N is the number of states.
The state transition matrix M is obtained by replacing the action a in the transition dynamics function of the FMDP is one exception. Given the current state is x 0 , in the FMDP model, x 0 [ X g ] is a probability distribution over a range of values due to the uncertainty of the adversary X  X  belief, while, in the risk computation Markov chain, P ( x 0 [ X g ] = g 1, g r,D e is the actual group size in D e . This is because the Markov chain already embed X  X  the adversary X  X  optimal policy, and consequently the adversary X  X  belief in the group size no longer matters. Instead, what matters is the actual group size. We assume that g r,D e is an input to the risk framework.

The value t max is the maximum number of time steps it takes for all the states to transit into a state where X t Figure 5: Runtime ( log 10 ) of the FMDP solving al-gorithms for a dataset of 5000 de-identified records. T (i.e., an absorbing sink state). Formally:  X  t max  X  x i ,x j  X  x i ,i  X  [0 ,N ] , if x j [ X t ] 6 = 0 ,M t max [ x
Our experiments make use of three resources. First, we use the freely available North Carolina voter registration (NCVR) list [21] as the identified external dataset. This consists of 6,018,999 records without missing values over 18 fields. These include explicit identifiers (e.g., personal name and phone number), as well as quasi-identifiers (e.g., age, gender, race, and ethnic group). For the purposes of this study, we restricted the dataset to a set of four quasi-identifying attributes, { Age , Race , Gender , 5-Digit ZIP Code } . Second, we use the Adult dataset from the UCI Machine Learning Repository, as the de-identified dataset. This con-sists of 32,561 records with 14 fields each, based on a sample of the U.S. Census, without missing values. This dataset contains Age , Race , and Gender , but not 5-Digit ZIP Code . As such, for each record in the Adult dataset, we synthesize and append a 5-digit NC ZIP code based on the population distribution in the US Census Bureau X  X  2010 Census Tables PCT12A-G. We also replaced a topcoded age value [90+] by a random value in the range of [90 , 120].

Third, we assume that both the de-identified and identi-fied datasets are sampled from the entire population of NC. In this case, it should be noted that the total size of the NC population, according to the census is 9,553,967.
In the experiments, the probability distribution of the value X g after the adversary takes the action to access the dataset P ( G r,D e ) is derived from the adversary X  X  knowledge about the external dataset D e or the population statistics. Here, we consider two scenarios. In the first scenario the adversary knows the target X  X  equivalence group size when starting the attack. Specifically, P ( G r,D e = g r,D e refer to this scenario as the known group model.

However, the adversary may not have such knowledge be-fore accessing D e . In this case, we assume the adversary knows only the total size of the external dataset, n , and the probability density of the target X  X  record, i.e., the joint prob-ability of the target X  X  quasi-identifying values P ( r [ QI ]), in the population. Assuming that the external dataset is sam-pled uniformly at random from the population, P ( G can be represented as a binomial distribution defined in Equation 6: We refer to this mechanism as the unknown group model. Figure 6: The equivalence group size for the target record in the NCVR dataset and the re-identification risk under the known group scenario .
We evaluated the runtime of the framework with 5000 ran-domly selected Adult records. In this analysis, we consider the unknown group scenario with a P ( G r,D e ) computed as Equation 6 in which the size of the external dataset is set to 3 different values: 5K, 50K, and 500K. We present the result of the unknown group scenario because the known group scenario yield FMDP with constant size state space, while unknown group scenario leads to increasing state space when the external dataset size n increases simply because of the interval of P ( G r,D e ) increase with n . The detection and penalty mechanism is set to h 0 =  X  4 . 59, h 1 =  X  ,n f = 1 (i.e., penalize only once and the probability of detection is 0.01 based on equation 4). The other parameters of the model were set to prior r,D e = 0 . 63, C d = 100, C e = 10, G = 8000 and C p = 10000.

The algorithms were implemented in Python and all ex-periments were run on an Ubuntu server with 24 Intel(R) Xeon(R) CPUs at 2.4 GHz and 64 GB of RAM. The LP solver was the IBM ILOG CPLEX optimizer.
 Figure 5 reports the runtime for the LP and Two-level LP algorithms. It can be seen that, as expected, the Two-level LP is always faster than the standard LP algorithm. The difference in speed is accentuated as the size of the external dataset grows. By the time there are 500K records in the external dataset, the runtime of the Two-level LP is approximately 21x faster (616 seconds vs. 13,444 seconds).
To perform a case study, we assume the Adult and NCVR records are random samples of the NC population. Thus, the prior probability that the individual corresponding to an Adult record is in the NCVR is the sample ratio, or prior r,D e = 6 , 018 , 999 / 9 , 553 , 967 = 0 . 63. The NCVR data is free; however, considering the effort to obtain it, we set the cost of accessing the external dataset C d to $100. The cost to exploit, gain and penalty values were set to C e = $10, G = $8000 and C p = $10000 for each record, re-spectively. We acknowledge these values may vary in prac-tice. The goal is to simulate a case in which the adversary will attack at least a subset of the records. This allows us to examine how different deterrence mechanisms and un-Figure 7: The equivalence group size of of the target record in the NCVR dataset and the adversary X  X  expected payoff under the known group scenario . Figure 8: The size of the equivalence group of the target record in the NCVR dataset and the actual number of individuals the adversary exploits before terminating under the known group scenario. certainty about the equivalence in the external datasets af-fects the adversary X  X  behavior and the re-identification risk. Therefore, these parameters are selected from a range in which the adversary will attack some of the records.
We compare the known group model to the risk model in [31]. This is the only available model for re-identification based on an adversary X  X  optimal decision. In the baseline, the adversary makes a single decision on when to attack based on the total payoff: If Payoff baseline &gt; 0, the adversary exploits a random in-dividual and the risk of re-identification is prior r,D e Otherwise, the risk is 0. The FMDP is configured under three detection and penalty settings: a) a constant detec-tion probability with repeated penalties (i.e., h 1 = 0 and n f =  X  ), b) a one-time penalty (i.e., h 1 =  X  and n f = 1) and c) an increasing rate of detection with repeated penal-ties (i.e., h 1 = 0 . 18 and n f =  X  ). In each setting, we set h 0 =  X  4 . 59. This yields a 0 . 01 detection rate for the first exploit, an increase to 0 . 012 for the next exploit, and so on.
The results are illustrated in Figure 6. There are three notable findings to highlight.
The  X  indicates that h 1 can be anything because only a single penalty is assigned. Finding 1: The baseline risk never exceeds the FMDP models. This is because the baseline assumes that the adversary can only select one random individual, which is suboptimal. Thus, as can be seen in Figure 7, the baseline adversary X  X  expected value drops at a faster rate than the adversary who acts according to the FMDP. Moreover, the adversary X  X  success rate is also lower for the baseline. This is because the adversary only exploits one random individual from the equivalence group. This indicates that the baseline model often underestimates the re-identification risk.
Finding 2: When the detection probability is con-stant (i.e., h 1 = 0 ) and there is no upper bound on the number of times a penalty is levied on the adver-sary (i.e., n f =  X  ), the adversary either exploits 1) all records in the equivalence group or 2) no records.
Finding 3: When the probability of detection grows with repeated attempts (i.e., h 1 &gt; 0 ) or there is an upper bound on the number of times a penalty is levied on the adversary (i.e., n f is a finite value), the adversary exploits a subset of the equivalence group. In the scenario represented by Finding 2, the ad-versary chooses not to issue an attack when the equivalence group size is  X  a threshold k , but the adversary exploits all the individuals in the equivalence group otherwise. Thus, the re-identification risk is either equal to the prior prob-ability prior r,D e or 0. This is because when the optimal action is to attack one individual in the NCVR equivalence group the subsequent optimal action is always to continue to exploit each of the remaining individuals provided that each exploitation has the same probability of being detected and the adversary will always be fined if detected. 2
In the scenario of Finding 3, the adversary may terminate the attack before exhausting the candidates in the equiva-lence group. Thus, the risk can be any value between 0 and the prior probability prior r,D e . This is due to two possible reasons. First, if h 1 &gt; 0, both the likelihood of detection and a successful re-identification are increasing when more individuals are exploited. Thus, the adversary stops when the increment in the expected penalty exceeds the increment in the expected payout, which can happen before the adver-sary exhausts all the candidates. Second, if n f is finite, and the adversary was not detected in the previous exploitations, the expected payout can decrease when the number of the remaining cadidates reduces.

Similar to Finding 2, if the group size is &lt; k , the adversary exploits all the individuals in the equivalence group. By contrast, if the group size is  X  k 0 , the adversary stops issuing an attack. For the group size in the range of ( k,k adversary X  X  optimal action is to stop before reaching the last candidate in the group. The actual number of candidates the adversary will exploit before termination is shown in Figure 8. In this case, k = 14 and k 0 = 29.

These two findings are contradictory to what is expected by the baseline model. In particular, the records with equiv-alence group size &lt; k all have the same level of risk according to the FMDP model, while the records with smaller equiv-alence groups have more risk than those with larger equiva-lence groups according to the baseline model. The indication of this finding from the data protection perspective is that applying mechanisms, such as generalization, to increase the equivalence group size can only effectively reduce risk if the
We omit a proof of this claim due to brevity, but will make it available in a longer technical report upon acceptance. Figure 9: The equivalence group size, population probability density and the re-identification risk of the record with inconsistent risk values in the known and unknown group scenarios ( n f = 1 ). equivalence group size  X  k . In other words, increasing the equivalence group size to any value &lt; k will only harm the utility of the data without reducing the risk.
The FMDP enables us to evaluate risk when the adversary is uncertain in the equivalence group size; i.e., the unknown group scenario. We assume that the adversary X  X  belief of the group size is as in equation 6 with n = 6018999, with p r [ QI ] equal to the probability density of the corresponding target record in the NC census population. The other parameters are the same as defined in the known group model. Our result illustrates the following findings.

Finding 4: The unknown group scenario can yield lower risk than the known group scenario.

Finding 5: The unknown group scenario can yield higher risk than the known group scenario. These findings illustrate that uncertainty in the group size can change the action of the adversary. To make this obser-vation more concrete, Figure 9 depicts the risk for the 1920 records that have exhibited different risk scores. 1118 of these records (or 58%) have a risk of 0.63 under the known group scenario and 0 under the unknown group scenario. The remaining 802 (or 42%) records have the exact opposite result. The former is due to the fact that the adversary un-derestimates the payoff by using the probability distribution of the equivalence group size. As a result, the adversary does not access D e , while the actual group size is &lt; the threshold k = 48 and in the known group scenario the adversary will access D e and attack. The latter is, on the other hand, due to adversary X  X  overestimation of the expected payoff based on their inaccurate belief about the equivalence group size. These cases are counterintuitive because one may argue that even if the adversary decides to access D e , he or she will not exploit and there is no risk because the actual equivalence group size is  X  k = 48. However, this is not always true be-cause after the adversary obtains D e , the cost C d (i.e., the cost of accessing the external dataset) became a sunk cost. As a consequence, the payoff is computed without consider-ing C d and the threshold the adversary can tolerate increases from 48 to 51. If the actual equivalence group size is between the two thresholds, the adversary with less knowledge (i.e., in the unknown group scenario) may be able to cause greater risk, even though the adversary does not necessarily obtain a higher payoff than the known group adversary.

Records resulting in different risk levels in the known and unknown group scenarios are not very common in this ex-periment setting. A majority of the records lead to the same risk (94%, or 30641 in total). The is due primarily to the fact that this analysis is dominated by records whose cor-responding equivalence group size is larger than 55. Specif-ically, 64%, or 20891 in total, satisfy this situation. This is notable because, even if a positive payoff expected from P ( G r,D e ) leads the unknown group adversary to access the external dataset, the adversary never chooses to exploit such records, yielding a risk of 0. Figure 10: Sensitivity analysis on group size thresh-old ( k ) as a function of (a) external dataset cost C a ; (b) exploit cost C e ; (c) Penalty ; and (d) detection probability P det .

In this section, we investigate how the deterrence param-eters influence the threshold k of the size of the equivalence group when the adversary walks away, such that the risk is 0 when equivalence group size is  X  k under the three scenarios studied above. For this analysis, we assume a known group scenario and both the de-identified and external datasets cover the entire population, such that prior r,D e = 1. We vary 1) the cost to access data, 2) the cost to exploit the tar-geted individual, 3) the penalty levied when re-identification attempts are detected, and 4) the detection probability. In the analysis, we vary one variable at a time while holding all other variables constant to: C p = $20000, G = $1000, C e = $10, C l = $0, C a = $100.

The result is unsurprising, but notable. Specifically, as illustrated in Figure 10, as the deterrence mechanism is ramped up, the expected payout is lower and the adver-sary tolerates less risk. For example, when the penalty is set to $10,000, the adversary always attacks when the group size is smaller than 9 individuals. By the time the penalty is raised to $50000, the adversary will only risk an attack if there is one individual in the group. This result clearly indi-cates that penalties and costs for access to data can quickly deter an adversary from committing an attack.
This research provides a formal process-based approach to characterize the privacy risks for published data and opens a novel direction in the field of data privacy. It also in-troduces a scalable algorithm based on linear programming to solve the attacker X  X  optimal planning problem. A core contribution of this approach is that it accounts for deter-rence mechanisms beyond data manipulation methods. We demonstrated the feasibility through a case study in a real world scenario, where an adversary uses a publicly available population registry (with over 6,000,000 individuals) to at-tack a record subject to a data obfuscation mechanism.
Our results reveal that a broadly accepted adversarial model in which the adversary will randomly choose one indi-vidual that matches the record to attack can be suboptimal, and an adversary may try and exploit every individual in the corresponding equivalence class. In addition to penalization mechanism, our result demonstrated that the adversary X  X  optimal decision depends on the information about the ex-ternal resources they may use (e.g., voter registration lists) before they access them to mount an attack. This work provides strong evidence that the risk to such systems in the real world is heavily dependent on the amount of effort an adversary needs to exert and the expected payout they can receive based on their attack. This investigation further provides intuition into how data perturbation techniques can be complemented by alternative disincentive strategies (e.g., charging for access to data or levying fines for malicious be-havior) to lower the risk inherent in data sharing.
Our approach has several limitations which can provide directions for future research in this area. First, if such a risk estimation procedure is to be put into practice, policy makers will need information about the nature of deterrence mechanisms, the existence and costs of external data re-sources, as well as the adversary X  X  potential gain. Moreover, our work shows that knowing the prior probability that the corresponding target is in an external resource is critical to the model. Our model assumes that the external dataset is a random sample from a large population that also covers the protected data. Such information is not always read-ily available to the data publisher when evaluating risk. In the event the publisher believes they could underestimate such parameters, they may lobby for larger fines on misuse, thus deterring users with legitimate interests from accessing their resource. Thus, a future direction for research is in the development of approaches to estimate such parameters of the attack process. This may be possible, for example, by building a model for the detection rate based on existing detection mechanisms.

Second, there are limitations in the scope of the adver-sary X  X  goals. Consider, the process model assumed an ad-versary targets only one record in the protected dataset at a time. It also assumes that the adversary has access to only one external resource to mount an attack. Perhaps more significantly, we assume that the success of an exploit will be confirmed. Yet, certain adversaries may be interested in multiple records in the protected data (or even the entire dataset) and may have access to multiple resources. Re-moving any of these assumptions will lead to an increase in the complexity of the adversary X  X  decision problem. We note that the process model can be extended to account for these scenarios by introducing more state variables and actions. However, this will lead to an explosion in the state space. Therefore, a future direction of research is to generalize the FMDP model while improving the scalability of the solver algorithm.

Finally, our empirical analysis was conducted on a specific type of data, namely the demographic information within the publicly available population registry. Such a process-based approach to privacy risk assessments is applicable to other types of data where the attack is not a linkage-based exploit, but focuses rather on other definitions of privacy, such as inferential disclosure. The adaptation of such a tech-nique will depend on the extent to which the adversary X  X  process for realizing their exploit can be represented. This work was sponsored by grants from the NIH (R01-HG006844, R01-LM009989, U01-HG006478, U01-HG006385), the NSF (CCF-0424422), the AFRL (FA8785-14-2-0180), and Sandia National Lab (contract 2191). [1] A. Bharadwaj, O. El Sawy, P. Pavlou, et al. Digital [2] L. Bonomi and L. Xiong. A two-phase algorithm for [3] R. Dewri, I. Ray, I. Ray, et al. POkA: Identifying [4] C. Dwork. Differential privacy. In Proc. Int X  X  [5] C. Dwork. The promise of differential privacy: A [6] K. El Emam, E. Jonker, L. Arbuckle, and B. Malin. A [7] M. Elliot and A. Dale. Scenarios of attack: the data [8] L. Fan and L. Xiong. Real-time aggregate monitoring [9] D. Freni, C. Ruiz Vicente, S. Mascetti, et al. [10] B. C. M. Fung, K. Wang, R. Chen, and P. S. Yu. [11] C. Guestrin, D. Koller, R. Parr, and S. Venkataraman. [12] R. Jones, R. Kumar, B. Pang, and A. Tomkins.  X  X  [13] O. Kwon, N. Lee, and B. Shin. Data quality [14] D. Lambert. Measures of disclosure risk and harm. [15] J. Letchford and Y. Vorobeychik. Optimal interdiction [16] A. Machanavajjhala, D. Kifer, J. Gehrke, et al. [17] E. Mackey and M. Elliot. Understanding the data [18] A. Narayanan and V. Shmatikov. De-anonymizing [19] A. Narayanan and V. Shmatikov. Myths and fallacies [20] M. E. Nergiz, M. Atzori, and C. Clifton. Hiding the [21] North Carolina Voter Registration Database, [22] M. L. Puterman. Markov Decision Processes: Discrete [23] L. Roderick. Discipline and power in the digital age: [24] P. Samarati and L. Sweeney. Protecting privacy when [25] D. Solove. A taxonomy of privacy. University of [26] M. Srivatsa and M. Hicks. Deanonymizing mobility [27] L. Sweeney. Uniqueness of simple demographics in the [28] P. Tallon. An application of game theory to [29] P. Tallon. Corporate governance of big data: [30] A. Tanner. Harvard professor re-identifies anonymous [31] Z. Wan, Y. Vorobeychik, W. Xia, et al. A game
