 A query independent feature, relating perhaps to document content, linkage or usage, can be transformed into a static, per-document relevance weight for use in ranking. The chal-lenge is to find a good function to transform feature values into relevance scores. This paper presents FLOE, a sim-ple density analysis method for modelling the shape of the transformation required, based on training data and without assuming independence between feature and baseline. For a new query independent feature, it addresses the questions: is it required for ranking, what sort of transformation is ap-propriate and, after adding it, how successful was the chosen transformation? Based on this we apply sigmoid transfor-mations to PageRank, indegree, URL Length and ClickDis-tance, tested in combination with a BM25 baseline. H.3.3 [ Information Search and Retrieval ] Experimentation Web Search, Ranking, Probabilistic IR
A relevance weighting scheme for query independent evi-dence is one way of combining a query independent feature such as Google X  X  PageRank [3], with a query dependent base-line. The idea is to attach a static relevance weight to each document, based on the feature. This weight is then linearly combined with the query dependent baseline score, to give a new score and ranking. The challenge is to choose a good method for transforming the feature into a per-document relevance weight to enhance retrieval effectiveness. In par-ticular we focus on combination with a BM25 baseline.
We are motivated by the wealth of potentially useful static ranking features. It is easy to think of a number of ex-amples. PageRank may indicate whether a page is a good Web search result. When searching for technical answers in a newsgroup, feature(s) indicating a message X  X  position in the thread might be useful (since the root usually contains the question, and later messages the answer). File creation or modification date may be an important search feature when searching a Personal Computer, if the user is more often searching for recent files. A lower-priced product, but not too low-priced, might be a better answer in e-commerce search. In all these cases, the question is whether the fea-ture can be used to adjust the ranking and improve search effectiveness, and if so how best to incorporate it.
Without some analysis, the answer is not immediately ob-vious. It is possible to guess a combination function and see if it works. If the guessed combination fails, this does not mean the feature is useless. If the guessed combination succeeds, this does not mean it is optimal. Further, the ap-propriate adjustment depends on the baseline ranking. A query independent feature which indicates relevance might not be needed at all, if the baseline already makes use of highly correlated features.

Our approach is to model the appropriate static score, for a given static feature, corpus and baseline ranking. Mod-elling this weight can tell us a number of things: 1. Whether the feature is needed. The model can predict 2. What shape the adjustment should take. This helps us 3. Whether mistakes have been made in combination. Af-Our model correctly predicts that PageRank, link indegree, URL length and ClickDistance (described in Section 4) are each useful static features when searching the TREC .GOV test collection from a BM25 baseline. It also suggests that sigmoid transformations are appropriate for turning the fea-tures into relevance weights. After applying a single feature, it predicts no further adjustment for that feature is neces-sary (indicating the combination was successful) except in the case of ClickDistance. After adding PageRank, it cor-Figure 1: For the .GOV test collection, PageRank has a power law distribution. rectly predicts that URL length is still useful, and that in-degree and ClickDistance are no longer needed.

First we survey past work on query independent evidence and its use in ranking. Then we evaluate a number of rel-evance weightings for PageRank and use this initial exper-iment to motivate the new approach. After describing the new approach we apply it to link indegree, URL Length and ClickDistance.
In a ranking scheme, some features pertain to query terms, so their usage depends on the current query. For example, in most ranking schemes we do not know which term frequency statistics will be used until we know the query. Other fea-tures, we know we will use before we know the query, and in that case we refer to them as query independent evidence. A well known example of query independent evidence is PageRank [3], a score assigned to each document in a Web crawl. It indicates how easy it would be to reach that page by randomly following links: the  X  X andom surfer X  model. A page can have higher PageRank if more pages link to it, or if the linking pages themselves have higher PageRank or lower out-degree, because each of these increases the chances of the random surfer reaching the page. In this paper we use standard PageRank, with a random jump probability of 1 / 7 and a mean PageRank value of 1.

The initial Google paper [3] did not describe how PageR-ank should be combined with a query dependent baseline. Combination is difficult because PageRank has a power law distribution [10]. This means a simple linear combination of scores, for example, would lead to most pages getting almost no score and a few getting a very large score. For example, calculating PageRank on TREC .GOV such that the aver-age PageRank is 1 gave the distribution in Figure 1. The top PageRank is 4522.6, which is 21,084 times the median PageRank of 0.2145.

Simple link counts may also be used as query independent evidence, such as a page X  X  indegree or outdegree. Further metrics can be found by collapsing all of a host X  X  nodes into a single node, and calculating host indegree, host outdegree and host PageRank (HostRank) [1]. URLs are also a source of useful static features, preferring short URLs or dividing URLs into different types (root, subroot, path and file)[9].
Non-web environments may also have query independent evidence which is worth exploiting, for example thread size, number of replies and message date might be useful static ranking features in a message archive. Query independent document usage information  X  such as aggregate clickthrough, visit frequency or dwell time statistics  X  may also prove use-ful in ranking. The approach described in this paper may be applied to any of these new features.

Document length is used in many retrieval models; it is used in every query, therefore in some sense it is a form of query independent evidence. However, it is usually ap-plied at the core of the query dependent retrieval model, for per-term normalization. This makes document length quite different from the other static features we consider, and we do not survey here different forms of length normalisation. Other papers studied length normalization [12, 9, 7].
There are three broad approaches to combining static fea-tures and a query dependent baseline ranking: rank-based, as a language modelling prior and as a relevance score ad-justment.

Rank-based combination has been used with some success [6, 4, 14]. This involves turning the baseline and static scores into two rankings, and combining based solely on ranks. This has the advantage of ignoring the score distributions, so for example it is impossible to fall foul of PageRank X  X  power law distribution by giving too much of a boost to pages with very high PageRank. However, it should be possible to do as well or better using scores, since they contain more in-formation. For example it is possible to generate document ranks given scores but not vice versa.

In a language modelling framework, prior probabilities were calculated for page length, indegree and URL type in [9]. For example, indegree was divided into bins on a log scale, and for each bin a prior was calculated. This was then combined with the language modelling probability (a multiplicative combination).

The other approach, and the one used here, is to rank based on a linear combination of baseline and static scores. This can be done using raw scores, for example BM25 with URL prior and PageRank [8]. However, it may be possible to do even better with a nonlinear transformation of the input feature. For example, using log(indegree) rather than raw indegree [13]. We introduce our approach with a detailed case study of PageRank relevance weighting in the TREC .GOV test col-lection. PageRank combination is a well known and difficult problem, so an obvious choice here. Our baseline ranking is BM25 with field weighting [11] and per-field length normal-isation parameters [15]. We use three text fields: body, title and referring anchor text.

The case study has three parts. First we explore the notion of relevance weighting for PageRank, by choosing three PageRank weighting functions, tuning their param-eters and evaluating their effectiveness. This gives us a pre-ferred method for adding PageRank, but not a reason for doing it that way. In the second part we develop a model for the appropriate adjustment, assuming independence. This is not a good match for the weighting found empirically, so we finally use a simple heuristic we call FLOE (feature X  X  log odds estimate) for taking dependence into account.
BM25, like a number of other ranking approaches, pro-duces a relevance score which is the sum of weights for each query term. In such a system it is natural to consider adding a further weight which depends on some query independent document feature. The question is how to turn a static feature into an appropriate relevance weight. In our ini-tial experiment, we try three different transformations on PageRank, tuning their parameters.

Our tuning set comprises 120 mixed topics from the TREC-2003 Web Track: 40 topic distillation, 40 homepage and 40 named page. Our test set is the full set of 225 mixed queries from the TREC-2004 Web Track [5]. In each case we rerank the top 1000 of our BM25 baseline using finalscore = BM25score + PageRankWeight . Limiting the reranking to the top 1000 is more efficient than reranking all documents with nonzero BM25 score, without making a large difference to system effectiveness.

Our first weighting function is: where S is the value of the static feature (PageRank) and w is the weight we use when combining with BM25. Tuning involved an exploration of w from 0 in steps of 0 . 01, in order to maximise mean average precision (MAP) on the training set (Figure 2). The best tuning, at w = 0 . 20, gave MAP of 0.508 on the test set, from a baseline of 0.430. Note, we performed a similar experiment without taking log. The result was a very low weight ( w = 0 . 005) and very poor test set performance of 0.439.

BM25 X  X  function for term frequency is of the form TF k + TF This function saturates, in the sense that it approaches 1 as TF increases ( TF , like the other features discussed in this paper, is non-negative). Our second weighting function for PageRank, named satu , is similar: There are two parameters: w is the maximum which is approached as S increases and k which is the value of S where satu is w/ 2. Tuning via extensive 2d exploration gave w = 1 . 34 and k = 1 . 36, and test MAP of 0.515 (better than log ).

The function satu can be reformulated as a sigmoid on log( S ); however, it is not the most general sigmoid. By introducing another parameter we gain more control over the rate of saturation, making the function more general and flexible. This third function, known as sigm , is defined as follows: Adding the extra parameter allows slightly better perfor-mance again: MAP of 0.523 with parameters w = 1 . 8, k = 1 and a = 0 . 6. For the sigmoid we only tune to one decimal place, since we are now working in three dimensions.
The three weightings (Figure 3) have remarkably similar slopes in the range -2..2, where 98% of pages in the collection lie. Note, there are gaps between the parallel line sections, Figure 2: Tuning w , for log PageRank combination. Figure 3: Weighting functions for PageRank tuned on a mixed set of WT03 queries. Numbers in square brackets are average precision on WT04, from a baseline run scoring 0.430. Functions have similar slopes in the range -2..2, where 98% of the corpus lies. but these gaps could be closed by adding a constant to the weighting, which would have no effect on the overall ranking i.e. it is the slope not the height of a line which is important.
The similarities suggest there is some underlying truth, or  X  X deal weighting X  for PageRank in this experiment, and that this is particularly important in the range -2..2. However, there are a number of problems. We can not tell how close our guesses are to the ideal (or see an estimate of the ideal function), we can not analyse where these went wrong, if at all, and we do not know if there are other transformations that are even better. Our method of density analysis de-scribed in the remainder of this section attempts to address these issues.
We wish to rank according to the probability that a doc-ument ( D ) is relevant ( R ) for the current query: P ( R | D ). When working with BM25 we rewrite this, in a way which preserves the rank order with respect to the query ( Q ): P ( R | D ) Q  X  log P ( R | D )
If we then consider that a document D has two compo-nents, its content match M and its static score S , we can separate it into two additive scores: So BM25 can be linearly combined with a term that depends on modelling S with respect to M and R .

One possibility at this point is to drop the M term, as-suming that content match and static score are independent. Under this independence assumption the correct BM25 score adjustment would be: We can now plot indep and see if it matches our empirical score adjustments from Figure 3. Our plot uses kernel den-sity estimation [2], which gives us smooth curves. However, we have done so equally successfully using histograms. We find the set of all relevant documents 1 for our 120 training queries, and denote such documents as R . We find the den-sity, over log PageRank, of two sets of documents: the rele-vant documents P ( S | R ) and the collection P ( S )  X  P ( S | These are in Figure 4. Our adjustment indep is log of the ratio of these two lines. This can then be plotted in com-parison to our best empirical adjustment.

Unfortunately, the result in Figure 5 is a curve which is much steeper than our empirical line. It has a slope of about 0.6, which would give quite bad performance compared to our training optimum of 0.2 (Figure 2). Note, there are very few relevant examples at the right-hand end of the plot (indicated by points). In this region the density estimate is less trustworthy.
We hypothesise that the mismatch in Figure 5 is due to the independence assumption. That is, BM25 is already retrieving high PageRank documents to some extent, so ad-justment indep overstates the score boost for high-PageRank pages, in some sense double-counting PageRank. This is not surprising, since PageRank is a link metric and links are also used in the anchor text field of our baseline. A page with many incoming links is likely to have a higher PageRank and higher BM25 score if there is an anchor match.
FLOE (feature X  X  log odds estimate) is one possible solu-tion to avoid double-counting. It involves finding an esti-mate of the levels of S already in the baseline, using the retrieved set . We include in the set the top r baseline re-sults for each query, where r is the number of known relevant results for that query. Similarly to R and  X  R , we denote doc-uments in this set as T and all other documents  X  T . Then we can find a density estimate as we did for the relevant set:
This can be thought of as a set of document query pairs, since we allow a document to appear multiple times if it is relevant for multiple queries. Figure 4: These are the density estimates used to model the curves in Figures 5 and 6. Figure 5: The estimated adjustment assuming inde-pendence ( indep ) does not match the empirical line ( sigm ) from Figure 3. The points labelled R are those in the relevant set we use for estimation. Figure 6: The floe line is a better match for the empirical line. The points labelled T are those in our retrieved set. Where data becomes sparse, our estimates break down. The upward slope of this line in Figure 6 indicates that BM25 already has a strong tendency to retrieve high-PageRank pages. Now that we have a model for the PageRank levels in our baseline, we can remove this from indep by taking the difference of the two lines: floe ( S,R,T ) = log P ( S | R ) The simplification on the right hand side takes into account that  X  R  X   X  T since both are approximately the whole corpus.
In Figure 6 the line given by floe has a slope closely match-ing our empirical line in the region -2..2. It is giving a much more accurate estimate of our best PageRank adjustment than indep .

Note, our estimate becomes unreliable where data is sparse, so in all subsequent plots of equation (9) we limit the range to the range of documents in the retrieved set. The es-timate is already unreliable at the right-hand end of this range, where there are only a few points, but beyond this range it becomes meaningless. This is a limitation of the model in the presence of sparse data, which could perhaps be ameliorated if the training set were larger. In a num-ber of plots we will see bumps and other shapes where data become sparse.

A well known previous study [12] also compared the rele-vant and retrieved sets, and indeed prompted us to identify a retrieved set, in order to correct indep . Singhal, Buckley and Mitra analysed document length normalisation intensively, whereas this paper suggests that this type of approach can be used with PageRank and other static features. Also, we attempt to demonstrate that the difference between our lines (equation 9) can be used as a model for the appropriate static weight. By contrast, using the bucketing and plotting methods of [12] we would give downward sloping lines. It would indicate a gap, but give less information about the appropriate adjustment.
In our first experiments we guessed functions for trans-forming PageRank into a relevance weight. Now we have a method for estimating the appropriate adjustment, we can apply it for new features: link indegree, URL length in characters and ClickDistance. ClickDistance is a link met-ric which measures the page X  X  minimum link distance from some root. In the case of .GOV we used firstgov.gov as the root, so pages it links to are at distance 1 and unseen pages linked to by them are distance 2 and so on. We gave docu-ments which were unreachable the median ClickDistance of the reachable pages (=7).

Using FLOE we can plot score adjustment estimates for each feature (Figure 7). We have one parameter, the density estimate X  X  kernel width (h), so we show three estimates in each plot. For smaller h, we see bumps and shapes which may be noise in our sample, rather than indicative for choos-ing a functional form. If we increase the width, we see smoother shapes emerging. All other plots in the paper use a kernel width ( h ) equal to 10% of the range of the retrieved set.

The estimated score adjustment is increasing for indegree, and decreasing for URL length and ClickDistance. In each case we see a flattening at the right-hand end of the plot, although this is also where in each case the data becomes Figure 7: Equation (9) adjustment for indegree, URL length and ClickDistance on the mixed set of 120 training queries. In each case we plot for three different kernel widths ( h ), of 5%, 10% and 15% of the data range of the retrieved set. sparse so we would expect our estimates to be less accurate. Nevertheless, the shapes suggest further use of sigmoid func-tions (strictly, sigmoids of log( S )). We use a function with downward slope: when S is URL length or ClickDistance, and one with up-ward slope: when S is Indegree (as used for PageRank). Each has three tunable parameters w , k and a .

Using these functions and tuning on our training set gives us the empirical lines in Figure 8. In each case in Figure 8, the floe line gives us a much better explanation of the ap-propriate relevance weighting than the indep line. The fact that this happens repeatedly gives us more confidence that estimate (9) is informative.

Effectiveness results for these tuned sigmoids are in Ta-ble 1. This repeats the result from Figure 3 that the sig-moid function had best performance for PageRank. It also indicates which static features worked best on our test set: PageRank &gt; Indegree &gt; URL Length &gt; ClickDistance. These results are in keeping with recent TREC results [5]: URL length is less useful in tests involving named page queries, and the query-independent link features used by the top 4 groups were PageRank, HostRank, nothing and indegree. Figure 8: For our three new features, floe matches the best sigmoid score adjustment found empirically, whereas indep overstates the score adjustment (it is too steep).
Another way of using FLOE is to assess the quality of combination. We do post-combination plots on the training set, since they are in some sense meant to uncover problems with our tuning before a test set is available.

Figure 9 shows that in most cases the floe line has be-come more uniform over S . For PageRank and indegree the Evidence Combination MAP MAP Baseline  X  0.458 0.430 PageRank linear w=0.005 0.498 0.439 PageRank log w=0.2 0.556 0.508 PageRank w=1.34 k=1.36 (a=1) 0.556 0.515 PageRank w=1.8 k=1 a=0.6 0.567 0.523 Indegree w=3.6 k=5 a=0.2 0.549 0.489 URL Length w=4.5 k=4 a=0.5 0.530 0.477 ClickDistance w=2.4 k=8 a=2.6 0.532 0.470
PageRank w=1.8 k=1 a=0.6 + URL Length w=1.9 k=6 a=0.2 0.572 0.532 Table 1: Static evidence performance on training and test sets. PageRank functions are linear, log, sigmoid with a=1 and full sigmoid. For indegree, URL Length and ClickDistance, we use full sigmoid only. line is very flat, indicating that no further adjustment is necessary. For URL length the line is non-zero in a region of sparse data, so perhaps does not indicate a systematic problem worth correcting. By contrast the ClickDistance line is distinctly non-uniform. We have so far been unable to find a function with better MAP performance than the sigmoid of ClickDistance. Either the plot is misleading, or there really is a better combination function we have not yet discovered. Note, we know it is possible for the Click-Distance line to be more uniform, for example, PageRank makes it so in Figure 11.

A limitation of this type of check is that, for our train-ing set, we were not able to see differences between the PageRank weighting functions in Section 3.1. The differ-ences in their effectiveness were too small to be reflected in the three corresponding post-check lines. This suggests that post-combination checking may be most useful for finding and understanding gross errors in combination, rather than deciding between functional forms which have similar per-formances. The level of noise in the estimates, at least on our training set, is too great to see finer differences.
To explore this, we plotted post-combination indegree lines for a range of w values (Figure 10). The numbers in brackets are the performance on the training set. Despite noise, the best-performing tunings do have the flattest lines. When there are gross errors in weighting, the slope of the line cor-rectly indicates the type of further upweighting or down-weighting required to correct the problem. We believe that when lines become flat in Figure 9 for PageRank, indegree and URL length, this indicates at least no gross errors of combination were made. More investigation is needed for ClickDistance.

Post-combination checking can also be used across fea-tures. For example, PageRank was the best single static feature (Table 1). Having added PageRank, does FLOE indicate that adjustment of other static features is now nec-essary? Figure 11 shows floe lines for indegree, URL length and ClickDistance on top of a baseline of BM25 plus PageR-ank. The indications for indegree and ClickDistance are that no further adjustment is necessary. This makes sense since they are both link metrics. The lines are flat, at least in the region where we have sufficient data. Figure 9: After adding a feature, the floe line tends to become flat for that feature, indicating that no further adjustment is necessary. ClickDistance is the exception. Figure 10: Post-correction floe lines for inde-gree. Overweighted indegree gives a downward slope, underweighted gives upward slope, with best-performing lines being flat.
 The floe line for URL length is above zero for very short URLs, which is the largest deviation at the left-hand (non-sparse) region of the three plots. This indicates it may still be useful to positively weight short URLs. URL length is not a link metric, so it is plausible that it contains information that is not in PageRank.

To test these predictions we tuned each feature on top of our best BM25+PageRank baseline. The weight of indegree went to zero and the weight of ClickDistance went to 0.1, making an improvement in MAP at the fifth decimal place. Therefore we take it that there was no improvement to be had, at least by our combination methods. By contrast URL Length gave an improvement, as predicted by FLOE. The result is at the bottom of Table 1.
Our starting position was that it is natural in a ranking system based on relevance weighting to add an extra weight for a query independent feature. In Section 3.2 we developed the correct relevance weight for use in a situation where we assume content match M and static feature S are indepen-dent ( indep ). However, when we estimate this weight it does not match weights we find empirically (Figures 5 and 8). Through a simple heuristic, adjusting for the levels of S already present in M , we develop a much more accurate predictor (FLOE).

We use this model as an indication of the appropriate functional forms for indegree, URL length and ClickDis-tance. We use it again as a test that we have made no gross errors in combination (suggesting we need to do more work on ClickDistance). Finally, we use it as a light-weight test, as an indication that on top of a BM25 and PageR-ank baseline, not much further adjustment is needed. In the scenario of an operational search system, correctly mak-ing such a prediction would allow a feature to be eliminated early, perhaps avoiding implementation effort.

There are two major limitations with this work. First, there is noise in the estimation, particularly when data be-comes sparse. The level of noise present in the estimates in Figure 10 means we talk about what the line  X  X ndicates X  Figure 11: For a baseline of BM25 plus PageR-ank, the flat floe lines indicate not much adjust-ment is needed. There is an upward slope for short URLs. rather than taking its meaning as precise. The second limi-tation is related to the first. If the estimate were very good, it would be possible to skip the step of tuning on the train-ing set and we could use the floe line itself as an adjustment. We have begun experiments in this direction, for example tuning a sigmoid directly to fit the estimate, but so far this gives inferior results.

One major outcome is an effective system for adding static features using relevance weights. Estimates for PageRank, link indegree, URL length and ClickDistance all indicate that a sigmoid functional form is appropriate, although more investigation is needed for ClickDistance. Each feature on its own can be used to improve effectiveness. However, after adding PageRank, the only additional static feature needed was URL length, and this gives only a slight improvement. So a very specific result of our experiments is a Web ranking scheme using field-weighted BM25 plus sigmoid-transformed PageRank and URL length.

FLOE has offered useful guidance in developing this sys-tem. We look forward to applying the model for new static features, perhaps in different settings. For example, there are several potentially interesting static features in email search, which might be explored in the planned TREC-2005 Enterprise Track.
 Thanks to Marc Najork for calculating PageRank on .GOV.
