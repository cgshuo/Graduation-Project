 We present a corpus-based approach to the class expansion task. For a given set of seed entities we use co-occurrence statistics taken from a text collection to define a member-ship function that is used to rank candidate entities for in-clusion in the set. We describe an evaluation framework that uses data from Wikipedia. The performance of our class ex-tension method improves as the size of the text collection increases.
 H.3 [ Information Storage and Retrieval ]: H.3.1 Con-tent Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [ Information Systems Applications ]: H.4.2 Types of Systems; H.4.m Miscellaneous Algorithms, Measurement, Performance Lexical acquisition, List expansion
Lexical resources such as thesauri and ontologies form es-sential ingredients of many intelligent information access ap-plications. Creating, maintaining, and expanding such re-sources by manual means is a tedious and expensive task. We are interested in one particular lexical acquisition task: the automatic extension of classes. I.e., given a few seed instances of some (unknown) class of entities, we consult a corpus in order to identify similar entities to add to the class. We are keen on corpus-based methods that can be deployed on arbitrary text collections, regardless of the language. We propose and evaluate one such method that uses a small Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. seed set obtained from the user and  X  X rows X  those seeds into sets of similar entities, together forming the extension of some concept. An example is provided by the seed set {
Raphael, Michelangelo, Leonardo da Vinci } ,whichmight be expanded so as to include, e.g., El Greco , Sandro Botti-cel li , Jan van Eyck , depending on our background corpus. We also describe an evaluation framework for class expan-sion methods that uses data from Wikipedia. 1 Evaluation of our class expansion method on English and Portuguese Wikipedias indicates that its performance improves as the frequency of candidate class members increases.
Roark and Charniak [4] describe a corpus-based method for expanding a nominal category from a small set of exam-ple  X  X eed X  words. The method selects new category mem-bers by considering co-occurre nce with the seed words. The co-occurrence statistics is based on noun conjunctions, lists and appositives. The focus is on common nouns ( X  car  X ,  X  pickup trucks  X ); no results are reported for named entities. A similar bootstrapping approach due to Thelen and Riloff [5] relies on a large body of extraction patterns that capture information about the behaviour of a word. E.g., patterns such as  X  X was arrested  X  X r X  murdered X  X  are likely to extract candidates of the category People ;awordiscon-sidered a good candidate for inclusion in a category if it is extracted by patterns that also extract known category members. Ghahramani and Heller [3] propose a probabilis-tic Bayesian framework for expanding a class from seed enti-ties. The method estimates the probability that a candidate belongs to a (hidden) class, based on the available informa-tion. The authors compare their class expansion algorithm to Google Sets 2 and show a significant improvement. Fi-nally, the task we are addressing is similar to the List Com-pletion task that is to be evaluated at INEX (Initiative for the Evaluation of XML Retrieval) [2, 1].
The task we are address is the expansion of entity classes: given a set S of entities (seeds) that belong to some class http://www.wikipedia.org http://labs.google.com/sets C ,andaset E of candidate entities, we wish to deter-mine which of the entities in E belong to C .Inother words, we want to  X  X row X  a class C from a few seed ex-amples, choosing elements from E .Consider,forexample, the seed S 1 = { Michelangelo , Leonardo da Vinci } . This seed may, in principle, be used to describe any of the classes C : {  X  X enaissance artists X  } , C 2 : {  X  X talian artists X  } {  X  X talian people X  } . Binary classification cannot deal with such descriptive vagueness in a straightforward way. In-stead, we approach the class expansion problem by defin-ing a class membership function 3  X  ( C, e ) that computes the degree of membership of entity e to class C (a value be-tween 0 and 1). Since in the class expansion task the ac-tual C is not known, we approximate  X  ( C, e )by  X  ( S, e the latter function defines the degree to which e belongs to the same entity class(es) as all elements of S .Forthe example above, we could define  X  ( C 1 , Jan van Eyck )=1 and  X  ( C 2 , Jan van Eyck ) = 0, whereas  X  ( S 1 , Jan van Eyck ) would be defined as a value between 0 and 1, probably, closer to 1. At the same time, we would like to have  X  ( S 1 , Linux )= 0, as Linux shares few (if any) semantic properties with the elements of S 1 . If we can compute the value of the member-ship function  X  ( S, e ) for a given seed set S and any candidate entity e , we can define the expansion of S as the set of elements of E with the highest values of  X  ( S, e ). Thus, for given S ,and E , we reduce the class expansion problem to the task of computing  X  ( S, e ) for all e  X  E .

We consider the membership function as a kind of simi-larity function between a set of seed entities ( S )andasingle entity ( e ), and compute the values of the membership func-tion using the vector space model (VSM). In the VSM, each element (in our case, a seed set of entities or a candidate en-tity) w j is represented by a vector of numerical features, Given such a representation we can compare elements using standard distance measures (e.g., cosine similarity). The choice of features defines the information that is captured and transferred to the vector space. We are mainly targeting type similarity . Based on the fact that humans easily group and list type similar objects, we assume that many explicit enumerations of entities that we find in text are lists of ob-jects of similar classes. Therefore, if we can identify such enumerations in text, we may be able to gather information about class similarity. Our assumption (also used in, e.g., [4, 5]) is that if two elements consistently co-occur in lists, they are likely to be of a similar semantic class. Identification of lists in text, however, may not be trivial. We propose a simple approximation that tries to identify pairs of elements that belong to textual lists. We assume that lists are composed by sequences of pairs of coordinated elements, which are either connected by explicit coordina-tional elements ( X  and  X ,  X  or  X  ...) or by commas. This intu-ition has been corroborated by studies that focus specifically on using information about coordinated words (see, e.g., [6]). In order to identify entity pairs that belong to lists, we look for structures like  X  ... ne a , ne b and ne c ...  X  X here ne b , etc. are named entities. E.g.,  X  I X  X e lived in NY, Paris, and Amsterdam . X  Another possibility would be  X  ... ne a , ne b or ne c ...  X  X sin X  Experience with Oracle, PostgreSQL or MySQL is required  X ). When instances of such patterns are found in a corpus, we can easily conclude that the pairs ( X  ne a  X , X  ne b  X ) and ( X  ne b  X , X  ne c  X ) co-occur in coordination. We borrow the terminology from Fuzzy Set Theory.
 Note that, for simplicity, we will not make any conclusions about  X  ne a  X  X nd X  ne c  X . Having extracted such co-occurence information for all entities in the corpus, we can represent our entities and entity sets as vectors encoding the frequency of co-occurence. Specifically, if ne 1 ,...,ne N are all (named) entities in the corpus that occur in coordination construc-tions, then the j -th component of the vector ne i is defined as the number of times ne i and ne j co-occur in coordina-tion. Similiarly, for an entity set S k , S k ( j ) is defined as the number of times ne j co-occurs with any element of S k in co-ordination. However, pairs of elements connected by comma are subject to a lot of noise because surface text structures of the form  X  ... X , Y , ...  X  may occur often without im-plying any class similarity between X and Y , but introduce apposition, or clarification, instead. Therefore, an alterna-tive is to take only information about pairs connected by explicit coordinations such as  X  ... ne a and ne b ...  X  X r X  ... ne a or ne b ...  X . The feature vectors generated with this restriction may be less noisy, though we are likely to hurt recall. We will refer to vector spaces built using only ex-plicitly coordinated pairs by VS X , and to those built from explicit coordinations and commas as VS . 4
With a given feature representation and vector space, the membership function between an entity e andanentityset S can be defined based on any standard distance measure for vector spaces. In our experiments we use the cosine similarity measure ,sothat  X  ( S, e )=cos( S, e ). Depending on the vector space used to obtain the vector representation of the entities and sets, we can have different definitions of the membership function. We will differentiate between  X  , which is calculated using VS ,and  X  X which is calculated using VS X . We will use  X  when the difference is immaterial.
Our motivation for using Wikipedia to develop the evalu-ation framework is two-fold. First, Wikipedia contains sev-eral explicit human-generated lists that can serve as gold standards for class expansion algorithms. Such lists corre-spond both to fairly common concepts (e.g.,  X  List of En-glish novelists  X ),aswellasratherexoticandveryspecific lists (e.g.,  X  List of fractals by Hausdorff dimension  X ). can thus obtain evaluation data from different domains and different levels of specificity. Second, Wikipedia has explicit information regarding the delimitation of named entities in text. Many articles in Wikipedia are about named enti-ties, and articles explicitly link to each other. Whenever in the text of an article A 1 we find a (hyper)link to another Wikipedia article A 2 such that the anchor text of the link starts with a capital letter, we may assume that the an-chor text is a mention of an entity, the one addressed in A . Using this heuristic, we automatically identify correctly delimited mentions of named entities in Wikipedia articles. We thus circumvent the problem of identifying named enti-ties in text, and focus on evaluating membership functions and class expansion algorithms.

In the definition of the class expansion task (Section 3), the input of a system contains the seed set S and the set of candidates E from which we will chose the ones that belong
Importantly, when applying our method, we will always be able to guarantee that  X  X  X  and  X  X  X  are in fact named entities (see Section 4).
See http://en.wikipedia.org/wiki/ name of the list . to the (unknown) class C, which is implicitly defined by the seed examples. Since the Wikipedia lists that we will use to create our evaluation sets are obviously not guaranteed to be complete and exhaustive (we can not be sure that all valid el-ements are included), instead of directly evaluating the class expansion task, we will evaluate the quality of rankings of E that our membership functions produce. Specifically, for a gold standard class C , which corresponds to a Wikipedia list, we will start by picking a subset that contains positive examples P ( P X  C ). Then, we will construct the set N that contains negative examples, that should include enti-ties that are somehow related to elements of C , but do not belong to C . In general, |N| |P| .Foragiven C and sets of positives P and negatives N ,a test case consists of a seed set taken from P , S  X  X  . To evaluate the quality of a membership function  X  on this test case, we construct the set of candidates E = P X  X \ S , rank the elements E by  X  (
S,  X  ) and assess the quality of the resulting ranking R us-ing average precision , a measure commonly used to evaluate the quality of a ranking: Here, P at ( r ) is the value of precision at rank r (i.e., the num-ber of positive examples among R (1) ,...,R ( r ), divided by r ), and I ( x  X  X ) is the indicator function of set member-ship (1 if x  X  X ,and0if x  X  X ). Average precision favors rankings that produce relevant items (in our case, positive examples) earlier. A test set is a collection of all test cases for a given C , P and N . The overall quality of a member-ship function  X  on a test set can be assessed using the Mean Average Precision of the rankings produced for all test cases: where m is the number of test cases and R i is a ranking produced for the test case i with seed S i .

The construction of the actual test sets is done in two stages, and is based on XML encoded dumps of Wikipedia. 6 We produced test sets using the English XML dump (1.6M files, 2.9Gb) and the Portuguese XML dump (0.21M files, 227MB). First, we detected lists in Wikipedia pages. Such pages can be easily identified by an expressive title:  X  X ist of ... X  for English and  X  X ista de ... X  for Portuguese. We only considered the subset of those pages which present in-formation using explicit HTML list structures , because they are easy to process (i.e., tables are ignored). Then, for each element of the extracted lists we obtain (i) the correspond-ing frequency in Wikipedia (i.e., number of times that it occurs as a link, as explained in previously), and (ii) the URL of the Wikipedia article that addresses that entity. For the elements that do not posses a corresponding article in Wikipedia (i.e., point to a page were the user is invited to start an article about that topic) we still extracted the element but we kept information regarding the absence of the article. Entirely numeric elements are ignored, to avoid long lists of dates or other numerical values (e.g., telephone codes) which are useless for evaluation purposes. We were able to extract 17,594 lists for English (referred to as L and 1,390 lists for Portuguese ( L pt ). Lists from L en
Available from the University of Amsterdam: http:// ilps.science.uva.nl/WikiXML . tained on average 92.4 elements, and 58.4 elements on aver-age were linked to a existing Wikipedia article. For L pt numbers are 90.3 and 43.4, respectively. One major differ-ence between lists in L en and L pt is the average frequency of their elements throughout the corresponding Wikipedias. Elements in L en have an average frequency of 286.2, whereas for L pt it is only 32.5.

We generated the sets of positives, P , and the correspond-ing sets of negatives, N ,from L en and L pt as follows. We first selected only those lists which contained a minimum number of elements, min a , with a dedicated Wikipedia arti-cle. This filtering was done to guarantee that the topic ad-dressed by the list is reasonably well covered in Wikipedia. We set min a = 10 for English and Portuguese and we thus obtained two more restrictive sets of list, L min a en and Then for each list  X  ( i )in L min a en and in L min a all items that have both a dedicated article and whose fre-quency in Wikipedia is higher than a given threshold f min These will constitute P cand ( i ), the candidates for set Next, for each element in P cand ( i ) we extract all entities from the corresponding Wikipedia articles. Such entities are added to set N cand ( i ), except those that belong to list  X  ( i ). The set N cand ( i ) will thus consist of all sorts of entities  X  X elated X  to elements from P cand ( i ) but which are known not to belong to the initial list  X  ( i ). Since sets P cand N cand ( i ) can be extremely large the final P ( i )and N ( sets are obtained by truncating the candidate sets. Thus, only the top n P most frequent elements from P cand ( i )are chosen (if there are less than n P , all are chosen). These will become set P ( i ). Again, we are trying to ensure that  X  tested on sufficiently frequent items. From N cand ( i ), we also chose the top n N most frequent elements with n N being set to twice the number of elements in P ( i ). These elements will become set N ( i ) Finally, we exclude all P ( i )and sets for which the number of elements in P ( i )islessthat5. We set f min = 100 both for English and Portuguese. Also, in both cases we set n P = 20. For practical reasons, this number can not be higher because the test procedure will involve combinations of elements from P ( i ). We were able to generate 3,219 test sets for English and 75 for Portuguese. In both cases, only about one third of the tests do actually reach the n P limit. Again, the major difference is the av-erage frequency of the elements contained in the P ( i )sets, measured over the corresponding Wikipedia. For English, that figure is 1,758.3 while for Portuguese it is 623.6, which is still high given the relative sizes of the two Wikipedias.
We collected pairs of coordinated named entities for En-glish and for Portuguese using the previously described XML dumps. As explained before, an important reason for hav-ing chosen Wikipedia as the source corpus for grounding our membership function  X  is that we can thus avoid the complex problem of identifying/delimiting named entities in text. By using a simple heuristic based on links found in Wikipedia, we can easily identify named entities in the articles. We should emphasize that the only textual infor-mation used for extracting the pairs of coordinated named entities was the text contained in paragraphs inside articles (8.8M paragraphs for English and 0.76M for Portuguese). Category and language links, information boxes and ex-plicit list information (both lists and tables) were ignored. Paragraphs in Wikipedia articles were scanned for struc-tures of the form  X ( ne a )( coordination connector )( ne We extracted features defining four vector spaces: VS X EN and VS X PT , using only explicit coordination, and VS EN VS 1). For English, we used the following explicit coordina-tion connectors  X  and the  X ,  X  and a  X ,  X  and  X ,  X  or the  X ,  X  or a  X ,  X  or  X . For Portuguese we used  X  eo  X ,  X  eum  X , X  ea  X ,  X  e uma  X ,  X  edo  X ,  X  eda  X ,  X  e  X ,  X  ou o  X ,  X  ou um  X ,  X  ou a  X ,  X  ou uma  X ,  X  ou  X .

We conducted evaluation of both  X  X and  X  for English and Portuguese over all pairs of sets P ( i )and N ( i ) obtained for each language. Each P ( i )sethaduptoamaximumof 20 elements and each N ( i ) set had exactly twice as many elements as the corresponding P ( i ). Due to the combina-torial nature of the evaluation procedure we restricted the size of the seed sets to only two elements. For each pair of P ( i )and N ( i ), we generated all possible seed combinations of 2 elements from P ( i ). Then, for each seed combination, S k we started by obtaining S k , the vector representation of S k in the corresponding vector space ( VS or VS X ). Next, we used  X  and  X  X to compute the degree membership of each of the candidate element e ( e  X  X  ( i ) its representation in the corresponding vector space ( VS or
VS X ). We proceeded by ranking all candidate elements according to the previously computed value of membership and computed the Average Precision (AP) of that ranking. Finally, Mean Average Precision scores were computed using all values of AP obtained for each seed combination.
The first two rows of Table 2 contain the average MAP values for  X  and  X  X taken over the complete test sets (3219 for English and 75 for Portuguese). For both languages the average performance obtained by  X  is considerably higher than the performance obtained by  X  X (+0.135 for English and + 0.116 for Portuguese). According to the one-tail sign test the improvement is significant in both cases ( p&lt; in both cases). For Portuguese the results for both  X  and are considerably better than for English. One explanation is that the threshold used for selecting the elements included in the set of Positives, P ( i ), was the same for English and Portuguese ( f min = 100) and, thus, relatively higher for Por-tuguese taken the relative sizes of both Wikipedias. There-fore, the resulting test sets for Portuguese contain relatively more frequent elements, benefiting corpora-based methods such as ours.

In order to further assess the impact of frequency val-ues in the performance of  X  , we developed additional test
Table 2: Average values of MAP over all test sets sets for Portuguese by choosing the least frequent elements from P cand ( i ) (keeping f min = 100). The corresponding sets of negatives, N ( i ) were kept the same. There are only 28 cases in which the new tests are actually different (i.e., #
P Cand ( i ) &gt; 20 elements). We will denote the 28 new test sets as P  X  28 . We compared these with the corresponding 28 sets used in the previous experience, which we will denote as P 28 .Thevalueof  X  f avg (average frequency of elements, averaged over all test sets) for P  X  28 dropped to 189.4, which clearly indicates that we are now dealing with sets of Pos-itives containing much less frequent elements than in the previous experiment. On the other hand, the corresponding  X  f avg value for P 28 increased to 982.2. We repeated the evalu-ation procedure on the newly created test sets, P  X  28 and The results are shown in two bottom rows of Table 2. The performance of both  X  ,  X  X decreases for the test sets ( N ). The decreases occur consistently over the 28 test sets (test sign: p&lt; 0 . 01 for  X  and p&lt; 0 . 0001 for  X  X for  X  X is more significant, but was expected since the asso-ciated vector space VS X PT is much smaller than VS PT and, thus, the probability o f not finding vector representation for some elements in the test set has increased. Moreover, the performance of  X  and  X  X is the highest on the test set ( N ), which has the highest average element frequency. This indicates that the performance of the membership functions improves as the frequency of the elements to which they are applied increases.
We presented a corpus-based method for the class ex-pansion task based on a class membership function, esti-mated from statistics of co-occurrence of named entities in coordination constructions. We also presented an evalua-tion framework based on entity lists automatically extracted from Wikipedia. We showed that the performance of our method improves as the frequencies of the candidate entities in the text corpus increase (which are related to the corpus size). In future work, we will experiment other association measures for building the vector spaces (e.g. log-likelihood ratio, mutual information). We also plan to compare the performance of our method with [3] and with Google Sets.
This work was partially supported by grant SFRH/BD/ 23590/2005 from FCT (Portugal), co-financed by POSI, and by the Netherlands Organization for Scientific Research (NWO) under project numb ers 220-80-001 and 612.000.106.
