 Natural Language Processing (NLP) techniques are believed to hold the potential to assist  X  X ag-of-words X  Information Retrieval (IR) in terms of retrieval accuracy. In this pa-per, we report a natural language based IR approach where the common syntactic structures between documents and the query is regarded to as a query-dependent feature for documents. Specifically, a  X  X tructural weight X  is proposed for query terms, which can be seen as a weight to model the degree of term X  X  involvement in the common syntactic structures. This structural weight is used together with the TF-IDF weighting scheme, which results in a new ranking function. The accumulation of this structural weight of all the query terms in the new ranking function will be seen as a measure of how much a document and a query share the common syntactic structures. The experimental results show that by using this ranking function, significant im-provements in the retrieval performance are achieved. H.3.3 [ Information Search and Retrieval ]: Search Pro-cess; H.3.1 [ Content Analysis and Indexing ]: Linguistic processing Design Information Retrieval, Ranking, Dependency Tree
In the line of research of utilizing syntactic structural in-formation of natural language to assist the purely statistical IR ( X  X ag of words X  IR), much research work has been done at the syntactic level, to use more accurate and meaningful syntactic phrases or patterns to supplement the single terms based document representation. Our approach [ ? ],  X  X yntac-tic Information Retrieval X  (SIR), aims to maximally capture common syntactic structures in the context of dependency trees (D-trees), and model the common syntactic structure as a document feature in the ranking stage. Figure ?? shows the SIR architecture. Serving as a benchmark, SIR remains all the structures and features where a  X  X ag of words X  X R sys-tem has. Moreover, syntactic structures of natural language appear and are manipulated in almost every procedure in a standard retrieval pipeline in SIR. In parsing ,allthedoc-uments will experience a full text syntactic analysis where syntactic structures are represented by D-trees. D-trees are  X  pruned  X  by removing functional words that also appear as leaves. In indexing , D-trees will be retained in a separate file (called T-sequence file) and the inverted index is revised to link to the T-sequence file. In ranking , the query will be also parsed into D-trees. Syntactic structures will be modeled in this stage to assist the  X  X ag of words X  IR which is the main contribution of SIR.
In SIR D-tree is chosen as the syntactic representation of the natural language text. A dependency relation holds between a head word X and a dependent word Y in a D-tree. A dependency link is an arrow pointing from a head toadependent:X-&gt; Y. A D-tree is a set of links connecting Figure 1: Syntactic Information Retrieval (SIR) Overview man heads to dependents which can easily form a tree with the main verb as its root. In Figure ?? , the left tree is the D-tree of the sentence  X  X he man caught the butterfly with a net X . The right tree is the D-tree after pruning by removing all the leaf nodes that also have the role of dependent (most of the cases, functional words), in this example,  X  X he X  and  X  X  X .
First, a sequence data structure which stores a tree with all the structural information retained is designed. We call it a T-sequence . This is the basic for storing a D-tree in the index. As we can see in Figure ?? , a T-sequence is actually based on an array of paths/edges in the tree, with each edge starting from the child to the parent. Taking the right tree in Figure ?? as an example, we have edges/dependency rela-&lt; net with &gt; which form the base of its T-sequence. In an edge such as &lt; man catch &gt; ,  X  X an X  is the child (dependent) and  X  X atch X  is the parent (head). All the edges are stored in its child X  X  order in the preorder traversal sequence of the tree. Additionally, two offsets are added, the number of tree nodes ,and the root node , in the beginning of the T-sequence. Figure 3: T-sequence: a sequence representation for atree In the example, it is  X 5 X  and  X  X atch X . Given the T-sequence, a unique tree can be generated by linking each child to its parent based on the information in edges.

Considering SIR approach of utilizing the syntactic struc-tures during the ranking stage, we need to additionally re-alize the mapping from documents to their syntactic struc-tures on the basis of the inverted index. We maintain a file, called T-sequence file , which is designed to be a collection of the pruned D-trees generated from a document collection in the form of T-sequence. All the D-trees generated from each document are grouped sequentially together. The start offset in the T-sequence file for each document is marked and kept. In the inverted index, the structure containing all the information about a document is extended by one attribute, which contains the offset in the T-sequence file wherealltheD-treesaretobestoredforthatdocument.
 Thus, given a document, it is able to trace their syntactic structures in terms of D-trees by seeking to the start offset in the T-sequence file.
In the ranking stage, a document is scored by adding the weights of the query terms it contains. The process of adding in contributions to the ranking score of a document of one query term at a time is known as term-at-a-time scoring or accumulation [ ? ]. The term frequencies, document frequen-cies, etc. which serve as document weights, are the most common features for characterizing the documents. The ranking function is demanded to be able to accumulate ev-idence of a document X  X  relevance from multiple sources [ ? ]. The novel features of SIR is to regard the common syntactic structures as a query-dependent document feature. This fea-ture appears in the ranking function as the proposed Struc-tural Weight, short for SW .
Taking the idea of both considering the document and query in the using of syntactic information in IR, the pro-posed SW will be computed only for the terms which occur in both the documents and query. The SW of the query terms that do not occur in the documents is actually zero since this kind of terms do not involve in any of the common syntactic structures. The SW of a query term can be seen as a weight to measure the degree of this term X  X  involvement in the common syntactic structures in a document-query matching. The accumulation of the SW of all the query terms in the ranking process will be seen as a measure of how much a document and a query share the common syn-tactic structures.

In a matching between a document and a query, we will Figure 4: a pair of DT i and QT i ,contextforcalcu-lating isw have a set of dependency trees from a document by reading from the T-sequence file in the SIR index, denoted as DT() . Also the query is parsed, resulting in a set of D-tree from the query, QT() . For one of the query term t ,in DT() and QT() , we will have the D-trees or subtree of the D-trees DT 1 , DT 2 , ... DT n and QT 1 , QT 2 , ... QT m ,which are all rooted at t .Each DT i and QT j will group a pair of D-trees. In the context of each individual pair, we will consider an Individual Structural Weight (isw) for term t , which represents t  X  X  involvement in the common syntactic structures in this particular DT i and QT j match. The SW of t is the sum of all the isw of t computed from all n  X  pairs. We can see SW is actually is a sum number. It can be obtained by summing up of all the isw .

The definition of isw is inspired by the kernel concept of summing up of all the common substructures as a measure of the similarity between discrete structures [ ? ]. A tree kernel function is able to measure the similarity between trees by counting the number of their common subtrees. In order to find a term X  X  involvement in all the common syntactic structures between a document and a query, we think it is reasonable to count all the common subtrees/syntactic structures which are all rooted at a query term as a measure. The isw of a query term t is defined in a pair of document D-tree DT i and a query D-tree QT j as the number of all the common subtrees rooted at t .Figure ?? gives an example of a pair of DT i and QT j rooted at a query term  X  X atch X . Since tree kernel methods count all the common subtrees as a measure, including the common subtrees rooted at any possible nodes in the trees, isw can be counted by extract-ing the subset of the results set computed from tree kernel methods which are all the common subtrees rooted at the root t . Partial Tree (PT) Kernel [ ? ] which allows most general subtree is used to define the isw of a query term.
We revised PT kernel algorithm so it only counts the num-ber of common subtrees for the root node and is used to form thebaseof isw .Figure ?? shows the isw of  X  X atch X  in the context of the two trees in Figure ?? . isw of  X  X atch X  X s 6 and counts the common subtrees: [catch [ butterfly ] ], [catch [ with ] ], [catch [ with [net] ] ], [catch [ butterfly with ] ], [catch [ butterfly with [net] ] ], and [catch]. For isw ,we also count the term node itself as a common subtree. This comes empirically from our ad hoc experiments, the com-bined use of idf and isw that counts the node itself in the ranking function (Equation (2)) leads to the best retrieval performance (better than isw which does not count itself). By using the same principle of computing isw ,the isw for other terms in pair of D-trees are  X  X atch X : 6;  X  X utterfly X : 1;  X  X ith X : 2;  X  X et X : 1. We can see that  X  X an X  and  X  X oman X  has no isw or their isw are 0 as neither of them occurs in both the document D-tree and query D-tree.
The ranking score of a document in SIR has two contri-butions, one is from the traditional statistical retrieval in terms of statistical term weights, the other is from the com-mon syntactic structural meas ure in terms of the proposed structural weights for query terms.

We use a instantiation of BM25 weighting function, known as QACW (query adjusted combined weight), as the repre-sentative of the statistical based retrieval model [ ? ]. We refer to the score computed for each query term t i with regard to a document  X  X  X  as  X  vsm ( d, t i ) X : where qtf t i is the term frequency of the query term in the query; tf t i ,d is the term frequency of the query term in the document d; idf t i is the inverse document frequency of the query term t i ; k 1 and b are two parameters where k 1 =1.2 and b =0.75; L is the length of the document and ave is the average length of all the documents in the documents collection.

For a query with the query terms t 1 , t 2 , ... t n ,wehave the ranking function in the Equation (1) from below, which serves as a benchmark of statistical IR model:
On the basis of the ranking function (1), we bring in the Structural Weight of the query terms. The Structural Weight is accumulated on the basis of vsm ( d, t i ), which re-sults in the ranking function SIR ( d, q ) (2) in our approach: where vsm ( d, t i ) is the score computed under the vector space model for the query term t i and the document d; idf is the inverted document frequency of t i , sw ( d, t i )isthe structural weight of t i with respect to the document d and is computed on the basis of the partial tree kernel. Since SW Table 1: Topics in different length querying dataset-1 of a query term is the number of all the common subtrees rooted at the term including the term itself, it has the same nature as the term frequency. Therefore, it is put into the ranking function in combination with idf . However, the per-formance improvements made by ranking function (2) is all contributed by the adds-on factor idf t i  X  sw ( d, t i ), since the replacement of idf t i  X  tf d,t i does not make any improvements at all in our IR experiments. There may have more than one ranking function under our approach, the one in Equation (2) is the one leading to the optimal retrieval performance in our experiments.
We run the ad hoc retrieval experiments in a prototype system that is developed to implement all the concepts and processes under SIR that described in the previous sections. Due to the fact that SIR requires extensive computation as a result of full syntactic analysis for each of the documents, we therefore have designed an ad hoc retrieval experiment on the scale of 1000 documents with a set of 50 queries. The documents are from the Wall Street Journal and Associated Press newswire sections of TREC disk 2. The topics are in the range of 251-300. We have two dataset, dataset-1 and dataset-2, each of both contains 1000 documents. We refer the retrieval results produced by the benchmark rank-ing function (1) in the SIR prototype as  X  X SM X , while the results under SIR approach is denoted as  X  X IR X .

We first conduct an ad hoc experiment on the dataset-1by querying the TREC topics in different lengths. A TREC topic has three fields &lt; title &gt; (shortest, two or three words), &lt; desc &gt; ,and &lt; narr &gt; (longest, a couple of sentence). From Table 2, we can see SIR works better than the benchmark with respect to longer queries ( &lt; narr &gt; , which is believed containing more syntactic structures) in terms of Mean Av-erage Precision (MAP) (0.4204 &gt; 0.3765). On the other hand, SIR does not make much difference in processing short queries ( &lt; title &gt; , &lt; desc &gt; ).

ThenwefocusonTRECtopicsintheir &lt; narr &gt; fields in the rest of experiments. We give a comprehensive evalua-tion of the proposed SIR approach in the ad hoc IR experi-ments by a range of evaluation measures. P(5), P(10), and P(100) is the precision of the top 5, 10, 100 documents in the ranked list respectively; P@(R=100%) is the precision at the point where all the relevant documents are retrieved (100%). We first report the ad hoc retrieval results produced on the dataset-1. Then we compare it with the results gained on the dataset-2 to verify the consistency and effectiveness of the ranking function (2) in SIR.

From Table 3 and Table 4, in terms of all the evalua-tion measures, SIR achieves better retrieval results than the benchmark on both datasets. In order words, the use of the ranking function (2) where the syntactic structures are measured in the form of SW, outperforms the use of ranking function (1) which represents a  X  X ag-of-words X  IR approach. Table 2: SIR vs. Benchmark (VSM) on dataset-1 Table 3: SIR vs. Benchmark (VSM) on dataset-2 SIR significantly improve the retrieval performance in terms of MAP by 11.6% and 9.8% on the two datasets respectively. Moreover, SIR makes improvements over all precision levels consistently and particularly makes more contribution in the high ranking documents. It also yields a high increase of im-provement on the high recall (a 15.1% increase and a 14.8% increase in terms of the precision at the level when recall achieves 100% on the two datasets).
Ad hoc IR experiments prove that the SIR approach works better with respect to longer natural language queries. SIR does help to improve the retrieval results compared with the statistical  X  X ag of words X  retrieval alone. It makes improve-ments over all precision levels consistently and in particular makes a great contribution on improving the precision in the high ranking documents. However, although SIR yields a significant improvement in the range of 9%-12% over the statistical IR model in regard to queries with meaningful and standard syntactic structures. It does not help much if the query contains no or less syntactic structures.
The author would like to thank Professor Alessandro Mos-chitti, Department of Information Engineering and Com-puter Science at University of Trento, for allowing access to and use of his Partial Tree Kernel source code. [1] Liu, C. Exploring and measuring dependency trees for [2] Christopher D. Manning, Prabhakar Raghavan and [3] Haussler, David. Convolution kernels on discrete [4] Alessandro Moschitti. Efficient Convolution Kernels for [5] Karen Sp Lrck Jones, Steve Walker, and Stephen E.
