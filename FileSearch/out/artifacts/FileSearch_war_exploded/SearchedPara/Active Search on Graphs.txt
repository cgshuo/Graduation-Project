 Active search is an increasingly important learning problem in which we use a limited budget of label queries to dis-cover as many members of a certain class as possible. Nu-merous real-world applications may be approached in this manner, including fraud detection, product recommenda-tion, and drug discovery. Active search has model learning and exploration/exploitation features similar to those en-countered in active learning and bandit problems, but algo-rithms for those problems do not fit active search.
Previous work on the active search problem [5] showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selec-tions to be made and proposed a truncated lookahead heuris-tic. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs. We suggest selecting points by maximizing a score considering the potential impact of se-lecting a node, meant to emulate lookahead while avoiding exponential search. We test the proposed algorithm empir-ically on real-world graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps.
 H.3.3 [ Information Search and Retrieval ]; H.2.8 [ Database Applications ]: Data mining Algorithms Active Learning, Graph Search
Many learning applications consider a large amount of un-labeled data for which we would like to obtain labels, but it is too expensive to collect them all. These applications have led to increasing interest in active learning algorithms that choose data points for labeling with the goal of optimizing a criterion based on the accuracy of the model learned from the chosen points. A typical algorithm builds a model from the labels already collected and iteratively uses it to select the next point for labeling that is expected to most improve the model.

In this paper, we focus instead on the active search prob-lem [5], where we seek points belonging to a certain positive class. Although we will still build a predictive model from the selected points, and may choose points to improve our model X  X  accuracy, we will ultimately be evaluated only by how many positives we find among our queried points. Many real-world applications are active search problems, including drug discovery (where  X  X ffective drugs X  are the sought class) and product recommendation (where  X  X urchased products X  are the sought class). In these examples, an accurate model is only useful if we can use it to locate more members of the desired class. We get no credit for model accuracy itself or correctly predicted labels themselves.

Although active search applications appear with many dif-ferent types of data, here we restrict our attention to graphs, where the graph structure is known but labels on nodes are expensive to collect. There are many interesting applica-tions of active search in graphs. In a marketing application, targeting a given individual might be quite expensive, but a social network might be available to infer the tastes of as-of-yet uncontacted users. A company might analyze a network of financial transactions in order to discover fraudsters, but investigating a particular selected entity is expensive. An academic or analyst might like to find papers on a particu-lar subject in a citation graph without having to read too many of them.

One might expect typical active-learning algorithms to be appropriate for active search as well because they can produce a good model that can be used to find positives. However, in active search a good algorithm must trade off the need to exploit (use the current model to collect posi-tives) against the need to explore (develop a better model to more accurately guess the positives in future selections). A traditional active learning method would focus entirely on exploring and only collect positives by accident. There-fore, strategies of a different nature are required. The explo-ration/exploitation feature of the problem might lead one to consider bandit algorithms for this task. This seems promis-ing except that in active search an algorithm can not repeat-edly make the same choice to collect more reward. Once it finds a positive, it must move on and look somewhere else for another one.

As is typical with active learning and bandit-style prob-lems, the optimal active learning solution, in general, re-quires an intractable lookahead search over an exponential number of possible future queries and label outcomes. An algorithm based on evaluating the expected utility over a truncated lookahead has been proposed, and good empirical results have been obtained by using a smart pruning strategy that, in some cases, reduces the cost by orders of magnitude and makes longer lookaheads possible [5]. In the same work, it was proven that arbitrarily better performance can occur with even one further step of lookahead. In empirical ex-amples, it seems that much better performance is available from looking ahead much further than is possible even with smart pruning.

Many successful algorithms for active learning and ban-dit problems do a myopic or 1-step evaluation of a well-crafted surrogate objective rather than directly optimizing expected utility. Inspired by their successes, we propose such a method for active search in graphs. We use a soft-label model for graphs, which attaches a  X  X seudonode X  to each original node that holds the observed labels. For our surrogate objective, we propose the probability of a positive (the exploitation) plus a measure of impact based on the number of additional positives likely to be identified (the exploration). Both the model and the impact factor can be efficiently computed using incremental updates to the model matrices. We compare our method to uncertainty sampling, a modified UCB algorithm, and a previously proposed model for graphs. On three real-world graph datasets, our method outperforms all the others.
There has been much research in the area of semi-supervised learning, where the setting is the learning algorithm re-ceives both a labeled training set and a set of unlabeled test points, and the objective is to predict the labels of the test points. Semi-supervised learning algorithms leverage the structure of unlabeled data during training to improve learning performance. Most of these works have been fo-cused on achieving good classification with partially labeled data. In [10], the authors propose a Markov random walk based algorithm to classify unlabeled points using the infor-mation of labeled ones as well as the graph structure. The authors adopt two techniques, maximum likelihood with EM and maximum margin subject to constraints, to estimate the unknown parameters that indicate the distribution of each data point over the class labels. In [11], the authors propose a semi-supervised label learning method which is based on the Gaussian random field model. The mean field is char-acterized by a harmonic function, and can be efficiently ob-tained by matrix computation or belief propagation. In [6], the authors adopt a relational active learning model to im-prove both model estimation and prediction after acquiring a node X  X  label. They propose a model which combines a network-based certainty score with semi-supervised ensem-ble learning, as well as relational resampling to utilize both the local relational dependency and sufficient global vari-ance. In [2], the authors analyze the stability of several transductive regression algorithms, where the problem set-ting is similar to that in semi-supervised learning. There also has been some work on efficient semi-supervised learn-ing, such as [3]. In this work the authors try to apply semi-supervised learning on 80 million images gathered from the Internet, with  X  X lean labels X  manually obtained on a small fraction. The authors have been able to obtain highly effi-cient approximations for semi-supervised learning that are linear in the number of images, compared to traditional methods that scale polynomially with the number of images.
Active graph search involves an exploration and exploita-tion dilemma, where the Upper Confidence Bound (UCB) algorithm [1, 9, 4] is a popular method of addressing this is-sue. The basic idea of UCB in multiarmed bandit problems is to sum the current estimate about the reward of each arm and the uncertainty about that arm. Choosing arms with a high expectation corresponds to exploitation and choos-ing those with high uncertainty corresponds to exploration. UCB is appealing because it comes with regret bounds but the setting is too confined to be used in the active search problem. UCB intends to repeatedly select good arms while the active graph search problem does not allow repeated se-lections. In [8], the authors propose contextual bandits with similarity information. We could use the graph structure to provide such information. However, this would not change the fundamental problem with bandit approaches for active search, which is that we will never select the same node more than once.

There is a more subtle issue as well. Ideally, the explo-ration component of an algorithm would optimize some mea-sure of information gained from a label. In a traditional independent-arm bandit problem, this is easily replaced by the uncertainty for a particular arm because the informa-tion gain is confined to that arm. When a Gaussian pro-cess model is used in a bandit problem [9], information is spread throughout the model. Because of the symmetric and homogenous properties of typical kernels, the informa-tion gain for sampling at a point can again be substituted with the current model uncertainty at that point. Typical graph models offer no such easy way out. The potential information to be gained by choosing a hub can be much larger than that of choosing a disconnected singleton even if the latter is much more uncertain. This property motivates the impact factor in our proposed method.
Here we formally define a binary graph active search prob-lem. We are given a finite set of n nodes, indexed { 1 ,...,n } , which have an unknown set of labels Y = { y 1 ,...,y n } where y  X  X  0 , 1 } and we want to identify the nodes for which y i 1. We are given a corresponding weight matrix W = [ w ij where w ij indicates the strength of the relationship between y and y j . Initially all nodes belong to the unlabeled set, U . At each iteration we choose a node i , find out y i move node i to the labeled set, L . Our performance after k iterations is the sum of the y i in L .
We begin by considering models for predicting the un-known values of Y . In [11], the authors propose a harmonic function f to represent their predictions, which minimizes the energy function E ( f ) = 1 2 P i,j w ij ( f ( i )  X  f ( j )) setting the derivative to zero we can get f = D  X  1 Wf , where D is the diagonal matrix with entry D ii representing the de-gree of node i , i.e., D ii = P j w ij . Separating the labeled points f l and the unlabeled values f u , and also the corre-sponding W matrix and D matrix, we get a more explicit form of f for unlabeled points: In practice we do not need to do the expensive matrix inver-sion. We can approximate f u by iteratively multiplying an initial value f by the matrix D  X  1 W and update only those entries in f u until convergence.

Each entry f i in the harmonic function is an indicator of the probability that a random walk starting from node i will hit a label 1 before it hits a label 0. However, this model has some problems, especially for active search. Suppose we first discover the hub node i of a star structure with label y = 0, which is connected to many nodes with label y j = 1 in its immediate neighborhood N i (node j  X  N i if w ij &gt; 0). Discovering any number of nodes with label y j = 1 in N will never increase any remaining element of f u from N since a random walk will always stop at the 0 label of node i . Figure 1 shows an example of this hub-blocking problem, where target nodes (with y = 1) are shaded solid.
 Figure 1: An example showing an original graph (Left) and the soft-label graph (Right)
In the original formulation of the harmonic function model [11], the resulting f i = P ( y i = 1 | L ) indicates the probability that a random walk starting from node i will hit a label 1 be-fore it hits a label 0. Hence hitting a label 0 will effectively end the random walk and assign a label 0 to the starting node i , which is the main cause of the hub-blocking problem mentioned above. We can resolve this issue by changing the stopping criteria to be indeterministic, i.e., we add a proba-bility  X  to the random walk, such that when it hits a labeled node, it stops with probability  X  , and with probability 1  X   X  it ignores the label and continues the random walk. More specifically, we attach a pseudo node to each labeled original node i to hold its label, and use the edge weight between the pseudo node and the original node to adjust  X  .

The harmonic function and the soft-label model can both take advantage of the structure information in the graph, but they utilize labeled information in different ways. Given a certain query node, the harmonic function is only able to use the labeled information from those nodes that have a path to this query node, and this path is not allowed to have any other labeled nodes on it (otherwise the labeled nodes would already have blocked the path). While the soft-label model can effectively utilize the labeled information from all the nodes in the graph, with the advantage that the closer labeled nodes have higher influence on the query node than the labeled nodes farther away.

Leaving f as the estimate of the original nodes associated with a label 1 and letting x l represent the labeled pseudo nodes (with entry 0 for unlabeled nodes), we get: where W is the original weight matrix, and D l is an n  X  n diagonal matrix with which indicates that there is a transition probability  X  from a labeled node i to its labeled pseudo node. D  X  is also a for i  X  L , acting as a row normalizing factor.
It is often useful to include prior information on labels and we can attach a pseudo node to the unlabeled original nodes for this purpose. We set the pseudo node label to be the value of the prior. The weight of the attached edge represents the strength of the prior. We set the weight of node i to be  X  0 D ii , where  X  0 is the strength, and D the degree of node i . By a similar derivation as above and absorbing the row normalizing factor we get: where Here x is a predetermined vector with labels in the entries corresponding to labeled nodes, and a value  X  for the prior in the entries corresponding to unlabeled ones. f will be a vector we want to compute, with f i indicating P ( y i = 1 | L ) for all the nodes, but we only care about those entries i with i  X  U .

A similar model would be the Cortes model [2], which is a generalization of the Gaussian Mean Fields model [11]. This model also has a kind of  X  X oftening X  and we could plug it into our method, but we prefer the way the priors in our model give a smooth transition of values going away from labeled nodes.

Adding a prior in the model has several advantages. First it enables the model to distinguish between nodes not con-nected to any labeled nodes and nodes that are connected to 0-label nodes. Second, it localizes the active search, which means we would rather first search the closest neighborhood of a node with label 1. Imagine a large connected component in the graph that has only one labeled node which is pos-itive. Using either the harmonic function or the soft-label model will result in the same f score for every node left in this component. After adding a prior we can get relatively higher scores in the neighborhood of this positive node, and the scores gradually decrease for the nodes farther away from it.
We propose a selection criterion with the following form: where f ( t ) i indicates the model X  X  prediction for node i after seeing t labels, IM ( t ) i is the expected impact on future pos-itives found by choosing node i now, and  X  is a parameter trading off exploration and exploitation. At each iteration, we evaluate score i for all unlabeled nodes i , and choose the node with the highest score.

There are many possibilities for defining IM. The entropy in f i would be an obvious choice, however that does a poor job of capturing how much effect node i has on the rest of the graph and especially how much it will increase the number of positives we find in the future after observing y We can consider P i  X  U f i as an indication of the number of positives we will find in the future. Therefore, we propose to explicitly condition on the expected value of y i and measure its potential to increase values of f in the unlabeled part of the graph. We propose: where Using f vector as before with each entry f i representing P ( y i = 1 | L ), we have an equivalent form: where f is the original prediction for each node and f 0 is the prediction conditioned on adding node i to the training set with label y i = 1.

Note that we do not condition on seeing y i = 0. Intuitively you might want to set up the impact criterion to marginal-ize over the unknown outcome. However, doing that would correspond to estimating the change in expected number of positives in this neighborhood under the assumption that your policy will continue choosing nodes in this neighbor-hood even if it sees a negative outcome. Of course this is not the policy we will follow. If a negative is observed, the policy will move to some other part of the graph. By doing it the way we propose we are representing both the unknown outcome and the decision that will follow (i.e. to continue choosing nodes in this neighborhood or not).

This impact factor is clearly heuristic and computing the true future expected increase in positives chosen is just as computationally intractable as implementing the full opti-mal policy. However, this definition of IM is able to tractably imitate a full look ahead by computing the full impact over all the nodes in the graph through the model. An example is enlightening. Imagine a graph of many separate compo-nents, each of which is a clique of widely varying size. A smart exploration algorithm would take samples from the cliques in descending order of their sizes. Observe that a truncated lookahead of k steps is only able to distinguish the value between cliques of size less than k . All cliques of size k or greater will look equal to the truncated look ahead algorithm. Such an algorithm will explore somewhat randomly until there are only cliques of size smaller than k left and suffer poor performance as a result. Our proposed IM, however, will exactly give all the nodes scores in propor-tion to their clique X  X  size and it will make good exploration choices from the beginning.
Evaluation of the selection criterion requires repeated con-ditioning on single new label observations, which would re-quire O ( n 3 ) time if we apply eq. 1 naively. Here we show two methods to make this computationally more efficient.
We can reduce the computation by following the efficient update procedure suggested in [12]. When we add only one label to the graph, only one row of matrix A and only one entry in the diagonal matrix D 0 will be affected. Denote the original matrix inverse as  X   X  1 = ( I  X  A )  X  1 , and the new inverse after one row is changed as ( X  0 )  X  1 = ( I  X  A 0
According to the matrix inversion lemma, the new inver-sion ( X  0 )  X  1 is given by: where e is a column vector with all entries 0 except the i th entry set to 1. Here we use ( i, :) to represent the i th row of the matrix, (: ,i ) to represent the i th column, and r denotes (1 +  X  0 )(1  X   X  ). If we precompute  X   X  1 , each time we add a label in the graph, it takes O ( n 2 ) to get the new inversion ( X  0 )  X  1 , where n is the number of nodes in the graph. We can also efficiently update f after querying node i , when we get its label I ( y i = 1). To update f , we first have the following equations (denote s =  X I ( y i = 1)  X   X  0  X  1+  X  Hence we can update f by: Using facts like f =  X   X  1 D 0 x , f i = A ( i, :) f + A ( i, :)  X   X  1 (: ,i ) is a 1  X  1 scalar so we can change the order of multiplications. Note here the denominator only multiplies the i th row of matrix A and the i th column of matrix  X   X  1 which only takes O ( n ). Then the equation only involves a column vector  X   X  1 (: ,i ) multiplied by a constant and addition of column vectors, which also takes O ( n ). Similarly, we can compute the impact factor efficiently after assigning a target label to each node we want to query, where the overall cost equals to querying all O ( n ) unlabeled examples, which is still O ( n 2 ). The complete algorithm is shown in Algorithm 1. In order to make the computation less expensive, [3] may offer an even faster alternative, but it is only an approximation to the Cortes model [2].
When the graph is very large, even the efficient updates of sec. 3.4.1 may not help because computing the original Algorithm 1 Active Search on Graphs Input:  X  0 , X , X  , precomputed  X   X  1 , budget.

Initialize the graph with one target and set its index to bestInd , initialize all entries in f (0) with  X  . Up-date labeled set L (0) = { bestInd } and unlabeled set
U (0) = { 1 ,  X  X  X  ,n }\ L (0) , t = 1. repeat until number of query equals to the budget inverse and/or storing the updated inverses is not realistic. For those cases, we propose label propagation techniques to compute the proposed methods in an accurate manner.
First under the soft-label model with prior information, the result is achieved by: f = [ A D 0 ][ f x ] &gt; = P [ f x ] gives f = ( I  X  A )  X  1 D 0 x . Instead of doing matrix inversion, we can initialize vector f with all zeros and multiply the matrix P iteratively to vector [ f x ] &gt; , and only update the entries in f until convergence.

To compute the impact, we need some approximation, and there are two ways of achieving that. Suppose node i is the node that we attach a pseudo label to, and we focus on the t th iteration.

The first approach is to recompute f 0 after adding a node with label 1. As before we have: f 0 = P 0 [ f 0 x x is obtained by changing x  X  X  i th entry from  X  to 1, P 0 is obtained by changing one row of matrix P = [ A D 0 ], D 0 in P is changed, if we initialize f 0 with the existing value of f ( t ) , f 0 will converge to the correct value after a small number of iterations of re-multiplying [ f ( t ) x 0 ] &gt;
The second approach is cheaper but less accurate. The idea is to compute the impact on its immediately connected neighbors after assigning an unlabeled node with label 1. In the first step, given the change of one row in P , we have: which results in In the second step, this change will propagate to node i  X  X  immediate neighbors. We can compute this change by:
To be more accurate we may compute the impact prop-agated not only to the immediate neighbors, but also to neighbors within two hops or even more.
The jump-to-label probability  X  depends on how we think each unlabeled node relates to the labeled nodes nearby. If we set  X  = 1 then the soft-label model without prior will degenerate to the harmonic function model [11]. Varying  X  is in some sense similar to varying the parameter k in a KNN model. If we are confident that the label of each unlabeled node should just depend on the nearest labeled node, then it is reasonable to use a larger  X  . However, if we would like to consider more nearby labels, we use a smaller  X  . In our experiments we set  X  = 0 . 5 and did not vary it. An alternative would be to use cross validation on a separate graph to find a good value.

In our experiments we found that the values of  X  0 and  X  do not affect the results much. However, the existence of  X  is crucial because without this parameter, matrix ( I  X  A ) can be non-invertible. The value of  X  has a large impact on performance because it controls the exploration/exploitation tradeoff. In our experiments we show results for a wide range of values. In the future work we discuss ideas for setting this parameter automatically.
Data. We demonstrate our approach on three real-world datasets.

The first dataset is a citation network with 14,117 nodes (papers) and 42,019 edges from citeseer, consisting of papers from the top 10 venues in Computer Science. The corre-sponding weight matrix has entry 1 if there is a citation link between two papers (undirected). The 1844 NIPS papers are labeled as targets.
 The second dataset consists of 5271 webpages related to Programming Languages from Wikipedia. The correspond-ing weight matrix has entry 1 if the two webpages i and j are linked together (also undirected). For each webpage we precompute its topic vector using the software available at [7]. We label webpages with topic  X  X bject oriented program-ming X  and related terms  X  X bject type class objects types classes method code languages programming X , etc. We set the threshold to be 0.4 to get a reasonable number of targets (202).

The third dataset is a graph built from 5000 concepts in the dbpedia 1 ontology marked as  X  X opulated places X . Each concept is a node in the graph and is backed by a Wikipedia page. We added an undirected edge between two places if one of their corresponding Wikipedia pages links to the other. The dbpedia ontology further divides populated places into  X  X dministrative regions X ,  X  X ountries X , ,  X  X ities X ,  X  X owns X  and  X  X illages X ; these five labels serve as class labels. 725 nodes labeled as  X  X dministrative regions X  are chosen as our targets.

Random subsets of the graphs are shown in fig. 2. The three graphs show quite different structures and distribu-tions of positive nodes. The citation graph has many small connected components, and the positive nodes are present in different connected components. The wikipedia graph has large hubs and cliques, and the positive nodes are mainly concentrated in one large component. The populated-place graph, however, is in between these extremes. It has large hubs and cliques, but also many small connected compo-nents, and the positives are also present in many different www.dbpedia.org connected components. This makes the active search task qualitatively different on the three graphs.

Baselines. We compare our approach with several base-lines. 1. Uncertainty Sampling. We use our proposed f func-2. Modified Upper Confidence Bound. UCB is not a nat-3. 2-step lookahead from [5]. Longer look aheads are too 4. Harmonic Function. We compute the f u for unlabeled
Experimental Setting. We perform 10 random tri-als of all methods using a single randomly chosen positive node to initialize each trial. We record the number of pos-itives found as a function of iteration number and aver-age over the 10 trials. We set  X  = 0 . 5 for both datasets,  X  is set to the true prior proportion of positives, and  X  is set to 1 /n , where n is the number of nodes. We test  X  = { 0 , 10  X  1 , 10  X  2 , 10  X  3 , 10  X  4 } .

Results. Figure 3 shows the performance of our proposed model (with its best value of  X  ) compared with the baselines on the three datasets. On all three datasets our proposed method is consistently better than all other baseline meth-ods. The closest competitor is the harmonic function on the citation and wiki dataset, while on the populated-place dataset, the 2-step lookahead method is the second best one. Our experiments show that the difference between our method and the closest competitor is statistically significant ( p &lt; 0 . 05 in a paired t-test) after roughly 1300 iterations in the citeseer data, 200 iterations in the wikipedia data, and 10 iterations in the populated-place data.

We also notice uncertainty sampling doing as well as sec-ond best in some places. This is because it is built on our soft-label model and the number of positives is very small. Therefore, it imitates our proposed method with  X  = 0 (the score falls below 0 . 5 after a certain number of iterations, hence picking the node with score closest to 0 . 5 is equiva-lent to picking the node with the highest score).

On the citation network, the gain of our proposed algo-rithm is quite substantial, with only 133 positives missed compared to 328 for the next nearest competitor, a 2.5-fold reduction. We have analyzed individual runs and observed that our method will effectively choose nodes in larger com-ponents and more connected portions of the graph first, which corresponds to a larger future gain. We also observe that our method effectively resolves the hub-blocking prob-lem (Sec. 3.2), which results in a better performance com-pared to the harmonic function.

The gain on the wiki data is smaller, though we again have the best performance. We select this dataset because it is a different type of graph, which consists of a large connected component with large hubs containing most of the positives and some small components (mostly negative). The only opportunity for better performance comes while exploiting the large connected component, which is why we see signif-icantly better performance at iteration 300. After that, all algorithms will complete the large component and be forced to search the small ones at random, hence the curves come together.

On the populated-place dataset, the gain of our proposed algorithm is even more substantial. We notice a large gain from the very beginning, and also through all the iterations till the end. When there are many small connected compo-nents with positive nodes in them, it is easier to discover those positive nodes first, hence harmonic function tends to exploit this information and repeat the procedure of ex-hausting the small components at first. However, our algo-rithm is more efficient at discovering the targets in the large component at the very beginning, thus causing a large ini-tial deviation and higher future gain. Then our algorithm will only turn to other places when it finds enough nega-tives in this large component. This means our algorithm can effectively explore the graph from the more  X  X ositive X  parts/clusters, to the less  X  X ositive X  ones.

The right figures in Figure 3 show the more detailed re-sults of carrying out paired t-tests among some of the com-petitive methods. We use the harmonic function as the base-line for comparison. The y-axis represents the difference The results are the averages and standard errors from 10 random trials. in the number of targets found by our model with varying  X  = { 0 , 10  X  1 , 10  X  2 , 10  X  3 , 10  X  4 } , compared to the harmonic function.

Table 1 further show the percentage of positives missed by our proposed method and baselines quantitively, with respect to the number of iterations (queries).
In this paper, we present a soft-label model for graphs that extends previous random walk style models and give efficient methods of conditioning these models. We propose an impact factor to be used as a criterion for node selection. The impact factor plays the role of encouraging exploration, which is often done in other settings using entropy, uncer-tainty, or variance. We point out that those concepts are not suitable for active search however, and show empirically that we achieve better performance using our proposed method.
Setting  X  remains an unresolved issue. An automated method might consider a budget, B , of remaining choices to be made and set it accordingly. A reasonable setting might be  X   X  ( B  X  X  L | ) /B , where | L | is the size of labeled set. In this way, as the size of the labeled set increases,  X  will au-tomatically decrease, corresponding to the natural strategy that in the beginning we want to explore more as we have more budget, and later we want to focus on exploitation.
Alternatively, we might follow the form of the UCB algo-rithms and determine an adaptive value of  X  that allows us to derive regret bounds. However, doing so may be challeng-ing given the fact that UCB methods are based on repeated pulls on  X  X est arms X , while active search does not allow same node selection.
 [1] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time [2] C. Cortes, M. Mohri, D. Pechyony, and A. Rastogi. [3] R. Fergus, Y. Weiss, and A. Torralba. Semi-supervised [4] A. Garivier and O. Cappe. The KL-UCB Algorithm [5] R. Garnett, Y. Krishnamurthy, X. Xiong, [6] A. Kuwadekar and J. Neville. Relational Active [7] A. McCallum. Mallet: A machine learning for [8] A. Slivkins. Contextual Bandits with Similartiy [9] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. [10] M. Szummer and T. Jaakkola. Partial Labeled [11] X. Zhu, Z. Ghahramani, and J. Lafferty.
 [12] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining
