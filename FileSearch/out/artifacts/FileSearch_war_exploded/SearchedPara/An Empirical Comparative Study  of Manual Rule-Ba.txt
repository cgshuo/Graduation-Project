 Question Classification (QC) plays an important role in various natural language applications, especially in question answer ing (QA) domains, such as Factoid Ques-tion Answering [1-3] and Community Question Answering [4]. According to [5], 36.4% of the Factoid QA errors were derived from question classifiers, which influ-enced the overall performance a great deal. 
The goal of question classification is to accurately classify a question into a ques-example, given a question  X  X here is the original site of the Olympics? X , if we can accurately classify the question as  X  X OCATION X  type, only answers labeled as  X  X O-CATION X  need to be evaluated. Thus, the accuracy of the system as well as the com-putation cost could be improved. 
Because of the importance of QC, a substantial number of works have addressed the issue in recent years. QC approaches can be classified into two broad categories, namely, rule-based approaches and statistical approaches. In rule-based approaches, usually domain experts produce a number of rules, which could be regular expres-sions, grammar rules, or just a set of phrases. By contrast, in statistical approaches, a and a model is trained in hopes that useful patterns for classification can be captured automatically. Most existing works are based on closed homogeneous data. To the best of our knowledge, there is currently no empirical comparison of rule-based and statistical approaches on heterogeneous unseen data as we did in this work. 
Many successful QA systems [6-8] rely on rule-based question classifiers, which seems to contradict the fact that nearly all recent QC works have focused on statistical approaches and regarded rule-based appro aches as impractical or high cost, unport-able solutions. To understand the reason fo r the contradiction, we compared the performance of a manually created rule-based question classifier and a statistical question classifier. We believe the comparison was empirical and unique because of the following characteristics. 
First, the experiments were conducted on unseen datasets collected after the ques-tion classifiers had been created. This eliminated the possibility that the question classifiers could have been influenced by the test data. Second, to test the strength of the question classifiers on heterogeneous data, we collected data from various sources, such as QA task organizers, QA task participants, and real QA system users. Finally, the approaches we adopted for the question classifiers in comparison have achieved high performance accuracy and have been used successfully in a state-of-the-art Chinese QA system [6]. The remainder of the paper is organized as follows. We review related work in Section 2, and introduce the compared question classifiers in Section 3. The develop-ment data and the heterogeneous unseen datasets are presented in Section 4. We de-scribe the experiments and report the results in Section 5. In Section 6, we discuss the experiment results; and in Section 7, we summarize our conclusions. Originally, most QA systems [9-11] used rule-based question classifiers with manu-ally created rules or templates (rule and template are interchangeable words in this paper) for all or some question types. However, as rule-based question classifiers were regarded as unportable, the research focus shifted to statistical (or machine learning) approaches. 
Extensive works on statistical question classification using various models and fea-tures have been reported in the literature. Studies have shown that classifying ques-tions semantically yields better factoid question-answering results than employing conceptual categories [12]. For example, using the Sparse Network of Windows (SNoW), Li and Roth [12] achieved over 90% accuracy. Meanwhile, many works have adopted Support Vector Machine (SVMs) as the machine learning method. Zhang and Lee [13] used SVM with only surface text features (bag-of-words and bag-of-ngrams) to derive coarse-grained categories with 0.86 accuracy and fine-grained categories with approximately 0.80 accuracy. By adding syntactic information, in-cluding sub-trees of the parse tree that has at least one terminal symbol or one produc-tion rule, Zhang and Lee achieved an accuracy rate of 0.90 for coarse-grained classes. words only; (2) words and named entities; (3) words and semantic information; and (4) words and NEs with semantic information. They measured on a question type hierarchy at different depths and achieved accuracy rates ranging from 0.95 at depth 1 to 0.75 at depth 4. 
Although statistical question classifiers have dominated research in recent years, most state-of-the-art QA systems in evaluation forums, such as CLEF, NTCIR, and TREC, still used rule-based question classi fiers as their main QC approaches. For example, it seems that Language Computer Corporation X  X  question answering system [8], which has been a top-performing English QA system at TREC conferences for many years, still uses a rule-based question classifier to classify questions and extract related semantic information. 
To the best of our knowledge, there have been very few comparative studies be-[15] compared a rule-based question classifier and a Ripper-based question classifier, both of which were created automatically; however, neither question classifier is [16] compared a rule-based question classi fier and an SVM-based question classifier; however, they only experimented on a closed development set, not on heterogeneous unseen data as we used in this work. Comparison of manual and statistical question classifiers needs to be handled care-fully because the development processes and resources (or features) are substantially different. By definition, creating a manual question classifier involves a great deal of manual effort, while the cost of a statistical question classifier mainly involves choos-ing proper features and machine learning models. On one hand, the features used for a manual question classifier are almost unlimited in that any features can be referenced for a set of rules or for a special case. However, such classifiers are usually developed in an ad hoc manner, so the quality of the classifier is less predictable. (We introduce hand, statistical classifier features are usually constrained by the adopted machine leaning model and the amount of training data. Moreover, the same set of features is applied to all the data without exception. The development process of a statistical question classifier is thus almost standardized and easy to follow. 
Because of the above differences, we believe that to make an effective comparison process must be considered. This because the development process plays a key role especially for manual question classifiers. We view our comparison in this work more like a case study than an experiment because only a few independent variables were controlled. 
The compared question classifiers were cr eated during the development of a state-of-the-art Chinese QA system. The development times of the question classifiers were almost the same (approximately six months). For the rule-based question classifier, there were two rule creators and one developer. Most of the time was devoted to de-veloping rules. For the statistical question classifier, there was one developer, who used the training data labeled by the above two rule creators. In this case, most of the time was expended on feature engineering, which involved choosing an optimum set grained classes or 62 fine grained-classes. 3.1 Test-Assisted Rule-Based Question Classifier of a rule-based question classifier as a knowledge engineering 1 process, which needs guidelines and tools to minimize the rule maintenance costs and maximize the cover-age and accuracy. 
We used a knowledge representation framework called InfoMap with a test-assisted rule creation process. InfoMap consists of a knowledge editor and a matching engine that can extract important concepts from a natural language text [17, 18]. The framework is designed to represent and match complex templates, which are syntactic rules with semantic labels. A template is comprised of several parts, such as a string, a to another template, the number of distinct templates can be reduced. To enable hu-man experts to maintain rules safely and quickly, we adopted a test-assisted approach that was inspired by the test-driven development (TDD) 2 [19] concept in agile soft-expert will become a bottleneck that slows down the rule development process. 
All the templates and rules are maintained by InfoMap knowledge editor, which provides a hierarchical editing interface th at allows several domain experts to edit rules from the same server simultaneously. We also have a continuous integration 3 server that periodically checks whether any templates or tests have been modified. Whenever templates or tests are modified, a test process is triggered automatically and uses the matching engine to compare the rules with the  X  X ests X  to determine if any test cases have been broken. The server then provides feedback to the human experts. The feedback includes information about how many questions have been tested as that the human experts do not have to wait for the results and can continue with rule development. 
Human experts were asked to increase the question coverage (i.e., the number of training questions that could be categorized by the rules) while maintaining high ac-curacy. Some questions will not be covered if they cannot be co nfidently categorized by rules or the cost of creating rules for them is too high. 
Fig. 1 shows an example of QC knowledge representation in InfoMap. The two-layer question taxonomy is created in a hierarchical way. For example, the fine-grained category  X  X OCATION_CITY X  is a sub node of the coarse-grained category  X  X OCATION. X  Each coarse-grained and fine-grained question type node has two function nodes,  X  X AS-PART X  and  X  X ule X . The rules for a question type are stored under the  X  X ule X  node. To reduce the number of redundant rules, sub-phrases or sub-rules shared by more than one rule are described by rules stored under the  X  X AS-PART X  node. For example, the rule or template for the LOCATION_CITY question  X 2004  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ? (In which city were the Olympics held in 2004?) X  in InfoMap can be formulated as: [Time]:[Organization]:[Q_Location]: ([LocationRe-latedEvent]) under the  X  X ule X  node of LOCATION_CITY. This rule has four ele-ments (referenced from the nodes under  X  X AS-PART X ), i.e.,  X  X ime X ,  X  X rganization X ,  X  X _LOCATION X , and  X  X ocationRelatedEvent. X  They are stored by rules under the  X  X AS-PART X  node to match  X 2004  X  (Year 2004) X ,  X   X  X  X  (the Olympics) X ,  X   X  X  X  X  X  X  X  X  (in which city) X , and  X   X  X  (is held) X  respectively. 3.2 The SVM-Based Question Classifier We adopted SVM as the statistical (machine learning) approach as it has been the most popular technique used in question classification [13, 20, 21]. We used SVMlight [22] to train the question classifier with the following syntactic features and semantic features reported in [16] for Chinese questions. Syntactic features: Our SVM model incorporates two syntactic features: bag-of words (ngrams) and part-of-speech (POS). (a) Bag-of-Words: Character-based bigram and word-based bigram features were used. (b) Part-of-Speech (POS): We used AutoTag[23], a Chinese segmentation tool developed by CKIP, Academia Sinica, to obtain the POS of given Chinese questions, and then adopted the POS features for QC. Semantic Features: We used  X  X owNet X  4 to derive the semantic features of given Chinese questions. Our SVM model uses two semantic features: HowNet Main Defi-nition and HowNet Definition. 
We tuned the SVM-based question classifier on the development data with 10-fold cross-validation and then trained the final SVM models on all the development data. We collected question classification datasets from various sources, as shown in Table 1. were created, collected or tran slated after the question classifiers had been developed. A QC dataset consists of pairs of questions and standard question types. The standard question types were designated by the human experts and used to judge the correctness of the results yielded by the question classifiers in our experiments. 
The dataset used to develop the three question classifiers were collected in 2005, which contains distinct questions created by the NTCIR-5 CLQA organizers (300 questions for the C-J subtask and 200 questions for the C-C subtask) and 850 ques-tions created by a task participant. The test data comprised seven datasets: Q-TREC, Q-CIRB20, Q-NTCIR5T, Q-NTCIR6T, Q-CTS, Q-CorpusDemo and Q-WebDemo. The Q-TREC dataset was translated from some TREC QA track English questions. The Q-CIRB20, Q-NTCIR5T, and Q-NTCIR6T datasets were created from Chinese newspapers. Since most QA experiments are conducted on newspaper data, we created a new dataset these artificially created questions, we co llected real user questions posted on an online Chinese QA demo site (http://asqa.iis .sinica.edu.tw/) since 2005. The site tar-geted two types of answer corpora: a news corpus and a web corpus. We compiled the Q-CorpusDemo and Q-WebDemo datasets acco rding to the answer corpus that the question was posted on. We applied the heterogeneous unseen data me ntioned in section 4 to the InfoMap-based were even chosen at random from the same question pool; hence, we think the experi-ments were not empirical enough to test a question classifier X  X  robustness and portabil-accuracy depends on the granularity of the question types, we defined two kinds of accuracy to assess a question classifier X  X  performance: the coarse-grained accuracy and questions as either coarse-grained and fine-grained types respectively. 
As shown in Table 2 , both question classifiers fit the development data well with very high accuracy, but the accuracy declines on the unseen test datasets. Overall speaking, the InfoMap-based question classifier outperformed the SVM-based ques-tion classifier in terms of the mean accuracy . According to t-test, only the comparison of fine-grained accuracy were significant (p-value &lt; 0.037). As shown by the experiment results, surprisingly, the InfoMap-based question classifier performed better than the SVM-based question classifier on most of the heterogeneous papers, such as [21], that rule-based question classifiers lack robustness and portability. We found that when a given question contained unpredicted words, the InfoMap-based question classifier was more likely to get the right question type. Unpredicted words are words that do not exist in the training data; or they appear in the training data, but in a different context to that of the input question. 
To deal with unpredicted words, a statis tical question classifier could incorporate semantic features, such as the HowNet features we used in the SVM-based question categories for each question type from the training data. However, since the quality and granularity of the used semantic categories varies, some semantic categories may not be suitable features for the question domain; and if the model learns such seman-tic categories, its performance on unseen data could be impacted. 
The semantic categories used in the SVM-based question classifier could also be referenced in the rules created by the human experts. But unlike the SVM-based ques-tion classifier, experts could review and judge the content of the referenced semantic categories, remove noisy content, and merge or split categories into the appropriate granularity, thereby alleviating some of the unsuitable semantic category problem. 
In addition, we observed that, under the test-assisted approach (which associates human experts (to only create high accuracy and high confidence rules), the cost of creating the InfoMap-based question classifi er was reduced and the experts had more time to improve the quality and coverage of the rules.  X  X ule bugs X  could be detected quickly by running the  X  X ests X  after the introduction of new rules or improper modifi-cations. If  X  X ugs X  are not detected early, it could be much more expensive if the rules rules. 
With the test-assisted approach, rules are well-protected by the tests and the con-tinuous integration process, it is easier to maintain collective ownership so that every-someone else; thus, the incidence of bottlenecks and risks can be reduced. (The risk in this context is similar to the Bus Factor 5 concept in software development.) 
Although the rules of the InfoMap-based question classifier were created carefully with the tools and guidelines, some questions could not be categorized correctly, especially on the Q-TREC dataset. Our analysis of the results revealed three prob-lems: 1) The rules of some question types handled by InfoMap (such as DEFINI-TION) but not included in our experiments were not carefully handled, which resulted in false positive cases. This showed that, similar to software programs, every piece of rules involved in the computation should be well-maintained or separated. 2) The Q-TREC dataset was translated from English questions. Some of the translations were not appropriate, so the rules output the wrong question type. Since the SVM-based problem. 3) A number of unseen questions could not be matched with any rules. Some of them were deliberately ignored because the cost of creating rules for them would have been too high; and some were just not discovered by the human experts during the development phase. This is a preliminary work designed to investigate the real-world performance of rule-based and statistical question classifiers. The results demonstrated the potential of rule-based QC approaches. Because manuall y created classifiers are influenced by various factors introduced by the engineering work conducted by humans, method-ologies commonly used to compare machine-learning or statistical methods are not suitable for our work. Therefore, we used post-hoc empirical experiments with het-erogeneous datasets sampled from various sources. 
The statistical hypothesis tests showed that, even when dealing with heterogeneous unseen datasets, the InfoMap-based question classifier (a manually created rule-based classifier) performed significantly better than the SVM-based question classifier in terms of fine-grained accuracy. This finding suggests that with proper design, a rule-based method can handle real-world cases with a high degree of confidence. The InfoMap-based question classifier did not encounter any serious robustness and port-ability issues reported in many QC works. We attribute the success of the rule-based question classifier to the test-assisted de velopment process and the guideline whereby question coverage is traded in favor of high accuracy rules. This research was supported in part by the National Science Council of Taiwan under Grants NSC 96-2752-E-001-001-PAE and 95-2416-H-002-047, and the Thematic Program of Academia Sinica under Grant AS95ASIA02. 
