 Existing feature-based recommendation methods incorporate aux-iliary features about users and/or items to address data sparsity and cold start issues. They mainly consider features that are or-ganized in a flat structure, where features are independent and in a same level. However, auxiliary features are often organized in rich knowledge structures (e.g. hierarchy) to describe their rela-tionships. In this paper, we propose a novel matrix factorization framework with recursive regularization  X  ReMF , which jointly models and learns the influence of hierarchically-organized fea-tures on user-item interactions, thus to improve recommendation accuracy. It also provides characterization of how different fea-tures in the hierarchy co-influence the modeling of user-item in-teractions. Empirical results on real-world data sets demonstrate that ReMF consistently outperforms state-of-the-art feature-based recommendation methods.
Recommender systems aim to model user preferences towards items, and actively recommend relevant items to users. To ad-dress the data sparsity and cold start problems [23], feature-based recommendation methods, such as collective matrix factorization (CMF) [25], SVDFeature [4], and factorization machine (FM) [21], quickly gain prominence in recommender systems. They enable the integration of auxiliary features about users (e.g. gender, age) and items (e.g. category, content) with historical user-item interactions (e.g. ratings) to generate more accurate recommendations [24].
While commonly arranged in a flat structure (e.g. user gender, age), auxiliary features can be organized in a  X  X eature scheme X , i.e. a set of features that includes relationships between those features. Hierarchies are a natural yet powerful structure to human knowl-edge, and they provide a machine-and human-readable descrip-tion of a set of features and their relationships. Typical examples of feature hierarchies include category hierarchy of on-line prod-ucts (e.g. Amazon web store [19]), topic hierarchy of articles (e.g. Wikipedia [10]), genre hierarchy of music (e.g. Yahoo! Music), etc.. The benefits brought by the explicit modeling of feature re-lationships through hierarchies have been investigated in a broad spectrum of disciplines, from machine learning [12, 11] to natural language processing [10]. How to effectively exploit feature hier-archies in recommender systems is still an open research question. We here provide a running example to illustrate how hierarchical feature structure can provide better recommendations.
 Running Example . Consider a point of interest (POI) recom-mender system [30], where the goal is to recommend a real-world POI (e.g. a restaurant) to a user. User preferences can be influenced by geo-cultural factors: for instance, the country of residence might have an influence on user preferences for restaurants X  cuisine (in-tuitively, an Italian and an American might have different culinary tastes). Likewise, fellow countrymen might show different prefer-ences according to their city of residence (arguably, citizens from Boston and San Diego can show incredible different culinary pref-erences). This scenario is represented in Figure 1: users are de-scribed by auxiliary features that characterize their countries and cities of residence. These features are organized in a hierarchy, where cities are related to countries by a locatedIn relationship.
We consider the historical POI interactions in a given city of four users in Figure 1: Alice and Bob, from Rome and Florence (Italy); Charlie and Dave from Boston and San Diego (US). Italian users (Alice and Bob) both show preferences towards Pizza, thus sug-gesting a  X  X ational imprint X  on their preferences. However, the two users are differently influenced by their country of residence, as Al-ice X  X  preference for pizza is weaker than that of Bob: Alice checks in more at Lamp chop restaurants, while Bob checks in more at Pizza restaurants. A similar observation holds also for Charlie and Dave: the influence of US is weaker than that of Boston on Charlie, while stronger than that of San Diego on Dave.

The example highlights how related features (a country and its cities, linked by the locatedIn relationship) can co-influence user preferences, although the strength of the co-influence varies across relationship instances (e.g. Italy-Rome, Italy-Florence). This ob-servation suggests the need for feature relationships (e.g. the lo-catedIn relationship) to be properly considered in recommendation methods. This co-influence could be known as a priori, but it is often best learnt from historical user-item interaction data.
Existing feature-based methods, e.g. SVDFeature [5], CMF [25] and FM [21], ignore the useful information provided by feature re-lationships, imposing a conversion step that transforms a hierarchi-cal structure into a flat one. To fully exploit feature hierarchies, the main challenge is to model the co-influence of features on user-item interactions, determined by both the feature relationships in the hi-erarchical structure and the historical user-item interaction data. Original Contribution . We propose a novel approach that mod-els the co-influence of hierarchically-organized features on user-item interactions, and learns the strength of such co-influence from historical user-item interaction data, to improve recommendation performance. We first define the influence of an individual feature as regularization on latent factors, then combine the regularization of individual features by weighting them recursively over the hi-erarchy, from root to leaves, according to their organization. The regularization of the feature hierarchy, named recursive regular-ization , is expressed as a regularization function parameterized by the weights associated to each feature. We then propose a novel recommendation framework ReMF , that integrates recursive regu-larization into the matrix factorization model to better learn latent factors. By learning the values of weights of each feature from the historical user-item interaction data, ReMF characterizes the influ-ence of different features in a hierarchy on user-item interactions. We demonstrate the effectiveness of ReMF with an extensive val-idation performed on two recommendation scenarios, namely POI and product recommendation, and on multiple real-world data sets. Empirical results show that ReMF outperforms state-of-the-art ap-proaches, scoring average improvements of 7.20% (MAE), 15.07% (RMSE) and 9.86% (AUC).
Incorporating auxiliary features into recommendation, i.e. feature-based recommendation [23, 24], has become a popular and effec-tive approach to address the data sparsity and cold start problems. A wide range of features has been explored, including user gender and age [1, 6], item category [13] and content [20, 7].
Many feature-based recommendation methods consider only fea-tures with a flat structure. For example, Singh et al. [25, 17] pro-pose the collective matrix factorization (CMF) method, which fac-torizes the user-item rating and user-feature matrices simultane-ously, to improve the recommendation performance. Chen et al. [4] devise a machine learning tookit, named SVDFeature. The basic idea is that a user X  X  (an item X  X ) latent factor is influenced by those of her (its) features. Rendle et al. [21, 22] design factorization ma-chines (FM) that combines the advantages of Support Vector Ma-chines with factorization model. However, all of these methods mentioned above cannot cope with hierarchical feature structure. Blending a feature hierarchy into these models requires converting the hierarchy into a flat structure, thus losing the information about feature relationships. To fully exploit a feature hierarchy, ReMF combines the distinct influence of different features on user-item interactions according to their structured relationships.
Some studies on taxonomy -aware recommendation incorporate hierarchy in recommendation. For example, Ziegler et al. [33] and Weng et al. [29] propose to model a user X  X  taxonomy preferences as a flat vector, where each element corresponds to the user X  X  pref-erence over a taxonomy feature. The user X  X  preference is modeled as the frequency the user rates items characterized by the feature. Albadvi et al. [2] propose a similar method, however it models each feature as a preference vector, where the elements are feature attributes (e.g. price, brand). All of these methods ignore feature relationships. Koenigstein et al. [13] design a new matrix fac-torization model for Yahoo! Music competition that incorporates the feature hierarchy of track album and artist. They predict user preferences by fusing item (e.g. track) latent factors with feature (e.g. album, artist) vectors. This idea is similar to SVDFeature [4]. Though feature relationships are considered, they cannot fully ex-ploit a feature hierarchy as they simply add feature latent factors to item latent factors, without taking into account the dependent influ-ence of hierarchically-organized features on user-item interactions.
Another related line of research focuses on integrating the struc-ture within users/items in recommendation, e.g. social network [27, 18, 26], webpage network [16], tag network [32]. These methods usually regularize latent factors of users/items that are linked in the network, based on heuristic definitions of similarity between users/items. For instance, Ma et al. [18] propose SoReg that regu-larizes user latent factors based on cosine similarity of ratings be-tween socially connected users. These methods consider the net-work within users/items, though can be applied in the case of fea-ture hierarchy, e.g. by constructing implicit connections between users/items according to their feature relationships in the hierarchy. However, an essential difference between these methods and ours is that the influence of features on user-item interactions considered in these methods is usually hard-coded with manually defined similar-ity between users/items; on the contrary our proposed framework can automatically learn the co-influence of different features from the historical user-item interaction data. Recently Wang et al. [28] propose to model the implicit hierarchical structure within users and items based on user-item interactions. Our work differs from this one, in that we consider leveraging explicit auxiliary features to guide the learning of latent factors.

In summary, existing methods are incapable to model the co-influence of hierarchically-organized features on user-item interac-tions, thus restricting their applications in recommendation. In con-trast, our framework can fully exploit an auxiliary feature hierarchy through the learning of hierarchical feature influence.
This section demonstrates the need for recommendation methods able to account for the co-influence on user-item interactions of hierarchically-organized features.

Inspired by the running example, we show on 8 different data sets how: 1) users from the same country (named Country Visi-tors) are more similar in terms of POI preferences compared with users from different countries (named Foreign Visitors); and 2) users from the same city (named City Visitors) are more similar in terms of POI preferences compared to users from different cities but the same country (named Domestic Visitors).
 Data Sets We collect data of Foursquare check-in X  X  performed over 3 weeks in 4 European capital cities (Amsterdam, London, Paris, Rome) and published on 2 social media platforms (Twitter, Insta-gram). Table 1 shows the statistics about the 8 data sets. We con-sider users X  residence city , country and continent as auxiliary infor-mation about users, as well as a root feature residence location . We use the method described in [3] to locate users X  residence locations. For conciseness, we only analyze the co-influence of country and city . Overall we consider 121 countries and 2,873 cities. Analysis Metrics. We denote all countries as Con 1 ,...,Con each country Con s is the parent of all cities in it, i.e. Con parent ( Cit 1 ,...,Cit t ) . Each user u i from a city Cit country Con s ( Con s = parent ( Cit t ) ), has a set of visited POIs, i.e. POI ( u i ) = { poi i 1 ,poi i 2 ,... } . Then we measure the sim-ilarity between the users u i and u k using Jaccard similarity, i.e. Jar ( u i ,u k ) = | POI ( u i )  X  POI ( u k ) | / | POI ( u We define u i  X  X  similarity with the other Country Visitors (City Visitors), and with all Foreign Visitors (Domestic Visitors) as respectively, where F is the country (or city) u i resides in, and | F | is the number of users characterized by the feature F . For instance, in the case of F = Con s , the similarity between u and the other Country Visitors, denoted by Sim ( Con the averaged similarity between u i and each of the other Coun-try Visitors; the similarity between u i and Foreign Visitors, de-noted by Sim ( Con s ,u a i ) , is the averaged similarity between u and each of the Foreign Visitors. The similarity between u the other City Visitors and Domestic Visitors can be similarly cal-culated. Now we define the overall similarity within a country (city) F , and across the country (city) and other countries (cities) as Sim ( F w ) = [ Sim ( F,u w 1 ) ,Sim ( F,u w 2 ) ,... ] and Sim ( F [ Sim ( F,u a 1 ) ,Sim ( F,u a 2 ) ,... ] , respectively. Then we compare the overall similarity within a country (city), and that across the country (city) and other countries (cities) by: where LogRatio ( F ) &gt; 0 indicates that the elements in Sim ( F is larger than that in Sim ( F a ) on average, and LogRatio ( F )) &lt; 0 otherwise. We test the significance of the difference between Sim ( F w ) and Sim ( F a ) with a Paired t-test.
 Observation 1 : Country Visitors are more similar with each other in terms of POI preferences than with Foreign Visitors.
 The distribution of LogRatio ( Con ) for all countries is shown in Figures 2(a-b) for Instagram and Twitter, respectively. More than 95% of the countries observed in both Instagram and Twitter have LogRatio ( Con ) &gt; 0 . Paired t-test shows that 95.88% countries in Instagram and 99.36% in Twitter have Sim ( Con w ) significantly larger than Sim ( Con a ) ( p -value &lt; 0 . 01 ). We thus conclude that Country Visitors are more similar with each other in terms of POI preferences than with Foreign Visitors.

Figures 3(a-b) show the LogRatio ( Con ) for countries with more than 100 cities observed in the two platforms. We can see that users from different countries have different similarities when visiting the same city; and that the similarity of users from the same coun-try varies across visited cities. These observations highlight the need for recommendation methods that can account for the vari-ability caused by user residence country as well as visiting cities. Interestingly, all countries with LogRatio ( Con ) &lt; 0 in both Fig-ures 3(a,b) are the ones whose capital cities are visited, indicating that in visiting the capital city of their own countries, Country Visi-tors are less similar than Foreign Visitors. We find that this is due to that Country Visitors are mostly commuters in visiting their capital cities, i.e. they go to work places in the capital cities. Observation 2 : City Visitors are more similar with each other in terms of POI preferences than with Domestic Visitors.

The distribution of LogRatio ( Cit ) for all cities in all countries is shown in Figures 2(c-d) for Instagram and Twitter. We can ob-serve that all cities have LogRatio ( Cit ) greater than 0; 88.44% of them in Instagram and 88.15% in Twitter have Sim ( Cit nificantly larger than Sim ( Cit a ) ( p -value &lt; 0 . 01 ). We therefore conclude that the similarity within City Visitors is higher than that with Domestic Visitors. Comparing the distribution of cities with that of countries, all cities have LogRatio ( Con ) &gt; 0 while there are some countries with LogRatio ( Con ) &lt; 0 (those whose capital cities are visited), indicating that users from the same city are more similar than users from the same country. Moreover, we find that generally cities have larger values of LogRatio than countries. For instance the mean values of the distribution of Amsterdam in Fig-ures 2(a,c) are 4.09 and 5.07, respectively. This observation hints that cities generally have larger influence than countries on their residents X  preferences.
We adopt the regularization technique to model the influence of auxiliary features. To do so, we have to consider feature relation-ships, and further allow for the learning of feature influence from historical user-item interaction data. For this we introduce a novel regularization method, named recursive regularization , that models the co-influence of features by recursively weighting each feature influence, traversing from root to leaves in the feature hierarchy.
We first introduce the notations. Let U = { u 1 ,u 2 ,...,u the set of m users, and V = { v 1 ,v 2 ,...,v n } be the set of n items. Given a user-item interaction matrix R  X  R m  X  n , R ij is a positive number denoting the rating given by u i to v j . O  X  R m  X  n the indicator matrix, where O ij = 1 indicates that u i rates v O ij = 0 otherwise. F = { F 1 ,F 2 ,...,F t } is the set of features, each of which describes at least one user in U .
The features are organized hierarchically in a tree structure, where each node represents a feature in F . The edge between a parent node F p  X  F and a child node F c  X  F represents a directed affil-iation relationship, i.e. F c belongs to F p . Figure 4a shows an ex-ample containing three leaf features F 1 ,F 2 ,F 3 , i.e. features with no children. F 1 ,F 2 are children of the internal feature F F 4 are children of the root feature F 5 . For simplicity, we assume that each user is explicitly associated with at most one leaf feature in F . Table 2 summarizes all the notations throughout this paper.
Our method is built on matrix factorization (MF) [15], which as-sumes the existence of latent structures in the user-item interaction matrix. By uncovering latent factors of users and items, it approx-imates the observed ratings and estimates the unobserved ratings. MF solves the following optimization problem: where U  X  R m  X  d and V  X  R n  X  d are the latent factors of users and items, respectively. d is the dimension of latent factors.  X  is the regularization coefficient to avoid over-fitting. The unobserved rating for user u i to item v j can be estimated by the inner product of the corresponding user and item latent factors, i.e.  X  R ij
Step by step, we model the influence from a single feature to the combinations of features and finally the entire feature hierarchy. Influence of an Isolated Feature. To start, we first define the reg-ularization by an isolated feature F p in the hierarchy as: where k U i  X  U k k 2 F is the squared Frobenius norm distance be-tween the latent factors of u i and u k characterized by feature F F p poses regularization on the cumulation of the pairwise distance between users associated with it. Thus, Dis ( F p ) can be considered as the influence of the isolated feature F p on user-item interactions by regularizing user latent factors. The definition here only con-siders the influence of an isolated feature, while the co-influence of the feature hierarchy contributed by the feature, i.e. influence of the feature in the hierarchy , is different from  X  but based on  X  the influence of the isolated feature, which will be illustrated later.
Our method models feature influence by regularizing user latent factors, and can be straightforward transferred to modeling the in-fluence by regularizing item latent factors, or both of them. Influence of an Isolated Feature Unit. Given the above definition, we now model the influence of an isolated combination of features, on learning user latent factors, by introducing the most important relationship among features in a hierarchy, i.e. parent-child rela-tionship, based on which other relationships among features in the hierarchy such as siblings , ancestors can be derived. We first define the feature unit , i.e. Fu ( F p ) , as the combination of a single parent node F p and its children nodes, namely: Two examples of feature units Fu ( F 5 ) and Fu ( F 4 ) are shown in the red dash boxes in Figure 4a.

Then we consider the influence of an isolated feature unit on learning user latent factors by regularization. For each isolated fea-ture unit Fu ( F p ) , we denote its influence as I 0 ( F two parameters g p ,s p , with the constraint g p + s p = 1 . Parameters g and s p are used to distribute the influence of the feature unit to two parts. One is given by the parent node, weighted by g other is given by the children nodes, weighted by s p . The influence of the isolated feature unit, i.e. I 0 ( F p ) , is then defined as:
For example, the influence of the isolated feature unit Fu ( F in Figure 4a, i.e. I 0 ( F 5 ) , is determined by both the influence of the parent node F 5 , i.e. Dis ( F 5 ) , weighted by g 5 , and the influ-ence of its children nodes, i.e. Dis ( F 3 ) and Dis ( F 4 s . The overall influence of this isolated feature unit is: I g
Dis ( F 5 ) + s 5 ( Dis ( F 3 ) + Dis ( F 4 )) . Compared with the in-fluence of the isolated feature F 5 , the influence of feature F Fu ( F 5 ) is different, in that Dis ( F 5 ) is weighted by g Influence of an Entire Feature Hierarchy. Based on the defini-tion of the influence of an isolated feature unit, we now proceed to model the influence of feature unit in the hierarchy , thus to for-mally derive the overall influence of an entire feature hierarchy on user latent factors. Note that the influence of a feature unit in the hierarchy is different from  X  but based on  X  the influence of the isolated feature unit, and can be achieved by recursively defining the regularization of the feature unit in the hierarchy , given by: D EFINITION 1 ( R ECURSIVE R EGULARIZATION ).

I ( F p ) = where | F p | is the number of users characterized by feature F
From the above definition, we can see the difference between the influence of a feature unit in the hierarchy I ( F p ) and the influence of an isolated feature unit I 0 ( F p ) , that is, I ( F p fined on I ( F c ) . Put another way, the influence of a child feature is included in the influence of its parent feature. Hence, the influ-ence of an entire feature hierarchy, denoted by I ( F ) , is equivalent to that of the root feature, as it recursively includes the influence of all features in the hierarchy. As an example, Equation 3 shows the influence of the feature hierarchy in Figure 4a.
The deduction of recursive regularization of a feature hierarchy is shown in Algorithm 1, where the co-influence of features is mod-eled as a regularization function parameterized by the weights of each feature in the hierarchy. These weights characterize the influ-ence of distinct features, and can be further learnt from historical user-item interaction data, as we introduce in the next section. Remark. By recursively weighting and combining feature influ-ence over a hierarchy from the root feature to the leaves, recursive regularization can model the influence of an arbitrarily deep feature hierarchy that can be either balanced or imbalanced.
We first introduce a novel recommendation framework ReMF , that integrates the recursive regularization into the MF model to exploit auxiliary feature hierarchy. Then an optimization method and the complexity analysis for ReMF are presented.
By incorporating recursive regularization into the MF, the ReMF framework is defined by:
D EFINITION 2 ( T HE R E MF F RAMEWORK ). where  X  is a regularization parameter that controls the impact of recursive regularization, i.e. I ( F ) .

Thanks to recursive regularization, ReMF can model the co-influence of features in the hierarchy to learn user latent factors.
It also characterizes the distinct influence of each feature, thus helping with the interpretation of the effect of each feature in the hierarchy on recommendation, illustrated as follows.

Considering the example of Figure 4, based on Equations 2 and 3, the feature hierarchy influence I ( F ) can be rewritten as: where the strength of the regularization between u 1 ,u 2 tors is ( g 5 + s 5 g 4 + s 5 s 4 ) , and that of u 1 ,u Algorithm 1: Recursive Regularization Deduction ( g 5 + s 5 g 4 ) . In fact, the strength of regularization is the combi-nation of influence of different features. For simplicity, we assume g = s = 0 . 5 for each internal feature. Therefore, the strength of regularization between u 1 ,u 2  X  X  latent factors is ( g s s 4 ) = 1 , from which we could see that the feature F 5 influence of g 5 = 0 . 5 , while its children features F 4 influence of s 5 g 4 = 0 . 25 and s 5 s 4 = 0 . 25 , respectively. Then, for u ,u 3 , the strength of regularization between their latent factors is ( g 5 + s 5 g 4 ) = 0 . 75 , where the features F 5 ,F 4 have influence of g = 0 . 5 ,s 5 g 4 = 0 . 25 , respectively. The distinct influence of fea-tures on learning user latent factors can therefore be characterized by certain functions of the weights ( g,s ).

To formally derive feature influence on an arbitrary pair of users, we define the regularization coefficient C ik to represent the strength of regularization between u i and u k , where a greater value of C indicates a higher correlation between the two users. Hence, I ( F ) can be reformulated as:
We next introduce two theorems for deriving C ik , which is the combination of the influence by different features on u i
T HEOREM 1. The regularization coefficient for any pair of users u , u k (i.e. C ik ) characterized by the same leaf feature is 1: where the list { F root ,F c 1 ,F c 2 ,...,F c l } is the set of the common features of u i and u k , ordered in a sequence from the root feature F root to the leaf feature F c l .
 Proof. This is straightforward to prove, due to the constraint g + s = 1 . Considering the example { u 1 ,u 2 } in Figure 4, the sum of the relevant regularization terms, i.e. g 5 Dis ( F 5 ) ,s s s 4 Dis ( F 1 ) , in Equation 3 is:
T HEOREM 2. For any pair of users u i , u k not characterized by a common leaf feature, the regularization coefficient (i.e. C where the list { F root ,F c 1 ,F c 2 ,...,F c l } is the set of the common features of u i and u k , ordered from the root feature F deepest common feature F c l .
 Proof. All possible features that can influence the regularization coefficient of u i ,u k are their deepest common feature, and the par-ents and ancestors of the deepest common feature.

According to the above theorems, the value of regularization co-efficient always falls into the range of [0 , 1] , with 1 indicating the Algorithm 2: ReMF Model Learning full regularization and 0 indicating no regularization. As an exam-ple, Figure 4b shows the regularization coefficients of the feature hierarchy in Figure 4a.

These regularization coefficients naturally connect ReMF to net-work based recommendation methods, which also consider pair-wise regularization on users. There are however two essential dif-ferences: 1) network-based regularization coefficients are usually hard-coded, while our regularization coefficients are modeled from the feature hierarchy structure, and expressed by the function of weights ( g,s ) . And, 2) ( g,s ) , which parametrizes the distinct fea-ture influence while being automatically learnt from the historical user-item interaction data, as we will address in the next subsection.
We adopt the stochastic gradient descent scheme [14, 15] to op-timize our objective function.
 Updating U , V . The gradients of U i , V j are given by: Updating ( g,s ) . ( g,s ) can be predefined heuristically, or hand-crafted by domain experts who can fairly quantify the influence of different features. Instead, we provide an effective data-driven so-lution that automatically learns ( g,s ) based on the historical user-item interaction data.

We only need to estimate ( g,s ) for internal features in the hier-archy, since the leaf features do not have children. For an internal feature F p , the gradients of g p ,s p are equivalent to the multipliers of g p ,s p in I ( F ) . Thus, we have:
According to the constraint g p + s p = 1 , we can update g using the gradient and the other by s p = 1  X  g p (or g p The detailed learning process is shown in Algorithm 2.
 Complexity Analysis The computational time is mainly taken by evaluating the objective function J and updating the related vari-ables. The time to compute the J is O ( d | R | + dm | R | is the number of non-zero observations in the rating matrix R . For all gradients  X  J  X  U are O ( d | R | + dm 2 ) , O ( d | R | ) , O d P layer  X  1 O ( | s p | ) , respectively. Wherein m l denotes the average number of users in each node at layer l , n l denotes the number of nodes at layer l , and | s p | ( | R | ) denotes the number of internal nodes. Particularly, we leverage s p = (1  X  g p ) to update s p . The overall computational complexity of Algorithm 2 is (# iteration  X  X  ( d | R | world applications m l is typically small (e.g. power-law distributed), thus making ReMF scalable to large data set.
We assess the performance of ReMF with a comparison with the state-of-the-art, feature-based, hierarchy-based recommenda-tion methods. The comparison is performed over 1) the data sets introduced in Section 3, for POI recommendation with user feature hierarchy; and 2) a data set from the Amazon Web store [19], for product recommendation with item feature hierarchy. Evaluation. We adopt the standard 5-fold cross-validation, and the following 3 metrics for evaluation: Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) [13, 22] to measure the error of predicted ratings; and Area Under the ROC Curve (AUC) [9, 31] to measure the quality of predicted ranking of items (ranked according to the predicted ratings). The smaller MAE and RMSE, and the larger AUC, the better the recommendation performance. Comparison Methods. The following methods are compared: (1) MF [15]: matrix factorization method; (2) CMF [25]: collec-tive MF; (3) TaxMF [13]: taxonomy-based MF; (4) SoReg [18]: network-based recommendation method incorporating social rela-tions; (5) FM [22]: factorization machine; (6) HieFM : factoriza-tion machine with hierarchy information.

HieFM is a variation of FM that considers each features path in the hierarchy (from root to leaf nodes) as an additional feature in the design vectors of FM. Similar to FM, CMF and TaxMF can also incorporate path-based features. As FM outperforms CMF and TaxMF (see Section 6.3), we limit our comparison with previous methods exploiting path-based features to HieFM.
 Parameter Settings. We empirically set optimal parameters for each method using a grid search in { 0 . 0001 , 0 . 001 , 0 . 01 , 0 . 05 } for both  X  (including 1-way and 2-way regularization of FM) and the learning rate  X  ;  X  = 0 . 5 for CMF;  X  = 0 . 01 for SoReg. For fair comparison, we set d = 10 (the dimension of latent factors) for all the methods, and adopt all features (i.e. continent, country, and city) as input in TaxMF, CMF, FM and HieFM. HieFM has path-based features as additional hierarchy information. In SoReg, we model the social relations among users by counting the num-ber of common features, under the assumption that the commonal-ity establishes implicit social relationships based on the geo-social correlation phenomenon [8]. Without loss of generality, we adopt f ( x ) = 1 / (1 + x  X  1 ) to map each #check-in R ij  X  R in POI data sets into the interval (0 , 1) [7].
We analyze the influence of recursive regularization on ReMF performance, and discuss how the weighting parameters g,s can help the interpretation of recommendation results.
 The Impact of  X  . In ReMF ,  X  controls the strength of recur-sive regularization of feature hierarchy. We apply a grid search in  X  on recommendation performance. Results are shown in Figure 5. As  X  varies from small to large, the performance first increases then decreases, with the maximum reached at the range [ 10  X  2 The performance variations across data sets suggest the need for data set-specific settings; the similarity in performance variation across  X  values shows the robustness of ReMF .
 Interpretation from ( g,s ) . We examine ( g,s ) for the internal fea-tures, i.e. continents and countries, learnt from data. Table 4 shows the list of continents and countries ranked according to their g val-ues. Recall that for a continent (country), g &gt; s means that the continent (country) has a stronger effect on user preferences than and its children features, i.e. countries (cities).

In general the continents have relatively smaller effects on user preferences (with g values all below 0.2), suggesting that continents have weaker effects than their countries. In addition, we observe a big variance in the g values of countries, indicating that different countries have different influence on user preferences. The high variance of countries X  g values proves the necessity of parameter-izing g,s in recommendation. We then compare the influence of countries and cities on their residents X  preferences. As cities of a country and the country comprise a feature unit, the influence of a city can be measured by s = 1  X  g , where g is the influence of the country. We can see from Table 4 that most countries have g &lt; 0 . 5 (only 3 countries have g &gt; 0 . 5 ), i.e. s &gt; 0 . 5 , indicating that the influence of cities in most countries have more influence on their residents X  preferences than the countries themselves.
 Rating Performance. Two views are created for each data set: 1) the  X  X ll X  view includes all users; while 2) the  X  X old start X  view in-dicates that only users with  X  5 ratings are involved in the test set. Table 3 compares the performance of the considered recommen-dation methods for all data sets. Unsurprisingly, the basic matrix factorization model is consistently outperformed by feature-based recommendation methods; this shows that, in the context of the tar-geted evaluation scenario, the usage of auxiliary information about users positively affects recommendation accuracy. In addition, FM outperforms CMF, TaxMF and SoReg. This could be explained by FM considering item-feature interactions, in addition to user-item and user-feature interactions.

HieMF in general outperforms FM, suggesting that information about feature relationships (paths) can help predicting user pref-erences. ReMF consistently outperforms the methods in the com-parison pool, with an average performance gain (w.r.t. the second best method) of 7.20% (MAE) and 15.07% (RMSE). Paired t-test shows that the improvements of ReMF on all data sets are signif-icant ( p -value &lt; 0 . 01 ). Such big improvements clearly show the effectiveness of recursive regularization, and the advantage derived from the full inclusion of information about feature relationships.
Table 3 (data view  X  X old start X ) reports the results with cold start users. As in the previous case, ReMF achieves the best performance compared with other methods, and significantly outperforms the second best methods in all data sets ( p -value &lt; 0 . 01 ) by 12.02% and 17.53% w.r.t. MAE and RMSE respectively. The relatively larger improvements on the testing view  X  X old start X  than on  X  X ll X  indicates that ReMF has higher capability in coping with the cold start problem compared to the state-of-the-art methods.
 Ranking Performance. We further evaluate the ranking quality of items recommended by ReMF and other methods in the comparison pool. Results are shown in Figures 6(a-b) for data sets from Insta-gram and Twitter, respectively. ReMF significantly outperforms the second best method ( p -value &lt; 0 . 01 ) on all data sets by 9.86% on average, reaching an averaged AUC of 0 . 8175 in Instagram and 0 . 7568 in Twitter. These observations show that the influence of feature hierarchy modeled by recursive regularization can effec-tively complement user-item interaction data in ranking prediction. Generalizability . We test the performance of ReMF on another task, i.e. product recommendation, using the data from Amazon web store [19]. Different from the POI data sets, here we consider the feature hierarchy of items. We focus on the product category of  X  X lothing, Shoes &amp; Jewelry X , having maximal depth of 7, and an unbalanced feature hierarchy. An example path in the hierarchy from the root feature to the leaf is  X  X lothing, Shoes &amp; Jewelry  X  Men  X  Accessories  X  Wallets X . We uniformly sample the raw data set to include 100 , 810 ratings performed by 34 , 817 users to 45 , 716 items. Table 5 compares the performance of ReMF and the other methods in the comparison pool, measured by RMSE, which is more indicative of large errors than MAE. As in the previous setting, ReMF significantly outperforms the second best method ( p -value &lt; 0 . 01 ), i.e. HieFM, by 5.46% on the testing view of  X  X ll X  and 7.42% on  X  X old start X . These results show that ReMF can be effective in multiple recommendation tasks, and with different topologies of features hierarchy.

Hierarchies are a common way to capture relationships between features. Yet, the value of this additional information is not fully exploited by state-of-the-art feature-based recommendation meth-ods. This paper proposes a novel regularization method named re-cursive regularization for modeling the co-influence of features in the hierarchy on user-item interactions. Based on this, a new rec-ommendation framework ReMF is proposed to learn hierarchical feature influence from historical user-item interaction data. Ex-perimental validation on real-word data sets shows that ReMF can largely outperform state-of-the-art methods, proving the value re-siding in the exploitation of feature hierarchies for better learning user and item latent factors.

We stress how recursive regularization does not only apply to tree-like data structures (hierarchy), but also to a forest of trees: adding a root feature transforms a set of trees to one tree. General-ization to graphs is less trivial, and therefore left to future work.
