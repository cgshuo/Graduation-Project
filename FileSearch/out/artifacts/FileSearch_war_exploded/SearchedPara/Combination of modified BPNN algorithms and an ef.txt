 1. Introduction
The development of the internet has led to a large number of documents and information on the internet which is studies and rapid progresses in automatic text categorization.

More and more approaches based on machine learning and intelligent agents have been applied to text categorization Banerjee, 2007; Wang &amp; Chiang, 2007).

Although many approaches have been proposed, automated text categorization is still a major research area because the high dimensional feature space and ignores relationships between terms and documents resulting in decreased categoriza-tion efficiency and accuracy. Many feature reduction and selection techniques have been proposed for text categorization. have been proved as efficient ways to improve categorization performance because they capture the semantic relationships between terms and documents.

A neural network is one of the most efficient approaches for text categorization. Many different neural networks have been applied to text categorization. Perceptron is the earliest and simplest form of a neural network and consists only of it showed surprisingly high performance. Zhangand Zhou (2006) proposed a multi-label neural network and its application to text categorization. Nonlinear neural networks are more sophisticated neural networks with some hidden layers between (one input, one output, and at least one hidden layer) and use back propagation as the learning mechanism. Ruiz and Srin-ivasan (1998) compared the classification results using a back propagation learning mechanism and a counter-propagation learning mechanism.

The back propagation neuralnetwork(BPNN) is the most popularof the neural networkapplications. It hasthe advantage of yieldinghighclassificationaccuracy.However,itsproblemsofslowlearningandthelikelihoodofitsbecomingtrappedinalocal duetothefactthatthelearningprocessofthestandardBPnetworkismechanicalandelementary.Itdoesnothavetheadvanced
BPNN have been developed during the past years. The basic goals of the modifications have been to make learning faster 2002), and to improve the generalization ability (Marcelo, Braga, &amp; de Menezes, 2003 ).

In this paper, we propose two novel algorithms to overcome the slow learning and local minima problems: MRBP (Mor-bidity Neuron Rectified BPNN) and LPEBP (Learning Phase Evaluation BPNN). These two algorithms use the concept of a learning phase (see definition in Section 2.3). We divided the entire learning process into many learning phases with each learning phase containing certain epochs. Based on experience, the limitations (slow learning and local minima) of BPNN are related to the morbidity neurons. This is because of the ill-conditioned nature of the standard BP method, while the MRBP can detect and rectify the morbidity neurons during each learning phase. The LPEBP calculates the learning effects in each values correspond to different learning models. The next learning phase adjusts the learning model based on the evaluation effects according to the previous learning phase. This algorithm can eliminate the blindness of the mechanical and repeated single learning phases (more details of these two algorithms are described in Section 2).

In order to perform text categorization, an important process is text representation. A generally used method is to create document vectors from terms frequently occurring in the documents. However, the inherent high dimensionality resulting (Caruana, Lawrence, &amp; Giles, 2001 ).

Ambiguity in the meaning of the terms can also prevent the classifier from choosing the categories deterministically, and will directly decrease the categorization accuracy. We introduce the semantic feature space (SFS) method to reduce the num-ber of dimensions and construct the latent semantics between terms which can improve the categorization precision and effi-ciency. This method uses a singular value decomposition technique. Singular value decomposition ( Noorinaeini &amp; Lehto, 2006; Yany, 1995 ) can project the original high dimensional space onto a low semantic dimensional space. These derived ships between terms. Therefore, even if two documents do not have any common words, associative relationships between them can be found because the similar contexts in the documents will have similar vectors in the semantic feature space. The rest of this paper is organized as follows: Section 2 describes the standard BPNN learning algorithm and the modified
BPNN learning algorithm. Section 3 describes the application of the semantic feature space method to our system. Experi-ments and results are shown in Section 4. Conclusions are given in Section 5. 2. Neural network learning algorithms 2.1. BPNN learning algorithm
The training of a network by back propagation involves three stages: the feed-forward of the input training pattern, the calculation and back-propagation of the associated error, and the adjustment of the weight and the biases.
Input pattern feed-forward. Calculate the neuron X  X  input and output. For the neuron j , the input I where w ij is the weight of the connection from the i th neuron in the previous layer to the neuron j , f ( I function of the neurons, O j is the output of neuron j , and h sigmoid activation function defined as the following equation: with the following formula:
Here n is the number of training patterns, l is the number of output nodes, and O absolute error falls below some threshold or tolerance level. The back propagation errors both in the output layer, d hidden layer, d j , are then calculated with the following formulas:
Here T l is the desired output of the l th output neuron, O the weights and biases in both the output and hidden layers.

Weights and biases adjustment. The weights, w ji , and biases, h
Here k is the number of the epoch and g is the learning rate. 2.2. The main problems of the standard BPNN
As mentioned above, the standard BPNN has the two main problems of slow training speed and the likelihood of entering into a local minimum.

Slow training speed. In the beginning, the learning process proceeds very quickly in each epoch and can make rapid pro-the standard BPNN. One of the approaches to speed up the training rate is to estimate optimal initial weights (Kolen &amp; learning progress. Second-order gradient techniques (Kramer &amp; Sangiovanni-Vincentelli, 1989; Shepherd, 1997; Watrous, 1987), such as the conjugate gradient and quasi-Newton methods, as opposed to the simple gradient descent technique, have been demonstrated to achieve rapid convergence. Since the sigmoid derivative which appears in the error function of the standard BP method has a bell shape, it sometimes causes slow learning progress when the output of a unit is near  X 0 X  or  X 1 X . Modification of the error function is proposed in Wang, Tang, Tamura, Ishii, and Sun (2004) to improve the standard 2004; Plagianakos &amp; Vrahatis, 2000). Another two commonly used methods of improving the speed of training for BPNNs are as training progresses. Convergence is sometimes faster if a momentum term is added to the weight update formula.
Local minima . When training a BPNN, it is easy to enter local minima. One of the most famous remedies is global search technology-simulated annealing (Owen &amp; Abunawass, 1993 ) which accepts deterioration in the error function with a non-search and optimization algorithm which is frequently combined with neural networks to select best architectures and avoid the drawbacks of the local minimization method. The electromagnetism algorithm (EM) ( Wu, Yang, &amp; Wei, 2004) is also a global optimization method which can be used to optimize the BP neural networks. This method simulates the electromag-netism theory of physics by considering each weight connection in a neural network as an electrical charge. Through the attraction and repulsion of the charges, weights move toward optimality without being trapped in local optima.
In this paper, we propose two new algorithms to modify the standard BPNN method to overcome these problems. 2.3. Morbidity neuron rectified back-propagation neural network (MRBP network)
As mentioned above, the standard neural network has an ill-conditioned nature. During the learning process, neurons face two kinds of problems: neuron saturation and neuron fatigue (defined below). We refer to these neurons as morbidity neu-rons. When the network experiences morbidity neurons, it repeats the single learning blindly and mechanically. The error results would not be satisfactory. Therefore, the neural network can learn well and efficiently to obtain good experimental results only when the appearance of morbidity neurons is avoided and rectified in time.
 Firstly, several concepts and definitions used in this paper will be introduced.

Definition 1. Learning phase. N iterations (or epochs) are chosen as a period during which important data is recorded and learning phase in which each learning period includes 50 epochs.
 approach to 1 or 1, and cause the back-propagation error to approach 0. The activation function (bipolar sigmoid function) is shown below: to neuron j as being saturated.
 one learning phase obeys, where k is the number of epochs in one learning phase, we refer to the neuron j as being fatigued.

It is noted that the neuron saturation originates from the derivation of the activation function. In the conventional acti-vation function, k uses one or more constants, whereas in our model, k is an adjustable value. Plagianakos and Vrahatis (2000) tried to train the network with a threshold activation function by change the value of the k in his paper. Actually, different combinations of k correspond to different learning models.

The MRBP improved method: For the neuron saturation problem, the maximum and minimum output values should be limited to the normal range. In our experiments, the range is ( 0.9, 0.9). Maxf  X  I and minimum input values which include the bias h k j during one learning phase, and k is the variable to control the slope of the activation value. So, the morbidity neuron of neuron saturation can be rectified by adjusting k using the following formula:
For the neuron fatigue problem, it is desirable to normalize the maximum and minimum input values in the previous phase in order to make them symmetric with respect to the origin. So, the morbidity neuron of neuron fatigue can be rectified by adjusting the bias h using the following formula:
In our study, the morbidity neurons were rectified in each learning phase after their evaluation. 2.4. Learning phase evaluation back propagation neural network (LPEBP network)
In order to improve the back propagation algorithm in terms of its rate of convergence and global search capability, we very much on the choice of learning rate.

The learning phase evaluation back propagation algorithm (LPEBP) uses a statistical technique to evaluate each learning learning phase.

During the learning phase, the system records the minimum error in the current learning phase ( CME ), the minimum er-process. formula used for adjusting the learning rate is where g is the learning rate and l is the coefficient.
 Tables 1 and 2 were designed for overcome the first problem and Table 3 was designed to overcome the second problem.
When the learning effects are progressive (Table 1), a rapid learning rate for the next learning phase is appropriately se-the system stays near a local minima, because when it approaches one, the training speed will be extremely slow. Therefore, ble 3 ). The adjusted rules used in our experiments are given in the following tables ( nLP is the learning phase, nLP =50 epochs, l is the coefficient of the learning rate g ). 2.5. Comparison of LPEBP with conventional adaptive learning rate
The difference between the LPEBP and conventional adaptive learning rates (Jacobs, 1988 ) is that although the conven-tiny adjustment range.

If we were to introduce the learning phase evaluation back propagation neural network to our system, it would greatly more quickly. Also, if the system found the learning rate to be too high and thus causing fluctuations, this method would reduce the learning rate appropriately and the network would train more steadily. 3. Application of SFS to our system 3.1. Vector space model (VSM)
The vector space model is a way of representing documents through words that they contain. It is implemented by cre-numbered from 1 to n , the feature vector of the documents can be represented as where w k , i is the term weight of the i th indexing term in the k th document. 3.2. Semantic feature space
The semantic feature space method attempts to appropriately capture the underlying conceptual similarity between terms and documents which is helpful for improving the categorization accuracy. This method requires singular value decomposition techniques. Singular value decomposition is a well developed method for extracting the dominant features of large data sets and for reducing the dimensionality of the data.

Our corpus can be represented as a term by a document matrix, A ( m n ). Once a term represented by a document matrix be used to represent conceptual term-document associations. The singular value decomposition of A is defined as vectors, respectively.

To reduce the dimensionality, we can simply choose the k largest singular values and the corresponding left and right singular vectors. The best approximation of A with a rank-k matrix is given by where R k = diag ( r 1 , ... , r k ) is the first k factors. U and documents, while ignoring noise due to word choice.

Singular value decomposition is a mathematical concept which is also commonly used in most Latent Semantic Indexing (LSI) methods. LSI was originally proposed as an information retrieval method (Deerwester, Dumais, Landauer, Furnas, &amp;
Harshman, 1990 ). The main difference between our method (SFS) and LSI is that, after decomposition, we use only the U by where ^ d is the new reduced feature vector and d T is the original feature vector.

Once all of the training and test examples are represented in this way, the new document vectors are performed by the neural network classifier as input vectors, and the neural network algorithm decides which categories the test samples should be assigned to. 4. Experiments and results 4.1. Data sets for experiments
In order to measure the performance of our systems, two data sets were used: the standard Reuter-21578 corpus ( Reu-ters21578 data set) and 20-newsgroup corpus (20-news-18828 version) (20-news-18828 version data set). In the first data set, 1500 documents were used for training and testing our system which were randomly chosen from ten categories: Acq,
Coffee, Crude, Earn, Grain, Interest, Money-fx, Ship, Sugar, and Trade. In the second data set, 1,200 documents were used which were randomly chose from ten categories: Alt.atheism, Comp.windows.x, Sci.crypt, Rec.motorcycles, Rec.sporthockey, iments. It should be noted that a small subset of the two datasets was used because otherwise, the processing and compu-tation would be intractable. After pre-processing of the documents, Porter X  X  stemming algorithms (Porter, 1980) were employed for word stemming.
Finally, a total of 7856 indexing terms were obtained from the training documents in the Reuter-21578 data set and 13,642 indexing terms from the training documents in the 20-newsgroup data set. 4.2. Feature selection and SFS networks with the vector space model (VSM)), it is necessary to transform the documents into a feature vector. So, for data set 1 and data set 2, each document has 7856 and 13,642 features, respectively. The feature vector of each document can be represented as where F j , i is the term weight of the i th indexing term in document j .

The main difficulty in the application of a neural network to text categorization is the high dimensionality of the input in the feature space, so that the size of the input of the neural network depends upon the number of stemmed words. There-the term weight is as follows: formula normalizes the length of the documents in order to replace the simple tf idf . Thus, the reduced documents can represented as
Then, the SFS method is used to further reduce the number of dimensions. In this way, each document in the original high dimensional feature vectors (1 m ) can be transformed into our desired k -dimension vectors (1 k )by where k is the our desired dimension and m is the reduced feature vectors (1000 and 1200 for data set 1 and data set 2, respectively).

We used two steps for feature selection. The feature can be directly reduced by SFS but the large sized matrix requires a very long time to decompose by SVD which obviously increases the cost of the system and the result is not better than the two step feature selection method. 4.3. Hidden nodes determination and error reduction
In our experiment, the number of input nodes is equal to the size of the feature vectors and the number of output nodes is ing the BPNN. In most researches, the number of hidden nodes is usually determined according to experience. If the number of hidden nodes is very few, the network will not be able to deal with complicated problems. But if the number of hidden nodes is too large, the network training time will increase sharply. Furthermore, excessive hidden nodes will result in the overfitting problem ( Haykin, 1999 ). In order to determine an appropriate number of hidden nodes, we used the following rules as a reference (Liming, 1993 ). ranging from 1 to 10. In our experiments, 15 hidden nodes were used based on the above rules and our experience.
In order to distinguish our improvement, the mean absolute errors among the different methods based on the Reuter-21578 data set were compared. The mean absolute error function is defined in formula (4).

In our experiments, the mean absolute errors of the standard BPNN and MRBP using SFS with 100, 250, and 400 dimen-sions as well as VSM with 1000 dimensions (Fig. 1 ), and the standard BPNN and LPEBP using SFS with 100, 250, and 400 dimensions as well as VSM with 1000 dimensions (Fig. 2 ) were compared.

All of our experiments were conducted on a Pentium personal computer. The algorithms were written in C++ and all of the programs were complied in VC++ 6.0. All of the figures in the experimental section were created by Matlab 6.5 using the data generated by the C++ program.

From Figs. 1 and 2 , it is seen that the mean absolute error is reduced when the number of epochs is increased. At the beginning of the training phase, the error decreased very rapidly. However, this reduction slowed down after a certain num-ber of epochs. The error for BPNN using SFS with 400 dimensions was smaller than with 250 dimensions, and the error for
BPNN using SFS with 250 dimensions was smaller than with 100 dimensions. The error for BPNN using VSM with 1,000 dimensions was the smallest among the four different BPNNs. MRBPs and LPEBPs can greatly accelerate the training speed with all kinds of dimensions. The mean absolute error decreased rapidly during all of the training phases and was much smaller than with each dimension in the standard BPNNs. Good improvements were obtained, especially in the later parts of the training phase.

It is seen that the modified BPNN algorithms are much faster and efficient than the standard BPNN. After 2500 epochs training, the mean error of the standard BPNN was 0.00042. But, for MRBP, it took 745 epochs to reach a mean error of 0.00042, about 3 X 4 times faster than the standard BPNN, and for LEPBP, it only took 489 epochs to reach a mean error of 0.00042, about 5 times faster than the standard BPNN. In our experiments, the three algorithms stopped training at 2500 epochs (similar training time). This seems to be a waste of time for MRBP and LEPBP to train for such a long time but they can enhance the text categorization performance. 4.4. Experimental results and analysis
The performance of text categorization systems can be evaluated based on their classification effectiveness. The precision, recall, and F-measure were used to evaluate our system and to compare the performances of BPNN with VSM, BPNN with SFS, MRBP with VSM, MRBP with SFS, LPEBP with VSM, and LPEBP with SFS. The F-measure is defined as
The macro-average method was used to obtain the average value of the precision and recall and the F-measure is based on the macro-average value.
 In our experiments with data set 1, the performances were compared when using the following values for the number of 200, 250, 300, 350, and 400. The number of input nodes for the neural network was equal to the number of dimensions of the document vectors. The network size and parameters are shown in Tables 4 and 5 . In our experiments, the neural networks stopped training after 2500 epochs. The categorization performances for data sets 1 and 2 are given in Figs. 3 and 4 , respectively.

From Fig. 3 , the F-measure of BPNN using VSM with 1000 dimensions is 0.899. The F-measure of BPNN using SFS was enhanced when the number of dimensions was increased. When the number of dimensions was more than 140, the F-mea-sure of BPNN using SFS was better than that of BPNN using VSM, and when the number of dimensions is 450, the best per-formance of 0.914 was obtained. From Figs. 1 and 2 , the mean absolute error of BPNN using SFS with 100, 250, and 400 dimensions is larger than that of BPNN using VSM with 1000 dimensions because the SFS constructs a semantic space be-tween 1000 terms. Therefore, the BPNN with SFS outperforms BPNN with VSM when the number of dimensions was more than 140. MRBP and LPEBP further enhanced the performance of the BPNNs. The F-measures of MRBP and LPEBP using VSM with 1,000 dimensions were 0.917 and 0.926, respectively. When the number of dimensions was more than 120, the F-mea-sure of MRBP using SFS was better than MRBP using VSM. When the number of dimensions was more than 80, the F-measure of LPEBP using SFS was better than LPEBP using VSM. For MRBP, the best performance of 0.932 was obtained when the num-ber of dimensions was 400. For LPEBP, the best performance of 0.942 was obtained when the number of dimensions was 350.
From Fig. 4 , the F-measures of BPNN using VSM with 1200 dimensions, MRBP using VSM with 1200, and LPEBP using VSM with 1200 dimensions are 0.828, 0.848, and 0.859, respectively. The F-measures of BPNN, MRBP, and LPEBP using SFS were also enhanced when the number of dimensions was increased. When the number of dimensions was more than 100, the F-measure of BPNN using SFS was better than that of BPNN using VSM. When the number of dimensions was more than 80, the F-measure of MRBP using SFS was better than MRBP using VSM. When the number of dimensions was more than 70, the F-measure of LPEBP using SFS was better than LPEBP using VSM. For BPNN, the best performance of 0.848 was obtained when the number of dimensions was 250. For MRBP, the best performance of 0.870 was obtained when the number of dimensions was 250. For LPEBP, the best performance of 0.888 was obtained when the number of dimensions was 200. 4.5. Computational time analysis The computational times of our systems include the training time for the neural networks and SVD decomposition time.
In our experiments, SVD was performed on OpenCV (OpenCV) (Open Source Computer Vision), which is a library of program-ming functions mainly aimed at real time computer vision. From Table 6 , the computational times of BPNN, MRBP, and LPEBP using VSM are similar because they all stopped training at 2500 epochs (the computational time of BPNN, MRBP, and LPEBP ber of dimensions increased. For all numbers of dimensions, the training time of neural networks using SFS were much faster than neural networks using VSM which required around 1184 s for data set 1 and around 746 s for data set 2. The testing time of neural networks was very fast. For all numbers of dimensions, it takes only 0.015 X 0.030 s. 5. Conclusion and discussion
This paper proposes advanced text categorization systems using two modified back propagation neural networks: MRBP and LPEBP. MRBP and LPEBP overcome the problem of slow training speed in standard BPNNs and can also escape from local minima. The SFS method not only reduces the number of dimensions drastically, but also overcomes the problems existing in the vector space model commonly used for text representation. The experimental results show that MRBP and LPEBP en-hance the performance of text categorization and SFS further improves its accuracy and efficiency.
 Acknowledgement This work was supported by a Korea Research Foundation Grant (KRF-2006-321-A00012) and by the second stage of Brain Korea 21 Project.

References
