 With the advance of internet, multi-modal data can be eas-ily collected from many social websites such as Wikipedia, Flickr, YouTube, etc. Images shared on the web are usually associated with social tags or other textual information. Al-though existing multi-modal methods can make use of asso-ciated text to improve image annotation, the disadvantages of them are that associated text is also required for a new image to be predicted. In this paper, we propose the cross-modal multi-task learning (CMMTL) framework for image annotation. Labeled and unlabeled multi-modal data are both levaraged for training in CMMTL, and it finally obtains visual classifiers which can predict concepts for a single im-age without any associated information. CMMTL integrates graph learning, multi-task learning and cross-modal learning into a joint framework, where a shared subspace is learned to preserve both cross-modal correlation and concept cor-relation. The optimal solution of the proposed framework can be obtained by solving a generalized eigenvalue problem. We conduct comprehensive experiments on two real world image datasets: MIR Flickr and NUS-WIDE, to evaluate the performance of the proposed framework. Experimen-tal results demonstrate that CMMTL obtains a significant improvement over several representative methods for cross-modal image annotation.
 H.2.8 [ Database Management ]: Database Applications X  Image databases; Data mining ; I.5.2 [ Pattern Recogni-tion ]: Design Methodology X  Classifier design and evalua-tion  X  Corresponding Author Algorithms; Theory; Experimentation Cross-modal learning; Image annotation; Multi-task learn-ing; Semi-supervised learning
With the advance of Web 2.0 era, there is an explosive growth of user generated multimedia data shared in social network websites such as Wikipedia 1 , Flickr 2 , YouTube etc. Effective semantic understanding methods for the mul-timedia such as images are increasingly demanded. Images on social websites are usually associated with some textu-al information, including tags, text descriptions, comments, etc. The textual information can be exploited to improve the semantic understanding of images. Figure 1 shows an example of images collected from Flickr. It is not difficult to find that images are associated with some noisy but infor-mative social tags. These tags may be used as an important source to improve the learning of concepts for images.
The goal of image annotation is to develop methods which can predict semantic concepts for a new image, it is es-sentially a multi-label classification problem. The semantic concepts are usually precisely defined, they may be scenes (e.g., indoor, outdoor, landscape, etc.), objects (e.g., ani-Semantic concepts are different to user tags, and tags are treated as textual information for images in this work. Tra-ditional annotation methods are usually trained on labeled images. Several generative methods such as Corr-LDA[4], MBRM[12], MRFA[32], and discriminative methods such as TagProp[13], Group Spasity[35], are used for the image annotation task. Recently, semi-supervised methods which leverage unlabeled images for learning are proposed [8, 24, 33], they relieve the required human labor in image labeling, and accurate classifier for image annotation can be learned based on a small amount of labeled images. However, all these methods only use image modality for learning, oth-http://en.wikipedia.org/wiki/Main Page https://www.flickr.com http://www.youtube.com Figure 1: Images collected from Flickr, they are as-sociated with textual tags. er informative modalities such as text which can be easily obtained from web are ignored.

Multi-modal learning is a traditional approach which can utilize associated textual information to improve the image annotation performance. In all multi-modal learning meth-ods, multi-modal fusion is a common approach for combin-ing multi-modal features, e.g., images and texts. The fusion of different modalities is generally performed at two levels: feature level (early fusion) and decision level (late fusion) [1]. In the early fusion methods [28], the features extracted from input data are firstly combined and then sent as input for annotation. In the late fusion methods [38, 22], the local decisions are firstly obtained based on different modalities, then these decisions are combined for the final decision. The major disadvantage of multi-modal methods is that multi-modal features are also required in the prediction process. For example, given a new image, multi-modal methods can-not work if it has not any associated information.
It is easy to collect multi-modal data such as images with tags from the web, but a new image to be predicted is not al-ways associated with other modalities such as textual tags. For example, images obtained by surveillance cameras are generally not associated with any textual tags. In order to solve this problem, cross-modal image annotation has gained some attention. It also exploit the social textual information to improve image annotation. The training process of cross-modal methods is similar to multi-modal methods, where multiple heterogeneous modalities are used. The advan-tage of cross-modal methods is that it can predict semantic concepts for a single image without any additional infor-mation. Therefore, cross-modal methods are more practical than multi-modal methods. The reason of how cross-modal learning improves the image annotation is straightforward. Given an image, if we have known the cross-modal corre-lation, then we are able to find the texts correlated to this image. And the annotation results can be benefited from these correlated texts. Based on this idea, some researchers try to learn the text representation to improve image anno-tation. In [31] a large collection of up to one million tagged images is used to obtain a textual representation of images without tags, and separate classifiers are learned based on the visual and textual features, then their scores are linear-ly combined using a third classifier, which can predict labels for images alone. Semi-supervised learning is shown to be effective in solving the cross-modal image annotation prob-lem. Co-training [5] is a semi-supervised learning method which is learned from multi-modal data, and then each clas-sifier is learned for each modality, the performance of image annotation can be improved by co-training [14]. Another semi-supervised method: MKL+LSR [14] uses a two-step process to learn an image classifier from multi-modal da-ta which contain images and tags. A drawback of existing methods is that they fails to exploit the cross-modal cor-relation explicitly, which limits their performance on image annotation.

In this paper, we propose the cross-modal multi-task learn-ing (CMMTL) to improve image annotation. CMMTL inte-grates three state-of-the-art learning approaches, including graph-based learning, multi-task learning and cross-modal learning, into a joint framework. Basing on labeled and un-labeled multi-modal data which can be easily collected from web, CMMTL learns a visual classifier which is more ef-fective than traditional methods for image annotation. In CMMTL, a subspace shared among different concepts, as well as different modalities, is learned by solving a general-ized eigenvalue problem. CMMTL has three main advan-tages, the first is that the cross-modal correlation is explic-itly learned and preserved in a shared subspace. The second is that the shared subspace also preserves the concept cor-relation. At last, CMMTL is based on the semi-supervised learning framework, which means only a small amount of labeled data are required for CMMTL. The experiments on two real world datasets: MIR Flickr [17] and NUS-WIDE [9], demonstrate the effectiveness of CMMTL. And its per-formance is superior to several representative methods for the problem of cross-modal image annotation.

The rest of this paper is organized as follows. Section 2 presents related work about semi-supervised learning, multi-task learning and cross-modal learning. In section 3 we de-scribe the framework of CMMTL and its optimization, then we discuss its connection to previous methods. Section 4 shows the experimental results on two real world dataset-s. Finally the conclusion and future work are presented in Section 5.
In this section we review existing work related to semi-supervised learning, multi-task learning and cross-modal learn-ing.
Semi-supervised learning makes use of labeled and unla-beled data for training. There are various methods pro-posed for semi-supervised learning. The first type is genera-tive model [25], where the unlabeled samples are treated as missing variables and expectation maximization (EM) algo-rithm is used to iteratively train the classifier. Another set of semi-supervised methods is low-density separation such as transductive support vector machines (TSVM) [19]. Fi-nally, there are several graph-based methods [2, 29, 37, 36] for semi-supervised learning. Graph-based methods are ef-fective in correlating unlabeled and labeled data, and they are more related to our work than other semi-supervised approaches.

Multi-task learning is an approach that learns a classifier together with other related classifiers at the same time by using shared information. Since image annotation aims to predict multiple concepts for an image, multi-task learning is shown to be effective for this problem. Many kinds of information can be shared in multi-task learning, such as hidden units [6], training samples [3], feature sets [30], priors [20], etc.

The work which is most related to this paper is [18], where a shared subspace among multiple labels is learned for the supervised image classification. Then the correla-tion information contained in different labels is preserved in this shared subspace. The optimal solution to the subspace learning can be obtained by solving a generalized eigenval-ue problem. The idea of shared subspace learning is also combined with semi-supervised methods for image annota-tion. In [33] the subspace learning and graph learning are integrated into a joint framework. The concept correlation in subspace is learned from both labeled and unlabeled im-ages. [23] proposes a semi-supervised multi-task learning framework which also integrates graph learning and shared subspace learning, but the graph model it used is different to [33].

Our framework is based on graph learning and shared subspace learning. But the main difference is that we ana-lyze the cross-modal correlation, which is ignored in previ-ous multi-task or semi-supervised learning framework. The subspace learned in our framework preserves both concept correlation and cross-modal correlation.
The other related framework is cross-modal learning, the goal of cross-modal learning is to analyze the correlation of heterogeneous modalities by learning a shared structure a-mong them. In cross-modal learning, different modalities can be mapped into correlated subspaces. Canonical cor-relation analysis (CCA) [16] is a widely used cross-modal learning method, it learns subspaces for two modalities re-spectively, where the correlation between them is maximal. Kernel CCA (KCCA) [15] is a nonlinear extension of CCA, it uses a method known as the  X  X ernel trick X  which implicitly maps data into a higher dimensional feature space.
Most cross-modal methods are applied to the cross-media retrieval problem, where different modalities can search each other. A typical solution for cross-media retrieval is map-ping different modalities into a uniform semantic space [10]. In fact, semantic mapping can be seen as an annotation (classification) process. CCA and KCCA have been used to improve the semantic learning of images. In [10], CCA and KCCA are used before classification, which both result in better retrieval performance. Graph models are another kind of methods which have been used to learn the cross-modal correlation. [27] analyzes inter-media consistency and intra-media consistency for the cross-modal learning. Inter-media consistency is learned from the association between images and texts, intra-media consistency is learned from the uni-modal similarity graph. In [34], heterogeneous met-ric learning with joint graph regularization is proposed for the cross-modal analysis, it integrates the structure of dif-ferent modalities into a joint graph regularization. Based on the heterogeneous metric, high-level semantic representation of images and texts are learned through label propagation. Finally, the most related cross-modal method is cross-modal factor analysis (CFA) [21]. It optimizes an objective func-tion which is similar to the cross-modal regularization in our framework. All modalities are projected into correlated spaces by CFA.

There have existed some researches which try to exploit cross-modal correlation to improve the performance of im-age annotation. [31] first obtains the textual representation of images by using KNN on a multi-modal dataset collected from the web, then the classification performance of images can be improved by this textual representation. [14] first ob-tains a multiple kernel learning (MKL) classifier using both the image and text, and uses it to score unlabeled images with tags, it then learns classifiers on visual features on-ly, and this visual classifier is shown to be more effective than classifier learned from uni-modal images. Although existing work have demonstrated the effectiveness of using cross-modal correlation for image annotation, they have not explicitly analyzed this correlation. In our work, we explic-itly analyze the cross-modal correlation in a regularization term, and preserve it in a shared subspace.
In this section we will describe the formulation and opti-mization of our framework: cross-modal multi-task learning (CMMTL). The graphical illustration of our framework is demonstrated in Figure 2, which contains the training pro-cess and prediction process of the image annotation. From this figure we can find both labeled data and unlabeled data are used for training, and all data are multi-modal. Cross-modal correlation is the shared information of differ-ent modalities, and concept correlation is the shared infor-mation among different concepts, they are preserved in a subspace. In the training process, the similarity graph is constructed from all data, the concept correlation and the cross-modal correlation are all integrated by CMMTL. Then the projection matrix  X  and weight matrix W are learned by CMMTL. In the prediction process, the new image is single and without any additional information. Its visual feature is first projected into the shared subspace by the projection matrix. Then these two features are combined and concepts are predicted by using the weight matrix.
In this work we consider two modalities: image and text, and each image is associated with text such as tags. Suppose where D 1 is the dimension of image feature, X 1 l  X  R N l is the set of the labeled images, and X 1 u  X  R N u  X  D 1 is the set of unlabeled images. N l is the size of labeled data, N is the size of unlabeled data, and N l + N u = N . The corre-sponding textual tags form the text set X 2 = X T 2 l ,X T R
N  X  D 2 , where D 2 is the dimension of text feature. Then we define the class indicator matrix of training images as Y = Y T l ,Y T u T  X  { 0 , 1 } N  X  C , where Y l  X  { 0 , 1 } notes the semantic concepts of corresponding images, Y u is a N u  X  C matrix with all zero. C is the number of semantic concepts, given an image x i 1 with its associated tags x belongs to c -th concept, then y ic = 1, otherwise y ic = 0.
CMMTL is based on the graph learning framework [2], thus we have to construct the similarity graph. Unlike pre-vious uni-modal image annotation methods which only con-struct visual similarity graph [33], we construct the multi-modal similarity graph A which both considers the visual similarity and textual similarity. A is defined by the follow-ing equation: where d ij is the multi-modal similarity of image-text pairs x where d 1 is the visual similarity and d 2 is the textual sim-ilarity, we use L2 distance for visual similarity, and cosine distance for textual similarity.  X  1 is the mean of all visual similarities,  X  2 is the mean of all textual similarities.  X   X  2 are combination weights, they show the importance of their corresponding modalities. Since we mainly focus on the cross-modal correlation but not multi-modal fusion, we simply set both the two weights as 0.5, and the experimental results will show this simple combination is effective.
The core idea of CMMTL is learning a shared subspace where both concept correlation and cross-modal correlation are preserved. Different from previous multi-task learning frameworks which only analyze concept correlation, CMMTL analyzes both two correlations. The subspace learned by CMMTL is not only shared by different semantic concepts, but also shared by different modalities. The objective func-tion of CMMTL is: where  X  m  X  R D m  X  K is the subspace projection matrix for the m -th modality, and K is the dimension of the shared subspace. k X k 2 F denotes Frobenius norm.  X  ,  X  m and  X  are regularization parameters. Tr (  X  ) denotes trace operator. L = D  X  A is the Laplacian matrix, D is a diagonal matrix with D ii = is a diagonal matrix to assign different weights to different data, its diagonal element U ii = N if a document x i is la-beled and U ii = 0 otherwise. F m is the prediction function for the m -th modality, it is defined as: where P m  X  R D m  X  C and V m  X  R K  X  C are the weight vec-tors. Let W m = P m +  X  m V m , the prediction function can represented as F m = X m W m .

Substitute F m and W m into (3), then the objective func-tion can be reformulated as:
The first term of the objective function (5) is similar to common graph based semi-supervised learning framework. The middle term assures the subspace is shared by con-cepts, then the concept correlation can be learned from this term. The third term of is the cross-modal regularization, which makes the subspace also shared by different modali-ties, thus it learns cross-modal correlation. CMMTL inte-grates all the three learning approaches into a joint frame-work, which result in a different objective function to pre-vious frameworks. Based on the cross-modal regularization term k X 1  X  1  X  X 2  X  2 k 2 F , the projection matrix  X  m the m -th modality into a subspace which contains both cross-modal correlation and concept correlation. Cross-modal cor-relation is usually ignored by previous multi-task learning methods. As a result, the prediction function for image an-notation is both improved by these two correlations.
The proposed framework shown in (5) is nonconvex, but the global optimum can be obtained by solving a generalized eigenvalue problem. By setting the derivative of (5) w.r.t V to zero and using the property that k B k 2 F = Tr B T B , we have: Substituting V m into (5), the objective function becomes: By setting the derivative w.r.t W m to zero, we have: where Then the objective function arrives at: max s . t .  X  T 1  X  1 = I It has been proven in [18] that (10) can be reformulated to the following trace maximization problem: where In order to solve (11), we can rescale  X  m to make  X  T m S I and use it to replace the constrains in (11), then we can convert the objective function to:
At last we can arrive at the following optimization prob-lem: where (14) can be solved as a generalized eigenvalue problem. Once we have obtained W m ,m = 1 , 2, given a test image or text, we can predict its classification score. In this work we solve the problem of image annotation, thus only W 1 is used, in fact W 2 can be used to predict concepts for texts. The train-ing process of CMMTL are shown in Algorithm 1.
 Algorithm 1 Training process of CMMTL Input: Output: 1: for each modality m do 2: Compute M m according to (9); 3: Compute S m and R m according to (12); 4: end for 5: Compute Z and S according to (15); 6: Compute  X  by solving the generalized eigenvalue prob-7: for each modality m do 8: Compute W m according to (8); 9: Compute V m according to (6); 10: end for
In the previous subsections we only focus on two modali-ties, in this subsection we will show that our framework can be easily extended to the case of more than two modalities, which makes it also practical in the scenario where images are associated with tags and audio. For the case of more than two modalities, the objective function is: where M &gt; 2 is the number of modalities, and the other terms in (16) is same to (11). The only difference is the modality number, as a result, more cross-modal regulariza-tion terms is introduced for more cross-modal correlations.
Then (16) can be transformed into the following eigenval-ue problem: where Z S
 X  = (18) After solving (17), W m , V m can be computed by the same procedure described in Algorithm 1.
In this subsection, we discuss the connection between the proposed framework and two related methods: CFA[21] and MRMTL [23].

MRMTL is a semi-supervised multi-task learning method which optimizes: min According to (19), we can find if  X  = 0 in (5), then CMMTL becomes a common semi-supervised multi-task methods which is similar to MRMTL. However, there are slight difference between CMMTL and MRMTL in the graph-based terms.
 In our framework we use the matrix U to assign different weights to different samples, labeled data are more impor-tant than unlabeled data, thus they have larger weights.
CFA is a cross-modal learning method which minimizes the following objective function: According to (20), we can also find CFA can be regarded as a special case of CMMTL. If we set  X  =  X  , then the subspace learning of CMMTL is equivalent to CFA.

Basing on these two frameworks, we can obtain a baseline method: CFA+MRMTL, where the subspace projection ma-trix  X  is learned by CFA and weight matrix W is learned by MRMTL. MRMTL+CFA also analyzes both cross-modal and concept correlation, but it is only a simple combina-tion of two frameworks. Our CMMTL is superior to C-FA+MRMTL, in that it optimizes both two correlations in a joint objective function. Generally, the joint optimization will result in better results than optimizing two correlation separately. In this work, two real world multi-modal image datasets: MIR Flickr [17] and NUS-WIDE [9] are used for evaluation. They both consist of image-text pairs and are labeled by several semantic concepts. The statistics of the two datasets are summarized in Table 1.
 Table 1: Statitics of MIR Flickr and NUS-WIDE
MIR Flickr dataset consists of 25,000 images downloaded from the Flickr through its public API, and each image is associated with textual tags. Images in MIR Flickr are an-notated for 24 concepts, including object categories such as clouds, tree or dog, as well as more general categories such as sky, water or people. For 14 of the 24 concepts a second, stricter annotation was made: for each concept a subset of the positive images is selected where the concept is salient in the image. These more strictly annotated classes are de-noted by using  X  as a suffix. In total there are 38 categories for this dataset. The tags that appear at least 50 times in this dataset are kept, resulting in a vocabulary of 457 tags. For MIR Flickr, 15 visual features 4 [14] are used for image modality, including 4 types of SIFT BOVW, 4 types of Hue BOVW, 4 types of color histograms and GIST. Tag features are 457-D binary vector represented by the presence of 457 tags.

NUS-WIDE dataset contains 269,648 image downloaded from Flickr. Each image is also associated with textual tags, and all images are labeled by 81 concepts. On NUS-WIDE dataset we directly use features 5 provided by [9]: image fea-tures include 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments and 500-D BOVW based on SIFT descriptions. Text features are 1000-D vectors represented by the presence of 1000 tags.

In our experiment, MIR Flickr is split to 12,500 training images and 12,500 test images [14], where 1,250 of the train-ing images are chosen as labeled, and the rest are assumed to be unlabeled. NUS-WIDE is split to 161,789 training images and 107,859 test images. On NUS-WIDE, 1,250 training im-ages is chosen as labeled, and the rest training images are assumed to be unlabeled. NUS-WIDE contains more images and concepts, and we choose the same size of labeled images to MIR Flickr, thus the annotation task on NUS-WIDE is more challenging.

Images in the two datasets are represented by multiple vi-sual features, and our method is only suited to single feature for each modality. Therefore, we should do the preprocess-ing for visual features. In this work, we use kernel PCA (KPCA) [26] to transform multiple visual features to a sin-gle feature. Due to the limit of memory, if the training set is very large (larger than 100,000), it is unfeasible to use all training data for KPCA training, thus on both two datasets we randomly select 10,000 training images. Histogram inter-section kernel is used for all image histograms and BOVWs,
All features of MIR Flickr can be downloaded from http://lear.inrialpes.fr/people/guillaumin/data.php
All features of NUS-WIDE can be downloaded from http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm and RBF Kernel is used for other visual features. All visual kernels are linearly combined to train the KPCA. Finally we obtain a 500-D visual feature for each image on MIR Flickr, and a 100-D visual feature for each image on NUS-WIDE.
We first compare our method with two baseline meth-ods, including Least Square Regression (LSR) which can show the performance of the traditional uni-modal super-vised learning, and CFA+MRMTL which also uses both cross-modal and concept correlation. We also compare our methods with several representative methods for the cross-modal image annotation. The details of all methods are described as follows:
In this section we show the experimental results on two datasets, we measure performance using the average preci-sion (AP) criterion for each concept, and also using the mean AP (mAP) over all concepts.

We choose the AP score used in PASCAL VOC challenge evaluation [11], it is defined as: where P ( r ) is the maximum precision over all recalls larger performance.
 Table 4: The comparison of mAP scores on two datasets
The AP scores of all 38 concepts on MIR Flickr are shown in Table 2, where the best results for each concept are marked in bold. From Table 2 we can observe that CMMTL per-forms best at majority concepts, it obtains the best AP scores for 27 concepts. For the other 11 concepts where CMMTL does not perform the best, its AP scores are close to the highest scores. LSR performs worse than other meth-ods, which shows that using either cross-modal correlation or concept correlation will improve the annotation perfor-mance. Co-training and MKL+LSR make use of the cross-modal correlation, and MRMTL is a multi-task learning framework which analyzes concept correlation. All of them perform significantly better than LSR. In addition, we can also find for most concepts CFA+MRMTL always perform-s better than other methods except CMMTL, which con-firms the advantages of combining cross-modal correlation and concept correlation. However, CFA+MRMTL simply combines two correlations, it is a special case of CMMTL. Therefore, CMMTL which integrates two correlations into a joint framework can significantly improve the performance. Table 3 shows the AP scores of all 81 concepts on NUS-WIDE, where the best results for each concept are marked in bold. NUS-WIDE is more challenging than MIR Flickr, therefore, although the same size (1,250) of labeled images are used for training, the scores on NUS-WIDE are much lower. The improvement of CMMTL on NUS-WIDE is not larger than MIR Flickr. CMMTL does not obtain majority best AP scores, but it obtains best AP scores at 23 concepts, which is still more than other methods. We can also find LSR performs the worst, which confirms the benefit of using cross-modal correlation or concept correlation. Finally, the performance of other methods is similar.

Table 4 shows the mAP scores obtained by six methods on two datasets. mAP scores show the overall performance for all concepts. We can find the results in Table 4 is consistent with AP scores in Table 2 and Table 3. CMMTL perform-s best on both two datasets ,and LSR performs the worst. CFA+MRMTL performs better than other methods except CMMTL, which demonstrates the effectiveness of combin-ing cross-modal and concepts correlation, and CMMTL can better combine these two correlations.
In this paper we propose the framework of cross-modal multi-task learning (CMMTL) which integrates two effective frameworks: semi-supervised multi-task learning and cross-modal learning. Concept correlation analyzed by multi-task learning is shown to be benefit to image annotation. Cross-modal correlation also can be used to improve the perfor-mance of annotation. The advantage of CMMTL is that it learns both two correlations in a shared subspace, which will bring a significant improvement. Although CMMTL has to use multi-modal data for training, the classifiers it learned can predict concepts for a single image without any additional information, thus it is more practical than tradi-tional multi-modal methods. Experimental results on two datasets: MIR Flickr and NUS-WIDE have shown CMMTL performs better than several previous related methods for image annotation.

Besides the task of image annotation, our framework may be applied to other promising applications. CMMTL can be used to learn the semantic concepts of videos. Videos shared on the web usually have three modalities at least, e.g., images, audio and text. Thus the annotation of video is different to image, and the extension of CMMTL can be used for video annotation. Furthermore, our CMMTL can be used for cross-modal multimedia retrieval. The typical solution for cross-modal retrieval is to learn an uniform rep-resentation for different modalities. In CMMTL, two type-s of features can be used to represent different modalities. The first is semantic concepts, we can obtain the features of semantic concepts for all modalities via our framework. The other is the shared subspace learned by CMMTL, s-ince all modalities is correlated in this subspace, it can be used to represent all modalities for retrieval. Both concepts and shared subspace learned by CMMTL are shown to be effective, thus the performance of CMMTL in cross-modal retrieval may be promising. [1] P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [3] S. Bickel, J. Bogojeska, T. Lengauer, and T. Scheffer. [4] D. M. Blei and M. I. Jordan. Modeling annotated [5] A. Blum and T. Mitchell. Combining labeled and [6] R. Caruana. Multitask learning. Machine learning , [7] C.-C. Chang and C.-J. Lin. Libsvm: a library for [8] X. Chen, Y. Mu, S. Yan, and T.-S. Chua. Efficient [9] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and [10] J. Costa Pereira, E. Coviello, G. Doyle, N. Rasiwasia, [11] M. Everingham, L. Van Gool, C. Williams, J. Winn, [12] S. Feng, R. Manmatha, and V. Lavrenko. Multiple [13] M. Guillaumin, T. Mensink, J. Verbeek, and [14] M. Guillaumin, J. Verbeek, and C. Schmid.
 [15] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. [16] H. Hotelling. Relations between two sets of variates. [17] M. J. Huiskes and M. S. Lew. The mir flickr retrieval [18] S. Ji, L. Tang, S. Yu, and J. Ye. A shared-subspace [19] T. Joachims. Transductive inference for text [20] J. M. Leiva-Murillo, L. G  X omez-Chova, and [21] D. Li, N. Dimitrova, M. Li, and I. K. Sethi. [22] N. Liu, E. Dellandrea, C. Zhu, C.-E. Bichot, and [23] Y. Luo, D. Tao, B. Geng, C. Xu, and S. J. Maybank. [24] Z. Ma, Y. Yang, F. Nie, J. Uijlings, and N. Sebe. [25] K. Nigam, A. K. McCallum, S. Thrun, and [26] B. Sch  X  olkopf, A. Smola, and K.-R. M  X  uller. Nonlinear [27] J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen. [28] N. Srivastava and R. Salakhutdinov. Multimodal [29] M. Szummer and T. Jaakkola. Partially labeled [30] A. Torralba, K. P. Murphy, and W. T. Freeman. [31] G. Wang, D. Hoiem, and D. Forsyth. Building text [32] Y. Xiang, X. Zhou, T.-S. Chua, and C.-W. Ngo. A [33] Y. Yang, F. Wu, F. Nie, H. T. Shen, Y. Zhuang, and [34] X. Zhai, Y. Peng, and J. Xiao. Heterogeneous metric [35] S. Zhang, J. Huang, Y. Huang, Y. Yu, H. Li, and [36] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [37] X. Zhu, Z. Ghahramani, J. Lafferty, et al.
 [38] A. Znaidia, A. Shabou, A. Popescu, H. Le Borgne,
