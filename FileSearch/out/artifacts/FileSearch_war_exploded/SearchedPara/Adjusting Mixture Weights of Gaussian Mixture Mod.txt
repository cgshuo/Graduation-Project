 Mixture models, such as Gaussian Mixture Model, have been widely used throughout the applications of data mining and machine learning. For example, Gaussian Mixture model (GMM) has been applied for time series classification [8], image texture detec-data points from a specific object or class (e.g., a speaker in speaker identification) are generated from a pool of Gaussian models with fixed mixture weights; it estimates mixture models from the training data using a maximum likelihood method; it pre-dicts test data with the classes that generate the test data with the largest probabilities. 
One general problem of modeling data with GMM is that GMM uses the same set of mixture weights for all the data points of a particular class, which limits the power of the mixture model in fitting the training data accurately. In contrast, a probabilistic latent semantic analysis (PLSA) [5][6] model allows each data point to choose its own mixture weights. Apparently, PLSA model is more flexible than GMM model in that a different set of mixture weights is introduced for each data point. However, as a trade-off, PLSA has a substantially larger parameter space than the GMM model; the exces-sive freedom of assigning data point dependant mixture weights invites the PLSA model to the potential overfitting problem given the limited amount of training data. 
In this paper, we propose a regularized probabilistic latent semantic analysis (RPLSA) model that addresses the overfitting problem in PLSA by regularizing the mixture weights. In particular, a regularization term is introduced in RPLSA, which punishes the objective function in RPLSA when different data points of the same class between GMM and PLSA: different mixture weights are allowed for data points; but similar mixture weights are favored for different data points in the same class. 
Empirical study for the application of speaker identification was conducted to show the effectiveness of the new RPLSA model. The NIST 1999 speaker recognition evaluation dataset with 539 speakers were used and the experiment results indicate that the RPLSA model achieves better results than both the GMM and PLSA. Fur-thermore, careful analysis shows that the advantage of RPLSA comes from the power of properly adjusting model flexibility. In this section, we only survey the most related research of mixture model. 2.1 Gaussian Mixture Model GMM is one of the most widely used mixture modeling techniques [4][7][8][9]. It is a simple model and is reasonably accurate when data are generated from a set of Gaus-from the ith class (e.g., a particular speaker). They are modeled by a total number of J Gaussians as follows: where i GMM  X  includes all the model parameters, i.e., { (), j Pz ,, jj u  X  1 jJ  X  X  X  }. 
Pxu  X  is the Gaussian distribution for the j-th class, with a mean vector u and a covariance matrix j  X  as: where D is the dimension of the feature vector t x . Usually j  X  is set to be a diagonal matrix as 2 {:1 } jd diag d D  X   X  X  X  in order to reduce the size of the parameter space [4]. 
It can be seen from Equation (1) that the data points of a specific class are gener-ample, in speaker identification, mixture weights for a vowel can be significantly differ-data point dependent mixture weights into the framework of mixture models. 2.2 Probabilistic Latent Semantic Analysis Unlike the Gaussian Mixture Model, the probabilistic latent semantic analysis model (PLSA) allows for data point specific mixture weights. Formally, the likelihood of training data for the ith class is written as: dummy variable t d is introduced for every data point, and therefore the mixture posed for the probabilistic semantic indexing (PLSI) technique of information re-trieval [5][6]. Both PLSI and PLSA allow data point specific mixture weights, but the PLSI model is based on multinomial distributions to model documents while the PLSA model is used here for modeling continuous data with Gaussian distributions. Note that the PLSA model shares the same idea with the tied-mixture model tech-nique [1], which assumes that speech data is generated from a common pool of Gaus-sian models and each data point can choos e its own mixture weights independently. 
Because the mixture weights are data point dependent, PLSA is capable to fit train-data. To alleviate this problem, a maximum posterior (MAP) smoothing technique can be used for estimating PLSA. In particular, priors are introduced for parameters in the Gaussian models, and the parameters are estimated by maximizing the posterior of training data: 
The first item on the right hand side is the likelihood of training data. The next two items are the conjugate priors for the means and variances in the Gaussian models. A and B are two constants that adjust the weights of priors. 00 (|,) j Pu u  X  is a Gaussian 
Pa  X   X  is an inverse gamma distribution with parameters 
Although maximum posterior smoothing can alleviate the overfitting problem in some extent, the PLSA model still suffers from the excessive freedom of assigning totally independent data point specific mixture weights. To further address this prob-lem, a novel method of regularizing mixture weights is proposed in this paper. 2.3 Latent Dirichlet Allocation Latent Dirichlet Allocation (LDA) [2] is a generative model for collections of discrete eled as a finite mixture over a set of topi cs (mixture models). LDA shares a common feature with the new research in this paper in that both of them choose moderate amount of model flexibility. LDA assumes that the mixture weights of items in a class data points in the same class are coupled instead of being chosen independently. 
However, LDA model requires sophiscated variational methods to calculate the model parameters both in training and testing phrases, which is time consuming and thus limits the application of LDA in practical work. Furthermore, LDA model does not work well when each item contains a very small number of data points (like documents contain small number of words by average, or in speaker identification each item of a speaker utterance is a single vector of acoustic features in multi-dimensional space). Specifica lly consider the extreme case when each item only con-tains a single data point. LDA models a class i X with single data point items as: data points. By switching the order of integration and summation and integrating out the parameter u , Equation (6) becomes: This is essentially a GMM model if we set ' the GMM model. From the above research, we find that bot h GMM and PLSA are two extreme cases of the mixture model family: GMM uses the same set of mixture weights for all data points of the same class, thus lacking flexibility; PLSA model allows each data point to choose its own mixture weights and therefore is prone to overfitting training data. A better idea is to develop an algorithm that can properly adjust the amount of model flexibility so that not only the training data can be fit well but also the model is robust semantic analysis model (RPLSA). 3.1 Model Description Similar to the PLSA model, RPLSA allows each data point to choose its own mixture weights. At the meantime, it requires mixture weights from different data points to be similar in order to avoid overfitting. This is realized by assuming that there is a com-mon set of mixture weights and mixture weights for different training data points should be close to the common set of mixture weights, formally as: log ( | ) log( ( | ) ( | , )) log ( | , ) Compared to the PLSA model in Equation (4), the above equation introduces a new regularization term, i.e., is a weighted sum of the Kullback-Leibler (KL) divergence between the common model flexibility. 
The role of the regularization term is to enforce mixture weights for different data weights are to the common set of mixture weights, the smaller the KL divergence will RPLSA model: A small C will lead to a large freedom in assigning different mixture weights to different data points, thus exhibiting a behavior similar to the PLSA model; A large C will strongly enforce different data points to choose similar mixture weights, thus close to the behavior of the GMM method. Therefore, the RPLSA model connects the spectrum of mixture models between GMM and PLSA. 3.2 Parameter Estimation The Expectation-Maximization (EM) algorithm [1] is used to estimate the model parameters of the RPLSA model. In the E step, the posterior probability of which mixture model each data point belongs to is calculated as follows: tions (10), (11) and (12) separately. where jd u and jd  X  are the dth element of the mean and variance respectively for the jth mixture, and td x is the dth element of the feature vector t x . 
Finally, the common set of mixture weights is updated as follows: which is essentially the geometric mean of the corresponding mixture weights that are attached to each data point. Note that ch oice of adaptively adjusting the common set of mixture weights in Equation (13) is different from the method that simply selecting a prior distribution of the mixture weights and estimating the model with maximum posterior smoothing. It can be imagined that the same set of prior of mixture weights weights) does not fit data with different ch aracteristics. The adaptive estimation of the common set of mixture weights in RPLSA is a more reasonable choice. The parameter estimation procedure for PLSA is a simplified version of that for formula as Equation (9) without the factor of the regularization item. In the maximi-a similar way as the Equations (10), (11) and (12). 3.3 Identification The RPLSA model is different from the GMM model in that some parameters Pz d need to be estimated for the test data in the identification phase. A plug-in EM procedure is used to acco mplish this. Specifically, the EM algorithm described in parameters are fixed. With the estimated new mixture weights, we can identify the 
X as: The identification process of PLSA is almo st the same as the procedure of RPLSA, which is not described due to space limit. This section shows empirical study that de monstrates the advantage of the new regu-larized probabilistic latent semantic model (RPLSA). Specifically, three models of GMM, PLSA and RPLSA are compared for the application of speaker identification. 4.1 Experiment Methodology The experiments were conducted on the NIST 1999 speaker recognition evaluation dataset 1 . There are a total of 309 female speakers and 230 male speakers. The speech signal was pre-emphasized using a coeffi cient of 0.95. Each frame was windowed with a Hamming window and set to 30ms long with 50% frame overlap. 10 mel fre-quency cepstral coefficients were extracted from each speech frame. Both the training data and the test data come from the same channel. The training data consists of speech data of 7.5 seconds for each training speaker. 
We present experiment results to address two issues: 1) Will the proposed RPLSA be more effective than the GMM and the PLSA models? 2) What is the power of the RPLSA model? What is the behavior of the RPLSA model with different amount of model flexibility by choosing different values for the regularization parameter C? 4.2 Experiment Results of Different Algorithms mixture models. The numbers of mixture models were chosen by cross-validation for the three models. Specifically, 30 mixtures for GMM model, 50 for both PLSA and RPLSA. The smoothing prior parameters of PLSA and RPLSA were set as follows: cates the number of items within a class). The regularization constant C of RPLSA was set to be 20 by cross-validation. To compare the algorithms in a wide range we tried various lengths of test data. The results are shown in Table1. Clearly, both PLSA and RPLSA are more effective RPLSA relax the constraint on mixture weights imposed by GMM. Furthermore, the RPLSA model outperforms the PLSA model. This is consistent with the motivation of the RPLSA model as it automatically adjusts the model flexibility for better recogni-tion accuracy. To further confirm the hypothesis that RPLSA model has advantage than both the GMM and PLSA methods, two more sets of experiments were conducted. The first set of extended experiments was to train a GMM model with smoothed Gaussian model parameters like that used for PLSA (Two smoothed items of Gaussian model parameters like that of Equation (4) were introduced into the GMM objective function with A and B roughly tuned to be five times smaller than that of the RPLSA setting). The second set of extended experiments was to regularize the mixture weights in PLSA using a Dirichlet prior as described in Section 3.2. It is different from the regu-larization scheme of Equation (9) in that a Dirichlet prior uses a fixed set of common PLSA is trained with a new likelihood function of Equation (4) with an additional item of a Dirichlet prior with the parameter values of 100 (roughly tuned). 
It can be seen from Table 2 that the ne w versions of GMM and PLSA give very small improvement of the original algorithms. The behavior of GMM model can be explained as that GMM has a much smaller parameter space than PLSA and RPLSA, smoothing does not give too much help. The results of the PLSA model with uniform Dirichlet prior indicates that the simple method of smoothing the mixture weights with a single prior does not successfully solve the overfitting problem. 4.3 Study the Behavior of the RPLSA Method The new proposed RPLSA is an intermediate model between GMM and PLSA: dif-for different data points are encouraged. The RPLSA is the bridge to connect a spectrum of mixture models with two extreme cases of GMM and RPLSA models. Therefore, it is very interesting to investigate the behavior of the RPLSA method with different amount of model flexibly and its relationship with the GMM and RPLSA models. 
Specifically, different values of parameter C in the RPLSA model of Equation (8) tailed results are shown in Figure 1. 
According to previous anal ysis in Section 3.1, we know that a smaller C value gives more freedom to the data points in choosing their own mixture weights, which servation from Figure 1. When C is as small as 10, RPLSA acquires a similar recogni-tion accuracy with that of PLSA. On the other hand, a larger value for C makes RPLSA behave more like GMM. As indicated in Figure 1, a larger C leads to worse recognition accuracy. 
For the middle part of the curve, with C ranging from 15 to 40, RPLSA acquires the best recognition accuracy; this suggests that RPLSA with reasonable amount of model flexibility reaches a better trade-o ff between enough model flexibility and model robustness.

The experiments in this section show the behavior of the new RPLSA model with different amount of model flexibility. It is consistent with our theoretical analysis that RPLSA has advantage than the GMM model and the RPLSA model in its better abil-ity to adjust the appropriate amount of model flexibility. Mixture models such as Gaussian mixture model (GMM) are very important tools for data mining and machine learning applications. However, classic mixture models like required to be generated from a pool of mixtures with the same set of mixture weights. Previous research such as the probabilistic latent semantic analysis (PLSA) model has been proposed to release this constraint. PLSA allows totally independent data point specific mixture weights. But the excessive model flexibility makes PLSA tend to suffer from the overfitting problem. 
This paper proposes a new regularized PLSA (RPLSA) model: On one hand, it is weights for different data points are required to be similar to each other. In particular, the new model has the ability in adjusting the model flexibility of the mixture weights through the regularization term. Experiment results for speaker identification applica-tion have shown that the new RPLSA model outperforms both the GMM and the PLSA models substantially. Choosing the appropriate amount of modeling flexibility paper can be naturally incorporated with other types of mixture models than the GMM model and be applied for other applications. We thank Alex Waibel and Qin Jin for their helpful discussion of this work. 
