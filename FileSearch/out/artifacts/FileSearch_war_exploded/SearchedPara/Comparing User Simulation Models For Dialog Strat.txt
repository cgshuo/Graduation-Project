 Recently, user simulation has been used in the de-velopment of spoken dialog systems. In contrast to experiments with human subjects, which are usually expensive and time consuming, user simulation gen-erates a large corpus of user behaviors in a low-cost and time-efficient manner. For example, user sim-ulation has been used in evaluation of spoken dia-log systems (L  X  opez-C  X  ozar et al., 2003) and to learn dialog strategies (Scheffler, 2002). However, these studies do not systematically evaluate how helpful a user simulation is. (Schatzmann et al., 2005) pro-pose a set of evaluation measures to assess the re-alness of the simulated corpora (i.e. how similar are the simulated behaviors and human behaviors). Nevertheless, how realistic a simulated corpus needs to be for different tasks is still an open question.
We hypothesize that for tasks like system eval-uation, a more realistic simulated corpus is prefer-able. Since the system strategies are evaluated and adapted based on the analysis of these simulated dia-log behaviors, we would expect that these behaviors are what we are going to see in the test phase when the systems interact with human users. However, for automatically learning dialog strategies, it is not clear how realistic versus how exploratory (Singh et al., 2002) the training corpus should be. A train-ing corpus needs to be exploratory with respect to the chosen dialog system actions because if a cer-tain action is never tried at certain states, we will not know the value of taking that action in that state. In (Singh et al., 2002), their system is designed to randomly choose one from the allowed actions with uniform probability in the training phase in order to explore possible dialog state spaces. In contrast,we use user simulation to generate exploratory training data because in the tutoring system we work with, reasonable tutor actions are largely restricted by stu-dent performance. If certain student actions do not appear, this system would not be able to explore a state space randomly .

This paper investigates what kind of user simula-tion is good for using Markov Decision Processes ( MDPs ) to learn dialog strategies. In this study, we compare three simulation models which differ in their efforts on modeling the dialog behaviors in a training corpus versus exploring a potentially larger dialog space. In addition, we look into the impact of different state space representations and different re-ward functions on the choice of simulation models. Our system is a speech-enabled Intelligent Tutor-ing System that helps students understand qualita-tive physics questions. The dialog policy was deter-ministic and hand-crafted in a finite state paradigm (Ai et al., 2006). We collected 130 dialogs (1019 student utterances) with 26 human subjects. Cor-rectness (correct( c ), incorrect( ic )) is automatically judged by the system 1 and kept in the system X  X  logs. Percent incorrectness ( ic% ) is also automatically calculated and logged. Each student utterance was manually annotated for certainty ( certain , uncer-tain , neutral , mixed ) in a previous study 2 based on both lexical and prosodic information. In this study, we use a two-way classification (certain( cert ), not-certain( ncert )), where we collapse uncertain , neu-tral , and mixed to be ncert to balance our data. An example of coded dialog between the tutor ( T ) and a student ( S ) is given in Table 1. 3.1 Learning Task Our current system can only respond to the cor-rectness of a student X  X  utterances; the system thus ignores other underlying information, for exam-ple, certainty which is believed to provide use-ful information for the tutor. In our corpus, the strength of the tutor X  X  minimal feedback (defined be-low) is in fact strongly correlated with the percent-age of student certainty (chi-square test, p &lt; 0.01). Strong Feedback ( SF ) is when the tutor clearly states whether the student X  X  answer is correct or incor-rect (i.e.,  X  X his is great! X ); Weak Feedback ( WF ) is when the tutor does not comment on the correct-ness of a student X  X  answer or gives slightly negative feedback such as  X  X ell X . Our goal is to learn how to manipulate the strength of the tutor minimal feed-back in order to maximize student X  X  overall certainty in the entire dialog. We keep the other parts of the tutor feedback (e.g. explanations, questions) so the system X  X  original design of maximizing the percent-age of student correct answers is utilized. 3.2 Simulation Models All three models we describe below are trained from the real corpus we collected. We simulate on the word level because generating student X  X  dialog acts alone does not provide sufficient information for our tutoring system to decide the next system X  X  ac-tion. Thus, the output of the three models is a stu-dent utterance along with the student certainty ( cert , ncert ). Since it is hard to generate a natural lan-guage utterance for each tutor X  X  question, we use the student answers in the real corpus as the candidate answers for the simulated students (Ai et al., 2006). In addition, we simulate student certainty in a very simple way: the simulation models output the cer-tainty originally associated with that utterance.
Probabilistic Model (PM) is meant to capture re-alistic student behavior in a probabilistic way. Given a certain tutor question along with a tutor feedback, it will first compute the probabilities of the four types of student answers from the training corpus: c and cert , c and ncert , ic and cert , and ic and ncert . Then, following this distribution, the model selects the type of student answers to output, and then it picks an utterance that satisfies the correctness and certainty constraints of the chosen answer type from the candidate answer set and outputs that utterance. We implement a back-off mechanism to count pos-sible answers that do not appear in the real corpus.
Total Random Model (TRM) ignores what the current question is or what feedback is given. It ran-domly picks one utterance from all the utterances in the entire candidate answer set. This model tries to explore all the possible dialog states.

Restricted Random Model (RRM) differs from the PM in that given a certain tutor question and a tutor feedback, it chooses to give a c and cert , c and ncert , ic and cert , or ic and ncert answer with equal probability. This model is a compromise between the exploration of the dialog state space and the re-alness of generated user behaviors. 3.3 MDP Configuration A MDP has four main components: states, actions, a policy, and a reward function. In this study, the ac-tions allowed in each dialog state are SF and WF ; the policy we are trying to learn is in every state whether the tutor should give SF and WF in order to maximize the percent certainty in the dialog.
Since different state space representations and re-ward functions have a strong impact on the MDP policy learning, we investigate different configura-tions to avoid possible bias introduced by certain configurations. We use two state space representa-tions: SSR1 uses the correctness of current student turn and percent incorrectness so far; and SSR2 adds in the certainty of the current student turn on top of SSR1 . Two reward functions are investigated: in RF1 , we assign +100 to every dialog that has a per-cent certainty higher than the median from the train-ing corpus, and -100 to every dialog that has a per-cent certainty below the median; in RF2 , we assign different rewards to every different dialog by multi-plying the percent certainty in that dialog with 100. Other MDP parameter settings are the same as de-scribed in (Tetreault et al., 2006). 3.4 Methodology We first let the three simulation models interact with the original system to generate different training cor-pora. Then, we learn three MDP policies in a fixed configuration from the three training corpora sep-arately. For each configuration, we run the sim-ulation models until we get enough training data such that the learned policies on that corpus do not change anymore (40,000 dialogs are generated by each model). After that, the learned new policies are implemented into the original system respectively 3 . Finally, we use our most realistic model, the PM , to interact with each new system 500 times to eval-uate the new systems X  performances. We use two evaluation measures. EM1 is the number of dialogs that would be assigned +100 using the old median split. EM2 is the average of percent certainty in ev-ery single dialog from the newly generated corpus. A policy is considered better if it can improve the percentage of certainty more than other policies, or has more dialogs that will be assigned +100. The baseline for EM1 is 250, since half of the 500 di-alogs would be assigned +100 using the old median split. The baseline for EM2 is 35.21%, which is obtained by calculating the percent certainty in the corpus generated by the 40,000 interactions between the PM and the original system. Table 2 summarizes our results. There are two columns under each  X  X tate representation+reward function X  configuration, presenting the results using the two evaluation approaches. EM1 measures ex-actly what RF1 tries to optimize; while EM2 mea-sures exactly what RF2 tries to optimize. However, we show the results evaluated by both EM1 and EM2 for all configurations since the two evaluation measures have their own practical values and can be deployed under different design requirements. All results that significantly 4 outperform the corre-sponding baselines are marked with  X  .

When evaluating using EM1 , the RRM signifi-cantly 4 outperforms the other two models in all con-figurations (in bold in Table 2). Also, the PM per-forms better (but not statistically significantly) than the TRM . When evaluating on EM2 , the RRM sig-nificantly 4 outperforms the other two when using SSR1 and RF1 (in bold in Table 2). In all other configurations, the three models do not differ signif-icantly. It is not surprising that the RRM outper-forms the PM in most of the cases even when we test on the PM . (Schatzmann et al., 2005) also ob-serve that a good model can still perform well when tested on a poor model.

We suspect that the performance of the PM is harmed by the data sparsity issue in the real cor-pus that we trained the model on. Consider the case of SSR1 : 25.8% of the potentially possible dialog states do not exist in the real corpus. Although we implement a back-off mechanism, the PM will still have much less chance to transition to the states that are not observed in the real corpus. Thus, when we learn the MDP policy from the corpus generated by this model, the actions to take in these less-likely states are not fully learned. In contrast, the RRM transitions from one state to each of the next possible states with equal probability, which compensates for the data sparsity problem. We further examine the results obtained using SSR1 and RF1 and evaluated by EM1 to confirm our hypothesis. When looking into the frequent states 5 , 70.1% of them are seen fre-quently in the training corpus generated by the PM , while 76.3% are seen frequently in the training cor-pus generated by the RRM . A higher percentage in-dicates the policy might be better trained with more training instances. This explains why the RRM out-performs the PM in this case.

While the TRM also tries to explore dialog state space, only 65.2% of the frequent states in testing phase are observed frequently in the training phase. This is because the Total Random Model answers 90% of the questions incorrectly and often goes deeply down the error-correction paths. It does ex-plore some states that are at the end of the paths, but since these are the infrequent states in the test phase, exploring these states does not actually im-prove the model X  X  performance much. On the other hand, while the student correctness rate in the real corpus is 60%, the RRM prevents itself from being trapped in the less-likely states on incorrect answer paths by keeping its correctness rate to be 50%.
Our results are preliminary but suggest interest-ing points in building simulation models: 1. When trained from a sparse data set, it may be better to use a RRM than a more realistic PM or a more ex-ploratory TRM ; 2. State space representation may not impact evaluation results as much as reward functions and evaluation measures, since when us-ing RF2 and evaluating with EM2 , the differences we see using RF1 or EM1 become less significant.
In our future work, we are going to further investi-gate whether the trends shown in this paper general-ize to on-line MDP policy learning. We also want to explore other user simulations that are designed for sparse training data (Henderson et al., 2005). More importantly, we are going to test the new policies with the other simulations and human subjects to validate the learning process.
 NSF (0325054, 0328431) supports this research. The authors wish to thank Tomas Singliar for his valuable suggestions, Scott Silliman for his support on building the simulation system, and the anony-mous reviewers for their insightful comments.
