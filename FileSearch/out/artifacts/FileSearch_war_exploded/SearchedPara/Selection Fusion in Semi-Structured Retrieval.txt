 Semi-structured retrieval aims at providing focused answers to the users queries. A successful retrieval experience in semi-structured environment would mean a satisfactory com-bination of (a) matching or scoring and (b) selection of ap-propriate and focused fragments of the text. The need to re-trieve items of different sizes arises today with users querying the retrieval systems with varied use case, user interface and screen-size requirements. Which means that different selec-tion scenario serve different requirements and constraints. Hence we propose, a novel type of fusion; the selection fu-sion  X  a fusion methodology which fuses an all-purpose and comprehensive ranking of elements with a specific selection scheme, and also enables evaluation of the ranking in many selection perspectives. With the standard Wikipedia XML test collection, we are able to demonstrate that a strong and competitive baseline ranking system improves retrieval quality irrespective of the selection criteria. Our baseline ranking system is based on data fusion over the official sub-mitted runs at INEX 2009.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models ; H.3.4 [ Information Storage and Retrieval ]: System and Software X  perfor-mance evaluation ; H.2.1 [ Database Management ]: Log-ical Design X  data models ; E.1 [ Data ]: Data structures X  trees ; E.5 [ Data ]: Files X  organization/structure Measurement, Performance, Design, Standardization, Ex-perimentation XML retrieval; Semi-structured data; Structural indices; Sch-ema agnostic search; Contextualization; Re-weighting; Fu-sion Copyright c  X  2013 ACM 978-1-4503-2263-8/13/10 ...$15.00.
A relevant document is not always completely relevant; instead, the relevant information may be embedded some-where in the document, available only in the part(s) of the document. Thus, the traditional definition of finding rele-vant documents applies rather to document retrieval than to information retrieval, which refers to finding relevant in-formation from the documents [11, 22]. Document retrieval leaves the latter task to the end-users, whereas semi-struc-tured retrieval endeavours to provide direct access to the relevant portion of the document. For semi-structured re-trieval documents X  logical structure needs to be denoted with a mark-up language, typically in the self-describing form, i.e., the XML. Hence XML Information Retrieval (XML IR) is a type of semi-structured retrieval [2].

Document retrieval delimits a retrieval unit in a simple way. In semi-structured retrieval, the retrievable unit has to be defined according to the user and situation specific requirements. Both the varying information needs and the varying screen-sizes of the devices set requirements for the retrievable units of the information to be returned. We refer to this as the granularity level of the retrieval units.
In semi-structured retrieval, the notion of selectivity has an important role to play [1, 9]. The granularity level (spe-cific part of the document -element) at which we want to present or user would like to see the results. The users in this domain are not interested in the whole document, as a search outcome, but rather in the most specific portion of document where the relevant information can be found. Thus, there are two essential tasks in XML IR: (1) the rank-ing of the retrieval units according to the relevance scores and (2) the selection of the appropriate granularity level or element type [6, 23, 29]. The standard evaluation of XML retrieval forces system developers to take both of the tasks into account simultaneously. This means that a good rank-ing performs poorly, if the selection is not successful. Un-fortunately, any of the contemporary metrics do not reveal what actually went wrong in that very common case.

Hence, there should be a method to study and develop the ranking of XML retrieval as it own case (independently) and then this ranking with different selection schemata. In this study, we suggest that ranking and selection can be successfully combined using two distinct systems for these tasks by performing a selection fusion . In a nutshell, in selection fusion, the following two component systems are fused together:
The ranking system 1 ranks all elements of the collection (the comprehensive list) according to their estimated rel-evance, and the selection scheme removes all structurally inadequate or not needed elements from that comprehen-sive list. In the experiments, we are able to show that a good ranking system performs well with any selection scheme. Our baseline ranking system is based on simplis-tic and primitive fusion methods, such as, CombSUM [30], reciprocal ranking [8] and structural contextualization [26]. Finally, the selection fusion for the ranking is done with the different runs of INEX 2009 as selection systems, one at a time.

Our scoring system, which is based on the fusion meth-ods [8, 24, 30] and the structural contextualization [25, 26], provides a firm basis for a mechanism or a methodology which makes possible the retrieval of items with varying length and specificity, and an above average performance.
In the structural fusion , a relevant but not sufficiently large-enough or deep element, in a semi-structured docu-ment, is boosted by the evidence lying in its structural sur-roundings [6, 26]. In the opposite case, the structurally surrounding elements should degrade the importance of the non-relevant element.

The hypothesis is that a sufficiently good-enough scoring system which is also capable of retrieving a comprehensive set of items on the query topic(s) (the structural fusion, in this study), blended (fused) with a selection scheme (INEX 2009 submitted run  X  itself a semi-structured retrieval re-sult, in this study), improves the retrieval effectiveness of the selection scheme, independent of the selection scheme.
A selection criteria or scheme, for example, which craves to retrieve only items at the  X  paragraph  X  granularity levels (deep and thorough elements), based on our propositions in this study, should get improved retrieval after the selection fusion methodology. Similarly, another example selection criteria, requiring only  X  article  X  level elements (larger re-sults), should as well be improved as a result of selection fusion methodology. The selection criteria could be a mix of different granularity levels; it could be taken from the users personalization settings; from system X  X  pre-or post-defined conventions; or it could be formed as a result of a particular domain or system specific constraints.

The proposed selection fusion methodology, in this study, is experimentally applied to the semantically annotated Wiki-pedia XML collection using the INEX 2 [28] evaluation mea-sures and test-bed. The multiple and diverse set of sub-mitted runs at INEX 2009 are fused together using the known fusion methods [8, 30] and then structurally contex-tualized [26]. The results obtained because of selection fu-sion methodology, from the ad-hoc track (INEX) on all the four tasks (i) focused, (ii) thorough, (iii) relevant in context (RiC) and (iv) best in context (BiC), measured as focused / semi-structured retrieval, exhibit clear improvements over
Ranking system is analogous to the scoring system, they are used interchangeably in this study.
INEX (Initiative for the Evaluation of XML retrieval) is a forum for the evaluation of XML and focused retrieval, offer-ing a test collection with topics and corresponding relevance assessments, as well as various evaluation metrics. Aside evaluating element retrieval, passage retrieval evaluation is also supported at INEX. the submitted runs at INEX 2009, and over a strong and competitive baseline system  X  data fusion over all INEX 2009 submitted runs (Section 3).

Summarizing, the contributions of this study include: 1. Creating decent scoring system using structural fusion 2. Developing selection fusion methodology -simple and 3. Construction of a test setting for evaluating the re-4. Experimentally evaluating the competitive scoring sys-
Section 5 concludes and highlights the future work. Figure 1: Sample semi-structured data and its tree representation.

Information Retrieval (IR) is about finding relevant mate-rial of an unstructured nature (typically text). The notion of  X  X nstructured-ness X  in the retrieved material / item refers to the distinction between structured data in the databases and unstructured text in the documents  X  considering the latter to be the focus of IR. However, many text documents, such as newspaper articles or books, have a well-defined struc-ture consisting of coherent blocks or parts such as, titles, paragraphs, sections, and so on. These coherent chunks of document parts form the retrievable units (or the logical documents  X  a document would contain many logical docu-ments) of semi structured retrieval. For digital data repre-sentation, storage, manipulation, and also for data seman-tics, the structure of a document is often presented using the mark-up language, typically XML or likes. Thus, XML retrieval is semi-structured in nature, and hence in an XML document, the coherent parts are referred to as elements .
Measuring semi-structured retrieval has a relatively long history in IR research. An essential part of the research car-ried out in the evaluation and standardization of XML IR has been done at the yearly INEX (Initiative for the Eval-uation of Xml retrieval) workshop since 2002 [10]. The ini-tiative also offers standard collections of documents (mostly or entirely in XML), with a set of query topics (per collec-tion per track) and corresponding relevance assessments, as well as various evaluation metrics and toolkits for the semi-structured retrieval 3 .

Within the INEX ad-hoc track, several tasks are pro-posed, tested and discussed over the time for being repre-sentative use cases of semi-structured retrieval [19, 32]. One of the common perspectives from most of the state of the art use cases in XML retrieval is;  X  X ow are the results or-ganized? X . The ability to describe the relevant retrievable content within the document with the metadata (structure) which is a part of the markup (in the form of tags around the content). The results organization defines the interpre-tation (may be semantically) of the enclosed content, to be presented to the users. The results organization at INEX 2009 has been done in the following ways: 1. element (fine grained) retrieval tasks: thorough and 2. in-Context retrieval tasks: Relevant-in-Context (RiC)
The element retrieval tasks differ in the way they treat the overlap in the result list [18]. The feature of elements being part of and containing other elements is called overlap-ping, or nested elements. In other words, in overlapping and nested results, the same text fragment may appear several times in the results list. For example, in Figure 1, element A is overlapping with all other elements. In addition, B is over-lapping with D and E, and C with F and G. In the thorough task , elements in the result list may be overlapping, whereas in focused task the elements in the result list should not overlap. The thorough task is considered system oriented, while there are no user interface nor user-related assump-tions underlying the task [7]. The task is to rank elements based on their relevance with overlap. The comprehensive result list with overlapping elements does not remove the challenge, apart from relevance, deciding which (kind / size) of elements should be preferred in the results.

In focused task, however, only one of the overlapping el-ements should be selected in the final result list, i.e., one element from each branch. For example, in Figure 1, from elements A and B, either A or B should be selected in the result list, not both (means overlap). The most straightfor-
It is worth noting that INEX result lists may contain arbi-trary passages in results. However, vast majority of the runs retrieve elements only, and in this paper we focus only on the semi-structured retrieval task (i.e., elements retrieval). ward way to perform this selection is to calculate a score for individual elements as if they were documents (logical docu-ments) and filter the resulting comprehensive ranked list to remove overlap (see [13]). A typical strategy is, to select the elements on the basis of their scores, so that among the over-lapping elements, the element with the best relevance score is selected. We call this score based selection as the first-come first-choice (FCFC) selecting principle. The FCFC principle is a widely used, but a rather straightforward tech-nique, having no pre-defined and / or intellectual consider-ations, for example, one based on screen-size requirements, user preferences or any other constraints.
In addition to handling the overlap, two different approach-es are also modelled at INEX; characterizing how the results ought to be grouped. In the focused and thorough tasks, the users prefer a single element that is relevant to the query, while in the so-called in-Context retrieval tasks, the users are more interested in elements within the relevant articles  X  they want to see parts of the (highly) relevant document that satisfy their information need most effectively [7, 16]. With in-Context retrieval tasks, the ideal user is interested only in elements within the highly relevant articles  X  they want to see which parts of the highly relevant document will best satisfy their information need. In RiC task, all the rele-vant, non-overlapping elements are sought together, whereas in BiC task, the focus is only on the best entry point. In RiC the challenge is to find appropriate amount of elements strictly with no overlaps. For the selectivity (as described earlier), FCFC or any other selection method could be used. Further details are found in [7, 16].
Traditionally, the aim of the semi-structured retrieval sys-tem primarily is, to retrieve (1) the relevant elements, which are (2) at the right level of granularity. In other words, only those elements should be answer to the user query which are; as focused as possible, while still covering the users X  query topic(s). In a nutshell, the evaluation of XML / semi-structured retrieval system, addresses the combination of (a) the scoring quality, i.e., how well a system scores the rele-vant elements, and (b) the selection quality, which means the selection of the right granularity level of elements, for the re-quired task and of appropriate size (as hinted earlier as well). The appropriate size or type is dependent on relevance as well as many application and user specific attributes.
Thus, the right granularity level, i.e., element size / type is very much subject to different use cases and user inter-faces [17, 26]. For example, the need for a shorter excerpt of information as a result-set, is felt when a user is querying the system using a device with screen-size constraints, e.g., from a smart-phone (which is quite common, nowadays). In a laptop device the relevant content is more easily obtained without automatic search by skim reading or using some simple interface gadget such as, find on page. Also, many other different use cases require retrieval of elements with various granularities. For a snippet retrieval (e.g., for re-sult list presentation) or fragment search, a small element is more suitable than a whole section or full article, or for example a user interested in abstracts of search results, to skim through them before opening the whole document it-self. One could imagine a number of other such use cases and user interface scenarios. However, apart from the tasks, measuring the performance follows rather a one-size-fits-all principle, where system developers have to guess, what is the correct granularity level appropriate for the metrics involved in the evaluation [17].
Based on the above considerations, we argue that selec-tion and the scoring can be considered as two different but related tasks and hence two distinct systems can be em-ployed separately for these two tasks. The selection result list may contain only the (unranked or poorly ranked) el-ements, that are required, for example, because of certain type / size constraint or for a particular use case scenario. While the scoring result list is a comprehensive and prop-erly ranked result list containing diverse, overlapping list of elements. Intuitively, one way of combining the scoring and the selection is by fusing the selection scheme of one system with the scoring (or ranking) ability of another. In other words, the elements provided by the selection result list are (re) ranked by the ranking or scoring result list. In the Sec-tion 3 and 3.2, we have defined our scoring and selection systems, for this study, and subsequently applied the selec-tion fusion methodology on them. The results of selection fusion on all the 98 submitted runs at INEX 2009 are dis-cussed in Section 4.3 and the improvements are graphically shown in Figures 2 and 3, for all the 4 ad-hoc track tasks described in this section.

In the next section we have outlined the dichotomy of the fusion in the semi-structured information retrieval settings.
Data fusion combines two or more retrieval results and has been shown to be effective in full document retrieval  X  better than the individual component systems [8, 24, 30]. In this study, we experiment with the fusion of semi-structured retrieval results from a set of individual and diverse compo-nent systems, where the retrievable units are treated as if they are documents. The outcome of such a fusion is a com-prehensive list of elements treated as the ranking system (Section 3.1). In other words, the fusion methods from doc-ument retrieval are applied directly to the semi-structured retrieval results without taking into account the features, such as the selectivity strategy (the granularity level). As this study purposefully separates the ranking and the se-lection systems  X  fusion in semi-structured retrieval is also dichotomized in a similar fashion. The fusion methods re-sponsible for the ranking system, and the selection fusion methods accountable for the selection system. More specif-ically we have: 1. Fusion for the ranking system. 2. Fusion for the selection system (Selection fusion).
Document retrieval fusion methods (rank-based and score-based) can be applied to semi-structured retrieval results as well, when considering the results as a flat list. For the scor-ing or ranking system in semi-structured retrieval settings, there is yet another family of fusion models  X  fusion meth-ods utilizing the structural or hierarchical relationships of elements in the re-scoring process.
There is an analogy between the fusion method based on rank and the social voting system [20, 24]. The rank of a particular retrievable item is decided based on how many different systems vote (in one or another form, e.g., in ma-joritarian view) for this particular retrievable item to be ranked to a particular rank (position in the result list). We have tested, in several intuitive settings, earlier [25, 26], the following rank-based fusion methods:
Reciprocal Rank fusion [8] is found to be well suited with our current theoretical and experimental settings (see Sec-tion 4.1). Hence, in the rank based fusion methods, we have reported only the results from reciprocal rank fusion (Sec-tion 4).
A fusion method which is concentrated on combining sear-ch results based on the similarity values of individual retriev-able items, in the semi-structured retrieval, for each query topic and from a set of varied runs (employing presumably different retrieval models). In the fusion based on scores (similarity values), we have explored some of the state of the art fusion methods, for combining the similarity scores from the set of runs used in this study:
We have found (in conjunction to what has also been re-ported by Shaw et. al [30]) that simply combining the simi-larity values in a linear fashion, summing the similarity val-ues, works better than trying to select a given similarity value. This fusion method is named as the CombSUM fu-sion [30]. CombSUM fusion method is therefore chosen, in the experimental evaluations, to fuse only the set of runs containing similarity scores in their result lists.
Here, we propose a type of fusion, which uses the struc-tural features in the semi-structured documents, for struc-turally fusing together a set of different semi-structured re-trieval systems, using the hierarchical structure (relation-ships) of the elements in the semi-structured documents. By doing that, the aim is to produce an extensive (cover-ing the query topics noticeably) and effective (good rank-ing outcomes) results list. The structural features in semi-structured documents could possibly originate from the struc-tural context (elements in the structural kinship [25]) of the relevant item (Section 3.1.3) or from the selection method-ology described in the Section 3.2.
 Structural context [25] is at the core of this type of fusion. Structural fusion is done in two steps: 1. The results from different semi-structured retrieval sys-2. The fused results are then structurally contextualized
This way we would be able to get results which are both comprehensive (containing thoroughly almost all the rele-vant elements on the query topics) and at the same time highly relevant as well [26].

With structural fusion, we are able to retrieve elements independent of their sizes (or independent of granularity levels). A small element, in term of size, can be viewed and hence scored in relation to its structural context, and its smaller size (which means having less textual evidence) doesn X  X  stop it from being selected as one of the top ranked results. The weight or score of a relevant and / or small ele-ment is adjusted (re-enforced) by the basic weights (content based weight) of the elements in its structural context (its contextualizing elements in hierarchy). In addition to basic weights, each element in the structural context of the con-textualized element, should possess an impact factor [26]. An higher impact factor shows the importance of the con-textualizing element and vice versa.
Selection fusion is also a fusion method based on struc-ture, as the name suggests, but intended for the selection process instead ranking or scoring the semi-structured re-sult sets. The structurally fused and structurally contex-tualized results; the scoring system, is selection fused with one particular semi-structured retrieval system. The reason for selection fusing the scoring and the selection system is to choose the selectivity of the overlapping elements of the scor-ing system from the selection system. Hence, the scoring of the focused elements come from the scoring system and the selectivity comes from the selection scheme. In a nutshell, we would like to retrieve highly relevant set of elements with controlled granularity levels (selectivity scheme).
A user querying the semi-structured retrieval system from a handheld device, the selection criteria in this case (should) take into account the limitations of the results presentation  X  they should preferably fit-in the limited screen-size  X  which in this example means that, they should be small and fo-cused enough to satisfy the device requirements and users X  needs. Therefore we name this type of fusion as the Selec-tion fusion . A toy example for the use of selection fusion is shown in the Table 1, with illustration in the caption. The selection fusion results in the Table 1 are based on the rank-ing order &lt;C, A, G, F, B, D, E&gt; , which is assumed to be the result of the scoring system.

Selection fusion approach formulated in this way, is flexi-ble and independent, any selection scheme and any retrieval or scoring systems could be applied. We have applied the selection methodology to a set of semi-structured retrieval systems, the INEX 2009 officially submitted runs by the participants, and got an overall steady and statistically sig-nificant results over most of the retrieval systems and ad-hoc retrieval tasks (as described in Section 2.1) at INEX 2009, measured as focused retrieval (see Figures 3 and 2). Table 1: Selection fusion ranks above are based on the ranking order (for this example, is assumed to be): &lt; C, A, G, F, B, D, E &gt; . The selection scenarios cover all combinations of the example in Figure 1, for the focused task. FCFC method delivers C, B.

S election schemes S election fusion results { A } &lt; A &gt; { B , C } &lt; C, B &gt; { B, F, G } &lt; G , F, B &gt; { D, E, C } &lt; C, D, E &gt; { D, E, F, G } &lt; G , F, D, E &gt;
In addition to the selection fusion methodology described above, there is another type of selection fusion as well, which is based on the selection fusion of  X  article  X  and element runs. In the in-Context tasks (RiC and BiC, Section 2.1.2), the articles are selected first and thereafter the elements within the article are selected. Thus, the two systems can be used for another type of fusion  X  document retrieval system fused with element retrieval system [14]. In this study, we are not considering this type of selection fusion.
In this section, the proposed ideas presented in this study are empirically tested and the results are analysed in light of the posed hypothesis and the theoretical foundations es-tablished. Rest of the section is organized as follows; in Section 4.1, we define the ranking system for experimen-tations; Section 4.2, we lay down the test settings in the semi-structured environment and Section 4.3, we interpret and assess the experimental outcomes.
Given the English Wikipedia test collection [12, 28], con-taining 2.66 million semantically annotated XML documents (50.7 Gb), 68 related topics, and 98 submitted runs 4 , pro-vided by the INEX 2009 ad-hoc track [9]; we performed a data fusion based on sum of normalized scores (Comb-SUM) [30] and reciprocal rank fusion [8]. The reason for using the INEX 2009 test topics (instead of 2010) is the larger variety of elements in the participants X  results which was primarily due to the existence of the thorough task.
The element scores (for each runs, per topic) were nor-malized for the fusion based on scores (CombSUM, Sec-tion 3.1.2) as follows:
A total of 173 runs were submitted by participants to INEX 2009. 13 runs were not element runs, i.e. they contained ranges of fragments or file-offset-lengths (FOL) as retriev-able units and were omitted from the fusion. In addition, in order to avoid noise, we made a deliberate decision to remove 61 runs having an extensive number of non-existing elements. Thus, a total of 98 runs from the participants of all tasks (best-in-context, relevant-in-context, focused and thorough) of the ad-hoc track were used in fusion. where, max ( scores ) and min ( scores ) denote the maximal and minimal scores respectively.

Although, the INEX 2009 runs have the largest variety in the results for the fusion  X  in comparison to other years of the initiative, most of the participants (56 of them), un-fortunately, did not report any real element scores, just the ranking orders (without relevance scores), because the INEX evaluation toolkit did not require that information. For those systems (without scores), an artificial score was computed for each element, based on their reciprocal rank, before applying the normalization function, Equation 1. For the reciprocal rank fusion, a score for an element e is calcu-lated as follows: where
In other words, as a result of Equation 2, when k = 0, the top ranked element in the result list is given a score of 1, the second ranked gets 1 / 2, third 1 / 3, fourth 1 / 4 and so on.
First, we apply the reciprocal rank fusion on the 56 runs (without scores), and then we apply the CombSUM fusion on the overall 98 runs, the resultant fusion run is named as the CombSUM Reciprocal fusion. Finally, the ComSUM Reciprocal fusion is structurally contextualized using the combination function, from our earlier work [26]: where tf e term frequency and ief inverse element frequency at focused granularities.

The final result-set constitute the ranking system  X  the structural fusion results (or CR  X  Contextualization Re-rank). For each of the 98 submitted runs (the selection run); we take the selectivity scheme from the selection run, while scoring is taken from the structural fusion run (CR), the ranking system. The selection fusion, for the rest of exper-iments, is the fusion of the ranking system formulated here with a particular selection run or system.

Figures 3 draw pictorial overview of the effects of selection fusion (fusion of the ranking system above, and the selection system) on each of the 98 INEX 2009 submitted runs (the selection runs). Each of the points (blue and orange) on the Figures 3 represent the gain / loss effects of selection fusion methodology (gain if above and loss if below the red diagonal line) on each of the participating (task-wise) semi-structured retrieval systems.
The effectiveness of selection fusion methodology is exper-imentally analysed using the INEX 2009 evaluation test-bed. We have conducted retrieval experiments within the ad-hoc track at INEX, featured by four (4) tasks (focused, thor-ough, RiC and BiC) and measured them as focused (semi-structured) retrieval. The results are pairwise (the selection fusion run and the selection run) compared, evaluated and reported, see Figures 3.

We report the improvements for Mean Average interpo-lated Precision (MAiP), interpolated Precision at interpo-lated Recalls (iP[@iRecall]), Mean Average generalized Pre-cision (MAgP) provided by the INEX evaluation toolkit, and the precision-recall curves. In addition, we calculate the specificity of the results in terms of how deep in the XML hierarchy the retrieved elements are on average, calculated over topics. We call this feature of the result set as Mean Average element Depth (MAeD). The depth of an element can be determined, for example, by calculating the number of slashes in its path expression.
 Table 2: Comparison between Mean Average inter-polated Precision and Mean Average element Depth ( &gt; 1) of INEX 2009 submitted runs, with correla-tion coefficient -0.4046, over all the runs including the ones with MAeD=1. Bold-face entries are top ranked runs at INEX 2009 thorough task.

In the focused retrieval tasks in the ad-hoc track of INEX, the aim is to retrieve the most focused elements satisfy-ing an information need without overlapping elements. An overlapping result list means that the elements in the re-sult list may have an ancestor-descendant relationship with (d) Best in Context -(MAgP in parentheses  X  M ) FCFC is applied after pruning the  X  article  X  elements. Where ( and ( M ) stat. significance at p &lt; 0 . 05 (1-tailed, t-test). each other and therefore share the same text content. For instance, the  X  sec  X  element within the  X  article  X  element, is overlapping with the parent  X  article  X  element (because they are nested). In this study, we are following the focused approach, considering a result list where only one of the over-lapping elements from each branch is selected. This means that including the  X  article  X  element would mean excluding the  X  sec  X  element in the result-set or vice versa, depending on which selectivity scheme is used.

The fused result list contains all the elements delivered by the 98 component systems. This comprehensive result list obviously (by definition) contains overlapping elements. In order to remove the overlap, one intuitive solution is to se-lect the elements having highest score from each branch, the FCFC approach, as described in Section 2. In Fig-ures 2(a-d), the FCFC approach is used to prune the over-lapping elements from the CR and CombSUM Reciprocal runs. The purpose of sketching Figures 2(a-d), is to demon-strate the effectiveness of the baseline scoring systems CR and CombSUM Reciprocal against the best INEX 2009 sub-mitted runs. The scoring systems are clearly and visibly better than the best reported runs at INEX 2009, in all the four tasks. The improvements are always statistically signif-icant at p &lt; 0 . 01 and p &lt; 0 . 05 in pairwise comparisons, as indicated in the Figure 2 caption. As a result of the findings from Figure 2, now the CR run is capable to be used in the selection fusion as the scoring system.

Many participants, however, returned runs containing only full-article results, which therefore led to a full-article bias in the fusion results, as can be visually seen in the Figures 3, orange points with MAeD=1, are in majority.

The selection fusion approach do not take any stand on which elements should be selected from each branch. It pro-vides a comprehensive set of highly relevant items (CR run) and a selection possibility. In the selection fusion process, we therefore have to perform a structural fusion, where we take the element-level selections from each of the 98 submitted runs at INEX 2009, one by one, and subsequently re-rank them. For instance, if an INEX submitted run suggests the  X  body  X  element of document d as the top (1 st ) ranked result for query q , while the structural fusion (CR) rank it at 5 position, the  X  body  X  element would eventually be selected to the 5 th rank position and so on.
For the focused task a ranked-list of non-overlapping re-sults (elements or passages) must be returned [9]. It is eval-uated at the early precision, interpolated Precision at 0.01 interpolated recall (iP[0.01]) measure. As it is visible from the Figures 2(b) and 3(a-d), in the focused task, the early precision values are improved notably both when FCFC se-lection approach (Figure 2(b)) is used and when selection fusion is applied (Figures 3(a-d)), respectively. Especially in Figure 3(b), for the runs with Mean Average element Depth (MAeD) &gt; 1, the improvements in iP[0.01] values are more than the systems with MAeD = 1 (orange points without numbers).

For the thorough task a ranked-list of results (elements or passages) by estimated relevance must be returned [9]. It is evaluated by Mean Average interpolated Precision (MAiP) measure. The comparison of MAeD and MAiP, for this task, is shown in Table 2. The correlation coefficient value of  X  0 . 4046, characterizes an inverse relationship between the two measures. A higher value of MAeD means more deep elements (result list with more hierarchies) or focused re-sults, and a negative correlation value implies that, INEX evaluation metrics penalizes the MAiP scores of the more focused runs (with MAeD &gt; 1). It has also been observed that the top ranked retrieval systems were mostly those runs retrieving only the  X  article  X  elements (having MAeD = 1, see also Table 2, bold-faced entries) [4]. As the runs con-tain thorough results, the margin of improvements in those runs with MAeD &gt; 1 is found to be larger, as it is depicted in the Figures 3(f-h), runs with MAeD &gt; 1 are in the top right corners (best improvements). Because of FCFC selec-tion approach, the behaviour of CR run in Figure 2(d) is no different than Figure 2(b).

For the RiC task, non-overlapping results (elements or passages) must be returned, these are grouped by docu-ments. It is evaluated by Mean Average generalized Preci-sion (MAgP) measure, where the generalized score per arti-cle is based on the retrieved highlighted text [9]. The overall improvement in this task is observed to be not as significant as it was in the focused and thorough tasks (as it is also ev-ident from the Figures 2(a) and 3(i)). The primary reason could be attributed to the complexity of the evaluation met-rics [9]. The other reason could be, as the Figure 3(j) also indicate, that most of the runs are  X  article  X  or document retrieval runs (MAeD=1), which is because of the definition of the task  X  the results are grouped by documents. There-fore, the room for improvements, in focused retrieval, was minimal in this task. The overall MAgP values of the top runs are also pretty low, which could again be attributed to the aforementioned challenges in this task.

For the BiC task, a single starting point (element X  X  start-ing tag or passage offset) per article must be returned. It is also evaluated by Mean Average generalized Precision (MAgP) measure, but with the generalized score (per arti-cle) based on the distance to the assessor X  X  best-entry point [9]. A similar reasoning as that of RiC task is applicable here as well, the runs are mostly document retrieval runs, the MAgP overall are very low, and the metric is complex [9]. The mar-gin of improvements was low because almost all of the runs having MAeD=1, i.e., document retrieval.

Selection fusion is directly applicable for focused, thor-ough, RiC, BiC tasks as well as Content-and-Structure tasks (i.e., CAS) queries [21], where the required elements are ex-plicitly expressed by path expressions, or the path expres-sions are used as mere structural hints of the possible loca-tion of the information needed [15]. However, in the RiC task, one has to decide the order of the documents first. Ac-cording to Kamps et. al [14], the article run determines the article ranking best. This means that, in the comprehensive ranking list the existence of the root element (  X  article  X  node) determines the article ranking.

The overall improvements in all the four tasks are found to be extraordinary. What makes the overall methodol-ogy work, in most of the cases, could be accredited to the comprehensiveness and the flexibility of the selection fusion method (the ability to cover the query topic exhaustively). It is comprehensive both on the documents and elements lev-els, which is essentially due to the large variety of documents and deep elements in the participants runs, which were then structurally fused (using structural contextualization). On elements level, the structural fusion help to provide a com-prehensive set of focused answers, using the structural con-text. These comprehensively focused and highly relevant set (with good relevance scores) [26] of answer-set help the selec-tion methodology to improve almost all of the runs irrespec-tive of their selection scheme. Thus we can conclude with theoretical, statistical and experimental confidence that a good comprehensive semi-structured scoring system can de-liver improved focused retrieval experience, flexible enough to serve a diverse set of selection schemes.
In this study, the semi-structured retrieval is executed in two phases; (i) ranking/scoring and (ii) selection. Accord-ingly, the dichotomy of fusion is presented, where the rank-ing system was based on the fusion of ranks and scores of the official submitted runs of INEX, together with utilizing the context of the relevant retrievable unit. The selection , in turn, was based on individual selection schemes, taken again from the INEX submitted runs, one by one. The retrieval ex-perience was notably improved after reordering the elements from the selection system, by fusing them with the rank-ing system. Extensive and favourable empirical results have validated the hypothesis, the selection fusion methodology enhances the retrieval of semi-structured retrieval item spec-ified in the selection criteria. The selection fusion methodol-ogy is independent of selection scheme as well as generalized enough that any method can be applied for fusing and scor-ing, based on the user, system or domain preferences.
Measuring the effectiveness of IR systems should be well-defined and intuitive, yet simple and convenient enough for the system developers to comprehend and subsequently im-prove their retrieval and representational approaches. This study argues that developing good ranking system without taking the selection criteria into account, would lead to an overall good performance in a number of different selection scenarios and retrieval tasks. The scoring or ranking of elements should be considered an independent task in the course of system development. In contrast, developing scor-ing methods for a particular or set of selection scenarios and corresponding metrics, might end up in favouring certain type or size of elements, and therefore, lead to over-fitting the systems according to the metrics [5, 26].

Fundamentally a number of issues need to be addressed in the further studies  X  (a) Application of the proposition (the selection fusion methodology) to a real world example; (1-tailed t-test). (b) Evaluating and finding the optimal number of retrieval systems (methods) needed for the selection fusion method-ology to perform even better; (c) Using another collection, if available, with different representation of semi-structured data, e.g., overlapping structures; (d) Finding a different se-lection criteria, for example, a selection criteria based on the users X  personalization. [1] S. Abiteboul. Querying semi-structured data . [2] S. Abiteboul, P. Buneman, and D. Suciu. Data on the [3] P. Arvola, M. Junkkari, and J. Kek  X  al  X  ainen. [4] P. Arvola, J. Kek  X  al  X  ainen, and M. Junkkari. Expected [5] P. Arvola, S. Geva, J. Kamps, R. Schenkel, [6] P. Arvola, J. Kek  X  al  X  ainen, and M. Junkkari. [7] C. Clarke, J. Kamps, and M. Lalmas. INEX 2006 [8] G. Cormack, C. Clarke, and S. Buettcher. Reciprocal [9] S. Geva, J. Kamps, M. Lehtonen, R. Schenkel, [10] N. G  X  overt and G. Kazai. Overview of the INitiative [11] D. A. Grossman and O. Frieder. Information retrieval: [12] W. Huang, S. Geva, and A. Trotman. Overview of the [13] K. Y. Itakura and C. L. Clarke. A framework for [14] J. Kamps and M. Koolen. The impact of document [15] J. Kamps, M. Marx, M. de Rijke, and [16] J. Kamps, M. Lalmas, and J. Pehcevski. Evaluating [17] J. Kamps, M. Koolen, and M. Lalmas. Locating [18] G. Kazai, M. Lalmas, and A. P. de Vries. The overlap [19] M. Lehtonen, N. Pharo, and A. Trotman. A [20] C. Macdonald and I. Ounis. Voting for candidates: [21] S. Malik, M. Lalmas, and N. Fuhr. Overview of INEX [22] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [23] Y. Mass and M. Mandelbrod. Component Ranking [24] M. Montague and J. A. Aslam. Condorcet fusion for [25] M. A. Norozi and P. Arvola. Kinship [26] M. A. Norozi, P. Arvola, and A. P. de Vries.
 [27] P. Ogilvie and J. Callan. Hierarchical Language [28] Schenkel, R. and Suchanek, F.M. and Kasneci, G. [29] T. Schlieder and H. Meuss. Querying and ranking [30] J. A. Shaw and E. A. Fox. Combination of multiple [31] B. Sigurbj  X  ornsson, J. Kamps, and M. De Rijke. An [32] A. Trotman, N. Pharo, and M. Lehtonen. XML-IR
