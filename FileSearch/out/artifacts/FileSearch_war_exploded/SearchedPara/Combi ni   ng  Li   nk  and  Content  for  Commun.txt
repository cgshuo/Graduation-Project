 In this paper, we consider the problem of combining link and content analysis for community detection from networked data, such as paper citation networks and Word Wide Web. Most existing approaches combine link and content infor-mation by a generative model that generates both links and contents via a shared set of community memberships. These generative models have some shortcomings in that they failed to consider additional factors that could affect the community memberships and isolate the contents that are irrelevant to community memberships. To explicitly ad-dress these shortcomings, we propose a discriminative model for combining the link and content analysis for community detection. First, we propose a conditional model for link analysis and in the model, we introduce hidden variables to explicitly model the popularity of nodes. Second, to allevi-ate the impact of irrelevant content attributes, we develop a discriminative model for content analysis. These two models are unified seamlessly via the community memberships. We present efficient algorithms to solve the related optimization problems based on bound optimization and alternating pro-jection. Extensive experiments with benchmark data sets show that the proposed framewo rk significantly outperforms the state-of-the-art approaches for combining link and con-tent analysis for community detection.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering Algorithms, Experimentation, Measurement, Theory Discriminative Model, Link Analysis, EM Algorithm, Two-Stage Optimization
As online repositories such as digital libraries and user-generated media(e.g. blogs) become more popular, analyz-ing such networked data has become an increasingly im-portant research issue. One major topic in analyzing such networked data is to detect salient communities among indi-viduals. Community detection has many applications such as understanding the social structure of organizations and modeling large-scale networks in Internet services [32]. While there are different formulations for community detection, in this work, we focus on the unsupervised learning, or the clustering viewpoint, a commonly accepted and well studied perspective.

A networked data set is usually represented as a graph where individuals in the network are represented by the nodes in the graph. The nodes are tied with each other by either directed links or undirected links, which represent the relations among the individuals. In addition to the links that they are incident to, nodes are often described by cer-tain attributes, which we refer to as contents of the nodes. For example, when it comes to the web pages, online blogs, or scientific papers, the contents are usually represented by histograms of keywords; in the network of co-authorship, the contents of nodes can be the demographic or affiliation information of researchers.

Many existing studies on community detection focus on either link analysis or content analysis. However, neither information alone is satisfactory in determining accurately the community memberships: the link information is usually sparse and noisy and often results in a poor partition of net-works; the irrelevant content attributes could significantly mislead the process of community detection. It is therefore important to combine the link analysis and content analy-sis for community detection in networks. Recently, several approaches have been proposed to combine link and content information for community detection. However, as we will survey in the next section, most of these approaches adopted a generative framework where a generative link model and a generative content model are combined through a set of shared hidden variables of community memberships. We argue that such a generative framework suffers from two shortcomings. First, community membership by itself is in-sufficient to model links X  X ink patterns are usually affected by factors other than communities such as the popularity of a node(i.e. how likely the node is cited by other nodes). Second, the content information often include irrelevant at-tributes and as a result, a generative model without feature selection usually leads to poor performance.
In this paper, we propose a discriminative model of com-bining link and content analysis for community detection that explicitly addresses the above shortcomings of exist-ing approaches. Our main contributions are summarized as follows.
To the best of our knowledge, the model proposed in this paper is the first that combines conditional link models and discriminative content models for community detection. We conduct extensive experimental studies by using several benchmark data sets. The experimental results show sig-nificant improvement over the state-of-the-art approaches. Additional experiments are conducted to further verify the effectiveness of each of our link model and content model, respectively.

The rest of the paper is organized as follows. In section 2 we give an overview of the related work. In Section 3 we present and analyze the conditional link model. In Section 4, we extend the link model to include the content information. Also in Section 4, we describe the two-stage optimization al-gorithm. In Section 5, we show extensions by combining our link model and content model with other existing content and link models. In Section 6, we show extensive exper-imental results on benchmark data sets. Finally, we give conclusion in Section 7.
In this section, we review the existing work for community-detection using link analysis, content analysis, and their combination.

Approaches in this area can be classified into two cate-gories: measure-based approaches and probabilistic model based approaches. In the literature of measure-based ap-proaches, a measure is proposed to quantify the quality of partition, and the partition is obtained either by optimizing the measure or by iteratively adding and removing edges or nodes from the existing partitions to improve the measure. Some of the most well-known measures include normalized cut [26, 31] and modularity [24], which have been exam-ined in many previous studies [26, 31, 6, 22, 23, 12]. Other measures used for clustering can be found in [3, 29]. Be-sides measure-based algorithms, many probabilistic models are developed for community detection. One such model is the stochastic block model [27], which assumes that links are generated with the probabilities that only depend on the communities of nodes. Variants of the stochastic block model include mixed-membership stochastic block model [2] and Bayesian stochastic block model [14]. In addition, some other probabilistic models identify the optimal communities by soft-graph clustering [33, 30, 25].

Most of the existing approaches either assume links are nondirectional or treat directional links as nondirectional ones. Additional studies are devoted to address directional links in network analysis, including PageRank [28], HITS [17], and PHITS [7]. In PageRank, each web page is assigned a score based on the random walk model. HITS [17] derives an authoritative and a hub score for each web pages from the link structure. Cohn et al. [7] proposed the PHITS al-gorithm that extends the HITS algorithm for community detection by a probabilistic model that is similar to Prob-abilistic Latent Semantic Analysis(PLSA) [14]. LDA-Link model [10] extends Latent Dirichlet Allocation (LDA) for link analysis by assuming a link distribution for each com-munity. Other link models proposed in the framework of LDA can be found in [9, 13]. R. Nallapti et al. [21] extend the mixed membership stochastic block (MMSB) model [2] to directional links.

One of the most well-known approaches for content anal-ysis is the topic model, where each topic is naturally inter-preted as a community in our framework. Two well-known topic models are PLSA [15] and LDA [5]. Most topic mod-els are generative and are vulnerable to the words that are irrelevant to the target topics. To overcome this problem, S. Lacoste-Julien et al. [18] proposed a discriminative LDA. The main problem with discriminative LDA is that it is a supervised learning algorithm and cannot be applied di-rectly to a unsupervised learning setup, which is the case of our problem. In contrast, the discriminative framework proposed in this paper does not require the labeling infor-mation. It automatically discovers an appropriate discrimi-native model that fits best with the link information.
As aforementioned, neither link information nor content information is sufficient to decide the community member-ships. Combining link and content for community detection usually achieves better performance, as revealed in stud-ies [8, 11]. PHITS-PLSA combines PHITS with PLSA for community detection [8]. E. Erosheva et al. [10] combine LDA with LDA-Link for network analysis, referred to as LDA-Link-Word model in this paper. R. Nallapti et al. [21] combine the mixed membership stochastic block model with LDA, and extend the LDA-Link-Word model by separating the citing documents and cited documents with LDA-Link-Word model on the citing documents and PLSA model on the cited documents. Other approaches that exploit LDA for combining link and content analysis include [9, 13]. One major problem with these approaches is that they apply a generative model for content analysis, which makes them vulnerable to the irrelevant keywords. In addition to proba-bilistic models, some other approaches that have been pro-posed to combine link and content information include ma-trix factorization[35] and kernel fusion[34] for spectral clus-tering.
In this section, we first present the proposed link model and followed by a maximum likelihood estimation method used to estimate the unknown parameters of the proposed model. In Section 4, we incorporate the content information into the proposed link model by a discriminative model.
Before going to the mathematical model, we first establish the assumptions and notations that are used in our model. All nodes in the network form a node space V = { 1 ,  X  X  X  ,n where the nodes could represent web pages, online blogs, etc. For each pair of ordered nodes ( i, j ), let s ij record the information of the link from node i to node j . s ij could either be { 0 , 1 } , N + , or any nonnegative values dependent on the type of the link. If s ij =0,wesaythereisadirectional link from node i to node j ,ornode i cites j (equivalently, node j is cited by node i ). Let E = { ( i  X  j ) | s ij denote all the directional links in the network. Each node i has an associated  X  X ink-in X  space denoted by LI ( i )  X  X  which is the set of nodes that could possibly cite node i . Similarly, each node i is associated with a  X  X ink-out X  space denoted by LO ( i )  X  X  , which is the set of nodes that could possibly been cited by node i . Although in most cases we have LI ( i )= LO ( i )= V , in some scenarios such as citation of publications, the link-out space of a paper is the set of all papers that are older than the paper itself, and the link-in space is the set of all papers that are newer than the paper itself. Let I ( i )= { j | s ji =0 } be the set of nodes that actually cite node i , O ( i )= { j | s ij =0 } be the set of nodes that are actually cited by node i ,and d in ( i )= |I ( i ) i . Finally, we denote by K the number of communities we aim to find.

In our link model, we focus on modeling Pr( j | i ), i.e., the probability of linking node i to node j among all the other candidates in LO ( i ). In other words, we model which node j among LO ( i ) is more likely to be cited by node i .This is in contrast to many existing approaches that explicitly model the presence or absence of link i  X  j , i.e., Pr( i j ). This modeling choice allows us to avoid modeling the absence of links, which was observed in [2, 19] as a major problem for link analysis. We introduce a set of hidden variables z i  X  X  1 ,  X  X  X  ,K } for each node i  X  X  1 ,  X  X  X  ,n denote the community of node i . On the other hand, to model how likely a node will receive a citation in general, in our model for Pr( j | i ), we introduce a popularity variable b  X  0 for each node i : the higher popularity of one node, the higher chance the node will be cited by other nodes. Given the popularity and community memberships of all nodes, the link probability Pr( j | i ) conditioned on the community variable z i of node i associated with this link is given as follows where  X  ik gives the community membership of node i in com-munity k . As indicated by the above expression, the condi-tional link probability Pr( j | i ) is proportional to b j ularity of the ending node of the link. By assuming a multi-nomial distribution for z i , i.e., z i  X  Mult (  X  i 1 ,  X  X  X  have Pr( j | i ) written as where  X  ik =Pr( z i = k ).

In Eq. (2), we assume that b i is independently from the community variable. As a result, each node will only have one copy of the popularity. An alternative approach is to have the popularity variable b i conditioned on the commu-nity variable. In other words, we have a different popularity variable b ik for each node i when it is in a different com-munity z i = k . Using the community dependent popularity b ,Pr( j | i ) is computed as or by integrating out z i Comparing Eq. (3) to (2), we see that Eq. (3) introduces the freedom of modeling the community dependent popularity at the price of increasing number of variables. As will be shown in our empirical study, Eq. (2) achieves better performance because of the reduced number of variables.
In this section, we analyze our link model by establishing the relation and comparing to PHITS model [7]. For the purpose of consistency, we assume LO ( i )= V for all i .
In PHITS, each community is assumed to have a multi-nomial distribution that specifies the probability for each node to be cited by the other nodes in the same community. We denote by  X  jk the probability for node j to be cited by any nodes in the k th community. Pr( j | i ) conditioned on community variable z i of node i for this link, and  X  is then expressed as Note that unlike our model in Eq. (1), the conditional link probability in PHITS model has nothing to do with the com-munity membership of node j . This leads to the problem of undetermined community membership for nodes that do not cite any other nodes for PHITS, as discussed in the next section. By integrating out z i ,wehavePr( j | i ) written as where  X  ik is the probability that node i is in the k th com-munity.

The following proposition allows us to establish the re-lationship between the PHITS model and the popularity-based conditional link model.

Proposition 1. The PHITS model specified in Eq. (4) is equivalent to the link model with Pr( j | i ) specified in Eq. (3). The above proposition is proved by observing the link be-tween  X  jk and the quantity  X  jk b jk / j  X  j k b j k .Asre-vealed by the above proposition, PHITS is in fact a relaxed version of the proposed PCL model by assuming that the popularity of each node depends on the community of the node.

We can also derive the proposed model in Eq. (2) from the PHITS model in Eq. (4) by considering the relationship between  X  jk and  X  jk , as revealed by the following proposi-tion.

Proposition 2. The popularity-based conditional link model specified in Eq. (2) is equivalent to the PHITS model spec-probability of selecting node j from the k th community. The above proposition follows the Bayes X  X  rule, i.e., The above proposition once again reveals that the proposed conditional link model is a restricted version of the PHITS model. We believe that it is the constraints introduced in the proposed conditional link model that lead to more reliable performance.
In this section, we present the method of maximum like-lihood for the PCL model specified in Eq. (2). Observing the directional links E = { ( i  X  j ) | s ij =0 } , we write the log-likelihood as where  X  s ij is normalized s ij such that j  X  X O ( i )  X  s find optimal  X  and b by maximizing the log-likelihood To derive the EM algorithm, we first have the following lemma for a low bound for log L .

Lemma 3. The log-likelihood log L in Eq. (5) at the t th iteration is lower bounded as follows log L X  Q ( b,  X  ; b t  X  1 , X  t  X  1 ) where the parameters  X  ik and q ijk are computed as and b t  X  1 , X  t  X  1 are the corresponding solutions in the t iteration.
 The above lemma follows from the Jensen X  X  inequality and the inequality of  X  log x  X  1  X  x . Using the result in the abovelemma,wesearchfor b and  X  at the t th iteration that maximize the lower bound of log L , i.e., max s.t. For this maximization problem, we have the following the-orem. Before stating the theorem, we first establish the notations for the purpose of representation: n in ( i, k )= n in ( i )= n ( i, k )= n in ( i, k )+ n out ( i, k ) m ( i, k )=
Theorem 4. The optimal solution to Eq. (8) satisfies the following conditions  X  i, d out ( i ) =0 ,d in ( i ) =0 ,  X  i, d out ( i )=0 ,d in ( i ) =0 ,  X  i, d out ( i ) =0 ,d in ( i )=0 ,  X  i, d out ( i )=0 ,d in ( i )=0 ,  X  ik is any non-negative value such that Due to the limit of space, we skip the detailed proof of the theorem.

Remark: As revealed in Eq. (9), b i is proportional to the number of nodes that cites node i , i.e., n in ( i ), which is consistent with interpreting b i as  X  X opularity X  or  X  X uthori-tative X  for node i . Advantage of PCL over PHITS can also be seen in the solution of  X  ik . It can be shown that the membership of node i in PHITS model only depends on the membership of the nodes that are cited by node i , i.e.,  X  ik  X  n out ( i, k ), and not affected by the nodes that cite node i .When n out ( i ) = 0, i.e., node i has no outgoing links, the membership  X  ik is not determined. In contrast, in PCL model, community membership of node i depends on the membership of all the nodes connected to node i .
In this section, we extend our link model to incorporate the content information of nodes. As we discussed in Sec-tions 1 and 2, most existing approaches combine link and content by a generative model that generates both links and content attributes via a shared set of hidden variables re-lated to community memberships. In this work, we propose a discriminative model, referred to as Discriminative Con-tent(DC) model, to incorporate the content into the pro-posed link model. Let x i  X  R d denote the content vector of node i . The content information is used to model the memberships of nodes by a discriminative model, given by where w k  X  R d is a d-dimensional weight vector for commu-nity k with each element corresp onding to each attribute. We can see that by incorporating the content model, the community membership is no longer specified by parame-ters  X  ik , but rather conditioned on the content through y by a softmax transformation. Then, the conditional link probability Pr( j | i ) expressed in Eq. (2) is modified as fol-lows where y ik depends on w as given in Eq. (10). As revealed in the above expression, we do not generate the content at-tributes as most topic models do. Instead, by using the discriminative model, with an appropriately chosen weight vector w k that assign large weights to important attributes and small weights or zero weights to irrelevant attributes, we avoid the shortcoming of the generative models, i.e., being misled by irrelevant attributes. Another benefit from the discriminative model is that we can use a non-linear trans-formation  X  ( x ): R d  X  R m on the content vector as the new attribute to obtain a non-linear model. In the sequel, we use  X  ( x ) rather than x for presentation.

The log-likelihood of the combined model is written as We maximize the log-likelihood over the free parameters w and b . Although we can use any gradient-based algorithm to optimize with w k and b i , we propose an efficient two-stage method as discussed in the next section, which helps us better understand the relation of link model and content model.
 In this section, we describe the method to maximize the log-likelihood in Eq. (11). We still use the EM algorithm to maximize the log-likelihood. In the E-step, we compute  X  ik and q ijk from y and b . In the M-step, we maximize the following problem: max subject to non-negative constraints on b .

Instead of maximizing over w , we convert Eq. (12) into a constraint optimization problem over y and b by max Algorithm 1 Algorithm for maximizing the log-likelihood 1. Input the number of iterations or convergence rate 2. Initialize w k to zeros, b i randomly,  X  to a fixed value 3. in the E-step, compute  X  ik and q ijk as in Eq. (6) and (7) 4. in the M-step, 5. repeat Step 3 and 4 until the input number of itera-6. Output  X  ik or y ik as the final membership where the domain  X  is defined as By having the domain of y given in Eq. (14) as a convex set, we can take a projection method to maximize the problem of Eq. (13), which leads to the two-stage method. In the first stage, we simply ignore the complex constraint for y imposed by the domain  X  and solve the optimization prob-lem in Eq. (13) with only sum-to-one constraint on y ik and non-negative constraints on b using the result in Theorem 4. In the second stage, we project the y ik into the domain  X . Let  X  y ik denote the optimal solution obtained from the first stage. The projection of  X  y ik , denoted by y ik , is obtained by minimizing the KL divergence between  X  y ik and y ik  X  which is equal to the following optimization problem max This problem is similar to the log-likelihood in multi-class logistic regression problem except that the class member-ship  X  y ik is not just binary but between 0 and 1 . As in logistic regression, we can add regularization term on w k make the solution more robust, which leads to the following optimization problem max where  X  is the regularization coefficient. This problem is a convex problem [4] and has a unique optimal solution, and can be maximized efficiently by the Newton-Raphson method.

By converting the optimization problem over w into the problem over y and taking the two-stage method, we are able to have a better understanding of our combined model X  X he link structure will first give us a noisy estimation of commu-nity memberships  X  y , and the noisy memberships are then used as supervised information for our discriminative con-tent model to derive high-quality memberships y .Thesees-timated memberships are further used in our EM iterations. Algorithm 1 summarizes the overall algorithms for combined link and content analysis for community detection. The al-gorithm has a time complexity of O ( M ( eKC 1 + nKC 2 + T where M is the number of iterations, e is the number of links in the network, n is the number of nodes in the network, C is a constant factor in computing q ijk and  X  ik , C 2 is a con-stant factor in computing  X  ik and b i ,and T 3 is the time for maximizing problem in Eq. (15) by the Newton-Raphson method.
In this section, we discuss two variants of the proposed framework for combining link information with content in-formation. In the first variant, referred to as PCL+PLSA , we present an approach that combines the proposed condi-tional link model with the PLSA model for content analy-sis. In the second variant, referred to as PHITS+DC ,we present an approach that combines the PHITS model for link analysis with the proposed discriminative approach for content analysis. These two combined models will serve as baselines in our experimental study.
Similar to [8] where the PHITS link model is combined with PLSA content model, we combine our PCL link model with PLSA. The combined log-likelihood is given by where  X  is combination coefficient,  X  s w ij is the normalized number of times that word j occurs in the content of node i ,
W ( i ) denotes the set of unique words that occur in the content of node i ,and  X  w jk = Pr(word j | C k ). To maximize the log-likelihood, we derive the EM-algorithm as follows. In the E-step, we compute q w ijk , q l ijk and  X  ik as In the M-step, we compute  X  w jk ,  X  ik and b i as where N ( j ) denotes the set of nodes whose content have similar as before.
In this variant, we combine the PHITS link model with our DC content model. The log-likelihood is given by we compute q ijk as In the M-step, we first compute  X  jk and the free form mem-bership  X  ik by Then we maximize the following objective to get w k and y In this section, we conduct several experimental studies. We first compare the PCL model with the PHITS model for the task of link prediction. Then we compare the perfor-mance of the PCL model with that of several state-of-the-art methods on the task of community detection by using two citation data sets. Before going into the details, we first de-scribe the data sets and the metrics used in the experiment and evaluation.
We used four data sets in our experiments. They are described in the following:
Political Blog Data Set is a social blog network, which is a directed network of hyperlinks between webblogs about the US political issues, recorded in 2005 by Adamic and Glance [1]. There are totally 1490 blogs, and each blog is labeled as either conservative or liberal. In the data set, we only have the link information and have no content informa-tion. So this data set is only used in the link prediction task to compare the PCL model with the PHITS model. The number of communities for this data set is set to K =2.
Wikipedia Data Set is a web page network which was crawled from Wikipedia web site by Gruber et al. [13]. This data set has 105 nodes and 799 links. This data set contains no explicit community label for each page. So we only use this data set in the link prediction task, with K setto20as suggested in [13].

Cora Data Set is a subset of the larger Cora citation data set [20]. This data set includes publications from the machine learning area, each of which is classified into 7 sub-categories as: Case-based reasoning, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learn-ing, Rule Learning and Theory. There are totally 2708 nodes, and 5429 links. Each node corresponds to one pa-per and is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary of 1433 unique words. We use this data set in both the link prediction task and the community detection task. The number of communities is set to be K =7.
Citeseer Data Set is a subset of the larger Citeseer data set 1 . The Citeseer data set con sists of 3312 sc ientific pub-lications labeled as one of 6 classes and 4732 links. Each http://citeseer.ist.psu.edu/ publication is described by a 0/1 valued word vector. The dictionary of word consists of 3703 unique words. This data set is used in both link prediction and community detection tasks. The number of communities is set to be K =6.
In the comparison of the PCL model and the PHITS model on the task of link prediction, we hide some links from the network, and run the two models on the remaining links. The performance is measured by the metric of Recal l .
Recal l is an Information Retrieval measure. For each node, we compute the probabilities for the node to generate links to the other nodes and then sort these probabilities in the decreasing order. The recall is computed at each posi-tion in the rank and defined as the fraction of target nodes that correspond to the hidden links. The recall is reported from positions 1 to 20 in the rank.

To measure the performance of community detection, we used four metrics among which two are supervised and the other two are unsupervised. The two supervised metrics are normalized mutual information (NMI) ,and pairwise F-measure (PWF) . These two metrics use the supervised label information. The other two unsupervised metrics are mod-ularity (Modu) and normalized cut (NCut) .Thesetwomet-rics measure the partition performance in terms of the link structure.

With the supervised label information, we can form the true community structure C = { C 1 ,...,C K } ,where C k con-tains the set of nodes that are in the k th community. The community structure given by the algorithms is represented by C = { C 1 ,...,C K } . Then the mutual information be-tween the two is defined as and the normalized mutual information is defined by where H ( C )and H ( C ) are the entropies of the partitions and C . The higher the normalized mutual information, the closer the partition is to the ground truth.

Let T denote the set of node pairs that have the same label, S denote the set of node pairs that are assigned to the same community, | T | denote the cardinality of set T .The pairwise F-measure is computed from the pairwise precision and recall, as the following The higher the PWF , the better is the partition.
Modularity is proposed by Newman et al. [24] for measur-ing community partitions. For a given community partition C = { C 1 ,...,C K } , the modularity is defined as where Cut ( C i ,C j )= p  X  C i ,q  X  C j w pq . As stated in [24], modularity measures how likely a network is generated due to the proposed community structure versus generated by a random process. Therefore, a higher modularity value indicates a community structure that better explains the observed network.

Normalized cut is the objective of the normalized cut algo-rithm ([31], which we refer to as NCUT). Given a community partition C = { C 1 ,...,C K } , the normalized cut is defined as where  X  C i denotes the set of nodes that are not in C i To validate the advantage of the PCL link model over the PHITS link model, we experiment them on the four data sets described in Section 6.1. The performance is reported in Figure 1 in terms of recall at positions 1 to 20. Each number in the figure is averaged over 5 runs. The PCL outperforms the PHITS in all the cases. To investigate the effects of the popularity parameter, b ,wealsoperformthe same experiments on PCL by setting b i =1forall i .The results are labeled as  X  X CL-b=1 X  in the figure. The perfor-mance given b i = 1 is worse than PCL and PHITS. It further confirms the importance of the popularity parameter. Over-all, this result validates our conjecture that the conditional link model outperforms the generative link model, at least for the task of link predication.
In this section, we investigate the performance of our model on the task of community detection. We perform ex-periments on the two scientific publication date sets, which have both link and content information.

To validate the advantage of our proposed model, we com-pare it with several baselines. Based on what information is used, the algorithms are categorized into 3 classes: Based on Link , we compare the following models: PHITS, PCL, LDA-Link, and Spectral Clustering (NCUT).
 BasedonContent , we compare the following: PLSA, LDA-Word, and Spectral Clustering. In spectral clustering, the similarity matrix is the kernel matrix computed from the content of each publication. Here we report two kernels, one is the RBF kernel, and the other is the probabilistic product kernel proposed in [16].
 BasedonLinkandContent , we compare the following: PHITS-PLSA, LDA-Link-Word, Link-Content-Factorization (LCF), Spectral Clustering, PCL-PLSA, PHITS-DC, and PCL-DC. Notice that PHITS-PLSA refers to the combina-tion of PHITS and PLSA proposed in [8], LDA-Link-Word refers to the mixed membership model proposed in [10], LCF refers to the model proposed in [35], Spectral Clustering is applied to linear combined kernel from the link matrix and content kernel, PCL-PLSA refers to the combination of the PCL and the PLSA model as described in Section 5, PHITS-DC refers to the PHITS model combined with the Discrimi-native Content model, and PCL-DC refers to the PCL model combined with the Discriminative Content model.

In the implementation, the feature vector used in our model is the original word indicator vector without any trans-formation; the spectral clustering we used is the normalized cut algorithm [31] (NCUT). For the algorithms that are de-(a) Recall on Political Blog pendent on some parameters such as the  X  parameter in RBF kernel, the combination coefficient in PHITS-PLSA, the combination coefficient of link matrix and content ker-nel for spectral clustering, the combination coefficient in PCL-PLSA, the regularization coefficient in PHITS-DC, we experiment on a wide range of values and choose the best one in terms of normalized mutual information and pair-wise F-measure. For example, the combination coefficients in PHITS-PLSA, PCL-PLSA, and combined link matrix and content kernel are tuned from 0.1 to 0.9 with 0.1 as the step size. The regularization coefficient for PHITS-DC is tuned from 0 to 50 with 5 as the step size. The regularization co-efficient for PCL-DC is set to a fixed value of 10. All the iterative algorithms are run until the relative difference of the objective is within 10  X  8 .
 Tables 1 show the results on the Cora data set and the Citeseer data set. For both data sets, PCL outperforms PHITS in all the cases, either using link only (PCL outper-forms PHITS), or combining link and content (PCL-PLSA outperforms PHITS-PLSA and PCL-DC outperforms PHITS-DC). When considering content, the approaches that dis-criminatively combine content (DC) outperform the approaches that combine content using PLSA. That is, PHITS-DC out-performs PHITS-PLSA, and PCL-DC outperforms PCL-PLSA. These results further confirm that the discriminative models (either the link model, or the content model, or the com-bination of the two) achieve better performance than the generative ones.

We also compared PCL and PCL-DC with the following algorithms. In the link-only case, the spectral clustering (NCUT) outperforms PCL. LDA-Link outperforms PCL in some metrics. When combining link and content, PCL-DC outperforms all algorithms except for the spectral cluster-ing (NCUT) algorithm in the normalized cut (NCut) metric. The main reason for the spectral clustering (NCUT) to have thebestperformanceintermsofnormalizedcutisthatitdi-rectly minimizes this metric. However, we argue that people would consider the NMI and PWF metrics as equally impor-tant, because the NMI and PWF metrics measure how good the partition derived by the algorithms matches the ground truth.

Finally, to reveal the performance of our model under dif-ferent parameters, we show the performance of the PCL-DC model under different regularization coefficient  X  on the two data sets in Figure 2. In both data sets, the performance achieves the highest level when  X  =5. Afterthat,thePCL-DC algorithm is not very sensitive to  X  .
In this paper, we proposed a unified model to combine link and content analysis for community detection. To ac-curately model the link patterns, a conditional link model is proposed to capture the popularity of nodes. In order to alleviate the problem caused by the irrelevant attributes, a discriminative model, instead of a generative model, is pro-posed for modeling the contents of nodes. The link model and content model are combined via a probabilistic frame-work through the shared variables of community member-ships. We observed that the combined model obtains signif-icant improvement over the state-of-the-art approaches for community detection. For future work, we plan to consider a full Bayesian model to compute the posterior of member-ship and parameters rather than computing the maximum likelihood estimation, and try to look at the performance of the proposed model on more data sets.
 Acknowledgement: We would like to thank Prof. Andrew McCallum for providing the Cora dataset and Prof. C. Lee Giles for providing the CiteSeer dataset. We also thank anonymous reviews for their valuable comments. [1] L. Adamic and N. Glance. The political blogosphere [2] E.M.Airoldi,D.M.Blei,S.E.Fienberg,andE.P.
 [3] J. Baumes, M. Goldberg, and M. Magdon-ismail. [4] C.M.Bishop. Pattern recognition and machine [5] D.M.Blei,A.Y.Ng,M.I.Jordan,andJ.Lafferty.
 [6] A. Clauset, M. E. J. Newman, and C. Moore. Finding [7] D. Cohn and H. Chang. Learning to probabilistically [8] D. Cohn and T. Hofmann. The missing link -a [9] L. Dietz, S. Bickel, and T. Scheffer. Unsupervised [10] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed [11] L. Getoor, N. Friedman, D. Koller, and B. Taskar. [12] S. Gregory. An algorithm to find overlapping [13] A. Gruber, M. Rosen-Zvi, and Y. Weiss. Latent topic [14] J. M. Hofman and C. H. Wiggins. A Bayesian [15] T. Hofmann. Probabilistic latent semantic indexing. In [16] T. Jebara, R. Kondor, A. Howard, K. Bennett, and [17] J. M. Kleinberg. Authoritative sources in a [18] S. Lacoste-Julien, F. Sha, and M. I. Jordan. DiscLDA: [19] A. McCallum and K. Nigam. A comparisoin of event [20] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. [21] R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. [22] M. E. J. Newman. Fast algorithm for detecting [23] M. E. J. Newman. Modularity and community [24] M. E. J. Newman and M. Girvan. Finding and [25] M. E. J. E. Newman and E. A. A. Leicht. Mixture [26] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [27] K. Nowicki and T. A. B. Snijders. Estimation and [28] L. Page, S. Brin, R. Motwani, and T. Winograd. The [29] G. Palla, I. Der  X  enyi,I.Farkas,andT.Vicsek. [30] W. Ren, G. Yan, X. Liao, and L. Xiao. Simple [31] J. Shi and J. Malik. Normalized cuts and image [32] X. Wang, N. Mohanty, and A. McCallum. Group and [33] K. Yu, S. Yu, and V. Tresp. Soft clustering on graphs. [34] S. Yu, B. D. Moor, and Y. Moreau. Clustering by [35] S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining
