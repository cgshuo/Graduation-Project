 Leveraging information from relevance assessments has been proposed as an effective means for improving retrieval. We introduce a novel language modeling method which uses in-formation from each assessed document and their aggregate. While most previous approaches focus either on features of the entire set or on features of the individual relevant doc-uments, our model exploits features of both the documents and the set as a whole. When evaluated, we show that our model is able to significantly improve over state-of-art feed-back methods.
 H.3 [ Information Storage and Retrieval ]: H.3.1 Con-tent Analysis and Indexing; H.3.3 Information Search and Retrieval X  Retrieval Models Algorithms, Theory, Experimentation, Measurement Language modeling, Query models, Relevance feedback
A query is usually a brief, sometimes imprecise expres-sion of an underlying information need [ 19]. Examining how queries can be transformed to equivalent, potentially better queries is a theme of recurring interest to the information retrieval community. Such transformations include expan-sion of short queries to long queries, paraphrasing queries us-ing an alternative vocabulary, mapping unstructured queries to structured ones [ 14], identifying key concepts in verbose queries [ 3], etc.

To inform the transformation process, multiple types of information sources have been considered. A recent one is search engine logs for query substitutions [ 20]. Another re-cent example is where users complement their traditional keyword query with additional information, such as exam-ple documents [ 2], tags [ 6], images [ 7], categories [ 21], or their search history [ 1]. The ultimate source of information for transforming a query, howe ver, is the user, through rel-evance feedback [ 16, 17]: given a query and a set of judged documents for that query, how does a system take advantage of the judgments in order to transform the original query and retrieve more documents that will be useful to the user? As demonstrated by the recent launch of a dedicated relevance feedback track at TREC [ 4], we still lack the definitive an-swer to this question.

Let X  X  consider an example to see what aspects play a role in transforming a query based on judgments for a set of initially retrieved documents. Suppose we have a set of doc-uments which are judged to be relevant to a query. These documents may vary in length and, furthermore, they need not be completely on topic because they may discuss more topics than the ones that are relevant to the query. While the users X  judgments are at the document level, not all of the documents X  sections can be assumed to be equally relevant. Most relevance feedback models that are currently available do not model or capture this phenomenon; instead, they attempt to transform the original query based on the full content of the documents. Clearly this is not ideal and we would like to account for the possibly multi-faceted charac-ter of documents. We hypothesize that a relevance feedback model that attempts to capture the topical structure of indi-vidual judged documents ( X  X or each judged document, what is important about it? X ) as well as of the set of all judged documents ( X  X hich topics are shared by the entire set of judged documents? X ) will outperform relevance feedback models that capture only one of these types of information.
We are working in a language modeling (LM) setting and our aim in this paper is to present an LM-based relevance feedback model that uses both types of information X  X bout the topical relevance of a document and about the general topic of the set of relevant documents X  to transform the original query. The proposed model uses the whole set of relevance assessments to determine how much each docu-ment that has been judged relevant should contribute to the query transformation. We use the TREC relevance feedback track test collection to evaluate our model and compare it to other, established relevance feedback methods. We show that it is able to achieve superior performance over all eval-uated models. We answer the following two research ques-tions in this paper. (i) Can we develop a relevance feedback model that uses evidence from both the individual relevant documents and the set of relevant documents as a whole? (ii) Can our new model achieve state-of-the-art results and how do these results compare to related models? The remainder of this paper is organized as follows. In Section 2 we recall some basic facts from the language mod-eling approach to information retrieval. Building on this, we introduce our new feedback model in Section 3.InSection 4 we detail the set-up of the experiments. We report on the outcomes in Section 5 and we end with a concluding section.
In this section we recall basic notions from the language modeling approach to information retrieval. In the approach based on multinomial unigram models [ 8], each document D is represented as a multinomial probability distribution P ( t |  X  D ), assuming term independence. At retrieval time, each document is ranked according to the likelihood of hav-ing generated the query Q . Building on this basic set-up, several authors proposed the use of the Kullback-Leibler di-vergence measure for ranking [ 10, 15]. Using KL-divergence, documents are scored by measuring the divergence between a query model  X  Q and each document model  X  D .Sincewe want to assign scores proportional to their similarity, the KL-divergence is negated for ranking purposes: Note that the sum is over the vocabulary V although terms which do not appear in Q have P ( t |  X  Q )=0. H (  X  Q , X  is the cross-entropy of the query model and the document model and H (  X  Q ) is the entropy of the query, a query spe-cific constant that can be ignored for ranking. When the query model  X  Q is estimated using the maximum-likelihood estimate, i.e., when P ( t |  X  Q )= P ( t |  X   X  Q )= n ( be shown that documents are ranked in the same order as using the query likelihood (QL). Thus, we will refer to this basic model as QL in the remainder of this paper.
In this section we introduce our relevance feedback model based on normalized log-likelihood. The goal of a relevance feedback algorithm is, given a query and a set of judged documents, to transform the original query and return more documents that will be useful to the user. Most relevance feedback approaches for LM use an improved estimate or estimation method for the query model to incorporate rel-evance feedback information. Typically, the initial query ismixedwithanexpandedpart  X  R , which is a distribu-tion over terms that represents the outcome of a trans-formation of the initial query [ 2, 13, 16, 22]. This mix-ture is usually modeled as a linear interpolation, effectively reweighing the initial query terms and providing smoothing: P ( t |  X  Q )=(1  X   X  Q ) P ( t |  X   X  Q )+  X  Q P ( t |  X  R ) .
If we were to have an infinite number of relevance judg-ments from the user and, hence, could fully enumerate the documents relevant to a query, we could simply use the em-pirical estimate of the terms in those documents to estimate  X  . Furthermore, given all sources of information available to the system (query, assessments, and documents in the col-lection), the parameters of this model would fully describe the information need from the system X  X  point of view. As-suming independence between terms, the joint likelihood of observing the terms given  X  R under this model is: where R denotes the set of relevant documents. Then, we can use a simple maximum likelihood estimate to obtain where n ( t, D ) indicates the count of t in D . Later, we will refer to this model as MLE. In contrast with this approach, however, a typical search engine user would not provide an infinite amount of data and only arrive at judgements on the relevance status of a small number of documents [ 18]. Even in larger-scale TREC evaluations, the number of as-sessments per query is still a fraction of the total number of documents in the collection [ 5]. So in any realistic scenario, the relevance of all remaining, non-judged documents is un-known and this fact jeopardizes the confidence we can put in the MLE model to accurately estimate  X  R .

Moreover, as we pointed out in the introduction, not ev-ery document in R is necessarily entirely relevant to the in-formation need. Ideally, we would like to weigh documents according to their  X  X elative X  level of relevance. As such, each relevant document should be considered as a separate piece of evidence towards the estimation of  X  R , instead of assum-ing full independence between documents as in Eq. 3.
Let X  X  consider the following sampling process to substan-tiate this intuition. We pick a relevant document according to some probability and then select a term from that docu-ment. Assuming that each term is generated independently of  X  R once we pick a relevant document, the probability of randomly picking a document and then observing t is P ( t, D |  X  R )= P ( D |  X  R ) P ( t | D ) . Then, the overall probabil-ity of observing all terms can be expressed as a sum of the marginals: The key term here is P ( D |  X  R ); it conveys the level of rele-vance of D . While we know that every D  X  R is relevant, we posit that documents that are more similar to  X  R are more topically relevant and should thus receive a higher proba-bility of being picked. We propose to base the estimate of P (
D |  X  R ) on the divergence between D and  X  R and we mea-sure this divergence by determining the log-likelihood ratio between D and  X  R , normalized by the collection C : Interpreted loosely, this mea sure indicates the average sur-prise of observing document D when we have  X  R in mind, normalized using a background collection, C . The measure has the attractive property that it is high for documents for which H (  X  D , X  C )ishighand H (  X  D , X  R ) is low. So, in order to receive a high score, documents should contain specific terminology, i.e., they should be dissimilar from the collec-tion model but similar to the topical model of relevance. Since we do not know the actual parameters of  X  R by which we could calculate this, we use R as a surrogate and lin-early interpolate it with the collection model (viz. Eq. 3): P ( that the sum in Eq. 5 is over the same event space for all language models involved and that zero-frequency issues are avoided. Then, in order to use this discriminative measure as a probability, we define a document-specific normalization factor Z D =1 /
As an aside, other ways of estimating P ( D |  X  R ) have been proposed, such as simply assuming a uniform distribution, the retrieval score of a document, the inverse thereof, or in-formation from clustered documents [ 2, 9]. Our approach is equivalent to using document cluster information under the assumption that only a single cluster is used, namely that which contains all relevant documents. Using the retrieval scores or, in an LM setting, the likelihood that a document generated the query, is a much simpler implementation of the same idea, essentially replacing  X  R with the initial query. And, since the initial query is quite sparse compared to  X  we avoid overfitting.

Finally, by putting the earlier equations together, we ob-tain the estimate of our expanded query model: This model, to which we refer as NLLR (normalized log-likelihood), effectively determines the expanded query model P ( t |  X  R ) based on information from each individual relevant document and the most representative sample we have of  X  ,namely R . We use the test data provided by the TREC Relevance Feedback track, where the task is to retrieve additional rel-evant documents given a query and an initial set of assess-ments [ 4]. Retrieval is done on the .GOV2 corpus, from which we remove stopwords and to which we apply Porter stemming. We use the titles of the 31 topics that received additional judgments. For each of these topics, a large set of relevance assessments is provided (159 relevant documents on average, with a minimum of 50 and a maximum of 338). Participating systems were to return 2500 documents, from which the initially provided relevant documents are removed. The resulting rankings were then pooled and re-assessed. This yielded 55 new relevant documents on average per topic, with a minimum of 4 and a maximum of 177. We follow the same setup and remove all initially judged documents from the final rankings in our experiments.

Below, we report on the following measures; precision at 5 (P5), precision at 10 (P10), mean average precision (MAP), and the number of retrieved relevant documents (relret).
We fix the estimation method of the document models and use Dirichlet smoothing which has been shown to achieve su-perior performance on a variety of tasks and collections [ 12, 23]. We set  X  = 1600 and we keep only the 10 terms with the highest probability for all models. We optimize the re-maining parameter settings on MAP using a grid search.
In our experiments, we compare our model with two other established relevance feedback methods. In particular, we look at Lavrenko and Croft X  X  relevance models [ 11]andZhai and Lafferty X  X  model-based feedback [ 22] which are indicated by RM and MBF respectively. For all our experiments we use the Lemur toolkit and use the provided implementations whenever possible. In this section we present our main experimental results. Table 1 shows the experimental results of applying the vari-ous approaches for estimating P ( t |  X  R ). As indicated earlier, these results are obtained by using the full set of judged relevant documents for estimation and subsequently remov-ing these from the rankings. Figure 1 further illustrates the differences using a precision-recall graph.

First, we observe that the query-likelihood results (QL) are on a par with the median of all submitted runs for the TREC Relevance Feedback track [ 4]andallmodelsde-scribed improve over this baseline. If we would have sub-mitted the results of the NLLR model, it would have ended up in the top-3 for this particular category. The RM run would have been placed at around rank 7.

Since these results are obtained by using the full set of rel-evance assessments, one might expect that the MLE achieves high scores since this set should be representative of the in-formation need and, hence, of the distribution of  X  R .Con-trary to this intuition, however, the MLE approach does not achieve the highest performance when new relevant docu-ments are to be retrieved; a finding in line with observations made by Buckley et al. [ 5]. MBF X  X hich re-estimates the MLE model X  X ainly has a precision-enhancing effect: re-call and MAP are hurt using this approach when compared against MLE.
 A precision enhancing effect is also visible when using NLLR and RM. Indeed, NLLR achieves the highest scores overall, except for the number of relevant retrieved docu-ments (RM retrieves 2 relevant documents more). We fur-ther observe that NLLR obtains a significant 63 . 7% im-provement in terms of MAP over the baseline. The MAP score is higher than the one obtained by RM and, moreover, NLLR obtains a statistically significant improvement with  X  =0 . 001.

Figure 1 shows that NLLR improves over all models on all recall levels. This figure also shows that the MBF ap-proach does not help as compared to MLE. However, MBF should be equivalent to MLE when the collection element is removed. It seems that under this particular model and the current experimental conditions, the introduction of the collection model deteriorates the results.

When we look at the individual topics, we find that topic 808 ( X  X orth korean counterfeiting X ) seems particularly diffi-cult and the retrieval performance is worst on this topic for all employed query models (although there are 530 judged relevant and 330 new relevant documents available). We relret 1122 1279 +14.0%* 1349 +20.2%** 1351 +20.4%** 1254 +11.8%* the baseline at the p &lt; 0 . 001 ,p &lt; 0 . 01 ,p &lt; note that, in general, NLLR is able to improve over the base-line on a larger number of topics than the other methods. RM works best for topic 766, on which NLLR also performs very well. The other two models (MBF and MLE) improve most on topic 814. Interestingly, this topic is also helped a lot by NLLR, but not by RM. These observations provide evidence that NLLR is indeed able to reap the benefits both of the individual relevant documents (like RM) and of the setasawhole(likeMBF).
Relevance assessments by a user are an important and valuable source of information for retrieval. In a language modeling setting, various methods have been proposed to estimate query models from these. Most of these models, however, attempt to update the initial query based on either the full contents of each assessed document or their aggre-gate. In this paper we have presented a novel query model-ing method which incorporates both sources of evidence in a principled manner. It leverages the distance between each relevant document and the set of relevant documents to in-form the query model estimates and, as such, it is more gen-eral than the methods proposed before. We have evaluated our proposed model on the TREC Relevance Feedback test collection and found that it improves over a query-likelihood baseline as well as over other established methods. This research was carried out in the context of the Virtual Laboratory for e-Science project and supported by the DuO-MAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments un-der project number STE-09-12 and the Netherlands Organi-sation for Scientific Research (NWO) under project numbers 017.001.190, 640.001.501, 640.002.501, 612.066.512, 612.061.-814, 612.061.815, 640.004.802. [1] J. Bai and J.-Y. Nie. Adapting information retrieval [2] K. Balog, W. Weerkamp, and M. de Rijke. A few [3] M.BenderskyandB.W.Croft. Discoveringkey [4] C. Buckley and S. Robertson. Relevance feedback [5] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. [6] M. Clements, A. de Vries, and M. Reinders. The [7] P. Clough, H. M  X  uller, T. Deselaers, M. Grubinger, [8] D. Hiemstra. A linguistically motivated probabilistic [9] O. Kurland. The opposite of smoothing: a language [10] J. Lafferty and C. Zhai. Document language models, [11] V. Lavrenko and B. W. Croft. Relevance models in [12] D. Losada and L. Azzopardi. An analysis on [13] E. Meij, W. Weerkamp, K. Balog, and M. de Rijke. [14] D. Metzler and B. W. Croft. A markov random field [15] K. Ng. A maximum likelihood ratio information [16] J. Rocchio. Relevance feedback in information [17] G. Salton and C. Buckley. Improving retrieval [18] A. Spink, B. J. Jansen, and C. H. Ozmultu. Use of [19] A. Spink, B. J. Jansen, D. Wolfram, and T. Saracevic. [20] X. Wang and C. Zhai. Mining term association [21] W. Weerkamp, K. Balog, and E. J. Meij. A generative [22] C. Zhai and J. Lafferty. Model-based feedback in the [23] C. Zhai and J. Lafferty. A study of smoothing
