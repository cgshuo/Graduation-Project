 Scalable data management in the context of Big Data has emerged as a new research frontier[6]. As a semi-struct ured data, XML has become an important de facto standard format for data storage and exchange. The rapid growth of XML data scale boosts the demands of distributed XML query processing.
To avoid a large number of intermediate results of binary structural join[1], holistic twig algorithm[2] utilizes node streams and chained stacks to realize the examination of whether each matched node contributes to the final results, so that all the nodes pushed into the corresponding stacks can be guaranteed to be a part of solution. TwigStack[2] achieves optimal when the twig pattern contains only ancestor-descendant relationship. Follow-up work [10,4,11,3] lead to better query processing performance and illustrate that holistic twig algorithms are the most efficient processing methods for twig queries.

Cloud computing is currently a major technology for processing large-scale data. Since centralized holistic twig algorithms cannot be applied directly to shared-nothing distributed settings, especially to the cloud environment, two challenging tasks have to be addressed: 1) partition and storage strategies of large-scale XML data on shared-nothing machines; 2) parallelized local comput-ing paradigms on each separated machine.

This paper focuses on distributed twig query processing in the case that large-scale XML data are partitioned and stored under the mechanism of the cloud computing framework, such as Hadoop. That is, our distributed XML storage strategy ensures the arbitrariness of partition without any knowledge of incom-ing queries . Distributed TwigStack (DTS) is p roposed based on MapReduce[8] and TwigStack[2], which processes distributed twig queries regardless of how the XML data are partitioned . ComMapReduce[9] framew ork with a coordinator is applied, which refines global keys in a lightweight manner . In summary, the contributions of this paper are as follows: 1. A distributed XML storage strategy is designed to maintain structural infor-2. A distributed twig query processing algorithm based on TwigStack is pro-3. Extensive experiments are conducted to verify the effectiveness and efficiency
The rest of the papers is organized as follows. Section 2 presents the re-lated work. Section 3 proposes the partitioning strategy, based on which the distributed holistic twig algorithm is proposed in section 4. Experimental results are presented in section 5. Section 6 draws the conclusion of this paper. MapReduce[8] realizes powerful parallel computing ability with high scalability, including three major computing phases: 1) map receives a unit of input data each time and processes it into a list of key, value pairs; 2) shuffle combines each group of the value s corresponding to the same key into key, list value ; 3) reduce aggregates all the value sofeach key to produce final results.
Machdi et al. in [13] proposed XML data partition strategies based on GMX [12], taking the advantages of both inter and intra query parallelism on static and dynamic data distribution. Wu in [15] proposed a polynomial-based workload distribution algorithm. Inverted list labels were distributed rather than raw XML documents. However, in both of these two papers, XML fragments have to be stored with priori knowledge of the query pattern .

Damigos et al. in [7] decomposed queries into single paths to realize dis-tributed XPath processing based on Map Reduce. However, it required each XML partition to contain full prefix path from the root node. As a consequence, the flexibility would also be limited.

Choi et al. designed a system HadoopXML in [5] to process multiple twig queries simultaneously. The input scans and intermediate results were shared, which saved many I/Os, and improved runtime load balance. However, despite of preprocessing, HadoopXML needs two consecutive MapReduce jobs for query processing, which we aim to avoid in this paper. 3.1 Intuition of Arbitrary Partition Different from traditional distributed environment, we pursue the arbitrariness of XML partitioning and storage in the cloud without knowledge of query pat-terns. To guarantee the completeness of results ,werepresentanXMLnodeas atriple label, rcode, type ,inwhich label is the label name, rcode is the region code represented as start, end, level , type indicates the partition type of the edges corresponding to this node. We also define that for node v , parent edge is the edge between v and its parent; an edge between v and one of its children is denoted as a child edge . Edge-cuts can be summarized in four partition types :  X  no-cut (type 0): neither the parent edge nor a child edge is cut;  X  child-edge-cut (type 1): at least a child edges is cut, but not the parent edge;  X  parent-edge-cut (type 2): the parent edge is cut, but not the child edges;  X  all-cut (type 3): both the parent edge and at least one child edge are cut. Example 1. Given an XML document[2] to be partitioned into four fragments as shown in Figure 1(a), while scanning the node stream, since fragAnc is empty, the first four nodes authors, (1 , 5:60 , 2) , 0 , author, (1 , 6:13 , 3) , 0 , fn 1 , (1 , 7:9 , 4) , 0 and jane 1 , (1 , 8 , 5) , 0 will not be processed. Then the jane 1 , (1 , 8 , 5) , 0 belongs to the first fragment and node ln 1 , (1 , 10 : 12) , 4 is partitioned into the second fragment. According to case 1, the type of node ln 1 , (1 , 10 : 12 , 4) , 0 is set to 2, which will be ln 1 , (1 , 10 : 12 , 4) , 2 ,andof which the ancestor nodes authors, (1 , 5:60 , 2) , 0 and author, (1 , 6:13 , 3) , 0 are saved in fragAnc . Then the scan continues from node ln 2 , (1 , 39 : 41 , 2) , 0 , of which the type is reset to 2; the type of node author 3 , (1 , 52 : 59 , 4) , 0 is reset to 3; the type of node fn 3 , (1 , 53 : 55 , 2) , 0 is reset to 2. The ancestor node author 3 , (1 , 52 : 59 , 4) , 0 is pushed into fragAnc . When the scan restarts 2. The final partitioned node stream is shown in Figure 1(b). 3.2 Partition Type Determination Once the partition size is set by some criteria (e.g., Hadoop block size), during the traversal of the whole node stream, the partition type of the nodes in the previous partitions have to be re-checked, because they may have child-edge-cuts with the nodes in the following partitions. Therefore, an array named fragAnc is maintained by the following strategies: 1) all the ancestors of the next node after each partition cut will be pushed into fragAnc ; 2) all the nodes at the same level as the node being traversed will be popped out from fragAnc .
Note that for the first node of each partition, only their ancestors may have cut edges with the remaining nodes have not been traversed; All the nodes at the same level with the node being traversed, let X  X  say node v , will not have edges with v and nodes following v .
 Algorithm 1. Arbitrary XML partitioning As described in Algorithm 1, given an XML document T , we first transform T into a node stream (Line 1). While scanning this node stream, let v be the current node being processed (Line 3). If v is the first node of the partition, which means the parent edge of v is cut, set type of v to 2 and push all the ancestors of v into fragAnc (Lines 4-6). If v is the last node of the partition (Lines 8-11), a new partition of XML is generated. Before saving this partition, if v is not a leaf node, set the type to 1 (Line 10), meaning it will have child-edge-cut for sure; else 0. Then we check whether v has a parent in fragAnc (Line 12).
Function checkFragAnc is designed to reset the type values of the nodes which have cut edges with the nodes being traversed. During the check of fragAnc , if v hasaparentoftype0(meaningnoedge-cut),resetitstypeto1(meaning has a child-edge-cut); if type 2 (indicating having a parent-edge-cut), reset it to 3 (all-cut) (Lines 13-18). If v doesn X  X  have a parent in fracAnc , we do nothing. At last, we pop out all the nodes in fragAnc at the same level as v (Line 19). MapReduce is now one of the most efficient computing models in the cloud environment. In this section, ba sed on MapReduce, we present a na  X   X ve solution to distributed twig processing, and propose a distributed holistic twig query processing algorithm on the basis of TwigStack. 4.1 Na  X   X ve Solution Since no data exchange among mappers is allowed, and neither among reducers, arbitrarily partitioned XML data brings possibilities of losing matches across multiple partitions. A na  X   X ve solution is to run two rounds of MapReduce:  X  Round 1 map: find all the matches to the query nodes on the local machine,  X  Round 1 reduce: collect the key-value pairs for the next round. Each final  X  Round 2 map: run a twig algorithm, e.g., TwigStack, to generate final results;  X  Round 2 reduce: collect all the final results. 4.2 Distributed TwigStack Intuition. The na  X   X ve method generates many unnecessary intermediate re-sults in the first round, in addition, the start-up of a MapReduce job is time-consuming. Our framework Distributed TwigStack (DTS) aims at using one MapReduce job and reducing intermediate results as well.

In DTS, map function finds all the partial path matches on the partition stored on this machine, emits key-value pairs with globally refined keys by the coordinator of ComMapReduce. The i mprovement of DTS can be summarized in two aspects: 1. The matching rules are relexed in DTS, that is, some incomplete matching 2. The introduced coordinator guarant ees that each group of matches need to Algorithm DTS. More specifically, as described in Algorithm 2, the map func-tion gets query Q from distributed cache (Line 2), and generates node streams qStream on this fragment F according to the nodes of Q (Line 3). Each map function invokes Relaxed Local TwigStack with Q , qStreams and partition nodes partitionN odes , i.e., nodes with cut edges, of fragment F as parameters (Line 6). Each machine running RL-TwigStack will send keys to the coordinator, and then the map function of Distributed TwigStack resets the keys of its own in-termediate results, of which the key is the reset region encoding and the value is the partial matches to the query (Lines 7-10). The reduce function collects all the intermediate matching results corresponding to the same key and combines them into final results (Lines 12-14). Due to space constraints, the processing of ComMapReduce[9] and refining global keys are not presented.

Algorithm 2. Distributed TwigStack
RL-TwigStack (Algorithm 3) invokes RL-GetNext function to guarantee that: 1) for any node v i  X  child ( v ), a node of the XML document h ( v ) has a descendant node h ( v i )instream T ,or h ( v ) has a cut child edge; 2) each v i  X  child ( v ) satisfies the first point recursively. The major difference between RL-GetNext and getNext in TwigStack is that: RL-GetNext outputs not only the nodes having exact matches along the path from themselves to the leaf nodes of the query, but also the nodes with one or more cut child edges.

With a returned node q act by RL-GetNext (Line 2), the clean stack operation (Lines 3, 4) ensures that the node in the parent stack of q act , i.e., Sparent ( q act ), is an ancestor of the node in node stream Tq act .If q act is the root node of the twig pattern, or the parent stack of q act is not null, push q act in to the stack Tq act after popping the top node in Tq act which is not an ancestor of q act (Lines 5-7). If q act is a leaf node of the twig pattern, or the descendant streams of q act , i.e., Tdesq act , have no descendant nodes of q act , RL-ShowSolution (as Algorithm 5) is invoked (Lines 8-10) to produce intermediate results, which will be further explained in algorithm 5. If q act is not the root node and the nearest ancestor stack is empty, the nodes in stream Tq act with cut parent edge is pushed into stacks (Lines 11-17).

Compared with TwigStack, the examination criteria of whether a node can be pushed into chained stacks in RL-TwigStack is relaxed, that is, even a node in the node streams does not satisfy the getNext function, if it has one or more cut child edge, it will also be pushed into its stack. Therefore, the criteria of generating a partial solution is also relaxed in algorithm Relaxed Local ShowSolution (RL-ShowSolution), which is presented as Algorithm 5.

Algorithm 3. RL-TwigStack(q,T,partitionNodes)
In RL-ShowSolutions, each index[ i ]( i =1... n , n is the number of stacks) indi-cates the position of the node being processed in its corresponding stack (Line 2). SN is the stack ID and SP is the node position in its stack. The keys of all the path solutions are set to the highest region encode. The value of the solutions is
Algorithm 4. RL-GetNext(q) a key-value pair pathT ype, path . pathT ype can be set to four values, including a full path type  X  X -m-l X , a path with only a root node  X  X -* X , a path with only a leaf node  X *-l X  and a path with only the middle nodes  X *-m-* X . There are four cases of generating a solution: 1) if the relative leaf node of this solution index[ originalSN ] has a cut child edge, and the relative root node index[ SN ] has a cut parent edge, solutions of types  X *-l X ,  X  X -* X  and  X  X -m-l X  will be gener-ated (Lines 6 and 7); 2) if the relative leaf node has a cut child edge, and the relative root node index[ SN ] has no cut parent edge, solutions of types  X  X -m-l X  and  X  X -* X  will be generated (Lines 8-9); 3) if the relative leaf node has no cut child edge, and the relative root node index[ SN ] has a cut parent edge, solutions of types  X  X -m-l X  and  X *-l X  will be generated (Lines 10-11); 4) if the relative leaf node has no cut child edge and the relative root node index[ SN ]hasnocut parent edge, only solutions of type  X  X -m-l X  will be generated (Lines 12-13). All the intermediate solutions will be generated recursively (Lines 15-17).
Algorithm 5. RL-ShowSolutions(SN,SP) 4.3 An Example of DTS Example 2. Given a query  X  // author[fn / text()= X  X ane X  ln / text()= X  X oe X  X  X , on fragment F1, RL-TwigStack recursively i nvokes method RL-GetNext. Since RL-GetNext( ln ) returns null , RL-GetNext( fn ) returns node fn , so that invocation RL-GetNext( a ) returns node a . Because stack Sa is empty and the current node in stream Ta is node a 1, node a 1 is pushed into Sa . In the next round, RL-GetNext( q ) returns node fn . The parent stack of Sfn is not empty, of which the top element a 1 is the parent of the current node in Tfn , therefore, node fn 1 is pushed into stack Sfn , and a pointer is set from fn 1to a 1 in the chained stack. In the third round, RL-GetNext( q ) returns j 1, which is pushed into Sj and linked to fn 1instack Sfn .Since j 1 is a leaf node and the type value of a 1is1, thus two key-value pairs are generated: 6:13 , ( j 1 ,fn 1 ,a 1) and 6:13 , (  X  ,a 1) . On fragment F2, nodes ln 1, a 2and fn 2arestoredin fragAnc .DuringRL-GetNext( ln ) in the first round, the child stream of Tj is empty, and the current node in stream Tln has no cut child edge, thus RL-GetNext( ln ) returns null .But the current node a 2instream Ta has a cut child edge, thus a 2 is pushed into stack Sa . In the second round, node fn 1 is pushed in to stack. Thus, two intermediate results are produced, which are 35 : 42 , (  X  ,a 2) and 35 : 42 , (  X  ,fn 2 ,  X  ) . On fragment F4, nodes fn 3and ln 3aresavedin fragAnc . The first round of RL-GetNext( q ) returns fn . The current node of stream Tfn has a cut parent edge, node fn 3 is pushed into stack. In the second round, node j 2 is pushed into stacks. Thus, the intermediate results of F4 are 53 : 55 , ( j 2 ,fn 3 ,  X  ) , 53 : 55 , ( j 2 ,  X  ) , 56 : 58 , ( d 2 ,ln 3 ,  X  ) and 56 : 58 , ( d 2 ,  X  ) .
The computation on F3 is similar to F2, producing intermediate results 39 : 41 , ( d 1 ,ln 2 ,  X  ) , 39 : 41 , ( d 1  X  ) and 52 : 59 , (  X  ,a 3) .
After the local computation, coordina tor receives all the keys sent from all the fragments, which are 6:13, 35:42, 53:55, 56:58, 39:41 and 52:59. Coordinator checks whether each two of these keys hav e ancestor-descendant relationships, and send back the refined keys to each machine running map functions. As a result, the keys on F1 and F2 remain the same; 39:41 on F3 is set to 35:42; all the keys on F4 are set to 52:59. Then all the refined intermediate results correspond-ing to a same key are sent to the same machine running reduce functions. That is, (j1, fn1, a1) and (*, a1) are sent to machine 1; (*, a1), (*, fn1, *), (d1, ln2, *) and (d1,*) are sent to machine 2; ( *,a3), (j2,fn3,*), (j2,*), (d2,ln3,*) and (d2, *) are sent to machine 3. Only machine 3 produces a final result (a3,fn3,ln3,j2,d2). 5.1 Experiments Setup The distributed experiments are conducted on a Hadoop cluster of 1 master node and 8 slave nodes, each of which has an Intel Core 2.66GHZ CPU, 4GB memory, CentOS 5.6 and Hadoop 0.20.2. We use a 5.4GB synthetic dataset based on XMark[14]. We choose the twig pattern  X //person[//country= X  X S X  X //zipcode X  as Q1,  X //people/person[/address/country= X  X S X  X /creditcard X  as Q2. 5.2 Evaluation Results The scalability comparison is presented in Figur e 3. The number of slaves changes from 1 to 8. Figures 3(a) and 3(b) indicate that DTS outperforms the na  X   X ve method, especially with relatively fewer slave nodes or more complex queries. Figures 3(c) and 3(d) indicate that the efficiency of the na  X   X ve method drops more dramatically than DTS when the size of dataset increases.

Figure 4(a) and 4(b) presents the speedup comparison, indicating that along with the increment of the slaves number, DTS has a better scalability due to its one-round MapReduce design, intermediate results matching and distribution mechanism. Figures 4(c) and 4(d) present the scaleup comparison, indicating that:1) the na  X   X ve method has a better speedup at first, because the startup of MapReduce jobs is the major part of the query time when the dataset is relatively small; 2) DTS gains a better sizeup when the scale of XML data increases. In order to answer twig queries over large-scale XML documents in the cloud, we propose an arbitrary XML partitioning strategy without priori knowledge of the query pattern, and a distributed query processing algorithm DTS based on MapReduce, regardless of how the XML data are partitioned. Experimental results show that DTS achieves good efficiency and scalability.
 Acknowledgment. This research is partially supported by the National Natu-ral Science Foundation of China under Grant Nos. 61272181 and 61173030; the National Basic Research Program of China under Grant No. 2011CB302200-G.
