 A keyword list is a list of five to ten words or noun phrases that represent, in condensed form, the essence of the topic of a document. Because these keywords are generally noun phrases of two or more words, they are sometimes called keyphrases. Keywords are widely used to identify text documents in applications such as documents analy-sis, document indexing, and information retrieval systems (IRS) and to improve the functionality and performance of other applications, such as digital library search-ing, Web content management, document clustering, categorization of large document collections, and text summarization [Rose et al. 2010].

The manual extraction of keywords by professional indexers according to well-established standards and widely-used thesauri or controlled vocabularies is time consuming, tedious, expensive, subjective, and impractical [Giarlo 2006]. Therefore, automatic techniques for the extraction of keywords from documents are required. Automatic keyword extraction is a systematic process that identifies a small set of important words or noun phrases from within the body of a document that can de-scribe the document X  X  content. Automatic keyword extraction is one of the main appli-cations of the text mining techniques that have received significant attention in the last decade.

Keyword extraction algorithms have been successfully developed and implemented for documents in Western and Eastern European languages [Cohen 1995; Giarlo 2006; Liu et al. 2009; Matsuo and Ishizuka 2004; Rose et al. 2010]. However, keyword ex-traction for Arabic documents is a little researched topic, and there are few related publications [El-Beltagy and Rafea 2008; El-Shishtawy and Al-Sammak 2009]. The Arabic language is seventh of the top ten languages used on the Internet, with 18.8% of the Arab population going online. Arabic content on the Internet and other digital media is increasing exponentially; it scored the highest percentage in growth (over 2,500%) among languages between 2000 and 2011 [ESCWA 2012]. However, only a very small portion of the Arabic texts available online have associated keywords.
In this work, we describe a new method for extracting keywords from Arabic docu-ments. This work attempts to take advantage of Arabic language features and existing natural language processing tools, such as morphological analysis and part-of-speech tagging to extract keywords that represent the document content accurately. To this end, we propose a keyword extraction technique that operates on an individual docu-ment and is based on both linguistic analysis and multiple levels of statistical analysis of words in the document. Because this method operates on a single document, it pro-vides context and domain-independent keywords that can be useful in many contexts and for different natural language analysis purposes.

This article is structured as follows. We introduce related works and main ap-proaches to keyword extraction in Section 2. In Section 3, we present the Arabic lan-guage particularities and features exploited in our technique. A detailed description of the proposed technique and its components is provided in Section 4. Section 5 de-scribes the datasets used for the evaluation of the proposed method and discusses the results of the conducted experiments. Automatic keyword extraction has been applied as early as the 1970s with the works of Jones [1972] and Salton et al. [1975]. Different approaches to keyword extraction have been presented in research papers demonstrating different ways to extract key-words from a text document. Rose et al. [2010] described the main keyword extraction systems used by the IRS and distinguished two main approaches: corpus-oriented methods and document-oriented methods. The first and oldest method focuses on corpus-oriented statistics of individual words. The keywords selected for a document are identified by comparing a word list of the text with a word list extracted from a larger reference corpus. Words are treated on an individual basis, and those occuring very frequently across the reference corpus are not generally selected as keywords. The second approach operates on individual documents. The selected keywords or key terms are representative of the input document regardless of the state of the reference corpus. With the increasing volume of electronic documents published online without prior classification, these techniques of working on individual documents start are being used more frequently in various applications because they do not require prior analysis of a large corpus.

Most of the early systems select keywords based on their frequency distribution in individual documents or within a corpus. Modern approaches combine statistics with natural language processing techniques or machine learning approaches to im-prove their performance and to achieve high-quality keyword extraction [Cohen 1995; Hulth 2003, 2004; Turney 1999; Witten et al. 1999]. Analysis of the linguistic proper-ties of words, sentences, and documents suggested by these approaches are evolving from using part-of-speech tags and syntactic structure to using semantic qualities. Hulth [2003, 2004] studied and compared the results of different keyword extraction methods based on linguistic features, showing that incorporating linguistic feature analysis greatly improves automatic keyword extraction.

Research on keyword extraction from Arabic documents is rare and limited to adapting general algorithms used for English and other European languages on with Arabic documents. El-Beltagy and Rafea [2008] have presented a keyphrase extrac-tion system for both Arabic and English documents. This system, called KP-Miner, was developed for the English language then adapted to work with the Arabic language. The system detects candidate keywords using TFxIDF measures with two conditions. The first condition states that a phrase has to have appeared at least n times in the document from which the keyphrases are to be extracted. The second condition is re-lated to the position where a candidate keyphrase first appears within an input docu-ment. Two cutoff values are defined as a threshold for satisfying these two conditions. El-Beltagy and Rafea [2008] proposed some modifications to adapt the English key-word extractor for Arabic documents; these changes mainly concern stop word removal and word stemming. They used a custom-built stemmer to reduce any word to its singular form by removing prefixes, suffixes, and infixes.

El-Shishtawy and Al-Sammak [2009] presented a supervised technique that uses linguistic knowledge and machine learning techniques to extract Arabic keywords. Linguistic information is used in three phases of the technique presented: tokeniza-tion, part of speech tagging, and word abstract form extraction. The candidate terms are presented by their abstract forms instead of their surface forms. To increase its speed, the system uses an annotated Arabic corpus to extract the linguistic knowledge instead of analyzing the morphology, syntax, and semantics of the text. It treats all types of words, such as verbs, names, adjectives, and function words equally, which may generate false candidate words. The validation of the sequence of words is based on syntactic rules generated from a limited annotated Arabic dataset of 30 documents built by the authors from a specific domain as the training documents. The keywords from the documents X  dataset used to evaluate their system were assigned manually. In contrast to successful English language keyword extraction systems, such as Extractor [Turney 1999], KEA [Witten et al. 1999], and TextRank [Mihalcea and Tarau 2004], keyword extraction systems for the Arabic language have very limited performance. When used with the Arabic language, existing keyword extraction sys-tems originally developed for English exhibit a significant drop in accuracy. For exam-ple, a comparison of the English version of KM-Miner with its Arabic adapted version shows that the precision achieved by the system deteriorates from 0.21 to 0.13 when the number of keywords is set to 20. The adaptation of these algorithms to Arabic in-volves new challenges because of the morphological and syntactical properties of the language. Arabic is a highly inflected language with a very rich and complex mor-phology, where the same word may differ in spelling according to the position in the sentence [Al-Sughaier and Al-Kharashi 2004]. An individual Arabic word appears in text much less often, and the token-to-type ratio for Arabic is much lower than that for English [Hmeidi et al. 1997]. In its raw surface form, Arabic text is therefore much sparser than English, a feature that significantly decreases the efficiency of statistical analysis. In addition, Arabic has a relatively free word order compared with English, where the word order is relatively fixed, so the selection of keywords consisting of more than one word is negatively affected. Finally, the Arabic language suffers from a lack of resources that can be used for building an accurate learning model. The number and size of documents with keywords used for building learning models are also limited and insignificant; therefore, these limitations considerably decrease the efficiency of training in a learning setting.

This article describes a new approach for extracting keywords from Arabic docu-ments that overcomes the previously listed challenges. The proposed approach is an unsupervised technique that uses a deeper linguistic features analysis of the language to create a new representation of the texts. This new representation is based on group-ing terms in classes according to their roots and stems to decrease the sparse na-ture of Arabic texts and make the statistics more significant. It analyzes the term co-occurrences in order to identify potential keywords. As it is an unsupervised tech-nique, it does not require building an annotated corpus. The Arabic language is a Semitic language with a rich and complex morphology. This feature of the language should be considered in developing keyword extraction meth-ods. Habash [2012] discussed the morphology of Arabic language and highlighted its features, mainly, the presence of templatic morphemes in addition to concatenative morphemes. Concatenative morphemes include stems, affixes, and clitics. There are three types of affixes: prefixes, suffixes, and circumfixes. A prefix can consist of as many as four concatenated prefixes or could be null. The suffix consists of as many as three concatenated suffixes or could be null. The circumfixes are generally combinations of prefix with suffixes. A clitic is a symbol of one to three letters that represents another token, such as a preposition, conjunction, definite article, or object pronoun. There are two types of clitics, namely, proclitics and enclitics. Proclitics precede a word, for ex-ample, the definite article. Enclitics follow a word, for example, object pronouns [Diab et al. 2007; Habash 2012]. Figure 1 illustrates the concatenative morphology of Arabic language by describing the different morphemes of the word  X   X  X nd they will write it, X  read in transliteration as  X  X a-sa-ya-ktub-uwna-hA X  [Habash et al. 2007]. The general structure of an Arabic word can be represented as follows:
Since all clitics and affixes are optional, the stem is a valid Arabic word that may be either derivative or nonderivative.

Most Arabic words (stems) are derivative words that are generated according to the root-and-pattern scheme or templatic morphemes. From one root, tens of words (surface form) can be derived according to a predefined list of standard patterns called morphological patterns or balances. A word may then be represented by its root along with its morphological pattern. The most commonly used roots in the modern Arabic language are three letters long and they number fewer than 5,000 [Beesley 1996]. Two other classes of words exist, namely, nonderivative words and stop words. Nonderivative words do not obey the standard templatic derivation rules. Examples of nonderivative words are words borrowed from foreign languages and proper names. On the other hand, stop words, sometimes called function words, include pronouns, prepositions, conjunctions, question words, and so on. Nonderivative words and stop words can receive affixes and clitics, for example, the word  X   X  X nd the democrats X  and is read in transliteration as  X  X a-Al-dymuqrATuwn, X  is a bor-rowed word from a foreign language with two proclitics and one suffix X  X he preposition  X   X   X   X  the definite article  X   X  X  , X  and the suffix  X   X  X  . X  The word  X   X   X  X   X   X  attached to the clitics  X   X   X   X  X nd X   X  X  . X 
Words in Arabic texts have irregular distributions. On the one hand, the most fre-quent words, mainly the stop words, account in general for more than 40% of words in texts; on the other hand, the majority of words in a text appear only once. The stop words are limited in number, but their frequency in texts is very high. Table I shows the irregularity of the word distribution in a collection of 1.7 million words (token) and 89,734 distinct words (types) extracted from the BBC Arabic news collection [Saad 2011]. It demonstrates the high frequency of stop words compared to other words. The ten most common stop words occur 215,259 times, while 34,637 words, representing 38.6% of the words in the list, occur once each. The significant words representing the corpus are generally placed after the most frequent stop words of the language. This work is motivated by the importance of extracting keywords from Arabic docu-ments and the rarity and incompleteness of current solutions. General and automatic techniques independent from manual annotated training sets are needed. These tech-niques should be designed based on statistical measures in addition to the linguistic features of the language.

We propose exploiting the distinguishing linguistic features of Arabic language, such as the templatic morphology, to reduce the number of candidate keywords. In Arabic, most words are derivative words that can be represented by their roots and their mor-phological templates or patterns. The root is the original form of the word before any transformation process; it provide the basic meaning of the word [Al-Sughaier and Al-Kharashi 2004]. Replacing derivative words by their roots reduces the dimension-ality of the text, and hence, it increases the accuracy of keyword extraction.
The proposed method uses currently existing NLP tools and the statistical analy-sis of a text X  X  words to create a list of candidate keywords. This method is designed to operate on an individual document without prior knowledge about its domain. The pro-posed method is composed of six different phases. In the first phase, the document is preprocessed and analyzed linguistically to extract linguistic information at the level of individual words. In this phase, the words of a text are classified into three categories (stop words, derivative words, and nonderivative words) and are assigned their part of speech (POS) tags. The second phase is document cleaning (filtering), which consists of discarding stop words, numerals, foreign characters, and frequent language terms. In the third phase, a vector representation of the document, which lists terms and their weights, is produced. In the fourth phase, the clustering phase, the words are clus-tered into equivalence classes in which the derivative words generated from the same root are grouped and their counts are accumulated. The nonderivative words, includ-ing Arabized words borrowed from foreign languages, are also grouped into classes according their stems. Only the equivalence classes representing the most frequent groups of words generated from the same root or stem will be retained and named after the root or stem from which their words are generated. In the fifth phase, a statistical analysis of N-grams is performed to extract the most frequent words and combinations of N words. The analysis considers single words, as well as bigrams and trigrams, and considers that a word or an N-gram is likely to be a keyword or at least an impor-tant structure if it appears frequently in a text. In this phase, weights are assigned to each candidate keyword. These weights take into account the words X  frequencies in the documents and their positions in the text. The last phase includes the extraction of information related to word combinations to represent the retained keywords cor-rectly. The different phases are illustrated using the Arabic version of the Universal Declaration of Human Rights in a five-page document with 1,436 words (Figure 2). In a text, a keyword may appear in different surface forms. Thus, the use of the word count directly will not be able to highlight properly the most important key ideas in the text. Previous works on Arabic and many other languages X  texts generally over-come this problem by working on the words X  stems rather than on their surface forms. The tests of this approach in concatenative languages yield good results, because the stem is the smallest part of the word that carries the main information in these lan-guages. In the case of the Arabic language, characterized by the presence of templatic morphemes in addition to concatenative morphemes [Habash 2012], the roots are the main carrier of information, whereas we may find tens of words with different stems but generated from the same root. For example, in a text about schools, the words (the school X   X  X  X  X  X  X   X  , , schools X   X  X  X  X  X  , teaching X   X  X  X  X  X  X  X  , and teachers X   X  X  X  X  X  X  stems but are generated from the same root (teach, d r s X   X  X  X  information. Therefore, it is important to consider the roots instead of the stems for representing Arabic derivative words.

The preprocessing phase aims at analyzing the words of the text to extract the lin-guistic information needed for keyword extraction, namely, the stem, root, pattern, and part of speech (POS). The root and stem are the carriers of the main informa-tion in derivative words and nonderivative words, respectively. The POS will be used to weight the words, since certain POS tags such as nominal tags are more likely to denote keywords. This phase consists of several steps: tokenization, stemming, root ex-traction, and POS tagging. Tokenization chops character streams into tokens or word like units and turns the text into a list of individual words or tokens [Manning et al. 2009]. Stemming aims to reduce the surface of the word to its base form or stem. The stem is either a morpheme representing a nonderivative word or a word generated from a root according to a standard pattern. Root extraction is a fundamental step that needs the implementation of a derivational morphological analyzer. Therefore, it is important to consider the roots instead of the stems when it comes to representing Arabic derivative words.

All of the text words are analyzed using the Alkhalil Morph-Syntactic System (AMSS) [Boudlal et al. 2010]. This is a freely available, open-source software devel-oped in Java, and it is easy to add or modify components. For a given word, AMSS identifies all the possible morphological and syntactic features, specifically proclitics, prefixes, stem, word type, word pattern, word root, POS, suffixes, and enclitics. How-ever, AMSS suffers from a number of shortcomings, including the over-generation of possible solutions, with an average of five different solutions provided for each input word; insufficiency in processing nonderivative words, mainly words borrowed from foreign languages; and insufficiency in detecting some stop words such as  X   X   X  X  X  X  X  . X  These shortcomings are overcome by using the Stanford Arabic parser [Green and Manning. 2010] and updating the stop words lexicon. Here, the POS tags provided by the Stanford Arabic parser are compared with those provided by AMSS, and only compatible solutions provided by AMSS will be retained as final features of the word. A simple greedy, regular expression expression X  X ased stemmer is developed to extract the stems of nonderivative words that AMSS fails to analyze [Awajan 2011]. The stem-mer is repeatedly applied until the word stops changing, producing a new representa-tion of each word as a sequence of clitics, suffixes, and stems. For example, the word  X   X  X  X  X  X  X  X  X  X  X  X  X  , X  which means  X  X nd the British, X  is transformed into  X  X a (proclitic: prepo-sition) + Al (definite article) + bryTAn (Stem) + iy (suffix: adjective tool) fix: plural). X  The preprocessing phase assigns to each input word a type (derivative, nonderivative, or stop word), the root and pattern for derivative words, the stem for the other words, and its POS tag. In general, keywords in a text are selected according to their frequency in that text, and the most frequent words are considered representative of the content of the docu-ment. However, some of the most frequent terms, such as stop words, have no discrim-inating power in representing the content of their text and their presence in the list of keywords need to be reconsidered based on their perceived importance in the docu-ment. The relevance of a word within a document depends on different factors. Certain factors are related to the language itself, others are related to the corpus or the domain of the document, and the third category of factors is related to the document itself.
The cleaning or filtering phase proceeds by eliminating the less significant words from the text. Because this work is domain independent and oriented to extract key-words from individual documents without prior knowledge about their contents or cat-egories, only the language-based factors are used to select the terms to be discarded from the document according to their significances.

When a word is used very frequently in a language, it is considered to be worthless as a keyword, and its scale should be lowered [Matsuo et al. 2004]. Therefore, keyword lists rarely contain stop words, verbs, or punctuation marks. For example, in natural language texts, stop words such as  X  X he, X   X  X , X  and  X  X nd X  in English and  X   X   X  X  X   X  in Arabic are very frequent in texts but are not carriers of information about the content of the text. These stop words may be considered uninformative or meaningless and need to be dropped from the list of frequent terms.

In addition, a good portion of the verbs in texts carry less significant information than nouns, and the vast majority of keywords are nouns or noun phrases with adjec-tives. Verbs such as  X  X o be, X   X  X ay, X  and  X  X ndicate X  in English and  X   X  X  X   X   X   X   X  X  X   X  X nd X  Arabic are frequently found in texts without providing any real representation of their content. Therefore, the cleaning phase eliminates all the verbs except those generated from the same root as the nouns found in the texts. Thus, the significance of their nouns will be increased. For example, although the verbs in the sentences  X  X e studied - X  X  X   X  and  X  X e is studying - X  X  X  X   X  will not be retained as keywords, their weight helps to increase the score of the keywords, including the noun  X  X tudy X   X  X  X  X  X   X .
The document is transformed at the end of this phase into a reduced list of terms that represent on average less than 40% of its original size. Each term is represented by its base form (root or stem) and its part of speech, as well as the morphological pattern if it is a derived word. The delimiters of sentences are saved at this level because the sentence information will be used in the detection of N-gram keywords. The selection of keywords from the list of words depends on their weight or score in the text. Therefore, each term in the document needs to be assigned a weight that measures its importance and significance in the text to rank these terms according to their potential as candidate keywords or part of keyphrase. This weight is generally calculated based on a number of features defining its relative importance in the text. These features include the frequency of the candidate keyword and its first occurrence in the text. The linguistic information, such as the word category and part of speech, is also required for a more significant and accurate list of keywords. For example, nouns are more likely to be considered as keywords than adverbs, adjectives, and verbs.
The position of the first occurrence of the keyword within the document and the doc-ument structural hierarchy play important roles in defining the term X  X  relative weight [El-Beltagy and Rafea 2008; Turney 1999]. The first paragraph, generally an abstract or introduction, and the last one, generally a conclusion, have greater influence on the selection of keywords than the other paragraphs in the document. Words within the same paragraph of the document are always assigned the same weight. Other elements of the document structure, such as title, subtitles, and captions, have more impact on the selection of keywords. More specifically, a document structure can be viewed as a sequence of N sectors as follows: title, first paragraph (abstract), internal paragraphs, and last paragraph (conclusion). A weighted score L i  X  [ 0, 1] such that L assigned to each sector i. If the number of occurrences of a word w in sector i of a doc-ument is given by Freq(w,i), the weight of w in the document will be calculated by the following formula:
The sectors weights L i could be identified by experts or determined by a learning approach using a training set of documents. In the implementation of our system, we used an empirical method to obtain these weights. A subset of 42 articles from the datasets used in the experiments and evaluation section was used to calculate the weights assigned to the document X  X  sectors. Only documents with at least ten sectors and provided along with author-assigned keywords were selected. They were first pre-processed to eliminate the stop words and extract the stems and roots. A word was considered a keyword if it shared the same root or stem with a manually-assigned keyword. The probability Pi of a word in sector i of being selected as a keyword is cal-culated at the level of each document. We found that the probability of a word from the title being a keyword is 0.51, that of a word in the first or last paragraph is 0.24, and that of a word in an internal paragraph is 0.11. Based on these results, all the internal paragraphs are assigned the same weight. The first and last paragraphs are assigned weighted scores that are two times that of an internal paragraph, and the title is given a weighted score that is four times that of an internal paragraph. These weights are then normalized to have a sum equal to one. Different surface forms of the same word may occur in the document, and groups of related words carrying similar meaning may differ only by their morphological pat-terns. Thus, term normalization is useful for grouping these words together according to their basic or canonic form.

Term normalization is the process of clustering words into groups or equivalence classes, where the derivative words generated from the same root and the nonderiva-tive words generated from the same stem are grouped together and their count is accumulated. The weight of an equivalence class C is then defined by
Each equivalence class is represented by the root or stem from which its words are generated and the most frequent of its words. This normalization process groups re-lated words that differ by their morphological patterns, thereby permitting such terms to reinforce each other in scoring and to reduce the number of redundant terms and concepts. Table II presents the most weighted classes, where each class is assigned its label (the most frequent of its terms) and its weight. An equivalence class represents a potential single-word keyword. However, most key-words consist of a noun phrase of two, three, or more words. Thus, there is a need to work on N-gram rather than on single words. In this article, a detailed description of scoring unigrams and bigrams will be given. The generalization to higher orders is straightforward.

Definition 1 . Two terms T 1 and T 2 co-occur if they appear consecutively in the same sentence after discarding the stop words and other meaningless words from the text.

Definition 2 . The terms X  equivalence classes C 1 and C 2 terms, T 1 from C 1 and T 2 from C 2 , such that T 1 and T
Definition 3 . Let L denote the number of term equivalence classes representing a document D. The term co-occurrence matrix Occ is a symmetric matrix of dimension L  X  L, where an element Occ(i,j) represents the number of co-occurrences of the class C i and class C j [Liu et al. 2009; Matsuo et al. 2004]. Figure 3 represents part of the co-occurrence matrix associated to the sample text used through this article.
Unigram Scoring. Rose et al. [2010] defined several metrics for scoring words based on their vertices X  degrees and frequencies in the co-occurrence graph of terms [Mihalcea and Tarau 2004]. These metrics are the word frequency and word degree. In this work, word frequency is replaced by the equivalence class weight given by Class-Weight. Word degree is replaced by the class degree calculated as the sum of the class weight and weights of all the possible co-occurrences of the class with other classes. Thus, the class degree favors terms occurring frequently in longer candidate keywords, and the class weight favors the frequent terms regardless their co-occurrence with other terms.

The score of a unigram (i.e., a one-equivalence class represented by its label as a one-word keyword candidate) has the same value of the weight, since a unigram has no components. Table III presents the scores for the most frequent unigrams that rep-resent the highest weighted equivalence classes of the sample text.

Bigram Scoring. The co-occurrence matrix gives the frequency of possible bigrams in the text (Figure 3). Therefore, a bigram is represented by the two base forms repre-senting classes C i and C j and its count, given by the element Occ(i,j) of the occurrence matrix. If two terms T 1 and T 2 co-occur in sector i of a text, they receive a weight equal to the weighted score L i assigned to this sector. The co-occurrences of terms T in the text are then counted and their weights accumulated, ignoring their order and grammatical information.

The score of bigram (T 1 ,T 2 ) is given by the sum of its weight and its members X  T T 2 weights. The degree of a bigram is defined as the sum of its weights and the weights of trigrams (the higher structure) containing this bigram. A sorted list of all bigrams according to their scores allows the extraction of the most common two-word keywords in the text. This list represents how often two different words appear together in the same sentences in the text. Table IV shows the metric scores for the most frequent bigrams of the sample text.

N-Gram Scoring. In this section, we generalize the basic keyword-scoring method presented earlier with the bigrams to cover variable keyword length (N-grams). We assume that an N-gram is a sequence of N words that appear consecutively in a sen-tence after discarding the meaningless tokens during the cleaning phase. The punctu-ation marks  X : X ,  X . X ,  X ! X ,  X : X ,  X   X   X ,  X  X ew Line X , and  X   X   X  are usually used to determine the boundaries of sentences in Arabic texts. The definitions that follow are introduced to generalize the keyword detection and scoring for N-grams.

Definition 4 . If the number of occurrences of an N-gram NG(T sector i of a document of M sectors is given by Freq(NG,i), L to sector i, term T j belongs to the equivalence class C j document is given by
Definition 5 . The score of an N-gram NG(T 1 ,T 2 ,...,T N T j belongs to equivalence class Cj, is calculated by the following formula:
Definition 6 . The degree of an N-gram NG in a document is defined as the sum of its weights and weights of all (N + 1)-grams (the higher structure) that include NG.
Tables V and VI show the results of the most frequent trigrams and quadgrams of the sample text. The representation of a document as a vector that captures the relative importance of its terms is known as the vector space model [Manning et al. 2009; Salon et al. 1975]. This model is fundamental for applications such as information retrieval and text mining. For the purpose of this work, we redefine vector V representing a docu-ment to reflect the most frequent terms and N-grams produced by the previous steps.
We denote by V(d, N) the vector derived from a document d with one component for each n-gram, 1  X  n  X  N, detected with significant weight. Each component stores an n-gram and its score. Vector V(d, N) is then sorted according to the descending order of the n-grams X  scores. Figure 4 presents the elements of V(d, N) that received a score greater than 0.50 considered as potential candidate keywords. Vector V(d,N) details the score for each detected N-gram in the document, and the highest scores determine the potential candidate keywords. The size of the keyword list depends mainly on the application particularity, document size, and user decisions. The selection of keywords is carried out according to the following rules. (1) The user specifies the desired number of keywords as well as the maximum number (2) Priority is given to the higher scores, and if two combinations have the same score, (3) If two candidate keywords have the same number of terms and the same score, (4) If an N-gram is selected, its possible internal components (terms) are removed from The selected keywords are represented by the roots/stems labeling their corresponding equivalence classes. However, these keywords rarely occur as roots or stems in the text; they always appear with suffixes and/or formed according to morphological patterns. Thus, the representation of each keyword needs to be changed to a readable form.
The final form of a keyword is obtained according the following rules. (1) If a unigram keyword is a root, it will be replaced by the noun generated accord-(2) If a unigram keyword is a stem (representing a nonderivative word), it will be (3) For keyphrases (2, 3, or more terms), each term is replaced by a word according to (4) Some function words may be added between the components of keyphrases because
Thus, the final list of keywords is reshaped by parsing the document to find the most common order of terms for each one of the selected N-grams. If two terms in a selected keyword occur frequently in the text separated by the same function word, this function word will be inserted to create the final keyword. Table VII shows the keywords extracted from the Arabic version of the Universal Declaration of Human Rights. Keyword extraction algorithms are usually evaluated and their performances mea-sured by comparing extracted keywords with manually-assigned keywords [Mihalcea and Tarau 2004; Rose et al. 2010; Turney 1999]. Therefore, there is a need to have a set of documents with predefined keywords to test and evaluate the proposed method. Due to the lack of benchmark corpus for evaluating Arabic keyword extraction, the experiments in this article use three datasets of 196 documents that we collected from different sources. The first dataset consists of 70 journal articles selected from six jour-nals. Each article has been assigned a list of keywords. The second dataset includes 36 Arabic abstracts for English language papers published by several journals. These abstracts also have associated keywords. The third dataset consists of 90 texts col-lected from Arabic Wikipedia and Aljazeera.net. These documents do not have pre-defined keywords. They were collected to overcome the limited number of available Arabic documents with keywords. A group of professional indexers was asked to man-ually assign a list of keywords to each of the third dataset texts. Table VIII presents some basic statistics for the datasets.

The primary analysis of these datasets shows that only 73% of the human-generated keywords appear in the document texts. Thus, any keyword extraction algorithm based on the frequency of terms in documents could generate keywords that match up to 73% of the manually-assigned keywords. To evaluate the performance and accuracy of the proposed method, the standard in-formation retrieval metrics of precision P, recall R, and F-measure F are used. We consider that a detected keyword W is a true positive detection if it verifies at least one of the following cases. (1) W is one of the manually-assigned keywords. (2) W is part of a manually-assigned keyword, which is a keyphrase in this case. (3) A word generated from the same stem or root of W belongs to the list of manually-(4) W is an N-gram keyword with at least one term that belongs to the human-
To evaluate the proposed method, three groups of experiments were conducted on the three datasets. In the first group, the numbers of extracted keywords were set to 5, 10, and 15. In the second group, the number of extracted keywords is set to the same number of keywords assigned manually to the documents so that the number of false-positive detections and false negative detections would be equal and the three measures P, R, and F would be identical. The third group of experiments aimed to compare the results of the proposed system to those obtained by other keyword extrac-tion systems.
 Table IX and Table X present the results for the two first groups of experiments. Good results were achieved with an average precision of 31% and average recall of 53% of recall. The best precision (0.39) was obtained when the number of keywords is set to 5 or equal to the number of manually-assigned keywords. The best recall (0.66) was achieved when the number of keywords was increased to 15. The results for the first experiment are always lower for dataset 2 (the abstracts) than for the other two datasets, indicating that longer texts will best relate the frequency of a word with its relevance to be selected as keyword. As expected, the number of correct keywords extracted by our system increased as the number of extracted keywords increased, with an insignificant increase after a value related mainly to the document size. The second experimental results showed better precision (and recall) at 0.45. These good results were obtained for all the datasets when the number of extracted keywords was set to the exact number of manually-assigned keywords.

The comparison of the performance of the proposed system against other keyword extraction systems was a challenging task due to the unavailability of a benchmark corpus to evaluate Arabic keyword extraction systems and the rarity of available systems and resources. Only two Arabic keyword extraction systems were found by the author, namely, the Sakhr Keyword Extractor and KP-Miner [El-Beltagy and Rafea 2008]. The Sakhr Keyword Extractor is a commercial product provided as a component of a text mining system. It favors name entities already stored in the sys-tem, regardless of their positions and relative weights in the text. In addition, there is insufficient information about its structure and internal engine. On the other hand, KP-Miner is a keyphrase extraction system for English and Arabic documents. It is the only Arabic-based keyword extractor that the author found available and documented. The third group of experiments compared the keywords extracted by our systems to those extracted by KP-Miner for the first dataset. Table XI shows the main findings of this experiment. The results show that the proposed system outperformed KP-Miner in terms of precision and recall.
 The results of the three experiments show improvement in terms of precision and recall. The empirical results indicate that keywords are extracted with an average precision of 0.31 and an average recall of 0.53. These results may be considered as good as those obtained by the well-known keyword extraction systems developed for English. Compared with those of Arabic adaptation of the English-based KP-Miner keyword extraction system, the experiment results are better, with a 0.43 and 0.50 improvement in precision and recall, respectively.

These improved results can be explained by the integration of linguistic information and language peculiarities in both the preprocessing and keyword extraction phases. The proposed system performs an in-depth linguistic analysis of Arabic texts, in which each word is replaced by its root along with its morphological pattern. Furthermore, the system groups the words generated from the same root in equivalence classes, so the total number of entries is considerably reduced. As a result, the sparse nature of the Arabic text is reduced, and the number of candidates for keywords is limited. On the contrary, the KP-Miner system has a limited level of linguistic analysis that is restricted to considering word stems. Therefore, the scores assigned by KP-Miner to words generated from the same root appearing in the text with different stems will be scattered, and their weights will be weakened. For example, the words school  X   X  X  X  X  X  X  X  : Al-madrasa -h X  and schools  X   X  X  X  X  X  X  X  : Al-madaris, X  which have two different stems but the same root, are considered two different keywords in existing keyword extraction systems and one single keyword by the proposed system. Therefore, they will have a greater chance of being selected when their frequencies in the text are accumulated. In addition, as Arabic has a relatively free word order, the selection and scoring of N-grams are developed ignoring the order of words, although in the selection of the final keyword representing the N-gram class, the most frequent order of words is retained.

Analysis of the errors shows that they arise from different sources. The main source of false negative errors is that only 73% of the manually-assigned keywords are se-lected from the text, and the rest of the keywords represent general terms or text topics. Out of 360 assigned keywords to the documents of the first dataset, 98 are not found in the text body of these documents. This type of error is general to all lan-guages and can never be eliminated. However, it can be reduced by providing rules and proposing a system for classifying documents. The second main source of error is found when different related words (synonyms) generated from different roots repre-sent the same concept or idea. For example, in a document of the first dataset, the keyword  X   X  X  X  X  X   X  X  X  X  X  X   X   X  which means  X  X ood Governance X  had not been detected by the system. Two synonyms generated from two different roots  X  sent in English the verbs  X  X overn X  and  X  X ead, X  respectively, were used in the concerned document to represent this concept. The other errors are mainly due to spelling and grammatical errors.
 Automatic keyword extraction from Arabic texts is still a new area of research. This area requires more efforts due to the significant increase in the size of online Ara-bic content and the rarity of manually-assigned keywords to available electronic documents.

In this work, we have investigated the applicability of linguistic and statistical key-word extraction methods on Arabic documents. The suggested method works on in-dividual documents from different domains that do not follow any specific structure or grammar conventions; therefore, the method is a general-purpose and domain-independent technique. The method uses available natural language processing tech-niques to extract useful linguistics information, improving the performance of the statistical approach of keywords extraction.

Experiments carried out in this work show that the proposed method achieves good results with an average precision of 31% and average recall of 53% when selecting a reasonable number of keywords. These results have shown that incorporating lan-guage knowledge is valuable for improving the extraction of keywords from text, mak-ing this approach suitable for real-world text, which is increasingly available on-line.
