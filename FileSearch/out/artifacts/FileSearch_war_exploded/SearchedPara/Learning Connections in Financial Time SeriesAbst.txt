 Gartheeban Ganeshapillai garthee@mit.edu John Guttag guttag@mit.edu Andrew W. Lo alo@mit.edu It is widely accepted that in designing a portfolio of equities there is a tradeoff to be made between risk and return. The root cause of the tradeoff is that more volatile equities typically have higher returns. Much of modern financial theory is based upon the assumption that a portfolio containing a diversified set of equities can be used to control risk while achieving a good rate of return. The basic idea is to choose equities that have high expected returns, but are likely to move down in tandem.
 Different investors have different goals. Often, in-vestors begin by choosing a minimum desired expected return as the independent variable. They then formu-late portfolio design as an optimization problem with return as a constraint and variance minimization as the objective function. Central to this optimization is the construction of a covariance matrix for the daily returns of the equities in the portfolio.
 A problem with this approach is that the covariance matrix uses correlation, which gives equal weight to positive and negative returns and to small and large returns. This is inappropriate in a world in which risk preference plays an increasingly important role. Some investors, for example, hedge fund managers, expect high returns, and in exchange, expect to bear corre-sponding risks. For such investors, it is critical to con-trol for tail risk , the risk of an improbable but poten-tially catastrophic negative return (Bae, 2003; Forbes &amp; Rigobon, 2002). Hence, selective as opposed to full-cover hedging is gaining popularity (Stulz, 2005). Learning the connectedness between equities in terms of large losses and exploiting this knowledge in port-folio construction is the topic of this paper. We refer to these large losses as events .
 We formulate the learning problem as given a set of eq-uities A , some of which had events and some of which didn X  X , which equities in a disjoint set B , are mostly to experience an event on the same day. It may seem that this is useless, because by the time we have the returns for equities in A , we would already know the returns for equities in B . However, the goal of this phase is not to learn to predict events, but to learn historical relationships among equities. This learned relationship will then be used to construct a portfolio containing assets that are less likely to have correlated events in the future.
 We apply our method to the daily returns for all 369 companies listed in S&amp;P500 as of Jan 1, 2012 that were publicly traded from Jan 1, 2000 through December 31, 2011.
 We use a factor model to describe the daily return of each equity in terms of the equity X  X  active return, market sensitivity, and the daily returns of other eq-uities in the sector (e.g., financial or energy) to which that equity belongs. We then train a regression model on the historical data using regularized least squares and estimate the parameters using gradient descent. In contrast to methods that quantify the connected-ness between equities using pairwise relationships, our method accounts for interactions with all other equi-ties in the portfolio. Since extreme events are rare, we use all of the historical data rather than just events. We use a cost function that differentially weights re-turns of different magnitudes.
 In our model, we update the weights daily and predict the returns for the following day. We rank the equities in B using the predicted returns. We compare this list against the true occurrences of events using mean average precision (MAP) scores. Using this metric, our approach consistently outperforms the most pop-ular techniques in the financial literature, e.g., t-copula (Nelsen, 2006).
 Our experiments provide strong evidence that by ex-ploiting these learned relationships we can build port-folios that outperform portfolios constructed using techniques drawn from the literature. The compari-son is done using minimum daily return, total cumu-lative return, maximum drawdown, and the Sharpe ratio (Sharpe, 1994).
 We make the following contributions in this paper.  X  A method of modeling returns using three factors:  X  An alternative to the usual approach of using  X  Formulating the problem of estimating one return We begin by discussing general methods that have been used to study correlations among returns. We then move on to discuss work specific to understand-ing extreme returns. 2.1. Correlation and Partial Correlation If one knows the correlation of equity e with all other equities, one can estimate the expected return of e as a weighted average over known returns of other equities. Correlation measures give equal weight to small and large returns, and therefore the differential impact of large returns may be hidden. Since the absolute val-ues of returns increase during volatile periods, uncon-ditional correlation values also rise even when the con-nectedness between two equities may remain the same (Longin &amp; Solnik, 1999). To address this, researchers have proposed conditional correlations to focus on cer-tain segments (St  X aric  X a, 1999).
 However, it has been shown that conditional correla-tion of multivariate normal returns will always be less than the true correlation. This effect also exists when a GARCH model generates the returns (Longin &amp; Sol-nik, 1999).
 Longin &amp; Solnik (1999) provides a formal statistical method, based on extreme value theory, to model the correlation of large returns. First, they model the tails of marginal distributions using generalized Pareto dis-tribution (GPD)(Castillo &amp; Hadi, 1997). Then, they learn the dependence structure between two univari-ate distributions of extreme values. Semi-parametric models have since been proposed to address the inflex-ibilities of such parametric models (Boldi &amp; Davison, 2007). A downside of these methods is that the link-age is learned between two time series independently of the rest.
 Partial correlation measures the degree of association between two time series while discounting the influ-ence of others. It is calculated by fitting a regression model for each of these two time series on the rest. The correlation between the residuals of these regres-sion models gives the partial correlation (Kendall &amp; Stuart, 1973). But, partial correlation doesn X  X  distin-guish extreme values. 2.2. Understanding Extreme Returns Correlation between stocks has traditionally been used when measuring co-movements of prices, and discov-ering contagion in financial markets (Richards, 1995; Bae, 2003). Researchers have also used partial cor-relations to build correlation-based networks. These networks are then used to identify the dominant stocks that drive the correlations present among stocks (Kenett et al., 2010).
 Bae (2003) distinguishes extreme returns in establish-ing the linkages between financial time series. They capture the transmission of financial shocks to answer questions such as how likely is it that two Latin Ameri-can countries will have extreme returns on a day given that two countries in Asia have extreme returns on that or the preceding day. There has been extensive re-search on multivariate extreme values (Coles &amp; Tawn, 1991; Pekasiewicz, 2007). Chen &amp; Chihying (2007) provides a method to model the temporal sequence associations for rare events. Arnold et al. (2007) ex-amines a host of algorithms that, loosely speaking, fall under the category of graphical Granger methods to quantify the connectedness in time series. If the closing prices of the equity j on day T and T  X  1 are p T,j and p T  X  1 ,j , the return for equity j on day T is we are given historical daily returns for m equities in a T  X  m matrix R = { r t,j } ; 1  X  t  X  T, 1  X  j  X  m . We use indexing t for days, and j,k for equities. When r t,j &lt;  X  0 . 1 (10% loss), we say that equity j had an event on day t .
 We assume that daily returns (rows of R ) are indepen-dent. While daily returns are generally believed to be heteroskedastic (White, 1980), since we focus on large returns that are rare, we can safely assume that the modeling errors are uncorrelated. We use regulariza-tion to tackle over fitting. The regularization parame-ter  X  is determined by cross validation.
 Factor model representation of returns is common in fi-nance and econometrics (Longin &amp; Solnik, 1999; Khan-dani &amp; Lo, 2007).
 We model the return of equity k on day t by In this model, we explicitly learn the factors for equity k : equity X  X  active return a k , equity X  X  sensitivity to the market b k , and equity X  X  connectedness with other equi-ties w j,k , where 1  X  j  X  m ; j 6 = k . The S&amp;P500 index return ( r t,  X  ) averages the returns of all the equities on a given day.
 Our focus is on negative returns, which are not nor-mally distributed. Therefore, we apply the Box-Cox transformation to make the daily return samples more normal distribution-like (Box &amp; Cox, 1964). We use least squares minimization to estimate the weights. We find that better performance is achieved, when we capture the differential impact of certain val-ues by weighting with a cost function f ( x ). The flexibility in choosing the cost functions allows us to optimize different aspects of the problem. For top-k ranking evaluation, we define events at or below  X  10% daily return. We use f ( x ) = e  X  ( x  X  r  X  ) 2 ; r  X  =  X  0 . 1 to achieve higher accuracy because the maximum ambi-guity is at the boundary. For portfolio construction problems, we consider all negative returns, and use a different cost function We can efficiently compute model parameters (  X  = { ( a k ,b k ,w j,k | 1  X  k  X  m, 1  X  j  X  m,j 6 = k } ) by es-timating the inner products. However, estimating the weights directly on the observed data is prone to over-fitting (Bell &amp; Koren, 2007). Therefore, we learn the parameters by solving the following regularized least squares problem: min We use gradient descent to minimize the regularized square errors. For each r t,k  X  R , we update the pa-rameters by: Here,  X  is the learning rate that is dynamically ad-justed using line search, and e t,k def = f ( r t,k )( r t,k We use the last 500 days in the historical data to train our model. We iterate 100 times for the initial esti-mate of the parameters. The model is updated daily to make predictions for the next day. Since this new training set differs from the previous for only two days, convergence is achieved within a few iterations. 3.1. Connectedness Matrix In portfolio construction, connectedness between equi-ties is used to find less correlated assets. Generally, a covariance matrix C is used to find the optimal diversi-fication, i.e., minimum variance portfolio. In contrast, our model uses the connectedness matrix G , which is learned using the factor model. We assume that the portfolio is built with m equities that belong to a sec-tor and the S&amp;P500 index (SPX).
 We build the connectedness matrix G  X &lt; ( m +1)  X  ( m +1) from the interpolation weights of the factor model, and demonstrate that it provides better diversification. A direct construction is given in Table 1. Here,  X  2 j is the variance of the daily returns of the equity j .  X  denotes SPX.
 Such a matrix should be positive semi-definite to be used in portfolio optimization involving quadratic programming. Since any positive semi-definite ma-trix G can be decomposed into PP T , where P  X   X  r min We begin the training by initializing P to  X  P , where  X  is the Cholesky factorization of the covariance matrix C , i.e., C =  X  P  X  P T . We compute the covariance matrix C on historical data. For each r t,k  X  R , we update P and a k by moving against the gradient. For 1  X  j  X  m ; j 6 = k , and 1  X  v  X  m , the updates are: 3.2. Discussion In our model (Equation 1), we represent the relation-ship between the returns of equities after discounting their interactions with the market. Thus, interpolation weights w j,k resemble partial correlation estimates. In the factor model, we simultaneously fit the regression model and learn the correlation weights. Further, reg-ularization is employed in our model to reduce the like-lihood of spurious estimates.
 For a matrix X , a column vector Y , and a regression problem expressed as Xw = Y , an explicit solution, denoted by  X  w is given by:  X  w = ( X T X )  X  1 X T Y . If the variables are mean adjusted,  X  is the covariance of X , and C is the covariance between Y and each column of X , it can be rewritten as,  X  w =  X   X  1 C . In Equation 1, we can observe the similarity between these variables ( X and Y ) and adjusted returns, i.e., y t  X  ( r t,k  X  d and x t,j  X  ( r t,j  X  d t,j ). The connectedness matrix G , built from the interpolation weights, models the pair-wise connection between the adjusted returns of two equities while discounting the connectedness among all other equities. Though univariate time series of daily equity returns lack both significant autocorrelation and stationarity, multivariate time series of returns exhibit consistent correlation that persist over time. This persistent cor-relation is what makes portfolio diversification possible (Borodin et al., 2004; Kalai &amp; Vempala, 2000; Kenett et al., 2010). 4.1. Data We use daily return data from CRSP 1 . We examine all 369 companies that were in the S&amp;P500 from 2000 to 2011. This time period contains two major finan-cial crises (2001 and 2008). The set of companies are from ten sectors: consumer discretionary, consumer staples, energy, financials, health care, industrials, in-formation technology, materials, telecommunications services, and utilities. 4.2. Top-K Ranking Given all returns for days 1 to T and returns on day T + 1 for equities in A , we predict which equities from B will have events (losses greater than 10%) on that day. We produce an ordered list of equities from B, ranked by their likelihoods of having events on day T + 1 based on their predicted returns  X  r T +1 . 4.2.1. Evaluation We use mean average precision (MAP) to evaluate the correctness of the ordered list. Average precision (AP) is a popular measure that is used to evaluate an or-dered list while taking into account both recall and precision (Zhu, 2004). MAP is the mean of average precision across the test set. For an ordered list of top-k items, MAP and AP are given by: Here, n is the size of the test set, p ( j ) is the precision at cut-off j , and  X  r ( j ) is the change in the recall from j  X  1 to j . We produce a top-10 list, and evaluate with MAP (10). 4.2.2. Experimental Results Since diversification inevitably involves using equities from multiple sectors, we focused on the question of which equities to own within sectors. We randomly select 20% of the companies in each sector for set A , and the rest for set B . We ran our experiments from 2001 to 2011, so that at the start of the experiment we will have at least a year of historical data to train on. We evaluated our methods only on days that had at least two events. Three sectors had less than 5 such days in the last decade, and therefore were excluded in the experiments. Across all the sectors, there were 539 days that had two events out of 3019 days in the full dataset.
 Table 2 compares the MAP scores (higher is better) for our factor model (FAC) with the scores for benchmark methods: correlation (CR), correlation of extreme val-ues (EVCR), and partial correlation (PCR). The re-sults are averaged over 100 runs. The best result for each sector is in bold face. Results for FAC are statis-tically different (p-value &lt; 0 . 001) from the results of every other method under paired t-test.
 For CR and PCR we use standard implementations. For EVCR, we apply a GARCH model to remove se-rial dependencies, if there are any. Then, we fit a uni-variate generalized Pareto distribution with tail frac-tion 0 . 1 on the innovations of the GARCH model. Fi-nally, we model the multivariate distribution with a t-Copula, and learn the linear correlations on extreme values (Cherubini et al., 2004).
 Our factor model consistently outperforms the other methods. EVCR often underperforms PCR, and at times, CR. We conjecture that the inflexibility of the parametric modeling of the tails and not consider-ing the relationship between non-extreme values con-tribute to this failure. The poor performance of EVCR is striking because t-copula is widely used in financial risk assessment, especially in the pricing of collateral-ized debt obligations (CDO) (Meneguzzo &amp; Vecchiato, 2004; MacKenzie, 2008).
 Table 3 compares the MAP scores for different sizes of known and unknown sets. We change the size of B as a fraction of the total number of companies available, using 10% , 20%, and 40%. Set A contains the rest. Even when we train on only 60% of the companies and test on 40%, our method remains effective. Notice that FAC trained with only 60% of the data outperforms other methods trained with 80% of the data.
 As an example, we look at an S&amp;P500 constituent, Bank of America (BAC). Between 2001 and 2011, BAC had 29 events, i.e., daily losses of at least 10%. Fig-ure 1 shows the parameters learned using the factor model for BAC. Notice that the market dependence drastically increases during the 2008 crisis. Further, the  X  X erding effect, X  as given by the spread in the weights, widens during the crises of 2001 and 2008/9. BAC becomes heavily connected to smaller number of other equities.
 4.3. Portfolio Construction The major application of our method is the reduction of large losses in equity portfolios. Since there is a tradeoff between risk and expected return, portfolio design usually starts by the investor choosing a de-sired level of expected return (or a risk tolerance). For a given desired expected return r e , in the absence of any side information, the minimum variance portfo-lio (MVP) is the optimal portfolio (Markowitz, 1959). For the MVP, portfolio weights  X  are derived by solv-ing the optimization problem: subject to Here, C is the covariance matrix of returns, and  X  r is the expected return of equity j . Typically, the co-variance and the expected return are calculated from historical data. Here, we assume fixed capital (no leverage), no short positions, daily readjusted portfolio weights, and we ignore the costs of transactions. We demonstrate our method X  X  utility by building port-folios with our connectedness matrix G (Section 3.1), and compare their performance to portfolios built us-ing methods drawn from the financial literature.  X  Our baseline is an MVP portfolio built using the  X  For the factor model, we replace C with connect-When a portfolio is heavily diversified, the expected return is smaller. Therefore, in our formulation the desired expected return r e governs the amount of di-versification. The range of achievable values for r e is the minimum and maximum of the expected returns of the equities. Maximum expected return is achieved by owning the equity with the highest historical re-turn. Minimum risk relative to the market is achieved by owning everything in that market.
 It has been shown that 90% of the maximum benefit of diversification is achieved with portfolios contain-ing roughly 5% of the market constituents (Reilly &amp; Brown, 2011). This led us to set r e to the 95 th per-centile of the expected returns of the equities. This setting causes the optimization to choose about 3  X  5 equities per sector (5% to 10%).
 Table 4 summarizes the return characteristics for the three sectors with the most events. We re-weight the portfolio daily, and estimate the returns daily. Cu-mulative return R T from day 1 to day T is given by R T = Q the overall return from year 2001 to 2011, i.e., R on December 31, 2011. Maximum drawdown is the largest drop from the maximum cumulative return. The Sharpe ratio measures the excess return for ad-ditional risks taken (Sharpe, 1994). It is given by S = E ( r  X  r  X  ) / p var ( r  X  r  X  ), where r is the daily return of the portfolio and r  X  is the reference return (return on the S&amp;P500 index). A positive Sharpe ra-tio implies that excess return is greater than the addi-tional risk taken. The expected shortfall (also known as CVaR) at 5% level gives the expected return on the portfolio in the worst 5% of the cases. Table 4 shows that by learning the connectedness between equities, our portfolios cannot only beat the market (positive Sharpe Ratio), but also beat the optimal (minimum-variance) portfolios.
 Figure 2 shows the impact of our method on returns in the energy sector. Until the 2008 crisis, because the energy sector remained calm, our FAC model per-formed comparably to COV. Note that 2001 crisis, un-like 2008 crisis, was limited to few sectors not including energy. After the collapse in May 2008, our model be-gan learning new connectivities related to large nega-tive returns and was able to reduce large losses (Figure 2(a)) late that year and going forward. It took about 200 days to learn the new model, but it persisted long enough to be useful. Figure 2(b) demonstrates the effectiveness of our method in making the large nega-tive returns smaller without significantly affecting pos-itive and small negative returns. The largest daily loss dropped 30%, i.e., from 23% to 16%.
 Figure 3 shows the equity weights learned using the connectedness matrix for the financials sector. Until August 2008, our factor model based portfolio consis-tently focused on two equities in the financial sector: BlackRock (ID 19), Inc. and Ventas, Inc. (ID 55). In the aftermath of the 2008 crisis, our model increases the diversification.
 Finally, in Table 5 we demonstrate the effectiveness of our model in constructing a market wide portfo-lio. We build a market wide portfolio by combining the portfolios built for each sector weighted equally. We compare our method with COV, PCR and EVCR.
 We also compare these  X  X VP-like X  portfolios with other benchmark portfolios: a equi-weighted portfo-lio (EW), where the portfolio is rebalanced to equal weights daily, a Min-CVaR portfolio where the opti-mization minimizes conditional variance at 5% level (Rockafellar &amp; Uryasev, 2000), and the S&amp;P500 in-dex (SPX). FAC 2 achieves annualized sharpe ratio of 0 . 21  X  sure, the portfolio built with our method outperforms the  X  X ard-to-beat X  equal-weighted portfolio (Plyakha et al., 2012). 4.4. Limitations Our portfolios achieve returns several times greater than minimum-variance portfolios. Like minimum-variance portfolios, our method rebalances the portfo-lio daily, and therefore incurs transaction costs. These costs are ignored here, therefore absolute returns are overstated for both methods. COV leads to slightly more transactions than our method, and would there-fore incur higher transaction costs.
 Relationships among equities can be viewed as oc-curring on three time scales: permanent (e.g., sec-tor grouping), long-term (e.g., based on fundamentals and technicals), and short-term (e.g., based on real-time news and announcements). In this work, we cap-ture only the long-term relationships. This may result in a slower response when market conditions change rapidly as in 2008 (Figure 2). We intend to address this in future work by attempting to incorporate news and sentimental analysis into our model.
 We show that by limiting the large losses, risk man-agement can be a source of excess returns. During the 2008 crisis, all equities became heavily correlated as the market crashed, and market risk governed returns (Figure 1). Without short positions (or derivatives that simulate short positions), this kind of risk cannot be diversified away. Our current portfolio construc-tion method (Equation 7) does not permit negative weights. We are currently working on extending our method to cover more kinds of portfolios. We presented a method for learning connections be-tween financial time series. We modeled daily returns using three factors: active return, market sensitivity, and connectedness of returns. We learned these fac-tors using a recursive regression. We solved the re-gression problem using an unconstrained least squares optimization that ensures that the resulting matrix is positive semi-definite so that it can be used in portfolio construction.
 We evaluated our method in two ways. First, we evalu-ated its accuracy in producing a list of equities ordered by their likelihoods of having large losses, given infor-mation about the behavior of other equities. We then presented and demonstrated the potential real world utility of a method that constructs portfolios using the learned relationships. The performance of portfolios constructed using our methods were compared to the performance of portfolios constructed using conven-tional approaches, including traditional correlation-matrix based methods. Portfolios constructed using our method not only  X  X eat the market, X  but also beat the so-called  X  X ptimal portfolios. X  We thank the anonymous reviewers for their helpful comments. This work was supported by Quanta Com-puters Inc.

