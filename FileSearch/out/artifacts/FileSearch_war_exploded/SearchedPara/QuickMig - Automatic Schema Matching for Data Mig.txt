 A common task in many database applications is the mi-gration of legacy data from multiple sources into a new one. This requires identifying semantically related elements of the source and target systems and the creation of mapping expressions to transform instances of those elements from the source format to the targ et format. Currently, data migration is typically done manually, a tedious and time-consuming process, which is difficult to scale to a high num-ber of data sources. In this paper, we describe QuickMig, a new semi-automatic approach to determining semantic cor-respondences between schema elements for data migration applications. QuickMig advances the state of the art with a set of new techniques exploiting sample instances, domain ontologies, and reuse of existing mappings to detect not only element correspondences but also their mapping expressions. QuickMig further includes new mechanisms to effectively incorporate domain knowledge of users into the matching process. The results from a comprehensive evaluation using real-world schemas and data indicate the high quality and practicability of the overall approach.
 H.2.5 [ Database Management ]: Heterogeneous Databases Algorithms, Experimentation Schema Matching, Mapping Discovery, Schema Mapping, Data Transformation, Data Migration Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. Figure 1: Running example showing a mapping of customer data
Data migration is the task of transforming and integrating data originating from one or multiple legacy applications or databases into a new one. Whenever a new software applica-tion is introduced to replace existing legacy applications or whenever the application landscape is consolidated the re-quirement to migrate data between applications arises. Dur-ing the migration process, data needs to be extracted from the source systems, transformed and loaded into the target system. This process requires solving two difficult tasks: i) schema matching to identify similar or semantically related elements between the source and target systems and ii) map-ping discovery to determine mapping expressions capable of transforming instance data from the source format to the target format.

As an example, consider the schemas depicted in Fig. 1. The DEBMAS schema describes the data format used by the legacy system to represent a customer, whereas the Customer schema describes the data format used by the target system. The values in brackets show example ele-ment values for one customer instance. Furthermore, the dotted lines indicate corresponding elements in the two schemas. For example, the ANRED element in the source schema corresponds to the FormOfAddressCode element in the target schema as they contain the same semantic in-formation namely the form of address. However, detecting this correspondence and determining a mapping expression to transform values of the ANRED element to those of the FormOfAddressCode element remains a challenge as the for-mer uses a textual representation whereas the latter utilizes specific codes. Another example is the complex correspon-dence between the source element STRAS and the target el-ements StreetName and HouseID , in which a value of the former needs to be split in order to form valid entries of the latter.

As surveyed in [7, 19], previous research work mostly fo-cuses on the schema matching task to semi-automatically identify semantic correspondences between schema elements. To determine the similarity, i.e. degree of relatedness, be-tween schema elements, proposed techniques exploit vari-ous kinds of information, including schema characteristics such as element names, data types and structural proper-ties, characteristics of data instances, as well as background knowledge from dictionaries and thesauri. However, only a few approaches, e.g. [12, 21, 6], have addressed the task of semi-automatic discovery of mapping expressions. They typically use the available data instances in data sources and apply sophisticated machine learning techniques to de-termine mapping expressions between the matching schema elements.

In this paper, we propose a new and integrated approach for schema matching and mapping discovery to support mi-gration and transformation of data between heterogeneous sources. Compared to previous work, our approach exhibits the following improvements:
Novel use of sample instances: Instead of exploit-ing unrelated instances between source and target system, we propose to define and use a uniform set of standard in-stances. The availability of such instances in the source and target system makes it possible to use much more efficient methods to detect matching elements.

New instance-based matchers: Exploiting the avail-ability of the same instances in the source and the target sys-tem, we developed a set of relatively simple instance-based matchers, which however are able to detect complex corre-spondences and mapping expressions in real-world schemas.
Comprehensive set of mapping categories: To cap-ture such mapping expressions for data transformation, we have developed a comprehensive set of mapping categories, indicating how instance data has to be transformed from the source to the target format. The majority of the categories can be automatically detected by our new matchers.
Enhanced mapping reuse: Based on the mapping reuse idea of [7, 9], we have developed a new reuse matcher that is able to derive new mappings with mapping expressions from existing ones. The reuse matcher can be easily com-bined with the other matchers for improved match results.
Schema reduction based on domain knowledge: To deal with large and complex schemas, we developed a new questionnaire technique for schema reduction. Specified in domain nomenclature, the questionnaire allows the user to specify portions of the schemas relevant for data migration according to his understanding of the domain.

Real-world evaluation: We have implemented our novel concepts in a prototype and performed a comprehensive evaluation with large schemas taken from real SAP busi-ness applications. The evaluation results indicate the high quality and practicability of our approach for real-world sce-narios.

The remainder of the paper is organized as follows. Sec-tion 2 briefly describes existing research related to our ap-proach. Next, Section 3 presents an overview of the Quick-Mig approach. After this, sections 4 and 5 further detail important aspects of the approach before an evaluation is presented in Section 6. The paper ends by providing a sum-mary and an outlook on future work in Section 7.
The work most closely related to ours is the recent schema matching tool COMA++ [7, 9]. COMA++ supports a large spectrum of schema-and reuse-based matchers and employs composition to combine the results of individually executed matchers. The result of a match operation in COMA++ is a structural mapping consisting of correspondences with a similarity value to indicate their plausibility. To deal with large schemas, COMA++ supports a fragment-based match approach, in which the input schemas are reduced to rele-vant schema fragments either chosen manually or identified automatically according to fragment similarity.

QuickMig is a further development of COMA++ and im-proves it in several aspects. First, it supports schemas with instances and includes a number of new instance-based match-ers which can be combined with the existing matchers in COMA++. Second, it provides new techniques exploiting sample instances and domain-specific knowledge to detect mapping expressions for match correspondences and com-plex matches, such as string splitting and concatenation. Third, the mapping reuse approach of COMA++ has been enhanced to support not only mappings with similarity val-ues, but also mappings with mapping expressions. Fourth, a new technique based on questionnaires in natural language is employed for schema reduction. The user can choose rel-evant fragments for matching according to his understand-ing of the business domain, and not based on the technical knowledge of the schemas as expected by the manual frag-ment selection in COMA++.

To determine corresponding schema elements, previous work typically exploits either schema information, such as element names, data types, and schema structure [1, 7, 9, 14, 15, 16, 18], or instance data [3, 2, 4, 6, 10, 11, 12, 13, 17]. So far, only a few approaches try to combine both schema-and instance-based techniques [6, 10, 21]. Inherit-ing the composite combination approach from COMA++, QuickMig is able to combine schema-based, instance-based and reuse matchers in a flexible way.

Previous instance-based approaches mostly rely on the availability of real instance data and apply sophisticated machine learning, statistical or information retrieval tech-niques to determine instance similarity of schema elements. However, the resulting quality of such approaches essentially depends on the quality of the available instance data. Our approach to provide and use sample instances makes it pos-sible to achieve high match quality independently from the availability and quality of real instance data.

Utilizing representative instances for schema matching was also proposed in the DUMAS prototype [4]. Unlike in Quick-Mig, such instances are determined using automatic dupli-cate identification, which may mislead the match operation with wrong duplicates. Furthermore, in contrast to DU-MAS, QuickMig is able to determine mapping expressions for match correspondences and to identify different kinds of complex matches, such as string splitting and concatenation.
To date, only a few matching approaches address the task of finding mapping expressions or at least try to return cor-respondences with a higher semantics. DIKE [18] can detect semantic relationships, like synonymy and hypernymy, be-tween schema elements. iMap [6] uses machine learning and [21] exploits a domain ontology to suggest complex matches, such as string concatenations and arithmetic operations. Starting from a set of correspondences provided by either the user or (semi-) automatic match, Clio [12] and HepTox [5] try to infer query mappings to query and transform in-stances of one schema to another one.

Unlike these prototypes, QuickMig employs several dis-tinct techniques to identify mapping expressions and com-plex matches. First, sample instance data makes it possible to identify string split and concatenation using simple string comparison. Second, sample instance data combined with domain knowledge, such as standard formats and structures for modeling date, time, address, phone, fax data, enables the detection of complex matches between these formats and structures. Finally, QuickMig is also able to derive mapping expressions for match correspondences by reusing existing mappings and the associated mapping expressions.
This section gives an overview of the QuickMig approach for schema matching and mapping discovery. We start by discussing the observations driving the design of our ap-proach (3.1) before presenting our migration process (3.2), the architecture (3.3) and the mapping categories (3.4).
The design of our approach to schema matching and map-ping discovery is driven by several observations which we made by analyzing practical data migration scenarios of var-ious SAP customers:
Limited value of schema information: We generally observed that legacy systems are mostly not optimized to-wards fostering system interoperability. The schemas typi-cally make extensive use of technical names, abbreviations, and proprietary structure (cf. the running example). This makes it difficult to determine correspondences, not to men-tion mapping expressions, between schema elements. As a consequence, additional kinds of information, especially in-stance data, domain knowledge, and previously determined mappings, need to be considered in order to achieve reason-able quality in the schema matching and mapping discovery process.

Availability of domain knowledge: The migration process needs knowledge about both the source and the tar-get system. Unfortunately, such knowledge is not always available at one place. The knowledge about the target sys-tem is available at its vendor, while only customers can pro-vide detailed knowledge about their source systems. As a consequence, a migration solution developed by a vendor of a target system, e.g. SAP, should exploit the knowledge about the target system, and at the same time support effective mechanisms to incorporate the customers X  knowledge about their source systems. In particular, the knowledge about the target system, e.g. field semantics, sample instances, data formats and code lists, can be specified in an ontology for automatic analysis. This manual effort is needed only once and can be quickly amortized over many migration projects.
Scope of data migration projects: Typically, the data sources involved in data migration projects are complex, re-sulting in large schemas to be matched. Depending on the need of the particular customer, only certain parts of the target system need to be populated with source data, which results in different target sch emas for different migration projects, even for the same target system. This potential for schema reduction needs to be exploited as much as pos-sible in order to reduce the complexity of the data migration tasks; Schema reduction can be performed with suitable in-volvement of the customer at the beginning of the migration process.

Accumulated mapping knowledge: In a given mi-gration project usually many different mapping tasks need to be solved, e.g. for customer data, supplier data, pur-chase orders, etc. As these schemas usually contain common parts, like address data, there is a high reuse potential for the results from previous mapping tasks, especially regard-ing complex mapping expressions which otherwise have to be developed manually. In our approach we consider this opportunity by the means of a dedicated matcher.
The problems and opportunities mentioned in the previ-ous paragraphs led to the migration process depicted in Fig. 2. 1. Answering a Questionnaire. This first step is man-ually performed by a person with some knowledge of the capabilities of the source system. The purpose of the ques-tionnaire is to collect as much information about the source system as possible. This information will be used to auto-matically reduce the complexity of the target schemas and thereby reduce the complexity of the matching process. A detailed discussion of the schema reduction will be given in Section 4.1. 2. Injection of Sample Instances. In the second step instances already existing in the target system are manually created in the source system by a user. These sample in-stances are used by the instance-based matching algorithms (see also Section 5.1) in order to determine correspondences between the source and the target schemas. A detailed de-scription of the sample data creation in the source system is given in Section 4.2. 3. Schema and Instance Import. The third step in the migration process is the import of the source schemas as well as the corresponding sample instances into the Quick-Mig system. 4. Matcher Execution. In the fourth step the schema matching algorithms will be executed and automatically de-termine a mapping proposal using different matching algo-rithms. This mapping proposal includes similarities between elements of the source and target schemas as well as a pro-posal for the mapping categories . 5. Review. A developer reviews and corrects the map-ping proposal in the final step. When the mapping proposal is accepted real mapping code is generated and the mapping is stored in a mapping repository for later execution or reuse.
The QuickMig system is based on COMA++ [7, 9]. An overview of the architecture of the QuickMig system is given in Fig. 3. QuickMig extends COMA++ by implementing three new instance-based matching algorithms, namely the Equality, the Split-Concat and the Ontology-based matcher, and by improving the reuse matcher. A detailed description of these algorithms can be found in Section 5. Each match-ing algorithm creates a list of correspondences between the source and the target schema as well as the associated map-ping category. The results of all algorithms are combined in order to create the final mapping presented to the user. Furthermore QuickMig adds functionalities to support in-stance data and the creation and management of mapping categories.
As described in the previous section the matching process not only returns correspondences between schema elements but also mapping categories associated to these correspon-dences. These mapping categories can be used to create parts of the necessary mapping expressions automatically or at least to provide a mapping expression template that can easily be completed by a developer.

As an example consider the NAME1 element in the run-ning example. This element corresponds to the element FirstLineName . The correct mapping category is Move . This means that the content of the source element can be copied into the target element without modification. There-fore the mapping expression for this category can be cre-ated automatically. In contrast to this consider the KUNNR element. This element corresponds to the InternalID ele-ment. The associated mapping category is InternalID mean-ing that an internal ID, which depends on the content of the source element, needs to be created in the target system. In this case only parts of the mapping expression can be gen-erated automatically. A developer needs to complete the expression with the code to create a correct internal ID.
Based on an examination of mapping expressions common in migration projects, we identified 11 mapping categories. Table 7 in the appendix provides an overview of these map-ping categories, together with a short explanation of each category. Furthermore, the table indicates in the third col-umn if the mapping code for this category can be created automatically by the QuickMig system, and in the fourth column if the category can be identified automatically. The next paragraph briefly explains the most important mapping categories.

Move is the most simple and also the most frequent map-ping category which we observe in a typical mapping. When this mapping category is assigned to a correspondence, in-stance data is simply copied from the source to the tar-get schema element. Other common mapping categories are Split and ValueMapping .Inthecaseof Split ,thecontents of a source schema element has to be split into several tar-get schema elements. The ValueMapping category indicates that the source values need to be mapped to specific target values. This mapping can either be performed using a stan-dard code list, if applicable, or using a custom translation table. Such a custom translation table maps certain source to certain target values and needs to be maintained manu-ally. The mapping category Complex indicates the need for a complex function to translate the source instance data. In this case no automatic suggestion is provided by the Quick-Mig system. Instead, a user needs to create the necessary mapping expression manually.

For those mapping categories for which mapping expres-sions can be generated automatically, the QuickMig system creates the correct ones. In the other cases, a user either has to complete the mapping expression (e.g. in the case of Val-ueMapping ) or has to create the whole mapping expression from scratch (in the case of Complex ). With this approach no work at all is necessary for the simple cases, the user only needs to take care of the complex ones.
This section describes, in more detail, the two manual steps of the QuickMig process, i.e. the answering of a ques-tionnaire in order to reduce the target schema and the cre-ation of sample data in the source system.
Usually not all complexity supported by a target system is necessary in a given migration project. In order to deal with large schemas, we suggest to identify and leave out irrelevant parts of a schema and to simplify complex sub-structures by exploiting the domain knowledge of the user. In particular we propose to provide a (electronic) question-naire for every target schema (or set of closely-related target schemas). Based on the answers to the questionnaire, the target schema(s) are reduced to the relevant parts, which in turn leads to a reduction of the complexity of the match-ing task. Examples of questions in such a questionnaire are:  X  X hall bank account data be migrated for a customer? X  or  X  X hall multiple addresses be migrated for each customer? X . If these questions are answered with no, the BankDetails sub-structure of the target schema can be removed and the Address sub-structure can be merged with the CustomerT sub-structure of the target schema. This approach is de-picted in Fig. 4.

The mapping between the reduced target schema and the original target schema is generated automatically. As both the target schema and the questionnaire, including all pos-sible answers, are known in advance, the necessary mapping expressions resulting from certain answers to the question-naire are stored together with the questionnaire. The initial development of the mappings is performed during the devel-opment of the questionnaire. Consequently, no additional matching effort arises from the schema reduction in a given migration project. Furthermore, these mappings are reused in many migration projects, therefore the effort of initially creating them will be amortized over time.
Various previous schema matching approaches already make use of instance data [3, 6, 10, 11, 12, 13, 17]. However, they mostly apply sophisticated statistical, machine learn-ing approaches to unrelated instances available in source and target systems in order to identify matching elements. Un-fortunately, similar instances, such as phone/fax numbers typically result in wrong matches, e.g. between customer and supplier phone and fax numbers as reported in [10].
Therefore we propose to deliver one sample instance for every target schema instead of analyzing sets of unrelated instances. This sample instance contains data for all rele-vant fields of the target schema. In the second step of the migration process relevant sample instances will be automat-ically selected based on the answers to the questionnaire and presented in a user-friendly way. A business user will then manually create the same instances in the source system. Note that creating these instance in these legacy systems can be done quickly by business users, as entering this kind of information into the system is their daily business.
Using sample data this knowledge can be exploited by the matching algorithms. By injecting sample data into the source system the matching algorithms do not only have ar-bitrary instances available but one dedicated instance which maps exactly to a specific instance of the target schema. This feature is later exploited by the matchers. Instead of comparing unrelated instances, the matchers use instances based on the same data to identify related schema elements.
The idea of injecting sample data into the source system also helps to verify concrete mapping expressions. After a complete mapping has been created, the sample instance of the source system can be translated using this mapping. If the result of this translation differs from the existing sam-ple instance in the target system, it is very likely that the mapping function is wrong and needs to be revised. As an example consider the ANRED element of the running exam-ple. If execution of a mapping does not result in the value 01 for the target schema element FormOfAddressCode ,the mapping expression is wrong.

However, providing sample data may not make sense for all schema elements. The reasons for this are twofold. First, the sample data needs to be easily understandable by a busi-ness user, who has to create the data in the source system. Second, the sample data should not contain similar values for different schema elements as this leads to wrong match-ing proposals. Both features can only be guaranteed for a subset of elements.
We implemented a number of new instance-based match-ers which have been added to the matcher library of COMA++. This allows combining the matchers in a flex-ible way. Furthermore, we enhanced the reuse matcher of COMA++ to support mappings containing similarity-based correspondences as well as correspondences with mapping categories. The following sections explain the new instance-based and reuse matchers (5.1, 5.2), and the combination of their results (5.3).
In the QuickMig system three instance-based matchers were developed, namely i) an Equality matcher, ii) a Split-Concat matcher and iii) an Ontology-based matcher. In the following we will briefly introduce the Equality and the Split-Concat matcher and then focus on the more complex Ontology-based matcher.
The equality matcher is the most simple instance-based matcher. This matcher tries to identify equal instance val-ues in the source and target schema. By this approach only matches with the mapping category Move can be identified. Although this matcher is rather simple, it fits nicely to the sample data injection as this approach leads to many iden-tical instance values in the source and the target schema.
In addition to the equality matcher the SplitConcat matcher checks the instance data for splitting or concatenation rela-tionships. As an example consider the STRAS element of the running example. Given the sample instance data shown, the SplitConcat matcher would be able to identify that the element containing 1, 5th Avenue matches to the elements containing the substrings 1 and 5th Avenue because the first value can be split into the latter two. Furthermore the Split-Concat matcher would identify Split as the correct mapping category. In general the SplitConcat matcher is able to iden-tify the Split and Concatenate mapping categories depend-ing on the direction of the identified substring relationship.
The Ontology-based matchers exploit background knowl-edge provided in a domain ontology in addition to instance data in order to identify corresponding schema elements. The Ontology-based matcher currently exploits the follow-ing types of background knowledge: Modeling alternatives for common data structures. Available standard code lists. For certain information
The following paragraphs describe how these types of back-ground knowledge are modeled in a domain ontology and how they are exploited by QuickMig.

The goal for the usage of a domain ontology was not only to support the mapping process but also to make the in-formation understandable for a human user. Therefore, the ontology was not modeled according to either of the used schemas. Instead we first conceptually modeled the do-main (information related to business partners in this case) using OWL DL [20] and annotated the target schema us-ing the resulting ontology. The background knowledge was added to the domain ontology using annotations of the re-spective concepts or properties. As an example consider the FormOfAddressCode element in the running example. This element would be annotated with the object property hasFormOfAddressCode of the concept Name .Theobject property hasFormOfAddressCode has an additional anno-tation property usedStandardCodeList which is linked to an instance of a StandardCodeList representing the specific SAP code list for the form of address codes. Listing 1 in the appendix shows this example in abstract owl syntax.
The Ontology-based matcher uses the information mod-eled in the ontology to find additional correspondences based on the available instance data. For each target schema el-ement the Ontology-based matcher checks the annotation properties of the ontology entity it is annotated with. If the annotation property contains information regarding a code list, the Ontology-based matcher uses this code list to trans-late the sample data related to the element. If a description of a modeling alternative is found, the matcher also checks for this alternative based on the sample data related to the element.

In the example presented previously, the specific SAP code list for form of address codes would be used to translate the string Company to its code representation 01 .Thereby the Ontology-based instance matcher is able to identify matching elements based on the set of instances consisting of Company and 01 .

It is important to note that the ontology, as well as the questionnaire and the sample data, only depends on the tar-get system. Therefore it only needs to be developed once per system and can be delivered as part of QuickMig.
The reuse of existing mappings represents a promising approach for solving new matching tasks, which are sim-ilar to already solved matching tasks, in an efficient way. COMA++ [7, 9] already includes a matcher applying map-ping reuse. The proposed approach assumes the transitivity of the similarity relationships and uses a special compose operation to aggregate the similarity of the identified tran-sitive match correspondences [9]. The Reuse Matcher in QuickMig is based on the same assumption of transitivity of match relationships. However, we have extended it in two ways. First, we provide an optimized pre-selection of the mapping path. Second, the representation of mappings and the compose operation are enhanced in order to support not only similarity-based correspondences but also correspon-dences with mapping categories.

The quality of the reuse matcher essentially depends on the quality of the mapping path selected for the compose operation. In our migration scenarios we observed that mi-gration projects typically cover different kinds of data such as customer data, supplier data, which, despite their differ-ence in semantics, often have a very similar structure. How-ever, this structural similarity typically exists only within the source and target systems, respectively, as they have been designed and evolved independently from each other. This intra-system similarity can be exploited in order to de-termine a good mapping path for our reuse matcher. In particular, to derive a mapping between a source schema S and a target schema T 2 , we first match the source schema to a previously matched schema S 1 to obtain a mapping m 1 Assuming that the mapping m 2 between S 1 and another tar-get schema T 1 was constructed in a pre vious migration task, and the mapping m 3 between the target schemas T 1 and T 2 has been specified and maintained during the development of the target system, we can compose m 1 , m 2 and m 3 to obtain a mapping between S 2 and T 2 .
 The implementation of the compose operation is based on Table 8, which describes how a mapping category can be composed with another one in case of transitive correspon-dences. In this table the column headings are the mapping categories of the mapping between T 1 and T 2 and the row headings the mapping categories between S 1 and T 1 .Be-tween S 2 and S 1 no mapping categories are calculated as only schema based matching approaches are used. Further-more LookUp , Code2Text , Default , Split , Concatenate and Complex should not occur between the target schemas as in the target system similar data will be stored consistently (e.g. using the same codes). As an example for the com-bination of mapping categories consider Fig. 5. The map-ping category resulting from the combination of Split and Move is the category Split again. Consequently Split is the mapping category that is assigned to the correspondence be-tween STRAS in schema S 2 and StreetName and HouseID in schema T 2 respectively.

Our reuse approach is particularly promising due to three reasons. First, compared to directly matching S 2 and T 2 only one, potentially much simpler match operation is re-quired, in particular to obtain the mapping m 1 between the source schemas S 1 and S 2 . With the expected structure similarity within the source system, efficient schema-level matchers considering schema information, such as element names, data types, and structural neighborhood, may al-ready be sufficient to derive m 1 with a high quality. Second, the inter-system heterogeneity, which actually makes it dif-ficult to directly match S 2 and T 2 , is intelligently solved by reusing the previously determined mapping m 2 between S 1 and T 1 . m 2 contains only confirmed correspondences and mapping expressions, also in cases which are difficult or impossible to detect automatically. Third, the mapping m 3 between the target schemas can be specified and maintained along with the development of the target system. Such map-pings can be delivered as part of the data migration solution and are thus readily available for reuse. In this step, the individual results of the Equality, the SplitConcat, the Ontology-based and the Reuse matcher are combined. The results of the instance-based matchers can be simply merged as the matchers focus on identify-ing correspondences with different mapping categories and thus complement each other. For the Reuse Matcher, we observe that it still delivers some wrong matches due to two reasons. First, the schema based matching of the source schemas might result in wrong matches. Second, the transi-tivity property does not hold, as already reported in regard to COMA in [9]. Using sample data we can easily identify potentially wrong matches. Element pairs with provided but unequal sample instances are most likely mismatches if they are not involved in some complex matches identified by the SplitConcat and Ontology matchers. Such element pairs are obtained by inverting the merged results of the three instance-based matcher s, and are then removed from the results of the Reuse matcher. Finally, the cleansed re-sult of the Reuse matcher is merged with the results of the instance-based matchers.
In order to assess the effectiveness of the developed ap-proach, an evaluation with real-world schemas and data was performed. The schema mapping process was executed au-tomatically and results were compared against the correct mapping. The correct mappings were extracted from ex-isting code implementing these mappings. To quantify the quality of the automatically obtained matching result, the standard measures Precision , Recall and F-Measure [8] were used. The precision with which the correct mapping cate-gories are proposed was also evaluated.

Note that even though the evaluation of QuickMig was only performed using XML schemas, the system is not lim-ited to them. As it is based on COMA++, it is, for example, also capable of supporting relational schemas.
To evaluate our approach the following SAP schemas were used as examples of possible source schemas 1 i) SAP R/3, Release 4.0, Customer Master Data IDoc and Vendor Mas-ter Data IDoc, ii) SAP ERP, Release 2005, Customer Mas-ter IDoc and Vendor Master IDoc and iii) SAP B1, Business Partner Data Model. The target schema in all our experi-ments was the Business Partner schema of new SAP solu-tions. Note that in the cases where two or more schemas are mentioned in the list above, the information regarding busi-ness partners is split across several schemas. Consequently all of these schemas need to be matched against the target schema. For the evaluation we will in the following treat each set of schemas as one evaluation scenario.

These schemas were chosen to evaluate QuickMig because they exhibit most of the characteristics expected in com-plex migration projects. The naming of the elements ranges from cryptic eight letter names in the first schema to very
The schemas used for evaluating QuickMig can be down-loaded at: http://tinyurl.com/2f99vk Table 1: Complexity of schemas used for evaluation
Scenario Number of Number of Target Schema 4639 4111 R/3 4.0 953 648 SAP ERP 2150 1691 SAP B1 480 293 Table 2: Complexity of the target schema after the reduction
Scenario Number of Number of
Target schema 4639 4111 reduced to 645 395
R/3 4.0 reduced to 612 494
SAP ERP reduced to 639 279
SAP B1 verbose naming in the target schema. Also the structure of the schemas ranges from flat structures in the case of the first schema to deeply nested structures in the case of the target schema. In addition, each schema groups the infor-mation differently. Finally, the selected schemas are quite large with respect to the number of schema elements. The size of the schemas used in the evaluation is given in table 1. The table shows the number of elements in each scenario as well as the number of elements that can occur multiple times.
As described in Section 3.2 the first step in the QuickMig process is to reduce the target schema by answering a ques-tionnaire. We answered the questionnaire according to the capabilities of the different source schemas. The results of our evaluation can be found in table 2. It shows the com-plexity of the reduced target schema based on source sys-tem capabilities. Generally speaking the schema reduction approach reduces the target schema to about 10-15% of its original size and many of the complex cases can be removed.
It is not possible to state exactly what elements of the target schema were removed as this largely depends on the scenario. However, a typical example is that the used target schema supports storing multiple, time dependent addresses per business partner. Some of the source systems in our experiments do not support that. Consequently these parts of the target schema could significantly be reduced.
As the original target schema contains about 4600 ele-ments which turned out to be difficult to handle, we only ex-ecuted the following experiments with the reduced schemas. Therefore we can only point out that the complexity of the target structure can be reduced significantly with the schema reduction approach, but we do not have absolute numbers on the increase of the mapping quality. Even if the automated mapping process would not be improved by the schema reduction approach, at least the benefits for the manual review of the resulting mappings are obvious. Table 3: Evaluation results using only sample data Table 4: Evaluation results using domain-specific background knowledge
In order to see how our approaches perform in com-parison to existing schema matching approaches we tested COMA++ [9] in the data migration use case. COMA++ mainly exploits schema information such as element names, description, data type, and schema structure. These schema-level matching approaches did not perform well on the used schemas. The explanation for this can be found by taking a close look at the used schemas. As in the introduc-tory example, the schemas use cryptic elements names and differ largely in their structure. Identifying corresponding elements in these schemas is very difficult even for a human.
Table 3 shows the matching results achieved by the Equal-ity matcher exploiting sample data. Depending on the sce-nario, the instance-based matcher achieves f-measure values between 0.43 and 0.74. Note that the precision in all cases is 1. This means that the instance-based matcher does not produce any false positives. The relatively low recall val-ues in some of the scenarios originate from two facts. First, sample data is only provided for a subset of elements (cf. Section 4.2). Second, some matches are not found as Equal-ity and SplitConcat matchers only identify mappings of the categories Move, Split and Concatenate.
Table 4 shows the matching results achieved by QuickMig using the Equality and the SplitConcat matcher in com-bination with the Ontology-based matcher. The combina-tion of these two approaches achieved F-measure values be-tween 0.66 and 0.79, while still retaining a precision of 1. This is a significant increase over just using the instance-based matcher. The results shows that by using background knowledge modeled in an ontology the matchers were able to find more complex matches which could not be identified based on sample data only.
In order to evaluate the performance of the Reuse matcher, the correct mapping between the source and the target schemas for vendor data (see 6.1) as well as between the target schemas were created. These mappings were available in the mapping repository for reuse. The Reuse matcher therefore had to perform one matching task be-Table 5: Result achieved by the reuse matcher Table 6: Result achieved by matcher combination tween the source schemas for vendor and customer data. Table 5 shows the results achieved by the Reuse matcher. It is important to note that, in contrast to the instance-based and ontology-based matchers, this matcher sometimes only achieves a precision of 0 . 80. The reason for the lower pre-cision is that both source schemas are unknown and are matched using only schema based approaches.
The final experiment evaluates the performance of the combination of the Equality matcher, the SplitConcat matcher, the Ontology-based matcher and the Reuse matcher.

The result of this experiment is shown in table 6. The re-sults show that the approach of using the sample instances in order to remove wrong matching proposals of the reuse matcher is valid. Furthermore, the Equality, the SplitCon-cat, the Ontology-based and the Reuse matcher cover dif-ferent kinds of matches. Consequently, the superset covers significantly more matches than the single matcher results.
In the experiments using real SAP schemas the Move mapping category was assigned to about 60% of the matches. Another 15% of the matches were of the category Complex , and 14% of the category ValueMapping . All other mapping categories occurred with a much lower frequency.
The evaluation of the quality with which the mapping cat-egories are proposed by QuickMig was performed differently than the evaluation of the matcher results. As mapping cate-gories are only proposed for matches identified by QuickMig, missing matches were not counted as false negatives when evaluating the quality of the proposed mapping categories. Consequently only the precision of finding the correct map-ping category for the proposed matches was calculated.
For the matches proposed by QuickMig the correct map-ping category was proposed with a precision of 0 . 98 in the R/3 4.0 scenario, with a precision of 0 . 99 in the SAP ERP scenarios and with a precision of 0 . 93 in the SAP B1 sce-nario. Furthermore, it is im portant to note that all the mapping categories introduced in sec. 3.4 occurred in the evaluation scenarios.
Taking all the results together we think that the effort to find and implement the mappings necessary in data migra-tion scenarios can be reduced significantly. Using the schema reduction approach, the complexity of the target structure can be reduced significantly, simplifying the verification of the matching results. Combining sample data, background-knowledge and reuse of existing mappings, about 75% of the matches are found automatically.

Of course, as we have a semi-automatic approach, we also have to consider the effort to answer the questionnaire and maintain sample data instances in the source system. Our experience is that these tasks are relatively simple, and could, in our case, be done in less than one hour per source system.

To summarize, we are convinced that our approach can provide a real benefit for migration projects.
This paper presented QuickMig, a system for the semi-automatic creation of schema mappings in data migration scenarios. The system uses a 5-step migration process. In the first step, the complexity of the mapping problem is re-duced by answering a questionnaire. Next, sample instance data is manually created in the source system and imported into QuickMig. In the fourth step instance, ontology and reuse based matching algorithms are used to create an ini-tial mapping, which is reviewed and completed in the fifth step.
 The approach was experimentally evaluated using real SAP schemas. In these experiments the QuickMig approach achieved an average precision of 0 . 99, an average recall of 0 . 72 and an average F-measure of 0 . 84. Furthermore, Quick-Mig not only returns matches between schema elements but also assigns mapping categories to these matches, enabling the automatic creation of parts of the mapping. In the experiments with real SAP schemas QuickMig was able to identify the correct mapping categories with an average pre-cision of 0 . 97.
 In the future we plan to prototypically integrate the Quick-Mig approach into SAP data migration tools and apply it in further scenarios. [1] S. Bergamaschi, S. Castano, M. Vincini, and [2] J. Berlin and A. Motro. Autoplex: Automated [3] J. Berlin and A. Motro. Database schema matching [4] A. Bilke and F. Naumann. Schema matching using [5] A. Bonifati, E. Chang, T. Ho, L. Lakshmanan, and [6] R. Dhamankar, Y. Lee, A. Doan, A. Halevy, and [7] H.-H. Do. Schema Matching and Mapping-based Data [8] H.-H. Do, S. Melnik, and E. Rahm. Comparison of [9] H.-H. Do and E. Rahm. COMA -a system for flexible [10] A. H. Doan, P. Domingos, and A. Halevy. Reconciling [11] A. H. Doan, J. Madhavan, P. Domingos, and [12] L. Haas, M. Hernandez, H. Ho, L. Popa, and M. Roth. [13] W. Li and C. Clifton. Semint -a tool for identifying [14] J. Madhavan, P. Bernstein, and E. Rahm. Generic [15] S. Melnik, H. Garcia-Molina, and E. Rahm. Similarity [16] T. Milo and S. Zohar. Using schema matching to [17] F. Naumann, C. Ho, X. Tian, L. Haas, and [18] L. Palopoli, Terracina, and D. Ursino. The system [19] E. Rahm and P. A. Bernstein. A survey of approaches [20] W3C. Owl web ontology language. online, 2004. [21] L. Xu and D. Embley. Discovering direct and indirect
Category Description Mapping Complex A complex mapping expression is required. manual manual
