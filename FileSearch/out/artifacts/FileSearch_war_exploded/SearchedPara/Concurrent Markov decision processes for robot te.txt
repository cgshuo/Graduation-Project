 1. Introduction
For multi-agent robot teams to become a more common fi xture in private and public industries, they must exhibit compliant individual and social learning behaviours. A multi-agent learning problem is commonly approached in the literature from either an individual or team perspective. To enhance the performance of a multi-agent team attempting a complex task in a realistic envir-onment, there needs to be a concurrent approach toward three ( Parker, 2012 ). Indeed, the operation of separate learning mechan-isms at the three interdependent learning levels can be challen-ging both analytically and heuristically.

To form a multi-layered learning approach, each layer of team learning may be characterized by a speci fi c learning problem: collectivism by individual performance, cooperation by task allo-cation, and collaboration by advice sharing. Afterward, these learning problems and their interrelationships may be considered analytically and solved by an appropriate learning mechanism. In this paper, Markov decision processes (MDPs) are used as learning problem models, and several concurrent individual and social learning approaches are utilized to discover locally optimal beha-viours, which are demonstrated in a heterogeneous team foraging scenario. These approaches are contrasted with a decentralized policy improvement approach as a control ( Bernstein et al., 2002 ).
The paper fi rst discusses the domain of multi-agent learning with a focus on Markov decision processes, investigating how analytical models of team behaviour can be applied to team scenarios. Decision Theory, sometimes characterized as an analy-tical discipline of Computer Science, has broad applications to many disciplines including Robotics ( Beynier and Mouaddib, 2012 ). A new method of learning problem formulation is then formalized; instead of modelling a learning problem as a single centralized or decen-tralized MDP, and designing analytical expressions in a holistic manner, we deconstruct a large MDP into a set of dissimilar dependent MDPs, which we denote as a Concurrent MDP (CMDP).
In Section 2 , the current decision theoretic paradigm is criti-cally compared with the needs of an empirical robotics scenario. In Section 3 , the concurrent individual and social learning (CISL) problem model is de fi ned in relation to the MDP paradigms. Section 4 details a heterogeneous robot team foraging case study. Section 5 discusses the performance of various CISL algorithms vs. that of a decentralized policy improvement approach. Some concluding remarks are made in Section 6 . 2. Background
In this research only Fully Observable MDPs (FOMDPs) are referenced, with S ; A ; T ; R tuples where S denotes a fi discrete states, s ; A denotes a set of fi nite actions, a ; T  X  s represents a true transitional probability between states; and R  X  s ; a ; s 0  X  denotes a reward function. When solving for an optimal policy, an in fi nite time horizon and a discounted reward setting are assumed. In some cases other MDP de fi nitions will be noted speci fi cally. We will restrict our attention to multi-agent planning problems, where all agents collectively seek a uniform outcome ( Boutlier, 2000 ).

In a direct sense, the simplest solution to such a planning problem is to include all relevant state information into a FOMDP, treating it as a multi-agent Markov decision process (MMDP); each agentcanbegivencompleteknowledgeoftheentiresystemaswell behavioural policies can be found using any reinforcement learning method. The MMDP modelling approach is attractive because issues related to competition, timing, and scheduling are not present, as there is one ideal utility function across all agents and all agents regress to it together. These problems are P-complete ( Mundhenk et al., 2000 ) or PSPACE-complete ( Papadimitriou and Tsitsiklis, 1987 ), and can be approached in polynomial time.

Two major issues exist with the aforementioned centralized approach. First, the state space size for real-world problems can be prohibitively large; hundreds of dimensions can easily exist, leading to problems of sparsity, interdependence, and computa-tion. Second, a single robot is unlikely to know the positions, utility functions, and future decisions of an entire robot team with much certainty. To deal with these major multi-agent issues a few approaches have been taken. 2.1. Decentralized representations
It has been shown that game theoretic worst-case estimates can be applied to multi-agent teams, in a partially or completely obser-2002 ). Decentralized MDP (DEC-MDP) modelling approaches forma-lize the multi-agent model by adding a set of agents, actions per agent and observations per agent to the MDP de fi nition. In a fi sense, worst-case computation to solve such problems has been readily shown to be super-expo nential in computation time and NEXP-Hard ( Bernstein et al., 2002 ).

Some attempts have been made to solve these intractable problems in an approximate sense. If it is assumed that all the agents are completely independent, such that they do not affect each other or each other  X  s observations, then a factored represen-tation of the DEC-MDP can be solved ( Becker et al., 2004 ). The anytime implementation of this model, an algorithm that is interruptible, is intractable for large problems, even if typical simulations land it roughly within 90% of the value earned by an optimal policy. Recent approaches, while encouraging in the amount of agents simulated, still only approach simple games, and lack the complexities of realistic simulations ( Pajarinen and Peltonen, 2011 ).

Albeit a complete and accurate model of a multi-agent system, the DEC-MDP model is one of the hardest models in terms of complexity, as it is NEXP-Hard. Thus, it lacks strong real-time algorithms for agents with limited memory and computational resources. 2.2. Partial state representations
To directly convert an MMDP or DEC-MDP problem into a more tractable one, a partial subset of the full state can be represented.
For example, it may be determined that some state information is irrelevant for each individual agent, or that a certain subset of this state information is unstable or unavailable. In this case, it may be desired to prune the state representation to a more reasonable size.
Approaches range from learned pruning, such as principal compo-nent analysis (PCA) ( Billon et al., 2008; Tamimi and Zell, 2004 )or the designer of a state space. This reduces the potential state space required for the agent to explore, and therefore lessens the computational burden across all agents. Much research is focused on state representation as an aid to machine learning, to speed convergence to an optimal policy. In the case of learned pruning, such as PCA or clustering, a reduction of state space may also result in noise reduction and performance improvement.

Another approach is to compress the state representation using various methods without loss of information. For example, a learned factor representation ( Boutlier, 2000 ) can lead to less memory usage, even if theoretical worst-case performance is equal to the full state space; the agents learn which state variables are dependent through experience. Conversely to discovering and modelling many dependencies between states, many state vari-ables can be assumed to be independent. This leads to exponen-tially less memory usage. In both of these schemes no information loss takes place in terms of raw state data, but the predictive power of the utility functions can decrease due to misrepresenta-tion of variable independence.

Lastly, it may be desired to limit the MDP to some local scope that is directly observable by an agent, or smaller, using an ad hoc method. In these cases a designer can take an MDP of extremely high dimension and convert it to an MDP of lower dimension; an intractable problem can be formulated as a weaker, inaccurate, and tractable model. A typical state vector in this case may include communications from other agents, sensor readings, and a variety of internal metrics. For robotics, such a step is a general requirement. 2.3. Concurrent MDP representations
A last approach, and the primary contribution of this paper, is a method of converting an intractable multi-agent MDP problem into separate dependent MDPs . Groups of behaviours can be addressed separately, with the main bene fi t being a reduction in action and state space. In fact, the nature of policies derived in a concurrent
MDP setting do not need to be of the same class or rigor, allowing the designer more freedom when compared with previous approaches.
This novel approach, while empirically common, is rarely formalized or analytically explored. We de fi ne this approach as the Concurrent
MDP (CMDP) approach, and analytically describe its application to simulation. As a result, the concurrent and individual and social learning (CISL) approach addresses multi-agent learning by solving two independent classes of MDP problems concurrently ( Ng and Emami, 2013, 2014 ). First, a limited scope individual performance MDP is solved. Second, a task allocation MDP is solved. Both of these
MDPs are compact and of low quality relative to a corresponding decentralized MDP.

For the CISL approach, the individual MDP is seen as indepen-dent of other non-cooperating agents  X  pairwise agent interactions are considered as unmodeled noise. The task allocation MDP is then developed to characterize the process of assigning tasks to agents, such that the reward obtained by the team of agents is a random process, which re fl ects team performance toward a team objective.

Similar approaches include small heuristics, such as Q-Learning with search, where the individual learning is augmented with a communication layer that raises the probability of discovering a foraging target ( Hayat and Niazi, 2005 ), to full heuristic frame-works such as Alliance. The Alliance framework ( Parker, 2001 ) uses a sub mechanism, called L-Alliance ( Parker, 1998 ), to divide a foraging problem into an individual learning problem and separate task allocation problem in a heuristic fashion. None of these approaches are proven analytically to minimize or maximize task allocation behaviours, rather they empirically improve team per-formance. In general, many multi-agent learning techniques lack rigorous analytical treatment.

In this research we will focus on the CISL approach, which is one example of a possible CMDP representation. We will also show that such an approach allows for real-time simulation of complex foraging scenarios with at least 8 heterogeneous agents.
The bene fi t of such a dependent MDP partition is that solutions may be analytically sound, computationally tractable, and broadly applied in experimental scenarios with off-the-shelf technologies  X  a step toward further uni fi cation of practical decision theoretic approaches and experimental robotics. 3. CISL model foundations 3.1. Markov decision process In this paper, we discuss partitioning a single agent  X  s learning
MDP into an individual performance MDP and a task allocation MDP , which run concurrently as a concurrent Markov decision process (CMDP). Fig. 1 illustrates the proposed learning model. A Con-current MDP is a set of two or more MDPs that have at least one dependent MDP relationship, meaning that one MDP somehow depends on the properties of another separate MDP. A relationship of this type is more general than a hierarchical relationship ( Barry, 2009 ), and a hierarchical MDP can be considered as a special type of Concurrent MDP. Further, we consider the set of CMDPs, for all agents in a team, as a transition independent Decentralized CMDP (DEC-CMDP) ( Becker et al., 2004 ).

The Concurrent MDP problem model assumes the individual performance and task allocation MDPs run in parallel as a CMDP set with two speci fi c dependencies. A task selection dependency models the reliance of an individual MDP on the task allocation
MDP; for every state s I A S I we assume that a subset e IT exists whose value is determined solely by the task allocation
MDP. Conversely, the team performance dependency de fi nes the reward function R T  X  s T ; a T ; s 0 T  X  and transition function T for the task allocation MDP based on the performance of indivi-dual agents. Other than the two dependencies, these MDPs are independent, and can be treated as separate processes for the purposes of fi nding an optimal policy. 3.1.1. Individual performance MDP Individual agent progress toward a sub-task can be de fi ned as a S ; A I ; T I ; R I tuple,
S I denotes a discrete set of states, whose intrinsic value is partially speci fi ed by the task allocation process through a set of evidence E IT .
 A I denotes a discrete set of actions.
 ability of executing action a I starting from state s I and ending up in state s 0 I .
 for transitioning from state s I into state s 0 I using action a The evidence value e IT A E IT is de fi ned by the task allocation
MDP, such that maximal visitation of all states at time in assured. Such a single-agent MDP can be seen as a straightforward process for the reinforcement learning. 3.1.2. Task allocation MDP Team progress toward a goal can be de fi ned as a S T ; A tuple,
S T denotes a discrete set of states, which captures the indivi-dual performance characteristics between all agents and all tasks.

A T denotes a discrete set of actions, i.e., a function that assigns all available tasks to available agents.
 ability of executing action a T starting from state s T and ending up in state s 0 T . The transition function is completely speci the individual performance process through a set of evidence received after taking a certain action a T within state s arriving in state s 0 T . The reward function is completely speci by the individual performance process through a set of evi-dence E TI .

The value of the reward R T and transition functions T T are assumed to be unpredictable up to some known discrete iteration t . After t s the reward and transition function for the task allocation process are assumed to have a constant expectation value, which may be affected by some unknown amount of zero mean noise.

It is worth emphasizing that within the Concurrent MDP model the two individual performance and task allocation processes explored concurrently. This concurrent learning approach distin-guishes the model from a single decentralized MDP that may consist of a set of states S U  X  S I S T , a set of actions A reward function R U , and a transitional model T U . Indeed, the decentralized MDP is used in this paper as a control to contrast with other learning models.

In all proposed MDPs it is assumed that the time horizon is in fi nite, the reward is discounted, and the time setting is discrete. 3.2. CISL approach A systemic presentation of the CISL approach is given in Ng and Emami (2013 , 2014) . In this paper, the approach is revisited in more depth. For each agent, the CISL approach is formulated toward two independent Markov decision processes; an individual learning MDP models agent behaviour toward an assigned task, while task selection MDP models agent task selection. The team learning CMDP model can be solved by a three-layered approach, whose layers represent three paradigms of multi-agent learning ( Parker, 2012 ): Collective , Cooperative , and Collaborative .
Collective learning in a multi-agent environment denotes a set of independent learning mechanisms developing their own inde-pendent behaviour policies. The collective layer learning problem is indeed the individual performance partition of the CMDP, which can be solved by a reinforcement learning algorithm, such as Q-Learning ( Watkins and Dyan, 1992 ).

Cooperative behaviours are described as behaviours that solve for the task allocation partition of the CMDP, i.e., which agent should work toward which task. To formulate this layer three algorithms are contrasted in this paper. First, Q-Learning is applied to the task allocation concurrent to the collective layer. Second, a heuristic approach is used, called L-Alliance ( Parker, 1998 ), and lastly, a second version of the L-Alliance algorithm is applied, which is a combination of stochastic gradient descent with the L-Alliance algorithm.

Collaborative behaviours enable agents to directly in fl uence each other  X  s policies. A heuristic mechanism, called Advice Exchange ( Nunes and Oliveira, 2005 ), is used which allows an agent with high average reward to share its individual performance Q-Learning utility values with other agents. The agents will then possibly bene the enhanced behaviours for a certain duration, and will include these lessons in their own policy regression.

This section discusses how each learning mechanism regresses to a policy for its corresponding Markov decision process. We assume n agents in the team, where each agent i derives its own policy,  X  i , and utility function, Q i , for each MDP. Each agent has two policies and two utility functions:  X  i I ;  X  i T ; Q subscript identi fi es individual or task allocation policy or utility function. 3.2.1. Collective learning: Individual performance policy regression
The individual policy  X  i I regression is performed by a Q-Learning algorithm ( Watkins and Dyan, 1992 ). The algorithm is a form of policy improvement in Decision Theory, which takes incremental
After random initialization of all the utility values, Q I ; 0 A S
I A I , each agent will execute reinforcement learning using a basic Q-Learning update rule: Q time t . The expected utility values are discounted by a constant which approaches zero as t approaches 1 satisfying two conver-gence criteria, a ; s reward function. Given these de fi nitions, it is expected that Q-
Learning will settle on a locally optimal policy for an in horizon problem ( Watkins and Dyan, 1992 ). To determine a suitable based on an agent  X  s known utility values for a state s I ; t  X  a where the action probability distribution  X  i I ; t  X  a I ; t ability that agent i will select action a I ; t A A I ; t time t. The temperature parameter  X  modi fi es the smoothness of the action probability distribution through scaling of the utility values.
The optimal policy for maximization of agent reward can be de as 3.2.2. Cooperative learning: Task allocation policy regression
This section begins by out lining three methods of fi nding and executing an optimal task allocation behavioural policy, namely
Concurrent Q-Learning, L-Alliance, and RL-Alliance. Afterward, it covers a tractable modi fi cation to L-Alliance to accommodate physical cooperation, enabling two agents to cooperate toward a single task. 3.2.3. Q-learning for task allocation
A direct approach to the task allocation problem can be the use of a separate Q-Learning mechanism concurrently with the indi-vidual performance Q-Learning mechanism (Eq. (1) ), which has identical theory as presented in Section 3.2.1 : Q
To operate this mechanism no online communication between agents is intrinsically required 3.2.4. L-Alliance
To assign all the available tasks to the agents, a task selection mechanism, called L-Alliance ( Parker, 1998 ), can be used. The
L-Alliance mechanism regresses to a task allocation policy iteration, every task j is assigned to the available agent i that has the highest motivation m t i ; j  X  X  , as described by each agent  X  s
A S T  X  : This motivation is initialized as zero, and can be updated in every iteration t of the individual performance MDP: m i ; j  X  X  X  m t 1 i ; j  X  X  X  p t i ; j  X  X  g t  X  i ; j  X  X  4  X 
The motivation m t i ; j  X  X  is derived using a pervious motivation p i ; are all satis fi ed at time t to keep the robot motivated; otherwise m i ; j  X  X  is reset to zero. These conditions include (a) whether task j more than two agents are assigned to the task j at time t , and (d) no other agents are present at time t that can accomplish task j better than robot i . If any of these conditions is not satis these conditions robots need to communicate online.

The impatience factor p t i ; j  X  X  is a real-positive function that is broken into three formulations depending on the average trial time agent i when assigned to task j : i p  X  i ; j  X  X 
The parameter  X  denotes a positive non-zero constant, and the scaling factor K ij is used to reduce the idle time of the agents, and is de fi ned as:
K  X 
The parameters  X  min and  X  max are de fi ned as the minimum and also de fi ned, respectively, as the maximum and minimum trial time over all robots k and tasks l . The smaller the average trial future. The Superior impatience rate is used when agent i Mediocre impatience rate is used.

Thus, the L-Alliance mechanism is a greedy algorithm that assigns the most motivated agent i to a corresponding task j . The L-Alliance policy  X  i T uses an agent  X  s latent transitional model, T to maximize for the reward function, R T . The maximization of reward can be characterized by minimizing the agents  X  average trial values: R s and is valued as either one (assigned) or zero (unassigned).
It may be desired to enable the L-Alliance mechanism to assign two agents to one task to enable physical cooperation, by considering every coalition average trial time ,  X  t i ; i 0 ; j ,where i and i two agents cooperating toward a task j ( Parker, 2012 ). The motivation formulation (5) and impatience formulation (6) can then still be used in case of physical cooperation, but resulting in coalition motivation m  X  i ; i 0 ; j  X  and coalition impatience p
It may be intractable to consider every task coalition pair. To enable tractable physical cooperation, it is assumed that a simpli-fi ed agent effort metric can be utilized in place of all coalition-task trial averages  X  t i ; i 0 ; j : i ; i 0 ; j  X 
Additionally, to discourage unn eeded cooperation, a limiting condition is added to the gating condition g t  X  i ; j  X  :(e)Thegating is completed by two cooperating agents, no  X  t i ; j  X  X  or are updated, as trial times denote individual effort. Thus, physical cooperation in task allocation is at tempted in a tractable manner by executing a minimization of agent  X  strialefforttowardtasks. 3.2.5. Reductive L-Alliance: Pareto-optimality for L-Alliance
The L-Alliance mechanism does not always perform optimal task selection. Two examples are task starvation and mediocre assignment .
Task starvation occurs if an agent i achieves an anomalously high average trial time  X  t i ; j  X  X  toward task j ;theagent i may become prevented from the allocation to task j in the future, despite having a historically low average trial time. Task starvation can prevent a potentially superior agent i from gaining assignment to task j .Second, a mediocre assignment can occur if the L-Alliance mechanism preferentially assigns mediocre agents over superior agents. The mediocre assignment event can be expressed analytically by for-mulating when a mediocre impatience rate will be greater than a superior impatience rate, given a superior agent s ,amediocreagent i and a task j :
Superior Impatience Rate o Mediocre Impatience Rate
Given Eq. (9) the mediocre assignment will occur when Eq. (10) is satis fi ed: s ; j  X  X  X   X  t i ; j  X  X  2 min k ; l  X  t k ; l  X  X  o  X  max  X 
The L-Alliance will perform mediocre assignment when  X  max set too high,  X  min is set too low or the minimum average trial time across the team min k ; l  X  t k ; l  X  X  is too large.
 A Pareto-optimal initial task assignment can be achieved by the
L-Alliance mechanism if, fi rst, the slow impatience rate p slow  X  second, a recursive stochastic update formulation is used for the average trial time instead of moving average, as follows: i ; j  X  X  X   X   X  i ; j  X   X  t 1 i ; j  X  X  X 
The frequency f ij denotes the number of attempts agent i has had toward task j . The parameter  X  2 denotes a constant scale factor, and can be called the learning rate. The variable I the time taken during the latest attempt by agent i toward task j . expression, which allows each agent to have many attempts and ade-quate training on all tasks. The parameters  X  3 and  X  4 control the slope and location of the softmax distribution. Thus,  X  t represent the expected trial time required by agent i when assigned to task j , assuming that the attempt time I ij is stationary by a certain time t s .

Three factors indicate that the modi fi ed L-Alliance mechanism, which we denote Reductive L-Alliance (RL-Alliance), fi nds a Pareto-optimal task allocation and an optimal solution for maximizing the reward, for an initial allocation of set of tasks to a set of agents:
The parameter  X  t i ; j  X  X  will accurately converge to the expected average trial time of an agent i toward task j .

By using a gradient descent update for parameter  X  t i ; j  X  X  and a motivation update that only utilizes the slow impatience rate in Eq. (5) , the RL-Alliance mechanism selects tasks in a Pareto-optimal manner, because an agent is assigned to the task it is most motivated toward if and only if no other robot is more motivated: m t i ; j  X  X  X  argmax
After all items are selected, a Pareto-optimal allocation is found, characterized by the inability to reassign tasks without performing a sub-optimal task selection for at least one other agent ( Kapralovy et al., 2013 ).

The RL-Alliance mechanism is a greedy optimization method, which results in maximizing the reward function R T , since the max-imization of the motivation towards a task implies a coincident maximization of the reward function: R hence, the RL-Alliance mechanism fi nds an optimal solution for the reward function, and also fi nds a Pareto-optimal task allocation across agents for the initial assignment of agents to tasks. 3.3. Collaboration: Advice exchange In order to allow agents to share their experience an Advice Exchange mechanism ( Nunes and Oliveira, 2005 )isused,which enables collaboration through the exchange of advised actions between agents. An agent i uses actions from a superior agent k policy  X  k I ; t , when agent k is acquiring its reward R on average than agent i . We implement the Advice Exchange mance MDP:
Eq. (14) states that three advice conditions must be met for agent i to accept advice from advisor agent k . These conditions are formulated using three quality parameters . These quality para-meters are based on the reward q received in a given iteration from the individual performance MDP, and they include the average quality over a single epoch ( q ), the current average quality current epoch, a set of consequent iterations, the parameter d represents a self-con fi dence parameter ranging from 0 to 1, and the potential advisor set Z i contains the indexes of all suitable advisors for agent i . The parameter  X  represents the signi fi cance of advisor experience. The average quality parameter for agent i can be calculated as a simple moving average at the end of an epoch with a total of M iterations: q
The current average quality of an epoch h is de fi ned as: where 0 r  X  r 1 is a decay parameter. Lastly, the best average quality for epoch h is de fi ned as: ^ q where 0 r  X  r 1 is a weighting factor. The initial value of all the three qualities can be chosen as zero. 3.3.1. Optimality for advice exchange The Advice Exchange mechanism does not prevent the Q-Learning algorithm from fi nding a locally optimum utility function
Q , because the mechanism still guarantees in fi nite visitations to action pair  X  a I ; s I  X  the action selection probability real positive number. Further, no parameters given to the Q-Learning formulation (Eq. (1) ) change when implementing the
Advice Exchange mechanism, as the reward function ( R I ) and transitional model ( T I ) remain identical. Thus, it is expected that the advised policy,  X  i I ; t , will maintain discovery of a locally optimum utility function Q i I ; t for the maximization of individual reward R I .

The effect of the Advice Exchange mechanism on task selection can be examined through the following deconstruction of the Team
Performance dependency, which is described in Fig. 1 .Thedepen-dency is entirely characterized by the environment and individual tion R T is calculated using the expected average trial times all individual behaviour policies. Thus, for the task allocation MDP to be Pareto solvable by the RL-Alliance mechanism, the transitional model and reward function must both be stationary processes after a certain iteration, t s ,asde fi ned in Section 3.1 . This stationary requirement is satis fi ed when using Q-Learning regression and softmax action selection alone, as the set of individual policies ; ... ;  X  n
Advice Exchange mechanism may invalidate this assumption, because there is always a possibility that at least one of the advised policies  X  1 I ; ... ;  X  n I ~  X  i ; j  X  X  X 
The L-Alliance mechanism, after the trial at time t , will update value for the agent  X  s expected advised policy ~  X  t i ; necessarily a stationary process. Despite this effect, the Advice
Exchange mechanism can support Pareto-optimal task allocation under two circumstances: (a) all potential individual policies converge to identical policies  X  1 I ; t  X   X   X   X  n I ; t t ; or (b) the Advice Exchange mechanism will cease at the time t
Therefore, one direct method to ensure that the Advice Exchange mechanism will not interfere with Pareto-optimal task selection is to vary the self-con fi dence parameter d in Eq. (14) such that the
Advice Exchange mechanism will cease advice taking before a certain time t s . A softmax expression such as the following can provide the desirable behaviour: d
The parameter K adjusts how the effect of Advice Exchange mechanism is decreased in time, such that:
K  X  ln  X  5 ln 100  X  5  X  20  X  where  X  5 is the inactivity percentage of the Advice Exchange mech-anism at time t s . Thus, the Advice Exchange mechanism allows sharing of behaviours between agents while still allowing the
L-Alliance mechanism to converge toward a Pareto-optimal solution for the task allocation MDP. 4. A case study: Heterogeneous team foraging
The objective of the case study is to move 8 items to a target zone, which has a diameter of 2 m. The 2 dimensional simulation world is sized as 10 m by 10 m. Each item is classi fi ed as heavy or light. There are 8 heterogeneous robots that are classi fi number within four categories: strong-slow, strong-fast, weak-slow, and weak-fast. Heavy items can be manipulated by two robots together, or one strong robot alone. The team must cooperate to forage all items into the target zone within 15,000 iterations, or the simulation is terminated. Each item is considered as a circle with diameter 0.50 m. Robots have a diameter of 0.25 m. There are four circular obstacles in the area, each with a diameter of 1 m. For every foraging trial, called a run, the initial positions of all simulation objects are randomized. Fig. 2 depicts the simulation scene of the case study.

In the following sections, three MDP models are presented for the aforementioned foraging scenario. Initially, a Concurrent MDP will be formulated for the Q-Learning and L-Alliance approach ( Section 4.1 ). Afterward, a second Concurrent MDP will be for-mulated using separate Q-Learning algorithms for individual performance and task allocation ( Section 4.2 ). Lastly, a decentra-lized MDP will be used as the control, solved by single Q-Learning algorithm for both individual performance and task allocation
MDPs ( Section 4.3 ). 4.1. Concurrent MDP: Q-learning and L-Alliance In this section we describe the implementation of Concurrent
MDP problem model for the foraging scenario. Initially, we des-cribe the individual performance MDP, and afterward we will describe three de fi nitions of the task allocation MDP. 4.1.1. Individual performance MDP The individual MDP used in this case study consists of a S ; A ; T I ; R I tuple de fi ned as follows:
State s I A S I :isde fi ned by s I  X  r x ; r y ; r  X  ; o g ; b x ; b y g , which includes the information about the robot r , nearest obstacle o ,assigneditem l ,targetzone g ,andworld border b . The subscripts x and y represent the x  X  y coordinates of the objects, and r  X  represents the robot  X  s orientation from the x -axis of the world frame. The variable l p represents the type of the assigned item as 1 for light and 2 for heavy.

Action a I A A I :is move_forward , move_backward , turn_right , turn_left ,or interact . Each robot moves at either 0.20 m (slow robot) or 0.40 m (fast robot) per move action. All turns are at  X  = 4 radians per turn. Interact attaches the robot to an assigned item when the robot is currently not gripping another item. tioning from one state s I into another state s 0 I , given an action a
Reward R I s I ; a I ; s 0 I : administers the reward (penalty) points based on whether the robot moves closer (farther) to the assigned item or moves the item closer (farther) to the target zone or delivers the item to the target zone. Additional reward is administered if the robot moves further from the target zone while not assigned to an item ( Table 1 ). 4.1.2. Task allocation MDP
The task allocation learning problem is to determine the optimal assignment of items to the set of agents, where a task selection action at each iteration t is considered optimal when the robots minimize their average item return effort,  X  t i ; single-item-single-robot assignment will be discussed using the average trial duration,  X  t i ; j  X  X  , and afterward the robots physical cooperation will be discussed. The task allocation MDP is modeled as a S T ; A T ; T T ; R T tuple: (1) State s T A S T : contains three pieces of information: each 4.1.3. Physical cooperation
To support physical cooperation , the coalition average trial time  X  i ; i 0 ; j can be considered, as discussed in Section 3.2.2 .A approach can modify the task allocation MDP  X  s state to character-ize all a pairwise robot coalition average trial times. The expansion of the sub-state s 1 T ; t can turn into: s
As mentioned in Section 3.2.2 , such an increase in state space by a factor of n may be intractable due to computational limita-tions. A second less direct approach is used to minimize the size of the state space matrix; each robot  X  s effort metric  X  t i de fi ned in Eq. (8) is stored to characterize the total team effort in Eq. (21) . 4.2. Concurrent MDP: Individual and task allocation Q-learning algorithms As a contrast to the full information MDP, as designed for the L-Alliance mechanism presented in Section 4.1 , we consider a task allocation MDP suitable for Q-Learning. The model for the indivi-dual performance MDP is kept identical with the approach discussed in Section 4.1 . The task allocation MDP for the Q-Learning mechanism can be modeled as a S T ; A T ; T T ;
State s T A S T : represents the set of the three nearest available items to robot i , and type of the j th nearest item, and is valued 0 : none 1 ment status of the robot i with item j , f 0 : unassigned 1 : another _ robot _ only ; 2 : current _ robot _ only ,3 :
Unavailable or fully utilized items are excluded from the state entirely. If an item is currently assigned, it will be represented in the fi rst state position ( j  X  1), regardless of the simulation geometry. Lastly, empty slots are fi lled with none or unassigned values. Thus, this state model does not re fl ect other robots performance values, drawing a contrast with the L-Alliance approach. The L-Alliance approach requires the monitoring of all robots  X  average trial time.

Action a T A A T : is the assignment or release of robot i to/from the j th item; and drop releases any gripped item. If only a single the robot to the single available item.
 ing from one state s T into another state s 0 T , given an action a
Reward R T  X  s T ; a T ; s 0 T  X  : for the task allocation MDP is de the average reward acquired by the robot i within a duration of the individual performance MDP. In addition to the average reward acquired by the individual process, which is assumed to be zero-mean by a time t s ( Table 2 ). 4.3. Decentralized MDP: Single Q-learning algorithm
The fi nal team learning model, which is provided as a control, is denoted as a decentralized MDP, which degenerately characterizes the individual performance MDP ( Section 4.1 ) and the task alloca-tion MDP ( Section 4.3 ) using a transition independent decentralized model ( Becker et al., 2004; Nair et al., 2003; Paquet et al., 2005 ). The model is expressed as a S U ; A U ; T U ; R U tuple: State s U A S U : is a combination of the individual performance
MDP state set ( Section 4.1 ) and the task allocation MDP state set ( Section 4.2 , two nearest items), S U S T S I .A drop_avail-able state value d a is added, which represents the availability (one) or non-availability (zero) of drop action:
Action a U A A U : is an action in the union of the individual performance MDP action set and the task allocation MDP action set,  X  A U A T [ A I  X  . To limit the drop action, one constraint is added to the model: When a robot transitions from not gripping to gripping an item, the drop action is unavailable for a duration  X  d of decentralized MDP iterations. The duration parameter  X  d allows for a similar implementation of the trial time for the L-Alliance mechanism, robots are given at least iterations with each item attempt.

Transition probabilities T U s U ; a U ; s 0 U : is the probability of transitioning from one state s U into another state s 0 U action a U .
 mance MDP reward and task allocation MDP reward as indi-cated in Sections 4.1 and 4.3 .
 R s ; a U ; s 0 U  X  R I s U ; a U ; s 0 U  X  R T s U ; a U ; s 4.4. Advice-sharing mechanism: Advice exchange
To enable advice sharing a set of potential advisors Z i is required for every robot i .Thus,wede fi ne the set of advisors as those agents advisors are all the weak-slow and weak-fast robots. For all CISL  X  R 5. Results
In this section the performance results of the concurrent MDPs and the decentralized MDP approaches are compared, as applied to the aforementioned foraging scenario. For each experiment 20 simulations were executed, each consisting of 300 runs. Each run was limited to 15,000 iterations. For each simulation the utility values and robot performance sub-states were set to their default values. In addition, due to the random nature of learning algo-rithms, a ten-point moving average was used to smooth the plot of each experiment. Finally, to consider the statistically distinctness of each simulation, a paired T -test was used on characteristic results.

To analyze the performances, the following metrics have been de fi ned:
Simulation time : The total simulation time required by individual robots to complete the foraging objective, in terms of number of iterations, which characterizes the agility of the team.
Total effort : The total number of actions executed by individual robots to complete the foraging objective, which characterizes the total team effort.

Individual reward : The average reward R I received by a single robot assigned to a single item.

Cooperative effort : The total number of cooperative actions executed per run, when completing the foraging scenario.
The enhancement of an individual robot  X  s foraging ability is considered fi rst, while comparisons between the Concurrent app-roaches and the Single Q-Learning approach will be studied next.
Fig. 3 illustrates the foraging performance of a typical robot before (a) and after (b) training by the individual Q-Learning algorithm. It is noted that the Q-Learning mechanism and individual perfor-mance MDP succeed in enabling robots to enhance their individual performance.

Fig. 4 depicts the variation in the average individual reward received by a typical robot over the training runs. It is noted that under all CMDP approaches, the individual reward obtained by the robot is maximized. The higher values of maximum average reward for the L-Alliance approaches, compared to that of Con-current Q-Learning, indicate the ef fi cacy of task allocation approaches in fi nding robots for each item, which results in a better overall performance.

The remaining comparison studies are highlighted in the following stages: First, the mean, standard deviation and mutual statistical signi fi cance of all learning approaches are considered.
Next, the effect of Advice Exchange mechanism on the learning and converged behaviour is studied. In the following discussions, the Single Q-Learning and Concurrent Q-Learning approaches are denoted as the Q-Learning approaches , and the L-Alliance and RL-Alliance approaches as the L-Alliance approaches .

Fig. 5 shows the team  X  s Simulation Time and Total Effort for the foraging scenario experiments. It is shown that convergent policies are empirically executed by all learning approaches. The CMDP approaches seem to dominate the Single Q-Learning method with respect to Simulation Time and Total Effort metrics. With regard to approaches tend to start from longer completion times but converge at a steeper rate, whereas the Q-Learning approaches, concurrent and single, start from shorter completion times with a more moderate convergence to their fi nal values. With regard to the team
Total Effort metric ( Fig. 5 b), both Q-Learning approaches initially than their L-Alliance counterparts do. Overall, the superior method, in terms of Simulation Time, seems to be the Concurrent Q-Learning approach, whereas the RL-Alliance and L-Alliance approaches seem to be doing better in terms of Total Effort.

A possible explanation for a superior performance of the CISL approaches over the Single Q-Learning approach is that the task allocation actions have a longer time horizon to generate reward, and thus may not have the same solutions on the scale of individual iterations. That is, it may not be possible to represent long-term rewards of the task allocation MDP alongside short-term rewards attained by individual performance MDP using identical learning parameters (  X  ,  X  )andasingleutilityfunction.Thus,abene fi
CMDP approach is its ability to represent reward on multiple state space and temporal scales.

The Q-Learning approaches require a lesser number of trials than the L-Alliance approaches for the robots to arrive at their stable performance level. However, under the Q-Learning approaches, the is because the L-Alliance approaches allow for physical cooperation only when a single robot is projected as incapable of handling a given item alone, whereas there is no mechanism to limit (unne-cessary) physical cooperation in the Q-Learning formulations. Thus, a larger number of physical cooperation actions can occur in the Q-Learning approaches, which results in increasing the total team effort. The Physical Cooperation metric is shown in Fig. 6 ;forthe given scenario where there are just enough robots of each type to handle items of either weight (heavy and light), the L-Alliance mechanisms minimize cooperative effort, which also reduces the Total Effort.

All learning approaches reduce the standard deviation of their performance metrics through the runs, as illustrated in Fig. 7 , exhibiting their ability to converge toward foraging performance with a stable expected value.
 Upon inspection of Fig. 7 it appears that both the Concurrent Q-Learning and the RL-Alliance mechanisms exhibit the lowest variance during the stable phase , which contains all of the runs after run 200. The Concurrent Q-Learning approach is optimal during much of the transient phase , which contains all of the runs up and including run 200. This conclusion will be further justi by studying the pairwise T -test results in the following.
To verify that the obtained performance metrics for the different approaches are statistically distinguished, pairwise inde-pendent Welch  X  s T -tests were performed for every run, with a population of 20 simulations ( Welch, 1947 ). Table 1 summarizes the average p -values of the pairwise T -tests for the transient and steady phases when considering Total Effort, as well as the percentage of runs whose p-value is below the alpha-level of 0.05.
As shown in Table 3 , all pairwise T -tests verify distinct behaviour, with the exception of the L-Alliance mechanism com-pared with the RL-Alliance mechanism, and the Concurrent Q-Learning mechanism compared with the L-Alliance mechanism
Lastly, we consider the application of the Advice Exchange mechanism to the RL-Alliance mechanism. Fig. 8 shows the team performance metrics. The Advice Exchange mechanism is expected to enhance team performance through the exchange of utility values between robots.

Fig. 8 illustrates the team learning performance using RL-Alliance algorithm with and without the Advice Exchange mechanism, in terms of Simulation Time and Total Effort. Using the Advice
Exchange mechanism reduces the mean expectation value for both the Simulation Time and Total Effort metrics. Since less experienced robots can utilize behaviours from higher quality robots, the con-vergence curve is smoothed and minimized. Comparing the stan-dard deviations of the two performances, shown in Fig. 9 ,also demonstrates an improvement in the team learning behaviour by using the Advice Exchange mechanism. When the Advice Exchange mechanism is utilized the RL-Alliance mechanism experiences reduced standard deviation.
 For the minimization of Total Effort, the RL-Alliance approach with Advice Exchange seems to be the most ef fi cient choice. Depending on the application objectives, different CMDP approaches may be appro-representation is constrained, a CMDP model may signi fi cantly out-perform a Single Q-Learning method, and tend to settle toward better performance. 6. Conclusion The CISL approaches consider the importance of concurrency in MDP designs when formulating a policy regression approach. It was shown in this paper that a CMDP model for decision making can be more ef fi cient than a decentralized MDP with high dimensionality.
Several CISL algorithms were developed and discussed theoretically based on the CMDP modelling approach at the three layers of collective, cooperative and collaborative team learning. Through a heterogeneous foraging scenario the performance of these algo-rithms was demonstrated and contrasted with that of approaches based on a decentralized MDP model. It was illustrated that in terms of both Simulation Time and Total Effort the CISL approaches have a superior performance when it comes to heterogeneous team learn-ing. The CISL learning algorithms themselves perform differently with respect to different metrics. For example, the RL-Alliance algorithm seems to exhibit a better performance in terms of Total
Effort, whereas the Concurrent Q-Learning algorithm shows better results with respect to Simulation Time. At a collaborative level, it was also shown that an advice-sharing mechanism can marginally enhance the team learning performance, given proper adjustments in agent  X  sself-con fi dence to ensure that the mechanism will not interfere with Pareto-optimal task selection in the team. More investigations are needed for optimizing the effects of advice-sharing in a team and also for improving the quality of advice as the agents gain more experience, which will be a future direction of this research. As well, the developed CISL approaches should be examined through other benchmark scenarios.
 References
