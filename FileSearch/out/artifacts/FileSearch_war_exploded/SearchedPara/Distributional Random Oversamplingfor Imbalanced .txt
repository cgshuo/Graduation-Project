 The accuracy of many classification algorithms is known to suffer when the data are imbalanced (i.e., when the distribu-tion of the examples across the classes is severely skewed). Many applications of binary text classification are of this type, with the positive examples of the class of interest far outnumbered by the negative examples. Oversampling (i.e., generating synthetic training examples of the minority class) is an often used strategy to counter this problem. We present a new oversampling method specifically designed for classi-fying data (such as text) for which the distributional hy-pothesis holds, according to which the meaning of a feature is somehow determined by its distribution in large corpora of data. Our Distributional Random Oversampling method generates new random minority-class synthetic documents by exploiting the distributional properties of the terms in the collection. We discuss results we have obtained on the Reuters-21578, OHSUMED-S, and RCV1-v2 datasets. Many applications of binary text classification exhibit se-vere data imbalance , i.e., are characterized by sets of data in which the examples of one class are far outnumbered by the examples of the other. Such cases are especially frequent in information retrieval and related tasks, where the binary distinction to be captured is between a class of interest and  X  X he rest X , i.e., between the (typically few) documents rele-vant to a certain concept (e.g., as expressed by a query) and the (typically many) documents unrelated to it. This phe-nomenon is exacerbated in applications of multi-label multi-class (MLMC) text classification, i.e., applications where, given a set C = { c 1 ,...,c |C| } of classes, each document may be labelled by several classes at the same time 1 . In these applications the average prevalence (i.e., relative frequency) of a class is low, since C typically exhibits a power-law be-
MLMC classification is typically solved by training |C| in-dependent binary classifiers, one for each class of interest. documents and F = { t 1 ,...,t | F | } its vocabulary. We use W | Tr | X | F | to denote the document-term matrix, where w ij  X  R is the weight of term t j in document d i as computed by a weighting function. By representation of document d i .

We present a general framework for oversampling, that we dub Latent Space Oversampling (LSO); our Distributional Random Oversampling method will be a specific instantia-tion of it. In LSO we oversample minority-class documents by extending the original feature space F with an additional latent space L . Each new synthetic example o k for a doc-ument d i will be expressed as where nal feature space (i.e., a copy of the i -th row of W ), and # v k  X  R | L | is the variable part in the latent space L , which is generated by some stochastic function.

The vector expansion involves a two-step process for each document d i , i.e., (i) the estimation of model parameters  X  i for d i via a parameter estimation criterion  X ( W,d i ), such that  X  i  X   X ( W,d i ) is calculated only once for each example d ; and (ii) the generation of the variable part obtained by means of a generation function G . This func-tion is called several times for each minority-class example until the desired level of balance is reached, and exactly once for each majority-class example, since we neither oversample nor undersample majority-class examples. The oversampled matrix is then re-weighted (e.g., in order to bring to bear updated idf values, and in order to perform correct length normalization) before training the classifier. Each test docu-ment d t is also expanded to the enlarged vector space before being fed to the classifier; the only difference with the expan-sion process we carry out for training documents is that any global knowledge involved in the estimation of parameters  X  t comes from the training data.

Different oversampling strategies could thus be defined by considering different parameter estimation criteria  X  and different generation functions G . In the following sections we first illustrate one possible such strategy, based on proba-bilistic topic models (Section 2.1); we then present our DRO method based on the distributional hypothesis (Section 2.2). One possible instantiation of the LSO framework is what we will here call Latent Dirichlet Oversampling (LDO). LDO relies on Latent Dirichlet Allocation (LDA  X  [1]), a prob-abilistic topic model that assumes, in order to define the model parameters and the generative function, that each (observed) document in a collection is generated by a mix-ture of (unobserved) topics. As the weight w ij we here take the raw number of occurrences of term t j in document d i .
As the parameter estimation criterion  X  LDO we may choose any Bayesian inference method (such as Variational Bayes or Gibbs Sampling). The document-specific model parame-ters are  X  i = [  X  i ;  X  ], where  X  i is the topic distribution of d i and  X  is the per-topic word distribution obtained from Tr .
We will choose a generation function G LDO that returns a vectorial representation of a bag of n words, each of which is drawn by first choosing a topic z k  X  Multinomial (  X  i ), and then choosing a term t j  X  Multinomial (  X  z k ). We set n = length ( d i ) (i.e., to the total number of word occurrences in d ) so that the synthetic bag of words will allocate the same number of term occurrences as the original document (thus preserving sparsity in the new space). Note that, in this case, the latent space is mirroring the original feature space, with several classes at the same time, which gives rise to |C| bi-nary classification problems, with C the set of classes in the dataset. For Reuters-21578 3 we use the standard ( X  X odApt  X e X ) split, which identifies 9,603 training documents and 3,299 test documents. We restrict our attention to the 115 classes with at least one positive training example. OHSUMED-S [4] consists instead of 12,358 training and 3,652 test MED-LINE textual records from 1987 to 1991, classified according to 97 MeSH index terms. RCV1-v2 4 comprises 804,414 news stories generated by Reuters from Aug 20, 1996, to Aug 19, 1997. In our experiments we use the entire training set, con-taining all 23,149 news stories written in Aug 1996; for the test set we pick the 60,074 news stories written during Sep 1996. We restrict our attention to the 101 classes with at least one positive training example.

As the evaluation measures we use microaveraged F 1 ( F  X  1 ) and macroaveraged F 1 ( F M 1 ).

We compare the performance of LDO 5 and DRO with the following baselines: (i) Random Oversampling (RO), a method that performs oversampling by simply duplicat-ing random minority-class examples; (ii) Synthetic Minor-ity Oversampling Technique (SMOTE  X  [2]), a method that generates new synthetic minority-class examples as convex linear combinations of the document d i being sampled and a document randomly picked among the k minority-class near-est neighbours of d i (typically using k = 5); (iii) Borderline-SMOTE (BSMOTE  X  [5]), a more recent version of SMOTE that only oversamples those borderline minority-class ex-amples that would be misclassified as negatives by a k -NN classifier; (iv) DECOM [3], a probabilistic topic model that assumes all documents belonging to the same class to follow the same topic distribution that, once determined, is used to oversample minority-class examples following the LDA gen-eration procedure 6 ; (v) a bag-of-words model (BoW) where no oversampling is performed. For LDA-based methods we follow the related literature and set the number of topics to 30; in order to favour convergence, i.e., to allow the system to find a stationary point for the distribution parameters  X  that maximize the posterior probability of the corpus, we set the number of iterations to 3,000 and perform 10 passes over the corpus in each iteration.
 As the learner of our experiments we adopt linear-kernel SVMs (in the popular SVM-light implementation 7 ); in all our experiments we use the default SVM-light parameters. All methods are fed with the same preprocessed version of the datasets where, for each distinct binary decision prob-lem, the top 10% most informative words have been selected, using mutual information as the selection function and tfidf as the weighting function. We perform oversampling of the minority class until a desired prevalence  X  for the minority-We do not consider undersampling in this paper, i.e., all negative examples are picked exactly once. The results we present are all averages across 5 random trials we have run for each setting. For each dataset we partition the classes
Available from http://bit.ly/1F8AFcO
Available from http://1.usa.gov/1mp7RGr
For LDO we used the Gensim implementation of LDA (see http://bit.ly/1Rl7pFV) which also allows estimating the document-topic distribution of test examples.
For this method, as suggested in [3], we used the MATLAB implementation of Gibbs sampling available at http://bit. ly/1Rl7DNl http://svmlight.joachims.org/ fairly small. This superiority is more pronounced for the VLP classes, where DRO obtained 23 out of 24 best results, almost always with very large margins. In the HP classes, instead, our results do not reveal any clear winner, since the best results are haphazardly distributed among all of the baselines. Moreover, the best system is not substantially better to BoW in the vast majority of cases, which makes the idea of oversampling such classes questionable.
In sum, the results seem to indicate that the smaller the prevalence of the minority class is, the higher is the gain that can be obtained due to the use of DRO. This is an ap-pealing feature for an oversampling method. We attribute this behaviour to DRO X  X  distributional nature, which en-ables the information of the entire collection to contribute in the generation of each synthetic example (whereas RO and SMOTE-based methods are limited to local informa-tion provided by one or two examples, respectively). This could be advantageous for ill-defined classes (as those be-longing to LP and VLP). It may instead introduce noise, or even some redundancy, for well-defined ones (i.e., those in HP); this suggests that the best policy may be that of applying DRO to low-or very-low prevalence classes only, while leaving high-prevalence classes untouched. We have presented a new oversampling method for imbal-anced text classification, based on the idea of assigning a probabilistic generative function to each minority-class doc-ument in the training set, a function that can be iteratively queried until the desired level of balance is reached. This probabilistic function is built upon distributional represen-tations of the words contained in the document being over-sampled, which allows the model to introduce some random variability in the new examples while preserving the under-lying semantic properties motivated by the distributional hypothesis.

