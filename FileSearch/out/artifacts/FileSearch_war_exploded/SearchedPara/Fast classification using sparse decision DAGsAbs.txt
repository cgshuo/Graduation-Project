 Djalel Benbouzid 1 djalel.benbouzid@gmail.com R  X obert Busa-Fekete 1 , 2 busarobi@gmail.com Bal  X azs K  X egl 1 balazs.kegl@gmail.com
LAL/LRI, University of Paris-Sud, CNRS, 91898 Orsay, France v  X ertan  X uk tere 1., H-6720 Szeged, Hungary There are numerous applications where the computa-tional requirements of classifying a test instance are as important as the performance of the classifier it-self. Object detection in images ( Viola &amp; Jones , 2004 ) and web page ranking ( Chapelle &amp; Chang , 2011 ) are well-known examples. A more recent application do-main with similar requirements is trigger design in high energy physics ( Gligorov , 2011 ). Most of these applications come with another common feature: the negative class (usually called noise or background) sometimes has orders of magnitudes higher probabil-ity than the positive class. Besides the testing time constraints, this also makes training difficult: tradi-tional classification-error-based measures are not ade-quate, and using prior class probabilities in construct-ing training samples leads to either enormous data sizes or little representativity of the positive class. A common solution to these problems is to design cas-cade classifiers ( Viola &amp; Jones , 2004 ). A cascade clas-sifier consists of stages . In each stage a binary clas-sifier attempts to eliminate background instances by classifying them negatively. Positive classification in inner stages sends the instance to the next stage, so detection can only be made in the last stage. By using simple and fast classifiers in the first stages,  X  X asy X  background instances can be rejected fast, shorten-ing the expected testing time. The cascade structure also allows us to use different training sets in differ-ent stages, having more difficult background samples in later stages.
 Cascade classifiers, however, have many disadvantages in both the training and test phases. The training pro-cess requires a lot of hand-tuning of control parame-ters, and it is non-trivial how to handle the trade-off between the performance and the complexity of the cascade. Also, each individual stage needs to be trained with examples that have been classified posi-tively by all the previous stages, which becomes dif-ficult to satisfy in the later stages. Moreover, during test time, the cascade structure itself has several draw-backs. First, for a given stage, the margin information of a test example is lost and is not exploited in the subsequent stages. Second, all the positive instances have to pass through all the stages for a correct clas-sification. Finally, extending the cascade architecture to the multi-class case is non-trivial. For example in web page ranking, it is just as crucial to make a fast prediction on the relevance of a web page to a query as in object detection ( Chapelle et al. , 2011a ), but unlike in object detection, human annotation often provides more than two relevance levels.
 In this paper we propose a method intended to over-come these problems. In our setup we assume that we are given a sequence of low-complexity, possibly multi-class base classifiers (or features) sorted by importance or quality. Our strategy is to design a controller or a decision maker which decides which base classifiers should be evaluated for a given instance. The con-troller makes its decision sequentially based on the out-put of the base classifiers evaluated so far. It has three possibilities in each step: 1) it can decide to continue the classification by evaluating the next classifier, 2) skip a classifier by jumping over it, or 3) quit and use the current combined classifier. The goal of the con-troller is to achieve a good performance with as few base classifier evaluations as possible. This flexible setup can accommodate any performance evaluation metric and an arbitrary computational cost function. Designing the controller can be naturally cast into a Markov decision process (MDP) framework where the roles are the following: the policy is the controller, the index of the base classifier and the output of classi-fier constitute the states, the alternatives correspond to the actions, and the rewards are defined based on the target metric and the cost of evaluating the base classifiers.
 Our approach has several advantages over cascades. First, we can eliminate stages. Similar to SoftCas-cade ( Bourdev &amp; Brandt , 2005 ), the base classifiers do not have to be organized into a small number of stages before or while learning the cascade. Sec-ond, we can easily control the trade-off between the average number of evaluated base classifiers and the quality of the classification by combining these two competing goals into an appropriate reward. The form of the reward can also easily accommodate cost-sensitivity ( Saberian &amp; Vasconcelos , 2010 ) of the base classifiers although we will not investigate this is-sue here. The fact that some base classifiers can be skipped has an important consequence: the resulting classifier is sparse , moreover, the number and identities of base classifiers depend on the particular instances. Third, eliminating stages allows each instance to  X  X e-cide X  its own path within the list of base-classifiers. Theoretically, we could have as many different paths as training instances, but, a-posteriori, we observe clus-tering in the  X  X ath-space X . Fourth, eliminating stages also greatly simplifies the design. Our algorithm is basically turn-key: it comes with an important design parameter (the trade-off coefficient between accuracy and speed) and a couple of technical hyperparame-ters of the MDP algorithm that can be kept constant across the benchmark problems we use. Finally, the multi-class extension of the technique is quite straight-forward.
 Allowing skipping is an important feature of the al-gorithm. The result of this design choice is that the structure of the learned classifier is not a cascade, but a more general directed acyclic graph or a decision DAG . In fact, the main reason for sticking to the cascade design is that it is easy to control with semi-manual heuristics. Once the construction is automatic, keep-ing the cascade architecture is no longer a necessary constraint. Allowing skipping is also a crucial differ-ence compared to the approach of ( P  X oczos et al. , 2009 ) who also proposed to learn a cascade in an MDP setup. While their policy simply designs optimal thresholds in stages of a classical cascade, MDDAG outputs a classifier with a different structure. Our method can also be related to the sequential classifier design of ( Dulac-Arnold et al. , 2011 ). In their approach the ac-tion space is much larger: at any state the controler can decide to jump to any of the base classifiers, and so the action space grows with the number of base learners. Whereas this design choice makes feature se-lection more flexible, it also generates a harder learning problem for the MDP.
 The paper is organized as follows. In Section 2 we describe the algorithm, then in Section 3 we present our experimental results. In Section 4 we discuss the algorithm and its connection with existing methods, and in Section 5 we draw some pertinent conclusions. We will assume that we are given a sequence of N base classifiers H = ( h 1 ,..., h N ). Although in most cases cascades are built for binary classification, we will describe the method for the more general multi-class case, which means that h j : X X  R K , where X is the input space and K is the number of classes. The semantics of h is that, given an observation x  X  X  , it votes for class ` if its ` th element h ` ( x ) is positive, and votes against class ` if h ` ( x ) is negative. The absolute value | h ` ( x ) | can be interpreted as the confidence of the vote. This assumption is naturally satisfied by the output of AdaBoost.MH ( Schapire &amp; Singer , 1999 ), but in principle any algorithm that builds its final clas-sifier as a linear combination of simpler functions can be used to provide H . In the case of AdaBoost.MH or multi-class neural networks, the final (or strong or averaged ) classifier defined by the full sequence H is f ( x ) = P N j =1 h j ( x ), and its prediction for the class in-dex of x is b ` = arg max ` f ` ( x ) . In binary detection , f is usually used as a scoring function. The observation x is classified as positive if f 1 ( x ) =  X  f 2 ( x ) &gt; X  and background otherwise. The threshold  X  is a free pa-rameter that can be tuned to achieve, for instance, a given false positive rate.
 The goal of the MDDAG (Markov decision direct acyclic graph) algorithm is to build a sparse final clas-sifier from H that does not use all the base classifiers, and which selects them in a way depending on the in-stance x to be classified. For a given observation x , we process the base classifiers in their original order. For each base classifier h j , we choose from among three possible actions: 1) we Eval uate h j and continue, 2) we Skip h j and continue, or 3) we Quit and return the classifier built so far. Let b j ( x ) = 1  X  I { a j = Skip  X  X  X  j 0 &lt;j : a j 0 = Quit } (1) be the indicator that h j is evaluated on x , where a j  X  {
Eval , Skip , Quit } is the action taken at step j and the indicator function I { A } is 1 if its argument A is true and 0 otherwise. Then the final classifier built by the procedure is The decision on action a j will be made based on the index of the base classifier j and the output vector of the classifier built up to step j . 1 Formally, a j =  X  ( s j ( x ) , where s j ( x ) = f is the state we are in before visiting h j , and  X  is a policy that determines the action in state s j . The initial state s 1 is the zero vector with K + 1 elements.
 This setup formally defines a Markov decision process (MDP). An MDP is a 4-tuple M = ( S , A , P , R ), where S is the (possibly infinite) state space and A is the countable set of actions. P : S X S X A X  [0 , 1] is the transition probability kernel which defines the random plying the action a ( t ) , and R : R  X S X A X  [0 , 1] reward r ( t ) for each state-action pair. A deterministic policy  X  assigns an action to each state  X  : S X  X  . We will only use undiscounted and episodic MDPs where the policy  X  is evaluated using the expected sum of rewards with a finite horizon T . In the episodic setup we also have an initial state ( s 1 in our case) and a terminal state s  X  which is impossible to leave. In our setup, action Quit brings the process to the terminal state s  X  . Note that in s ( T ) only the Quit action is allowed. 2.1. The rewards As our primary goal is to achieve a good performance in terms of the evaluation metric of interest, we will penalize the error of f ( t ) when the action a ( t ) = Quit is applied. The setup can handle any loss function. Here, we will use the multi-class 0-1 loss function and the multi-class exponential loss function where the training observations ( x ,` )  X  R d  X  { 1 ,...,K } are drawn from a distribution D . Note that in the binary case, L I and L exp recover the classical bi-nary notions.
 With these notations, the reward for the Quit action comes from the distribution R ( r | s ( t ) , Quit ) = From now on we will refer to our algorithm as MDDAG. I or MDDAG.EXP when we use 0-1 loss or exponential loss, respectively. In principle, any of the usual convex upper bounds (e.g., logistic, hinge, quadratic) could be used in the MDP framework. The exponential loss function was inspired by the setup of AdaBoost ( Freund &amp; Schapire , 1997 ; Schapire &amp; Singer , 1999 ).
 To encourage sparsity, we will also penalize each eval-uated base classifier h by a uniform fixed negative re-ward where  X  is the Dirac delta and  X  is a hyperparame-ter that represents the accuracy-speed trade-off. Note that, again, this flexible setup can accommodate any cost function penalizing the evaluation of base classi-fiers. Finally, choosing the Skip action does not incur any reward, so R ( r | s , Skip ) =  X  (0).
 The goal of reinforcement learning (RL) in our case is to learn a policy which maximizes the expected sum of rewards ( 5 ). Since in our setup, the transition P is deterministic given the observation x , the expectation in ( 5 ) is taken with respect to the random input point ( x ,` ). This means that the global objective of the MDP is to minimize 2.2. Learning the policy There are several efficient algorithms available for learning the policy  X  using an iid sample D = 1998 ). When P and R are unknown, model-free methods are commonly used for learning the policy  X  . These methods directly learn a value function (the expected reward in a state or for a state-action pair) and derive a policy from it. Among model-free RL algorithms, temporal-difference (TD) learning al-gorithms are the most widely used. They can be di-vided into two groups: off-policy and on-policy meth-ods. In the case of off-policy methods the policy search method learns about one policy while following an-other, whereas in the on-policy case the policy search algorithm seeks to improve the current policy by main-taining sufficient exploration. On-policy methods have an appealing practical advantage: they usually con-verge faster to the optimal policy than off-policy meth-ods.
 We shall use the SARSA (  X  ) algorithm ( Rummery &amp; Niranjan , 1994 ) with replacing traces to learn the policy  X  . For more details, we refer the reader to ( Szepesv  X ari , 2010 ). SARSA (  X  ) is an on-policy method, so to make sure that all policies can be vis-ited with nonzero probability, we use an -greedy ex-ploration strategy. To be precise, we apply SARSA in an episodic setup: we use a random training instance x from D per episode. The instance follows the current policy with probability 1  X  and chooses a random action with probability . The instance observes the immediate rewards defined based on some loss func-tion, or ( 7 ) after each action. The policy is updated during the episode according to SARSA (  X  ).
 In our experiments we used AdaBoost.MH 2 to ob-tain a pool of weak classifiers H , and the RL Tool-box 2.0 3 for training the MDDAG . We ran Ad-aBoost.MH for N = 1000 iterations, and then trained SARSA (  X  ) on the same training set. The hyperparameters of SARSA (  X  ) were kept constant throughout the experiments. We set  X  to 0 . 95. In principle, the learning rate should decrease to 0, but we found that this setting forced the algorithm to con-verge too fast to suboptimal solutions. Instead we set the learning rate to a constant 0 . 2, we evaluated the current policy after every 10000 episodes, and we se-lected the best policy based on their performance also on the training set (overfitting the MDP was a non-issue). The exploration term was decreased gradually as 0 . 3  X  1 / d 10000  X  e , where  X  is the number of training episodes. We trained SARSA (  X  ) for 10 6 episodes. As a final remark, note that maximizing ( 8 ) over the data set D is equivalent to minimizing a margin-based loss with an L 0 constraint. If r I ( 6 ) is used as a reward, the loss is also non-convex, but minimizing a loss with an L 0 constraint is NP-hard even if the loss is con-vex ( Davis et al. , 1997 ). So, what we are aiming at is an MDP-based heuristic to solve an NP-hard problem, something that is not without precedent ( Ejov et al. , 2004 ). This equivalence implies that even though the algorithm would converge in the ideal case (with a de-creasing learning rate), in principle, convergence can be exponentially slow in n . In practice, however, we had no problem finding good policies in reasonable training time. In Section 3.1 we first verify the sparsity and hetero-geneity hypotheses on a synthetic toy example. In Section 3.2 , we compare MDDAG with state-of-the-art cascade detectors on three object detection bench-marks. After, in Section 3.3 we show how the multi-class version of MDDAG performs on a benchmark web page ranking problem. 3.1. Synthetic data The aim of this experiment was to verify whether MDDAG can learn the subset of  X  X seful X  base clas-sifiers in a data-dependent way. We created a two-dimensional binary dataset with real-valued features where the positive class was composed of two easily separable clusters (see Figure 1 (a)). This is a typical case where AdaBoost or a traditional cascade is sub-optimal since they both have to use all the base classi-fiers for all the positive instances ( Bourdev &amp; Brandt , 2005 ).
 We ran MDDAG. I with  X  = 0 . 01 on the 1000 decision stumps learned by AdaBoost.MH . In Figure 1 (b), we plot the number of base classifiers used for each individual positive instance as a function of the two-dimensional instance itself. As expected, the  X  X asier X  the instance, the smaller the number of base classifiers are needed for classification. Figure 1 (c) confirms our second hypothesis: base classifiers are used selectively, depending on whether the positive instance is in the blue or red cluster.
 The lower panel of Figure 1 shows a graphical represen-tation of the MDDAG classifier f acting on a data set D . The nodes of the directed acyclic graph (DAG) are the base classifiers in H . Each observation ( x ,` )  X  X  determines a set of edges In other words, we take all the base classifiers that are evaluated on the instance ( x ,` ) and connect the nodes representing these base classifiers with a direct edge. The edge set U x is called the classification path of x which constitutes a directed path by definition. The DAG we plot in Figure 1 (d) includes all of the edges U = S ( x , 1)  X  X  U x generated by the positive in-stances taken from the training data D . The width of an edge ( j,j 0 ) is proportional to its multiplicity # { x : ( j,j 0 )  X  U x , ( x , 1)  X  X } . The color of an edge ( j,j 0 ) represents the proportion of observations taken from the blue and red sub-classes, whose classification path includes ( j,j 0 ). Similarly, the size of the node is proportional to # { x : b j ( x ) = 1 , ( x , 1)  X  X } , and the color of the nodes represent sub-class proportions. The structure of the DAG also agrees with our orig-inal intuition, namely that the bulk of the two sub-classes are separated early and follow different classifi-cation paths. It is also worth noting that even though the number of possible classification paths is exponen-tially large, the number of realized paths is quite small. Some  X  X oisy X  points along the main diagonal (border between the subclasses) generate rare subpaths, but the bulk of the data mostly follows two paths. 3.2. Binary detection benchmarks In these experiments we applied MDDAG on three image data sets often used for benchmarking object de-tection cascades. VJ ( Viola &amp; Jones , 2004 ) and CBCL are face recognition benchmarks, and DPED ( Munder &amp; Gavrila , 2006 ) is a pedestrian recognition data set. We divided the data sets into training and test sets. We compared MDDAG to three state-of-the-art ob-ject detection algorithms (the original Viola-Jones cascade VJCascade ( Viola &amp; Jones , 2004 ), FC-Boost ( Saberian &amp; Vasconcelos , 2010 ), and Soft-Cascade ( Bourdev &amp; Brandt , 2005 )). VJCascade builds the cascade stage-by-stage by running Ad-aBoost in each stage. It stops adding base clas-sifiers to the m th stage when the false positive rate (FPR) falls below p m fpr and true positive rate (TPR) exceeds p m tpr , where p tpr and p ftr are hyperparam-eters of the algorithm. The total number of stages is also a hyperparameter. FCBoost also adds base classifiers iteratively to the cascade, but the base clas-sifier can be inserted into any of the stages. The goal is to minimize a global criterion which, similarly to ( 8 ), is composed of a performance-based term and a complexity-based term. The number of iterations and the parameter  X  that determines the trade-off between the two competing objectives are hyperparameters of the algorithm. SoftCascade , like MDDAG , builds a cascade on the output of AdaBoost , where each stage consists of exactly one base classifier. The final number of base classifiers is decided beforehand by the hyperparameter N . In each iteration j , the base clas-sifier with the highest balanced edge is selected, and the detection threshold  X  j is set to achieve a TPR of 1  X  exp (  X j/N  X   X  I {  X &lt; 0 } ), where  X  is a second hy-perparameter of the algorithm. Both the TPR and the number of base classifiers increase with  X  , so the choice of  X  influences the speed/accuracy trade-off (although not as explicitly as our  X  or FCBoost  X  X   X  ). Comparing test-time-constrained detection algorithms is quite difficult. The usual trade-off between the false positive rate (FPR) and the true positive rate (TPR) can be captured by ROC curves, but here we also have to take into account the computational effi-ciency of the detector. In ( Bourdev &amp; Brandt , 2005 ) this problem is solved by displaying three-dimensional FPR/TPR/number-of-features surfaces. Here, we de-cided to show two-dimensional slices of these surfaces: we fix the FPR to reasonable values and plot the TPR against the detection time. In each of the algorithms we used Haar features as base classifiers, so the de-tection time can be uniformly measured in terms of the average number of base classifiers needed for de-tection. In typical detection problems the number of background (negative) instances is orders of magni-tudes higher than the number of signal (positive) in-stances, so we computed this average only over the negative test set. It turns out that using this measure, vanilla AdaBoost is fairly competitive with tailor-made cascade detectors, so we also included it in the comparison.
 Computing the TPR versus number-of-features curve at a fixed FPR cannot be done in a generic algorithm-independent way. For AdaBoost , in each iteration j (that is, for each number j of base classifiers) we tune the detection threshold  X  to achieve the given test FPR, and plot the achieved test TPR versus j . In the other three algorithms we have 2-3 hyperparam-eters that explicitly or implicitly influence the aver-age number of base classifiers and the overall perfor-mance. We ran the algorithms using different hyper-parameter combinations. In each run we set the detec-tion threshold  X  to achieve the given test FPR. With this threshold, each run k determines a TPR/average-number-of-features pair ( p k ,N k ) on the training set and ( p 0 k ,N 0 k ) on the test set. For each N , we find the run k  X  ( N ) = arg max k : N training p k using at most N base classifiers, and plot the test TPR p 0 k  X  ( N ) versus N . Although overfitting is not an issue here (the complexity of the classifiers is relatively low), this setup is important for a fair com-parison. If overfitting were an issue, the optimization could also be carried out on a validation set, indepen-dent of both the test and the training sets. Optimizing the TPR on the training set and plotting it on the test set also explains why the curves are non-monotonic. Figure 2 shows the results we obtained. Although the differences are quite small, MDDAG outper-forms the three benchmarks algorithms consistently in the regime of low number base classifiers, and it is competitive with them over the full range. MDDAG. I is slightly better at low complexities, whereas MDDAG.EXP is more effective at a slightly higher number of base classifiers. This is not sur-prising as in the low complexity regime the 0-1 error is more aggressive and closer to the measured TPR, whereas when most of the instances are classified cor-rectly, without the margin information it is impossible to improve the policy any further. 3.3. Ranking with multi-class DAGs Although object detection is arguably the best-known test-time-constrained problem, it is far from being unique. In web page ranking, the problem is simi-lar: training time can be almost unlimited, but the learned ranker must be fast to execute. State-of-the-art techniques often use thousands of trained models in an ensemble setup ( Chapelle et al. , 2011b ), so ex-tracting lean rankers from the full models is an impor-tant practical issue. One of the difficulties in this case is that relevance labels may be non-binary, so classi-cal object-detection cascades cannot be applied. At the same time, the principles used to design cascades re-surface also in this domain, although the setup is rather new and the algorithms require a fair amount of manual tuning ( Cambazoglu et al. , 2010 ). Despite this, MDDAG can be used for this task as is . To evaluate MDDAG on a multi-class classi-fication/ranking problem, we present results on the MQ2007 and MQ2008 data sets taken from LETOR 4.0 . In web page ranking, observations come in the form of query-document pairs, and the perfor-mance of the ranker is evaluated using tailor-made loss or gain functions that take as input the order-ing of all the documents given a query. To train a ranker, query-document pairs come with manually an-notated relevance labels that are usually multi-valued ( { 0 , 1 , 2 } in our case). One common performance mea-sure is the Normalized Discounted Cumulative Gain (NDCG m ) ( J  X arvelin &amp; Kek  X al  X ainen , 2002 ) which is based on the first m documents in the order output by the ranker. We used the averaged NDCG score ndcg , provided by LETOR 4.0, that takes an average of the query-wise NDCG m values to evaluate the al-gorithms. In these experiments the base learners were decision trees with eight leaves.
 The goal of MDDAG is similar to the binary case, namely to achieve a comparable performance to Ad-aBoost.MH using fewer base learners. To make the comparison fair, we employed the same calibration method to convert the output of the multi-class clas-sifiers to a scoring function and then to a ranking ( Li et al. , 2007 ). Since the goal this time was not de-tection, we simply evaluated the average NDCG for each run k to obtain ( ndcg k ,N k ) on the training set and ( ndcg 0 k ,N 0 k ) on the test set. We then selected k ( N ) = arg max k : N ndcg 0 k  X  ( N ) against N . Figure 3 tells us that MDDAG performs as well as AdaBoost.MH with roughly two-fold savings in the number of base classifiers. Besides ( P  X oczos et al. , 2009 ) and ( Dulac-Arnold et al. , 2011 ), MDDAG has several close relatives in the fam-ily of supervised methods. It is obviously related to algorithms taken from the vast array of sparse meth-ods. The main advantage here is that the MDP setup allows one to achieve sparsity in a dynamical data-dependent way. This feature relates the technique to unsupervised sparse coding ( Lee et al. , 2007 ; Ranzato et al. , 2007 ) rather than to sparse classification or re-gression. On a more abstract level, MDDAG is also similar to ( Larochelle &amp; Hinton , 2010 ) X  X  approach to  X  X earn where to look X . Their goal is to find a sequence of two-dimensional features for classifying images in a data-dependent way, whereas we do a similar search in a one-dimensional ordered sequence of features. In this paper, we introduced an MDP-based design of decision DAGs. The output of the algorithm is a data-dependent sparse classifier, which means that ev-ery instance  X  X hooses X  the base classifiers or features that it needs to predict its class index. The algo-rithm is competitive with state-of-the-art cascade de-tectors on object detection benchmarks, and it is also directly applicable to test-time-constrained problems involving multi-class classification, such as web page ranking. However, in our view, the main benefit of the algorithm is not necessarily its performance, but its simplicity and versatility. First, MDDAG is ba-sically a turn-key procedure: it comes with one user-provided hyperparameter with a clear semantics of di-rectly determining the accuracy-speed trade-off. Sec-ond, MDDAG can be readily applied to problems dif-ferent from classification by redefining the rewards on the Quit and Eval actions. For example, one can easily design regression or cost-sensitive classification DAGs by using an appropriate reward in ( 6 ), or add a weighting to ( 7 ) if the features have different evalu-ation costs.
 This work was supported by the ANR-2010-COSI-002 grant of the French National Research Agency.
