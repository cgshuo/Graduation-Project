 We investigate the effectiveness of both the standard evalua -tion measures and the opinion component for topical opinion retrieval. We analyze how relevance is affected by opinions by perturbing relevance ranking by the outcomes of opinion-only classifiers built by Monte Carlo sampling. Topical opin -ion rankings are obtained by either re-ranking or filtering t he documents of a first-pass retrieval of topic relevance. The proposed approach establishes the correlation between the accuracy and the precision of the classifier and the perfor-mance of the topical opinion retrieval. Among other results , it is possible to assess the effectiveness of the opinion comp o-nent by comparing the effectiveness of the relevance baselin e with the topical opinion ranking.
 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Performance evaluation (efficiency and effectiveness) General Terms: Theory, Experimentation Keywords: Sentiment Analysis, Opinion Retrieval, Classi-fication
Opinion mining aims to classify sentences or documents by polarity of opinions. The application of opinion mining to IR (named Topical Opinion Retrieval) deals with ranking documents according to both topic relevance and opinion content. Topical Opinion Retrieval goes back to the novelty track of TREC 2003 [11] and the Blog tracks of TREC [7, 4, 8].However, there is not yet a comprehensive study of the interaction and the correlation between relevance and sent i-ment assessments. For example, the best runs based on the best official topic relevance baseline (baseline4) in the blo g track of TREC 2008 (short topics 1001-1050) [8] achieve the MAP R value equal to 0.4724, that drops to the MAP O | R of opinion equal to 0.4189, and to MAP equal to 0.1566 and 0.1329 for the polarity tasks (positive and negative opin-ionated rankings respectively). Performance degradation is intuitively expected because any variable which is additio nal to relevance, for example opinion, deteriorates system per -formance.
 There is no way to separate and evaluate the effective-Figure 1: MAP O | R and P@10 O | R by classifier accu-racy. Opinionated document filtering with Monte Carlo sampling from the five TREC official base-lines. ions with respect to relevance, P ( O|R ). Assuming that each document has an unknown topic as a hidden variable, O is a sample of opinionated documents of the whole collection, and the prior for opinionated but not relevant documents, i.e. P ( O| R ), is provided by P ( O|R ) (i.e. it is de facto postulated the independence between relevance and opinion content). The second step consists in constructing a Monte Carlo sampling of opinionated documents from test data, and in obtaining thus opinion-only classifiers with differen t accuracy k . The MAP O | R is averaged on rankings built with all classifiers with the same accuracy k . The relevance rank-ings are modified according to:  X  filtering , that is not opinionated documents are removed from the relevance baseline;  X  re-ranking , that is opinionated documents receive a  X  X e-ward X  in their relevance ranking.

Topical opinion retrieval by Monte Carlo sampling is eas-ily conducted with the filtering approach. Re-ranking ap-proach requires the combination of two different scores, re-lated to content and to opinion (e.g. [2, 9]). Opinion scores can be easily obtained with a lexicon-based approach [5, 10, 6, 3, 1]. The official relevance baselines and the topical opinion scores are provided by the blog TREC, while the opinion scores are not available, so that we use the lexicon-based scores and the re-ranking methodology that can be found in [1]. This lexicon-based approach has a good per-formance and achieves the MAP O | R of 0.4006 with respect the same baseline (baseline4 of the blog TREC 2008). Monte Carlo sampling consists in assigning the lexicon-based sco re to opinionated documents and a null score to non opinion-ated ones, assuming an accuracy 0  X  k  X  1 of the classifier.
To improve MAP O | R with respect to its baseline requires a high accuracy of the opinion-only classifiers (at least arou nd 80% with the filtering approach and 70% with the re-ranking technique of [1] as shown in Figures 1 and 2). However, smaller accuracy even close to that of the relevance baselin e improves P@10 more easily than for MAP. The best value of MAP O | R with the filtering approach achieves an empirical value around the MAP R of the relevance baseline. There is almost a linear correlation between the MAP O | R and the ac-
