 Tianqi Chen tqchen@apex.sjtu.edu.cn Shanghai Jiao Tong University, China Hang Li hangli.hl@huawei.com Huawei Noah X  X  Ark Lab, Hong Kong Qiang Yang qyang@cse.ust.hk Huawei Noah X  X  Ark Lab, Hong Kong Yong Yu yyu@apex.sjtu.edu.cn Shanghai Jiao Tong University, China Matrix factorization has been proved to be one of the most successful approaches to collaborative fil-tering. It assumes that the users X  preference matrix Y  X  R n  X  m can be factorized into a product of t-wo low rank matrices U  X  R d  X  n and V  X  R d  X  m as  X  Y ( i,j ) = U T i V j , and conducts collaborative filtering by exploiting U and V . In real world scenarios, many types of auxiliary information are available besides the matrix Y , and the use of them may enhance the predic-tion accuracy. For example, information such as time and location can help better predict users X  preferences based on the contexts, while information such as user-s X  ages and gender can help make better prediction on the basis of user profiles. From the viewpoint of learn-ing, the use of auxiliary information can overcome the sparsity of the matrix data.
 The use of auxiliary information in matrix factoriza-tion has been studied, various models (Agarwal &amp; Chen, 2009; Rendle et al., 2011; Weston et al., 2012) have been proposed. In these models, auxiliary in-formation is encoded as features and factorization of the matrix amounts to linearly mapping the feature vectors in the feature space into a latent space. The key question here is how to encode useful features to enhance the performance of the model. Ideally one would like to automatically construct  X  X ood X  feature functions in the learning process.
 To the best of our knowledge, little work has been done on automatic feature function construction in matrix factorization. In this paper, we try to tackle the chal-lenge by employing general functional matrix factor-ization and gradient boosting. In our method, feature functions are automatically constructed via searching in a functional space during the process of learning. The learning task is effectively and efficiently carried out with the gradient boosting algorithm. The con-tribution of the paper is as follows. (1) We propose a general formalization of functional matrix factoriza-tion. (2) We introduce an efficient algorithm to solve the optimization problem using gradient boosting. (3) We give two specific feature construction algorithms using time and demographic information. (4) We em-pirically demonstrate the effectiveness of the proposed method on three real-world datasets.
 The rest of the paper is organized as follows. Section 2 gives the general description of our model. Section 3 describes two specific algorithms for feature func-tion construction. Experimental result is provided in Section 4. Related work is introduced in Section 5. Finally, the paper is concluded in Section 6. 2.1. Model Matrix factorization with auxiliary information can be represented by the following equation, where the aux-iliary information is denoted as x . Note that for simplicity we always assume that there is an extra constant column in both U and V to model the bias effect. Further generalization can be consid-ered, which involves the sum of multiple factorizations. To simplify the discussion, we restrict our model to a single factorization. The latent factor model in Equa-tion (1) assumes that prediction  X  Y is a function of user i , item j and auxiliary information x in the cur-rent context. The matrix U is constructed by linear combination of features. The k th dimension of U is calculated by the following equation 1 The matrix V can be formalized similarly, while in many applications it seems to be sufficient to define one of them in a functional form (either U or V ). F is the set of candidate feature functions (space of functions) which is closed under scaling and can be infinite. The novelty of our model lies in that it in-troduces functions as model parameters, which allows us to construct  X  X ood X  feature functions f k,s  X  F in learning of the latent factor model. To simplify the notations, we will also use  X  Y ij for  X  Y ( i,j, x ), U ki U k ( i, x ), and V kj for V k ( j, x ).
 One very important factor in our method is the defi-nition of the functional space F . As shown in Fig. 1, we will give two examples of feature functions. The first one is a user-specific k-piece step function that captures users X  interest change over time; the second one is a user-independent decision tree to model users with their properties. The details will be introduced in Section 3.
 Relationship with Existing Models General functional matrix factorization contains ex-isting models as its special cases in which certain con-straints are imposed on the model. The model degen-erates to the conventional factorization models using auxiliary information, when the feature functions f k,s are predefined and fixed. The model becomes that of functional matrix factorization in (Zhou et al., 2011), when the feature functions in all the latent dimensions are assumed to be the same decision tree. 2.2. Training The objective function of our method is defined in the following equation where l is a differentiable convex loss function that measures the difference between the prediction  X  Y and Algorithm 1 GFMF Training repeat until converge the target Y , and the summation is over all the ob-served entries. The second term  X  measures the com-plexity of the model (i.e., the feature functions) to avoid over-fitting. With the objective function, a mod-el employing simple and useful feature functions will be selected as the best model. Because the model includes functions as parameters, we cannot directly use tra-ditional methods such as stochastic gradient descent to find the solution. We propose exploiting gradien-t boosting to tackle the challenge. More specifically, we construct the latent dimensions additively: in each step, we pick a latent dimension k and try to add a new feature function f to the dimension. During the step, search over the functional space F is performed to find the feature function f that optimizes the ob-jective function.
 For a general convex loss function l , the search of f can be hard. We approximate it by using the quadratic loss function to simplify the search objective. We define Suppose that we add a new feature f ( i, x ) to U k ( i, x ) while fixing the rest of parameters. The first part of the objective function can be approximated by Taylor expansion as: We use  X  L ( f )+ X ( f ) to find the best feature function f . The general procedure is shown in Algorithm 1, where is a shrinkage parameter to avoid over-fitting. Convergence of the Training The choice of h ij does not necessarily require that the second order gradient is utilized to approximate the loss function. The algorithm converges as long as the following inequality holds l ( Y ij ,  X  Y ij + X  y )  X  l ( Y ij ,  X  Y ij )+ g ij  X  y + Proof. Denoting the upper bound as  X  L ( f ), we have L ( f  X  ) +  X ( f  X  )  X   X  L ( f  X  ) +  X ( f  X  )  X   X  L (0) +  X (0) = L (0). Here we assume 0  X  X  and  X (0) = 0. Thus the objective function decreases at each step. 2 Time Complexity The time complexity for calculating the sufficient s-tatistics in the general algorithm is O ( kN ), if we buffer the predictions of observed pairs in the training data, where N represents the number of observed pairs in the matrix. Furthermore, the time complexity of the training algorithm depends on the specific setting of feature function learning. We will provide the time complexity of the specific learning algorithms in Sec-tion 3, which is of log-linear order.
 Integration with Existing Methods The training algorithm is an iterative optimization procedure like coordinate descent. This allows us to easily integrate our approach with existing methods for learning a latent factor model: to use our update algorithm for the feature function construction and to use stochastic gradient descent or coordinate descent to optimize the other parts of the model. 2.3. Segmentation Function Learning An important part of the training algorithm is to learn the feature function f in each step. In this section, we consider a general method for learning a class of fea-ture functions. It first segments the data into several parts using auxiliary information, and then learns an independent parameter in each segment The function p ( i, x ) defines the cluster index of an in-stance, and a weight  X  is assigned to each cluster. As-sume that there are | C | clusters and the instance set The complexity of feature function is defined as The first term represents the number of segments; the second term represents the norm of weights in each cluster. For this problem, the optimal  X  can be found analytically for a given segmentation as follows the value will be mainly determined by  X  when the second order term is small. Putting  X   X  back to  X  L ( f ), we obtain the objective function as  X  L ( f )+ X ( f ) =  X  L (0)  X  We can use Equation (10) to guide the search for a segmentation function. The basic motivation of the segmentation function is to make the latent dimensions of data in the same segments as similar as possible. One of the most widely used segmentation functions is decision tree (Friedman et al., 2001). However, for specific types of segmentation functions, we can use specific algorithms rather than decision tree. We will use the general methodology in this section to guide the feature function constructions in Section 3. 3.1. Time dependent Feature Function Time is very important auxiliary information in col-laborative filtering. One important reason is that the users X  preferences change over time. To model users X  local preferences, following the methodology in (Ko-ren, 2009), we can define an indicator function for each localized time bin as feature function; the model can be written as The idea is to learn a localized latent factor in each time bin to capture the time dependent preference of the user. The time bin size is fixed and needs to be predefined. In real world scenarios, different users may have different patterns of preference evolution. Some users may change their interest frequently, while oth-ers may like to stick to their interest. The patterns of changes for different topics may also differ; a us-er might have constant favor on classical music for a long time while changing her preference on pop music very often. These patterns may be hard to capture with a fixed size of time bins. We can utilize our mod-el to learn more flexible feature functions over time. Specifically, we define the user latent factor as where each f k,s,i ( t ) is a user-specific | C | -piece step function in t defined by segmentations on time. Fig. 2 gives an illustration of the model. Note that time bin indicator function can also be viewed as a spe-cial type of our time feature function. The difference is that our method decides the number of segments, changing points, and function values based on train-ing data, which offers more flexibility. Using the result in Section 2.3, the objective for discovering time seg-mentation can be written as follows The optimal solution of the objective can be found using a dynamic programming algorithm in quadratic time complexity. We use a faster greedy algorithm; it starts from all instances as independent segments, then greedily combines segments together when an im-provement of objective function can be made, until a local minimum is reached.
 Algorithm 2 Tree Split Finding with Missing Value Require: I : instance set, n p : number of properties gain  X  0 for p = 1 to n p do end for output split and default direction with max gain Time Complexity Let m i to be the number of observed entries in the i -th row. The time complexity of the greedy algorithm for user i is O ( m i log m i ), where the log-scale factor comes from the cost for maintenance of the priority queue. The overall training complexity for one itera-tion over the dataset is O ( kN log m ). As for the space complexity, addition of two step functions can be com-bined into a single step function, which allows us to store U k ( i,t ) efficiently and do not need to buffer U for each instance. 3.2. Demographic Feature Function Data sparsity is a major challenge for collaborative fil-tering. The problem becomes especially severe for han-dling new users, who may have only a few or even no records in the system. One important type of auxiliary information that can be used to deal with this prob-lem is user demographic data such as age, gender and occupation. The information can be relatively easily obtained and has strong indication to user preferences. For example, a young boy is more likely to favor ani-mation, and a student of computer science may have an interest in IT news. To learn useful feature func-tions from the user demographic data for latent factor model construction, we use x to represent the demo-graphic information, and define the latent factor to be where each f k,s ( x ) is a user-independent feature func-tion. We define as the feature function set F the set of regression trees over user properties x . Fig. 3 gives an intuitive explanation of the model. We place in the leaf nodes example values that are used to illustrate the structure of the trees, whereas in the real feature trees they are calculated using Equation (9). The ad-vantage of using trees as features is that it is possible to select useful attributes, create high-order combination-s of attributes, and automatically segment continuous attributes to construct meaningful features.
 Next, we describe the tree learning algorithm. The tree construction is also guided using Equation (10) in Section 2.3. In each round, we greedily enumerate the attributes and choose to make a split on the attribute that can give the maximum loss reduction. The pro-cess is repeated until some stopping condition is met (e.g maximum depth), and then a pruning process is carried out to merge the nodes with less loss reduction. One problem that we may face here is the existence of missing values. That is, not all the attributes are available for each user. To solve this problem, we de-cide a default direction (either left child or right child) for each node and adopt the default direction for clas-sification when a missing value is encountered. The splitting construction algorithm also enumerates de-fault directions to find the best split; the algorithm is shown in Algorithm 2 3 . Time Complexity The time complexity for one iteration over the dataset is O ( kDN p log( n ) + kN ). D is the maximum depth of the tree and N p is the number of observed entries in the user attribute matrix. We note that the com-putation only depends on the size of observed entries. We also note that the complexity for tree construction (first part) does not depend on the number of observed entries N . This is because we can make statistics for each user before tree construction and thus have the construction step only depend on the number of users. Similarly, we only need to buffer U for each user. 3.3. Advantage of Our Method Next, we will discuss why our method can enhance the prediction accuracy. As previously explained, user preference patterns are dynamic and hard to capture. Fig. 4 illustrates our method of feature construction. In the time dependent case, different users may have different interest changing patterns. Our method can effectively detect the dynamic changes in interest over some topics along the time axis. The regularization in our method will also lead to selection of a simple mod-el when no change occurs in user preference. In the user demographic case, our method can automatical-ly create high order features and build the tree based complex feature functions. The goal of optimization is to find the feature functions which are simple and have good fit to the data; automatically constructing high order features and automatically detecting change points make this possible and give our model the po-tential of achieving better results. 4.1. Dataset and Setup We use three real-world datasets in our experiments. The statistics of the datasets are summarized in Table 1. The first dataset is the Movielens dataset ml-1M, which contains about 1 million ratings of 3,952 movies by 6,040 users with timestamps. We use five-fold cross validation to evaluate the performance of our method and the baseline methods. The second dataset is the Yahoo! Music Track1 dataset 4 , which contains 253 million ratings by 1 million users over 624 thousand tracks from the Yahoo! Music website. The data is split into training, validation, and test sets by time. We use the official training and validation datasets to train the model and the test set to make evaluation. For the two datasets, we use RMSE (root mean square error) as evaluation measure. The first two datasets are utilized to evaluate the performance of our method with time dependent feature function construction. The third dataset we use is the Tencent Microblog 5 . It is one of the largest Twitter-like social media services in China. The dataset contains the recommendation records of 2.3 million users over a time period of about two months. The item set is defined as celebrities and information sources on Tencent Microblog. The dataset is split into training and testing data by time. Furthermore, the test set is split into public and pri-vate set for independent evaluations. We use the train-ing set to train the model and use the public test set to conduct evaluation. We also separate 1/5 data from the training data by time for parameter selection. The dataset is extremely sparse, with only on average two positive records per user. Furthermore, about 70% of users in the test set never occur in the training set. User demographic information (age and gender) and social network information is available in the dataset. For this dataset, we use MAP @ k (mean average preci-sion) as evaluation measure. The dataset is exploited to evaluate the performance our method with demo-graphic feature construction.
 In all the experiments, the parameter  X  of our model is heuristically set to 2. We use the validation set (in ml-1M via cross validation) to tune the meta parameters for the baseline models and our model. 4.2. Results on Movielen We compare six methods on the ml-1M dataset. The compared methods are as follows:  X  MF is the basic matrix factorization model that  X  TimeMF uses predefined time bins to model  X  GFMF is our model that learns time dependent  X  SVD++ is the matrix factorization model with  X  TimeSVD++ uses the same time feature func- X  GFMF++ utilizes the GFMF part to train a We train the baseline models and our model, when the latent dimensionality d is 8, 16, 32 and 64, and find that the results are consistent. We show the results in two parameter settings n Table 2. Both TimeMF and SVD++ give 0 . 004 reduction over MF in terms of RMSE. GFMF is able to give a further 0 . 003 re-duction of RMSE. Finally, the GFMF++ model that combines implicit feedback information into our model achieves the best result. We can see that our model brings more than 75% improvement compared with TimeMF , which uses predefined time bins. This is due to the fact that our method adapts the time bin size and time split points for each user and can capture users X  preference evolution more accurately. 4.3. Results on Yahoo! Music We also investigate the performance of our method on the Yahoo! Music dataset. We compare three methods on this dataset, with the same model definitions as in the previous section. For a time stamp that does not appear in the training set, we assign a constant feature to it, which is equal to the feature of the corresponding last time bin in the training set. We add user and item time bias terms to all the models to offset the bias effect and focus on modeling of latent dimensions. We compare the methods with different latent dimensions (ranging in 8, 16, 32, 64, 128), and obtain consistent results. We report the results for d = 64 and d = 128. The experimental results are shown in Table 3. The comparison shows GFMF++ gives 40% more RMSE reduction than TimeSVD++ over SVD++ . This in-dicates that our method can learn good feature func-tions in a large real-world dataset. 4.4. Results on Tencent Microblog We compare six methods on the Tencent Microblog dataset. We use logistic loss to train all the models. Due to the extreme sparsity of the dataset, the per-formance of matrix factorization is similar to the bias model and thus is not reported. Because this is a so-cial network dataset, it is interesting to see the impact of social information as well, and thus we also include social-aware models in the experiment.  X  item bias is the basic model that only includes a  X  DemoMF is a latent factor model that utilizes  X  GFMF is our model that learns demographic fea- X  SocialMF follows the approach of social-aware  X  SocialDemoMF and SocialGFMF are inte-We train the model, when the latent dimensionality d is 8, 16, 32, and 64. The results for all the models are optimal when d = 32, and thus we only report those results here, as shown in Table 4. From the results, we can first find that the bias effect is strong in this dataset. However, improving the performance is stil-l possible using social network and user demographic information. The performances of GFMF are consis-tently better than DemoMF . DemoMF improves up-on the bias model in terms of MAP @1 by about 1 . 5%, while GFMF is able to further improve the perfor-mance by about 0 . 4%. This means GFMF can learn good feature functions to utilize the user demographic information. The results of SocialMF and GFMF are similar, which means user demographic informa-tion is as important as social information in this task. Finally, SocialGFMF utilizing both the social and user demographic information achieves the best per-formance among all the methods. This work is concerned with matrix factoriza-tion (Salakhutdinov &amp; Mnih, 2008; Koren et al., 2009; Lawrence &amp; Urtasun, 2009), which is the among most popular approaches to collaborative filtering. There are many variants of factorization models to incor-porate various auxiliary information, including time information (Koren, 2009), attribute information (A-garwal &amp; Chen, 2009; Stern et al., 2009), context in-formation (Rendle et al., 2011), and content informa-tion (Weston et al., 2012). These models make use of predefined feature functions to encode the auxil-iary information. In (Zhou et al., 2011) a latent factor model is proposed for a different task of guiding us-er interview, which can be viewed as a special case of our model. Their method cannot be applied to solve the problems in our experiments because of its specific model formulation (e.g., it cannot handle time depen-dent features). Our method is unique in that it simul-taneously learns the feature functions and the latent factor model for CF which is, to our best knowledge, is the first work in the literature.
 From the perspective of infinite feature space, one re-lated general framework is (Abernethy et al., 2009), which implicitly defines features using kernel function-s. Our method allows explicit definition and automatic construction of features, and is scalable to deal with a large amount of data. Our method employs gra-dient boosting (Friedman, 2001), which is equivalent to performing coordinate descent in the functional s-pace. Gradient boosting has been successfully used in classification (Friedman et al., 2000) and learning to rank (Burges, 2010). To our knowledge, this is the first time it is used in matrix factorization. In this paper, we have proposed a novel method for general functional matrix factorization using gradien-t boosting. The proposed method can automatically search in a functional space to construct useful fea-ture functions based on data for accurate prediction. We also give two specific algorithms for feature func-tion construction on the basis of time and demographic information. Experimental results show that the pro-posed method outperforms the baseline methods on three real-world datasets. As future work, we plan to explore other feature function settings. We are also interested in implementation of the method in a dis-tributed environment to scale up to larger problems. Finally, it is also interesting to apply our method to other problems than collaborative filtering.
 We thank the anonymous reviewers for their very helpful reviews. Yong Yu is supported by grants from NSFC-RGC joint research project 60931160445. Qiang Yang is supported in part by Hong Kong CERG projects 621010, 621307, 621211, and NSFC-RGC project N HKUST624/09. We also appreciate the dis-cussions with Wei Fan, Zhengdong Lu, Weinan Zhang, and Fangwei Hu.
 Abernethy, J., Bach, F., Evgeniou, T., and Vert, J.P.
A new approach to collaborative filtering: Operator estimation with spectral regularization. Journal of Machine Learning Research , 10:803 X 826, 2009.
 Agarwal, D. and Chen, B.C. Regression-based la-tent factor models. In Proceedings of the 15th
ACM SIGKDD international conference on Knowl-edge discovery and data mining , KDD  X 09, pp. 19 X  28, New York, NY, USA, 2009. ACM.
 Burges, C. From ranknet to lambdarank to lamb-damart: An overview. Learning , 11:23 X 581, 2010. Friedman, J., Hastie, T., and Tibshirani, R. Additive l-ogistic regression: a statistical view of boosting. The annals of statistics , 28(2):337 X 407, 2000.
 Friedman, J., Hastie, T., and Tibshirani, R. The el-ements of statistical learning , volume 1. Springer Series in Statistics, 2001.
 Friedman, J.H. Greedy function approximation: a gra-dient boosting machine. Annals of Statistics , pp. 1189 X 1232, 2001.
 Jamali, M. and Ester, M. A matrix factorization tech-nique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender systems , RecSys  X 10, p-p. 135 X 142, New York, NY, USA, 2010. ACM.
 Koren, Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Pro-ceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data min-ing , KDD  X 08, pp. 426 X 434, New York, NY, USA, 2008. ACM.
 Koren, Y. Collaborative filtering with temporal dy-namics. In Proceedings of the 15th ACM SIGKD-
D international conference on Knowledge discovery and data mining , pp. 447 X 456, 2009.
 Koren, Y., Bell, R., and Volinsky, C. Matrix factor-ization techniques for recommender systems. Com-puter , 42, August 2009.
 Lawrence, N.D. and Urtasun, R. Non-linear matrix factorization with gaussian processes. In Proceed-ings of the 26th International Conference on Ma-chine Learning , pp. 601 X 608, Montreal, June 2009. Omnipress.
 Rendle, S., Gantner, Z., Freudenthaler, C., and
Schmidt-Thieme, L. Fast context-aware recommen-dations with factorization machines. In Proceedings of the 34th ACM SIGIR Conference on Reasearch and Development in Information Retrieval . ACM, 2011.
 Salakhutdinov, R. and Mnih, A. Probabilistic matrix factorization. In Advances in Neural Information Processing Systems 20 , pp. 1257 X 1264. MIT Press, Cambridge, MA, 2008.
 Stern, D.H., Herbrich, R., and Graepel, T. Match-box: large scale online bayesian recommendations.
In Proceedings of the 18th international conference on World wide web , WWW  X 09, pp. 111 X 120, New York, NY, USA, 2009. ACM.
 Weston, J., Wang, C., Weiss, R., and Berenzweig, A.
Latent collaborative retrieval. In International Con-ference on Machine Learning (ICML) , Edinburgh, Scotland, June 2012.
 Zhou, K., Yang, S.H., and Zha, H. Functional ma-trix factorizations for cold-start recommendation. In
Proceedings of the 34th international ACM SIGIR conference on Research and development in Infor-mation Retrieval , SIGIR  X 11, pp. 315 X 324, New Y-
