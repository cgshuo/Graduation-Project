 Balaraman Ravindran ravi@cs.umass.edu Andrew G. Barto bar to@cs.umass.edu Techniques for scaling decision theoretic planning and learning metho ds to complex environmen ts with large state spaces have attracted much atten tion lately , e. g. (Giv an et al., 2003; Dietteric h, 2000; Sutton et al., 1999). Learning approac hes suc h as the MaxQ algo-rithm (Dietteric h, 2000), Hierarc hies of Abstract Ma-chines (Parr &amp; Russell, 1997), and the options frame-work (Sutton et al., 1999) decomp ose a complex prob-lem into simpler sub-tasks and emplo y the solutions of these sub-tasks in a hierarc hical fashion to solv e the original task. The sub-problems chosen are not only simpler than the original task but often are sub-tasks whose solutions can be rep eatedly reused.
 Mo del minimization metho ds (Giv an et al., 2003; Hartmanis &amp; Stearns, 1966; Ravindran &amp; Barto, 2001) also address the issue of planning in large state spaces. These metho ds attempt to abstract away redundancy in the problem de nition to deriv e an equiv alen t smaller mo del of the problem that can be used to de-rive a solution to the original problem. While reducing entire problems by applying minimization metho ds is often not feasible, we can apply these ideas to various sub-problems and obtain useful reductions in problem size. In ref. (Ra vindran &amp; Barto, 2002) we sho wed that by applying mo del minimization we can reduce a family of related sub-tasks to a single sub-task, the solution of whic h can be suitably transformed to re-cover the solutions for the entire family . We extended the options framew ork (Sutton et al., 1999) to accom-mo date minimization ideas and introduced the notion of a relativize d option : an option de ned without an absolute frame of reference. A relativized option can be view ed as a compact represen tation of a related family of options. We borro w the terminology from Iba (1989) who dev elop ed a similar represen tation for related families of macro operators.
 In this article we explore the case in whic h one is given the reduced represen tation of a sub-task and is re-quired to choose the righ t transformations to reco ver the solutions to the mem bers of the related family . Suc h a scenario would often arise in cases where an agen t is trained in a small protot ypical environmen t and is required to later act in a complex world where skills learned earlier are useful. For example, an agen t may be trained to perform certain tasks in a particu-lar room and then be ask ed to rep eat them in di er-ent rooms in the building. In man y oce and univ er-sity buildings, rooms tend to be very similar to one another. Thus the problem of navigating in eac h of these rooms can be considered simply that of suitably transforming the policy learned in the rst room. An appropriate set of candidate transformations in this case are re ections and rotations of the room. We build on the options framew ork and prop ose a trans-formation selection mec hanism based on Bayesian pa-rameter estimation. We empirically illustrate that the metho d con verges to the correct transformations in a gridw orld \building" environmen t with multiple simi-lar rooms. We also consider a game like environmen t inspired by the Pengi (Agre, 1988) video game. Here there are man y candidate transformations and the con-ditions for minimization are only satis ed very appro x-imately . We introduce a heuristic mo di cation to our transformation selection metho d and again empirically demonstrate that this metho d performs adequately in the game environmen t.
 We emplo y Mark ov Decision Pro cesses (MDPs) as our basic mo deling paradigm. First we presen t some no-tation regarding MDPs and partitions (Section 2), follo wed by a brief summary of MDP homomor-phisms and our MDP minimization framew ork (Sec-tion 3). We the introduce relativized options (Sec-tion 4) and presen t our Bayesian parameter estimation based metho d to choose the correct transformation to apply to a sub-task (Section 5). We then brie y de-scrib e appro ximate equiv alence and presen t a heuristic mo di cation of our likeliho od measure (Section 6). In Section 7 we conclude by discussing relation to existing work and some future directions of researc h. A Markov Decision Process is a tuple h S; A; ; P; R i , where S is a nite set of states, A is a nite set of ac-tions, S A is the set of admissible state-action pairs, P : S ! [0 ; 1] is the transition probabil-ity function with P ( s; a; s 0 ) being the probabilit y of transition from state s to state s 0 under action a , and R : ! IR is the exp ected rew ard function, with R ( s; a ) being the exp ected rew ard for performing ac-tion a in state s . Let A s = f a j ( s; a ) 2 g A denote the set of actions admissible in state s . We assume that for all s 2 S , A s is non-empt y.
 A stochastic policy , , is a mapping from to the real interv al [0 ; 1] with For any ( s; a ) 2 , ( s; a ) gives the probabilit y of ex-ecuting action a in state s . The value of a state-action pair ( s; a ) under policy is the exp ected value of the sum of discoun ted future rew ards starting from state s , taking action a , and follo wing thereafter. The action-value function , Q , corresp onding to a policy is the mapping from state-action pairs to their values. The solution of an MDP is an optimal policy , ? , that uniformly dominates all other possible policies for that MDP .
 Let B be a partition of a set X . For any x 2 X , [ x ]
B denotes the blo ck of B to whic h x belongs. Any function f from a set X to a set Y induces an equiv-alence relation on X , with [ x ] f = [ x 0 ] f if and only if f ( x ) = f ( x 0 ).
 An option (or a temp orally extended action) (Sutton et al., 1999) in an MDP M = h S; A; ; P , R i is de-ned by the tuple O = hI ; ; i , where the initiation set I S is the set of states in whic h the option can be invoked, is the option policy , and the termination function : S ! [0 ; 1] gives the probabilit y of the op-tion terminating in any given state. The option policy can in general be a mapping from arbitrary sequences of state-action pairs (or histories) to action probabili-ties. We restrict atten tion to Mark ov options in whic h the policies are functions of the curren t state alone. The states over whic h the option policy is de ned is kno wn as the domain of the option. Minimization metho ds exploit redundancy in the def-inition of an MDP M to form a reduced mo del M 0 , whose solution yields a solution to the original MDP . One way to achiev e this is to deriv e M 0 suc h that there exists a transformation from M to M 0 that maps equiv alen t states in M to the same state in M 0 , and equiv alen t actions in M to the same action in M 0 . An MDP homomorphism from M to M 0 is suc h a trans-formation. Formally , we de ne it as: De nition: An MDP homomorphism h from an MDP M = h S; A; ; P; R i to an MDP M 0 = ned by a tuple of surjections h f; f g s j s 2 S gi , with h (( s; a )) = ( f ( s ) ; g s ( a )), where f : S ! S 0 and g : A s ! A 0 f ( s ) for s 2 S , suc h that 8 s; s 0 2 S; a 2 A We call M 0 the homomorphic image of M under h . We use the shorthand h ( s; a ) to denote h (( s; a )). The surjection f maps states of M to states of M 0 , and since it is generally man y-to-one, it generally induces non trivial equiv alence classes of states s of M: [ s ] f Eac h surjection g s reco des the actions admissible in state s of M to actions admissible in state f ( s ) of M 0 . This state-dep endent reco ding of actions is a key inno vation of our de nition, whic h we discuss in more detail below. Condition (1) says that the transition probabilities in the simpler MDP M 0 are expressible as sums of the transition probabilities of the states of M that f maps to that same state in M 0 . This is the stochastic version of the standard condition for homomorphisms of deterministic systems that requires that the homomorphism comm utes with the system dynamics (Hartmanis &amp; Stearns, 1966). Condition (2) says that state-action pairs that have the same image under h have the same exp ected rew ard. A policy in M 0 induc es a policy in M and the follo wing describ es how to deriv e suc h an induced policy . De nition: Let M 0 be the image of M under homo-morphism h = h f; f g s j s 2 S gi . For any s 2 S , g 1 s denotes the set of actions that have the same image a M 0 . Then lifte d to M is the policy M suc h that for any a 2 g 1 s ( a 0 ), M ( s; a ) = ( f ( s ) ; a 0 ) An optimal policy in M 0 when lifted to M yields an optimal policy in M (Ra vindran &amp; Barto, 2001). Thus one may deriv e a reduced mo del of M by nding suit-able homomorphic image. Our minimization frame-work is an extension of the approac h prop osed by Dean and Giv an (Giv an et al., 2003). In ref. (Ra vindran &amp; Barto, 2003) we explore application of minimization ideas in an hierarc hical setting and sho w that the ho-momorphism conditions are a generalization of the safe state abstraction conditions introduced by Dietteric h (2000). Consider the problem of navigating in the gridw orld environmen t sho wn in Figure 1(a). The goal is to reac h the cen tral corridor after collecting all the ob-jects in the gridw orld. There exists no non-trivial ho-momorphic image of the entire problem. But there are man y similar comp onen ts in the problem, namely , the ve sub-tasks of getting the object and exiting room i . Since the rooms are similarly shap ed, we can map one sub-task onto the other by applying simple transforma-tions suc h as re ections and rotations. We can mo del this similarit y among the sub-tasks by a \partial" ho-momorphic image|where the homomorphism condi-tions are applicable only to states in the rooms. One suc h partial image is sho wn in Figure 1(b). A relativized option (Ra vindran &amp; Barto, 2002) com-bines partial reductions with the options framew ork to represen t compactly a family of related options. Here the policy for achieving the option's sub-goal is de ned in a partial image MDP (option MDP). When the op-tion is invoked, the curren t state is pro jected onto the option MDP and the policy action of the option MDP is lifted to the original MDP based on the states in whic h the option is invoked. For example, action E in the option MDP will get lifted as action W when invoked in room 3 and as action N when invoked in room 5. Formally we de ne a relativized option as follo ws: De nition: A relativize d option of an MDP M = h S; A; ; P; R i is the tuple O = h h; M O ; I ; i , where I S is the initiation set, : S 0 ! [0 ; 1] is the ter-mination function and h = h f; f g s j s 2 S gi is a partial homomorphism from the MDP h S; A; ; P; R O i to the option MDP M O = h S 0 ; A 0 ; 0 ; P 0 ; R 0 i with R O chosen based on the sub-task.
 In other words, the option MDP M O is a partial ho-momorphic image of an MDP with the same states, actions and transition dynamics as M but with a re-ward function chosen based on the option's sub-task. The homomorphism conditions (1) and (2) hold only for states in the domain of the option O . The option policy : 0 ! [0 ; 1] is obtained by solving M O by treating it as an episo dic task. Note that the initiation set is de ned over the state space of M and not that of M O . Since the initiation set is typically used by the higher level when invoking the option, we decided to de ne it over S . When lifted to M , is transformed into policy fragmen ts over , with the transformation dep ending on the state of M the system is curren tly in.
 Going bac k to our example in Figure 1(a), we can now de ne a single get-obje ct-and-le ave-r oom relativized option using the option MDP of Figure 1(b). The policy learned in this option MDP can then be lifted to M to pro vide di eren t policy fragmen ts in the dif-feren t rooms. In (Ra vindran &amp; Barto, 2002) we explored the issue of learning the relativized option policy while learning to solv e the original task. We established that relativized options signi can tly speed up initial learning and en-able more ecien t kno wledge transfer. We assumed that the option MDP and the required transforma-tions were speci ed beforehand. In a wide variet y of problem settings we can specify a set of possible trans-formations to emplo y with a relativized option but lack sucien t information to specify whic h transformation to emplo y under whic h circumstances. For example, we can train a two-arm ambidextrous rob ot to accom-plish certain sub-tasks like grasping and moving ob-jects using one arm and a small set of object orien ta-tions. If the rob ot is then supplied a set of rotations and re ections, it could learn the suitable transforma-tions required when it uses the other arm and when it encoun ters objects in di eren t orien tations. Giv en a set of candidate transformations H and the op-the righ t transformation to emplo y at eac h invocation of the option? Let s be a function of the curren t state s that captures the features of the states necessary to distinguish the particular sub-problem under consid-eration. 2 We form ulate the problem of choosing the righ t transformation as a family of Bayesian parame-ter estimation problems, one for eac h possible value of s .
 We have a parameter, , that can tak e a nite num ber of values from H . Let p ( h; s ) denote the prior proba-bilit y that = h , i.e., the prior probabilit y that h is the correct transformation to apply in the sub-problem represen ted by s . The set of samples used for comput-ing the posterior distribution is the sequence of tran-executing. Note that the probabilit y of observing a transition from s i to s i +1 under a i for all i , is inde-penden t of the other transitions in the sequence. We emplo y recursiv e Bayes learning to update the poste-rior probabilities incremen tally .
 Let p n be the posterior probabilit y n time steps after the option was invoked. We start by setting p 0 ( h; s ) = p ( h; s ) for all h and s . Let E n = h s n ; a n ; s n +1 be the transition exp erienced after n time steps of option execution. We update the posteriors for all h = h f; f g s j s 2 S gi as follo ws: where P r ( E n j h; s ) = P 0 ( f ( s n ) ; g s is the probabilit y of observing the h -pro jection of transition E n in the option MDP and N = a normalizing factor. When an option is executing, at time step n we use b h = arg max h p n ( h; s ) to pro ject the state to the option MDP and lift the action to the original MDP . After exp eriencing a transition, we update the posteriors of all the transformations in H using (3). 5.1. Results We tested the algorithm on the gridw orld in Figure 1. The agen t has one get-obje ct-and-exit-r oom relativized option de ned in the option MDP in Figure 1(b). Con-sidering all possible com binations of re ections about the x and y axes and rotations through integral multi-ples of 90 degrees, we have eigh t distinct transforma-tions in H . For eac h of the rooms in the world there is one transformation in H that is the desired one. In some ways this is a con triv ed example chosen so that we can illustrate the correctness of our algorithm. Re-duction in problem size is possible in this domain by more informed represen tation schemes. We will dis-cuss, brie y , the relation of suc h schemes to relativized options in the Section 7. The agen t emplo yed hier-archical SMDP Q -learning with -greedy exploration, with = 0 : 1. The learning rate was set at 0 : 01 and at 0.9. We initialized the priors in eac h of the rooms to a uniform distribution with p 0 ( h; s ) = 0 : 125 for all h 2 H and s . The trials were terminated either on completion of the task or after 3000 steps. The results sho wn in Figure 2 are averaged over 100 indep enden t runs. We rep eated the exp erimen t with di eren t levels of stochasticit y, or slip, for the actions.
 The probabilit y with whic h an action \fails", i.e., re-sults in movemen t in a direction other than the desired one, is called the slip in the environmen t. The greater the slip, the harder the problem. As sho wn in Figure 2 the agen t rapidly learned to apply the righ t transfor-mation in eac h room under di eren t levels of stochas-ticit y. We compared the performance to an agen t learning with primitiv e actions alone. The primitiv e action agen t didn't start impro ving its performance until after 30,000 iterations and hence we did not em-ploy that agen t in our exp erimen ts. We also compared the performance to an agen t that knew the righ t trans-formations to begin with. As can be seen in Figure 2 the di erence in performance is not signi can t. 3 In this particular task our transformation-c hoosing algo-rithm manages to iden tify the correct transformations without much loss in performance since there is noth-ing catastrophic in the environmen t and the agen t is able to reco ver quic kly from wrong initial choices. Typically the agen t iden ti es the righ t transformation in a few updates. For example in room 5 the agen t quic kly discards all pure re ections and the posterior for transform 5, a rotation through 90 degrees, con-verges to 1 : 0 by the ten th update. The task in Figure 1 exhibits perfect symmetric equiv-alence. Suc h a scenario does not often arise in practice. In this section we consider a more complex game exam-ple with imp erfect equiv alences inspired by the Pengi environmen t used by Agre (1988) to demonstrate the e ectiv eness of deictic represen tations. The layout of the game is sho wn in Figure 3(a). As in the previous example, the objectiv e of the game is to gather all the objects in the environmen t and the environmen t has standard stochastic gridw orld dynamics.
 Eac h room also has sev eral rob ots, one of whic h migh t be a delayer . If the agen t happ ens to occup y the same square as the dela yer then it is prev ented from mov-ing for a randomly determined num ber of time steps, given by a geometric distribution with parameter hold . When not occup ying the same square, the dela yer pur-sues the agen t with some probabilit y, chase . The other rob ots are benign and act as mobile obstacles, exe-cuting random walks. The agen t does not have prior kno wledge of the hold and chase parameters, nor does the agen t recognize the dela yer on entering the room. But the agen t is aware of the room num bers, and x and y co-ordinates of all the rob ots in the environmen t, all the have features, its own room num ber, and x and y co-ordinates.
 The option MDP (Figure 3(b)) we emplo y is a symmet-rical room with just one rob ot|a dela yer with xed chase and hold parameters. The features describing the state space of the option MDP consists of the x and y co-ordinates of the agen t and the rob ot and a boolean variable indicating possession of the object. Unlik e in the previous example, no room matc hes the option MDP exactly and no rob ot has the same chase and hold parameters as the dela yer in the image MDP . In fact room 2 does not have a dela yer rob ot. While emplo ying the relativized option, the agen t has to not only gure out the righ t orien tation of the room but also whic h rob ot is the dela yer, i. e. , whic h rob ot's location it needs to pro ject onto the option MDP's de-layer's features. Thus there are 40 candidate transfor-mations, comprising of di eren t re ections, rotations and pro jections. 4 While this game domain is also a con triv ed example, pro jection transformations arise often in practice. In an environmen t with objects, selecting a subset of ob-jects (say blo cks) with whic h to perform a sub-task can be mo deled as pro jecting the features of the chosen ob-jects onto an option MDP . Suc h pro jections can also be used to mo del indexical represen tations (Agre, 1988). In this game example, nding the pro jection that cor-rectly iden ti es the dela yer is seman tically equiv alen t to implemen ting a the-r obot-chasing-me pointer. In ref. (Ra vindran &amp; Barto, 2002) we extended our minimization framew ork to accommo date appro xi-mate equiv alences and symmetries. Since we are ig-noring di erences between the various rooms there will be a loss in asymptotic performance. We discussed bounding this loss using Bounded-parameter MDPs (Giv an et al., 2000) and appro ximate homomorphisms. We cannot emplo y the metho d we dev elop ed in the previous section for choosing the correct transforma-tions with appro ximate homomorphisms. In some cases even the correct transformation causes a state transition the agen t just exp erienced to pro ject to an imp ossible transition in the image MDP , i.e., one with a P 0 value of 0. Thus the posterior of the correct trans-formation migh t be set to zero, and once the posterior reac hes 0, it stays there regardless of the positiv e evi-dence that migh t accum ulate later.
 To address this problem we emplo yed a heuristic| lower bound the P 0 values used in the update equation (3). We compute a \weigh t" for the transformations using: where P 0 ( s; a; s 0 ) = max ( ; P 0 ( s; a; s 0 )), and N = P malizing factor. Thus even if the pro jected transition has a probabilit y of 0 in the option MDP , we use a value of in update. The weigh t w ( h; s ) is a measure of the likeliho od of h being the righ t transformation in s and we use this weigh t instead of the posterior probabilit y.
 We measured the performance of this heuristic in the game environmen t. Since none of the rooms matc h the option MDP closely , keeping the option policy xed leads to very poor performance. So we allo wed the agen t to con tinually mo dify the option policy while learning the correct transformations. As with the ear-lier exp erimen ts, the agen t sim ultaneously learned the policies of the lower level and higher level tasks, using a learning rate of 0 : 05 for the higher level MDP and 0 : 1 for the relativized option. The discoun t factor was set to 0.9, to 0 : 1 and to 0 : 01. We initialized the initial weigh ts in eac h of the rooms to a uniform distribution with w 0 ( h; s ) = 0 : 025 for all h 2 H and s . The trials were terminated either on completion of the task or after 6000 steps.
 We compared the performance of this heuristic against that of an agen t that had 4 di eren t regular Mark ov options. The agen t learned the option policies si-multaneously with the higher level task. The results sho wn in Figure 4 were averaged over 10 indep en-den t runs. As sho wn in Figure 4 the agen t using the heuristic sho ws rapid impro vemen t in performance initially . This vindicates our belief that it is easier to learn the \correct" transformations than to learn the policies from scratc h. As exp ected the asymptotic performance of the regular agen t is better than the relativized agen t. We couldn't compare the heuristic against an agen t that knew the correct transformations beforehand since there are no correct transformation in some of the cases.
 Figure 5 sho ws the evolution of weigh ts in room 4 dur-ing a typical run. The weigh ts have not con verged to their nal values after 600 updates, but transform 12, the correct transformation in this case, has the biggest weigh t and is picked consisten tly. After about thousand updates the weigh t for transform 12 reac hed nearly 1 and stayed there. Figure 6 sho ws the evolu-tion of weigh ts in room 2 during a typical run. The weigh ts oscillate a lot during the runs, since none of the transforms are entirely correct in this room. In this particular run, the agen t con verged to transform 5 after about 1000 updates, but that is not alw ays the case. But the agen t can solv e the sub-task in room 2 as long as it correctly iden ti es the orien tation and emplo ys any of transforms 1 through 5.
 While none of the transformations are completely \cor-rect" in this environmen t, esp ecially in room 2, the agen t is able to complete eac h sub-task successfully after a bit of retraining. One can easily construct en-vironmen ts where this is not possible and we need to consider some fail-safe mec hanism. One alternativ e is to learn the policy for the failure states at the primi-tive action level and use the option only in states where there is some chance of success. Another alternativ e is to spa wn a cop y of the relativized option whic h is available only in the failure states and allo w the agen t to learn the option policy from scratc h thus prev ent-ing harmful generalization. We have not explored this issue in detail in this pap er, since we were looking to dev elop a metho d that allo ws us to select the righ t transformation under the assumption that we have ac-cess to suc h a transformation. In this article we presen ted results on choosing from among a set of candidate partial homomorphisms, as-suming that the option MDP and the policy are com-pletely speci ed. We are curren tly working on relax-ing these assumptions. Speci cally we are looking at learning the option policy online and working with par-tial speci cation of the option MDP . We are also ap-plying this approac h to more complex problems. In particular we are applying this to solving a family of re-lated tasks on the UMass torso (Wheeler et al., 2002). The torso is initially trained on a single task, and is later required to learn the transformations to apply to deriv e solutions to the other tasks in the family . The exp erimen ts rep orted in this pap er emplo y a two-level hierarc hy. Our ideas generalize naturally to multi-lev el hierarc hies. The semi-Mark ov decision pro-cess (SMDP) framew ork is an extension of the MDP framew ork whic h is widely emplo yed in mo deling hier-archical systems (Dietteric h, 2000; Sutton et al., 1999). We recen tly dev elop ed the notion of SMDP homomor-phism and used it in de ning hierarc hical relativized options (Ra vindran &amp; Barto, 2003). The option is now de ned using an option SMDP and hence can have other options as part of its action set. We are cur-ren tly applying the metho ds dev elop ed in this pap er to domains that require hierarc hical options. We emplo yed the options framew ork in this work, but our ideas are applicable to other hierarc hical frame-works suc h as MAX Q (Dietteric h, 2000) and Hierar-chies of Abstract Mac hines (Parr &amp; Russell, 1997). Sub-tasks in these framew orks can also be relativized and we could learn homomorphic transformations be-tween related sub-tasks. Relativization can also be com bined with automatic hierarc hy extraction algo-rithms (McGo vern &amp; Barto, 2001; Hengst, 2002) and algorithms for building abstractions speci c to di er-ent levels of the hierarc hy (Dietteric h, 2000; Jonsson &amp; Barto, 2001).
 In some environmen ts it is possible to choose repre-sen tation schemes to implicitly perform the required transformation dep ending on the sub-task. Examples of suc h schemes include ego-cen tric and deictic repre-sen tations (Agre, 1988), in whic h an agen t senses the world via a set of pointers and actions are speci ed with resp ect to these pointers. In the game environ-men t we emplo yed pro jection transformations to iden-tify the dela yer rob ot. As men tioned earlier, this is a form of indexical represen tation with the available pro-jections specifying the possible pointer con gurations. Thus choosing the righ t transformation can be view ed as learning the-r obot-chasing-me pointer. While em-ploying suc h represen tations largely simpli es the so-lution of a problem, they are frequen tly very dicult to design. Our work is a rst step toward systematiz-ing the transformations needed to map similar sub-tasks onto eac h other in the absence of versatile sen-sory mec hanisms. The concepts dev elop ed here will also serv e as stepping stones to designing sophisticated represen tation schemes.
 In our approac h we emplo y di eren t transformations to get di eren t interpretations of the same MDP mo del. Researc hers have investigated the problem of emplo ying multiple mo dels of an environmen t and com bining the predictions suitably using the EM al-gorithm (Haruno et al., 2001) and mixtures of exp erts mo dels (Do ya et al., 2002). We are investigating spe-cializations of suc h approac hes to our setting where the multiple mo dels can be obtained by simple transforma-tions of one another. This close relationship between the various mo dels migh t yield signi can t simpli ca-tion of these architectures.
 We wish to thank Mohammad Gha vamzadeh, Dan Bernstein, Am y McGo vern and Mik e Rosenstein for man y hours of useful discussion. This material is based upon work supp orted by the National Science Foun-dation under Gran t No. ECS-0218125 to Andrew G. Barto and Sridhar Mahadev an. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect the views of the National Science Foundation. Agre, P. E. (1988). The dynamic structur e of everyday life (Technical Rep ort AITR-1085). Massac husetts Institute of Technology .
 Dietteric h, T. G. (2000). Hierarc hical reinforcemen t learning with the MAX Q value function decomp osi-tion. Arti cial Intel ligenc e Research , 13 , 227{303. Doya, K., Samejima, K., Katagiri, K., &amp; Kawato, M. (2002). Multiple mo del-based reinforcemen t learn-ing. To app ear in Neural Computation.
 Giv an, R., Dean, T., &amp; Greig, M. (2003). Equiv alence notions and mo del minimization in Mark ov decision pro cesses. To app ear in Arti cial Intelligence. Giv an, R., Leac h, S., &amp; Dean, T. (2000). Bounded-parameter Mark ov decision pro cesses. Arti cial In-telligenc e , 122 , 71{109.
 Hartmanis, J., &amp; Stearns, R. E. (1966). Algebr aic structur e theory of sequential machines . Englew ood Cli s, NJ: Pren tice-Hall.
 Haruno, M., Wolp ert, D. M., &amp; Kawato, M. (2001).
MOSAIC mo del for sensorimotor learning and con-trol. Neur al Computation , 13 , 2201{2220.
 Hengst, B. (2002). Disco vering hierarc hy in reinforce-men t learning with HEX Q. Proceedings of the 19th
International Confer ence on Machine Learning (pp. 243{250).
 Iba, G. A. (1989). A heuristic approac h to the dis-covery of macro-op erators. Machine Learning , 3 , 285{317.
 Jonsson, A., &amp; Barto, A. G. (2001). Automated state abstraction for options using the u-tree algorithm.
Proceedings of Advanc es in Neur al Information Pro-cessing Systems 13 (pp. 1054{1060). Cam bridge, MA: MIT Press.
 McGo vern, A., &amp; Barto, A. G. (2001). Automatic dis-covery of subgoals in reinforcemen t learning using diverse densit y. Proceedings of the 18th Interna-tional Confer ence on Machine Learning ICML 2001 (pp. 361{368).
 Parr, R., &amp; Russell, S. (1997). Reinforcemen t learn-ing with hierarc hies of mac hines. Proceedings of Ad-vanc es in Neur al Information Processing Systems 10 (pp. 1043{1049). MIT Press.
 Ravindran, B., &amp; Barto, A. G. (2001). Symmetries and model minimization of Markov decision pro-cesses (Technical Rep ort 01-43). Univ ersit y of Mas-sachusetts, Amherst.
 Ravindran, B., &amp; Barto, A. G. (2002). Mo del mini-mization in hierarc hical reinforcemen t learning. Pro-ceedings of the Fifth Symp osium on Abstr action, Re-formulation and Appr oximation (SARA 2002) (pp. 196{211). New York, NY: Springer-V erlag.
 Ravindran, B., &amp; Barto, A. G. (2003). SMDP homo-morphisms: An algebraic approac h to abstraction in semi-Mark ov decision pro cesses. To app ear in the Proceedings of the Eighte enth Internatinal Joint Confer ence on Arti cial Intel ligenc e (IJCAI 2003) . Sutton, R. S., Precup, D., &amp; Singh, S. (1999). Bet ween
MDPs and Semi-MDPs: A framew ork for temp oral abstraction in reinforcemen t learning. Arti cial In-telligenc e , 112 , 181{211.
 Wheeler, D. S., Fagg, A. H., &amp; Grup en, R. A. (2002).
Learning prosp ectiv e pick and place beha vior. Pro-ceedings of the International Confer ence on Devel-
