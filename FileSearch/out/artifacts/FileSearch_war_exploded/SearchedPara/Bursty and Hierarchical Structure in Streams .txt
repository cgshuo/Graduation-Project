
A fundamental problem in text data mining is to extract meaning-ful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text min-ing work ih this area is the following intuitive premise --that the appearance of a topic in a document stream is signaled by a "burst of activity," with certain features rising sharply in frequency as the topic emerges. for modeling such "bursts," in such a way that they can be robustly and efficiently identified, and can provide an organizational frame-work for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions; in some ways, it can be viewed as drawing an analogy with models from queueing the-ory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them. tings we also experience their arrival over time. E-mall and news articles provide two clear examples of such document streams: in both cases, the strong temporal ordering of the content is necessary for making sense of it, as particular topics appear, grow in inten-sity, and then fade away again. Over a much longer time scale, the published literature in a particular research field can be mean-*Supported in part by a David and Lucile Packard Foundation Fel-lowship, an ONR Young Investigator Award, NSF ITR/IM Grant IIS-0081334, and NSF Faculty Early Career Development Award CCR-9701399. permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. ingfully understood in this way as well, with particular research themes growing and diminishing in visibility across a period of years. Work in the areas of topic detection and tracking [2, 3, 5, 64, 65], text mining [37, 59, 60, 61], and visualization [27, 45, 63] has explored techniques for identifying topics in document streams comprised of news stories, using a combination of content analysis and time-series modeling. 
Underlying a number of these techniques is the following in-tuitive premise --that the appearance of a topic in a document stream is signaled by a "burst of activity," with certain features ris-ing sharply in frequency as the topic emerges. The goal of the present work is to develop a formal approach for modeling such "bursts," in such a way that they can be robustly and efficiently identified, and can provide an organizational framework for ana-lyzing the underlying content. At one level, the approach presented here can be viewed as drawing an analogy with models from queue-ing theory for bursty network traffic (see e.g. [33]). In addition, however, the analysis of the underlying burst patterns reveals a la-tent hierarchical structure that often has a natural meaning in terms of the content of the stream. My initial aim in studying this issue was a very concrete one: I wanted a better organizing principle for the enormous archives of personal e~mail that I was accumulating. Abundant anecdotal evidence, as well as academic research [6, 44, 62], suggested that my own experience with "e-mail overload" corresponded to a near-universal phenomenon --a consequence of both the rate at which e-mail arrives, and the demands of managing volumes of saved per-sonal correspondence that can easily grow into tens and hundreds of megabytes of pure text content. And at a still larger scale, e-mail has become the raw material for legal proceedings [35] and historical investigation [8, 39, 46] --with the National Archives, for example, agreeing to accept tens of rnillions of e-mail messages from the Clinton White House [48]. In sum, there are several set-making sense of large volumes of e-mail. 
An active line of research has applied text indexing and clas-sification to develop e-mall interfaces that organize incoming mes-sages into folders on specific topics, sometimes recommending fur-53, 54, 56, 57] --in effect, this framework seeks to automate a kind of filing system that many users implement manually. There has also been work on developing query interfaces to fully-indexed collections of e-mail [7]. 
My interest here is in exploring organizing structures based more explicitly on the role of time in e-mail and other document streams. Indeed, even the flow of a single focused topic is modulated by the rate at which relevant messages or documents arrive, dividing naturally into more localized episodes that correspond to bursts of activity of the type suggested above. For example, my saved e-mail contains over a thousand messages relevant to the topic "grant pro-posals" --announcements of new funding programs, planning of proposals, and correspondence with co-authors. While one could divide this collection into sub-topics based on message content --certain people, programs, or funding agencies form the topics of some messages but not others --an equally natural and substan-tially orthogonal organization for this topic would take into account the sequence of episodes reflected in the set of messages --bursts that surround the planning and writing of certain proposals. In-deed, certain sub-topics (e.g. "the process of gathering people to-gether for our large NSF ITR proposal") may be much more easily characterized by a sudden confluence of message-sending over a particular period of time than by textual features of the messages themselves. One can easily argue that many of the large topics rep-resented in a document stream are naturally punctuated by bursts in this way, with the flow of relevant items intensifying in certain key periods. A general technique for highlighting these bursts thus has the potential to expose a great deal of fine-grained structure. 
Before moving to a more technical overview of the methodol-ogy, let me suggest one further perspective on this issue, quite dis-tant from computational concerns. If one were to view a particular folder of e-mall not simply as a document stream but also as some-thing akin to a narrative that unfolds over time, then one immedi-ately brings into play a body of work that deals explicitly with the bursty nature of time in narratives, and the way in which particu-lar events are signaled by a compression of the time-sense. In an early concrete reference to this idea, E.M. Forster, lecturing on the structure of the novel in the 1920's, asserted that This role of time in narratives is developed more explicitly in work of Genette [20, 21], Chatman [11], and others on anisochronies, the non-uniform relationships between the amount of time spanned by a story's events and the amount of time devoted to these events in the actual telling of the story. Modeling Bursty Streams. Suppose we were presented with a document stream --for concreteness, consider a large folder of e-mail on a single broad topic. How should we go about identifying the main bursts of activity, and how do they help impose additional structure on the stream? The basic point emerging from the dis-cussion above is that such bursts correspond roughly to points at which the intensity of message arrivals increases sharply, perhaps from once every few weeks or days to once every few hours or min-typically rise smoothly to a crescendo and then fall away, but rather exhibits frequent alternations of rapid flurries and longer pauses in close proximity. Thus, methods that analyze gaps between consec-utive message arrivals in too simplistic a way can easily be pulled into identifying large numbers of short spurious bursts, as well as fragmenting long bursts into many smaller ones. Moreover, a sim-ple enumeration of close-together sets of messages is only a first step toward more intricate structure. The broader goal is thus to ex-tract global structure from a robust kind of data reduction --iden-tifying bursts only when they have sufficient intensity, and in a way that allows a bursll to persist smoothly across a fairly non-uniform pattern of message arrivals. My approach here is to model the stream using an infinite-state automaton ..4, which at any point in time can be in one of an under-lying set of states, and emits messages at different rates depending on its state. Specifically, the automaton .,4 has a set of states that correspond to increasingly rapid rates of emission, and the onset of a burst is signaled by a state transition --from a lower state to a higher state. By assigning costs to state transitions, one can control the frequency of such transitions, preventing very short bursts and making it easier to identify long bursts despite transient changes in the rate of the stream. The overall framework is developed in Section 2. It can be viewed as drawing on the formalism of on-off 
Markov sources used in modeling bursty network traffic (see for ex-ample the overview article by Kelly [33]), as well as the formalism of hidden Markov models [51]. Using an automaton with states that correspond to higher and higher intensifies provides an additional source of analytical lever-age --the bursts associated with state transitions form a naturally nested structure, with a long burst of low intensity potentially con-taining several bursts of higher intensity inside it (and so on, re-cursively). For a folder of related e-mail messages, we will see in 
Sections 2 and 3 that this can provide a hierarchical decomposi-tion of the temporal order, with long-running episodes intensifying into briefer ones according to a natural tree structure. This tree can thus be viewed as imposing a fine-grained organization on the sub-episodes within the message stream. Following this development, Section 4 focuses on the problem of enumerating all significant bursts in a document stream, ranked by a measure of "weight." Applied to a case in which the stream is comprised not of e-mail messages but of research paper titles over the past several decades, the set of bursts corresponds roughly to the appearance and disappearance of certain terms of interest in the underlying research area. The approach makes sense for many other datasets of an analogous flavor; in Section 4, I also discuss an example based oil U.S. Presidential State of the Union Addresses from 1790 to 2002. Section 5 discusses the connections to related work in a range of areas, particularly the striking recent work of 
Swan, Allan, and Jensen [59, 60, 61] on overview timelines, which forms the body of research closest to the approach here. Finally, 
Section 6 discusses some further applications of the methodology --how burstiness in arrivals can help to identify certain messages as "landmarks" in a large corpus of e-mail; and how the overall framework can be: applied to logs of Web usage. Perhaps the simplest randomized model for generating a sequence of message arrival times is based on an exponential distribution: messages are emitted in a probabilistic manner, so that the gap x in time between messages i and i + 1 is distributed according to the "memoryless" exponential density function f(x) = ae -~'~, for a parameter a &gt; 0. (In other words, the probability that the gap ex-ceeds x is equal to e-'~.) The expected value of the gap in this model is a-1, and hence one can refer to a as the rate of message arrivals. Intuitively, a "bursty" model should extend this simple formula-tion by exhibiting periods of lower rate interleaved with periods of higher rate. A natural way to do this is to construct a model with multiple states, where the rate depends on the current state. Let us start with a basic :model that incorporates this idea, and then extend it to the models that will primarily be used in what follows. gaps, which has the form fq(Xl .... , x,~) = Ht~=l fit (xt). If b denotes the number of state transitions in the sequence q --that is, the number of indices it so that qit ~ qit+a --then the (prior) probability of q is equal to (In this calculation, let io = 0, since .A starts in state qo.) Now, where Z is the normalizing constant ~q, Pr [q'] fq, (x). Finding a state sequence q maximizing this probability is equivalent to find-ing one that minimizes 
Since the third and fourth terms are independent of the state se-quence, this latter optimization problem is equivalent to finding a state sequence q that minimizes the following cost function: lem that can be motivated intuitively on its own terms, without re-course to the underlying probabilistic model. The first of the two terms in the expression for c (q t x) favors sequences with a small number of state transitions, while the second term favors state se-quences that conform well to the sequence x of gap values. Thus, one expects the optimum to track the global structure of bursts in the gap sequence, while holding to a single state through local pe-riods of non-uniformity. Varying the coefficient on b controls the amount of "inertia" fixing the automaton in its current state. that also extracts hierarchical structure from the pattern of bursts. 
An infinite-state model. Consider a sequence of n + 1 messages that arrive over a period of time of length T. If the messages were spaced completely evenly over this time interval, then they would arrive with gaps of size ~ = Tin. Bursts of greater and greater intensity would be associated with gaps smaller and smaller than ~. This suggests focusing on an infinite-state automaton whose states correspond to gap sizes that may be arbitrarily small, so as to capture the full range of possible bursts. The development here will use a cost model as in the two-state case, where the underlying goal is to find a state sequence of minimum cost. 
Thus, consider an automaton with a "base state" qo that has an associated exponential density function fo with rate ao = ~-1 = n/T --consistent with completely uniform message arrivals. For each i &gt; 0, there is a state q~ with associated exponential density fi having rate c~i = ~-ls~, where s &gt; 1 is a scaling parameter. (i will be referred to as the index of the state q~.) In other words, the infinite sequence of states qo, ql,.., models inter-arrival gaps that decrease geometrically from ~; there is an expected rate of message arrivals that intensifies for larger and larger values of i. 
Finally, for every i and j, there is a cost r(i,j) state transition from qi to q.~. The framework allows considerable flexibility in formulating the cost function; for the work described here, r(., .) is defined so that the cost of moving from a lower-intensity burst state to a higher-intensity one is proportional to the number of intervening states, but there is no cost for the automaton to end a higher-intensity burst and drop down to a lower-intensity one. Specifically, when j &gt; i, moving from q~ to q~ incurs a cost cost is 0. See Figure l(a) for a schematic picture. 
This automaton, with its associated parameters s and % will be between message arrivals, the goal --by analogy with the two-state minimizes the cost function the set of possible q is infinite, one cannot automatically assert that the minimum is even well-defined; but this will be established in Theorem 2.1 below. As before, minimizing the first term is consis-tent with having few state transitions --and transitions that span only a few distinct states --while minimizing the second term is consistent with passing through states whose rates agree closely with the inter-arrival gaps. Thus, the combined goal is to track the sequence of gaps as well as possible without changing state too much. 
Observe that the scaling parameter s controls the "resolution" real-valued gaps; the parameter ,7 controls the ease with which the automaton can change states. In what follows, 3' will often be set to a default value of 1; we can use ,,4: to denote ,A:,i. Computing a minimum-cost state sequence. Given a a sequence of positive gaps x = (xl, x2,... , xn) between message arrivals, 93 consider the algorithmic problem of finding a state sequence q = (qi~ .... ,q~,) in A;,~ that minimizes the cost c(ql x); such a sequence will be called optimal. To establish that the minimum is well-defined, and to provide a means of computing it, it is useful to first define a natural finite restriction of the automaton: for a natural number k, one simply deletes all states but qo, qi,... , qk-1 from A~,~, and denotes the resulting k-state automaton by A~,~. 
Note that the two-state automaton .A2~,.r is essentially equivalent (by an amortization argument) to the probabilistic two-state model described earlier. 
It is not hard to show that computing an optimal state sequence in .A;,7 is equivalent to doing so in one of its finite restrictions. 
THEOREM 2.1. Let 6(x) = min~=x xi and state sequence in .A~,7, then it is also an optimal state sequence in 
PROOF. Let q* = (qq,... , qe,) be an optimal state sequence in A~,~. As before, set go = io = 0, since both sequences start in state qo; for notational purposes, it is useful to define  X ,~+1 = 
If q does not contain any states of index greater than k -1, this inequality follows from the fact that q" is an optimal state 
Now, for a particular choice of t between 1 and n, consider the = 27t -1. Thus, ifj is such that aj. &lt; x 7 &lt; a~*+l, then Since k = rl~ -lo~ T + log, $(x)-1], one has -In fit (xt), since it &gt; i~ = k --1. 
Combining these inequalities for the state transition costs and the gap costs, one obtains c(qlx).l computes an optimal state sequence in an automaton of the form 
A ks,r-This can be done by adapting the standard forward dynamic programming algorithm used for hidden Markov models [51] to the model and cost function defined here: One defines C~ (t) to be the minimum cost of a state sequence for the input :r~, z2,..., zt that must end with state q~, and then iteratively builds up the values of C~ (t) in order of increasing t using the recurrence relation C~ (t) = -In f~(zt) + mine(Ce(t -1) + r(t,j)) with initial conditions an optimal state sequence in A:,~ r can be found by restricting to a number of states k that is a very small constant, always at most 25. quence is carried out by recourse to a finite-state model, working with the infinite model has the advantage that a number of states k is not fixed a priori; rather, it emerges in the course of the compu-tation, and in this way the automaton ,A:,7 essentially "conforms" to the particular input instance. 
Extracting hierarchical structure. From an algorithm to com-pute an optimal state sequence, one can then define the basic repre-sentation of a set of bursts, according to a hierarchical structure. arrival gaps x = (zl,z2,... ,z,~), suppose that an optimal state sequence q = (qil,qi2,... ,qi,~) in ,A:,~ r has been determined. 
Following the discussion of the previous section, we can formally define a burst of intensity j to be a maximal interval over which q is in a state of index j or higher. More precisely, it is an interval undefined if t -1 &lt; 0 or t' + 1 &gt; n). of intensity j may contain one or more sub-intervals that are bursts of intensity j  X  1; these in turn may contain sub-intervals that are bursts of intensity j + 2; and so forth. This relationship can be represented by a rooted tree I', as follows. There is a node corre-sponding to each burst; and node v is a child of node zt if node u represents a burst B~, of intensity j (for some value of j), and node v represents a burst By of intensity j + i such that B~ C B~. 
Note that the root of r' corresponds to the single burst of intensity 0, which is equal to the whole interval [0, n]. the underlying stream. Figure l(b) shows the transformation from an optimal state sequence, to a set of nested bursts, to a tree. 
Hierarchy in an e-mail stream. Let us now return to one of the initial motivations for this model, and consider a stream of e-mall messages. What does the hierarchical structure of bursts look like in this setting? 
I applied the algorithm to my own collection of saved e-mail, consisting of messages sent and received between June 9, 1997 and August 23, 2001. (The cut-off dates are chosen here so as to roughly cover four academic years.) First, here is a brief summary of this collection. Every piece of mail I sent or received during this period of time, using my cs.comell.edu e-mall address, can be viewed as belonging to one of two categories: first, messages con-sisting of one or more large files, such as drafts of papers mailed between co-authors (essentially, e-mail as file transfer); and sec-ond, all other messages. The collection I am considering here con-sists simply of all messages belonging to the second, much larger category; thus, to a rough approximation, it is all the mail I sent and received during this period, unfiltered by content but excluding long files. It contains 34344 messages in UNIX mailbox format, totaling 41.7 megabytes of ascii text, excluding message headers. 1 
Subsets of the collection can be chosen by selecting all messages that contain a particular string or set of strings; this can be viewed as an analogue of a "folder" of related messages, although mes-sages in the present case are related not because they were manually filed together but because they are the response set to a particular query. Studying the stream induced by such a response set raises two distinct but related questions. First, is it in fact the case that the appearance of messages containing particular words exhibits a "spike," in some informal sense, in the (temporal) vicinity of sig-nificant times such as deadlines, scheduled events, or unexpected developments? And second, do the algorithms developed here pro-vide a means for identifying this phenomenon? 
In fact such spikes appear to be quite prevalent, and also rich enough that the algorithms of the previous section can extract hier-archical structure that in many cases is quite deep. Moreover, the algorithms are efficient enough that computing a representation for the bursts on a query to the full e-mail collection can be done in real-time, using a simple implementation on a standard PC. To give a qualitative sense for the kind of structure one obtains, Figures 2 and 3 show the results of computing bursts for two dif-ferent queries using the automaton .A~. Figure 2 shows an analysis of the stream of all messages containing the word "ITR," which is prominent in my e-mail because it is the name of a large National Science Foundation program for which my colleagues and I wrote two proposals in 1999-2000. There are many possible ways to or-ganize this stream of messages, but one general backdrop against which to view the stream is the set of deadlines imposed by the NSF for the first run of the program. Large proposals were sub-mitted in a three-phase process, with deadlines of 11/15/99, 1/5/00, and 4/17/00 for letters of intent, pre-proposals, and full proposals respectively. Small proposals were submitted in a two-phase pro-cess, with deadlines of 1/5/00 and 2/14/00 for letters of intent and full proposals respectively. I participated in a group writing a pro-posal of each kind. 
Turning to the figure, part (a) is a plot of the raw input to the automaton ,A~, showing the arrival time of each message in the re-sponse set. Part (b) shows a nested interval representation of the set of bursts for the optimal state sequence in .A~; the intervals are annotated with the first and last dates of the messages they contain, and the dates of the NSF deadlines are lined up with the intervals that contain them. Note that this is a schematic representation, de-and centering of the intervals in the drawing are not significant. Part (c) shows a drawing of the resulting tree I'. The root corresponds to the single burst of intensity 0 that is present in any state sequence. 1These figures reveal that I receive less e-mail per day than many of my colleagues; one contributing factor is that I do not subscribe to any high-volume mailing lists based outside Cornell. corresponds to a long burst, and each contains two shorter, more intense bursts for the particular prelims. Specifically, the three chil-dren of the root are centered over the semesters in which the three undergraduate courses were taught (Spring 1999, Spring 2000, and 
Fall 2000); and the sub-trees below these children split further into two sub-trees each, concentrated either directly over or slightly pre-ceding the two prelims given that semester. might naturally be divided into a hierarchical set of sub-folders around certain key events, based only on the rate of message ar-rivals. The appropriateness of Forster's comments on the time-sense in narratives is also fairly striking here: when organized by burst intensities, the period of time covered in the e-mail collection proceeding uniformly. perform a type of enumeration: for every word w that appears in the collection, one computes all the bursts in the stream of messages po = R/D, and p~ = pos i. Since it does not make sense for p~ to documents according to a binomial distribution with probability p~. fined as follows. If the automaton is in state q~ when the t th batch arrives, a cost of 
Word Interval of burst data 1975 SIGMOD ~ 1979 SIGMOD base 1975 SIGMOD-1981 VLDB application 1975 SIGMOD-1982 SIGMOD bases 1975 SIGMOD m 1982 VLDB relational 1975 SIGMOD ~ 1989 VLDB model 1975 SIGMOD ~ 1992 VLDB large 1975 VLDB ~ 1977 VLDB schema 1975 VLDB --1980 VLDB theory 1977 VLDB --1984 SIGMOD distributed 1977 VLDB --1985 SIGMOD data 1980 VLDB --1981 VLDB statistical 1981 VLDB --1984 VLDB database 1982 SIGMOD ~ 1987 VLDB nested 1984 VLDB --1991 VLDB deductive 1985 VLDB --1994 VLDB transaction 1987 SIGMOD --1992 SIGMOD objects 1987 VLDB --1992 SIGMOD object-oriented 1987 SIGMOD --1994 VLDB parallel 1989 VLDB --1996 VLDB object 1990 SIGMOD --1996 VLDB server 1996 SIGMOD --2000 VLDB sql 1996 VLDB --2000 VLDB warehouse 1996 VLDB --similarity 1997 SIGMOD approximate 1997 VLDB --web 1998 SIGMOD indexing 1999 SIGMOD-xml 1999 VLDB 
Figure 4: The 30 bursts of highest weight in B22, using titles of all papers from the database conferences SIGMOD and VLDB, 1975-2001. Word Interval of burst grammars 1969 STOC ~ 1973 FOCS automata 1969 STOC-1974 STOC languages 1969 STOC-1977 STOC machines 1969 STOC-1978 STOC recursive 1969 STOC --1979 FOCS classes 1969 STOC--1981 FOCS some 1969 STOC --1980 FOCS sequential 1969 FOCS --1972 FOCS equivalence 1969 FOCS --1981 FOCS programs 1969 FOCS --1986 FOCS program 1970 FOCS --1978 STOC on 1973 FOCS --1976 STOC problems relational 1975 FOCS --1982 FOCS logic 1976 FOCS --1984 STOC vlsi 1980 FOCS --1986 STOC probabilistic how learning 1987 FOCS --1997 FOCS competitive 1990 FOCS --1994 FOCS randomized 1992 STOC --1995 STOC ._approximation 1993 STOC-improved 1994 STOC --2000 STOC codes 1994 FOCS ..approximating 1995 FOCS --quantum 1996 FOCS --is incurred, since this is the negative logarithm of the probability that rt relevant documents would be generated using a binomial distribution with probability Pl. There is also a cost of r(it, it+x) associated with the state transition from qlt to qit+l, where this cost is defined precisely as for .A* A state sequence of minimum total cost can then be computed as in Section 2. 
In the analysis of conference paper tides here, the main goal is to enumerate bursts of positive intensity, but not to emphasize hier-archical structure. Thus, the two-state automaton B~ is used; given an optimal state sequence, bursts of positive intensity correspond [tt, t2], we can define the weight of the burst to be 
In other words, the weight is equal to the improvement in cost in-curred by using state ql over the interval rather than state qo. Ob-serve that in an optimal sequence, the weight of every burst is non-negative. Intuitively, then, bursts of larger weight correspond to more prominent periods of elevated activity. (This notion of weight can be naturally extended to larger numbers of states, as well as to the automaton model from Section 2.) 
In Figure 4, this framework is applied to the titles of SIGMOD and VLDB papers for the years 1975-2001. For each word w (in-
There are several points to note about this analysis. First, the 61] developed a method for constructing overview timelines of a set of news stories. For each named entity and noun phrase in the cor-pus, they perform a X 2 test to identify days on which the number of occurrences yields a value above a certain threshold; contiguous sets of days meeting this condition are then grouped into an inter-val that is added to the timeline. Thus, the high-level structure of their approach is parallel to the enumerative method in Section 4. 
However, the underlying methodology is quite different from the present work in two key respects. First, Swan et al. note that the use of thresholds makes it difficult to construct long intervals of ac-tivity for a single feature --such intervals are often broken apart by brief gaps in which the feature does not occur frequently enough, and subsequent heuristics are needed to piece them together. The present work, by modeling a burst as a state transition with costs, allows for a long interval to naturally persist across such gaps; es-sentially, in place of thresholds, the optimization problem inherent in finding a minimum-cost state sequence adaptively groups nearby high-intensity intervals together when it is advantageous to do so. 
Second, the work of Swan et al. does not attempt to infer any type of hierarchical structure in the appearance of a feature. over a very short time scale, searching for features that can deter-mine whether one message is a response to another [38]. This is applied to develop robust techniques for identifying threads, a pop-ular metaphor for organizing e-mail and newsgroup postings [15, 23]. In a very different context, Grosz and Sidner develop struc-tural models for discourse as a means of analyzing communication [22]; their use of stack models in particular results in a nested orga-nization that bears an intriguing, though distant, relationship to the nested structure of bursts studied here. series analysis and sequence mining [10, 26]; connections to related probabilistic frameworks such as bursty on-off sources [33] and hidden Markov models [51] have already been discussed above. 
There has also been work incorporating a notion of hierarchy into the framework of hidden Markov models [17, 47]; this goes beyond the type of automaton we use here to allow more complex kinds of hierarchies with potentially large state sets at each "level." Ehrich and Foith [16] proposed a method for constructing a tree from a one-dimensional time series, essentially by introducing a branch-point at each local minimum and a leaf at each local maximum (see also [58]). In the context of the applications here, this approach would yield trees of enormous complexity, due to the ruggedness of the underlying temporal data, with many local minima and maxima. of Section 2 and 4 can also be viewed as a search for approximate level sets in a time series, and hence related to the large body of work on piece-wise function approximation in both statistics and data mining (see e.g. [24, 25, 28, 32, 34, 36, 41, 43]). In a dis-crete framework, work on mining episodes and sequential patterns (e.g. [1, 12, 26, 42]) has developed algorithms to identify particular configurations of discrete events clustered in time, in some cases obeying partial precedence constraints on their order. Finally, there is an interesting general relationship to work on traffic analysis in the areas of cryptography and security [55]; in that context, tempo-ral analysis of a message stream is crucial because the content of the messages has been explicitly obscured. the temporal information and the underlying content. The role of temporal data is clear; but the content of course plays an integral role as well: Section 3 deals with streams consisting of the response set for a particular query to a larger stream; and Section 4 consid-ers streams with batched arrivals, in which a particular subset of each batch is designated as relevant. And in fact, there is strong evidence that the interplay between content and time is crucial here that an arbitrary set of messages with same sequence of arrival times would not exhibit an equally strong set of bursts. Adapting a permutation test from Swan and Jensen [61], one can start with a complete e-mail corpus having arrival times tl, t2,... , tiv, choose a random permutation 7r, and shuffle the corpus so that message The resulting shuffled corpus has the same set of arrival times and the same messages, but the original correspondence between the two is broken; do equivalently strong "spurious" bursts appear in this new sequence? In fact, they clearly do not: when the weight of bursts for all words (with respect to ,A~) is computed using the e-mall corpus in Section 3, the total weight associated with the true corpus is more than an order of magnitude greater than the average total weight over 100 randomly shuffled versions (369,980 versus 25,141). Moreover, the shuffled versions exhibit almost no non-trivial hierarchical structure; the average total number of words generating bursts of intensity at least 2 (i.e. inducing trees I" with two or more levels below the root) is 16.7 over the randomly shuf-fled versions, compared with 3865 in the true corpus. 
The overall framework developed here can be naturally applied to Web usage data --for example, to clickstreams and search en-gine query logs, where bursts can correspond to a collective focus applied the methods discussed here to Web clickstream data col-lected by Gay et al. [19]. The dataset in [19] was compiled as part of a study of student usage of wireless laptops: The browser clicks of roughly 80 undergraduate students in two particular classes at Cornell were collected (with consent) from wireless laptops over a period of two and a half months in Spring 2000. Bursts with re-spect to .A:,. v can be computed by an enumerative method, as in Section 4: for every URL w, all bursts in the stream of visits to w are determined; the full set of bursts is then ordered by weight. Each burst, associated with a URL w, now has an additional quan-tity associated with it: the number of distinct users who visited w during the interval of the burst. This allows one to distinguish be-tween collective activity involving much of the class and that of volve at least 10 distinct users, then many of those with the highest weight involve the URLs of the on-line class reading assignments, centered on intervals shortly before and during the weekly sessions at which they were discussed. 
A final observation is that the use of a model based on state tran-sitions leads to bursts with sharp boundaries; they have clear begin-nings and ends. In particular, this means that for every burst, one can identify a single message on which the associated state transi-tion occurred. This is akin to the TDT study's notion of (retrospec-tive) first story detection [2], although in the automaton model of the present work, identifying initial messages does not constitute a separate problem since it follows directly from the definition of the state transitions. In the context of e-mall, the contents of such an initial message can often serve as a concentrated summary of the circumstances precipitating the burst --in other words, there is frequently something in the message itself to frame the flurry of message-sending that is about to occur. For example, one sees in "ITR" messages occurs at a single piece of e-mall on October 28, 1999; such a phenomenon suggests that this message may play an interesting role in the overall stream. And for messages on which bursts for several different terms are initiated simultaneously, this phenomenon is even more apparent; these messages often represent natural "landmarks" at the beginning of long-running episodes. In many domains, we are accumulating extensive and detailed records of our own communication and behavior. The work dis-cussed here has been motivated by the strong temporal character of this kind of data: it is punctuated by the sharp and sudden onset of particular episodes, and can be organized around rising and falling patterns of activity. In many cases, it can reveal more than we real-ize. And ultimately, the analysis of these underlying rhythms may offer a means of structuring the information that arises from our patterns of interacting and communicating. 
Acknowledgements. I thank Liilian Lee for valuable discussions and suggestions throughout the course of this work. [1] R. Agrawal, R. Srikant, "Mining sequential patterns," Proc. Intl. Conf. on Data Engineering, 1995. [2] J. Allan, J.G. Carbonell, G. Doddington, J. Yarnron, Y. Yang, 'Topic Detection and Tracking Pilot Study: Final Report;' Proe. DARPA Broadcast News Transcription and Understanding Workshop, Feb. 1998. [3] J. Allan, R. Papka, V. Lavrenko, "On-line new event detection and tracking," Proc. SIGIR Intl. Conf. Information Retrieval, 1998. [4] K. Becker, M. Cardoso, "Mall-by-Example: A visual query interface for managing large volumes of electronic messages,' Proc. 15th Brazilian Symposium on Databases, 2000. [5] D. Beeferman, A. Berger, J. Lafferty, "Statistical Models for Text Segmentation," Machine Learning 34(1999), pp. 177-210. [6] H. Berghel, "E-mail: The good, the bad, and the ugly;' Communications of the ACM, 40:4(April 1997), pp. 11-15. [7] A. Birrell, S. Perl, M. Schroeder, T. Wobber, The Pachyderm E-mail System, 1997, at http://www.research.compaq.com/SRC/pachyderm/. [8] T. Blanton, Ed., White House E-mail, New Press, 1995. [9] G. Boone, "Concept features in Re:Agent, an intelligent e-mall agent," Proc. 2nd Intl. Conf. Autonomous Agents, 1998. [10] C. Chatfield, The Analysis of lime Series: An Introduction, Chapman and Hall, 1996. [11] S. Chatman, Story and Discourse: Narrative Structure in Fiction and Film, Cornell Univ. Press, 1978. [12] D. Chudova, P. Smyth, "Unsupervised identification of sequential patterns under a Markov assumption;' KDD Workshop on Temporal Data Mining, 2001. [13] W. Cohen. "Learning rules that classify e-mall." Proc. AAAI Spring Syrup. Machine Learning and Information Access, 1996. [14] T. Cover, P. Hart, "Nearest neighbor pattern classification," IEEE Trans. Information Theory IT-13(1967), pp. 21-27. [15] W. Davison, L. Wall, S. Barber, trn, 1993 http:llweb.mit.edulafslsipb/projectltrnlsrcltrn-3.61. [16] R. Ehrich, J. Foith, "Representation of Random Waveforms by Relational 'Trees," IEEE Trans. Computers, C25:7(1976). [17] S. Fine, Y. Singer, N. Tishby, "The hierarchical hidden Markov model: Analysis and applications;' Machine Learning 32(1998). [18] E.M. Forster, Aspects of the Novel, Harcourt, Brace, and World, Inc. 1927. [19] G. Gay, M. Stefanone, M. Grace-Martin, H. Hembrooke, "The effect of wireless computing in collaborative learning environments," Intl. J. Human-Computer Interaction, to appear. [20] G. Genette, Narrative Discourse: An Essay in Method, 
English translation (J.E. Lewin), Cornell Univ. Press, 1980. [21] G. Genette, Narrative Discourse Revisited, English translation (J.E. Lewin), Cornell Univ. Press, 1988. [22] B. Grosz, C. Sidner, "Attention, intentions, and the structure of discourse," Computational Linguistics 12(1986). [23] T. Gruber, Hypermail, Enterprise Integration Technologies. [24] V. Guralnik, J. Srivastava, "Event detection from time series data," Intl. Conf. Knowledge Discovery and Data Mining, 1999. [25] J. Han, W. Gong, Y. Yin, "Mimng Segment-Wise Periodic Patterns in Time-Related Databases", Proc. Intl. Conf. 
Knowledge Discovery and Data Mining, 1998. [26] D. Hand, H. Mannila, P. Smyth, Principles of Data Mining, 
MIT Press, 2001. [27] S. Havre, B. Hetzler, L. Nowell, "ThemeRiver: Visualizing Theme Changes over Time," Proc. IEEE Symposium on 
Information Visualization, 2000. [28] D. Hawkins, "Point estimation of the parameters of piecewise regression models," Applied Statistics 25(1976) [29] B. Heckel, B. Hamann, "EmVis -A Visual e-Mail Analysis Tool," Proc. Workshop on New Paradigms in Information Visualization and Manipulation, in conjunction with Conf. on 
Information and Knowledge Management, 1997. [30] J. Helfman, C. Isbell, "Ishmail: Immediate identification of important information," AT&amp;T Labs Technical Report, 1995. [31] E. Horvitz, "Principles of Mixed-Initiative User Interfaces," 
Proc. ACM Conf. Human Factors in Computing Systems, 1999. [32] D. Hudson, "Fitting segmented curves whose join points have to be estimated," Journal of the American Statistical 
Association 61(1966) pp. 1097-1129. [33] EP. Kelly, "Notes on effective bandwidths," in Stochastic Networks: Theory and Applications, (F.P. Kelly, S. Zachary, I. 
Ziedins, eds.) Oxford Univ. Press, 1996. [34] E. Keogh, P. Smyth, "A probabilistic approach to fast pattern matching in time series databases," Proc. Intl. Conf. Knowledge 
Discovery and Data Mining, 1997. [35] J.l. Klein et al., Plaintiffs' Memorandum in Support of Proposed Final Judgment, United States of America v. Microsoft Corporation and State of New York, ex rel. Attorney General Eliot Spitzer, et al., v. Microsoft Corporation, Civil 
Actions No. 98-1232 (TPJ) and 98-1233 (TPJ), April 2000. [36] M. Last, Y. Klein, A. Kandel, "Knowledge Discovery in 
Time Series Databases," IEEE Transactions on Systems, Man, and Cybernetics 31B(2001). [37] V. Lavrenko, M. Schmill, D. Lawfie, P. Ogilvie, D. Jensen, J. Allan, "Mining of Concurrent Text and Time-Series," 
KDD-2000 Workshop on Text Mining, 2000. [38] D.D. Lewis, K.A. Knowles, "Threading electronic mail: A preliminary study," Inf. Proc. Management 33(1997). [39] S.S. Lukesh, "E-mall and potential loss to future archives and scholarship, or, The dog that didn't bark," First Monday 4(9) (September 1999), at http://firstmonday.org [40] P. Maes, "Agents that reduce work and information overload," Communications of the ACM 37:7(1994), pp. 30-40. [41 ] H. Mannila, M. Salmenkivi, "Finding simple intensity descriptions from event sequence data," Proc. Intl. Conf. on 
Knowledge Discovery and Data Mining, 2001. [42] H. Mannila, H. Toivonen, A.I. Verkamo, "Discovering frequent episodes in sequences," Proc. Intl. Conf. on 
Knowledge Discovery and Data Mining, 1995. temporal data," KDD Wkshp. Temporal Data Mining, 2001. 
Negative Effects of Electronic Communication on Social Life at Work," ACM Trans. Info. Sys. 12(1994), pp. 119-149. Wavelet-Based Text Visualization System," Proc. IEEE 
Visualization, 1998. Marciano, M. Wan, W. Schroeder, A. Gupta, "Collection-Based 
Persistent Digital Archives -Part 2," D-Lib Magazine, 6(2000). 
HMMs," Advances in Neural Information Processing Systems (NIPS) 14, 2001. From Supercomputer Researchers," Chronicle of Higher 
Education, August 24, 1999. investigation of learning issues in a mail agent interface," 
Applied Artificial Intelligence 11(1997), pp. 1-32. 
Trans. Office Automation Systems 6(3):232-254, 1988. selected applications in speech recognition," Proc. IEEE 77(1989). 
Proc. AAAI Workshop on Case-Based Reasoning, 1998. e-mail filtering," Proc. KDD Workshop on Text Mining, 2000. Bayesian approach to filtering junk email," Proc. AAAI 
Workshop on Learning for Text Categorization, 1998. organizing e-mail," Proc. Intl. Conf. Autonomous Agents, 1999. 
Proc. Intl. Conf. on Machine Learning, 2000. 
Waveforms as Trees," IEEE Transactions on Acoustics, Speech, and Signal Processing 38:2(1990) features from text," Proc. 8th Intl. Conf. on Information 
Knowledge Management, 1999. timelines," Proc. SIGIR Intl. Conf. Information Retrieval, 2000. with Statistical Models of Word Usage," KDD-2000 Workshop on Text Mining, 2000. personal information management of e-mail," Proc. ACM 
SIGCHI Conf. on Human Factors in Computing Systems, 1996. "Visualizing sequential patterns for text mining," Proc. IEEE 
Information Visualization, 2000 categorization methods for event tracking," Proc. SIGIR Intl. 
Conf. Information Retrieval, 2000. Retrospective and On-line Event Detection," Proc. SIGIR Intl. 
Conf. Information Retrieval, 1998. 
