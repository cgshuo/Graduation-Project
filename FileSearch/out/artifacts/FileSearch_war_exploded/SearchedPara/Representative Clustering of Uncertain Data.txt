 This paper targets the problem of computing meaningful clusterings from uncertain data sets. Existing methods for clustering uncertain data compute a single clustering with-out any indication of its quality and reliability; thus, deci-sions based on their results are questionable. In this paper, we describe a framework, based on possible-worlds seman-tics; when applied on an uncertain dataset, it computes a set of representative clusterings, each of which has a prob-abilistic guarantee not to exceed some maximum distance to the ground truth clustering, i.e., the clustering of the ac-tual (but unknown) data. Our framework can be combined with any existing clustering algorithm and it is the first to provide quality guarantees about its result. In addition, our experimental evaluation shows that our representative clus-terings have a much smaller deviation from the ground truth clustering than existing approaches, thus reducing the effect of uncertainty.
In a variety of application domains, our ability to unearth a wealth of new knowledge from a data set is impaired by unreliable, erroneous, obsolete, imprecise, and noisy data. Reasons and sources of such uncertainty are many. Sensing devices are inherently imprecise (e.g., due to signal noise, instrumental errors and transmission errors [15]). Moving objects can only be monitored sporadically, such that at a certain time the position of an object is not explicitly known [50]. Integration of data from heterogeneous sources may incur uncertainty, for example due to uncertain schema matchings between different data sources [3]. Uncertainty may also be injected to the data on purpose, for privacy preservation reasons [19].

Ignoring data uncertainty in a mining task (e.g., replacing any uncertain values by their expectations) may compromise the quality of the result. On the other hand, by considering the uncertainty directly in the mining process, we can assess the reliability of the result, giving the user a notion about its quality and giving an intuition of how likely it is identical, or at least similar, to the result of the mining task when applied to the true (but unknown) data values [48]. For instance, in association rule mining on uncertain data, confidence values of the probability that a given itemset is frequent are derived [5]. This notion of confidence allows the user to make a more educated judgement of the data, thus enhancing the underlying decision-making process.

This paper targets the problem of deriving a meaningful clustering from an uncertain dataset. For this purpose, our aim is not to develop a new clustering algorithm, but rather to allow clustering algorithms designed for certain data to return meaningful, reliable and correct results in the pres-ence of uncertainty. To illustrate the challenge that arises by considering uncertain data, consider the work-flow de-picted in Figure 1. Figure 1(a) shows a dataset containing six two-dimensional points, which correspond to the posi-tions of moving objects at some point of time t . The shaded region in Figure 1(a) corresponds to a lake, which none of the moving objects may cross. To cluster the locations of these objects, a domain expert may opt to choose a clustering al-gorithm C from a suite of available options (e.g., a density-based algorithm [34], such as DBSCAN [17] or HDBSCAN [8], or one of the numerous variants from the k-means family [27]). Assuming that the true locations of the objects are known, C can compute the clustering shown in Figure 1(b). However, the true locations of the objects could be unknown and we may only have access to the last reported observa-tions of these objects (e.g., by their GPS devices), shown as triangles in Figure 1(c). In such a scenario, an uncertainty data model is typically used to capture the distribution of the possible object locations. For instance, past observations as well as empirically learned moving patterns of an object can be used to obtain a probability function for the position at a time after the object X  X  last observation [16]. Examples of probability density functions (PDFs) around the observa-tions are shown in the Figure.

Object A , for instance, is likely to be moving around the lake (since movement inside the lake is impossible), while the movements of other objects are less constrained. If we follow a simplistic approach for clustering the data, by clustering the expected values of the objects according to the uncer-tain data model, then we may end up in deriving a cluster-ing as shown in Figure 1(c) which arbitrarily deviates from the clustering of the actual (but unknown) locations of the objects shown in Figure 1(b). Therefore, approaches that aggregate the information given by the uncertain model to expected values and then apply clustering may yield results of poor quality, due to information loss and potentially due to invalid input to the clustering process (e.g., the expected location of A after aggregating all its possible positions in Figure 1(d) is in the lake).

In this paper, our goal is neither the definition of new un-certain data models suited for clustering, nor the proposal of new clustering algorithms tailored for uncertain data. In-stead, we aim at making possible the application of any ex-isting clustering algorithm C on an uncertain database DB , based on any given uncertain data model for DB , assuming that C would be appropriate for DB , if DB was certain. For example, we take that the clustering of Figure 1(b) by algo-rithm C is the ideal one, but infeasible to derive, given that we do not know the actual locations of the objects. The objective of our framework is to use the data of Figure 1(c) and algorithm C to derive a clustering that has a probabilis-tic guarantee (according to the uncertain data model used) to be very similar to that of Figure 1(b).

Our approach performs sampling and then represents the original uncertain database DB as a set of sample determin-istic databases. We then run the clustering algorithm C on each of the sampled databases, to derive a set of possible clusterings PC . Our main contribution is to combine the resulting set PC into a concise set RPC of representative clusterings . Furthermore, using all clusterings in PC , we estimate a probability  X  of each representative clustering in RPC , defined as the probability that this representative clustering does not exceed some maximum distance  X  to the true clustering. Since  X  has to be estimated from sampled databases, we obtain a lower bound of  X  that is significant at a user specified level. We provide a methodology to derive an RPC for a given value of  X  ; and therefore impose quality constraints on the uncertain clustering results, unlike previ-ous approaches on uncertain data clustering [41, 21, 23, 38, 32, 22, 31, 36, 37] which cannot provide quality guarantees.
In summary, our contributions are as follows.  X  We propose a sampling-based solution to cluster uncertain data. This solution is generally applicable to all data do-mains and with any suitable uncertain data model, allow-ing the application of any existing clustering algorithm, originally designed for certain data. As opposed to previ-ous work on uncertain clustering, our approach conforms to the possible worlds semantics and also considers any dependencies between objects.  X  We present a methodology via which we can assess the quality of a clustering of a possible world compared to the true clustering of the data.  X  We show how the confidence of clustering results on possi-ble worlds can be improved by computing a set of multiple representative clusterings, each having a significant likeli-hood to resemble the true, unknown clustering.
 The rest of the paper is organized as follows. Section 2 surveys existing methods for clustering uncertain data. Sec-tion 3 gives general definitions used in the remainder of this work. Section 4 shows how we can estimate the probability of the clustering result on a possible world to be the clus-tering of the true data values. Section 5 shows how, from a set of possible clusterings, we can find representative clus-terings that are probabilistically guaranteed to be similar to the real clustering. Section 6 evaluates our framework experimentally. Section 7 concludes this work.
Clustering is undoubtedly one of the most important tools for unsupervised classification. A large number of cluster-ing algorithms has been developed, as reflected in numer-ous surveys [28, 34, 45]. Although clustering has proved its applicability in many different domains and scenarios, the problem of clustering uncertain data has only gained little attention so far. Uncertain data clustering approaches ei-ther use expected distances between objects or assume that the distances between different pairs of objects are indepen-dent. In this work we review these methods and discuss their drawbacks.
 Clustering Using Expected Distances. The main draw-back of approaches based on expected distances [41, 21, 23] is the information loss incurred by describing a complex prob-ability distance function by a single scalar. Considering ad-ditional moments of a probability distance function, such as deviation [23] works well in specific applications where objects have little uncertainty. Still, the quality of such ap-proaches cannot be assessed, rendering them inappropriate for applications where decisions are to be made based on the computed clustering. As an example consider the set-ting of Figure 2, having two certain objects A and B , and an uncertain object U having two possible values U 1 and U Now, assume a deterministic clustering algorithm C which clusters two objects only if there distance does not exceed dist ( A,U 1 ) (= dist ( B,U 2 )). Clearly, in the example of Fig-ure 2, there are two possible clusterings, either the clustering having cluster { A,U } and outlier B , or the clustering having cluster { U,B } and outlier A . The probabilities of these pos-sible clusterings equal the probabilities 0 &lt; P ( U 1 ) &lt; 1 and P ( U 2 ) = 1  X  P ( U 1 ) of alternatives U 1 and U 2 of U . However, using the expected distance between A and U given by
E ( dist ( A,U )) = P ( U 1 )  X  dist ( A,U 1 ) + P ( U 2 )  X  dist ( A,U objects A and U can never be located in the same cluster, since it holds that E ( dist ( A,U )) &gt; dist ( A,U 1 ). The same holds for B and U . Thus, for the example of Figure 2, an approach using expected distance yields a clustering, which is strictly impossible. Summarizing, the use of expected distances is a heuristic to obtain a single clustering that represents the whole realm of possible clusterings, however, the derived result is not necessarily similar to any of them. Most clustering algorithms using expected distances focus on improving efficiency rather than addressing this draw-back [38, 32, 41]. Gullo et al. [22] propose a method, called UK-medoids, for which they utilize the expected distance between two uncertain objects. Jiang et al. [31] propose to use the KL divergence between two uncertain objects which is often used to reflect similarity between two probabilis-tic density functions. They investigate the applicability to both K-means [40] and DBSCAN [17]. Since all these ap-proaches are based on expected distances, their results are not in accordance with the possible worlds semantics and do not carry any quality guarantee.
 Clustering Assuming Independent Distances. Kriegel and Pfeifle [36, 37] assume that pairwise distances between uncertain objects are mutually independent. This assump-tion may yield wrong results; in addition, the results are biased toward overestimating the size of clusters. In par-ticular, they rely on the following assumption on distances between uncertain objects: P ( X  X  Y ) denotes the probability that the distance be-tween the two uncertain objects X and Y is smaller than a threshold . However this assumption does not hold in the general case. For example, in Figure 2, the two random events A  X  U and U  X  B are negatively correlated: for the case where dist ( A,U 1 ) &lt; &lt; dist ( A,U 2 ), it even holds that both random events are mutually exclusive. That is, if U is close to A then it cannot be close to B and vice versa. Relaxing this assumption of independent distances yields a computationally hard problem, as a distance value may depend on a large number of uncertain objects.
 Discussion. To our knowledge, there is no previous work on uncertain data clustering that conforms to the possible worlds semantics. A likely reason for this that general induc-tion on uncertain data is a #P-hard problem [13]. In order to avoid the exponential run-time cost of considering all pos-sible worlds, a common approach to handle uncertain data, in general, is sampling [30]. Given a sample of instances of the database (each corresponding to a possible world), a query or data mining task can be performed on each of them, and common results can be returned, associated with confi-dences of these results to be equal (or sufficiently similar) to the result on the true (but unknown) data. This is exactly the approach that we are following in this paper. We provide a small number of representative clusterings that have high confidence values to be similar to the true (but unknown) clustering.
This section gives definitions for uncertain databases and clustering, which generalize all previous models and defi-nitions. While existing works [37, 36, 38, 9, 23] generally assume that objects are points in a multi-dimensional fea-ture space, we allow objects to have any abstract type O , where O is an object space, such as a multi-dimensional fea-ture space, the set of all strings, the set of all images, etc. Each uncertain object is then represented by a set of val-ues in O , each associated with a non-zero probability. This probability distribution can be a continuous probability den-sity function (PDF) or a discrete probability mass function (PMF). To model the uncertainty of an object in a general way that captures both continuous [36, 42] and discrete [49, 42] models, we use the following general definition. Definition 1 (Multivariate Uncertain Object).
 A multivariate uncertain object o is defined by two functions pdf o : O  X  IR + 0 and pmf o : O X  X  X  X  [0 , 1] such that Value  X  is used to model existentially uncertainty, i.e., with a probability of pmf o (  X  ) object o does not exist at all in the database. By setting either pdf o ( x ) or pmf o ( x ) to the zero function, which maps any value to zero, the above def-inition can simulate discrete and continuous models, while also allowing mixed models. 1
Consequently, if there is at least one uncertain object in a database, the state of the database becomes a random variable. To model the semantics of a database being a random variable, the concept of possible worlds is commonly used [43, 46, 54, 39]: an uncertain database is defined by a (potentially infinite) set of possible database states, called possible worlds . Each possible world is associated with its corresponding probability to be the true database state. Definition 2 (Uncertain Database).
 An uncertain database DB is defined by a set of uncertain objects DB = { o 1 ,...,o |DB| } spanning a (potentially infinite) set of possible worlds W and a constructive generation rule G to draw possible worlds from W in an unbiased way. The probability to draw a world w equals the probability P ( w ) of this world being the true (but unknown) world oracle( DB ) . Wherever independence is assumed between objects [25, 6, 10], the generation rule G is implicitly given by drawing samples from each object individually. In scenarios with interdependencies between uncertain objects (for example, expressed by a Bayesian network [44], or by lineage of rela-tions [2]), a possible world can be drawn by using the factor-ized representation of the Bayesian network, and iteratively drawing objects conditioned to previously drawn objects. 2 To the best of our knowledge, our assumption of having a generation rule G is met in all state-of-the-art uncertain database management systems. The task of clustering can be defined as follows.
Note that this definition avoids the (wrong) claim that a discrete distribution can be seen as a special case of a contin-uous distribution without any form of continuity correction.
The factorized representation guarantees that at each it-eration there must be one random variable for which the required conditions are met. Definition 3 (Clustering).
 A clustering C ( S ) of a set S = { a 1 ,...,a N } of determinis-tic objects is a partitioning of S into pairwise disjoint sub-sets C 1 ,...,C k  X  S , such that S 1  X  i  X  k C i = S . Each subset C , 1  X  i  X  k is called a cluster.
 This abstract definition of clustering intentionally omits any objective rules toward a  X  X ood clustering X , such as the re-quirement that similar objects should be in the same clus-ter. The reason is that our approach should be used in conjunction with any clustering algorithm C , independently to the algorithm X  X  objective. Due to the theoretical result that general query processing (or mining) on uncertain data is # P -complete [13], coupled with the fact that an uncer-tain database may yield a number of possible clusterings exponential in the size of the database, we now explore the possibility of using a Monte-Carlo approach to perform clus-tering over uncertain data.
Let DB be an uncertain database and let C be a clustering algorithm. Let X = { X 1 ,...,X | X | } be a multiset 3 of possi-ble worlds of DB generated from DB using generation rule G and let C ( X ) denote the multiset of clusterings obtained by clustering each sample world in X . We denote the set of dis-tinct clusterings in C ( X ) as the set PC of possible clusterings obtained from sample X . For any clustering C in PC , the support C.supp of C is defined as P | X | i =1 I ( C ( X i I (  X  ) is an indicator function that returns 1 if its operand is true and 0, otherwise. Simply speaking, C.supp is the num-ber of occurrences of clustering C in the multiset C ( X ). Lemma 1 (Approximation of the True Clustering).
 For any possible clustering C  X  PC , the probability  X  P ( C ) = | X | is an unbiased estimator the probability P ( C ) that C is the true clustering C (oracle( DB )) of DB ; i.e., E ( P ( C ) .
 Proof.
 E (  X  P ( C )) = E C.supp | X | = E Since the expectation of a non-random variable is the iden-tity, we obtain Since all sample databases X i are drawn independently, and since the expectation of a sum of independent random vari-ables is the sum of their expectations, we get: Due to the assumption that each sample X i is drawn unbi-ased from the distribution of all worlds of DB , which implies that E ( I ( X i = C )) = P ( X i ), we obtain
Due to independent sampling, the same sample may be drawn multiple times.

Such a straightforward sampling approach works well for small databases, including the running example depicted in Figure 2, where the number of possible clusterings C is rea-sonably small. In a large database setting, where the proba-bility of finding exactly the same clustering on two samples in X approaches zero, this approach becomes inapplicable. The reason is twofold. First, the probabilities P ( C ) of a clustering C being the true clustering of DB , become very small. Due to independent samplings X i , 1  X  i  X  | X | , the number of samples where C ( X i ) = C follows a binomial B (  X  = P ( C ) ,n = | X | ) distribution. Estimating the proba-bility parameter  X  of a binomial distribution given a sample, requires a very large sample size n if  X  is small. A rule of thumb is that n  X   X   X  5 [11, 52]. Second, the large number of possible clusterings combined with small probabilities makes the exact results meaningless for a user. A huge set of possi-ble clusterings, potentially exponentially large in the number of uncertain objects, where many may be very similar, yet different between each other, is of little use.
Our goal is to reduce the (potentially huge) set of cluster-ings produced by the Monte-Carlo approach to a small set of possible clusterings, which are diverse and at the same time guaranteed to be similar to the clustering on the real (but unknown) database. In Section 5.1, we discuss a general concept for determining one representative from a set, the medoid approach [47]. In Section 5.2, we generalize this ap-proach to select a set of multiple representative clusterings and show how we can estimate how well they can approxi-mate the real clustering.

It is a common trend in the clustering community to provide several, different ( X  X lternative X  [20, 18, 14]) results rather than just one. On the other hand, it is also con-sensus to avoid an abundance of redundant results [35, 55]. The eminent question is then, how many solutions to provide and how representative these solutions are. For the problem of clustering uncertain data, we therefore present, in Sec-tion 5.3, a methodology for selecting a set of representative clusterings of guaranteed quality.
Let PC denote the set of possible clusterings derived from sampled worlds X = { X 1 ,...,X n } . Let D be the distance | PC | X | PC | matrix such that Here, dist denotes a distance measure between two cluster-ings, such as, e.g., the Adjusted Rand Index (ARI) [26]. Similarity usually takes a value between 0 (no agreement) and 1 (identical partitionings) and can be converted to a distance after subtraction from 1.
 The median of PC can be defined as
Arguably, the median clustering can be the most represen-tative clustering out of all sampled clusterings C ( X i ). How-ever, we do not have confidence information for Median( PC ), i.e., the deviation of the true clustering C (oracle( DB )) from Median( PC ) is impossible to assess.

It is important to note that Median( PC ), albeit derived using expected distances between clusterings, does not suf-fer from the same drawbacks as existing works on cluster-ing uncertain data using expected object positions and ex-pected distances (cf. Section 2). The main difference is that Median( PC ) is a clustering derived from a possible database instance that was generated consistently to the uncertainty data model, i.e., considering the value distributions and stochastic dependencies between objects.
The possible clusterings of an uncertain database may be very heterogeneous; depending on object attribute values of a world, an individual cluster may become noise, may shatter into multiple clusters, or may be absorbed by an-other cluster in some worlds, but not in others. Such large changes in the overall clustering may be caused by minimal changes in the underlying dataset: the density of a criti-cal region may drop below the threshold of a density-based clustering algorithm; a partition-based cluster representa-tive may change slightly, yielding a new data partitioning and leading into a spiral of changes. Keeping this poten-tial heterogeneity of possible clusterings in mind, a single sample medoid clustering could be insufficient: it may be an unlikely pivot between a number of likely clusterings and it may not even be similar to the most likely possible worlds. Instead, a user may be more interested in a smaller set of clusterings, all having a significantly high probability to be similar (but not necessarily equal) to the true clustering, i.e., being representative. We define a representative clustering as follows: Definition 4 (Representative Clustering).
 Let DB be an uncertain database and let C be a clustering algorithm. We call a clustering C ( X i ) a  X  - X  -representative clustering, if the probability that the true clustering C (oracle( DB )) of DB has a distance dist( C ( X i ) , C (oracle( DB ))) of at most  X  is at least  X  . Lemma 2 (Approximation of Representatives).
 Let X = { X 1 ,...,X | X | } be a set of possible worlds of DB generated from DB using generation rule G and let dist be a distance measure on clusterings. Let PC be the set of clusterings obtained from X associated with their supports. The probability is an unbiased estimator of the probability that cluster representative X i has a distance of at most  X  to the true clustering of DB .

Proof. Analogous to Lemma 1, by substituting the pred-icate (dist( C ( X i ) , C ( X j ))  X   X  ) for ( C = X i ). Albeit unbiased, the probability  X  P ( X i , X  ) cannot be used di-rectly to assess the probability P ( X i , X  ) of cluster X a distance of at most  X  to the true clustering C (oracle( DB )). Thus, X i can not simply be returned as a  X  - X  =  X  P ( X representative according to Definition 4, because the estima-tor  X  P ( X i , X  ) may overestimate the true probability P ( X To return  X  - X  representative clusters to the user, our aim is to find a lower bound  X  P ( X i , X , X  ) such that we can guar-antee that P ( X i , X  )  X   X  P ( X i , X , X  ) with a probability of  X  , where  X  is a domain specific level of significance (typically,  X  = 0 . 95).

To derive such a significant lower bound of P ( X i , X  ) we may exploit the fact that sampled possible worlds were drawn independently. Therefore, the absolute number  X  P  X | X | of sampled worlds which are represented by X i follows a bi-nomial B ( P ( X i , X  ) , | X | ) distribution. To estimate the true probability P ( X i , X  ), given realization  X  P  X  | X | , we borrow techniques from statistics to obtain a one sided 1  X   X  con-fidence interval of the true probability P ( X i , X  ). A simple way of obtaining such confidence interval is by applying the central limit theorem to approximate a binomial distribution by a normal distribution.
 Definition 5 (  X  -Confidence Probabilities).
 Let DB be an uncertain database. For a set of drawn database instances X , and for a possible clustering C ( X i ) ,X i distance threshold  X  and a level of significance  X  , the proba-bility  X  P ( X i , X , X  ) =  X  P ( X i , X  )  X  z  X  is called  X  -confidence probability of  X  -representative X z is the 100  X  (1  X   X  ) percentile of the standard normal dis-tribution.
 The  X  -confidence probability  X  P ( X i , X , X  ) can be used to re-turn the clustering C ( X i ) as a  X  - X  -representative clustering to the user, as it guarantees, that by a user specified level of confidence  X  , the true probability P ( X i , X  ) is guaranteed to be larger than  X  P ( X i , X , X  ). To compute  X  P ( X i Definition 5 we argue that in our setting the central limit theorem is applicable, since the sample size | X | should be sufficiently large (  X  30 as a rule of thumb [7]). Furthermore, the probability P ( X i , X  ) should not be extremely small, since a cluster representative having an extremely small value of P ( X i , X  ) is meaningless and should not be returned to the user in the first place. In the case where all cluster repre-sentatives have an extremely small P ( X i , X  ) value, the pa-rameter  X  should be increased to obtain meaningful repre-sentatives. Yet, we note that more accurate approximations can be obtained using Wilson Score Intervals [53] or using exact binomial confidence intervals [11].
Using the techniques of Section 5.2 we can estimate, for a given  X  the probability of any drawn possible world to be a  X  -representative. In this section, we show how good representatives having a high confidence and low  X  can be extracted automatically from a set of sampled worlds. Fur-thermore, when more than a single representative world is returned, a requirement is to minimize redundancy between sets of worlds represented by each representative [12, 29, 55]. This requirement is important in order to avoid overly similar clustering representatives. To solve this challenge, we propose a general approach to first derive a clustering of the set of clusterings PC that have been obtained by apply-ing the domain specific clustering algorithm C to sampled possible worlds X . Then, a single representative clustering R is chosen from each cluster of PC such that  X  is min-imized while the fraction of drawn possible clusterings is maximized. Formally: Definition 6 (Representative Worlds Clustering).
 Let PC denote the set of possible clusterings derived from sampled worlds X = { X 1 ,...,X n } . Let D be a | X | X  | X | matrix such that Let C 0 be a metric clustering algorithm based on dist and let C ( PC ) denote the meta-clustering returned by applying C to the set PC of possible clusters. For each meta-cluster Y  X  C ( PC ) , a Representative Worlds Clustering returns a triple ( R, X ,  X  P ( R, X , X  )) , where R  X  Y is the clustering chosen to represent Y , and R is an  X  -significant representative (cf. Definition 5) with a probability  X  of at least  X  P ( R, X , X  ) . In Definition 6, two parameters are undefined, the choice of the clustering algorithm C 0 ( PC ) and a heuristic to obtain a representative from each meta-cluster in C 0 ( PC ). For the choice of clustering algorithm C 0 , any clustering algorithm which is appropriate for general metric spaces could be used [33, 28, 34]. For the problem of defining a representative for each a meta-cluster Y , we propose the following two heuristics. Our first heuristic requires all possible clusterings in a meta-cluster Y  X  X  0 ( PC ) to be represented.
 Definition 7 (Complete Representative).
 For a meta-cluster Y  X  X  0 ( PC ) , the complete representative is the clustering which has the minimum maximum distance to all other clusterings in Y .
 This representative R complete can be returned as a  X  - X  -repre-sentative with confidence probability  X  =  X  P ( R complete using a user specified level of confidence  X  as described in Section 5.2.

A drawback of the complete representative approach is that the value of  X  may grow arbitrarily large, being at least half of the corresponding clusters diameter. A  X  -representative having an overly large  X  value, such as an ARI-distance [26] value greater than 0 . 2, may have no semantic meaning to the user, as the space of clusterings represented by this  X  -representative grows too large to allow meaningful decision making. Furthermore, a large value of  X  yields overlapping clusters. For instance, for a pair of complete representatives R i and R j ,i 6 = j , where R i is an  X  i -representative and R an  X  j representative, it may hold that for a single sampled clustering X k  X  X that D ( X k ,R i )  X   X  i and D ( X k ,R This drawback of complete representatives can be particu-larly bad, if the underlying clustering algorithm C allows clusters to have a large diameter (e.g., C is k-means). In contrast, complete representatives may yield good results in settings where density-based clustering algorithms such as DBSCAN are used.

For the general case, we propose a different approach, where a maximum threshold for  X  is provided. This pa-rameter, which is specific to the chosen distance function dist, should be chosen in a way that a user should treat two clusterings, having a distance of no more than  X  as similar. Definition 8 (  X  max -Clustering).
 Given a  X  max threshold, for a cluster C  X  C 0 ( PC ) a  X  representative is a  X  - X  -representative, such that  X   X   X  given by R Again, this representative R  X  max can be returned as a  X  - X  -representative by computing a confidence probability  X  =  X  P ( R  X  max , X  max , X  ) with a user specified level of confidence  X  as described in Section 5.2.

The main drawback of  X  max clusterings is that large frac-tions of possible clusterings may not be assigned to any  X  -re-presentative. The semantics of such result, however, may be useful, indicating that a large fraction of possible clusterings deviate too much from other clusterings. This indication of high heterogeneity of possible clusterings has to be consid-ered when making decisions based on the uncertain data set DB .
The focus of this paper is to mitigate the effect of un-certainty by obtaining an uncertain clustering that is simi-lar to applying algorithm C on the real, unknown data set oracle( DB ), independent of the choice of C .
 Datasets and Ground Truth. Evaluations have been run on synthetic data as well as on the datasets summarized in Table 1. For reasons of comparability, we normalized all datsets to [0,1] in each dimension. In a preparation step, we apply a traditional (certain) clustering algorithm, DBSCAN [17], to obtain the ground-truth clustering C (oracle( DB )). We then tuned the parameters and MinPts in order to yield a high F -measure for predicting the class labels of each database object. Those parameters are specified along with the datasets in Table 1.

Then, we discarded the class-information from the datasets, and treated the result of C as the ground truth C (oracle( DB )). Recall that our goal is to compute clustering results on an uncertain version of each dataset similar to C (oracle( DB )), independent of the quality of C in terms of its F -measure. Yet, the parameters of C should have meaningful values in our setting, to avoid effects such as having only a single cluster or no clusters at all.
 Uncertainty Generation. In an uncertain setting we do not have access to the certain database oracle( DB ) and are rather given an uncertain database DB . Thus, for each object o  X  DB , we draw a new object using a multivari-ate Gaussian or multivariate uniform distribution. In both cases, we use a parameter ext to describe the uncertainty. In the Gaussian case we uniformly chose a standard deviation  X   X  [0; ext/ 4] in each dimension and generated a center of a generating probability distribution by drawing one sample point g from the Gaussian PDF with  X  = o . Using g as observation of o , we generate i  X  1 additional points from the normal distribution  X  = g . The resulting i points corre-spond to samples of an uncertain object observed at location g .

In case of uniform distribution, a rectangle r was con-structed having an extent chosen uniformly in the interval [0 ,ext ] in each dimension. The resulting new object u is chosen uniformly from this interval. Then, the rectangle r is centered at u and i  X  1 more points are drawn uniformly from r . In addition to o , which is guaranteed to be inside r by construction we generated i  X  1 additional points uni-formly distributed in r . All generated uncertain objects form the uncertain database DB .
 For our experiments, we used ext = 0 . 04 and i = 10. Our approach sampled | X | = 100 possible worlds, assuming mutual independence of objects.
 Algorithms. In our experiments we set the parameters of our framework to C = DBSCAN [17] and C 0 = PAM [33], a k-medoid variant, and dist = 1  X  ARI, i.e., a distance between clusterings, based on the Adjusted Rand Index [26].
As baseline, we use a Median Clustering (MC) of the data set, which performs DBSCAN on the uncertain objects by reducing each uncertain object to one single possible alter-native which corresponds to the median of its alternatives. This approach is a representative of na  X   X ve approaches [41, 21, 23] which reduce the uncertainty information of an un-certain object to a single point (the median in this case). A comparison partner from the literature is FDBSCAN [36]. The parameters were chosen identically for MC, FDBSCAN, and our approach.
 All algorithms were obtained from or implemented in the ELKI-Framework [1] and executed on a 64-Bit Linux Com-puter with eight cores at 3.40GHz and 32GB RAM.
Before evaluating the proposed approach in a broad exper-imental setting, we first demonstrate the difference regarding the result of the clustering task between our technique and previous work on clustering uncertain objects (represented by MC). For this purpose, we generated a toy example con-sisting of three Gaussian distributed point clouds { A,B,C } which represent our ground truth data. After adding Gaus-sian uncertainty, as described in the previous section, all objects consist of several sample points which can be cov-ered by a minimum bounding rectangle; these rectangles are shown in Figures 3 and 4.

Figure 3(a) illustrates the clustering of the original data set without uncertainty. Objects belonging to the same clus-(a) Clustering of original data (b) Median Clustering (MC) ter are plotted using the same color. Outliers are plotted in a different color. Figure 3(b) shows the result of MC, which yields a different clustering compared to the original one, since the lower two point clouds are merged to a single clus-ter.

Next consider the results of our approach when generat-ing four representative clusterings { X 1 ,...,X 4 } in Figure 4. First, note that the four results coarsely reflect the four expected possible results of a density based clustering ap-( { A, B, C } ). The corresponding confidence probabilities  X  P ( X i , X , X  ) (cf. Definition 5) are shown for each representa-tive. For instance, representative X 1 , shown in Figure 4(a), is an  X  = 0 . 95-significant representative having a probabil-ity of 0 . 38 to have an ARI-distance of at most  X  = 0 . 075 to the ground-truth clustering C (oracle( DB )).
 The real ARI-distances of the four representatives to C (oracle( DB )) are 0.038, 0.400, 0.404, and 0.851, respec-tively. In this toy example, the clustering with the smallest ARI to the base clustering has the highest probability. This is not always the case and our approach might also return a result having a large distance with the highest probability. Yet, our approach usually returns at least one possible clus-tering having a very high similarity with the true clustering. However, more importantly, unlike existing approaches, our approach is able to assess the probabilistic quality of its re-sults and can provide multiple representative clusterings for the user to choose from. Clustering Quality. Figure 5 illustrates the results under the default values on all tested datasets. Shown is the Ad-justed Rand Index (ARI) [26] to the certain clustering on the original dataset. Thus, a value of one means that the method produces the same clustering result on the uncer-tain data than on the certain data, whereas a value closer to zero means that the two clusterings differ drastically. As observed, in several cases the rather simple MC performs better than the more sophisticated FDBSCAN. This might be because of the shortcomings of FDBSCAN regarding the consideration of possible worlds semantics as discussed in Section 2. Yet still, although the MC approach returns a clustering of a possible world, it cannot assign any measure of confidence to it, possibly resulting in a highly unlikely world. This becomes obvious when revisiting the results in Figure 5. Even for the case were only one representative is returned by our method (REP1), this representative re-sembles the original clustering better and in addition it also carries a confidence about its similarity to the true clus-tering. For instances of our method with multiple repre-sentatives, the figure shows the ARI of the representative with the minimum distance. Thus, increasing the number of representatives (REP4, REP10) ensures that at least one representative resembles the original clustering very closely. Number of Representatives. An important question is how many representatives should be presented to the user. Presenting the user too few representatives may yield an in-sufficient understanding of the possible outcomes and the result may not contain a clustering which is close to the  X  X rue X  clustering at all, while presenting too many represen-tatives may overwhelm the user. In Figure 6, we show the averaged ARI over all considered datasets when increasing the number of representatives. Observe that the average ARI of all our representatives (weighted by the confidence of the representatives) decreases in comparison to the MC approach (we exclude the FDBSCAN in this graph due to its larger deviation). This can be explained by the diver-
Figure 6: ARI vs. the number of representatives. sity that increases with a higher number of representatives. On the other hand, the closest representative to the original clustering yields a higher score when increasing the number of representatives  X  as a larger number of representatives increases the likelihood of having a representative close to the ground truth clustering. Summarizing, our experiments show that the quality of our result gains only up to about ten representatives returned, after which no significant im-proval can be seen anymore. Furthermore, if becomes appar-ent that a set of four cluster representatives already yields fair results in most cases, while it can still be considered as concise enough to be represented to an average user. Number of Samples. In our next experiment, we investi-gate how many samples | X | are required in order to obtain significant results on the D31 data set [51] using the same pa-rameter setting as for yeast (cf. Table 1). For this purpose, we aggregated the  X  P ( X i , X , X  ) of all cluster representatives X i for  X  = 0 . 1,  X  = 0 . 95 for different values of | X | . The result is shown in Figure 7, where it can be observed that a larger sample size | X | increases the lower probability bounds obtained by Definition 5. More information on obtaining confidence intervals for a binomial probability function such as P ( X i , X  ) can be found in the literature [24]. Runtime. The runtime of our approach directly corre-sponds to the number of samples we utilize. Thus our ap-proach will always be slower in terms of runtime than other approaches like MC for the exchange of more valuable in-formation and insights into the dataset, which is normal for data mining tasks such as clustering. Thus we present, in Figure 8, which modules of the process effect runtime the most. We divided the procedure of finding cluster represen-tatives into three steps:  X  Cluster Samples: This step includes drawing possible in-stances of the database and clustering them using DB-
SCAN. This process is repeated until | X | = 100 instances have been processed.  X  Compute Pairwise Distances: During the last step, ARI-distances between clustered database instances are required several times, thus we precomputed the 100  X  99 2 pairwise distances.  X  k-Medoid Clustering: In this step, we perform k-medoid clustering of the sampled (certain) clusterings.

The first two steps of the overall procedure are the compu-tational bottleneck. Obviously, applying C a large number of times is computationally expensive. The second step of com-puting the pairwise distances of the clustering results usually takes less time. This step strongly depends on the character-istics of the outcome of the first step. Specifically, comput-ing the ARI-distance between two clusterings becomes more expensive if the clusterings contain more clusters. ARI is based on the pre-computation of the cluster contingency ta-ble which counts the number of objects in each pair of clus-ters of two clusterings. The number of clusters in a cluster-ing is of course dependent on the dataset and the parameter settings.
We presented a general solution for clustering of uncer-tain objects. Our challenge was to develop a framework making any clustering that has been developed for certain data applicable for the case of uncertain data. We ap-proached this challenge by employing a sampling approach to obtain a number of possible database instances from the uncertain database. Applying a domain specific clustering algorithm to each obtained database instance yields a (pos-sibly large) set of different clusterings. Therefore, the chal-lenge is to find a representative solution for all these possi-ble clusterings. For this purpose, we defined the notion of  X  - X  -representative clusterings: a  X  - X  -representative cluster-ing is a clustering having probability at least  X  to have a distance of at most  X  to the actual clustering of the data if the data were certain. Our solution follows a sampling approach, which returns clusterings that are guaranteed to be  X  - X  -representative clusterings at a user specified level of significance. To the best of our knowledge, our approach is the first to yield clusterings associated with confidences, al-lowing the user to assess the quality of the clustering result, and conforming to the possible worlds semantics. Further-more, by returning multiple representative clusterings to the user, we can improve the quality (and therefore usefulness) of results, as shown by our experimental study. [1] E. Achtert, H.-P. Kriegel, E. Schubert, and A. Zimek. [2] P. Agrawal, O. Benjelloun, A. D. Sarma, C. Hayworth, [3] P. Agrawal, A. D. Sarma, J. Ullman, and J. Widom. [4] K. Bache and M. Lichman. UCI machine learning [5] T. Bernecker, H.-P. Kriegel, M. Renz, F. Verhein, and [6] J. Boulos, N. Dalvi, B. Mandhani, S. Mathur, C. Re, [7] L. D. Brown, T. Cai, and A. DasGupta. Interval [8] R. J. G. B. Campello, D. Moulavi, and J. Sander. [9] M. Chau, R. Cheng, B. Kao, and J. Ng. Uncertain [10] R. Cheng, S. Singh, and S. Prabhakar. U-DBMS: a [11] C. Clopper and E. S. Pearson. Probable inference, the [12] Y. Cui, X. Z. Fern, and J. G. Dy. Non-redundant [13] N. Dalvi and D. Suciu. Efficient query evaluation on [14] X. H. Dang, I. Assent, and J. Bailey. Multiple [15] A. Deshpande, C. Guestrin, S. R. Madden, J. M. [16] T. Emrich, H.-P. Kriegel, N. Mamoulis, M. Renz, and [17] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [18] I. F  X  arber, S. G  X  unnemann, H.-P. Kriegel, P. Kr  X  oger, [19] G. Ghinita, P. Karras, P. Kalnis, and N. Mamoulis. [20] D. Gondek and T. Hofmann. Non-redundant [21] F. Gullo, G. Ponti, and A. Tagarelli. Clustering [22] F. Gullo, G. Ponti, and A. Tagarelli. Minimizing the [23] F. Gullo and A. Tagarelli. Uncertain centroid based [24] W. Hoeffding. Probability inequalities for sums of [25] J. Huang, L. Antova, C. Koch, and D. Olteanu. [26] L. Hubert and P. Arabie. Comparing partitions. J. [27] A. K. Jain. Data clustering: 50 years beyond k-means. [28] A. K. Jain, M. N. Murty, and P. J. Flynn. Data [29] P. Jain, R. Meka, and I. S. Dhillon. Simultaneous [30] R. Jampani, F. Xu, M. Wu, L. Perez, C. Jermaine, [31] B. Jiang, J. Pei, Y. Tao, and X. Lin. Clustering [32] B. Kao, S. D. Lee, D. W. Cheung, W. S. Ho, and [33] L. Kaufman and P. J. Rousseeuw. Finding Groups in [34] H.-P. Kriegel, P. Kr  X  oger, J. Sander, and A. Zimek. [35] H.-P. Kriegel, P. Kr  X  oger, and A. Zimek. Subspace [36] H.-P. Kriegel and M. Pfeifle. Density-based clustering [37] H.-P. Kriegel and M. Pfeifle. Hierarchical [38] S. D. Lee, B. Kao, and R. Cheng. Reducing uk-means [39] J. Li, B. Saha, and A. Deshpande. A unified approach [40] J. MacQueen. Some methods for classification and [41] W. K. Ngai, B. Kao, C. K. Chui, R. Cheng, M. Chau, [42] J. Pei, M. Hua, Y. Tao, and X. Lin. Query answering [43] A. D. Sarma, O. Benjelloun, A. Halevy, and [44] P. Sen, A. Deshpande, and L. Getoor. Prdb: Managing [45] K. Sim, V. Gopalkrishnan, A. Zimek, and G. Cong. A [46] M. A. Soliman, I. F. Ilyas, and K. C.-C. Chang. Top-k [47] A. Struyf, M. Hubert, and P. Rousseeuw. Clustering [48] L. Sun, R. Cheng, D. W. Cheung, and J. Cheng. [49] Y. Tao, R. Cheng, X. Xiao, W. K. Ngai, B. Kao, and [50] G. Trajcevski, R. Tamassia, P. Scheuermann, [51] C. J. Veenman, M. J. T. Reinders, and E. Backer. A [52] S. Wallis. Binomial confidence intervals and [53] E. B. Wilson. Probable inference, the law of [54] K. Yi, F. Li, G. Kollios, and D. Srivastava. Efficient [55] A. Zimek and J. Vreeken. The blind men and the
