 1. Introduction
Cluster analysis is a data analysis tool used for grouping data with similar characteristics. It has been used in data mining tasks such as unsupervised classification and data summation. The basic objective in cluster analysis is to discover natural groupings of objects. Cluster analysis techniques have been used in many areas such as qualitative interpretation and data compression, process monitoring, local model development, analysis of chemical com-pounds for combinatorial chemistry, discovering of clusters in DNA dinucleotides, classification of coals, manufacturing and produc-tion (process optimization and troubleshooting), medicine (several diagnostic information stored by hospital management systems), nuclear science, financial investment (stock indexes and prices, interest rates, credit card data, fraud detection), radar scanning and research and development planning, telecommunication network (calling patterns and fault management systems) ( Kao et al., 2008;
Cao and Cios, 2008; Zalik, 2008; Krishna and Murty, 1999; Mualik and Bandyopadhyay, 2000; Fathian and Amiri, 2007; Laszlo and
Mukherjee, 2007; Shelokar et al., 2004; Ng and Wong, 2002; Sung and Jin, 2000; Niknam et al., 2008a, 2008b, 2009; Niknam and
Amiri, 2010; Bahmani Firouzi et al., 2010 ). Generally cluster analysis algorithms have been utilized where the huge data are stored. Data clustering algorithms can be either hierarchical or partitional. In this paper we focus on the partitional clustering and in particular, a popular partitional clustering method called K-means clustering. The K-mean clustering algorithm is one of the most efficient clustering algorithms ( Kao et al., 2008; Cao and Cios, 2008; Zalik, 2008; Krishna and Murty, 1999; Mualik and Bandyopadhyay, 2000; Fathian and Amiri, 2007; Laszlo and Mukherjee, 2007; Shelokar et al., 2004; Ng and Wong, 2002;
Sung and Jin, 2000; Niknam et al., 2008a, 2008b, 2009; Niknam and Amiri, 2010; Bahmani Firouzi et al., 2010 ). It starts by initializing the K cluster centers. The input vectors (data points) are assigned to one of the existing clusters according to the square of the Euclidean distance from the clusters. The mean (centroid) of each cluster is then computed in order to update the cluster center.
This update occurs as a result of the change in the membership of each cluster. The process of reassigning the input vectors and the update of the cluster centers are repeated until the values of the cluster centers do not change. However, the K-means algorithm suffers from several drawbacks. The objective function of the K-means is not convex and it may contain many local minima.
Consequently, in the process of minimizing the objective function, there exists a possibility of getting stuck at local minima. The outputsof the K-means algorithm, therefore, heavilydepend onthe initial choice of the cluster centers. To overcome this drawback, many clustering algorithms based on evolutionary algorithms such as GA, TS and SA have been introduced ( Kao et al., 2008; Cao and Cios, 2008; Zalik, 2008; Krishna and Murty, 1999; Mualik and Bandyopadhyay, 2000; Fathian and Amiri, 2007; Laszlo and Mukherjee, 2007; Shelokar et al., 2004; Ng and Wong, 2002; Sung and Jin, 2000; Niknam et al., 2008a, 2008b, 2009; Niknam and Amiri, 2010; Bahmani Firouzi et al., 2010 ). For instance, Kao et al. (2008) have proposed a hybrid technique based on combining the K-means algorithm, Nelder-Mead simplex search, and PSO for cluster analysis. Cao and Cios (2008) have presented a hybrid algorithm according to the combination of GA, K-means and logarithmic regression expectation maximization. Zalik (2008) has introduced a K-means algorithm that performs correct cluster-ing without pre-assigning the exact number of clusters. Krishna and Murty (1999) have presented an approach called genetic K-means algorithm for clustering analysis. Mualik and Bandyopadhyay (2000) have proposed a genetic algorithm based method to solve the clustering problem and experiment on synthetic and real life data sets to evaluate the performance. It defines a basic mutation operator specific to clustering called distance-based mutation. Fathian and Amiri (2007) have proposed the HBMO algorithm to solve the clustering problem. A genetic algorithm that exchanges neighboring centers for K-means clus-tering has presented by Laszlo and Mukherjee (2007) . Shelokar et al. (2004) have introduced an evolutionary algorithm based on ACO algorithm for clustering problem. Ng and Sung have proposed an approach based on TS for cluster analysis ( Ng and Wong, 2002; Sung and Jin, 2000 ). Niknam et al. (2008a, 2008b) have presented a hybrid evolutionary optimization algorithm based on the combi-nation of ACO and SA to solve the clustering problem. Niknam et al. (2009) have presented a hybrid evolutionary algorithm based on PSO and SA to find optimal cluster centers. Niknam and Amiri (2010) have proposed a hybrid algorithm based on a fuzzy adaptive PSO, ACO and K-means for cluster analysis. Bahmani Firouzi et al. (2010) have introduced a hybrid evolutionary algorithm based on combining PSO, SA and K-means to find optimal solution.
However, most of evolutionary methods such as GA, TS, etc, are typically very slow to find optimum solution. Recently researchers have presented new evolutionary methods such as ACO, PSO and MICA to solve hard optimization problems which not only have a better response but also converge very quickly in comparison with ordinaryevolutionary methods( Kao et al., 2008; Cao and Cios, 2008; Zalik, 2008; Krishna and Murty, 1999; Mualik and Bandyopadhyay, 2000; Fathian and Amiri, 2007; Laszlo and Mukherjee, 2007; Shelokar et al., 2004; Ng and Wong, 2002; Sung and Jin, 2000; Niknam et al., 2008a, 2008b, 2009; Niknam and Amiri, 2010; Bahmani Firouzi et al., 2010; Morales and Erazo, 2009; Sakai and Imiya, 2009; Twellmann et al., 2008 ). MICA is a novel algorithm which has ability in dealing with different types of optimization problems. The basic idea in this algorithm is to divide countries into two types: imperialist states and colonies ( Rajabioun et al., 2008a, 2008b ; Atashpaz-Gargari and Lucas, 2007a, 2007b ; Atashpaz-Gargari et al., 2008a, 2008b ; Roshanaei et al., 2008 ; Jasour et al., 2008 . Imperialistic competition and assimilation policy cause the colonies to converge to optimum position. In this approach, initial countries are generated by using evolutionary algorithm and MICA X  X  movement rule is applied to collapse weak empires and they are taken over by powerfulones. MICA should be taken into account as a powerful technique. Nevertheless, it may be trapped in local optima especially when numbers of imperialists increase. To alleviate this drawback, mutation can help to divert the movement of colonies toward their relevant imperialist into new positions also a chaotic local search (CLS) isused toget rid of the local optima. This approach provides better opportunity of exploring for colonies. To use the benefits of K-means and MICA, and reduce their disadvantages a novel hybrid evolutionary optimization method, called Hybrid K-MICA is presented in this paper, for optimum clustering N objects into K clusters. This hybrid algorithm not only has a better response but also converges more quickly than ordinary evolutionary algo-rithms. In this method, after generating initial countries, K-means is applied to improve the position of colonies.

The main contributions of the paper are as follows: (i) present a modified ICA algorithm, (ii) combine the modified ICA algorithm with a CLS algorithm and (iii) present a new algorithm for cluster analysis.

The paper is organized as following: In Section 2, the cluster analysis problem is discussed. In Section 3 Imperialist Competitive Algorithm is introduced. In Sections 4 X 6, modified MICA, the Hybrid K-MICA and application of Hybrid K-MICA in clustering are shown, respectively. In Section 7, the feasibility of the Hybrid K-MICA is demonstrated and compared with K-MICA, MICA-K, ICA, MICA, ACO, PSO, SA, GA, TS, HBMO and K-means for different data sets. Finally, Section 8 includes summary and the conclusion. 2. Cluster analysis problem
K-means is one of the simplest unsupervised learning algo-rithms. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume K clusters) fixed as a priori. The main idea is to determine K centroids. These centroids should be placed in a cunning way as different locations cause different results. So, the best choice is to place them as much as possible far away from each other. The next step is to take each point referring to a given data set and associate it to the nearest centroid. When no point is left out, the first step is completed and an early grouping is done. At this point we need to recalculate K new centroids as centers of the clusters resulting from the previous step. After we have these K new centroids, a new binding has to be done between the same data set points and the nearest new centroid. A loop has been generated. As a result of this loop, we may notice that the K centroids change their location step by step until no more changes are done. In other words, centroids do not move any further. Finally, the goal of this algorithm is to minimize an objective function , which in this case is a squared error function ( Morales and Erazo, 2009; Sakai and Imiya, 2009; Twellmann et al., 2008 ).

The objective function has been calculated as follows: cost  X  X  X  X  where : Y i  X  X j : is a chosen distance measurement between a data input Y i and the cluster center X j . N and K are the number of input dataandthenumberofclustercenters,respectively. Fig.1 showsits flowchart.
 The algorithm is composed of the following steps:
Place K points into the space represented by the objects that are clustered. These points represent initial group centroids. Assign each object to the group that has the closest centroid.
When all objects have been assigned, recalculate the positions of the K centroids.

Repeat Steps 2 and 3 until the centroids are immobilized. 3. Original ICA
ICA is a population-based stochastic search algorithm. It has been introduced by Atashpaz and Lucas, recently ( Rajabioun et al., 2008a, 2008b; Atashpaz-Gargari and Lucas, 2007a, 2007b; Atashpaz-Gargari et al., 2008a, 2008b; Roshanaei et al., 2008; Jasour et al., 2008 ). Since then, it is used to solve some kinds of optimization problem ( Rajabioun et al., 2008a, 2008b; Atashpaz-Gargari and Lucas, 2007a, 2007b; Atashpaz-Gargari et al., 2008a, 2008b; Roshanaei et al., 2008; Jasour et al., 2008 ). The algorithm is inspired by imperialistic competition. It attempts to present the social policy of imperialisms to control more countries and use their sources when colonies are dominated by some rules. If one empire loses its power, the rest of them will compete to take its possession. In ICA, this process is simulated by individuals that are known as countries.

This algorithm starts with a randomly initial population and objective function which is computed for them. The most powerful countries are selected as imperialists and the others are colonies of these imperialists .Then the competition between imperialists take place to get more colonies .The best imperialist has more chance to possess more colonies. Then one imperialist with its colonies makes an empire. Fig. 2 shows the initial populations of each empire ( Rajabioun et al., 2008a, 2008b; Atashpaz-Gargari and Lucas, 2007a, 2007b; Atashpaz-Gargari et al., 2008a, 2008b;
Roshanaei et al., 2008; Jasour et al., 2008 ). If the empire is bigger, its colonies are greater and the weaker ones are less. In this figure
Imperialist 1 is the most powerful and has the greatest number of colonies.

After dividing colonies between imperialists, these colonies approach their related imperialist countries. Fig. 3 represents this movement. Based on this concept each colony moves toward the imperialist by a units and reaches its new position. Where a is a random variable with uniform (or any proper) distribution, b ,a number greater than 1, causes colonies move toward their i=1 object. imperialists from different direction and S is the distance between colony and imperialist a U  X  0 , b S  X  X  2  X 
If after this movement one of the colonies possess more power than its relevant imperialist, they will exchange their positions. To begin the competition between empires, total objective function of each empire should be calculated. It depends on objective function of both an imperialist and its colonies. Then the competition starts, the weakest empire loses its possession and powerful ones try to the most powerful empire will take the possession of other empires and wins the competition.

To apply the ICA for clustering, the following steps have to be taken ( Rajabioun et al., 2008a ):
Step 1: The initial population for each empire should be generated randomly.
 Step 2: Move the colonies toward their relevant imperialist.
Step 3:Exchangethepositionofacolonyandtheimperialistifits cost is lower.
 Step 4: Compute the objective function of all empires.
Step 5: Pick the weakest colony and give it to one of the best empires.
 Step 6: Eliminate the powerless empires.
 Step 7: If there is just one empire, stop, if not go to 2.
The last Imperialist is the solution of the problem. 4. Modified ICA
In order to improve the convergence velocity and accuracy of the ICA, this article recommends a modified imperialist competi-tive algorithm (MICA). Premature convergence may occur under differentsituations:thepopulationconvergestolocaloptimaofthe objective function or the search algorithm proceeds slowly or does not proceed at all. Mutation is a powerful strategy which diversifies the ICA population and improves the ICA X  X  performance on pre-venting premature convergence to local minima. In this article a new mutation operator is used.

During the assimilation policy each colony ( X ) moves toward its relevant imperialist by a unit, where the initial distance between number of iteration): a U  X  0 , b S  X  X
X t j and X t  X  1 move , j are j th colony of each empire. After this movement for each colony X , a mutant colony X t  X  1 mut is generated as follows ( Atashpaz-Gargari and Lucas, 2007a ): X X Then the selected colony would be: X x where rand (.) is a random number between 0 and 1, g is a number less than 1. m 1, m 2, m 3 are three individuals which are selected from initial colonies randomly. In order to cover the entire colonies uniformly, it is better to select them as m 1 a m 2 K is the number of clusters and the dimension of each cluster center will be d .

To choose the best colony between X move, j and X new, j to replace j th colony ( X j ), objective function is used X 5. Hybrid K-MICA
As mentioned before, K-means is used for its easiness and simplicity for applications. However, it has some drawbacks. First, its result may depend on initial values. Also, it may converge to local minimum. Recently, numerous ideas have been used to alleviate this drawback by using global optimization algorithms suchasGA( KrishnaandMurty,1999 ),TS( NgandWong,2002 ),PSO ( Kao et al., 2008 ), hybrid PSO-SA ( Niknam et al., 2009 ), hybrid PSO-ACO-K ( Bahmani Firouzi et al., 2010 ), HBMO ( Fathian and Amiri, 2007 ), ACO ( Shelokar et al., 2004 ), hybrid ACO-SA ( Niknam et al., 2008a, 2008b ), PSO-SA-K ( Morales and Erazo, 2009 ) and hybrid K-PSO ( Kao et al., 2008 ). In this article a new algorithm has been presented which mix both algorithms K-means and MICA. There are different methods for combination K-means with MICA. In the first case K-means is used to generate the population, its output initializes MICA. The second type, MICA initializes the population and competition will be done, the last remaining empire will be given to K-means. And the last one, population is generated with MICA and initial empires form, then K-means is applied to improve the position of empire X  X  colonies and imperialists, the result of this algorithm, again, will be given to MICA. Although the results of these procedures are better than the MICA, however the convergence speed and accuracy of the last one is the best.
The final algorithm is called Hybrid K-MICA. According to original ICA, first primary population generates and then empires with their possessions take place. Applying K-means to each empire causes us to improve the initial population of colonies. It makes the hybrid algorithm converges more quickly and prevents it from falling into local optima. The outputs of K-means form the initial empires of modified ICA.

To improve the income of algorithm, it is better, that the powerless imperialist would not be removed when it loses all possessions. This imperialist is one of the best answers and that can be contributed in imperialistic competition as a weak colony or to be given to the powerful empire. Flowchart and pseudo-code of these proposed algorithms are shown in Figs. 4 and 5 , respectively. 6. Application of hybrid K-MICA on clustering
In this section, the application of Hybrid K-MICA on clustering problem is presented. To approach, the following steps should be taken and repeated. There are some inputs data which would be cluster and the control variables are the cluster centers. Step 1: Generate an initial population
An initial population of input data is generated by chaos initialization as follows: Population  X  X  X  Country i  X   X  Center 1 , Center 2 , ... , Center K i  X  1 , 2 , ... , N em p ire colony Yes H  X  k d X  X  X  x 1 0 , x 2 0 , ... , x H 0 x  X  rand  X  :  X  X  x j max x j min  X  X  x j min , j  X  1 , 2 , ... , H X  X  X  x 1 i , x 2 i , ... , x H i , i  X  2 , ... , N intial x  X  4 x j i 1  X  1 x j i 1  X  , j  X  1 , 2 , ... , H x where Center j is j th cluster center for i th country. X countries. N initial is the numberof populationand d isthe dimension maximum and minimum value of each point referring to the j th cluster center which are in order. K is the number of clusters. H is the number of state variables. X o is an initial solution. Step 2: Calculate objective function value
Suppose that we have N sample feature vectors. The objective function is evaluated for each country as follows: Step 2-1: i  X  1 and Objec  X  0.
 Step 2-2: select the i th sample.
 Step 2-3: calculate the distances between the i th sample and Center j ( j  X  1, y , K ).

Step 2-4: add the value of Objec with the minimum distance calculated in Step 2-3 ( Objec  X  Object  X  min  X  Center i Y 2 , ::: , K  X  ).

Step 2-5: if all samples have been selected, go to the next step, otherwise i  X  i +1 and return to step 2-2.
 Step 2-6: Cost ( X )  X  Objec.
 The objective function is calculated mathematically as below: Cos  X  X  X  X   X  N  X  number of input data  X  X  8  X 
Step 3: Sort the initial population based on the objective function values.

The initial population is ascended based on the value of their objective function.
 Step 4: Select the imperialist states.

Countries that have the minimum objective function are selected as the imperialist states and the remaining ones form the colonies of these imperialists.
 Step 5: Divide colonies among imperialist.

Based on power of each imperialist the colonies are divided amongthem.Thepowerofeachimperialistcalculatedasfollowings:
C  X  min p  X 
C  X  round f p n  X  N col  X g X  11  X 
In above equations, cost n is the cost of n th imperialist and C normalized cost of each one. The normalized power of each imperialist introduced as p n , then the initial number of colonies for each empire will be C norm n where N col and N imp are number of all colonies and imperialists.
 Step 6: Use K-means algorithm for each empire.

Step 7: Move colonies toward their imperialist states as described in Section 3.

Step 8: Use mutation to change the direction of colonies. It is mentioned in modified ICA.
 Step 9: Check the cost of all colonies in each empire.
During the previous steps cost of each colony might have changed .Check the cost of all colonies of an empire if there is one that have a lower cost than its relevant imperialist, exchange their position. Step 10: Check total cost of each empire.

Cost of each empire depends on power of both imperialist and its colonies. It is calculated as follows:
TC  X  cost  X  imperialist n  X  X  x mean f cost  X  colonies of empire
TC n is the total cost of n th empire, x is an attenuation coefficient between 0 and 1 to reduce the effect of colonies cost. Step 11: Do imperialistic competition
All empires according their power (total cost), try to get colonies of weakest empire.

TC n  X  TC n max j f TC j g X  13  X  P  X  where TC norm n is normalized total cost of n th empire and the possession probability of each empire is P P n
Theroulettewheelcanbeusedforstochasticselectionofthewinner empire which will dominate the weakest colony of weakest empire.
To perform Roulette Wheel algorithm, it is necessary to calculate cumulative probability as follows: C  X 
According to this equation cumulative probability for n  X  1is equal to its probability, while for the last n it corresponds to one.
Then a random number with uniform distribution generates and compares with all C P n . Each sector with higher probability will have morechancetobechosen.Thereforethewinnerempirewillspecify.
As it is mentioned to use Roulette Wheel algorithm, computing cumulative distribution function is essential. To reduce this time consuming step an approach has been presented as below: P  X  X  P
R  X  X  r 1 , r 2 , ... , r N imp , r 1 , r 2 , ... , r N imp U  X  0 , 1  X  X  16  X 
D  X  P R  X  X  D 1 , D 2 , ... , D N imp
P is the vector of possession probability of all empires and R is a vector with uniformly distributed random numbers. Maximum index in D shows Winner Empire that gets the colony.

After realizing the winner empire, the weakest colony of the weakest empire will be given to the winner one. Then we should subtract one of the populations of this weak empire and add one to the winner X  X  population.
 Step 12: Remove weakest empire.

If there is any empire without colony, eliminate it. Replace one of the weakest colonies of best empire (low cost) with this imperialist.

Step 13: Apply chaotic local search (CLS) to search around the global solution
ICA has gained much attention and widespread applications in different fields. However, it often converges to local optima. In order to avoid this shortcoming, a CLS algorithm is used to search around the global solution in the paper. CLS is based on the logistic equation as follows:
Cx  X  X  cx 1 i , cx 2 i , ... , cx H i 1 H , i  X  0 , 1 , 2 , ... , N cx  X  4 cx j i  X  1 cx j i  X  , j  X  1 , 2 , ... , H cx  X  rand  X  :  X  cx Z  X  0 , 1 , cx j 0 = 2f 0 : 25 : 0 : 5 , 0 : 75 g
In the CLS, the best solution is considered as an initial solution ( X 0 ) for CLS. X 0 cls is scaled into (0,1) according to the following equation:
X
Cx  X  X  cx 1 0 , cx 2 0 , ... , cx H 0  X  19  X  cx  X  The chaos population for CLS is generated as follows:
X x  X  cx j i 1  X  x j max x j min  X  X  x j min , j  X  1 , 2 , ... , H where cx j i indicates the j th chaotic variable, N choas individuals for CLS. rand(.) is a random number between zero and one.
Theobjectivefunctionisevaluated for allindividuals ofCLS. One country selected randomly is replaced with the best solution among them.
 Step 14: Check number of empire.

If there is just one empire remained, stop. Else go to step 7. 7. Experimental results
The experimental results comparing the Hybrid K-MICA clustering algorithm with several typical stochastic algorithms including K-MICA,
MICA-K, MICA, ICA, ACO, PSO, SA, GA, TS, HBMO and K-means are presented.Theyareusedforfourreal-lifedatasets( Iris, Wine, Vowel and Contraceptive Method Choice ( CMC )), which are described as follows:
Iris data ( N  X  150, d  X  4, K  X  3): The Iris flower data set is a multivariate data set introduced by Sir Ronald Aylmer Fisher (1936) as an example of discriminant analysis. The data set consists of 50 samples from each of three species of Iris flowers ( Iris setosa , Iris virginica and Iris versicolor ). For each species there are 50 observations including sepal length, sepal width, petal length, and petal width in centimeters. Based on the combination of the four features, Fisher developed a linear discriminant model to distin-guish the species from each other. It is used as a typical test for many other classification techniques. It has three classes (with some overlap between classes 2 and 3). Therefore, the value of K is chosen to be 3 for this data ( Bahmani Firouzi et al., 2010 ).
Wine data ( N  X  178, d  X  13, K  X  3) : This isthe winedata set,which is also taken from MCI laboratory. These data are the results of a chemical analysis of wines grown in the same region in Italy extracted from three different cultivars. There are 178 instances with 13 numeric attributes in wine data set. 106 instances are for training, 36 for validation, and 36 for testing. All attributes are continuous.Therearethreeclassescorrespondingtothreedifferent cultivars. The attributes are alcohol, malic acid, ash, alcalinity of ash, magnesium, total phenols, flavanoids, nonflavanoid phenols, proanthocyanins, color intensity, hue, OD280/OD315 of diluted wines and proline. There are no missing attribute values ( Bahmani Firouzi et al., 2010 ).

Contraceptive Method Choice ( N  X  1473, d  X  10, K  X  3): This data set is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey. The samples were married women who either were not pregnant or did not know if they were at the time of interview. The problem is to predict the current contraceptive method choice (no use of long-term methods, or short-term methods) of a woman based on her demographic and socio-economic characteristics. The attributes are wife X  X  age, wife X  X  education, husband X  X  education, number of children ever born, wife X  X  religion, wife X  X  now working, husband X  X  occupation, stan-dard-of-living index, media exposure, and contraceptive method used ( Bahmani Firouzi et al., 2010 ).
 Vowel data set ( N  X  871, d  X  3, K  X  6). This data set consists of 871
Indian Telugu vowel sounds. There are six overlapping vowel classes and three input features. These data sets cover examples of data of low, medium and high dimensions. This consists of a three dimensional array including speaker, vowel, input. The speakers are indexed by integers 0 X 89. The vowels are indexed by integers 0 X 10. For each utterance, there are ten floating-point input values, with array indices 0 X 9. All entries are integers ( Bahmani Firouzi et al., 2010 ).
 The parameters required for implementation of the Hybrid
K-MICAalgorithmare b , x , a , N pop , N imp .Theresultsof10runsofthe algorithm for iris data set with different parameters are depicted in the Table 1 . It illustrates that this hybrid algorithm have a little dependency on variation of the parameters.
 The algorithms are implemented by using Matlab 7.6 on a Pentium IV, 2.8 GHz, 1 GB RAM computer.
 Tables 2, 4, 6 and 8 , present a comparison among the results of
Hybrid K-MICA, K-MICA, MICA-K, MICA, ICA, ACO ( Shelokar et al., 2004; Niknam et al., 2008a, 2008b, 2009; Niknam and Amiri, 2010; Bahmani Firouzi et al., 2010 ), PSO ( Kao et al., 2008 ), SA ( Niknam et al., 2008a ), GA ( Krishna and Murty, 1999 ), TS( Ng and
Wong, 2002 ), HBMO ( Fathian and Amiri, 2007 ) and K-means ( Niknam et al., 2008a, 2008b, 2009; Niknam and Amiri, 2010;
Bahmani Firouzi et al., 2010; Morales and Erazo, 2009 ) for 100 random tails on four real-life data sets. Centers related to the best results for Hybrid K-MICA on these data sets are shown in Tables 3, 5, 7 and 9 .
 The simulation results given in Tables 2 X 9 show that Hybrid
K-MICA is very precise and reliable. In other words, it provides the optimum value and small standard deviation in comparison to those of other methods. The results obtained on the iris data set show that Hybrid K-MICA converges to the global optimum of 96.6554 each time while the best solutions of K-MICA, MICA-K,
MICA, ICA, PSO, SA, TS, GA, ACO, HBMO and K-means are 96.6564, 96.6556, 96.6562, 96.6997, 96.8942, 97.4573, 97.3659, 113.9865, 97.1007,96.7520and97.3330.Thestandarddeviationofthefitness function for this algorithm is zero, which is significantly less than other methods. Table 4 indicates the results gained on wine data set. It brings us into the conclusion that although the basis of Hybrid K-MICA, K-MICA and MICA-K are the same but this new hybrid algorithm converges to the best global solution and has a better average and worst solution in comparison with those two methods. For CMC data set, the standard deviation of K-MICA, MICA-K, MICA, ICA, PSO, SA, TS, GA, ACO, HBMO and K-means are 1.111793, 1.268275, 7.397884, 8.000562, 46.959690, 50.867200, 40.84568, 50.3694, 45.634700, 12.690 and 47.16 while this is 0.05354 for Hybrid K-MICA which shows much difference compared to others. For the vowel data set, the best global solution, the average and worst solutions and standard deviation of the Hybrid K-MICA algorithm are 148,967.24, 149,100.35, 149,116.72 and 8.583 which are much smaller than those of other algorithms.
Comparing results of ICA and MI CA depict the effect of applying mutation. For instance the consequ ences of these two algorithms on Iris data set are 96.6562, 96.6664, 96.6919 and 0.0114455 for MICA and 96.6997, 96.8466, 97.0059 and 0.1114908 for ICA which show better results on the best, aver age and the worst solutions and standard deviation, respectively. The results of Hybrid K-MICA, K-MICA and MICA-K in comparison t o MICA display the advantages of using K-means. For vowel data set, the best global solutions for Hybrid K-MICA, K-MICA, MICA-K and MICA are 148,967.2408, 149,279.9922, 149,244.6165 and 149,332.1800 that appear mean-ingful variation. Algorithmic parameters for all algorithms are illu-strated in Table 10 .
 Figs. 6 X 17 show the convergence characteristics of Hybrid K-MICA, K-MICA, MICA-K, MICA, ICA, and K-means for the best solutions on Iris and wine data set.

Simulation results of the figures show that K-means algorithm converges faster than the other algorithms but converges prematurely to a local optimum. For the iris data set Hybrid K-MICA converges to the global optimum after 49 iterations while K-MICA, MICA-K, MICA and ICA converge to the global optimum in about69,74,78and109iterations.Theconvergencecharacteristics of these algorithms for wine data show that combining K-means with MICA converges to better results. While ICA and MICA converge to the global solution after 161 and 146 iterations,
MICA-K, K-MICA and Hybrid K-MICA converge after 132, 141 and 97iterations. 8. Conclusion
The imperialist competitive algorithm is a new method, which has great abilities to cope with different types of optimization problems. However, it is still in its infancy and intensive studies are needed to improve its performance. In this paper, a novel hybrid methodology called Hybrid K-MICA is introduced and debated in detail. Hybrid K-MICA is a combination of two powerful optimiza-tion algorithms; K-means and Imperialist Competitive Algorithm.
Inthis new algorithm,we use K-meansfor each empire to select the best empires just before competition starts by MICA. It has been shown that this combination can produce better empires and causes the most-fit imperialist last. Hybrid K-MICA algorithm has been developed in this paper to solve clustering problems. The results illustrate that the proposed Hybrid K-MICA optimization algorithm can be considered as a viable and an efficient heuristic method to find optimal or nearly optimal solutions for clustering problems of allocating N objects to K clusters.

The experimental results indicate that the proposed optimiza-tion algorithm is comparable to other algorithms in terms of best, worst and average solutions and standard deviation. The conver-gence of the proposed hybrid algorithm to the global optimum solution is better than that of other evolutionary algorithms. Regardless of robustness and efficiency of Hybrid K-MICA algo-rithm, it is applicable when the number of clusters is known as a priori.
 References
