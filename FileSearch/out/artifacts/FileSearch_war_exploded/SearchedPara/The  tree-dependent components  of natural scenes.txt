 Many models of natural image statistics have been proposed in recent years [1, 2, 3, 4]. A common goal of many of these models is finding a representation in which components or sub-components of the image are made as independent or as sparse as possible [5, 6, 2]. This has been found to be a difficult goal, as natural images have a highly intricate structure and removing dependencies between components is hard [7]. In this work we take a different approach, instead of minimizing dependence between components we try to maximize a simple form of dependence -tree dependence.
 It would be useful to place this model in context of previous works about natural image statistics. Many earlier models are described by the marginal statistics solely, obtaining a factorial form of the likelihood: The most notable model of this approach is Independent Component Analysis (ICA), where one seeks to find a linear transformation which maximizes independence between components (thus fit-ting well with the aforementioned factorization). This model has been applied to many scenarios, and proved to be one of the great successes of natural image statistics modeling with the emergence of edge-filters [5]. This approach has two problems. The first is that dependencies between compo-nents are still very strong, even with those learned transformation seeking to remove them. Second, it has been shown that ICA achieves, after the learned transformation, only marginal gains when measured quantitatively against simpler method like PCA [7] in terms of redundancy reduction. A different approach was taken recently in the form of radial Gaussianization [8], in which compo-nents which are distributed in a radially symmetric manner are made independent by transforming them non-linearly into a radial Gaussian, and thus, independent from one another.
 A more elaborate approach, related to ICA, is Independent Subspace Component Analysis or ISA. In this model, one looks for independent subspaces of the data, while allowing the sub-components Figure 1: Our model with respect to marginal models such as ICA (a), and ISA like models (b). Our model, being a tree based model (c), allows components to belong to more than one subspace, and the subspaces are not required to be independent. of each subspace to be dependent: This model has been applied to natural images as well and has been shown to produce the emergence of phase invariant edge detectors, akin to complex cells in V1 [2].
 Independent models have several shortcoming, but by far the most notable one is the fact that the resulting components are, in fact, highly dependent. First, dependency between the responses of ICA filters has been reported many times [2, 7]. Also, dependencies between ISA components has also been observed [9]. Given these robust dependencies between filter outputs, it is somewhat peculiar that in order to get simple cell properties one needs to assume independence . In this work we ask whether it is possible to obtain V1 like filters in a model that assumes dependence. In our model we assume the filter distribution can be described by a tree graphical model [10] (see Figure 1). Degenerate cases of tree graphical models include ICA (in which no edges are present) and ISA (in which edges are only present within a subspace). But in its non-degenerate form, our model assumes any two filter outputs may be dependent. We allow components to belong to more than one subspace, and as a result, do not require independence between them. Our model is comprised of three main components. Given a set of patches, we look for the parame-ters which maximize the likelihood of a whitened natural image patch z : Where y = Wz , T is the tree structure, pa i denotes the parent of node i and  X  is a parameter of the density model (see below for the details). The three components we are trying to learn are: All three together describe a complete model for whitened natural image patches, allowing likeli-hood estimation and exact inference [11].
 We perform the learning in an iterative manner: we start by learning the tree structure and density model from the entire data set, then, keeping the structure and density constant, we learn the filters via gradient ascent in mini-batches. Going back to the tree structure we repeat the process many data, and are continuously updated during learning. In the following sections we will provide details on the specifics of each part of the model. Figure 2: Shape of the conditional (Left three plots) and joint (Right three plots) density model in log scale for several values of  X  , from dependence to independence. 2.1 Learning tree structure In their seminal paper, Chow and Liu showed how to learn the optimal tree structure approximation for a multidimensional probability density function [12]. This algorithm is easy to apply to this scenario, and requires just a few simple steps. First, given the current estimate for the filter matrix W , we calculate the response of each of the filters with all the patches in the data set. Using these responses, we calculate the mutual information between each pair of filters (nodes) to obtain a fully connected weighted graph. The final step is to find a maximal spanning tree over this graph. The resulting unrooted tree is the optimal tree approximation of the joint distribution function over all nodes. We will note that the tree is unrooted , and the root can be chosen arbitrarily -this means that there is no node, or filter, that is more important than the others -the direction in the tree graph is arbitrary as long as it is chosen in a consistent way. 2.2 Joint probability density functions Gabor filter responses on natural images exhibit highly kurtotic marginal distributions, with heavy tails and sharp peaks [13, 3, 14]. Joint pair wise distributions also exhibit this same shape with varying degrees of dependency between the components [13, 2]. The density model we use allows us to capture both the highly kurtotic nature of the distributions, while still allowing varying degrees of dependence using a mixing variable. We use a mix of two forms of finite, zero mean Gaussian Scale Mixtures (GSM). In one, the components are assumed to be independent of each other and in the other, they are assumed to be spherically distributed. The mixing variable linearly interpolates between the two, allowing us to capture the whole range of dependencies: When  X  = 1 the two components are dependent (unless p is Gaussian), whereas when  X  = 0 the two components are independent. For the density functions themselves, we use a finite GSM. The dependent case is a scale mixture of bivariate Gaussians: While the independent case is a product of two independent univariate Gaussians: Estimating parameters  X  k and  X  2 k for the GSM is done directly from the data using Expectation Maximization. These parameters are the same for all edges and are estimated only once on the first iteration. See Figure 2 for a visualization of the conditional distribution functions for varying values of  X  . We will note that the marginal distributions for the two types of joint distributions above are the same. The mixing parameter  X  is also estimated using EM, but this is done for each edge in the tree separately, thus allowing our model to theoretically capture the fully independent case (ICA) and other degenerate models such as ISA. 2.3 Learning tree dependent components Given the current tree structure and density model, we can now learn the matrix W via gradient ascent on the log likelihood of the model. All learning is performed on whitened, dimensionally reduced patches. This means that W is a N  X  N rotation (orthonormal) matrix, where N is the number of dimensions after dimensionality reduction (see details below). Given an image patch z we multiply it by W to get the response vector y : Now we can calculate the log likelihood of the given patch using the tree model (which we assume is constant at the moment): Where pa i denotes the parent of node i . Now, taking the derivative w.r.t the r -th row of W : Where z is the whitened natural image patch. Finally, we can calculate the derivative of the log likelihood with respect to the r -th element in y : Where C ( r ) denote the children of node r . In summary, the gradient ascent rule for updating the rotation matrix W is given by: Where  X  is the learning rate constant. After update, the rows of W are orthonormalized. This gradient ascent rule is applied for several hundreds of patches (see details below), after which the tree structure is learned again as described in Section 2.1, using the new filter matrix W , repeat-ing this process for many iterations. 3.1 Validation Before running the full algorithm on natural image data, we wanted to validate that it does produce sensible results with simple synthetic data. We generated noise from four different models, one is f independent Gaussian noise with 8 Discrete Cosine Transform (DCT) filters, the second is a simple ICA model with 8 DCT filters, and highly kurtotic marginals. The third was a simple ISA model -4 subspaces, each with two filters from the DCT filter set. Distribution within the subspace was a circular, highly kurtotic GSM, and the subspaces were sampled independently. Finally, we generated data from a simple synthetic tree of DCT filters, using the same joint distributions as for the ISA model. These four synthetic random data sets were given to the algorithm -results can be seen in Figure 3 for the ICA, ISA and tree samples. In all cases the model learned the filters and distribution correctly, reproducing both the filters (up to rotations within the subspace in ISA) and the dependency structure between the different filters. In the case of 1 / f Gaussian noise, any whitening transformation is equally likely and any value of beta is equally likely. Thus in this case, the algorithm cannot find the tree or the filters. 3.2 Learning from natural image patches We then ran experiments with a set of natural images [9] 1 . These images contain natural scenes such as mountains, fields and lakes. . The data set was 50,000 patches, each 16  X  16 pixels large. The patches X  DC was removed and they were then whitened using PCA. Dimension was reduced from 256 to 128 dimensions. The GSM for the density model had 16 components. Several initial Figure 3: Validation of the algorithm. Noise was generated from three models -top row is ICA, middle row is ISA and bottom row is a tree model. Samples were then given to the algorithm. On the right are the resulting learned tree models. Presented are the learned filters, tree model (with white edges meaning  X  = 0 , black meaning  X  = 1 and grays intermediate values) and an example of a marginal histogram for one of the filters. It can be seen that in all cases all parts of the model were correctly learned. Filters in the ISA case were learned up to rotation within the subspace, and all filters were learned up to sign.  X  values for the ICA case were always below 0.1, as were the values of  X  between subspaces in ISA. conditions for the matrix W were tried out (random rotations, identity) but this had little effect on results. Mini-batches of 10 patches each were used for the gradient ascent -the gradient of 10 patches was summed, and then normalized to have unit norm. The learning rate constant  X  was set to 0.1. Tree structure learning and estimation of the mixing variable  X  were done every 500 mini-batches. All in all, 50 iterations were done over the data set. 3.3 Filters and tree structure Figures 4 and 5 show the learned filters ( WQ where Q is the whitening matrix) and tree structure ( T ) learned from natural images. Unlike the ISA toy data in figure 3, here a full tree was learned and  X  is approximately one for all edges. The GSM that was learned for the marginals was highly kurtotic.
 It can be seen that resulting filters are edge filters at varying scales, positions and orientations. This is similar to the result one gets when applying ICA to natural images [5, 15]. More interesting is Figure 4: Left: Filter set learned from 16  X  16 natural image patches. Filters are ordered by PCA eigenvalues, largest to smallest. Resulting filters are edge filters having different orientations, po-sitions, frequencies and phases. Right: The  X  X eature X  set learned, that is, columns of the pseudo inverse of the filter set. Figure 5: The learned tree graph structure and feature set. It can be seen that neighboring features on the graph have similar orientation, position and frequency. See Figure 4 for a better view of the feature details, and see text for full detail and analysis. Note that the figure is rotated CW. Figure 6: Correlation of optimal parameters in neighboring nodes in the tree graph. Orientation, frequency and position are highly correlated, while phase seems to be entirely uncorrelated. This property of correlation in frequency and orientation, while having no correlation in phase is related to the ubiquitous energy model of complex cells in V1. See text for further details.
 Figure 7: Left: Comparison of log likelihood values of our model with PCA, ICA and ISA. Our model gives the highest likelihood. Right: Samples taken at random from ICA, ISA and our model. Samples from our model appear to contain more long-range structure. the tree graph structure learned along with the filters which is shown in Figure 5. It can be seen that neighboring filters (nodes) in the tree tend to have similar position, frequency and orientation. Figure 6 shows the correlation of optimal frequency, orientation and position for neighboring filters in the tree -it is obvious that all three are highly correlated. Also apparent in this figure is the fact that the optimal phase for neighboring filters has no significant correlation. It has been suggested that filters which have the same orientation, frequency and position with different phase can be related to complex cells in V1 [2, 16]. 3.4 Comparison to other models Since our model is a generalization of both ICA and ISA we use it to learn both models. In order to learn ICA we used the exact same data set, but the tree had no edges and was not learned from the data (alternatively, we could have just set  X  = 0 ). For ISA we used a forest architecture of 2 node trees, setting  X  = 1 for all edges (which means a spherical symmetric distribution), no tree structure was learned. Both models produce edge filters similar to what we learn (and to those in [5, 15, 6]). The ISA model produces neighboring nodes with similar frequency and orientation, but different phase, as was reported in [2]. We also compare to a simple PCA whitening transform, using the same whitening transform and marginals as in the ICA case, but setting W = I .
 We compare the likelihood each model gives for a test set of natural image patches, different from the one that was used in training. There were 50,000 patches in the test set, and we calculate the mean log likelihood over the entire set. The table in Figure 7 shows the result -as can be seen, our model performs better in likelihood terms than both ICA and ISA.
 Using a tree model, as opposed to more complex graphical models, allows for easy sampling from the model. Figure 7 shows 20 random samples taken from our tree model along with samples from the ICA and ISA models. Note the elongated structures (e.g. in the bottom left sample) in the samples from the tree model, and compare to patches sampled from the ICA and ISA models. Figure 8: Left: Interpretation of the model. Given a patch, the response of all edge filters is computed ( X  X imple cells X ), then at each edge, the corresponding nodes are squared and summed to produce the response of the  X  X omplex cell X  this edge represents. Both the response of complex cells and simple cells is summed to produce the likelihood of the patch. Right: Response of a  X  X omplex cell X  in our model to changing phase, frequency and orientation. Response in the y-axis is the sum of squares frequency, it is rather invariant to phase. 3.5 Tree models and complex cells One way to interpret the model is looking at the likelihood of a given patch under this model. For the case of  X  = 1 substituting Equation 4 into Equation 3 yields: els of complex cells in V1 [2, 4]. In Figure 8 we draw a simple two-layer network that computes the likelihood. The first layer applies linear filters ( X  X imple cells X ) to the image patch, while the sec-ond layer sums the squared outputs of similarly oriented filters from the first layer, having different phases, which are connected in the tree ( X  X omplex cells X ). Output is also dependent on the actual response of the  X  X imple cell X  layer. The likelihood here is maximized when both the response of the parent filter y pa i and the child y i is zero, but, given that one filter has responded with a non-zero value, the likelihood is maximized when the other filter also fires (see the conditional density in Figure 2). Figure 8 also shows an example of the phase invariance which is present in the learned "complex cell" (energy of a pair of learned filters connected in the tree) -it seems that sum squared response of the shown pair of nodes is relatively invariant to the phase of the stimulus, while it is selective to both frequency and orientation -the hallmark of  X  X omplex cells X . Quantifying this re-sult with the AC/DC ratio, as is common [17] we find that around 60% percent of the edges have an AC/DC ratio which is smaller than one -meaning they would be classified as complex cells using standard methods [17]. We have proposed a new model for natural image statistics which, instead of minimizing dependency between components, maximizes a simple form of dependency -tree dependency. This model is a generalization of both ICA and ISA. We suggest a method to learn such a model, including the tree structure, filter set and density model. When applied to natural image data, our model learns edge filters similar to those learned with ICA or ISA. The ordering in the tree, however, is interesting -neighboring filters in the tree tend to have similar orientation, position and frequency, but different phase. This decorrelation of phase, in conjunction with correlations in frequency and orientation are the hallmark of energy models for complex cells in V1.
 Future work will include applications of the model to several image processing scenarios. We have started experimenting with application of this model to image denoising by using belief propagation for inference, and results are promising.
 This work has been supported by the AMN foundation and the ISF. The authors wish to thank the anonymous reviewers for their helpful comments. [1] Y. Weiss and W. Freeman,  X  X hat makes a good model of natural images? X  Computer Vision [2] A. Hyvarinen and P. Hoyer,  X  X mergence of phase-and shift-invariant features by decomposition [3] A. Srivastava, A. B. Lee, E. P. Simoncelli, and S.-C. Zhu,  X  X n advances in statistical modeling [4] Y. Karklin and M. Lewicki,  X  X mergence of complex cell properties by learning to generalize [5] A. J. Bell and T. J. Sejnowski,  X  X he independent components of natural scenes are edge filters, X  [6] B. Olshausen et al. ,  X  X mergence of simple-cell receptive field properties by learning a sparse [7] M. Bethge,  X  X actorial coding of natural images: how effective are linear models in removing [8] S. Lyu and E. P. Simoncelli,  X  X onlinear extraction of  X  X ndependent components X  of natural [9] A. Hyvrinen, P. Hoyer, and M. Inki,  X  X opographic independent component analysis: Visualiz-[10] F. Bach and M. Jordan,  X  X eyond independent components: trees and clusters, X  The Journal of [11] J. Yedidia, W. Freeman, and Y. Weiss,  X  X nderstanding belief propagation and its generaliza-[12] C. Chow and C. Liu,  X  X pproximating discrete probability distributions with dependence trees, X  [13] E. Simoncelli,  X  X ayesian denoising of visual images in the wavelet domain, X  LECTURE [14] A. Levin, A. Zomet, and Y. Weiss,  X  X earning to perceive transparency from the statistics of [15] J. van Hateren,  X  X ndependent component filters of natural images compared with simple cells [16] C. Zetzsche, E. Barth, and B. Wegmann,  X  X he importance of intrinsically two-dimensional [17] K. Kording, C. Kayser, W. Einhauser, and P. Konig,  X  X ow are complex cell properties adapted
