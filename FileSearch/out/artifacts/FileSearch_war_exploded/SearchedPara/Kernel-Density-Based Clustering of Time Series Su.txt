
Noise levels in time series subsequence data are typi-cally very high, and properties of the noise differ from those of white noise. The proposed algorithm incorporates a con-tinuous random-walk noise model into kernel-density-based clustering. Evaluation is done by testing to what extent the resulting clusters are predictive of the process that gener-ated the time series. It is shown that the new algorithm not only outperforms partitioning techniques that lead to trivial and unsatisfactory results under the given quality measure, but also improves upon other density-based algo-rithms. The results suggest that the noise elimination prop-erties of kernel-density-based clustering algorithms can be of significant value for the use of clustering in preprocess-ing of data.
Finding patterns in time series subsequence data is a no-toriously difficult problem. Standard clustering techniques, such as k-means and hierarchical clustering, result in clus-ters that are largely independent of the time series from which they originate [13]. Kernel-density-based clustering can lead to meaningful results, especially if an appropri-ate noise model is chosen [6]. Noise elimination in kernel-density-based clustering is based on the concept of a noise threshold, below which maxima in the density distribution are not considered as cluster centers [10]. Most time series data follow a noise distribution that differs from standard assumptions on randomness, and it can be beneficial to in-corporate a more accurate noise distribution. While previ-ous work [6] assumed a discrete random-walk model, the current paper is based on a continuous model that is much more realistic in most settings.

Time series clustering algorithms have been used di-the concepts and techniques relevant to the problem, section 3 explains the concept underlying the algorithm; section 4 describes the implementation; section 5 discusses the ex-perimental evaluation, and section 6 concludes the paper.
The term  X  X ime series X  is typically used for sequential real-valued data that are collected at regular time intervals. Many other types of sequential data can be distinguished, such as sequences of integer-valued and categorical data. Examples of time series data include ECG measurements, weather recordings, and stock market data.
 Definition 1 : A time series T = t 1 ,...,t n is a sequence of real numbers, corresponding to values of an observed quan-tity, collected at equally spaced points in time. Definition 2 : A subsequence of time series T = t 1 ,...,t n , with length w , is a sequence S = t m ,...,t m + w  X  1 with 1  X  m  X  n  X  w +1 . The process of extracting subsequences by incrementing m in steps of one is called application of a sliding window.

Subsequences are commonly represented as vectors in a w -dimensional vector space. Comparison of subsequences can be done through any measure defined on a vector space, such as an L p norm. A typical choice is the L 2 norm, i.e., Euclidean distance. Some distance measures that are specific to time series data, such as dynamic time warp-ing (DTW) [2] and longest common subsequence similarity (LCSS) [17] are not suitable for the short subsequences ( w between 8 and 16) considered in this paper.

Using Euclidean distance on the raw time series data leads to clusterings that are dominated by the mean of the subsequence. Hence, it is common to normalize the data. A common type of normalization is Z-normalization in which the mean is subtracted and the subsequence is divided by its standard deviation [9]. In this paper a different approach, which follows more naturally from the random-walk model, is used. The mean is subtracted as in Z-normalization, but the time series is then normalized based on the root-mean-square of differences between adjacent data points. Note that the differences are only used for the normaliza-tion process and not as the actual data to be compared as in [7].

A very simple model of a time series is  X  X trict white noise X  as defined in [16].
 Definition 3 : A Strict white noise time series is a normally distributed sequence of values { e t } corresponding to time t . Mean  X  and variance  X  are assumed to be the same for all time points.
 The Gaussian kernel is rad ially symmetric. A profile can, therefore, be defined through where c k,d is a normalization constant that guarantees the normalization in equation (4). Cluster centers are defined as local maxima in the density landscape. In [10] these lo-cal maxima are called density attractors and in [4] modes. While [10] uses a feature space grid to assist in the search for maxima, [4] parses the table of data points for each hill climbing step. The latter approach was chosen in this paper because it avoids representing the high-dimensional feature space. To identify modes, all data points are taken as start-ing points and their location is updated through a sequence of hill climbing step. Updates, i.e. differences between ten-tative cluster center locations for successive steps, are com-puted as where g ( x )=  X  k ( x ) is the negative derivative of the ker-nel profile. Data points are associated with the cluster center to which they are attracted. Only modes above a threshold t are considered cluster centers. Data points that are attracted to modes below t are outliers or noise. When testing data points for cluster membership only those points were con-sidered for which the density at the original location already exceeded t . In a standard application of density-based clus-tering t is considered a parameter that has to be selected by the user. The number of free parameters is often quoted as drawback of density-based clustering. In this paper a comparison with random-walk data is used to determine t , effectively eliminating this parameter.

The next section addresses differences between standard clustering problems and time series subsequence clustering and their impact on kernel-density-based clustering.
Kernel-density-based clustering is robust against noise, provided the noise leads to an approximately constant den-sity surface. Constant contributions to the density distribu-tion do not affect the position of maxima. This section will demonstrate that, by default, the density landscape corre-sponding to a random-walk time series cannot be consid-ered constant. Using a standard implementation of density-based clustering would result in cluster centers that are due to random-walk behavior, i.e., noise. This section also demonstrates how to scale the coordinate system to avoid this problem. it becomes rotationally invariant. As a first step, the argu-ment of the second exponential function ( S ( x ) ) is expanded  X  ( x )  X  d x 1  X  X  X  d x w exp  X  with S ( x ) can be written in matrix form S = x T Ax S ( x )=
Matrix A is then diagonalized. In practice, this is done numerically. For w =4 the diagonalization can be done analytically, so this case will be used as an example. The diagonalized matrix for w =4 is It can be seen that the system has four distinct eigenvalues. The corresponding eigenvectors are u u
The first eigenvector, u 1 , corresponding to eigenvalue 0 , does not contribute in the practical implementation, because the mean of each subsequence is subtracted as a normaliza-tion step. It can be shown, through approximate evaluation of the density integral in equation (8) that the eigenvector corresponding to the smallest non-zero eigenvalue, namely u , is responsible for the density maximum in the untrans-formed system. The calculation is omitted due to lack of space. Figure 1 shows the corresponding eigenvalue for w =8 together with a result of density-based clustering without scaling. In the scaled coordinate system rotational invariance is sat-isfied, and the original  X  describes the distribution for all dimensions 3.1. Intuitive Interpretation
It may initially seem counter-intuitive that one random-walk time series should correspond to a higher kernel-density than all others, and thereby dominate the system unless scaling is applied. This section will discuss a sys-tem that is defined in a slightly different way, and captures the essential behavior already in two dimensions.
A 2-dimensional system can be defined by looking at time series subsequences with w =3 for which the first time point is assumed to be fixed at x 0 =0 . The density distribution for the remaining two time points is then The resulting equation for S ( x ) analogous to equation (12) is This system can be visualized by plotting the line of S ( x )=2 which corresponds to the normalization condi-tion equivalent to equation (16). Figure 2 shows the result-ing ellipse. In this simple two-dimensional case it is easy to cluster centers, is given by Note that the inversion of matrix is diagonal. Strictly speaking, the matrix D from equation (13) cannot be inverted because of its eigenvalue 0 . However, only the ( w  X  1) eigenvectors corresponding to the non-zero eigenvalues have to be considered as basis vectors for the transformed space, due to the normalization. Transformation from the ( w  X  1) -dimensional space back into the w -dimensional original space is straight forward, and unaffected by the eigenvalue 0 .

A density landscape is constructed based on the trans-formed data points. Comparis ons between cluster centers are done using the same algorithm that was described for individual sequences, and matches between cluster centers may involve a time shift. This significantly reduces appar-ent redundancies between cluster centers. The procedure re-quires an additional heuristic cut-off distance. In the current paper, only those modes of the density distribution are con-sidered as separate cluster centers that do not have a clus-ter center with higher density within a range of 0 . 1 threshold has to be identified below which cluster centers are considered as noise. For this purpose a clustering is performed on the first 500 points of the random-walk time series from [12]. The maximum density of cluster centers in this clustering is chosen as noise threshold. The distribu-tion of densities of cluster centers was also used to test the algorithm. Figure 3 shows that the density for the first 13 cluster centers shows a much slower decrease for the algo-rithm that uses scaling than for the same algorithm without scaling. A threshold that does not result in cluster centers due to noise in the scaled algorithm can still return noise-generated cluster centers in the unscaled algorithm. If the noise-threshold is chosen higher, the number of clusters that can be identified is reduced.

The testing is done as follows. Normalization and scal-ing are done in the same way as when constructing clus-ters. Assignment to a cluster center is achieved using hill-climbing on the same density landscape. Only those data points that have a density exceeding the noise threshold be-fore beginning the hill-climbing are assigned to a cluster center. Due to the shifting procedure a subsequence can be assigned to multiple clusters. In the final evaluation a sub-sequence is considered as predicting a particular time series if it is assigned to at least one of the clusters of that time series. Note that evaluation is done on a different part of the time series for all experiments. A correct assignment cor-responds to an instance of a subsequence being assigned to at least one cluster derived from a time series that has the same source. An incorrect assignment signifies that a sub-sequence matches at least one cluster of a time series of a different source.
 500 data points. Results list the fraction of correctly / incor-rectly assigned sequences. A subsequence can be assigned to a cluster from the correct time series, and can possibly also be assigned to a cluster of any of the other time series. In fact, k-means and other partitioning algorithms assign a subsequence to one cluster of every time series against which it is tested. The relative number of correct assign-ments is therefore guaranteed to be one, and the relative number of incorrectly assigned subsequences is one as well. This result is trivial and doe s not require an implementation.
Five algorithms were compared besides k-means. Den-sity based clustering with compensation for disrete random-walk noise was performed using the parameters from [6]. Parameters for the density-based clustering with w =8 with and without scaling were h =2 and threshold t = 5 e  X  5 .For w =12 the choice was h =2 and t =6 e  X  7 . For w =16 the width of the kernel function was chosen to be h =3 and the threshold t =5 e  X  11 . Note that the choice of threshold was determined from the density distrib-ution of a random-walk time series that was clustered under the same conditions.
 Figure 4 compares the average results over all data sets. It can be seen that the average number of correct assign-ments is relatively large for traditional algorithms. K-means and other partitioning algorithms trivially have the largest possible relative number of correct and incorrect assign-ments, namely one. Density-based algorithms, in general, allow outliers and both types of assignments are likely to be smaller than their maximum value. The implementation from [6] leads to the largest number of correct and incorrect assignments and the poorest ratio among the density-based algorithms. To understand this it is important to note that the normalization in the current paper is based on differ-eral others. Results for the random-walk time series are not listed since the recognition threshold was picked based on the random data sets, and the number of matches was 0 for all runs of the new algorithm. Another data set, the buoy data set also leads to no assignments to clusters for the al-gorithm presented in this paper. This is an indication that the cluster result from [6] was still due to noise despite the compensation for discrete random-walk noise and that com-pensating for continuous rather than discrete random-walk noise is important. It can, furthermore, be seen that the sub-sequences are assigned much more reliably for some data sets than for others. Time series with a deterministic physi-cal origin lead to far better results than measurements from less deterministic sources. The best results were achieved for speech data, the balloon data set, and ecg data. For these data sets, correct assignments were more likely than incor-rect ones by about two orders of magnitude for w =16 .
A clustering algorithm for time series subsequences was introduced that considers continuous random-walks as its noise model. The algorithm makes use of a coordinate transformation on the feature space that results in a uni-form noise threshold for all valid input sequences. Eval-uation was based on a new measure that specifically tests the success of distinguishing cluster members from noise. According to the new evaluation measure, the quality of re-sults is improved by more then two orders of magnitude on some data sets compared with k-means. These results suggest that the algorithm can be productively used in time series subsequence clustering and may also hold concepts that can be generalized beyond this particular application. The concept of using kernel-density-based clustering with a specific noise-model for improved noise elimination has the potential of being applied to many other settings. Thanks to Alan Denton for proofreading the paper.
