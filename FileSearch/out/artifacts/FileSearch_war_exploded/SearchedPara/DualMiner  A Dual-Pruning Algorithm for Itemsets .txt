
Mining frequent itemsets in the presence of constraints {A} {B} It} [D} {A,B} {A,C} [A,D} {B,C} {B,D/ [C,D] lAmB,C} {A,B,D] {A,C,D} {B,C,D} VS, J : (J C S C M A P(S)) =~ P(J) VS, J: (S C J C M A Q(S)) ~ Q(J) not satisfy Q). Thus we have a choice. We choose an ele-ment, such as E, from CHILD(a) and explore what happens when a set contains this element. This leads to the creation of node fl with state (E, ABED, 0). Since no itemset con-tains both .E and A (i.e. EA does not satisfy P), we can remove A from consideration and move it from CHILD(fl) to 
OUT(fl). Now fl has state (E, BeD, A). The same is done with B, C and D and fl ends up in state (E, 0, ABED). can interpret this to mean that every set that contains E and Q. So the first algebra that we found is the singleton to node 7 to explore sets that do not contain E. For this reason 3, has the initial state (0, ABED, E). We cannot prune using P, but now we see that a set not containing 
E has no hope of satisfying Q unless it contains A. Thus the state of 7 changes to (A, BCD, E). Another iteration of pruning show that any superset of A not containing E does not satisfy Q unless it also contains B. The state of becomes (AB, CD, E). (Note that IN(3') = {A, B} satisfies both P and Q and none of 7's ancestors have this property. 
This means that AB is the minimal set of a subalgebra). At this point we cannot prune with any of our predicates (and so 3' has reached its final state). Once again we have to make a choice. A choice on C leads to the node 8 with state (ABe, D, E) (here we examine what happens when a set contains A, B and C but not E). We cannot do any pruning with state (ABED, 0, E). We knew that AB satisfied Q and now we know ABeD satisfies P (otherwise D would have been moved to OUT(5)). From the monotone property of Q and antimonotone property of P we know that (AB, ABED) is an algebra that satisfies P and Q (all supersets of are subsets of ABeD satisfy both P and Q). In addition to this, knowing that ABeD satisfies P means that we do not have to examine the right children of nodes 8 and e. It is clear that those nodes would only examine sets that belong to the algebra (AB, ABED). Thus the algorithm completes with the output (E,E) and (AB, ABCD). The sequence two sections, we will refer back to this example and Figure 3 for illustrative purposes. Unless otherwise mentioned, when final states. 
Each node ~-E 7 corresponds to a subalgebra of 2 M, which we refer to as SUBALG(r). Note that these are not nec-essarily "good" subalgebras. Each subalgebra is associated with the following objects. 
Collectively, we refer to these objects as the state either determined or undetermined and the classification of each node will change during the course of the algorithm. By default, every node starts out as undetermined. If a node is undetermined, then the state of r may change. Because the state may change, we define the sth-iteration r to be the s th state assigned to it during the algorithm. of IN(v) is IN(rS). If we simply write 7", then we mean the most recent iteration of r in our algorithm. Note that we do not consider r s to be a parent of r s+l. Thus the node has three iterations and IN(7 I) = {A} (because we start counting at 0). 
At each stage, we visit a node that is undetermined and make it determined. We continue until all the nodes of 7 are determined. Our traversal strategy is irrelevant; any traversal strategy that visits parents before children is ac-ceptable. This allows our algorithm some flexibility for the sake of optimizations. Therefore, at the highest level, we have DUAL.MINER, illustrated in Algorithm 1. 
Algorithm 1 : DUAL_MINER 1: 2:R~-0 3: while There are undetermined nodes do 4: Traverse to the next undetermined node 7. 5: G ~--EXPAND.NODE(r) 6: 7~ ~--~U G 7: Mark r as determined. 8: end while 9: Answer = "R. 
Let A be the initial node. IN(A) = OUT(A) = O. Note that by ore7 definition of A, SUBALG(A) is the algebra 
The algorithm EXPAND.NODE(r) expands the tree 9" to break the algebra SUBALG(r) into smaller, disjoint sub-algebras that may be more easily searched. This is done by adding children to r that specify further subalgebras of 
SUBALG(r). It is important that the children of r repre-sent disjoint subalgebras of SUBALG(r). This is not dif-ficult because our algorithm ensures that no undetermined node has any children. All we have to do is chose some item x E CHILD(r), and split SUBALG(7) into those sets containing x and those sets not containing x. The result is EXPAND_NODE, shown in Algorithm 2. Algorithm 2 : EXPAND.NODE(r) Require: r is an undetermined node. 
Returns: G, a good subalgebra or  X  1: G ~-PRUNE(r) 2: if CHILD(r) is not empty then 3: Choose some x E CHILD(T). 5: pne~_out, ~new.Jn ~--0 0: Add p and V as children of r. 7: end if 8: return G 
As mentioned above, the pruning strategy in PRUNE uses both ends of our algebra, evaluating both P and Q to gener-ate successive states for r. When we prune with respect to 
P, we look at each item x E CHILD(r). If IN(r) t.J {x} does not satisfy P, then the antimonotone property of P implies that no superset of IN(r) LJ {x} satisfies P. This is equiv-alent to saying that no element of SUBALG(r) containing restricting SUBALG(r). Putting this all together, we get MONO_PRUNE, which is shown in Algorithm 3. An analogous result holds for pruning with respect to Q. 
If we replace everything in the algorithm MONO_PRUNE by its dual notion, we get ANTI_PRUNE, the algorithm for pruning with respect to Q. This is shown in Algorithm 4. Algorithm 3 : MONO_PRUNE(7 ~) Require: IN(r ~) ,,satisfies P 
Returns: r TM 2: rnew_ou t ~ 7~ew_ou t 3: for all x E CHILD(r s) do 4: if IN(r s) t3 (x} does not satisfy P then 6: CHILD(r :~+1) ~--CHILD(r s+l) -{x} 7: end if 8: end for Algorithm 4 : ANTI_PRUNE(r ~) Require: OUT(T s) satisfies Q 
Returns: r s+z 3: for all x E CHILD(r s) do 4: if OUT(r s) t3 (x} does not satisfy Q then 5: 7;~ewj. r,;~w_l . u {x} 6: CHILD(r ~*') ~-CHILD(r ~+I) -{x} 7: end if 8: end for Both of these algorithms assume that SUBALG(r) is an interesting algebra. It is possible that, as ANTI.PRUNE puts elements into 7,ew_ln, IN(r) no longer satisfies P. In this case, no element of the subalgebra satisfies P, so we will not need to do further pruning or to construct children for this node. The easiest way to signify this is to empty 
CHILD(r) by adding CHILD(r) to rn .... t. We represent this straightforward action as EMPTY_CHILD. It is clear that these pruning algorithms affect each other. 
The output of MONO_PRUNE is determined by IN(r), which is in turn modified by the algorithm ANTI_PRUNE. Simi-larly, MONO_PRUNE modifies OUT(r), which determines the output of ANTI.PRUNE. Therefore, it makes sense to interleave these algorithms until we reach a fixed point. The resulting pruning algorithm PRUNE(r) is shown in Algo-rithm 5. Note that PRUNE is the most extreme pruning strat-egy. It will not affect the correctness of our algorithm to do less pruning. We may chose only to do a fixed number of passes on each of the algorithms MONO_PRUNE and 
ANTI_PRUNE. We may even choose to skip one or both of them altogether. This allows us some flexibility for opti-mization, as discussed in Section 4. The correctness of this algorithm should be somewhat clear from the accompanying discussion. However, for a more rigorous proof, we present the following. Definition 4. The depth of a node r s is an ordered pair (p, s) where p is the number of nodes (excluding r) whose descendants include 7, and s is the most recent iteration of (p2, s2) if pl &gt; p2 or (pl = p2) ^ (sl &gt; s2). THEOREM 1. An set A satisfies P A Q i/and only if A is an element o/a subalgebra returned by PRUNE at some point in the algorithm. 46 Algorithm 5 : PRUNE(r) 
Returns: A good subalgebra or 0. 1: s ~ 0 (Note, at this point r = 7 -0 by definition) 2: repeat 3: if IN(r s) satisfies P then 4: r ~+x ~--MONO..PRUNE(r ~) 5: else 6: r ~+1 ~--EMPTY_CHILD(v') 7: end if 8: s ~--s + 1//Pruning has changed the iteration 9: if OUT(r ~) satisfies Q then 10: .?r ~+l ~--ANTI_PRUNE(r') 11: else 12: r ~+I ~--EMPTY_CHILD(r ~) 13: end if 14: s ~--s + 1//Pruning has changed the iteration 16: if IN(r ~) satisfies Q and OUT(T ~) satisfies P then 17: return (IN(re), OUT(r')) 18: else 19: return 0 20: end if 
PROOF. We will prove the direction assuming that A sat-INCLUDED~ as: Note that INCLUDEDx0(A) is true (where A is the root clearly we are done. Otherwise, after calculating IN(r s) and OUT(T s) in PRUNE, the algorithm would either call EMPTY_CHILD(re), go through another pruning iteration, or add a child to r ~ in EXPAND.NODE. 
If the algorithm called EMPTY_CHILD(r ~) is called then 
In the first case, since P is monotone, -~P is antimonotone and this implies that A fails to satisfy P, a contradiction. The second case is similar because of symmetry. 
If we go through another pruning iteration, then we get r ~+1 from either MONO_PRUNE or ANTI.PRUNE. In ei-ther case, it is clear that INCLUDED~.,+I (A) holds, which contradicts the maximality of the depth of r ~. 
If we add a child to r ~ then let x E CHILD(r) be the item defining the two children of r in EXPAND.NODE. By symmetry, assume without loss of generality that x E A, and let p be the child of 7-such that x E pnew.dn. Then INCLUDEDpo(A) holds, which is also contradiction. [] 
The algorithm outlined above is in its most primitive form in order to make it easy to follow. There are severM places in which it can be optimized. The most obvious optimiza-tion is to remove calls to MONO_PRUNE or ANTI_PRUNE that we know will not actually do any pruning. For ex-ample, if rnSew.an is empty and a is the parent of r, then IN(r') = IN(a). Therefore, there is nothing to be gained calling MONO_PRUNE on r s when r~ew.an is empty. A sim-ilar result holds for ANTI_PRUNE when r~ew_out is empty. 
Similarly, in the non-degenerate case, we run the same pruning algorithm on r ~+2 that we run on r'. Hence there is no point pruning r s+2 with MONO_PRUNE if r,s~+~.an = r~ew_in. Part of this rationale is captured by the repeat-until loop in PRUNE. However, this loop continues until both rnew_in and rnew_out achieve a fixed point. 
A more subtle optimization is an analogue of the HUT strategy from the MAFIA algorithm [6]. In a standard depth-first traversal (of a frequent itemset algorithm), we know that if every element of the leftmost branch satisfies P, then everything to the right must also satisfy P (i.e. ev-erything to the right is a subset of a set in the leftmost branch). To generalize that concept, we introduce the fol-lowing definition. 
Definition 5. A complete left chain is a sequence of nodes {rk}~&lt;,~ such that the following all hold. 1. CHILD(r0) = 0 2. rk+l is the parent of rk for all k &lt; n. 3. (rk) ..... t = 0 for all k &lt; n. Looking at Figure 3, we see that {e, ~, 3'} forms a complete left chain. If we replace (rk)new..out with (rk)new_in in the previous definition, we get a complete right chain. 
The antimonotone property of P and monotone property of Q imply the following. 
PROPOSITION 2. If {rk}k&lt;n is a complete left chain, ev-ery element of SUBALG(r,~) satisfies P. Furthermore, if IN(Tn) satisfies Q, then every element of SUBALG(rn) sat-plete right chain. This means that once we find a node r such that CHILD(r) = 0 and rnew.o,~t = 0, we need only find the least a ~ 1" such that 1. There is a complete left chain from r to a. 2. IN(a) satisfies Q. These two properties imply that all of SUBALG(a) satisfy P A Q. In this case, we should consider every descendant of a to be determined, and choose the sibling of o" to be our next node in our traversal strategy. 
We are interested in complete left chains from T to o" even if IN(a) does not satisfy Q. We still know that every ele-ment of SUBALG(o') satisfies P. Hence we no longer need to evaluate MONO.PRUNE for nodes in SUBALG(a) even though we have to traverse them. A similar argument holds for Q if IN(a) satisfies Q. 
Because we value complete right chains as much as com-plete left chains in our algorithm, it may be advantageous to take a "steady state" approach to our traversal strategy. (i.e. a child r such that r~ X ew_out = 0), we should continue choosing the left child as we descend the tree. Similarly, if we are visiting the right child of a node, we should continue choosing the right child as we descend the tree. 
The traversal strategy also depends on the cost structure of P. For example, suppose P is a support constraint and support counting is done using bitmaps, as in the MAFIA algorithm. If we continue to visit left children (starting from 
Other MAFIA optimizations [6], such as keeping a partial 
We can consider the antimonotone predicate P and mono-Theory of ~ : Positive Border of ~ : B+(fl) = B(fl) n Th(n) Negative Border of ~ : B-(~) = B(fl) n Th(-~fl) 
PROPOSITION 3. Given a monotone or antimonotone pred-
PROPOSITION 4. Computing Th(P A Q) using the oracle PROOF. The search space consists of four regions: Th(P)O [ Wh(Q) n B+(P)]p + ] Th(Q) n B+(P)[Q +[ Th(Q) n B-(P)[p + [ Th(P) o B(Q)I Q = [ Wh(Q) n B(P)[p + [ Wh(P) n B(Q)[Q for which MAFIA evaluates P. Then MAFIA will evaluate P for at most NIS[ sets outside Th(P). Thus MAFIA uses heuristics to reduce the size of S. It also has smaller mem-ory requirements that Apriori (due to its depth-first rather than breadth-first traversal of the search space) and avoids Apriori's expensive candidate generation algorithm. In ad-dition, there is experimental evidence to show that it is more efficient than Apriori [6]. 
Computing Th(P A Q) can be done naively by separately running Apriori or MAFIA (using P), the corresponding dual algorithm (using Q) and then intersecting the results. This algorithm, which will be referred to as INTERSECT, clearly has a worst case bound [ Th(P)UB-(P)IP+I Th(Q)t3 B-(Q)[Q oracle queries (when using Apriori). Another naive approach, POSTPROCESS, computes Th(P) and then post-processes the output by evaluating Q for each element of the output. This can be done using [Th(P) U B-(P)[p + [ Th(P)[Q oracle evaluations. POST-PROCESS does not leverage the monotone properties of Q, and so can be improved as in [12]: for each x E Th(P) that is found, evaluate Q(x) unless we know Q(t) is true for some t c x. This algorithm (when using Apriori), which we will refer to as CONVERTIBLE, evaluates Q on every set in Th(P) n (Th(-~Q) t.A B+(Q)) and thus uses [ Th(P) tJ B-(P)Ip + [ Th(P) nTh(~Q)[Q + [ Th(P) n B+(Q)[Q pred-icate evaluations. Assuming P is more selective than Q (as is usually true when P contains a support constraint), the CONVERTIBLE algorithm will tend to dominate both POSTPROCESS and INTERSECTION. It will also be very small. 
One more alternative to DualMiner is an enhanced version of Mellish's algorithm [13]. This algorithm outputs two sets S and G (the collection of tops of all maximal subalgebras and the collection of all bottoms, respectively). While this representation is very compact, considerable work needs to be done to output Th(P A Q) from its result. The algo-where each cz is either a monotone or antimonotone pred-icate. MELLISH+ computes S and G for Cl (using a top-down or bottom-up levelwise algorithm similar to Apriori). The borders are then refined using c2 and a levelwise algo-rithm that starts at the appropriate border S or G (depend-ing on the type of the constraint c2). This process is then repeated for the predicates ca,..., c,. This algorithm is very likely to waste computation because it does not combine all the monotone predicates into one (more selective) monotone predicate (through the use of conjunctions) and similarly for antimonotone predicates. The result is that predicate eval-uations occur for sets that could have already been pruned. 
For this reason the running time is heavily influenced by the order in which the predicates are presented to the algorithm. To avoid this, we can merge all the antimonotone predicates into one (and similarly for the monotone ones) and then use the antimonotone predicate first (since it is likely to be more selective). Assuming P has a support constraint (and is thus probably more selective than Q) it makes sense to run the bottom-up levelwise algorithm for P first. If top-down algorithm is then run for Q, the resulting complexity is similar to CONVERTIBLE. If the bottom-up version is used for Q then we can get the following upper bounds: 
I Th(P) U S(P)[p + [ Wh(P) N Th(Q)[Q + [ Th(P) n B'(Q)[Q 
The main distinction between DualMiner and its com-As the results for MAFIA indicate, it is possible that 
The output representation with subalgebras is more com-
An example that illustrates the fragmentation problem is (C, AIA2...AnC), which happens if the algorithm first He-
While DualMiner interleaves the pruning of P and Q, the and then refine the result (any algorithm that does this will be called a "2-phase" algorithm). If Q has little selectivity, then CONVERTIBLE is expected to be the best algorithm, since it examines Th(-,Q) (which should be a small set). 
However, if Q is selective then it is possible that 2-phase algorithms waste too much time finding Th(P). Our ex-periments show that in this case DualMiner beats even a "super" 2-phase algorithm (where the second phase is com-puted at no cost). To make this evaluation, we assume that 
P is more expensive than Q to evaluate. If P is a sup-port constraint and Q is a constraint on the sum of prices, it is reasonable to expect that P is about N times more expensive to compute than Q (where N is the number of transactions in the database). We use a more conservative estimate and say that P is only 100 times as expensive as Q (P = 100Q). For the first phase of the 2-phase algorithms, we used a MAFIA implementation since it turns out to be more efficient than Apriori. We used the IBM data genera-tor to create the transaction files. Prices were selected using either uniform or Zipf distributions. ing depth-first traversal) and an implementation of CON-
VERTIBLE that uses a MAFIA-style traversM for the first phase. The predicate Q was of the form sum(price) &gt; qthreshold (the value of qthreshold is taken from the x-axis), and P was a support constraint. Here we also show results for DualMiner+q, an optimization which chooses the most expensive item to split on. 
CONVERTIBLE, DualMiner and DualMiner+q vs. the se-lectivity of Q, using a uniform and a Zipf price distribution, respectively. The y-axis is the number of evaluations of Q and the x-axis is the threshold that the sum of prices in an itemset must exceed. As expected, CONVERTIBLE makes much less oracle calls when Q is not selective, but DualMiner spike in the graph for the Zipf distribution. At this point 
DualMiner needs more evaluations of Q even though it is more selective. This could be a characteristic of the distri-bution and the fact that DualMiner may evaluate Q outside Th(Q) when it is looking for the bottom of a subalgebra. and the first phase of the 2-phase algorithms (using uniform and Zipf price distributions, respectively). Here P is 100 times as expensive as Q. The y-axis is the weighted sum 
Ep -t-~ (where Ep is the number of evaluations of P, and similarly for Q) and the x-axis is the same as before. the first phase of the 2-phase algorithms. The y-axis is the weighted sum Ep + EQ (P and Q are weighted equally) and the x-axis is the same as before. and is also selective (keeping in mind that all 2-phase algo-rithms need to do an extra refinement step with Q). When Q is expensive and not very selective, DualMiner performs too many evaluations of Q (in this circumstance, CONVERT-
IBLE would be the algorithm of choice, since it does not waste time pruning with an ineffective Q and only looks at 
Th(-~Q), which is a relatively small set). 
Figure 6: Oracle Calls (Ep + 1E0~0) VS Selectivity of Q quent itemsets I1, 3, 2]. This work was later generalized to include constraints other than support in [10, 9]. These pa-pers introduced the concepts of monotone and antimonotone constraints and introduced methods for using them to prune the search space. The classes of monotone and antimono-tone constraints were further generalized by Pei et al. [12] and also studied by Pei and Han [11]. This problem was 
