 The Aspect Model [1, 2] and the Latent Dirichlet Allocation Model [3, 4] are latent generative models proposed with the objective of modeling discrete data such as text. Though it is not explicitly published (to the best of our knowledge), itisreasonabl ywellkn owninth eresear chcommunitythat the Aspect Model does not perform very well in supervised settings and also that latent models are frequently not iden-tifiable, i.e. their optimal parameters are not unique.
In this paper, we make a much stronger claim about the pitfalls of commonly-used latent models. By constructing a small, synthetic, but by no means unrealistic corpus, we show that latent models have inherent limitations that pre-vent them from recovering semantically meaningful param-eters from data generated from a reasonable generative dis-tribution. In fact, our experiments with supervised classifi-cation using the Aspect Model, showed that its performance was rather poor, even worse than Naive Bayes, leading us to the synthetic study.

We also analyze the scenario of using tempered EM and show that it would not plug the above shortcomings. Our analysis suggests that there is also some scope for improve-ment in the Latent Dirichlet Allocation Model(LDA) [3, 4]. We then use our insight into the shortcomings of these mod-els, to come up with a promising variant of the LDA, that does not suffer from the aforesaid drawbacks. This could potentially lead to much better performance and model fit, in the supervised scenario.
 Categories and Subject Descriptors: I.2.6 [Computing Methodologies]: Parameter Learning General Terms: Algorithms, Design, Theory.
 Keywords: Latent Models, Corpus Modeling
Most latent models set up a likelihood function that de-pends on the parameters to be estimated and fixed but partially-observed data. The model then maximizes the like-lihood wrt to the data [1, 2, 3, 4]. The likelihood surface, plotted against the parameter space, is complex, and most procedures that search for the peak of this surface are im-perfect, i.e., can stop at local maxima. However, for the moment, assume we have a perfect maximizer, and consider only the properties of the surface. In this section, we con-struct a simple corpus from which a human can get a very intuitive and semantically meaningful estimate of the opti-mal model parameters. Then we show that under the aspect model [1, 2], the likelihood achieves an equally large value for an infinite number of other parameter choices, which are all inferior to the intuitive choice. Thus, even if our esti-mation algorithm were perfect, we would have a vanishingly small chance of hitting upon the semantically meaningful optimum in the parameter space.

Consider a simple example of a corpus of two training documents containing two terms each.
Let our classes be A, B and N(A Noise class in which we would like to capture noise words especially stopwords). Traditionally, feature selection has been isolated from super-vised learning as a preprocessing step, but with the aspect model, there is no reason that the learning step cannot di-rectly model the common noise superposed with each class.
We want to permit each document to be generated from one of the signal classes A and B and the Noise class, N. To enforce supervision in the Aspect model, we force the parameters Pr( d 1 | B ) and Pr( d 2 | A ) to zero. If we choose to estimate parameters with EM, all we have to do is to initialize these parameters to 0. It is easy to check that they cannot take non-zero values in subsequent iterations.
It can be seen from table 1 that along with our single de-sirable global optimum, lie infinite other undesirable global optima. This is a rather serious flaw because all these pa-rameterizations lead to different models . This is not similar to the case of throwing in redundant features into a log-linear classifier leading to infinite global optima, such that all of them define the same model, or that of SVD which can have multiple factorizations that are isomorphic. The fact that our different parameters lead to different models can be seen from the fact that different parameters lead to different predictions of Pr( class | document ) for test documents. The inferiority of the other models, is quite easy to see. For example, suppose a model corresponding to solution (3) in table 1 , with x 1 = 0 is learned. As per this model, d is generated entirely by class N and d 2 is generated entirely by class B . This model would predict the best class for a test document { t 1 ,t 3 } (identical to d 1 ) as N and the next best class as B , instead of correctly predicting A as the best non-noise class. It can be seen from the set of solutions that d getting generated entirely by class A and d 2 getting gen-erated entirely by class B is also a globally optimal solution. Thus, the Aspect model fails to properly reward partitioning of the term space among different classes.
The objective of Tempered EM was to find relatively ro-bustsolutions ,reduc esensitivi tytolocaloptim aandcheck overfitting. For the sake of completeness, we examine if tempered EM helps overcome the aforesaid problem. The objective function to be optimized is called Helmholtz free energy which contains an annealing parameter,  X  (  X  = 1 cor-responds to the scenario with no annealing).

It is relatively hard to get the set of global optima for the modified objective function corresponding to Tempered EM in a closed form. We therefore analyze the tempered objective function as follows. We iterate over the permissible
No. Pr( A ) Pr( B ) Pr( N ) Pr( t 1 | A ) Pr( t 2 | A ) Pr( t | B ) Pr( t 2 | B ) Pr( d 1 | N ) Pr( d 2 | N ) Pr( t 1 | N ) Pr( t 1. 0.25 0.25 0.5 1 0 0 1 0.5 0.5 0 0 2. 0.5 x 1 0.5 -x 1 0.5 0 0 x 2 0 1 0 x 3 3. x 1 0.5 0.5 -x 1 x 2 0 0 0.5 1 0 x 3 0 { x 1 , x 2 , x 3 } are 0  X  x 1  X  0 . 5 , 0 &lt; = x 2 ,x 3 &lt; = 1 and x + (0 . 5  X  x 1 ) x 3 = 0 . 25 . Thus both (2) and (3) which are  X  1 .9 .8 .7 .6 .5 .4 .3 .2 .1 0 % .01 .1 1.1 4.5 11 20 31 41 51 59 100 Table 2: Table showing the percentage of points at which the objective function is greater than or equal to that at our desirable point, for different values of the annealing parameter  X  , in the Aspect Model. values of our parameters from 0 to 1 in steps of 0.1. We keep Pr( t 2 | A ) and Pr( t 1 | B ) at 0, since increasing these will only hurt the objective function. The total number of such points is 5.8 x 10 6 .

From table 2, it can be seen that the number of points at which the objective function is greater than our desired solution increases as the value of  X  is reduced. Our desirable parameter is itself reduced to a local optimum, precisely the kind of points that annealing looks to avoid! This is very strong evidence that the problem of poor model fit does not improve with tempered EM.
The objective function of LDA is rather complicated, mak-ing it hard to subject it to detailed analysis as in the case of Aspect. Let us look at a simpler example of a single doc-ument d with terms { t 1 ,t 2 } and two classes, A and B. One possible fit is for class A to generate one term, and for class B to generate the other(with Pr( t 1 | A ) = Pr( t 2 | B ) = 1). Another possibility is for both terms to be generated by one class alone(with Pr( t 1 | A ) = Pr( t 2 | A ) = 0 . 5). Interestingly, LDA considers both of these to be equally good solutions, and the optimal value of the objective function is 0.25 for both cases. It is easy to check that Aspect too considers both solutions to be equally good. Intuitively, it would seem that the first solution is more desirable since it involves better partitioning of the term space among the classes. Thus, it appears that LDA too could have scope for improvement on this front.
We would like a model in which there is adequate reward for partitioning the terms space into different classes. In other words, we would like each class to specialize in certain terms(for which Pr( t | c ) is high). In both Aspect and LDA however, since a class is selected with probability Pr( c ) dur-ing the generation of each ( d,t ) pair, the partitioning goal is partly sabotaged. This is because, we could have a class that generates disproportionately high number of terms and thereby attain a high value of Pr( c ) in Aspect, and equiva-lently a high value of  X  c in LDA. Thus even though Pr( t | c ) cannot be kept high for most of the terms generated by class c , this effect is offset by the fact that Pr( c )(or  X  c high. Using this intuition, we propose a variant of LDA, which does not have this problem.
We propose a variant of LDA called Latent Multinomial model(LMM), which eliminates the aforesaid problem by doing away with the selection of a class in separate trials for writing each term in a document. The model has parameters P (i.e. p 1 ,p 2 ,...,p k summing to 1), where k is the number of classes. It also has  X  parameters that are identical to those in LDA. For brevity, we omit the likelihood function and describe the generation process of each document which is as follows: tion(as in the case of LDA). a multinomial distribution with parameters N, P . independently from class 2, ... , n k terms independently from class k, with the appropriate values of Pr ( term | class ) encoded in the  X  parameters.

This model completely eliminates the problem of not re-warding term partitioning adequately, that this paper has dealt with. For example, the value of the likelihood func-tion corresponding to our desirable parameter values (where class A generates t 1 , class B generates t 2 , class N generates t ) is 2  X  4 while for a solution where class A generates the whole document d 1 and class B generates the whole docu-ment d 2 , the value of the likelihood function is 2  X  8 . Like-wise, for the example in section 1.4, the objective function at our desirable solutions is 0.5, and have value 0.25 for the unpartitioned case.

It would be insightful to run experiments on real-life datasets and compare the perplexity of this model with that of LDA and Aspect and also compare its performance in supervised learning with other generative and discriminative classifiers. Also, one could analyze if this insight helps in improving the recently proposed GaP model [5], further.
 Acknowledgments: I thank my research advisor, Prof. Soumen Chakrabarti for his Aspect Classifier implementa-tion and for the enlightening discussions we shared.
