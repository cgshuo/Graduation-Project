 Word subject domains have been ubiquitously used in dictionaries to help human readers pin-point the specific sense of a word by specifying technical usage, e.g. see  X  X ubject field codes X  in Procter (1978). In computational linguistics, word subject domains have been widely used to improve the performance of machine translation systems. For example, in a review of commonly used features in automated translation, Mowatt (1999) reports that most of the machine transla-tion systems surveyed made use of word subject domains. Word subject domains have also been used in information systems. For example, San-filippo (1998) describes a summarization system where subject domains provide users with useful conceptual parameters to tailor summary re-quests to a user X  X  interest. 
Successful usage of word domains in applica-tions such as machine translation and summari-zation is strongly dependent on the ability to assign the appropriate subject domain to a word in its context. Such an assignment requires a process of Word Domain Disambiguation (WDD) because the same word can often be as-signed different subject domains out of context (e.g. the word partner can potentially be re-
Interestingly enough, word subject domains have been widely used to improve the perform-ance of Word Sense Disambiguation (WSD) algorithms (Wilks and Stevenson 1998, Magnini et al. 2001; Gliozzo et al. 2004). However, com-paratively little effort has been devoted so far to the word domain disambiguation itself. The most notable exceptions are the work of Magnini and Strapparava (2000) and Suarez &amp; Palomar (2002). Both studies propose algorithms specific to the WDD task and have focused on the dis-ambiguation of noun domains. 
In this paper we explore an alternative ap-proach where word domain disambiguation is achieved via word sense disambiguation. More-over, we extend the treatment of WDD to verbs and adjectives. Initial results show that this ap-proach yield very strong results, suggesting that WDD can be addressed in terms of word sense disambiguation with no need of special purpose algorithms. Our approach relies on the use of WordNet Do-mains (Bagnini and Cavagli X  2000) and can be outlined in the following two steps: WordNet Domains is an extension of WordNet ( http://wordnet.princeton.edu/ ) where synonym sets have been annotated with one or more sub-ject domain labels, as shown in Figure 1. Subject domains provide an interesting and useful classi-fication which cuts across part of speech and WordNet sub-hierarchies. For example, doc-tor#n#1 and operate#n#1 both have sub-ject domain MEDICINE , and SPORT includes both athlete#n#1 with top hypernym life-form#n#1 and sport#n#1 with top hy-pernym act#n#2 . To assign a sense to each word in the input text, we used the WSD algorithm presented in San-filippo et al. (2006). This WSD algorithm is based on a supervised classification approach that uses SemCor 1 as training corpus. The algo-rithm employs the OpenNLP MaxEnt imple-mentation of the maximum entropy classification algorithm (Berger et al. 1996) to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs. 
Following Dang &amp; Palmer (2005) and Ko-homban &amp; Lee (2005), Sanfilippo et al. (2006) use contextual, syntactic and semantic informa-tion to inform our verb class disambiguation system.  X  Contextual information includes the verb  X  Syntactic information includes grammatical  X  Semantic information includes named entity 
We chose this WSD algorithm as it provides some of the best published results to date, as the comparison with top performing WSD systems in Senseval3 presented in Table 1 shows---see http://www.senseval.org and Snyder &amp; Palmer (2004) for terms of reference on Senseval3. To evaluate our WDD approach, we used both the SemCor and Senseval3 data sets. Both cor-pora were stripped of their sense annotations and processed with an extension of the WSD algo-rithm of Sanfilippo et al. (2006) to assign a WordNet sense to each noun, verb and adjective. The extension consisted in extending the train-ing data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). 
The original hand-coded word sense annota-tions of the SemCor and Senseval3 corpora and the word sense annotations assigned by the WSD algorithm used in this study were mapped into subject domain annotations using WordNet Domains, as described in the opening paragraph of section 2 above. The version of the SemCor and Senseval3 corpora where subject domain annotations were generated from hand-coded word senses served as gold standard. A baseline for both corpora was obtained by assigning to each lemma the subject domain corresponding to sense 1 of the lemma. 
WDD results of a tenfold cross-validation for the SemCor data set are given in Table 2. Accu-racy is high across nouns, verbs and adjectives. 2 To verify the statistical significance of these re-sults against the baseline, we used a standard proportions comparison test (see Fleiss 1981, p. 30). According to this test, the accuracy of our system is significantly better than the baseline. 
The high accuracy of our WDD algorithm is corroborated by the results for the Senseval3 data set in Table 3. Such corroboration is impor-tant as the Senseval3 corpus was not part of the data set used to train the WSD algorithm which provided the basis for subject domain assign-ment. The standard comparison test for the Sen-seval3 is not as conclusive as with SemCor. This is probably due to the comparatively smaller size of the Senseval3 corpus. Our WDD algorithm compares favorably with the approach explored in Bagnini and Strap-parava (2000), who report 0.82 p/r in the WDD tasks for a subset of nouns in SemCor. 
Suarez and Palomar (2002) report WDD re-sults of 78.7% accuracy for nouns against a baseline of 68.7% accuracy for the same data set. As in the present study, Suarez and Palomar derive the baseline by assigning to each lemma the subject domain corresponding to sense 1 of the lemma. Unfortunately, a meaningful com-parison with Suarez and Palomar (2002) is not possible as they use a different data set, the DSO corpus. 3 We are currently working on repeating our study with the DSO corpus and will include the results of this evaluation in the final version of the paper to achieve commensurability with the results reported by Suarez and Palomar. Current approaches to WDD have assumed that special purpose algorithms are needed to model the WDD task. We have shown that very com-petitive and perhaps unrivaled results (pending on evaluation of our WDD algorithm with the DSO corpus) can be obtained using WSD as the basis for subject domain assignment. This im-provement in WDD performance can be used to obtain further gains in WSD accuracy, following Wilks and Stevenson (1998), Magnini et al. (2001) and Gliozzo et al. (2004). A more accu-rate WSD model will in turn yield yet better WDD results, as demonstrated in this paper. Consequently, further improvements in accuracy for both WSD and WDD can be expected through a bootstrapping cycle where WDD re-sults are fed as input to the WSD process, and the resulting improved WSD model is then used to achieve better WDD results. We intend to explore this possibility in future extensions of this work. We would like to thank Paul Whitney for help with the evaluation of the results presented in Section 3. 
