 Digital documents are easy to copy at very low costs. With the proliferation of Web community tools such as blog and wiki that simplify Web publishing, users (or blog-gers ) can easily post their opinions on news events or topics. For common users with-articles by  X  X opy-and-paste X  part of the document and adding their own comments. From the content manager's point of view, these documents are not well-organized. fragments in separate bloggers' articles. It would be useful if there are effective ways of identifying partially duplicated documents and issuing early warning for possible citations or plagiarism. 
Conventional copy detection methods span a wide range of spectrum in terms of what we need to detect. Since exact copi es among whole documents can be simply detected with checksums, most research focuses on detecting partial copies or near-duplicates that are almost identical. The simplest approach models the process of copying as a series of edit operations such as insertion, deletion, and substitution, and useful only if the difference is small. Some approaches focus on measuring document relationships such as containment and resemblance [3] using simple sketches of documents called shingles , which are basically word n -grams. The more shingles two documents share, the more likely they are near-duplicates. Storing and matching all word n -grams takes too much space and time, which people try to reduce by docu-ment fingerprinting approaches. Hashing word n -grams can save the space, but takes extra time for hash function computation; while selecting subsets of word n -grams as the signature can reduce computation, but details might be lost. Other approaches utilize information retrieval techniques to identify "relevant" documents based on  X  X ag-of-word X  similarity. We are more interested in partial copies among documents, namely, the mutual-inclusive type of near-duplicates, as shown in Fig. 1. 
As shown in Fig. 1, three documents A, B, and C are mostly different, except for the only identical parts that are shared among them, i.e., blocks AB 1 and AB 2 between documents A and B, and block BC 1 between documents B and C. These shared blocks might start at any location in different orders for different documents holding the differentiating the original and the copy. This definition is different from ordinary ones such as Broder [3] and Heintze [11]. First, documents could share partial con-tent, which is not necessarily a containment relationship. Second, they are not "roughly the same" in the sense of resemblance [3] because the percentage of overlap is not large enough. They might only contain small  X  X ey X  portions of overlap that make them "very similar". Third, it X  X  also different from IR approaches using bag-of-word similarity. On the one hand, documents sharing similar words are not necessar-ily near-duplicates. For example, news articles talking about the same people on the same topic might relate to different events. On the other hand, near-duplicates might only share a small portion of contents with different word distributions. Finally, com-puting word-level similarity is time-consuming. In practice, we need an efficient way of near-duplicate detection. 
To effectively identify the mutual-inclusive type of near-duplicates, the following issues have to be addressed. First, it is important to measure the degree of duplication with suitable similarity criteria. Second, the efficiency of document matching highly depends on the similarity criteria. To address these issues, we could exploit different granularity levels of document features: from characters, words, phrases, sentences, paragraphs, to the whole document. At both extremes, it X  X  either too rough to repre-different characters as in edit distance approaches. Also, matching the distributions of word frequencies as in IR techniques might not help. Users usually copy-and-paste in larger blocks than word n -grams and matching large number of word n -grams takes costs, but the detection robustness might be significantly influenced. To avoid unnec-essary details while capturing enough useful information for near-duplicate detection, we propose to measure the degree of duplication at sentence-level. 
The major contribution of this paper is to provide a comprehensive empirical study of sentence-level detection algorithms. First, we propose a simple but effective sen-tence-level feature, the sequence of sentence lengths , for near-duplicate copy detec-tion. Instead of comparing the sentences themselves, we only count the number of words in sentences with a sliding window. It is a very rough document overview which is independent of specific linguistic content. Also, we exploit sentence-level features in document fingerprinting techniques, and compare their effectiveness and efficiency with conventional word-level features. As the experimental results demon-strate, our proposed approach can achieve comparable precision and recall rates as the full fingerprinting and shingling methods more efficiently. Also, our approach takes significantly less time and storage for indexing. We also discuss the possible applica-tions and limitations to our approach. The rest of the paper is organized as follows. Section 2 lists some related works. In Section 3, the methods of the proposed approach are illustrated. Section 4 details the experiments and discussions. In Section 5, we list the conclusions. The field of document copy detection has received much attention thanks to many popular applications, for example, spam site detection using  X  X hrase X -level replication detection techniques [10], duplicate Web page detection [12] and removal in Web search engines, versioned and plagiarized doc ument detection [13] in digital libraries, and duplicate document removal in databases [2, 21]. Conventionally, there are sev-eral general approaches to near-duplicat e document copy detection: edit distance, document fingerprinting, shingling, and bag-of-word similarity. The edit distance [9] between two documents can be measured by the minimum number of edit operations to make two documents identical. Broder [3] measured the concept of "roughly the same" and "roughly contained" by resemblance and containment metrics. Relatively small sketches or shingles of documents which can be computed fairly fast were pro-posed. The document fingerprinting approaches by Manber [15], Heintze [11], and Manku et al. [16] detected almost-identical documents with hash-value based signa-tures. Earlier copy detection systems such as SCAM [21] and COPS [2] are some examples. The bag-of-words approach identified similar documents according to the distribution of word frequencies. Charikar [7] developed a locality sensitive hashing approach based on random projection of words. Henzinger [12] evaluated two state-of-the-art algorithms, that is, Broder [3] an d Charikar [7], for detecting near-duplicate Web pages. I-Match [8] used collection statistics such as inverse document frequency for duplicate detection. 
Recent advances showed rising interests in developing new algorithms for near-duplicate copy detection. For example, Yang and Callan [24] applied an instance-level constrained clustering approach that incorporates document attributes and content structure into the clustering proce ss to form near-duplicate clusters. Huffman et al. [14] improved the recall while maintaining high precision by combining multi-techniques by exploiting the ordering information in prefix filtering. SpotSigs [22] combined stopword antecedents with shor t chains of adjacent content terms. 
Web community applications such as blogs also raised new challenges to the con-ventional approaches. People can easily create more partial copies, which near-duplicate detection techniques might not di rectly apply. Therefore, more research focuses on "intermediate-level X  or partial copy detection. For example, Metzler et al. [17], calculated sentence-level similarities by comparing the word overlap, TF-IDF, and relative frequency measures between sentences. Then, document similarity is estimated by combining sentence-level similarity. The ideas of local text reuse detec-tion [20] and accurate discovery of co-derivative documents [1] are similar to the mutual-inclusive type of near-duplicates, but the methods and the unit of document without shingling twice. To compare different features in various applications, we exploit a flexible system duplicate candidates of a given query, or within a document collection. Second, we types of media such as images or videos if we replace the corresponding feature extraction modules. Here we focus on finding near-duplicate candidates of a given textual query document using sentence-level features. 
There are four main functional modules in the architecture: text content conver-sion, indexing and querying, scoring and ranking, and application processing. First, each document in the index data set is converted into a feature string by the algorithm used in the text content conversion module. Then, feature vectors are extracted from database, which contain the signatures for identifying documents. 
In a query session, the same procedure as described above is applied to the query to Then, document scores are calculated and highly-ranked documents are included in the candidate set by the scoring and ranking module, which are the potential near-duplicates of the query. Other post-processing steps can be further applied depending on the application processing module. 3.1 Sentence-Level Features Before extracting sentence-level features from documents, the pre-defined delimiter set [5, 6] decides the sentence boundaries. Basically, sentences are separated by char-acters in the delimiter set. Since delimiters influence what sentences would be and also the quality of subsequent functions, they should be chosen carefully. After the sentence boundaries are determined, we extract two different sentence-level features: sentence-level fingerprints and sentence lengths . Sentence-level Fingerprints (SL-FP). One of the most popular strategies for implementing fingerprinting schemes is to extract word n -grams as the fingerprints. For example, in the Full-Fingerprinting (Full-FP) scheme, every word n -gram is extracted and hashed by standard hash algo rithms, such as Rabin [4] or NIST's SHA-1 [18]. Shingling methods (SG) are similar to Full-FP except that only selected n -grams each sentence employed as the unit of decomposition. The feature string is a sequence of sentence fingerprints, and each feature vector is simply one hashed sentence. Sentence Lengths. We extract a sequence of sentence lengths from the text as the feature string by the sliding window approach [5, 6]. Specifically, a fixed-length are extracted. The window moves forward a constant number of tokens (the step width) each time. The idea of dynamic jump ing is to skip repeated patterns from consecutive windows [5]. The granularity of comparison depends on the window size and step width. On the one hand, we want to store more feature vectors to have higher chances of matching common feature vectors with near-duplicate documents. On the other hand, we want to minimize the number of feature vectors stored to save the space and time for matching. Thus, there X  X  a tradeoff between accuracy and efficiency depending on window sizes and step widths [5]. 
For accuracy concerns, a larger window si ze matches longer substrings at a time, which will achieve higher precision but miss shorter substrings. A smaller window size will match more substrings, thus achieving higher recall rates with more false positives. With regards to efficiency, for a given window size WS and step width SW , the number of feature vectors extracted from document d i in the data set D is: storing all documents in D is approximately proportional to: fragments which implies more feature vectors to be stored and higher recall rates. A smaller WS and a larger SW will produce a more compact index, which greatly re-duces the comparison time as we will verify in our experiments. 3.2 Scoring and Ranking After querying the feature database with feature vectors extracted from the query, we obtain a candidate set where each document contains at least one common feature vectors with the query. Next, we need to estimate the degree of similarity and deter-mine the ranking among these documents. We denote the N ordered feature vectors of the query Q as: feature vector. The candidate set C after searching all N feature vectors is: verse document frequency of feature vectors in Q is then expressed as: For each document d k in C , we construct another binary vector: which indicates the co-occurrence between the document d k and the query Q . Then the similarity score of d k is calculated by: Equation (3) is co nsidered as the match ratio of d k with Q , where the numerator is the and the match ratio of d k , a document is more likely to contain duplicate content with the query if it has a higher score. The ranking among candidates can then be deter-mined by sorting their scores. Equation (3) can be simplified by ignoring the denomi-nator since it X  X  the same for a given query. In our experiments, we selected six English document collections from the CLIR task in NTCIR workshop [19] as the index data set in Table 1. 
Among the 182,067 documents in the index data set , 180 documents were ran-domly selected as the query data set which was equally divided into three subsets. For each document in the first subset, a selected number of beginning sentences (10 at most, and about 6 on average) were copied and concatenated into a single query document . For the second and third subsets, the middle and ending sentences were similarly copied and added respectively. The resulting query document is assumed to be the  X  X ear-duplicate X  of each document in the query data set. 4.1 Data Analysis and Algorithm Configuration and 70.81 sentences per document on average. The average sentence length is 5.74, which will be later used to determine the substring size for fingerprinting and shin-sentences (97.46%) contain 1-20 words. 
To compare the performance of sentence-level and word-level features, we evalu-ated the performance of the proposed approach and other word-level algorithms with various parameter configurations as follows: Full Fingerprinting (Full-FP). Every substring of 6 words (or 6-grams) in the docu-ments is extracted and hashed by SHA-1. The substring size 6 is determined by the average sentence length (5.74) of the index data set. Shingling (SG-63 and SG-66). It is similar to Full-FP except that one in every w 6-grams is hashed, where w is set to 3/6 for SG-63/SG-66, respectively. Sentence Length. The algorithm as proposed in Section 3.1 with various parameter jumping. Most punctuation marks such as {, ; " . ? !} are included in the delimiter set. Sentence-level Fingerprinting (SL-FP and SL-FP-SD). The algorithm as proposed in Section 3.1. SL-FP uses the same delimiter set as in the sentence length algorithm, while SL-FP-SD only includes {. ; ! ?} in the delimiter set. 
The collision rates in the feature database are explored in Table 2. From indexing perspective, a collision occurs when documents contain an identical feature vector. The efficiency will suffer from high collision rates, since extra mechanisms are needed to handle the collisions in the database. This will be verified in Section 4.2. 
For sentence length algorithms, the collision rate for each WS setting is slightly re-duced as SW increases, and it X  X  also improved with the dynamic jumping ( DJ ) scheme enabled. Note that SL-FP has the highest collision rate among all algorithms. The reason might be having most punctuation marks in the delimiter set leads to higher chances of extracting common phrases. 4.2 Duplicate Detection For each algorithm in Section 4.1 we apply the same feature extraction method at the indexing and query stages, except the following. At querying stage, full fingerprinting was applied to the query document for shingling (SG) algorithms, and non-jumping scheme ( SW = 1) was applied to sentence length algorithms. To illustrate how the symmetric procedure of feature extraction at the indexing and query stages influences the result, we evaluated another algorithm SG-63-Sym, which is identical to SG-63 except that the same shingling scheme was used at both stages. Efficiency Measurements. We first explored the efficiency at the indexing stage for each algorithm in Table 3. 
The pre-processing step involves HTML tag removal and punctuation mark recog-nition where all algorithms have similar performance. In converting documents to simple computation and compact representation, while the fingerprinting algorithms used a time-consuming hash function (SHA-1). Word-level fingerprinting algorithms (Full-FP and SG) produce a larger index structure in terms of the number of features, index size, and access time than sentence-level algorithms as we expected in Section 4.1. Note that the size of index structure produced by the sentence-level fingerprinting is comparable with the sentence length algo rithm. To summarize, sentence-level fea-tures, including sentence length and sentence fingerprints, are more compact and efficient than word-level features. Effectiveness Measurements. We evaluated the effectiveness of different algorithms for searching as listed in Table 4. 
As shown in Table 4, valid result is the number of documents in the candidate set, and recognition indicates how many  X  X ear-duplicate X  documents can be found in the LCR ) document in the ranked list can help determine the appropriate threshold quality. For fingerprinting algorithms, although the overall precision rates are low, their ranking qualities are good. This indi cates the proposed scoring function can assign appropriate weights to the candidates so that near-duplicates can be found in top ranks of the valid results. Note that SL-FP took almost 100 times longer than SL-FP-SD for searching. One possible reason could be its much higher collision rate as shown in Table 2. 
To compare the effectiveness of each algorithm, we checked the top-k precision, recall and F 1 rates (for k =5 to 225, in increments of 5) using the test query. For better visual effects, partial results are shown in Fig. 4. (a)
In Fig. 4(a), the top-k precision rates showed an earlier drop for shingling and sen-tence-length algorithms with larger window sizes than for sentence fingerprinting and sentence-length algorithms with smaller window sizes. All algorithms showed similar performance in top-k recall rates, which is not shown here. Most algorithms differ in top-k F 1 rates within the range of k =150-200, which we further explored as in Fig. 4(b). Specifically, Table 5 lists the top-180 precision, recall, and F 1 rates. 
As shown in Table 5, SL-FP-SD achieved the best effectiveness for searching among all algorithms. It also showed more efficient computation than word-level fin-gerprinting features for indexing in Table 3. Full-FP achieved high top-180 recall rates, but it takes much more space and time to keep details of the original contents. In sum-mary, SL-FP-SD achieves a good balance between efficiency and effectiveness. 4.3 Discussion There are several potential applications of the proposed approach. For example, it can be used to detect quotations without explicit references on blog sites. When a blogger posts a new article, the system can check the submitted content against the document archive on the site. Another example is to detect the possible implicit links under the assumption that documents containing the same duplicate contents could be regarded relevant to each other. 
There is one potential limitation to the proposed sentence-level approach: short documents are difficult to detect. Since feat ures are extracted in sentence-level, short documents will have short sequence of features, which makes detection of possible copies more difficult. Other examples include poems or lyrics that have regular pat-terns and fixed sentence lengths. From sentence lengths alone, we cannot effectively distinguish among different documents. In this case, word-based approaches could be adopted to complement the sentence-level approach. We proposed a sentence-level approach to the mutual-inclusive type of near-duplicate detection. A simple but effective feature, the sequence of sentence lengths, was pro-posed, which can be directly applied to languages with fixed sentence delimiters without knowledge of the specific language. We also compared sentence-level with word-level features for near-duplicate detection. The experimental results showed the potential of the effectiveness and efficiency for sentence-level features. In the future, we intend to investigate other sentence-level features for partial copy detection. 
