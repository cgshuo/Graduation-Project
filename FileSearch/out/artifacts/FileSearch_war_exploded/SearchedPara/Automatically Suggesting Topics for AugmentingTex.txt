 We present a method for automated topic suggestion. Given a plain-text input document, our algorithm produces a rankin g of novel topics that could enrich the input document in a meanin g-ful way. It can thus be used to assist human authors, who often fail to identify important topics relevant to the context of the d ocuments they are writing. Our approach marries two algorithms origi nally designed for linking documents to Wikipedia articles, prop osed by Milne and Witten [15] and West et al. [22]. While neither of them can suggest novel topics by itself, their combination does h ave this capability. The key step towards finding missing topics cons ists in generalizing from a large background corpus using principa l com-ponent analysis. In a quantitative evaluation we conclude t hat our method achieves the precision of human editors when input do cu-ments are Wikipedia articles, and we complement this result with a qualitative analysis showing that the approach also works w ell on other types of input documents.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  linguistic processing ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  text analysis ; I.7.1 [ Document and Text Processing ]: Document and Text Editing X  document man-agement Algorithms, Experimentation Topic Suggestion, Principal Component Analysis, Eigenart icles, Data Mining, Wikipedia
As of 2010, machines cannot think. They cannot understand na t-ural language, and they cannot produce it in creative ways. T hese are still human prerogatives. However, while humans are int elligent and creative, they are sometimes forgetful, or not thorough enough: when writing text documents, we often fail to mention all rel evant topics, be it that we are unaware of the missing topics being r ele-vant, or be it that we simply forget to include them. Computer s, on the other hand, excel at large-scale book-keeping and fast r etrieval, as long as no deep understanding is required. Hence, althoug h not creative per se, computers can still support the creativity of humans.
The main contribution of this paper is a method that can assis t human creativity by automatically suggesting topics to aut hors of text documents. To the best of our knowledge, we are the first t o ad-dress this task. Given a plain-text document, our algorithm outputs a ranked list of novel topics that could enrich the input docu ment in a meaningful way. The author can then inspect the suggestion s and decide which of them should be incorporated into the documen t.
Our approach couples two algorithms originally designed fo r linking documents to Wikipedia articles, proposed by Milne and Witten [15] and West et al. [22]. While neither of them can suggest novel topics by itself, their combination does have this cap ability. The key step towards finding missing topics consists in gener aliz-ing from a large background corpus, such as Wikipedia. Intui tively, a missing topic is suggested if it appears in many documents o f the corpus that are similar to the input document. Technically, the gen-eralization is performed using principal component analys is.
An automated topic suggestion system could be widely applic a-ble. For instance, it is often impossible for journalists to be experts in all the fields about which they write. In this scenario, the jour-nalist would first write a draft of the article, feed it to our s ystem, and use some of the resulting topic suggestions to make the ar ticle more complete. There are many other user groups that could al so profit from automated topic suggestion, e.g., lawmakers try ing to avoid loopholes in a legal text, or Wikipedia contributors w orking to make an article as comprehensive as possible, to name but a few.
The remainder of this paper is structured as follows: Sectio n 2 summarizes related work. In Section 3 we provide a high-leve l overview of our method, while Section 4 describes the techni cal details. Section 5 contains a quantitative evaluation show ing that our method achieves the precision of human editors when inpu t documents are Wikipedia articles. In Section 6 we complemen t this result with a qualitative analysis showing that the app roach also works well on other types of input documents. Finally, Secti on 7 contains conclusions and avenues for future research.
To the best of our knowledge, the task of topic suggestion has not been studied extensively yet. While we are not aware of a t opic suggestion algorithm for plain-text documents, Maguitman et al. [10] developed such a method for  X  X oncept maps X , i.e., seman tic network X  X ike graphical representations that can  X  X acilit ate knowl-edge capture for human examination and sharing X  [10]. Their sys-tem can assist people during the process of drawing a concept map, by proposing topics that are novel yet related to the concept map produced so far.

Fortuna et al. [7] propose a solution for the similar task of con-structing ontologies, i.e., networks of topics interconne cted by re-lations. In Fortuna et al.  X  X  setting, the ontology is built by a human knowledge engineer with the help of their software. Whereas Ma-guitman et al.  X  X  focus is on generating topics that are both related and novel to the current context, novelty does not play a cent ral role in Fortuna et al.  X  X  system. Instead, it suggests potential subtopics for the topic node on which the knowledge engineer is current ly working.

Wang et al. [20] deal with visual topic suggestion for group brainstorming situations. Their system analyzes the utter ances of the participants of a brainstorming session and chooses and dis-plays images that are related to the current context, with th e goal of enhancing creativity by triggering novel ideas in the human s per-ceiving these visual stimuli.

Closely related to topic suggestion is keyphrase extractio n: be-fore one can find novel topics to add to a document, one should know which ones it already contains. Consequently, all of th e aforementioned systems have a component that identifies the key-phrases, or main topics, of the current context. Note that we use the terms  X  X opic X  and  X  X eyphrase X  interchangeably, which diff ers from some other authors X  nomenclature; e.g., in latent Dirichle t alloca-tion [3] topics are defined as probability distributions ove r words.
Keyphrase extraction is usually distinguished from keyphr ase as-signment [18]. In keyphrase extraction any n -gram of the input document can potentially be returned as a keyphrase, wherea s in keyphrase assignment a predetermined set of keyphrase cand idates is assumed. Both types of applications are usually cast as su per-vised classification tasks [18].
 Wikipedia link prediction is similar to keyphrase assignme nt. Given a plain-text document, the task is to (1) find the n -grams that should serve as anchors for links to Wikipedia articles , and (2) for each anchor, identify the correct target article. Th is problem has also been tackled with supervised machine learning tech niques, e.g., by Mihalcea and Csomai [12] and by Milne and Witten [15] .
Another class of algorithms for Wikipedia link prediction l ever-ages unsupervised machine learning techniques. Fissaha Ad afre and de Rijke [6] use clustering, while West et al. [22] use princi-pal component analysis. Unlike the supervised algorithms [ 12, 15], these methods do not operate on plain text but augment articl es that already contain a number of Wikipedia links, by adding new li nks that are justified by the pre-existing hyperlink structure.
We leverage some of these techniques [15, 22] as part of our approach to topic suggestion.
Figure 1 sketches the components of our system and the infor-mation flow between them. In this section, we will provide a hi gh-level overview, while Section 4 contains technical descrip tions of all components.

Keyphrase assignment. The input to our method is a plain-text document, i.e., a sequence of words d . It is fed to the first compo-nent of our processing cascade, a keyphrase assignment algo rithm. This module identifies the main topics of the input document a nd produces a high-dimensional, binary vector representatio n v of it. In this topic vector, each entry corresponds to a candidate topic (recall that in keyphrase assignment, the set of candidate t opics is predetermined and static); the topics appearing in the inpu t docu-ment have a value of 1, all others are 0. Since only a small frac tion of all candidate topics appear in a document, v is extremely sparse.
Generalization. The centerpiece of our system is the general-ization module. While the left and right boxes of Figure 1 may be thought of as pre-and postprocessing steps, respectively, the actual topic suggestion algorithm resides here. It takes as input t he topic vector v that results from keyphrase assignment, and produces as output a vector  X  v , called generalized topic vector, of the same di-mensionality as v . While each entry of  X  v still refers to the same can-didate topic as the corresponding entry in v , the values differ, and unlike the sparse and binary v , vector  X  v is dense and real-valued. Entries that are zero in v but much greater in  X  v correspond to top-ics that are not keyphrases of the input document but that are sug-gested as such by our algorithm. We construct  X  v by generalizing from a large background corpus using principal component an al-ysis (PCA), a mathematical technique that is commonly used f or reducing noise in data. Its usage within our algorithm may be un-derstood intuitively in terms of noise reduction, too: if th e absence of a topic j from the input document d (i.e., the entry v topic vector v ) is caused by noise, then this noise can be eliminated by adding j to d (i.e., by giving the entry  X  v j a positive value, rather than zero). We say that the absence of topic j is due to noise if it constitutes a significant deviation from the overall patter ns present in the background corpus, i.e., if many documents similar to d con-tain topic j .

Filtering and ranking. After generalization, the topic sugges-tions are implicitly given in the vector  X  v , as the entries that are much greater than they were in v . However, we want to exclude those topics that are not novel, i.e., that are already conta ined in the input document d as n -grams (not necessarily as keyphrases). This constitutes the filtering step. The remaining topic sug gestions are ranked in order of decreasing quality, as is common pract ice in information retrieval systems. Fortunately, the generali zation step generates meaningful numerical values that can directly se rve as indicators of suggestion quality, so we simply rank the sugg ested topics according to their values in the generalized topic ve ctor  X  v .
After the high-level view of the previous section, we will no w go into more detail regarding each component of our topic sugge stion system.
The task of the keyphrase assignment component is to identif y the important topics of a given plain-text document. In prin ciple, any keyphrase assignment algorithm can be used in this step. As mentioned in Section 2, all such algorithms work with a prede ter-mined set of candidate topics (let us call it T ). This set defines the dimensions of the topic vectors v and  X  v of Figure 1. As we want our method to be domain-independent, T should be as general as possible. We define T as the set of topics for which a Wikipedia article exists, since Wikipedia X  X  coverage is so vast that n early any conceivable topic has a corresponding Wikipedia article. I f for any reason only domain-specific candidate topics should be cons idered, T can be restricted accordingly.

According to the Wikipedia linking guidelines, the keyphra ses of an article should serve as anchors for hyperlinks to other ar ticles: links should represent  X  X elevant connections to the subjec t of an-other article that will help readers to understand the curre nt article more fully X  [25]. Consequently, a program that can successf ully identify the Wikipedia anchors of an input document (i.e., t he n -grams that should serve as link anchors to Wikipedia article s) can also be used for the task of domain-independent keyphrase as sign-ment. In Section 2 we have mentioned two methods [12, 15] that can augment plain text with Wikipedia links and that can ther efore be plugged into our system as keyphrase assignment modules. We use Milne and Witten X  X  method [15] because it outperforms Mi hal-cea and Csomai X  X  [12] and because it is publicly available as part of the WikipediaMiner toolkit [13].

In a nutshell, Milne and Witten X  X  algorithm is a machine lear n-ing classifier that decides for each n -gram of a plain-text input doc-ument to which Wikipedia article (if any) it refers and with w hat probability it should serve as an anchor to that article. It i s trained in a supervised manner on Wikipedia articles, which can serve a s la-beled examples, since each article contains numerous link a nchors. Although it is trained entirely on Wikipedia, the classifier performs as well on newswire stories as it does on Wikipedia articles, as shown in a human user evaluation. This is important because w e want our method to work on arbitrary text documents, not only on Wikipedia articles.

Milne and Witten X  X  link prediction algorithm offers severa l pa-rameters that can be configured, but we use the default settin gs for all of them. Since the classifier outputs a probability for ea ch phrase of the input text, we need to define a threshold above which we a c-cept proposed keyphrases. We set this parameter to 0.5, i.e. , we take a phrase to be a keyphrase if the classifier considers thi s more likely than not.

Note that if the input document already contains links to Wik ipe-dia articles, keyphrase assignment is trivial. In this case we simply use the targets of the existing links as the document X  X  keyph rases.
The generalization component can be understood as reducing the noise in the topic vector v in the context of a large background corpus. We do not use this corpus in its plain-text form; rath er, we preprocess it using PCA. In this section we explain the use of PCA for generalization, in both intuitive and technical terms.
The background corpus can be any sufficiently large collecti on of text documents, e.g., Wikipedia articles, newswire stor ies, scien-tific papers, parliament debates, or law texts, to name but a f ew. Let C = { c 1 , c 2 ,..., c M } be the set of documents in the corpus, where M is the number of documents. The same keyphrase assignment algorithm we use to find the topics of the input document (cf. S ec-tion 4.1) is also run on each document c i of the corpus, in order to obtain its topic vector t i . If we take topic vectors as row vectors, then we can combine them in a matrix T whose i -th row is t call T the document X  X opic matrix . Recall that the entries of a topic vector correspond to the candidate topics T . Thus, if there are N topics, T is of size M  X  N . When we define T as the set of Wiki-pedia articles X  X s we do throughout this paper X , N will be very large, and it follows that M  X  N .
Geometrically, T can be interpreted as a cloud of M points in an N -dimensional Euclidean space (called document space ), where each topic vector (row of T ) is represented by one point. This cloud will in general not extend equally in all directions but rath er appear squished along certain axes and elongated along others, due to cor-relations in the data. PCA finds a set of orthogonal axes along which the data cloud is maximally spread out. More specifical ly, the first axis found by PCA, the so-called first principal comp onent, is the vector in the N -dimensional space along which the variance of the data is maximized (when the data is projected onto it); the second principal component is constrained to be orthogonal to the first and chosen such that the variance is again maximal when t he data is projected onto it; and so on up to M . For technical reasons (cf. Section 4.2.3), the principal components are called eigenarti-cles, 1 and the space spanned by them eigenspace. The less impor-tant eigenarticles merely account for minor variations in t he data, variance being small along their directions. Such variatio ns can be considered noise, and by eliminating them, the noise in the d ata is reduced. By convention, all eigenarticles are normalized t o a length of 1. We write eigenarticles as row vectors and stack them on t op of each other, thus obtaining the eigenarticle matrix E of size M  X  N .
Now assume we want to reduce the noise in a new topic vector v . We can achieve this in three steps. First, project v onto the basis spanned by the eigenarticles; this way we obtain v  X  X  eigenspace representation Second, eliminate the noise that manifests itself as variat ion along minor eigenarticles by setting the respective co-ordinate s to zero, keeping only the entries ( p 1 , p 2 ,..., p K ) as non-zero, for some fixed K (the so-called eigenspace dimensionality ). Call the resulting vec-tor  X  p . Third, project  X  p from eigenspace back into the original docu-ment space spanned by the canonical basis vectors. This yiel ds the generalized topic vector
In the above explanation, we have included the second step ju st to be conceptually clear. In practice, it suffices to store on ly the first K eigenarticles. (We use  X  E to denote the resulting eigenarticle matrix of reduced size K  X  N .) Equations (1) and (2) may then be combined, and the generalized topic vector computed as
After finding the eigenarticles, the background corpus C need not be stored explicitly. We only need its reduced eigenarticle matrix  X  E . It is important to note that  X  E has to be computed only once, in an offline preprocessing step. During normal operation of o ur al-gorithm, only single projections into eigenspace and back a re per-formed, according to (3).
For clarity X  X  sake, we have so far glanced over several techn ical issues. In this section we provide some details that are impo rtant for
The term  X  X igendocument X  would be more appropriate, but we stick to the nomenclature of the paper in which the concept wa s first introduced [22]. There, the authors use the term  X  X igen article X  because all their documents are Wikipedia articles. making our algorithm mathematically sound and computation ally efficient.

Topic candidate selection. Above, we have defined the set T of candidate topics as the set of all Wikipedia articles. Howev er, the Wikipedia snapshot we use [24] has about 2.7 million article s, i.e., the number of topics N  X  2 . 7  X  10 6 , which would make the reduced eigenarticle matrix  X  E too large to fit into memory. We therefore follow the approach of West et al. [22] and include in T only those Wikipedia articles with at least 15 incoming and 15 outgoing links. This way many unimportant articles are discarded and N is reduced to 468,510, or 17% of the original size.

Topic weighting. As defined in Section 3, topic vectors v are binary. However, not all topics present in a document are equ ally informative; e.g., the fact that a document mentions a rare c oncept such as CN TOWER is much more salient than the fact that it talks about something more common such as CANADA . We therefore ap-ply the IDF weighting scheme also used by Milne and Witten [14 ] and West et al. [22], according to which a topic gets more weight if it appears in fewer documents of the background corpus C : before feeding topic vector v to the generalization component, we weight its j -th entry by a factor of log ( M / M j ) , where M j the documents containing the j -th topic. The weighting is also per-formed on each row of the document X  X opic matrix T before we find its eigenarticles.

Computing PCA efficiently. We call the principal components of T eigenarticles because mathematically they are the eigenve ctors of the covariance matrix (and thus the scatter matrix) of T . Let m =  X  M i = 1 t i be the mean topic vector, and M a matrix of M rows, all of which are equal to m . Then the scatter matrix is ( T  X  M ) which has dimensionality N  X  N . To speed up computation and reduce memory requirements, one may compute the eigenvecto rs of another matrix L = ( T  X  M )( T  X  M )  X  and obtain the eigenarticles by pre-multiplying these eigenvectors by ( T  X  M )  X  (cf. West et al. [22] for a derivation). Recall that M  X  N , so the M  X  M matrix L is much smaller than the N  X  N scatter matrix.

Since each document contains only a small fraction of all can di-date topics, T is extremely sparse. But subtracting the mean maps most zeros to negative numbers. So T  X  M is dense, and the na X ve approach of first computing T  X  M and then multiplying it with its transpose to obtain L = ( T  X  M )( T  X  M )  X  takes very long. ever, calculating L as takes only a few seconds (as opposed to several hours), since TT can be computed fast due to T  X  X  sparsity, MM  X  has the constant mm  X  everywhere, and all columns of TM  X  are equal to Tm  X 
Projecting into eigenspace efficiently. As outlined above, the task of the generalization component is to project a topic ve ctor v into reduced eigenspace and then back into the original docu ment space. Since we mean-center the topic vectors of the backgro und corpus before finding the eigenarticles, we must also center v . Pro-jecting into eigenspace in fact amounts to computing ( v  X  m ) Here, too, we may decrease running time (by up to four orders o f magnitude) by reformulating to v  X  E  X   X  m  X  E  X  , where the first term can be computed efficiently because v is sparse and the second term does not depend on v , i.e., needs to be calculated only once, offline.
If principal components are computed without mean-centeri ng, as the eigenvectors of T  X  T , the first principal component will approx-imate the data mean rather than the direction of maximum vari ance. While this is acceptable in certain applications of PCA, it a ffects performance negatively in our case, since it results in  X  v (cf. (3)) being skewed towards the mean, which in turn makes topic sugg es-tions less meaningful.
The goal of the filtering and ranking step is to prepare the top ic suggestions for human inspection.

If the input document d does not contain the j -th topic of the candidate set T and  X  v j  X  v j after generalizing, then this suggests that j should be considered for inclusion in d . The difference  X  v  X  v , which we call reconstruction gain vector, attributes to each topic in T a real number quantifying how good a suggestion it would be. Therefore, our algorithm X  X n the manner of typical infor ma-tion retrieval systems X  X oes not select a subset of topics th at are suggested for inclusion in the input document, but rather ra nks all potential candidate topics, by sorting them in order of decr easing reconstruction gain.

As our goal is to suggest only novel topics, a topic should only be considered if the input document d does not already mention it. So certainly a topic should be filtered from the list of sugges tions if it is a keyphrase of d . But even if it is not a keyphrase, it might still appear in the plain text as an ordinary phrase and shoul d thus be filtered, too. There are several ways of determining wheth er d mentions a topic j . Recall that we represent topics as Wikipedia articles. The simplest solution would be to check if d contains the title of j  X  X  Wikipedia article. This is, however, overly restrictive and results in low recall; e.g., we could not tell that a document that has the n -gram  X  X aple Leaf Flag X  implicitly contains the concept
We therefore adopt a more robust approach, by leveraging Wi-kipedia links. The key observation is that the anchor texts u sed in Wikipedia to link to the article about j are in many cases synonyms of j , e.g., the anchor  X  X aple Leaf Flag X  links to the article abou t FLAG OF CANADA . However, the word  X  X lag X , too, is used to link to this article, so simply accepting all anchors of an articl e j as names for it would result in low precision (since any flag migh t be referred to as  X  X lag X ). To trade off precision against recall , we con-sider as names of j only anchors that link to j with high probability. Specifically, we accept a phrase a as a name for j only if its anchor likelihood Pr ( Target = j | Anchor = a ) is at least 30% (we chose this value by hand). Then, if the plain text of d contains phrase a we say that d talks about topic j and exclude j from our suggestion list. Note that, while this approach draws on Wikipedia, d need not necessarily be a Wikipedia article itself.

Let X be the set of topic suggestions, i.e., the set of topics we do not exclude in the filtering step. It is interesting to note that its complement, T \ X , can be interpreted as the set of Wikipedia link suggestions. Recall that during keyphrase assignment we in fact augment d with links to Wikipedia (cf. Section 4.1). That is, a high-ranking suggestion j  X  T \ X , which does have an anchor in d , corresponds to a Wikipedia link that could be contained in d and whose absence is caused by noise. Consequently, with a modified filtering step, our method can be used for finding miss ing Wikipedia links, as done by West et al. [22].
In this section we evaluate our topic suggestion algorithm q uan-titatively. Without querying human raters, this is rather d ifficult to achieve for general input documents, since judging the qual ity of topic suggestions is a highly subjective task. Instead, we a ttempt to get some insight into the performance of our method by con-straining input documents to be Wikipedia articles and defin ing an automated evaluation heuristic based on the articles X  edit history, such that the precision and recall of our method can be gauged on a large number of test documents. The results are presented i n Sec-tion 5.1. Since this automated evaluation has several short comings, we re-evaluate precision on a smaller test set by querying hu man raters. Those results are presented in Section 5.2.

In this first set of experiments, both the background corpus C and all input documents d are Wikipedia articles. As a typical Wikipe-dia article d already contains links to other articles (which normally represent the keyphrases of d ; cf. Section 4.1), we need not run the keyphrase assignment step to obtain the document X  X erm matr ix T and the input document X  X  topic vector v . In principle, we may sim-ply define T as Wikipedia X  X  adjacency matrix. In practice, we com-press its size by keeping as columns T only the candidate topics as defined in Section 4.2.3, and as rows C only a sample of 5,503 im-portant articles (we follow West et al. [22] and choose the articles that are also included in the 2008/9 Wikipedia Selection for schools [23]).

We use the Wikipedia snapshot of March 6, 2009 [24]. In Sec-tion 4.2.3 we refer to the sparsity of T . To express it in numbers, we note that, using this snapshot, only 2.7% of T  X  X  roughly 30 million entries are non-zero.

We set the eigenspace dimensionality to K = 1,000 in all exper-iments that use Wikipedia as a background corpus. This param eter has been hand-picked based on computational constraints an d was not optimized via learning.
Given an input document d , let the set X k contain the top k sug-gestions returned by our algorithm for d , and let R be the ground-truth set of relevant novel topics that are missing from d . Then, precision at k is | X k  X  R | / | X k | , or the number of relevant sugges-tions divided by the number of suggestions, while recall at k is | X k  X  R | / | R | , or the number of relevant suggestions divided by the number of relevant novel topics.

As mentioned above, the notion of relevance is highly subjec tive and cannot be defined in absolute terms [11], so precisely defi ning the set of relevant novel topics R is not possible. Instead, we have to recur to a reasonable heuristic definition of R . Consider two versions of an input article d , one from March 2009 (the time of the Wikipedia snapshot we are using), the other from April 20 10 (i.e., about one year later). More often than not, human edit ors will have added several novel topics to the article during th is one-year period. We make the assumption that editors mark import ant new topics by linking them to the respective Wikipedia artic les, in accordance with Wikipedia X  X  linking guidelines (cf. Secti on 4.1), and define the set of relevant novel topics R as the set of links contained in the new but not in the old version of d . Measuring precision and recall with respect to this ground truth, we ca n then estimate how well our algorithm matches the heuristically d efined editing capabilities of human experts.

Since we are interested in novel topics (rather than merely novel links), we include in R only links that correspond to topics which do not appear in the plain text of the old version of the input a rticle. To determine whether this is the case, we take the link anchor  X  based approach described in Section 4.3.

In order to avoid the most obscure articles, we consider as te st documents only articles with at least 100 incoming and 100 ou tgo-ing links. Also, to bound the size of R and thus make precision and recall comparable across input documents, we consider only test documents to which editors added between 10 and 20 new links i n the one-year period.

We use two evaluation sets. One comprises 100  X  X ommonsense X  articles. We chose this set with the human user evaluation wh ich we present later in mind (cf. Section 5.2). The rationale is t o fa-cilitate the evaluation process by making sure raters have a basic understanding of the article for which they judge topic sugg estions without having to read the article. To find these 100 articles , we presented lists of random article titles to 10 members of our group not involved in this research and asked each of them to select about 20 titles with the following property:  X  X ach selected title should represent a topic of which you have at least some basic knowle dge. You don X  X  have to be an expert in the topic, but you should have a rough idea what it is about. X  This way, 213 titles were identi fied. Some of them were non-obvious, and manually sifting these, w e kept 100 commonsense topics.

The other evaluation set consists of 1,000 articles. Unlike the set of 100 commonsense articles, these were randomly select ed and can therefore serve for measuring the performance of our alg orithm on typical Wikipedia articles.

The results are plotted in Figure 2, as functions of k , the number of top suggestions we return. First of all, note that the curv es look rather similar, with the algorithm performing slightly bet ter on the 100 commonsense articles than the 1,000 randomly selected a rti-cles. Recall increases superlinearly, which implies that a t the top of our ranking, relevant suggestions are denser than further d own, as desired. Recall at 1,000 is 26% for commonsense articles and 20% for arbitrary articles, i.e., the top 0.21% of the full ranki ng of all N = 468,510 candidate topics contain 26% or 20%, respectively, of the novel topics that were added as new links by human edito rs during the period of one year.
 While these recall values seem satisfying, precision is rat her low. Figure 2 plots two precision curves. For calculating strict preci-sion, we count a suggestion j as relevant only if it is in R , i.e., if a link to the article about j was added to the input article in the one-year period. In practice, this is often too restrictive, sin ce an editor might have added a mention of j to the article without also adding a link to the article about j . This is accounted for by soft preci-sion, which counts the suggestion j as relevant if a mention of it was first added to the plain text of the input article within th e one-year period. To decide if a document mentions a topic, we agai n use the link anchor X  X ased method of Section 4.3. Figure 2 sho ws that the soft precision attained by our algorithm is somewha t higher than strict precision, as expected. (Both strict and soft pr ecision are reported in their interpolated form [11].)
Although soft precision is clearly a more realistic metric t han strict precision, it still does not appropriately capture t he perfor-mance of our method. Consider, e.g., the top suggestions of o ur algorithm for the article about COMPUTER PROGRAMMING , listed in Table 1. Among the top five are topics such as TURING COM these topics are doubtless relevant, they are not counted as such because even the newer version of the article (after the one-year period) does not mention them yet. This is an inherent limita tion of the evaluation paradigm that compares an old and a new versio n of an article: by defining ground truth based on the current vers ion of an article, it assumes the latter to be perfect. Not only woul d this make suggesting further topics X  X nd hence our work X  X ointle ss, it is also simply not the case.

Recall is affected in a similar way: often, human editors add novel links that do not correspond to relevant topics, or onl y mar-ginally so. For instance, a link to the article about 1947 was added to the article about COMPUTER PROGRAMMING (cf. Table 1). Our method rightfully does not rank it amongst the top suggestio ns, yet this results in a decreased recall value.
Because of the shortcomings of the automated evaluation met h-Table 1: Left: The 18 novel links (i.e., topics) human editors added to the Wikipedia article about COMPUTER PROGRAM -MING between March 2009 and April 2010. An asterisk signi-fies that the respective topic is among our algorithm X  X  top 1, 000 suggestions. Right: The 18 top suggestions of our algorithm. odology described in the previous section, we found it neces sary to complement these results with a human evaluation. Measur -ing recall remains elusive, since it would require defining R , the ground-truth set of relevant novel topics, for each single t est docu-ment, which is practically prohibitive, and theoretically impossible due to the extremely subjective nature of the task. Precisio n, on the contrary, can be readily estimated by querying humans, s ince it does not require the a-priori definition of all of R ; rather, we can have human raters assess relevance ad hoc, for each single sugges-tion.

In the experiments reported below, our goal is to compare the precision of our algorithm to that achieved by human Wikiped ia editors. We define the set of human topic suggestions for an ar -ticle d as the R of Section 5.1, i.e., as the set of hyperlinks that were introduced to d during the period of one year and that do not correspond to topics already mentioned in the plain text of t he old version of d . Suppose there are k such new links. In the set X Figure 3: Average precision on the set of 100 commonsense ar-ticles. Left: Our topic suggestion algorithm. Right: Human Wikipedia editors. Error bars show 95% confidence intervals . we pool the same number of top suggestions from the ranking pr o-duced by our algorithm. We then ask human raters to decide, fo r each suggestion in R and X k , whether it is relevant, and compute precision values for both sets of suggestions.

As our evaluation platform, we used Amazon Mechanical Turk [2]. Each evaluation task dealt with one input article d about a topic  X  . The rater was asked to assess the relevance of all suggestio ns for d , human and automated, following these instructions:
This task description was followed by the topics from R  X  X randomized order to avoid any bias. As test documents we used the 100 commonsense articles introduced in Section 5.1, which c ontain each between 10 and 20 new human-added links, so the combined list of topic suggestions had at most 40 entries. To make the c ol-lected data more reliable, we protected the forms against We b bots using reCAPTCHA [19] and had the suggestions for every input ar-ticle evaluated by five different raters. When interpreting the data, we say that a suggestion is relevant if a majority of the five ra ters (i.e., at least three) said so.

Figure 3 shows the average precision attained by our algorit hm and by human editors, where the average is taken over the 100 t est articles. The method we propose has an average precision of 4 3% (i.e., on average, 43% of the suggestions we make are assesse d as relevant by a majority of raters), while human editors achie ve 42%. The difference is not statistically significant at the p &lt; 0 . 05 level (determined via bootstrap resampling), which suggests tha t human Wikipedia editors and our method perform equally well at thi s task.
The precision of 43% attributed to our method under this eval u-ation methodology is an order of magnitude higher than the st rict precision (at k = 15) of 3.5% shown in Figure 2 (we consider k = 15 because k  X  { 10 ,..., 20 } in the human evaluation). Even soft preci-sion as calculated there (10%) underestimates considerabl y.
These results confirm the observation of Section 5.1 that usi ng the links, or topics, added by human editors as the ground tru th of relevance is not a good heuristic. By definition, the ground t ruth should have perfect precision and recall. However, we show t hat precision is only 42%; also, the fact that many of our relevan t sug-gestions lie outside the  X  X round truth X  implies that the lat ter has far from perfect recall.

The precision values reported in this section were obtained using the set of 100 commonsense articles. Referring back to the au to-mated evaluation summarized in Figure 2, note that the stric t pre-cision curves for the 100 commonsense articles and the 1,000 ran-dom articles look virtually identical, while soft precisio n is slightly higher for commonsense articles. We conjecture that the act ual pre-cision (as measured in the human evaluation) scales analogo usly, being only slightly higher for commonsense than for random a rti-cles.
In the previous section we have demonstrated through a quant i-tative evaluation that our algorithm can match the performa nce of human editors when the input documents are Wikipedia articl es. However, it is important to note that our algorithm is design ed to work not only on Wikipedia articles but on arbitrary plain-t ext doc-uments. We illustrate this in the present section.

A quantitative evaluation is considerably harder for arbit rary in-put documents than for Wikipedia articles: in order to be abl e to assess the relevance of link suggestions, human raters woul d have to read the entire input document first. In the case of Wikiped ia ar-ticles, this is usually not necessary when testing on common sense articles. For this reason, we provide a more qualitative ana lysis here, as follows: In Section 6.1, we show the output of our al-gorithm for a variety of input documents and comment on how the  X  X enre X  of a document affects the quality of topic sugges tions. While in all these examples we use Wikipedia as a background c or-pus, Section 6.2 demonstrates that the quality of suggestio ns can be improved by using a domain-specific background corpus. We us e the U.S. Congressional Record to illustrate this point.
Table 2 lists the top 10 topic suggestions for four input docu -ments, which have been chosen because they belong to rather d if-ferent genres. In the following, we comment briefly on each o f these, in order to characterize qualitatively how our metho d can be expected to work on different types of documents.

Newspaper articles. The topic suggestions our algorithm makes for newspaper articles tend to be of high quality. In Table 2, we show the novel topics found for a Time magazine article about a currency crisis Europe is facing at the time of writing [4]. O ur top suggestion, e.g., is EUROPEAN CENTRAL BANK , which is doubt-less relevant, given that it is the institution in control of Europe X  X  currency. The reason why newspaper articles are well suited as in-put documents is their factual style and high content in name d enti-ties, which makes them similar to Wikipedia articles. This h as two consequences: First, Milne and Witten X  X  method (cf. Sectio n 4.1), since it is trained on Wikipedia, works as well on newswire st o-ries as on Wikipedia articles [15], so the topic vector will b e rich in meaningful keyphrases. Second, the generalization comp onent can then augment this topic vector with high-quality sugges tions because the input document lives in a part of document space t hat is densely populated with Wikipedia articles from the backg round corpus.

Frequently asked questions. Many websites contain sections with frequently asked questions. To identify the issues to b e dis-cussed there can be difficult, since recall and precision sho uld be balanced: an FAQ section should answer all relevant questio ns, while it should also be short enough to not overwhelm readers . Au-tomated topic suggestion can be helpful in this scenario, by offer-ing the FAQ editor a ranked list in which he can find relevant to pics through manual filtering. As a specific example, Table 2 shows the suggestions for the W3C Semantic Web FAQ [8]. While probably not all suggested topics should be mentioned in the FAQ secti on (in order to keep it concise), human editors might find some  X  X  ood for thought X ; e.g., it might be useful to mention whether dif ferent OPERATING SYSTEMS offer different means to interact with the Semantic Web, or what impact the Semantic Web could have on
Philosophical essays. As mentioned above, an important crite-rion influencing the quality of topic suggestions is whether the char-acter of the input document resembles that of Wikipedia arti cles. For many essays, especially those containing numerous tech nical terms, this is the case. Table 2 provides the example of Alan T ur-ing X  X  seminal 1950 paper Computing Machinery and Intelligence [17], in which he introduces what was to become later known as the Turing test. We conjecture that Turing would have apprec i-ated our suggestions to contrast METAPHYSICS with the SCIEN TIFIC METHOD in his musings about whether machines can think, and to complement his mention of Charles Babbage X  X  inventio n of the digital computer with GOTTFRIED LEIBNIZ  X  X  discovery of the binary system. Of course, this should be taken with a grain of salt, since our Wikipedia background corpus contains many concep ts that exist only because of Turing X  X  influence and as such could not have been suggested 60 years ago, such as COMPUTER SCIENCE and ARTIFICIAL INTELLIGENCE . However, this is not the case for the other suggestions shown, which are due to the large numbe r of scientific and philosophical references contained in Tur ing X  X  pa-per and which could well have been made in 1950, had Wikipedia existed then.

Fictional texts. To characterize our method fully, it is important to also identify scenarios in which it fails. We found that th is is frequently the case with fictional texts. More often than not , their purpose is to tell a story rather than to explain concepts, wh ich sets them apart from Wikipedia articles and the aforementioned g enres, on which our algorithm tends to work well. As an example, con-sider Alice X  X  Adventures in Wonderland [5], for which we list the top 10 suggestions in Table 2. Keyphrase assignment results in a topic vector containing only seven non-zero entries (as opp osed to 38 for the Time article and 50 for Turing X  X  essay, in spite of these documents being significantly shorter), which do not summar ize TEA , RABBIT , PIG , IF I FELL (a Beatles song). The fact that most of our suggestions refer to British history is due to WILLIAM THE CONQUEROR , while the suggestion GESTATION (the carrying of babies in the female uterus) is caused by RABBIT and PIG .
In summary, our topic suggestion method works better the mor e the  X  X haracter X  of the input document resembles that of Wiki pe-dia articles. The most striking characteristic of Wikipedi a articles, Table 3: The top 20 suggestions for a U.S. Congress speech dur -ing the debate on the Lawsuit Abuse Reduction Act of 2005 [1]. Left: Using Wikipedia as background corpus. Right: Using the Congressional Record as background corpus. in turn, is their explanatory nature, which implies numerou s refer-ences to other concepts and named entities. Keyphrase assig nment works best in this setting, since the algorithm we use is trai ned on Wikipedia articles. The same holds for the generalization c ompo-nent if eigenarticles are computed using a background corpu s of Wikipedia articles.
In the previous section, we argued that our PCA-based gener-alization component works best for documents that resemble Wi-kipedia articles. It is important to note that this is only th e case if we use Wikipedia, which constitutes a general-purpose ba ck-ground corpus, for computing eigenarticles. However, as st ated in Section 4.2.1, any sufficiently large document collectio n can be employed for that purpose. If we use a domain-specific corpus , the eigenarticles found by PCA will be fine-tuned to the respe c-tive type of input documents, which will result in more meani ngful topic suggestions than if we were to use Wikipedia as a generi c background corpus. The goal of the current section is to illu strate this effect.
 The background corpus we consider now is based on the U.S. Congressional Record and consists of all debates from the Ho use of Representatives of 2005, compiled by Thomas et al. [16, 9]. We compute eigenarticles based on the 2,046 speeches containe d in Thomas et al.  X  X  test and training sets, where a speech is defined as the concatenation of all utterances a single speaker made in a single debate. We refer the reader to the original paper [16] for details concerning this corpus.

Table 3 lists the top suggestions for a speech given by Repres en-tative Mark Udall in October 2005 (available online [1]) dur ing the debate on the Lawsuit Abuse Reduction Act, which was meant to prevent frivolous lawsuits. While the topics our algorithm suggests when using the general-purpose Wikipedia background corpu s are generally from the realm of U.S. politics, with a bias toward s juris-diction, none of them is fully relevant. On the contrary, whe n the domain-specific Congressional Record corpus is used, sugge stions are much more focused, and we find highly pertinent topics suc h as reducing lawsuit abuse), FORUM SHOPPING (a common practice in lawsuit abuse), and PRODUCT LIABILITY (a common pretense for lawsuit abuse). Note that we did not include the debate conta ining the example speech in the eigenarticle computation. Rather , these useful suggestions are the result of the algorithm automati cally gen-eralizing from another debate, on the Class Action Fairness Act, that took place in Congress in February 2005, some months bef ore Udall X  X  speech.

The Congressional Record is better suited as a background co r-pus because the eigenarticles we compute from it are fine-tun ed to the domain of the input document. To illustrate this, let us t ake a look at the eigenspace. Table 4 visualizes the mean topic vec tor and the first four eigenarticles of the Wikipedia corpus, while T able 5 does the same for the Congressional Record corpus. Each vect or has as many entries as there are candidate topics, i.e., N = 468,510 in our case (cf. Section 4.2.3), but for clarity X  X  sake we sho w only the 20 with the highest values. The mean topic vectors (at the far left of each table) give an impression of the most common them es across the entirety of the respective corpus: in Wikipedia t hese are mainly geographical regions and the two world wars, whereas in the Congressional Record the most common topics are the main U.S. political institutions and the more ubiquitous issues such as
The eigenarticles indicate directions in document space al ong which there is much deviation from the mean topic vector of th e corpus. For instance, in the first eigenarticle of Wikipedia , dates prevail (most of them are not shown in Table 4), which is due to the fact that each year has a Wikipedia article listing importan t events and containing links to their dates, such that on the one hand , there are many articles containing many dates, but on the other han d, many other articles do not contain any dates at all, resultin g in high variance with respect to the date content of articles. The su bse-quent eigenarticles are more interesting: the strongest en tries of the second eigenarticle are from the realms of chemistry and ato mic physics, while in the third, it is British and in the fourth, e conomic and political topics that prevail. By finding the major varia tions in the corpus, PCA effectively identifies its dominant semanti c clus-ters.

The same effect can be observed in the Congressional Record corpus. Here, the first eigenarticle summarizes the stem cel l con-troversy, the second and fourth are mostly about terrorism a nd anti-terror legislation, while in the third, budgetary topics we igh the heaviest.

Not only do the eigenarticles correspond to intuitive seman tic classes, they also differ considerably between the corpora , which is reasonable and to be expected: on the one hand, articles abou t ter-rorism constitute only a small fraction of Wikipedia, on the other hand, debates about atomic physics are rare in Congress. Usi ng the Wikipedia corpus amounts to injecting Wikipedia topics into the input document. While we have shown in Section 6.1 that th is works in many cases, it might not always be appropriate. By us -ing a domain-specific corpus, eigenarticles will be fine-tun ed to the idiosyncrasies of the input genre, which in turn results in m ore pre-cise topic suggestions. Therefore, the Congressional Reco rd cor-pus would be most appropriate for a Congressman who wants to b e sure to cover all relevant topics previous speakers have men tioned in similar contexts.

It should, however, be noted that in certain situations the l ess fo-cused suggestions obtained when using a general-purpose co rpus such as Wikipedia might in fact be desirable, e.g., if our Con gress-man intends to give his speech a fresh twist by introducing a t opic that is related to the context but which previous speakers ha ve not mentioned yet.
In this paper we propose an algorithm for suggesting novel to pics to human authors. Given a plain-text document, our method le ver-ages principal component analysis in order to find relevant n ew top-ics by generalizing from a large background corpus.

While we demonstrate the quality of topic suggestions for Wi ki-pedia articles quantitatively, such an evaluation is inher ently diffi-cult for general input documents, due to the highly subjecti ve na-ture of the task. Using examples from different genres of inp ut doc-uments, we therefore illustrate in a more qualitative way th at many user groups beyond Wikipedia editors could profit from our sy s-tem. We observe that our method works better on documents wit h factual contents than on fictional texts. However, a general ization-based algorithm would not be an appropriate tool for inspiri ng au-thors of fictional texts to begin with, since their objective is typ-ically to surprise readers with entirely novel stories, not to write texts that are coherent with a background corpus.

In a typical use-case scenario, the human author would inspe ct the ranked output list of missing topics and decide which of t hese are actually worthwhile incorporating into the document. I n prin-ciple, the algorithm can then be run again, on the modified inp ut document. The effects of such a feedback loop remain to be inv es-tigated as part of our future work.

Beyond topic suggestion, eigenarticles have so far been use d for finding missing Wikipedia links [22] and computing semantic re-latedness [21]. In this paper, we demonstrate that the eigen arti-cles also cluster the topics of the background corpus semant ically, thereby summarizing its most important contents. An intere sting avenue for future research could therefore utilize eigenar ticles for yet another purpose, as a tool for the exploratory analysis o f text corpora. We acknowledge financial support by the Natural Sciences and Engineering Research Council of Canada (NSERC). We also tha nk David Milne for making WikipediaMiner public and Cosmin P  X  adu-raru for many fruitful discussions about this research. [1] Congressional Record , 151(1):H9318, 2005. [2] Amazon. Mechanical Turk. Website, 2009. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet [4] W. Boston. Germany tries to save the euro X  X ll by itself. [5] L. Carroll. Alice X  X  Adventures in Wonderland . Project [6] S. Fissaha Adafre and M. de Rijke. Discovering missing [7] B. Fortuna, D. Mladeni  X  c, and M. Grobelnik. Semi-automatic [8] I. Herman. W3C Semantic Web FAQ. Website, 2009. [9] L. Lee. Convote dataset v1.1. Website, 2008. [10] A. Maguitman, D. Leake, and T. Reichherzer. Suggesting [11] C. D. Manning, P. Raghavan, and H. Sch X tze. Introduction to [12] R. Mihalcea and A. Csomai. Wikify! Linking documents to [13] D. Milne. WikipediaMiner toolkit. Website, 2009. [14] D. Milne and I. H. Witten. An effective, low-cost measur e of [15] D. Milne and I. H. Witten. Learning to link with Wikipedi a. [16] M. Thomas, B. Pang, and L. Lee. Get out the vote: [17] A. Turing. Computing machinery and intelligence. Mind , [18] P. D. Turney. Coherent keyphrase extraction via Web min ing. [19] L. von Ahn, B. Maurer, C. McMillen, D. Abraham, and [20] H.-C. Wang, D. Cosley, and S. R. Fussell. Idea Expander: [21] R. West. Extracting semantic information from Wikiped ia [22] R. West, D. Precup, and J. Pineau. Completing Wikipedia  X  X  [23] Wikipedia. 2008/9 Wikipedia Selection for schools. We bsite, [24] Wikipedia. Data dump of March 6, 2009. Website, 2009. [25] Wikipedia. Wikipedia:Linking. Website, 2010.
