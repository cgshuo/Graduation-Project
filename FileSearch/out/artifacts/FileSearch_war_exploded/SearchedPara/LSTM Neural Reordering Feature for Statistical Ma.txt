 In statistical machine translation, the language model, translation model, and reordering model are the three most important components. Among these models, the reordering model plays an important role in phrase-based machine translation (Koehn et al., 2004), and it still remains a major challenge in current study.

In recent years, various phrase reordering meth-ods have been proposed for phrase-based SMT sys-tems, which can be classified into two broad cate-gories: (1) Distance-based RM : Penalize phrase displace-(2) Lexicalized RM : Conditions reordering proba-
Furthermore, some researchers proposed a re-ordering model that conditions both current and previous phrase pairs by utilizing recursive auto-encoders (Li et al., 2014).

In this paper, we propose a novel neural reorder-ing feature by including longer context for pre-dicting orientations. We utilize a long short-term memory recurrent neural network (LSTM-RNN) (Graves, 1997), and directly models word pairs to predict its most probable orientation. Experimen-tal results on NIST OpenMT12 Arabic-English and Chinese-English translation show that our neural re-ordering model achieves significant improvements over various baselines in 1000-best rescoring task. Recently, various neural network models have been applied into machine translation.

Feed-forward neural language model was first proposed by Bengio et al. (2003), which was a breakthrough in language modeling. Mikolov et al. (2011) proposed to use recurrent neural net-work in language modeling, which can include much longer context history for predicting next word. Experimental results show that RNN-based language model significantly outperform standard feed-forward language model.
Devlin et al. (2014) proposed a neural network joint model (NNJM) by conditioning both source and target language context for target word predict-ing. Though the network architecture is a simple feed-forward neural network, the results have shown significant improvements over state-of-the-art base-lines.

Sundermeyer et al. (2014) also put forward a neural translation model, by utilizing LSTM-based RNN and Bidirectional RNN. In bidirectional RNNs, the target word is conditioned on not only the history but also future source context, which forms a full source sentence for predicting target words.
Li et al. (2013) proposed to use a recursive auto-encoder (RAE) to map each phrase pairs into contin-uous vectors, and handle reordering problems with a classifier. Also, they suggested that by both in-cluding current and previous phrase pairs to deter-mine the phrase orientations could achieve further improvements in accuracy (Li et al., 2014).

By far, we have noticed that this is the first time to use LSTM-RNN in reordering model. We could in-clude much longer context information to determine phrase orientations using RNN architecture. Fur-thermore, by utilizing the LSTM layer, the network is able to capture much longer range dependencies than standard RNNs.

Because we need to record fixed length of history information in SMT decoding step, we only utilize our LSTM-RNN reordering model as a feature in 1000-best rescoring step. As word alignments are known after generating n-best list, it is possible to use LSTM-RNN reordering model to score each hy-pothesis. In traditional statistical machine translation, lexical-ized reordering models (Koehn et al., 2007) have been widely used. It considers alignments of current and previous phrase pairs to determine the orienta-tion.

Formally, when given source language sentence f = { f 1 , ..., f n } , target language sentence e = { e 1 , ..., e n } , and phrase alignment a = { a 1 , ..., a n the lexicalized reordering model can be illustrated in Equation 1, which only conditions on a i  X  1 and a , i.e. previous and current alignment.
In Equation 1, the o i represents the set of phrase orientations. For example, in the most commonly used MSD-based orientation type, o i takes three val-ues: M stands for monotone , S for swap , and D for discontinuous . The definition of MSD-based orien-tation is shown in Equation 2.
For other orientation types, such as LR and MSLR are also widely used, whose definition can be found
Recent studies on reordering model suggest that by also conditioning previous phrase pairs can im-prove context sensitivity and reduce reordering am-biguity. In order to include more context information for de-termining reordering, we propose to use a recurrent neural network, which has been shown to perform considerably better than standard feed-forward ar-chitectures in sequence prediction (Mikolov et al., 2011). However, RNN with conventional back-propagation training suffers from gradient vanishing issues (Bengio et al., 1994) .

Later, the long short-term memory was proposed for solving gradient vanishing problem, and it could catch longer context than standard RNNs with sig-moid activation functions. In this paper, we adopt LSTM architecture for training neural reordering model. 4.1 Training Data Processing For reducing model complexity and easy implemen-tation, our neural reordering model is purely lexical-ized and trained on word-level.

We will take LR orientation for explanations, while other orientation types (MSD, MSLR) can be induced similarly. Given a sentence pair and its alignment information, we can induce the word-based reordering information by following steps. Note that, we always evaluate the model in the or-der of target sentence. (1) If current target word is one-to-one alignment, (2) If current source/target word is one-to-many (3) If current source/target word is not aligned to
Figure 1 shows an example of data processing. 4.2 LSTM Network Architecture After processing the training data, we can directly utilize the word pairs and its orientation to train a neural reordering model.

Given a word pair and its orientation, a neural re-ordering model can be illustrated by Equation 3.
Where e i 1 = { e 1 , ..., e i } , f a i 1 = { f 1 , ..., f clusion of history word pairs is done with recurrent neural network, which is known for its capability of learning history information.

The architecture of LSTM-RNN reordering model is depicted in Figure 2, and corresponding equations are shown in Equation 4 to 6. p ( o i | e i 1 , f a i 1 , a i  X  1 , a i ) = sof tmax ( W
The input layer consists both source and target language word, which is in one-hot representation. Then we perform a linear transformation of input layer to a projection layer, which is also called em-bedding layer. We adopt extended-LSTM as our hid-den layer implementation, which consists of three gating units, i.e. input, forget and output gates. We omit rather extensive LSTM equations here, which can be found in (Graves and Schmidhuber, 2005). The output layer is composed by orientation types. For example, in LR condition, the output layer con-tains two units:  X  lef t  X  and  X  right  X  orientation. Fi-nally, we apply softmax function to obtain normal-ized probabilities of each orientation.
 5.1 Setups We mainly tested our approach on Arabic-English and Chinese-English translation. The training corpus contains 7M words for Arabic, and 4M words for Chinese, which is selected from NIST System Dev Test1 Test2 Ar-En Zh-En OpenMT12 parallel dataset. We use the SAMA to-segmenter for Chinese words. The English part of parallel data is tokenized and lowercased. All de-velopment and test sets have 4 references for each segment. The statistics of development and test sets are shown in Table 1.

The baseline systems are built with the open-source phrase-based SMT toolkit Moses (Koehn et al., 2007). Word alignment and phrase extrac-tion are done by GIZA++ (Och and Ney, 2000) with L0-normalization (Vaswani et al., 2012), and grow-diag-final refinement rule (Koehn et al., 2004). Monolingual part of training data is used to train a 5-gram language model using SRILM (Stolcke, 2002). Parameter tuning is done by K-best MIRA (Cherry and Foster, 2012). For guarantee of re-sult stability, we tune every system 5 times inde-pendently, and take the average BLEU score (Clark et al., 2011). The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance test is also car-ried out with paired bootstrap resampling method with p &lt; 0 . 001 intervals (Koehn, 2004). Our mod-els are evaluated in a 1000-best rescoring step, and all features in 1000-best list as well as LSTM-RNN reordering feature are retuned via K-best MIRA al-gorithm.

For neural network training, we use all parallel text in the baseline training. As a trade-off be-tween computational cost and performance, the pro-jection layer and hidden layer are set to 100, which is enough for our task (We have not seen signifi-cant gains when increasing dimensions greater than 100). We use an initial learning rate of 0.01 with standard SGD optimization without momentum. We trained model for a total of 10 epochs with cross-entropy criterion. Input and output vocabulary are set to 100K and 50K respectively, and all out-of-vocabulary words are mapped to a  X  unk  X  token. 5.2 Results on Different Orientation Types At first, we test our neural reordering model (NRM) on the baseline that contains word-based reordering model with LR orientation. The results are shown in Table 2 and 3.

As we can see that, among various orienta-tion types (LR, MSD, MSLR), our model could give consistent improvements over baseline system. The overall BLEU improvements range from 0.42 to 0.79 for Arabic-English, and 0.31 to 0.72 for Chinese-English systems. All neural results are sig-nificantly better than baselines ( p &lt; 0 . 001 level).
In the meantime, we also find that  X  X eft-Right X  based orientation methods, such as LR and MSLR, consistently outperform MSD-based orientations. The may caused by non-separability problem, which means that MSD-based methods are vulnerable to the change of context, and weak in resolving re-ordering ambiguities. Similar conclusion can be found in Li et al. (2014) .
 5.3 Results on Different Reordering Baselines We also test our approach on various baselines, which either contains word-based, phrase-based, or hierarchical phrase-based reordering model. We only show the results of MSLR orientation, which is relatively superior than others according to the re-sults in Section 5.2.
In Table 4 and 5, we can see that though we add a strong hierarchical phrase-based reordering model in the baseline, our model can still bring a maximum gain of 0.59 BLEU score, which suggest that our model is applicable and robust in various circum-stances. However, we have noticed that the gains in Arabic-English system is relatively greater than that in Chinese-English system. This is probably be-cause hierarchical reordering features tend to work better for Chinese words, and thus our model will bring little remedy to its baseline. We present a novel work that build a reordering model using LSTM-RNN, which is much sensitive to the change of context and introduce rich con-text information for reordering prediction. Further-more, the proposed model is purely lexicalized and straightforward, which is easy to realize. Experi-mental results on 1000-best rescoring show that our neural reordering feature is robust, and could give consistent improvements over various baseline sys-tems.

In future, we are planning to extend our word-based LSTM reordering model to phrase-based re-ordering model, in order to dissolve much more am-biguities and improve reordering accuracy. Further-more, we are also going to integrate our neural re-ordering model into neural machine translation sys-tems.
 We sincerely thank the anonymous reviewers for their thoughtful comments on our work.

