 How to evaluate the performance of search engines promptly, accurately and objec-gine system engineers. The evaluation of information retrieval (IR) systems, which is the procedure of assessing how well a system satisfies user information requirements Saracevic [15],  X  X valuation became central to R&amp;D in IR to such an extent that new designs and proposals and their evaluation became one. X  
Kent et al. [10] was the first to propose the criterion of relevance and the measures of precision and relevance (later renamed recall) for evaluating IR systems. While most current IR evaluation researches, including the famous workshop TREC (Text Retrieval Conference), are based on the Cranfield methodology [4]. A Cranfield-like approach is based on a query set and corresponding answers (called qrels). Queries in the query set are processed by an IR system. Then results are compared with qrels the value of the relevance metrics. 
The annotation of qrels for a query set is usually the most difficult part in IR sys-tem evaluations. Manual assessment is such a time-consuming task that Voorhees [19] estimated that about nine person-months is required to judge one topic for a col-lection of 8 million documents. For the task of search engine performance evaluation, the content of Web is significantly larger than TREC-based corpuses, and real time character is of vital importance for engin eering concerns. Therefore, the evaluation method should not be one in which lots of human efforts are involved. 
Recently, the wisdom of crowds is paid much attention to in the area of Web re-search (eg. [6][9]). This paper focuses on inferring implicit preference of Web search users by observing their querying and clicking behavior and the wisdom of crowds is used to evaluate the performance of Web search engines reliably and timely. 
The contributions of the paper are: 1. A framework of automatic search engine evaluation is proposed, which is based on click-through data analysis and estimate the performance reliably and timely. 2. An automatic qrel annotation algorithm is designed to annotate Web search que-ries with user behavior analysis. 3. A user behavior model (called Multiple Click-through Rate model, MCTR ) which combines user behavior information from multiple search engines is proposed to reduce potential bias in information collected from a single search engine. Several attempts have been made towards automatic IR system evaluation to tackle difficulties related to manual assessment. Some of methods avoid manual assessments by adopting pseudo-relevance feedback information. Soboroff et al. [17] randomly selected a subset of documents from the results of each topic as relevant documents. Nuray and Can [12] made similar attempts by selecting documents with the highest RSV values from the result pool as relevant documents. While these methods bypass manual assessment, they have to compromise the loss of the accuracy and reliability because pseudo-relevance feedback cannot provide reliable relevance judgment. 
Oard and Kim [13] were among the first to model Web users X  information acquisi-tion process by behavior analysis. They presented a framework for characterizing observable user behaviors in order to understand the underlying behavior goals and the capability scope provided by information systems. Later, through the eye-tracking queries and results. In 2006, Agichtein et al. [1] proposed an idea of aggregating in-formation from many unreliable user search session traces instead of treating each user as a reliable  X  X xpert X  and pointed out that user behaviors were only probabilisti-cally related to relevance preferences. Dou et al. [9] studied the problem of using aggregate click-through logs, and found that the aggregation of a large number of user clicks provided a valuable indicator of relevance preference. 
Different from the previous work, our approach assesses the relevant documents by settings. Hence, rather than manually judged by few experts, our assessment is deter-mined by thousands even millions concerned users, robust to the noise inherently in individual interactions. A Cranfield-like evaluation framework is constructed to evaluate the performance of search engines automatically with the wisdom of crowds . As outlined in the introduction, our evaluation method measures an engine's quality by examining users X  querying and clicking behavior extracted from real world search logs. The general process is similar to the Cranfield-like approach in the adoption of a query topic set and corresponding answers (qrels). But both the topic set and the an-swer set are generated automatically acco rding to user clic k-through logs. 
An advantage of adopting the Cranfield-like framework is that there is no need to design new evaluation metrics. Existing metrics such as MAP, MRR, P@N and Bpref work is based on them. Once the topic and qrel sets are generated automatically, Cranfield approach is a better choice than adopting new architectures and metrics. 
The flow chart of our automatic evaluation framework is shown in Fig. 1. First, click-through logs are pre-processed and se veral user-behavior features are extracted. These features are then adopted in query selection, classification and annotation steps. Queries are sampled and grouped into two categories, navigational type and informa-formance is evaluated using different metrics. After annotating relevant documents respectively, search engines X  result of these queries are crawled and extracted. Finally, traditional Web IR evaluation metrics are used to assess the performance of search engines by comparing their result lists and the automatic annotated answers. How to construct a suitable query set is challenging for Cranfield-like evaluation methods. For traditional methods such as those adopted in TREC, query topic sets are developed by assessors or participants. Sometimes, queries are selected from search engine query logs by human efforts but the query set usually contains no more than several hundred query topics. 
For the evaluation purpose, we construct a query set representing the information needs of Web users. Since our qrel annotate method needs to aggregate a reasonable number of user clicks to provide a valuable indicator of relevance and we cannot extract the reliable relevance preference from an individual click, we select queries with a number of user requests to construct the query set from click-through data. 
After the query set is constructed, the information need of each query should be annotated automatically. According to Broder [2] and Rose et al. [14], there are three major types of Web search queries: navigational, informational and transactional. For evaluation purposes, these three query types should be treated respectively. MRR (Mean Reciprocal Rank) is adopted in the evaluation of navigational queries. For both informational and transactional queries, ther e is usually more than one correct answer. Mean average precision (MAP) and precision at top N documents (P@N) are there-fore used in evaluation of these two types of queries. Because transactional queries use the same metrics with informational ones, we use  X  X nformational query X  instead of  X  X nformational and transactional query X  in the latter part of this paper. 
Because different metrics should be adopted according to queries X  types, we have we adopt the classification method proposed in [11]. In that paper, two new user be-havior features, nCS and nRS, were proposed based on user click-through logs, and the decision tree learning algorithm was adopted to identify the types of queries automatically. The most important step in our evaluation method is automatic qrel annotation. Web search user interaction with search engines is noisy and we cannot treat each user as an  X  X xpert X . Therefore, not all clicks are related to relevance preferences which should be derived from massive user click logs using statistics strategy. 5.1 The Feature of Click Distribution Click-through logs provide detailed and valuable information about users X  interaction with search engines. Based on previous studies [8][9], we have the intuitive idea that one result with a large number of clicks might be more relevant to the query than the one with less clicks probabilistically. 
We defined the number of clicks on result d for query q , N(d|q) , and the probability that d is relevant to q , P(d is relevant to q) , then we have the following assumption: for query q.

Assuming that we have a search query q , assumption 1 says that a single click on a document d does not mean d is relevant to q absolutely. However, if d 1 is clicked M times and d 2 is clicked N (M&gt;N) times by users who propose q , d 1 may be more rele-vant to q than d 2 probabilistically. Due to different queries with different frequency of user visits, we normalize N(d|q) as CTR(d|q) (Click-through Rate) for query q : According to Joachim X  X  work [8], the assumption 1 does not hold well, and the rank bias shows that the results at top positions have more chance to be viewed and clicked. Fig. 2 shows the relevance precision of query-doc pairs with different CTR values. From the figure, we can know that the relevance precision of pairs with large CTR values higher than the ones with lower CTR values, and it indicates that the pairs with large CTR values is more likely to be relevant in a probabilistic notion. 5.2 Multiple Click-Through Rate ( MCTR ) Model Due to differences in crawling, indexing and ranking strategies, search engines may summarizations or snippets of results is a factor to affect user X  X  click behavior. Hence, users may click different result pages using different search engines. 
For example, Fig. 3 shows the click distributions of one sample query in four dif-ferent Chinese search engines (referred to as SE1, SE2, SE3 and SE4). We collected ten of the most-frequently clicked result from each of these four engines and con-structed a result pool which contained 27 unique pages. The  X  X lick Distribution X  axis is the CTR value of result pages and the curves show the different click distributions. Meanwhile, each point on the  X  X licked Pages X  axis represents one click result. Ac-cording to Fig. 3, we note the discrepancy of user clicks in different engines and find SE1 focus on the 1st clicked page, while SE2 focuses on the No.3 and No.10. 
Since click distributions in different search engines are discrepant and biased to the able to be used directly. It is not reasonable to use click-through data from one single search engine for assessing another search engine. There may be some strategies to decrease or prevent the bias and shortcomings, and comprehensive utilization of user behavior from multiple search engines is one of these accessible ways. We give the second assumption here: Assumption 2. The click-through data from multiple search engines are more informational.
 Now, we see that it is important to extract and combine user behavior information query. However, there remains a problem, how to exploit user interaction information from multiple engines. Different search engines have different market shares and user visits. If click-through logs are aggregated simply, there would be biased to the search engines with relatively larger user visits. Therefore, we propose a model of integrat-ing user behavior information based on click distributions of individual search en-gines to generate multiple click distribution. To our best knowledge, there are no studies about click-though data of multiple search engines. 
In section 5.1, we mentioned the CTR feature normalized in probability, which is domly on a result document. Supposing that there is a virtual meta-search engine to characterize the click-through informatio n from multiple search engines. This vir-tual engine indexes more Web data than any subsistent search engines, has lower bias to single search engine, and gathers more accurate user behavior information. click-through rate with integrating click distributions from single search engines. Since MCTR (d|q) is a probability distribution function, it follows some probability principles and is rewritten as Pi(d|q) , which means the probability that a click locates randomly on a result d giving query q . According to the full probability distribution, the equation is constructed as follows: weight of search engine se j for query q and we use Bayes theorem and maximum likelihood estimation method to rewrite it as: where, Pi(q) is the proportion of queries submitted by users in whole Web search, engine se j . Combining expressions (2) and (3), we obtain: After normalized by all Pi(d|q) for all documents, we have the following formula: This means that MCTR of each result d for query q is able to be achieved from CTR of single search engines with some additional user session information. 5.3 Automatic Qrel A nnotation for Query Set According to [16], users only view or click a few top-ranked pages of the result list. sumption, we assume that relevant Web pages should appear in search engines X  result annotated by our method are really qrels. 
For informational queries, there is a set of frequently-clicked results, due to the fact that informational queries don X  X  have a fixed search target. A search engine may re-turn several related results, and users click some of them according to their informa-tion needs. In Section 5.1 and 5.2, the CTR and MCTR features are described to reveal the relative relevance of clicks statistically based on two assumptions. According to the relative relevance, queries are able to be annotated and the most relative relevant pages are regarded as the results of corresponding queries. We give the third assump-tion here: Assumption 3. Search target pages for a corresponding query are several results with the largest CTR or MCTR values.
 According to this assumption, the annotation process is described as follows: 
In the annotation process, there are two parameters: T and N . T is used as a lower annotating accuracy. N is used as an upper limit for the qrels numbers of queries. The choice of T and N selects the tradeoff between recall and relevant reliability. 
For navigational queries, there is a fixed search target and the purpose of the query is to reach a particular Web page. For most navigational request, the target is unique. Therefore, the result R 1 , which is the most frequently clicked in search engines X  result set, is likely to be the correct answer probabilistically. Thus, navigational queries are considered as special cases of informationa l queries and we use the process described in Algorithm 1 to annotate navigational queries by setting N to 1. 
When the search engines failed to return correct answers in result lists, it is almost impossible for users to click those answers and as such the annotation process return  X  X  cannot be annotated X . 6.1 Experiment Settings With the help of a popular commercial Chinese search engine, click-through logs of four most frequently-used Chinese search engines were collected from October 1st to November 20th, 2008. These click-through logs (recording altogether 53,367,427 querying and clicking events, about 33.0M, 9.4M, 9.0M and 1.9M respectively) were applied in our evaluation experiment. The four search engines are referred to as SE1, SE2, SE3 and SE4. 
In order to verify the reliability of the proposed qrel annotation method and user behavior model, we first performed experiment to examine the correctness of the automatically-annotated qrels. After that, effectiveness of our evaluation method was examined by comparison with manual-based evaluation results. 
As for the manual annotation process, we had three product engineers to work as assessors. All of them are familiar with search engine products and is able to estimate user information needs accordi ng to context of queries. The correctness of their anno-tation was also examined by co-checking each other X  X  judgment results and the final labeling results were determined by the majority opinion of assessors. 6.2 Qrel Annotation Experiments With the query set construction strategy proposed in Section 4, we randomly selected 7000 queries from query logs to evaluate search engines X  performance. After auto-matic classification and annotation, our methods successfully annotated 5996 queries with qrels (the others were regarded as  X  X annot be annotated X  by algorithm 1). Among these queries, 1225 are navigational and the rest are informational. Annotation of Navigational Queries One qrel was automatically annotated to each of these navigational queries using the algorithm 1 with setting N to 1, and the results were compared with manual annota-tion. Table 1 shows the consistent and inconsistent results number. 
The inconsistent results are mainly caused by two reasons, and most of these in-consistent queries meet users X  inform ation need in real world scenario: they want to visit the homepage. For example, the query  X 163 X  is annotated with http://mail.163.com/ instead of http://www.163.com/ with our method. The reason is that http://mail.163.com/ is the most famous free e-mail service provider in China. People are more likely to visit this site to get free mail service rather than navigating the main Web site. Therefore, most users who query  X 163 X  visit the mail sub-site and the wisdom of crowds annotates the frequently visited URL as the qrel. For this kind of queries, we believe that the automatically-annotated answer is more suitable than the manual ones because it meets users X  information need more closely. [2] There are also several cases in which the automatically-annotated qrels are not so suitable. Some of these wrongly-annotated cases are due to query-type misclassifi-cation. These queries are then reclassified as informational ones. Other problems are caused by mirror sites, which mean our algorithm annotates a certain URL but the assessors considers a different URL with the same content. Annotation of Informational Queries In our experiments, altogether 4771 informational queries were sent to be annotated and assessors were asked to check the correctness of annotation results. 1000 (about 21%) queries were selected by random sampling and labeled by assessors. 
Fig. 4 compared the precision of qrels annotated by MCTR and CTR of four search engines. We find that the proposed MCTR strategy from multiple search engines has better performance than the CTR strategies from individual search engines. 
In order to verify the reliability of our automatic method, we compared our method X  X  annotation results with those of human assessors. The annotated results of our method and all three assessors were checked with a cross validation method. In Table 2, the result annotated by a certain assessor was used to evaluate another asses-sor X  X  or our automatic method X  X  performance. The results in Table 2 show that human assessors do not agree with each other totally. The results of our automatic method are similar to that of assessors. With assessor 2 X  X  annotation results as the correct answer, the precision of assessor 3 X  X  annotation is about 92.2%, while our method gains a precision of 88.6%, which means the automatic annotated results are close to the one of assessor 3 X  X  using assessor 2 X  X  results as the correct answer. 6.3 Performance Evaluation Results With the query set constructed according to th e descriptions in Section 4 and the qrel set annotated in Section 5, search engine performance was evaluated with traditional IR metrics. MRR (Mean Reciprocal Rank) and P@10 (precision of the top 10 docu-ments) were used for evaluating navigational and informational type queries, sepa-rately. Besides that, MAP (Mean Average Precision) was adopted in both query types X  evaluation processes. 
In Fig. 5(a), MAP values equal to MRR because queries are all navigational and annotated with one correct answer only. The differences in MAP between Fig. 5(a) and (b) reveal that search engines have better performance while processing naviga-tional type queries than informational ones. This conclusion accords with previous studies in [5][7]. 6.4 The SearchE System A demo called SearchE System (http://searchE.thuir.cn/) is constructed to evalu-ate online commercial search engines based on the framework and MCTR model proposed in this paper. It collects click-through information of multiple search engines, generates query set, annotates them with MCTR , collects results from online search engines, and show evaluation results on the Web. The performances of these search engines and top queries with corresponding results are provided every day. In this paper, we introduced an automatic method to evaluate the Web search engines' performance with the wisdom of crowds . It involves a statistically significant number of users for unbiased assessment, is robust to the noise in individual interactions and evaluates the performance reliably and timely. 
First, we proposed a framework of automatic performance evaluation for Web search engines. It is a Cranfield-like approach based on users X  click-through behavior information. Second, the method of large scale query set construction was introduced and queries were classified into informational and navigational categories automati-cally. Third, qrel annotation algorithm was described to annotate queries. The MCTR strategy combined user behavior information from multiple search engines. 
Experimental results show that most of the qrels are annotated correctly and auto-matic evaluation results are highly correlated with manual-based evaluation results. By this approach, different with traditional, automatic search engine evaluation based on large scale queries for real world is easy to conduct. 
Future study will focus on the following aspects: How much click-through data is needed for a reliable and efficiency evaluation method? How to improve the evalua-tion reliability using more user behavior except click-through data? 
