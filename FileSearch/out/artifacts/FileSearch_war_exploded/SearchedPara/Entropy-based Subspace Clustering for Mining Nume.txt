
Modern technology provides efficient and low-cost meth-ods for data collection. However, raw data is rarely of direct benefit for higher level management, decision making or more intelligent analysis. Data mining aims at the construction of semi-automatic tools for the anal-ysis of large data sets. The mining of binary associa-but databases in the real world usually have numer-ical attributes in addition to binary attributes. Un-fortunately, mining numerical data is a more difficult problem and relatively few works have been done on this topic. Some previous works include [15, 13, 141. 
Here we attempt to mine numerical data using cluster-ing techniques. We consider a database consisting of numerical attributes. We can view each transaction of this database as a multi-dimensional vector. Clustering is to discover homogeneous groups of objects based on the values of these vectors. Hence, we can study the behaviour of the objects by looking at the shapes and locations of clusters. See Figure 1 for an example. 
We learn from statistics that it is possible to find correlation among different factors from raw data, but we cannot find the direction of implication and it can be risky to conclude any causal relationship from raw data [16]. Clustering is a method that finds correlations while not infering any causal relationship. 
Not all clustering algorithms are suitable for our problem. They must satisfy some special requirements in order to be useful to us. One important requirement is the ability to discover clusters embedded in subspaces of high dimensional data. Given a space X with dimensions formed from a set of attributes S, a space 
Y with dimensions formed from a subset of S is called a subspace of X. Conversely, X will be called a superspace of Y. For instance, suppose there are three attributes A, B and C. Clusters may exist inside the subspace formed by A and B, while C is independent of A and B. In such case, C is a noise variable. Since high dimensional information is hard to interpret, it is more desirable if the clustering algorithm can present the cluster in the subspace AB rather than in the full space ABC. Real-life databases usually contain many attributes so that either there is no proper cluster in the full space, or knowing the existence of a cluster in the full space is of little use to the user. Therefore, the ability to discover embedded clusters is important. This problem is called subspace clustering in [2]. 
Data mining by definition deals with huge amount space S into non-overlapping rectangular units, which that is covered by the dense units. Subspaces with high coverages are selected and those with low coverages are pruned away. fied, the clusters in each subspace are to be determined. 
Recall that clusters are connected dense units. We can simply use a depth-first search algorithm [3] to find the connected components. The final step is to generate minimal cluster descriptions. The description is given This is equivalent to a union of some hyper-rectangular regions. The regions can be found by a greedy growth method. We start with any dense unit and greedily grow a maximal region in each dimension. The pro-cess is repeated until the union of all regions cover the whole cluster. Then we need to remove the redundant regions. This is achieved by repeatedly removing the smallest redundant region until no maximal region can be removed. There are many factors to be considered for a cluster-ing algorithm in data mining. We mentioned some of these in the introduction: efficiency, shape of clusters, sensitivity to outliers, and the requirements of param-eters. A clustering algorithm will assume a certain set good clustering given a set of data. 
In addition to the clustering problem, we would like to handle the problem of determining subspaces that have  X  X ood clustering X . We therefore need addition criteria for determining which of two clustering for two different sets of data is better. In the following we introduce a number of such criteria. 3.1 Criterion of High Coverage The first criterion that we use for goodness of clustering is the coverage as defined for CLIQUE. This is a reasonable criterion since a subspace with more distinguished clusters will have high coverage, whereas a subspace with close to random data distribution will have low coverage. 3.2 Criterion of high density Other than coverage, we believe that other criteria are also needed. The first criterion that we add is the criterion of high density. 
Suppose we use only the coverage for measurement of goodness. A problem case is illustrated in Figure 2. It shows the probability density function of a random variable X. The value of coverage can be represented by the area of the shade portion since coverage is the fraction of the database that is covered by the dense units. In this example, both cases (a) and (b) have 
Figure 2: Example of two data sets with equal coverage but different densities. The area of the shaded portion is the value of coverage. the same coverage. However, this contradicts with our intuition, because the points in case (b) is more closely packed and more qualified as a cluster. 3.3 Correlation of dimensions The third criterion that we consider is related to the correlation of dimensions. We note that finding subspaces with good clustering may not always be helpful, we also want the dimensions of the subspace to be correlated. The reason is that although a subspace may contain clusters, this may not be interesting to us if the dimensions are independent to each other. For example, Figure 3 shows such a scenario in 2D. In this example, since all the data points projected on X lies on [zl, ~2) and projected on Y lies on [yl, y2), the data objects must be distributed at [xl, ~2) x [yl, y2) in the joint space. If the points are uniformly distributed at the joint space gives us no more knowledge than looking at each of the dimensions independently. 
Hence, we also require the dimensions of the subspace to be correlated. Note that when we say correlated here, we mean the dimensions are not completely independent but it need not exist a very strong correlation. 
Having identified a number of criteria for clustering, we shall find a metric that can measure all the criteria simultaneously. A subspace which has good clustering by the criteria will have high score in this metric. Then we can set a threshold on this measurement and find subspaces which exceed this threshold. The metric that we use is the entropy, which we shall discuss in the next section. We propose to use an entropy-based method. The method is motivated by the fact that a subspace with clusters typically has lower entropy than a subspace without clusters. Entropy is a measure of uncertainty of a random variable. Let X be a discrete random variable, X be the set of possible outcomes of X and p(z) be the probability mass function of the random variable X. The entropy H(X) is defined by the following expression [9]. there are more than one variable, we can calculate the joint entropy to measure their uncertainty. When the probability is uniformly distributed, we are most uncertain about the outcome. Entropy is the highest in this case. On the other hand, when the data points have a highly skewed probability mass function, we know that the variable is likely to fall within a small set of outcomes so the uncertainty and the entropy are low. 4.1 Calculation of Entropy Similar to CLIQUE, we divide each dimension into intervals of equal length A, so the high-dimensional space is partitioned to form a grid. Suppose the data set is scanned once to count the number of points contained be found. Let X be the set of all cells, and d(x) be the density of a cell 2 in terms of the percentage of data contained in x. We define the entropy of the data set to be: 
When the data points are uniformly distributed, we are most uncertain where a particular point would lie on. The entropy is the highest. When the data points are closely packed in a small cluster, we know that a particular point is likely to fall within the small area of the cluster, and so the uncertainty and entropy will be low. Figure 4 shows the result of an experiment studying the relationship between the area of cluster in a two dimensional space [O,l) x [O,l). The smaller the area of the cluster, the more closely packed the points and the lower the entropy. 
The size of interval A must be carefully selected. If the interval size is too small, there will be many cells so that the average number of points in each cell can be too small. On the other hand, if the interval size is too large, we may not able to capture the differences in density in different regions of the space. We suggest the use of at least 35 points in each cell on the average since 35 is often considered as the minimum sample size for large sample procedures [lo]. The size of interval A can be set accordingly. 
In Section 3, we propose the use of three criteria for the goodness of clustering: high coverage X , high density and dimensional correlation. In this section, we discuss how the use of entropy can relate to the criteria we have chosen for the selection of subspaces. First we list the symbols used in the discussion in Table 1. 5.1 Entropy and the coverage criterion 
To investigate the relationship between entropy and the coverage, we consider the following case. By assuming there are k dense units out of n total units, it follows that Theorem 1 v &lt;Oifandonlyifp~...pndc 2 1. Proof 
Theorem 2 Suppose that % 2 0 for i = 1, . . , k and maayl&lt;j&lt;, (pj). Then we have 
Proof 5.2 Entropy and the density criterion to p. The total number of dense units is k and thus the total number of non-dense units is n -k. Then we have Theorem 3 _ q&lt;Oifandonlyifa&gt;p. 
Proof Note that 5.3 Entropy and variable correlation The problem of correlated variables can be easily handled by entropy because the independence and dependence of the variables can be detected using the following relationships in entropy [8]. We shall make use of the above property in the following section. In this section, we introduce the proposed algorithm ENCLUS in more details. There are two variations of ENCLUS, which are discussed at Section 6.2 and 6.3. The overall strategy consists of three main steps: 1. Find out the subspaces with good clustering by an 2. Identify the clusters in the subspace found. 3. Present the result to the users. In Step 2 and Step 3, we can adopt the method in CLIQUE or some of the existing clustering algorithms. We examine Step 1. Previously we use the term good clustering to indicate that a subspace contains a good set of clusters in an intuitive sense. Here we shall give the term a more concrete definition by means of entropy. We need to set a threshold w. A subspace whose entropy is below w is considered to have good clustering. The proposed algorithm uses a bottom-up approach similar to the Apriori algorithm [l] for mining association rule. In Apriori, we start with finding large 1-itemsets. It is used to generate the candidate 2-itemsets, which are checked against the database to determine large 2-itemsets. The process is repeated with increasing itemset sizes until no more large itemset is found. 
Similarly, our bottom-up algorithm starts with find-ing one-dimensional subspaces with good clustering. Then we use them to generate the candidate two-dimensional subspaces and check them against the raw data to determine those that actually have good clus-tering. The process is repeated with increasing dimen-sionalities until no more subspaces with good cluster-ing is found. We note a downward closure property for entropy. This is given by the non-negativity of Shan-non X  X  information measures2 [8]. The correctness of the X  bottom-up approach is based on this property. 
Lemma 1 (Downward closure) If a k-dimensional sub-spacexl,... , X,+ has good clustering, so do all (k -l)-dimensional projections of this space. 
Proof Since the subspace Xi, . , Xk has good cluster-ing, H(Xr, . . .,X,) &lt; w. 
Hence, the (k -1)-dimensional projection X1, . . . , Xk also has good clustering. The above proof can be repeated for other (k -1)-dimensional projections. 0 6.1 Dimensions Correlation 
In Section 3.3 we discuss the criterion of dimensional correlation. In Section 5.3 we examine how entropy can be related to dimensional correlation. Here we show the upward closure property of this criterion. Let 
P(Zl, 22,  X  X , xi) be the joint probability mass function of variables Xi, X2, . . . . Xi. In the following lemma, variables Xi, X2, . . . . X, are considered not correlation 
Lemma 2 (Upward closure) If a set of dimensions S is correlated, so is every superset of S. 
Proof We proof by contradiction. Suppose Xi and X2 are correlated, but X1, X2 and Xs are not. 
Hence Xr and X2 are not correlated, which is a contradiction. 0 
Traditionally, the correlation between two numerical variables can be measured using the correlation coeffi-cient. However, we can also detect correlation by en-tropy. Since we are already using entropy in the algo-rithm, using entropy to detect correlation introduces a negligible computational overhead. A set of variables 
Xl,..., X, are correlated if Equation 1 of Section 5.3 is not satisfied. To express it more precisely, we define the term interest3 as below. interest((X1,. . . , X,))=~H(Xi)-H(xl,...,X,) 1 clustering but not minimally correlated The higher the interest, the stronger the correlation. We consider the variables to be correlated if and only if the interest exceeds a predefined threshold E. The interests of one-dimensional subspaces are always 0. 6.2 Mining Significant Subspaces A pair of downward and upward closure properties is used in [6], which proposes an algorithm for mining correlation rules. It is pointed out that downward closure is a pruning property. If a subspace does not satisfy this property, we can cross out all its superspaces because we know they cannot satisfy this property either. Upward closure, by contrast, is a constructive property. If a subspace satisfies the property, all its superspaces also satisfy this property. However, upward closure property is also useful for pruning. The trick is that we only find minimally correlated subspaces. If we know a subspace is correlated, all its superspaces must not be minimally correlated. Therefore, upward closure becomes a pruning property. 
Now we call the subspaces with good clustering and minimally correlated to be significant subspaces. Due to the upward closure property, the subspaces we are interested in form a border. The border stores all the necessary information. Refer to Figure 5 for an example. In this figure, the subspaces below the dotted lines all have good clustering (downward closed) and the subspaces above the solid lines are all correlated 
Algorithm 1 ENCLUS-SIG (w , E) 1 
Figure 6: Algorithm for mining significant subspaces (upward closed). The border {XiXs, XsXs, X1X4) stores all the significant subspaces, i.e. minimally correlated subspaces with good clustering. 
The details of the algorithm, called ENCLUSSIG, are given in Figure 6. Table 2 lists the notations used. 
The description of the procedures used in the algorithm is given as follows. 
Cal-density(c) Build a grid to count number of points 
Cal-entropy(f,(.)) Calculate the entropy using the candidategen(l\r&amp;) Generate the candidate sub-6.3 Mining Interesting Subspaces 
Since correlation can usually be detected at low dimen-sion, the mining of high dimensional clusters is often avoided. This is good because low dimensional clus-ters are easier to interpret and the time for mining high 
Algorithm 2 ENCLUSINT(w, E X ) 2 Let Cr, be ah one-dimensional subspaces. 3 For each subspace c E ck do 4 fc(.) = Cal-density(c) 5 H(c) = cal-entropy(f,(.)) 6 If H(c) &lt; w then 7 If interest-gain(c) &gt; E X  then 9 else 11 End For 12 Ck+l = candidate-gen(lk U Nlk) 13 If ck+l = 8, go to step 16. 15 Go to step 3. 
Figure 7: Algorithm for mining interesting subspaces dimensional clusters can be saved. However, [6] did not consider that sometimes we are interested in non-minimally correlated subspaces. For instance, A and B are correlated, but we may be interested in the sub-space ABC if ABC are more strongly correlated than A and B alone. To measure the increase in correlation, we define the term interest gain4. The interest gain for subspace Xi, . . . , X, is defined as follows. The interest gain for one dimensional subspace is defined to be 0. The interest gain of a k-dimensional subspace is the interest of the given subspace minus the maximum interest of its (k -1)-dimensional projections. In other words, it is the increase in interest for adding an extra dimension. 
Our new goal becomes mining subspaces whose entropy exceeds w and interest gain exceeds a new threshold et. We call such subspaces to be interesting subspaces. The mining of significant subspace algorithm can be modified slightly to mine interesting subspaces. Figure 7 shows the modified algorithm ENCLUS-INT. Since we relax one of the pruning criteria, we expect more candidates and a longer running time. Figure 9: Interest Threshold vs Running Time (EN-
CLUSSIG) 
To evaluate the performance of the algorithms, we im-plemented our algorithms on Sun Ultra 5/270 work-station using GNU C++ compiler. High dimensional synthetic data were generated which contains clusters embedded in the subspaces. Our data generator allows the user to specify the dimensionality of data, the num-ber of subspaces containing clusters, the dimensionality of clusters, the number of clusters in each subspace and the number of transactions supporting each cluster. Un-less otherwise specified, we use data of 10 dimensions and 300,000 transactions in the experiments. Some five-dimensional clusters are embedded in the subspaces. 
Figure 8 shows the performance of the algorithms under different values of w. We do not have a smooth curve here, because when w increases to a certain value, candidates of a higher dimension are introduced which impose a considerable amount of extra computation. 
From the figure, we can see the running time of the algorithm ENCLUS-SIG ceases to increase when w is high enough, because after that point, the pruning power of entropy is negligible and most pruning is attributed to the upward closure property which is independent of w. As for the algorithm ENCLUS-INT, the running time keeps on increasing with w because only the entropy is utilized for pruning. with k attributes can be seen as a data point in a of clustering in subspaces: coverage, density and ees for helpful suggestions. [I] R. AgrawaI and R. S&amp;ant. Fast algorithms for mining [2] Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunop-[3] A. Aho, J. Hopcroft, and 3. UIIman. The Design and [4] P. S. Bradley, Usama Fayyad, and Cory Reina. Scaling [5] P. S. Bradley, 0. L. Mangasarian, and W. Nick Street. [6] Sergey Brin, Rajeev Motwani, and Craig Silverstein. [7] David K. Y. Chiu and Andrew K. C. Wong. Synthesiz-[8] Thomas M. Cover and Joy A. Thomas. Elements of [9] I. Csiszk and J. Korner. hformation Theory: Coding [lo] Jay L. Devore. Probability and Statistics for Engineer-[ll] Martin Ester, Hans-Peter Kriegel, J&amp;g Sander, Michael [12] Martin Ester, Hans-Peter Kriegel, J&amp;g Sander, and [13] Takeshi Fukuda, Yasuhiki Morimoto, Shinichi MO;-[14] Takeshi Fukuda, Yasuhiko Morimoto, Shin&amp;i Mor-[15] Takeshi Fukuda, Yasuhiko Morimoto, Shim&amp;i Mor-[16] Clark Glymour, David Madigan, Daryl Pregibon, and [17] Sudipto Guha, Rajeev Rastogi, and Kyuseok Shim. [18] John A. Hartigan. Clustering algorithms. Wiley, 1975. [19] Pierre Michaud. Clustering techniques. In Future [20] Raymond T. Ng and Jiawei Han. Efficient and [al] J.R. Quinlan. Induction of decision trees. In Machine [22] J.R. QuinIan. C4.5: Programs for Machine Learning. [23] Erich Schikuta. Grid-clustering: An efficient hierarchi-[24] Xiaowei Xu, Martin Ester, Hans-Peter Kriegel, and [25] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. 
