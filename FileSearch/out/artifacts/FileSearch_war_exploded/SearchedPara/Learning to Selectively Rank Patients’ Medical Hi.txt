 Two main approaches have emerged in the literature for the effective deployment of a search system to rank patients having a medical history relevant to a query. The first ap-proach is to directly rank patients based on the relevance of their medical history, represented as a concatenation of their associated medical records. Instead, the second ap-proach initially retrieves the relevant medical records of pa-tients, and then ranks the patients based on the relevance of their retrieved medical records. However, these two ap-proaches may be useful for different queries. In this work, we propose a novel supervised approach that can effectively identify when to use either of the two aforementioned patient ranking approaches to attain effective retrieval performance. In particular, our approach deploys a classifier to learn to select a ranking approach when ranking patients, by using query difficulty measures, such as query performance pre-dictors and the number of medical concepts detected in a query, as learning features. We thoroughly evaluate our ap-proach using the standard test collections provided by the TREC Medical Records track. Our results show significant improvements over existing strong baselines.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search &amp; Retrieval]: Search process Keywords: Medical Records Search; Selective Ranking Ap-proach; Regression-trees
Electronic medical records are increasingly used to im-prove the quality of healthcare services and the patients safety [8]. For example, these medical records can be used to find patients who have a medical history relevant to a so-called inclusion criteria, in order to possibly recruit them for a clinical trial [18, 19]. Specifically, when conducting a comparative effectiveness research of a particular medical procedure (e.g. a treatment), a set of inclusion criteria, in-cluding the medical conditions of patients such as diagnoses and symptoms, are developed and used to search for patients with such conditions. In order to facilitate this process, an effective information retrieval (IR) system is used for iden-tifying the patients whose medical records are relevant to these inclusion criteria.

In the literature, two main types of approaches are used to rank patients based on the relevance of their medical history (e.g. [5, 9, 10, 11, 13, 21]). The first approach (e.g. [5, 9]), which we refer to as the patient model , represents a patient using all of the medical records of that patient. In particu-lar, this approach naturally represents patients using all of their medical history as a unit of retrieval [5]. For example, Demner-Fushman et al. [5] and King et al. [9], whose sys-tems achieved the highest retrieval effectiveness among the TREC 2011 participants, concatenated the medical records of each patient into a history document and used the result-ing history documents for indexing and retrieval.
On the other hand, the second approach (e.g. [10, 13, 21]), referred to as the document model , uses a medical record as a unit of retrieval, alleviating the problem of the variation in the size of the patients X  medical history. Indeed, the second approach firstly ranks medical records based on their rele-vance towards the query, and then the relevance scores of the retrieved medical records are aggregated to estimate the relevance of their associated patients [13]. For example, Lim-sopatham et al. [13] effectively deployed the expCombSUM voting technique [14], which had previously been developed for expert search, to estimate the relevance of patients based on the relevance scores of their associated medical records.
However, it is not clear in the literature under which con-ditions a particular ranking approach should be deployed to rank patients. Recently, Zhu and Carterette [21] proposed to use a data fusion technique [16], such as CombSUM and CombMAX, to merge the relevance scores from both the pa-tient model and the document model in order to exploit the effectiveness of both patient ranking approaches. Instead, we hypothesise that a selective approach that can appropri-ately identify which of the two ranking approaches is more effective for a given query can further improve retrieval per-formance.

In this work, we propose a novel selective approach for ranking patients based on the relevance of their medical his-tory. In particular, we postulate that some queries are better served with different patient ranking approaches. Therefore, our proposed approach aims to effectively apply a ranking approach that can accomplish a better retrieval performance for a particular query. Specifically, we deploy a regression-trees classifier to learn how to select a ranking approach using query difficulty measures such as AvIDF [3]. We evaluate our proposed approach in the context of the TREC 2011 and 2012 Medical Records track [18, 19]. Our results show that our selective approach to rank patients is effective. In particular, it significantly outperforms exist ing effective baselines, such as, when the relevance scores of both models are combined using data fusion techniques.
The main contributions of this paper are three-fold: 1. We introduce an approach that effectively selects which 2. We propose to exploit query difficulty measures, such 3. We thoroughly evaluate our approach using the stan-
The remainder of the paper is organised as follows. Sec-tion 2 introduces our novel classification approach that se-lectively applies either the patient or the document model when ranking patients on a per-query basis. Sections 3, and 4 discuss our experimental setup and results. Finally, we provide concluding remarks in Section 5.
In this section, we describe our selective ranking approach, which applies on each query either the patient model or the document model when ranking patients. As mentioned above, we hypothesise that queries benefit differently from the patient model or the document model. Indeed, some queries might be better served by the patient model, while others might be better handled by the document model. We propose an automatic decision mechanism that chooses to apply either the patient model or the document model for a particular query. Specifically, we propose to deploy a regression-trees classifier that, given a particular query, se-lects a specific patient ranking approach using learning fea-tures, such as query performance predictors. Our proposed approach consists of three components: (1) learning fea-tures; (2) the learned decision mechanism; and (3) the learn-ing process. The remainder of this section discusses each of the components of our proposed selective ranking approach. We first describe the learning features used in this paper. An effective learning feature should correlate well with the labelled data in a training set and should generalise across queries. To effectively select between the patient model and the document model, we learn a classifier using the query features listed in Table 1. Indeed, these features, which measure the difficulty of a query, can be categorised into two groups. The first group (Features 1-10) measures the relative difference of the query performance predictor scores between the patient model and the document model. We choose to use the obtained scores as our learning features, since we hy-pothesise that to attain an effective retrieval performance, a search system should deploy the ranking approach that finds the query the least difficult. The intuition is that an easy query leads to a better retrieval performance. For exam-ple, Features 1-4, including the clarity score [4], SCQ [20], MAXCQ [20] and NSCQ [20], measure the ambiguity of a query based on the coherence of the language used in a par-ticular set of documents. If the query model is similar to the language model of the collection, an effective retrieval per-formance is likely expected. Features 5-8 measure the speci-ficity of a query for a document collection. The more spe-cific the query, the better retrieval performance is expected. These features are AvICTF [3], AvIDF [3], EnIDF [3], and Query Scope (  X  ) [7]. Features 9-10,  X  1 [7] and  X  2 [7], exam-ine the distribution of the informativeness among the query Table 1: List of the query features used by our clas-sifier to decide to apply either the patient model or the document model.
 terms. The more a query term is informative, the better the r etrieval effectiveness.

On the other hand, the second group of features (Fea-tures 11-16) measures the difficulty of a query by using the information from the query itself. Feature 11 is the num-ber of non-stopword query terms. Moreover, since medi-cal queries normally focus on four aspects of the medical decision criteria (namely: symptom, diagnostic test, diag-nosis and treatment) [12], Features 12-16 are based on the occurrences of the medical concepts associated with these four aforementioned aspects. In particular, Feature 12 is the number of the concepts related to the medical decision criteria, which can be extracted from the query. Specifically, following Limsopatham et al. [12], we deploy MetaMap [2]  X  a medical concept detection tool  X  to identify those medical concepts in the query 2 . Features 13, 14, 15, and 16 estimate the probability that the medical concepts detected in a query are related to symptom, diagnostic test, diagnosis and treat-ment, respectively. The probability is estimated using the maximum likelihood by counting the number of medical con-cepts detected in the query. We hypothesise that the more medical concepts are detected in the query, the more likely that the query is difficult.
In this section, we describe our decision mechanism, which is based on a learned classifier. In particular, the classifier decides to apply the patient model or the document model for a particular query using the introduced query features. By doing so, we benefit from the fact that several query difficulty measures, which are used as learning features, can be taken into account when choosing an effective patient ranking approach.

While any classifier can be deployed, in this work, we use the Gradient Boosted Regression Trees (GBRT) [17] classi-fier to effectively decide when to apply the patient model or the document model for a given query, because of its simplicity and its effectiveness in several tasks (e.g. [6, 11, 17]). Specifically, we deploy the default setting of GBRT as implemented in the jforests package [6] 3 . http://code.google.com/p/jforests
T o effectively train the classifier, on a training set, we label each query with 1, if the patient model can achieve a better retrieval performance on a particular target measure; otherwise, it is labelled -1. This allows the classifier to learn which ranking approach is more effective for a particular query. Next, when training the GBRT classifier, we use the obtained accuracy as the loss function. We deploy the query difficulty measures extracted for each query, discussed in Section 2.1, as learning features to train the classifier.
This section discusses the experimental setup for evalu-ating our selective ranking approach. Indeed, Sections 3.1 and 3.2 describe the used test collections, and our ranking strategies, respectively.
We evaluate our proposed approach using the TREC 2011 and 2012 Medical Records track test collections [18, 19]. The task is to retrieve patient visits relevant to a query. Each patient visit contains all medical records associated to a pa-tient X  X  visit to a hospital. Due to the privacy concerns [19], a patient visit is used to represent a patient. The collection contains 101,711 medical records, which can be mapped into 17,265 patient visits. The official measure of TREC 2011 is bpref, while the official measures of TREC 2012 are infAP and infNDCG, because of the deemed possible incomplete-ness of the gold-standard relevance judgements [18, 19].
We conduct experiments using Terrier [15] 4 , applying Port-er X  X  English stemmer and removing stopwords. In addition, since dealing with negated language has been shown both in TREC 2011 and TREC 2012 to be useful for this task, we fol-low [10] and tokenise terms differently depending on whether they appear in a positive or negative context. For example, the term  X  X ever X  is represented as  X  X ever X  or  X  X $fever X , depend-ing on whether it appears in sentences such as  X  X igh fever X  or  X  X o fever observed X , respectively.
As a well-established representative patient model, we fol-low [9] and concatenate the medical records associated to the same patient visit into a large visit document . We use these visit documents to represent all the patient visits. Indeed, we index these visit documents using Terrier, and apply the effective parameter-free DPH weighting model [1] for rank-ing the patient visits.
As a representative of the document model, we follow [13] and deploy the expCombSUM voting technique [14] when ranking patient visits, since it has been shown to be effec-tive for this task both in TREC 2011 and TREC 2012 [10, 13]. Specifically, the parameter-free DPH weighting model is used to firstly rank medical records. Then, the relevance scores of the retrieved medical records are aggregated to es-timate the relevance scores of their associated patients. In particular, expCombSUM calculates the relevance score of a patient visit v towards a query Q as follows [13]: h ttp://terrier.org where R ( Q )  X  profile ( v ) is the set of medical records as-sociated to the patient visit v that are also in the ranking R ( Q ); score ( d; Q ) is the relevance score of medical record d for query Q , as obtained from the DPH model. In this work, we follow [13] and limit the number of the medical records voting for the relevance of patient visits ( | R ( Q ) | ) to 5,000.
To evaluate our proposed selective ranking approach, we use a 5-fold cross-validation across the 34 topics of TREC 2011 and the 47 topics of TREC 2012, where each fold has completely separated training and test query sets. We sep-arate the set of queries used in the two years of TREC, because the used relevance assessment mechanism in each query set is different. Indeed, TREC 2011 deploys absolute judgement, while TREC 2012 uses a sampling technique [18, 19]. When labelling the training set, we target the retrieval performance in terms of bpref and infNDCG for TREC 2011 and 2012, respectively.
We compare the retrieval performance of our selective ranking approach with existing strong baselines using the TREC 2011 and 2012 Medical Records track test collections. Indeed, our baselines include the patient model, the docu-ment model, and the combination of the relevance scores computed from both the patient and document models us-ing either CombSUM or CombMAX, as suggested in [21].
Table 2 compares the retrieval performance of our pro-posed approach with the aforementioned baselines, in terms of bpref, infNDCG, and infAP. In addition, to gauge the potential effectiveness of our deployed classifier, we also re-port the best possible retrieval performances that could be attained by our approach (i.e. an oracle), when the classifier correctly identifies an effective patient ranking approach for all of the queries. Firstly, we observe that the patient model and the document model attain a comparable retrieval effec-tiveness. Indeed, the document model outperforms the pa-tient model in terms of the bpref and infNDCG measures, for TREC 2011 and 2012, respectively (bpref 0.5141 vs. 0.5006 and infNDCG 0.4481 vs. 0.4459), while the patient model performs better in terms of infAP for TREC 2012 (0.1865 vs. 0.1857). Moreover, we find that applying the data fu-sion techniques, including CombMAX and CombSUM, as suggested in [21], does not in general improve the retrieval performance over both the patient and the document mod-els. On the other hand, we find that, with the 5-fold cross-validation setting, our selective approach outperforms all of the baseline approaches. Specifically, in terms of bpref, our approach (bpref 0.5261) significantly (paired-t test, p &lt; 0 : 05) outperforms the patient model, the CombSUM, and the CombMAX baselines for up to 5.9%. However, in terms of infNDCG and infAP, the increased performances are not statistically different. The observed results demonstrate the effectiveness of our deployed approach in selecting the right ranking approach for a particular query. Note that the CombSUM and CombMAX do not perform as effectively as in [21], partially because we use DPH to rank documents in the patient model and to rank medical records at the first phase of the document model , instead of a language model. In addition, when aggregating the relevance scores of the medical records in the document model, we use the exp-CombSUM voting technique, instead of just summing up the relevance scores as in [21]. This suggests that the per-formances of CombSUM and CombMAX depend on the un-alternative approach are denoted x , xx and xxx , respectively. derlying used retrieval models. In contrast, our proposed approach overcomes this problem by appropriately learning from the features when selecting a patient ranking model. Furthermore, we find that if our classifier could make the correct decisions for each of the queries, the retrieval perfor-mances can further improve (See the oracle row in Table 2).
In addition, we compare the difference between the re-trieval performance obtained using the patient model and the document model on every query, in Figure 1. Note that the difference in the retrieval performance is positive when the patient model is more effective (see Section 2.3). From this figure, we find that neither the patient model nor the document model is consistently more effective for all of the queries. We also find that the most effective patient ranking approach depends on the used measure (e.g. for query# 183, the patient model is more effective on infNDCG, while for the same query, the document model is more effective for infAP). This confirms the importance of deploying selective ranking approaches when ranking patients.
In this paper, we have argued for the importance to de-ploy a selective approach that appropriately selects the most appropriate patient ranking approach on a per-query basis. Our proposed selective approach exploits several query per-formance predictors as learning features within a regression-trees classifier, in order to suitably choose between the pa-tient or the document models when ranking patients on a per-query basis. The results demonstrate the effectiveness of our proposed approach in comparison to recent strong baselines from the literature. Our proposed approach can be easily extended to include new learning features, as well as further possible patient ranking approaches.
