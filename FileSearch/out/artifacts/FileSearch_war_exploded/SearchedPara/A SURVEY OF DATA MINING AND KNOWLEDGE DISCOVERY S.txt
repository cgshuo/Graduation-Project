 Knowledge discovery in databases is a rapidly growing field, few years knowledge discovery tools have been used mainly in research environments, sophisticated software products are now rapidly emerging. In this paper, we provide an overview of common knowledge discovery tasks and approaches to solve these tasks. We propose a feature classification scheme that can be used to study knowledge and data mining software. This scheme is connectivity, and data mining characteristics. We then apply our which are either research prototypes or commercially available. knowledge discovery software to possess in order to accommodate its users effectively, as well as issues that are either not addressed or insufficiently solved yet. Knowledge discovery in databases, data mining, surveys. The rapid emergence of electronic data management methods has lead some to call recent times as the "Information Age." Powerful database systems for collecting and managing are in use in transaction that does not generate a computer record somewhere. Each year more operations are being computerized, all accumulate data on operations, activities and performance. All these data hold valuable information, e.g., trends and patterns, which could be used to improve business decisions and optimize success. However, today  X  s databases contain so much data that it becomes almost impossible to manually analyze them for valuable decision-making information. In many cases, hundreds of independent attributes need to be simultaneously considered in order to accurately model system behavior. Therefore, humans need assistance in their analysis capacity. This need for automated extraction of useful knowledge from huge amounts of data is widely recognized now, and leads to a rapidly developing market of automated analysis and discovery tools. Knowledge discovery and data mining are techniques to discover strategic information hidden in very large databases. Automated discovery tools have the capability to analyze the raw data and present the extracted high level information to the for himself or herself. environments. Now we are at a stage where sophisticated tools, which aim at the mainstream business user, are rapidly emerging. New tools hit the market nearly every month. The Meta Group from $50 million in 1996 to $800 million by 2000 ([35]). The aim of this study is fourfold: 3) Investigate existing knowledge discovery and data mining included in this report. Section 1.3 lists some other currently ongoing survey projects and available information resources. Section 2 discusses the process of analyzing high-volume data and solving these tasks. Section 3 presents the proposed feature classification scheme and the review of existing software using provided in [24]). Section 4 draws some conclusions about the current state of existing discovery tools, and identifies some truly useful, thus providing directions for future research. A major goal of our study is to provide a market overview of off-the-shelf software packages whose main purposes are to aid in knowledge discovery and data mining. In order to keep the scope considered in this study: mining tools and does not perform analysis itself, such as Geneva from Price Waterhouse LLP ([43]). intended use lies somewhere else, such as MATLAB  X  s Neural Network Toolbox ([16]) or statistical software packages. discovery software, but in reality is not much more than a reporting or visualization tool, such as the Oracle Discoverer ([41]). of industry-specific solutions. This survey however is only about off-the-shelf products. While we tried to make the information in this survey as complete and accurate as possible, we also f ound quite some companies that did not want to disclose their technologies and algorithms used because of competitive advantages. Some attempts to provide surveys of data mining tools have been made, for example: papers, and two large data mining bibliographies. It attempts to provide links to as much of the available data mining information on the net as is possible. comprehensive catalog of tools for discovery in data, as well as back issues of the KDD-Nuggets mailing list. approximately 15 of the top products and has links directly from the features table to each pr oduct. The site, which is still under construction, will also have tutorials on various data mining technologies and problems, illustrated with real examples that, at the same time, show how various products work. related WWW pages ([56]). Among others, there is a list more than 60 data mining software vendors, a list with software patents related to data mining, and general information (tutorials and papers) related to data mining. In our work, we want to provide a method to study software tools and apply this method to investigate a comprehensive set of 43 existing tools. For each tool, we examine key features and provide detailed descriptions as well as summary tables. Our aim is to users. discovery, and serves as background explanation for our feature classification scheme and the software feature tables in section 3. discovery and data mining in section 2.1, we examine issues related to the database connection of the discovery endeavor in methods and research areas that are promising in solving these analysis tasks. There is still some confusion about the terms Knowledge Discovery in Databases (KDD) and data mining. Often these two the overall process of turning low-level data into high-level knowledge. A simple definition of KDD is as follows: Knowledge discovery in databases is the nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data ([20]). We also adopt the commonly used from observed data. Although at the core of the knowledge discovery process, this step usually takes only a small part (estimated at 15% to 25 %) of the overall effort ([8]). Hence data for example involve: the goals of the data mining process There are several very good papers on the overall endeavor of knowledge discovery. The interested reader may refer to ([8], [19], [20], or [21]) as good introductory readings. Any realistic knowledge discovery process is not linear, but rather earlier steps, thus producing a variety of feedback loops. This motivates the development of tools that support the entire KDD process, rather than just the core data-mining step. Such tools transformation etc. statistics' community. Such tools usually operate separately from the data source, requiring a significant amount of time spent with knowledge discovery tool and the analyzed database, utilizing the existing DBMS support, is clearly desirable. For the reviewed knowledge discovery tools, the following features are inspected: Ability to access a variety of data sources: In many cases, the data to be analyzed is scattered throughout the corporation, it has to be gathered, checked, and integrated before a meaningful analysis sources can thus greatly reduce the amount of data transforming. Online/Offline data access : Online data access means that queries are run directly against the database and may run concurrently with other transactions. In offline data access the analysis is performed with a snapshot of the data source, in many cases involving an export/import process from the original data source online or offline data access becomes especially important when one has to deal with changing knowledge and data: In financial markets for example, rapidly changing market conditions may make previously discovered rules and patterns invalid. The underlying data model: Many tools that are available today just take their input in form of one table, where each sample case (record) has a fixed number of attributes. Other tools are based on database. Object-oriented and nonstandard data models, such as current KDD technology ([21]). Maximum number of tables/rows/attributes: These are theoretical limits on the processing capabilities of the discovery tool. Database size the tool can comfortably handle: The anticipated choosing a discovery tool. While the maximum numbers of tables/rows/attributes are theoretical limitations, there are also practical limitations that are posed by computing time, memory requirements, expressing and visualization capabilities etc. A tool that holds all data in main memory for example may be not maximum number of rows is unlimited. Attribute types the tool can handle: Some discovery tools have example, tools based on neural networks usually require all [49] may not be able to handle continuous (real) data, etc. Therefore, the attribute types present in the data source should be considered when selecting an analysis tool. Query language: The query language acts as an interface between process data and knowledge and to direct the discovery process. Some tools do not have a query language: human interaction is allow querying of the data and/or knowledge via queries formulated in some query language, which may be a standard language like SQL or an application specific language. Querying interface (GUI). The reviewed tools differ greatly in each of the features described above. The choice of a tool therefore depends on application specific requirements and considerations, such as form and size of the data available, goals of the discovery process, needs and training of the end user, etc. extracting patterns from data. These methods can have different goals, dependent on the intended outcome of the overall KDD result. For example, to determine which customers are likely to buy a new product, a business analyst might need to first use regression to predict buying behavior for each cluster.
 Most data mining goals fall under the following categories: Data Processing: Depending on the goals and requirements of the KDD process, analysts may select, filter, aggregate, sample, clean and/or transform data. Automating some of the most typical data process may eliminate or at least greatly reduce the need for programming specialized routines and for data export/import, thus improving the analyst  X  s productivity. Prediction: Given a data item and a predictive model, predict the value for a specific attribute of the data item. For example, given a predictive model of credit card transactions, predict the likelihood used to validate a discovered hypothesis. Regression: Given a set of data items, regression is the analysis of the dependency of some attribute values upon the values of other attributes in the same item, and the automatic production of a model that can predict these attribute values for new records. For example, given a data set of credit card transactions, build a model that can predict the likelihood of fraudulence for new transactions. Classification: Given a set of predefined categorical classes, For example, given classes of patients that correspond to medical treatment responses, identify the form of treatment to which a new patient is most likely to respond. Clustering: Given a set of data items, partition this set into a set of classes such that items with similar characteristics are grouped subgroups of customers that have a similar buying behavior. relationships between attributes and items such as the presence of relations may be associations between attributes within the same data item (  X  Out of the shoppers who bought milk, 64% also purchased bread  X  ) or associations between different data items (  X  Every time a certain stock drops 5%, a certain other stock raises referred to as  X  sequential pattern analysis  X  . Model Visualization: Visualization plays an important role in interpretable by humans. Besides, the human eye-brain system itself still remains the best pattern-recognition device known. Visualization techniques may range from simple scatter plots and histogram plots over parallel coordinates to 3D movies. Exploratory Data Analysis (EDA): Exploratory data analysis dependence on preconceived assumptions and models, thus attempting to identify interesting patterns. Graphic representations of the data are used very often to exploit the power of the eye and human intuition. While there are dozens of software packets available that were developed exclusively to support data approaches into an overall KDD environment. It should be clear from the above that data mining is not a single technique, any method that will help to get more information out of data is useful. Different methods serve different purposes, each method offering its own advantages and disadvantages. However, most methods commonly used for data mining can be classified into the following groups. Statistical Methods: Historically, statistical work has focused mainly on testing of preconceived hypotheses and on fitting underlying probability model. In addition, it is generally assumed that these methods will be used by statisticians, and hence human intervention is required for the generation of candidate hypotheses and models. Case-Based Reasoning: Case-based reasoning (CBR) is a use of past experiences and solutions. A case is usually a specific problem that has been previously encountered and solved. Given a particular new problem, case-based reasoning examines the set of solution is applied to the new problem, and the problem is added to the case base for future reference. Neural Networks: Neural networks (NN) are a class of systems modeled after the human brain. As the human brain consists of millions of neurons that are interconnected by synapses, neural networks are formed from large numbers of simulated neurons, connected to each other in a manner similar to brain neurons. Like in the human brain, the strength of neuron interconnections may change (or be changed by the learning algorithm) in response to a presented stimulus or an obtained output, which enables the network to  X  learn  X  . Decision Trees: A decision tree is a tree where each non-terminal Depending on the outcome of the test, one chooses a certain branch. To classify a particular data item, we start at the root node and follow the assertions down until we reach a terminal node (or leaf). When a terminal node is r eached, a decision is made. set, characterized by their hierarchical organization of rules. occurrence of certain attributes in a data item, or between certain data items in a data set. The general form of an association rule is predict Y with a confidence C and a significance S . Bayesian Belief Networks: Bayesian belief networks (BBN) are from co-occurrence counts in the set of data items. Specifically, a BBN is a directed, acyclic graph, where the nodes represent dependencies between the attribute variables. Associated with each node are conditional probability distributions that describe the relationships between the node and its parents. Genetic algorithms / Evolutionary Programming: Genetic algorithms and evolutionary programming are algorithmic optimization strategies that are inspired by the principles observed in natural evolution. Of a collection of potential problem solutions combined with each other. In doing so, one expects that the overall goodness of the solution set will become better and better, Genetic algorithms and evolutionary programming are used in data mining to formulate hypotheses about dependencies between formalism. Fuzzy Sets: Fuzzy sets form a key methodology for representing and processing uncertainty. Uncertainty arises in many forms in vagueness, etc. Fuzzy sets exploit uncertainty in an attempt to make system complexity manageable. As such, fuzzy sets noisy or imprecise data, but may also be helpful in developing uncertain models of the data that provide smarter and smoother performance than traditional systems. Since fuzzy systems can tolerate uncertainty and can even utilize language-like vagueness too expensive. Rough Sets: A rough set is defined by a lower and upper bound of a set. Every member of the lower bound is a certain member of the set. Every non-member of the upper bound is a certain non-between the lower bound and the so-called boundary region. A member of the set. Therefore, rough sets may be viewed as fuzzy sets with a three-valued membership function (yes, no, perhaps). Like fuzzy sets, rough sets are a mathematical concept dealing with uncertainty in data ([42]). Also like fuzzy sets, rough sets are seldom used as a stand-alone solution; they are usually combined with other methods such as rule induction, classification, or clustering methods. A discussion of the advantages and disadvantages for the different approaches with respect to data mining as well as pointers to introductory readings are given in [24]. study knowledge discovery and data mining tools. We then apply Although not exhaustive, we believe that the reviewed products are representative for the current status of technology. As discussed in Section 2, knowledge discovery and data mining tools require a tight integration with database systems or data characteristics in terms of data model, database size, queries tasks and employ different methods to achieve their goals. Some may require or support more interaction with the user than the other. Some may work on a stand-alone architecture while the other may work on a client/server architecture. To capture all can be used to study knowledge discovery and data mining tools. called general characteristics, database connectivity, and data mining characteristics which are described below. Product: Name and vendor of the software product. Production Status: Status of product development. P=Commercial strength product, A=Alpha, B=Beta, R=Research Prototype. Legal Status: PD=Public Domain, F=Freeware, S=Shareware, C=Commercial. a special free or reduced-cost academic licensing available. Demo: Specifies if there is a demo version available. D=Demo version available for download on the internet, R=Demo available on request, U=Unknown. Architecture: The computer architecture on which the software runs. S=Standalone, C/S=Client/Server, P=Parallel Processing. Operating Systems: Lists the operating systems for which run time version of the software can be obtained. Data sources: Specifies possible formats for the data that is to be analyzed. T=Ascii text files, D=Dbase files, P=Paradox files, F=Foxpro files, Ix=Informix, O=Oracle, Sy=Sybase, Ig=Ingres, A=MS Access, OC=Open database connection (ODBC), SS=MS SQL Server, Ex=MS Excel, L=Lotus 1-2-3. DB Conn.: Type of database connection. Onl.=Online: Queries are run directly against the database and may run concurrently with other transactions. Offl.=Offline: The analysis is performed with a snapshot of the data source. Size: The maximum number of records the software can comfortably handle. S=Small (up to 10.000 records), M=Medium (10.000 to 1.000.000 records), L=Large (more than 1.000.000 records). Model: The data model for the data to be analyzed. R=Relational, O = Object Oriented, 1=One table. Attributes: The type of the attributes the software can handle. Co=Continuous, Ca=Categorical (discrete numerical values), S=Symbolic. Queries: Specifies how the user can formulate queries against the knowledge base and direct the discovery process. S=Structured query language (SQL or derivative), Sp.=an application specific interface language, G=Graphical user interface, N=Not applicable, U=Unknown. Discovery Tasks: Knowledge discovery tasks that the product is intended for. Pre.=Data Preprocessing (Sampling, Filtering), Clu=Clustering, A=Link Analysis (Associations), Vis=Model Visualization, EDA = Exploratory Data Analysis. Discovery Methodology: Type of methods used to discover the knowledge. NN=Neural Networks, GA=Genetic Algorithms, FS=Fuzzy Sets, RS=Rough Sets, St.=Statistical Methods, DT=Decision Trees, RI=Rule Induction, BN=Bayesian Networks, CBR = Case Based Reasoning. Human Interaction: Specifies how much human interaction with A=Autonomous, G=Human guided discovery process, H=Highly Interactive. classification scheme to study 43 existing knowledge discovery access a wide variety of data sources is widely recognized by now, support only a small number of data formats. Surprisingly few continuous as well as discrete and symbolic attribute types. transforming symbolic variables into numerical ones or splitting of continuous attribute ranges into intervals.
 From table 3.3 we can observe that most of the tools employ trees, and statistical methods. Techniques from other promising begun yet to find their way into knowledge discovery software. Detailed descriptions for each software pr oduct are provided in [24]. Knowledge discovery can be broadly defined as the automated discovery of novel and useful information from commercial discovery process, dealing with the extraction of patterns and relationships from large amounts of data. databases. Many of them have recognized the potential value of these data as an information source for making business decisions. The dramatically increasing demand for better decision support is answered by an extending availability of knowledge discovery and developed at various universities as well as software products from commercial vendors. In this paper, we provide an overview of common knowledge discovery tasks, approaches to solve these tasks, and available software tools employing these approaches. However, despite its rapid growth, KDD is still an emerging field. The development of successful data mining applications still remains a tedious process ([21]). The following is a (naturally incomplete) list of issues that are unexplored or at least not satisfactorily solved yet: carry out data analysis. From section 2 it follows immediately that there is no best technique for data analysis. The issue is therefore not which technique is better than another, but rather which (continued after results tables ... ) DBMiner (SFU) [30] R C S N C/S x x x ToolDiag (UFES) [49] P F -D 
S Weka (Univ. of Waikato) [58] P C F D 
S x Data Surveyor (Data Destilleries) [14] x x x x x x x L 
R x x x S DBMiner (SFU) [30] x x L 
R x x x S, G ToolDiag (UFES) [49] x x 
S 1 x N Weka (Univ. of Waikato) [58] x x M 
R x x x N Data Surveyor (Data Destilleries) [14] x x x x x x x x x x x x x x DBMiner (SFU) [30] x x x x x x x x x SuperQuery (AZMY) [4] x x x x x x x x x x ToolDiag (UFES) [49] x x x x x x has to provide a wide range of different techniques for the solution of different problems. Extensibility. This is another consequence from the fact that different techniques outperform each other for different problems. With the increasing number of proposed techniques as well as reported applications, it becomes clearer and clearer that any fixed arsenal of algorithms will never be able to cover all arising problems and tasks. It is therefore important to provide an architecture that allows for easy synthesis of new methods, and adaptation of existing methods with as little effort as possible. Seamless integration with databases. Currently available data drill-down analysis and reporting, provided by vendors of RDBMS  X  s, sometimes in association with on-line analytical processing (OLAP) vendors. These systems provide a tight connection with the underlying database and usually deploy the processing power and scalability of the DBMS. They are also automatically extracting patterns and models. The second category consists of (usually stand-alone) pattern discovery tools, which rely on keeping all their data in main memory, thus lacking consuming repeated export-import processes. Support for both analysis experts and novice users. There are usually three stages at deploying KDD technology in an organization: demands and also bring different prerequisites. Most of the unaffordable amount of training before being useful to novice end managers. These users are less skilled in complex data analysis have a thorough understanding of their occupation domain. Furthermore, they are usually not interested in using advanced their everyday business questions. End users need simple-to-use tools that efficiently solve their business problems. Existing software packages lack sufficient support for both directing the analysis process and presenting the restricted to a very limited set of techniques and problems. Optimally, a better usability by novice users would have to be achieved without giving up other desirable features such as flexibility and/or analysis power. Managing changing data. In many applications, including the vast variety of nearly all business problems, the data is not stationary, but rather changing and evolving. This changing data may make previously discovered patterns invalid and hold new repeat the same analysis process (which is also work-intensive) in knowledge bases. standard data such as numbers and strings but also large amounts of nonstandard and multimedia data, such as free-form text, audio, image and video data, temporal, spatial and other data types. Those data types contain special patterns, which can not be handled well by the standard analysis methods. Therefore, these applications require special, often domain-specific, methods and algorithms. While there are fundamental problems that remain to be solved, there have also been numerous significant success stories reported, and the results and benefits are impressive ([53]). approaches with limited capabilities, reassuring results have been achieved, and the benefits of KDD technology have been convincingly demonstrated in the broad range of application domains. The combination of urgent practical needs and the strong research interests lets us also expect a future healthy grow applications. [1] AbTech Corp. ModelQuest 2.0. http://www.abtech.com. [2] Acknosoft Inc. KATE-Tools 6.0. http://www.acknosoft.com. [5] Bissantz K  X  ppers &amp; Company GmbH. Delta Miner 3.5. [6] Bjorvand, A.T. Rough Enough --Software Demonstration. [7] Bourgoin, M.O., and Smith, S.J. Big Data --Better Returns, [8] Brachman, R., and Anand, T. The process of knowledge [9] Brooks, P. Visualizing Data --Sophisticated Graphic [10] Brunk, C., Kelly, J., and Kohavi, R. MineSet: An Integrated [12] Clark, P., and Boswell, R. Rule induction with CN2: Some [14] Data Destilleries Inc. Online Data Mining (tm) --Data [16] Demuth, H., and Beale, M. The Neural Network Toolbox for [19] Fayyad, U. Data Mining and Knowledge Discovery: Making [20] Fayyad, U., Piatetsky-Shapiro, G., and Smyth, P. From Data [21] Fayyad, U., Piatetsky-Shapiro, G., and Smyth, P. The KDD [22] Flori, R.D. Product Review: BrainMaker Professional Neural [23] Gilliland, S. Scenario 1.0: Mine Your Data for Statistical [24] Goebel, M., and Gruenwald, L. A Survey of Knowledge [27] Information Discovery Inc. The IDIS Information Discovery [29] Isoft SA. Alice 5.1. http://www.alice-soft.com, 1998. [30] Kamber, M., Han, J., and Chiang, J.Y. Using Data Cubes for [31] Kohavi, R., Sommerfield, D., and Dougherty, J. Data Mining [33] Lippmann, R.P. An Introduction to Computing with Neural [35] Meta Group Inc. Data Mining: Trends, Technology, and [37] Nicolaisen, N. WizRule may be the key to avoiding database [38] NASA COSMIC. IND 1.0. University of Georgia, NASA [40]  X  hrn, A., Komorowski, J., Skowron, A., and Synak, P. The [41] Oracle Inc. Discoverer 3.0 User  X  s Guide, 1997. [42] Pawlak, Z. Rough Sets: Theoretical Aspects of Reasoning [43] Price Waterhouse LLP. Geneva V/T 3.2. Geneva [44] Piatetsky-Shapiro, G. The Knowledge Discovery Mine, [45] Pryke, A. The Data Mine. http://www.cs.bham.ac.uk/~anp/, [46] Quadrillion Corp. Q-Yield 4.0. http:// www.quadr illion.com, [47] Quinlan, J.R., C4.5 --Programs for Machine Learning. [48] Ramoni, M., and Sebastiani, P. Learning Bayesian Networks [49] Rauber, T.W. ToolDiag 2.1. Universidade Federal do [50] Reich, Y. Building and Improving Design Systems: A [52] Segal, R., and Etzioni, O. Learning decision lists using [53] Simoudis, E. Reality Check for Data Mining, IEEE Expert, [54] Sommer, E., Emde, W., Kietz, J., Morik, K., and Wrobel, S. [56] Thearling, K. Data Mining and Database Marketing WWW [57] University of Lyon. Sipina-W 2.5. Laboratoire E.R.I.C., [58] Waikato ML Group. User Manual Weka: The Waikato [59] Wrobel, S., Wettschereck, D., Sommer, E., and Emde, W. Michael Goebel received his BS degree in Computer Science from the University of Braunschweig, Germany and MS degree in Computer Science from the University of Oklahoma in 1998. He is currently a doctoral candidate in the PhD program in Computer Science at the University of Auckland, New Zealand. His primary research interests are in Machine Learning, Data Mining, and Reasoning under Uncertainty. Professor in the School of Computer Science at The University of Oklahoma. She received her Ph.D. in Computer Science from Southern Methodist University in 1990, MS in Computer Science from the University of Houston in 1983, and BS in Physics from the University of Saigon, Vietnam in 1978. She was a Software Engineer at WRT, a Lecturer in the Computer Science and Engineering Department at Southern Methodist University, and a Member of Technical Staff in the Database Management Group at the Advanced Switching Laboratory of NEC America. Her major research interests include Real-Time Databases, Object-Oriented Databases, Data Warehouse and Data Mining, Multimedia Databases, and Distributed Mobile Databases. She is a member of 
