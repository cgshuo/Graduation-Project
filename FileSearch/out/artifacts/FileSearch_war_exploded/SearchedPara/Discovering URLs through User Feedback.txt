 Search engines rely upon crawling to build their Web page collections. A Web crawler typically discovers new URLs by following the link structure induced by links on Web pages. As the number of documents on the Web is large, discovering newly created URLs may take arbitrarily long, and depending on how a given page is connected to others, such a crawler may miss the pages altogether. In this paper, we evaluate the benefits of integrating a passive URL discovery mechanism into a Web crawler. This mechanism is passive in the sense that it does not require the crawler to actively fetch documents from the Web to discover URLs. We focus here on a mechanism that uses toolbar data as a representative source for new URL discovery. We use the toolbar logs of Yahoo! to characterize the URLs that are accessed by users via their browsers, but not discovered by Yahoo! Web crawler. We show that a high fraction of URLs that appear in toolbar logs are not discovered by the crawler. We also reveal that a certain fraction of URLs are discovered by the crawler later than the time they are first accessed by users. One important conclusion of our work is that web search engines can highly benefit from user feedback in the form of toolbar logs for passive URL discovery.
 H.3.3 [ Information Storage Systems ]: Information Re-trieval Systems Performance, Experimentation Search engines, Web crawling, URL discovery, toolbar Web crawling is an integral part of a search engine. A Web crawler continuously discovers new URLs and fetches their content to populate a large Web repository, manda-tory to build an inverted index to serve user queries. Even though the basic mechanism of a crawler is simple, crawling efficiently and effectively is a difficult problem. The Web is very large and has a dynamic nature. To keep up with the rate of changes to Web pages, the crawler not only has to continuously enlarge its repository by expanding its fron-tier, but also needs to refresh previously fetched pages to incorporate in its index the changes on those pages.
In practice, crawlers prioritize the pages to be fetched, taking into account various constraints: available network bandwidth, peak processing capacity of the backend system, and politeness constraints of Web servers [24]. As a result of these constraints, the delay to discover a Web page can be quite long after its creation and some Web sites may be only partially crawled. Another important challenge is the discovery of hidden Web content, which crawlers cannot reach by following links on Web pages [4]. Hidden Web pages often contain dynamic content and are backed by a database. Fetching such pages requires different techniques [32], which a Web crawler does not typically implement. Pages that are disconnected or are part of small connected components are also unlikely to be discovered by a crawler. Consequently, the crawler misses the opportunity to incorporate such pages into its repository, and the search engine cannot serve them.
The main goal of our work is to understand whether user feedback can be beneficial to improve the URL discovery process in a Web crawler. Even though different forms of user feedback ( e.g. , click-through history [31] and eye track-ing [19]) have been considered to enhance the search result quality, to the best of our knowledge, our work is the first to evaluate the benefits of using the URLs collected from a Web browser toolbar as a form of user feedback to the crawl-ing process. In this work, we quantify the effectiveness of this technique, which we refer to as toolbar-enhanced Web crawling, through extensive experimental analysis.
 Contributions. Our work seeks answers to these questions:
To answer the above-raised questions, we conduct a large-scale experimental study on a snapshot of the entire Yahoo! Web crawl and the URL access logs obtained via the Yahoo! toolbar. Our study shows that there is a non-negligible frac-tion of Web pages, accounting for 80% of URLs in daily tool-bar logs, which are not discovered by the crawler. If these pages are incorporated to the search engine index, 26.2% of the top-k query results are among them. We also observe that the crawler, on average, has a high delay of 121 days in discovering 2% of URLs that appear in daily toolbar logs. These URLs are shown to have higher link-based quality and content-based quality than those only discovered by the crawler, and serve up to 60 million queries before they are discovered by the crawler.
 Roadmap. The paper is organized as follows. We introduce the active and passive URL discovery concepts in Section 2, together with the concept of toolbar-enhanced Web crawl-ing. We discuss our experimental methodology in Section 3. We carry out a user study, in Section 4, to quantify use-fulness of URLs. The impact of the proposed approach on the content and runtime systems of a search engine are dis-cussed in Sections 5 and 6, respectively. Section 7 surveys the previous work. The paper is concluded in Section 8.
Most Web crawlers rely on the same basic mechanism to fetch the content on the Web. The crawler first fetches the pages pointed by a set of seed links. The downloaded pages are stored in a repository. The crawler then parses the con-tent of these pages to discover links that are not seen before. The newly discovered links are stored in a download queue, referred to as the frontier of the crawler. The entire pro-cess, which involves fetching, storing, and parsing of pages, is iteratively repeated using the links in the frontier. In commercial Web crawling systems, additionally, previously downloaded pages are selectively refetched. This helps main-taining the freshness of the content in the repository.
Unfortunately, this form of crawling, where the Web is actively harvested by the crawler, has two important weak-nesses. The first weakness is its limited coverage. In active Web crawling, the content discovery is limited to following links, and not all content on the Web can be accessed by fol-lowing the link structure of Web pages [4]. For example, a high fraction of the content on the Web is accessible through Web forms. This type of dynamically generated content is hidden to a standard Web crawler as exposing such pages requires that the crawler knows how to fill out such forms.
The second weakness is the inaccuracy in estimating im-portance of the URLs in the frontier. In practice, it is not always trivial for the crawler to accurately estimate the im-portance of a Web page. 1 Traditional importance estimation techniques rely on the connectivity of a page to decide on
Typically, the crawler prioritizes the URLs in the frontier its importance, e.g. , pages that are highly linked by other pages are predicted to be high-quality or important. Unfor-tunately, these techniques can only capture the importance attributed to a page in the past, but they fail to capture how important the page currently is or will be in the future. More specifically, a page that has accumulated many incom-ing links in the past, may not be relevant at the moment. In fact, a newly created page that has not yet accumulated enough links may be very important. More recent techniques use feedback from the popularity of the page in search re-sults, e.g. , highly viewed or clicked pages are deemed to be important and are fetched earlier. Such techniques perform better in estimating the current, user-perceived importance of pages. However, although these techniques are useful for prioritizing the URLs in the refresh queue, they are not use-ful for the URLs in the frontier since, for these URLs, no popularity information can be obtained from search results. Web pages can be discovered by means other than active URL discovery, which relies on link following. For example, users may push the URLs of their newly created Web pages or even their content to the search engine. 2 Other alterna-tives include obtaining URLs from emails, instant messages, and browser toolbars. In an extreme scenario, URLs can be discovered and pushed to the crawler by ISPs or even net-work routers while the content is being consumed by users.
All examples mentioned above are different forms of pas-sive URL discovery, where URLs are discovered by means other than link following. Passive URL discovery has three advantages over the active URL discovery approach. First, it has the potential to improve the coverage of the crawler as certain URLs that are not accessible by the crawler are discoverable through such techniques. For example, a link to a hidden Web page can be extracted from the email logs. In fact, this is especially useful to improve the coverage of real-time generated content. For example, a link to a newly created streaming video of a soccer game can be extracted from instant messages. It may take hours or even days af-ter the game has ended to discover such links via active URL discovery. Second, it improves the accuracy of the importance estimations by using the popularity ( i.e. , the occurrence frequency) of passively obtained URLs, which is particularly useful for newly generated content that has not accumulated enough in-links but is worth fetching ear-lier. Third, performing the necessary processing for URL discovery in external agents or software components does not impose additional workload on the crawler.
Major commercial search engines provide a toolbar soft-ware that can be deployed on users X  Web browsers. These toolbars provide additional functionality to users, such as quick search option, shortcuts to popular sites, and mal-ware detection. However, from the perspective of the search engine companies, their main use is on branding and col-lecting marketing statistics. A typical toolbar tracks some of the actions that the user performs on the browser ( e.g. , typing a URL, clicking on a link) and reports these actions to the search engine, where they are stored in a log file. in decreasing order of importance so that pages that are predicted to be more important are fetched earlier. http://siteexplorer.search.yahoo.com/submit Figure 1: Toolbar-enhanced crawling architecture.

In this work, our goal is to use the feedback obtained from toolbar logs to improve effectiveness of Web crawling, i.e. , we specifically focus on a particular type of crawling en-hanced with passive URL discovery: toolbar-enhanced Web crawling, where URLs are extracted from toolbar logs and pushed to the crawler X  X  frontier. We note that our approach carries all potential benefits of passive URL discovery.
Fig. 1 illustrates the toolbar-enhanced crawling idea in more detail. As described in Section 2.1, the crawler contin-ues to extend its frontier by fetching pages from the Web, ex-tracting new links, and storing them in the download queue. In the mean time, users continue to access the content on the Web. As mentioned above, all page accesses that are performed through the browser are reported by the toolbar software to the search engine, i.e. , the URLs accessed via the browser are simply transferred to the search engine and are stored in a common log file. 3 A URL filtering daemon takes new URLs discovered through the toolbar and selec-tively adds them to the frontier of the crawler. We note that only the URLs that are not seen by the crawler are se-lected. 4 Further filtering is possible ( e.g. , malformed URLs can be eliminated here). Since our focus herein is not on the design of the URL filter, we omit such details. In this work, we are more specifically interested in the impact of tool-bar data on crawling and the degree of potential benefits in toolbar-enhanced Web crawling.
In experiments, we use a snapshot of the full Yahoo! Web crawl. The snapshot contains all URLs  X  X iscovered X  by the crawler before March 23, 2010. We note that the original snapshot does not contain any URLs obtained through pas-sive URL discovery. For confidentiality reasons, we cannot disclose the size of the crawl, but we could safely claim that it is one of the biggest Web collections ever crawled to date and is of the order of hundreds of billions URLs.

We also use a very large log of URLs accessed by users through Yahoo! toolbar. Again, we cannot disclose the size of the toolbar log, but it is safe to state that it is one of the most popular toolbars currently used in Internet browsers. The toolbar log contains information about the access times of each URL.
Alternatively, along with the URL, the links extracted from the page content can also be transferred.
This can be performed through a standard URL seen test, available in crawlers running in active mode [20].
 Figure 2: The four sets formed based on the discov-ery time of URLs by the crawler and the toolbar.

To assess the benefits that toolbar-enhanced crawling brings to the runtime system of a search engine, i.e. , the improvement in search result quality, we also use a large web query log obtained from the Yahoo! search engine. The query log on each day used in our experiments represents a fraction of the search engine X  X  daily workload. For each query, the query log records the query terms along with the top-k matching results. All of our experiments are con-ducted on a very large Hadoop cluster. We do not provide details on our computing platform and the software we de-veloped, as this potentially deserves a separate study on its own.

Our experimental methodology divides the URL space into different sets, based on URL X  X  discovery time in toolbar logs and by the crawler. We then observe, at certain check-point dates, the distribution of URLs in these sets to obtain lower and upper bounds on the potential gains in using the toolbar data. Specifically, we form the following four sets (Fig. 2).
Since the toolbar logs do not affect the URLs in the CO set, we focus our study mostly on the URLs discovered through the toolbar ( i.e. , the TO , TF , and CF sets). We note that only the URLs in the TO and TF sets have a potential for positive impact on the search engine.

Among the identified sets, the size of the TO set gives us an upper-bound on the portion of the Web discoverable through toolbar, but not through Web crawling. Moreover, the size of the TF set gives us a lower-bound on the fraction of URLs that can be discovered in toolbar logs before they are discov-ered by the crawler. These are only bounds because some portion of the URLs in the TO set may later be discovered by the crawler and move into the TF set.

In this section, as an initial experimental result, we report the sizes of the TO , TF , and CF sets, for our data. In this experiment, we select two sets of checkpoint days, sampled in different ways. The first set consists of the first seven days (a) Consecutive checkpoints. of September 2009 (starts from 203 days before the date of the crawl snapshot). The second set consists of the first day of each month from September 2009 to March 2010. For each checkpoint day, we identify all URLs crawled before the checkpoint day and the toolbar URLs obtained on that particular day. We then compute the URL distribution over the three sets at every checkpoint day.

Fig. 3 shows the fraction of URLs that fall in the three sets. We observe that more than 80% of the URLs accessed by users within a day are not discovered by the crawler ( TO ). This is a much higher number than what one may expect. However, in the next section, we will show through a user study that this is only a loose bound on the potential cov-erage of toolbar URLs. We also observe that about 2% of the URLs are first accessed through the toolbar ( TF ). If the checkpoint dates are consecutive (Fig. 3(a)), the fractions re-main stable. However, when we use checkpoints with enough time distance (Fig. 3(b)), the increase in the size of the CF set is evident. This is simply because the crawler is given more time to discover URLs. Nevertheless, there is still a large fraction of toolbar URLs that are not discovered early enough or at all by the crawler. More concretely, the URLs in the TO and TF sets of each checkpoint day count for about 0 . 02% of the full Web crawl we use. Given the huge size of our crawl (in order of hundreds of billions), this percentage corresponds to a large number of Web pages that are worth exploiting. We assess in this section the importance of URLs in the TO set. We quantify the fraction of URLs that are really useful for enhancing the crawler (Section 4.1) and analyze the reasons for which such URLs are not discovered by the crawler (Section 4.2). Since no information is available to the search engine about the content of the URLs in the TO set, the only option to evaluate the usefulness and discoverability of these URLs is to obtain their content and conduct a user study. To this end, we independently sample 10 sets of URLs from the TO set observed on September 1, 2009. Each sample set contains 1000 URLs selected uniformly at random.
As stated above, we first aim to understand through a user study whether the URLs that are only viewed by the users are useful for a crawler as a complementary source of in-formation. Our user study comprises two steps. In the first Avg. percentage 77.2 13.2 7.4 0.5 1.9 step, for each sample URL, we send an HTTP get request to its server. We place the URLs in different bins according to the HTTP status codes returned by their servers. This au-tomatic classification step gives us an idea about the reach-ability of the sample URLs. In this step, we filter out all URLs that are invalid or not accessible. In the second step, we download the remaining URLs and manually check their content to place them under different categories according to their usefulness.

Automated filtering. According to RFC 2616 5 , the status codes in the form of 2** indicate that the requests are successfully received, understood, and accepted; the codes in the form of 3** mean that the requested resource resides under another URL and the current URL is redirected to that URL; the codes in the form of 4** or 5** correspond to the client-side or the server-side errors, respectively. If a request fails to get any response, it is due to the fact that the requested URL is invalid, i.e. , the URL cannot be correctly parsed using the URL syntax. 6 This happens mainly due to misspellings introduced by users when typing URLs.

In Table 1, we provide the distribution of HTTP status codes obtained for our 10 sets of sample URLs. We observe that about 90.4% of the sample URLs are valid URLs that receive a 2** or 3** code. 7 This indicates that if the URLs in the TO set are queued for crawling, there is a high potential to increase the coverage of the crawler. The URL with status codes 4** and 5** are clearly not useful for the crawler. We thus filter out these URLs as well as those that cannot be parsed and focus only on the remaining URLs for the manual classification step, conducted next.

Manual classification. We classify the URLs under the following classes, according to the usefulness of their con-tent. In our classification, URLs can be uniquely assigned to classes, except for the Adult class. Hence, we assumed that the Adult class has higher precedence and a URL as-signed to it would not be assigned to any other class. Regular ( Regular ): These are pages that contain useful in-Private with useful content ( Private-Useful ): These
Hypertext Transfer Protocol  X  HTTP/1.1, http://www. rfc-archive.org/getrfc.php?rfc=2616.
Uniform Resource Locators (URL), http://www. rfc-archive.org/getrfc.php?rfc=1738. Although URLs with a 3** code may be redirects to known URLs, we make an optimistic assumption and include them in our manual classification. Private without useful content ( Private-Useless ): Web search engine results ( Search-Web ): These pages Database search results ( Search-DB ): In some online No content ( Blank ): These pages are simply those having Adult ( Adult ): These pages contain adult content. Al-Link not found ( Soft404 ): Some websites display cus-
We manually classify the sampled 1000 URLs in the set whose status code distribution is the closest to the average distribution over the 10 sample URL sets. This sample con-tains 905 valid URLs after the automated filtering step. Ta-ble 2 shows the distribution of URLs in the usefulness classes we defined. The distribution of URLs from a CO sample is shown to give a general idea of the usefulness of URLs. Not surprisingly, compared to the crawler, the users are more likely to access private pages that require login. However, there is still half of the pages that contain useful information and might be useful to the search engine.
We now analyze the characteristics of the useful URLs in order to understand why they are not discovered by the crawler. One obvious reason is the robot exclusion protocol, through which website owners can advisorily prevent crawl-ing of certain pages in their sites. To understand the impact of the robot exclusion protocol, we retrieve the robots.txt files from the server of every URL that is identified as use-Table 4: Fraction of static and dynamic URLs.
 ful. Then, we check whether the URL is excluded by the protocol or not. Table 3 displays the fraction of the use-ful URLs that are excluded or not excluded. According to the table, about one-third of the useful URLs are excluded and hence cannot be crawled ( +R ). Interestingly, most of the excluded URLs are in the Search-Web category. This is be-cause search engines use the robot exclusion protocol as a precaution to prevent their URLs from being scraped and stolen by the competitors.

We continue our analysis with the remaining two-thirds of useful URLs that are not excluded by the robot exclusion protocol. A very large number of Web pages are in the hidden Web, which are only accessible by submitting queries to a database. As a result, regular crawlers are unable to find these dynamic pages if there is no link pointing to them. Table 4 compares the fraction of static and dynamic URLs 8 in the TO and CO sets. The high variation in distributions confirms our claim that the toolbar data can enhance a Web crawler X  X  coverage by providing dynamic pages that are often not discoverable by the crawler.

Some commercial search engine crawlers are known to limit the number of query parameters in dynamic pages. In Fig.4(a), we compare the number of query parameters in dynamic URLs for the TO and CO sets. 10 Not surprisingly, most URLs in the CO set (90%) have no more than three pa-rameters. In contrast, the number of parameters are evenly distributed for the URLs in the TO set and 27% of such URLs have more than three parameters.
 Crawlers typically limit the depth of the URLs they follow. To observe if this has an impact on coverage, we compute the depth distribution for static URLs in the TO and CO sets. Fig. 4(b) confirms that toolbar is more effective for discovering the static URLs with depth not smaller than 5.
In this section, we assess the impact of toolbar data on the content quality of a search engine. We first evaluate the importance of the URLs in different sets. Next, we inves-
Note that this number is a lower bound for the proportion of dynamic URLs since some dynamic URLs do not have a query parameter and may appear to be static.
Dynamic versus static URLs, available at http: //googlewebmastercentral.blogspot.com/2008/09/ dynamic-urls-vs-static-urls.html
URLs without a parameter are dynamic URLs rewritten in the form of a static URL. Figure 4: Distribution of URL properties: (a) Dis-tribution of dynamic URLs, according to the num-ber of query parameters in the URL; (b) Distribu-tion of static URLs, according to depth measured by the number of slashes in the URL. tigate to which extent the URLs discovered in toolbar logs can affect the content discovery time.
We use two different proprietary metrics to assess the im-portance of pages. The first is a link analysis metric ( link metric ) that measures the importance of a page based on the number of links it receives from other pages. The second is a content quality metric ( content metric ) that uses the textual content of a page to decide on its importance. In both metrics, increasing values indicate higher importance.
Unfortunately, the metric values are available to us only for pages discovered by the crawler. Therefore, we focus on the URLs discovered only by the crawler ( CO ) and those dis-covered by both the toolbar and the crawler ( TF ). For these experiments, we sample about 15 million URLs, uniformly at random, from the CO set and use all URLs in the TF set obtained on September 1, 2009.

Fig. 5(a) shows the distribution of link metric values for the TF and CO sets. The average value of the link metric is 0.27 for the TF set, whereas it is 0.21 for the CO set. It is also seen that there are more URLs in the TF set with a larger link metric value. Fig. 5(b) shows the distribution of the content metric . We similarly observe that the URLs in the TF set have larger content metric values. 11 Know-ing that a crawler always crawls important URLs first, the fact that more URLs in the TF set have positive content metric indirectly confirms our observation. These two ex-periments indicate that the URLs discovered by both toolbar and crawler are more important than the URLs discovered only by the crawler, i.e. , there is an opportunity to discover high-quality content via user feedback.
We have seen that the URLs discovered both in toolbar logs and by the crawler are more important than those dis-covered only by the crawler. Yet, these URLs would be useful for improving the content system of a search engine only if they are discovered earlier in toolbar logs. We are thus interested in knowing how early a toolbar can discover such URLs ( i.e. , those in the TF set).

Fig. 6 illustrates the fraction of URLs discovered first in
The value of content metric is 0 if a URL has not yet been crawled. URLs with a value of 0 are omitted from Fig. 5(b). Figure 6: Fraction of URLs that were initially in the TO set (on September 1, 2009) and then moved into the TF set as they were discovered by the crawler. toolbar logs on September 1, 2009 and then discovered by the crawler before the date indicated on the x-axis. We observe that the fraction of URLs that moved from the TO set to the TF set increases linearly over time. By the date of our crawl snapshot, 2% of the URLs have moved from the TO set to the TF set. These URLs are certainly useful for improving the content system.

Herein, we further evaluate how long it takes for the crawler to discover a URL already discovered in toolbar logs. Fig. 7(a) compares the distribution of delay for the URLs in the TF set that are discovered on each day of the first week of September 2009. As expected, the distributions are similar. It is important to note that more than 60% of the URLs in the TF set are discovered by the toolbar at least 90 days earlier than the crawler. This result illustrates the potential in using toolbar logs to improve search engine coverage and to compute more accurate page importance values for the known URLs. Fig. 7(b) further compares the distribution of the delay for the URLs in the TF set that are discovered on the first day of each month from September 1, 2009 to March 1, 2010. The difference in the maximum delay is due to the fact that fewer days are available to the crawler to discover the same URLs after they are discovered in toolbar logs. Yet, even though the crawler had only 23 days to dis-cover the URLs first discovered in toolbar logs, 40% of the URLs in the TF set were discovered 7 days later.
Typically, after a URL is indexed, it starts to appear in the top-k result sets of queries to which the URL is considered to be relevant by the search engine. Obviously, a URL cannot be incorporated into any set of search results before it is Figure 7: Distribution of the delay in discovery of TF URLs by the crawler. discovered, crawled, and indexed. Consequently, some of the queries that could benefit from a URL cannot retrieve it until the URL is made available through the index. Herein, our goal is to estimate the impact of the discovery of TF and TO URLs on the result quality of a search engine, as well as to identify the type of queries for which the discovery of such URLs is important.
We evaluate the impact of the early discovered URLs, i.e. , those in the TF set, on the search result quality by answer-ing two questions: (i) given a set of queries, how many of them would retrieve a TF URL in their top-10 results, before these TF URLs are discovered by the crawler? We refer to such queries and TF URLs as benefited queries and beneficial URLs respectively; (ii) given a set of benefited queries, how frequently would they be issued before their beneficial URLs are discovered by the crawler? In the experiments, as the toolbar URLs, we use the TF URLs discovered on September 1, 2009. We also use queries from the daily query logs of the Yahoo! search engine. We rely on the Yahoo! search engine X  X  ranking mechanism to assess the relevance of a TF URL to a given query. Since a TF URL would not appear in the search results of any query before it is discovered by the Yahoo! Web crawler, we first identify a set of queries from the query logs which contain in their top-10 results at least one URL from the TF set, referred to as relevant queries , and then derive the benefited queries by comparing a given set of previously issued queries against them.

Fig. 8(a) displays the fraction of the relevant queries (ob-served on each day between April 1, 2010 and May 30, 2010) that contain in their top-10 results at least one URL from the TF set (obtained on September 1, 2009). The figure also provides the same information using unique relevant queries ( i.e. ignoring repetition). On average, 0.03% of queries is-sued during a day have at least one URLs from the TF set in their top-10 results and are relevant to our experiment. The number goes to 0.14% when the relevant queries are unique, indicating that the URLs in the TF set are more likely to appear in the results of infrequent queries.

The first question we aim to answer is the fraction of ben-efited queries among a given set of queries. To this end, we focus on the queries issued on September 1, 2009 and count the occurrences of the unique relevant queries shown in Fig. 8(a) during that day. We observe from Fig. 8(b) that the number of benefited queries increases linearly with the Figure 8: Distribution of queries related to TF URLs Figure 9: Distribution of benefited queries w.r.t. different sets of relevant queries length of observed period ( i.e. , x-axis days). If the period of 60 days is considered, up to 1.8% of the queries on Septem-ber 1, 2009 have a TF URL as a top-10 result, assuming those URLs are incorporated to the index once they are accessed via toolbar. In fact, this number is only a lower bound and more benefited queries should be expected if the relevant queries are obtained in a longer period. Note that this num-ber is much larger than the fraction of the unique benefited queries. This is due to the fact that given a specific day the probability for an infrequent query (having a TF URL in its top-10 results) to be issued is low.

Now, we address the second issue by measuring the occur-rences of the benefited queries issued on September 1, 2009, before all the corresponding beneficial URLs are discovered by the crawler, i.e. , during the period from September 1, 2009 to the date of our crawl snapshot. We consider three sets of relevant queries in this experiment. Each set con-sists of the unique relevant queries of a randomly selected observed day, as shown in Fig. 8(a).

Fig. 9(a) shows the total number of the benefited queries issued from September 1, 2009 to the x-axis day. We present absolute numbers instead of percentages to give an idea of the magnitude of the results. We observe that the number of benefited queries increases over time. Before all the TF URLs accessed via toolbar on September 1, 2009 are discovered by the crawler, the beneficial ones appear up to 60 million times in the top-10 results of the benefited queries. Fig. 9(b) shows the number of the unique benefited queries on the x-axis day. Issuing a query later reduces the probability that a TF URL appears in its top-10 results so that the number of unique benefited queries decreases with time.

Fig. 10 shows the distribution of benefited queries as a function of the crawler X  X  delay. Only the queries related to Figure 10: Query distribution for different delays. Benefited queries 22.09% 0.01%
Ordinary queries 6.16% 3.42% the first set of relevant queries are shown. Interestingly, the number of queries in each delay interval is independent of the length of the delay. In other words, even a small delay may significantly impact the query results. This observation confirms that discovering URLs earlier via toolbar impacts a large number of queries.
We have observed that the TF URLs help improving the search result quality of certain queries. We are now inter-ested in the characteristics of the benefited queries to un-derstand for which queries the TF URLs are crucial. To this end, we evaluate the content, the frequency and the length of the benefited queries, and compare them against those of ordinary queries. In this experiment, we use all the benefited queries issued during the period from September 1, 2009 to the date of our crawl snapshot with respect to the first set of relevant queries. All the queries issued during the same period are considered as ordinary queries.

We analyze the content of the queries by measuring the fraction of domain queries and question queries , which can be easily identified by matching the query terms with pre-defined keywords. Here, by domain query we mean a query that contains a domain name, such as  X .com X ,  X .edu X , etc. A question query is a query starting with an interrogative word, such as  X  X hat X ,  X  X hy X , etc. For each type of queries, we develop a complete list of relevant keywords and run an automatic classifier against it. Table 5 summarizes our ob-servation that a significant fraction of benefited queries are domain queries while very few queries look for specific infor-mation. Fig. 11 depicts the distribution of query frequency, following a power-law distribution. We observe that none of the benefited queries occurs more than 10 , 000 times during the observed period, while 90% of them occur more than once. In contrast, some ordinary queries appear up to 10 times during the same period. Table 6 shows the average length of queries. Not surprisingly, since there are much more domain queries among the benefited queries, they are shorter than the ordinary queries, on average. Generally, the URLs first discovered via toolbar are crucial for the do-main queries with medium frequency to retrieve the recently created pages earlier. Benefited queries 1.52 1.08
Ordinary queries 2.01 0.08
As we have seen, a large fraction of URLs discovered through toolbar logs are not discovered by the crawler but contain useful information. To understand the contribution of such URLs, i.e. , those in the TO set, to the search re-sults, we conduct the following experiments. We obtain the content of two million URLs, half of which are randomly sampled from the TO set on September 1, 2009 and the re-maining half are randomly sampled from our crawl snapshot. We index these URLs using the open source information re-trieval platform Terrier 12 and evaluate one million queries randomly selected from the query logs against this index. The relevance of a URL to a query is computed by summing the BM25 score of each query term. For each top-k result set retrieved for a query, we are interested in the number (fraction) of TO URLs in that set.

Fig. 12 displays the distribution of queries according to the number of TO URLs in their top-10 results. On average, 2.6 results out of 10 are from the TO set, confirming that incorporating the URLs discovered through toolbar logs into the search engine index positively impacts the search results. We infer from the different distribution of unique queries that TO URLs are more likely to appear in the top-10 results of infrequent queries, i.e. , queries looking for tail content. Finally, we repeat this experiment with different values of k and observe that the fraction of TO URLs in the top-k result sets remains stable, and 26.2% of the top-k results are from the TO set on average. This independence of k further confirms the impact of TO URLs on the runtime system of the search engine.
Web crawling is a well-studied topic [29]. In the litera-ture, there have been a significant number of design alter-natives, including sequential [24], parallel [5, 8, 20, 33, 36], and geographically distributed [6, 14, 15] Web crawlers. The three main quality objectives, common to most crawling ar-chitectures, were achieving high collection quality through download scheduling [10, 26], maintaining page freshness [7, 9, 16, 30, 35], and obtaining high Web coverage [11, 23]. http://terrier.org/ . Figure 12: Distribution of queries w.r.t. number of TO URLs in the top-10 result set.

Quality. A notable technique to improve the download content quality is to fetch the links in the frontier in de-creasing order of the predicted importance values [10, 26]. In former studies, the importance of a link mainly depends on the structure of the Web graph and is link-based [10, 13] ( e.g. , pages are prioritized based on their PageRank values). A more recent work [30] took into account the impact of a page on search result quality. In this metric, newly created pages and tail pages that have not received enough in-links but are likely to appear in the search results are given more priority to improve the utility of crawled pages.

Freshness. Keeping the page repository fresh requires refetching already downloaded pages. A number of studies investigated the evolutionary properties of the Web [2, 17, 27] and proposed page refreshing policies for crawlers [7, 9, 16, 30, 35]. Some policies aimed to refresh more often the content that is more important [7] or more likely to change [9, 12]. Others tried to maximize the impact on search result quality [16, 30, 35].

Coverage. To achieve a high Web coverage, a crawler needs to continuously discover new pages. Although out-dated, an analysis of the coverage of commercial search en-gines is available in [23]. The discoverability of newly created pages is intensively analyzed in [11]. Not surprisingly, this study showed that about a quarter of the new content is not easily discoverable by following the links in known pages.
Hidden Web. In practice, the size of the hidden Web [4, 32] is a limiting factor for the coverage of a Web crawler. The main challenge in hidden Web crawling is to discover entry points to hidden Web sites [3] and to sample them via automated form filling [18, 25, 28]. Moreover, the hid-den Web crawler should try to discover useful information without putting too much burden on sites.

Passive URL discovery. As we discussed in Section 2.2, a promising technique is to incorporate other sources of in-formation rather than solely relying on the crawler to dis-cover new pages. The client-side applications, like Web browsers and toolbars, appear to be good candidates for gathering such information. To the best of our knowl-edge, the only previous work exploring this idea is a recent patent [34], which proposes using search toolbar data to lo-cate and index multimedia resources that are not discover-able by Web crawlers. However, the patent is mostly about how multimedia resources can be identified and added to the search index, but does not give any evidence to which extent a search engine can benefit from passive URL discovery.
URL discovery through user feedback. A recent work [21] proposes using the URLs posted on social book-marking sites (in this particular case, del.icio.us) to improve Web search. The study reports that approximately 25% of URLs posted by users are new, not indexed pages. Since the existence of URLs in the search engine is checked over the index, 25% forms an upper bound on the URLs not discov-ered by the crawler. The same work states that the fraction of unique URLs obtained via social bookmarking represents a tiny fraction of the URLs on the Web. In this respect, our toolbar-based URL discovery approach has a much larger coverage. Moreover, our approach focuses on tail content rather than the popular content in social bookmarking. Fi-nally, we note that the work in [21] does not form an in-stance of passive URL discovery since the URLs in social bookmarking sites are accessible through link following.
Toolbar data. In [22], through an analyses of Yahoo! toolbar data, authors propose a new taxonomy for online browsing behavior of users. In [1], URL revisitation patterns of users is analyzed using URLs accessed by a toolbar. In [2], the change in Web content is studied by restricting the URLs to the toolbar data. All these studies use toolbar data for either characterization or filtering purposes, but do not give any clue about its potential use.
We conducted a large-scale study, using toolbar logs, a large Web crawl, and search query logs obtained from Ya-hoo!, to investigate the role toolbar data can play in im-proving the content and the runtime systems of a search engine. Through a user study, we showed that there exist a large portion of useful URLs that are not discovered by the crawler, but are accessed by the users via Web browsers. We then showed that, on average, URLs accessed by the users are more important than those found in the reposi-tory of a crawler. We found that the crawler has a signifi-cant delay in discovering URLs that are first accessed by the users. Finally, we have shown that URL discovery via tool-bar have positive impact on search result quality, especially for queries seeking recently created content and tail content.
There are a few potential extensions to our work we envi-sion. One extension is to repeat our study with other forms of passive URL discovery ( e.g. , email messages, communica-tion logs of Internet messengers). An interesting yet chal-lenging example of passive URL discovery is to delegate the discovery process to the network itself ( e.g. , agents on Inter-net routers or ISPs). This way URLs can be discovered at a much larger scale with almost no delay. Another extension is to perform passive crawling in a privacy-preserving fashion, since proper anonymization of URLs is important to prevent search results from leaking confidential user information. This work has been partially supported by the COAST project (ICT-248036), funded by the European Community. [1] E. Adar, J. Teevan, and S. T. Dumais. Resonance on [2] E. Adar, J. Teevan, S. T. Dumais, and J. L. Elsas. The [3] L. Barbosa and J. Freire. An adaptive crawler for [4] M. K. Bergman. White paper: The deep web: [5] P. Boldi, B. Codenotti, M. Santini, and S. Vigna. [6] B. B. Cambazoglu, V. Plachouras, F. Junqueira, and [7] J. Cho and H. Garcia-Molina. The evolution of the [8] J. Cho and H. Garcia-Molina. Parallel crawlers. In [9] J. Cho and H. Garcia-Molina. Effective page refresh [10] J. Cho, H. Garcia-Molina, and L. Page. Efficient [11] A. Dasgupta, A. Ghosh, R. Kumar, C. Olston, [12] J. Edwards, K. McCurley, and J. Tomlin. An adaptive [13] N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking [14] J. Exposto, J. Macedo, A. Pina, A. Alves, and [15] J. Exposto, J. Macedo, A. Pina, A. Alves, and [16] D. Fetterly, N. Craswell, and V. Vinay. The impact of [17] D. Fetterly, M. Manasse, M. Najork, and J. L. Wiener. [18] A. d. C. Fontes and F. S. Silva. SmartCrawl: A new [19] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking [20] A. Heydon and M. Najork. Mercator: A scalable, [21] P. Heymann, G. Koutrika, and H. Garcia-Molina. Can [22] R. Kumar and A. Tomkins. A characterization of [23] S. Lawrence and C. L. Giles. Accessibility of [24] H.-T. Lee, D. Leonard, X. Wang, and D. Loguinov. [25] J. Madhavan, D. Ko, L. Kot, V. Ganapathy, [26] M. Najork and J. L. Wiener. Breadth-first crawling [27] A. Ntoulas, J. Cho, and C. Olston. What X  X  new on the [28] A. Ntoulas, P. Zerfos, and J. Cho. Downloading [29] C. Olston and M. Najork. Web crawling. Found. [30] S. Pandey and C. Olston. User-centric web crawling. [31] F. Radlinski and T. Joachims. Query chains: Learning [32] S. Raghavan and H. Garcia-Molina. Crawling the [33] V. Shkapenyuk and T. Suel. Design and [34] H. E. Williams. Discovering web-based multimedia [35] J. L. Wolf, M. S. Squillante, P. S. Yu, J. Sethuraman, [36] D. Zeinalipour-Yazti and M. D. Dikaiakos. Design and
