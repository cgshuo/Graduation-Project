 on a movie might be one of do-not-bother , only-if-you-must , good , very-good , and eral multiclass classification.
 a common property: they are modified from well-known binary classification approaches. tremendous efforts in theoretical analysis.
 sets validate the usefulness of our framework in practice.
 Section 5, and conclude in Section 6. is then defined as where C is a K  X  K cost matrix with C Naturally we assume C a small C ( r, P ) .
 an example ( x , 4) with r prediction than the rule r C A simple C with V-shaped rows is the classification cost matrix , with entries C matrix , which is defined by C preference. Its rows are not only V-shaped, but also convex . That is, C classification problem. Some of the results may require the rows to be convex. 2.1 Reducing ordinal regression to binary classification not be easily analyzed.
 rank, and generalization analysis can immediately follow. Assume that f would be f a reasonable ranking rule based on the binary answers is r ( x ) = y 0 = 1 + min { k : f Equivalently, Although the definition can be flexibly applied even when f usually desired in order to introduce a good ranking rule r . puts. That is, when k is farther from the rank of x , the answer f with f answers f monotonic, a rank-monotonic f is usually desired.
 the cost of the ranking rule r on an example ( x , y ) is Define the extended examples ( x ( k ) , y ( k ) ) with weights w Because row y in C is V-shaped, the binary variable y ( k ) equals the sign of ( C latter is not zero. Continuing from (2), proved from The inequality above holds because ( C are exactly ( r ( x )  X  1) zeros and ( K  X  r ( x )) ones in the values of all training examples ( x as from f using (1). The cost bound in (4) leads to the following theorem. Theorem 1 (reduction) An ordinal regression problem with a V-shaped cost matrix C can be re-2.2 Thresholded model popular approach to obtain such a function f is to use a thresholded model [1, 4, 5, 7]: As long as the threshold vector  X  is ordered , i.e.,  X  thresholds? X  A mild but sufficient condition is shown as follows. cation algorithm minimizes the loss P switching  X  with y value by Because ` (  X  ) is non-increasing, the change is non-positive. For an example with y Note that row y is non-increasing, the change above is also non-positive. The case for examples with y similar and the change there is also non-positive.
 shows that ( g  X  ,  X   X  ) is also optimal.
 change (6) is strictly negative. Thus, the optimal  X   X  for any g  X  is always ordered. denotes the k -th row of E , is appended after x . We will mostly work with E =  X  I a positive scalar and I 3.1 Perceptron-based algorithms regression algorithm that employs the thresholded model with f ( x , k ) =  X  u , x  X   X   X  was also proposed [1].
 With the simple encoding scheme E = I other perceptron algorithms, such as a batch-mode algorithm rather than an online one. 3.2 SVM-based algorithms extended kernel as the original kernel plus the inner product between the extensions, the coding matrix E defined with e When E =  X  I classifier f ( x , k ) has the form  X  u ,  X  ( x )  X   X   X  The explicit (SVOR-EXP) and implicit (SVOR-IMC) approaches of Chu and Keerthi [4] can be SVOR-IMC is implied from Theorem 2. In addition, they found that SVOR-EXP performed better classification cost and SVOR-IMC comes from using the absolute cost. the approaches of Chu and Keerthi in practice. lems with any cost matrix. A simple result that comes immediately from (4) is: Theorem 3 (reduction of generalization error) Let c contains the encoding of ( x , k ) and Y is a binary label, such that P ROOF We prove by constructing  X  P . Given the conditions, following (4), we have where P from P ( x , y ) and P the margins of the associated binary classifier f i.e., for all n , the margins y ( k ) data-dependent bounds, see the work of, for example, Bartlett and Shawe-Taylor [10]). classifier (see the work of Lin and Li [7] for one of such bounds). Theorem 4 (data-dependent bound for support vector ordinal regression) Assume that If every rank rule r based on f has generalization error no more than P One way to extract such a subset is to choose independent k The subset would be named T = ( x ( k n ) i.i.d. outcomes from  X  P , which is the case of T , Let b variable has mean c  X  1 when each b The desired result can be obtained by combining (8), (9), and (10) with a union bound. matrix, we can fairly compare our results with those of SVOR-IMC [4]. We tested our framework with E =  X  I also implemented SVOR-IMC with the perceptron kernel and the same parameter selection proce-dure in LIBSVM.
 SVM-perceptron is significantly better than the other two.
 Within the three SVM-based approaches, the two with the perceptron kernel are better than SVOR-IMC with the Gaussian kernel in test performance.
 Our direct reduction to the standard SVM performs similarly to SVOR-IMC with the same perceptron kernel, but is much easier to implement. In addi-tion, our direct reduction is significantly faster than SVOR-IMC in training, which is illustrated in Fig-cause to the time difference is the speedup heuris-tics. While, to the best of our knowledge, not much algorithms. structed from our framework with the state-of-the-art SVOR-IMC algorithm. Acknowledgments Fellowship, and Hsuan-Tien Lin was supported by the Caltech EAS Division Fellowship. References
