 TheMultinomialNaiveBayes(MNB)model[1,2,3,4]hasanumberofattrac-tive features for most text classification tasks. It is simple and can be trivially scaled for large numbers of classes, unlike discriminative classifiers. It is generally robust even when its assumptions are violated. Being a probabilistic model, it is very easy to extend for structured modeling tasks, such as multi-field documents and multi-label classes. The main limit to the use of MNB is its strong modeling assumptions. The same simplifying assumptions that make it efficient and reli-able also result in a severe performan ce gap when compared to discriminative classifiers such as Support Vector Machines [5, 6, 7] (SVM).

In practice many applications of MNB use modifications for correcting the strong assumptions, requiring parameters external to the actual model, also called metaparameters. An example of this is the mandatory MNB modifica-tion: smoothing of the class-conditional multinomials. Due to the data sparsity problem in natural language, maximum-likelihood estimates for words will result in zero-estimates and correct ing this effectively requires a parameterized smooth-ing technique. Combinations of modifications to MNB has been suggested as a high-performing solution to text classification [2, 4]. This however results in a number of interacting metaparameters, making simple grid searches insufficient to optimize the resulting  X  X lack-box X  or direct search problem.

With the arrival of both multi-core processors and cloud computing it has become convenient to optimize machine l earning systems by using parallel direct search algorithms. This enables optimization of all metaparameters concurrently, with respect to any measure of system pe rformance. Recently random search methods have been advocated for optimi zing the metaparameters required by machine learning systems [8]. Random searches offer a paradigm of direct search that makes few assumptions and is ideally suited to the noisy, multi-modal and low-dimensional task encountered here.

In this paper we explore the use of random search methods for optimizing modifications to MNB. We evaluate a total of 7 metaparameters to improve MNB performance with 4 different rando m search algorithms, including a mod-ified random search proposed in this paper. We compare the results to similarly optimized SVM classifiers across five different datasets. In addition, comparison results are made to unoptimized classifiers and to the individual modifications.
The paper is organized as follows. Sectio n 2 presents MNB, feature transforms and the evaluated modifications. Sectio n 3 presents the random search methods. Section 4 presents results from the optimi zation experiments on the datasets and Section 5 completes the paper with a discussion. 2.1 Model Definition MNB is a generative model; a model of the joint probability distribution p ( w ,c ) of word count vectors w =[ w 1 , ..., w N ] and class variables c :1  X  c  X  M , where N is the number of possible words and M the number of possible classes. Bayes classifiers use the Bayes theorem to factorize the generative joint distribu-tion into a class prior p ( c )anda class conditional p ( w | c ) models with separate tional assumption that the class conditional probabilities are independent, so that p ( w | c )  X  n p ( w n ,n | c ). MNB parameterizes the class conditional proba-bilities with a Multinomial distribution, so that p ( w n ,n | c )= p ( n | c ) w n .
In summary, MNB takes the form: where p ( c ) is Categorical and p ( w | c ) Multinomial.

The main strength of MNB is its scalability. Training a MNB is done by summing the counts w n found for each pair ( c, n ) in training documents and normalizing these over n to get p ( n | c ). Omitting the normalization, this has the time complexity O ( De ( s ( w ))), where D is the number of training documents and e ( s ( w )) is the average number of unique words per document. Classification is done by choosing the class c to maximize p ( w ,c ). This has the time complexity O ( s ( w ) M ), where s ( w ) is the number of unique words in the document. 2.2 Feature Normalization Feature normalizations such as TF-IDF [9] have been successfully used to correct some of the multinomial data assumptions with MNB [2, 3, 4]. A variety of TF-IDF functions exist in information retrieval literature. The generic version of TF-IDF used here takes the form: where w u n is the unsmoothed word count, D n the number of training documents the word n occurs in, and D the number of training documents.

The first factor in the TF-IDF function performs unique length normalization [10] and word frequency log transform. Length normalization corrects for varying document lengths. Other common choices for length normalization are word count and cosine length normalizations, but unique length normalization is more consistent over varying datasets [10]. Th e word frequency log transform corrects for the  X  X urstiness X  effect in word counts, by damping high counts to better fit the Multinomial distribution assumed by MNB [2]. The second factor in the TF-IDF function performs IDF weighting of words. This gives more weight to informative words, causing the weighted counts to better separate the classes. 2.3 Modifications for MNB The MNB model as defined in Equation 1 is simple, robust and efficient. It however suffers from a number of incorr ect assumptions when applied to text classification, and in the following we introduce modifications that have been used in generative modeling. For optimization, the required metaparameters are represented using a vector of scalar values a . The original unmodified probabil-ities are denoted p u . All modifications listed next can be turned off by setting the associated metaparameter to a non-influencing value.  X  a 1 Length scaling (length scale)  X  a 2 IDF lifting (idf lift)  X  a 3 Conditional uniform smoothing (cond unif)  X  a 4 Conditional background smoothing (cond bg)  X  a 5 Prior scaling (prior scale)  X  a 6 Prior uniform smoothing (prior unif)  X  a 7 Evaluation list pruning (list prune) In virtually all applications of machine learning some metaparameters are used, most commonly heuristic values used fo r feature normalization. When only a few metaparameters are in volved, simple methods such as grid searches can be used for calibration. But as the systems become more complex, the performance becomes a function of metaparameters with complex interactions. It is then not guaranteed that grid searches or heuristic values will give a realistic measure of calibrated performance. A solution t hat has been proposed is direct search with random search methods to optimize the performance [8]. In this paper the metaparameter vector a is optimized using four basic random search methods.
Direct search optimization [15, 16] deals with optimization of an unknown function f , when values for the function can be computed with point evalua-tions f ( a ). Since the function f is unknown, it can only be approximately op-timized. The optimization algorithm of choice for a particular problem depends on many factors, most importantly the amount of points that can be computed and any simplifying assumptions that can be made about the function, such as unimodality and smoothness. As direct search problems are encountered in so many different fields, the literature on the subject is fragmented and consists of hundreds of extensively researched approaches to the problem. Even the problem definition itself goes under different names, including black-box optimization and metaheuristics. Hansen [17] provides a recent evaluation of the most advanced optimization algorithms over a variety of synthetic test functions.

Random search [18, 19] algorithms are an approach to direct search that doesn X  X  rely on strong assumptions about the function. In random searches the function is randomly sampled, guided by information from previous points. This is well suited for low-dimensional optimization, where a small number of ran-domly generated points can consistentl y improve the function value. Not relying on strong assumptions makes the algorithms more robust when used in the very multi-modal and noisy functions encountered with real-word optimization problems.

In random search the current best point a is improved by generating a new point d = a +  X  with step  X  and replacing a with d if the new point is as good or better, if f ( a )  X  f ( d ), a = d . The iterations are then continued to a maximum number of iterations I . Many variations of this basic random search exist. An important special case is Pure Random Search, that makes no assumptions and replaces the step generation by uniform sampling of the value ranges. Adaptive Random Searches [19] use stepsizes g t adapted heuristically according to past points. Steepest Ascent Hill-climbing and other parallelized random searches generate J points per iteration and each point d j = a +  X  j is considered for replacement. Evolution Strategies takes this further, by maintaining a set of Z best points a z instead of a single point a . Further refinement of this with recombination of best points leads to Continous Genetic Algorithms.

In this paper we evaluate optimization with four variants of parallelizable random search methods. The first one is Pure Random Search (prs), sampling the value ranges of the parameters uniformly, currently advocated for optimizing machine learning systems [8]. The second one is a parallelized Random Search (rs), using multivariate Gaussian distribution for sampling points and a log-curve stepsize decrease, by multiplying stepsizes g t by 0.9 after each it eration. Random search with Gaussian step generation is also known as Random Optimization, whereas the log-curve stepsize decrea ses are very a common strategy used in optimization algorithms, including neural network training algorithms and the Luus-Jaakola random search. The third random search we use is a modifica-tion proposed in this paper called Modified Random Search (rs+), using four heuristics that are useful with random search methods. As the fourth method we evaluate Covariance Matrix Adapted Evolution Strategies (cmaes) 1 ,astate of the art random search method that forms the basis for many of the lead-ing algorithms for hard optimization problems [20, 17]. This extends Evolution Strategies by using successful steps to adapt the directions of step generation.
The modified random search rs+ proposed here combines four heuristics: multiple best points, adaptive stepsizes, Bernoulli-Lognormal sampling, and mirrored step directions:  X  Multiple best points  X  Adaptive stepsizes  X  Bernoulli-Lognormal sampling  X  Mirrored step directions The Bernoulli-Lognormal sampling is novel in this paper, whereas the other heuristics are commonly found across different optimization algorithms. For all optimization algorithms heuristic initial values a 1 are chosen and each meta-parameter is constrained to a permissible range d jt = min ( max ( a 1+ j % Z +  X  jt ,min t ) ,max t ). 4.1 Experiment Setup Five standardized and publicly available datasets 2 [22] were used for the exper-iments. These are R8, R52, WebKb, 20Ng and Cade. To cope with the limited training data for these datasets, random 5-fold segmentation was used to split the training data, reserving 200 documents per fold for a development set. This sums to 1000 documents per dataset for optimizing Accuracy. For all experiments 50x40 (50 iterations with 40 points) random searches were used for optimiza-tion. Paired one-tailed t-tests with p&lt; 0 . 05 were used to test for significance of evaluation set results.
 TF-IDF was used in all experiments to normalize the features. The Multiclass SVM provided by Liblinear toolkit 3 [7] was used for the SVM comparison. This is based on multiclass formulation of the SVM classifier [5, 6]. The SVM C-parameter was optimized together with the TF-IDF transform for the SVM results. 4.2 Experiment Results We first evaluated the optimization algorithms, optimizing the 7 metaparameters for MNB and 3 for SVM. Figure 1 shows the mean Accuracies ac ross the training dataset folds for each iteration of optimization. Table 1 shows the corresponding evaluation set accuracies and a baseline  X  X one X  result from using unoptimized classifiers. For smoothing the unoptimized classifiers the SVM C-parameter was set to 1 and the MNB cond bg-parameter was set to 0 . 1.
The mean differences between the optimiz ation algorithms would suggest that the local random searches rs and rs+ work better in the higher-dimensional MNB optimization task. However, these diffe rences between the algorithms are not statistically significant. Differences between the classifier configurations are sig-nificant and substantial. On average the use of optimized modifications improves MNB performance by 4 . 18 percent absolute, or over 20% relative ( p =0 . 010). For SVM the corresponding improvement is smaller 1 . 08 and nonsignificant. SVM is significantly more accura te for both unoptimized ( p =0 . 009) and optimized clas-sifiers ( p =0 . 036). The use of optimization reduces the mean difference between SVM and MNB performance by over 60% relative.

Given the large improvement in MNB performance from the modifications, it would be useful to know which modifications were causing this. As a second experiment, we altered the MNB rs+ co nfiguration by fixing each of the meta-parameters to a default non-influencing value. The model optimizations were otherwise done the same way as in the first experiment. This measures the influ-ence of each modification separately, while taking modification interactions into account. Figure 2 shows the shows the t raining set mean A ccuracies and table 2 shows the resulting evaluation set Accuracies.
Comparing the results in table 2 shows that two of the modifications have substantial and significant eff ects on MNB performance: cond unif ( p =0 . 003) and prior scale ( p =0 . 016). Three other modifications have small but non-significant effects: idf lift, cond bg and length scale.
 Earlier work in MNB text classification has suggested combination of modifica-tions [4] and used combination with heuristic values [2]. This paper evaluated the combination of 7 basic modifications and proposed the use of direct search algorithms for this task. Averaged over the evaluation sets of 5 standard text clas-sification datasets, a relative error reduction of over 20% was achieved compared to baseline MNBs. The performance gap to similarly optimized SVM classifiers was reduced by over 60%. This raises the question whether generative modelling for text classification could be improved to the same level of performance as discriminative classifiers.

Two of the modifications had large and significant effects on MNB perfor-mance. Both scaling of prior probabilities and uniform smoothing of multino-mials increased accuracy by over a per cent absolute. It should be noted that neither modification is generally used for MNB text classification. Three of the other modifications had small effects as well, but it will take further experiments to see how significant these are. Taking into account earlier work in two-stage smoothing of IR language models [13] and in optimized TF-IDF transforms [11], it is likely these modifications are beneficial. Future research could include mod-ifications such as smoothing by class clusters, as well as the common Dirichlet prior and absolute discounting methods.

Four different random search algorithms were explored to optimize the com-bination. While the differences between th e algorithms were nonsignificant, each of them performed succesfully at this task . Taking the decades of extensive work in the field of optimization into account, it can be said that the prs strategy is preferred when no assumptions can be made about the function. Once assump-tions can be made about the optimization problem, more elaborate algorithms such as cmaes and the proposed rs+ are more likely to perform better. It is also likely that complex optimizers such as cmaes will excel once the number of optimized parameters is sufficiently large. However, in practice the best way to improve optimization is simply removing unnecessary parameters and selecting the permissible value ranges correctly.

Optimization with direct search has some similarities to discriminative model training, although not many. Both optimize a measure related to classification performance instead of likelihood. Direct search works on the small set of model metaparameters, whereas discriminative training optimizes the actual model pa-rameters. Optimized metaparameters such as those for smoothing and feature transforms can potentially transfer to similar datasets, whereas the same does not apply to the outcome of discriminative training. An important question left for further research is comparison of discriminative training to direct search optimization.

The use of optimized modifications and direct search algorithms can be highly useful in constructing generative models of text for uses other than text clas-sification. In text modeling many potential methods such as decision trees and mixture models require additional metaparameters to work optimally. In future research we will attempt direct search algorithms for integrating more complex statistical models into text modeling.

