
In many practical problems, a learning task can usually involve multiple related tasks. Solving those related tasks together is expected to be more advantageous than solving them independently because the knowledge from different tasks can help to improve the generalization ability of the classifier constructed on each individual task. In recent years, the problem of learning multiple related tasks (usually re-ferred to as multi-task learning ( MTL )) has been investigated extensively [2][3][11][15][4][16], and the related approaches have been applied to a variety of application areas, such as marketing [5], computer vision [19] and bioinformatics[9].
Clearly, the exploration of task relationships is at the heart of multi-task learning. Many algorithms have been proposed to address this issue. Sheldon [18] and Kato et al. [15] assume task relationships are known beforehand; Argyriou et al. [3] assumes the data associated with those tasks share some common latent features and derive an optimization strategy to obtain these features; Evgeniou and Pontil [11] suppose that the classifiers of all these related tasks share a common part of parameters; Allenby and Rossi [1] and Arora et al. [5] applied a hierarchical Bayesian model with a common prior for the classifier parameters of all tasks.
As another research line, semi-supervised learning ( SSL ) [8] provides an alterative way to improve the classification performances. SSL assumes we are given a partially labeled data set, and it makes inferences by making use of both labeled and unlabeled points [2], [16][21][22]. There are also some works that integrate MTL and SSL together, where they assume that there are some labeled and unlabeled points our method.
 In the multi-task setting, we are given a set of data points X = { x t ( t =1 , 2 ,  X  X  X  ,m ) . We denote by I t the set of indices of the data points associated to the task t . For semi-supervised multi-task learning, we assume that for each task t we have l labeled points, and we denote the indices of the labeled samples in task t as I L t , the indices od the unlabeled sam-we denote its label by y i . For convenience, we assume that  X  i  X  of x in task t can be determined by sign ( f ( x )) . We denote by W =[ w 1 , w 2 ,  X  X  X  , w m ]  X  R d  X  m the matrix whose columns are the successive vectors we want to estimate.
We use square loss to measure the prediction quality of be Then the total prediction loss becomes We define the data matrix associated with the t -th task as X write w t as a linear combination of the and data points in task t , i.e., cient vector. Define the label truncated identity matrix for task t as J t  X  R n t  X  n t , which is a diagonal matrix with and the initial label vector for task t as y t  X  R n t  X  1 with We further define four concatenated matrices where G t = X t X t is the Gram matrix for task t . Then we have B. Inter-Task Regularization
Besides exploring the information within each task, an-other important issue is how to explore the task relationships in multi-task learning. Some previous works assume that the inter-task relationships are known beforehand [18][15]. In this paper, we will not make such assumption and we will design a paradigm to explore such information auto-matically. Specifically, we assume there are hidden clusters among those tasks, and the tasks in similar clusters should share similar weight vectors. Using the k-means criterion, assuming the tasks can formulate k clusters, we can write the objective where T t represents the t -th task,  X  i denotes the i -th task cluster. w i is the mean classification vector of the i -th task cluster.
 Now we introduce an m  X  k task cluster indicator matrix E with its ( i, j ) -th entry Let Then Eq.(20) can be rewritten as C. Semi-Supervised Multi-Task Learning with Task Regular-izations
Combining all things together, we can derive the following objective for semi-supervised multi-task learning:
J = J which is convex with respect to both H and M .Nowwe will analyze the constraints on H and M .  X  The constraint on M is a bit more complicated because Eq.(28) 1 , where M  X  should be computed by Considering the eigenvalue decomposition of M : where  X  = diag (  X  1 , X  2 ,  X  X  X  , X  m )  X  R m  X  m is a diagonal matrix with  X  i  X  X  the eigenvalues of M . Then where O m is a set of orthogonal matrices in R m  X  m .We further consider the singular value decomposition ( SVD )of XH : Then when  X  = V , then tr ( XHMH X )= tr ( U X  X  X U )= Therefore Then problem (30) becomes an eigenvalue optimization problem as which is a linear programming problem that can be solved efficiently. After we get the optimal  X  i  X  X , we can get M  X  and proceeds with the gradient calculation. Note that since H has a special block-diagonal structure, we only need to update the non-zero entries during the gradient descent process. E. Kernelization
In this section, we will present the kernelized form of our algorithm. Specifically, we assume that there are some nonlinear map  X  : R d  X  F that maps the data points from the space R d to some high-dimensional (possibly infinite) feature space F , and then we will run our task regularization method in F .Let A. A Toy Example We first construct a toy example to evaluate our method. There are totally 6 tasks with binary classes. The data from each class was generated from a Gaussian mixture model ( GMM ). For Tasks 1, 2, and 3 the GMM param-eters for class one (denoted with a blue star) are drawn from a three-component mixture defined as follows. Mix-two-dimensional means: (1 , 1) , (3 , 3) and (5 , 5) ; and re-spective covariance matrices  X  1 = 3 . 00 . 0 0 . 03 . 0 and  X  3 = in Tasks 1, 2, and 3 are drawn from a single Gaussian with mean (2 . 5 , 1 . 5) and diagonal covariance with symmetric variance 0.5 . For Tasks 4, 5 and 6 the GMM parameters for class one (denoted with blue star) are drawn from a three-component mixture defined as follows. The three mixture covariances are  X  2 ,  X  3 ,  X  1 . In Tasks 4, 5, 6 the data for class two are drawn from a single Gaussian with mean (2 , 3) , and diagonal covariance with symmetric variance 0.5. Besides our method, we also implemented the Multi-Task Parameterized Neighborhood-Based Classification ( MTPNBC ) method [16], the Supervised Cluster-based Multi-Task Learning ( SCMTL ) method [13] and the Single-Task Gaussian Random Field ( STGRF ) [23]. Note that MTPNBC is a semi-supervised multi-task learning method with probabilistic classifiers, SCMTL is a supervised multi-task learning method, and STGRF is a semi-supervised single-task learning method. For our Semi-Supervised Multi-Task Learning with Task Regularization ( SSMTTR ) method, the regularization parameters  X  1 and  X  2 are all set by 5-fold cross validation from the exponential grid 2 [  X  4:1:4] .The graph Laplacian for each task is constructed in the same way as in [23]. For the Kernel SSMTTR ( KSSMTTR ) method, we just use the Gaussian kernel with its width set by 5-fold cross validation from 2 [  X  4:1:4] .

Each curve in Figure 2(a) represents the mean Area Under the ROC Curve ( AUC ) score over 50 independent trials as a function of the number of labeled data, from which we can clearly observe the superiority of our methods. To gain a better understanding of the clustered structure that SSMTTR finds for the six tasks, we plot the Hinton diagram [12] of the between-task sharing matrix found by the Linear SSMTTR ( LSSMTTR ), averaged over the 50 trials. The ( i, j ) -th element of sharing matrix is equal to exp(  X  w the Hinton diagram, with a larger square indicating a larger value of the corresponding element. The Hinton diagram in Figure 2(b) also shows the agreement of the sharing mechanism of the SSMTTR with the similarity between the tasks. on the remaining data. The AUC averaged over the 19 tasks is presented in Figure 3(a), as a function of the number of labeled data, where each curve represents the mean calcu-lated from the 100 independent trials (the parameters in our algorithms are also set by cross validation from the same grid as the experiments in last section). The results of MTPNBC, SCMTL and STGRF are also reported, which shows that our algorithm outperforms the competitors significantly. The Hinton diagram of the between-task sharing matrix found by the Linear SSMTTR ( LSSMTTR ), averaged over the 100 trials, is shown in Figure 3(b), where the ( i, j ) -th element of sharing matrix is equal to exp(  X  w i  X  w j 2 / 2) .From the figure we can easily discover the task-cluster structure.
In this paper we propose a novel semi-supervised multi-task learning algorithm based on task regularizations. Our algorithm assumes that the multiple tasks form several task clusters and the tasks in the same cluster will share similar classification vectors. After relaxations, we show that our algorithm can be cast in to a convex optimization problem and can thus be efficiently solved. Finally the experimental results on both toy and real world problems are presented
