 Unlabeled high-dimensional text-image web news data are produced every day, presenting new challenges to unsuper-vised feature selection on multi-view data. State-of-the-art multi-view unsupervised feature selection methods learn pseudo class labels by spectral analysis, which is sensitive to the choice of similarity metric for each view. For text-image data, the raw text itself contains more discrimina-tive information than similarity graph which loses informa-tion during construction, and thus the text feature can be directly used for label learning, avoiding information loss as in spectral analysis. We propose a new multi-view un-supervised feature selection method in which image local learning regularized orthogonal nonnegative matrix factor-ization is used to learn pseudo labels and simultaneously robust joint l 2 , 1 -norm minimization is performed to select discriminative features. Cross-view consensus on pseudo labels can be obtained as much as possible. We system-atically evaluate the proposed method in multi-view text-image web news datasets. Our extensive experiments on web news datasets crawled from two major US media chan-nels: CNN and FOXNews demonstrate the efficacy of the new method over state-of-the-art multi-view and single-view unsupervised feature selection methods.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Fea-ture Evaluation and Selection Multi-View Unsupervised Feature Selection
Reading web news articles is an important part of peo-ple X  X  daily life, especially in the current  X  X ig data X  era that we are facing a large amount of information every day due to the advancement and development of information technol-ogy. One ideal way is to automatically group the web news per their content into multiple clusters, e.g., technology and health care, then one can choose to read the latest and the most representative news articles in a group of interest. This procedure can be done recursively so that one can explore the news in different resolution hierarchically. Clustering web news is also an effective way to organize, manage, and search news articles. Unlike traditional document cluster-ing, images play an important role in web news articles as is evident from the fact that almost all news articles have one picture associated. How to effectively and efficiently group web news articles of multiple modality is challenging because different data types have different properties and different feature spaces and also because the dimensionality of feature spaces is usually very high. For example in text feature space, the vocabulary size can be over a million. Be-sides, there are a lot of unrelated and noisy features which oftenleadtolowefficiencyandpoorperformance.

Multi-view unsupervised feature selection is desirable to solve the problem mentioned above, since it can select most discriminative features while considering the consensus from data of multiple views in an unsupervised fashion. Fea-ture size can be extremely reduced and feature quality can be greatly enhanced. As a result, not only computation can be more efficient but clustering performance can also be greatly improved. However, not much work have been done to be able to solve this problem well, especially for multi-view clustering on web news data. State-of-the-art un-supervised feature selection methods [ 2, 13]formulti-view data use spectral clustering across different views to learn the most consistent pseudo class labels and simultaneously use the learned labels to do feature selection. More specif-ically, Adaptive Unsupervised Multi-view Feature Selection (AUMFS) [ 2] uses spectral clustering on a combined data similarity graph from different views to learn the labels that have most consensus across different views, and then use l 1 -norm regularized robust sparse regression to learn one weight matrix for all the features of different views to best approximate the cluster labels. [ 13] presents a new unsuper-vised multi-view feature selection method called Multi-View Feature Selection (MVFS). MVFS also uses spectral clus-tering on the combined data similarity graph from different views to learn the labels, but learn one weight matrix for each view to best fit the learned pseudo class labels by joint squared Frobenius norm (fitting term) and l 2 , 1 -norm (rowise sparsity-inducing). Both [2] and [ 13] share the disadvantage that they X  X e sensitive to the combined data similarity graph, especially when there are quite a number of unrelated and noisy features in the feature space, and there is information loss during graph construction.

We propose to directly utilize raw features in the main view (e.g., text for text-image web news data) to learn pseudo cluster labels which should also have the most consensus with other views (e.g., image), and meanwhile the discrimi-native features in the feature selection process will win out to contribute more on label learning process, and in return the improved cluster labels will help to select more discrimi-native features for each view. Technically, we propose a new method called Multi-View Unsupervised Feature Selection (MVUFS) to do unsupervised feature selection for multi-view clustering, especially focused on analyzing text-image web news data. We propose to minimize the sum of regular-ized data matrix factorization error and data fitting error in a unified optimization setting. We use local learning regu-larized orthogonal nonnegative matrix factorization to learn pseudo cluster labels and simultaneously learn rowise sparse weight matrices for each view by joint l 2 , 1 -norm minimiza-tion guided by the learned pseudo cluster labels. The label learning process and feature selection process are mutually enhanced. For label learning, we factorize the data matrix in the main view (e.g. text) and ensure that the learned in-dicator matrix is as consistent as local learning predictors on other views (e.g. image). To objectively evaluate the new method, we build two text-image web news datasets from two major US news media web sites: CNN and FOXNews. Our extensive experiments show that MVUFS significantly outperforms state-of-the-art single-view and multi-view un-supervised feature selection methods.
Throughout this paper, matrices are written as boldface capital letters and vectors are denoted as boldface lowercase letters. For matrix M =( m ij ), its i -th row, j -th column are denoted by m i , m j respectively. M F is the Frobe-nius norm of M . For any matrix M  X  X  r  X  t ,its l 2 , 1 -norm Assume that we have n instances X = { x i } n i =1 .Let X R n  X  d v denote the data matrix in the v -th view where the i -in the v -th view. For text-image web news data, X 1 is text view data matrix, and X 2 is image view data matrix. Sup-pose these n instances are sampled from c classes and denote Y =[ y 1 ,  X  X  X  , y n ] T  X  X  0 , 1 } n  X  c , where y i  X  X  0 , 1 cluster indicator vector for x i . The scaled cluster indicator matrix G is defined as G =[ g 1 ,  X  X  X  , g n ] T = Y Y T Y where g i is the scaled cluster indicator of x i .Itcanbeseen that G T G = I c , where I c  X  X  c  X  c is an identity matrix.
It is often easier to produce good predictions on some local regions of the input space instead of searching a good global predictor f , because the function set f ( x ) may not contain a good predictor for the entire input space. And it is usually more effective to minimize prediction cost for each local region. We adopt the local learning regularization proposedin[ 3]. Let N ( x i ) denote the neighborhood of x the local learning regularization aims to minimize the sum of prediction errors between the local prediction from N ( x and the cluster assignment of x i : where f k i ( x i ) is the locally predicted label for k -th clus-ter from N ( x i ),  X  is a positive parameter, K i is the ker-nel matrix defined on the neighborhood of x i ,i.e., N ( x with size of n i , k i is the kernel vector defined between x and N ( x i ), g k i is the cluster assignments of N ( x i ( A  X  I ) T ( A  X  I ), I  X  X  n  X  n is an identity matrix, and A R n  X  n is defined by A ij =  X  ij , if x j  X  X  ( x i )
MVUFS solves the following optimization problem: where  X ,  X  are nonnegative parameters. To learn the most consistent pseudo labels across different views, we use or-thogonal nonnegative matrix factorization on the text view regularized by local learning prediction error on the image view. F is the basis matrix with each row being a clus-ter center. The fitting term 2 v =1 G  X  X v W v 2 , 1 will also push the pseudo labels to be close to the linear prediction by the feature weight matrices for each view, which gives the desirable mutual reinforcement between label learning and feature selection. Nonnegative and orthogonal constraints imposed on the cluster indicator matrix variable are desir-able to give a single non-zero positive entry on each row of the label matrix. For feature selection, we adopt joint l norm minimization [ 6] to learn rowise sparse weight matrices for each view. The sparsity-inducing property of l 2 /l 1 pushes the feature selection matrix W v to be sparse in rows. More specifically, w j v shrinks to zero if the j -th feature is less correlated to the pseudo labels Y . We can thus filter out the features corresponding to zero rows of W v .
 We apply alternating optimization to solve problem ( 1). To optimize G given F , W v ,v =1 , 2, and G t in the last iteration, we solve the following subproblem: It can be proved (due to space limit, we omit the proof) that if G t +1 is the solution of problem (2), G t +1 will monotoni-cally decrease the objective function of problem ( 1). Denote the objective function in problem ( 2)by J ( G ), the Lagrange function is given by L ( G ,  X  ,  X  )= J ( G )  X  Tr  X  G T G  X 
Tr  X  T G . The optimal G must satisfy the KKT condis-tions: guaranteed to be nonnegative, we can ignore  X  ,wethushave W v = W We then obtain the following update formula for G by ap-plying the auxiliary function approach in [ 11]: followed by column-wise normalization. When converges, we have (  X  J ( G )  X  2 G X  ) G = 0 , which is exactly the KKT complementary slackness condition.
 To optimize F , we solve the subproblem: min Since the objective function is quadratic, and F  X  X  columns are mutually independent, we can use blockwise coordinate descent to update one row at a time in a cyclic order, and the objective function value is guaranteed to decrease. The updating formula for F is
To optimize W v , we need to solve the unconstrained prob-lem min There X  X e several optimization strategies that can solve it. Here we adopt the simple algorithm given in [ 6]. Algorithm 1 MVUFS
WecrawledCNNandFOXNewswebnewsfromJan.1st, 2014 to Apr. 4th, 2014. The category information contained in the RSS feeds for each news article can be viewed as reli-able ground truth. Titles, abstracts, and text body contents are extracted as the text view data, and the image associ-ated with the article is stored as the image view data. Since the vocabulary has a very long tail word distribution, We filtered out those words that occur less than or equal to 5 times. All text content is stemmed by portStemmer [ 8], and we use l 2 -normalized TFIDF as text. For image features, we use 7 groups of color features: Color features include RGB dominant color, HSV dominant color, RGB color moment, HSV color moment, RGB color histogram, HSV color his-togram, color coherence vector [ 7], and 5 textural features: four Tamura textural features [12 ] (coarseness, contrast, di-rectionality, line-likeness) and Gabor transform [4, 10].
Two widely used evaluation metrics for measuring cluster-ing performance: accuracy (ACC) and Normalized Mutual Information (NMI) are used. We compare MVUFS with KMeans on text with all features (KM-TXT), KMeans on image with all features (KM-IMG), state-of-the-art single view unsupervised feature selection methods: NDFS [ 5]-Joint nonnegative spectral analysis and l 2 , 1 -norm regular-ized regression and RUFS [9] -joint local learning regular-ized robust NMF and robust l 2 , 1 -norm regression; multi-view spherical KMeans with all features (MVSKM) [ 1], state-of-the-art multi-view unsupervised feature selection: AUMFS [ 2] -spectral clustering and l 2 , 1 -norm regularized robust sparse regression and MVFS [13] -spectral cluster-ing and l 2 , 1 -norm regression. For single-view unsupervised feature selection methods, KMeans is used to calculate the clustering performance. For multi-view unsupervised fea-ture selection methods, multi-view spherical KMeans [ 1]is used for multi-view clustering. We set the neighborhood size to be 5. We use cosine similarity to build text graph and Gaussian kernel for image graph. All feature selection meth-ods have two parameters:  X  for regression, and  X  for sparsity control. We do grid search for  X  in 10  X  2 , 10  X  1 ,..., 10 and  X  in  X   X  10  X  2 , 10  X  1 ,..., 10 2 . We vary the number of selected text features as { 100 , 300 , 500 , 700 , 900 } berofselectedimagefeaturesishalfofselectedtextfeatures. Since K-means depends on initialization, we repeat cluster-ing 10 times with random initialization.
We need to answer several questions. First, is multi-view clustering always better than single view clustering? From Table 2,Table 3, and Figure 1, we can see that the an-swer is no. It depends on the feature quality of different views. Here the color and texture features we used for im-age view is not tightly tied with clustering measures, which does severely hurt the performance of multi-view cluster-ing (MVSKM behaves much worse than KM-TXT). Fortu-nately, if discriminative features are selected by using multi-view feature selection methods, the multi-view clustering performance may be significantly improved and can be bet-ter than single-view performance. For example, MVUFS significantly outperforms all single-view methods. Second, is multi-view feature selection better than single-view feature selection? We see that AUMFS, MVFS, and MVUFS out-perform standard single view features election methods such as NDFS and RUFS, which indicates that different views can mutually bootstrap each other. It X  X  interesting to see that both NDFS and RUFS even behave worse than without do-ing feature selection. At last, it turns out that MVUFS  X  means statistical significance at 5% level.
  X  means statistical significance at 5% level.
 Figure 1: ACC and NMI with varying number of selected features. Figure 2: ACC v.s. different  X  ,  X  ,andnumberof selected features on FOX dataset for MVUFS. outperforms both single-view clustering and feature selec-tion methods and multi-view clustering and feature selection methods. Since the major difference between MVUFS and AUMFS, MVFS is label learning, we conclude that directly learning labels from raw features from one view while ensur-ing the most consensus with other views could select a more discriminative feature set for all views, and spectral clus-tering relies on the combined similarity graphs of all views which may result in loss of discriminative information and could undermine the performance.
We plot ACC versus different  X  ,  X  , and number of se-lected features on FOXNews for MVUFS in Figure 2 (simi-lar figures for NMI and on CNN dataset) due to space limit. We see that an appropriate combination of these parameters is crucial. However, it is unknown to us theoretically how to choose the best parameter setting. It may depends on datasets and measures. In practice, like many other meth-ods, one can build a validation set in a mild scale to tune parameters by e.g., grid search.
We propose a new unsupervised feature selection meth-ods for multi-view clustering: MVUFS where local learn-ing regularized orthogonal nonnegative matrix factorization is performed to learn pseudo class labels on raw features. We built two web news text-image datasets from CNN and FOXNews, and systematically evaluate MVUFS with state-of-the-art single-view and multi-view unsupervised feature selection methods. Experimental results validate the effec-tiveness of the proposed method.
 This material is based upon work supported by the National Science Foundation under Grant Number CNS-1027965.
