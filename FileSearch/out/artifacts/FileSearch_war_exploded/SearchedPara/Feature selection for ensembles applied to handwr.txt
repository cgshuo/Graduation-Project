 ORIGINAL PAPER Luiz S. Oliveira  X  Marisa Morita  X  Robert Sabourin Abstract Feature selection for ensembles has shown to be an effective strategy for ensemble creation due to its ability of producing good subsets of features, which make the clas-sifiers of the ensemble disagree on difficult cases. In this pa-per we present an ensemble feature selection approach based on a hierarchical multi-objective genetic algorithm. The un-derpinning paradigm is the  X  X verproduce and choose X . The algorithm operates in two levels. Firstly, it performs feature selection in order to generate a set of classifiers and then it chooses the best team of classifiers. In order to show its ro-bustness, the method is evaluated in two different contexts: supervised and unsupervised feature selection. In the former, we have considered the problem of handwritten digit recog-nition and used three different feature sets and multi-layer perceptron neural networks as classifiers. In the latter, we took into account the problem of handwritten month word recognition and used three different feature sets and hid-den Markov models as classifiers. Experiments and compar-isons with classical methods, such as Bagging and Boosting, demonstrated that the proposed methodology brings com-pelling improvements when classifiers have to work with very low error rates. Comparisons have been done by con-sidering the recognition rates only.
 Keywords Ensemble of classifiers  X  Feature selection  X  Handwriting recognition  X  Multi-objective optimization  X  Genetic algorithms 1 Introduction Ensemble of classifiers has been widely used to reduce model uncertainty and improve generalization performance. Developing techniques for generating candidate ensemble members is a very important direction of ensemble of classi-fiers research. It has been demonstrated that a good ensem-ble is one where the individual classifiers in the ensemble are both accurate and make their errors on different parts of the input space (there is no gain in combining identical clas-sifiers) [1 X 4]. In other words, an ideal ensemble consists of good classifiers (not necessarily excellent) that disagree as much as possible on difficult cases.
 sets used by each member of the ensemble should help to promote this necessary diversity [4 X 7]. Traditional feature selection algorithms aim at finding the best trade-off be-tween features and generalization. On the other hand, en-semble feature selection has the additional goal of finding a set of feature sets that will promote disagreement among the component members of the ensemble. The Random Sub-space Method (RMS) proposed by Ho in [ 5 ] was one early algorithm that constructs an ensemble by varying the subset of features. More recently some strategies based on genetic algorithms (GAs) have been proposed [ 4 , 8 , 9 ]. All these strategies claim better results than those produced by tra-ditional methods for creating ensembles such as Bagging and Boosting. In spite of the good results brought by GA-based methods, they still can be improved in some aspects, e.g., avoiding classical methods such as the weighted sum to combine multiple objective functions. It is well known that when dealing with this kind of combination, one should deal with problems such as scaling and sensitivity towards the weights.
 multi-objective genetic algorithm (MOGA) is a very power-ful tool for finding a set of good classifiers [ 10 , 11 ], since GA is quite effective in rapid global search of large, non-linear and poorly understood spaces [ 12 ]. Besides, it can overcome problems such as scaling and sensitivity towards the weights. Kudo and Sklansky [ 13 ] have compared sev-eral algorithms for feature selection and concluded that GAs are suitable when dealing with large-scale feature selection (number of features is over 50). This is the case of most of the problems in handwriting recognition, which is the test problem in this work.
 tion approach based on a hierarchical MOGA. The under-lying paradigm is  X  X verproduce and choose X  [ 14 , 15 ]. The algorithm operates in two levels. The former is devoted to generate a set of good classifiers by minimizing two crite-ria: error rate and number of features. The latter combines these classifiers in order to find an ensemble by maximizing the following two criteria: accuracy of the ensemble and a measure of diversity.
 of classifiers has been widely discussed. Several works have demonstrated that there is a weak correlation between di-versity and ensemble performance [ 16 , 17 ]. In light of this, some authors have claimed that diversity brings no benefits in building ensemble of classifiers [ 18 ], on the other hand, others suggest that the study of diversity in classifier combi-nation might be one of the lines for further exploration [ 19 ]. performance, we argue that diversity might be useful to build ensembles of classifiers. We demonstrated through experi-mentation that using diversity jointly with performance to guide selection can avoid overfitting during the search. In or-der to show robustness of the proposed methodology, it was evaluated in two different contexts: supervised and unsuper-vised feature selection. In the former, we have considered the problem of handwritten digit recognition and used three different feature sets and multi-layer perceptron (MLP) neu-ral networks as classifiers. In such a case, the classification accuracy is supplied by the MLPs in conjunction with the sensitivity analysis. This approach makes it feasible to deal with huge databases in order to better represent the pattern recognition problem during the fitness evaluation. In the lat-ter, we took into account the problem of handwritten month word recognition and used three different feature sets and hidden Markov models (HMM) as classifiers. We demon-strate that it is feasible to find compact clusters and com-plementary high-level representations (codebooks) in sub-spaces without using the recognition results of the system. Experiments and comparisons with classical methods, such as Bagging and Boosting, demonstrated that the proposed methodology brings compelling improvements when classi-fiers have to work with very low error rates. It is worth of remark that the comparisons have been done by considering the recognition rates only.
 Section 2 presents a brief review about the methods for ensemble creation. Section 3 provides a overview of the strategy. Section 4 introduces briefly the the multi-objective genetic algorithm we are using in this work. Section 5 describes the classifiers and feature sets for both supervised and unsupervised contexts. Section 6 introduces how we have implemented both levels of the proposed methodology and Sect. 7 reports the experimental results. Finally, Sect. 8 discusses the reported results and Sect. 9 concludes the paper. 2 Related works Assuming the architecture of the ensemble as the main cri-terion, we can distinguish among serial, parallel, and hier-archical schemes, and if the classifiers of the ensemble are selected or not by the ensemble algorithm we can divide them into selection-oriented and combiner-oriented methods [ 20 , 21 ]. Here we are more interested in the first class, which try to improve the overall accuracy of the ensemble by di-rectly boosting the accuracy and the diversity of the experts of the ensemble. Basically, they can be divided into resam-pling methods and feature selection methods.
 ent hypotheses. For instance, bootstrapping techniques [ 22 ] may be used to generate different training sets and a learning algorithm can be applied to the obtained subsets of data in order to produce multiple hypotheses. These techniques are effective especially with unstable learning algorithms, which are algorithms very sensitive to small changes in the train-ing data. In bagging [ 23 ] the ensemble is formed by making bootstrap replicates of the training sets, and then multiple generated hypotheses are used to get an aggregated predic-tor. The aggregation can be performed by averaging the out-puts in regression or by majority or weighted voting in clas-sification problems.
 ment using a uniform probability distribution, in boosting methods [ 24 ] the learning algorithm is called at each itera-tion using a different distribution or weighting over the train-ing examples. This technique places the highest weight on the examples most often misclassified by the previous base learner: in this manner the classifiers of the ensemble focus their attention on the hardest examples. Then the boosting algorithm combines the base rules taking a weighted major-ity vote of the base rules.
 based on feature selection. The concept behind these ap-proaches consists of reducing the number of input features of the classifiers, a simple method to fight the effects of the classical curse of dimensionality problem. For instance, the random subspace method [ 5 , 7 ] relies on a pseudorandom procedure to select a small number of dimensions from a given feature space. In each pass, such a selection is made and a subspace is fixed. All samples are projected to this subspace, and a classifier is constructed using the projected training samples. In the classification a sample of an un-known class is projected to the same subspace and clas-sified using the corresponding classifier. In the same vein of the random subspace method lies the input decimation method [ 25 ], which reduces the correlation among the er-rors of the base classifiers, by decoupling the classifiers by training them with different subsets of the input features. It differs from the random subspace as for each class the cor-relation between each feature and the output of the class is explicitly computed, and the classifier is trained only on the most correlated subset of features.
 ensemble of classifiers. Kuncheva and Jain [ 26 ] suggest two simple ways to use genetic algorithm to design an ensemble of classifiers. They present two versions of their algorithm. The former uses just disjoint feature subsets while the latter considers (possibly) overlapping feature subsets. The fitness function employed is the accuracy of the ensemble, how-ever, no measure of diversity is considered. Gerra-Salcedo and Withley [ 8 ] used a simple GA to explore the space of all possible feature subsets, and then create an ensemble based on them. In their experiments, this GA-based ap-proach outperformed classical methods such as Bagging and Boosting. In spite of the fact they achieved interesting results, they did not consider any measure of diversity. A more elaborate method, also based on GA, was proposed by Optiz [ 4 ]. In his work, he stresses the importance of a diversity measure by including it in the fitness calculation. The drawback of this method is that the objective functions are combined through the weighted sum. It is well known that when dealing with this kind of combination, one should deal with problems such as scaling and sensitivity towards the weights. More recently Gunter and Bunke [ 27 ]have applied feature selection in conjunction with floating search to create ensembles of classifiers for the field of handwriting recognition. They used handwritten words and HMMs as classifiers to evaluate their algorithm. The feature set was composed of nine discrete features, which makes simpler the feature selection process. A drawback of this method is that one must set a priori the number of classifiers in the ensemble. 3 Methodology overview In this section, we outline the hierarchical approach pro-posed. As stated before, it is based on an  X  X verproduce and choose X  paradigm where the first level generates sev-eral classifiers by conducting feature selection and the sec-ond one chooses the best ensemble among such classifiers. Figure 1 depicts the proposed methodology. Firstly, we carry out feature selection by using a MOGA. It gets as inputs a trained classifier and its respective data set. Since the algo-rithm aims at minimizing two criteria during the search, 1 will produce at the end a two-dimensional Pareto-optimal front, which contains a set of classifiers (trade-offs between the criteria being optimized). The final step of this first level consists in training such classifiers.
 level is suggested to pick the members of the team which are most diverse and accurate. Let A = C 1 , C 2 ,..., C L asetof L classifiers extracted from the Pareto-optimal and B a chromosome of size L of the population. The relation-ship between A and B is straightforward, i.e., the gene i of the chromosome B is represented by the classifier C i from A . Thus, if a chromosome has all bits selected, all classi-fiers of A will be included in the ensemble. Therefore, the algorithm will produce a 2-dimensional Pareto-optimal front which is composed of several ensembles (trade-offs between accuracy and diversity). In order to choose the best one, we use a validation set, which points out the most diverse and accurate team among all. Later in this paper, we will discuss the issue of using diversity to choose the best ensemble. one-point crossover, and bit-flip mutation. In our experi-ments, MOGA used is the Non-dominated Sorting Genetic Algorithm (NSGA) with elitism proposed by Srinivas and Debin[ 29 ], which is briefly introduced in the next section. 4 Multi-objective genetic algorithm Since the concept of multi-objective genetic algorithm (MOGA) will be explored in the remaining of this paper, this section briefly introduces it.
 of a number of objectives and is associated with a number of inequality and equality constraints. Solutions to a multi-objective optimization problem can be expressed mathemat-ically in terms of nondominated points, i.e., a solution is dominant over another only if it has superior performance in all criteria. A solution is said to be Pareto-optimal if it cannot be dominated by any other solution available in the search space. In our experiments, the algorithm adopted is the Non-dominated Sorting Genetic Algorithm (NSGA) with elitism proposed by Srinivas and Deb in [ 28 , 29 ].
 method is used to emphasize good points and a niche method is used to maintain stable subpopulations of good points. It differs from simple GA only in the way the selection op-erator works. The crossover and mutation remain as usual. Before the selection is performed, the population is ranked based on an individual X  X  nondomination. The nondominated individuals present in the population are first identified from the current population. Then, all these individuals are as-sumed to constitute the first nondominated front in the pop-ulation and assigned a large dummy fitness value. The same fitness value is assigned to give an equal reproductive poten-tial to all these nondominated individuals. This is exempli-fiedinFig. 2 a. In such a case, a population of six individuals was classified into three nondominated fronts and each indi-vidual of the first front received a large dummy fitness (6.00 in this example).
 these classified individuals are then shared with their dummy fitness values. Sharing is achieved by performing selection operation using degraded fitness values obtained by dividing the original fitness value of an individual by a quantity proportional to the number of individuals around it. After sharing, these nondominated individuals are ignored temporarily to process the remaining population in the same way to identify individuals for the second nondominated front. These new sets of points are then assigned a new dummy fitness which is kept smaller than the minimum shared dummy fitness of the previous front. This process is continued until the entire population is classified into several fronts. This process is illustrate in Fig. 2 b. It can be observed from this Figure that the individuals  X 1 X  and  X 3 X  had their fitness shared because they are close to each other. In this case, their fitness were reduced from 6.00 to 4.22. Then, the dummy fitness is assigned to the individuals of the second front by multiplying the lowest value of the first front by a constant k (let us say k = 0 . 95 for this example). Therefore, the individuals of the second front will receive a dummy fitness of 4.00 (4 . 22  X  0 . 95 ) . Since the two individuals of the second front are not close to each other, their dummy fitness is maintained and a dummy fitness is assigned to the individual of the last front (4 . 00  X  0 . 95 = 3 . 80). dummy fitness values. Since individuals in the first front have the maximum fitness value, they get more copies than the rest of the population. This was intended to search for the nondominated regions of Pareto-optimal fronts. The ef-ficiency of NSGA lies in the way multiple objectives are reduced to a dummy fitness function using nondominated sorting procedures.
 similar to a simple GA except for the classification of non-dominated fronts and the sharing operation. The sharing in each front is achieved by calculating a sharing function value between two individuals in the same front as: Sh ( d ( i , j )) = where d ( i , j ) is the distance between two individuals i and j in the current front and  X  share is the maximum distance allowed between any two individuals to become members of a niche. In any application of sharing, we can implement either genotypic sharing, since we always have a genotype (the encoding), or phenotypic sharing. However, Deb and Goldberg in [ 30 ] indicate that in general, phenotypic shar-ing is superior to genotypic sharing. Thus, we have used a phenotypic sharing which is calculated from the normalized Euclidean distance between the objective functions.  X  where q is the desired number of distinct Pareto-optimal so-lutions and p is the number of decision variables. Although the calculation of  X  share depends on this parameter q ,ithas been shown [ 29 ] that the use of the above equation with q  X  10 works in many test problems. 5 Classifiers and feature sets As stated before, we have carried out experiments in both su-pervised and unsupervised contexts. The remaining of this section describes the feature sets and classifiers we have used. 5.1 Supervised context To evaluate the proposed methodology in the supervised context, we have used three base classifiers trained to recog-nize handwritten digits of NIST SD19. Such classifiers were trained with three well-known feature sets: Concavities and Contour (CC sc )[ 31 ], Distances (DDD sc )[ 32 ], and Edge Maps (EM sc )[ 33 ]. It should be noted, though, that the orig-inal feature set of distances proposed by Oh and Suen [ 32 ] contains 256 features. After carrying out some experiments with different strategies of zoning, we realized that using 96 features (6 zones: 3 horizontal and 2 vertical) we could achieve the same results as using 256 features (16 symmet-rical zones).
 ent descent applied to a sum-of-squares error function [ 34 ]. The transfer function employed is the familiar sigmoid func-tion. In order to monitor the generalization performance dur-ing learning and terminate the algorithm when there is no longer an improvement, we have used the method of cross-validation. Such a method takes into account a validation set, which is not used for learning, to measure the gener-alization performance of the network. During learning, the performance of the network on the training set will continue to improve, but its performance on the validation set will only improve to a point, where the network starts to over-fit the training set, that the learning algorithm is terminated. All networks have one hidden layer where the units of in-put and output are fully connected with units of the hidden layer, where the number of hidden units were determined empirically (see Table 1 ). The learning rate and the momen-tum term were set at high values in the beginning to make the weights quickly fit the long ravines in the weight space, then these parameters were reduced several times according to the number of iterations to make the weights fit the sharp curvatures.
 tested, the one proposed by Fumera et al [ 35 ] provided the better error-reject trade-off for our experiments. Basically, this technique suggests the use of multiple reject thresholds for the different data classes ( T 0 ,..., T n ) to obtain the op-timal decision and reject regions. In order to define such thresholds we have developed an iterative algorithm, which takes into account a decreasing function of the threshold variables R ( T 0 ,..., T n ) and a fixed error rate T error from all threshold values equal to 1, i.e., the error rate equal to zero since all images are rejected. Then, at each step, the algorithm decreases the value of one of the thresholds in order to increase the accuracy until the error rate exceeds T are composed of 195,000 and 28,000 samples from hsf 0123 series respectively while the test set (TSDB sc ) is composed of 30,089 samples from the hsf 7. We consider also a sec-ond validation set (VLDB2 sc ), which is composed of 30,000 samples of hsf 7. This data is used to select the best ensem-ble of classifiers. Figure 4 shows the performance on the test set of all classifiers for error rates varying from 0.10 to 0.50%, while Table 1 reports the performance of all clas-sifiers at zero-rejection level. The curves depicted in Fig. 4 are much more meaningful when dealing with real applica-tions since they describe the recognition rate in relation to a specific error rate, including implicitly a corresponding re-ject rate. This rate also allows us to compute the reliability of the system for a given error rate. It can be done by using Eq. ( 3 ).
 Reliability = digits is still an open problem when very low error rates are required. Consider for example our best classifier, which reaches 99.13% at zero-rejection level on the test set. If we allow an error rate of 0.1%, i.e., just one error in 1,000, the recognition rate of such classifier drops from 99.13% to 91.83%. This means that we have to reject 8.07% to get 0.1% of error (Fig. 4 a). We will demonstrate that the ensemble of classifiers can significantly improve the performance of the classifiers for low error rates. 5.2 Unsupervised context To evaluate the proposed methodology in unsupervised con-text we have used three HMM-based classifiers trained to recognize handwritten Brazilian month words ( X  X aneiro X ,  X  X evereiro X ,  X  X aro X ,  X  X bril X ,  X  X aio X ,  X  X unho X ,  X  X ulho X ,  X  X gosto X ,  X  X etembro X ,  X  X utubro X ,  X  X ovembro X ,  X  X ezem-bro X ). The training (TRDB uc ), validation (VLDB1 uc ), and testing (TSDB uc ) sets are composed of 1,200, 400, and 400 samples, respectively. In order to increase the train-ing and validation sets, we have also considered 8,300 and 1,900 word images, respectively, extracted from the legal amount database. This is possible because we are consider-ing character models. We consider also a second validation set (VLDB2 uc ) of 500 handwritten Brazilian month words [ 36 ]. Such data is used to select the best ensemble of classi-fiers.
 age is transformed as a whole into a sequence of observa-tions by the successive application of preprocessing, seg-mentation, and feature extraction. Preprocessing consists of correcting the average character slant. The segmentation al-gorithm uses the upper contour minima and some heuris-tics to split the date image into a sequence of segments (graphemes), each of which consists of a correctly seg-mented, an under-segmented, or an over-segmented charac-ter. A detailed description of the preprocessing and segmen-tation stages is given in [ 37 ].
 propriate elementary HMMs, which are built at letter and space levels. Two topologies of letter models were chosen based on the output of our grapheme-based segmentation al-gorithm which may produce a correct segmentation of a let-ter, a letter under-segmentation or a letter over-segmentation into two, three, or four graphemes depending on each let-ter. Considering uppercase and lowercase letters, we need 42 models since the legal amount alphabet is reduced to 21 let-ter classes and we are not considering the unused ones. Thus, regarding the two topologies, we have 84 HMMs which are trained using the Baum-Welch algorithm with the Cross-Validation procedure.
 of concavity and contour features (CC uc )[ 31 ]. In this case, each grapheme is divided into two equal zones (horizontal) where for each region a concavity and contour feature vector of 17 components is extracted. Therefore, the final feature vector has 34 components. The other two classifiers make use of a feature set based on distances [ 32 ]. The former uses the same zoning discussed before (two equal zones), but in this case, for each region a vector of 16 components is ex-tracted. This leads to a final feature vector of 32 components (DDD32 u c ). For the latter we have tried a different zoning. The grapheme is divided into four zones using the refer-ence baselines (see Fig. 5 ), hence, we have a final feature vector composed of 64 components (DDD64 u c ). Table 2 re-ports the performance of all classifiers on the test set at zero-rejection level. Figure 6 shows the performance of all classi-fiers for error rates varying from 1% to 4%. The strategy for rejection used in this case is the one discussed previously. We have chosen higher error rates in this case due to the size of the database we are dealing with.
 with error fixed at 1% are very poor, hence, the number of rejected patterns is very high. We will see in the next sec-tions that the proposed methodology can improve these re-sults considerably. 6 Implementation This section introduces how we have implemented both lev-els of the proposed methodology. First we discuss the super-vised context and then the unsupervised. 6.1 Supervised context 6.1.1 Supervised feature subset selection The feature selection algorithm used in here was introduced in [ 11 ]. To make this paper self-contained, a brief description is included in this section.
 sified into two categories based on whether or not feature selection is performed independently of the learning algo-rithm used to construct the classifier. If feature selection is done independently of the learning algorithm, the technique is said to follow a filter approach. Otherwise, it is said to follow a wrapper approach [ 38 ]. While the filter approach is generally computationally more efficient than the wrapper approach, its major drawback is that an optimal selection of features may not be independent of the inductive and rep-resentational biases of the learning algorithm that is used to construct the classifier. On the other hand, the wrapper approach involves the computational overhead of evaluat-ing candidate feature subsets by executing a given learning algorithm on the database using each feature subset under consideration.
 is to promote diversity among the classifiers. To tackle such a task we have to optimize two objective functions: mini-mization of the number of features and minimization of the error rate of the classifier. Computing the first one is sim-ple, i.e., the number of selected features. The problem lies in computing the second one, i.e., the error rate supplied by the classifier. Regarding a wrapper approach, in each generation, evaluation of a chromosome (a feature subset) requires train-ing the corresponding neural network and computing its ac-curacy. This evaluation has to be performed for each of the chromosomes in the population. Since such a strategy is not feasible due to the limits imposed by the learning time of the huge training set considered in this work, we have adopted the strategy proposed by Moody and Utans in [ 39 ], who use the sensitivity of the network to estimate the relationship be-tween the input features and the network performance. fined as: with  X  where x  X  j is the  X  th input variable of the j th exemplar. S  X  measures the effect on the training ASE (average square er-ror) of replacing the  X  th input x  X  by its average  X  x  X  . Moody and Utans show that when variables with small sensitivity values with respect to the network outputs are removed, they do not influence the final classification. So, in order to evalu-ate a given feature subset we replace the unselected features by their averages. In this way, we avoid training the neural network and hence turn the wrapper approach feasible for our problem. We call this strategy modified-wrapper. Such a scheme has been employed also by Yuan et al in [ 40 ], and it makes it feasible to deal with huge databases in order to better represent the pattern recognition problem during the fitness evaluation. 2 Moreover it can accommodate multiple criteria such as the number of features and the accuracy of the classifier, and generate the Pareto-optimal front in the first run of the algorithm. Figure 7 shows the evolution of the population in the objective plane and its respective Pareto-optimal front.
 is composed of several different classifiers. In order to get a better insight about them, they were classified into three dif-ferent groups: weak, medium, and strong. It can be observed that among all those classifiers there are very good ones. To find out which classifiers of the Pareto-optimal front com-pose the best ensemble, we carried out a second level of search. Once we did not train the models during the search (the training step is replaced by the sensitivity analysis), the last step of feature selection consists of training the solutions provided by the Pareto-optimal front ( 1 ).
 would like to stress the importance of using MOGA for fea-ture selection. Therefore, we show a result achieved by a single GA, where the objective (number of features and error rate of the classifiers) were combined through the weighted-sum approach (Fig. 8 ). As expected, the results achieved by the weighted-sum approach presented a premature con-vergence to a specific region of the search space instead of maintaining a diverse population. This kind of behavior can be explained by the sensitivity towards weight presented by the weighted-sum approach. Since we have chosen weights to favor solutions with a small error rate rather than a small number of features, the selection pressure drove the search totheregionwheretheerrorratesarelower.Thus,aftersev-eral trials of using different weights we did not succeed in finding the Pareto-optimal front but rather an approximation of the Pareto-optimal solutions. 6.1.2 Choosing the best ensemble As defined in Sect. 3 each gene of the chromosome is repre-sented by a classifier produced in the previous level. There-fore, if a chromosome has all bits selected, all classifiers of will compose the team. In order to find the best ensemble of classifiers, i.e., the most diverse set of classifiers that brings a good generalization, we have used two objective func-tions during this level of the search, namely, maximization of the recognition rate of the ensemble and maximization of a measure of diversity. We have tried different measures such as overlap, entropy [ 41 ], and ambiguity [ 3 ]. The results achieved with ambiguity and entropy were very similar. In this work we have used ambiguity as diversity measure. The ambiguity is defined as follows: a ( x where a i is the ambiguity of the i th classifier on the example x , randomly drawn from an unknown distribution, while V i and V are the i th classifier and the ensemble predictions, respectively. In other words, it is simply the variance of en-semble around the mean, and it measures the disagreement among the classifiers on input x . Thus the contribution to di-versity of an ensemble member i as measured on a set of M samples is: A and the ambiguity of the ensemble is  X  A = 1 where N is the number of classifiers. So, if the classifiers implement the same functions, the ambiguity  X  A will be low, otherwise it will be high. In this scenario the error from the ensemble is E =  X  E  X   X  A (9) where  X  E is the average errors of the single classifiers and  X  A is the ambiguity of the ensemble. Equation ( 9 ) expresses the trade-off between bias and variance in the ensemble, but in a different way than the common bias-variance relation in which the averages are over possible training sets instead of ensemble averages. If the ensemble is strongly biased the ambiguity will be small, because the classifiers implement very similar functions and thus agree in inputs even outside the training set [ 3 ].
 eralization of the ensemble, therefore, it will be necessary to use a way of combining the outputs of all classifiers to get a final decision. To do this, we have used the average, which is a simple and effective scheme of combining predictions of the neural networks [ 42 ]. Other combination rules such as product, min, and max have been tested but the simple aver-age has produced slightly better results. In order to evaluate the objective functions during the search described above we have used the validation set VLDB1 sc . 6.2 Unsupervised context 6.2.1 Unsupervised feature subset selection A lot of work done in the field of handwritten word recog-nition take into account discrete HMMs as classifiers, which have to be fed with a sequence of discrete values (symbols). This means that before using a continuous feature vector, we must convert it to discrete values. A common way to do that is through clustering. The problem is that for the most of real-life situations we do not know the best number of clus-ters, what makes it necessary to explore different numbers of clusters using traditional clustering methods such as the K-means algorithm [ 43 ] and its variants. In this light, clus-tering can become a trial-and-error work. Besides, its result may not be very promising especially when the number of clusters is large and not easy to estimate.
 lution to this problem. The literature contains several stud-ies on feature selection for supervised learning, but only re-cently, the feature selection for unsupervised learning has been investigated [ 44 , 45 ]. The objective in unsupervised feature selection is to search for a subset of features that best uncovers  X  X atural X  groupings (clusters) from data according to some criterion. In this way, we can avoid the manual pro-cess of clustering and find the most discriminative features in the same time. Hence, we will have at the end a more compact and robust high-level representation (symbols). also presents a multi-criterion optimization function, where the objective is to find compact and well separated hyper-spherical clusters in the feature subspaces. Differently of the supervised feature selection, here the criteria optimized by the algorithm are a validity index and the number of fea-tures. [ 46 ].
 clustering process, we have used the Davies-Bouldin (DB)-index [ 47 ] over 80,000 feature vectors extracted from the training set of 9,500 words. To make such an index suitable for our problem, it must be normalized by the number of selected features. This is due to the fact that it is based on geometric distance metrics and therefore, it is not directly applicable here because it is biased by the dimensionality of the space, which is variable in feature selection problems. as the number of features increases. We have correlated this effect with the normalization of DB-index by the number of features. In order to compensate this, we have considered as second objective the minimization of the number of fea-tures. In this case, one feature must be set at least. Figure 9 depicts the Pareto-optimal front found after the search, the relationship between the number of clusters and number of features and the relationship between the recognition rate on the validation set and the number of features.
 classifiers of the Pareto into classes. In this case, we have realized that those classifiers with very few features are not selected to compose the ensemble, and therefore, just the classifiers with more than 10 features were used into the sec-ond level of search. In Sect. 7.2 we discuss this issue in more detail. Figure 9 shows the Pareto-optimal front where a line divides the classifiers into two different groups: weak (less than 10 features) and strong (more than 10 features). The way of choosing the best ensemble is exactly the same as introduced in Sect. 6.1.2 . 7 Experimental results All experiments in this work were based on a single-population master-slave MOGA. In this strategy, one mas-ter node executes the genetic operators (selection, crossover and mutation), and the evaluation of fitness is distributed among several slave processors. We have used a Beowulf cluster with 17 (one master and 16 slaves) PCs (1.1 Ghz CPU, 512 Mb RAM) to execute our experiments.
 levels: population size = 128, number of generations 1,000, probability of crossover = 0.8, probability of mu-tation = 1 / L (where L is the length of the chromosome), and niche distance (  X  share ) =[ 0 . 25 , 0 . 45 ] . The length of the chromosome in the first level is the number of components in the feature set (see Table 1 ), while in the second level is the number of classifiers picked from the Pareto-optimal front in the previous level.
 tation, we have used the one-max problem, which is prob-ably the most frequently-used test function in research on genetic algorithms because of its simplicity [ 48 ]. This func-tion measures the fitness of an individual as the number of bits set to one on the chromosome. We have used a stan-dard genetic algorithm with a single-point crossover and the maximum generations of 1,000. The fixed crossover and mu-tation rates are used in a run, and the combination of the crossover rates 0.0, 0.4, 0.6, 0.8 and 1.0 and the mutation rates of 0 . 1 / L ,1 / L and 10 / L ,where L is the length of the chromosome. The best results were achieved with P c = 0 . and P m = 1 / L . Such results confirmed the values reported by Miki et al in [ 49 ]. The parameter  X  share was determined initially through Eq. ( 2 ) and then tuned empirically. we have noticed that the best solution was stable over the runs. Using the aforementioned cluster the experiments in the supervised context took about 2 h, while in the unsu-pervised took about 8 h. The latter is more time consum-ing because of the clustering algorithm used during feature selection. 7.1 Experiments in the supervised context Once all parameters have been defined, the first step, as de-scribed in Section 6.1.1 , consists of performing feature se-lection for a given feature set. As depicted in Fig. 7 , this pro-cedure produces quite a large number of classifiers, which should be trained for use in the second level. After some experiments, we found out that the second level always chooses  X  X trong X  classifiers to compose the ensemble. Thus, in order to speed up the training process and the second level of search as well, we decide to train and use in the second level just the  X  X trong X  classifiers. This decision was made after we realized that in our experiments the  X  X eak X  and  X  X edium X  classifiers did not cooperate with the ensemble at all. To train such classifiers, the same databases reported in Sect. 5.1 were used. Table 3 summarizes the  X  X trong X  classi-fiers produced by the first level for the three feature sets we have considered.
 level of the algorithm provided 81  X  X trong X  classifiers which have the number of features ranging from 24 to 125 and recognition rates ranging from 90.5% to 99.1% on TSDB sc . This shows the great diversity of the classifiers produced by the feature selection method. Based on the classifiers reported in Table 3 we define four sets of base clas-sifiers as follows: S 1 ={ CCsc 0 ,..., CCsc 80 } , S 2 {
DDDsc 0 ,..., DDDsc 53 } , S 3 ={ EMsc 0 ,..., EMsc 77 } , and S 4 ={ S 1  X  S 2  X  S 3 } . All these sets could be seen as ensembles, but in this work we reserve the word ensem-ble to characterize the results yielded by the second-level of the algorithm. In order to assess the objective functions of the second-level of the algorithm (generalization of the ensemble and diversity) we have used the validation set (VLDB1 sc ).
 possible solutions which are the trade-offs between the gen-eralization of the ensemble and its diversity. Thus the prob-lem now lies in choosing the most accurate ensemble among all. Figure 10 depicts the variety of ensembles yielded by the second-level of the algorithm for the four sets of base classifiers being considered. The number over each point stands for the number of classifiers in the ensemble. In or-der to decide which ensemble to choose we validate the Pareto-optimal front using VLDB2 sc , which was not used so far. Since we are aiming at performance, the direct choice will be the ensemble that provides better generalization on VLDB2 sc .Table 4 summarizes the best ensembles produced for the four sets of base classifiers and their performance at zero-rejection level on the test set. For facility, we reproduce in this table the results of the original classifiers. classifiers have very similar performance at zero-rejection level. On the other hand, Fig. 11 shows that the ensembles respond better for error rates fixed at very low levels than single classifiers. The most expressive result was achieved for the ensemble S 3 , which attains a reasonable performance at zero-rejection level but performs very poorly at low er-ror rates. In such a case, the ensemble of classifiers brought an improvement of about 8%. We have noticed that the en-semble reduces the high outputs of some outliers so that the threshold used for rejection can be reduced and conse-quently the number of samples rejected is reduced. Thus, aiming for a small error rate we have to consider the impor-tant role of the ensemble.
 achieves a performance similar to S 1 at zero-rejection level (see Table 4 ). Besides, it is composed of 24 classifiers, against four of S 1 . The fact worth noting though, is the per-formance of S 4 at low error rates. For the error rate fixed at 1% it reached 95.0% against 93.5% of S 1 . S 4 is composed of 14, 6, and 4 classifiers from S 1 , S 2 ,and S 3 , respectively. At this point, we would like to stress that the ensembles can improve results even when just one feature set is available, as depicted in Fig. 11 a, b, and c. However, when more orig-inal classifiers are available, it is expected that the algorithm builds better ensembles. This is what we observe in Fig. 11 d for the ensemble S 4 .
 we present in Fig. 12 some examples of misclassification generated by the ensembles, where those signed with a square would represent mislabelling. We can observe that most of this errors are pretty difficult to recognize, with the exception of digits  X 1 X , which are not that difficult, but barely appear in the training set of NIST. 3 7.2 Experiments in the unsupervised context The experiments in the unsupervised context follow the same vein of the supervised one. As discussed in Sect. 6.2.1 , the main difference lies in the way the feature selection is carried out. In spite of that, we can observe that the num-ber of classifiers produced during unsupervised feature se-lection is quite large as well. In light of this, we have applied the same strategy of dividing the classifiers into groups (see Fig. 9 ). After some experiments, we found out that the sec-ond level always chooses  X  X trong X  classifiers to compose the ensemble. Thus, in order to speed up the training process and the second level of search as well, we decide to train and use in the second level just  X  X trong X  classifiers. To train such classifiers, the same databases reported in Sect. 5.2 were considered. Table 5 summarizes the  X  X trong X  classifiers (af-ter training) produced by the first level for the three feature sets we have considered.
 first level of the algorithm provided 15  X  X trong X  clas-sifiers which have the number of features ranging from 10 to 32 and recognition rates ranging from 68.1 to 88.6% on VLDB1 uc .Thisshowsthegreat diversity of the classifiers produced by the feature se-lection method. Based on the classifiers reported in Ta b l e 5 we define four sets of base classifiers as follows: F D 32 uc 20 } , F 3 ={ DDD 64 uc 0 ,..., DDD 64 uc 49 } ,and F second-level of the algorithm for the four sets of base clas-sifiers. The number over each point stands for the number of classifiers in the ensemble. Like in the previous experiments, the second validation set (VLDB2 uc ) was used to select the best ensemble. After selecting the best ensemble the final step is to assess them on the test set. Table 6 summarizes the performance of the ensembles on the test set. For the sake of comparison, we reproduce in Table 6 the results presented in Ta b l e 2 .
 generated with all base classifiers available, i.e., Ensemble F . Like in the previous experiments (supervised context), the result achieved by the ensemble F 4 shows the ability of the algorithm in finding good ensembles when more base classifiers are considered. The ensemble F 4 is composed of 9, 11, and 25 classifiers from F 1 , F 2 ,and F 3 , respectively. set, which, based on our experience, has a good discrimina-tion power when combined with other features such as con-cavities. This feature set, which we call  X  X lobal features X , is composed of primitives such as ascenders, descenders, and loops. The combination of these primitives plus a primitive that determines whether a grapheme does not contain ascen-der, descender, and loop produces a 20-symbol alphabet. For more details, see [ 50 ]. In order to train the classifier with this feature set, we have used the same databases described in Sect. 5.2 . The recognition rates at zero-rejection level are 86.1% and 87.2% on validation and testing sets, respectively. This performance compares with the CC uc classifier. base classifiers must be modified to cope with it. Thus, F F classifier trained with global features. Table 7 summarizes the ensembles found using these new sets of base classifiers. It is worthy of remark the reduction of the size of the teams. This shows the ability of the algorithm in finding not just diverse but also uncorrelated classifiers to compose the ensemble [ 51 ]. Besides, it corroborates our claim that the classifier G when combined with other features bring an improvement to the performance.
 ensembles reported in Table 7 . Like the results at zero-rejection level, the improvement observed here also are quite impressive. Table 7 shows that F 1 G and F 4 G reach similar results on the test set at zero-rejection level, however, F contains just two classifiers against 23 of F 4 G . On the other hand, the latter features a slightly better error-reject trade-off in the long run (Fig. 14 d).
 firm that the unsupervised feature selection is a good strat-egy to generate diverse classifiers. This is made very clear in the experiments regarding the feature set DDD64. In such a case, the original classifier has a poor performance (about 65% on the test set), but when it is used to gen-erate the set of base classifiers, the second-level MOGA was able to produce a good ensemble by maximizing the performance and the ambiguity measure. Such an ensem-ble of classifiers brought an improvement of about 15% in the recognition rate at zero-rejection level. Figure 15 some examples of well-classified and misclassified images of our database.
 8 Discussion The results obtained here attest that the proposed strategy is able to generate a set of good classifiers in both supervised and unsupervised contexts. To better evaluate our results, we have used two traditional ensemble methods (Bagging and Boosting) in the supervised context. Figure 16 reports the results. As we can see, the proposed methodology achieved better results, especially when considering very low error rates. It also can be observed from this figure that the gap among ensemble feature selection and bag-ging/boosting gets bigger as the discriminative power of the feature sets gets lower. However to verify whether this holds, experimentation with other feature sets would be necessary.
 discussing ensemble of classifiers. As we have mentioned before, some authors advocated that diversity does not help at all. In our experiments, most of the time, the best ensem-bles of the Pareto-optimal also were the best for the unseen data. This could lead one to agree that diversity is not impor-tant when building ensembles, since even using a validation set the selected team is always the most accurate and with less diversity.
 serve that there are cases where the validation curve does not have the same shape of the Pareto-optimal. In such cases diversity is very useful to avoid selecting overfitted solutions.
 entire final population, perhaps the similar solutions found in the Pareto-optimal produced by the MOGA will be there. To show that it does not happen, we have carried out some experiments with a single GA where the fitness function was the maximization of the ensembles accuracy. Since a single-objetive optimization algorithm searches for an optimum so-lution, it is natural to expect that it will converge towards the fittest solution, hence, the diversity of solutions presented in the Pareto-optimal is not present in the final population of the single genetic algorithm.
 GA to find ensemble in F 2 (unsupervised context). The pa-rameters used here are the same we have used for the MOGA (Sect. 7 ). Figure 17 a plots all the classifiers found in the last population of the genetic algorithm. For the sake of com-parison we reproduce Fig. 13 binFig. 17 b. As we can see, the population is very homogeneous and it converged, as ex-pected, towards the most accurate ensemble.
 He combined accuracy and diversity through the weighted-sum approach. As stated somewhere, when dealing with this kind of combination, one should deal with problems such as scaling and sensitivity towards the weights. We believe that our strategy offers a clever way to find the ensemble using genetic algorithms. 9Conclusion We have described a methodology for ensemble creation un-derpinned on the paradigm  X  X verproduce and choose X . It takes two levels of search where the first level overproduces a set of classifiers by performing feature selection while the second one chooses the best team of classifiers.
 comprehensive experiments carried out in the context of handwriting recognition. The idea of generating classifiers through feature selection was proved to be successful in both supervised and unsupervised contexts. The results attained in both situations and using different feature sets and base classifiers demonstrated the efficiency of the proposed strat-egy by finding powerful ensembles, which succeed in im-proving the recognition rates for classifiers working with a very low error rates. Such results compare favorably to tra-ditional ensemble methods such as Bagging and Boosting. build ensembles. As we have seen, using diversity jointly with the accuracy of the ensemble as selection criterion might be very helpful to avoid choosing overfitted solutions. Our results certainly brings some contribution to the field, but this still is an open problem.
 References
