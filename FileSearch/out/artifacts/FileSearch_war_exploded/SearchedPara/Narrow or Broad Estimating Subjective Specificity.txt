 Supporting exploratory search is a very challenging prob-lem, not least because of the dynamic nature of the exercise: both the knowledge and interests of the user are subject to constant change. Moreover, whether the results for a query are informative is strongly subjective. What is informative to one user, is too specific for the other; specificity differs between users depending on their intent and accumulated knowledge about the domain.
 We propose a formal model X  X otivated by Information Foraging Theory X  X or predicting the subjective specificity of search results based on simple observables such as result-clicks. Through two studies including both controlled and free-form exploratory search we show our model allows us to differentiate between levels of subjective result specificity with regard to the current information need of the user. H.1 [ Models and Principles ]: User/Machine Systems X  Human information processing Exploratory search; models of search behavior; click data; subjective specificity; Information Foraging Theory.
Search tasks are commonly divided into two broad types: navigational, or known-item search, and exploratory search. In the former the user has specific search results in mind, while in the latter the problem is open ended, the user does not yet know exactly what she wants to find, and her goals may change as the search progresses [19, 36]. Traditional information retrieval (IR) techniques concentrate mostly on known-item search. Exploratory search is less well-studied, even though it is rapidly gaining importance as more and more knowledge is available through the web and knowledge bases [25]. While exploratory search is naturally challenging for users, at the same time it is also rather difficult for IR sys-tems to offer support: search goals are poorly defined, users lack knowledge to formulate precise queries, user knowledge, search goals and information needs can all change through-out the search process [25, 36]. Recent years show an in-creased interest in techniques to support exploratory search, such as novel user interfaces [24], retrieval techniques [2, 6, 14], and studies of exploratory search [3, 24, 25]. One of the key open problems is that we need a better understanding of the dynamic nature of the user X  X  information needs in ex-ploratory search. In this paper we formalize a model that allows us to estimate the specificity of search results with respect to the user X  X  subjective information need.

Exploratory search starts when a user has an interest in finding information on a topic in which she has little or no knowledge [36]. Generally, the user starts with vague queries using broad search terms, which allows them to ob-tain cues about new keywords and repetitively reformulate queries with specific terms [36]. Formulating good queries, however, is difficult X  X s is reformulating queries when the results are not satisfactory. When users try out queries ex-ploratorily, some queries will return results that are overly specific with regard to the knowledge of the user by going into far too much detail. Alternatively, results can also be too broad, covering so many sub-topics that it is difficult for the user to get an overview.

For example, consider an undergraduate who has just started a course on data mining issuing  X  X ata mining X  as her first query to explore this domain. Data mining is a broad subject and so the search results might cover a di-verse information scope, which might make the results too broad for the user. Later this user might obtain cues about a new keyword  X  X ubgroup discovery X  and formulate a new query. If the user has gained sufficient knowledge on this topic, then the search results might be just right for her. If not, however, then the search results may contain very specific technical details that are not comprehensible for a novice and so the user might find the results too narrow .
We are interested in modeling the specificity of search re-sults with respect to the user X  X  information need, which we refer to as subjective result specificity , here on we use  X  X ub-jective specificity X  as a short hand. We envision IR sys-tems that automatically detect the subjective specificity of a result so that they can effectively support the user X  X  ex-ploratory search. In particular, depending on the subjective specificity, users will benefit from different types of support. For example, if the results are too broad, then visualizations of the information space and guided tours would help the user to understand the new domain [14, 16]. If the results are too narrow, users might prefer introductory material ex-plaining the new concepts, such as Wikipedia articles, or literature reviews [3]. Subjective specificity detection is also useful for IR systems supporting exploratory search through techniques such as results clustering, keyword suggestion or query expansion to determine whether the generated results are too broad or narrow for the user. To the best of our knowledge, there is no prior work for estimating subjective specificity in exploratory search.

We formalize a model that allows an IR system to infer subjective specificity from easily observable aspects of user behavior. That is, our model relies only on implicit click data, and we do not require any extra sensors such as eye-trackers [7]. Further, our model is sensitive in a predictable manner to moderating factors such as prior search experi-ence and in-session learning.

The model captures how information gain [27] in explo-ration behavior is affected by the subjective specificity. We define information gain as the number of search results that a user clicks expressed as a function of the number of search results seen by the user. We assume that the information gain follows a natural logarithmic distribution. We adapt the formalism of Information Foraging Theory (IFT) [27] to predict how the slope of the information gain curve, the rate at which users click results, changes when subjective speci-ficity becomes low (broad) or high (narrow) with respect to the user-specific reference curve.

The key idea is that the same search result can have very different information content for a user depending on how well it matches her current information needs. Consider the user in our previous example with two search queries:  X  X ata mining X  and  X  X ubgroup discovery X . The first query would retrieve broad results that include information about many areas, inviting the user to explore further. Consequently, the user would spend more time on every item [33], hence a higher slope of the curve. The second query would retrieve too narrow results with overly specific titles that make lit-tle sense to a novice, so she would probably estimate only few items as informative and worthy of further exploration, making the slope of the information gain curve shallow.
To evaluate our model, we design two experiments that capture the key elements of exploratory search. We focus on information-gathering [19] in the context of scientific es-say writing X  X ut note that our modeling approach is suited for other tasks as well. In the first study, we let computer science students explore scientific information to gather ma-terial for an essay on a domain they are not familiar with. We varied the subjective specificity at three levels (broad, intermediate, and narrow) and the order in which these ap-peared in a search session. It is our belief that this is the first study to manipulate the subjective specificity in a search ses-sion consisting of several searches on the same topic. In the second study, we consider the natural setting of free-form exploratory search, having users explore a topic of their in-terest. Empirical evaluation shows that our model estimates subjective specificity well in both settings.

In summary, our main contributions are: 1. a formal model to predict subjective specificity based on user behavior; 2. extensive empirical validation of the model; including 3. ex-amination of moderating factors, such as prior search expe-rience and in-session learning.
In recent years, exploratory search has attracted attention from, among others, IR, HCI, and cognitive science research communities. Below, we review contributions of these com-munities to understand user behavior, and develop retrieval techniques and user models to support exploratory search.
We briefly review studies on exploratory search to un-derstand how user strategies affect observable exploratory search behavior. Overall, they clearly point to the dynamic nature of the exploratory search process which motivates our model. According to prior studies, exploration begins with concept formulation and then it narrows down to more specific concepts [36, 25], and as domain knowledge changes so do search tactics [37]. Previous studies provide detailed descriptions of exploratory search strategies such as narrow-ing and broadening search queries [31, 30, 33], users spend-ing more time evaluating unfamiliar topics than familiar ones [20], change in search behavior with increasing domain knowledge [34]. Prior studies show that people with ex-ploratory information needs are inclined to click more results following a query [36]. Literature suggests that users browse many results at the beginning of complex search tasks but they become selective when search goals become clearer [33].
IR and machine learning communities propose several tech-niques to facilitate exploratory search. Some of the initial so-lutions include result clustering [8], relevance feedback [22], and faceted search [40]. However, these techniques are rarely used in practice, perhaps due to the high additional cogni-tive load of providing feedback for a large number of items [22]. In response, new techniques were designed to visu-alize search results and engage the user into the feedback loop. Some of them include interactive visualizations com-bined with learning algorithms to support users to com-prehend the search results [6], and visualization and sum-maries of results [24]. These solutions give users more con-trol, however, they do not adapt to the moment-by-moment information-needs of the user [32]. Recently, reinforcement learning (RL) techniques have been used to facilitate ex-ploratory search [14, 21, 29]. These systems look promis-ing, however, the modeling process can take a few iterations while the user has to deal with suboptimal results. With the help of our model, RL-based and other adaptive exploratory search systems could improve their performance.
Many models of information-seeking have been designed to disambiguate user behaviors in known-item search, in-cluding search satisfaction [11, 17], frustration [9], and strug-gling [18]. Other related work includes models using eye-gaze [7] to predict the domain knowledge of the searcher, however, such models require extra sensors such as eye-trackers and are not sensitive to in-session knowledge gain and changes in user interests [37]. There exist probabilistic models for predicting the next interactions of the searcher [9], disambiguating short-term search interests [35], and esti-mating the relevance of results to information need [1]. Prior work suggests that there is a positive correlation between the information need specificity and query length [26]. How-ever, in exploratory search users may issue queries of varying Figure 1: Hypothetical example of information gain as a function of the number of articles seen, and clicked to be explored in closer detail (Seen X  X licked). g u ( n ) is the user-specific effective information gain function. Our model pre-dicts that the gradient of this Seen X  X licked curve increases (e.g., g broad ( n )) when results become more broad, and the gradient decreases (e.g., g narrow ( n )) when the results become more specific than the current information need of the user. lengths with little understanding. This work shows the im-portance of user models in improving IR systems. Despite the benefits, the development of user models for exploratory search has seen little research attention.
 Exploratory information-seeking is related to Information Foraging Theory (IFT) [27]. IFT includes several quantita-tive models of user search. The key idea is that decisions on what to do are made according to the expectation of information gain. As a user is searching and learning more about the content, she is continuously updating  X  X nformation scent X , i.e. her estimate of information gain by selecting a particular item. Information scent, in turn, affects whether to investigate an element or not. The theory makes predic-tions on how information gain, expressed as a function of time, changes with interface design [8]. When search results are unordered, information gain is a linear function of time. When they are ordered, it shifts to a diminishing returns curve. IFT has been used to explain how presentation tech-niques, such as result clustering, change the information gain rates and when is the optimal time to stop searching. More recently [13] used information scent to predict rankings of links. Existing work on IFT, however, does not consider the effect of specificity of search results and user-specific param-eters, such as background knowledge. While IFT shows the basic shape of the gain function, it does not give a mathe-matical formulation that we can use in an IR system.
Berry-picking [5] is another human-centered model that assumes search is a constantly evolving phenomenon with the user constantly updating her cognitive model of the in-formation being searched for. Like IFT, this model lacks a quantifiable formal approach that can be easily applied in an IR system. We contribute to IFT by providing a formal model that allows us to predict subjective specificity in ex-ploratory search thus allowing IFT to be incorporated into an IR system.
Our goal is to predict the effect of subjective specificity on exploratory information-seeking where multiple search en-gine result pages (SERPs) are examined. We aim to capture the iterative and evolving nature of search, i.e. as the user explores a new domain, the search results become narrower and user knowledge expands.

Our model links two observable aspects of user exploratory behavior into what we call effective information gain , i.e. 1. the number of search results seen on a list; 2. number of search results clicked . By clicked we mean the action of opening a link to a search result for further investigation. We introduce a formal model for the effective information gain, ( g ), curve of a user, ( u ), as a function of the number of result items seen, ( n ), as shown, g u ( n ), in Figure 1. We refer to this graph as the Seen X  X licked curve. Any gain function is affected by the objective relevance of the search results. In our case, when results are ranked according to relevance, the function takes the shape of a diminishing returns curve.
We describe the relationship as a logarithmic regression model parameterized by  X  and  X  : where n is the number of items seen so far on a result list which is a positive integer with no upper bound and  X  deter-mines the slope of this curve.  X  is a case-specific term which affects the maximum gain X  X t is determined by several fac-tors; subjective specificity and case-specific factors such as the search task, and the maximum number of search results the user is expecting to gain. We make an assumption that when n is one item,  X  is -1 if the subjective specificity is broad and 0 otherwise. However, in reality n will be greater than one. We used a logarithmic function to capture the information gain, ( g ), as this is the most natural foraging distribution [15] commonly used in human behavior models [10]. In our model, we focus on the gradient of the gain function,  X  , which is dependent on two parameters: The user-specific factor,  X  u , may depend on the user X  X  ex-perience with exploratory information-seeking or the search tool. For every user of a search tool a distinct Seen X  X licked curve is defined by  X  u . The g u ( n ) curve plotted in Figure 1 shows an instance of such a Seen X  X licked graph.

The results X  X pecificity factor,  X  r , determines the effect of the subjective specificity on the gradient of the curve. For search results with high subjective specificity, narrow , the gradient of the curve reduces to a new effective gain function, as shown in graph g narrow ( n ) in Figure 1. An in-stance of a Seen X  X licked curve for search results that have low subjective specificity, broad , is shown in g broad ( n ). Al-though a single click carries little information about sub-jective specificity, our empirical data show that aggregated clicking behavior on a result page suffices for distinguishing among three levels (broad, intermediate, narrow).

An IR system would monitor the clicking and viewing ac-tions of a user in a session. It would derive  X  user X  X  previous session, and throughout a given session it would derive  X  r from the user actions. Thus, the gain func-tion in Equation 1 can be a combination of  X  u and  X  r :
A parameterized model predicts the subjective specificity of SERPs for the user and then compares the gradient of the Seen X  X licked graph based on the user X  X  clicks in a current query with that of the user X  X  baseline Seen X  X licked graph. Such a baseline graph can be constructed by observing the everyday interactions of a user with a search tool. Then, if this user formulates a particular query to explore a research topic, the gradient of the new Seen X  X licked graph can be compared against the gradient of her baseline graph, and so the system can predict whether the search results are too narrow or too broad for her information-need X  X nd adjust the behaviour of the system accordingly.
The purpose of this study is to validate our model in a controlled setting as well as explore how prior experience and in-session learning affect the model. To this end, we designed the study by manipulating subjective specificity at three levels and permuting them over a session consisting of three result pages.
Prior to Study 1, we observed the information seeking be-havior of computer scientists in order to understand their natural exploratory search behaviors. Our sample included two participants from each category: PhD students, post-doctoral and senior researchers. We asked them to inform us when they are exploring literature for a real need. We then visited their workstations and uninterruptedly observed and video recorded their search process. Based on these obser-vations and prior work [4], we identified a common search strategy in exploratory search which initiates with query formulation, followed by scanning the SERP while clicking links of results that seem interesting. According to our ob-servations, users process the clicked links after scanning the SERP. This information helped us to plan the design pa-rameters of the data collection process.
We recruited 24 university-based computer science research-ers who were not overly familiar with the topics of the search tasks. We selected computer science researchers as these generally have much experience with electronic literature search tools [3]. In order to explore the influence of prior experience on our model, we selected participants with vary-ing levels of experience, that is, MSc and PhD students. Ten of the participants were in the process of writing their mas-ter X  X  thesis and 14 were PhD students. Nine of the partici-pants were female and 15 were male. The average age of the participants was 26 . 7 years, with the minimum age being 23 and the maximum 37. With a pre-study questionnaire we quantified their experience with scientific information-seeking ( mean  X  std . dev ) [ PhD students (5 . 6  X  1 . 2), MSc students (5  X  1 . 7)], frequency of exploratory search [PhD students (4 . 0  X  1 . 03), MSc students (3 . 7  X  1 . 6)] (ratings are given in a 7 point Likert scale where 1 = X  X ot at all famil-iar/never X  and 7 =  X  X ery familiar/often X ). Table 1 reports the search topics and participant X  X  familiarity with them.
The study involved performing exploratory search on dif-ferent topics. Every task involved going through three article lists generated from three queries that retrieved results with varying specificity: broad (B), intermediate (I) and narrow (N). The broad results covered a wide information scope; Table 1: Tasks and queries used in the study. (B = Broad, I = Intermediate, N = Narrow). Familiarity with search topics is rated in a 7-point Likert scale and mean  X  standard deviation of the participants X  familiarity with each topic is given below the topic.
 the intermediate ones a sub-field of the broad topic; and the narrow ones a very specific topic. To explore how in-session learning affects the model, we altered the order of present-ing the results, which resulted in six permutations: Broad followed by Intermediate followed by Narrow (or, BIN for short), and likewise BNI , INB , IBN , NIB and NBI .
In order to cover all the six permutations, we created six unique tasks for six different topics. We asked senior re-searchers from these six computer science disciplines to de-fine a task on their topic of expertise consisting of three search queries to retrieve results of varying specificity for a novice information-seeker in that domain. The experts also analyzed Google Scholar results for each query to en-sure that they complied with the subjective specificity. The search topics and the queries are given in Table 1. For the purpose of counterbalancing, we randomised the order of the tasks and the query permutation for each participant.
The tasks were defined in accordance with a task template designed to situate the participants in a scientific essay writ-ing scenario, which is most suitable for creating exploratory search tasks [38]. To preserve consistency among the tasks, all the task descriptions followed the same template. Note that we refer to the results of one query as a list of articles: topic X. We provide you with three lists of articles that we have retrieved using three different queries. Go through each list in the order we give you and tick articles that you are interested in further reading to consider in your essay. Follow your natural scientific literature review-ing style when scanning the article lists. You have three minutes to go through each list. We will inform you when the three minutes are over and then you can move on to the next list. X 
Google Scholar is the most commonly used literature search tool by computer scientists [3]. We used Google Scholar to retrieve 100 articles per query (from 10 SERPs), ranked ac-cording to the relevance for that query. As each task con-sisted of three queries, we retrieved 300 articles in total per task (100 articles/query  X  3 queries/task). According to user observations (see Section 4.1), searchers decide to click on a result based on the Google Scholar information snippet, therefore we extracted all the primary information provided with each result item in Google Scholar.

Prior work suggests that search queries affect the user per-ception of search results [26], hence the query display could prime the search behavior. However, users do not always see the actual search query in systems that support exploratory search through techniques such as query expansion. There-fore, to avoid the influence of the search query on the search behavior, we only displayed results of the query and not the query itself. Participants could see the results retrieved for one query at a time. Once they completed scanning and ticking interesting articles from one list, then the list of next 100 articles for the next query was displayed. We provided a tick-box on the left side of every article and the participants could tick the articles that they were interested in. We in-formed the participants that ticking an article is analogous to clicking the URL and opening an article.
We conducted the experiment on a desktop computer in a controlled room. We first gave the participants the printed task description. Next, we provided the first list of articles. We logged all the articles that the participants ticked and the time. While the participants were performing the tasks, we logged their gaze distribution over the articles to corrob-orate the number of articles seen before clicking an article. We instructed the participants to think aloud while perform-ing the tasks and we used a voice recorder to record their thinking aloud. A pilot study showed it takes approximately three minutes to examine one list of articles without getting overly exhausted. Hence, the participants were given three minutes to go through one list. We used a timer and in-formed the participants when the three minutes had passed.
Every participant performed six search tasks and each search task involved searching through three lists of articles, therefore we obtained data from 432 search sessions (3 re-sults lists  X  6 search tasks  X  24 participants). All together there were 4,414 click actions. We used all data without removing any outliers to keep the prediction task realistic.
According to our model, the gradients of the Seen X  X licked curves should decrease with the increase in the subjective specificity (or narrowness of the results), and they should follow a natural logarithmic distribution. In order to confirm this, we analysed the overall distribution of the user informa-tion gain over information seen for the three types of results. Table 2: Logarithmic regression models and model fit ( R for number of articles Seen X  X licked. Breakdown per Broad, Intermediate and Narrow search results.
 Results Type Model Fit ( R 2 ) Broad 3 . 83 ln( n )  X  3 . 59 0.97 Intermediate 2 . 40 ln( n )  X  2 . 06 0.97 Narrow 2 . 05 ln( n )  X  1 . 96 0.97 Table 3: The Wilcoxon signed-ranked test on (left) the gra-dients between the models and (right) the case-specific term,  X  , for Broad (B), Intermediate (I), and Narrow (N) results. Results Z p-val r Z p-val r B &amp; I  X  4 . 20 &lt;. 001  X  . 60  X  3 . 77 &lt;. 001  X  . 54 B &amp; N  X  4 . 29 &lt;. 001  X  . 62  X  3 . 60 &lt;. 001  X  . 52 I &amp; N  X  2 . 71 &lt;. 01  X  . 39  X  . 057 . 954  X  . 01 Figure 2a shows the overall number of articles Seen X  X licked averaged over all the participants over the three types of search results. As our model predicts, the gradient of the Seen X  X licked curve decreases as the results become narrower for the user X  X  information need.

Next, we constructed gain curves for the three types of results for each participant averaging over the six tasks they performed. Using logarithmic regression, we calculated the model including the gradient (  X  ) and the case-specific term (  X  ) of predicted curves for every participant (Section 3). Table 2 provides the summary of the prediction models of the three types of results and the model fit, R 2 , calculated for the overall click data.

We used Wilcoxon signed-ranked test to statistically com-pare the gradients of the predicted models of each type of results. Gradients of the broad results ( median 3 . 56) were significantly greater than those of intermediate (3 . 08) and narrow results (2 . 04). The gradients of the predicted mod-els of the intermediate results were significantly greater than that of narrow results.

To see whether, and how, subjective specificity affects the case-specific term  X  , we conducted a Wilcoxon signed-ranked test. For broad results, the values for  X  ( median 2 . 41) were significantly greater than for either intermedi-ate (1 . 16) or narrow results (1 . 69). However, the difference was not significant between intermediate and narrow results, suggesting that the case-specific term is not as sensitive to subjective specificity as the gradient. Table 3 provides test results for both  X  and  X  .

To summarize, the results confirm that when the subjec-tive specificity increases, the gradient of the Seen X  X licked curve decreases. This suggests that the effective information gain reduces with an increase in narrowness of the results, which is validated by our model.
Since the participants went consecutively through the three types of article lists, it is necessary to ensure that the or-der of the articles lists has no effect on the information gain results. Note the difference in gradients for the Narrow results. curves in Figure 2. To this end, we compared the number ( mean  X  std . dev ) of articles all the users clicked from the first (6 . 1  X  2 . 6), second (5 . 6  X  1 . 9), and third (6 . 0  X  2 . 1) re-sult lists according to the order they were presented. The results show that the order of results presentation has no effect on the number of articles clicked. In order to validate this statistically, we performed a Friedman test on the av-erage number of articles participants clicked from the first, second, and third article lists. It shows there are no signif-icant differences between the number of articles clicked in each list ( p = . 717).
To understand the effect of prior experience on our model, we compared the gradients of the models predicted for the results with broad, intermediate, and narrow subjective speci-ficity between participants with different levels of experi-ence. The pre-study questionnaire confirms that PhD stu-dents have more experience than MSc students in scientific information-seeking and exploratory search (Section 4.2). According to our model, we expect the prior experience of the participants to affect the gradients of the Seen X  X licked graphs. As expected, Figure 3 shows that the gradients of the Seen X  X licked graphs for the participants with lower level of experience (MSc students) is higher than that of the more experienced participants (PhD students).
 Table 4: Results of the Mann-Whitney X  X  U test for MSc and PhD students comparing gradients of predicted models of the broad, intermediate, and narrow results. The Seen X  Clicked curves for MSc students show significantly steeper gradients for all three types of results.
 Results U Z p-value r (effect size) Broad 36  X  1 . 99 &lt; . 05  X  . 41 Intermediate 33  X  2 . 17 &lt; . 05  X  . 44 Narrow 26  X  2 . 58 &lt; . 05  X  . 53
We used Mann-Whitney X  X  U test to compare the gradients of the models predicted between the two groups. The results are summarized in Table 4. The gradients of the predicted models of broad results of MSc students ( median 4 . 48) were Table 5: Correlation analysis between the model gradi-ents for Broad, Intermediate and Narrow results with re-spect to user familiarity with scientific information-seeking (left), and how often they explore unfamiliar research topics (right). ( N = 24) Query r  X  p-value r  X  p-value Broad  X  . 35 &lt; . 05  X  . 46 &lt; . 01 Intermediate  X  . 49 &lt; . 01  X  . 47 &lt; . 01
Narrow  X  . 52 &lt; . 01  X  . 39 &lt; . 05 significantly steeper than that for the PhD students (2 . 94). The gradients of the predicted models for results with inter-mediate subjective specificity of MSc students (3 . 08) were also significantly greater than that of PhD students (1 . 67). Similarly, the gradients of the predicted models of the nar-row results of the MSc students (2 . 46) were significantly greater than that of the PhD students (1 . 49).

The results clearly show that the participants with lower level of experience in scientific information-seeking (MSc students) click more results indicating a lower subjective specificity for all the three types of results than the more experienced participants (PhD students). In order to un-derstand this behavior we analyzed the correlation between prior experience and gradients of the models, and think-aloud recordings.
 The correlation analysis suggests that the gradients for Seen X  X licked curves are lower for the users with more experi-ence in scientific information-seeking and exploratory search (PhD students) (Table 5). According to the voice recordings, PhD students had more specific criteria for the type of arti-cles they needed. For example, 11 PhD students explained that they were more interested in review articles than arti-cles about a specific topic. They also distinguished scientific articles from books to refrain from clicking too many books and paid more attention to the publication year to avoid older articles. However, MSc students clicked all the articles that have relevant titles.
To investigate whether in-session learning have an effect on our model, we analyzed the Seen X  X licked graphs per per-mutation condition (Section 4.3). These permutations emu-late transitions in results-specificity during a search session.
For each query permutation, Seen X  X licked graphs of the broad, intermediate and narrow results followed our model. As expected, when narrow results were considered after the broad ones, the gradients of Seen X  X licked graphs of the nar-row results were greater than when narrow results were con-sidered before broad results. A possible reason for this differ-ence is an increase in user knowledge or in-session learning, which can be seen in Figure 2b and 2c. Table 6 shows the model prediction for these two scenarios. We can see that the gradient of the predicted model of the narrow results has increased from 1 . 9 to 2 . 3 when narrow is given after broad.
We conducted statistical test on the gradients of the mod-els predicted for the three types of results given for the six tasks for every participant (3 queries  X  6 tasks = 18 mod-els/participant). The Friedman test shows no significant difference between the gradients for broad ( p = . 051) and intermediate curves ( p = . 46) among the six query permuta-tions. However, the difference is significant for the gradients of the models predicted for narrow curves among the six query permutations (  X  2 (5) = 19 . 4 ,p &lt; . 01). In order to en-sure this difference for narrow results was due to whether the broad results were before or after, we split the gradients predicted for narrow curves in to two groups; narrow re-sults were presented before broad results (respectively NIB , NBI , INB ) and vice-versa ( BIN , BNI , IBN ). A Friedman test showed no significant difference between the gradients of the narrow results in permutations NIB , NBI , and INB ( p = . 86). Similarly, the difference was not significant be-tween BIN , BNI , and IBN permutations ( p = . 95).

These results suggest the gradients of the predicted mod-els of narrow results change only if they are presented after the broad results. An explanation for this difference is that when results gradually become narrower, the user is likely to make better use of the narrow results than in the oppo-site direction. As a result of this behavior, when the narrow results are presented after the broad results, the number of articles clicked by the user increases and the effective in-formation gain approaches that of the intermediate results. Further, the number of articles that overlap between the three lists is less than 4%. Therefore, we conclude that this difference is not due to the article overlap between results but rather a result of the learning effect.
 Table 6: Logarithmic regression models and fit ( R number of articles Seen X  X licked for broad, intermediate, and narrow results with regard to whether the Broad results were considered before (left) or after (right) the Narrow results. Results Model R 2 Model R 2 Broad 4 . 1 ln( n )  X  3 . 3 0 . 97 3 . 7 ln( n )  X  3 . 9 0 . 96 Intermediate 2 . 3 ln( n )  X  2 . 3 0 . 96 2 . 4 ln( n )  X  1 . 8 0 . 97 Narrow 2 . 3 ln( n )  X  1 . 5 0 . 98 1 . 9 ln( n )  X  2 . 5 0 . 95 Table 7: Cross-validation results for the broad, intermediate, and narrow models built for MSc and PhD students.
 Results MSc ( R 2 ) PhD ( R 2 ) Broad 0 . 71 0 . 88 Intermediate 0 . 86 0 . 95
Narrow 0 . 86 0 . 60
In order to further validate the model, we report results obtained using leave-one-out cross-validation. Since the model is affected by the prior experience of the users, we split the data into two groups by experience (MSc and PhD). For each group we construct separate models per subjective speci-ficity level, by leaving one participant out and fitting the model over the others. We then use this model to predict the Seen X  X licked curves for the left-out participant, calcu-lating the model fit ( R 2 ) with the actual Seen X  X licked curve of that participant. We iterate over all participants, and re-port the average R 2 per group. We obtained reasonably high R 2 values for both groups for the three subjective specificity levels as reported in Table 7.
Last, we perform a preliminary study evaluating the prac-tical applicability of our model. To this end, we investigate how well the subjective specificity can be predicted based on the model gradient over the first 33 out of 100 articles. That is, we check whether the system can infer the subjec-tive specificity while the user is still going over the full list, and can hence offer targeted assistance in doing so.
We use Weka [39] to train C4.5 decision trees [28] using 10-fold cross-validation. Despite our small training data, we already obtain 72 . 1% accuracy and an AUC of 0 . 687 when classifying between broad and narrow results. This means we beat the baseline, resp. 50% and 0 . 5 by a clear margin. When considering three classes of results, we obtain an accuracy of 48 . 1% and an AUC of 0 . 589 against a baseline of 33%, and 0 . 5, again a clear improvement. It is interesting to note that performance is stable between the first 33, 50, or all 100 articles. Given the stark differences in slopes seen (see, e.g., Fig. 3) it seems reasonable that with more training data and more advanced classifiers reliable calls can be made given only the first 10 or so articles.
Overall, study 1 confirms that we can model information gain in exploratory search with a logarithmic function of the number of results seen by the user. It validates that the gra-dient of the Seen X  X licked curves decrease with an increase of the subjective specificity, hence we can estimate subjective specificity using our model. Further, the results suggest that our model is sensitive to both in-session learning and prior experience of the user. When the user has more experience with exploratory search and scientific information-seeking the gradient of the Seen X  X licked curve decreases, because she has a specific criteria for the type of information she needs. If a user gradually moves from broad to narrow re-sults, then in-session knowledge gain would help the user to recognize more useful articles even from narrow results, increasing the gradient of the Seen X  X licked curve. These re-sults suggest that our model could be used to predict when a user actually needs help with narrow results. Preliminary classification indicates the applicability of our model in a real IR system.
In order to validate our model in a more natural setting we conducted a second study involving ten computer science students exploring scientific articles for an actual informa-tion need. Participants of this study were not involved in our Study 1. Four were MSc students looking for scientific liter-ature to include in their theses. The other participants have just finished their MScs and were exploring new research topics to prepare their PhD proposals. Google Scholar is the search tool they all use, therefore we implemented an interface similar to Google Scholar which enabled the par-ticipants to issue search queries and view results that we extracted from Google Scholar. We displayed 40 articles per page with same information as in Google result snip-pets and allowed every participant to conduct their natural exploration using our search interface for two hours. We did not impose any restrictions on the search process, and they could conduct search in the same way as with Google Scholar, i.e. click articles, read opened articles, and make notes. We logged their search queries, retrieved results, and clicked articles with time. We used experts in each search topic to assign the search results of every query in to one of the three categories: broad, intermediate, and narrow. The experts were either postdoctoral researchers or profes-sors specializing in the search topic. Most of the experts (6/10) were supervisors of the participants and so had an idea about the level of knowledge of the participants X  to predict the subjective specificity. To measure the quality of Figure 4: Seen X  X licked curves constructed by averaging over all the tasks performed by all the users with the broad, in-termediate, and narrow search results in Study 2. Note that in Study 2, a SERP contained 40 articles unlike 100 arti-cles in Study 1 which makes the Seen X  X licked curves of two studies slightly different. categorization, part of the assessments were conducted by two experts (6/10). We run Cohen Kappa test to measure the inter-annotator agreement between the experts. Kappa indicated a substantial agreement (Kappa = . 67 , p &lt;. 01).
In total all the participants have issued 142 search queries where 36% of them has retrieved broad search results, 37% has retrieved intermediate results, while 27% has retrieved narrow search results. All together there were 339 clicks.
We plotted the Seen X  X licked curves for broad, intermedi-ate, and narrow results by taking the average over all the participants, as shown in Figure 4. As in Study 1, the gra-dients of the Seen X  X licked curves decrease predictably with increasing subjective specificity. We computed the model for each curve using logarithmic regression. As expected, the broad curve has the highest gradient (  X  = 1 . 20) with model fit R 2 = 0 . 96, intermediate curve has the second high-est gradient (  X  = 0 . 73 ,R 2 = 0 . 98), while the narrow curve has the lowest gradient (  X  = 0 . 50 ,R 2 = 0 . 99). To further confirm that the difference between the gradients of three curves are statistically significant we computed the model for individual Seen X  X licked curves for each participant and conducted Wilcoxon signed-ranked test. The test results in-dicate that predicted models of the Seen X  X licked curves of broad search results have a significantly higher gradient than that of both intermediate ( Z =  X  2 . 31 , p &lt;. 05) and narrow ( Z =  X  2 . 67 , p &lt;. 01) search results. However, the differ-ence between the gradients of the predicted models of the intermediate and narrow search results was not significant ( Z =  X  1 . 836 , p = . 06). We could expect the difference be-tween the Seen X  X licked curves of intermediate and narrow search results to be small, because as the results of Study 1 indicate (see Section 4.4.4) users gain knowledge when they gradually transit from broad to narrow search results. Fur-ther, in Study 2 participants could browse through clicked articles, therefore they spent time reading them in addi-tion to scanning results, which explains why the gradients of models in Study 2 are less than those in Study 1.
Overall, these results suggest that our model is applicable to natural exploratory search tasks.
We also analyzed the percentage of search queries that re-trieved broad, intermediate, and narrow search results with time to verify that search results become increasingly nar-row over time. Most of the search queries (56%) that were issued within the first 20 to 40 minutes have retrieved broad results. Interestingly, some participants (10%) started their search session with queries retrieving already narrow results. After the first 1.5 hours, percentage of search queries that retrieve narrow results has increased to 42%. This confirms the common sense insight that during exploration over time users gradually narrow down their search queries using spe-cific terms [36]. Our model helps an IR system to infer whether these broad or narrow search results correlate with what the user is actually expecting.
Overall, study 2 confirms that our model can be used to predict subjective specificity in natural exploratory search tasks. The analysis of broad, intermediate, and narrow search results with time indicate that over time users issue narrower search queries. However, exploratory search may continue over several hours or even years and would involve offline learning through other media such as books, and so-cial networks [25]. Therefore, it is not feasible to use time as a parameter to model subjective specificity. This study further verifies that the number of search results users click in exploratory search is affected by in-session learning. We postpone the classification experiments here, as these re-quire us knowing the actual user information need for each query. Moreover, proper implementation requires a longitu-dinal study to build a reference model per user such that we can compare the Seen X  X licked curve against this reference model. This is beyond the scope of this paper.
This paper has contributed a model for predicting the subjective specificity of search results. The model builds on earlier insights about exploratory search and Information Foraging Theory, assuming that for every individual there is an idiosyncratic baseline curve for information gain. Given this curve as a reference point, it predicts whether the cur-rent search results are too broad or narrow for the user X  X  information need. We empirically validated this model in two studies which show that when search results become too narrow X  X r, high in subjective specificity, X  X he gradient of the Seen X  X licked graph decreases significantly.

We show that our model applies in both a controlled en-vironment and in realistic open ended settings. Our classifi-cation results show that our model indeed captures valuable information about the subjective selectivity of results. Al-though the exercise is preliminary, the results are promising: ideally one would train over much more data, over more re-sults, use a more advanced classifier, and, in particular, take timings between clicks into account. However, these results do tell us that our model could be employed within an IR system to quickly obtain an educated guess on how nar-row/broad the current search results are with regard to the current state of the users X  information seeking process X  X nd adapt its behavior accordingly.

The model has valuable implications for exploratory search systems. For example, it has potential applications in sys-tems that support exploratory search by making query sug-gestions [12], organizing information according to facets [40], directing search by predicting keywords [14], or providing visualizations and summaries of results [24]. These systems could use our model to predict whether the suggested results are broader or narrower than the information need of the user. Furthermore, our model could be used as a substitute for relevance feedback techniques that put the user through tedious feedback loops. Even though a hierarchical ontol-ogy such as Open Directory Project (ODP, www.dmoz.org) could be used to suggest whether a search query is referring to a broad/narrow topic, such an ontology cannot predict whether the search results are actually broad/narrow with respect to the current information need of the user. Our model could also be applied to reinforcement learning based solutions to predict the right balance between exploration and exploitation according to the subjective specificity [14]. For example, we could increase the level of exploration for novice researchers in a given field based on their current in-formation need in order to expose them to a large area of the information space. On the other hand, we would decrease the level of exploration for more advanced researchers thus exposing them to narrower search results. This model could also be used by IR systems to real-time update search results and visualizations according to subjective specificity [23].
An important open challenge is to incorporate our model into a running IR system. Our preliminary classification study shows a system without extra sensors and using only a simple classifier can obtain informed estimates on the sub-jective specificity while the user is interacting with its re-sults. By leveraging larger training data and more sophisti-cated classification/regression algorithms significant improve-ments can be expected X  X n particular when user click/view timings and history data are taken into account. In the future, we will collect longitudinal data from users to con-struct reference models and evaluate the predictive power of the model. The cross-validation results suggest we could build a common reference model for users with similar back-grounds, and hence we may be able to build reference mod-els for a set of known background levels and apply it to new users without building individual reference models.

To conclude, our model is useful for the design of person-alized exploratory search systems that adjust the search re-sults according to the evolving information needs and knowl-edge of the user in a given topic.
A. Oulasvirta and J. Vreeken are supported by the Clus-ter of Excellence  X  X ultimodal Computing and Interaction X  within the Excellence Initiative of the German Federal Gov-ernment. D. G lowacka was supported by The Finnish Fund-ing Agency for Innovation (under projects Re:Know and D2I) and by the Academy of Finland (under the Finnish Centre of Excellence in Computational Inference). This work has been partly supported by MindSee (FP7  X  ICT; Grant Agreement # 611570). [1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [2] O. Alonso, R. Baeza-Yates, and M. Gertz. Exploratory [3] K. Athukorala, E. Hoggan, A. Lehti  X  o, T. Ruotsalo, [4] F. Baskaya, H. Keskustalo, and K. J  X  arvelin. Modeling [5] M. J. Bates. The design of browsing and berrypicking [6] D. H. Chau, A. Kittur, J. I. Hong, and C. Faloutsos. [7] M. J. Cole, J. Gwizdka, C. Liu, N. J. Belkin, and [8] D. R. Cutting, D. R Karger, J. O. Pedersen, and [9] H. A. Feild, J. Allan, and R. Jones. Predicting [10] P. M. Fitts and J. R. Peterson. Information capacity [11] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and [12] W.-T. Fu, T. G. Kannampallil, and R. Kang.
 [13] W.-T. Fu and P. Pirolli. Snif-act: A cognitive model of [14] D. G lowacka, T. Ruotsalo, K. Konyushkova, [15] R. L. Goldstone and B. C. Ashpole. Human foraging [16] A. Hassan and R. W. White. Task tours: helping users [17] A. Hassan and R. W. White. Personalized models of [18] A. Hassan, R. W. White, S. T. Dumais, and Y. Wang. [19] M. Hearst. Search user interfaces . Cambridge [20] I. Hsieh-Yee. Research on web search behavior. Libr. [21] M. Karimzadehgan and C. Zhai. Exploration [22] D. Kelly and X. Fu. Elicitation of term relevance [23] J. Y. Kim, M. Cramer, J. Teevan, and D. Lagun. [24] B. Kules, M. Wilson, M. C. Schraefel, and [25] G. Marchionini. Exploratory search: from finding to [26] N. Phan, P. Bailey, and R. Wilkinson. Understanding [27] P. Pirolli and S. Card. Information foraging. Psych. [28] J.R. Quinlan. C4.5: Programs for Machine Learning . [29] F. Radlinski, R. Kleinberg, and T. Joachims. Learning [30] J-F. Rouet. The skills of document use: From text [31] A. G. Sutcliffe, M. Ennis, and S. J. Watkinson. [32] J. Teevan, C. Alvarado, M. S. Ackerman, and D. R. [33] P. Vakkari. Task complexity, problem structure and [34] P. Vakkari, M. Pennanen, and S. Serola. Changes of [35] R. W. White, P. N. Bennett, and S. T. Dumais.
 [36] R. W. White and R. A. Roth. Exploratory search: [37] B. M. Wildemuth. The effects of domain knowledge on [38] B. M Wildemuth and L. Freund. Assigning search [39] I.H. Witten and Eibe Frank. Data Mining: Practical [40] K.-P. Yee, K. Swearingen, K. Li, and M. Hearst.
