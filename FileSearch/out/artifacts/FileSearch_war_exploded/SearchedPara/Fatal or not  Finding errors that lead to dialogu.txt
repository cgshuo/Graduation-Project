 Chat-oriented or open-domain dialogue systems have recently been attracting attention from so-cial and entertainment aspects (Bickmore and Cas-sell, 2001; Banchs and Li, 2012; Wilcock and Jokinen, 2013). However, since they need to deal with open-domain utterances, which current natural language processing techniques are not mature enough to handle appropriately, the sys-tem inevitably makes errors. This discourages users from talking to the system, leading to di-alogue breakdowns in conversation (Martinovsky and Traum, 2003). Here, dialogue breakdowns de-note points in dialogue where users are unable to continue the conversation.

This paper aims to find errors that lead to dia-logue breakdowns in chat-oriented dialogue sys-tems. Our approach is two-fold: (1) identify error types in chat-oriented dialogue systems, and (2) calculate their impact on dialogue breakdown. For (1), we first collect chat dialogues between an au-tomated system and users, annotate the dialogues with dialogue breakdown labels, and collect com-ments that describe the error that led to the break-downs. After that, we apply automatic cluster-ing methods to the comments to obtain clusters of comments, each of which corresponds to a partic-ular error type. For (2), we calculate the correla-tion between an error type and the degree of dia-logue breakdown it incurs. This way, we can quan-tify the impact of an error type on dialogue break-down. By our approach, we hope to distinguish fatal errors from non-fatal ones, providing useful guidelines for dialogue system developers to avoid breakdowns in chat-oriented dialogue systems. For data collection, we asked dialogue researchers and their collaborators in Japan to use our chat-oriented dialogue system. The system is text-based and enables chatting in Japanese. It was built by wrapping a chat API provided by NTT DOCOMO (Onishi and Yoshimura, 2014). Since the API works on the basis of (Higashinaka et al., 2014), which uses a number of natural language processing techniques to understand and generate utterances, we expected to obtain a wide variety of dialogues, and hence, a variety of errors.
A total of 116 users chatted with the system, re-sulting in 1,146 dialogues. Here, each dialogue was controlled to be 21 utterances long: one sys-tem prompt with ten utterances each from the user and system. Then, we randomly sampled 100 dialogues (called the init100 data set) for di-alogue breakdown annotation. Twenty-four anno-tators subjectively labeled each system utterance in init100 with one of the following three dialogue breakdown labels: (NB) Not a breakdown: The conversation is (PB) Possible breakdown: The conversation is (B) Breakdown: The conversation is difficult to The annotators were instructed to provide volun-tary comments to describe the errors that led to the breakdowns. Table 1 shows the statistics of the di-alogue breakdown labels and comments. Figure 1 shows an example dialogue with the number of breakdown labels and comments for each system utterance. In this example, S5 and S6 were anno-tated with nine and eleven breakdown labels, re-spectively, both having four comments.

The inter-annotator agreement of dialogue breakdown annotation in Fleiss X  kappa was 0.276, which seems relatively low. One reason for this is obviously the subjective nature of the task. An-other possible reason is that we intentionally did not set rigid guidelines for dialogue breakdown annotation so as to explore possible error types in chat-oriented dialogue systems. When we merge PB and B and make it a two-class annotation, the agreement becomes 0.396 (moderate agreement), showing that the subjects share some common conception about dialogue breakdown.
 Breakdown label # of labels # of comments Table 1: Statistics related to breakdown labels and comments in init100 data set. Note that NB also had some opinions as comments. 3.1 Automatic clustering of comments We first need to identify the error types in chat-oriented dialogue systems. For this, we applied an automatic clustering method to the comments to obtain clusters of comments. Our idea is that, since each comment describes a particular error that led to a breakdown, a cluster of comments is likely to represent an error type. Since the num-ber of clusters is difficult to know in advance, we turn to a non-parametric Bayesian method called the Chinese restaurant process (CRP) as a cluster-ing method. CRP can infer the number of clusters automatically from data (Pitman, 1995).

We applied CRP to the 1,511 comments given
ID Size Interpretation Representative words in the cluster 10 35 Diversion story, different 15 25 Mismatch in conversation match, story to breakdown labels (B labels). For the clustering, we used the same procedure as (Higashinaka et al., 2011); each datum (comment) was represented by a word frequency vector, and the probability that it belonged to a particular cluster was calculated by using the likelihood that the words are gener-ated from the word distribution of that cluster. The hyper-parameters  X  and  X  were both set to 0.1 and the number of iterations for Gibbs sampling was 10,000. See (Higashinaka et al., 2011) for the de-tails of the procedure.

Table 2 shows the clustering results. We ob-tained 17 clusters. For each cluster, we mined representative words by a log-likelihood ratio test, which uses a two-by-two matrix to test the inde-pendence of a word to a particular cluster. By looking at the representative words and also the raw comments, we came up with the interpreta-tions of the clusters as indicated in the table. Al-though we do not go into the details of the clus-ters one by one, each cluster seems to success-fully represent a certain error type in chat-oriented dialogue systems. We also applied CRP to the 3,329 comments given to PB and B to obtain sim-ilar clusters except that we additionally had clus-ters whose interpretations are as follows: inability to handle invalid user input, missing topic, miss-ing information, mismatch in response, no reac-tion, and no information. They account for about 13.3% of the comments and mostly concern miss-ing elements (such as missing arguments) in dia-logue. Since such missing elements can be com-plemented by follow-on utterances in dialogue, they only appear in the comments for PB; they do not lead to an immediate dialogue breakdown.
To further categorize the clusters, we applied a hierarchical clustering (an agglomerative cluster-ing) to the clusters. Here, a cluster was represented by the word frequency vector of all comments con-tained in the cluster, and the similarity of the clus-ters was calculated by cosine similarity of word frequency vectors. For the linkage criterion, we used Ward X  X  method. Figure 2 shows the cluster-ing results. The figure indicates that there are the following eight main error categories (E1 X  X 8): (E1) Clusters 2 and 16 concern the general ability (E2) Clusters 7, 5, and 8 relate to context aware-(E3) Clusters 13, 3, and 14 concern the language (E4) Clusters 4 and 6 concern the response abil-(E5) Cluster 1 relates to the exhibition of an in-Figure 2: Hierarchical clustering applied to the ob-tained clusters. The numbers in parentheses de-note cluster size. (E6) Clusters 9 and 12 relate to the social ability: (E7) Clusters 0 and 15 concern the understand-(E8) Clusters 10 and 11 relate to the awareness of 3.2 Analyzing the impact of error types Having identified the error types and error cate-gories, we investigated their impact on dialogue breakdown. For this purpose, we examined the correlation between an error type and its degree of breakdown: the higher the correlation, the more it is related to dialogue breakdown. Specifically, we calculated the correlation ratio (  X  ) between the existence of a comment belonging to a particular cluster (error type) and the number of breakdown labels (B labels). Note that the correlation ratio
ID Interpretation Cat  X  13 Word usage error E3 0.18 16 Analysis failure E1 0.17 12 Violation of common sense E6 0.15 10 Diversion E8 0.09 11 Topic-change error E8 0.06 15 Mismatch in conversation E7 0.06 14 Expression error E3 0.02 Table 3: Correlation ratio (  X  ) between the exis-tence of a comment of a cluster (error type) and the number of breakdown labels.  X  X at X  denotes the error category of an error type. is equivalent to Pearson X  X  correlation coefficient except that it can be applied to categorical data. The  X  ranges from 0 to 1. For calculation, we first extracted data that had one or more B labels and one or more corresponding comments (we had 556 such samples in our data). Then, we calculated the correlation ratios.

Table 3 shows the correlation ratios for the er-ror types. Clearly, not all error types have the same level of correlation. At the top of the ta-ble, there are four salient error types with similar  X  values:  X  X ot understandable X ,  X  X gnore user ques-tion X ,  X  X eneral quality X , and  X  X nclear intention X . Putting aside  X  X eneral quality X , which seems to concern the overall dialogue ability, the error types that we need to consider as fatal would seem to be the other three. Other errors seem to be less im-portant with lower  X  values.  X  X xpression error X , which concerns the use of unnatural expressions, was found the least important.

When we look at the error categories, we can see an interesting result that it is NOT the error cat-egory that determines the fatality of errors but the specificity of error types. For example,  X  X ot un-derstandable X  and  X  X ismatch in conversation X  are both under error category E7 but have totally dif-ferent effects on perceived breakdown. The same can be said for error types in E2.
Note that, although the values of correlation ra-tio seem rather low, the correlation often becomes low when it comes to subjective judgments (Hi-gashinaka et al., 2004). Considering that we deal with chat-oriented dialogues, which are less re-stricted than task-oriented ones, we consider the current values of correlation ratio to be accept-able. Here, the important finding is that several error types are comparatively more important than the others. Few studies have analyzed breakdowns in conver-sation. One exception is the study by Martinovsky and Traum (2003), who discussed possible causes of breakdowns they observed. Our work is differ-ent in that we systematically identify error types and quantitatively evaluate their effect. Our work can be seen as listing up errors in dialogue sys-tems. A number of studies have aimed to create a taxonomy of errors (Bernsen et al., 1996; M  X  oller, 2002; Paek, 2003), but their taxonomies are cre-ated manually and focus on task-oriented dialogue systems. By processing dialogue data with dialogue break-down annotations and comments, this paper iden-tified 17 error types that can be further categorized into eight error categories. By calculating corre-lation ratios, we discovered three error types that can be fatal:  X  X ot understandable X ,  X  X gnore user question X , and  X  X nclear intention X . To avoid dia-logue breakdowns, it is suggested that we need to make clear the meanings of system utterances, not ignore user questions, and show some intention behind system utterances. The findings will be useful for dialogue system developers who want to realize smooth human-machine interaction in chat-oriented dialogue systems and possibly in di-alogue design as a whole.

For future work, we plan to consider ways to improve systems on the basis of our findings and also verify the generality of the results on data using other systems. To accurately detect dia-logue breakdowns, dialogue systems researchers will need to collaborate. To this end, we are plan-ning to organize an evaluation workshop on dia-logue breakdown detection. For use in the eval-uation workshop as well as in dialogue research in general, we have released our data with all the The  X  X roject Next NLP X  project in Japan is ad-dressing the problems related to analyzing errors in natural language processing systems. One sub-group, comprising more than 32 researchers from 15 institutions, is collaborating in the performance of a dialogue task. We thank the members of this sub-group for the data collection, annotation, and fruitful discussions. We also thank NTT DO-COMO for letting us use their chat API for data collection.

