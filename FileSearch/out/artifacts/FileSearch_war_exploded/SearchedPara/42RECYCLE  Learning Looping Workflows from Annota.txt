 Workflows have drawn an enormous amount of attention in business environments and general problem-solving environments. Workflows are broadly used to analyze and describe any structured process, including business, scientific, and military, to improve efficiency, define responsibilities, and capture dependencies. A workflow captures a sequence of operations, the conditions and dependencies among them, along with the roles of participants, resource flows, and information flows. Business workflows tend to be control flow oriented, while problem-solving workflows (called scientific workflows) tend to be data flow oriented.

There has been a tremendous amount of work in workflow design, for example, Berry and Parastatidis [2003], Garcia et al. [2008], Microsoft [2010], and Lu et al. [2010], and an acknowledgement that it is extremely hard to design a complete and correct workflow from scratch [Bowers et al. 2006; Gil et al. 2007; Santos et al. 2009; Zinn et al. 2009]. In contrast, it is much easier for humans to demonstrate the solution than to state the solution declaratively. Our work therefore aims to learn the underlying workflow given some example demonstrations, that is, steps followed in the workflow for a specific case.

This article presents our approach to learning workflow models from example demon-stration traces. A demonstration is an annotated plan that represents a specific real-ization of a workflow given specific starting conditions and goals. Our motivation was to design a system that will first learn the target workflow from a small set of anno-tated plans carefully chosen by teachers (domain experts) and then use this workflow for a variety of purposes, including training novices, monitoring progress on tasks for workflow management systems, and directly automating simple tasks. For example, a workflow can help users navigate through software applications by highlighting which buttons to click in a complex Web form. Because experts X  time is extremely limited and expensive in most domains, we assumed that we would have a very small number of examples. R E C YCLE can therefore learn from asingle demonstration trace. Note that one cannot learn branches or loops from a single example, so it is important that the demonstrated tasks contain loops. Given that any algorithm that generalizes from a fi-nite number of instances may not completely capture the characteristics of the domain, R E C YCLE can revise the workflow based on observations from additional traces.
Our algorithm, R E C YCLE , captures control flow, data flow, and enablement conditions (i.e., the logic that guards branch selection). R E C YCLE is also able to learn structures in which the same action name has different purposes, and create abstract structures in which different action names have similar purposes. R E C YCLE is inspired by the previous work from the workflow mining and AI learning-to-plan literature. Like previous work in learning-to-plan [Hogg et al. 2008; Winner 2008], R E C YCLE utilizes explicit dependencies between the actions to generalize the plan. Unlike previous work in the planning community for learning from example traces, R E C YCLE does not require a complete, a priori model of the actions in the domain. Similar to workflows produced by the workflow mining community, such as, van der Aalst et al. [2004] and Yaman et al. [2009], R E C YCLE produces control flow with partially-ordered actions, loops, and branches. The key difference is that the workflow mining research ignores the data flow aspects of the problem; these approaches infer action dependency solely on the observed order of actions in the event logs. These approaches therefore recognize reorderings of tasks only when the reordering has been demonstrated. R E C YCLE ,on the other hand, leverages data flow analysis to discover dependencies between actions, and thus requires only one example to correctly infer ordering constraints. R E C YCLE is also able to compensate for some errors in data flow analysis.

We have demonstrated R E C YCLE in two domains: a Web service domain for scheduling patient transport, and a stand-alone system used by the military (built on top of .NET) for dispatching packages.

In this article, we describe the phases of R E C YCLE  X  X  learning algorithm: substructure analysis and node abstraction. To ground the discussion, we present a simplified flight reservation system with some of the important characteristics of the real domains we worked with. We present some real results from the patient transport domain, but for the sake of space do not further discuss the package dispatch domain. We define an action as an atomic operation A that has zero or more typed input parameters, I , and zero or more typed output parameters, O , and a set of execution conditions C that describe the conditions under which the action is applicable A ( I ) C  X  X  X  O . The type of a parameter may be simple (e.g., string, integer), complex (e.g., a tuple), or a collection whose elements are simple or complex. For example, the following action returns a set of flights between the from and to, which are of type Location : where C ={ airport (from)  X  airport (to) } . The execution conditions in C ensure that all from and to are airports. An action instance is an action with assignments to its inputs and outputs parameters and its execution condition is a set of quantifier-free predicates in which the input parameters are instantiated to assigned values. For example, an instance of lookupFlights is
A trace is a sequence of action instances. Each trace contains actions corresponding to multiple examples ( X  X ases X  in the event-log literature) and the actions are not explicitly associated with the specific examples. Note that one cannot learn branches or loops from a single example, so it is important that R E C YCLE be given a trace containing several examples (or multiple traces) that cover the breadth of alternatives in the action space.

There are implicit dependencies among the actions, that is, a trace element uses the output of a prior action. A data-dependency analysis algorithm such as IODA [Burstein et al. 2010] or LAPDOG [Gervasio and Murdock 2009] can be used to infer such depen-dencies between action instances. The output of such an analysis is a partially-ordered data flow graph such that the nodes correspond to action instances and an arc from n 1 to n 2 , denoted n 1 v  X  n 2 , indicates that n 1 produced value v in an output parameter; and v was consumed as an input parameter by n 2 . Essentially a data flow graph imposes a partial order on the nodes, ordering a producer action before all of its consumers. We will use the term annotated trace to denote the pair of a trace and its data flow graph. We present a simple flight reservation domain to illustrate the concepts and algorithms we present. The domain contains the following actions:  X  getPassengers () C 0  X  X  X  passengerList : Set[Passenger]
Returns a set of passenger objects with at least the fields, passengerID , origin and destination to denote the identification of a passenger, the initial, and final destina-tions.  X  lookupFlights ( from : Location ,to: Location ) C 1  X  X  X  flights : Set[Flight]
Returns a set flights between from and to .  X  getPrice (flight : Flight ) C 2  X  X  X  price : Currency
Returns the cost of the input flight.  X  reserve (flight: Flight , price : Currency ) C 3  X  X  X  reservation : ReservationID Reserves the flight and returns a reservation ID for reference.
  X  aisle ( reservation : ReservationID , passenger : Passenger ) C 4  X  X  X  seat : SeatID
Books an aisle seat for the given reservation and returns a seat ID.  X  window ( reservation : ReservationID , passenger : Passenger ) C 5  X  X  X  seat : SeatID
Books a window seat for the given reservation and returns a seat ID.  X  bookHotel ( passenger : Passenger , reservation : ReservationID ) C 6  X  X  X 
If the reservation indicates a need for overnight stay, books a hotel and returns a reservation ID.

Using these seven preceding actions, the workflow for flight booking can be repre-sented algorithmically, as shown in Table I, or visually, as shown in Figure 1. For simplicity, Table I performs seat selection before hotel booking, but reversing the order of those actions is also acceptable. To demonstrate this fact, Figure 1 contains two copies of the optional action bookHotel (before and after the seat selection action). Also note that in this domain the actions aisle and window are similar because both serve the purpose of booking a seat for a passenger, and in any given reservation usually only one of the two actions would be executed. Intuitively these two actions are two different ways to achieve an abstract action such as bookSeat . Depending on the passenger, the appropriate course of action can be taken, that is, if the passenger prefers aisle seats, then execute aisle otherwise window . Section 8 we will explain how such conditions can be learned and Section 6.1 describes how R E C YCLE recognizes that these actions can be abstracted into a single action.

Example 1 . Consider a trace in which flight and hotel reservations are booked for three passengers. Figure 2 shows an example data flow graph for this trace containing 17 actions. In addition to the action names the nodes have unique identifiers. The interesting observation is the fact that for each traveler only one of the getPrice actions have outgoing edges (i.e., provide inputs for the rest of the actions), suggesting the existence of an unobserved operation such as a mental choice among the options. A comparison of the sample data flow in Figure 2 and the algorithmic workflow in Table I highlights another important requirement for R E C YCLE .R E C YCLE must infer the additional data flow dependencies from the  X  X nused X  getPrice actions. In the original trace, only one getPrice action contributed to the reservation. In the learned workflow, however, any of the getPrice calls might be the correct one. R E C YCLE must infer missing data flow dependencies for both abstract nodes and loop nodes, accumulating values (such as price f ) where needed. R
E C YCLE is a structural loop learner inspired by the previous work from AI learning-to-plan, workflow mining, and programming by demonstration. With the exception of WIT [Yaman et al. 2009], all of this previous work only learned control flow. Data flow analysis, meanwhile, has been examined in Web services research under the guise of provenance , where control flow concepts are not examined.
 In the AI planning community, learning-to-plan is an active area of research; see Minton [1994] and Zimmerman and Kambhampati [2003] for surveys. The majority of the work concentrates on learning search control knowledge in the form of control rules [Minton et al. 1990; Aler et al. 2002], macro operators [Botea et al. 2005; Coles and Smith 2007], heuristic functions [Yoon et al. 2008], and reactive policies [Mart  X   X n and Geffner 2004; Yoon et al. 2005]. These concepts are fundamentally different from learning workflows. Notably, R E C YCLE  X  X  workflows are equivalent to learning a domain-specific problem solver whose execution does not involve search.

The abstract actions (loops and branches) in R E C YCLE workflows align more closely with the work on learning hierarchical task networks from example plans [Hogg et al. 2008; Ilghami et al. 2002; Li et al. 2009; Nejati et al. 2006]. In general, this work needs the complete action definitions, that is, preconditions and effects, to learn from previous plans. Some even need more information, such as the list of high-level actions [Hogg et al. 2008], decomposition trees [Ilghami et al. 2002], or the hierarchical relation-ships among tasks [Nejati et al. 2006]. Li et al. [2009] apply probabilistic grammar inference to identify frequently used patterns in the plans and do not need action models; however, the work ignores the action parameters and conditions completely.
R E C YCLE was originally based on the concepts introduced in L OOP DISTILL [Winner 2008], which creates a domain-specific problem solver whose execution does not in-volve search. Moving to a more complex, realistic domain required that we remove the following limitations.  X  X n addition to the trace, L OOP DISTILL requires action semantics in the form of
STRIPS preconditions and effects to drive action dependencies. R E C YCLE operates on the annotated trace, and infers action preconditions and dependencies based on data flow.  X  X  OOP DISTILL cannot support data flow. R E C YCLE  X  X  workflows capture this critical requirement, including inferring missing data flow edges.  X  X  OOP DISTILL cannot learn both loops and conditional branches from the same trace, nor can it learn complex loop structure. R E C YCLE learns nested loops and branching structure by recursing over previous generalizations.  X  X  OOP DISTILL cannot learn the loop variable (it assumes that the loop variable is the first parameter of every action inside the loop). R E C YCLE infers the loop variable from the most-utilized set in the loop body, even if some actions do not use that variable.
In workflow/process mining community, similar approaches include petri-net rep-resentations [van der Aalst et al. 2004; 2002; Kindler et al. 2006], grammar induc-tion [Cook and Wolf 1995; Herbst and Karagiannis 1998; Yaman et al. 2009], and probabilistic models like HMMs [Aires da Silva and Ferreira 2009; Silva et al. 2005]. With the exception of WIT [Yaman et al. 2009], all of these workflow mining algorithms are strictly control flow oriented, that is, focus on learning the acceptable sequence of actions, identifying loops and branching structure. The main difference between this work and R E C YCLE is that they ignore the data flow aspects of the problem; they infer action dependency based solely on the observed order of actions in the event logs. These approaches therefore recognize reorderings of tasks only when the reordering has been demonstrated. R E C YCLE , on the other hand, uses data flow analysis to discover depen-dencies between actions, and thus requires only one example to correctly infer ordering constraints. In addition, most of this work is unable to recognize workflows containing more than one instance of an action (i.e., where a given action is used in two different contexts). Finally, probabilistic models have the additional disadvantage of requiring many training examples.

Among these workflow mining approaches, WIT [Yaman et al. 2009] is the most similar to R E C YCLE because it is the only one that utilizes data flow information. WIT operates on the data flow and the control flow graphs, and iteratively identifies and merges nodes that are similar in both graphs. Because WIT requires mergeable nodes to consume data from the same sources, WIT also overcomes the limitation of workflows that contain more than one instance of an action. WIT learns a workflow that preserves the demonstrated order of actions, thus capturing a recommended or typical path. R
E C YCLE meanwhile learns a class of workflows that captures all valid paths. WIT identifies the loops and branching structure in the workflow, but does not infer the branching conditions.

ProM [van Dongen et al. 2005] is a notably comprehensive workflow mining and management framework that supports several different process mining techniques such as the ones cited earlier, including petri nets, transitional systems, and heuristic models. As noted earlier, these techniques are unable to infer action-reordering unless the reordering has been demonstrated, and cannot handle cases where an action is used in two different contexts. ProM algorithms also ignore the dataflow aspects of the problem because the case is a complex data structure where values are assigned to the fields of the structure, and thus there is no explicit input/output relationship or explicit data-dependency models for the events. ProM offers mining algorithms to discover other aspects of the workflows that are not explored by R E C YCLE ; for example, ProM supports the discovery of models related to time, transactions, and resources. ProM also offers process mining algorithms to handle noisy event logs reliably. Programming by demonstration is another research area closely related to our work. Shen et al. [2009] describe a domain-specific system that identifies frequent patterns in the information flow graph (where the nodes are resources and edges are labeled with possibly multiple actions) to identify actions related to the same task. Shen et al. first ignore the edge labels and mine for weakly connected frequent patterns in the graph and then try to select the labels that are most repeated. The workflows learned do not contain choices, conditionals, or the partial-order constraints on the steps. Lau [2001] proposes a concept of text editor macros by reasoning about how to generalize examples using version space algebras, which is a very different approach than presented in this article.

The concept of provenance is used in Web services research to track the quality of provided services and overall system reliability. Some work has extended this concept to analyze system data flow. Leake and Kendall-Morwick [2008] analyze data flow to determine the similarity between workflows. Dustdar and Gombotz [2007] use detailed logs to learn simple workflows composed of Web services. The log includes certain types of interactions between services and is used to identify subsequences of a trace that cor-respond to executions of different composite services. We believe that these approaches are well-suited for incorporation into algorithms that create the partially-ordered data flow graph for R E C YCLE , such as IODA [Burstein et al. 2010] and LAPDOG [Gervasio and Murdock 2009]. R
E C YCLE is a structural loop learner that learns a workflow of a domain from a demon-stration trace. R E C YCLE can detect nested loops and conditional branches, and can generalize from similar but unidentical patterns.

R E C YCLE takes as input an annotated trace (the trace and its data flow) and identifies parallel, repeated substructures. (It does not require a description of action semantics.) If the demonstrator creates several traces, R E C YCLE merges the independent data flow analyses into a single data flow graph, and analyzes the combined structure.
R E C YCLE  X  X  output is an executable workflow description, complete with the control flow of precondition branch statements, looping structure (including nested loops), and partially-ordered sequences of actions, fully annotated with the inferred data flow. The focus of this article is learning the structure of the workflow, that is, the abstract actions, loops, and branching structure, and the data flow between these high-level constructs. We will briefly discuss the algorithm for learning branching conditions in Section 8. The workflow models learned by R E C YCLE maintain the partial order of the data flow. R  X  X dentify and abstract nodes with similar purposes,  X  X apture all control flow constructs including loops (and nested loops) that iterate over a collection of items, branch structure and conditions to determine whether a node is applicable (or optional), and mandatory sequencing of actions 1 ,and  X  X apture all data flow dependencies of the original partially-ordered data flow graph, compensate for some errors in data flow analysis, and infer additional dependencies for abstract nodes and loop nodes that were not explicitly revealed by actions in the demonstration trace.
 The two phases to R E C YCLE are substructure analysis and node abstraction .R E C YCLE  X  X  algorithm, sketched in Table II, iterates over the graph looking for repeated parallel structures until it can make no more modifications to the graph. A parallel, repeated structure is a set of groups of connected nodes in the graph G . Each group contains similar nodes that are connected to each other in similar ways; in other words, they are sequences of actions that are repeated multiple times. R E C YCLE finally replaces the collection of substructures with a loop node.
 Figure 3 illustrates the identification of abstract and loop nodes for the trace in Example 1. There are three parallel repeated structures for R E C YCLE to learn and iden-tify: one for each passenger. R E C YCLE must also identify the nested loop corresponding to the getPrice action, the optional getHotel action, and the abstract action corre-sponding to seat selection. R E C YCLE first identifies the inner loop containing getPrice actions (Figure 3(a)). Figure 3(b) shows the loop after the repeated structures (actions for each passenger) are merged into a single structure, and the loop variable is detected. Note that merging the repeated structures produces abstract nodes such as bookSeat , which contains the window and aisle actions and their execution conditions to ensure that the correct one will be invoked. Figure 3(c) creates a loop of the final getHotel ac-tions which were left out of the main loop because getHotel was missing for the second passenger. Note that even if this action were contained in the main loop, the execution condition discovered for getHotel will invoke it only for travelers for whom the condi-tion is satisfied. Finally Figure 3(d) reveals the internals of the abstract node bookSeat , specifically the inputs and outputs of the action the abstract node represents, how they are connected to the internal action nodes, and the execution conditions corresponding to each internal action node.

The following sections detail the steps of the R E C YCLE algorithm: Section 6 describes substructure analysis, and Section 7 describes node abstraction. We now present the algorithms R E C YCLE uses to identify repeated similar structures in the data flow graph. To recognize beyond identical patterns, R E C YCLE uses the notion of node similarity which groups the actions based on their similarities along several criteria. Then, using those groups as starting points, R E C YCLE searches the graph for similar patterns. Section 7 describes how these structures are turned into composite actions.
 Prior approaches generally consider only nodes similar (mergeable) if and only if they have identical names, ignoring other contextual clues from the trace. This is a substan-tial limitation for two reasons. First, nodes may have different names but serve the same or very similar purposes; we would like the learner to treat these as equal for the purposes of recognizing repeated or abstract structures. In our example flight reserva-tion system, the actions aisle and window serve the same purpose. Second, some nodes have the same name, but serve different purposes. For example, lookupFlights might instead be represented with a more general lookupTransport , and return ground or air transport depending on the types of the inputs. Most prior art attempts to merge these actions, potentially overgeneralizing the workflow. To understand the effects of overgeneralizing, consider a trace that contains a series of mouse clicks to forward a message to someone from an address book. Existing methods, which ignore the param-eters, learn a loop of mouse clicks (because they are all  X  X licks X ) instead of a sequence of clicks each with a special purpose. The parameters of the mouse clicks, that is, what the user clicked on, are key to differentiating the clicks, for example, open the address book, select a contact, or send the message. WIT [Yaman et al. 2009] is the sole exception in that mergeable nodes must have identical names and also consume data from the same sources.

To recognize these similarities and dissimilarities, we must support a flexible simi-larity metric. R E C YCLE therefore uses other contextual features of the trace, including node name, number and type of input and output parameters, the nodes from whom they consume values, and the nodes for whom they produce values. Other domains may have other relevant cues that suggest whether nodes should or should not be merged.
Let I n parameters for node n i . We therefore define the similarity of nodes n 1 and n 2 as a weighted sum of the following set of domain-independent factors; note that each value normalizes to 1.0 2 :  X  X ame: Binary 0 or 1 based on string match. (Domains with semantically meaningful action names may be suitable for a spelling-related metric, such as edit distance [Cormen et al. 1992].)  X  X umber of Input Parameters:  X  X umber of Output Parameters:  X  X ypes of Input Parameters:  X  X ypes of Ouput Parameters:  X  X umber of Nodes producing input parameters:  X  X umber of Nodes consuming output parameters:
Nodes are considered similar (or dissimilar) based on whether the weighted sum score passes a configurable threshold,  X  . A higher threshold requires more similarity among nodes of a single family. A node family , N F , is a cluster of nodes all of whom are similar to each other. Assuming equal weighting for the components, and  X  is 0.72, then { r , r actions, h , are merged with the aisle and window actions, a and w .

In general, we recommend exploring these potential node similarity features and other contextual cues in the trace to find a set of features and weights that appropriately discriminate groups of similar actions. For example, in our experiments, we chose not to use any metrics on the similarity of input or output nodes, where the input nodes must be similar to each other, because the recursive computation does not provide enough discriminatory benefit. The key issue is to expand from the prior approach of mandating that node names be identical. The method findParallelStructures() , shown in Table III, finds the best abstraction candidate in the graph G . The best abstraction candidate is a set of parallel, repeated structures : a collection of substructures that are similar to each other and represent repeated actions in the trace. We call each structure a track , and a set of matching track group { [ f 1 , p 2 , r 1 ] , [ f 2 , p 4 , r 2 ] } .

To find the best track group, findParallelStructures() collects all of the possible track groups in the graph G , and returns the best candidate. findParallelStructures() starts by identifying a set of seed nodes, and then recursively grows (by adding more edges and nodes from the graph) matching parallel structures from those seed nodes. The seed nodes are structurally similar nodes , in that they belong to the same family N
F , meaning that they have similar inputs, outputs, prior nodes in G , subsequent nodes in G , preconditions, and effects (if any).

To grow the tracks, R E C YCLE finds a set of structurally similar nodes that are each connected to the seed nodes in a similar way; that is,  X  m i , m j  X  N F , find n i , n j  X  G such that n i is structurally similar to n j , and the edge between n i and m i is similar to the edge between n j and m j .R E C YCLE recursively grows the tracks until no parallel matches can be found. findParallelStructures() selects the track group (collection of structures) that best represents repeated actions in the trace; Section 6.3 describes the heuristic for selecting the best track group. findStructuralExtensions() , shown in Table IV, grows a track group from an initial seed. The initial seed, T ,isasetof k tracks, each containing m nodes. The first call from findParallelStructures() contains k tracks, each containing 1 node, that is, one track per seed node. findStructuralExtensions() selects one track T i , one node n 1 in T , and one edge e connected to n 1 .Ifthe T i can be extended with the node n 2 connected to n 1 by edge e , it calls findOneStepExtensions() to search through the other tracks for structurally similar matches.
 findStructuralExtensions() then iterates over the set of one-step extensions S ,and recursively calls itself to look for further candidates. findStructuralExtensions() returns the set S , which contains many track groups representing all the possible extensions of the initial seed T : extensions with one additional node, with two additional nodes, etc., until no nodes can be added to the repeated structures. Because S is a set, repetitions are not allowed. In this way, R E C YCLE finds a set of structural extensions to the original track group, where each track in the group is extended in a similar way. Note that the algorithm can build extensions that are not applicable to all seeds, in which case the returned track group would contain fewer tracks than the number of seeds.

Example 2 . Consider the nodes in Example 1. findParallelStructures() finds the { a findParallelStructures() will call findStructuralExtensions() with each of the families containing more than one node. Assume that { r 1 , r 2 , r 3 } is the first set of seed nodes selected; the initial call to findStructuralExtensions() will have the that track is selected for structural extension. findStructuralExtensions() consid-ers the nodes p 2 , a 1 and h 1 for possible additions to track [ r 1 ]. If a 1 is selected then findOneStepExtensions() will find the similar extensions for other tracks outputting { [ r produce arbitrarily long tracks, findStructuralExtensions() will return the following track groups:  X  G 0 ={ [ r 1 ] , [ r 2 ] , [ r 3 ] } ,  X  G 1 ={ [ p 2 , r 1 ] , [ p 4 , r 2 ] , [ p 5 , r 3 ] } ,  X  G 2 ={ [ r 1 , a 1 ] , [ r 2 , a 2 ] , [ r 3 ,w 1 ] } ,  X  G 3 ={ [ h 1 , r 1 ] , [ h 2 , r 3 ] } ,  X  G 4 ={ [ p 2 , r 1 , a 1 ] , [ p 4 , r 2 , a 2 ] , [ p 5 , r 3 ,w 1 ] } ,  X  G 5 ={ [ p 2 , r 1 , h 1 ] , [ h 2 , p 5 , r 3 ] } ,  X  G 6 ={ [ p 2 , r 1 , f 1 ] , [ f 2 , p 4 , r 2 ] , [ f 3 , p 5 , r 3 ] } ,  X  G 7 ={ [ h 1 , r 1 , a 1 ] , [ h 2 ,w 1 , r 3 ] } ,  X  G 8 ={ [ p 2 , r 1 , a 1 ] , [ p 4 , r 2 , a 2 ] , [ p 5 , r 3 ,w 1 ] } ,  X  G 9 ={ [ h 1 , a 1 , r 1 ] , [ h 2 , r 3 ,w 1 ] } ,  X  G 10 ={ [ h 1 , p 2 , r 1 ] , [ h 2 , p 5 , r 3 ] } ,  X  G 11 ={ [ f 1 , p 2 , r 1 , a 1 ] , [ f 2 , p 4 , r 2 , a 2 ] , [ f 3 , p 5 , r 3 ,w 1 ] } ,  X  G 12 ={ [ h 1 , p 2 , r 1 , a 1 ] , [ h 2 , p 5 , r 3 ,w 1 ] } ,  X  G 13 ={ [ f 1 , h 1 , p 2 , r 1 ] , [ f 3 , h 2 , p 5 , r 3 ] } ,  X  G 14 ={ [ f 1 , p 1 , p 2 , r 1 ] , [ f 3 , p 5 , p 6 , r 3 ] } ,  X  G 15 ={ [ f 1 , p 2 , p 3 , r 1 ] , [ f 3 , p 5 , p 7 , r 3 ] } ,  X  G 16 ={ [ f 1 , h 1 , p 2 , r 1 , a 1 ] , [ f 3 , h 2 , p 5 , r 3 ,w 1 ] } ,  X  G 17 ={ [ f 1 , p 1 , p 2 , r 1 , a 1 ] , [ f 3 , p 5 , p 6 , r 3 ,w 1 ] } ,  X  G 18 ={ [ f 1 , p 1 , p 2 , r 1 , a 1 ] , [ f 3 , p 5 , p 7 , r 3 ,w 1 ] } ,  X  G 19 ={ [ f 1 , p 2 , p 3 , r 1 , a 1 ] , [ f 3 , p 5 , p 6 , r 3 ,w 1 ] } ,  X  G 20 ={ [ f 1 , p 2 , p 3 , r 1 , a 1 ] , [ f 3 , p 5 , p 7 , r 3 ,w 1 ] } ,  X  G 21 ={ [ f 1 , h 1 , p 1 , p 2 , r 1 ] , [ f 3 , h 2 , p 5 , p 6 , r 3 ] } ,  X  G 22 ={ [ f 1 , h 1 , p 1 , p 2 , r 1 ] , [ f 3 , h 2 , p 5 , p 7 , r 3 ] } ,  X  G 23 ={ [ f 1 , h 1 , p 2 , p 3 , r 1 ] , [ f 3 , h 2 , p 5 , p 6 , r 3 ] } ,  X  G 24 ={ [ f 1 , h 1 , p 2 , p 3 , r 1 ] , [ f 3 , h 2 , p 5 , p 7 , r 3 ] } ,  X  G 25 ={ [ f 1 , p 1 , p 2 , p 3 , r 1 ] , [ f 3 , p 5 , p 6 , p 7 , r 3 ] } ,  X  G 26 ={ [ f 1 , h 1 , p 1 , p 2 , r 1 , a 1 ] , [ f 3 , h 2 , p 5 , p 6 , r 3 ,w 1 ] } ,  X  G 27 ={ [ f 1 , h 1 , p 1 , p 2 , r 1 , a 1 ] , [ f 3 , h 2 , p 5 , p 7 , r 3 ,w 1 ] } ,  X  G 28 ={ [ f 1 , h 1 , p 2 , p 3 , r 1 , a 1 ] , [ f 3 , h 2 , p 5 , p 6 , r 3 ,w 1 ] } ,  X  G 29 ={ [ f 1 , h 1 , p 2 , p 3 , r 1 , a 1 ] , [ f 3 , h 2 , p 5 , p 7 , r 3 ,w 1 ] } ,  X  G 30 ={ [ f 1 , p 1 , p 2 , p 3 , r 1 , a 1 ] , [ f 3 , p 5 , p 6 , p 7 , r 3 ,w 1 ] } ,
Note that findParallelStructures() never includes the node n 1 ,whichisthe getPassengers action, because the inclusion of n 1 connects all of the tracks, thus ter-minating the parallelism between the tracks.

Also note that the preceding list is an almost complete list of all possible parallel structures in G . The only missing track group is:  X  G 32 ={ [ p 1 ] , [ p 2 ] , [ p 3 ] , [ p 4 ] , [ p 5 ] , [ p 6 ] , [ p 7 ] } which can only be grown from the seed { p 1 , p 2 , p 3 , p 4 , p 5 , p 6 , p 7 } .
R E C YCLE must explore structural extensions for each initial seed candidate because of variations or errors in the graph structure. In general, there is a lot of redundancy among extensions, and hence our implementation of R E C YCLE takes care not to repeat computation work 3 . Additionally, one can make code optimizations based on the chosen heuristic for selecting the best structure (Section 6.3). Given a potentially huge set of candidate extensions for the initial seed, findParallel-Structures() selects the best extension according to a heuristic metric. There are many metrics one can use to compare two parallel structure groups. In R E C YCLE ,wehave explored the following.  X  Length of the track group. Number of nodes in each of the tracks. This metric is the one used by L OOP DISTILL.  X  Width of the track group. Number of tracks in the parallel structure group, equivalent to the most frequent common subplan.  X  Length times width of the track group. Many parallel structures with many nodes.  X  Parallelism of equal-scoring structures. This metric prefers the structure group that contains the most similar structures, for example, in which the number of edges, nodes, preconditions, or other features are most similar. This metric is extremely important in our domain because of errors in data flow analysis, nodes with different names but similar purpose, and the many different ways that goals can be achieved. In R E C YCLE , we have settled on using width as the primary metric, and then length and parallelism as tie-breakers in the same order. Using the width metric, R E C YCLE picks the track group G 32 in Example 2 as the best candidate. Preferring wider tracks helps R E C YCLE identify the nested loops. The intuition is simple: the inner loop in a nested loop structure will be executed more frequently than the outer loop, and thus the graph will contain more parallel tracks corresponding to iterations of the inner loop.

Winner states that there may be many possible parallel matching structures and that there is no way to know which is the  X  X est X  representation [Winner 2008, page 54]. L
OOP DISTILL therefore uses the longest common subplan. However, this heuristic fails in two ways: (1) it cannot learn nested loops, and (2) it does not learn correct workflow in domains with conditional actions. Figure 4(a) shows a partially-ordered data flow graph with a conditional node c 4 .L OOP DISTILL selects the common subplans with three nodes, generating the graph in Figure 4(b). R E C YCLE , on the other hand, generates the graph shown in Figure 4(c) because it prefers the most frequent common subplan. In this way, R E C YCLE learns conditional and optional actions.

Example 2 demonstrates that width is an effective heuristic that allows R E C YCLE to learn nested loops. In the same domain (assuming that L OOP DISTILL supported node families), L OOP DISTILL would select G 31 as the best structure because each track contains seven nodes X  X ot only do we lose third parallel structure ( f 2 , p 4 , r 2 , a 2 ), but also the nested loop of getPrice nodes.
 Abstraction is the task of replacing a set of nodes with a single abstract node that rep-resents a composite action, either because the set of nodes represents several different ways to archive the same task (e.g., booking an aisle seat versus window seat) or the composite action requires multiple simpler actions to produce a particular result.
Furthermore, an abstraction that is repeated multiple times in the graph (as iden-tified by findParallelStructures() algorithm) is a loop. We define a loop node as a special abstract node that also has an associated loop variable. Because the body of a loop node is an abstract node, in this section we will first explain how to make abstract nodes and then how to make loop nodes. When making an abstract node from a set of nodes, the key steps are: (1) to maintain the dependencies of the specific nodes within the new abstract node while merging the identical copies of the nodes, (2) to accurately merge input parameters, and (3) to accurately infer output parameters that might occur in future executions of the learned workflow, but that did not appear in the original trace. Figure 3(b) contains several abstract nodes, some more complex than the others. For example, the nodes lookupFlights and reserve are simple abstractions, produced by merging { f 1 , f 2 , f 3 } and { r 1 , r 2 , r 3 } respectively. The abstract node lookupFlights has exactly the same pre-conditions, input parameters, and output parameters as the original nodes { f 1 , f 2 , f 3 } . The same is true for the abstract node reserve . On the other hand, bookSeat is a more complex abstract node because the nodes it replaces are not homogeneous. Figure 3(d) sketches the internals of the abstract node bookSeat : specifically the inputs and out-puts of the action the abstract node represents, how they are connected to the internal actions, and finally the execution conditions discovered for each internal action node. In addition to representing alternatives, an abstract node can represent multistep pro-cedures. For example, the loop body in Figure 3(b) is also an abstract node with four steps, lookupFlights , get prices for all flights, reserve and bookSeat . The dependencies between those actions are preserved in the abstract node.

The function makeAbstractNode() , shown in Table V, makes an abstract node N from a set of nodes N . It makes a structural copy of the nodes within the new compound node, thus maintaining dependencies of the internal nodes. It then creates (merged) input edges, and (inferred) output edges and finally removes each of the original nodes in
N (and their edges) from the graph G . makeAbstractNode() first creates a structural copy , N s , of the nodes in N to store internally in the new compound node. A structural copy of a set of nodes N is a copy of all the nodes and all of the edges among them. Maintaining a complete structural copy of the nodes in an abstract node is important so that future execution of the learned methods (or instruction sets) maintains all internal dependencies. For example, imagine an abstract node m consisting of nodes n 1 , n 2 ,..., n i .If n i generates an output o that is consumed by n j ( j &gt; i ), then the method corresponding to the abstract node must invoke n i before n j . During this process, the nodes with identical structure, that is, action name, input and output parameters, and preconditions, are merged into a single node. makeAbstractNode() then analyzes the edges connecting the nodes in N to prior or postnodes, and creates new edges that will connect the abstract node to the rest of the graph G . The set of prior nodes, N b , consists of all those nodes not in the set of nodes N , directly connected to nodes in N . The set of postnodes, N a , are the actions that occur (not node d 4 because it is not directly connected to the node b 4 ). makeAbstractNode() collects the edges between the prior nodes N b and the internal nodes N , and calls createEdgesIn() to aggregate and remove these edges as needed. createEdgesIn() merges edges for which both the output parameter type matches, and the input parameter type matches; for example, both edges consume the origin property of a Passenger . createEdgesOut() performs a similar service for the edges between the internal nodes N and the postnodes N a . createEdgesOut() also infers edges that do not exist in the original graph but might be needed for the generalized case. For example, in Figure 2, for each passenger, only one of the getPrice actions has an output edge. This does not mean, however, that the output from other nodes are never useful; it simply means that in the demonstration trace they were not. As another example, if we had multiple ways to check the price, such as getPriceFromOrbitz and getPrice-FromExpedia , then only one of the nodes, say getPriceFromOrbitz , would provide the output for the abstract node. Execution of the learned workflow might therefore fail because the output of getPriceFromExpedia is dropped. createEdgesOut() therefore must infer these missing output dependencies, as outlined in Table VI.
 For this work we focus on loop structures that run for every element in a collection of objects. R E C YCLE does not identify loops that are run for a fixed number of times or until certain conditions are satisfied. Thus parallel repeated structures in a graph, as discovered by findParallelStructures() , indicate the existence of loops. When making a loop node from a set of parallel repeated structures, the key steps are: (1) to handle nested loop formations: (2) to correctly merge each set of parallel structures into a single structure, (3) to identify the loop variable, and (4) to generate proper inputs and outputs.

Table VII shows the algorithm makeLoopNode() for creating a loop from a set of parallel, repeated structures, known as a track group . Given a graph G , and a track group T , makeLoopNode() partitions the track group by input providers, and then builds the body of the loop. The main approach in makeLoopNode() is to find each set of parallel, enabled nodes, and replace them with an abstract node. The set of abstractions is then wrapped in a loop construct. For example, the graph in Figure 3(a) is first converted to the graph in Figure 5(a), where nodes f 1 , f 2 , f 3 have been replaced by the single abstract node F ; nodes PL 1 , PL 2 , PL 3 have been replaced by the single abstract node have been replaced by the single abstract node B . Then, in Figure 5(b), these abstract nodes are wrapped with the loop construct.

The first step in makeLoopNode() is to partition the tracks of the track group T with respect to their providers, and then to recursively call makeLoopNode() with each partition. partition() returns a partitioning of T based on the sets from which inputs are consumed. If partitioning is unnecessary, then it returns T as the single partition. Partitioning allows R E C YCLE to capture nested loops. Once again consider Example 1. Section 6 already established that findParallelStructures() would select the parallel note that this track group contains loop iterations for three different passengers. To discover nested loops correctly in this graph, we must avoid merging loop iterations across passengers: merging all of these nodes into a single loop node would reduce the parallelism in the graph, and in the next iteration R E C YCLE would not be able to discover the outer loop over the passengers. We avoid over-generalization by first partitioning the track group based on their input providers (the nodes whose output is used as inputs in the track nodes) and then creating loop nodes for each partition. For example, in to note that even though L 2 has a single element (because the output of f 2 wasaset containing single element) it is still recognized as a loop because L 1 and L 3 are similar situations and they contain repetitions; in other words, the existence of L 1 and L 3 support the hypothesis that L 2 is also a loop node.
 Before starting its core while loop, makeLoopNode() sets the group of visited nodes, V , to be all the nodes in G that occur before any of the nodes in the track group T . makeLoopNode() then iterates over the track group T , removing parallel, enabled nodes and replacing them with abstract nodes. It first finds all the enabled nodes E ,thatis, for whom all of its prior nodes are in V . E may contain multiple nodes per track. It then selects a set of parallel nodes P within the enabled nodes E ; it will select exactly one node per track such that all nodes n in P belong to the same family. makeLoopNode() then calls makeAbstractNode() to convert the parallel, enabled nodes P into a single abstract node, N i . To complete the loop, it removes all the nodes in P from the track group T , and adds them to the visited nodes V . For example, in Figure 3(a) the track groups that Initially E is { f 1 , f 2 , f 3 } because those are the only nodes that do not have incoming edges from the nodes in the track group. In this case P is same as E because it contains exactly one node from each track and all nodes belong to the same family 4 . Thus nodes f , f abstract node ( PL in Figure 5(a)). Eventually, the algorithm processes all nodes in an order such that each node is processed before all of its successors, producing the graph in Figure 5(a).
 After all the nodes n in T have been replaced by appropriate abstract nodes, make-LoopNode() creates an abstract node containing all the nodes in Abstractions . That is, Figure 5(a) is transformed into Figure 5(b).
 The algorithm completes by creating a loop variable for the new abstract loop node, N , and adjusting the loop node X  X  inputs and outputs accordingly. To identify the loop variable, R E C YCLE looks at all output parameters in G that supply an input to the loop node N , and selects the single set-typed parameter from which the most elements are consumed. Within N , the parameters that are supplied from the selected set are replaced by a single parameter corresponding to the entire set and the edges are adjusted accordingly. The new parameter will be the loop variable, an iterator over the set. R E C YCLE thereby infers that all elements should be processed, whether or not the original trace did so. Note that preconditions on actions will ensure that actions are executed only for relevant elements of the set. Finally, for each step inside the loop that returns a value, R E C YCLE creates a new output parameter by creating a new collection map that maps each internally calculated value to its corresponding loop variable element. In a R E C YCLE workflow, each action is guarded by execution enablement conditions, that is, the action is executed if and only if the conditions are satisfied. Intuitively these execution conditions serve as branching conditions. For example, when executing the abstract action bookSeat , which contains the window and aisle actions, only the one action that corresponds to the passenger X  X  preference is executed. When executing an abstract action composed of several internal steps, all steps whose execution conditions are satisfied are executed in an order that is consistent with the partial-order depen-dency graph. This branch condition representation is also used in L OOP DISTILL. The major difference between L OOP DISTILL and R E C YCLE conditions is how they are com-puted. L OOP DISTILL operates on an a priori semantic definition of the domain actions (STRIPS-like precondition and effect formulae). R E C YCLE , on the other hand, operates on action instances contained in the demonstration trace. The trace captures inputs, outputs, and results of the actions. The results represent how the outputs are related to real-world entities. For example, if the output of a booking action is a reservation number, then the result states that somewhere in the system there is a reservation ob-ject that has an ID that is same as the output. This formalism is common for semantic Web services [Martin et al. 2007]. Note that no preconditions are captured in the trace at all, and results are specific observed instances rather than general semantics.
In R E C YCLE , we have developed the P RE P AR algorithm to learn the execution condi-tions of the workflow steps. For each action instance A in the trace, P RE P AR computes asubsetof A  X  X  results that tie its outputs to at least one of its inputs. That is, for each of
A  X  X  outputs o , it calculates which results are required to provide binding closure to one of A  X  X  inputs i .Notethatif A has no input parameters then such a path cannot be found and it is assumed there are no preconditions. P RE P AR is essentially a path find-ing algorithm from the outputs to the inputs of A , and the output is a conjunction of result statements. P RE P AR annotates each dependency A v  X  B with the set of execution statements corresponding to v .WhenR E C YCLE generates the final workflow methods, it places these statements as execution guards that ensure appropriate execution of the action. Further details of this method will be presented in a future paper. We have demonstrated R E C YCLE in two domains [Burstein et al. 2008]: a Web service domain for scheduling patient transport, and a stand-alone system used by the military (built on top of .NET) for dispatching packages. For the sake of brevity in this article, we present only the results in the first of these domains.

In Section 9.1 we present analyses of R E C YCLE as a stand-alone component. We also deployed R E C YCLE as a component of a much larger system, POIROT, where the purpose was to compare POIROT X  X  learning ability to novice humans X  impressive ability to generalize from a single example. We showed, at a 99% confidence level, that it could generalize the action choices and input/output data connections of a workflow better than 46 novice humans [Burstein et al. 2008]. In the medical evacuation domain (MedEvac), the goal is to transport wounded per-sonnel from disaster zones to safe locations. The procedure is very similar to making travel arrangements with the additional complexity that the human can create a flight if none of the existing flights is suitable for a patient. To give the overview of the domain, Figure 6 hides the details of complex steps, representing them as 3D boxes; each complex task represents a subworkflow. For example, making a reservation for a patient is a complex procedure that involves not only reserving a bed for the patient, but also reserving the equipment and nurses that need to accompany the patient. Similarly, arranging local transportation to/from the origin/destination airports is a complex task that requires querying for available transportations from two different sources and selecting the most appropriate one for the patient. There are a total of 33 available actions, several of which must be used in more than one context, and some of which are applied multiple times per patient (thus forming nested loops). To analyze the stand-alone correctness of R E C YCLE , we ran R E C YCLE with a variety of expert-generated traces from the MedEvac domain. The traces demonstrate the transportation of 2 X 6 patients. R E C YCLE is trained with a single trace, containing as few iterations as possible. We then manually analyze the correctness of the learned workflow compared to the target. Each patient requires between 27 and 40 actions. On average, the traces contain more than 100 actions and 300 data bindings. Appendix A describes one of these analyses in more detail.

To learn a workflow that captures loops over all of the possible conditional actions, the trace needs to contain two or more demonstrations of each option. When care-fully constructed, a demonstration containing six patients can sufficiently cover all of the options in this domain (for a total of 181 actions in the trace). We assume that a teacher generates the trace, and that a data-dependency analysis algorithm such as IODA [Burstein et al. 2010] or LAPDOG [Gervasio and Murdock 2009] infers depen-dencies between action instances (and thus might contain errors).

Figure 7 is the output of R E C YCLE when it learns from such a six-patient trace. This model is similar to the top-level depiction in Figure 6. Note that the target workflow captures a typical execution sequence conducted by a human, and is thus a totally ordered sequence; it does not capture action dependencies. The learned model, on the other hand, recognizes when actions could be executed in a different order. This difference means that the learned model is an accurate generalization of the target.
R E C YCLE workflows generally contain more loops than the target workflow. For exam-ple, instead of a single loop that finds the origin airport, destination airport, and then books (or proposes) a flight per patient, R E C YCLE might have three loops: the first one finds the origin airports for each patient, the second loop finds the destination airport for each patient, and, finally, the third loop books (or proposes) a flight for each patient. The main reason for these separated loops is that R E C YCLE raises optional actions to the top-level view. In Figure 7, for example, then node reserveEquipment ( rEq ) requests equipment for only those patients who need equipment. R
E C YCLE can overcome errors in data flow analysis. As long as there is one edge that connects two similar nodes in the same way, R E C YCLE identifies the parallelism between similar structures and infers the missing edges. In real-world domains, with multiple contributing analysis engines, the analysis pipeline might introduce errors; R E C YCLE  X  X  approach to looking for structurally similar tracks helps compensate for, and even correct, certain kinds of errors. A detailed example of this behavior is described in Section A.1.

To evaluate the resilience of R E C YCLE to errors in data flow analysis, we performed two tests: a test of resilience to missing data flow edges, and a test of resilience to altered data flow edges. We selected a trace involving two patients and no conditional actions. From this trace, IODA [Burstein et al. 2010] infers a partially-ordered data flow graph that contains 187 edges. R E C YCLE generates the model in Figure 8, similar to the top-level description in Figure 6 where a large loop follows the single action that returns the list of patients. The learned model is an accurate generalization of the target, in that it recognizes when actions could be executed in a different order; for example, the two lookupAirport ( luApt ) actions (one for the origin and one for the destination) are independent.

Degraded Data Flows: A test of resilience to missing data flow edges. For the first test, we removed edges at random from the partially-ordered data flow graph and evaluated the correctness of the model learned by R E C YCLE . Figure 9 shows the results, for 60 trials for each of 1, 2, 5, 10, or 20 edges removed. A given result could be identical to the original workflow, logically correct, or contain errors. A logically correct result is one in which all action dependencies are preserved and the data flow is correct. If one were to execute plans using a logically correct workflow, the result would be complete and correct. In all of the logically correct cases that were not identical to the original workflow, R E C YCLE pulled one or more actions into separate loops, as illustrated in Figures 10 and 11. Even when 20 edges (10.6%) are removed from the data flow graph, R E C YCLE generates a logically correct workflow 60% of the time.

All of the error cases represent incomplete workflows (with no incorrect actions or structure). R E C YCLE learns the correct structure of the model, in the sense that actions are sequenced correctly, but loses loop information. Figure 12 illustrates that R
E C YCLE lost the loop structure for setPatientAPOD ( sPAPOD ). A trace generated from this workflow would set the airport of departure for only one patient.

Given that the original trace had only two patients, R E C YCLE will be prone to losing loop structure more easily than in a trace with more patients. In other words, if a direct edge containing the loop variable (the set of patients) is lost, then R E C YCLE must infer the loop from supporting evidence (similar or neighboring nodes in the graph). As the data flow graph degrades, there is less and less supporting evidence available; a trace with several patients will have more redundant information, and thus missing edges are less likely to affect the accuracy of the learned workflow.

Altered Data Flows: A test of resilience to altered data flow edges. In our second test, IODA generated a report of all the actions that might have produced a given value, along with the heuristically selected action. 31 of the 187 data dependencies had between 1 and 6 alternate choices. We randomly selected edges to alter by replacing the source of the selected data flow edge with one of the alternates.

In all cases, R E C YCLE generalizes better from data flows with altered edges than it does with missing edges, as shown in Figure 9. Note that for 1, 2, and 5 altered edges, R
E C YCLE generates 100% identical or logically correct results. R E C YCLE will always pro-duce similar or better logical correctness results for the altered edges than for missing edges because alternate edges often come from a different output of the same node as the correct edge. For example, a lookupTransportUnit ( luTU ) action may generate several units that can travel between the given locations; each unit has an associated mode (Ambulance, Helicopter, Humvee), service (Marine, Army), and depot. Normally, a correct data flow analysis would use the three values coming from the same transport unit. A random alteration of one of these edges breaks this correlation, but the alter-nate edge may be selected from another transport unit from the same luTU action, such as mode and service from Unit1, and depot from Unit2. In these cases, R E C YCLE is able to infer the correct workflow structure.
 However, R E C YCLE is overzealous in binding potential data flow edges. In other words, R
E C YCLE infers the existence of both correct edges and the altered edges. Take, for example, two nodes n 1 and n 2 that consume the output of node m 1 and m 2 respectively. The correct input graph should have two edges, m 1  X  n 1 and m 2  X  n 2 . When one of those edges is missing (as in the degraded data flow test), R E C YCLE can infer its existence, namely m i  X  n i . When one of those edges is instead replaced, say p 1  X  n 2 , then R E C YCLE infers both m i  X  n i and p i  X  n i . As a result, the workflow might have two nodes for the same action, both of which can be executed (depending on the conditions computed for the nodes) but one of them will be redundant and/or wrong. As seen in our running example, as well as in the evaluation domain (Appendix A), optional and conditional actions often produce workflows with fragmented loops. For example, the optional bookHotel action in Figure 2 leads to a workflow (shown in Figure 3(c)) that has two loops over the passengers, one for booking the flight and one for booking the hotel. In the second loop bookHotel is executed for a passenger only if the reservation is overnight (inferred from the precondition of bookHotel as defined in Section 3). While this workflow is still logically correct, that is, the action dependencies are preserved, some humans might find it less intuitive. For future work, we will investigate applying compiler optimization techniques such as loop fusion to merge the loops (possibly containing conditional executions) that have the same loop variable into a single loop node if the new node does not introduce cycles in the graph. For example, loop fusion would convert the algorithm in the left column of Table VIII into the algorithm shown in the right column.

We are also planning to exploit the observed sequence of actions in the demonstra-tion to heuristically bias the model R E C YCLE learns. For example, the first algorithm of Table IX is more correct when each passenger X  X  reserve action succeeds all getPrice ac-tions. The second algorithm is more correct when reserve actions appear between get-Price actions, that is, R E C YCLE can infer that the first workflow is not the target work-flow if in the demonstration the node r 1 in Figure 2 is observed before nodes p 2 or p 3 . Another open issue is to extend the class of loops R E C YCLE can learn. Presently, R E C YCLE learns  X  X or all X  loops but does not learn loops with counters (i.e., serial loops), nor does it learn  X  X oop until condition X  or  X  X hile condition do X  loops. This article presents our approach to learning workflow models from example demon-stration traces. R E C YCLE learns nonsequential workflows in which dependencies among actions are driven by data flow requirements. The workflows capture all of the control flow, including branch and enablement conditions, loops (including nested loops), and partially-ordered sequences of actions. Workflows also accurately group abstract nodes with similar purposes. R E C YCLE  X  X  approach analyzes a partially-ordered data flow graph to identify repeated, structurally similar components. It replaces these components with abstract or loop nodes, complete with fully annotated branching conditions, loop variables, and (possibly inferred) data flow connections.

Only WIT and R E C YCLE combine analysis of control flow and data flow, thus increasing accuracy of the learned model. R E C YCLE is unique in its ability to leverage data flow information to learn a control flow that is more general than the demonstrated action orderings.
 Unlike previous work in the planning community for learning from example traces, R E C YCLE does not require a complete, a priori, model of the actions in the domain. Unlike previous work in the workflow mining community, R E C YCLE can recognize legal reorderings of actions based on data flow requirements (rather than solely based on observed action sequences). In many domains, the experts X  time is extremely limited, and R E C YCLE can therefore learn from a single demonstration trace with loops. If the demonstrator creates several traces, R E C YCLE merges the independent data flow analyzes into a single data flow graph, and analyzes the combined structure.
Two of R E C YCLE  X  X  main contributions compared with prior work are: (1) its ability to recognize nested loops, and (2) its ability to recognize loops even in domains with conditionally executed actions. Note that when a loop contains conditional actions, the complexity of the learning task is significantly increased as the similarity of one iteration to the next is variable.

Another major contribution of R E C YCLE is the general framework for defining what similarity means for workflow nodes (Section 6.1). In doing so, it extends the previous work that considers action name as the only sign of similarity. R E C YCLE calculates the similarity of nodes based on their relationships to other nodes and their inputs and outputs. This approach lets R E C YCLE learn structures in which the same action name has different purposes, and create abstract structures in which different action names have similar purposes.

We have demonstrated R E C YCLE  X  X  efficacy in learning workflows in two real-world military domains, but for the sake of brevity this article presents only the results for the medical evacuation domain. In real-world domains, with multiple contributing analysis engines, the analysis pipeline might introduce errors into the data flow; as demonstrated by our experiments, R E C YCLE  X  X  approach to looking for structurally simi-lar tracks helps compensate for, and even correct, certain kinds of errors (Section 9.3). We have demonstrated R E C YCLE in two domains [Burstein et al. 2008]: a Web service domain for scheduling patient transport (as explained in detail in Section 9.1), and a stand-alone system (built on top of .NET) for dispatching packages. Here we present a detailed case study in the first of these domains.
 We ran R E C YCLE with a variety of expert-generated traces from MedEvac domain. The traces demonstrate the transportation of 2 X 6 patients. R E C YCLE is trained with a single trace, containing as few iterations as possible. To increase the generalization accuracy of the learner, the traces should cover all of the optional actions two or more times. Each patient requires between 27 and 40 actions. On average, the traces contain more than 100 actions and 300 data bindings.
 Figure 8 illustrates a model learned from a trace involving two patients and no condi-tional actions. This model is a large loop following the single action that returns the list of patients. R E C YCLE recognizes when actions could be executed in a different order, meaning that the learned model is an accurate generalization of the target.
To understand how this main loop was generated, Figure 13(a) shows an excerpt of a partially-ordered data flow graph. Each lookupTransportUnit ( luTU ) action returns a set of transport units, each of which is inspected with a getTransTimes-Army or -Marines ( gTTA/gTTM ) action. Figure 13(b) shows the same excerpt of the graph after the first iteration of R E C YCLE : it replaced the gTTA/gTTM actions with loops. Now the similarity of the subgraphs is much more evident, and in R E C YCLE  X  X  next iteration, it recognizes the parallel patient substructures, and replaces them with the loop shown in Figure 8.

Figure 13(a) also shows one error in data flow analysis; for one patient, the nPA ac-tion for the origin is incorrectly dependent on the destination. This difference does not stop R E C YCLE from recognizing the similarity of the structures. As long as there is one edge that connects two similar nodes in the same way, R E C YCLE identifies the paral-lelism. In this case, both structures have an edge between lookupMission ( luMsn )and notifyPickup-Army or -Marines ( nPA/nPM ), and also an edge between the loopGTT and nPA . In real-world domains with multiple contributing analysis engines, the analysis pipeline might introduce errors; R E C YCLE  X  X  approach to looking for structurally similar tracks helps compensate for, and even correct, certain kinds of errors.
 The partially-ordered data flow graph of Figure 14 was generated from a trace involving four patients with explicit optional actions. Because this trace does not contain all of the possible conditional actions, we know we cannot correctly learn all of the conditional situations. 5 Some of the patients in this trace have medical conditions that require additional equipment. These actions correspond to internal components of the Make Reservations action shown in Figure 6. There are four patients with the following needs.  X  X atient 1 requires a seat. We call the reservePassage ( rPass ) action with the patient
ID and a seat constant. (Figure 15(a).)  X  X atient 2 requires a seat and a ventilator. We call rPass with the patient ID and a seat constant, and we reserve the ventilator with the actions reserveEquipment ( rEq ), setPatientOriginLegEquipment ( sPOLE ), setPatientFlightEquipment ( sPFE ), and setPatientDestinationLegEquipment ( sPDLE ). (Figure 15(b).)  X  X atient 3 requires a litter. We call the rPass action with the patient ID and a litter constant, and also call sPOLE and sPDLE (but not rEq or sPFE ). (Figure 15(c).)  X  X atient 4 requires a seat and a nurse (also travelling in a seat). We call rPass action first with the patient ID and a seat constant, and we reserve the nurse with the actions requestPersonnel ( rPer ), and setPatientPersonnel ( sPP ), and also call rPass a second time with the nurse X  X  ID and a seat constant. (Figure 15(d).) Figure 16 shows the extract of the learned workflow corresponding to these actions. The key difference between this workflow and the one shown in Figure 8 (which had no conditional actions) is that all of the conditional actions have been exposed in the top method. Moreover, there are four loops over all of the patients , and a fifth loop for rPass over all of the persons in the trace. R E C YCLE separates the loops whenever it encounters conditional actions, similar to the example described in Figure 4(c).  X  Node 157 is a loop for each patient over the abstract action sPOLE and sPDLE .  X  Node 151 is a loop for each patient over the two luApt calls (inferred from nodes 2 and 13), and calls to sPAPOD and sPAPOE (not shown in Figure 15). Despite the fact that the demonstration trace reused the two luApt calls, R E C YCLE was able to infer the loop because the outputs of the luApt calls are directly used by the sPAPOD and sPAPOE actions for each patient. R E C YCLE has therefore correctly generalized the need to find two airports for each patient (one near the origin and one near the destination).  X  Node 15 represents the single instance that luMsn was called the demonstration trace. Despite being able to recognize that luApt should be called for each patient, there was no similar evidence that allowed R E C YCLE make the same inference for luMsn . The fact that all of the patients converge at this luMsn node is the reason why
R E C YCLE separates the other loops.  X  Node 145 is a loop for each patient over the actions involving the origin: luTU (from the hospital to the origin airport), loopGetTransTimes ( lGTT ), a variety of setPatient-
Origin* (sPO*) calls, and a call to notifyPickup-Army or -Marines ( nPA/nPM ).  X  Node 136 is a loop for each patient over the actions involving the destination: lookup-TransportUnit ( luTU ) (from the destination airport to the destination hospital), loop-
GetTransTimes ( lGTT ), a variety of setPatientDestination* (sPD*) calls, and a call to notifyPickup-Army or -Marines ( nPA/nPM ).  X  Node 155 is a loop for each of the persons in the problem. In Figure 8, the rPass action is embedded within a single loop over all of the patients. Adding a nurse to this demonstration allowed R E C YCLE to recognize that rPass should be invoked for all of the people who will be traveling, not just the patients.  X  Nodes 54, 109, and 51 are cases where the demonstration trace had only one instance of conditional actions. Two instances are enough to infer loops, that is, if two patients had required nurses, then R E C YCLE would have inferred loops for both nodes 54 rPer and 51 sPP ; if two patients had required portable equipment like the ventilator then
R E C YCLE would have inferred a loop for node 109 rEq . In general, the more patients that the demonstration trace contains (and the more coverage of optional actions), the better the generalization accuracy of the learner.
 To learn a workflow that captures loops over all of the possible conditional actions, the trace needs to contain two or more demonstrations of each option. When the trace is carefully constructed by a teacher, six patients can sufficiently cover all of the options in this domain (for a total of 181 actions in the trace). The graph in Figure 7 shows the graph R E C YCLE learns when the demonstration trace contains these six patients, despite errors in the partially-ordered data flow graph.

While much of the structure is similar to Figure 16, the lookupMission ( luMsn )task is part of the large central loop, and the actions for requestPersonnel ( rPer )and reserveEquipment ( rEq ) have both been incorporated into loops. The additional node luApt , iTM , sTMDT , cM , sPM ,and rP correspond to the actions required to create a mission. The most interesting change between Figure 16 and Figure 7 is the abstract node immediately following lookupRequirements ( luRqts ) that groups four loops: R E C YCLE initially created each of the loops independently; each new loopnode then takes a SetOfPatientRecords as input, and thus R E C YCLE recognizes that they can be grouped as a single abstract node. This abstract node is an ideal candidate for the compiler optimization techniques discussed in Section 10.

