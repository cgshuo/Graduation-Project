 Computer Science Department, National Institute of Astrophysics, Optics and Electronics, Puebla, Mexico Research Center of Technologies on Information and Systems, Autonomous University of State of Hidalgo, Hidalgo, Mexico Faculty of Engineering, Universidad Autonoma de San Luis Potosi, SLP, Mexico 1. Introduction
Nowadays there are a lot of supervised classi fi cation problems described through large training sets, which contain a lot of previously classi fi ed instances, described by numeric and non numeric attributes (mixed data).

Decision trees [29] are among the most used supervised classi fi cation algorithms. A Decision Tree (DT) is a tree structure formed by internal nodes and le aves. Each internal node ha s a splitting attribute and one or more children. If the s plitting attribute of an internal node is non numeric, then a child is associated with each value of the splitting attribute. But, if the splitting attribute is numeric, then the node has a splitting test of the form X V and two output edges. The leaf nodes contain a class label.
Currently, there are a lot of algorithms for building decision trees [3,5,6,8,26 X 29,33,34,36] however these algorithms cannot handle large training sets. There are some algorithms that have been developed for building DTs from large training sets: SLIQ [25], SPRINT [32], CLOUDS [2 ], Rainforest [16], BOAT [15], I CE [40], VFDT [9], BOAI [39], IIMDT [10] and IIMDTS [11], however all of them have some restrictions (see Section 2).
In this work, we introduce a new fast heuristic for b uilding traditional DTs from large training sets described by numeric and non numeric attributes that overcomes the restrictions of the above algorithms. The proposed heuristic allows processing all the instances of the training set without storing the whole training set in main memory. Besides, our heuristic uses only one parameter that can be easily determined by the user.

The rest of the paper is organized as follows: Section 2 describes the previous algorithms for DT induction from large datasets. In Section 3, the proposed heuristic for building DTs from large training sets is introduced. Section 4 presents our experimental results. Finally, in Section 5, conclusions are presented. 2. Related work Currently, there are a lot of algorithms for building DTs, for example: ID3 [24], C4.5 [29], CART [5], QUEST [33], ModelTrees [8], CTC [28], FDT [19], etc. Besides, several algorithms have been developed for building DTs in an incremental way, such as UFFT [13], ID5R [36], PT2 [37] and ITI [38]. However, all these algorithms have to keep the whole training set in memory for building a DT, therefore they cannot handle large datasets. For this reason, all these algorithms will not be used for comparing our algorithm. Although our algorithm and VFDT [9] could be used over streaming data, both were designed for fast building DTs for large datasets, therefore we will not include a description of algorithms for building DTs over streaming data [4,18,20] nor a comparison against them.
 The algorithms that have been developed for building DTs from large training sets are: SLIQ [25], SPRINT [32], CLOUDS [2], Ra inforest [16], BOAT [ 15], ICE [40], VFDT [9] , BOAI [39], IIMDT [10] and IIMDTS [11].

SLIQ, SPRINT and CLOUDS represent each attribute of the training set as a list, which stores the lists, these algorithms require at least the double of the space required for storing the whole training set.
Following this approach, RainForest and BOAI also use lists for representing the attributes, but, in order to maintain the lists in main memory, these algorithms reduce the lists storing only different values for each attribute. However, when a l arge training set is bei ng processed, these str uctures are still too big, therefore they cannot be stored in main memory.

BOAT and ICE do not store the whole training set in main memory, instead, these algorithms use subsets of instances for building the DT. However, BOAT and ICE use extra time for fi nding those subsets of instances. For example, ICE divides the training set in several parts and then it uses a tree-based sampling technique based on local clustering [41] for fi nding subsamples, which together will form the subset of instances used for building the DT.

VFDT is an algorithm that uses Hoeffding bounds [17] for building DTs from large training sets. The algorithm VFDT needs three parameters for building a DT, but values for these parameters could be very dif fi cult to be a priori determined by the user.

IIMDT [10] builds multidimensional DTs from large numeric training sets using the mean of the instances of a leaf as splitting value to create new nodes. The algorithm IIMDT uses the whole set of attributes to sp lit nodes. In this way, the process to select a s plitting attribute is avoided, which saves processing time and allows to handle large training sets. In IIMDTS [11] a similar approach is followed but different subsets of attributes are used for expanding each node of the tree.

The previous described algorithms have some restrictions. Almost all of them have spatial restrictions (SLIQ, SPRINT, CLOUDS, RainForest and BOAI) because a representation of the attributes that requires more space than the whole training set is commonly used; or they have to keep the whole training set in main memory. Other algorithms (BOAT and ICE) use only a subset of instances for building a DT, but additional time is required for computing this subset, which could be too expensive for large training sets. In VFDT several parameters are used, which could be very dif fi cult to be a priori determined by the user. Finally, IIMDT and IIMDTS are algorithms that build multidimensional DTs, which are dif fi cult to be interpreted by the user. Besides, these algorithms only can process large numeric training sets, since when a leaf is expanded they create a new node for each class of instances, usi ng as sp litting val ue the mean of the instances of each class.
 For these reasons, in this paper, we propose a new fast h euristic for build ing traditional unidimensional DTs from large training sets described by numeric and non numeric attributes. The proposed algorithm, named DTLT, uses all the instances of the training set without storing the whole training set in main memory. Besides, this heuristic only needs one parameter that is easy to determine by the user. 3. Proposed algorithm
In this work, we introduce a new heuristic for building DTs from Large Training sets (DTLT) that overcomes some of the shortcomings of the algorithms described in the previous section. In order to avoid storing the whole training set in main memory, DTLT processes the training instances one by one, updating the DT with each one and discarding those instances that have been used for expanding a node. Besides, in order to allow a fast building of the DT, DTLT expands a node when it has a small amount of instances; this amount is a parameter of DTLT. In this way, DTLT only maintains in main memory a few instances of the training set and uses a fast expansion process, which together allow DTLT to process large training sets.
 A DT built by DTLT has the same structure that a DT built by a conventional DT induction algorithm. It contains internal and leaf nodes. An internal node has associated a s plitting attribute. If the splitting attribute is numeric, then the node has an sp litting test of the form X V and two output edges. The categorical , then the internal node has one edge for each po ssible value of its splitting attribute. On the other hand, each leaf has associated a class label. Since the structure of a DT built by DTLT is the same of a DT built by traditional DT induction algorithms (like C4.5), the traversing of a DT built by DTLT is the same as the one of traditional DTs.

Several criteria for splitting attribute selection i n DTs have been proposed [7 ,22]. In this work we use the Gain Ratio Criterion [29], one of the most used criteria for selecting the splitting attribute for expanding a node [30]. This criterion allows to fi nd the attribute that best divides in an homogeneous way the instances in a node.

DTLT will process the training instances one by one, updating the DT with each one and discarding those instances that have already been used for expanding a node. However, incrementally processing a long sequence of instances from only one class, for building a DT, makes that the nodes do not have diversity of instances(instances from different classes). In order to avoid this situation, DTLT reorganizes the fi rst class, the second instance from the second class and so on, if there are r classes, the instance in the position r +1 is from class 1 and so on. In our experiments the runtime of DTLT includes the time spent by this reorganization process.

The algorithm starts building a tree with an empty root node without descendants (a leaf). Each instance traverses the DT until it reaches a leaf, where the instance is stored. Since, at the beginning, the DT only has the root node, the fi rst instances are stored in that node. When the root node has s instances ( s is a parameter of DTLT) the node is expanded. In DTLT, unlike traditional DT algorithms, all the training instances are not used for selecting a splitting attribute at the fi rst step. Instead of it, DTLT uses the s instances stored in the root node (the reorganizing step guaranties that the root node has instances from more than one class). Thus DTLT takes into account these s instances to select the splitting attribute for e xpanding the root node. We use the parameter s in order to process only a small amount of instances for expanding a node, in this way the expansion of a node is fast. Table 1 shows the main algorithm of DTLT and Table 2 shows the recursive function for updating the DT with each instance of the training set.

For expanding a node, DTLT applies the Gain Ratio Criterion [29] to each attribute, but only using the s instances stored in the node. For each attribute X , fi rst, we obtain the information gain from the s instances in the leaf using Eqs (1) and (2). Where S is the set having the s instances stored in the leaf, p i and n is the number of possible splits of the attribute X .

Then, we use Eq. (3) for measuring the Gain Ratio of each attribute.
The attribute with the highest Gain Ratio is chosen as splitting attribute. The number of edges or splits is created according to the type of th e selected sp litting attribute. For numeric attributes the expanded attribute. In both cases, each generated edge leads to a new empty leaf.

Once the edges have been created, DTLT assigns to each leaf a temporary class. For this, DTLT fi rst stored in the expanded node is placed in the leaf that has the edge with the value that corresponds to the value of the instance in that attribute. Otherwis e, if the expanded node has a numeric splitting attribute, then each instance is placed in the leaf th at corresponds with the splittin g test X V , i.e., the instances Afterward, the temporary class of each leaf is the majority class of the instances that arrive to the leaf, if two classes have the majority number of instances, then the fi rst class is chosen. If a leaf does not receive any instance, then the temporary class assigned to this leaf is the majority class of the instances used for expanding the node. Finally, the s instances used for the expansion are deleted, leaving all the new leaves empty, and DTLT marks the expanded node as an internal node. The algorithm for the expansion process is shown in Table 3.

Once the root node has been expanded, DTLT continues processing the remaining instances in the training set. Each instance traverses the DT built so far, until the instance reaches a leaf, where it is stored. When a leaf has s instances stored in it, DTLT expands the node as we explained before. However, in some of these leaves could happen that all the s instances are from only one class, i.e., they are homogeneous nodes. For a node of this kind, DTLT veri fi es if the temporary class assigned to this node is the same that the class of the s instances stored in the node. If this is not the case, DTLT assigns as new temporary class of the node the class of these s instances. Finally, DTLT deletes the s instances stored in the leaf, so it can receive other s instances.
Finally, when all the training instances have been processed, DTLT assigns to each leaf the class label of the majority class of the instances stored in that leaf. If a leaf does not have instances (it is empty), then DTLT assigns the temporary class as class label of that leaf.
 Once a DT has been built, DTLT uses it for classifying new instances in the same way as a traditional DT. An instance traverses the DT until it reaches a leaf, where the class label associated to that leaf is assigned to the new instance. 3.1. Time complexity analysis of DTLT algorithm
In order to obtain the time complexity of DTLT, we take into account its main steps for building a DT: reorganization of the training set, traversing the tree and expanding nodes.

For a training set with m instances, described by x attributes and divided in c classes, the reorganization step is O ( m ) . Since only one traverse of the training set is needed for alternating instances from each class.
 The complexity for traversing a DT with each instance depends on the number of levels in the DT. In a DT, in the worst case, we have at most O ( m/s ) levels since DTLT uses s instances for each expansion, and each expansion could generate a new level, then traversing the DT with all m instances is O ( m  X  m/s )= O ( m 2 ) in the worst case.

In the expansion process, for a single node, DTLT c hooses the best s plitting attribut e applying to each attribute the Gain Ratio Criterion, but using only the s instances stored in the leaf to be expanded, this is O ( s  X  x ) . As the maximum number of expansions that DTLT does for building a DT is in the worst case O ( m/s ) (since for each expansion DTLT uses s instances), then the whole expansion process in DTLT is O ( s  X  x  X  ( m/s )) = O ( x  X  m ) , however, for large training sets x m then the complexity of all the expansions for building a DT is O ( m ) .

Finally, the complexity of building a DT with DTLT is the sum of the complexities of the reorganizing, traversing and expansion steps, this is: [40] analyzed the complexity of ICE, which is O ( m  X  log( m )) . The complexity of BOAI is also O ( m  X  log( m )) [39] and the complexity of VFDT is, in the worst case, O ( m 2 ) [23]. As it can be observed, the time complexity of DTLT is, in the worst case, equal to VFDT X  X  and greater than ICE X  X  and BOAI X  X . Nevertheless, the complexities of DTLT and VFDT are for the worst case, therefore, in Section 4 we show an experimental analysis of the runtime spent by each algorithm for building DTs from large training sets. 3.2. Memory consumption analysis of DTLT algorithm
For building a DT, DTLT has to keep in main memory only the instance that is being processed at each moment and the DT already built with the previous instances. The maximum number of expansions that DTLT can do is m/s , then the space required for DTLT is O ( m/s )= O ( m ) .

The space that ICE needs for building the DT depends on the number of epochs ( e ) and the proportion of instances ( 0 &lt;p 1 ) that must be extracted from each epoch. Each epoch contains m/e instances, thus the size of each subsample is p  X  ( m/e ) , therefore the space required for storing all the subsamples is p  X  ( m/e )  X  e = p  X  m . Finally, since only one epoch and all the subsamples need to be stored at each moment, the number of instances that ICE needs to store is O (( p  X  m )+( m/e )) = O ( m ) ,since 0 &lt;p 1 and, for large training sets, e m .
 VFDT needs to store the instance that is being processed at each moment and the DT built previously. Therefore, the space required for this algorithm depends on the maximum number of expansions that VFDT can make, which is m/n ,where n is the number of instances used for verifying if a node must be expanded, and n m , then the space required by VFDT is O ( m/n )= O ( m ) .
 BOAI is an algorithm that needs to store the whole training set in main memory for building the DT. The instances are stored in the leaves of the DT as a set of lists, then the space that BOAI requires is O ( m ) .
 According to [10,11], the space require for IIMDT and IIMDTS is O ( m ) .

As it can be noticed, the space required by BOAI, VFDT, ICE, IIMDT, IIMDTS and DTLT for building a DT is, in the worst case, O ( m ) , the difference is in the effective space (the amount of memory that they use for building a DT). Therefore, in Section 4 we show an experimental analysis of the amount of memory used for each algorithm. 4. Experimental results
In this section, fi rst we analyze the behavior of DTLT when the maximum number of instances stored in a node (the parameter s ) varies. Then, we show a comparison of DTLT against ICE, VFDT, and BOAI (the most recent algorithms for building DTs) when the number of instances and attributes in the training set increases. For those tests using numeric training sets we included IIMDT and IIMDTS in the comparison. Besides, we show the behavior of DTLT when the order of the training instances varies. Finally, we present a comparison of the amount of memory that each algorithm uses for building a DT, since in the previous section we observed that the memory consumption complexity of DTLT is the same as that of ICE, VFDT, BOAI, IIMDT and IIMDTS.
 The datasets used in these experiments are described in Table 4. The fi rst dataset was obtained from the UCI Repository [35], the second dataset was obtained from the KDD Cup [21], the third was obtained from SDSS [31] and the last three datasets are synthetic datasets built using the database generator developed by Agrawal et al. using the Functions 1, 2 and 7 [1]. GalStar and all synthetic datasets are balanced datasets, since their classes have the same number of instances. The distribution of Forest CoverType is 211,840 instances in class 1, 283,301 in class 2, 35,754 in class 3, 2,747 in class 4, 9,493 in class 5, 17,367 in class 6 and 20,510 in class 7. KDD X  X  distribution is 972,781 instances in class 1 and 3,827,219 in class 2. Therefore, Forest CoverType and KDD Cup are imbalanced datasets. Additionally, all these datasets does not contain noise or missing values.

For all the experiments, we used 10-fold cross validation and each plotted result includes the 95% con fi dence interval, however, in some cases these con fi dence intervals are not visible in the fi gures because they are very small. The results show the processing time (including building and classi fi cation time, and for DTLT also the preprocessing time) and the accuracy rate for each dataset.
Our algorithm was implemented in C and all our experiments were performed on a PC with a Pentium 4 at 3.06 GHz, with 2 GB of RAM running Linux Kubuntu 7.10. For ICE, we implemented in C language a version based on [40], using 5 epochs and extracting 10 % of the instances in each epoch for generating the subsample used for building the fi nal DT (this percentage of instances was chosen according to the experiments presented by [40]). For fi nding the subsample in each epoch of ICE, we used a tree-based sampling technique that chooses from each leaf node of the DT some instances of the corresponding epoch. For VFDT we got the authors X  version (implemented in C) and for this algorithm we did the experiments using the parameter values recommended by the authors [9]. For BOAI we also got the authors X  version (an executable fi le), establishing 30 iterations for building the DT (this value was chosen based on [39]). Finally, IIMDT and IIMDTS were implemented in C language, according to [10,11]. 1 4.1. Parameter s
This experiment was done in order to analyze the behavior of DTLT when the maximum number of instances stored in a node, the parameter s , changes. For each dataset in Table 4, we evaluated several values for s : 50 and from 100 to 600 with increments of 100.

Figure 1 shows the results for DTLT when the value of the parameter s varies. As we can observe, the processing time increases when the value of s increases. With respect to the accuracy rate, for each dataset DTLT obtains similar accuracy no matter the value of s .

Taking into account these results, we recommend to use small values for s . In the next experiments, we will use s = 100 , since with this value DTLT spent less time than using other values, and the accuracy was the highest of all tested values for s , in all the datasets. 4.2. Increasing the number of instances in the training set
For these experiments we used the datasets described in Table 4. For each dataset we created several training sets, increasing the number of instances in each one.
With the Forest CoverType dataset we created training sets containing 50,000 to 550,000 instances (with increments of 50,000 instances). With this dataset we cannot apply BOAI, since this algorithm is only for two-class problems. Figure 2 shows the processing time and the accuracy rate obtained for this dataset. As we can notice, DTLT, VFDT and ICE obtained similar accuracy, but DTLT was up to 4.5 and 31.5 times faster than VFDT and ICE, respectively.

For the KDD dataset, we created different-size training sets from 500,000 to 4,500,000 with increments of 500,000 instances. Figure 3 presents a comparison among DTFS, ICE and VFDT. The algorithm BOAI does not appear in this fi gure because it could not process training sets with more than 200,000 instances, since for bigger training sets the algorithm presented a memory failure. From Fig. 3, we can notice that DTLT was faster than ICE and VFDT. With respect to the processing time, DTLT was up to 23.5 times faster than ICE and up to 22.5 times faster than VFDT. About the accuracy rate, DTLT and VFDT obtained similar results, and both of them were better than ICE. It is important to highlight that despite the fact that Forest CoverType and KDD datasets are imbalanced, DTLT obtained good results.

For GalStar we created different-size traini ng sets (from 500,000 to 4,000,000 with increments of 500,000). Figure 4 presents a comparison between DTLT, ICE and VFDT. The algorithm BOAI does not appear in this fi gure because for this dataset it could not process training sets with more than 300,000 instances, since this algorithm presented a memory failure. Besides, since GalStar is a numeric training set, we included IIMDT and IIMDTS algorithms in the comparison. As it can be noticed, all the algorithms obtained similar accuracy rates, but DTLT and IIMDTS were about 9.16, 1.13 and 1.10 times faster than ICE, VFDT, and IIMDT respectively. For this experiment IIMDTS was the fastest algorithm, however it is important to highlight that IIMDTS is an algorithm that only can process numeric training sets, moreover the DT built by IIMDTS is a multidimensional DT.

For the Agrawal datasets, we created, for each one, training sets from 500,000 to 6,500,000, with increments of 500,000 instances. Figure 5 shows the results obtained by DTLT, ICE, VFDT and BOAI with Agrawal F1, F2 and F7 datasets. In Fig. 5, we only show the results of BOAI until the training set of 2,000,000 of instances, since the processing time spent by this algorithm increased much faster than the processing time of the other algorithms. We can notice from Fig. 5 that DTLT is the fastest. For the Agrawal datasets, DTLT was, in average, up to 2.72 and 6.25 times faster than ICE and VFDT, respectively. With respect to the accuracy rate, DTLT and VFDT obtained similar results, and both of them were better than ICE. The algorithm ICE showed this behavior, since for these training sets in our experiments this algorithm did not fi nd a representative subset of instances for building the DTs. 4.3. Increasing the number of attributes in the training set
In this experiment, we use the KDD dataset for showing the behavior of DTLT when the number of attributes in a training set is increased. We used this dataset since it has the major number of attributes among the datasets in Table 4. For this experiment we created several training sets with different number of attributes, with 5 and from 10 to 40, with increments of 10 attributes. We compare our results only against ICE and VFDT, because BOAI could not process any of these training sets, since this algorithm presented a memory failure.

The results of this experiment are shown in Fig. 6. From this fi gure we observe that, when the number of attributes in the training set was increased, the processing time of DTLT increased slowly, while the processing time of ICE and VFDT increased much more faster. With respect to the accuracy rate, DTLT and VFDT obtained similar results, and both of them were better than ICE. 4.4. Varying the order of the training instances
In order to show the behavior of DTLT with respect to the preprocessing step, we have used GalStar dataset to create ten different training set orders. Each training set has a different random order of the data. Results of accuracy and time are shown in Fig. 7, where we can observe that does not matter the order of the input data, DTLT has a stable behavior. The processing time spent by DTLT as well as the accuracy rate were very similar for all the training sets.
 4.5. Memory use
In Section 3.2 we showed that ICE, VFDT, BOAI, IIMDT, IIMDTS and DTLT have the same memory consumption complexity. Therefore, in this experiment we show the difference among these algorithms in terms of the amount of memory that they use for building a DT. For measuring the amount of memory used by each algorithm, we use the memory tool [14] of Linux.
 In this experiment, we used the training sets created in Section 4.2 for the datasets described in Table 4. Figures 8 X 13 show the results obtained for each dataset. The algorithm BOAI was not included in these experiments, since it could not build a DT for all the training sets.
 From Fig. 8, we observe that DTLT and ICE used less memory than VFDT for building a DT with the Forest CoverType dataset. From Fig. 9, we observe, that for the KDD dataset, DTLT used less memory than ICE and VFDT. With respect to GalStar (a numeric dataset), see Fig. 10, ICE and IIMDT used less memory for building a DT than DTLT and IIMDTS, and they used less memory than VFDT. For the three Agrawal datasets, Figs 11 X 13, we can notice that DTLT and ICE used similar amount of memory for building a DT, and both algorithms used less memory than VFDT.

From these experiments, we can observe that DTLT use less memory than VFDT and similar amount of memory than ICE, IIMDT and IIMDTS. However, IIMDT and IIMDTS only can process numeric training sets and according the experiments of Sections 4.2 and 4.3, ICE is slower and it obtains lower accuracy results than DTLT. 5. Conclusions
In this work, we have introduced a new fast heuristic for building DTs from large training sets (DTLT) that overcomes the restrictions of the state of the art algorithms for building DTs from large training sets. For building a DT, DTLT processes all the instances in the training set without storing the whole training set in memory. The algorithm DTLT processes one by one the instances, in an incremental way, updating the DT with each one. Besides, in order to process large training sets, DTLT uses a small number of instances for expanding each leaf, discarding them after each node expansion.

DTLT has one user-de fi ned parameter ( s ), but our experiments showed that for different datasets the behavior of DTLT with respect to this parameter is very stable.

For large training sets described only by numeric attributes, we observed that DTLT is competitive in accuracy and time with IIMDTS, which is an algorithm designed for this kind of training sets. However, IIMDTS builds multidimensional DTs, while DTLT builds DTs which have the same structure that DTs built by a conventional DT induction algorithm.

From our experiments, we conclude that DTLT is faster than ICE, VFDT and BOAI, the most recent algorithms for building DTs from large training sets described by numeric and non numeric attributes, maintaining compe titive accuracy. Besid es, one important ch aracteristic of our al gorithm is that, when the number of attributes in the training set increases, DTLT has better behavior than ICE, VFDT and BOAI, since our algorithm slightly increases its processing time while ICE, VFDT and BOAI drastically increase their processing time. Moreover, we also observe, the preprocessing step of DTLT does not affect the results of the algorithm, since accuracy and processing time of DTLT conserve a similar behavior with different training set orders.

Based on the memory experiments, we noticed that DTLT uses less memory than VFDT and BOAI, while DTLT and ICE use similar amount of memory, however, ICE is slower and it obtains smaller accuracy results than DTLT.

Finally, we can conclude that DTLT is the best option for building DTs from large mixed training sets, specially when the dataset has a big number of attributes. As future work, we are planning to modify our algorithm in order to deal with noise and missing data. Also, the application of our algorithm over streaming data will be studied.
 References
