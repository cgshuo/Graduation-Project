 implying that also E  X  E  X  = E  X  . 2.: where some of the equalities follow from property 1, and the last equality holds since the alternating sum of binomial coefficients equals to 0. 3.: Since ( E  X  I ) has rank n  X  1, the eigenspace of E corresponding to the eigenvalue 1 is one X  X imensional. So meaning that the rows of E  X  are multiples of  X  . Also, we have ( E  X  I ) E  X  = 0, meaning that the columns of E  X  are constant vectors. Combining all together we get E  X  = 1  X  &gt; . 4.: Observe that ( E  X  E  X  ) n is Cesaro summable to 0, i.e. due to property 2. So, by Kemeny &amp; Snell (1960) (Thm. 1.11.1) and property 2, 5.: Follows from replacing Z with the right-hand side of Eq. (2).
 The proof is essentially identical to the proof of Theorem 1 in Schweitzer (1968), which is, however, restricted to Markov chain transition matrices. We have, E 1 and Z ( E 2  X  E which proves Eq. (3). Finally, 1.
 Recall the concept of a system dynamics matrix (SDM) (Singh et al., 2004), also known as prediction matrix represented by a k -dimensional linear PSR, and the square submatrix of SDM of dimension |A X O| k will be of rank k (e.g. see James (2005)).
 According to Wiewiora (2007), the system resulting from combining a k -dimensional linear PSR with control and a policy with memory of size l can be represented with a linear PSR without control whose dimension is at most k  X  l . We will denote the distribution over future sequences in such a system with P  X  . Let n be the largest rank of a SDM induced by some policy  X  , implying that a minimal dimensional PSR for this a stochastic process represented by an OOM for a fixed  X   X   X : 1. If G  X  is m -dimensional, { g (0)  X  ,..., g ( m  X  1)  X  } is a basis for G  X  . 2. the state evolution operator can be represented by the matrix E  X   X  R m  X  m relative to this basis, such that matrices E  X  1  X   X  , E  X  2  X   X  , concluding the proof.
 Note that  X  is a direct parametrization of a policy, meaning that elements of G  X  are polynomials in  X  . Thus, the column space of G  X  , such that these basis vectors are rational functions of  X  . Now we can define c ( i )  X  in the following recursive way: do not have singularity points, or in other words, are well defined for all  X   X   X .
 stationary mean of the stochastic process S induced by policy  X  due to the following: this vector represents the invariant state of the system with respect to column vectors in G  X  , which are by themselves linear combinations rational functions of policy parameters. Therefore, we get that each entry of the stationary distribution of PSR is also a rational function of policy parameters.
 To complete the proof, note that the stationary distribution of PSR that we have obtained coincides with the empirical distribution of sequences observed from data since the stationary distribution is unique, which it turn is due to the ergodicity assumption.
 Proposition 1. Let G  X   X  R m  X  n ,m  X  n, be a rational matrix X  X alued function of  X   X   X  , such that G  X  is bounded is a well defined orthogonal basis for the column space of G  X  for all  X   X   X  .
 defined. Suppose that such a singularity occurs to some element of b ( i )  X  generality assume that b ( j )  X  elements of the following terms (0  X  j &lt; i ) must approach singularity as  X   X   X  0 : But meaning that g ( i )  X  must have a singularity point at  X  0 since b ( j )  X  tion. The proof of this theorem uses two results that are given in Propositions 2 and 3 with their proofs attached. Proposition 2 provides a clear way of defining parameters of a PSR without control given a PSR (with control) and the parameters of a finite memory policy. Proposition 3 shows that one can obtain the state of the PSR (with control) from the state of the PSR without control by applying a linear operator. not assume that the process is AMS (otherwise there is nothing to prove), only that the process is ergodic. where R min is the lowest achievable reward (recall that the reward is bounded). From the definition we have that pointwise everywhere. Since { f k } are uniformly bounded, ensures that  X  E  X  ( f ) = E  X  (lim k  X  X  X  R k )  X  R min 3 ,  X  f = E  X  ( f ) almost surely.
 the PSR evolution matrix. Due to the properties of this matrix discussed earlier we have that Therefore, where (3) follows from the definition of the linear PRP; (4) and (8) are due to Proposition 3 with V  X  being the linear operator transforming the state of the PSR without control to the state of the PSR (with control); (5) is by definition of a linear PSR; (6) is due to the properties of the PSR evolution matrix W  X ,  X  , where  X   X  is the probability of taking action a .
 rational functions of  X  as well since they can be immediately obtained from  X   X  and  X   X  ( a ) through summation control state  X   X  is a vector of joint predictions).
 { c 0 , { T o } o  X  X  , { A a } a  X  X  } be the representation of a stochastic finite state policy  X  of size l . Let PSR without control induced by the policy  X  . In particular, Proof.
 For clarity, here is the description of the meaning of the stochastic finite state policy parameters:  X  c 0 -the initial distribution over the states of the policy  X  A a -the diagonal matrix with [ A a ] ii being equal to probability of taking action a in state i  X  T o -the transition matrix defining the state dynamics of the policy corresponding to observing percept o Let C ao , A a T o . Recall that for any history h , where 1 is a column vector of ones of an appropriate size. Observe that By induction (similarly to Wiewiora (2005)) we get, where we used the fact that C ao Now, the first property holds since The second property also holds because Finally, the last property is due to linear PSR without control representing this process. Then, the state of PSR (with control), p , can be obtained from the state of PSR without control p  X  by applying a linear operator.
 Proof. We will refer to p as both the PSR (with control) and the state of this PSR (which one is meant should be clear from the context). Equivalently, we will refer to p  X  as both the PSR without control and the state of this PSR.
 { c be the state of this  X  X SR-like X  construct after observing history h . Then, Therefore we can recover the state of the environment p from the state of our construct by applying a linear operator. What is missing yet, is the connection between this construct and the PSR without control p  X  . here the conclusion follows.
 We can represent the operator V  X  with matrix V  X   X  R n  X  k . V  X  can be obtained by solving the following system to the same histories.
 In both representations the average reward function is linear in the stationary distribution as a function of the policy parameters, hence the complexity of the stationary distribution gives the upper bound on the complexity of the average reward. Let k be the size of the stochastic finite state controllers we consider. We analyze the complexity with respect to the POMDP representation first.
 Observe that the HMM representing the combined system and the policy will have km hidden states. Since the Markov chain defined over the hidden states induced by any of the policies of interest is irreducible due to the ergodicity assumption, the corresponding transition matrix satisfies the conditions of Theorem 1 from Schweitzer (1968), which is a special case of Theorem 2. Each entry of this transition matrix is a polynomial of a fixed E 2 be our parametrized transition matrix. One can analytically invert H that each entry of H 1  X  2 is a rational function whose degree is O ( km ). Therefore, by Theorem 2 the degree of the stationary belief state  X  2 is also O ( km ).
 Now we consider the reward process being represented by a linear PRP. Observe that the linear PSR without control representing the action X  X bservation stochastic process induced by the policy above is at most kn dimen-sional. Each entry of the SDM describing this action X  X bservation stochastic process is a rational function with the leading degree equal to the length of the history plus test. Therefore, the column vectors stored in G  X  in the proof of Theorem 3 are of degree at most kn . The elements of the basis constructed from the columns of G  X  are also of degree at most O ( kn ) (see Proposition 1), as well as the entries of the bottom row of the evolution matrix E  X  constructed in Theorem 3. Similarly to a POMDP case, we need to apply Theorem 2 for some fixed E , Z 1 ,  X  1 , where E 2 , E  X  is the function of the policy parameters. Observe that from Theorem 2 we also have same order.
 To conclude the proof, recall that the number of dimensions required to represent the same system in a linear PSR framework ( n ) is at most equal to the number of hidden states in the corresponding POMDP ( m ), and can be, at times significantly, smaller (Jaeger, 2000; Littman et al., 2001).
 Faigle, U. and Schonhuth, A. Asymptotic mean stationarity of sources with finite evolution dimension. Infor-mation Theory, IEEE Transactions on , 53(7):2342 X 2348, 2007.
 Jaeger, H. Observable operator models for discrete stochastic time series. Neural Computation , 12(6):1371 X 1398, 2000.
 James, M.R. Using predictions for planning and modeling in stochastic environments . PhD thesis, The University of Michigan, 2005.
 Kemeny, J.G. and Snell, J.L. Finite Markov chains , volume 356. van Nostrand Princeton, NJ, 1960. Littman, M.L., Sutton, R.S., and Singh, S. Predictive representations of state. Advances in neural information processing systems , 14:1555 X 1561, 2001.
 1968.
 Singh, S., James, M.R., and Rudary, M.R. Predictive state representations: A new theory for modeling dynamical Press, 2004.
 conference on machine learning , pp. 964 X 971. ACM, 2005.

 Yuri Grinberg ygrinb@cs.mcgill.ca School of Computer Science, McGill University, Canada Doina Precup dprecup@cs.mcgill.ca School of Computer Science, McGill University, Canada Partial observability is prevalent in practical domains, in which one has to model systems based on noisy observations. In domains with partial observabil-ity and finite action and observation spaces, a pop-ular framework for modeling and control is that of finite state partially observable Markov decision pro-cesses (POMDP) (Kaelbling et al., 1998). POMDPs generalize the well-known framework of Markov deci-sion processes (MDPs); hence, they inherit many use-ful properties from MDPs. In particular, the results of Schweitzer (1968) imply that the stationary distri-bution of an ergodic Markov chain is a rational func-tion of the changes in the transition matrix. Under mild conditions, this result can be applied immediately to finite state POMDPs: the stationary distribution of the Markov chain induced by some finite memory policy exists and is a rational function of policy pa-rameters. This is important when one has to estimate expectations of different quantities (such as returns) with respect to the policy, since these expectations are taken with respect to the stationary behavior of the process. In particular, one can show that the average reward in finite state POMDPs is a linear function of the stationary distribution. Thus, knowing that the stationary distribution is robust to small changes in the policy implies that the estimates of different quan-tities based on the stationary distribution are robust as well.
 About a decade ago, a new modeling framework for finite action and observation spaces was introduced, named (linear) predictive state representations (PSR) (Littman et al., 2001; Singh et al., 2004). PSRs alle-viate model learning challenges encountered in finite state POMDP models, because their state represen-tation is grounded in measurable quantities. More importantly, PSRs are capable of representing some POMDPs with smaller dimension than the num-ber of hidden states. Moreover, there are systems which can be represented by a linear PSR but not a POMDP (Jaeger, 2000).
 PSRs are formulated in terms of actions and obser-vations, but rewards do not play as special role in the original framework, as they do in MDPs and POMDPs. The planning problem using linear PSRs has already been tackled by several authors, e.g. James et al. (2004); Boots et al. (2010); Izadi &amp; Pre-cup (2003) but little was said about how their specific assumptions on the reward function affect the com-bined reward-PSR process. The main motivation for our work is to provide a theoretical framework for re-ward processes based on PSRs, and analyze the be-havior of the average reward in this framework. There are two practical implications of this idea. First, this would enable the development of learning and plan-ning algorithms which are directly based on evaluat-ing reward averages, rather than learning a full predic-tive representation first, and then using it to estimate policy values. In the MDP and POMDP framework, methods based on value functions and/or policy search are known to scale better to very large problems than methods based on a full model estimation. We would like to bring this advantage to linear PSRs as well. Second, defining an appropriate reward process based on PSRs enables an easier theoretical analysis of any control methods.
 In Section 4, we present a (linear) predictive state re-ward process (PRP), which is built on top of a (linear) PSR and provides a suitable framework for average re-ward optimization. We analyze some of its properties and discuss its relationship to the POMDP framework. Our most important result connects the size of the linear PSR representation underlying the linear PRP to the complexity of the average reward as a func-tion of policy parameters. We prove that a compact PSR representation will induce a reward process with a simple average reward function. We provide some examples in which the linear PRP representation is much smaller than the corresponding POMDP repre-sentation; hence, using the PRP for policy search, for example, could potentially be much easier and yield a solution faster.
 The key ingredient in analyzing the PRP is to analyze the behavior of its stationary distribution as a function of the policy used, similar to the case of a POMDP. This stationary distribution is in fact the stationary distribution of the underlying linear PSR, which has been shown to exist (Faigle &amp; Schonhuth, 2007) given a fixed policy. However, its form has not been stated before. In Section 3, we show that the stationary dis-tribution induced by a finite memory policy in a linear PSR is a rational function of the policy parameters, as long as the process is ergodic, similar to a finite state POMDP framework. This result is a novel contribu-tion in itself, and can be useful if a stability / perturba-tion analysis of a linear PSR is desired. Throughout the paper, we omit the proofs, for brevity; they are included in the appendix provided as supplementary material. We consider systems with control having finite action and observation/percept spaces, denoted as A and O respectively, and rewards coming from a bounded set R  X  R . We define a reward process as a tuple ( X  , F , T , X  0 ), where ( X  , F ) is a measurable space,  X  is a probability kernel representing the effect of ac-tions, and T is a shift operator. In our setting, the elements of  X  can be viewed as infinite sequences of action-observation-reward triples,  X   X   X  :  X  =  X  a 1 ,o 1 ,r 1 ,a 2 ,o 2 ,r 2 ,...  X  ,a i  X  X  ,o i  X  X  ,r In this setting, T is defined by:
T(  X  a 1 ,o 1 ,r 1 ,a 2 ,o 2 ,r 2 ,...  X  ) =  X  a 2 ,o 2 ,r T  X  1 (  X  ) = {  X  0 | T(  X  0 ) =  X  } .
 At each time step, the reward process waits for the agent to take an action and outputs an observation-reward pair. We are interested in the average reward setting, where the goal is to find a behavior (or pol-icy)  X  maximizing lim n  X  X  X  1 n P n t =1 R t where R t is the random variable denoting the reward observed at time t , when following policy  X  . The average reward set-ting is in fact preferable to discounted rewards in par-tially observable systems when the dynamics are not known 1 (Singh et al., 1994). We consider the classical policy definition, in which the action choice depends (stochastically) only on actions and observations, but not rewards:  X  k  X  N , X  :  X  A,O  X  k  X  [0 , 1] |A| . Repre-senting any arbitrary policy of this kind is infeasible, since it requires infinite memory. Therefore, only poli-cies having a finite memory representation are con-sidered. Such policies can be implemented through (stochastic) finite state controllers, whose parameters represent the transition probabilities between internal states of the policy, and the probabilities of actions conditioned on the policy state. We call this a di-rect policy parametrization . In particular, if the policy is memoryless and open-loop (i.e., it chooses actions with a fixed distribution regardless of the time step and history), its direct parametrization will simply be the vector representing the distribution over actions. Fixing a policy  X  generates a stochastic process whose elements are action-observation-reward triples. We de-note the distribution of this process by P  X  or P  X  where  X  is the parametrization of  X  , and the expectation with respect to P  X  by E  X  or E  X  . P  X  is asymptotically mean stationary (AMS) (Gray &amp; Kieffer, 1980) if  X  F  X  X  :  X  P  X  ( F ) = lim  X  P  X  is called stationary mean ; let E  X   X  be the expectation with respect to  X  P  X  . The AMS property is a necessary and sufficient condition for the running averages of any bounded quantity to converge; in particular, it guaran-tees the existence of the average reward. For example, if the reward process is a finite state Markov decision process (MDP), the stationary mean is the stationary distribution of the Markov chain induced by policy  X  . Moreover, P  X  is ergodic if where G  X  X  is a set of invariant events (T  X  1 G = G ). Thus, if P  X  is AMS and ergodic, meaning that the average reward is constant almost surely (for more details see Gray (2009)). In a finite state MDP, the ergodicity of the process corresponds to the Markov chain induced by  X  being irreducible. 2.1. Predictive State Representations The idea of a predictive state of a system is rooted in the following observation: knowing the distribution of any finite sequence of observations given any se-quence of actions describes completely and accurately the future dynamics of that system. In the setting we consider, the action and observation spaces are finite, meaning that one can represent the predictive state of the system with a countably infinite vector enumer-ating probabilities of finite sequences of observations given sequences of actions. We will refer to this vector as the state of the system. In general, it might not be possible to represent this vector concisely.
 In Littman et al. (2001), a new representation for the action-observation controllable process (defined with-out rewards) was proposed, called predictive state rep-resentation (PSR). It captures processes in which pre-dictions of a finite number of sequences are enough to compute the prediction of any other sequence. For-mally, let h,t  X   X  A  X  O  X   X  be sequences of action-observation pairs of finite length, where h is a his-tory , i.e., a sequence observed until now, and t is a sequence that might occur in the future, called test . Let Q = { q 1 ,...,q n } be a set of tests, called core tests , and p ( h ) , [P psr ( q 1 | h ) ,..., P psr ( q n | h )] diction vector where P psr ( q | h ) ,  X  0 ( hq )  X  probability of observing percepts from q given that the agent has seen history h and is going to take actions from q . Then, p ( h ) is a predictive state representation of dimension n if and only if for any test t and history h , where f t : [0 , 1] n  X  [0 , 1] is any function independent of history. If f t is linear for all t , then the representation is called linear PSR, and where the function f t is replaced by a linear projection m t . It has been shown (Singh et al., 2004; Wiewiora, 2007) that a linear PSR possesses a finite parametriza-tion of the form satisfying the following properties: 2.  X  a  X  X  : P o  X  X  M ao m  X  = m  X  , where p 0 is the starting state of the linear PSR. Using property 3 one can verify that represents the PSR state after observing ao , given the previous PSR state p ( h ). Predictions of future se-quences can be calculated using the new PSR state and the rest of the linear PSR parameters only. For better readability, from now on we drop the prefix  X  X in-ear X  from linear PSR and explicitly state  X  X on-linear X  when referring to the general PSR framework.
 Any finite memory policy  X  in a PSR induces an action-observation stochastic process, which can also be represented by a PSR, possibly of a larger dimen-sion (Wiewiora, 2007). This process is often called PSR without control. In PSRs without control, prop-erty 2 reduces to P ao  X  X  X O M ao m  X  = m  X  , and in property 3,  X  X  psr  X  is replaced with  X  X   X   X , where  X  is the policy at hand. A PSR without control can be represented in at least two more frameworks: observ-able operator models (OOM) (Jaeger, 2000) and trans-formed PSRs (TPSR) (Boots et al., 2010; Rosencrantz et al., 2004). These frameworks can be seen as differ-ent parametrizations of the same process, while keep-ing the same dimension to represent its state (Singh et al., 2004; Boots et al., 2010; James, 2005). There-fore, one can use any of these frameworks equivalently to analyze properties of the system. In particular, our analysis uses results obtained under the OOM frame-work (Faigle &amp; Schonhuth, 2007).
 Finally, the key property of these frameworks is that certain systems can be represented more compactly compared to the classical finite state hidden Markov model (HMM) representation (Jaeger, 2000). More-over, some of these systems might not be representable using finite-state HMMs at all. This is the reason why we focus on this type of representation. In this section, we investigate how the stationary dis-tribution of a PSR behaves as the policy changes, where by stationary distribution we mean the station-ary state of the system represented by a PSR. For this purpose, we focus on the stationary distribution of a PSR without control induced by some policy, and see how changes in this policy affect this distribution. Let  X   X  be the stationary distribution of a PSR without control induced by policy  X  2 . Let  X  t  X  N : p t be the expected state of the PSR at time t . Assuming that p 0 =  X   X  , we have, by definition: Now, let be the PSR evolution matrix. This is called evolution matrix due to the following, which can be easily seen from property 3: The matrix M  X  will in fact define the stationary dis-tribution of the PSR. Under the appropriate basis, it satisfies the conditions of Lemma 1 (Faigle &amp; Schon-huth, 2007), and it can be seen as a generalized Markov transition matrix. More precisely, its spectral proper-ties are the same as those of a usual Markov transition matrix, and the rows sum to 1, but some of the values might be negative (see Faigle &amp; Schonhuth (2007) Sec-tion IV). The following Lemma shows that several im-portant properties of Markov transition matrices gen-eralize to evolution matrices as well (represented under the appropriate basis). Lemma 1. Let E  X  R n  X  n be such that exists, and the rows of E sum to 1. Also assume that ( E  X  I ) has rank n  X  1 , where I is an identity matrix of appropriate size. Then the following holds: 1. EE  X  = E  X  E = E  X  E  X  = E  X  . 2. ( E  X  E  X  ) n = E n  X  E  X  . 3. E  X  = 1  X  &gt; , where 1 is a column vector of ones;  X  4. Let Z , [ I  X  ( E  X  E  X  )]  X  1 , then Z is well defined The next theorem extends Theorem 1 of Schweitzer (1968) to the case in which the two matrices at hand are evolution matrices.
 Theorem 2. Let E 1 , E 2 be as in Lemma 1, and let E 1 , E Lemma 1 with respect to E 1 and E 2 respectively. Then: exists and is given by H 1  X  2 = Z  X  1 1 [ I  X  E  X  1 + E  X  Moreover, Theorem 2 lies in the heart of our work, since it estab-lishes an exact relationship between the eigenvectors corresponding to eigenvalue 1 of two evolution matri-ces. Similarly to the theory of Markov chains, these eigenvectors represent the stationary distributions of the corresponding PSRs. Specifically, let E 1 , Z 1 and  X  1 be fixed, while E 2 varies. Theorem 2 shows that  X  2 is a rational function of the entries in E 2 . This is due to the fact that the matrix inverse can be calculated analytically through Cramer X  X  rule.
 The next theorem is the main result of this section. Theorem 3. Let an action-observation controllable process S be representable by a finite dimensional lin-ear PSR, and let  X   X   X  be a direct parametrization of a stochastic finite memory policy. If the stochastic process generated from S induced by any policy  X   X   X  is ergodic, then the stationary state of S is a rational function of  X  .
 The proof (included in the supplementary material) shows how to construct the parametrized evolution matrix in an appropriate basis, given a PSR without control parametrized by a policy. The construction guarantees that the entries of the evolution matrix are rational functions of the policy parameters. We show that this construction is well defined, and then invoke Theorem 2 to complete the proof.
 As mentioned, Theorem 3 requires the ergodicity con-dition for any policy under consideration. A violation of this condition implies the existence of a policy for which the initial state of the system affects its asymp-totic behavior. In the theory of Markov chains, for example, this translates into the irreducibility condi-tion on the chains induced by any policy from the set of policies considered. In particular, the irreducibility condition(s) is either satisfied for all strictly stochas-tic policies or not satisfied for any policy. Hence, this assumption is considered standard in the policy search literature (Baxter &amp; Bartlett, 2001; Peters &amp; Schaal, 2008). In this section we define a (not necessarily linear) PSR-based reward process. The goal is to construct a re-ward process that inherits some of the useful proper-ties of the PSR framework on one hand, while being general enough and suitable for average reward opti-mization on the other hand. In particular, we let the reward be continuous. Although we keep the defini-tion quite general, we are interested in systems that are based on the linear PSR framework, and only this setting is treated afterwards.
 Let h ( t ) represent the sequence of action-observation-reward triplets of length t , and  X  h ( t ) represent the cor-responding sequence of action-observation pairs only. Definition 1. A predictive X  X tate reward process (PRP) is a reward process whose action-observation component can be represented by a (possibly non-linear) PSR with some state vector s  X  R n , and whose reward R t at time t satisfies  X  t  X  N : where s (  X  h ( t ) ) is the PSR state after observing  X  { f
R,a } a  X  X  are functions independent of A linear PRP is a PRP whose action-observation com-ponent can be represented by a linear PSR, and in which f R,a t are linear functions: where  X  a  X  A : r a  X  R n . The dimension of the PRP is defined to be the dimension of the underlying PSR. Note that Definition 1 in fact formalizes a setting un-der which most of the planning problem in linear PSRs has been tackled. For example, in Boots et al. (2010) the reward function is obtained by applying a linear regression from PSR states to the observed rewards. James et al. (2004) assumes that the rewards are dis-crete and incorporates them directly into the observa-tion vector. Izadi &amp; Precup (2003) defines the reward signal itself to be a linear function of the state of a linear PSR. Thus, current PSR X  X ased planning algo-rithms either implicitly assume that the underlying re-ward process is a linear PRP, or have more restrictive explicit assumptions.
 The rest of this section is devoted to the analysis of the behavior of the average reward as a function of a policy in systems represented by a linear PRP. Two questions arise in this context. First, it is not clear whether the average reward is well defined for any fi-nite memory policy. It has been shown that systems represented by a linear PSR are AMS for any finite memory policy (Grinberg &amp; Precup, 2012). This fact guarantees that averages of different observable quan-tities from action X  X bservation pairs are well defined, but not necessarily the average reward itself. Second, when/if the average reward is well defined, we want to characterize its behavior as a function of the policy pa-rameters. Both questions are addressed in Theorem 4. We show that under mild conditions the average re-ward exists and is linear in a quantity derived from the stationary distribution of the underlying PSR. Theorem 4. Let a n -dimensional linear PRP be er-godic for a collection of stochastic finite state policies of size m given by direct parametrization  X   X   X  . Let  X   X  be the stationary state of the reward process with-out control induced by policy  X  . Denote by  X  PRP  X   X  R n and  X  Pol  X   X  R m the stationary states of the PRP and the policy correspondingly, obtained from  X   X  . Also, let  X  a  X  A :  X  PRP  X  ( a )  X  R n be the PRP state obtained from starting the PRP at state  X  PRP  X  and taking ac-tion a . Then, the average reward is a rational function of the policy parameters, defined by: where the rewards are collected by following policy  X  , and  X  a  X  A :  X  a represents the vector of probabilities of taking action a in each state of the policy. The form of the average reward in a linear PRP is not surprising, since the average reward takes the same form in the MDP/POMDP setting. However, this re-sult is important not only due to the generalization of the average reward behavior from POMDPs to lin-ear PRPs, but because of its implications on the com-plexity of the average reward function. The following corollary connects the complexity of the average re-ward function to the dimensionality of the PSR repre-sentation and the number of hidden states in the cor-responding POMDP representation. Since the average reward is a rational function of policy parameters in both cases, we measure the complexity of this func-tion in terms of the degree of the multivariate polyno-mials appearing in either numerator or denominator, whichever is larger.
 Corollary 5. Consider a system that can be rep-resented by both POMDP with m hidden states and n -dimensional linear PRP. Let k be the size of the stochastic finite state controllers under consideration. The degree of the average reward function, when an-alyzed using the linear PRP representation is O ( kn ) , which can be (significantly) smaller compared to the function of degree O ( km ) obtained in the POMDP framework.
 In the following subsections, we address two points that were not yet discussed in enough detail. First, we discuss the representational power of a linear PRP, compared to that of a POMDP. Second, we pro-vide several synthetic examples of linear PRPs whose dimension is smaller than that of a corresponding POMDP. These examples highlight the benefits of the linear PRP framework. 4.1. Representation power of a linear PRP As outlined above, existing planning algorithms in PSRs fall under the linear PRP representation as-sumptions. However, certain reward processes can be represented with a POMDP but not with a linear PRP. Although this might seem as a disadvantage, we argue that such systems are ill X  X odeled from the perspective of reinforcement learning.
 The discrepancy arises from the fact that although the belief state of a POMDP is always sufficient to pre-dict the expected reward, the predictive state for the future action X  X bservation sequences is not necessarily sufficient. For example, one can think of a POMDP with aliased hidden states that only differ in terms of their reward function. However, this setting under-mines the classical approach of constructing policies solely based on actions and observations, since it is clear that knowing the reward signal allows making better predictions of future rewards in this case. If the model of the system is not known a priori, it is even less clear how to learn such a model. To the best of our knowledge, current state-of-the-art model learn-ing techniques estimate the reward function after the action X  X bservation process has been modeled. Even when the model is known, peculiar behavior might take place if the policy ignores the reward signal, e.g., the average reward can depend on the initial hidden state of the POMDP, despite the hidden dynamics being policy independent (Yu &amp; Bertsekas, 2008). Hence, in reinforcement learning, it is reasonable to assume that the observations provide enough information about the reward, as is the case in the linear PRP setting. 4.2. Linear PRP examples We now present a few synthetic examples of dynamical systems with control that can be represented exactly by a PRP significantly more compactly, compared to the most compact representation of the system in the POMDP framework. The examples are based on the probability clock example without control from Jaeger (2000), described in the OOM framework. Although these are synthetic examples, such  X  X locks X  are in fact common in nature, e.g. in bistable biological systems (Chaves et al., 2008). The shared property among our examples is the fact that at least one of the PSR ma-trices is a rotation matrix. The angle of the rotation will, in fact, determine the minimum number of states that the POMDP needs to represent the system. We first describe the probability clock example, since all the following systems are based on the same concept. Consider a system with two observations O = { o 1 ,o 2 } , and 3-dimensional state. The initial state is a vector s represent the change of state corresponding to each of the observations: M o 1 performs the rotation of the state by an angle  X  around the first axis, and M o 2 resets the state back to s . The predictions of the system are given by : The name of this example is due to the interesting behavior of the one step prediction of o 1 : if we observe only sequences of o 1 -s this prediction oscillates (the pattern is as in Fig. 1). The minimum number of states required for an HMM to represent this system exactly is the minimum k such that k  X   X  is a multiple of 2  X  (Jaeger, 2000). Hence, depending on  X  , one might require an arbitrarily large number of states in the corresponding HMM for an exact representation; an infinite number of states is needed if  X  is an irrational degree.
 We now present several systems with control and re-ward functions that generalize the idea of the prob-ability clock example to POMDPs. All the exam-ples consider two-action ( A = { a,b } ) two-observation ( O = { o 1 ,o 2 } ) systems, such that policies with small memory perform (at times significantly) better than constant policies. The first and perhaps the most in-teresting scenario is a simple extension of the three dimensional probability clock. Let where I is an identity matrix of appropriate dimension. Hence, taking action a will result in the same behavior as that of the probability clock example. Taking action b , though, will result in an i.i.d sequence of coins flips -observations o 1 ,o 2 -but will not change the state. We equip this system with a reward signal and let its expectation be equal to P( o 1 | s ,a ), where s is the current system X  X  state. This specification satisfies the requirement of a linear PRP, since the expected reward is a linear function of the system X  X  state. The optimal behavior is to take action a until the system reaches the state with largest P( o 1 | s ,a ), then choose action b thereafter.
 The above example should be interpreted as the sys-tem that can operate under different modes. Action b runs the system in a given mode while action a changes the mode of the system in a stochastic fashion. Clearly, one can generalize this example to the setting in which the system X  X  operation itself requires several hidden states to represent, more actions are involved, etc. Op-erating the system in different modes does not affect the dynamics between these states, but potentially af-fects the reward obtained from each state-action pair. Following the same reasoning as above, it is clear that such a system might require significantly more hidden states than number of dimensions, if represented in the POMDP framework. We note that the lac operon (see e.g. Chaves et al. (2008)), as well as other genetic networks related to metabolizing different types of nu-trients, exhibit this type of  X  X ultiple mode X  operation, with rewards dependent on the mode.
 The rest of the examples illustrate the flexibility of the probability clock with respect to the choice of param-eters. The reward function is the same for all cases: 1 for observation o 1 and 0 for o 2 ; hence, the objec-tive is to maximize the average probability of o 1 per step. As before, we let o 1 be the observation that ro-tates the state and o 2 be the reset. Figure 1 illustrates a system in which both actions behave as probability clocks starting from the same state, but having dif-ferent rotation angles. The policy that chooses either action a or b at all times obtains an average reward of  X  0 . 69, while the policy that takes action a three times given that no reset occurred, and then b until the next reset, obtains an average reward of  X  0 . 72. Figure 2 illustrates an example in which two actions can have different resetting states, rotation angles and magnitudes of the cycles. A constant policy choos-ing one action at all times will obtain average reward less than 0 . 36. Yet, the policy that  X  X limbs the hill X  using action b and, once reset, chooses action a until the next reset, achieves an average reward of  X  0 . 66. Such a policy requires only two internal states. The behavior of the average reward as a function of some of the two-state policy parameters is also presented in Figure 2. Another example of a system whose ac-tions rotate the state in opposite directions and have slightly different resetting states can be found in the supplementary material depicted in Figure 3. A con-stant policy choosing one action at all times will obtain average reward  X  0 . 43, while the policy that changes its action right after observing a reset achieves average reward of  X  0 . 66. As in the previous example, such a policy requires only two internal states, with average reward behaving smoothly as shown in Figure 3. All these examples are meant to give a sense of a type of systems that can benefit from the linear PRP representation. Although these systems require a much larger number of hidden states to be represented in the POMDP framework, they only require a 3-dimensional linear PRP representation. This guaran-tees that the average reward is a simple function of a policy with small memory, suggesting that appropriate policy search techniques can be very efficient. We proposed a formal definition for the reward pro-cess based on the linear PSR framework, and analyzed some of its properties. We proved that the average re-ward is a rational function of the policy parameters, and its complexity depends on the dimension of the underlying linear PSR. This suggests that systems rep-resented by a small linear PRP should be amenable to effective policy search techniques. For example, one can use gradient-based policy search methods such as GPOMDP (Baxter &amp; Bartlett, 2001) to efficiently find a policy with an acceptable performance. However, previous policy search methods do not seem to exploit the shape of the average reward function derived in this paper. As a result, this is a particularly interesting avenue for global policy search methods since proper-ties like smoothness of the function are typically easy to exploit using this type of search. Moreover, the knowledge about the possible dimensionality of the system can boost the policy search even more. The development of suitable approaches and their analysis remains the main direction for future work.
 Another direction currently under investigation is the analysis of the behavior of the average reward func-tion for policies dependent on the predictive sufficient statistic of the process, i.e. the underlying state of the linear PSR (Aberdeen et al., 2007). This analysis could further shed light on how the error in an approx-imated PSR state affects the policy performance. Finally, the established result on the stationary distri-bution of a linear PSR can also be useful in the future development of stability guarantees for linear PSRs. Aberdeen, D., Buffet, O., and Thomas, O. Policy-gradients for PSRs and POMDPs. In Proceedings of international conference on artificial intelligence and statistics , 2007.
 Baxter, J. and Bartlett, P.L. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelli-gence Research , 15:319 X 350, 2001.
 Boots, B., Siddiqi, S.M., and Gordon, G.J. Clos-ing the learning-planning loop with predictive state representations. In Proceedings of the 9th interna-tional conference on autonomous agents and multi-agent systems , pp. 1369 X 1370, 2010.
 Chaves, M., Eissing, T., and Allgower, F. Bistable bi-ological systems: A characterization through local compact input-to-state stability. Automatic Con-trol, IEEE Transactions on , 53(Special Issue):87 X  100, 2008.
 Faigle, U. and Schonhuth, A. Asymptotic mean sta-tionarity of sources with finite evolution dimension.
Information Theory, IEEE Transactions on , 53(7): 2342 X 2348, 2007.
 Gray, R.M. Probability, random processes, and ergodic properties . Springer, 2009.
 Gray, R.M. and Kieffer, JC. Asymptotically mean sta-tionary measures. The Annals of Probability , 8(5): 962 X 973, 1980.
 Grinberg, Y. and Precup, D. On average reward pol-icy evaluation in infinite X  X tate partially observable systems. In Proceedings of international conference on artificial intelligence and statistics , 2012. Izadi, M.T. and Precup, D. A planning algorithm for predictive state representations. In Proceedings of the 18th international joint conference on artifi-cial intelligence , pp. 1520 X 1521. Morgan Kaufmann Publishers Inc., 2003.
 Jaeger, H. Observable operator models for discrete stochastic time series. Neural Computation , 12(6): 1371 X 1398, 2000.
 James, M.R. Using predictions for planning and mod-eling in stochastic environments . PhD thesis, The University of Michigan, 2005.
 James, M.R., Singh, S., and Littman, M.L. Planning with predictive state representations. In The 2004 international conference on machine learning and applications , 2004.
 Kaelbling, L.P., Littman, M.L., and Cassandra, A.R.
Planning and acting in partially observable stochas-tic domains. Artificial Intelligence , 101(1):99 X 134, 1998.
 Littman, M.L., Sutton, R.S., and Singh, S. Predictive representations of state. Advances in neural infor-mation processing systems , 14:1555 X 1561, 2001. Peters, J. and Schaal, S. Natural actor-critic. Neuro-computing , 71(7):1180 X 1190, 2008.
 Rosencrantz, M., Gordon, G., and Thrun, S. Learning low dimensional predictive representations. In Pro-ceedings of the twenty-first international conference on machine learning , pp. 88, 2004.
 Schweitzer, P.J. Perturbation theory and finite Markov chains. Journal of Applied Probability , pp. 401 X 413, 1968.
 Singh, S., James, M.R., and Rudary, M.R. Predictive state representations: A new theory for modeling dynamical systems. In Proceedings of the 20th con-ference on uncertainty in artificial intelligence , pp. 512 X 519. AUAI Press, 2004.
 Singh, S.P., Jaakkola, T., and Jordan, M.I. Learn-ing without state-estimation in partially observable
Markovian decision processes. In Proceedings of the eleventh international conference on machine learn-ing , volume 31, pp. 37. Citeseer, 1994.
 Sutton, R. http://www.incompleteideas.net/ sutton/book/errata.html , 1998.
 Wiewiora, E.W. Modeling probability distributions with predictive state representations . PhD thesis, The University of California at San Diego, 2007. Yu, H. and Bertsekas, D.P. On near optimality of the set of finite-state controllers for average cost
POMDP. Mathematics of Operations Research , 33
