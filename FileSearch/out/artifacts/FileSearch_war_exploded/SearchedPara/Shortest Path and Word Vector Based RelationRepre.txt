 Upon the era of big data, it X  X  urgent to extract useful information automatically from vast amount of data. As for the text data, relation extraction crucial for natural language understanding have been paid much attention. Primarily, the and semi-supervised methods. The formers require the relations comparison to be exact string matching. Therefore, it X  X  sensitive to diverse word forms such as polysemy or synonyms. While for the supervised methods, relations are extracted by manually defined features which need to be trained and classified. Therefore, it X  X  difficult to measure their distance directly.
 for relation representation by integrating dependency trees and Word2vec method[ 1 ]. It primarily advantaged in that relation was directly represented by accumulating words vector along the path so that further applications such as relations clustering could be performed conveniently and flexibly by any data mining techniques. 2.1 Semantic Relation Representation To characterize the essential relation between the queried entities, we use Stan-ford typed dependencies 1 for grammatical analysis. Then, by viewing the depen-dencies tree as a graph, the relation between any two entities can be obtained by applying shortest path searching such as Dijkstra algorithm[ 2 ]. In Natural Language Processing related applications, a fundamental require-ment is to reliably compute the semantic distance between given words or phrases. Typical previously publications apply the tree distance or Information Content by the aid of hierarchy concepts such as WordNet, or measuring their similarity in semantic space such as TF-IDF, Explicit Semantic Analysis (ESA)[ 3 ]. While the former method is usually impacted by corpus insufficiency and diversity in words forms, ESA describes the word based on huge-dimension Wikipedia cepts and brings unbearable computation. Compared with ESA, Word2vec pro-words with Skip-gram model. Due to its simple model architectures and evidently lower computational complexity compared with other methods, it can compute by accumulating the vectors along the shortest path. 2.2 Relation Clustering For deeper text understanding and knowledge acquisition, further techniques especially relations clustering should be addressed. On the basis of our relation representation, any data mining techniques can be applied on the collection of relations to implement further applications such as clustering. Here, the method of K-means clustering was employed. 3.1 System Implementation and Data Preparation The system was implemented by using Java7.0 and developed in the environment of Eclipse 4.2.1. And Prefuse 3 was employed for graphical presentation. To deal with the problem of data sparsity, We used Wikipedia corpus and extracted 43335 thousand sentences in total. Also, we obtain a training corpus derived from 3.06 million Wikipedia normal pages for the Word2vec training. The dimension of trained vector was set to 500 in our experiments. 3.2 Relation Clustering and Discussion We manually selected 10 relationships in Freebase as follows:  X  /location/country/capital  X  /people/person/nationality  X  /film/director/film  X  /olympics/olympic host city/olympics hosted  X  /book/book character/appears in book  X  /business/company brand relationship  X  /geography/river/mouth  X  /award/award/presented by  X  /military/military person/participated in conflicts  X  /astronomy/orbital relationship/orbits pus. Subsequently, relations clustering was performed by using K-means method based on Cosine measure. Experimental results were shown in Fig.1(the class number was set to 10 and pairs in same color belong to identical cluster). Com-pared with the benchmark(Freebase relations), our accuracy reaches 93.2%. In this paper, an new method based on shortest path and word vector was put ily advantaged in: 1)Convenient and flexible processing on vectors. 2)Reliable relations comparison based on semantic representation. 3)Evidently lightweight compared with millions of concepts-based methods like ESA.
