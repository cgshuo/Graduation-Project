 Saarland Univ. &amp; MPI INF Semi-supervised learning has attracted a lot of research fo cus in recently years. Most of the existing approaches can be roughly divided into two cate gories: (1) the clustering-based methods [12, 4, 8, 17] assume that most of the data, including both the labeled ones and the unlabeled ones, should be far away from the decision boundar y of the target classes; (2) the manifold-based methods make the assumption that most of dat a lie on a low-dimensional manifold in the input space, which include Label Propagatio n [21], Graph Cuts [2], Spectral Kernels [9, 22], Spectral Graph Transducer [11], and Manifo ld Regularization [1]. The comprehensive study on semi-supervised learning techniqu es can be found in the recent surveys [23, 3].
 Although semi-supervised learning wins success in many rea l-world applications, there still remains two major unsolved challenges. One is whether the un labeled data can help the classification, and the other is what is the relation between the clustering assumption and the manifold assumption.
 of unlabeled data based on the cluster assumption. They show that unlabeled data may be useful for improving the error bounds of supervised learn ing methods when the margin between different classes satisfies some conditions. Howeve r, in the real-world problems, it is hard to identify the conditions that unlabeled data can he lp.
 On the other hand, it is interesting to explore the relation b etween the low density assump-tion and the manifold assumption. Narayanan et al. [14] impl ied that the cut-size of the graph partition converges to the weighted volume of the boun dary which separates the two regions of the domain for a fixed partition. This makes a step f orward for exploring the connection between graph-based partitioning and the idea s urrounding the low density as-sumption. Unfortunately, this approach cannot be generali zed uniformly over all partitions. Lafferty and Wasserman [13] revisited the assumptions of sem i-supervised learning from the perspective of minimax theory, and suggested that the manif old assumption is stronger than the smoothness assumption for regression. Till now, the und erlying relationships between the cluster assumption and the manifold assumption are stil l undisclosed. Specifically, it is unclear that in what kind of situation the clustering assump tion or the manifold assumption should be adopted.
 In this paper, we address these current limitations by a unifi ed solution from the perspective of the regularization strength of the unlabeled data. Takin g Transductive Support Vector Machine (TSVM) as an example, we suggest an framework that in troduces the regularization strength of the unlabeled data when estimating the decision boundary. Therefore, we can obtain a spectrum of models by varying the regularization st rength of unlabeled data which corresponds to changing the models from supervised SVM to Tr ansductive SVM. To select the optimal model under the proposed framework, we employ th e manifold regularization assumption that enables the prediction function to be smoot h over the data space. Further, the optimal function is a linear combination of supervised m odels, weakly semi-supervised models, and semi-supervised models. Additionally, it prov ides an effective approach towards combining the cluster assumption and the manifold assumpti on in semi-supervised learning. The rest of this paper is organized as follows. In Section 2, w e review the background of Transductive SVM. In Section 3, we first present a framework o f models with different reg-ularization strength, followed by an integrating approach based on manifold regularization. In Section 4, we report the experimental results on a series o f benchmark data sets. Section 5 concludes the paper. Before presenting the formulation of TSVM, we first describe the notations used in this and the unlabeled ones. We assume that the first l examples within X are labeled and the TSVM [12] maximizes the margin in the presence of unlabeled d ata and keeps the boundary traversing through low density regions while respecting la bels in the input space. Under the maximum-margin framework, TSVM aims to find the classific ation model with the maximum classification margin for both labeled and unlabele d examples, which amounts to solve the following optimization problem: where C and C  X  are the trade-off parameters between the complexity of the fu nction w and Note that we remove the bias term in the above formulation, si nce it can be taken into account by introducing a constant element into the input pat tern alternatively. As in [19] and [20], we can rewrite (1) into the following opti mization problem: The optimization problem held in TSVM is a non-linear non-co nvex optimization [6]. During past several years, researchers have devoted a significant a mount of research efforts to solving this critical problem. A branch-and-bound method [5] was de veloped to search for the optimal solution, which is only limited to solve the problem with a small number of examples due to involving the heavy computational cost. To apply TSVM for large-scale problems, Joachims [12] proposed a label-switching-retraining proc edure to speed up the optimization procedure. Later, the hinge loss in TSVM is replaced by a smoo th loss function, and a gradient descent method is used to find the decision boundary in a region of low density [4]. In addition, there are some iterative methods, such as deter ministic annealing [15], concave-convex procedure (CCCP) [8], and convex relaxation method [ 19, 18]. Despite the success of TSVM, the unlabeled data not necessarily improve classifi cation accuracy.
 To better utilize the unlabeled data, unlike existing TSVM a pproaches, we propose a frame-work that tries to control the regularization strength of th e unlabeled data. To do this, we intend to learn the optimal regularization strength configu ration from the combination of a spectrum of models: supervised, weakly-supervised, and se mi-supervised. errors of unlabeled data. Note that the penalization on the m argin errors of unlabeled data can be included if needed. Therefore, we have the following f orm of TSVM that can be derived through the duality: 3.1 Full Regularization of Unlabeled Data In order to adjust the strength of the regularization raised from the unlabeled examples, we introduce a coefficient  X   X  0, and modify the above problem (3) as below: Obviously, it is the standard TSVM for  X  = 1. In particular, the larger the  X  is, the stronger the regularization of unlabeled data is. It is also importan t to note that we only take into account the classification errors on the labeled examples in the above equation. Namely, we only denote  X  i for each labeled example.
 prediction for the labeled and the unlabeled examples, resp ectively. According to the inverse where When the unlabeled data are loosely correlated to the labele d data, namely when most of the elements within K u,l are small, this leads to M u  X  K u . We refer to this case as  X  weakly unsupervised learning  X . Using the above equations, we rewrite TSVM as follows: tion problem: To understand the regularization function  X  ( f l ,  X  ), we first compute the dual of the problem (6) by the Lagrangian function: the derivatives vanish for optimality, we have where I is an identity matrix.
 Replacing f u in (6) with the above equation, we have the following dual pro blem: The above formulation allows us to understand how the parame ter  X  controls the strength of regularization from the unlabeled data. In the following , we will show that a series of learning models can be derived through assigning various va lues for the coefficient  X  . 3.2 No Regularization from Unlabeled Data First, we study the case of  X  = 0. We have the following theorem to illustrate the relation ship between the dual problem (7) and the supervised SVM.
 Theorem 1 When  X  = 0 , the optimization problem is reduced to the standard superv ised SVM.
 Proof 1 It is not difficult to see that the optimal solution to (7) is  X  = 0 . As a result,  X  ( f l ,  X  ) becomes becomes According to the matrix inverse lemma, we calculate M  X  1 l as below: Finally, the optimization problem is simplified as unlabeled data are not employed to regularize the decision b oundary when  X  = 0 . 3.3 Partial Regularization of Unlabeled Data Second, we consider the case when  X  is small. According to (7), we expect  X  to be small when  X  is small. As a result, we can approximate ( M u  X  M u D(  X  ) M u )  X  1 as follows: Consequently, we can write  X  ( f l ,  X  ) as follows: where  X  ( f l ,  X  ) is the output of the following optimization problem i = 1 , . . . , n  X  l , where  X  1 ( M u ) represents the maximum eigenvalue of matrix M u . The resulting simplified problem becomes As the above problem is a linear programming problem, the sol ution for  X  can be computed as: From the above formulation, we find that  X  plays the role of a threshold of selecting the unlabeled example, the above formulation can be interprete d in the way that only the unla-beled examples with low prediction confidence will be select ed for regularizing the decision boundary. Moreover, all the unlabeled examples with high pr ediction confidence will be ignored. From the above discussions, we can conclude that  X  determines the regularization strength of unlabeled examples.
 Then, we rewrite the overall optimization problem as below: This is a min-max optimization problem and thus the global op timal solution can be guar-anteed. To obtain the optimal solution, we employ an alterna ting optimization procedure, which iteratively computes the values of f l and  X  . To account for the penalty on the margin error from the unlabeled data, we just need to add an extra con straint of  X  i  X  2 C for i = 1 , . . . , n  X  l .
 By varying the parameter  X  from 0 to 1, we can indeed obtain a series of transductive mode ls for SVM. When  X  is small, we call the corresponding optimization problem as weakly semi-supervised learning. Therefore, it is important to find an ap propriate  X  which adapts for the input data. However, as the data distribution is usually unk nown, it is very challenging to directly estimate an optimal regularization strength para meter  X  . Instead, we try to explore an alternative approach to select an appropriate  X  by combining the prediction functions. Due to the large cost in calculating the inverse of kernel mat rices, one can solve the dual problems according to the Representer theorem. 3.4 Adaptive Regularization As stated in previous sections,  X  determines the regularization strength of the unlabeled data. We now try to adapt the parameter  X  according to the unlabeled data information. where  X  1 = 0 and  X  m = 1. This is equivalent to selecting the optimal f from a list of semi-supervised learning [7], we assume that the optimal f comes from a linear combination of the base functions { f i } . We then have: where  X  i is the weight of the prediction function f i and  X   X  R m . One can also involve a priori to  X  i . For example, if we have more confidences on the semi-supervi sed classifier, we can introduce a constraint like  X  m  X  0 . 5. It is important to note that the learning functions in ensemble methods [7] are usually weak learners , while in our approach, the learning functions are strong learners with different degre es of regularization. In the following, we study how to set the regularization stre ngth adaptive to data. Since TSVM naturally follows the cluster assumption of semi-supe rvised learning, in order to complement the cluster assumption, we adopt another princi ple in semi-supervised learning, i.e., the manifold assumption. From the point of view of mani fold assumption in semi-supervised learning, the prediction function f should be smooth on unlabeled data. To this end, the approach of manifold regularization is widely adopted as a smoothing term in semi-supervised learning literatures, e.g., [1, 10]. In the following, we will employ the manifold regularization principle for selecting the regul arization strength.
 The manifold regularization is mainly based on a graph G = &lt; V , E &gt; derived from the whole nodes. In general, a graph is built in the following four step s: (1) constructing adjacency graph; (2) calculating the weights on edges; (3) computing t he adjacency matrix W ; (4) obtaining the graph Laplacian by L = diag( P n j =1 W ij )  X  W . Then, we denote the manifold regularization term as f &gt; L f .
 For simplicity, we denote the predicted values of function f i on the data X as f i , such that f of all prediction functions. Finally, We have the following minimization problem: the labeled data.  X  is a trade-off parameter. The above optimization problem is a simple quadratic programming problem, which can be solved very effic iently. It is important to note that the above optimization problem is less sensitive to the graph structure than Laplacian SVM as used in [1], since the basic learning functions are all strong learners. It also saves a huge amount of efforts in estimating the parameters compare d with Laplacian SVM. The above approach indeed provides a practical approach tow ards a combination of both the cluster assumption and the manifold assumption. It is em pirically suggested that com-bining these two assumptions helps to improve the predictio n accuracy of semi-supervised learning according to the survey paper on semi-supervised S VM [6]. Moreover, when  X  = 0, supervised models are incorporated in the framework. Thus t he usefulness of unlabeled in naturally considered by the regularization. This therefor e provides a practical solution to the problems described in Section 1. In this section, we give details of our implementation and di scuss the results on several benchmark data sets for our proposed approach. To conduct a c omprehensive evaluation, we employ several well-known datasets as the testbed. As summa rized in Table 1, three image data sets and five text data sets are selected from the recent b ook ( www.kyb.tuebingen. mpg.de/ssl-book/ ) and the literature ( www.cs.uchicago.edu/ ~ vikass/ ).
 Table 1: Datasets used in our experiments. d represents the data dimensionality, and n denotes the total number of examples.
 For simplicity, our proposed adaptive regularization appr oach is denoted as ARTSVM. To evaluate it, we conduct an extensive comparison with seve ral state-of-the-art ap-proaches, including the label-switching-retraining algo rithm in SVM-Light [12], CCCP [8], and  X  TSVM [4]. We employ SVM as the baseline method.
 In our experiments, we repeat all the algorithms 20 times for each dataset. In each run, 10% of the data are randomly selected as the training data and the remaining data are used as the unlabeled data. The value of C in all algorithms are sel ected from [1 , 10 , 100 , 1000] stated in Section 3.4, ARTSVM is less sensitive to the graph s tructure. Thus, we adopt a simple way to construct the graph: for each data, the number o f neighbors is set to 20 and binary weighting is employed. In ARTSVM, the supervised, we akly semi-supervised, and semi-supervised algorithms are based on implementation in LibSVM ( www.csie.ntu.edu. tw/ ~ cjlin/libsvm/ ), MOSEK ( www.mosek.org ), and  X  TSVM ( www.kyb.tuebingen.mpg. de/bs/people/chapelle/lds/ ), respectively. For the comparison algorithms, we adopt th e original authors X  own implementations.
 Table 2 summarizes the classification accuracy and the stand ard deviations of the proposed ARTSVM method and other competing methods. We can draw sever al observations from the results. First of all, we can clearly see that our propose d algorithm performs signif-icantly better than the baseline SVM method across all the da ta sets. Note that some very large deviations in SVM are mainly because the labeled d ata and the unlabeled data may have quite different distributions after the random samp ling. On the other hand, the unlabeled data capture the underlying distribution and hel p to correct such random error. Comparing ARTSVM with other TSVM algorithms, we observe tha t ARTSVM achieves the best performance in most cases. For example, for the digi tal image data sets, espe-cially digit1, supervised learning usually works well and t he advantages of TSVM are very limited. However, the proposed ARTSVM outperforms both the supervised and other semi-supervised algorithms. This indicates that the appropriat e regularization from the unlabel data improves the classification performance.
 Table 2: The classification performance of Transductive SVM s on benchmark data sets. This paper presents a novel framework for semi-supervised l earning from the perspective of the regularization strength from the unlabeled data. In par ticular, for Transductive SVM, we show that SVM and TSVM can be incorporated as special cases within this framework. In more detail, the loss on the unlabeled data can essentiall y be regarded as an additional regularizer for the decision boundary in TSVM. To control th e regularization strength, we introduce an alternative method of data-dependant regular ization based on the principle of manifold regularization. Empirical studies on benchmark d ata sets demonstrate that the proposed framework is more effective than the previous trans ductive algorithms and purely supervised methods.
 For future work, we plan to design a controlling strategy tha t is adaptive to data from the perspective of low density assumption and manifold regu larization of semi-supervised learning. Finally, it is desirable to integrate the low dens ity assumption and manifold regularization into a unified framework.
 The work was supported by the National Science Foundation (I IS-0643494), Na-tional Institute of Health (1R01GM079688-01), Research Gr ants Council of Hong Kong (CUHK4158/08E and CUHK4128/08E), and MSRA (FY09-RES-OPP-103). It is also affili-ated with the MS-CUHK Joint Lab for Human-centric Computing &amp; Interface Technologies.
