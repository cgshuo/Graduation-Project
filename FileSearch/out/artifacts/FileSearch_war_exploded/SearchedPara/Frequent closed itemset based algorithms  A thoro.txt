 As a side effect of the digitalization of unprecedented amount of data, traditional retrieval tools proved to be unable to ex-tract hidden and valuable knowledge. Data Mining, with a clear promise to provide adequate tools and/or techniques to do so, is the discovery of hidden information that can be retrieved from datasets. In this paper, we present a struc-tural and analytical survey of f requent c losed i temset ( FCI ) based algorithms for mining association rules. Indeed, we provide a structural classification, in four categories, and a comparison of these algorithms based on criteria that we introduce. We also present an analytical comparison of FCI -based algorithms using benchmark dense and sparse datasets as well as  X  X orst case X  datasets. Aiming to stand beyond classical performance analysis, we intend to provide a focal point on performance analysis based on memory con-sumption and advantages and/or limitations of optimization strategies, used in the FCI -based algorithms. The survival of the association rule extraction technique is owed to the retrieval of compactly sized with added-value knowledge. In this respect, the last decade witnessed a particular interest in the definition of condensed represen-tations, e.g. , closed itemsets [31], free itemsets [6], non-derivable itemsets [7], essential itemsets [9], etc. The def-inition of these condensed representations mainly relies on the inclusion-exclusion principle and Bonferroni inequali-ties, which were of extensive use in addressing many enu-meration problems [14].
 The study of the extraction of closed itemsets grasped the interest of the data mining community. Indeed, f requent c losed i temset ( FCI ) based algorithms were introduced to mainly tackle two complementary problems. On the one hand, FCI -based algorithms present an effective mining ap-proach for dense extraction contexts. In such contexts, large equivalence classes are obtained. FCI s, standing on the top of the hierarchy induced by each equivalence class, allow to informatively infer the supports of FI s, standing within the f requent m inimal g enerators ( FMG s) and their associated FCI s. Note that the FMG s correspond to the frequent 0-free itemsets [6] and to the frequent key patterns [35]. Unfortunately, typically sparse contexts represent a  X  X ight-mare X  for FCI -based algorithms, which have to bear useless and costly closure computations. The low performances on sparse contexts are quite expected, since the FCI search space tends to overlap with that of FI s.
 On the other hand, FCI -based algorithms, which heavily draw on Formal Concept Analysis (FCA) mathematical set-tings [40], present a novel alternative with a clear promise to dramatically reduce, without information loss, the size of the association rule set that can be drawn from both syn-thetic and real-life datasets. The result of such a reduction is a reasonably-sized subset of association rules that can be seen as an irreducible nucleus of association rules, commonly known as  X  X eneric basis X  of association rules [30]. In this paper, we present a structural and analytical com-parative study of FCI -based algorithms. Hence, we clas-sify FCI -based algorithms into four disjoint categories. We then introduce some features (or dimensions) allowing to highlight the major differences between the most prominent FCI -based algorithms for mining association rules (current and future). Performances of these algorithms are assessed and compared on benchmark dense and space datasets as well as  X  X orst case X  datasets [18]. Interestingly enough, the proposed analytical comparison goes beyond those respec-tively proposed by Zheng et al. [43; 44], in which only sparse datasets were of interest, and Goethals and Zaki [16], where only performance curves are showed. Indeed, we try not only to show performance curves, but also to explain these performances based on advantages and/or limitations of op-timization strategies used in these algorithms. To obtain an in-depth insight, we also assess the memory consumption of the surveyed algorithms in conjunction with the evolution of gathered information, in main memory, during the mining process.
 It is important to mention that in this survey, we put the fo-cus on FCI -based algorithms especially designed for datasets having much more transactions than items. Nevertheless, some recent algorithms are dedicated to other kind of datasets like the COFI-Closed algorithm [11] for mining extremely large datasets (having millions of transactions) and the Car-penter [28] and Farmer [10] algorithms for mining genomic datasets (having much more items than transactions). We think that this survey can be regarded as complemen-tary to some existing ones. For example, for FI -based algo-rithms, we can cite the survey of Hipp et al. [20], that of Aggarwal [1] for maximal FI -based algorithms and that of Kuznetsov and Ob X  X dkov [22] for concept lattice algorithms. As the FCI representation is concise w.r.t. that of FI s, this survey is also closely related to that of Calders et al. [8] in which the authors give a general overview about different concise representations (like closed itemsets, free itemsets, non-derivable itemsets, etc).
 The remainder of the paper is organized as follows: sec-tion 2 presents basic definitions of the FCA mathematical settings. Section 3 sketches a critical classification and a comparison of FCI -based algorithms thanks to criteria that we introduce. Section 4 reports an analytical comparison of FCI -based algorithms on benchmark and  X  X orst case X  datasets. Section 5 concludes this paper and points out fu-ture perspectives. In this section, we present basic definitions that will be of use in the remainder.

Definition 1. (Formal context) A formal context (or an extraction context) is a triplet K = ( O , I , R ), where O represents a finite set of objects (or transactions), I is a finite set of items (or attributes) and R is a binary (incidence) relation ( i.e. , R  X  O  X  I ). Each couple ( o,i )  X  R expresses that the object o  X  O contains the item i  X  I .
 The closure operator  X  induces an equivalence relation on the power set of items portioning it into disjoint subsets called equivalence classes . The largest element ( w.r.t. the number of items) in each equivalence class is called a closed itemset and is defined as follows:
Definition 2. (Closed Itemset) An itemset I  X  I is said to be closed if and only if  X  ( I ) = I [31]. The support of I , denoted by Supp( I ), is equal to the number of objects in K that contain I . I is said to be frequent if Supp( I ) is greater than or equal to a user-specified minimum support threshold, denoted minsup . The frequency of I in K is equal
Definition 3. (Iceberg Galois lattice) Let FCI K be the set of the FCI s extracted from an extraction context K . When the set FCI K is partially ordered with set inclusion, the resulting structure (  X  L ,  X  ) only preserves the Join oper-ator [15]. This structure is called a join semi-lattice or an upper semi-lattice and is, hereafter, referred to as  X  Iceberg Galois lattice  X  [27; 35].

Definition 4. (Upper cover) The upper cover of an FCI f (denoted Cov u ( f )) consists of the FCI s that immediately cover f in the Iceberg Galois lattice. The set Cov u ( f ) is given as follows: Cov u ( f ) = { f 1 | f 1  X  FCI K and f  X  f and @ f 2  X  FCI K s.t. f  X  f 2  X  f 1 } .

Definition 5. (Minimal Generator) An itemset g  X  I is said to be a minimal generator of a closed itemset f , if and only if  X  ( g ) = f and @ g 1  X  g s.t.  X  ( g 1 ) = f [3]. Thus, the set MG f of the minimal generators associated to a closed itemset f is: MG f = { g  X  I |  X  ( g ) = f and @ g 1  X  g s.t.  X  ( g 1 ) = f } .
 Therefore, the problem of mining association rules might be reformulated, under the point of view of the FCI -based algorithms, as follows: 1. Discover both distinct  X  X losure systems X , i.e. , sets of 2. From the information discovered in the first step, i.e. , In the next section, we present a structural survey of FCI -based algorithms. In this section, we start by introducing a classification of these algorithms after which we give their main characteris-tics. In general, the criterion used to classify FCI -based algo-rithms is inherited from the FI mining stuff, i.e. , the tech-nique used to traverse the search space. Nevertheless, we add a supplementary category that we call  X  X ybrid without duplication X  and this for different reasons explained here-after. Hence, FCI -based algorithms can be roughly split into four categories, namely  X  X est-and-generate X ,  X  X ivide-and-conquer X ,  X  X ybrid X  and  X  X ybrid without duplication X . 1. First category :  X  X est-and-generate X  technique : The 2. Second category :  X  X ivide-and-conquer X  technique : 3. Third category :  X  X ybrid X  technique : Algorithms 4. Fourth category :  X  X ybrid without duplication X  Based on the reformulation of the mining problem from the point of view of the FCI -based approach and its promises to losslessly reduce in size the association rule set, we present in what follows some features (or dimensions) allowing: on the one hand, to assess the percentage of achievement of such promised goals and on the other hand, to highlight the major differences between the FCI -based algorithms for mining association rules (current and future). 1. Exploration technique : three techniques are used 2. Architecture : the architecture dimension depends on
Called tidlists in [25; 26] and denotations in [36; 37].
In the case of the first category, an FCI can be computed more than once. 3. Parallelism strategy : parallel algorithms can be fur-4. Data source type : this feature indicates the type of 5. Information storage : different data structures are 6. Generator choice : some algorithms chose FMG s as 7. Closure computation : the closure of an itemset X 8. Possibility of redundant closure computation : 9. Output type : here we focus on the type of the de-10. Generated output : this feature indicates the knowl- X  ( X ) indicates the tidset of the itemset X .
 Table 1 summarizes a categorization of the FCI -based al-gorithms w.r.t. the previously described basic dimensions. The last but one entry in Table 1 sheds light on the fact that, unfortunately, none of the reported algorithms is able to fulfill the problem reformulation goals, mentioned in the previous section. The origin of this failure is due to the fact of neglecting the importance  X  especially from a data min-ing frenzy towards performances  X  of maintaining the order covering the relationship between the FCI s. This is quite expected since the aforementioned algorithms avoid bear-ing a costly precedence-order construction fee. Hence, no more than the generic basis of exact association rules can be straightforwardly derived. To be able to extract approx-imate generic association rules, they need to be associated with another appropriate algorithm allowing to build the Iceberg Galois lattice, e.g. , that proposed by Valtchev et al. [38].
 By comparing the main FCI -based algorithms, we can also note the following remarks: It is important to note that, for all considered FCI -based al-gorithms, a suitable buffer management scheme is missing. Indeed, this scheme is required to handle this problem when-ever necessary since generating a huge number of candidate sets might cause the memory buffer to overflow. To handle this issue, the candidate generation step should be modified in order to consider that a portion of the required (gener-ated) information may be disk resident (saved on disk). In this case, we may need to perform an external sort as done in [34]. In this section, we give an analytical survey of the FCI -based algorithms considered in the previous section. Ex-periments were carried out on a Pentium IV with a CPU clock rate of 2.4 GHz and 512 MB of main memory (with 2 GB of swap space). To rate the different behaviors of the considered algorithms, we ran experiments on both bench-mark and  X  X orst case X  datasets, whose characteristics are detailed in what follows. The Close , A-Close , Titanic , ChARM (with  X -h -e 1 -d -H 1 X  options), FP-Close , DCI-Closed and LCM ver. 2 algorithms were tested on the Linux distribution S.u.s.e 9.0, and their original makefile versions were compiled using gcc 3.3.1. Since Closet+ (4) was provided as a Windows executable, it was compared under Windows XP Professional on the same experimental environment. The source codes of the Close , A-Close , Ti-tanic , FP-Close , ChARM , DCI-Closed and LCM algo-rithms and the Closet+ Windows binary executable were kindly provided by their respective authors.
Since in [39], Closet+ performances already proved to be definitely better than those of Closet , we did not use Closet in the tests.
 To report our results, we use runtime , i.e. , the period be-tween input and output, instead of using the CPU time mea-sured in some literature ( e.g. , [17; 37]). The memory con-sumption is measured using the GNU glibc tool memusage , considering only the maximum heap size since stack use is much smaller than heap size. To make the measurements more reliable, no other application was running on the ma-chine while experiments were running and none of the dif-ferent executables displays the list of the discovered FCI s on the screen, nor writes it down on a disk-resident file. Algorithms were tested on two types of datasets: benchmark and  X  X orst case X . A  X  X orst case X  context is introduced thanks to the following definition:
Definition 6. [18] A  X  X orst case X  context is a triplet K = ( O , I , R ) where I is a finite set of items of size n , O repre-sents a finite set of objects of size ( n +1) and R is a binary (incidence) relation ( i.e. , R  X  O X I ). In such context, each item belongs to n distinct objects. Each object, among the first n ones, contains ( n -1) distinct items while the last one is verified by all items.
 Thus, in a  X  X orst case X  context, each CI is equal to its unique MG . Hence, from a  X  X orst case X  context of dimen-sion equal to n  X  ( n +1), 2 n FCI s can be extracted when the absolute minsup value is set to 1 . Even if the worst case is rarely encountered in practice,  X  X orst case X  datasets al-low to scrutinize the behavior of an algorithm on extremely sparse ones and hence to assess its scalability. Indeed, when n is increased by 1 , the number of FCI s grows up by a fac-tor exactly equal to 2 . Hence, for a given algorithm, these datasets allow to check how it performs with the evolution of the FCI number. To the best of our knowledge, none of the previous algorithm performance surveys has considered such an extreme case. Figure 1 (Top on the left) presents an example of a  X  X orst case X  dataset for n = 4 . Figure 1 (Top on the right) presents parameter settings of synthetic datasets. Figure 1 (Down) summarizes the characteristics of benchmark dense and sparse datasets (5) and those of a  X  X orst case X  dataset. For more clarity, we have split the corresponding figures into two pools: the first one depicts reported statistics for the representatives of the first category, namely the Close , A-Close and Titanic algorithms. The second focuses on com-paring performances of the representatives of the remaining categories, i.e. , the Closet+ , FP-Close , ChARM , DCI-Closed and LCM algorithms. This splitting was motivated by the large gap in the inter-pool execution times. In fact, we noticed that the first pool algorithm performances can not compete with those of the second pool and gathering them in the same curve will be misleading. The second pool will also be divided into two sub-groups w.r.t. the technique used to traverse the search space, i.e. ,  X  X ivide-and-conquer X  or  X  X ybrid X  technique. Hence, we find the Closet+ and FP-Close algorithms in the first group, while the ChARM , DCI-Closed and LCM algorithms are in the second group. For each dataset, we begin by comparing performances of
All benchmark datasets are downloadable from: http://fimi.cs.helsinki.fi/data . Figure 1: ( Top on the left ) A  X  X orst case X  dataset for n = 4 . ( Top on the right ) Synthetic dataset settings. ( Down ) Dataset characteristics. algorithms belonging to the same group (an  X  X ntra-group X  comparison). Then, we compare the five algorithms perfor-mances (an  X  X nter-group X  comparison) by giving the best algorithm(s) by minsup interval. First pool: In the first pool, Close presents better perfor-mances than A-Close for all datasets. This was expected since Close is known to perform better than A-Close for dense datasets [29].
 For the Mushroom dataset, Titanic outperforms Close for minsup values ranging between 20% and 2% . However, the contrary happens for minsup values lower than 2% , where Close largely outperforms Titanic . Indeed, to com-pute the FCI s, Titanic tries to extend any FMG with the appropriate items by carrying out costly support computa-tions, especially for low minsup values. Indeed, for minsup = 0.1% , 116 items  X  among 119  X  are frequent and the maximum size of an FMG is only equal to 10 items. Note that for this dataset, Close , A-Close and Titanic suf-fer from redundant computation of the FCI s. Indeed, for a minsup value equal to 0.1% , the number of FMG s, equal to 360, 166 , is almost 2.2 times the number of FCI s, equal to 164, 117 (as shown by Figure 5).
 For the Chess dataset, the counting inference [4] adopted by Titanic is more efficient than that based on intersection computations adopted by the Close algorithm. Indeed, Ti-tanic outperforms Close for the different tested minsup values. However, for the same reasons as in the case of the Mushroom dataset, this efficiency tends to decrease propor-tionally with the decrease of minsup values.
 For the Connect dataset, Titanic largely outperforms Close for all minsup values. In fact, the task of Close is consid-erably complicated by performing intersections on a large number of highly sized transactions. However, due to lack of memory capacity, the execution of Titanic did not come to an end starting from a minsup value equal to 50% . It is important to mention that, even if this dataset is dense, each FCI has a unique FMG (as shown by Figure 5 in which both associated curves are overlapping).
 Second pool: For the Mushroom dataset, performances are as follows. For the first group and for all minsup values, FP-Close outperforms Closet+ and the gap tends to increase for lower minsup values. As Mushroom is a dense dataset, both Closet+ and FP-Close are obliged to recursively build the physically projected FP-trees. The difference be-tween the performances of both algorithms comes from the fact that FP-Close stores the previously mined FCI s in recursively constructed CFI-trees (C losed F requent I temset trees), while Closet+ keeps track of all mined FCI s in a global prefix-tree. Consequently, in the case of FP-Close , subsumption checking cost is by far less expensive than that for Closet+ , and the difference gets sharper when minsup values decrease. In addition, FP-Close stores only a part of each FCI in a CFI-tree [45] which even more reduces the cost of subsumption checking. In the second group and for min-sup values greater than or equal to 0.3% , DCI-Closed and LCM performances are very similar, with a slight advantage for the latter. Recall that as mentioned in section 3.1, both algorithms belong to the fourth category and share their main characteristics. For the same minsup values, they out-perform ChARM , the representative of the third category. This fact can be explained by, on the one hand, the effi-ciency of duplicate detection strategies performed by DCI-Closed and LCM , and the several optimization techniques used in both algorithms, on the other hand. For minsup values lower than 0.3% , LCM still outperform ChARM . However, surprisingly enough, for a minsup value equal to 0.2% , the execution of DCI-Closed stops for lack of mem-ory capacity after more than one hour and a half. The main reason is that DCI-Closed uses a heuristic allowing to as-sess whether the dataset is dense or sparse. Using this infor-mation  X  the nature of the dataset  X  DCI-Closed launches a slightly modified version of the level-wise sweeping k DCI algorithm (6) [24] in the case of a sparse dataset. Hence, strangely in the case of the dense Mushroom dataset and for a minsup value equal to 0.2% , k DCI was executed in-stead of the procedure normally to be run in the case of dense datasets. This explanation is argued by the subset of FCI s extracted before that the execution stops. Indeed, using DCI-Closed with the option allowing to write the output on a disk-resident file, we found that the extracted FCI s are written in an increasing order of their respective sizes ( i.e. , by level). Hence, in this particular case, DCI-Closed browses the search space in a breadth-first manner (using k DCI ) instead of a depth-first manner, which leads to runtime degradation and to memory saturation. By com-paring performances of the five algorithms, FP-Close is the best for minsup values greater than or equal to 3% . DCI-Closed takes the best for minsup values greater than or equal to 0.3% . For minsup values less than 0.3% , LCM is the best.
 For the Chess dataset, on the one hand, FP-Close largely outperforms Closet+ . For the same reasons as for the Mushroom dataset, the difference between their respective performances sharply increases as much as we lower minsup values. For example, FP-Close performs almost 8.5 (resp. 27 ) times better than Closet+ for a minsup value equal k DCI is a breadth-first algorithm that mines FI s. An ad-ditional test is thus added to check which FI is also closed. to 40% (resp. 30% ). On the other hand, ChARM out-performs both DCI-Closed and LCM for minsup values greater than or equal to 90% . For minsup values between 90% and 70% , LCM is the best and for minsup values lower than 70% , DCI-Closed is the best. By comparing the per-formances of both groups, ChARM , LCM and DCI-Closed are the best, respectively, for the previously mentioned in-tervals. It is worth noting that we stopped the execution of ChARM for a minsup value equal to 20% after more than two days and a half. This confirms the prohibitive cost of tidset intersections and subsumption checking, for very low minsup values.
 For the Connect dataset and for the first group, the sce-nario of both previous datasets was repeated. Indeed, for all minsup values, FP-Close outperforms Closet+ . However, both algorithm performances are similar for minsup values greater than or equal to 30% and the advantage is clearly taken by FP-Close for supports lower than 30% (almost 2.5 times for a minsup value equal to 10% ). For the second group, ChARM is largely outperformed by both algorithms of the fourth category, i.e. , DCI-Closed and LCM . For ex-ample, DCI-Closed performs almost 30 times better than ChARM for minsup values equal to 10% . LCM performs better than DCI-Closed for minsup values greater than or equal to 40% , even though the performance gap is very tight. For minsup values lower than 40% , DCI-Closed takes the lead on LCM and performs almost twice better than the latter for a minsup value equal to 10% . By com-paring both groups X  performances, FP-Close is the best for a minsup value equal to 60% . For the other tested min-sup values greater than or equal to 40% , LCM is the best, whereas DCI-Closed takes the top on the four other algo-rithms for minsup values lower than 40% . First pool: For the T10I4D100K dataset, and for a min-sup value equal to 0.5% , Titanic outperforms both Close Figure 5: The size of the FCI set, of the FMG set and of the negative border of MG s (denoted Negative border) for, respectively, the Mushroom and Connect datasets. and A-Close . For the same minsup value, A-Close out-performs Close . We can explain this by the fact that, for this minsup value, each FMG is equal to its closure ( i.e. , the variable Level , used in A-Close algorithm to check this case, is equal to 0 (7) ) and then A-Close does not perform any closure computation. However, A-Close is handicapped by the traversal of the ( k -1)-FMG set for each k -candidate g . This traversal allows it to compare the support of g with those of its ( k -1)-subsets to check if g is an FMG or not. As outlined earlier, Titanic avoids this overhead by simply comparing the support of g to its estimated support (equal to the minimum of the supports of its ( k -1)-subsets). For minsup values lower than 0.5% , Close outperforms both A-Close and Titanic . For a minsup value equal to 0.2% , Level is equal to 3 . Thus, A-Close has to compute the clo-sure of the FMG s of size greater than or equal to 2 . For a minsup value lower than 0.2% , A-Close computes the closure of all FMG s since Level is equal to 2 . Titanic per-formances decrease in a significant way for the same reason evoked earlier. Indeed, for minsup = 0.02% , 859 items, among 1, 000 , are frequent and the maximum size of an FMG is only equal to 10 items. In addition, Titanic needs to maintain, in main memory, a highly sized negative border of MG s [21] (as shown by Figure 9).
 For the T40I10D100K dataset, and for minsup values ranging between 10% and 1.5% , Titanic outperforms both Close and A-Close . For this interval of minsup , Level is equal to 0 and the same scenario as that of the T10I4D100K dataset when minsup value is equal to 0.5% seems to be repeated. For a minsup value lower than 1.5% , Level is equal to 4 . However, Close performs better than A-Close most of times. This can be explained by the fact that the size of
Level is initialized with 0 and takes as value the size k of the smallest FMG candidate having the same support as one of its ( k -1)-subsets. the longest FMG is equal to 18 and hence A-Close has to compute the closures of FMG s of sizes between 3 and 18 . The decrease of Titanic performances, when lowering minsup values, can be explained by what follows: when com-puting the closure of an FMG and aiming to extend it by the appropriate items, Titanic has to explore a particularly very large search space since the negative border of MG s is maintained (as shown by Figure 9).
 For the Retail dataset and for minsup values greater than 0.08% , Close and A-Close performances are similar with a slight advantage for A-Close . For minsup values greater than 0.06% , Close performances considerably degrade. The decrease of its performances can be explained by the enor-mous influence of the high number of items in the Retail dataset. Indeed, Close is handicapped by a very high num-ber of candidates to which it is obliged to compute respective closures, even though a large number of them is infrequent. The number of candidates also affects the performances of A-Close which, in addition to the closure computation cost, has to traverse the ( k -1)-FMG set for each k -candidate. Note that these infrequent candidates belong to the neg-ative border of MG s stored by Titanic what explains why its search space is very large. In addition, Titanic is con-siderably penalized by the high number of frequent items to consider in closure computations. Indeed, for minsup = 0.04% , 4, 643 items are frequent and the maximum size of an FMG is only equal to 6 items. For support values lower than 0.04% , Titanic executions stop for lack of memory capacity.
 Second pool: For the tested datasets, DCI-Closed uses k DCI algorithm for all minsup values. This fact was con-firmed by checking DCI-Closed output written on a disk resident file using a specific option.
 For the T10I4D100K dataset, and for the first group, the rep-resentatives of the second category of FCI -based algorithms, i.e. , Closet+ and FP-Close , have very similar perfor-mances. For minsup values greater than or equal to 0.03% , FP-Close slightly outperforms Closet+ . However, it is the opposite for minsup values lower than 0.03% . As the T10I4D100K dataset is a sparse one, Closet+ uses the top-down pseudo tree-projection method. Hence, it avoids the recursive construction of FP-trees and has only to traverse the global FP-tree. On its side, FP-Close profits from using an array-based technique to avoid traversing previ-ously built FP-trees to construct the respective header ta-bles of the new entries. For the second group, it was ex-pected that DCI-Closed outperforms ChARM and LCM , since it uses the k DCI algorithm. Interestingly enough, ex-periments showed the opposite. Indeed, for minsup values greater than or equal to 0.08% , ChARM and DCI-Closed have very similar performances with a slight advantage to ChARM and both outperform LCM with a peak of almost 3 times for a minsup value equal to 0.5% . However, the ten-dency changes for support values lower than 0.08% since LCM takes the top on ChARM and DCI-Closed with a peak of almost 3 (resp. 8 ) times better than ChARM (resp. DCI-Closed ) for a minsup value equal to 0.02% . By comparing the five algorithms, ChARM (resp. LCM ) is the best for high (resp. low) minsup values. It is impor-tant to mention that for a minsup value equal to 0.02% , LCM algorithm outputs 107, 825 FCI s. However, this number does not match with that obtained with the other surveyed algorithms ( 107, 822 FCI s for Close , A-Close , ChARM , Closet+ and FP-Close , and 107, 823 for Ti-tanic and DCI-Closed since both consider the equivalence class whose the FMG is the empty set). Hence, this fact points out the need for the proof of the algorithm correct-ness as formerly claimed by Zheng et al. [43], especially when many optimizations are used. However, it is worth noting that, in their performance comparison, Zheng et al. omit-ted to use the  X -H 1 X  option allowing ChARM to perform subsumption checking to discard non FCI s. This explains why they found, for a minsup value equal to 0.01% , 303, 610 instead of 283, 397 FCI s. The difference of 20, 213 represents non FCI s, not discarded by ChARM since  X -H 1 X  option was not used.
 For the T40I10D100K dataset and for the first group, FP-Close largely outperforms Closet+ , for all minsup values. The difference reaches a peak of almost 10 times for a min-sup value equal to 0.5% . Indeed, subsumption checking highly affects Closet+ performance as it stores all FCI s in the same tree (the number of these itemsets reaches 1, 275, 940 for minsup = 0.5% ). For the second group, ChARM is the best for minsup values greater than or equal to 5% . DCI-Closed takes the top for minsup values between 5% and 1.5% and LCM is the best for minsup values lower than 1.5% . In this pool, ChARM and DCI-Closed are the best for the respective intervals previously mentioned, whereas FP-Close slightly outperforms LCM for minsup values lower than 1.5% .
 For the Retail dataset and for the first group, the oppo-site of the scenario of the T40I10D100K dataset happens. Indeed, Closet+ largely outperforms FP-Close since the former performs between 1.5 and 7.5 times better than the latter. This is due to the high number of frequent items extracted for very low minsup values. For example, for a minsup value equal to 0.01% , 9, 300 items are frequent. Such number highly increases the cost of the array based technique adopted by FP-Close , whereas Closet+ , ben-efiting from the use of the top-down pseudo tree-projection method, needs only to traverse the global FP-tree. For the second group, LCM largely outperforms ChARM and DCI-Closed and the gap tends to sharply increase proportionally to the decrease of minsup values. For example, LCM per-forms almost 2 (resp. 1.4 ) times better than ChARM (resp. DCI-Closed ) for a minsup value equal to 0.1% , whereas for a minsup value equal to 0.01% , the difference reaches almost 6 (resp. 10 ). In this pool, LCM is the best for all minsup values. These experiments show that, unfortunately, there is no out-standing algorithm to be qualified as the best for all datasets or at least for a given dataset type, i.e. , dense or sparse. Moreover, in general, for a given dataset, there is no best algorithm for all minsup values. Indeed, algorithm perfor-mances closely rely on minsup values. A change in the min-sup value can lead to different information to be treated by the algorithm and so, an optimization or a heuristic that performs better for a given minsup value can slow down performances due to this change. Thus, the use of a mul-titude of optimizations is not sufficient. Indeed, the most important is to be able to precisely decide when to apply the most appropriate optimization according to the handled input data. On average, LCM performances are not badly affected by the minsup value changes. Figure 9: The size of the FCI set, of the FMG set and of the negative border of MG s (denoted Negative border) for, respectively, the T10I4D100K and T40I10D100K datasets. First pool: We tested 25 datasets showing the variation of n from 1 to 25 . The absolute minsup value was fixed to 1 . Execution times of the three algorithms began to be dis-tinguishable only starting from the value of n equal to 15 . In a  X  X orst case X  dataset, each FMG is equal to its closure and hence, A-Close does not perform any closure compu-tation thanks to the use of Level . A-Close and Titanic largely outperform Close for the different values of n . The difference between their respective performances began to be clear for a value of n greater that 21 . Note that Titanic performs a traversal of the ( k -1)-FMG set for each k -FMG g . By this sweeping, Titanic tries to reduce the cost of the computation of g  X  closure by collecting items belonging to the respective closures of its ( k -1)-subsets. In addition, Titanic performs an incremental search to try to find other items belonging to the closure of g . Hence, the higher is the value of n , the worse is the influence of such treatments on Titanic performances. For lack of memory space, execu-tions of Close and Titanic stop for a value of n equal to 24 . It is the same for A-Close when n reaches 25 . Second pool: We tested 30 datasets showing the variation of n from 1 to 30 . The absolute minsup value was fixed to 1 . Execution times of the five algorithms began to be distinguishable only starting from the value of n equal to 15 . DCI-Closed and LCM performances are largely bet-ter than those of Closet+ , FP-Close and ChARM , with a clear advantage for DCI-Closed for the different values of n . For the tested datasets, DCI-Closed executed, against any waiting, the procedure used in the case of dense datasets and not the k DCI algorithm. We expected the latter to be run since  X  X orst case X  contexts are the most evident rep-resentation of sparse ones. Due to subsumption checking performed in a large set of FCI s, ChARM performances are not interesting even for low values of n . The execu-tion of FP-Close stops for a value of n equal to 22 with  X  The blocks are used up  X  error message. For n = 21 , we stopped the execution of Closet+ after more than four hours. The respective curves of DCI-Closed and LCM have almost the same slope. They grow linearly as much as the value of n increases. This fact confirms that, in the con-trary to algorithms belonging to the first three categories, the runtime of the fourth category X  X  algorithms is a linear function of the size of the FCI set. Nevertheless, the fact that DCI-Closed presents, in general, very interesting per-formances must not hide the difficulties encountered by this algorithm to correctly assess the right type (dense or sparse) of a given dataset. Indeed, more than once, DCI-Closed uses k DCI instead of the expected procedure to be run for dense dataset (case of the Mushroom dataset) and inversely (case of the  X  X orst case X  datasets).
 It is important to mention that the treatments performed by all algorithms except A-Close , to compute closures, are useless since each frequent (minimal) generator is equal to its closure. Since memory consumption also demonstrates the quality of an algorithm (and in general, of a category of algorithms), we recorded the main memory consumption peak displayed out for different datasets. The tested algorithms are still divided into two pools.
 First pool: Figure 11 plots the peak of the main mem-ory consumption of the Close , A-Close and Titanic al-gorithms when running them on the Mushroom and T10I4-D100K datasets. For both datasets, Titanic uses the max-imum amount of main memory. This is a straightforward consequence of the counting inference adopted by Titanic . Indeed, to correctly apply this mechanism, Titanic needs to maintain in main memory the set of the FMG s as well as the negative border of MG s. This border is in general of high size (as shown by Figures 5 and 9). By comparing the memory consumption of Close and A-Close , Close uses the lowest amount of main memory for the dense dataset and A-Close for the sparse one. It is worth noting that using an off-line closure computation, A-Close requires saving, in main memory, each mined FMG until obtaining the whole FMG set. Hence, this manner of closure computation is also greedy in memory space. Close , in the contrary of A-Close and Titanic , does not need to maintain the set of the FMG s in main memory. However, it presents an-other drawback. Indeed, Close computes the closure of the FMG candidates while counting their respective supports. Hence, Close needs to maintain, in main memory, several CI s whose support values are probably lower than the min-sup value. The number of such CI s is much higher for sparse datasets. Figure 11: The memory consumption for, respectively, the Mushroom and T10I4D100K datasets.
 Second pool: Figure 12 plots the peak of the main memory consumption of the FP-Close , ChARM , DCI-Closed and LCM algorithms when running them, respectively, on the Mushroom , Connect , T10I4D100K and T40I10D100K datasets. Obtained results on both dense datasets confirm the claim of the authors of DCI-Closed and LCM about the memory consumption of their respective algorithms. Indeed, with an advantage for DCI-Closed , the amount of memory used by both algorithms is nearly constant since they do not need to maintain the previously mined FCI s in main memory. Conversely, the amounts of main memory, respectively used by FP-Close and ChARM , rapidly grow up as much as the minsup value decreases. Indeed, this increase is due to the huge number of previously extracted FCI s that must be maintained in main memory. For example, considering the Connect dataset when the minsup value decreases from 20% to 10% , the cardinal of the FCI set grows up from a value equal to 1, 483, 200 to a value equal to 8, 073, 778 . While, the memory consumption of DCI-Closed (resp. LCM ) is only multiplied by a factor equal to 1.16 (resp. 1.34 ), that of ChARM (resp. FP-Close ) is multiplied by a factor equal to 5.91 (resp. 4.91 ). It is worth noting that even if FP-Close stores only a part of each FCI in the CFI-trees, the storage of multiple CFI-trees still needs a lot of main mem-ory.
 In the case of both sparse datasets T10I4D100K and T40I10-D100K , DCI-Closed uses the k DCI algorithm. Hence, it is expected that DCI-Closed memory requirement will not be constant, in contrary to the case of previously examined dense datasets. Indeed, k DCI needs to maintain, in main memory, the set of the k -FI s and that of the ( k -1)-FI s. Nevertheless, for the T10I4D100K dataset, the memory re-quirement of DCI-Closed can still be considered as con-stant, as for dense datasets. Indeed, the maximum multi-plicative factor is equal to 1.33 . This can be explained by the fact that the peak of DCI-Closed memory consump-tion always occurs when both the set of the 3 -FI s and the set of the 2 -FI s are maintained in main memory. The size of these sets increases when minsup values decrease, but not exponentially. Hence, DCI-Closed memory requirement appears as if it were constant. Another surprising result, about the T10I4D100K dataset, is that ChARM uses the lowest amount of main memory. As for dense datasets, the amount of memory used by LCM is nearly constant and is slightly lower than that used by DCI-Closed . FP-Close uses the highest amount of main memory since it needs to maintain recursively built FP-trees, known to be large and bushy for sparse datasets, as well as multiple CFI-trees. For the T40I10D100K dataset, the memory requirement of DCI-Closed is not constant but this algorithm has the best space-efficiency for low minsup values. ChARM uses the lowest amount of main memory for high minsup values. However, for the T40I10D100K dataset and for low minsup values, the number of FCI s considerably increases (equal to 1, 275, 940 for minsup = 0.5% ). Hence, since ChARM needs to maintain such number of FCI s in main memory, its memory requirement rapidly grows up when the support threshold decreases. This explains why DCI-Closed , and even if it browses the search space in a level-wise manner, uses less main memory for low minsup values. As for the T10I4D100K dataset, FP-Close uses the highest amount of main memory. By comparing the memory requirements of the representative of the third (resp. fourth) category, i.e. , ChARM (resp. LCM ) for both minsup values 1.5% and 0.5% , we find that the amount used by ChARM (resp. LCM ) grows up by a factor equal to 83.07 (resp. 1.03 ). The main reason is that the size of the FCI set, to be main-tained in main memory by ChARM , grows up from a value equal to 6, 540 , for minsup = 1.5 , to a value equal to 1, 275, 940 for minsup = 0.5% . As the fourth category avoids this storage, its memory requirement is nearly constant and it is far from indicating that the size of the FCI set grows up exponentially. In this paper, we presented a structural and analytical com-parative study of FCI -based algorithms, towards a theoreti-cal and empirical guidance to choose the most adequate min-ing algorithm. The main report of this comparative study is to shed light on an  X  X bsessional X  algorithmic effort to reduce Figure 12: The memory consumption for, respectively, the Mushroom , Connect , T10I4D100K and T40I10D100K datasets. the computation time of the interesting itemset extraction step.
 The obtained success is primarily due to an important pro-gramming effort combined with strategies for compacting data structures in the main memory. However, it seems ob-vious that this frenzied activity loses sight of the essential objective of this step, i.e. , to extract a reliable knowledge, of exploitable size for end-users. Thus, almost all these algo-rithms were focused on enumerating CI s, presenting a fre-quency of appearance considered to be satisfactory. The dark side of this success is that this enumeration will gener-ate an impressive and not exploitable number of association rules, even for a reasonable context size. Indeed, apart from modest performances on sparse contexts, these algorithms concentrated on extracting the FCI s while neglecting to de-termine the underlying subset-superset relationship. We can notice, as highlighted by the second entry in Table 1, the absence of attempts to parallelize FCI -based algorithms. Indeed, most parallel algorithms for mining association rules are based on the sequential Apriori algorithm (8) . We also believe that it will be a very promising approach to combine both paradigms: task parallelism and data parallelism. In this respect, both preliminary works [5; 13] on distributed or parallel computation of the FCI s are worth noting. Finding a powerful and costless metric for the assessment of an extraction context density is another important com-pelling issue to be tackled. In fact, FCI -based algorithm performances  X  behaving badly on sparse datasets on the contrary of FI -based algorithms  X  motivate setting up a  X  X ensity depending X  approach. This latter seeks to apply the appropriate extraction approach depending on the ex-traction context density. For example, based on the  X  X ivide-and-conquer X  technique, such a novel approach recursively divides the extraction context into sub-contexts. The appro-priate frequent (closed) itemset extraction process is applied to each sub-context, depending on its accurate density as-
An excellent survey is given in [41] classifying algorithms by load-balancing strategy, architecture and parallelism. sessment.
 An in-depth study of dataset characteristics is bashfully be-ginning to grasp the community interest. In fact, these characteristics have an important impact on algorithm per-formances. A finer structural characterization of handled datasets  X  beyond the number of transactions/items, the average transaction size or the density  X  permits a better understanding of the behavior of algorithms. In this re-spect, two preliminary works are worth mentioning. The first studies the distribution of the negative border and its complement, the positive one [12], while the second work proposes some results about the average number of frequent (closed) itemsets, using probabilistic techniques ( e.g. , the Bernoulli law) [23].
 Other avenues for future work address the challenging issues of mining richly structured and genomic contexts. For ex-ample, in genomic contexts, mined knowledge is closely de-pendent of the discretization method, i.e. , how the original context is translated to a binary one. Usually, a gene is con-sidered to be present in a biological situation if its expression level exceeds (or not) a given threshold ( e.g , its expression level average in the different biological situations). Clearly, such translation is far from being information lossless. Ac-tually, the main drawback is that the discretization is not able to describe the  X  X ctual X  biological situation. For this reason, it is of paramount importance to handle an extended extraction context, i.e. , without binarizing the original con-text. In this respect, introducing soft computing techniques seems to be a promising issue.
 Finally, well adapted visualization models are badly missing. In fact, useful relationships between nonintuitive variables are the jewels that data mining techniques hope to locate. However, the unmanageably large association rule sets, com-pounded with their low precision, often makes the perusal of knowledge ineffective, their exploitation time-consuming and frustrating for the user. This fact is reinforced since the user does not know beforehand what the data mining process will discover. It is a much bigger leap to take the output of the system and to translate it into a natural rep-resentation matching with what the end-user has in mind. We are grateful to the anonymous reviewers for their help-ful comments. We also thank C. Dubois, J.J. Givry and C. Zimny for English proofreading. This work is partially sup-ported by the French-Tunisian project CMCU 05G1412.
