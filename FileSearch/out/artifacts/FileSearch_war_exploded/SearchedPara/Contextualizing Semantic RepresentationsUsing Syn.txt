 In the logical paradigm of natural-language seman-tics originating from Montague (1973), semantic structure, composition and entailment have been modelled to an impressive degree of detail and formal consistency. These approaches, however, lack coverage and robustness, and their impact on realistic natural-language applications is lim-ited: The logical framework suffers from over-specificity, and is inappropriate to model the per-vasive vagueness, ambivalence, and uncertainty of natural-language semantics. Also, the hand-crafting of resources covering the huge amounts of content which are required for deep semantic processing is highly inefficient and expensive.
Co-occurrence-based semantic vector models of-fer an attractive alternative. In the standard ap-proach, word meaning is represented by feature vectors, with large sets of context words as dimen-sions, and their co-occurrence frequencies as val-ues. Semantic similarity information can be ac-quired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include in-formation retrieval (Manning et al., 2008), word-sense discrimination (Sch X tze, 1998) and disam-biguation (McCarthy and Carroll, 2003), to name but a few.

Standard vector-space models have serious lim-itations, however: While semantic information is typically encoded in phrases and sentences, distri-butional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related prob-lem is caused by word-sense ambiguity and con-textual variation of usage. Frequency counts of context words for a given target word provide in-variant representations averaging over all different usages of the target word. There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares .

Several approaches for word-sense disambigua-tion in the framework of distributional semantics have been proposed in the literature (Sch X tze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mu-tual contextualization of words in a phrase in a com-positional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Pad X  (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, as-suming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its de-pendent at the micro-level of relation-specific co-occurrence frequencies. It turns out that the benefit to precision is considerable.

Using syntactically enriched vector models raises problems of different kinds: First, the use of syntax increases dimensionality and thus may cause data sparseness (Pad X  and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct ob-ject knowledge , typically have different syntactic environments, which implies that their vector repre-sentations encode complementary information and there is no direct way of combining the information encoded in the respective vectors.

To solve these problems, we build upon pre-vious work (Thater et al., 2009) and propose to use syntactic second-order vector representations . Second-order vector representations in a bag-of-words setting were first used by Sch X tze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparse-ness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way.

We conduct two experiments to assess the suit-ability of our method. Our first experiment is car-ried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms other unsupervised methods that have been pro-posed in the literature to rank words with respect to their semantic similarity in a given linguistic context. In a second experiment, we apply our model to the  X  X ord sense similarity task X  recently proposed by Erk and McCarthy (2009), which is a refined variant of a word-sense disambiguation task. The results show a substantial positive effect. Plan of the paper. We will first review related work in Section 2, before presenting our model in Section 3. In Sections 4 and 5 we evaluate our model on the two different tasks. Section 6 con-cludes. Several approaches to contextualize vector repre-sentations of word meaning have been proposed. One common approach is to represent the mean-ing of a word a in context b simply as the sum, or centroid of a and b (Landauer and Dumais, 1997).
Kintsch (2001) considers a variant of this simple model. By using vector representations of a predi-cate p and an argument a , Kintsch identifies words that are similar to p and a , and takes the centroid of these words X  vectors to be the representation of the complex expression p ( a ) .

Mitchell and Lapata (2008), henceforth M&amp;L, propose a general framework in which meaning rep-resentations for complex expressions are computed compositionally by combining the vector represen-tations of the individual words of the complex ex-pression. They focus on the assessment of different operations combining the vectors of the subexpres-sions. An important finding is that component-wise multiplication outperforms the more common addi-tion method. Although their composition method is guided by syntactic structure, the actual instanti-ations of M&amp;L X  X  framework are insensitive to syn-tactic relations and word-order, assigning identical representation to dog bites man and man bites dog (see Erk and Pad X  (2008) for a discussion). Also, they use syntax-free bag-of-words-based vectors as basic representations of word meaning.

Erk and Pad X  (2008), henceforth E&amp;P, represent the meaning of a word w through a collection of vectors instead of a single vector: They assume selectional preferences and inverse selectional pref-erences to be constitutive parts of the meaning in addition to the meaning proper. The interpretation of a word p in context a is a combination of p  X  X  meaning with the (inverse) selectional preference of a . Thus, a verb meaning does not combine di-rectly with the meaning of its object noun, as on the M&amp;L account, but with the centroid of the vec-tors of the verbs to which the noun can stand in an object relation. Clearly, their approach is sensitive to syntactic structure. Their evaluation shows that their model outperforms the one proposed by M&amp;L on a lexical substitution task (see Section 4). The basic vectors, however, are constructed in a word space similar to the one of the M&amp;L approach.
In Thater et al. (2009), henceforth TDP, we took up the basic idea from E&amp;P of exploiting selec-tional preference information for contextualization. Instead of using collections of different vectors, we incorporated syntactic information by assuming a richer internal structure of the vector represen-tations. In a small case study, moderate improve-ments over E&amp;P on a lexical substitution task could be shown. In the present paper, we formulate a general model of syntactically informed contextu-alization and show how to apply it to a number a of representative lexical substitution tasks. Eval-uation shows significant improvements over TDP Figure 1: Co-occurrence graph of a small sample corpus of dependency trees. and E&amp;P. In this section, we present our method of contex-tualizing semantic vector representations. We first give an overview of the main ideas, which is fol-lowed by a technical description of first-order and second-order vectors (Section 3.2) and the contex-tualization operation (Section 3.3). 3.1 Overview Our model employs vector representations for words and expressions containing syntax-specific first and second order co-occurrences information.
The basis for the construction of both kinds of vector representations are co-occurrence graphs. Figure 1 shows the co-occurrence graph of a small sample corpus of dependency trees: Words are represented as nodes in the graph, possible depen-dency relations between them are drawn as labeled edges, with weights corresponding to the observed frequencies. From this graph, we can directly read off the first-order vector for every word w : the vec-tor X  X  dimensions correspond to pairs ( r , w 0 ) of a grammatical relation and a neighboring word, and are assigned the frequency count of ( w , r , w 0 ) .
The noun knowledge , for instance, would be rep-resented by the following vector: This vector talks about the possible dependency heads of knowledge and thus can be seen as the (inverse) selectional preference of knowledge (see Erk and Pad X  (2008)).

As soon as we want to compute a meaning rep-resentation for a phrase like acquire knowledge from the verb acquire together with its direct ob-ject knowledge , we are facing the problem that verbs have different syntactic neighbors than nouns, hence their first-order vectors are not easily com-parable. To solve this problem we additionally introduce another kind of vectors capturing infor-mations about all words that can be reached with two steps in the co-occurrence graph. Such a path is characterized by two dependency relations and two words, i.e., a quadruple ( r , w 0 , r 0 , w 00 ) , whose weight is the product of the weights of the two edges used in the path. To avoid overly sparse vec-tors we generalize over the  X  X iddle word X  w 0 and build our second-order vectors on the dimensions corresponding to triples ( r , r 0 , w 00 ) of two depen-dency relations and one word at the end of the two-step path. For instance, the second-order vector for acquire is In this simple example, the values are the prod-ucts of the edge weights on each of the paths. The method of computation is detailed in Section 3.2. Note that second order vectors in particular con-tain paths of the form ( r , r  X  1 , w 0 ) , relating a verb w to other verbs w 0 which are possible substitution candidates.

With first-and second-order vectors we can now model the interaction of semantic informa-tion within complex expressions. Given a pair of words in a particular grammatical relation like acquire knowledge , we contextualize the second-order vector of acquire with the first-order vec-tor of knowledge . We let the first-order vector with its selectional preference information act as a kind of weighting filter on the second-order vector, and thus refine the meaning representation of the verb. The actual operation we will use is point-wise multiplication, which turned out to be the best-performing one for our purpose. Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting.

In our example, we obtain a new second-order vector for acquire in the context of knowledge : Note that all dimensions that are not  X  X icensed X  by the argument knowledge are filtered out as they are multiplied with 0 . Also, contextualisation of ac-quire with the argument share instead of knowledge would have led to a very different vector, which reflects the fact that the two argument nouns induce different readings of the inherently ambiguous ac-quire . 3.2 First and second-order vectors Assuming a set W of words and a set R of depen-dency relation labels, we consider a Euclidean vec-tor space V 1 spanned by the set of orthonormal space whose dimensions correspond to pairs of a re-lation and a word. Recall that any vector of V 1 can be represented as a finite sum of the form  X  a i ~ e r , w with appropriate scalar factors a i . In this vector space we define the first-order vector [ w ] of a word w as follows: where  X  is a function that assigns the dependency triple ( w , r , w 0 ) a corresponding weight. In the sim-plest case,  X  would denote the frequency in a cor-pus of dependency trees of w occurring together with w 0 in relation r . In the experiments reported be-low, we use pointwise mutual information (Church and Hanks, 1990) instead as it proved superior to raw frequency counts:
We further consider a similarly defined vec-tor space V 2 , spanned by an orthonormal basis { ~ e fore correspond to triples of two relations and a word. Evidently this is a higher dimensional space than V 1 , which therefore can be embedded into V 2 by the  X  X ifting maps X  L r : V 1 ,  X  V 2 defined by L fore on all vectors of V 1 ). Using these lifting maps we define the second-order vector [[ w ]] of a word w as Substituting the definitions of L r and [ w 0 ] , this yields [[ w ]] =  X  which shows the generalization over w 0 in form of the inner sum.
 For example, if w is a verb, r = O BJ and r 0 = O
BJ  X  1 (i.e., the inverse object relation), then the the distribution of verbs w 00 which share objects with w . 3.3 Composition Both first and second-order vectors are defined for lexical expressions only. In order to represent the meaning of complex expressions we need to com-bine the vectors for grammatically related words in a given sentence. Given two words w and w 0 in relation r we contextualize the second-order vector of w with the r -lifted first-order vector of w 0 : Here  X  may denote any operator on V 2 . The ob-jective is to incorporate (inverse) selectional pref-erence information from the context ( r , w 0 ) in such a way as to identify the correct word sense of w . This suggests that the dimensions of [[ w ]] should be filtered so that only those compatible with the context remain. A more flexible approach than simple filtering, however, is to re-weight those di-mensions with context information. This can be expressed by pointwise vector multiplication (in terms of the given basis of V 2 ). We therefore take  X  to be pointwise multiplication.

To contextualize (the vector of) a word w with multiple words w 1 ,..., w n and corresponding rela-tions r 1 ,..., r n , we compute the sum of the results of the pairwise contextualizations of the target vec-tor with the vectors of the respective dependents: In this section, we evaluate our model on a para-phrase ranking task. We consider sentences with an occurrence of some target word w and a list of paraphrase candidates w 1 ,..., w k such that each of the w i is a paraphrase of w for some sense of w . The task is to decide for each of the paraphrase candidates w i how appropriate it is as a paraphrase of w in the given context. For instance, buy , pur-chase and obtain are all paraphrases of acquire , in the sense that they can be substituted for acquire in some contexts, but purchase and buy are not para-phrases of acquire in the first sentence of Table 1. Sentence Paraphrases
Teacher education students will acquire the knowl-edge and skills required to [. . . ] gain 4; amass 1; receive 1; obtain 1
Ontario Inc. will [. . . ] acquire the remaining IXOS shares [. . . ] buy 3; purchase 1; gain 1; get 1; procure 2; obtain 1 4.1 Resources We use a vector model based on dependency trees obtained from parsing the English Gigaword corpus (LDC2003T05). The corpus consists of news from several newswire services, and contains over four million documents. We parse the corpus using the Stanford parser 1 (de Marneffe et al., 2006) and a non-lexicalized parser model, and extract over 1 . 4 billion dependency triples for about 3.9 million words (lemmas) from the parsed corpus.

To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. The complete dataset contains 10 instances for each of 200 target words X  X ouns, verbs, adjec-tives and adverbs X  X n different sentential contexts. Systems that participated in the task had to generate paraphrases for every instance, and were evaluated against a gold standard containing up to 10 possible paraphrases for each of the individual instances.
There are two natural subtasks in generating paraphrases: identifying paraphrase candidates and ranking them according to the context. We follow E&amp;P and evaluate it only on the second subtask: we extract paraphrase candidates from the gold standard by pooling all annotated gold-standard paraphrases for all instances of a verb in all con-texts, and use our model to rank these paraphrase candidates in specific contexts. Table 1 shows two instances of the target verb acquire together with its paraphrases in the gold standard as an example. The paraphrases are attached with weights, which correspond to the number of times they have been given by different annotators. 4.2 Evaluation metrics To evaluate the performance of our method we use generalized average precision (Kishida, 2005), a variant of average precision.

Average precision (Buckley and Voorhees, 2000) is a measure commonly used to evaluate systems that return ranked lists of results. Generalized aver-age precision (GAP) additionally rewards the cor-rect order of positive cases w.r.t. their gold standard weight. We define average precision first: where x i is a binary variable indicating whether the i th item as ranked by the model is in the gold standard or not, R is the size of the gold standard, and n is the number of paraphrase candidates to be ranked. If we take x i to be the gold standard weight of the i th item or zero if it is not in the gold standard, we can define generalized average precision as follows: where I ( x i ) = 1 if x i is larger than zero, zero oth-erwise, and y i is the average weight of the ideal ranked list y 1 ,..., y i of gold standard paraphrases.
As a second scoring method, we use precision out of ten ( P 10 ). The measure is less discriminative than GAP. We use it because we want to compare our model with E&amp;P. P 10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007): where M is the list of 10 paraphrase candidates top-ranked by the model, G is the corresponding anno-tated gold-standard data, and f ( s ) is the weight of the individual paraphrases. 4.3 Experiment 1: Verb paraphrases In our first experiment, we consider verb para-phrases using the same controlled subset of the lexical substitution task data that had been used by TDP in an earlier study. We compare our model to various baselines and the models of TDP and E&amp;P, and show that our new model substantially outperforms previous work.
 Dataset. The dataset is identical to the one used by TDP and has been constructed in the same way as the dataset used by E&amp;P: it contains those gold-standard instances of verbs that have X  X ccording to the analyses produced by the MiniPar parser (Lin, 1993) X  X n overtly realized subject and object. Gold-standard paraphrases that do not occur in the parsed British National Corpus are removed. 2 In total, the dataset contains 162 instances for 34 dif-ferent verbs. On average, target verbs have 20 . 5 substitution candidates; for individual instances of a target verb, an average of 3 . 9 of the substitution candidates are annotated as correct paraphrases. Below, we will refer to this dataset as  X  X ST/SO. X  Experimental procedure. To compute the vec-tor space, we consider only a subset of the complete set of dependency triples extracted from the parsed Gigaword corpus. We experimented with various strategies, and found that models which consider all dependency triples exceeding certain pmi -and frequency thresholds perform best.

Since the dataset is rather small, we use a four-fold cross-validation method for parameter tuning: We divide the dataset into four subsets, test vari-ous parameter settings on one subset and use the parameters that perform best (in terms of GAP) to evaluate the model on the three other subsets. We consider the following parameters: pmi -thresholds for the dependency triples used in the computa-tion of the first-and second-order vectors, and frequency thresholds. The parameters differ only slightly between the four subsets, and the general tendency is that good results are obtained if a low pmi -threshold (  X  2 ) is applied to filter dependency triples used in the computation of the second-order vectors, and a relatively high pmi -threshold (  X  4 ) to filter dependency triples in the computation of the first-order vectors. Good performing frequency thresholds are 10 or 15. The threshold values for context vectors are slightly different: a medium pmi -threshold between 2 and 4 and a low frequency threshold of 3.

To rank paraphrases in context, we compute con-textualized vectors for the verb in the input sen-tence, i.e., a second order vector for the verb that is contextually constrained by the first order vec-tors of all its arguments, and compare them to the unconstrained (second-order) vectors of each para-phrase candidate, using cosine similarity. 3 For the first sentence in Table 1, for example, we compute [[ acquire Baselines. We evaluate our model against a ran-dom baseline and two variants of our model: One variant ( X  2 nd order uncontexualized X ) simply uses contextually unconstrained second-order vectors to rank paraphrase candidates. Comparing the full model to this variant will show how effective our method of contextualizing vectors is. The sec-ond variant ( X  1 st order contextualized X ) represents verbs in context by their first order vectors that specify how often the verb co-occurs with its argu-ments in the parsed Gigaword corpus. We compare our model to this baseline to demonstrate the bene-fit of (contextualized) second-order vectors. As for the full model, we use pmi values rather than raw frequency counts as co-occurrence statistics. Results. For the LST/SO dataset, the generalized average precision, averaged over all instances in the dataset, is 45.94%, and the average P 10 is 73.11%.
Table 2 compares our model to the random base-line, the two variants of our model, and previous work. As can be seen, our model improves about 8% in terms of GAP and almost 7% in terms of P 10 upon the two variants of our model, which in turn perform 10% above the random baseline. We conclude that both the use of second-order vectors, as well as the method used to contextualize them, are very effective for the task under consideration.
The table also compares our model to the model of TDP and two different instantiations of E&amp;P X  X  model. The results for these three models are cited from Thater et al. (2009). We can observe that our model improves about 9% in terms of GAP and about 7% in terms of P 10 upon previous work. Note that the results for the E&amp;P models are based Model GAP P 10 Random baseline 26.03 54.25 E&amp;P (add, object) 29.93 66.20 E&amp;P (min, subject &amp; object) 32.22 64.86
TDP 36.54 63.32 1 st order contextualized 36.09 59.35 2 nd order uncontextualized 37.65 66.32 Full model 45.94 73.11 on a reimplementation of E&amp;P X  X  original model X  the P 10 -scores reported by Erk and Pad X  (2009) range between 60.2 and 62.3, over a slightly lower random baseline.

According to a paired t-test the differences are statistically significant at p &lt; 0 . 01. Performance on the complete dataset. To find out how our model performs on less controlled datasets, we extracted all instances from the lexical substitution task dataset with a verb target, exclud-ing only instances which could not be parsed by the Stanford parser, or in which the target was mis-tagged as a non-verb by the parser. The resulting dataset contains 496 instances. As for the LST/SO dataset, we ignore all gold-standard paraphrases that do not occur in the parsed (Gigaword) corpus.
If we use the best-performing parameters from the first experiment, we obtain a GAP score of 45.17% and a P 10 -score of 75.43%, compared to random baselines of 27.42% (GAP) and 58.83% ( P 10 ). The performance on this larger dataset is thus almost the same compared to our results for the more controlled dataset. We take this as evi-dence that our model is quite robust w.r.t. different realizations of a verb X  X  subcategorization frame. 4.4 Experiment 2: Non-verb paraphrases We now apply our model to parts of speech (POS) other than verbs. The main difference between verbs on the one hand, and nouns, adjectives, and adverbs on the other hand, is that verbs typically come with a rich context X  X ubject, object, and so on X  X hile non-verbs often have either no depen-dents at all or only closed class dependents such as determiners which provide only limited contextual informations, if any at all. While we can apply the same method as before also to non-verbs, we might expect it to work less well due to limited contextual Noun 535 46.38 42.54 30.01 Table 3: GAP-scores for non-verb paraphrases us-ing two different methods. information.

We therefore propose an alternative method to rank non-verb paraphrases: We take the second-order vector of the target X  X  head and contextually constrain it by the first order vector of the target. For instance, if we want to rank the paraphrase candidates hint and star for the noun lead in the sentence (1) Meet for coffee early, swap leads and get per-we compute [[ swap OBJ :lead ]] and compare it to the lifted first-order vectors of all paraphrase candi-dates, L OBJ ([ hint ]) and L OBJ ([ star ]) , using cosine similarity.

To evaluate the performance of the two methods, we extract all instances from the lexical substitution task dataset with a nominal, adjectival, or adverbial target, excluding instances with incorrect parse or no parse at all. As before, we ignore gold-standard paraphrases that do not occur in the parsed Giga-word corpus.

The results are shown in Table 3, where  X  X 1 X  refers to the method we used before on verbs, and  X  X 2 X  refers to the alternative method described above. As one can see, M1 achieves better results than M2 if applied to nouns, while M2 is better than M1 if applied to adjectives and adverbs. The second result is unsurprising, as adjectives and ad-verbs often have no dependents at all.

We can observe that the performance of our model is similarly strong on non-verbs. GAP scores on nouns (using M1) and adverbs are even higher than those on verbs. We take these results to show that our model can be successfully applied to all open word classes. In this section, we apply our model to a different word sense ranking task: Given a word w in context, the task is to decide to what extent the different WordNet (Fellbaum, 1998) senses of w apply to this occurrence of w .
 Dataset. We use the dataset provided by Erk and McCarthy (2009). The dataset contains ordinal judgments of the applicability of WordNet senses on a 5 point scale, ranging from completely differ-ent to identical for eight different lemmas in 50 different sentential contexts. In this experiment, we concentrate on the three verbs in the dataset: ask , add and win .
 Experimental procedure. Similar to Pennac-chiotti et al. (2008), we represent different word senses by the words in the corresponding synsets. For each word sense, we compute the centroid of the second-order vectors of its synset members. Since synsets tend to be small (they even may con-tain only the target word itself), we additionally add the centroid of the sense X  X  hypernyms, scaled down by the factor 10 (chosen as a rough heuristic without any attempt at optimization).
 We apply the same method as in Section 4.3: For each instance in the dataset, we compute the second-order vector of the target verb, contextually constrain it by the first-order vectors of the verb X  X  arguments, and compare the resulting vector to the vectors that represent the different WordNet senses of the verb. The WordNet senses are then ranked according to the cosine similarity between their sense vector and the contextually constrained target verb vector.

To compare the predicted ranking to the gold-standard ranking, we use Spearman X  X   X  , a standard method to compare ranked lists to each other. We compute  X  between the similarity scores averaged over all three annotators and our model X  X  predic-tions. Based on agreement between human judges, Erk and McCarthy (2009) estimate an upper bound  X  of 0 . 544 for the dataset.
 Results. Table 4 shows the results of our exper-iment. The first column shows the correlation of our model X  X  predictions with the human judgments from the gold-standard, averaged over all instances. All correlations are significant ( p &lt; 0 . 001 ) as tested by approximate randomization (Noreen, 1989).
The second column shows the results of a frequency-informed baseline, which predicts the ranking based on the order of the senses in Word-Net. This (weakly supervised) baseline outper-forms our unsupervised model for two of the three verbs. As a final step, we explored the effect of
Word Present paper WN-Freq Combined average 0.279 0.291 0.361 Table 4: Correlation of model predictions and hu-man judgments combining our rankings with those of the frequency baseline, by simply computing the average ranks of those two models. The results are shown in the third column. Performance is significantly higher than for both the original model and the frequency-informed baseline. This shows that our model cap-tures an additional kind of information, and thus can be used to improve the frequency-based model. We have presented a novel method for adapting the vector representations of words according to their context. In contrast to earlier approaches, our model incorporates detailed syntactic information. We solved the problems of data sparseness and incompatibility of dimensions which are inherent in this approach by modeling contextualization as an interplay between first-and second-order vectors.
Evaluating on the SemEval 2007 lexical substitu-tion task dataset, our model performs substantially better than all earlier approaches, exceeding the state of the art by around 9% in terms of general-ized average precision and around 7% in terms of precision out of ten. Also, our system is the first un-supervised method that has been applied to Erk and McCarthy X  X  (2009) graded word sense assignment task, showing a substantial positive correlation with the gold standard. We further showed that a weakly supervised heuristic, making use of WordNet sense ranks, can be significantly improved by incorporat-ing information from our system.

We studied the effect that context has on target words in a series of experiments, which vary the target word and keep the context constant. A natu-ral objective for further research is the influence of varying contexts on the meaning of target expres-sions. This extension might also shed light on the status of the modelled semantic process, which we have been referring to in this paper as  X  X ontextu-alization X . This process can be considered one of mutual disambiguation, which is basically the view of E&amp;P. Alternatively, one can conceptualize it as semantic composition: in particular, the head of a phrase incorporates semantic information from its dependents, and the final result may to some extent reflect the meaning of the whole phrase.

Another direction for further study will be the generalization of our model to larger syntactic con-texts, including more than only the direct neighbors in the dependency graph, ultimately incorporating context information from the whole sentence in a recursive fashion.
 Acknowledgments. We would like to thank Ed-uard Hovy and Georgiana Dinu for inspiring discus-sions and helpful comments. This work was sup-ported by the Cluster of Excellence  X  X ultimodal Computing and Interaction X , funded by the Ger-man Excellence Initiative, and the project SALSA, funded by DFG (German Science Foundation).

