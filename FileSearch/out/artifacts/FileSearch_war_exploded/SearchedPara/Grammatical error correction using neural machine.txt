 Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in text written by non-native English writers. Un-like building machine learning classifiers for spe-cific error types (e.g. determiner or preposition er-rors) (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), the idea of  X  X ranslating X  a grammatically incorrect sen-tence into a correct one has been proposed to handle all error types simultaneously (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). Sta-tistical machine translation (SMT) has been suc-cessfully used for GEC, as demonstrated by the top-performing systems in the CoNLL-2014 shared task (Ng et al., 2014).

Recently, several neural machine translation (NMT) models have been developed with promis-ing results (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike SMT, which consists of com-ponents that are trained separately and combined during decoding (i.e. the translation model and lan-guage model) (Koehn, 2010), NMT learns a single large neural network which inputs a sentence and outputs a translation. NMT is appealing for GEC as it may be possible to correct erroneous word phrases and sentences that have not been seen in the train-ing set more effectively (Luong et al., 2015). NMT-based systems thus may help ameliorate the lack of large error-annotated learner corpora for GEC.
However, NMT models typically limit vocabu-lary size on both source and target sides due to the complexity of training (Sutskever et al., 2014; Bah-danau et al., 2014; Luong et al., 2015; Jean et al., 2015). Therefore, they are unable to translate rare words, and out-of-vocabulary (OOV) words are re-placed with UNK symbol. This problem is more se-rious for GEC as non-native text contains not only rare words (e.g. proper nouns), but also misspelled words (i.e. spelling errors). By replacing all the OOV words with the same UNK symbol, useful in-formation is discarded, resulting in systems that are not able to correct misspelled words or even keep some of the error-free original words, as in the fol-lowing examples (OOV words are underlined):
Inspired by the work of Luong et al. (2015), we propose a similar but much simpler two-step ap-proach to address the rare word problem: rather than annotating the training data with alignment infor-mation, we apply unsupervised alignment models to find the sources of the words in the target sentence. Once we know the source words that are responsible for the unknown target words, a word level transla-tion model learnt from parallel sentences is used to translate these source words.
 This paper makes the following contributions. First, we present the first study using NMT for GEC, outperforming the state-of-the-art. Second, we pro-pose a two-step approach to address the rare word problem in NMT for GEC, which we show yields a substantial improvement. Finally, we report results on two well-known publicly available test sets that can be used for cross-system comparisons. NMT systems apply the so-called encoder-decoder mechanism proposed by Cho et al. (2014) and Sutskever et al. (2014). An encoder reads and en-codes an entire source sentence x = ( x 1 , x 2 , ..., x T into a vector c : where a hidden state h t at time t is defined as:
A decoder then outputs a translation y = ( y 1 , y 2 , ..., y T 0 ) by predicting the next word y t based on the encoded vector c and all the previously pre-dicted words { y 1 , y 2 , ..., y t  X  1 } : p ( y ) = where s t is the hidden state of the decoder.
Different neural network models have been proposed, for example, Kalchbrenner and Blun-som (2013) proposed a hybrid of a recurrent neural network (RNN) and a convolutional neural network, Sutskever et al. (2014) used a Long Short-Term Memory (LSTM) model, Cho et al. (2014) proposed a similar but simpler gated RNN model, and Bah-danau et al. (2014) introduced an attentional-based architecture.
 In this work, we use the RNNsearch model of Bahdanau et al. (Bahdanau et al., 2014), which con-tains a bidirectional RNN as an encoder and an attention-based decoder. The bidirectional RNN en-coder has a forward and a backward RNN. The for-ward RNN reads the source sentence from the first word to the last, and the backward RNN reads the source sentence in reverse order. By doing this, it captures both historical and future information. The attention-based model allows the decoder to fo-cus on the most relevant information in the source sentence, rather than remembering the entire source sentence. The rare word problem in NMT has been noticed by (Sutskever et al., 2014; Bahdanau et al., 2014; Lu-ong et al., 2015; Jean et al., 2015). Jean et al. (2015) proposed a method based on importance sampling that uses a very large target vocabulary without in-creasing training complexity. However, no matter how large the target vocabulary size is, there are still OOV words. We also notice that in GEC, the source side vocabulary size is much larger than that of the target side as there are many incorrect words in the source (e.g. spelling mistakes and word form errors) (see Section 4.1). Luong et al. (2015) introduced three new annotation strategies to annotate the train-ing data, so that unknown words in the output can be traced back to their origins. The training data was first re-annotated using the output of a word align-ment algorithm. NMT systems were then built using this new data. Finally, information about the OOV words in the target sentence and their corresponding words in the source sentence was extracted from the NMT systems and used in a post-processing step to translate these OOV words using a dictionary.
We propose a similar two-step approach: 1) align-ing the unknown words (i.e. UNK tokens) in the tar-get sentence to their origins in the source sentence with an unsupervised aligner; 2) building a word level translation model to translate those words in a post-processing step. In order to locate the source words that are responsible for the unknown target words, we apply unsupervised aligners directly and use only the NMT model output instead of first re-annotating training data, and then building new NMT models using this newly annotated data as pro-posed by Luong et al. (2015). Our approach is much simpler as we avoid re-annotating any data and train only one NMT model. Due to the nature of error cor-rection (i.e. both source and target sentences are in the same language), most words translate as them-selves, and errors are often similar to their correct forms. Thus, unsupervised aligners can be success-fully used to align the unknown target words. Two automatic alignment tools are used: GIZA++ (Och and Ney, 2003) and METEOR (Banerjee and Lavie, 2005). GIZA++ is an implementation of IBM Mod-els 1-5 (Brown et al., 1993) and a Hidden-Markov alignment model (HMM) (Vogel et al., 1996), which can align two sentences from any languages. Un-like GIZA++, METEOR aligns two sentences from the same language. The latest METEOR 1.5 only supports a few languages, and English is one of them. METEOR identifies not only words with ex-act matches, but also words with identical stems, synonyms, and unigram paraphrases. This is use-ful for GEC as it can deal with word form, noun number, and verb form corrections that share iden-tical stems, as well as word choice corrections with synonyms or unigram paraphrases. To build a word level translation model for translating the source words that are responsible for the target unknown words, we need word-aligned data. The IBM Mod-els are used to learn word alignment from parallel sentences. 4.1 Dataset We use the publicly available FCE dataset (Yan-nakoudakis et al., 2011), which is a part of the Cambridge Learner Corpus (CLC) (Nicholls, 2003). The FCE dataset contains 1,244 scripts produced by learners taking the First Certificate in English (FCE) examination between 2000 and 2001. The texts have been manually annotated by linguists using a taxon-omy of approximately 80 error types. The publicly available FCE dataset contains about 30,995 pairs of parallel sentences for training (approx. 496,567 tokens on the target side) and about 2,691 pairs of parallel sentences for testing (approx. 41,986 tokens on the target side). Since the FCE training set is too small to build good MT systems, we add train-ing examples extracted from the CLC. Overall, there are 1,965,727 pairs of parallel sentences in our train-ing set. The source side contains 28,823,615 words with 248,028 unique words, and the target side con-tains 29,219,128 words with 143,852 unique words. As we can see, the source side vocabulary size is much larger than that of the target side. Training and test data is pre-processed using RASP (Briscoe et al., 2006). 4.2 Evaluation System performance is evaluated using three au-tomatic evaluation metrics: I-measure (Felice and 2012) and GLEU (Napoles et al., 2015). In the I-measure, an Improvement (I) score is computed by comparing system performance with that of a baseline which leaves the original text uncorrected scorer in the CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014), with F 0.5 being the reported met-ric in the 2014 edition. GLEU is a simple variant of BLEU (Papineni et al., 2002), which shows better correlation with human judgments on the CoNLL-2014 shared task test set. 4.3 SMT baseline Following previous work (e.g. Brockett et al. (2006), Yuan and Felice (2013)), we build a phrase-based SMT error correction system as the baseline. Pi-align (Neubig et al., 2011) is used to create a phrase translation table. In addition to default features, we add character-level Levenshtein distance to each mapping in the phrase table as proposed by Fe-lice et al. (2014). Decoding is performed using Moses (Koehn et al., 2007). The language model used during decoding is built from the corrected sentences in the learner corpus, to make sure that the final system outputs fluent English sentences. The IRSTLM Toolkit (Federico et al., 2008) is used to buid a 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995). 4.4 NMT training details Our training procedure and hyper-parameters for the NMT system are similar to those used by Bahdanau et al. (2014). We train models with sentences of length up to 100 words, which covers about 99.96% of all the training examples. In terms of vocabulary size, we limit the target vocabulary size to 30K, and experiment with three different source vocabulary for approximately 5 days using a Tesla K20 GPU.
The output sentences from the NMT systems are aligned with their source sentences using GIZA++. In addition, alignment information learnt by ME-TEOR is used by GIZA++ during aligning. All the UNK tokens in the output sentences are replaced with the translation of the source words that are re-sponsible for those UNK tokens. The translation is performed using a word level model learnt from IBM Model 4. 4.5 Results From the results in Table 1, we can see that NMT-based systems alone are not able to achieve compa-rable results to an SMT-based system. It is proba-bly because of the rare word problem, as increasing the source side vocabulary size helps. The perfor-mance of the best NMT system alone ( NMT 80K-30K ), without replacing UNK tokens, is still worse than the SMT baseline. When we replace the UNK tokens in the NMT output, using GIZA++ for un-known word alignment improves the system per-formance for all three NMT systems in all three evaluation metrics. We can see that our proposed approach is more useful for NMT systems trained Source 60.39 0 0 SMT baseline 70.15 52.90 2.87 NMT-based systems
NMT 30K-30K 69.04 46.10 -1.30 + GIZA++ 70.89 52.79 3.89 + METEOR 71.16 53.49 3.94
NMT 50K-30K 68.95 46.78 -1.14 + GIZA++ 70.31 52.02 2.86 + METEOR 70.40 52.35 2.89
NMT 80K-30K 70.02 49.17 -1.04 + GIZA++ 71.18 53.48 2.40 + METEOR 71.18 53.49 2.41 on a small source side vocabulary (e.g. 30K) than a large vocabulary (e.g. 50K, 80K). The larger the vocabulary size, the smaller the gain after replac-ing UNK tokens. The introduction of the METEOR alignment information to GIZA++ yields further im-provements. Our best system ( NMT 30K-30K + GIZA++ + METEOR ) achieves an F 0.5 score of 53.49%, an I score of 3.94%, and a GLEU score of 71.16%, outperforming the SMT baseline in all three evaluation metrics.

Comparing the output of the SMT baseline with that of the NMT system reveals that there are some learner errors which are missed by the SMT system but are captured by the NMT system. One possi-ble reason is that the phrase-based SMT system is trained on surface forms and therefore unaware of syntactic structure. In order to make a correction, it has to have seen the exact correction rule in the training data. Since the NMT system does not rely on any correction rules, in theory, it should be able to make any changes as long as it has seen the words in the training data. For example:
The SMT system fails to correct the word form er-ror as the correction rule ( kidnaps  X  kidnappings ) is not in the SMT phrase table learnt from the training data. Since these two words ( kidnaps and kidnap-pings ) have been seen in the training data, the NMT system corrects this error successfully. The CoNLL-2014 shared task on grammatical er-ror correction required participating systems to cor-rect all errors present in learner English text. The official training and test data comes from the Na-tional University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013). F 0.5 was adopted as the evaluation metric, as reported by tem generalises, we apply our best system trained on the CLC to the CoNLL-2014 shared task test data directly without adding the NUCLE training data or tuning for the NUCLE. The state-of-the-art F 0.5 score was reported by Susanto et al. (2014) after the shared task. By combining the outputs from two classification-based systems and two SMT-based systems, they achieved an F 0.5 score of 39.39%. Re-sults of the uncorrected baseline, our best NMT-based system, Susanto et al. (2014) X  X  system and the top three systems in the shared task are presented in Table 2. We can see that our NMT-based sys-tem outperforms the top three teams, achieving the highest F 0.5 , I and GLEU scores. It also outperforms the state-of-the-art combined system from Susanto et al. (2014). Our system achieves the best F 0.5 score of 39.90% even though it is not trained on the NU-CLE data. This result shows that our system gen-eralises well to other datasets. We expect these re-sults might be further improved by retokenising the test data to be consistent with the tokenisation of the We have shown that NMT can be successfully ap-plied to GEC once we address the rare word prob-lem. Our proposed two-step approach for UNK re-placement has been proved to be effective, and to provide a substantial improvement. We have de-veloped an NMT-based system that generalises well to another dataset. Our NMT system achieves an F 0.5 score of 53.49%, an I score of 3.94%, and a GLEU score of 71.16% on the publicly available FCE test set, outperforming an SMT-based system in all three metrics. When testing on the official CoNLL-2014 test set without alternative answers, our system achieves an F 0.5 score of 39.90%, out-performing the current state-of-the-art. In future work, we would like to explore other ways to ad-dress the rare word problem in NMT-based GEC, such as incorporating the soft-alignment information generated by the attention-based decoder, or using character-based models instead of word-based ones. We would like to thank Cambridge English Lan-guage Assessment and Cambridge University Press for granting us access to the CLC for research purposes as well as the anonymous reviewers for their comments and suggestions. We acknowledge NVIDIA for an Academic Hardware Grant. This work also used the Wilkes GPU cluster at the Uni-versity of Cambridge High Performance Comput-ing Service, provided by Dell Inc., NVIDIA and Mellanox, and part funded by STFC with indus-trial sponsorship from Rolls Royce and Mitsubishi Heavy Industries.
