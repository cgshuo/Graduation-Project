 Dorothee Beermann  X  Pavel Mihaylov Abstract Interlinear Glossed Text (IGT) is a well established data format within philology and the structural and generative fields of linguistics. The best known format for an IGT is the one found in linguistic publications, where one line of text is followed by one line of glosses and one line of free translation. Although used in different functions, IGTs are ubiquitous in linguistic research and publications. Yet they also have been criticised for being fabricated and unreliable in some of their uses. However that might be, IGTs represent linguistic knowledge, and in particular for less-resourced languages, they are not rarely the only structured data available. Under the auspices of the Digital Humanities, linguists increasingly focus on the advantages of Semantic Web technologies. Presenting the modules and procedures of the web-based linguistic application TypeCraft (TC), we outline how the creation of IGTs can become an integral part of a shared linguistic methodology. Linguistic services have the potential of allowing efficient data management, and their strength lies in facilitating new forms of collaboration beyond social networking. They pave the way towards what one might call shared methodologies. In this paper we would like to discuss the linguistic value of web-based technology. By presenting the functionalities of TC and giving a detailed summary of online linguistic data cre-ation and retrieval, we will present external and internal criteria for a single system evaluation of TC centred on usage objectives.
 Keywords Digital Humanities  X  Web-based linguistic tools  X  Language data management  X  IGT  X  Shared linguistic methodologies  X  Single system usage evaluation 1 Introduction With more linguists interested in the description of endangered and less-described languages, we see a rising interest in the electronic creation and management of linguistic data, which is true also for fields outside of Language Documentation.
From the beginning, computers have played an increasing role in linguistic research, yet the way they were used before and the way they are used now has changed drastically. Enhanced by Semantic Web technologies, the possibilities offered by electronic data management have started to change the ways linguistic research is conducted. 1 In particular, modern approaches to Language Description and Documentation (LDD) would not have been thinkable without the technology that allows the creation, retrieval and storage of diverse data types. A field that aims to provide a comprehensive record of language constructions (Himmelmann 1998 ) is crucially dependent on software that supports the effort and an infrastructure that can sustain a sophisticated digital environment.

Gibbon ( 2008 ) envisions language documentation as a trans-disciplinary cooperation between phonetics, the descriptive fields of linguistics, speech and text technology and computational linguistics. Now, only a few years later, the interest in linguistic tools has grown beyond these fields. Electronic grammaticog-raphy (Nordhoff 2012 ) is a recent development uniting descriptive and computa-tional linguists interested in the integration of data exploration and grammar development, either in the form of descriptive grammars (Ameka et al. 2006 )orin the form of automated grammars (Bender et al. 2012 ).

For many years, linguistic tools such as Toolbox were  X  X ontraptions X  used primarily by descriptive linguists engaged in field work. Yet, carried by the success of LDD, new or upgraded tools were developed with new improved functionalities. This new generation of linguistic software has attracted new user groups (Schmidt 2010 ). With tools like FLEx 2 and ELAN 3 and on the basis of a growing availability of linguistic data, linguists in different sub-disciplines discover the potential of data management. In this way the interest in the exchange of data grows as well. Given that the majority of linguistic tools are designed as single-user desktop systems, the exchange of data is a problem. Only very recently online services start to appear that have the potential of facilitating the exchange of linguistic data from different applications (Ferreira et al. 2012 ).

The use of web-based technology for linguistic data management, also outside of the fields of phonetics and NLP, is by no means a new idea. In 2006 TypeCraft (TC) was presented as the first linguistic tool that allowed the creation, storage and exchange of IGT online (Beermann and Prange 2006 ). Also in the same year, EOPAS was launched (Schroeter and Thieberger 2006 ), however, while TC focuses on the creation of IGTs and their exchange, EOPAS has its main focus on the presentation and storage of multi-media data.

Although the Open Language Archives Community has been in existence since 2000, the idea of creating and trading IGT online was new then. When TC was released, the ramifications of Open Access for linguistic research data were still very much under discussion. How could the rights of the data owners and the data providers be protected, and how could one guarantee the quality of data that did not come from officially released databases, but from individual users interested in sharing their data online? These were only two of the questions that needed answers.
Under the auspices of the Digital Humanities (Gold 2012 ; Burdick et al. 2012 ) and the migration of all types of research relevant material into virtual environments, e-research became more widely endorsed. Now, using semantic web technologies, electronic research data can be made traceable and in this way citable very much like books and other on-paper publications. The next generation of linguistic tools will most likely be hybrid systems, partially offering tools for download, partially offering services online. This type of application is often referred to as Service Oriented Architecture (SOA). 4 The idea is that through the chaining of tools or through the construction of data bridges (Beermann et al. 2012 ), the researcher can select from among the available tools those that are best for the task at hand. Such a development requires not only the architecture but also the structural and conceptual standards to make data interchangeable and worth exchanging.

Having set the stage, this article now focuses on the question how useful already existing web based linguistic technology is in supporting the management of linguistic data. Linguistic data in the context of this article is Interlinear Glossed Text (IGT). In linguistics IGT is the most common form of data and as such an integral part of linguistic research and publications. Surrounded by linguistic prose, IGT generally is demarcated through indenting, numbering, and spacing above and under the example. One line of text is followed by one line of glosses, and a line with free translation completes the pattern. An IGT 5 occurs in isolation and normally lacks any index to where and when it occurred or any other information that would identify it as a particular instance of a language (Lehmann 2004a ). The theme that we would like to develop here is that IGT data is an important linguistic resource which warrants the creation of a research infrastructure supporting their wide-scale electronic exchange.

We start our discussion by reviewing the linguistic form and function of IGT. It is very well known that IGT is a  X  X ery-noisy X  type of data. Still, it is a valuable resource due to the fact that it relates in a systematic fashion grammatical forms to grammatical categories. IGT are linear linguistic representations of form-function packages, and while Bow et al. ( 2003 ) have looked at their formal properties, we here would like to consider their linguistic properties in some more detail. The paper is organised as follows. In Sect. 2 we describe the present day use of IGT. We will isolate those properties of IGTs that prevent them from being the prime linguistic resource that they could be. In Sect. 3 we outline how web-based linguistic technology might help to make IGTs a more reliable resource. As a concrete example of such technology, we present the TC application which we evaluate using different types of criteria. Section 4 concludes the paper. 2 Interlinear glossed text: problems and potential Since interlinear glosses became a standard for linguistic publications, very many IGTs have been created which, either in paper form or, more and more so, online, have become part of the scientific discourse. When IGTs occur in publications they normally do that as isolated sentences, and even when identified as belonging to a larger corpus, they normally are evaluated without any access to the broader linguistic context in which they occur. This is not necessarily a problem since the function of IGT in research and in linguistic publications is not uniform.
In the logical tradition, where linguists follow in the footsteps of the philosophical and mathematical sciences, an IGT is an idealised representation of the linguistic reality that the theory describes. The work of Louis Hjelmslev is an example of this approach, and of course Noam Chomsky X  X  work stands in this tradition. Authors like Ross, Postal, McCawley and Bach are notable early US representatives of this research tradition where the creation of linguistic data is based on linguistic expertise alone, leading to a characteristic style of exposition where IGTs serve as threads of the discussion. Lyons ( 1977 ) calls the corresponding type of linguistic data System-sentences .

The function of IGT within the empirically-oriented fields of linguistics is different. Here an IGT serves as a Data Sample . It might have been gathered through linguistic interviews or other elicitation methods. However, considering the format of the data alone, there are no principled differences between IGT across linguistic traditions. What is different is the emphasis that is put on the representativeness and authenticity of the data; this is where the real difference between the two main schools of linguistics seems to lie. 6
Recently linguistic data has come under scrutiny. Researchers from different linguistic fields have questioned its validity, and the integrity of theories that  X  X re built X  on this kind of data. From the psycholinguistic side it has been claimed that linguists are not (sufficiently) concerned with methods that regulate data collections. With Schu  X  tze ( 1996 ) as prime reference, it has been pointed out that IGTs which are mostly based on binary grammaticality judgements, should, like any other data which is based on human judgement, be exposed to empirical control in order to assure a reliable mode of their elicitation (Keller 2000 ; Keller and Asudeh 2007 ).
Also on the side of functional linguistics, in particular from the ranks of linguists working with LDD, methodological issues have been raised, calling for the standardisation of linguistic annotations and improved methods of data management and data provenance. The standards called for thus fall into two categories, the internal/conceptual and the structural/book-keeping standards (Chiarcos et al. 2012 ). Especially for IGTs occurring in publications on less researched languages, we notice long citation histories so that data provenance, which means in its simplest form that citation chains are recorded, becomes an important issue, which tends to be neglected by authors and editors alike (Lewis 2006 ). A further consideration is the format or structure of IGT. In a meticulous study, Bow et al. ( 2003 ) inspect linguistic glossing and propose a XML-based general-purpose model for the representation of IGTs.
 With the introduction of new methodologies and a rising access to data, what Kertsz and Rkosi ( 2012 ) call the  X  X tandard View Of Linguistic Data X  is changing. Reflections about the relation of language specific data to grammatical categories has led to questions of the adequacy of our terminological apparatus and the relation between terminology and linguistic theory, calling the latter into question (Haspelmath 2012 ).

In the present context, we will reflect on the relation between data and linguistic analysis from a methodological point of view. Given the right tools, linguistics today is in an excellent position to collect, organize, and analyse more data than ever before. In this context we would like to know how useful web based linguistic technologies can be.

Before we now turn to the discussion of IGTs, one more general remark seems in order. Text based linguistic data in its most traditional linguistic form resides in morpheme or word level annotated linguistic phrases. These IGTs are the result of symbolic processing, they are linear representations consisting of form-function packages. It is sometimes claimed that they constitute the empirical foundation that a linguistic analysis rests upon, yet they can hardly be that, since they are a form of linguistic analysis themselves. The joint publication of linguistic research and IGT corpus increases the validity of an analysis, as it allows us to test to which extent certain generalisations are supported by the data. Joint publication of analysis and data however does not prevent methodological chicken-and-egg situations, where annotations closely reflect a particular analysis, such that data analysis only provides what we intended to find. Nevertheless, publishing both research and research data is still a good idea, as it allows extended explorations and probing of the material that the research is built on. Is the data consistent, is it representative, these are two of the questions that need to be asked. And again, we would like to know whether modern linguistic tools can help the linguist to manage the relation between the data and its further analysis.
 We now turn to a more detailed discussion of IGTs.

As pointed out by Lewis ( 2006 ) in his discussion of the ODIN database most published linguistic data reflects the  X  X isplaycentric X  function of the IGT. Not only the typical system sentence but IGTs in general when embedded in linguistic prose are meant as a convenience to the reader. When extracted from their publication, their value tends to be minimal, since information essential to their understanding is given in the paper rather than through annotations of the examples themselves. But there are additional problems, some of which we would like to discuss by probing the linguistic information contained in a small set of IGTs which we exported from the ODIN database. We chose Akan 7 data for the simple reason that we can rely on our own informants for the evaluation of the examples, we furthermore chose examples coming from articles representing work from different fields of linguistics. The first example, presented here as (1), comes from Haspelmath ( 2001 ):
The second example is extracted from a paper by Ameka ( 2003 ) who cites Osam ( 1994 ): The third example is quoted in a manuscript by Wunderlich ( 2005 ).
 The fourth interlinear gloss comes from a manuscript by Drubig ( 2000 ): All authors use examples which are excerpted from the literature. This is not unusual, in fact, examples from African languages tend to have a long citation history. Thus example (3) goes back to Sedlak ( 1975 ), 142f and Stewart ( 1963 ), 145ff, as indicated by Wunderlich. Considering the examples (1) X (4) we notice:  X  Comparing (2) with (3) n X  is glossed as  X  X EF X  in (2) and  X  X hat X  in (3). According  X  In example (2), the verb ma X  9 meaning  X  X ake X , must, if a prosodic rendering of  X  In (3) the last vowel of p  X  nk  X   X  must be an open o.  X  In (3) the free translation of the object as his horse is inconsistent with the word-
Each of these points must be viewed in its larger linguistic context. Most IGTs lack part of speech (POS) annotations, which is not only a disadvantage for further linguistic processing, but also from a purely linguistic point of view questionable. IGTs are syntagmatic representations, and we just saw that the determiner n X  might have a distal meaning, that is, function as a demonstrative. The phenomenon is pervasive also in Akan. Akan verbs serve in prepositional functions, the verb ma  X  X ive X  from example (1) introduces beneficiary arguments and therefore often translates into  X  X or X , while relational nouns are used to express topological relations and in this function follow the noun that is being located. Creating IGTs means categorising words and ascribing functional classifications; to facilitate this process the annotation tool needs to implement the conceptual space which allows the expression of function and form in a conspicuous way.

A further issue apparent through inconsistent tone marking is that original text strings in IGTs might either be orthographic representations or phonemic renderings. Clear conventions do not exist. In Akan for example, the Akan orthography does not represent tone and neither do a majority of IGTs found in the linguistic literature on Akan. Yet, Akan features lexical as well as grammatical tone which makes it desirable to provide additionally a phonemic representation next to an orthographic representation. An IGT annotation editor should facilitate the rendering of original scripts, and needs to allow for the encoding of tone since grammar (and the lexicon) cannot be analysed without reference to prosodic features. At least for tone languages that seems true.

Turning now to the morphological analysis, we further notice relative to the examples (1) X (4) that:  X  The verb m X  X  receives no morphological analysis in (1).  X  Due to lack of word internal analysis, we miss the fact that the verb initial re in  X  The general lack of part of speech information makes it impossible to determine  X  In (3) the gloss 3sgP is ambiguous between three singular Personal Pronoun and
The points made in the discussion of example (1) X (4) are symptomatic of factors well-known from the literature. But what is the reason for IGTs to remain cursory? We have already mentioned their displaycentric function as one reason, a second reason is again mentioned in Thieberger and Berez ( 2012 ): IGT creation is time-consuming; efficient data management therefore often leads to cursory annotations. Here IGT annotation editors can have a facilitating function. Morphological segmentation must become not only easier but also more automated. Moving on, we not only can identify the reasons why IGT remain cursory, we can also identify good reasons why annotations must be incomplete (Gippert et al. 2006 ). In particular Mosel ( 2006 ) and Schultze-Berndt ( 2006 ) address questions relating to the creation of annotation as part of a linguistic discovery process. Since IGT is as much a result of linguistic research as it serves as its input, annotated data reflects a current stage of knowledge, which sheds a new light on the fact that IGTs are sometimes ambiguous and incomplete. Mosel ( 2006 ) stresses that IGTs develop incrementally. From a methodological point of view that means that annotation is not a linear but a cyclic process. Morphological segmentation might have to be revised, for example. An IGT editor therefore should not only allow for partial annotation, but also for flexible morphological tokenisation. Moreover annotations should more readily integrate the expression of uncertainty, for example in the form of notes that can be stored together with the annotations.

To systematise these factors mentioned and in order to integrate them more readily in the creation of IGT, we would like to summarise them using a tool-oriented view. Factors now become system features and methodological consider-ations translate into functionalities in Table 1 .

Table 1 is not meant to be comprehensive but indicative. It will be used for evaluation purposes in the following section, where we will try to find answers for the questions how useful the TC linguistic editor is in producing the output that we wish for (product-oriented evaluation), and how useful it is as a research instrument (process-oriented evaluation). We will also evaluate its search functionality using external standards.

TC is a web-based linguistic service, and its description and evaluation should allow us to better relate published expectations concerning the impact of SOA (Hinrichs et al. 2012 ) to what has already been implemented. 3 TypeCraft system description 3.1 Introduction TC is a linguistic web application specialized on the creation, retrieval and sharing of IGT. Its strength and weaknesses will be discussed and evaluated in order to explore the potential of linguistic services.

TC consists of a thin client running inside a web browser and a backend running on a remote server. From a user X  X  point of view, the only prerequisite for using TC is a modern standards-compliant browser.

TC is a multi-lingual database with the majority of corpora coming from less-studied languages. Figure 1 gives an overview over this material by representing the number of annotated words relative to language. TC is an open-ended resource which at the time of writing consists of a 100,000 word corpus distributed over 109 languages. 11
TC has at present 340 users. The majority are individual users working on self-defined projects. Part of the group of individual users are guided users, graduate students and sometimes participants of TC workshops. The remaining group of users are independent projects.

Corpora in TC are mono-lingual text corpora consisting of coherent texts or collections of sentences. They are either managed by projects or by individuals. Projects on TC belong to either external projects, like the Paunaka corpus 12 and the Upper Tanara corpus, 13 or belong to projects administered by TC. The Lule Sami corpus and the Akan corpus are TC project languages. For the corpora managed by individuals, we need to distinguish between free individual corpora, such as the Ganda 14 or the Brasilian Portuguese corpus, 15 and coordinated individual corpora, like the Runyankore-Rukiga 16 corpus which was created by three native speaker linguists who work together on different aspects of the corpus. Some of the corpora are public while others remain private, but none of the corpora are officially published.

TC, as a web application, consists of the TC editor, used for creating and editing of IGTs, and the TC wiki. The editor is seamlessly integrated into the wiki interface providing the user with an individual space for the administration of his private and shared texts, and which provides the possibility to embed annotated IGTs into wiki pages. The wiki is based on MediaWiki. 17 In addition to the functionalities just named the wiki serves as a general entry point and a collaboration tool.
Table 2 provides a short overview of the system X  X  main data handling functionalities. We distinguish four categories: Creation, Search, Dissemination and Export .

We will first briefly comment on dissemination and export, yet our focus is on data creation and search. These are also the two functions that we will self-evaluate. Next to data creation and data search, data dissemination is a central aim of the TC system. We perceive of TC as an IGT Bank which allows the trading of IGT type research data. TC texts and phrases are identified by their unique URL. This means that annotated texts, as well as individual IGTs can be exchanged on the web and shared across mobile technologies. When appearing in electronic publications, IGT can therefore be traced back to a corpus. The TC project supports an Open Data Access approach to the sharing of IGT and defines itself as an online Brokerage Service for IGTs.

The TC GUI supports data export directly from the TC Editor or from the search interface. Several or individual IGTs can be exported to Microsoft Word, OpenOffice Writer and LaTeX (also XML export is possible X  X e will come back to XML export in Sect. 5 ). To integrate an example into this paper we made use of the system X  X  LaTeX export. The result is shown below:
The example illustrates locative inversion in Ruyankore-Rukiga, a Bantu language spoken in Uganda. The translational and functional glosses, which belong to two distinct tiers in the TC Editor, appear as one line when exported. Although glossing on several tiers is conceptually more appropriate, linguistic publications often require the more condensed format illustrated above.

This brief remark on data dissemination and data export ends the general introduction to the TC system. Before we turn to data creation and search, we would like to present some of the more technical aspects of the system. Since MediaWiki is an external project we will focus on the core TC functionality, and how we have integrated it into the wiki. 3.2 Under the hood The TC editor client relies solely on HTML and JavaScript to provide the user interface. The client talks to the backend using AJAX and a private JSON-based protocol. The TC search interface is implemented as special wiki pages and has no relation to MediaWiki X  X  search, which can be used to search for pages as in any other MediaWiki-based wiki.

TC X  X  backend is a Java application. It is responsible for storing, validating and searching of annotated tokens. It also keeps track of static data such as languages, part of speech tags, glossing tags and global tags. The data is persisted into a relational database. There are two main types of data: individual and shared. Individual data is each user X  X  annotated data. The individual data references in turn the shared data. Figure 2 shows a simplified class diagram of the data model with individual data on the left and shared data on the right.

The basic building block is Text. Each Text can have any number of Phrases which can contain any number of Words and each Word can have any number of Morphemes. Every type of data has specific data properties, e.g., Text has a Language but no POSTag.

It is important to note that the shared data is a means to provide standards. For example, Language describes a language according to ISO 639-3, while POSTag and GlossTag define part of speech and glossing tags referencing GOLD (General Ontology for Linguistic Description) where possible.

A TC project presently under development concerns the support of text management. It will lead to an improved Metadata management on the one hand and new functionality for the annotation of referential chains and discourse topic shifts on the other.

Even though TC uses a relational database internally, it never exposes that database to the end user. All communication to the outside world is done through XML according to TC X  X  XML schema. The schema closely follows the classes shown in Fig. 2 , i.e., it describes texts containing phrases with words and morphemes, but it also allows for a collection of phrases without their respective texts. This is intended as a convenience when one is interested only in the annotated phrase data, e.g., single examples from large annotated texts. The XML schema is publicly accessible. 18 Figure 3 represents the XML for the Akan phrase: a ` kye  X  r  X  w n  X  ho ` ma  X  no `  X  He has written the letter.  X 
Annotation graphs (Bird and Liberman 2001 ) and graph algorithms have become more widely used to represent layers of annotations, but also to link annotations of different origin (Ide and Suderman 2012 ). For the later purpose, TC uses POIO API 19 which is an open source Python library to access and analyse language documentation data. In a cooperation with CIDLeS, 20 we convert TC XML into annotation graphs as defined in ISO 24612. POIO uses GrAF the  X  X raph Annotation Framework X  as pivot format (Ide and Suderman 2007 ). This is ongoing work, and will not further be described here.

The second development project currently on the way, addresses the limited interconnectivity to other applications. A public API will provide export and import of annotated data conforming to our XML schema or convertible to it, when using efficient methods of XML type conversion.

Export was successfully tested under a joint project with ELAN (Beermann et al. 2012 ). 3.3 Data creation We will now look at data creation by considering the functionalities provided by the TC online editor. The TC editor is specialised for the creation of IGT and can be accessed through the TC mediawiki. Figure 4 illustrates how an Akan IGT looks, seen from within the TC editor. 3.3.1 Data tokenisation and presentation The workflow is initiated by loading text data to the editor for sentence and morpheme tokenising. Sentence tokenisation is automatic while the initial decomposition of words into prospective morphemes is mostly done manually before the annotation table is instantiated. In this way morpheme segmentation informs the instantiation of the annotation interface. IGT annotation in TC is performed in a predefined table environment. Rows correspond to tiers which provide a succinct lay-out for the representation of distinct linguistic properties. The phrase tier represents the orthographic form of the annotation token. Since TC uses Unicode, every script that the users can input and display on their computers can be entered into the browser and is preserved in the database. TC gives access to an online IPA editor 21 which allows the easy input of phonemic transcriptions. TC allows the storage of original scripts as well as their Latin transliteration. At present the system has automated Pinyin transliteration of Mandarin Chinese, and an automated transliteration procedure for other scripts is technically possible. As illustrated in Fig. 4 , the orthographic tier is matched by a free-translation tier. The core of the TC word-level annotation table 22 consists of six tiers: the Word, Morph, Baseform, Meaning, Gloss and POS tier . While the morph tier is reserved for the representation of morphological surface forms, the baseform tier is a hybrid tier. It represents stems and the morphemic form of bound morphemes. We will come back to this point. The gloss tier is reserved for functional glosses and the POS tier for part of speech tags.

The columns of the annotation table align words and morphs vertically with their baseform, their gloss and POS specifications, which leads to a clean and easy-to-read representation of IGT form-function packages. Morph forms can receive zero to many glosses and values can be assigned as sets; they can occur in any order. The TC editor allows the morphological re-analysis and the addition or deletion of words during the annotation process for each table cell of the word and morph tier. While the workflow initial tokenisation initiates the set-up of the annotation interface it does not prevent that words and morphemes can be easily added or removed at any point. 3.3.2 Con fi dence and consistency representation Manual annotation is one of the methods of data generation which is inadvertently affected by human error and confidence issues. Needless to say, representational clarity leads to human errors reduction. Confidence management however requires additional measures. Incremental annotation is already a step in this direction. Annotation can now proceed in a cyclic fashion, so that glosses reflecting grammatical core knowledge can be done immediately whereas uncertain cases can be postponed. The effect of incremental annotation is that data consistency increases as the depth of annotation decreases, at least initially. From a tool point of view several measures can facilitate confidence management. A simple but important procedure that must be supported is note-taking. Important for individual users, notes are crucial under collaborative annotation, which TC supports. The TC editor features a note-taking field as part of the annotation table which is large enough to accommodate notes of several annotators. Notes are stored together with the annotations that they relate to and are rendered as part of the TC XML.
Confidence management depends crucially on access to information affecting glossing, and standards can be helpful in this connection. We would like to distinguish between object standards and social standards. TC tries to make use of both. Let us first look at object standards, starting with Metadata. The assignment of language IDs, which is one such Metadata, to text is an integral part of the workflow for a multi-lingual database like TC. Each corpus text needs to be identified as belonging to a particular language, a process which is regulated through the use of an imported external standard, the ISO-639-3 language code. Its implementation is designed such that the TC editor connects each text via its language ID directly to Ethnologue (Paul et al. 2013 ) which allows the direct look up of SIL compiled information for the language that the text represents.

Next to structural object standards(Metadata), we need to consider the conceptual object standards. Here TC insists on a pre-defined set of glosses which are rooted in the Leipzig Glossing Rules, but have been extended to meet the needs of annotation in a multi-lingual setting. Glosses have been grouped into classes such as: Agreement, Aspect, Case, Modality , to mention some. These tool internal standards are related to GOLD, which is the system X  X  external reference standard. GOLD is an ontology of grammatical types created by Farrar and Langendoen ( 2003 ). As an OWL ontology it presents grammatical properties in terms of categories and their relations.

Why are standards so important for confidence management? The idea here is that the enforcement of standards, de facto as well as de jure ones, facilitates the linguistic annotation process, including confidence management. The challenge that remains is that grammatical form-function relations seldom are of the 1-to-1 type, but especially multi-lingually seen, are 1-to-many relations. This leads to multiple cross-classifications. A participle, for example, may be more adjective-like in some languages and more verb-like in others. The latter happens in some Bantu languages where participles may give rise to dependent verb forms featuring full inflectional pattern (for example in Runyankore-Rukiga (Taylor 1985 )). It is this wealth of linguistic dependencies that makes it desirable for linguists to find data in a form that can be queried and compared, as Simons ( 2008 ) remarks.  X  X o achieve this we will need to embrace a full set of standards that will allow us to align all the puzzle pieces. X  (Simons 2008 ). TC offers at present GOLD 2010 as external referential frame to for example provide definitions for central concepts relying on citations found in the linguistic literature, or bibliographic resources which allow one to trace the origin of specific concepts and to identify their use. GOLD is only one of the external resources that can be used in this capacity, another suitable resource is the WALS online (Dryer and Haspelmath 2011 ). 23
A further means to facilitate confidence management are peer review mecha-nisms (social standards). To call on peer-review can be done in many ways, and in fact it is very much up to the individual linguists to decide what is interesting or compelling about a data set to evoke a public discussion. TC wiki facilitates annotations blogging, as one might call it, by allowing direct import of IGT data to the wiki using the TC editor.
 The TC project itself supports the creation of a set of TC wiki pages called Annotating [My language] and Annotation Akan is an example of such a blog. 24 TC also offers a template for writing sketch grammars, called Typological Features Template . The Ga feature template is an exemplification of how a sketch grammar can be written using a TC template. 25 Between TC users a popular way to manage uncertainty under annotation is the Ask-a-friend method. Users exchange IGT data with co-students and friends via mobile technologies. Similar techniques have been discussed in the e-learning context as successful means to engage learners and also in the present context the advantage of mobile annotation is apparent (Fig. 5 ). 3.4 Evaluation: Part 1 For the evaluation of TC we deem a usage-approach as the most relevant (Amar et al. 2008 ). Our evaluation is a single-system self-evaluation. Part 1 of the evaluation looks at the TC editor, part 2 at the TC search interface.

In addition to the self-evaluation presented here the system is under constant user evaluation. The latest survey can be accessed on TC. 26 One common practise for the evaluation of linguistic software is to use Best Practice Guidelines (Bird and Simons 2003 ; Nordhoff 2008 ). Yet, a potential problem lies in their character. They are guidelines written by experts which can be of the type  X  X his has worked for me X , or they are imperatives where should-statements target fieldwork practise, software use, editorial and formatting standards; in short everything of relevance to the field. Given this special character, Best Practice Guidelines must be general, and, as Nordhoff points out, can only describe values to which a user might or might not subscribe. For this reason Best Practice Guidelines are not very suitable as evaluation criteria.
 In order to calibrate our evaluation we would like to use the Feature-Functionality list from Table 1 in Sect. 1 . We will determine whether TC is useful in producing quality IGT (product-oriented evaluation) and useful as research instrument (process-oriented evaluation). Let us mention that we are aware of the circumstance that what constitutes quality of data can change with different practise and in different contexts. However, Sect. 1 of this paper discusses some of the criteria that we consider as essential, thus adding to the transparency of the evaluation.
 With that being said, let us now consider the TC editor.

TC offers an annotation interface that strives to visualise linguistic distinctions in a clear fashion. Yet, the system is not in all cases successful. While it makes some crucial distinctions such as between translation glosses (meaning) and functional glosses, and separates them from POS glosses, it offers no clearly defined locus for prosodic annotations. As a consequence annotators have chosen different solutions. Although not per se a problem, for collaborative corpora compiled by several annotators over time this leads to data inconsistency. A further factor contributing to data inconsistency is the hybrid character of the baseform tier. Stems have been interpreted as citation forms by some annotators, as morphological  X  X eep forms X  by others. Again this is not a problem as long as these choices are declared as part of the Metadata declaration, yet this is often not the case.

Future development needs to provide an improved GUI for the declaration of corpus specific properties which in addition to the Metadata declaration should be made accessible. An IGT annotation editor should facilitate the rendering of original scripts but also allow easy marking of prosodic features. More needs to be done in this respect.
Let us summarise: In the interest of facilitating the creation of citable IGT, we consider annotation editors with a predefined lay-out and a predefined tag-set an asset. Yet, future development should allow for customisation. For example the definition of several free translation tiers should be possible for the translation into a local lingua franca. Customisation of the annotation interface will have to lead to a customisation of IGT export, a topic which would lead us too far afield here.
Turning now to modelling annotation as a cyclic process, it seems to be well taken care of by TC. Annotations can be partial and tokenisations can be revised. Object and social standards are integrated into the overall design allowing a linked data approach to confidence management. In addition to the functionality already offered, global reanalysis of annotations should be made possible also from the GUI. At present the GUI only allows the replacement of individual annotation. Thus, in order to replace an annotation across a text, the user has to search for that annotation and then work through the search result replacing each annotation individually. That is very clumsy and needs to be changed.
A larger issue which has not received enough attention is the marking of IGT for annotator confidence. Flagging of data quality in general is at present not possible in TC. 3.5 TypeCraft search A TC search operates on phrases, which means that the result of a query is a phrase level representation. The search is parametrised such that a search result can either be represented as a list of sentences containing the search term or as interlinear glossed texts. Search results consisting of lists of sentences can be evaluated more easily than lines of concordances. Search results as lines of sentences allow a first quick scan of the data, while the IGT format gives the linguist access to the sentence internal annotations. Search results can be exported individually or as whole sets, and be stored on individual machines. As HTML files they can be browsed for further data exploration using general browser functionality.

TC allows for complex searches on several tiers from the search interface. Word or morpheme queries can relatively freely be combined with a search for specific glosses or combinations of glosses, co-occurring either in a phrase, or in a word. The latter distinction is useful when we want to compare tokens containing for example TAM markings on a single verb and distinguish it from TAM marking which is spread over the phrase in the form of a periphrastic construction.

Searches can be specified for specific domains, and they can target the user X  X  own data, as opposed to data that (s)he can edit, or data that is made public on TC. Search in group data serves, e.g., to establish inter-annotator consistency. For example search in the collaborative Runyankore-Rukiga 27 corpus on TC shows that the annotators disagree on the meaning of the morpheme -ire . The preferred annotation is PRF (perfective) aspect, but it is also annotated as PAST, ANT(erior) and STAT(ive). However, when the morpheme occurs in a negative context, 51 out of the 53 cases are annotated as perfective. 28 3.6 Evaluation: Part 2 As criteria for the evaluation of tool-based search, we would like to make use of the factors compiled by Bouda and Helmbrecht ( 2012 ). Since TC is not a file-based system, the phrasing of the criteria had to be changed slightly to become applicable. 1. Search results should be presented as interlinear text. 2. The user should be able to search on all existing tiers. 3. It should be possible to define relationships among search terms on individual 4. Fewer dialogue windows is to be preferred. 5. Search result export for future reference is desirable. 6. The user should be able to find the source utterance in its context. 7. Subsequent searches should be possible. 8. Search for sub-strings and regular expression search should be possible.
The TC search interface satisfies desiderata 1 X 5 but not 6. Yet, to integrate original text and annotated phrases in a setting that allows the inspection of sentences in their context is already on its way. We are also working on a better integration of aggregation functions into the user interface, and, addressing 7, we will in the future allow subsequent searches. At present the search interface indicates only the number of retrieved sentences. Concerning desideratum 8, we are not so certain that the search method really matters. What does matter is that the information crucial for the exploration of the stored material can be found in an efficient way. 4 Conclusion and the way ahead TC is a web-based linguistic data management and sharing tool. It allows for the creation, storage and exchange of IGT, which is a well established data format within philology and the structural and generative fields of linguistics. Novel about the TC project is the collaborative and Open Data Access approach to the creation of linguistic resources.

In this paper we have considered the contribution that TC can make to the creation of linguistically in-depth annotated data. Only quality data is likely to be exchanged by the linguistic research community, and we have discussed some of the linguistic considerations that are relevant to create data for linguistic reuse. Annotation for research purposes is incremental and requires confidence manage-ment. Active use of internal and external linguistic standards and of peer-review mechanisms as an integral part of the annotation process are further points that we have discussed.

In-depth annotated IGT from less-documented languages is a sought-after commodity, and here we have described ways of improving their quality through the use of web-based linguistic technology.

Tools meant for the exploration of less-documented languages should be low-tech solutions to allow non-computer-oriented linguists, language specialists and language communities to use them comfortably. TC values simplicity in design, where advanced features never get in the way of performing tasks with ease. It shares with all web-based applications that it can be used in an architecture together with other NLP tools, and efforts mapping TC XML into the GrAF (Graph Annotation Framework) standard, using POIO API (Ferreira et al. 2012 ), are on the way.

While it is the strength of NLP techniques to compensate by smart processing for lacking or partial information coming from IGT (Lewis and Xia 2010 ), it nevertheless in many cases is useful to work with IGT which provides POS annotations as a default. TC data is mostly rich in annotation and therefore the ideal source for grammar induction from less-resourced languages. We would like to mention Bender et al. ( 2012 ) and Hellan ( 2010 ) as interesting examples of work that uses NLP approaches for the further processing of IGT data using DELPH-IN technology. 29 An alternative platform is the NLTK 30 an NLP platform which allows the further processing of field data and which offers techniques that also work on small and noisy data sets (Bird 2009 ).
 References
 Dorothee Beermann  X  Pavel Mihaylov Abstract Interlinear Glossed Text (IGT) is a well established data format within philology and the structural and generative fields of linguistics. The best known format for an IGT is the one found in linguistic publications, where one line of text is followed by one line of glosses and one line of free translation. Although used in different functions, IGTs are ubiquitous in linguistic research and publications. Yet they also have been criticised for being fabricated and unreliable in some of their uses. However that might be, IGTs represent linguistic knowledge, and in particular for less-resourced languages, they are not rarely the only structured data available. Under the auspices of the Digital Humanities, linguists increasingly focus on the advantages of Semantic Web technologies. Presenting the modules and procedures of the web-based linguistic application TypeCraft (TC), we outline how the creation of IGTs can become an integral part of a shared linguistic methodology. Linguistic services have the potential of allowing efficient data management, and their strength lies in facilitating new forms of collaboration beyond social networking. They pave the way towards what one might call shared methodologies. In this paper we would like to discuss the linguistic value of web-based technology. By presenting the functionalities of TC and giving a detailed summary of online linguistic data cre-ation and retrieval, we will present external and internal criteria for a single system evaluation of TC centred on usage objectives.
 Keywords Digital Humanities  X  Web-based linguistic tools  X  Language data management  X  IGT  X  Shared linguistic methodologies  X  Single system usage evaluation 1 Introduction With more linguists interested in the description of endangered and less-described languages, we see a rising interest in the electronic creation and management of linguistic data, which is true also for fields outside of Language Documentation.
From the beginning, computers have played an increasing role in linguistic research, yet the way they were used before and the way they are used now has changed drastically. Enhanced by Semantic Web technologies, the possibilities offered by electronic data management have started to change the ways linguistic research is conducted. 1 In particular, modern approaches to Language Description and Documentation (LDD) would not have been thinkable without the technology that allows the creation, retrieval and storage of diverse data types. A field that aims to provide a comprehensive record of language constructions (Himmelmann 1998 ) is crucially dependent on software that supports the effort and an infrastructure that can sustain a sophisticated digital environment.

Gibbon ( 2008 ) envisions language documentation as a trans-disciplinary cooperation between phonetics, the descriptive fields of linguistics, speech and text technology and computational linguistics. Now, only a few years later, the interest in linguistic tools has grown beyond these fields. Electronic grammaticog-raphy (Nordhoff 2012 ) is a recent development uniting descriptive and computa-tional linguists interested in the integration of data exploration and grammar development, either in the form of descriptive grammars (Ameka et al. 2006 )orin the form of automated grammars (Bender et al. 2012 ).

For many years, linguistic tools such as Toolbox were  X  X ontraptions X  used primarily by descriptive linguists engaged in field work. Yet, carried by the success of LDD, new or upgraded tools were developed with new improved functionalities. This new generation of linguistic software has attracted new user groups (Schmidt 2010 ). With tools like FLEx 2 and ELAN 3 and on the basis of a growing availability of linguistic data, linguists in different sub-disciplines discover the potential of data management. In this way the interest in the exchange of data grows as well. Given that the majority of linguistic tools are designed as single-user desktop systems, the exchange of data is a problem. Only very recently online services start to appear that have the potential of facilitating the exchange of linguistic data from different applications (Ferreira et al. 2012 ).

The use of web-based technology for linguistic data management, also outside of the fields of phonetics and NLP, is by no means a new idea. In 2006 TypeCraft (TC) was presented as the first linguistic tool that allowed the creation, storage and exchange of IGT online (Beermann and Prange 2006 ). Also in the same year, EOPAS was launched (Schroeter and Thieberger 2006 ), however, while TC focuses on the creation of IGTs and their exchange, EOPAS has its main focus on the presentation and storage of multi-media data.

Although the Open Language Archives Community has been in existence since 2000, the idea of creating and trading IGT online was new then. When TC was released, the ramifications of Open Access for linguistic research data were still very much under discussion. How could the rights of the data owners and the data providers be protected, and how could one guarantee the quality of data that did not come from officially released databases, but from individual users interested in sharing their data online? These were only two of the questions that needed answers.
Under the auspices of the Digital Humanities (Gold 2012 ; Burdick et al. 2012 ) and the migration of all types of research relevant material into virtual environments, e-research became more widely endorsed. Now, using semantic web technologies, electronic research data can be made traceable and in this way citable very much like books and other on-paper publications. The next generation of linguistic tools will most likely be hybrid systems, partially offering tools for download, partially offering services online. This type of application is often referred to as Service Oriented Architecture (SOA). 4 The idea is that through the chaining of tools or through the construction of data bridges (Beermann et al. 2012 ), the researcher can select from among the available tools those that are best for the task at hand. Such a development requires not only the architecture but also the structural and conceptual standards to make data interchangeable and worth exchanging.

Having set the stage, this article now focuses on the question how useful already existing web based linguistic technology is in supporting the management of linguistic data. Linguistic data in the context of this article is Interlinear Glossed Text (IGT). In linguistics IGT is the most common form of data and as such an integral part of linguistic research and publications. Surrounded by linguistic prose, IGT generally is demarcated through indenting, numbering, and spacing above and under the example. One line of text is followed by one line of glosses, and a line with free translation completes the pattern. An IGT 5 occurs in isolation and normally lacks any index to where and when it occurred or any other information that would identify it as a particular instance of a language (Lehmann 2004a ). The theme that we would like to develop here is that IGT data is an important linguistic resource which warrants the creation of a research infrastructure supporting their wide-scale electronic exchange.

We start our discussion by reviewing the linguistic form and function of IGT. It is very well known that IGT is a  X  X ery-noisy X  type of data. Still, it is a valuable resource due to the fact that it relates in a systematic fashion grammatical forms to grammatical categories. IGT are linear linguistic representations of form-function packages, and while Bow et al. ( 2003 ) have looked at their formal properties, we here would like to consider their linguistic properties in some more detail. The paper is organised as follows. In Sect. 2 we describe the present day use of IGT. We will isolate those properties of IGTs that prevent them from being the prime linguistic resource that they could be. In Sect. 3 we outline how web-based linguistic technology might help to make IGTs a more reliable resource. As a concrete example of such technology, we present the TC application which we evaluate using different types of criteria. Section 4 concludes the paper. 2 Interlinear glossed text: problems and potential Since interlinear glosses became a standard for linguistic publications, very many IGTs have been created which, either in paper form or, more and more so, online, have become part of the scientific discourse. When IGTs occur in publications they normally do that as isolated sentences, and even when identified as belonging to a larger corpus, they normally are evaluated without any access to the broader linguistic context in which they occur. This is not necessarily a problem since the function of IGT in research and in linguistic publications is not uniform.
In the logical tradition, where linguists follow in the footsteps of the philosophical and mathematical sciences, an IGT is an idealised representation of the linguistic reality that the theory describes. The work of Louis Hjelmslev is an example of this approach, and of course Noam Chomsky X  X  work stands in this tradition. Authors like Ross, Postal, McCawley and Bach are notable early US representatives of this research tradition where the creation of linguistic data is based on linguistic expertise alone, leading to a characteristic style of exposition where IGTs serve as threads of the discussion. Lyons ( 1977 ) calls the corresponding type of linguistic data System-sentences .

The function of IGT within the empirically-oriented fields of linguistics is different. Here an IGT serves as a Data Sample . It might have been gathered through linguistic interviews or other elicitation methods. However, considering the format of the data alone, there are no principled differences between IGT across linguistic traditions. What is different is the emphasis that is put on the representativeness and authenticity of the data; this is where the real difference between the two main schools of linguistics seems to lie. 6
Recently linguistic data has come under scrutiny. Researchers from different linguistic fields have questioned its validity, and the integrity of theories that  X  X re built X  on this kind of data. From the psycholinguistic side it has been claimed that linguists are not (sufficiently) concerned with methods that regulate data collections. With Schu  X  tze ( 1996 ) as prime reference, it has been pointed out that IGTs which are mostly based on binary grammaticality judgements, should, like any other data which is based on human judgement, be exposed to empirical control in order to assure a reliable mode of their elicitation (Keller 2000 ; Keller and Asudeh 2007 ).
Also on the side of functional linguistics, in particular from the ranks of linguists working with LDD, methodological issues have been raised, calling for the standardisation of linguistic annotations and improved methods of data management and data provenance. The standards called for thus fall into two categories, the internal/conceptual and the structural/book-keeping standards (Chiarcos et al. 2012 ). Especially for IGTs occurring in publications on less researched languages, we notice long citation histories so that data provenance, which means in its simplest form that citation chains are recorded, becomes an important issue, which tends to be neglected by authors and editors alike (Lewis 2006 ). A further consideration is the format or structure of IGT. In a meticulous study, Bow et al. ( 2003 ) inspect linguistic glossing and propose a XML-based general-purpose model for the representation of IGTs.
 With the introduction of new methodologies and a rising access to data, what Kertsz and Rkosi ( 2012 ) call the  X  X tandard View Of Linguistic Data X  is changing. Reflections about the relation of language specific data to grammatical categories has led to questions of the adequacy of our terminological apparatus and the relation between terminology and linguistic theory, calling the latter into question (Haspelmath 2012 ).

In the present context, we will reflect on the relation between data and linguistic analysis from a methodological point of view. Given the right tools, linguistics today is in an excellent position to collect, organize, and analyse more data than ever before. In this context we would like to know how useful web based linguistic technologies can be.

Before we now turn to the discussion of IGTs, one more general remark seems in order. Text based linguistic data in its most traditional linguistic form resides in morpheme or word level annotated linguistic phrases. These IGTs are the result of symbolic processing, they are linear representations consisting of form-function packages. It is sometimes claimed that they constitute the empirical foundation that a linguistic analysis rests upon, yet they can hardly be that, since they are a form of linguistic analysis themselves. The joint publication of linguistic research and IGT corpus increases the validity of an analysis, as it allows us to test to which extent certain generalisations are supported by the data. Joint publication of analysis and data however does not prevent methodological chicken-and-egg situations, where annotations closely reflect a particular analysis, such that data analysis only provides what we intended to find. Nevertheless, publishing both research and research data is still a good idea, as it allows extended explorations and probing of the material that the research is built on. Is the data consistent, is it representative, these are two of the questions that need to be asked. And again, we would like to know whether modern linguistic tools can help the linguist to manage the relation between the data and its further analysis.
 We now turn to a more detailed discussion of IGTs.

As pointed out by Lewis ( 2006 ) in his discussion of the ODIN database most published linguistic data reflects the  X  X isplaycentric X  function of the IGT. Not only the typical system sentence but IGTs in general when embedded in linguistic prose are meant as a convenience to the reader. When extracted from their publication, their value tends to be minimal, since information essential to their understanding is given in the paper rather than through annotations of the examples themselves. But there are additional problems, some of which we would like to discuss by probing the linguistic information contained in a small set of IGTs which we exported from the ODIN database. We chose Akan 7 data for the simple reason that we can rely on our own informants for the evaluation of the examples, we furthermore chose examples coming from articles representing work from different fields of linguistics. The first example, presented here as (1), comes from Haspelmath ( 2001 ):
The second example is extracted from a paper by Ameka ( 2003 ) who cites Osam ( 1994 ): The third example is quoted in a manuscript by Wunderlich ( 2005 ).
 The fourth interlinear gloss comes from a manuscript by Drubig ( 2000 ): All authors use examples which are excerpted from the literature. This is not unusual, in fact, examples from African languages tend to have a long citation history. Thus example (3) goes back to Sedlak ( 1975 ), 142f and Stewart ( 1963 ), 145ff, as indicated by Wunderlich. Considering the examples (1) X (4) we notice:  X  Comparing (2) with (3) n X  is glossed as  X  X EF X  in (2) and  X  X hat X  in (3). According  X  In example (2), the verb ma X  9 meaning  X  X ake X , must, if a prosodic rendering of  X  In (3) the last vowel of p  X  nk  X   X  must be an open o.  X  In (3) the free translation of the object as his horse is inconsistent with the word-
Each of these points must be viewed in its larger linguistic context. Most IGTs lack part of speech (POS) annotations, which is not only a disadvantage for further linguistic processing, but also from a purely linguistic point of view questionable. IGTs are syntagmatic representations, and we just saw that the determiner n X  might have a distal meaning, that is, function as a demonstrative. The phenomenon is pervasive also in Akan. Akan verbs serve in prepositional functions, the verb ma  X  X ive X  from example (1) introduces beneficiary arguments and therefore often translates into  X  X or X , while relational nouns are used to express topological relations and in this function follow the noun that is being located. Creating IGTs means categorising words and ascribing functional classifications; to facilitate this process the annotation tool needs to implement the conceptual space which allows the expression of function and form in a conspicuous way.

A further issue apparent through inconsistent tone marking is that original text strings in IGTs might either be orthographic representations or phonemic renderings. Clear conventions do not exist. In Akan for example, the Akan orthography does not represent tone and neither do a majority of IGTs found in the linguistic literature on Akan. Yet, Akan features lexical as well as grammatical tone which makes it desirable to provide additionally a phonemic representation next to an orthographic representation. An IGT annotation editor should facilitate the rendering of original scripts, and needs to allow for the encoding of tone since grammar (and the lexicon) cannot be analysed without reference to prosodic features. At least for tone languages that seems true.

Turning now to the morphological analysis, we further notice relative to the examples (1) X (4) that:  X  The verb m X  X  receives no morphological analysis in (1).  X  Due to lack of word internal analysis, we miss the fact that the verb initial re in  X  The general lack of part of speech information makes it impossible to determine  X  In (3) the gloss 3sgP is ambiguous between three singular Personal Pronoun and
The points made in the discussion of example (1) X (4) are symptomatic of factors well-known from the literature. But what is the reason for IGTs to remain cursory? We have already mentioned their displaycentric function as one reason, a second reason is again mentioned in Thieberger and Berez ( 2012 ): IGT creation is time-consuming; efficient data management therefore often leads to cursory annotations. Here IGT annotation editors can have a facilitating function. Morphological segmentation must become not only easier but also more automated. Moving on, we not only can identify the reasons why IGT remain cursory, we can also identify good reasons why annotations must be incomplete (Gippert et al. 2006 ). In particular Mosel ( 2006 ) and Schultze-Berndt ( 2006 ) address questions relating to the creation of annotation as part of a linguistic discovery process. Since IGT is as much a result of linguistic research as it serves as its input, annotated data reflects a current stage of knowledge, which sheds a new light on the fact that IGTs are sometimes ambiguous and incomplete. Mosel ( 2006 ) stresses that IGTs develop incrementally. From a methodological point of view that means that annotation is not a linear but a cyclic process. Morphological segmentation might have to be revised, for example. An IGT editor therefore should not only allow for partial annotation, but also for flexible morphological tokenisation. Moreover annotations should more readily integrate the expression of uncertainty, for example in the form of notes that can be stored together with the annotations.

To systematise these factors mentioned and in order to integrate them more readily in the creation of IGT, we would like to summarise them using a tool-oriented view. Factors now become system features and methodological consider-ations translate into functionalities in Table 1 .

Table 1 is not meant to be comprehensive but indicative. It will be used for evaluation purposes in the following section, where we will try to find answers for the questions how useful the TC linguistic editor is in producing the output that we wish for (product-oriented evaluation), and how useful it is as a research instrument (process-oriented evaluation). We will also evaluate its search functionality using external standards.

TC is a web-based linguistic service, and its description and evaluation should allow us to better relate published expectations concerning the impact of SOA (Hinrichs et al. 2012 ) to what has already been implemented. 3 TypeCraft system description 3.1 Introduction TC is a linguistic web application specialized on the creation, retrieval and sharing of IGT. Its strength and weaknesses will be discussed and evaluated in order to explore the potential of linguistic services.

TC consists of a thin client running inside a web browser and a backend running on a remote server. From a user X  X  point of view, the only prerequisite for using TC is a modern standards-compliant browser.

TC is a multi-lingual database with the majority of corpora coming from less-studied languages. Figure 1 gives an overview over this material by representing the number of annotated words relative to language. TC is an open-ended resource which at the time of writing consists of a 100,000 word corpus distributed over 109 languages. 11
TC has at present 340 users. The majority are individual users working on self-defined projects. Part of the group of individual users are guided users, graduate students and sometimes participants of TC workshops. The remaining group of users are independent projects.

Corpora in TC are mono-lingual text corpora consisting of coherent texts or collections of sentences. They are either managed by projects or by individuals. Projects on TC belong to either external projects, like the Paunaka corpus 12 and the Upper Tanara corpus, 13 or belong to projects administered by TC. The Lule Sami corpus and the Akan corpus are TC project languages. For the corpora managed by individuals, we need to distinguish between free individual corpora, such as the Ganda 14 or the Brasilian Portuguese corpus, 15 and coordinated individual corpora, like the Runyankore-Rukiga 16 corpus which was created by three native speaker linguists who work together on different aspects of the corpus. Some of the corpora are public while others remain private, but none of the corpora are officially published.

TC, as a web application, consists of the TC editor, used for creating and editing of IGTs, and the TC wiki. The editor is seamlessly integrated into the wiki interface providing the user with an individual space for the administration of his private and shared texts, and which provides the possibility to embed annotated IGTs into wiki pages. The wiki is based on MediaWiki. 17 In addition to the functionalities just named the wiki serves as a general entry point and a collaboration tool.
Table 2 provides a short overview of the system X  X  main data handling functionalities. We distinguish four categories: Creation, Search, Dissemination and Export .

We will first briefly comment on dissemination and export, yet our focus is on data creation and search. These are also the two functions that we will self-evaluate. Next to data creation and data search, data dissemination is a central aim of the TC system. We perceive of TC as an IGT Bank which allows the trading of IGT type research data. TC texts and phrases are identified by their unique URL. This means that annotated texts, as well as individual IGTs can be exchanged on the web and shared across mobile technologies. When appearing in electronic publications, IGT can therefore be traced back to a corpus. The TC project supports an Open Data Access approach to the sharing of IGT and defines itself as an online Brokerage Service for IGTs.

The TC GUI supports data export directly from the TC Editor or from the search interface. Several or individual IGTs can be exported to Microsoft Word, OpenOffice Writer and LaTeX (also XML export is possible X  X e will come back to XML export in Sect. 5 ). To integrate an example into this paper we made use of the system X  X  LaTeX export. The result is shown below:
The example illustrates locative inversion in Ruyankore-Rukiga, a Bantu language spoken in Uganda. The translational and functional glosses, which belong to two distinct tiers in the TC Editor, appear as one line when exported. Although glossing on several tiers is conceptually more appropriate, linguistic publications often require the more condensed format illustrated above.

This brief remark on data dissemination and data export ends the general introduction to the TC system. Before we turn to data creation and search, we would like to present some of the more technical aspects of the system. Since MediaWiki is an external project we will focus on the core TC functionality, and how we have integrated it into the wiki. 3.2 Under the hood The TC editor client relies solely on HTML and JavaScript to provide the user interface. The client talks to the backend using AJAX and a private JSON-based protocol. The TC search interface is implemented as special wiki pages and has no relation to MediaWiki X  X  search, which can be used to search for pages as in any other MediaWiki-based wiki.

TC X  X  backend is a Java application. It is responsible for storing, validating and searching of annotated tokens. It also keeps track of static data such as languages, part of speech tags, glossing tags and global tags. The data is persisted into a relational database. There are two main types of data: individual and shared. Individual data is each user X  X  annotated data. The individual data references in turn the shared data. Figure 2 shows a simplified class diagram of the data model with individual data on the left and shared data on the right.

The basic building block is Text. Each Text can have any number of Phrases which can contain any number of Words and each Word can have any number of Morphemes. Every type of data has specific data properties, e.g., Text has a Language but no POSTag.

It is important to note that the shared data is a means to provide standards. For example, Language describes a language according to ISO 639-3, while POSTag and GlossTag define part of speech and glossing tags referencing GOLD (General Ontology for Linguistic Description) where possible.

A TC project presently under development concerns the support of text management. It will lead to an improved Metadata management on the one hand and new functionality for the annotation of referential chains and discourse topic shifts on the other.

Even though TC uses a relational database internally, it never exposes that database to the end user. All communication to the outside world is done through XML according to TC X  X  XML schema. The schema closely follows the classes shown in Fig. 2 , i.e., it describes texts containing phrases with words and morphemes, but it also allows for a collection of phrases without their respective texts. This is intended as a convenience when one is interested only in the annotated phrase data, e.g., single examples from large annotated texts. The XML schema is publicly accessible. 18 Figure 3 represents the XML for the Akan phrase: a ` kye  X  r  X  w n  X  ho ` ma  X  no `  X  He has written the letter.  X 
Annotation graphs (Bird and Liberman 2001 ) and graph algorithms have become more widely used to represent layers of annotations, but also to link annotations of different origin (Ide and Suderman 2012 ). For the later purpose, TC uses POIO API 19 which is an open source Python library to access and analyse language documentation data. In a cooperation with CIDLeS, 20 we convert TC XML into annotation graphs as defined in ISO 24612. POIO uses GrAF the  X  X raph Annotation Framework X  as pivot format (Ide and Suderman 2007 ). This is ongoing work, and will not further be described here.

The second development project currently on the way, addresses the limited interconnectivity to other applications. A public API will provide export and import of annotated data conforming to our XML schema or convertible to it, when using efficient methods of XML type conversion.

Export was successfully tested under a joint project with ELAN (Beermann et al. 2012 ). 3.3 Data creation We will now look at data creation by considering the functionalities provided by the TC online editor. The TC editor is specialised for the creation of IGT and can be accessed through the TC mediawiki. Figure 4 illustrates how an Akan IGT looks, seen from within the TC editor. 3.3.1 Data tokenisation and presentation The workflow is initiated by loading text data to the editor for sentence and morpheme tokenising. Sentence tokenisation is automatic while the initial decomposition of words into prospective morphemes is mostly done manually before the annotation table is instantiated. In this way morpheme segmentation informs the instantiation of the annotation interface. IGT annotation in TC is performed in a predefined table environment. Rows correspond to tiers which provide a succinct lay-out for the representation of distinct linguistic properties. The phrase tier represents the orthographic form of the annotation token. Since TC uses Unicode, every script that the users can input and display on their computers can be entered into the browser and is preserved in the database. TC gives access to an online IPA editor 21 which allows the easy input of phonemic transcriptions. TC allows the storage of original scripts as well as their Latin transliteration. At present the system has automated Pinyin transliteration of Mandarin Chinese, and an automated transliteration procedure for other scripts is technically possible. As illustrated in Fig. 4 , the orthographic tier is matched by a free-translation tier. The core of the TC word-level annotation table 22 consists of six tiers: the Word, Morph, Baseform, Meaning, Gloss and POS tier . While the morph tier is reserved for the representation of morphological surface forms, the baseform tier is a hybrid tier. It represents stems and the morphemic form of bound morphemes. We will come back to this point. The gloss tier is reserved for functional glosses and the POS tier for part of speech tags.

The columns of the annotation table align words and morphs vertically with their baseform, their gloss and POS specifications, which leads to a clean and easy-to-read representation of IGT form-function packages. Morph forms can receive zero to many glosses and values can be assigned as sets; they can occur in any order. The TC editor allows the morphological re-analysis and the addition or deletion of words during the annotation process for each table cell of the word and morph tier. While the workflow initial tokenisation initiates the set-up of the annotation interface it does not prevent that words and morphemes can be easily added or removed at any point. 3.3.2 Con fi dence and consistency representation Manual annotation is one of the methods of data generation which is inadvertently affected by human error and confidence issues. Needless to say, representational clarity leads to human errors reduction. Confidence management however requires additional measures. Incremental annotation is already a step in this direction. Annotation can now proceed in a cyclic fashion, so that glosses reflecting grammatical core knowledge can be done immediately whereas uncertain cases can be postponed. The effect of incremental annotation is that data consistency increases as the depth of annotation decreases, at least initially. From a tool point of view several measures can facilitate confidence management. A simple but important procedure that must be supported is note-taking. Important for individual users, notes are crucial under collaborative annotation, which TC supports. The TC editor features a note-taking field as part of the annotation table which is large enough to accommodate notes of several annotators. Notes are stored together with the annotations that they relate to and are rendered as part of the TC XML.
Confidence management depends crucially on access to information affecting glossing, and standards can be helpful in this connection. We would like to distinguish between object standards and social standards. TC tries to make use of both. Let us first look at object standards, starting with Metadata. The assignment of language IDs, which is one such Metadata, to text is an integral part of the workflow for a multi-lingual database like TC. Each corpus text needs to be identified as belonging to a particular language, a process which is regulated through the use of an imported external standard, the ISO-639-3 language code. Its implementation is designed such that the TC editor connects each text via its language ID directly to Ethnologue (Paul et al. 2013 ) which allows the direct look up of SIL compiled information for the language that the text represents.

Next to structural object standards(Metadata), we need to consider the conceptual object standards. Here TC insists on a pre-defined set of glosses which are rooted in the Leipzig Glossing Rules, but have been extended to meet the needs of annotation in a multi-lingual setting. Glosses have been grouped into classes such as: Agreement, Aspect, Case, Modality , to mention some. These tool internal standards are related to GOLD, which is the system X  X  external reference standard. GOLD is an ontology of grammatical types created by Farrar and Langendoen ( 2003 ). As an OWL ontology it presents grammatical properties in terms of categories and their relations.

Why are standards so important for confidence management? The idea here is that the enforcement of standards, de facto as well as de jure ones, facilitates the linguistic annotation process, including confidence management. The challenge that remains is that grammatical form-function relations seldom are of the 1-to-1 type, but especially multi-lingually seen, are 1-to-many relations. This leads to multiple cross-classifications. A participle, for example, may be more adjective-like in some languages and more verb-like in others. The latter happens in some Bantu languages where participles may give rise to dependent verb forms featuring full inflectional pattern (for example in Runyankore-Rukiga (Taylor 1985 )). It is this wealth of linguistic dependencies that makes it desirable for linguists to find data in a form that can be queried and compared, as Simons ( 2008 ) remarks.  X  X o achieve this we will need to embrace a full set of standards that will allow us to align all the puzzle pieces. X  (Simons 2008 ). TC offers at present GOLD 2010 as external referential frame to for example provide definitions for central concepts relying on citations found in the linguistic literature, or bibliographic resources which allow one to trace the origin of specific concepts and to identify their use. GOLD is only one of the external resources that can be used in this capacity, another suitable resource is the WALS online (Dryer and Haspelmath 2011 ). 23
A further means to facilitate confidence management are peer review mecha-nisms (social standards). To call on peer-review can be done in many ways, and in fact it is very much up to the individual linguists to decide what is interesting or compelling about a data set to evoke a public discussion. TC wiki facilitates annotations blogging, as one might call it, by allowing direct import of IGT data to the wiki using the TC editor.
 The TC project itself supports the creation of a set of TC wiki pages called Annotating [My language] and Annotation Akan is an example of such a blog. 24 TC also offers a template for writing sketch grammars, called Typological Features Template . The Ga feature template is an exemplification of how a sketch grammar can be written using a TC template. 25 Between TC users a popular way to manage uncertainty under annotation is the Ask-a-friend method. Users exchange IGT data with co-students and friends via mobile technologies. Similar techniques have been discussed in the e-learning context as successful means to engage learners and also in the present context the advantage of mobile annotation is apparent (Fig. 5 ). 3.4 Evaluation: Part 1 For the evaluation of TC we deem a usage-approach as the most relevant (Amar et al. 2008 ). Our evaluation is a single-system self-evaluation. Part 1 of the evaluation looks at the TC editor, part 2 at the TC search interface.

In addition to the self-evaluation presented here the system is under constant user evaluation. The latest survey can be accessed on TC. 26 One common practise for the evaluation of linguistic software is to use Best Practice Guidelines (Bird and Simons 2003 ; Nordhoff 2008 ). Yet, a potential problem lies in their character. They are guidelines written by experts which can be of the type  X  X his has worked for me X , or they are imperatives where should-statements target fieldwork practise, software use, editorial and formatting standards; in short everything of relevance to the field. Given this special character, Best Practice Guidelines must be general, and, as Nordhoff points out, can only describe values to which a user might or might not subscribe. For this reason Best Practice Guidelines are not very suitable as evaluation criteria.
 In order to calibrate our evaluation we would like to use the Feature-Functionality list from Table 1 in Sect. 1 . We will determine whether TC is useful in producing quality IGT (product-oriented evaluation) and useful as research instrument (process-oriented evaluation). Let us mention that we are aware of the circumstance that what constitutes quality of data can change with different practise and in different contexts. However, Sect. 1 of this paper discusses some of the criteria that we consider as essential, thus adding to the transparency of the evaluation.
 With that being said, let us now consider the TC editor.

TC offers an annotation interface that strives to visualise linguistic distinctions in a clear fashion. Yet, the system is not in all cases successful. While it makes some crucial distinctions such as between translation glosses (meaning) and functional glosses, and separates them from POS glosses, it offers no clearly defined locus for prosodic annotations. As a consequence annotators have chosen different solutions. Although not per se a problem, for collaborative corpora compiled by several annotators over time this leads to data inconsistency. A further factor contributing to data inconsistency is the hybrid character of the baseform tier. Stems have been interpreted as citation forms by some annotators, as morphological  X  X eep forms X  by others. Again this is not a problem as long as these choices are declared as part of the Metadata declaration, yet this is often not the case.

Future development needs to provide an improved GUI for the declaration of corpus specific properties which in addition to the Metadata declaration should be made accessible. An IGT annotation editor should facilitate the rendering of original scripts but also allow easy marking of prosodic features. More needs to be done in this respect.
Let us summarise: In the interest of facilitating the creation of citable IGT, we consider annotation editors with a predefined lay-out and a predefined tag-set an asset. Yet, future development should allow for customisation. For example the definition of several free translation tiers should be possible for the translation into a local lingua franca. Customisation of the annotation interface will have to lead to a customisation of IGT export, a topic which would lead us too far afield here.
Turning now to modelling annotation as a cyclic process, it seems to be well taken care of by TC. Annotations can be partial and tokenisations can be revised. Object and social standards are integrated into the overall design allowing a linked data approach to confidence management. In addition to the functionality already offered, global reanalysis of annotations should be made possible also from the GUI. At present the GUI only allows the replacement of individual annotation. Thus, in order to replace an annotation across a text, the user has to search for that annotation and then work through the search result replacing each annotation individually. That is very clumsy and needs to be changed.
A larger issue which has not received enough attention is the marking of IGT for annotator confidence. Flagging of data quality in general is at present not possible in TC. 3.5 TypeCraft search A TC search operates on phrases, which means that the result of a query is a phrase level representation. The search is parametrised such that a search result can either be represented as a list of sentences containing the search term or as interlinear glossed texts. Search results consisting of lists of sentences can be evaluated more easily than lines of concordances. Search results as lines of sentences allow a first quick scan of the data, while the IGT format gives the linguist access to the sentence internal annotations. Search results can be exported individually or as whole sets, and be stored on individual machines. As HTML files they can be browsed for further data exploration using general browser functionality.

TC allows for complex searches on several tiers from the search interface. Word or morpheme queries can relatively freely be combined with a search for specific glosses or combinations of glosses, co-occurring either in a phrase, or in a word. The latter distinction is useful when we want to compare tokens containing for example TAM markings on a single verb and distinguish it from TAM marking which is spread over the phrase in the form of a periphrastic construction.

Searches can be specified for specific domains, and they can target the user X  X  own data, as opposed to data that (s)he can edit, or data that is made public on TC. Search in group data serves, e.g., to establish inter-annotator consistency. For example search in the collaborative Runyankore-Rukiga 27 corpus on TC shows that the annotators disagree on the meaning of the morpheme -ire . The preferred annotation is PRF (perfective) aspect, but it is also annotated as PAST, ANT(erior) and STAT(ive). However, when the morpheme occurs in a negative context, 51 out of the 53 cases are annotated as perfective. 28 3.6 Evaluation: Part 2 As criteria for the evaluation of tool-based search, we would like to make use of the factors compiled by Bouda and Helmbrecht ( 2012 ). Since TC is not a file-based system, the phrasing of the criteria had to be changed slightly to become applicable. 1. Search results should be presented as interlinear text. 2. The user should be able to search on all existing tiers. 3. It should be possible to define relationships among search terms on individual 4. Fewer dialogue windows is to be preferred. 5. Search result export for future reference is desirable. 6. The user should be able to find the source utterance in its context. 7. Subsequent searches should be possible. 8. Search for sub-strings and regular expression search should be possible.
The TC search interface satisfies desiderata 1 X 5 but not 6. Yet, to integrate original text and annotated phrases in a setting that allows the inspection of sentences in their context is already on its way. We are also working on a better integration of aggregation functions into the user interface, and, addressing 7, we will in the future allow subsequent searches. At present the search interface indicates only the number of retrieved sentences. Concerning desideratum 8, we are not so certain that the search method really matters. What does matter is that the information crucial for the exploration of the stored material can be found in an efficient way. 4 Conclusion and the way ahead TC is a web-based linguistic data management and sharing tool. It allows for the creation, storage and exchange of IGT, which is a well established data format within philology and the structural and generative fields of linguistics. Novel about the TC project is the collaborative and Open Data Access approach to the creation of linguistic resources.

In this paper we have considered the contribution that TC can make to the creation of linguistically in-depth annotated data. Only quality data is likely to be exchanged by the linguistic research community, and we have discussed some of the linguistic considerations that are relevant to create data for linguistic reuse. Annotation for research purposes is incremental and requires confidence manage-ment. Active use of internal and external linguistic standards and of peer-review mechanisms as an integral part of the annotation process are further points that we have discussed.

In-depth annotated IGT from less-documented languages is a sought-after commodity, and here we have described ways of improving their quality through the use of web-based linguistic technology.

Tools meant for the exploration of less-documented languages should be low-tech solutions to allow non-computer-oriented linguists, language specialists and language communities to use them comfortably. TC values simplicity in design, where advanced features never get in the way of performing tasks with ease. It shares with all web-based applications that it can be used in an architecture together with other NLP tools, and efforts mapping TC XML into the GrAF (Graph Annotation Framework) standard, using POIO API (Ferreira et al. 2012 ), are on the way.

While it is the strength of NLP techniques to compensate by smart processing for lacking or partial information coming from IGT (Lewis and Xia 2010 ), it nevertheless in many cases is useful to work with IGT which provides POS annotations as a default. TC data is mostly rich in annotation and therefore the ideal source for grammar induction from less-resourced languages. We would like to mention Bender et al. ( 2012 ) and Hellan ( 2010 ) as interesting examples of work that uses NLP approaches for the further processing of IGT data using DELPH-IN technology. 29 An alternative platform is the NLTK 30 an NLP platform which allows the further processing of field data and which offers techniques that also work on small and noisy data sets (Bird 2009 ).
 References
