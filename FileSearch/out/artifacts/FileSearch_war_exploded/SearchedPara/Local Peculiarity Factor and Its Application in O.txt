 Peculiarity oriented mining (POM), aiming to discover pecu liarity rules hidden in a dataset, is a new data mining method. In the p ast few years, many results and applications on POM have been re-ported. However, there is still a lack of theoretical analys is. In this paper, we prove that the peculiarity factor (PF), one of the m ost important concepts in POM, can accurately characterize the pecu-liarity of data with respect to the probability density func tion of a normal distribution, but is unsuitable for more general dis tributions. Thus, we propose the concept of local peculiarity factor (LP F). It is proved that the LPF has the same ability as the PF for a norma l distribution and is the so-called  X  -sensitive peculiarity description for general distributions. To demonstrate the effectivene ss of the LPF, we apply it to outlier detection problems and give a new o ut-lier detection algorithm called LPF-Outlier. Experimenta l results show that LPF-Outlier is an effective outlier detection alg orithm. H.2.8 [ Database Applications ]: Data mining; I.2.6 [ Learning ]: Knowledge acquisition Algorithms, Theory data mining, peculiarity factor, local peculiarity factor ,  X  -sensitive peculiarity description, outlier detection
A fundamental task of data mining is to discover useful knowl -edge hidden in a dataset. The knowledge may be expressed in many forms, such as association rules, classification rules , excep-tion rules, clusters, frequent patterns, trends and so on [1 3, 15]. Zhong et al . considered a new type of knowledge, called pecu-liarity rules, which may be hidden in a small subset of record s in a large dataset [23]. The associated peculiarity oriented m ining (POM) aims to find peculiarity rules by focusing on a small sub -set of interesting data called peculiar data. Peculiar data have two essential properties, one is that they represent cases desc ribed by a relatively small number of objects, and the other is that th ey are very different from other objects in the dataset [15].

The necessity and importance of studying peculiar data and p e-culiarity rules can be seen from many real world problems. In many situations, datasets are collected with data polluted by no ises from hardware systems, human factors or other malfunctions. Det ecting the noises from polluted data is an important preprocessing step. This may be done by considering noises as peculiar data. It ma y also happen that only a small sub-dataset consisting of data very different from others is interesting and important to a part icular ap-plication. For example, in the stock market supervising onl y a small subset of abnormal data is interesting and important to the o rgani-zation of securities regulatory. These abnormal data are pe culiar data in the dataset. In fact, peculiar data have been interpr eted in many terms, such as outliers, data points with very small pro babil-ity density values and so on. POM is therefore closely relate d to other data mining or machine learning approaches, includin g the outlier detection and the probability density estimation.
Studies of POM have produced useful theoretical results and ap-plications. Zhong et al . used the RVER (reverse variant entity-relationship) model to represent peculiar data and concept ual rela-tionships among peculiar data discovered from multi-datab ase [23]. The model has been applied to mine peculiarity rules in many d ata-bases, such as Japan-survey, amino-acid, weather, super-m arket, hepatitis, fMRI brain images and EEG brain waves [16, 21, 22] . The results show that POM is an effective data mining method. A preliminary analysis of the peculiarity factor (PF), one of the most important concepts in POM, was given in [20], which demonstr ates that the PF indeed reflects our intuitive understanding of th e two properties of peculiar data. The concept and a framework of r ela-tional POM were proposed in [21], which combines results fro m relational mining and POM at the record level. Techniques fr om inductive logic programming for relational inductive lear ning [18] were used in this framework. A more systematic and detailed s tudy of relational POM based on the record PF was given in [15]. Par tic-ularly, two basic tasks of relational POM, namely, descript ion and explanation, were discussed. A prototype system was implem ented to find peculiar records and first-order peculiarity rules fr om mul-tiple relations. The analysis of a database from China Stati stics Yearbook by using the system has produced interesting pecul iarity rules [15].

When compared with applications of POM, there is still a lack of theoretical and quantitative analysis of the sensitivity, reasonability, and applicability of the PF. In this paper, we present a theor etical analysis of the PF. We prove that, from the viewpoint of low fr e-quency, the PF can characterize accurately the peculiar dat a of a normal distribution, but is incapable of describing the pec uliar data of general distributions. To resolve this difficulty, we int roduce the concept of local peculiarity factor (LPF). It is shown that t he LPF has the same ability as the PF in describing the peculiar data of a normal distribution. In addition, it is the so-called  X  -sensitive pecu-liarity description with respect to a more general distribu tion. We apply LPF to outlier detection problems and give an outlier d etec-tion algorithm called LPF-Outlier. Experimental results o n a syn-thetic dataset show that the LPF can characterize the peculi ar data of a general distribution more accurately than the PF. Exper imental results on several real-world datasets demonstrate that th e proposed LPF-Outlier is an effective outlier detection method.

The rest of the paper is organized as follows. In Section 2, we first analyze the original peculiarity factor and then propo se the concept of local peculiarity factor. Then we prove that the l ocal peculiarity factor has two desirable properties. Finally w e propose the LPF-Outlier algorithm. In Section 3, we report the resul ts from a set of extensive experiments by using both synthetic and re al-world datasets. Conclusions are given in Section 4. Due to th e page limit, we only provide the proof of one theorem in the Appendi x.
By extending the concept of peculiarity factor (PF), we intr o-duce the notion of local peculiarity factor (LPF). For conve nience, we first give some explanations and notations. In the followi ng dis-cussions, unless noted, we only discuss the PF and the LPF for a one-dimensional distribution. We mainly consider the attr ibute PF and the attribute LPF, since the record PF and the record LPF c an be easily obtained from attribute ones. We suppose that p ( x ) is the probability density function (PDF) of a one-dimensional co ntinu-ous random variable, and D : R  X  R  X  R is a continuous and shift-invariant conceptual distance on R , that is, D ( x represented by a function of | x 1  X  x 2 | , x 1 , x 2  X  R . We denote  X  ( | x 1  X  x 2 | ) = D ( x 1 , x 2 ) and further suppose that  X  is strictly increasing. Obviously, D ( x 1 , x 2 ) =  X  ( | x 1  X  x 2 | ) = | x with  X  &gt; 0 are such conceptual distances.
A central notion of peculiarity oriented mining (POM) is the pe-culiarity factor (PF) introduced by Zhong et al. [15, 24]. In particu-lar, two levels of peculiarity can be identified, representi ng attribute peculiarity and record peculiarity.

D EFINITION 1. ([15, 24]) Suppose that { Z 1 , Z 2 ,  X  X  X  , Z sample set with n points and each point Z i = ( Z i 1 , Z is described by attributes A 1 , A 2 ,  X  X  X  , A m . Then for the dataset, the attribute PF of the attribute value Z ij is defined by: where  X  is a parameter. And for the dataset, the record PF of the point Z i is defined by: where  X  j is the weight of attribute A j , and P F ( Z ij Eq. (1).

The peculiarity factor is determined by the parameter  X  , which may be adjusted by users and  X  = 0 . 5 is used as default. For the record PF, we also need the weights  X  j  X  X  for attributes, which are given by users and  X  j = 1 is used as default.
 In general, one can define the record PF as follows:
P F ( Z i ) = That is, we use general p -norm instead of Euclidean distance in Eq. (2). This is more consistent with the attribute PF. But if the attribute PF can accurately describe the peculiarity of the data on each attribute, we can simply define the record PF of a point by the weighted sum of the attribute PF values on all attributes, th at is, Meanwhile the record PF given by Eq. (3) has a good property. That is, for a one-dimensional data point, its attribute PF i s same to its record PF.

In the following discussions of this paper, we use Eq. (3) to g et the record level PF from the attribute level PF and briefly ref er to the record PF as PF.

In order to gain more insights about the attribute PF, we can a na-lyze it from the viewpoint of statistics. Thus we first genera lize the definition of the PF to the case of a one-dimensional continuo us distribution.

D EFINITION 2. Suppose that P is a one-dimensional distribu-tion with continuous PDF p ( x ) , then for the distribution, the PF of any point x 0 sampled from P is defined by:
The idea hidden in Definition 2 is same to that in Definition 1, and we only use the one-dimensional continuous distributio n and the integral to replace the dataset and the sum in Definition 1 , re-spectively. Thus we can use the PF given by Definition 2 to anal yze the property of the attribute PF from the viewpoint of statis tics. For a normal distribution, we have the following important theo rem.
T HEOREM 1. For a data point sampled from a one-dimensional normal distribution, its PF value strictly increases with i ts concep-tual distance to the mean of the distribution.

From Theorem 1, we know that for a normal distribution and a given proper conceptual distance, data points further away from the mean have larger PF values. Such points have lower probabili ties and should be considered to be more peculiar than other data p oints Figure 1: The PDF values and PF values (  X  = 1 or  X  = 0 . 5 ) of data sampled from the normal distribution N (0 , 4) . closer to the mean. It follows that the PF correctly reflects o ur in-tuitive interpretation of peculiarity for a normal distrib ution. That is, for a normal distribution, the PF can indicate the locati on of a point in the distribution, which is closely related to the fr equency-based interpretation of peculiarity. Furthermore, from Th eorem 1 we know that the PF can indicate the location of any point in a n or-mal distribution. Thus the PF can give a description of the PD F from the viewpoint of data analysis. The description is impo rtant and valuable, since in most practical problems there are onl y some data points sampled from an unknown distribution and the est ima-tion of the PDF is very important but quite difficult.
 We demonstrate the ideas of Theorem 1 by a concrete example. We sampled 2000 data points from the normal distribution N (0 , 4) and calculated their PDF values and PF values. Since we only w ant to illustrate the relation between PDF and PF values, which i s de-termined only by the relative magnitude of these values, we c om-pressed these PF values into (0,0.25] and plotted PDF values and PF values (with  X  = 1 or  X  = 0 . 5 in Eq. (1)) in one figure, as shown in Fig. 1. We can see that points with smaller PDF values indeed have larger PF values.

In general, points sampled from a non-normal distribution m ay not have the property given in Theorem 1. This can be illustra ted by another example.
 E XAMPLE 1. Let X  X  consider a mixed normal distribution with PDF p ( x ) , p ( x ) = 1 2 ( N (2 , 1) + N (  X  2 , 1)) = ( x +2) 2 2 ) . For the distribution, there exist data points which have both smaller PDF values and smaller PF values. That is, the PF does not necessarily have higher values for lower frequent d ata points, and hence does not reflect our intuitive interpretat ion of peculiarity.

To illustrate the inability of the PF in characterizing pecu liar data of the distribution 1 2 ( N (2 , 1) + N (  X  2 , 1)) , we sampled 2000 data points and plotted their PDF values and PF values in Fig. 2, wh ere PF values were calculated with  X  = 1 or  X  = 0 . 5 in Eq. (1), and compressed into (0,0.25]. We can see that points close to the origin have smaller PDF values and smaller PF values, and poi nts with their absolute values larger than 4 have smaller PDF val ues and larger PF values. Hence the PF cannot describe the freque ncy based interpretation of peculiarity.

From Fig. 2, we observe that for points near the origin most sa m-Figure 2: The PDF values and PF values (  X  = 1 or  X  = 0 . 5 ) of data sampled from the distribution 1 2 ( N (2 , 1) + N (  X  2 , 1)) . ple data are near to them, but for points with absolute values larger than 4 almost half sample data are far from them. Hence the PF value of a point with conceptual distances to all the other sa mple data cannot describe its location in the distribution accur ately. This is the reason for the inability of the PF in characterizing th e PDF of general distributions. Intuitively the PF value with con ceptual distances between the point and its near neighbors can indic ate its location in the distribution. Justly we review the result of Theo-rem 1 for a unimodal non-normal distribution. We know that a point sampled from the distribution does not necessarily ha ve the property of Theorem 1. But a bit modification on the PF can make it have the property. We divide the distribution into two reg ions, the left region and the right region of the peak value. If we consi der the PF only in one of the regions, that is, the PF value of a point sa m-ple from the left region is defined only in the left region and t he PF of a point sample from the right region is defined only in the ri ght region, one can prove that Theorem 1 is still true for each reg ion. This shows that by defining the PF locally it can have more good properties. These motivated us to improve the definition of t he PF in a local manner. Consider first the following theorem.

T HEOREM 2. Suppose that c 0  X  (0 , 1] is a constant and p ( x ) is a continuous PDF of a one-dimensional distribution. For a ny x with p ( x 0 ) &gt; 0 , the sum of its conceptual distances to all points belonging to an interval with its probability equal to c 0 Then the function F ( x 0 ,c 0 ) reaches its minimum value at a = b , that is,
From Theorem 2 it can be seen that for a point of a distribution and a given parameter c 0 , the interval minimizing the sum of the weighted conceptual distances from the point to all its near neigh-bor points belonging to an interval with probability c 0 is centered at the point. The point x 0 and the constant c 0 can determine uniquely the interval. This result enables us to introduce the concep t of local peculiarity factor (LPF) for a one-dimensional continuous distribu-tion.

D EFINITION 3. Suppose that P is a one-dimensional distribu-tion with continuous PDF p ( x ) and c 0  X  (0 , 1] is a constant, then the attribute LPF of a point x 0 sampled from P is defined by: where a satisfies
The constant c 0 is the probability of all the data points selected to describe the peculiarity of a point, which introduces a loca l manner to improve the definition of the PF. Definition 3 is a local vers ion of Definition 2, and if c 0 = 1 , we have a = +  X  . In the case, for a one-dimensional continuous distribution the attribu te LPF is equivalent to the attribute PF.

For a one-dimensional normal distribution, the attribute L PF has the same property as the PF.

T HEOREM 3. For any c 0  X  (0 , 1] , the attribute LPF value of a point sampled from a one-dimensional normal distribution s trictly increases with its conceptual distance to the mean of the dis tribu-tion.
 From Theorem 3, we know that for any c 0  X  (0 , 1] , the attribute LPF value of a point sampled from a normal distribution can in di-cates its position in the distribution, namely, a point with a smaller PDF value has a larger attribute LPF value and is farther to th e mean point. For the distribution, the attribute LPF with any c has the same ability in describing the peculiar data as the PF . For more general distributions, by selecting a proper c 0 , the attribute LPF can describe their PDF at any accuracy. This requires us t o introduce first the notion of  X  X escription at any accuracy X .
D EFINITION 4. Suppose that P is a one-dimensional distribu-tion with continuous PDF p ( x ) , f is a description of P and  X  &gt; 0 is a given constant. If for all x 1 , x 2 with p ( x 1 )  X  p ( x inequality f ( x 1 ) &lt; f ( x 2 ) holds, f is called an  X  -sensitive pecu-liarity description of P .

The parameter  X  can be adjusted by users according to the re-quirement of the problem or their own prior knowledge on the p rob-lem. For different problems or different aspects of applica tions, we are always required to describe one distribution with diffe rent pre-cisions, which corresponds to different requirements on sa mpling from the distribution. This idea is captured by introducing a param-eter  X  into the definition. The smaller the  X  value, the more accurate the description, which implies more sample data needed. The ef-fect of all data points with the PDF value smaller than  X  is ignored in an  X  -sensitive peculiarity description.

With the definition of  X  -sensitive peculiarity description, the at-tribute LPF has the following property.

T HEOREM 4. For a one-dimensional distribution P with con-tinuous PDF p ( x ) and any given  X  &gt; 0 , there exists a constant c 0  X  (0 , 1] such that for any c  X  (0 , c 0 ) the attribute LPF is an  X  -sensitive peculiarity description of P .

From Theorem 4 we know that the attribute LPF can describe the PDF of a distribution at any accuracy, which is determine d by the parameter  X  . Generally speaking, for a given distribution the smaller the parameter  X  , the smaller the constant c 0 , and hence the more accurate the description.

As given in Theorem 3 and Theorem 4, the attribute LPF for a one-dimensional distribution has better statistical pro perties than the PF. Thus we can improve the definitions of the attribute PF and the record PF for a dataset by introducing the idea of localit y. For the case of a dataset, the constant c 0 in Eq. (4) is replaced by a fixed number of neighbor points and the integral is replaced by a su m.
D EFINITION 5. Suppose that { Z 1 , Z 2 ,  X  X  X  , Z n } is a sample set with n points and each point Z i = ( Z i 1 , Z i 2 ,  X  X  X  , Z scribed by attributes A 1 , A 2 ,  X  X  X  , A m . Then for the dataset, the attribute LPF of the attribute value Z ij is defined by: where  X  is a parameter and N k ( Z ij ) is the set of k near neighbors of Z ij in the set { Z 1 j , Z 2 j ,  X  X  X  , Z nj } , that is, N k near neighbors of Z i on attribute A j . And the record LPF of the point Z i is defined by: where  X  j is the weight of attribute A j , and LP F ( Z ij Eq. (5).

The above definition gives a large class of LPF measures deter -mined by the weights of attributes. Those weights value can b e adjusted by users to derive a particular LPF measure, and  X  is used as default.

The introduction of the LPF enables us to identify peculiar d ata at both the attribute and the record level. As shown in the nex t section, they can easily be incorporated into a peculiar dat a identi-fication algorithm. Similar to the PF, we briefly refer to the r ecord LPF as LPF. For a one-dimensional data point its attribute LP F is also same to its record LPF.
We can design many LPF based algorithms for many data min-ing problems. As an application of peculiar data identificat ion, we use it to solve outlier detection problems. Outliers are a ki nd of peculiar data and outlier detection is a very important prob lem in data mining aiming to find outliers. There are already many st udies on the problem [1, 4, 7, 8, 9, 10, 11, 17, 19]. We give an outlier detection algorithm called LPF-Outlier in Algorithm 1. Its compu-tational complexity is O (( k + log n ) mn ) , which is acceptable for many real world problems.
 Algorithm 1 The LPF-Outlier algorithm for outlier detection 1: Input dataset { Z 1 , Z 2 ,  X  X  X  , Z n } , the near neighbor parameter 2: Output q outliers. 3: for i = 1 to n do 4: calculate the LPF value of Z i , LP F ( Z i ) , by Eq. (5) and 5: end for 6: Sort these LP F ( Z i ) s in descending order and mark sample Figure 3: The PDF values and LPF values ( k = 700 , and  X  = 1 or  X  = 0 . 5 ) of data sampled from the distribution 1 2 ( N (2 , 1) + N (  X  2 , 1)) .
Unless noted, in the following experiments we select D ( Z = | Z ij  X  Z ij |  X  ,  X  = 1 and  X  j = 1 to calculate PF values and LPF values.
For a dataset with its generating PDF known, we intend to il-lustrate that a data point with a smaller PDF value has a large LPF value, and that the LPF can describe the PDF more accurate ly than the PF. We sampled 2000 data points from the distributio n ( N (2 , 1) + N (  X  2 , 1)) and calculated their PDF values and LPF values to demonstrate that the LPF can describe the PDF accur ately. We selected  X  = 1 or  X  = 0 . 5 in Eq. (6) to calculate LPF values and compressed them into the range (0, 0.25]. The PDF values and LPF values with k = 700 and k = 900 are plotted in Fig. 3 and Fig. 4, respectively. It can be seen that for this distrib ution, data points with smaller PDF values have larger LPF values. Hence LPF can really describe the peculiarity of low frequency data po ints. By comparing Fig. 2 to Fig. 3 and Fig. 4, we can see that the LPF can more accurately characterize the PDF of the distribution th an the PF.

To more accurately demonstrate the fact that data points wit h smaller PDF values have larger LPF values, we sort these PDF v al-ues in ascending order and LPF values in descending order, an d compare the two ordered sequences. A usual measure between t wo rankings in learning to ranking problems of machine learnin g is Kendall X  X  Tau distance [2]. We give its definition for ordere d PDF values and LPF values as follows.

D EFINITION 6. ([2]) Suppose that Z = { Z 1 , Z 2 ,  X  X  X  , Z a dataset, p ( Z ) and LP F ( Z ) are the set of PDF values and LPF values of Z i s. Sort p ( Z ) in ascending order and denote I ( i ) the in-dex of the i th small element of the sorted p ( Z ) , that is, p ( Z p ( Z I (2) )  X   X  X  X   X  p ( Z I ( n ) ) . Then the Kendall X  X  Tau distance be-tween the sorted LP F ( Z ) and p ( Z ) is defined by  X  ( LP F ( Z ) , p ( Z )) = 2 n ( n  X  1) P i&lt;l S (( p ( Z where S is the sign function.

The measure  X  ( LP F ( Z ) , p ( Z )) is a function of LP F ( Z ) and p ( Z ) with range [-1, 1]. If the index sequence of the sorted p ( X ) Figure 4: The PDF values and LPF values ( k = 900 , and  X  = 1 or  X  = 0 . 5 ) of data sampled from the distribution 1 2 ( N (2 , 1) + N (  X  2 , 1)) . (ascending order) and the index sequence of the sorted LP F ( Z ) (descending order) are completely different, we have  X  ( LP F ( Z ) , p ( Z )) =  X  1 , and if the two index sequences are completely same, we have  X  ( LP F ( Z ) , p ( Z )) = 1 . For general cases, it can be seen from Eq. (7) that the more similar the two sequences are, the l arger  X  ( LP F ( Z ) , p ( Z )) is. Thus the LPF can describe the PDF more accurately.

We use  X  ( LP F ( Z ) , p ( Z )) to quantitatively analyze the ability of the LPF in describing PDF and to intuitively demonstrate t he im-pact of the parameter k on the performance of the LPF, which can help us to select a better parameter value in calculating LPF values. We sampled 2000 data points from the distribution 1 2 ( N (2 , 1) + N (  X  2 , 1)) , calculated their PDF values and LPF values as plot-ted in Fig. 3 and Fig. 4, and calculated the Kendall X  X  Tau dist ance between the two sets of ordered values. We did 50 random exper -iments for each k , and the means and standard deviations of these distances are plotted in Fig. 5. It can be seen that for the dis tri-bution, the LPF accurately characterizes the PDF when k is in the range [500, 1000]. When k is increased further, the ability of the LPF drops. The description is worst with k equal to the size of the dataset, at which the LPF is equivalent to the PF. That is to sa y, for the distribution, the LPF can always characterize the PD F more accurately than the PF.

Fig. 5 also shows that the performance of the LPF is reasonabl y good and stable with k in a large range. This makes the selection of k easy. For a dataset with its generating PDF unknown, we rough ly estimate the number of the peaks in the PDF by our prior knowl-edge. If the number is big, we select a small parameter k , and if the number is small, we select a large k . The parameter k should be the size of the dataset while there is only one peak in the PD F, in which case the LPF is same as the PF.
The LPF-Outlier algorithm has been implemented on four UCI datasets, a KDD dataset and a Mammography dataset, and its re -sults have been compared with that of some other outlier dete ction methods. These experiments are divided into two groups. The first group is on two UCI datasets, Lymphography dataset and Wisco n-sin breast cancer dataset [14], and compared outlier detect ion al-gorithms includes KNN [17], RNN [7], FindCBLOF [8] and Find-FPOF [9]. The second group is on two other UCI datasets, Ann-Figure 5: The means and covariances of the Kendall X  X  Tau dis-tances between ordered PDF values and ordered LPF values with different k , where data points were sampled from the dis-tribution 1 2 ( N (2 , 1) + N (  X  2 , 1)) , and the parameter  X  in the LPF is  X  = 1 or  X  = 0 . 5 . thyroid dataset and Shuttle dataset, KDD dataset and Mammog ra-phy dataset. On these datasets we compare LPF-outlier with m eth-ods including LOF [3, 4], Feature Bagging [11], Bagging, Boo sting and Active-Outlier [1].
The Lymphography dataset consists of 148 instances with lab els 1, 2, 3 or 4, and each record is described by 18 continuous or di s-crete attributes. Only 6 (4.1%) of the records are labeled wi th 1 or 4, and 142 (95.9%) are labeled with 2 or 3. We take instances wi th label 1 or 4 as outliers, and with label 2 or 3 as normal instanc es. The data was preprocessed by the method in [9, 12], and the exp er-imental results in [12] and [9] are directly cited for compar ison.
The Wisconsin breast cancer dataset consists of 458 instanc es with label benign and 241 instances with label malignant, an d each record is described by 9 attributes. We followed the experim ental Figure 6: The ratio of the smallest number of top ranked records with all rare classes being detected to the total num -ber of the records for LPF-Outlier with different k s. technique in [7] and [9] by removing some of the malignant rec ords to form a very unbalanced distribution, and the resultant da taset has 39 (8%) malignant and 444 (92%) benign records. The dataset c an be used to evaluate the performance of an outlier detection a lgo-rithm by its ability in detecting the malignant records.
The results of LPF-Outlier, FindCBLOF, FindFPOF and KNN on Lymphography dataset are given in Table 1, where T op ratio is the ratio of the number of detected records to the total dat a, coverage is the ratio of the number of detected outliers to the total outliers, and the results of LPF-Outlier were calculated wi th k = 30 in Eq. (5). We can see that LPF-Outlier and FindFPOF are both the best ones at T op ratio = 5% , and at all other T op ratio  X  X  LPF-Outlier can detect outliers more effectively than Find CBLOF, FindFPOF and KNN. The results of LPF-Outlier, FindCBLOF, Fi nd-FPOF, RNN and KNN on the Wisconsin breast cancer dataset are given in Table 2, where the results of LPF-Outlier were calcu lated with k = 200 in Eq. (5). It can be seen that LPF-Outlier, Find-CBLOF and KNN are all the best ones at the T op ratio = 0 . 83% , Figure 7: The ratio of the smallest number of top ranked records with all malignant being detected to the total numbe r of the records for LPF-Outlier with different k s. and at all other T op ratio  X  X  LPF-Outlier can detect outliers more effectively than FindCBLOF, FindFPOF, RNN and KNN.

To investigate the impact of the parameter k on the performance of LPF-Outlier, the values of T op ratio with different k s and coverage = 100% for the Lymphography dataset and the Wis-consin breast cancer dataset are plotted in Fig. 6 and Fig. 7, re-spectively. Fig. 6 shows that the performance of LPF-Outlie r is comparatively stable while k varying and LPF-Outlier with k  X  28 performs better than the PF based outlier detection algorit hm (LPF-Outlier with k = 148 ). Fig. 7 shows that the performance of LPF-Outlier is also comparatively stable and while k  X  183 it performs better than the outlier detection algorithm based on the PF ( LPF-Outlier with k = 483 ).
For the four datasets of the second group, the set-up for our e x-periments followed the ones in [1, 5, 11], and our results wer e di-rectly compared with the results reported in these papers. I n partic-ular, in order to preprocess discrete attributes, we use the concept of inverse document frequency (IDF), where each value of categ orical attribute is represented with the inverse frequency of its a ppearance in the dataset, and the values of each attribute are normaliz ed to [0,1]. IDF is already used in outlier detection problems [6] .
The Mammography dataset 1 consists of 10923 records with label 1 and 260 records with label 2, and each record is characteriz ed by 6 continuous attributes. We take records with label 2 as outl iers and others as normal records.

KDD Cup 1999 was prepared for network intrusion detection and is available from the UCI KDD Archive (http://kdd.ics.u ci.edu/). There are six sub-datasets at the address and we selected the test data with corrected labels (corrected.gz) to extract the KD D dataset. Each data in the dataset is specified by 41 attributes (34 cont inuous and 7 categorical) and a label describing attack type, where all la-bels except  X  X ormal X  indicate one of the four classes of atta ck, i.e., U2R, DOS, R2L and Probe. The KDD dataset consists of the nor-mal records (60593) and the records of U2R (228), which is the smallest class of intrusion.

The Shuttle dataset consists of 14500 instances with label 1 , 2, 3, 4, 5, 6 or 7, and each record has 9 numerical attributes. We cre ated
We would like to thank Professor Nitesh V. Chawla for providi ng us this dataset. Figure 8: ROC curves for LPF-Outlier for the KDD dataset ( k = 13000 ) and the Mammography dataset ( k = 11183 ). five data sets by selecting classes 2, 3, 5, 6 and 7 to be detecte d as outliers compared to the biggest remaining class 1.

The Ann-thyroid dataset consists of 3428 instances with lab el 1, 2 or 3 and each record is characterized by 15 binary attribu tes and 6 continuous attributes. We take the records of class 1 an d class 2 as outliers versus the class 3 as the normal (majority ) class, and two datasets, Ann-thyroid 1 and Ann-thyroid 2, were crea ted respectively.

Before giving the experimental results on above datasets, w e first introduce the concept of receive operating characteri stic curve (ROC) and area under the curve (AUC), which have been used to evaluate the performance of outlier detection algorithm s[1, 11]. Suppose that the dataset consists of actual outliers and act ual nor-mal records, predicted outliers include true positives (TP ) and false positives (FP), and predicted normal records include false nega-tives (FN) and true negatives (TN). Then the detection rate a nd the false alarm rate are defined by TP/(TP+FN) and FP/(FP+TN), re -spectively. The ROC curve is the curve plotted with the detec tion rate as x-coordinates and the false alarm rate as y-coordina tes. The AUC is the area under the ROC curve. The ideal ROC curve has 0% false alarm rate and 100% detection rate. However, the ide al ROC curve can only be approximated in practice and AUC quan-titatively evaluate the approximation. Outlier detection algorithms with AUC closer to 1 have ROC closer to the ideal ROC, and are better algorithms.
 The ROC curves for LPF-Outlier on the KDD dataset and the Mammography dataset are plotted in Fig. 8, where LPF values i n the LPF-Outlier were calculated with k = 13000 and k = 11183 , respectively. The AUC achieved by LPF-Outlier with differe nt k  X  X  for the KDD dataset and the Mammography dataset are given in F ig 9.and Fig. 10, where D ( Z ij , Z lj ) = | Z ij  X  Z lj | or D ( Z | Z Outlier, Active-Outlier, Bagging, Boosting, LOF and Featu re Bag-ging for all the four datasets are given in Table 3. The result s of LPF-Outlier given in Table 3 are the best ones with k shown in Fig. 9 and Fig. 10 for the KDD dataset and the Mammography dataset, is the mean of the best ones obtained on its five sub-d atasets for the Shuttle dataset, and are chosen using the best parame ter choices for Ann-thyroid 1 and Ann-thyroid 2. The AUC for othe r algorithms are directly copied from [1]. From Fig. 9, Fig. 10 and Table 3, we can see that for the KDD dataset and the Mammog-raphy dataset, LPF-Outlier with any k can detect outliers more ef-the second group.
 Figure 9: The AUC achieved by LPF-Outlier with different k s for the Mammography dataset. fectively than Active-Outlier, Bagging, Boosting, LOF and Feature Bagging. For the Shuttle dataset and the Ann-thyroid 1 datas et, LPF-Outlier performs quite well. However, for the Ann-thyr oid 2 dataset, LPF-Outlier performs worse than four of the other five methods. The unfavorable performance may be explained by th e fact that there are too many discrete attributes comparing t o con-tinuous attributes and the IDF may be ineffective in preproc essing such a dataset. In summary, these experimental results show that LPF-Outlier is effective for outlier detection.
In this paper, we have studied the PF from both theoretical an al-ysis and experimental evaluation. We have proved that the PF can describe the PDF of a normal distribution accurately and is u nsuit-able for characterizing the PDF of general distributions. T he con-cept of LPF was proposed to solve the difficulty. The LPF was proved to have the same power as the PF in characterizing the P DF of a normal distribution and be the so-called  X  -sensitive peculiarity description for general distributions. Based on the LPF, an out-lier detection algorithm, LPF-Outlier, was proposed for ou tlier de-tection problems. Experiments on a synthetic dataset have s hown that the LPF can characterize the PDF of a distribution more a ccu-rately than the PF and experiments on several real life datas ets have demonstrated that LPF-Outlier is a novel and quite effectiv e outlier detection method.
 The first author would like to thank Fuxin Li for his helpful di s-cussions. This work was partially supported by the National Grand Fundamental Research 973 Program of China (No. 2004CB31810 3), Figure 10: The AUC achieved by LPF-Outlier with different k s for the KDD dataset. the National Science Foundation of China (No. 60575001 and N o. 60673015), the China Postdoctoral Science Foundation (No. 20070 420016), and the grant-in-aid for scientific research (No. 1 8300053) from the Japanese Ministry of Education, Culture, Sports, S cience and Technology. [1] N. Abe, B. Zadrozny, and J. Langford. Outlier detection b y [2] N. L. Bhamidipati and S. K. Pal. Comparing rank-inducing [3] L. Breiman. Bagging predictors. Machine Learning , [4] M. M. Breunig, H. P. Kriegel, R. T. Ng, and J. Sander. Lof: [5] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
 [6] L. Ertoz. Similarity Measures . Ph.D. Dissertation, University [7] S. Harkins, H. He, G. J. Willams, and R. A. Baster. Outlier [8] Z. Y. He, X. F. Xu, and S. C. Deng. Discovering cluster [9] Z. Y. He, X. F. Xu, Z. X. Huang, and S. C. Deng. A frequent [10] E. Knorr and R. Ng. Algorithms for mining distance-base d [11] A. Lazarevic and V. Kumar. Feature bagging for outlier [12] B. Liu, W. Hsu, and Y. Ma. Integrating classification and [13] K. Mcgarry. A survey of interestingness measures for [14] G. Merz and P. Murphy. Uci repository of machine learnin g [15] M. Ohshima, N. Zhong, Y. Y. Yao, and C. Liu. Relational [16] M. Ohshima, N. Zhong, Y. Y. Yao, and S. Murata. Peculiari ty [17] S. Ramaswamy, R. Rastogi, and S. Kyuseok. Efficient [18] D. Saso and L. Nada. An introduction to inductive logic [19] Y. Y. Yao, F. Y. Wang, J. Wang, and D. D. Zeng. Rule + [20] Y. Y. Yao and N. Zhong. An analysis of peculiarity orient ed [21] N. Zhong, C. Liu, Y. Y. Yao, M. Ohshima, M. X. Huang, and [22] N. Zhong, M. Ohshima, and S. Ohsuga. Peculiarity orient ed [23] N. Zhong, Y. Yao, and M. Ohshima. Peculiarity oriented [24] N. Zhong, Y. Y. Yao, M. Ohshima, and S. Ohsuga.
 The Proof of Theorem 1: Without loss of generality, we may sup-pose that the mean of the distribution locates at the origin, that is, p ( x ) = p (  X  x ) for all x  X  R + . We only prove that P F ( x ) strictly increases with nonnegative x and a similar deduction can be done for x &lt; 0 . Furthermore, we suppose that 0  X  x 1 &lt; x prove that P F ( x 1 ) &lt; P F ( x 2 ) .
 and decreasing, and H ( x ) =  X  ( x 2  X  x )  X   X  ( x  X  x 1 on [ x 1 , x 2 ] , there exists a  X   X  [ x 1 , x 2 ] such that Hence we can get Since the following inequalities hold Therefore = Z &gt; 0 .
 Then we get = Z &gt; 0 .
 That is to say, P F ( x ) strictly increases with x for x  X  0 .
