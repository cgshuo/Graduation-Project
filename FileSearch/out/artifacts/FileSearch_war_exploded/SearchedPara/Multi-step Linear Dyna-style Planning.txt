 feature.
 we actually use the LSTD (Bradtke &amp; Barto, 1996; Boyan, 1999) solution for planning. through a number of iterations, each of which sweeps the data set and alternates between LSTD and policy improvement. Sutton et al. (2008) explored the us e of linear function approximation the LSTD solution, we actually propose an online LSTD contro l algorithm. of k makes the k -step linear Dyna for control perform the best. Given a Markov decision process (MDP) with a state space policy evaluation is to predict the long-term reward of a pol icy  X  for every state s  X  where r 7 X  entries are  X  where  X  matrix of policy  X  by P  X  , whose ( i, j ) th component is P  X  state i in one step. Linear Dyna tries to estimate a compressed model of policy  X  : estimated online through gradient descent: respectively, where the features and reward are all from rea l world experience and  X  step-size.
 Van Roy, 1997; Sutton et al., 2008), that satisfies where I At the same time we accumulate the rewards generated in the pr ocess of this iterating: the expected accumulated rewards in k steps from  X  . Notice that which is a generalization of the Bellman equation, V  X  = R  X  +  X P  X  V  X  . 3.1 The Iterated Multi-step Linear Model along the way: k -step linear Bellman equation: where  X   X  is the linear TD( 0 ) solution. 3.2 The  X  -model number of features ( n ) is large.
 if they can be computed. Given an imaginary feature  X   X  feature by applying F ( k ) : the future: single-step model and infinite-step model.
 a gradient-descent estimate. multi-step linear model, which we call the Dyna( k )-iterate .
 planning using the  X  -model for any finite k . 4.1 Dyna(  X  ): Planning using the Infinite-step Model 2002) and solve f (  X  ) by matrix inversion methods or recursive least-square meth ods. the infinite-step model to get the expected future features a nd all the possible future rewards: Next, a generalized linear TD( 0 ) is applied on this simulated experience. Because  X   X  (  X  ) = 0 , this simplifies into We call this algorithm Dyna(  X  ), which actually uses the LSTD solution for planning. Algorithm 1 Dyna( k ) algorithm for evaluating policy  X  (using any valid k -step model).
Initialize  X 
Select an initial state for each time step do end for 4.2 Planning using the  X  -model infinite-step models: respectively, where the infinite-step model is estimated by f (  X  ) = ( A  X  nary feature  X   X  , we look k steps ahead to see the future features and rewards: k -step version of linear TD( 0 ): We call this algorithm the Dyna( k )-lambda planning algorithm. When k = 1 , we obtain another k  X  X  X  , we obtain the Dyna(  X  ) algorithm. based on these tracking models we build the iterated model an d the  X  -model. Our extension of linear Dyna to control contains a TD control step (we use Q-learning), and we which is max a feature Algorithm 2 Dyna-Q( k )-lambda: k -step linear Dyna-Q algorithm for control (using the  X  -model).
Initialize F
Select an initial state for each time step do end for Estimation of the single-step reward model, f , is the same as in policy evaluation. while vector b is updated in the same way as in LSTD. Given A and b , we can solve them and get f compute the  X  -model in a similar manner to policy evaluation. The complet e multi-step Dyna-Q used for control, giving an online LSTD control algorithm.
 (which we call Dyna-Q(k)-iterate) is straightforward and t hus not shown. 6.1 Boyan Chain Example The problem we consider is exactly the same as that considere d by Boyan (1999). The root mean Dyna algorithms, we used different step-sizes for learning , modeling and planning. (1) Learning step-size . We used here the same step-size rule for TD as Boyan (1999), w here  X  = Dyna( k )-lambda, we used  X  (3) Planning step-size . In our experiments all linear Dyna algorithms simply used  X  Figure 1: Results on Boyan Chain. Left: comparison of RMSE of Dyna( k )-iterate with LSTD. Right: comparison of RMSE of Dyna( k )-lambda with TD and LSTD.
 algorithms sampled a unit basis vector whose nonzero compon ent was in a uniformly random loca-averaged over 30 (identical) sets of trajectories.
 Figure 1 (left) shows the performance of Dyna( k )-iterate and LSTD, and Figure 1 (right) shows the performance of Dyna( k )-lambda, LSTD and TD. All linear Dyna algorithms were found to be much faster than single-step linear Dyna algorithms. Matri x A of LSTD and Dyna( k )-lambda needs LSTD, we tried initialization of A  X  in which A  X  to were compared under the same setting (Dyna( k )-lambda also used A used  X  = 0 . 9 . 6.2 Mountain-car We used the same Mountain-car environment and tile coding as in the linear Dyna paper (Sutton we only present here the results of Dyna-Q( k )-lambda.
 independent algorithm and the sub-procedure of Dyna-Q( k )-lambda. The planning step-size was 0 . 1 . The matrix F is much more dense than A and leads to a very slow online performance. To tackle this problem, we avoided computing F explicitly, and used a least-squares computation of over 30 independent runs, each of which contains 20 episodes. Figure 2: Results on Mountain-car: comparison of online ret urn of Dyna-Q( k )-lambda, Q-learning and linear Dyna for control.
 Dyna-Q( 20 )-lambda and Dyna-Q(  X  ) gave poorer performance than Dyna-Q( 5 )-lambda and Dyna-Q( 10 )-lambda. This shows that some intermediate values of k trade off the model accuracy and the depth of looking ahead, and performed best. In fact, Dyna-Q(  X  ) and LSTD control algorithm were and looking infinite steps into the future, LSTD and Dyna-Q(  X  ) magnify the errors in the model. performance than existing single-step linear Dyna-style p lanning on Boyan chain and Mountain-performance by using different step-sizes for learning, mo deling, and planning than using a uni-control.
 Our work also sheds light on why previous attempts on develop ing independent online LSTD control become unstable because they magnify the model errors by loo king infinite steps into the future. Current experiments do not include comparisons with any oth er LSTD control algorithm because 2008); however, LSTD there is still not an independent contr ol algorithm. acknowledge their help in improving the paper in many aspect s. We also thank Alborz Geramifard for sending us Matlab code of tile coding. This research was s upported by iCORE, NSERC and the Alberta Ingenuity Fund. function approximation. Learning and Approximate Dynamic Programming (pp. 231 X 255). IEEE Press.
 Boyan, J. A. (1999). Least-squares temporal difference lea rning. ICML-16 . Machine Learning , 22 , 33 X 57.
 AAMAS-8 .
 Peters, J., &amp; Schaal, S. (2008). Natural actor-critic. Neurocomputing , 71 , 1180 X 1190. mating dynamic programming. ICML-7 .
 Sutton, R. S. (1995). TD models: modeling the world at a mixtu re of time scales. ICML-12 . Sutton, R. S., &amp; Barto, A. G. (1998). Reinforcement learning: An introduction . MIT Press. linear function approximation and prioritized sweeping. UAI-24 .
 approximation. IEEE Transactions on Automatic Control , 42 , 674 X 690.
 methods. Journal of Artificial Intelligence Research , 16 , 259 X 292.
