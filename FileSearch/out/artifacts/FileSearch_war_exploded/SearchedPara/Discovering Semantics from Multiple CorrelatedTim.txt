 Time series data have emerged in a wide range of applications from almost every domain. Examples include economi c index data in stock markets, patient medical observation data , experimental biologica l data, to name a few. As a result, it is of utmost importance to find inherent semantics from time series data. Take the medical examination for example, doctors often estimate the physical status of a patient by monitoring and collecting multiple correlated time series data from electrocardiogram s (ECG), electroen cephalograms (EEG), heart beat rate (HR), and blood pressure observations. Obviously, it is very hard to make accurate estimation by only relying on a single time series data (e.g., HR increases does not mean the patient is suffering from severe illness, perhaps she/he just had exercises). Hence, it is n ecessary to combine all these time series data for estimation.
 Many methods have been proposed to analyze multiple time series in [3][5][11]. In addition, there are some popular time series models for time series forecasting, such as the Vector Auto-regression (VAR) and Linear-regression (LR) models. These approaches focus on frequent patterns in the time series, which cannot explain observations using the internal dynamics of systems. The dynamics of a system can be considered as a mechani sm of system-work as equivalent to semantics. Here, the system is unseen and unknown, which determines observed time series. It can help us to know more about observation generating rules and state transforming rules underneath data by learning the mechanism. A previous work [12] also studies the semantics det ection problem from time series data. The difference is that it uses the pattern-based Hidden Markov Model (pHMM) to describe the univariate time series data where a line segmentation method is used to obtain significant segment patterns. In our work, the patterns are irregularly summarized from multiple time series, and we use the Hidden Markov Random Field (MRF) as the solution.

Supposing all time series are observed synchronously at each time point, ob-servations from all time series compose of a tuple at each moment and each tuple is regarded as an output generated by a certain latent state. Each state demon-strates a certain pattern of fluctuation. In this paper, the pattern of fluctuation is taken as a generating rule, conforming with generated observations. Semantics learning basically learns both observation value generating rules and transfor-mation rules among latent states. It is widely admitted that the Hidden Markov Model (HMM) can be used to learn semantics. In HMM, state assignment is mainly determined by aligning observation value production and state transmis-sion. However, a single observation tuple value contains little information. We illustrate this by a real medical example in Figure 1. Figure 1 demonstrates 6 vital body signals of a patient in an operating theatre. At the t 0 time point, the operation starts. Apparently, observation values at t 1 are very similar with observation values at t 2 . Actually, they represent different situations of the pa-tient. At t 1 , the observation value represents a natural reaction of the patient to an outside emergency which can be regarded as the operation. At t 2 ,theobser-vation value represents physical state of patient during the operation. If we only consider observation values of one tuple, we cannot obtain significant semantics for a practical problem. Actually, wha t is different from other common data is that time series data has natural temporal ordering. The temporal ordering of observation data is not directly consid ered in HMM. According to the temporal ordering of time series, each observation tuple is strongly correlated with the previous tuple. In Figure 1, it is obvious that correlation between t 1 tuple and t  X  1 tuple is different from correlation between t
Hence, we propose Correlation field-based Semantics Learning Framework (CfSLF) to learn semantics underneath multiple correlated time series in this paper. In the framework, generating rules and state transforming rules will be learned. In order to comprehensively co nsidering both temporal ordering and observation value of tuple, we use hidden Markov Random Field (HMRF) to obtain approximately optimal latent state assignment to learn generating rules. Then, state transforming rules are learned from label sequence in the framework.
The rest of the paper is structured as follows. Section 2 introduces a math-ematical description of the problem. Section 3 describes detailed proposed Cf-SLF. Section 4 introduces two main applications: Observation Value Prediction &amp; Anomaly Detection. Section 5 report s experimental results to show the ad-vantage of our CfSLF model compared with some other algorithms. Section 6 introduces related works. We c onclude the paper in Section 7. In this paper, we propose a Correlation field-based Semantics Learning Frame-work (CfSLF) to learn latent semantics underlying multiple time series which represent a mechanism of system work. It contains two parts: generating rules learning and state transforming rules learning.

Given multiple time series X = { X 1 ,...,X n } , X i = { x i 1 ,...,x im } . X i rep-resents an observed tuple from the m time series at the i th moment and x ij represents observed data from the j th time series at i th moment. Assume that and represents latent state of the i th tuple. The value of the variable is in the range from 1 to k . Assume z i = j , which represents that the latent state of the i th tuple is the j th state s j . In order to learn data generating rules, we need to obtain optimal latent state assignment and the corresponding generating rules to maximize production probability of observation tuples. The problem can be described as follow: which is maximized to obtain state assignment. According to the continuity of tuples sequence, the label variable set c onstituted of finite states can be seen as a latent state sequence. After we obtain optimal state assignment, transforming rules among latent states are learned fr om the state sequence based on Markov chain characteristics. In this section, we discuss how to learn semantics by our proposed CfSLF. 3.1 Construction of Dependence Relationship In time series, temporal ordering can be seen as the relationship between the current and previous tuples. In this paper , we simply consider temporal ordering as changing the trend to represent the relationship. We introduce a definition of a local trend as follow, Definition 1. Local Trend: In time series, we consider the direction and volume of changing from the last node to the current node as the local trend along time axis.
 Suppose that the current tuple is x t with observation value o t . We directly obtain local trend of by T d t = o t  X  o t  X  1 . For the local trend of the tuple, we apply the attribute to represent temporal ordering of the current tuple. Then, we simply use the cosine distance to measure the similarity among local trends of different tuples. Intuitively, continuity of time series can be considered as integrating temporal ordering of all tuples. We use the local trend of tuple to represent temporal ordering, which can be seen as an independent attribute of each tuple. In doing so, each tuple has two attributes: its observation value and its local trend. For each tuple, the observation value represents its individual character and local trend represents sequence chara cter. As a result, the tuple series with size n is divided into n independent observation tuples 3.2 Latent State Assignment In the procedure of state assignment, tuples with similar local trends and ob-servation values are more likely to hav e the same latent state. Therefore, we use HMRF to learn the rules. A correlation field is built based on local trend similarity. In the correlation field, the assignment of labels depends on corre-sponding brotherhood set. In this paper, the similarity matrix is considered as a correlation network describing dependency relationship among latent labels of tuples in the correlation field. In the network, similarity is seen as a weight of dependency relationship.

Suppose that we have a correlation network denoted as M . Here, M is the symmetric n  X  n matrix, where w ij is the link weight between labels z i and z j . The links in M induce dependence relationships among latent labels, with the rationale that if the link weight is higher between labels z i and z j , then they are more likely to have the same value equivalent to the same state.

We define a brotherhood set of the i th label as a label set consisting of labels which have the same latent state value, B i = { z j , i = j &amp; z i = z j } . The random field defined over hidden label variable Z is a Markov random field, where the Markov property is satisfied by p ( z i | z B i ). It indicates the probability of z i de-pending on z i  X  X  brotherhood set. By introducing the HMRF model, latent labels of tuples are mapped to a correlation field, where assignment of labels depends on a corresponding brotherhood set without considering tuple value.

Because the observation value of tuple is generated by the corresponding state, it is irrelevant to other states and only depends on its latent state. Thus, the values of tuples are conditional independent given their labels.
 We assume that the observation value of the i th tuple generated by the k th latent state is characterized by a set of parameters  X  k , i.e., as we mainly consider mul-tiple time series as real data, we propose to model observation data by Gaussian distribution, because of its flexibility in approximating a wide range of continu-ous distributions. Therefore, we use the parameter mean vector  X  and variance matrix to describe the k th latent state,  X  k =(  X  k , X  k ).
 We first assume model parameters  X  = {  X  i , i from 1 to k } are known a prior. In order to obtain approximately optimal assignment of latent variables for each observation tuple, we transform to find the optimal configuration that maximizes the posterior distribution given  X  .

As discussed in Eq. (1), the probability distribution of Z is given by P ( Z )= exp (  X   X  ij  X  ( z i  X  z j )) /H .

In the above equation, H is a constant value, which can be neglected. We use the Iterated Conditional Modes (ICM) algorithm [5] to estimate the maximum a posteriori probability (MAP). The greedy algorithm can be used by calculating local minimization iteratively, which converges after a few iterations. The basic idea is to sequentially update the label of each object, keeping the labels of the other objects fixed. At each step, the algorithm updates z i given x i and the label by maximizing the conditional posterior probability, p ( z i | x i = o i ,Z I  X  X  i } ). Actually, H 2 can be considered as a constant variable. In doing so, we take the logarithm of the posterior probability, and transform the MAP estimation problem into the minimization of the conditional posterior energy function as shown in the following equation, where  X  is a predefined parameter that represents the importance of the temporal ordering correlation.  X &gt; 0 represents the confidence of the temporal ordering correlation network. To minimize U i ( k ), we find the latent state k of the tuple i by k = argmax k U i ( k ). 3.3 Parameter Estimation In this part, we consider the problem of estimating unknown  X  in order to iter-atively learn optimal state assignment.  X  describes the pattern conformed with the time stamp that x is generated. We first seek to find  X  to maximize P ( X |  X  ), which can be considered as the maximal likelihood estimation for  X  . However, since both the hidden label and the parameter are unknown and inter-dependent, it is intractable to directly maximize the data likelihood. We view it as an  X  X ncomplete-data X  estimation problem, and use the Expectation-Maximization (EM) algorithm as the solution.

The basic procedure is as follows. We start with an initial estimate  X  0 . Assume that there exist k latent states, where  X  0 is obtained by a simple K-Means algorithm. In the E-step, we calculate the conditional expectation Q (  X  |  X  ( t )), Next, in the M-step, we find  X  t +1 by computing the derivation of the maximizing function Q (  X  |  X  t ). In each iteration, we have obtained the optimal latent state assignment from the is 1 when z i is at state j ,else0.Asaresult,theE-stepandM-stepcanbe recursively computed until Q (  X  |  X  t ) converges to a local optimal solution. 3.4 State Transforming Learning After we obtain the optimal latent state assignment, each observation tuple is assigned a label corresponding to the lat ent state set of tuples. The labels can be seen as a sequence consisting of limit states along the time axis. We then model correlation among states to reveal system dynamics. Here we regard correlation as the transforming probability p ( s i | s j ) representing the probability from the state j to state i . A Markov chain model can be use d to approximately estimate where N ( s j ) represents the amount of labels with s j value in label series and N ( s j s i ) represents the amount of adjacent labels with s j and s i values in label series.
 Observation Value Prediction. Time series semantics can be used to make the following value prediction. We first introduce the next 1-step value predic-tion. Assume that we have learned the semantics from the training data, we then have  X  and the state transformation rule. In test step, we consider time series X , as we care about the local trend of tuples, let x t  X  1 and x t are current tuples. Our task is to predict x t +1 . We first compute the current latent state of x t .When we assign a label to the current tuple, we need to predict the label of the next tuple according to the Markov chain chara cteristics. Assume the current label z is s c , the next label z t +1 can be obtained by maximizing p ( z t +1 = i | z t = s c ). Additionally, according to time series continuity, we can estimate that the next tuple close to the current tuple with high probability. Therefore, it is safe to say that the next state maintains the continuity with high probability. That is, next state can be predicted by, Because observation tuples produced by a state have similar values and similar trends, we approximately predict observation values of the state by, where E Z t +1 represents the Expectation of the ob servation value of the predicted state, and V Z t +1 represents the exception of the local trend of the predicted state. We use the Euclidean distance to measure similarity. By computing the derivative of function, the prediction value x can be obtained. Then we extend next 1-step value prediction to the next n-step value prediction. We iteratively apply the predicted value as the new observation value to forecast the next value until n steps have been performed.
 Anomaly Detection. Our proposed model also can be used to detect data anomaly. In time series, there is no apparent and definite label to represent which observation is normal or abnormal. So, it is not a classification problem. Generally speaking, we only know that anomaly occurs in a certain period. Take a finance application for example. The worldwide economical recessions have occurred several times in history. The r ecession always lasts for a period of time, which is regarded as recession date. It impacts all business activities. Compared with economic affairs in other periods, economical affairs in recessions can be seen as anomaly. Hence, we propose the method based on a semantics model only qualitatively to indirectly reflect anomalies.

According to time series continuity and semantics rules, we know that the current tuple is similar to the last tuple with high probability, and the current state has high transformation probability from the last state. Considering both rules, we can measure the probability with which the current tuple normally is function is generally used to obtain a degree of the energy. Thus, we compute the anomaly score of the current tuple by  X  logf ( x t ), which measures the significance the current tuple deviates from that emanating from the normal producing rule. Intuitively, high scores indicate anomalous data with high probability. In this section, we present extensive expe riments on real-world multiple time se-ries data to validate performance of our proposed approach. All experiments are conducted on a 3.0GHZ CPU with 2 GB RAM. The experimental environment is windows XP with Matlab.
 Benchmark Data. We adopt four multiple time series data as our test-bed: 1) Price data 1 2) Mote data sets 2 3) Medical data 3 4) Financial data 4
Baseline Methods. As discussed above, our proposed semantics learning method can be applied to value prediction. We compare our method with following value prediction algorithms: (1) Multivariate Autoregression Model. (2) Hidden Markov Model (HMM). 5.1 Experiment Results We first consider the time cost of model learning and hyper parameter  X  sensi-tivity for data. Then we compare our proposed model with benchmark method on multiple step prediction accuracy. Additionally, our proposed model can be used to indirectly reflect latent anomalies, which are hard to see from the original data. Here we mainly analyze financial data to discover financial mark depres-sion.
We use relative error to measure accu racy of prediction. We compute the real value. Thus, the lower relative error is, the higher the accuracy is. Time Complexity. Suppose the number of tuples is N in multiple correlated time series. In M-step,the time complexity is O(N). In E-step, the time complex-ity is O( N 2 ) because of aggregating the effect of the labels of brotherhood set of v to compute P ( Z ). Actually, semantics rules seen as latent pattern repeatedly exist in the multiple time series. Hen ce, we do not need to learn a model based on entire training set. We can approximately obtain the semantics from part of the data set. In the experiment, we conduct our proposed model on all of benchmark data set, and compare the runtime and next 1-step value prediction accuracy under different N , the selection ratio of boundary points. For example, assume size of train-ing is 2000, and 0.05 means we choose 0.05*2000 =100 time points to train the model. The results are shown in Table 1. In Table 1, the error is average relative error for applying learned CfSLF to 200 testing tuples sampled from testing set. It can be seen that runtime gets longer and accuracy gets higher when ratio N gets bigger. We can see that in every data set, when the percentage is equal to or larger tha n a certain value, the accuracy is not affected much.
 Hyper Parameter Sensitivity. In our proposed model, parameter  X  represents the confidence of the temporal ordering correlation network. Different values of  X  determine different effects of the corre-lation field. We separately conduct ex-periments on all of the data sets to demonstrate hyper parameter  X  setting and effect. In each experiment, we vary  X  from 0.1 to 2 separately, and compute the corresponding relative error of the next 1-step value prediction under the prede-fined  X  . HMM is used as baseline method to compare with our proposed method. The outcome is shown in Figure 2. It can be seen that there are slight changes in performance when parameters are va ried and CfSLF has better performance than the HMM and Multivariate Autoregression models.
 Application of the Model. In the former section, we have tested performance on the next 1-step value prediction. In the following, we will discuss additional next n-step value prediction and anomaly detection.
 Next n-step Value Prediction. In this experiment, we test the accuracy of CfSLF for the next n-step value prediction. The experiments are conducted for all data sets. We select 10 points randomly. For each selected point, we predict the values after 1, 2, 5, 10, 20, and 50 steps respectively. The CfSLF is compared with a Hidden Markov Model (denoted by HMM). The HMM can be used to learn system-work mechanisms unde rlying time series. In the experiment, we suppose that each tuple is produced by a latent state and the producing procedure conforms to Gaussian distribution. We first find latent state of the tuple at a selected time point, then make state predictions at 6 future time points. Prediction value is corresponding expectation of predi cted state. We use relative error as the measurement. The r esults are shown in Figure 3. It can be seen that on all data sets, CfSLF is more accurate than HMM.

Anomaly Detection. In the experiment, we use CfSLF on financial data to verify its performance for anomaly detect ion. The experimenta l results indicate that the proposed method detected deviations from that emanating from the normal producing rule as anomalies and these corresponded to actual economic events. The degree of anomaly in these time series is shown in Figure 4. In Figure 4, we see that two apparently peak deviated from other scores. Each peak corresponded to big economic events occurring in corresponding month. The first peak appeared on January 2008, where the Federal Reserve lowered its federal funds rate, which impacts how much consumers pay on credit card debt, home equity lines of credit and auto loans, to 3.5 percent from 4.25 percent, which was the biggest rate cut by the Fed since October 1984. The second peak appeared on September 2008, where Lehman Brothers announced its bankruptcy. The second peak indicates that the proposed method detected the depression whictarted in September 2008, as anomalies. There are many works on analysing time series, such as summary learning, time series segmentation, forecasting and so on, which have always been popular top-ics [8][9][11][12][13][14][15][16]. However, they just can be used to analyse sin-gle time series. Additionally, pattern learning from time series based on sliding windows has attracted more and more attention [1][2][3][4][5][6][7][17]. However, these methods cannot reveal g lobal system-work rules. In recent years, semantics mining has been always a popular topic. In time series analysis, semantics can mainly be seen as system-work mechanisms. While, there are few studies on it. In [12], pHMM is proposed to learn time series semantics. However, it is just used to analyze a single time series. G enerally, a Hidden Markov Model can be used to learn the semantics rules. Som e other improved methods based on HMM are applied to learn latent system rules [16]. However, in multiple time series, a single observation value contains little information. Compared with these meth-ods, our proposed model introduces loca l trends to extend information of tuple, and learn semantics from multiple time series based on both observation values and local trend correlation. In this paper, we present a new Correlation field-based Semantics Learning Framework (CfSLF) to model multiple correlated time series. Our model aims to find semantics underneath multiple time series, by detecting data generating rules and transforming rules. Experiments have demonstrated the utility of the proposed method. The contribution of the study is three folder: (1) The Hid-den Markov Random Field (HMRF) is used to model the data observations and corresponding states, by which the irregular patterns can be summarized from multiple correlated time series. (2) A value prediction method is presented based on semantics learned by CfSLF. (3) An a nomaly detection method is proposed basedondatasemantics.
 Acknowledgments. This research was supported by the ARC Linkage Project (LP100200682) named  X  X eal-time and Self-Adaptive Stream Data Analyser for Intensive Care Management X ; Internation S&amp;T Cooperation Program of China No.2011DFA12910; National Scienc e Foundation of China (NSFC) Grant (71072172; 61103229; 61272480; 61003167); the  X  X trategic Priority Research Pro-gram X  of the Chinese Academy of Sciences Grant (XDA06030200); 863 Program Grant (2011AA01A103).
