 Consider a collection of surveillance cameras monitoring at an airport. The cameras learn a model of their environment without supervision. Moreover, they learn for many years without significant interruption. Gradually, as more data is captured, the cameras build a joint model of visual object categories.
 This problem is akin to the way children learn to under-stand the world through the continuous process of mostly unsupervised learning. As children grow up they build an increasingly sophisticated internal representation of object categories that continuously restructures itself. In this paper we ask ourselves: What statistical techniques are suitable for this  X  lifelong learning task  X ? First, we need a class of models that can naturally expand as more data arrives, i.e. it X  X  capacity should not be bounded a priori. Second, these models should allow efficient learning algo-rithms, both in terms of time and space. For instance, we should not have to store every single piece of information that has been captured. Our technique must produce a se-quence of model estimates that reflect new information as it arrives, and the time required to produce each model up-date must scale modestly as more data is acquired. Finally, we require that the sequence of learned models are suffi-ciently similar to those that would be produced by a batch algorithm with access to the entire history of data observed at the time of each model update.
 Nonparametric Bayesian techniques such as the Dirichlet Process (DP) (Ferguson, 1973) and the Hierarchical Dirich-let Process (HDP) (Teh et al., 2006) satisfy our first desider-atum, in that they naturally increase their model complex-ity with the available data. However, most existing Non-parametric Bayesian approaches are batch algorithms: they require every single data-point to be stored and revisited during learning. A batch algorithm could be naively ap-plied to the continuous learning scenario, but all data would need to be cached and a new batch learning process would be run on the entire dataset to produce each model update. This would violate our second criterion in that the time and space requirements would increase unacceptably as the sys-tem ages.
 Here we propose a more flexible setup, where we impose a bound on the available memory but still allow the model order to increase with more data. We compress the data and the internal representation of the model without losing much in terms of model accuracy. The effect is that time and space requirements scale much more gradually over the lifetime of the system. The memory bound does impose a limit on the total capacity of the model, but this trade-off is flexible and can be adjusted online, i.e. as the model is learned. Experiments with a memory bounded variational approximation to HDP show that this technique can handle datasets many times larger than the standard implementa-tions and results in substantially shorter run-times. At a high level the idea is to use a variational approxi-mation related to LDA (Blei et al., 2003) and HDP (Teh et al., 2006). Memory savings are achieved by  X  X lump-ing X  together data-cases. That is, we constrain groups of datapoints to have equal topic assignment variational dis-tributions: q ( z ij ) = q ( z i 0 j 0 ) = q ( z c ) when points x and x i 0 j 0 are members of the clump c . This allows us to achieve memory savings, because variational optimization performed under this constraint requires only the sufficient statistics of the data-cases in a clump, and the system can forget the exact identities of the summarized data points. Similarly, we will also clump entire documents (or im-ages) by tying their variational distributions over topics: q (  X  j ) = q (  X  j 0 ) = q (  X  s ) if document j and j 0 the same document group s . This tying of variational distri-butions guarantees that learning optimizes a lower bound to the exact free energy objective function, where the bound is increasingly loose with more tying. This idea was also leveraged in (Verbeek et al., 2003) and (Kurihara et al., 2006) to accelerate learning of Mixtures of Gaussians and DP Mixtures of Gaussians by using KD-trees.
 In the following we will talk about documents, but we note that this refers to other structured objects such as images as well. 2.1. The Variational Topic Model The following Bayesian topic model is our starting point, where x ij is word i in document j and z ij denotes the topic that generated x ij .  X  j denotes the mixture of topics that generated the words in document j , with P k  X  jk = 1 .  X  are distributed according to a Dirichlet distribution with pa-rameter  X  . Boldface symbols denote vector valued quanti-ties. In this expression we will assume that p ( x | z,  X  ) is in the exponential family 1 , and p (  X  |  X  ) is conjugate to p ( x | z,  X  ) , p (  X  k |  X  ) = exp The posterior distributions over  X  ,  X  , z are approximated variationally as where we have introduced variational parameters {  X  kl , X  kj ,q ijk } , the latter subject to P k q ijk = 1 . Furthermore, D denotes a Dirichlet distribution while q (  X  k ;  X  k ) is also conjugate to p ( x | z = k,  X  ) , q (  X  k ;  X  k ) = exp By writing down the variational free energy and minimiz-ing it over  X  ,  X  we find the following intuitive updates, and where Z ij enforces the constraint P k q ijk = 1 and the expectations are over q (  X  ) .
 To learn the parameters {  X  k } we first introduce gamma pri-ors, Using the bounds in (Minka, 2000) we can derive the fol-lowing updates if we first insert the updates for  X  and  X  into the free energy, with  X  j = P k  X  kj and N j = P k N kj . 2.2. Optimizing the Number of Topics K Our strategy to search for a good value of K is to truncate the topic distributions as q ( z ij &gt; K ) = 0 (see also (Teh et al., 2008)). This will have the effect that most terms in the free energy with k &gt; K will cancel, the exception being the prior terms p (  X  k ) , k &gt; K . For these terms we know that the value for  X  k minimizing the free energy is given by the MAP value of the gamma-prior  X  k = a  X  1 b , k &gt; K . Inserting this back into the free energy we accumulate K max  X  K terms
 X  = a log b  X  log  X ( a )+( a  X  1) log where K max is the maximum number of topics.
 It is guaranteed that there exists a solution with lower free energy if we increase K . The reason is that we relax a self-imposed constraint on variational parameters (that q ( z ij K ) = 0 ). As K increases the relative improvement in free energy quickly attenuates. The final value for K is obtained by thresholding this relative improvement.
 The nesting property (models with larger K are better) is the same for variational approximations to the DP in (Kuri-hara et al., 2006) and HDP (Teh et al., 2008). This raises the question if we can take the infinite limit for our model as well. The problem is that ( K max  X  K ) X   X   X  as K max  X   X  . This can be traced back to the fact that we should have added a proper prior p ( K ) which would have diminished the contribution at large K . Instead we choose an improper, constant prior to avoid the need to estimate likely values for K a priori. However, it is still possible to work with infinite free energies because we are only inter-ested in the relative change in free energy after increasing K , which is a finite quantity.
 In our experiments we chose a = 1 and b = 0 . 5 , so that the MAP prior value of  X  k is 0 . 2.3. Clumping Data-Items and Documents We will now tie some of the variational distributions { q across different data-items within and across documents (images) to a  X  X lump distribution X  q ck . Similarly, we will tie some document specific distributions over topics { q (  X  j ) } into a document group q (  X  s ) . Note that since we impose constraints on the variational distributions this has the effect of loosening the variational bound.
 Define D s to be the number of documents in a document group, N c the number of data-items in a word clump, N cs the number of words in document group s and word clump c and finally  X  c kl , P ij  X  c  X  kl ( x ij ) . In terms of these we further define, With these definitions we derive the following  X  X lumped X  update rules for the variational parameters  X  kl and  X  ks and q The update for  X  becomes An expression for the free energy, after inserting expres-sions 18, 19 and 20, is given by eq. 29 in the appendix. Our algorithm processes data in small groups composed of E documents, which we refer to as epochs . After the ar-rival of each epoch the algorithm proceeds in two stages: a model building phase during which a new model estimate is produced, and a compression phase in which decisions are made as to which words and documents to clump. The sufficient statistics of each clump are computed and data summarized by clumps are purged from memory. The as-signment distributions q ( z ) of purged data and topic distri-butions of merged documents q (  X  ) are discarded as well. The clump sufficient statistics are retained along with the current model estimate, which serves as a starting point for the next round of learning.
 Model Building Phase (Algorithm 3.1)
Input: Previous model {  X  kl , X  ks , X  k ,  X  c kl ,N cs ,D current epoch of E documents.
 Initialize  X  jk =  X  k for j = | S | + 1 ,  X  X  X  , | S | + E
Iterate eqs. 21, 18, 19, 20, and 22 until convergence repeat until Change in eq. 29 is less than threshold 3.1. Model Building Phase The model building phase optimizes the free energy un-der the parameter tying constraints induced by the choice of clumps in previous compression phases. We perform a split-merge procedure similar to (Ueda et al., 1999) to de-termine the number of topics, using the heuristics in that work to rank topic suitability for split or merge. In our ex-periments we use Gaussian topic distributions, so splits are proposed along the principal component of the topic. The split proposals are refined by restricted variational updates. That is: equations 21, 18, 19, 20, and 22 are iterated but only for data-points whose highest responsibility is to the split topic, and the points may be assigned only to the two descendent topics. Merges are carried out by instanti-ating a new topic with the data-points with highest respon-sibility to the merged topics. A total of 10 splits and 10 merges are proposed, and evaluated by the resultant change in free energy (eq. 29). The top ranked change is then used to initialize full variational updates (which involve all data points). The model building phase halts once the change in free energy divided by its previous value is below a thresh-old, which was chosen to be 1 E  X  5 in our experiments. The procedure is summarized in algorithm 3.1. 3.2. Compression Phase The goal of the compression phase is to determine groups of data-points that are to be summarized by clumps, and to identify documents that are to be merged into document groups.
 Clumps are identified using a greedy top down splitting procedure. Because datapoints summarized by clumps are ultimately discarded, the compression process is irre-versible. Therefore it is of fundamental importance to pre-dict the locations of future data when deciding which points to clump. In order to estimate this, we rank cluster splits ac-cording to a modified free energy (eq. 30) in which the data sample size is artificially increased by a factor T pts P the number of documents is scaled by T docs P and T docs are the target number of data-points and docu-ments expected during the lifetime of the system. This is equivalent to using the data empirical distribution as a pre-dictive model of future data. If we determine clumps using the standard free energy, then the algorithm fails to split large groups of points that are likely to split once more data has arrived. Instead, it wastes memory by placing  X  X tray X  points in their own clumps.
 We initialize the process by hard assigning each clump or data-point to the cluster with highest responsibility dur-ing the previous model building phase. We then proceed through each cluster and split it along the principal compo-nent, and refine this split by iterating restricted variational updates equations for the points in the cluster. The updates are modified by the data magnification factors: Updates for q ck and  X  ks are unchanged. After the clusters are refined, the data-points are then hard assigned to the sub-cluster with greatest responsibility, and the proposed split is ranked according to the resultant change in eq. 30. We then greedily split the cluster with highest rank. The process repeats itself, with new clusters ranked in the same way described above. We cache the results of each split evaluation to avoid redundant computation. After we have reached a given memory bound we extract the partitions resulting from this recursive splitting procedure as our new clumps.
 Each clump must store sufficient statistics for full covari-ance Gaussian components which require d 2 +3 d 2 values, where d is the dimension of the feature space. In addi-tion, | S | (the number of document groups) values must be Clump Compression (Algorithm 3.2)
Input: Output from model building phase: { q ck ,  X  c kl ,N cs ,D s } , current epoch of E documents and memory bound M .

Hard partition clumps: r c = arg max k q ck while MC &lt; M (eq. 26) do end while stored to represent the counts N cs for each clump. Note that from this perspective, it only makes sense to create clumps within a cluster if it contains more than d +3 2 + data-points. If not, then it is more efficient to store the indi-vidual data-points and we refer to them as  X  X inglets X . The total memory cost of summarizing the data is then MC = where | N c &gt; 1 | is the number of clumps with more than 1 data-item in them, and | N c = 1 | is the number of sin-glets. The clump compression procedure is summarized in algorithm 3.2.
 Document merging provides another way of controlling the memory cost, by reducing the number of image groups | S | . We use the following simple heuristic to rank the suitability of merging document groups s and s 0 : Clumping and document merging enable a number of po-tential schemes for controlling space and time costs, de-pending on the application. We note that the time com-plexity per variational iteration scales as O ( K ( | N c | N c = 1 | ) + | S | K ) and the space required to store q ( z distributions is O ( K ( | N c &gt; 1 | + | N c = 1 | )) . We test our approach with two machine vision experiments. The first is an image segmentation task, and the second is an object recognition and retrieval task. 4.1. Joint Image Segmentation Our first experient is a joint image segmentation problem. The dataset is the Faces-Easy category of the Caltech 101 image dataset (Fei-Fei et al., 2004) consisting of 435 im-ages. Each image contains a face centered in the image, but the lighting conditions and background vary. In terms of the vocabulary of the preceding sections, each image is a document and each pixel in the image is a word. Pixels are represented as five dimensional vectors of the following features: X and Y position relative to the center of the im-age, and three color coordinates in the CIELAB colorspace. The goal of our experiment is to find similar image regions across the multiple images, in an unsupervised way. We emphasize that our main objective is to study the efficiency of our algorithm, not to produce a state of the art image segmentation algorithm.
 The images were scaled to be 200 by 160 pixels in size. Thus, the total size of the dataset is 32,000 pixels per im-age, times 435 images, times 5 features per pixel equals 69,600,000 real numbers. Each pixel requires an assign-ment distribution. Our baseline implementation (i.e. a batch algorithm that processes all images in memory at once and does not use pixel clumping or image merging) was only able to jointly segment 30 images simultaneously, before running out of memory. The majority of memory is used to store the assignment distributions of pixels, and this is problematic as the number of topics increases during learning, since the space requirements scale as O ( NK ) , where N is the total number of pixels and K is the number of topics.
 We first compare the memory bounded approach to the baseline implementation on a joint segmentation task of 30 images in order to judge the impact of the pixel clumping approximation. We vary the upper limit on the number of clumps used to summarize the data during the compression phase, and compare the free energy bounds produced by the memory bounded algorithm to those produced by the baseline implementation. We define the free energy ratio ferent subsets of 30 images from the dataset. In the mem-ory bounded approach, images were processed in epochs of five images at a time. Figure 1 summarizes the results. We find that performance tends to saturate beyond a certain number of clumps.
 We also note a significant run time advantage of the mem-ory bounded algorithm over the batch method. The average run time of the batch method was 3 . 09 hours versus 0 . 68 hours for the memory bounded approach.
 Next we study the impact of image (document) merges on the relative performance of the memory bounded algorithm versus the baseline batch algorithm, while varying the max-imum number of image (document) groups permitted. The results are shown in figure 1.
 We find little qualitative difference between segmentations produced by the baseline and memory bounded algorithms. The possible exception is in the case when the memory bounded algorithm is run with a large number of image merges, in which case the algorithm seemed to discover fewer topics than the batch and memory bounded algorithm with only word clumping. Example image segmentations and clump distributions are shown in figure 2.
 Finally, we demonstrate the memory bounded algorithm on the full dataset of 435 images, which is more than an order of magnitude larger than can be handled with the baseline algorithm. We process images in epochs of 10 images at a time, for a total of 44 learning rounds. The upper limit on the number of clumps was set to 1000, which was likely many more than required since there were only 85 inferred topics. Because the number of documents was relatively small, we chose not to use document merges. The total run time of the algorithm was 15 hours. Figure 3 shows the number of topics as a function of the number of im-ages processed, and the run time required during each im-age round. The run time is longer during learning rounds in which more new topics are discovered, because more split-merge operations are necessary. The memory required for the memory bounded algorithm was 22 MB to store the cur-rent image epoch and clumps, less than 1MB for the current model estimate, and 235 MB for assignment distributions, for a total of 257 MB. In contrast, the baseline batch imple-mentation would have required 531 MB to store all 435 im-ages, 8.8155 GB to store assignment distributions for each pixel assuming 85 topics, and less than 1 MB for the model, for a total of 9.3 GB. (All memory amounts assume double precision floating point.) The memory bounded implemen-tation therefore achieved a memory savings factor of about 38 with very little loss in accuracy.
 Figure 4 shows example joint segmentations produced by the memory bounded algorithm. These images were re-trieved by first computing responsibilities for every image in the dataset, with respect to the final model estimate pro-duced by the MB algorithm. Then, the images were sorted according to those that have the most pixels assigned to the largest topic. The largest topic indeed corresponds to a face, and is represented by the olive green segment in the figure. Other topics shared across images include hair and certain backgrounds. 4.2. Object Recognition and Retrieval Our object recognition and retrieval experiment involves all 101 object categories in the Caltech 101 dataset. We ran-domly select 3000 training images and 1000 test images. We extract 128-dimensional SIFT (Lowe, 2004) local ap-pearance descriptors from 500 randomly chosen locations in each image. The scale of each feature is also chosen ran-domly. In the language of topic models, each feature de-scriptor is a word, and the collection of feature descriptors in an image forms a document. This image representation is known as  X  X ag-of-features X , because images are modeled as unordered collections of feature descriptors whose geo-metric positions are ignored. This dataset proved too large to compare directly to the batch algorithm We train a single topic model on all training images, us-ing epochs of 60 images at a time. Because hundreds of topics are discovered we use diagonal covariance Gaus-sians and adjust equation 26 accordingly. Given a test im-age  X  x , retrieval is performed by ranking each training im-age X  X  similarity to the test image. To develop the similarity measure we begin with log Q i p (  X  x ij | x ) , which is the log-probability that the detections in the test image were gen-erated by training image j given the training set. Then we variationally lower bound this quantity to obtain a test free energy and drop all constant terms not involving the test image and index j . Finally we lower bound this quantity by assuming that detections in the test image are hard as-signed to the topic with highest responsibility (this leads to an expression that is much faster to evaluate with neglible impact on retrieval performance.) The retrieval score is: where the expectations are with respect to q (  X  ) learned dur-ing training and  X  kl and  X  kj are from training as well.  X  are re-estimated for images that were merged into a docu-ment group during training. We compute nearest neighbor (1-NN) classification accuracy by classifying the test im-age to the class label of the highest scoring image in the training set.
 Figure 5 shows the training set free energy and 1-NN class-fication accuracy as a function of the memory bound M (measured as the equivalent number of data points that could be stored in the same space.) Because we used diag-onal covariance matrices, there were enough clumps even at low levels of memory to maintain comparable classifi-cation performance. We note that the training free energy increases with memory as expected, and that the 1-NN ac-curacy tends to saturate as memory increases.
 Figure 6 shows the 1-NN accuracy and training free en-ergy when the percentage of document groups relative to the number of total images processed is varied (the mem-ory bound M is held fixed at 10000). We note that the classification performance suffers substantially when only small numbers of document groups are permitted. We use a heuristic for determining documents to merge (eq. 27). It is possible that a well motivated criterion (perhaps derived from the free energy) would give better performance. Machine learning has largely focussed on algorithms that run for a relatively short period of time, fitting models of finite capacity on a data-set of fixed size. We believe that this scenario is unrealistic if we aim at building truly intel-ligent systems. We have identified nonparametric Bayesian models as promising candidates that expand their model complexity in response to new incoming data. The flip-side is that nonparametric Bayesian algorithms are  X  X xample-based X  and as such require one to cache and process repeat-edly every data-case ever seen. The objectives of infinite, adaptive model capacity on the one hand and efficiency, both in time and space on the other therefore seem to be fundamentally at odds with each other.
 In this paper we have made a first step towards resolving this issue by introducing a class of models that can adapt their model complexity adaptively but are able to do so at a fraction of the memory requirements and processing times necessary for their batch counterparts. There is no magic of course: with a fixed memory budget there is a limit to how complex the model can be, but we have shown that one can learn much larger models reliably with much less memory than a naive implementation would allow. Moreover, our learning algorithms allow a flexible tradeoff between mem-ory requirements and model complexity requirements that can be adapted online.
 Intuitively, our method may be thought of as a two level clustering process. At the bottom level, data is clustered into clumps in order to limit time and space costs. At the top level, clumps are clustered to form topics in order to ensure good generalization performance.
 Potential application areas of the techniques introduced here are manyfold. For instance, we can imagine learning topic models from very large text corpora or the world wide web to understand its structure and facilitate fast searching algorithms. Another exciting direction is to build a taxon-omy of visual object categories from a continuous stream of video data captured by surveillance cameras. 5.1. Acknowledgements We thank the anonymous reviewers for their helpful com-ments. This material is based on work supported by the Na-tional Science Foundation under grant numbers 0447903 and 0535278, the Office of Naval Research under grant numbers 00014-06-1-0734 and 00014-06-1-0795, and The National Institutes of Health Predoctoral Training in Inte-grative Neuroscience grant number T32 GM007737. 5.2. Appendix The following expressions for the free energy are used in the main text. Note that they are only valid after the updates for  X  and  X  have been performed.

