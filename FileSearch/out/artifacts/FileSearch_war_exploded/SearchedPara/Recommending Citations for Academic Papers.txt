 We approach the problem of academic literature search by considering an unpublished manuscript as a query to a search system. We use the text of previous literature as well as the citation graph that connects it to find relevant related material. We evaluate our technique with manual and auto-matic evaluation methods, and find an order of magnitude improvement in mean average precision as compared to a text similarity baseline.
 Categories and Subject Descriptors: H3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Design, Experimentation Keywords: Bibliometrics
Most current literature search systems concentrate on short queries that are unlikely to describe fine details of the user X  X  true information need. In this work, we instead suppose that the user is able to provide the system with a very long query; we assume that the user has already written a few pages about the topic, and is able to submit this document to the search system as the query. We conjecture that this additional information can improve the effectiveness of the ranked list of documents. Instead of assuming that the user wants documents that are topically similar to the query, we assume the user wants documents that the query document might cite. This is particularly challenging because the con-cept of relevance is much stricter than in ad hoc retrieval; most papers could cite hundreds of topically similar papers, but contain just a few highly relevant citations.

We have built a system to explore this citation recommen-dation problem. In the process, we have found that simple text similarity computation is not enough for this task. We show that it is necessary to use graph-based features in the retrieval process to achieve high quality retrieval results, al-though many seemingly useful features offer little benefit.
Our system uses a two stage process to find a set of doc-uments to rank. In the first step, the system searches a collection of over a million papers, and returns the top 100 most similar papers to the query document as the set R . In the second step, all papers cited by any paper in R are significantly outperform the  X  X o Katz X  method (Wilcoxon, p = 0 . 01 )
To evaluate this system, we treated published research pa-pers as queries. These papers were drawn from an early copy of the Rexa 1 database (Table 2). Note that while there are almost a million entries in this collection, only about 10% of them contain the full text of the paper. We performed a small manual evaluation of search result quality, but space restrictions keep us from reporting those results here. 2 . To evaluate the system without manual intervention, we consid-ered the references list from the query paper as the relevant citations, then evaluated our retrieval system on its ability to find the references in this list. We chose 1000 documents from the Rexa collection to use as sample queries. In order to have the best possible generalization to full text collec-tions, we chose query documents where a large percentage of their citations were full-text Rexa entries.

We used a text similarity baseline, which is the first stage of our experimental algorithm, with no additional features. Since other models may return more than 100 documents, we also perform a truncated evaluation for each model, where only the top 100 documents are considered. The truncated column allows a fair comparison between the text similarity baseline and the other models.

In order to assess the usefulness of particular features, we performed experiments that removed each feature from the model in isolation. We expect that if a feature is very useful, the retrieval effectiveness of the system will drop dra-matically when a feature is removed; if it is not useful, we expect effectiveness to stay the same. Note that we did not re-train the model for these tests; we only set the weight of the removed feature to zero.
The results of our experiments are shown in Table 3. Our experimental results show the effectiveness of our system in various modes against a text similarity baseline. The confidence intervals come from the t distribution. We also performed the distribution-free Wilcoxon signed rank test ( p&lt; 0 . 01) for significance. From this, we find that all ex-perimental models significantly outperform the text similar-ity baseline. Also, we find that the  X  X o Katz X  experimental model is significantly outperformed by all other experimen-http://www.rexa.info
Full details in the technical report version of this paper.
