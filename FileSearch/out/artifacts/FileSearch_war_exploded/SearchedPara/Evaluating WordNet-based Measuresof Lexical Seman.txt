 University of Toronto
WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content X  X ased measure proposed by Jiang and
Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness. 1. Introduction
The need to determine semantic relatedness or its inverse, semantic distance, be-tween two lexically expressed concepts is a problem that pervades much of natural language processing. Measures of relatedness or distance are used in such applica-tions as word sense disambiguation, determining the structure of texts, text sum-marization and annotation, information extraction and retrieval, automatic indexing, to note that semantic relatedness is a more general concept than similarity; similar entities are semantically related by virtue of their similarity ( bank X  X rust company ), but meronymy ( car X  X heel ) and antonymy ( hot X  X old ), or just by any kind of functional rela-tionship or frequent association ( pencil X  X aper, penguin X  X ntarctica, rain X  X lood ). Computa-tional applications typically require relatedness rather than just similarity; for example, money and river are cues to the in-context meaning of bank that are just as good as trust company .
 competing approaches that have been proposed for determining lexical semantic re-latedness. Given a measure of relatedness, how can we tell whether it is a good one or a poor one? Given two measures, how can we tell whether one is bet-ter than the other, and under what conditions it is better? And what is it that makes some measures better than others? Our purpose in this paper is to compare the performance of a number of measures of semantic relatedness that have been proposed for use in applications in natural language processing and information retrieval. 1.1 Terminology and Notation
In the literature related to this topic, at least three different terms are used by differ-ent authors or sometimes interchangeably by the same authors: semantic relatedness, similarity, and semantic distance.
 of example.  X  Cars and gasoline  X , he writes,  X  X ould seem to be more closely related paper. Among other relationships that the notion of relatedness encompasses are the various kinds of meronymy, antonymy, functional association, and other  X  X on-classical relations X  (Morris and Hirst 2004).
 when talking about either just similarity or relatedness in general. Two concepts are they are  X  X istant X . Most of the time, these two uses are consistent with one another, but not always; antonymous concepts are dissimilar and hence distant in one sense, and yet are strongly related semantically and hence close in the other sense. We would thus have very much preferred to be able to adhere to the view of semantic distance as the inverse of semantic relatedness, not merely of similarity, in the present paper.
Unfortunately, because of the sheer number of methods measuring similarity, as well as those measuring distance as the  X  X pposite X  of similarity, this would have made for an awkward presentation. Therefore, we have to ask the reader to rely on context and semantically close mean in each particular case.
 acknowledging the polysemy of language, in this paper the term concept will refer to a particular sense of a given word . We want to be very clear that, throughout this paper, when we say that two words are  X  X imilar X , this is a short way of saying that they denote similar concepts; we are not talking about similarity of distributional or co-occurrence behavior of the words, for which the term word similarity has also been used (Dagan 2000; Dagan, Lee, and Pereira 1999). While similarity of denotation might be inferred from similarity of distributional or co-occurrence behavior (Dagan 2000; Weeds 2003), the two are distinct ideas. We return to the relationship between them in Section 6.2. link and edge to refer to the relationships between nodes; we prefer the former term when our view emphasizes the taxonomic aspect or the meaning of the network, and the latter when our view emphasizes algorithmic or graph-theoretic aspects. In running text, examples of concepts are typeset in sans-serif font, whereas examples of words are given in italics; in formulas, concepts and words will usually be denoted by c taken the liberty of altering the original notation accordingly in some other authors X  formulas. 2. Lexical Resource X  X ased Approaches to Measuring Semantic Relatedness
All approaches to measuring semantic relatedness that use a lexical resource construe the resource, in one way or another, as a network or directed graph, and then base the measure of relatedness on properties of paths in this graph. 14 2.1 Dictionary-based Approaches
Kozima and Furugori (1993) turned the Longman Dictionary of Contemporary English (
LDOCE ) (Procter 1978) into a network by creating a node for every headword and word controlled defining vocabulary of LDOCE thus becomes the densest part of the network: the remaining nodes, which represent the headwords outside of the defining vocabulary, can be pictured as being situated at the fringe of the network, as they are linked only to defining-vocabulary nodes and not to each other. In this network, the similarity function sim KF between words of the defining vocabulary is computed by of LDOCE by representing each word as a list W = { w definition; thus, for instance, measure that takes into account the  X  X ssociative direction X  of a given word pair. For example, the context { car, bus } imposes the associative direction of vehicle (close words are then likely to include taxi, railway, airplane , etc.), whereas the context imposes the direction of componentsofcar ( tire, seat, headlight ,etc.). 2.2 Approaches Based on Roget-structured Thesauri
Roget-structured thesauri, such as Roget X  X  Thesaurus itself, the Macquarie Thesaurus (Bernard 1986), and others, group words in a structure based on categories within which there are several levels of finer clustering. The categories themselves are grouped into a number of broad, loosely defined classes. However, while the classes and categories are named, the finer divisions are not; the words are clustered without attempting to the index, which contains category numbers along with labels representative of those categories for each word. Polysemes are implicitly disambiguated, to a certain extent, by the other words in their cluster and in their index entry. Closely related concepts might or might not be physically close in the thesaurus:  X  X hysical closeness has some categories, and each category often points to a widely scattered selection of cate-gories X  (Morris and Hirst 1991). Methods of semantic distance that are based on Roget-index and on the pointers within categories that cross-reference other categories. In part as a consequence of this, typically no numerical value for semantic distance can be obtained: rather, algorithms using the thesaurus compute a distance implicitly and return a boolean value of  X  X lose X  or  X  X ot close X .
 identified five types of semantic relations between words. In their approach, two words satisfy any one of the following conditions: 1. They have a category in common in their index entries. 2. One has a category in its index entry that contains a pointer to a category 3. One is either a label in the other X  X  index entry or is in a category of the 4. They are both contained in the same subcategory. 5. They both have categories in their index entries that point to a common
These relations account for such pairings as wife and married , car and driving , blind and perhaps the most intuitively plausible ones  X  the first two in the list above  X  were found to validate over 90% of the intuitive lexical relationships that the authors used as a benchmark in their experiments.

Thesaurus ; but because this measure is based strictly on hierachy rather than the index structure, we discuss it in Section 2.4 below. 2.3 Approaches Using WordNet and Other Semantic Networks
Most of the methods discussed in the remainder of Section 2 use WordNet (Fellbaum 1998), a broad coverage lexical network of English words. Nouns, verbs, adjectives, and adverbs are each organized into networks of synonym sets ( synsets ) that each represent one underlying lexical concept and are interlinked with a variety of relations. (A polysemous word will appear in one synset for each of its senses.) In the first versions of WordNet (those numbered 1. x ), the networks for the four different parts of speech were not linked to one another. 1 The noun network of WordNet was the first to be richly developed, and most of the researchers whose work we will discuss below therefore limited themselves to this network. 2 hypernymy ), which accounts for close to 80% of the relations. At the top of the hier-archy are 11 abstract concepts, termed unique beginners, such as entity ( X  X omething having concrete existence; living or nonliving X ) and psychologicalfeature ( X  X  feature of the mental life of a living organism X ). The maximum depth of the noun hierarchy is 16 nodes. The nine types of relations defined on the noun subnetwork, in addition to the synonymy relation that is implicit in each node are: the hyponymy ( 16 and its inverse, hypernymy; six meronymic ( PA RT -MEMBER -OF and SUBSTANCE -OF and their inverses; and antonymy, the OF relation.
 2.4 Computing Taxonomic Path Length
A simple way to compute semantic relatedness in a taxonomy such as WordNet is to view it as a graph and identify relatedness with path length between the concepts: 1995). This approach was taken, for example, by Rada and colleagues (Rada et al. 1989;
Rada and Bicknell 1989), not on WordNet but on MeSH (Medical Subject Headings), a semantic hierarchy of terms used for indexing articles in the bibliographic retrieval system Medline. The network X  X  15,000 terms form a nine-level hierarchy based on the
BROADER -THAN relationship. The principal assumption of Rada and colleagues was that  X  X he number of edges between terms in the MeSH hierarchy is a measure of conceptual distance between terms X . Despite the simplicity of this distance function, the authors were able to obtain surprisingly good results in their information retrieval task. In part, their success can be explained by the observation of Lee, Kim, and
Lee (1993) that while  X  X n the context of . . . semantic networks, shortest path lengths between two concepts are not sufficient to represent conceptual distance between those concepts . . . when the paths are restricted to IS does measure conceptual distance. X  Another component of their success is certainly the specificity of the domain, which ensures relative homogeneity of the hierarchy.
Notwithstanding these qualifications, Jarmasz and Szpakowicz (2003) also achieved good results with Roget X  X  Thesaurus by ignoring the index and treating the thesaurus as a simple hierarchy of clusters. They computed semantic similarity between two words as the length of the shortest path between them. The words were not explicitly disambiguated. tic distance algorithm from Roget X  X  Thesaurus to WordNet. strengths of semantic relations in WordNet. Two words are strongly related if one of the following holds: 1. They have a synset in common (for example, human and person ). 2. They are associated with two different synsets that are connected by the 3. One of the words is a compound (or a phrase) that includes the other and
Two words are said to be in a medium-strong, or regular, relation if there exists an allowable path connecting a synset associated with each word (for example, carrot and apple ).Apathis allowable if it contains no more than five links and conforms to one of eight patterns, the intuition behind which is that  X  X he longer the path and the more changes of direction, the lower the weight X . The details of the patterns are outside of the scope of this paper; all we need to know for the purposes of subsequent discussion links on the same path may vary (among upward ( hypernymy and meronymy ), downward ( hyponymy and holonymy )and horizontal ( antonymy )). Hirst and St-Onge X  X  approach may thus be summarized by the following formula for two WordNet concepts c where C and k are constants (in practice, they used C = 8and k = 1), and turns( c the number of times the path between c 1 and c 2 changes direction. 2.5 Scaling the Network
Despite its apparent simplicity, a widely acknowledged problem with the edge-counting approach is that it typically  X  X elies on the notion that links in the taxonomy represent uniform distances X , which is typically not true:  X  X here is a wide variability taxonomies (e.g., biological categories) are much denser than others X  (Resnik 1995).
For instance, in WordNet, the link rabbitears IS -A televisionantenna narrow distance, whereas whiteelephant IS -A possession covers an intuitively wide one.
The approaches discussed below are attempts undertaken by various researchers to overcome this problem. 2.5.1 Sussna X  X  Depth-relative Scaling. Sussna X  X  (1993, 1997) approach to scaling is based on his observation that sibling-concepts deep in a taxonomy appear to be more closely related to one another than those higher up. His method construes each edge 18 in the WordNet noun network as consisting of two directed edges representing inverse relations. Each relation r has a weight or a range [min r it: for example, hypernymy, hyponymy, holonymy, and meronymy have weights between min r = 1 and max r = 2. 4 The weight of each edge of type r from some node c reduced by a factor that depends on the number of edges, edges leaving c 1 :
The distance between two adjacent nodes c 1 and c 2 is then the average of the weights on each direction of the edge, scaled by the depth of the nodes: where r is the relation that holds between c 1 and c 2 and r is its inverse (i.e., the relation that holds between c 2 and c 1 ). Finally, the semantic distance between two arbitrary nodes c i and c j is the sum of the distances between the pairs of adjacent nodes along the shortest path connecting them. 2.5.2 Wu and Palmer X  X  Conceptual Similarity. In a paper on translating English verbs into Mandarin Chinese, Wu and Palmer (1994) introduce a scaled metric for what they call conceptual similarity between a pair of concepts c 1 factor can be seen more clearly if we recast equation 5 from similarity into distance: 2.5.3 Leacock and Chodorow X  X  Normalized Path Length. Leacock and Chodorow (1998) proposed the following formula for computing the scaled semantic similarity between concepts c 1 and c 2 in WordNet:
Here, the denominator includes the maximum depth of the hierarchy. 2.6 Information-based and Integrated Approaches
Like the methods in the preceding subsection, the final group of approaches that we present attempt to counter problems inherent in a general ontology by incorporating an additional, and qualitatively different, knowledge source, namely information from a corpus. 2.6.1 Resnik X  X  Information-based Approach. The key idea underlying Resnik X  X  (1995) approach is the intuition that one criterion of similarity between two concepts is  X  X he extent to which they share information in common X , which in an be determined by inspecting the relative position of the most-specific concept that subsumes them both. This intuition seems to be indirectly captured by edge-counting methods (such as that of Rada and colleagues; section 2.4 above), in that  X  X f the minimal path of IS -A links between two nodes is long, that means it is necessary to go high in the taxonomy, to more abstract concepts, in order to find a least upper bound X . An example given by Resnik is the difference in the relative positions of the most-specific subsumer of nickel and dime  X  coin  X  and that of nickel and creditcard  X  mediumofexchange ,as seen in Figure 1.
 ity of encountering an instance of concept c . Following the standard definition from information theory, the information content of c ,IC( c ), is then define the semantic similarity of a pair of concepts c 1 and c
Notice that p is monotonic as one moves up the taxonomy: if c
For example, whenever we encounter a nickel , we have encountered a coin (Figure 1), subsumer for given two concepts in the taxonomy (i.e., the more abstract it is), the lower will be 1, so if the most-specific subsumer of a pair of concepts is the top node, their similarity will be  X  log(1) = 0, as desired.
 20 mated from noun frequencies gathered from the one-million-word Brown Corpus of method is that an individual occurrence of any noun in the corpus  X  X as counted as an occurrence of each taxonomic class containing it X . For example, an occurrence of the noun nickel was, in accordance with Figure 1, counted towards the frequency of nickel,coin , and so forth. Notice that, as a consequence of using raw (non-disambiguated) data, encountering a polysemous word contributes to the counts of all its senses. So in the case of nickel , the counts of both the coin and the metal senses will be increased.
Formally, where W ( c ) is the set of words (nouns) in the corpus whose senses are subsumed by concept c ,and N is the total number of word (noun) tokens in the corpus that are also present in WordNet.
 (see Section 2.5) by generally downplaying the role of network edges in the determi-nation of the degree of semantic proximity: Edges are used solely for locating super-ordinates of a pair of concepts; in particular, the number of links does not figure in any of the formulas pertaining to the method; and numerical evidence comes from corpus of the taxonomy has its drawbacks, one of which is the indistinguishability, in terms of semantic distance, of any two pairs of concepts having the same most-specific subsumer. For example, in Figure 1, we find that sim R ( money , credit ) = sim cause in each case the lso is mediumofexchange , whereas, for an edge-based method such as Leacock and Chodorow X  X  (Section 2.5.3), clearly this is not so, as the number of edges in each case is different. 2.6.2 Jiang and Conrath X  X  Combined Approach. Reacting to the disadvantages of
Resnik X  X  method, Jiang and Conrath X  X  (1997) idea was to synthesize edge-and node-based techniques by restoring network edges to their dominant role in similarity com-putations, and using corpus statistics as a secondary, corrective factor. A complete exegesis of their work is presented by Budanitsky (1999); here we summarize only their conclusions.
 semantic distance of the link connecting a child-concept c to its parent-concept par ( c ) is proportional to the conditional probability p( c | par ( c )) of encountering an instance of c givenaninstanceof par ( c ). More specifically,
By definition,
If we adopt Resnik X  X  scheme for assigning probabilities to concepts (Section 2.6.1), its parent. Then, and, recalling the definition of information content, the semantic distance between an arbitrary pair of nodes was taken, as per common practice, to be the sum of the distances along the shortest path that connects the nodes: where Path ( c 1 , c 2 ) is the set of all the nodes in the shortest path from c panding the sum in the right-hand side of equation (14), plugging in the expression for parent X  X hild distance from equation (13), and performing necessary eliminations results in the following final formula for the semantic distance between concepts c 2.6.3 Lin X  X  Universal Similarity Measure. Noticing that all of the similarity measures known to him were tied to a particular application, domain, or resource, Lin (1998b) attempted to define a measure of similarity that would be both universal (applicable to arbitrary objects and  X  X ot presuming any form of knowledge representation X ) and theoretically justified ( X  X erived from a set of assumptions X , instead of  X  X irectly by a formula X , so that  X  X f the assumptions are deemed reasonable, the similarity measure necessarily follows X ). He used the following three intuitions as a basis: 1. The similarity between arbitrary objects A and B is related to their 2. The similarity between A and B is related to the differences between them; 3. The maximum similarity between A and B is reached when A and B are
Lin defined the commonality between A and B as the information content of  X  X he proposition that states the commonalities X  between them, formally 22 and the difference between A and B as where descr( A , B ) is a proposition describing what A and B are.
 Lin proved the following: theorem: (equation (9)). 3. Evaluation Methods
How can we reason about and evaluate computational measures of semantic related-ness? Three kinds of approaches are prevalent in the literature.
 posed measure for those mathematical properties thought desirable, such as whether parameter-projections are smooth functions, and so on. In our opinion, such analyses act at best as a coarse filter in the comparison of a set of measures and an even coarser one in the assessment of a single measure.
 human judgments of similarity and relatedness are deemed to be correct by definition, this clearly gives the best assessment of the  X  X oodness X  of a measure. Its main drawback lies in the difficulty of obtaining a large set of reliable, subject-independent judgments for comparison  X  designing a psycholinguistic experiment, validating its results, and so on. (In Section 4.1 below, we will employ the rather limited data that such experiments have obtained to date.) in the framework of a particular application. If some particular NLP system requires a measure of semantic relatedness, we can compare different measures by seeing which constant.
 to compare several different measures (Sections 4 and 5 respectively). We focus on measures that use WordNet (Fellbaum 1998) as their knowledge source (to keep that as a constant) and that permit straightforward implementation as functions in a programming language. Therefore, we select the following five measures: Hirst and
St-Onge X  X  (Section 2.4), Jiang and Conrath X  X  (Section 2.6.2), Leacock and Chodorow X  X  (Section 2.5.3), Lin X  X  (Section 2.6.3), and Resnik X  X  (Section 2.6.1). a measure of semantic relatedness because it uses all noun relations in WordNet; the others are claimed only as measures of similarity because they use only the hyponymy relation. We implemented each measure, and used the Brown Corpus as the basis for the frequency counts needed in the information-based approaches. 4. Comparison with Human Ratings of Semantic Relatedness
In this section we compare the five chosen measures by how well they reflect human in this section to set closeness thresholds for the application-based evaluation of each measure in Section 5. 4.1 Data similarity of meaning (synonymy) X , Rubenstein and Goodenough (1965) obtained  X  X ynonymy judgements X  from 51 human subjects on 65 pairs of words. The pairs ranged from  X  X ighly synonymous X  to  X  X emantically unrelated X , and the subjects were asked to rate them, on the scale of 0.0 to 4.0, according to their  X  X imilarity of mean-4...), 10 from the intermediate level (bet ween 1 and 3), and 10 from the low level (0 to 1) of semantic similarity X , and then obtained similarity judgments from 38 sub-jects, given the same instructions as above, on those 30 pairs (see Table 2, columns 2 and 3). 7 4.2 Method
For each of our five implemented measures, we obtained similarity or relatedness scores for the human-rated pairs. Where either or both of the words had more than one synset in WordNet, we took the most-related pair of synsets. For the measures of Resnik, Jiang and Conrath, and Lin, this replicates and extends a study by each of the original authors of their own measure. 24 4.3 Results
The mean ratings from Rubenstein and Goodenough X  X  and Miller and Charles X  X  original experiments (labeled  X  X umans X ) and the ratings of the Rubenstein X  X oodenough and Miller X  X harles word pairs produced by (our implementations of) the Hirst X  X t-Onge,
Jiang X  X onrath, Leacock X  X hodorow, Lin, and Resnik measures of relatedness are given in Tables 1 and 2, and in Figures 2 and 3. 8 4.4 Discussion
When comparing two sets of ratings, we are interested in the strength of the linear asso-ciation between two quantitative variables, so we follow Resnik (1995) in summarizing the comparison results by means of the coefficient of correlation of each computational measure with the human ratings; see Table 3. (For Jiang and Conrath X  X  measure, the coefficients are negative because their measure returns distance rather than similarity; so for convenience, we show absolute values in the table.) 4.4.1 Comparison to Upper Bound. To get an idea of the upper bound on perfor-mance of a computational measure, we can again refer to human performance. We have such an upper bound for the Miller and Charles word pairs (but not for the complete set of Rubenstein and Goodenough pairs): Resnik (1995) replicated Miller and 26 Charles X  X  experiment with 10 subjects and found that the average correlation with the
Miller X  X harles mean ratings over his subjects was 0.8848. While the difference between the (absolute) values of the highest and lowest correlation coefficients in the  X  X &amp;C X  column of Table 3 is of the order of 0.1, all of the coefficients compare quite favorably with this estimate of the upper bound; furthermore, the difference diminishes almost twofold as we consider the larger Rubenstein X  X oodenough dataset (column  X  X &amp;G X  of
Table 3). 10 In fact, the measures are divided in their reaction to increasing the size of the dataset: the correlations of rel HS ,sim LC ,andsim R sim L deteriorate. This division might not be arbitrary: the last two depend on the same three quantities, log p( c 1 ), log p( c 2 ), and log p( lso ( c evidence, this connection remains hypothetical. 4.4.2 Differences in the Performance and Behavior of the Measures. We now examine the results of each of the measures and the differences between them. To do this, we will sometimes look at differences in their behavior on individual word pairs.

Hirst X  X t-Onge and Leacock X  X hodorow measures is much more apparent than that of the others: i.e., the values that they can take on are just a fixed number of levels .Thisis, of course, a result of their being based on the same highly discrete factor: the path length.
As a matter of fact, a more substantial correspondence between the two measures can be recognized from the graphs and explained in the same way. In each dataset, the upper portions of the two graphs are identical: namely, the sets of pairs affording the highest and second-highest values of the two measures (rel happens because these sets are composed of WordNet synonym and parent-child pairs, respectively. 11 still follow each other quite closely in the middle region (2.4 X 3.2 for sim rel HS ). For the larger set of Rubenstein and Goodenough X  X , however, differences appear.
The pair automobile X  X ushion (#22), for instance, is ranked even with magician X  X racle (#37) by the Hirst X  X t-Onge measure but far below both magician X  X racle (#37) and bird X  X rane (#42) by Leacock X  X hodorow (and, in fact, by all the other measures). The cause of such a high ranking in the former case is the following meronymic connection in WordNet:
Since rel HS is the only measure that takes meronymy (and other WordNet relations beyond IS -A ) into account, no other measure detected this connection  X  nor did the hu-man judges, whose task was to assess similarity , not generic relatedness ; see Section 4.1). ferent, because rel HS assigns all weakly-related pairs the value of zero. (In fact, it is this cut-off that we believe to be largely responsible for the relatively low ranking of the correlation coefficient of the Hirst X  X t-Onge measure.) In contrast, two other measures,
Resnik X  X  and Lin X  X , behave quite similarly to each other in the low-similarity region. In particular, their sets of zero-similarity pairs are identical, because the definitions of both measures include the term log p ( lso ( c 1 , c 2 )), which is zero for the pairs in question. instance, for the pair rooster X  X oyage (M&amp;C #29, R&amp;G #2), the synsets rooster and voyage have different  X  X nique beginners X , and hence their lso  X  in fact their sole common whose probability is 1: 28 30
Analogously, although perhaps somewhat more surprisingly for a human reader, the same is true of the pair asylum X  X emetery (R&amp;G #16): sideration the other three measures, we can make a couple more observations. First, the graphs of all of the measures except Resnik X  X  exhibit a  X  X ine X  of synonyms (comprising four points for the Miller X  X harles dataset and nine points for Rubenstein X  X oodenough) at the top (bottom for Jiang and Conrath X  X  measure). In the case of Resnik X  X  measure, continuous, as one might expect from the graphs of the human judgments: For the
Miller X  X harles set, for instance, the line includes pairs 1, 2, 7, and 8, but omits pairs 3 X 6. This peculiarity is due entirely to WordNet, according to which gem and jewel (#2) and magician and wizard (#7) are synonyms, whereas journey and voyage (#3), boy and lad (#4), and even asylum and madhouse (#6) are not, but rather are related by similarity regions differ, there appears to be an interesting commonality at the level of general structure: in the vicinity of sim = 2, the plots of human similarity ratings for both the Miller X  X harles and the Rubenstein X  X oodenough word pairs display a very clear horizontal band that contains no points. For the Miller X  X harles data (Figure 3), the band separates the pair crane X  X mplement (#16) from brother X  X onk (#14),
Rubenstein-Goodenough set (Figure 2), it separates magician X  X racle (#37) from crane X  implement (#38).
 with at most a few points X  X o more than two points for the Miller X  X harles set and no more than four for the Rubenstein X  X oodenough set. These regions are shown in
Figures 3(b) X (f) and 2(b) X (f). This commonality among the measures suggests that if we were to partition the set of all word pairs into those that are deemed to be related and those that are deemed unrelated, the boundary between the two subsets for each measure (and for the human judgments, for that matter) would lie somewhere within these regions. 4.4.3 The Limitations of this Analysis. While comparison with human judgments is the ideal way to evaluate a measure of similarity or semantic relatedness, in practice the tiny amount of data available (and only for similarity, not relatedness) is quite inadequate.
But constructing a large-enough set of pairs and obtaining human judgments on them would be a very large task. 14 proach. It was implicit in the Rubenstein X  X oodenough and Miller X  X harles experiments that subjects were to use the dominant sense of the target words or mutually triggering related senses. But often what we are really interested in is the relationship between the concepts for which the words are merely surrogates; the human judgments that we need are of the relatedness of word-senses, not words. So the experimental situation would need to set up contexts that bias the sense selection for each target word and yet don X  X  bias the subject X  X  judgment of their apriori relationship, an almost self-contradictory situation. 15 5. An Application-based Evaluation of Measures of Relatedness measures that tries to overcome the problems of comparison to human judgments that were described in the previous section. Here, we compare the measures through the 32 performance of an application that uses them: the detection and correction of real-word spelling errors in open-class words, i.e., malapropisms .
 particularly appropriate for evaluating measures of semantic relatedness. Naturally words (Halliday and Hasan 1976; Morris and Hirst 1991; Hoey 1991; Morris and Hirst 2004). That is, they implicitly contain human judgments of relatedness that we could practice just which pairs of words in a text are and aren X  X  related. We can get around this problem, however, by deliberately perturbing the coherence of the text  X  that is, introduding semantic anomalies such as malapropisms  X  and looking at the ability of the different relatedness measures to detect and correct the perturbations. 5.1 Malapropism Detection and Correction as a Testbed
Our malapropism corrector (Hirst and Budanitsky 2005) is based on the idea behind that of Hirst and St-Onge (1998): Look for semantic anomalies that can be removed by small changes to spelling. 16 Words are (crudely) disambiguated where possible by accepting senses that are semantically related to possible senses of other nearby words. If all senses of any open-class, non X  X top-list word that occurs only once in the text are found to be semantically unrelated to accepted senses of all other nearby words, but some sense of a spelling variation of that word would be related (or is identical to another token in the context), then it is hypothesized that the original word is an error and the variation is what the writer intended; a user would be warned of this possibility. For example, if no nearby word in a text is related to diary but one or more are related to dairy , we suggest to the user that it is the latter that was intended. The exact window size implied by  X  X earby X  is a parameter to the algorithm, as is the precise definition of spelling variation ; see Hirst and Budanitsky (2005).

While the performance of the malapropism corrector is inherently limited by these assumptions, we can nonetheless evaluate measures of semantic relatedness by com-
Regardless of the degree of adequacy of its performance, it is a  X  X evel playing field X  for comparison of the measures. Hirst and Budanitsky (2005) discuss the practical aspects of the method and compare it with other approaches to the same problem. 5.2 Method
To test the measures in this application, we need a sufficiently large corpus of malapropisms in their context, each identified and annotated with its correction. Since no such corpus of naturally occurring malapropisms exists, we created one artificially.
Following Hirst and St-Onge (1998), we took 500 articles from the Wall Street Journal corpus and, after removing proper nouns and stop-list words from consideration, replaced one word in every 200 with a spelling variation, choosing always WordNet nouns with at least one spelling variation. 18 For example, in a sentence beginning federal court . . . , the word case was replaced by cage . This gave us a corpus with 1,408 malapropisms among 107,233 candidates. 19 We then tried to detect and correct the malapropisms by the algorithm outlined above, using in turn each of the five measures of semantic relatedness. For each, we used four different search scopes , i.e., window sizes: just the paragraph containing the target word (scope = 1); that paragraph plus one or two adjacent paragraphs on each side (scope = 3 and 5); and the complete article (scope = MAX ).
 is because the malapropism-detection algorithm requires a boolean related X  X nrelated judgment, but each of the measures that we tested instead returns a numerical value of relatedness or similarity, and nothing in the measure (except for the Hirst X  X t-Onge measure) indicates which values count as  X  X lose X . Moreover, the values from the dif-ferent measures are incommensurate. We therefore set the threshold of relatedness of
Goodenough pairs (the near-synonyms) from the lower level, as we described in Sec-tion 4.4.2. 5.3 Results
Malapropism detection was viewed as a retrieval task and evaluated in terms of preci-sion, recall, and F -measure. Observe that semantic relatedness is used at two different places in the algorithm  X  to judge whether an original word of the text is related to any nearby word and to judge whether a spelling variation is related  X  and success in malapropism detection requires success at both stages. For the first stage, we say that awordis suspected of being a malapropism (and the word is a suspect ) if it is judged to be unrelated to other words nearby; the word is a correct suspect if it is indeed a malapropism and a false suspect if it isn X  X . At the second stage, we say that, given a that the alarm is a true alarm and that the malapropism has been detected; otherwise, it is a false alarm. Then we can define precision ( P ), recall ( R ), and F -measure ( F )for 34 suspicion ( S ), involving only the first stage, and detection ( as follows: Suspicion: Detection: statistics across our collection of 481 articles, which constitute a random sample from the population of all WSJ articles. All the comparisons that we make below, except for comparisons to baseline, are performed with the Bonferroni multiple-comparison technique (Agresti and Finlay 1997), with an overall significance level of .05. 5.3.1 Suspicion. We look first at the results for suspicion  X  just identifying words that have no semantically related word nearby. Obviously, the chance of finding some word that is judged to be related to the target word will increase with the size of the scope of the search (with a large enough scope, e.g., a complete book, we would probably find a relative for just about any word). So we expect recall to decrease as scope increases, because some relationships will be found even for malapropisms (i.e., there will be more false negatives). But we expect that precision will increase with scope, as it becomes more likely that (genuine) relationships will be found for non-malapropisms (i.e., there will be fewer false positives), and this factor will outweigh the decrease in the overall number of suspects found.
 combinations of measure and scope. The values of precision range from 3.3% (Resnik, scope = 1) to 11% (Jiang X  X onrath, scope = MAX ), with a mean of 6.2%, increasing with scope, as expected, for all measures except Hirst X  X t-Onge. More specifically, differences in precision are statistically significant for the difference between scope = 5 and scope = MAX for Leacock X  X hodorow and between 1 and larger scopes for Lin, Resnik, and
Jiang X  X onrath; there are no significant differences for Hirst X  X t-Onge, which hence ap-pears flat overall. The values of recall range from just under 6% (Hirst X  X t-Onge, scope = scope, as expected. All differences in recall are statistically significant, except between scope = 3 and scope = 5 for all measures other than Resnik X  X . F ranges from 5% (Hirst X 
St-Onge, scope = MAX ) to 14% (Jiang X  X onrath, scope = 5), with a mean of just under 10%. Even though values at the lower ends of these ranges appear small, they are that, especially for small search scopes, there will be words other than our deliberate malapropisms that are genuinely unrelated to all others in the scope.
 measure and scope to determine whether the performance of the five measures was significantly different and whether scope of search for relatedness made a significant difference.

Scope differences. For Jiang X  X onrath and Resnik, the analysis confirms only that scope 3 is significantly better than scope 1; for Leacock X  X hodorow, that 3 is significantly betterthan1and MAX better than 3; and for Hirst X  X t-Onge, that worse than 3. (From the standpoint of simple detection of unrelatedness (suspicion in malapropism detection), these data point to overall optimality of scopes 3 or 5.)
Differences between measures. Jiang X  X onrath significantly outperforms the others in all scopes (except for Leacock X  X hodorow and Lin at scope MAX but not significantly so), followed by Lin and Leacock X  X hodorow (whose performances are not significantly different from each other), in turn followed by Resnik. Hirst X  X t-
Onge, with its irregular behavior, performs close to Lin and Leacock X  X hodorow for scopes 1 and 3 but falls behind as the scope size increases, finishing worst for scope 36
Thus the Jiang X  X onrath measure does best for the suspicion phase (and is optimal with scope = 5). 5.3.2 Detection. We now turn to the results for malapropism detection. During the detection phase, the suspects are winnowed by checking the spelling variations of each for relatedness to their context. Since (true) alarms can only result from (true) suspects, recall can only decrease (or, more precisely, not increase) from that for suspicion (cf. equations (22) and (25)). However, if a given measure of semantic relatedness is good, we expect the proportion of false alarms to reduce more considerably  X  far fewer false suspects will become alarms than correct suspects  X  thus resulting in higher precision for detection than for suspicion (cf. Equations 21 and 24).
 scope combinations, determined by the same method as those for suspicion. The values of recall range from 5.9% (Hirst X  X t-Onge, scope = MAX
Chodorow, scope = 1). While these values are, as expected, lower than those for suspi-cion recall  X  R D for each measure X  X cope combination is from 1 to 16 percentage points out of the 20 combinations.
 25% (Jiang X  X onrath, scope = MAX ), increasing, as expected, from suspicion precision; each combination increases from 1 to 14 percentage points; the increase is statistically significant for 18 out of the 20 combinations. Moreover, the increase in precision out-weighs the decline in recall, and thus F , which ranges from 6% to 25%, increases by 7.6% on average; the increase is significant for 17 out of the 20 combinations. Again, even the 38 lower ends of the precision, recall, and F ranges are significantly ( p &lt;. 001) better than 18% precision, 50% recall for Jiang X  X onrath at scope = 1, which had the highest F though not the highest precision or recall), despite the fact that the method is inherently limited in the ways described earlier (Section 5.1). (See Hirst and Budanitsky (2005) for discussion of the practical usefulness of the method.)
Scope differences. Our analysis of scope differences in F shows a somewhat different picture for detection from that for suspicion: There are significant differences between scopes only for the Hirst X  X t-Onge measure. The F graphs of the other four methods thus are not significantly different from being flat  X  that is, scope doesn X  X  affect the results. (Hence we can choose 1 as the optimal scope, since it involves the least amount of work, and Jiang and Conrath X  X  method with scope = 1 as the optimal parameter combination for the malapropism detector.)
Differences between measures. The relative position of each measure X  X  precision, the precision and F graphs for Hirst X  X t-Onge, which slide further down. Statistical testing for F confirms this, with Jiang X  X onrath leading, followed by Lin and Leacock X 
Chodorow together, Resnik, and then Hirst X  X t-Onge. 5.4 Interpretation of the Results
In our interpretation, we focus largely on the results for suspicion; those for detection both add to the pool of relatedness judgments on which we draw and corroborate what we observe for suspicion.
 measure simply marks too many words as potential malapropisms X  X t  X  X nder-relates X , being far too conservative in its judgments of relatedness. For example, it was the only measure that flagged crowd as a suspect in a context in which all the other measures found it to be related to house : crowd IS -A gathering/assemblage SUBSUMES house/ household/family/menage . 20 Indeed, for every scope, Resnik X  X  measure generates more suspects than any other measure X  X .g., an average of 62.5 per article for scope = 1, compared to a range of 15 to 47, with an average of 37, for the other measures. The
Leacock X  X hodorow measure X  X  superior precision and comparable recall (the former significantly better F -value, indicate its better ability at discerning relatedness.
Even though both use the same information-content X  X ased components, albeit in differ-ent arithmetic combinations, and show similar recall, the Jiang X  X onrath measure shows superior precision and is best overall (see above). The Lin and Leacock X  X hodorow measures, in turn, have statistically indistinguishable values of F and hence similar ratios of errors to true positives.
 from those of the other four measures in Figure 4 evidently reflects the corresponding 40 difference in precision behavior: The Hirst X  X t-Onge suspicion precision graph is statis-tically flat, unlike the others. Ironically, given that this measure is the only one of the five that promises the semantic relatedness that we want rather than mere similarity, this poor performance appears to be a result of the measure X  X   X  X ver-relating X   X  it is far too promiscuous in its judgments of relatedness. For example, it was the only measure that considered cation (a malapropism for nation ) to be related to group : bound together as a single unit and forming part of a molecule X ). Because of its promiscuity, the Hirst X  X t-Onge measure X  X  mean number of suspects for scope = 1is scope = MAX ; the number of articles without a single suspect grows from 1 to 93. By comparison, for the other measures, the number of suspects drops only to around a third or a quarter from scope = 1 to scope = MAX , and the number of articles with no suspect stays at 1 for both Leacock X  X hodorow and Resnik and increases only from 1 to 4 for Lin and from 1 to 12 for Jiang X  X onrath. 6. Related Work 6.1 Other Applications of WordNet-based Measures Since the first publication of the initial results of this work (Budanitsky and Hirst 2001),
Pedersen and his colleagues (Pedersen, Patwardhan, and Michelizzi 2004) have made available a Perl implementation of the five WordNet-based measures (plus Wu and
Palmer X  X  and their own; see below) that has been used by a number of researchers in published work on other NLP applications. Generally, these results are consistent with our own. For example, Stevenson and Greenwood (2005) found Jiang X  X onrath to be the best measure (out of  X  X everal X , which they do not list) for their task of pattern induction for information extraction. Similarly, Kohomban and Lee (2005) found Jiang X  X onrath the best (out of  X  X arious schemes X , which they do not list) for their task of learning coarse-grained semantic classes. In word-sense disambiguation, Patwardhan, Banerjee, and Pedersen (2003) found Jiang X  X onrath to be clearly the best of the five measures evaluated here, albeit edged out by their own new  X  X esk X  measure based on gloss overlaps; 21 and McCarthy et al. (2004) found that the Jiang X  X onrath and Lesk measures gave the best accuracy in their task of finding predominant word senses, with the results of the two being  X  X omparable X  but Jiang X  X onrath being far more efficient. On the other hand, Corley and Mihalcea (2005) found little difference between the measures when using them in an algorithm for computing text similarity. 6.2 Measures of Distributional Similarity as Proxies for Measures we have dealt with in this paper is a notion distinct from that of lexical distributional or co-occurrence similarity. However, a number of researchers, such as Dagan (2000), have promoted the hypothesis that distributional similarity can act as a useful proxy for semantic relatedness in many applications because it is based on corpus-derived data rather than manually created lexical resources; indeed, it could perhaps be used to automatically create (first-draft) lexical resources (Grefenstette 1994). It is therefore natural to ask how distributional-similarity measures compare with the WordNet-based measures that we have looked at above.
 w and w 2 , we mean that they tend to occur in similar contexts, for some definition of context; or that the set of words that w 1 tends to co-occur with is similar to the set that w tends to co-occur with; or that if w 1 is substituted for w (Weeds 2003; Weeds and Weir 2005) is unchanged. The context considered may be a small or large window around the word, or an entire document; or it may be a syntactic relationship. For example, Weeds (2003; Weeds and Weir, 2005) (see below) took verbs as contexts for nouns in object position: so they regarded two nouns to be similar to the extent that they occur as direct objects of the same set of verbs. Lin (1998b, 1998a) considered other syntactic relationships as well, such as subject X  X erb and modifier X  noun, and looked at both roles in the relationship.
 ity have been proposed; see Dagan (2000), Weeds (2003), or Mohammad and Hirst (2005) for a review. For example, the set of words that co-occur with w with w 2 may be regarded as a feature vector of each and their similarity measured as the cosine between the vectors; or a measure may be based on the Kullback X  X eibler di-vergence between the probability distributions P ( w | w 1
Lee X  X  (1999)  X  -skew divergence. Lin (1998b) uses his similarity theorem (equation (19) above) to derive a measure based on the degree of overlap of the sets of words with which w 1 and w 2 , respectively, have positive mutual information. lated concepts, and vice versa, as the following examples demonstrate. Weeds (2003), in her study of 15 distributional-similarity measures, found that words distributionally similar to hope (noun) included confidence, dream, feeling, and desire ; Lin (1998b) found their role in the world will be similar, so similar things will be said about them, and so the contexts of occurrence of the corresponding words will be similar. And conversely (albeit with less certainty), if the contexts of occurrence of two words are similar, then similar things are being said about each, so they are playing similar roles in the world and hence are semantically similar  X  at least to the extent of these roles. Nonetheless, the limitations of this observation will become clear in our discussion below. immediately apparent. First, while semantic relatedness is inherently a relation on con-cepts, as we emphasized in Section 1.1, distributional similarity is a (corpus-dependent) relation on words. In theory, of course, if one had a large-enough sense-tagged corpus, one could derive distributional similarities of word-senses. But in practice, apart from the lack of such corpora, distributional similarities are promoted exactly for applications 42 such as various kinds of ambiguity resolution in which it is words rather than senses that are available (see Weeds (2003) for an extensive list).
 potentially asymmetrical relationship. If distributional similarity is conceived of as substitutability, as Weeds and Weir (2005) and Lee (1999) emphasize, then asymme-tries arise when one word appears in a subset of the contexts in which the other appears; for example, the adjectives that typically modify apple are a subset of those that modify fruit ,so fruit substitutes for apple better than apple substitutes for fruit . While some distributional similarity measures, such as cosine, are symmetric, many, such as  X  -skew divergence and the co-occurrence retrieval models developed by Weeds and which substitutability is far too strict a requirement: window and house are semantically related, but they are not plausibly substitutable in most contexts.
 knowledge resource, whereas distributional similarity is relative to a corpus. In each paper and Weeds (2003) show, and anomalies can arise. for semantic relatedness is created by humans and may be presumed to be (in a weak sense)  X  X rue, unbiased, and complete X . A corpus, on the other hand, is not. Imbalance in the corpus and data sparseness is an additional source of anomalous results even for  X  X ood X  measures. For example, Lin (1998b) found  X  X eculiar X  similarities that were  X  X easonable X  for his corpus of news articles, such as captive X  X esterner (because in the news articles, more than half of the  X  X esterners X  mentioned were being held captive) and audition X  X ite (because both were infrequent and were modified by uninhibited ). semantic relatedness in NLP applications such as malapropism detection. Weeds (2003) considered the hypothesis in detail. She carried out a number of experiments using data gathered from the British National Corpus on the distribution of a set of 2000 nouns with respect to the verbs of which they were direct objects, comparing a large number of proposed measures of distributional similarity. She applied ten of these measures to the Miller and Charles word-pairs (see Section 4.1 above); the absolute values of the correlations with the Miller and Charles human judgments was at best .62 (and at worst .26), compared with .74 to .85 for the semantic measures (Table 3 above). Weeds also compared these measures on their ability to predict the k words that are semantically closest to a target word in WordNet, as measured by Lin X  X  semantic similarity measure, sim L . She found performance to be  X  X enerally fairly poor X  (page 162), and undermined by the effects of varying word frequencies.
 rection, much as we have defined it in Hirst and Budanitsky (2005) and in Section 5.1 above, but replacing the semantic relatedness measures with distributional similarity measures. However, she varied the experimental procedure in a number of ways, with the consequence that her results are not directly comparable to ours: her test data was the British National Corpus; scope was measured in words, not paragraphs; and relat-edness thresholds were replaced by considering the k words most similar to the target word (and k was a parameter). The most significant difference, however, arose from the limitations due to data sparseness that are inherent in methods based on distributional similarity: the very small size of the set of words that could be corrected. Specifically, only malapropisms for which both the error and the correction occurred in the set of 2,000 words for which Weeds had distributional data could be considered; and the abil-ity to detect and correct the malapropism depended on other members of that set also being within the scope of the target word. It is therefore not surprising that the results were generally poor (and so were results for sim L run under the same conditions). This severe limitation on the data means that this was not really a fair test of the principles underlying the hypothesis; a fair test would require data allowing the comparison of any two nouns (or better still, any two words) in WordNet, but obtaining such data for less-frequent words (possibly using the Web as the corpus) would be a massive task. 7. Conclusion
Our goal in this paper has been to evaluate resource-based measures of lexical semantic distance, or, equivalently, semantic relatedness, for use in natural language processing applications. Most of the work, however, was limited to the narrower notion of mea-sures of similarity and how well they fill the broader role, because those measures are what current resources support best and hence what most current research has focused on. But ultimately it is the more-general idea of relatedness, not just similarity, that we need for most NLP methods and applications, because the goal, in one form or another, is to determine whether two smaller or larger pieces of text share a topic or some kind of closeness in meaning, and this need not depend on the presence of words that denote similar concepts. In word sense disambiguation, such an association with the context and Pedersen 2003); in our malapropism corrector, a word should be considered non-anomalous in the context of another if there is any kind of semantic relationship at all apparent between them. These relationships include not just hyponymy and the non-hyponymy relationships in WordNet such as meronymy but also associative and ad hoc relationships. As mentioned in the introduction, these can include just about any kind of functional relation or frequent association in the world. of relationships. Some elements from a typical list (that of Spellman, Holyoak, and
Morrison (2001)) are shown in Table 6. Morris and Hirst (2004, 2005) have termed these non-classical lexical semantic relationships (following Lakoff X  X  (1987) non-classical cat-egories), and Morris has shown in experiments with human subjects that around 60% 2006). There is presently no catalogue of instances of these kinds of relationships let alone any incorporation of such relationships into a quantification of semantic distance.
Nonetheless, there are clear intuitions to be captured here, and this should be a focus for future research.
 also arise ad hoc in context (Barsalou 1983, 1989)  X  in particular, as co-membership of an ad hoc category . For example, Morris X  X  subjects reported that the words sex, drinking, and drag racing were semantically related, by all being  X  X angerous behaviors X , in the context of an article about teenagers emulating what they see in movies. Thus 44 lexical semantic relatedness is sometimes constructed in context and cannot always be determined purely from an a priori lexical resource such as WordNet. how ad hoc semantic relationships could be quantified in any meaningful way, let alone compared with prior quantifications of the classical and non-classical relationships.
However, ad hoc relationships accounted for only a small fraction of those reported by Morris X  X  subjects (Morris 2006). The fact of their existence does not undermine the usefulness of computational methods for quantifying semantic distances for non X  ad hoc relationships, and the continued development of such methods is an important direction for research.
 Acknowledgments References 46
