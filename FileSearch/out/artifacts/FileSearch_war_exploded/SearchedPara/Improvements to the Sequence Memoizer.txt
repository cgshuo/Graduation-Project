 The sequence memoizer (SM) is a Bayesian nonparametric model for discrete sequence data producing state-of-the-art results for language modeling and compression [ 1 , 2 ]. It models each symbol of a sequence using a predictive distribution that is conditioned on all previous symbols, and thus can be understood as a non-Markov sequence model. Given the very large (infinite) number of predictive distributions needed to model arbitrary sequences, it is essential that statistical strength be shared in their estimation. To do so, the SM uses a hierarchical Pitman-Yor process prior over the predictive distributions [ 3 ]. One innovation of the SM over [ 3 ] is its use of coagulation and fragmentation properties [ 4 , 5 ] that allow for efficient representation of the model using a data structure whose size is linear in the sequence length. However, in order to make use of these properties, all concentration parameters, which were allowed to vary freely in [3], were fixed to zero.
 In this paper we explore a number of further innovations to the SM. Firstly, we propose a more flexible setting of the hyperparameters with potentially non-zero concentration parameters that still allow the use of the coagulation/fragmentation properties. In addition to better predictive performance, the setting also partially mitigates a problem observed in [ 1 ], whereby on encountering a long sequence of the same symbol, the model becomes overly confident that it will continue with the same symbol. The second innovation addresses memory usage issues in inference algorithms for the SM. In particular, current algorithms use a Chinese restaurant franchise representation for the HPYP, where the seating arrangement of customers in each restaurant is represented by a list, each entry being the number of customers sitting around one table [ 3 ]. This is already an improvement over the na  X   X ve Chinese restaurant franchise in [ 6 ] which stores pointers from customers to the tables they sit at, but can still lead to huge memory requirements when restaurants contain many tables. One approach to mitigate this problem has been explored in [ 7 ], which uses a representation that stores a histogram of table sizes instead of the table sizes themselves. Our proposal is to store even less, namely only the minimal statistics about each restaurant required to make predictions: the number of customers and the number of tables occupied by the customers. Inference algorithms will have to be adapted to this compact representation, and we describe and compare a number of these. In Section 2 we will give precise definitions of Pitman-Yor processes and Chinese restaurant processes. These will be used to define the SM model in Section 3, and to derive the results about the extended hyperparameter setting in Section 4 and the memory-efficient representation in Section 5. As a side benefit we will also be able to give an elementary proof of the coagulation and fragmentation properties in Section 4, which was presented as a fait accompli in [ 1 ], while the general and rigorous treatment in the original papers [4, 5] is somewhat inaccessible to a wider audience. A Pitman-Yor process (PYP) is a particular distribution over distributions over some probability space  X  [ 8 , 9 ]. We denote by PY(  X ,d,G 0 ) a PYP with concentration parameter  X  &gt;  X  d , discount parameter d  X  [0 , 1) , and base distribution G 0 over  X  . We can describe a Pitman-Yor process using its associated Chinese restaurant process (CRP). A Chinese restaurant has customers sitting around tables which serve dishes. If there are c customers we index them with [ c ] = { 1 ,...,c } . We define a seating arrangement of the customers as a set of disjoint non-empty subsets partitioning [ c ] . Each subset is a table and consists of the customers sitting around it, e.g. {{ 1 , 3 } , { 2 }} means customers 1 and 3 sit at one table and customer 2 sits at another by itself. Let A c be the set of seating arrangements of c customers, and A ct those with exactly t tables. The CRP describes a distribution over seating arrangements as follows: customer 1 sits at a table; for customer c + 1 , if A  X  A c is the current seating arrangement, then she joins a table a  X  A with probability | a | X  d  X  + c and starts a new table with conditional probabilities together, where [ y ] n d = Q n  X  1 i =0 y + id is Kramp X  X  symbol. Note that the denominator is the normalization constant. Fixing the number of tables to be t  X  c , the distribution, denoted as CRP ct ( d ) , becomes: where the normalization constant S d ( c,t ) = P A  X  X  number of type (  X  1 ,  X  d, 0) [ 10 ]. These can be computed recursively [ 3 ] (see also Section 5). Note that conditioning on a fixed t the seating arrangement will not depend on  X  , only on d . Suppose G  X  PY(  X ,d,G 0 ) and z 1 ,...,z c | G iid  X  G . The CRP describes the PYP in terms of its effect on z 1: c = z 1 ,...,z c . In particular, marginalizing out G , the distribution of z 1: c can be described as follows: draw A  X  CRP c (  X ,d ) , on each table serve a dish which is an iid draw from G 0 , finally let variable z i take on the value of the dish served at the table that customer i sat at. Now suppose we wish to perform inference given observation of z 1: c . This is equivalent to conditioning on the dishes that each customer is served. Since customers at the same table are served the same dish, the different values among the z i  X  X  split the restaurant into multiple sections, with customers and tables in each section being served a distinct dish. There can be more than one table in each section since multiple tables can serve the same dish (if G 0 has atoms). If s  X   X  is a dish, let c s be the number of z i  X  X  with value s (number of customers served dish s ), t s the number of tables, and A s  X  A c s t s the seating arrangement of customers around the tables serving dish s (we reindex the c s customers to be [ c s ] ). The joint distribution over seating arrangements and observations is then: 1 where t  X  = P s  X   X  t s and similarly for c  X  .We can marginalize out { A s } from (3) using (2): Inference then amounts to computing the posterior of either { t s ,A s } or only { t s } given z 1: c ( c s are fixed) and can be achieved by Gibbs sampling or other means. In this section we review the sequence memoizer (SM) and its representation using Chinese restaurants [ 3 , 11 , 1 , 2 ]. Let  X  be the discrete set of symbols making up the sequences to be modeled, and let  X   X  be the set of finite sequences of symbols from  X  . The SM models a sequence x 1: T = x ,x 2 ,...,x T  X   X   X  using a set of conditional distributions: where G u ( s ) is the conditional probability of the symbol s  X   X  occurring after a context u  X   X   X  (the sequence of symbols occurring before s ). The parameters of the model consist of all the conditional distributions { G u } u  X   X   X  , and are given a hierarchical Pitman-Yor process (HPYP) prior: where  X  is the empty sequence,  X  ( u ) is the sequence obtained by dropping the first symbol in u , and H is the overall base distribution over  X  (we take H to be uniform over a finite  X  ). Note that we have generalized the model to allow each G u to have its own concentration and discount parameters, whereas [1, 2] worked with  X  u = 0 and d u = d | u | (i.e. context length-dependent discounts). As in previous works, the hierarchy over { G u } is represented using a Chinese restaurant franchise [ 6 ]. Each G u has a corresponding restaurant indexed by u . Customers in the restaurant are draws from G u , tables are draws from its base distribution G  X  ( u ) , and dishes are the drawn values from  X  . For each s  X   X  and u  X   X   X  , let c u s and t u s be the numbers of customers and tables in restaurant u served dish s , and let A u s  X  X  c u s t u s be their seating arrangement. Each observation of x i in context x 1: i  X  1 corresponds to a customer in restaurant x 1: i  X  1 who is served dish x i , and each table in each restaurant u , being a draw from the base distribution G  X  ( u ) , corresponds to a customer in the parent restaurant  X  ( u ) . Thus, the numbers of customers and tables have to satisfy the constraints where c x u s = 1 if s = x i and u = x 1: i  X  1 for some i , and 0 otherwise.
 restaurants (and possibly the concentration and discount parameters). The joint distribution can be obtained by multiplying the probabilities of all seating arrangements (3) in all restaurants: The first parentheses contain the probability of draws from the overall base distribution H , and the second parentheses contain the probability of the seating arrangement in restaurant u . Given a state of the restaurants drawn from the posterior, the predictive probability of symbol s in context v can then be computed recursively (with P  X   X  (  X  ) ( s ) defined to be H ( s ) ): In [ 1 ] the authors proposed setting all the concentration parameters to zero. Though limiting the flexibility of the model, this allowed them to take advantage of coagulation and fragmentation properties of PYPs [ 4 , 5 ] to marginalize out all but a linear number (in T ) of restaurants from the hierarchy. We propose the following enlarged family of hyperparameter settings: let  X   X  =  X  &gt; 0 be free to vary at the root of the hierarchy, and set each  X  u =  X   X  ( u ) d u for each u  X   X   X  \{  X  } . The discounts can vary freely. In addition to more flexible modeling, this also partially mitigates the overconfidence problem [ 2 ]. To see why, notice from (9) that the predictive probability is a weighted average of predictive probabilities given contexts of various lengths. Since  X  v &gt; 0 , the model gives higher weights to the predictive probabilities of shorter contexts (compared to  X  v = 0 ). These typically give less extreme values since they include influences not just from the sequence of identical symbols, but also from other observations of other symbols in other contexts.
 Our hyperparameter settings also retain the coagulation and fragmentation properties which allow us to marginalize out many PYPs in the hierarchy for efficient inference. We will provide an elementary proof of these results in terms of CRPs in the following. First we describe the coagulation and fragmentation operations. Let c  X  1 and suppose A 2  X  A c and A 1  X  A | A arrangements where the number of customers in A 1 is the same as that of tables in A 2 . Each customer in A 1 can be put in one-to-one correspondence to a table in A 2 and sits at a table in A 1 . Now consider re-representing A 1 and A 2 . Let C  X  X  c be the seating arrangement obtained by coagulating (merging) tables of A 2 corresponding to customers in A 1 sitting at the same table. Further, split A 2 into sections, one for each table a  X  C , where each section F a  X  X  | a | contains the | a | customers and tables merged to make up a . The converse of coagulating tables of A 2 into C is of course to fragment each table a  X  C into the smaller tables in F a . Note that there is a one-to-one correspondence between tables in C and in A 1 , and the number of customers in each table of A 1 is that of tables in the corresponding F a . Thus A 1 and A 2 can be reconstructed from C and { F a } a  X  C . Theorem 1 ([ 4 , 5 ]) . Suppose A 2  X  A c , A 1  X  A | A related as above. Then the following describe equivalent distributions: (I) A 2  X  CRP c (  X d 2 ,d 2 ) and A 1 | A 2  X  CRP | A (II) C  X  CRP c (  X d 2 ,d 1 d 2 ) and F a | C  X  CRP | a | (  X  d 1 d 2 ,d 2 ) for each a  X  C . Proof. We simply show that the joint distributions are the same. Starting with (I) and using (1),
P ( A 1 ,A 2 ) = We used the identity [  X  X  +  X  ] n  X  1  X  =  X  n  X  1 [  X  + 1] n  X  1 1 for all  X , X ,n . Re-grouping the products and expressing the same quantities in terms of C and { F a } , We see that conditioning on C each F a  X  CRP | a | (  X  d 1 d 2 ,d 2 ) . Marginalizing { F a } out using (1), So C  X  CRP c (  X d 2 ,d 1 d 2 ) and (I)  X  (II) . Reversing the same argument shows that (II)  X  (I) . Statement (I) of the theorem is exactly the Chinese restaurant franchise of the hierarchical model G 1 | G 0  X  PY(  X ,d 1 ,G 0 ) , G 2 | G 1  X  PY(  X d 2 ,d 2 ,G 1 ) with c iid draws from G 2 . The theorem shows that the clustering structure of the c customers in the franchise is equivalent to the seating arrangement in a CRP with parameters  X d 2 ,d 1 d 2 , i.e. G 2 | G 0  X  PY(  X d 2 ,d 1 d 2 ,G 0 ) with G 1 marginalized out. Conversely, the fragmentation operation (II) regains Chinese restaurant representations for both G 2 | G 1 and G 1 | G 0 from one for G 2 | G 0 .
 This result can be applied to marginalize out all but a linear number of PYPs from (6) [ 1 ]. The resulting model is still a HPYP of the same form as (6) , except that it only need be defined over the prefixes of x 1: T as well as some subset of their ancestors. In the rest of this paper we will refer to (6) and its Chinese restaurant franchise representation (8) with the understanding that we are operating in this reduced hierarchy. Let U denote the reduced set of contexts, and redefine  X  ( u ) to be the parent of u in U . The concentration and discount parameters need to be modified accordingly. Current inference algorithms for the SM and hierarchical Pitman-Yor processes operate in the Chinese restaurant franchise representation, and use either Gibbs sampling [ 3 , 11 , 1 ] or particle filtering [ 2 ]. To lower memory requirements, instead of storing the precise seating arrangement of each restaurant, the algorithms only store the numbers of customers, numbers of tables and sizes of all tables in the franchise. This is sufficient for sampling and for prediction. However, for large data sets the amount of memory required to store the sizes of the tables can still be very large. We propose algorithms that only store the numbers of customers and tables but not the table sizes. This compact representation counts are already sufficient for prediction, as (9) does not depend on the table sizes. We will also consider a number of sampling algorithms in this representation.
 Our starting point is the joint distribution over the Chinese restaurant franchise (8) . Integrating out the seating arrangements { A u s } using (2) gives the joint distribution over { c u s ,t u s } : Note that each c u s is in fact determined by (7) so in fact the only unobserved variables in (10) are { t u s } . With this joint distribution we can now derive various sampling algorithms. 5.1 Sampling Algorithms Direct Gibbs Sampling of { c u s ,t u s } . It is straightforward derive a Gibbs sampler from (10) . Since which for t u s in the range { 1 ,...,c u s } has conditional distribution sampler is that we need to compute S d u ( c,t ) for all 1  X  c,t  X  c u s . If d u is fixed these can be precomputed and stored, but the resulting memory requirement is again large since each restaurant typically has its own d u value. If d u is updated in the sampling, then these will need to be computed each time as well, costing O ( c 2 u s ) per iteration. Further, S d ( c,t ) typically has very high dynamic range, so care has to be taken to avoid numerical under-/overflow (e.g. by performing the computations in the log domain, involving many expensive log and exp computations).
 Re-instantiating Seating Arrangements. Another strategy is to re-instantiate the seating arrange-Section 5.2 below), then performing the original Gibbs sampling of seating arrangements [ 3 , 11 ]. This produces a new number of tables t u s and the seating arrangement can be discarded. Note however that when t u s changes this sampler will introduce changes to ancestor restaurants (by adding or removing customers), so these will need to have their seating arrangements instantiated as well. To implement this sampler efficiently, we visit restaurants in depth-first order, keeping in memory only the seating arrangements of all restaurants on the path to the current one. The computational cost is O ( c u s t u s ) , but with a potentially smaller hidden constant (no log/exp computations are required). Original Gibbs Sampling of { c u s ,t u s } . A third strategy is to  X  X magine X  having a seating arrange-ment and running the original Gibbs sampler, incrementing t u s if a table would have been created, and decrementing t u s if a table would have been deleted. Recall that the original Gibbs sampler operates by iterating over customers, treating each as the last customer in the restaurant, removing it, then adding it back into the restaurant. When removing, if the customer were sitting by himself, a table would need to be deleted too, so the probability of decrementing t u s is the probability of a customer sitting by himself. From (2), this can be worked out to be The numerator is due to a sum over all seating arrangements where the other c u s  X  1 customers sit at the other t u s  X  1 tables. When adding back the customer, the probability of incrementing the number of tables is the probability that the customer sits at a new table of the same dish s : customer removed. This sampler also requires computation of S d u ( c,t ) , but only for 1  X  t  X  t u s which can be significantly smaller than c u s . Computation cost is O ( c u s t u s ) (but again with a larger constant due to computing the Stirling numbers in a stable way). We did not find a sampling method taking less time than O ( c u s t u s ) .
 Particle Filtering. (13) gives the probability of incrementing t u s (and adding a customer to the parent restaurant) when a customer is added into a restaurant. This can be used as the basis for a particle filter, which iterates through the sequence x 1: T , adding a customer corresponding to s = x i in context u = x 1: i  X  1 at each step. Since no customer deletion is required, the cost is very small: just O ( c u s ) for the c u s customers per s and u (plus the cost of traversing the hierarchy to the current restaurant, which is always necessary). Particle filtering works very well in online settings, e.g. compression [2], and as initialization for Gibbs sampling. 5.2 Re-instantiating A u s given c u s ,t u s To simplify notation, here we will let d = d u ,c = c u s ,t = t u s and A = A u s  X  X  ct . We will use the forward-backward algorithm in an undirected chain to sample A from CRP ct ( d ) given in (2) . First we re-express A using two sets of variables z 1 ,...,z c and y 1 ,...,y c . Label a table a  X  A using the index of the first customer at the table, i.e. the smallest element of a . Let z i be the number of tables occupied by the first i customers, and y i the label of the table that customer i sits at. The variables satisfy the following constraints: z 1 = 1 , z c = t , and z i = z i  X  1 in which case y i  X  [ i  X  1] or z i = z i  X  1 + 1 in which case y i = i . This gives a one-to-one correspondence between seating arrangements in A ct and settings of the variables satisfying the above constraints. Consider the following distribution over the variables satisfying the constraints: z 1 ,...,z c is distributed according to a Markov network with z 1 = 1 , z c = t , and edge potentials: It is easy to see that the normalization constant is simply S d ( c,t ) and Given z 1: c , we give each y i the following distribution conditioned on y 1: i  X  1 : Figure 2: (a), (b) Number of context/symbol pairs and total number of tables (counted after particle filter Thus we can sample A by first sampling z 1: c from (15) , then each y i conditioned on previous ones using (16) , and converting this representation into A . We use a backward-filtering-forward-sampling algorithm to sample z 1: c , as this avoids numerical underflow problems that can arise when using forward-filtering. Backward-filtering avoids these problems by incorporating the constraint that z c has to equal t into the messages from the beginning.
 Fragmenting a Restaurant. In particle filtering and in prediction, we often need to re-instantiate a restaurant which was previously marginalized out. We can do so by sampling A u s given c u s ,t u s for each s , then fragmenting each A u s using Theorem 1, counting the resulting numbers of customers and tables, then forgetting the seating arrangements. In order to evaluate the proposed improvements in terms of reduced memory requirements and to compare the performance of the different sampling schemes we performed three sets of experiments. 3 In the first experiment we evaluated the potential space saving due to the compact representation. Figure 2 shows the number of context/symbol pairs and the total number of tables as a function of data set size. While the difference does not seem dramatic, there is still a significant amount of memory that can be saved by using the compact representation, as there is no additional overhead and memory fragmentation due to variable-size arrays. The comparison between the byte-level model and the word-level model in Figure 2 also demonstrates that the compact representation saves more space when |  X  | is small (which leads to context/symbol pairs having larger c u s  X  X  and t u s  X  X ). Finally, Figure 2 illustrates another interesting effect: the number of tables is generally larger after a few iterations of Gibbs sampling have been performed after the initialization using a single-particle particle filter [2]. The second experiment compares the computational cost of the compact original sampler and the sampler that re-instantiates full seating arrangements. The main computational cost of the original sampler is computing the ratio (12) , while sampling the seating arrangements is the main computational cost of the re-instantiating sampler. Figure 2(c) shows the time needed for one iteration of Gibbs sampling as a function of data set size. The re-instantiating sampler is found to be much more efficient, as it avoids the overhead involved in computing the Stirling numbers in a stable manner (e.g. log/exp computations). For the original sampler, time can be traded off with space by tabulating all required Stirling numbers along the path down the tree (as was done in these experiments). However, this leads to an additional memory overhead that mostly undoes any savings from the compact representation.
 The third set of experiments uses the re-instantiating sampler and compares different modes of prediction and the effect of the non-zero concentration parameter. The results are shown in Table 1 . Predictions with the SM can be made in several different ways. After obtaining one or more samples from the posterior distribution over customers and tables (either using particle filtering or Gibbs sampling on the training set) one has a choice of either using particle filtering on the test set as well (online setting), or making predictions while keeping the model fixed. One also has a choice when making predictions involving contexts that were marginalized out from the model: one can either re-instantiate these contexts by fragmentation or simply predict from the parent (or even the child) of the required node. While one ultimately wants to average predictions over the posterior distribution, one may consider using just a single sample for computational reasons. In this paper we proposed an enlarged set of hyperparameters for the sequence memoizer that re-tains the coagulation/fragmentation properties important for efficient inference, and we proposed a new minimal representation of the Chinese restaurant processes to reduce the memory requirement of the sequence memoizer. We developed novel inference algorithms for the new representation, and presented experimental results exploring their behaviors. We found that the algorithm which re-instantiates seating arrangements is significantly more efficient than the other two Gibbs sam-plers, while particle filtering is most efficient but produces slightly worse predictions. Along the way, we formalized the metaphorical language often used to describe Chinese restaurant processes in the machine learning literature, and were able to provide an elementary proof of the coagula-tion/fragmentation properties. We believe this more precise language will be of use to researchers interested in hierarchical Dirichlet processes and its various generalizations.
 We are currently exploring methods to compute or approximate the generalized Stirling numbers, and efficient methods to optimize the hyperparameters in the sequence memoizer. A parting remark is that the posterior distribution over { c u s ,t u s } in (10) is in the form of a standard Markov network with sum constraints (7) . Thus other inference algorithms like loopy belief propagation or variational inference can potentially be applied. There are however two difficulties to be resolved before these are possible: the large domains of the variables, and the large dynamic ranges of the factors. Acknowledgments We would like to thank the Gatsby Charitable Foundation for generous funding. [1] F. Wood, C. Archambeau, J. Gasthaus, L. F. James, and Y. W. Teh. A stochastic memoizer for [2] J. Gasthaus, F. Wood, and Y. W. Teh. Lossless compression based on the Sequence Memoizer. [3] Y. W. Teh. A Bayesian interpretation of interpolated Kneser-Ney. Technical Report TRA2/06, [4] J. Pitman. Coalescents with multiple collisions. Annals of Probability , 27:1870 X 1902, 1999. [5] M. W. Ho, L. F. James, and J. W. Lau. Coagulation fragmentation laws induced by general co-[6] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal [7] P. Blunsom, T. Cohn, S. Goldwater, and M. Johnson. A note on the implementation of [8] J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived from a stable [9] H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. Journal of the [10] L. C. Hsu and P. J.-S. Shiue. A unified approach to generalized Stirling numbers. Advances in [11] Y. W. Teh. A hierarchical Bayesian language model based on Pitman-Yor processes. In [12] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model.
