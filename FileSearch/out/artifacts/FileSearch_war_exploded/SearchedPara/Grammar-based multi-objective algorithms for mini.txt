 1. Introduction Given the growing interest in information storage, both the number of available datasets and their sizes are increasing.
Nowadays, the extraction of knowledge or high level information hidden in data has become essential for predicting future behavior. A popular technique for discovering knowledge in datasets is association rule mining (ARM) [1 learning method that includes approaches having a descriptive nature [5,6]. Let I X  i item-sets, i.e., A  X  i 1 ; ... ; i j  X  I and C  X  i 1 ; ... ;
C  X 
C will also be satisfied. ARM was originally designed for market basket analysis to obtain relations between products like diapers  X  beer that describes the high probability of someone buying diapers also buying beer. It would allow shop-keepers to exploit this relationship by moving the products closer together on the shelves.

Originally, the ARM problem was studied under an exhaustive search strategy. The first algorithm in this field was a priori ,an approach suggested by Agrawal et al. [8,9] that served as the starting point for many algorithms in the ARM field [10
Nevertheless, these sorts of algorithms require a very high computational cost and large amount of memory. Also, more and more companies currently gather useful information and, sometimes, this information is purely numeric, so exhaustive search algorithms require a previous discretization of the numerical attributes. To solve these issues, the study of association rules by means of evolutionary algorithms (EA), and especially genetic algorithms (GA), is obtaining promising results [13]. Recently, an initial grammar-guided genetic programming (G3P) [14] proposal was presented in the ARM field. G3P is considered as an extension of genetic programming (GP) [15] that makes use of a grammar to enforce syntactic constraints on GP trees. This new
G3P algorithm, called Grammar-Guided Genetic Programming Association Rule Mining (G3PARM) [16], has turned into an area of interest for further exploration.

An important issue in ARM is that, regardless of the methodology used for the extraction of these rules, it is necessary to evaluate them properly. The process of extracting association rules from a dataset often requires the application of more than one between them. These problems, called multi-objective optimization problems, need to simultaneously reach more than one objective but do not have a single solution that optimizes them all. Multi-objective algorithms has been used in ARM [37,38] to evaluate rules based on different measures [17,18] but only using nominal attributes. Support and confidence are two of the most commonly used measures. The former states the frequency of occurrence of the rule, while the latter stands for the reliability of the rule. However, as discussed in subsequent sections, these two measures have some limitations, so the lift measure is adequate.
Lift calculates how many times the antecedent and the consequent occur together more frequently than expected if they were statistically independent. At this point, we consider dealing with the ARM problem under a multi-objective methodology and for any application domain, not requiring a previous discretization step. Thus, application of multi-objective optimization together with the GP3 methodology could give rise to a promising model especially well suited to optimizing rules in diverse application domains, and using different quality measures.

In this paper, we present two new G3P proposals for mining association rules following a multi-objective strategy. These proposals benefit from the advantages of both G3P [14] and consequently EA [19], and combine them with those of multi-objective models [20]. More specifically, the proposals presented here are based on two well-known multi-objective algorithms: the Non-dominated Sort Genetic Algorithm (NSGA-2) [21] and the Strength Pareto Evolutionary Algorithm (SPEA-2) [22]. Because of the specific grammar definition, these G3P proposals enable the extraction of rules from both numerical and categorical domains. Finally, in order to demonstrate the usefulness of the proposed algorithms, different measures are considered as objectives to obtain a set of optimal solutions. More specifically, the experiments performed combine both the support-confidence and support-lift measures. The results obtained have shown to be very frequent (with support values above 95% in most cases) and reliable (with confidence values close to 100%). Furthermore, for the trade-off between support and lift, the multi-objective proposals also produce very interesting and representative rules.

This paper is structured as follows: Section 2 presents some related work; Section 3 describes the multi-objective G3P proposals; Section 4 describes the datasets used in the experimental stage, the experimental set-up and the results obtained; finally, some concluding remarks are outlined in Section 5 . 2. Related work
This section presents the most widely used measures in the ARM field. Next, an introduction to the most relevant multi-objective approaches is outlined, paying special attention to their applicability in the ARM field. We consider that expert readers in Evolutionary Computation could omit this section since it provides basic background in both fields. 2.1. Quality measures
Despite the large number of measures used to evaluate the quality of association rules, most researchers [9,10,23] concur with the application of support and confidence measures because of their simplicity when determining the frequency and reliability of an association rule ( A  X  C ). Given a set of all transactions T X  t number of transactionssatisfied by theitem-set, which is considered frequentiff support A  X  X 
C .
For better understanding, suppose a sample dataset has ten market baskets, i.e., ten transactions, as shown in Table 1 . Five of them comprise diapers, six include beer, and four comprise milk. According to the support measure, three out of ten market baskets include beer and milk, i.e., 30% of them.

On the other hand, the confidence of an association rule is defined in Eq. (2) as the proportion of the number of transactions that include A and C among the transactions that include A .
Back to the sample market basket dataset, the confidence measure serves to calculate how much a given product depends on another. Observe that two out of five basket markets that include diapers also include beer, i.e., 40%.

Even though both support and confidence are the most widely used measures in ARM, sometimes it may be necessary to and high confidence values may be uninteresting if the confidence of the rule is equal to the marginal frequency of the rule consequent, which means that the antecedent and consequent of the rule are independent. Under these circumstances, the rule would not provide any new information. Besides, if an association rule has a confidence value less than the consequent support, the rule is not of interest. The occurrence of the antecedent does not imply an increment in the occurrence of the consequent.
In the example above, the expected confidence represents the proportion of occurrences in an item-set. Thus, the expected 10 customers bought beer regardless of other products. However, notice that the rule diapers confidence of 40%, is not of interest since the fact of buying beer (with an expected confidence of 60%) is more reliable than the fact of buying beer having already purchased diapers. In other words, among all the customers buying diapers, the proportion of customers buying beer is even lower than in the total group of customers and, in consequence, this rule does not provide any novel information.

The lift measure, defined in Eq. (3) as the relation between the confidence of the rule and the expected confidence or support rule is greater than the support of the consequent, then the rule is of interest, otherwise, it holds no significance.
Again for the rule diapers  X  beer , where confidence is 40% and the consequent appears in 60% of the instances, the lift value is equal to 0.66  X  1. So, it could be concluded that this rule does not provide any novel information.

All these measures serve in different ways to calculate the quality of a single rule. However, the use of multi-objective necessary to define the quality measures for the set R of rules properly. The support measure is obtained by the sum of the support of all the rules divided by the total number of rules (see Eq. (4)).
Similarly to the support measure, the confidence and lift measures for R are obtained by means of the sum of the measured values of all the rules divided by the total number of rules (see Eqs. (5) and (6) for the confidence and lift measures, respectively). likely that more than one solution could cover the same transaction. Hence, having a dataset comprising n transactions, the coverage measure (see Eq. (7)) for a set R of solutions is defined as the percentage of transactions comprising at least one rule that satisfies this transaction. 2.2. Evolutionary algorithms for multi-objective optimization acceptable. Mathematically speaking, given a set of objective functions F X  f no other solution s  X  that dominates it. A solution s  X  dominates s if and only if f
However, there are problems where it is necessary to minimize the objectives instead of maximizing them. In such problems, a solution s belongs to the POF if there is no other solution s
As depicted in Fig. 1 , having five solutions ( s 1 , s 2 comprises s 1 and s 2 . Note that s 1 is not dominated by any other solution because f dominated by any other one because f 2 ( s 2 ) is the best value for objective f f ( s 1 ) b f 1 ( s 3 ) and f 2 ( s 1 ) b f 2 ( s 3 ). Focusing on s dominated by both s 1 and s 2 . Therefore, POF comprises s is better than the other one for all the objectives.

Even though the application of multi-objective optimization techniques commenced in the early 50s, the use of EAs to address multi-objective optimization problems was first implemented in the mid-80s [26]. Nowadays, the number of applications using multi-objective EAs in real-world problems has increased significantly [27,28] , mainly motivated by their ability to deal optimization problem play the role of individuals in a population, whereas the fitness function measures how close a given solution is to the objective. Also, evolutionary operators operate on the individuals in the EA in an attempt to generate new individuals with a better fitness function. The original individuals are named parents, whereas the new individuals obtained by the evolutionary operators are known as offspring.

Multi-objective EAs have been applied to many areas, engineering being one of the most popular fields. This is mainly because engineering applications normally have mathematical models that can be directly associated with a multi-objective search [29].In this area, Tang et al. [30] presented a GA using Pareto ranking to design a Wireless Local Area Network (WLAN). Their aim was to minimize four objectives: the number of terminals with their path loss higher than a certain threshold, the number of base-stations required, the mean of the path loss predictions of the terminals in the design space, and the mean of the maximum field, the algorithm minimizes two objectives: the project duration and the cost of the project. The algorithm was tested and compared to the results produced by exhaustive enumeration, showing excellent results.

Multi-objective optimization has been also applied in medicine. Krmicek and Sebag [32] use an algorithm to maximize the length, area and alignment of a 4D captured brain image. In the experimental stage, some active areas in the brain were identified and attempts made to relate them to specific cognitive processes.

Nowadays, most evolutionary proposals for multi-objective optimization follow an elitist strategy, i.e., the individual with the best fitness value is maintained throughout the evolutionary process. A relevant algorithm in this field is PAES ( Pareto Archived
Evolution Strategy ) [33]. In each generation this algorithm obtains an offspring from a parent so they are both compared to then a new offspring is obtained by means of the evolutionary operator. Once the offspring dominates the parent, it is kept to fixed, the algorithm discards solutions from regions comprising a higher number of solutions when it reaches its upper limit.
NSGA-2 [21], which is an improved version of the previous NSGA [28], is another important approach for multi-objective optimization. The goal of this algorithm is to return a predefined number of optimal solutions, so the solutions should be are not dominated by any other solution. Once this first front is determined, the process is repeated with the remaining solutions and new fronts are obtained. For the sake of determining which solution is better in each front, the algorithm determines the density of solutions surrounding a particular solution. Thereafter, for each objective function, the boundary solutions (solutions value is calculated as the sum of the distance values for each objective. Finally, the algorithm returns a predefined number of optimal solutions, returning the best ones, i.e., those having a higher distance, from the first front until the desired number of second front is considered, and so on.

Another well-known multi-objective algorithm is called SPEA-2 [22], which is an improved version of SPEA [34]. In this algorithm, solutions are organized in fronts and solutions from the same front are ranked according to a fitness value obtained from two measures. The first measure is the number of solutions dominated by each solution. The second measure is the Cartesian distance from their k -th nearest neighbors in the population. If the number of desired solutions is lower than the number of in the POF is lower than the number of desired solutions, then the second front is analyzed and ranked. The process is repeated until a predefined number of solutions is reached. 2.3. Multi-objective association rule tasks
Although there are many proposals that consider the ARM task as a single-objective problem [9,11,35] , Ghosh et al. [36] proposed that it might be considered as a multi-objective optimization problem, instead. They used comprehensibility, interestingness, and predictive accuracy as measures to simultaneously optimize them and proposed a GA to mine rules from market basket type databases. In order to deal with numerical attributes in this approach, some value ranges were defined.
Association rules using a multi-objective strategy may be used in different problems. Ishibuchi [37] used association rules categorized into two approaches. The former evaluates each rule according to the support and confidence measures, while the latter evaluates each rule set according to its accuracy and complexity.

Another important multi-objective approach is called MODENAR [38], which extracts numeric association rules. In this approach, a search strategy for mining accurate and comprehensible rules is carried out by applying a multi-objective differential evolution method [39]. During the mining process, four objectives are considered. For each association rule, the support, confidence, and comprehensibility measures need to be maximized, whereas the amplitude of the intervals within each rule needs to be minimized.

P. Kumar et al. [40] proposed an approach for mining association rules by using the well-known multi-objective evolutionary algorithm NSGA-2. During the evaluation stage, different measures were used, such as interestingness, comprehensibility, support, confidence, etc. Finally, a series of experiments were carried out by taking three different measures each time and making a comparison with the traditional a priori algorithm.

The objective of this work was to exploit the benefits of the multi-objective optimization in ARM. Due to the promising results obtained in the original G3P proposal, and since trade-off between the measures used for association rules may be obtained, this paper presents two multi-objective G3P proposals with excellent results using the most widely used measures in this field. 3. G3P for mining association rules
This section proposes two multi-objective G3P proposals for extracting association rules from different domains and types of datasets. Both proposals are founded on two well-known multi-objective algorithms: NSGA-2 [21] and SPEA-2 [22]. The use of
G3P allows us to define expressive and understandable individuals in both numerical and categorical domains. Both proposals have several characteristics in common, such as the encoding criterion or the genetic operators used throughout the evolutionary process. In this section, the main characteristics of both proposals are outlined. 3.1. Encoding
G3P enables different types of data to be handled without producing invalid individuals by using a grammar to enforce syntactic constraints on the GP [19]. A context-free grammar (CFG) is defined as a fourtuple ( the alphabet of terminal symbols and  X  N the alphabet of non-terminal symbols. Notice that they have no common elements, commencing with the start symbol S . A production rule is defined as noted that in any CFG there may appear the production rule derivation syntax tree is obtained for each individual by applying the proper production rules, where internal nodes contain only non-terminal symbols, and leaves contain only terminals.

Fig. 3 shows the grammar used to represent each individual. Notice that the terminal symbol terminal symbol  X  name  X  may adopt any value, such as color, size, shape, area or perimeter. Once the attribute for this terminal symbol is assigned, a random value is then selected. For instance, the attribute color may be assigned to different values such as red, green, blue and black.

One of the most important features of these proposals is that they permit us to represent individuals in both numerical and categorical domains. Notice that a categorical attribute obtains a value u from a discrete and unordered domain extitD, so support to be reached in domains where u does not appear so frequently. For example, using the categorical attribute color, which is defined in the domain D ={red,green,blue,black}, as shown in Table 2 , and since the support of the condition color=green is 0.21, then the support of color !=green would be equal to 0.79. On the other hand, in order to deal with numerical attributes, values of the numerical attribute. In order to avoid rules that always occur (i.e., those that do not provide the user with new information), the highest and lowest bounds for each attribute range will not be taken into account. For example, using the perimeter are: 2.5, 5 and 7.5. Therefore, conditions such as perimeter
Finally, note that the process of producing an individual begins from the start symbol Rule and continues by randomly applying production rules belonging to the set P until a valid derivation sequence is successfully completed. The maximum number of derivations to perform, defined by the configuration parameters of the algorithm as the derivation size, determines the maximum number of production rules to be generated. Therefore, the length of the rules obtained may be configured for the problem to be addressed or to address specific data miner needs. 3.2. Genetic operators
In order to generate new individuals in a given generation of the evolutionary algorithm process, two genetic operators are presented: crossover and mutation. These genetic operators search for individuals with a support value greater than the original ones. To this end, these genetic operators work on the lowest support condition within each rule and obtain another one with a higher support. 3.2.1. Crossover
To facilitate the discovery of new individuals with a higher support, this genetic operator swaps the condition with the lowest frequency of occurrence within a parent with the one that has the greatest frequency of occurrence in another parent. As shown removed from the set parents (line 4). The next step is to generate a random value. If this value is less than the crossover probability (line 5), then those conditions with a maximum and minimum support are selected (lines 6 to 7), i.e., those to be swapped (lines 9 and 14). If the value is not lower than the crossover probability, the crossover operation is not carried out between the two selected parents. Note that this genetic operator checks each individual to guarantee that none comprises repeated conditions (lines 8 and 13). Finally, once all the parents are crossed, this genetic operator returns the set offspring generated by the process (line 20).

Taking the sample metadata from Table 2 , it should be noted that two sample rules are selected for crossover (see Fig. 4 ). In order to select a condition to be swapped from one parent, the condition with the highest support is chosen (i.e., in this example). Similarly, the condition with the lowest support from the other parent is selected (i.e., Finally, two new individuals are obtained by swapping these two conditions.
 3.2.2. Mutation
Like the crossover genetic operator, this operator tries to generate a new individual with a higher support than the original one. This operator obtains a new individual from only one parent by working on the lowest frequency of occurrence condition. As shown in the pseudocode in Listing 2, the set parents is required. In the process of generating new individuals, this genetic mutation probability (line 3), then the condition with the minimum support is selected (line 4) and a new individual is obtained by changing this condition (line 5) with a new randomly obtained one. Finally, once all the parents have been mutated, this genetic operator returns the set offspring obtained during the process (line 8).

According to Fig. 5 and following the example in Table 2 , a sample rule is selected to be mutated. By using the lowest support support than the original one. 3.3. The NSGA-G3PARM multi-objective algorithm
The proposal called NSGA-G3PARM is founded on the NSGA-2 [21] multi-objective algorithm, which is adapted to the characteristics of G3P. The pseudocode of the NSGA-G3PARM algorithm is shown in Algorithm 3. In this proposal, different repetition will be removed from the population resulting from joining the current population with the recently created set, mutatedPopulation (line 11). New individuals are evaluated to determine the values of the quality measures (line12). Since the the algorithm continues to identify those solutions from the entire set that belong to the POF, i.e., those solutions that are not dominated by any other. After obtaining a first front, the process is repeated on the remaining solutions, so new fronts are calculated. Ascertaining the density of the solutions surrounding a particular solution serves to determine which solution is best in each front. To this end, the average distance to each solution around each of its objectives is calculated (line 14). Those solutions having the highest and lowest values of each objective are assigned an infinite distance value. On the other hand, intermediate solutions are assigned a distance value equal to the absolute normalized difference in the objective function values higher distance, starting from the first front and continuing with the rest of fronts, if necessary (lines 18 to 22). Once the algorithm reaches a certain number of generations max _ generations , the resulting set paretoOptimalFront is returned (line 25). 3.4. The SPEA-G3PARM multi-objective algorithm In this case, the SPEA-G3PARM algorithm has been adapted to conform to the SPEA-2 [22] algorithm and the characteristics of
G3P. The pseudocode of this algorithm is shown in Algorithm 4. The algorithm starts by obtaining the set population of individuals creating new individuals by means of genetic operators. The main characteristic of this algorithm is that each individual is evaluated (see Eq. (8)) according to the Cartesian distance with its k -th nearest neighbors in the population and the number of individuals that dominate each individual ( raw value). If the individuals establish few dominant relationships among each other nearest neighbor density estimation. Thus, given an individual i , the higher the number of individuals dominated by i and the of this algorithm is to minimize the fitness value.
In each generation, the algorithm generates the POF, which is stored in the set paretoOptimalFront , from the set that results be downsized by choosing the best individuals ranked according to the fitness function (lines 10 to 11). On the other hand, if the (lines 13 to 14). Once the POF is generated, a set of parents is chosen by merging the current population and the new POF (line 17), and new solutions are obtained with the genetic operators (lines 18 to 20). Finally, after completing a given number of generations, the resulting set paretoOptimalFront is returned (line 24). 4. Experimental section
Different experiments were carried out, the results of which are presented in this section. Firstly, the experimental set-up and the datasets used are explained. Thereafter, a series of analyses are performed to determine the quality of each POF mined and the behavior of the different quality measures in the proposed G3P multi-objective optimization algorithms. 4.1. Experimental set-up
Since evolutionary proposals have a number of parameters that should be fixed prior to their execution, a series of previous experiments were performed on the G3PARM algorithm in order to obtain the best found combination of the parameters, i.e., those determined when checking a reasonable number of parameter combinations and that enable to get the best results. To do this, different parameter values were combined and tested (e.g., population size, number of generations, crossover probability, etc.) using different datasets. It is worth mentioning that no single combination of parameter values performed better for all datasets, as was to be expected. Also, since both multi-objective algorithms are based on G3PARM, and in order to make a fair comparison, the same parameters set-up is used for the three algorithms.

Table 3 shows the best found combination of parameters. The best results were obtained using a population size of 50 individuals, 100 generations, 90% crossover probability, 16% mutation probability, and a maximum derivation size of 24. For the
G3PARM algorithm, the external population size is 20 and the thresholds of support and confidence are set to 70% and 90%, respectively. For the SPEA-G3PARM, the Cartesian distance is calculated with the fifth nearest neighbor and the maximum Pareto front size is 20 to perform a fair comparison against G3PARM.

The results 1 shown in this experimental section correspond with the average values calculated after running each algorithm 30 times with different seeds. Ten datasets with different numbers of instances and attributes were used (see Table 4 ). All the experiments used in this study were performed on a 12Gb main memory Intel Core i7 machine, running CentOS 5.4. The algorithms were written in Java using JCLEC 2 [41], a Java library for evolutionary computation. 4.2. Experimental study tests used in this experimental study is carried out. Secondly, an analysis of the POF obtained by each proposal is presented, and finally, the quality of the extracted rules is evaluated. 4.2.1. Statistical tests
A series of statistical tests [42,43] were performed to demonstrate the behavior of the algorithm proposed here. They allow for precise analysis of whether there are any significant differences between them.
 calculating the average rank according to the F-distribution ( F null-hypothesis indicating that there are significant differences, then a posteriori test such as the Bonferroni compares a control algorithm against the rest, can be implemented to reveal those differences. The quality of two algorithms is significantly different if the corresponding average of their rankings is at least as great as its critical difference (CD). Finally, it is possible to compare means of two samples to make inferences about differences between two populations. The
Wilcoxon signed rank test [42] is a non-parametric statistical test used when comparing two paired samples to assess whether their population mean ranks differ. 4.2.2. Analysis of the POF quality
Many performance measures, which evaluate different POF characteristics, have been proposed in the literature [29] . Three of the most widely used  X  spacing, hyper-volume and coverage of sets above and using a support-confidence framework are shown in Table 5 . The spacing measure numerically describes the spread of the spaced than SPEA-G3PARM, its value being the lowest one. Using the hyper-volume, which is defined as the area of the POF coverage with respect to the objective space, the NSGA-G3PARM algorithm obtains the highest value, therefore, its POF covers a higher area than the POF of SPEA-G3PARM. Finally, the coverage of the two sets is evaluated. This measure determines the relative coverage comparison of the POF from two different algorithms. The results show that NSGA-G3PARM produces the highest value, dominating the outcomes of SPEA-G3PARM. Taking all these measures into account, it could be said that NSGA-G3PARM obtains a higher quality POF when support and confidence measures are used.

Since this analysis is based on the average values of several measures across datasets that have different characteristics, it is 0.08 for the hyper-volume, and 0.02 for the two set coverage measure. So, at a significance level of significant differences between the two multi-objective approaches.

Studying the POF quality with a support-lift framework (see Table 6 ), it is possible to determine that NSGA-G3PARM provides more equally spaced solutions than SPEA-G3PARM. Focusing on the hyper-volume measure, SPEA-G3PARM produces the greatest value, so its POF covers a greater area. Finally, focusing on the dominance of each POF, NSGA-G3PARM is seen to achieve a greater value than SPEA-G3PARM, so NSGA-G3PARM dominates the outcomes of SPEA-G3PARM. Taking all these results into account, it can be stated that NSGA-G3PARM obtains a higher quality POF than SPEA-G3PARM.

Since the results obtained from this analysis are not sufficiently meaningful, the Wilcoxon signed rank test should be performed. The test shows a 0.002 p -value for spacing, 0.037 for hyper-volume, and 0.006 for the two set coverage measure. Ata significance level of  X  =0.01, there are significant differences between the two multi-objective versions for the spacing measure and the NSGA-G3PARM version is statistically better. On the contrary, at the same significance level, the SPEA-G3PARM version is statistically better for the two set coverage measure. Finally, at a significance level of between the two multi-objective versions for the hyper-volume measure. Therefore, regardless of whether a support-confidence framework or a support-lift framework is used, no significant differences are seen to exist between the two multi-objective proposals. In consequence, both approaches should be compared with the original G3PARM. The results are shown in Tables 7 and 9 (the best results for each measure are highlighted in bold). 4.2.3. Analysis of the rules mined
In this study, we evaluate the performance of the G3P proposals by comparing them in terms of their average support, average measure, showing the critical difference (CD) for different p values. In such a way, it is easy to determine whether significant differences exist between the algorithms.

Focusing on the results presented in Table 7 , note that the three algorithms mine highly representative rules, with a support value above 0.95 in most cases. NSGA-G3PARM achieves the highest support values for most datasets. All algorithms obtain very reliable association rules (see Table 7 ) and there are no apparent differences between them. Analyzing the coverage measure (see Table 7 ), the SPEA-G3PARM algorithm produces the best results, covering all the instances in most datasets. An analysis of the
G3PARM and NSGA-G3PARM algorithms shows that they cover a large amount of instances (above 0.97) and, in some datasets, they manage to cover all the instances, e.g., in the MAROB and Mush datasets. Finally, the number of rules mined is homogeneous the size of its POF (20 rules). On the other hand, the G3PARM algorithm also obtains 20 rules at most, constrained by its external population size. However, using some datasets, the latter does not reach its population size limit (e.g., HH and WDatBC datasets).
On the other hand, the NSGA-G3PARM algorithm discovers a heterogeneous set of between 1 and 60 rules, depending on the dataset. As mentioned above, this algorithm does not have a maximum POF size, so the number of rules mined may vary greatly. The Friedman average ranking statistics for average support measure distributed according to F degrees of freedom is 24.3; 8.6 for the average confidence measure; and 3.4 for the coverage measure. The support does not the support measure using  X  =0.01. Using the same critical interval [0,( F null-hypothesis that all algorithms perform equally well for the confidence measure. Finally, using the critical interval whether there are significant differences among the three algorithms using all the measures, the Bonferroni for p =0.05; and 1.2 for p =0.01.

With regard to the support measure (see Fig. 6 a), the results indicate that at a significance level of p =0.01 (i.e., with a probability of 99%), there are significant differences between NSGA-G3PARM and G3PARM, the performance of the former being
SPEA-G3PARM and NSGA-G3PARM, the performance of the latter being statistically better. Finally, it is not possible to assert that there are significant differences between G3PARM and SPEA-G3PARM, despite the fact that SPEA-G3PARM produces the best ranking.

If we focus on the confidence measure, as shown in Fig. 6 b, with a probability of 99%, it is possible to state that there are significant differences between NSGA-G3PARM and SPEA-G3PARM, the former being statistically better. However, it is not possible to state that there are significant differences between G3PARM and both multi-objective proposals, NSGA-G3PARM being statistically better.
 differences between both multi-objective proposals, the performance of SPEA-G3PARM being statistically better. Moreover, it is not there are any significant differences between the two algorithms but the latter obtains the best ranking.
To conclude this analysis, the G3P-based multi-objective proposals perform very well when using support and confidence as objectives to be maximized. In such a situation, they offer a good alternative to G3PARM by mining more representative and reliable rules, especially the NSGA-G3PARM proposal. However, some situations require discovering rules with the lowest support but of high interest, as mentioned in Section 2.1. An example can be found when analyzing the average results obtained using the MAROB dataset. Regardless of the algorithm used, the results obtained with this dataset are always equal to 1.0 for both support measures are used as objectives to be maximized. In order to make a fair comparison, G3PARM was modified for mining only rules with a lift value greater than one, i.e., rules with interest.

The results presented in Table 9 show that multi-objective proposals discover rules with a high interest at the expense of a decrease in frequency (see Table 9 ), as is to be expected. Focusing on the lift measure shown in Table 9 , the best results are obtained with the SPEA-G3PARM algorithm, while G3PARM discovers more reliable rules (see Table 9 ). It is worth mentioning that the confidence of the rules mined with the multi-objective G3Pproposals decreases more than with G3PARM. Studying the number of rules discovered (see Table 9 ), it is seen that the G3PARM and the SPEA-G3PARM algorithms mine a small, rules, constrained by its external population size. If we focus on the NSGA-G3PARM algorithm, it obtains a large set of rules (between 32 and 49 rules depending on the dataset used). Finally, when analyzing the coverage measure (see Table 9 ), the
NSGA-G3PARM algorithm is seen to mine the best results, covering all the instances in many datasets thanks to the high number of rules mined.
 Different statistical tests [42,43] were carried out based on the average ranking for each algorithm (see Table 10 ). The Friedman average ranking statistics for average support measure distributed according to F
None of them belong to the critical interval [0,( F F ) 0.01,2,18 equally well for these measures using  X  =0.01. In order to analyze whether there are any significant differences between the three algorithms, the Bonferroni  X  Dunn test is used to reveal the difference in performance (see Fig. 7 ), where the critical difference (CD) value is 0.9 for p =0.1; 1.0 for p =0.05; and 1.2 for p =0.01.

The results indicate that for support (see Fig. 7 a), at a probability of 99%, there are significant differences between G3PARM and SPEA-G3PARM, the performance of G3PARM being statistically better. On the other hand, at a significance level of p =0.05, it is not possible to state that there are any significant differences between G3PARM and NSGA-G3PARM. However, there are at a probability of 90%, there are significant differences between both multi-objective proposals and SPEA-G3PARM is found to be statistically better.

Focusing on lift (see Fig. 7 b), at a probability of 99%, there are significant differences between G3PARM and SPEA-G3PARM, the there are any significant differences between NSGA-G3PARM and SPEA-G3PARM, although the latter obtains the best ranking. With regard to confidence (see Fig. 7 c), at a probability of 99%, there are significant differences between G3PARM and SPEA-G3PARM, the former performing statistically better. At a probability of 90%, there are significant differences between
G3PARM and NSGA-G3PARM, and G3PARM continues to perform statistically better. Similarly, at a probability of 90%, there are significant differences between both multi-objective proposals but now NSGA-G3PARM performs better in statistical terms.
Finally, as shown in Fig. 7 d for the coverage measure, at a probability of 99%, there are significant differences between G3PARM and NSGA-G3PARM, the performance of the latter being statistically better. At a probability of 90 between G3PARM and SPEA-G3PARM, although the latter obtains the best ranking.

Concluding the analysis of using support and lift as objectives, multi-objective proposals are seen to perform very well for discovering rules of high interest. The discovery of interesting rules implies a decrease in the average support so G3PARM obtains a higher average support than multi-objective proposals when support and lift are used as objectives to be maximized. Despite the fact that the G3PARM algorithm generates very frequent and reliable rules, these rules are of slight interest.
In summary, the synergy of connecting G3P and multi-objective models for mining association rules provides important characteristics. The proposed multi-objective algorithms have demonstrated themselves to perform better than G3PARM when support and confidence measures are used together as the objectives to be optimized. For these two measures, the
NSGA-G3PARM algorithm performs better than the others. As far as the coverage measure is concerned, SPEA-G3PARM obtains behave better than G3PARM for the lift measure, although G3PARM obtains better results for both support and confidence. Finally, the NSGA-G3PARM algorithm always performs better than the others with regard to the coverage measure. 5. Concluding remarks
The ARM problem has been already addressed using the multi-objective methodology. However, existing approaches in this regard work on either categorical or nominal domains, which make necessary a previous step to discretize continuous attributes.
Recently, a promising G3P algorithm, called G3PARM, was proposed to work on any domain. It is able to restrict the search space by means of a grammar, too. Using G3P for the discovery of association rules enables highly representative and understandable rules to be extracted. Moreover, any kind of valid association rule constrained by the grammar definition can be produced.
In this paper, we propose two different models that provide the advantages of using G3P together with the advantages of multi-objective optimization. These two approaches are based on the well-known multi-objective algorithms NSGA-2 and SPEA-2, which have served to produce the NSGA-G3PARM and SPEA-G3PARM algorithms.

One of the most remarkable characteristics of any ARM algorithm is focused on the evaluation of the rules mined. In the literature, a large number of measures are used to evaluate association rules and sometimes a trade-off has to be reached between some of them at the same time. Therefore, several different experiments were performed in the experimental stage combining the support-confidence and support-lift measures. Results obtained are promising, and the experimental analysis shows the strength of the proposed model. In fact, the set of rules mined, i.e., those belonging to the POF, are extremely representative, interesting and reliable. Furthermore, these algorithms enable most dataset instances to be covered by the set of rules mined.
Because of the specific grammar definition, these G3P proposals allow for rules from both numerical and categorical domains be extracted. Results have shown that when searching for frequent and reliable rules, those with a frequency above 95% and high accuracy are obtained. On the other hand, when searching for interesting and frequent rules, those mined cover a larger percentage of the dataset instances. Therefore, the efficiency of G3P in multi-objective environments for mining association rules was proven, showing that it is a competitive model for ARM and a promising area of study for the near future. Acknowledgments
This work was supported by the Regional Government of Andalusia and the Spanish Ministry of Science and Technology projects P08-TIC-3720, TIN2008-06681-C06-03 and TIN-2011-22408, respectively, and FEDER funds. This research was also supported by the Spanish Ministry of Education under the FPU grant AP2010-0041.

References
