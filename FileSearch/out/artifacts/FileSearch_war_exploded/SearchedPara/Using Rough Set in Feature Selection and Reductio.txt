 theory, with its idea of reducts, becomes an attractive and potential approach for this Component Analysis. The simple nearest  X  neighbor classifier is used in recognition recognition accuracy and rough set based algorithms in choosing significant features. basis of the presented reduction algorithms in the next sections. conditional attributes and D is the set of all decision attributes . concerned. 
B indiscernibility relation which is defined as followed: Set approximation : Let U X A B  X   X  , . We can approximate X using only the followed: followed: decision system ) , ( D C A U  X  = in which U is the set of all feature vectors of face based on rough set theory will be applied to the information system to get the reduced conditional attribute set ) ( C R R  X  . In the training phase, we use Learning Vector classifier, will be used to test the performance of the system in the recognition phase. All of the two phases are discribed in Figure 1 and 2. 
In the next sections, we introduce three feature set reduction algorithms used in our model : Johnson algorithm ([4]) based on greedy strategy, random algorithm ([4]) and the algorithm that combines rough set theory and heuristic for generating rules ([3]). 3.1 Johnson Algorithm feature in R in which i v and j v differ from each other). step, we will choose the feature in which there are the most feature vectors differing from each other. selected at the next step 1 + i if: Johnson Algorithm Input: Information system ) , ( D C U  X  Output: Reduced feature set C R  X  persons, u and v differ from each other in at least one feature in R ) 3.2 Random Algorithm at each step. At step 1 + i , the probability to choose the feature i RCa \  X  is: Random algorithm Input: Information system ),( DCU  X  Output: Reduced feature set CR  X  persons, u and v differ from each other in at least one feature in R ) 3.3 Combining Rough Set Theory and Heuristic for Generating Rules Note that the ability of conditional attribute set C to classify feature vectors into classes of persons is measured by the cardinal of the set ) ( D POS C . On the other hand, the relative core ) ( C CORE D is the set of features which will change the positive region ) ( D POS C if being eliminated. So, the initial reduced feature set 
Next, we will introduce the heuristic for adding the other features into R gradually system GDT-RS. The attribute R C a \ 0  X  will be chosen based on two comments: 2. For any R C a \ 0  X  , the partition of all consistent objects (or feature the final standard to choose features in R C \ . Algorithm 
Input: Output: Reduced feature set C R  X  . Then : Stop this database is quite small, we change some images before proceeding. In table 1 and 2 below, the second column (size of initial feature vectors) corresponds to the number above using these reduced vectors. 
Some good results in the above tables are in bold type. We have following remarks. from reduced feature vectors is higher than from the original feature vectors (e.g. six  X  feature vectors: 94.33 % vs 87% in Table 1, ten  X  feature vectors: 95.5 % vs 85.25 % in Table 2). Remark 2. Reducing the size of feature vectors increase the recognition accuracy (e.g. from 12 to 7 features: 92% vs 97.67% in Table 1; from 15 to 10 features: 86.5 % vs 95.5 % in Table 2). by Johnson and Random algorithms. 
No. Size of 4 9 90.67 9 90.67 8 90.33 9 90.67 5 10 91.33 8 89.67 6 94.33 8 97.67 6 11 91.33 8 89.67 6 94.33 9 90.67 7 12 92 8 89.67 7 97.67 8 90.33 8 13 92.33 8 89.67 7 97.67 9 90.67 9 14 92.33 9 91.67 7 97.67 10 88.67 10 15 92.33 9 91.67 7 97.67 8 89.67 that the reduced feature sets created by a ll rough set based approachs contain the most feature extracting using PCA. However, we actually need to study more investigation into the problem of using rough set in feature selection in recognition systems.
An important note is that the feature corresponding to the largest eigen value in the 
