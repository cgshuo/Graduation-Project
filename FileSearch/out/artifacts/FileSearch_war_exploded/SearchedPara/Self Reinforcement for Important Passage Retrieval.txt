 In general, centrality-based retrieval models treat all ele-ments of the retrieval space equally, which may reduce their e ff ectiveness. In the specific context of extractive summa-rization (or important passage retrieval), this means that these models do not take into account that information sour-ces often contain lateral issues, which are hardly as impor-tant as the description of the main topic, or are composed by mixtures of topics. We present a new two-stage method that starts by extracting a collection of key phrases that will be used to help centrality-as-relevance retrieval model. We ex-plore several approaches to the integration of the key phrases in the centrality model. The proposed method is evaluated using di ff erent datasets that vary in noise (noisy vs clean) and language (Portuguese vs English). Results show that the best variant achieves relative performance improvements of about 31% in clean data and 18% in noisy data. H.3 [ Information Storage and Retrieval ]; I.2.7 [ Natural Language Processing ]: Text analysis Algorithms, Experimentation, Theory Passage Retrieval, Extractive Summarization, Automatic Key Phrase Extraction, AKE, Centrality
One of the most popular [6] retrieval models is PageR-ank [1]. Although born in the web page retrieval research topic, it was the basis for several models proposed in dif-ferent disciplines [4, 6]. Namely, in the automatic impor-tant passage retrieval/summarization area, models like Tex-tRank [18] or LexRank [5] are still used as baselines or as the foundation of state-of-the-art work [3, 28]. However, as noted by Ribeiro and de Matos [20], this kind of models treats all passages of an information source equally. This is aproblembecauseinformationsourcesoftencontainlateral issues, which are hardly as important as the description of the main topic, or are composed by mixtures of topics. One possible solution is to use a biased centrality model [20].
In this work, we propose to use, on top of a biased central-ity model, a method to better reinforce the most important passages. We achieved that by devising a two-stage retrieval method, in which the first step consists in the automatic ex-traction of a collection of key phrases that will further bias the centrality model. To evaluate our method we use both clean and noisy datasets, in di ff erent languages.
In this document, the next section describes the related work; section 3 presents the datasets; sections 4 and 5 detail the two stages of our method; the results are included and discussed in section 6. The conclusions close the document. Automatic Key Phrase Extraction (AKE) is a Natural Language Processing (NLP) task that selects the most im-portant words or phrases from a document. Key phrases are phrases consisting of one or more significant words (key-words). As they capture semantic metadata, search engines can use them to enhance indexing, to help users in queries completion [30] and improve web tra ffi cprediction[12]. Sev-eral NLP applications, such as summarization, information retrieval, information extraction, and question answering, can benefit from their extraction as well. Several unsuper-vised key phrase methods have been proposed, such as lan-guage modeling, graph-based ranking and clustering [16]. However, the TF-IDF across di ff erent methods remains a strong unsupervised baseline [7]. Both supervised and un-supervised approaches have been explored to perform AKE. Supervised methods formalize this problem as a binary clas-sification problem of two steps [17, 14]: candidate generation and filtering of the phrases selected before.
Assessing the relevant content is the first step of automatic summarization systems. On the one hand, extractive sum-marization consists of determining the most relevant seg-ments, usually sentences, of one or more information sources. On the other hand, automatic abstractive summarizers also need to identify the most relevant content that, then, will be submitted to transformation and generation stages.
Text and speech information sources influence the com-plexity of the approaches di ff erently. For text summariza-tion it is common to use comple xinformation,suchassyn-tactic [27], semantic [25], and discourse information [26], either to assess relevance or reduce the length of the out-put. However, speech summarization approaches have a extra layer of complexity caused by speech-related issues like recognition errors or disfluencies. As a consequence, it is necessary to use speech-specific information (for ex-ample, acoustic/prosodic features [15] or recognition confi-dence scores [31]) or by improving both the assessment of relevance and the intelligibility of the output of an auto-matic speech recognition system (by using related informa-tion [21]). These problems not only increase the di ffi culty in determining the salient information, but also constrain the applicability of text summarization techniques to speech summarization. Nevertheless, shallow text summarization approaches such as Latent Semantic Analysis (LSA) [9] and Maximal Marginal Relevance (MMR) [2] seem to achieve performances comparable to the ones using specific speech-related features [19]. In addition, discourse features start to gain some importance in speech summarization [15, 33].
Closely related to our work are the unsupervised key phrase extraction approaches that have been explored to reinforce summarization [32, 29, 11, 22, 24]. Namely, Litval and Last [11] and Riedhammer et al. [22] propose the use of key phrases to summarize news articles [11] and meetings [22]. Litval and Last explore both supervised and unsupervised methods to extract key phrases as a first step towards ex-tractive summarization. We use a feature-rich supervised method for key phrase extraction, unlike the ones proposed by Litval and Last which are grounded on ad-hoc and struc-tural features and use a graph-based representation. More-over, our adaptation of the centrality summarization model plays an important role in the whole process, an inexistent step in their work. In that sense, Riedhammer et al. propose the method closest to ours: the first stage consists in a simple key phrase extraction step, based on part-of-speech patterns; then, these key phrases are used to define the relevance and redundancy components of a MMR summarization model.
In order to assess the quality of our method, we analyzed its performance using two di ff erent datasets.
The PT BN dataset, used in previous work [21], consists of the automatic transcriptions of 18 BN stories in Euro-pean Portuguese, which are part of a news program. News stories cover several generic topics like  X  X ociety X ,  X  X olitics X , and  X  X ports X , among others. For each news story, there is ahuman-producedabstract,usedasreference.Theaverage word recognition error rate is 16.5% and automatic sentence segmentation attained a slot error rate (SER, commonly used to evaluate this kind of task) of 81.5%. Although this dataset was used in previous work, news story segmentation problems were corrected in one case. However, this does not change the relevant properties of the corpus. To evaluate our method using clean input, we used the Concisus Corpus of Event Summaries [23]. The used sub-corpus is composed by 78 event reports and respective sum-maries, distributed across three di ff erent types of events: aviation accidents, earthquakes, and train accidents.
In previous work, Marujo et al. [14] expanded the MAUI toolkit with shallow semantic features, such as number of Named Entities, POS tags, 4 n-gram domain model proba-bilities. These expansion improved the quality of their ap-proach to generate tag clouds of Portuguese Broadcast News. As a result of the domain similarity and the good results, we used the same approach to extract key phrases in our summarization experiments for Portuguese.

In the following year, Marujo et al. [13] adapted the AKE work to English and investigated additional semantic fea-tures and pre-processing steps, namely Light Filtering and Co-reference normalization. These new features included the detection of rhetorical devices, Freebase sub-categories, and news articles top categories. Including such new fea-tures and pre-processing steps improved the key phrase ex-traction results beyond the state-of-art. Therefore, we used the methodology of Marujo et al. [13] in our summariza-tion experiments in English corpus. However, since the pre-processing steps, namely Light Filtering, could have im-pact on the outcome of our experiments, they were removed. This fact lead to the exclusion of the Freebase sub-categories which were only beneficial in combination with pre-processing steps. Unfortunately, the news articles top categories were not available. Therefore, the inclusion of rhetorical devices features is the main di ff erence between the PT and EN AKE.
To determine the most important sentences of an informa-tion source, we use the centrality model described by Ribeiro and de Matos [20]. The reasons to choose this model are its adaptability (the authors of the model suggest how to integrate additional information sources), the language in-dependence, and the state-of-the-art performance on both clean and noisy input.

This centrality model is based on the notion of support set: after dealing with the representational aspects, the first step of the method is to compute a set consisting of the most semantically related passages, designated support set. Then, the most important passages are the ones that occur in the largest number of support sets.

Given a segmented information source I ! p 1 ,p 2 ,...,p N asupportsetiscomputedforeachpassage p i (Eq. 1, sim () is a similarity function, and  X  i is a threshold). Passages are ranked in accordance to Eq. 2. For our two-stage important passage retrieval method, we adapted the model in three di ff erent ways: KP-Centrality is the approach where key phrases are considered regular passages (augmenting the number of support sets). In method OKP-Centrality ,passagesthatdonotcontainkeyphrases are removed from the support sets; CKP-Centrality weights the passages using the bagged dec ision tree co nfidence s cores.
To assess the quality of our method we made experiments using the two datasets presented in section 3. To evalu-ate the detection of the most important sentences, we used ROUGE [10], namely ROUGE-1, which is the most widely used evaluation measure for this scenario. In the exper-iments using the PT BN dataset, the summary size was determined by the size of the reference human summaries, which consisted in about 10% of the input news story. In the experiments using the EN ER dataset, we generate 3 sentence summaries, commonly found in online news web sites, like Google News.

We compare the new 3 di ff erent approaches described in section 5 to the baseline (the centrality-as-relevance raw model), with the number of key phrases ranging from 5 to 40. The metric used to configure the centrality model was the cosine (using IDF). The heuristic used to compute the size of each support set was the one based on the selection of the sentences with less distance to sentence under analy-sis [20]. LexRank performance was also included for a better understanding of the improvements.
Figure 1 shows the results for the PT BN dataset. As we can see, the best performing approach is method KP-Centrality. Method OKP-Centrality only outperforms the baseline for 10 key phrases and it is clearly worse for 5 key phrases. However, its performance remains similar to baseline for the other variations. Method CKP-Centrality performance is always similar to the baseline X  X  performance. In figure 2 is possible to observe the performance of the Figure 1: ROUGE-1 scores for the PT BN dataset. same methods when applied to the EN ER dataset. In this dataset, both methods KP-Centrality and CKP-Centrality have a better performance than the baseline. Similarly to what happens in the PT BN dataset, KP-Centrality achieves the best results. However, in this dataset the performance improves directly with the number of keys phrases (until 60), while in the other dataset the best results are achieved around 10 key phrases. The performance of CKP-Centrality does not vary with the number of key phrases. Method Figure 2: ROUGE-1 scores for the EN ER dataset.
 OKP-Centrality achieves a performance similar in both data-sets, although in the EN dataset it does not outperform the baseline.
We start by noting that the Portuguese BN dataset is noisy (it is a ff ected by speech-related problems like recog-nition and segmentation errors), while the English one is clean. Another important aspect is that the performance of the AKE is better for English than Portuguese due to the contribution of the rhetorical signals. The influence of this last aspect can be seen in the performance of KP-Centrality, which keeps improving on the English dataset (until 60 key phrases), what does not happen in the Por-tuguese BN dataset. The two mentioned aspects can be the justification for the results achieved by CKP-Centrality: it outperforms the baseline in the English dataset, while hav-ing a performance similar to the baseline in the Portuguese dataset. The stability of the performance of OKP-Centrality and CKP-Centrality can be justified by their nature: they do not generate more support sets than the base model, while in KP-Centrality, new support sets are also computed for key phrases. The poor performance of OKP-Centrality shows that the removal of passages does not improve the base mode, since it already has the capability of distinguish-ing between the main topic and lateral issues.
In this work, we introduced a two-stage method for im-portant passage retrieval. Popular centrality-based models treat equally all elements of the retrieval space, impacting the retrieval task negatively. In line with recent work [8, 20], that begins to address this problem, we show that our method can improve the performance of a retrieval model that already addresses this issue. The method we propose starts by extracting a collection of key phrases that will be used to bias a centrality-as-relevance retrieval model. We explore three di ff erent approaches to the integration of the key phrases and experiment using noisy (automatic speech transcriptions) and clean (event reports) data. One of the approaches (KP-Centrality) clearly improves the baseline model in both noisy (by 18%) and non-noisy data (by 31%). The rhetorical devices used in the English dataset can be a possible justification for the performance di ff erence between the two datasets. On the other hand, the approach where passages that do not contain key phrases are removed does not achieve as good results, which means di ff erent aspects are captured in the two stages of our method. Key phrases and this centrality model seem to complement each other.
In the future, we plan to explore the use of key phrases in the computation of the similarity between passages, and im-prove the current methods of integration of the key phrases.
We would like to thank FCT for supporting this research through PEst-OE/EEI/LA0021/2011, the Carnegie Mellon Portugal Program, and grant SFRH/BD/33769/2009. [1] S. Brin and L. Page. The anatomy of a large-scale [2] J. Carbonell and J. Goldstein. The Use of MMR, [3] H. Ceylan, R. Mihalcea, U.  X  Ozertem, E. Lloret, and [4] Y. Ding, E. Yan, A. Frazho, and J. Caverlee. Pagerank [5] G. Erkan and D. R. Radev. LexRank: Graph-based [6] M. Franceschet. PageRank: Standing on the Shoulders [7] K. Hasan and V. Ng. Conundrums in unsupervised [8] O. Kurland and L. Lee. PageRank without [9] T. K. Landauer and S. T. Dumais. A Solution to [10] C.-Y. Lin. ROUGE: A Package for Automatic [11] M. Litvak and M. Last. Graph-Based Keyword [12] L. Marujo, M. Bugalho, J. P. Neto, A. Gershman, and [13] L. Marujo, A. Gershman, J. Carbonell, R. Frederking, [14] L. Marujo, M. Viveiros, and J. P. Neto. Keyphrase [15] S. R. Maskey and J. Hirschberg. Comparing Lexical, [16] Y. Matsuo and M. Ishizuka. Keyword extraction from [17] O. Medelyan, V. Perrone, and I. H. Witten. Subject [18] R. Mihalcea and P. Tarau. TextRank: Bringing Order [19] G. Penn and X. Zhu. A Critical Reassessment of [20] R. Ribeiro and D. M. de Matos. Revisiting [21] R. Ribeiro and D. M. de Matos. Multi-source, [22] K. Riedhammer, B. Favre, and D. Hakkani-T  X  ur. Long [23] H. Saggion and S. Szasz. The concisus corpus of event [24] R. Sipos, A. Swaminathan, P. Shivaswamy, and [25] R. I. Tucker and K. Sp  X  arck Jones. Between shallow [26] V. R. Uz X  eda, T. A. S. Pardo, and M. das Gra  X  cas [27] L. Vanderwende, H. Suzuki, C. Brockett, and [28] X. Wan, H. Li, and J. Xiao. EUSUM: Extracting [29] X. Wan, J. Yang, and J. Xiao. Towards an iterative [30] F. Wei, W. Li, Q. Lu, and Y. He. Query-sensitive [31] K. Zechner and A. Waibel. Minimizing Word Error [32] H. Zha. Generic summarization and keyphrase [33] J. J. Zhang, R. H. Y. Chan, and P. Fung. Extractive
