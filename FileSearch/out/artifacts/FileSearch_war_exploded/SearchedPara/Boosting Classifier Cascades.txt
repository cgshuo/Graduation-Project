 limitations of the VJ design, namely how to guarantee overal l cascade optimality. where S example x loss, whose risk is the probability of classification error. Hence, R where C ( f ( x )) is the complexity of evaluating f ( x ) , and L the overall complexity. In this case, which we assume throug hout this work, L and L where S  X  which can be solved with Lagrangian methods. These minimize a Lagrangian Figure 1 illustrates this trade-off, by plotting the evolut ion of R target ( y = 1 ) if and only if it is declared a target by all stages of H , i.e. h is illustrated in Figure 1, where the evolution of R introducing a mathematical model for a detector cascade. 3.1 Cascade predictor Let H ( x ) = { h where larger. Letting F to m More generally, the following recursion holds with initial condition F leads to with initial conditions T Since T cascade predictor, F , on the predictor of the k th stage. 3.2 Differentiable approximation Letting F ( f uation of both F ( f direction g These are straightforward for the last stage since, from (10 ), where It follows that 3.3 Cascade complexity the complexity of evaluating f with initial conditions C ( F In practice, f complexity. For example, the set of projections into Haar fe atures, in which C ( f the number of features g used in a previous cascade stage, e.g. f case, f The second is the set O ( f also available and require minimal computation (multiplic ation by the weight c running sum). The third is the set N ( f overall computation is wavelets,  X   X  1 and the complexity of the cascade to with 3.4 Neutral predictors tion that we will exploit is that reason n ( x ) = f In this section, we introduce a boosting algorithm for casca de design. 4.1 Boosting direction of weak leaner g is where w 4.2 Cascade risk minimization Using (13) and (19), &lt;  X R L ( F ( f k )) ,g &gt; = where w k descent direction and step size for the k th stage are then In general, because the b k be used. Note that, since a k ( x evaluated and the stage of largest impact is updated. 4.3 Adding a new stage at each boosting iteration, or consist of a single weak learn er. 4.4 Incorporating complexity constraints requires the computation of the functional derivatives where y s which follows from (24). Using (26), with  X  k then with w k the optimal descent direction and step size for the k th stage are then A pair ( g  X  FCBoost supports a number of extensions that we briefly discu ss in this section. 5.1 Cost Sensitive Boosting ple, the risk of CS-AdaBoost : R E
Pedestrian 1,000 10,000 200 2,000 Figure 2: Left: data set characteristics. Right: Trade-off between the error ( R 5.2 Bootstrapping the false positive rate of the cascade being learned reaches 50% . as weak learners. Figure 2 summarizes the data sets.
 FCBoost cascades. Figure 2 plots the accuracy component of t he risk, R complexity component, R FCBoost (  X  = 0 . 02 ) was used in the last experiment. Method VJ [13] FloatBoost [5] ChainBoost [15] WaldBoost [9] [8] FCBoost date.
 Applying (9) recursively to (8) where T Let C ( f with and initial conditions P with initial conditions C ( F
