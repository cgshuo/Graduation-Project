 1. Introduction
There is an increasing drive towards measuring the research quality of academics whether it is for the purposes of pro-motions, jobs, or assessing the performance of departments, research institutes or even whole universities. Quality can only be judged through the activities and publications produced, especially journal papers as that, in Business and Management, is the primary currency. However, assessing the quality of individual papers by peer review is itself time consuming, requires expert(s) in the area, and is open to disagreement. Partly for these reasons, the quality of the journal that the paper is pub-lished in is often taken as a proxy for the quality of the paper itself. This then displaces the problem to judging the quality of journals (and assuming that all papers within are of equal quality)  X  hence the proliferation of journal rankings.
There are two main ways of generating rankings  X  stated preference (peer review), where some group of experts deter-mines a ranking, and revealed preference where actual publishing behaviour is measured usually in terms of the citation 2007 ). There are many ranking lists available on the Harzing website ( http://www.harzing.com/ ) but for Business and Man-agement the list produced by the Association of Business Schools (ABS) ( http://www.the-abs.org.uk/?id=257 ) has become extremely important in the UK. There is evidence from, for example, the Committee of Professors in Operational Research  X  (COPIOR) that it is being used in decisions about appointments, promotions and even probation/tenure as well as to influ-ence academics X  publication decisions. There are two major problems, however  X  the lists do not agree with each other, and in reality there will be a range of quality within any one journal.

The use of journal rankings as a proxy for quality is actually extremely contentious. For example, Paul (2007, 2008) , who was a member of the 2008 RAE Panel, states that  X  X  X ne major conclusion appears to be that journal rankings are not a good indicator of the quality of any paper published in that journal, nor necessarily the combined quality of all the papers X  X  ( Paul, 2008, p. 324 ). Macdonald and Kam (2007) , in a bitter critique, suggest that the whole world of academic publishing in man-agement is one of gamesmanship and game playing with the so called quality journals simply reproducing standard, consen-sual research within a small elite community. Clark and Wright (2007) , then editors of the J. of Management Studies , disagreed and argued that journals do develop and change in response to their communities, and that the reviewing processes of high quality journals do in fact lead to high quality papers. Adler and Harzing (2009) provide another strong critique of the dys-functional effects of academic ranking systems and journal rankings in particular. The main complaint is that they lead to a narrowing of the discipline, concentrating research into the narrow confines of established journals and discouraging inno-vation and interdisciplinary work.

In the light of these debates, the latest (2008) UK Research Assessment Exercise (RAE) provides a major opportunity to investigate the extent to which journal rankings are concordant with direct judgements of the quality of individual papers.
A quality profile was generated in terms of the proportion of the department X  X  research that was judged to be on a 4-point scale plus unclassified. Full details of the RAE can be found in various reports issued by HEFCE ( RAE, 2004, 2005, 2006 ) prior to it, and the results were announced in 2008 ( RAE, 2008 ). Also available online are the subject overview report for the Busi-ness and Management Panel ( RAE, 2009a ); the complete submissions ( RAE, 2009b ); and the quality profiles ( RAE, 2009c ).
Quality itself was defined in terms of three characteristics  X  originality, significance and rigour  X  and the levels were: 4  X  : Quality that is world-leading, that has become, or is likely to become, a primary point of reference in the field or sub-field. 3  X  : Quality that is internationally excellent, that has become, or is likely to become, a major point of reference in the field or sub-field. 2  X  : Quality that is recognised internationally, that has made, or will make, a contribution to knowledge, theory, policy or practice. 1  X  : Quality that is recognised nationally, that has made or will make a limited contribution.

Unclassified (0  X  ): Quality that falls below the standard of nationally recognised work or which does not meet the defini-tion of research.

The requirement of the Panel was to assess a department X  X  quality in terms of three dimensions: their submitted research outputs (publications of all types, although predominately journal papers); the research environment; and the esteem of the staff members. A profile was generated for each dimension and these were combined (70%, 20%, 10%) to produce the overall 1  X  . From the profile a single-valued grade point average (GPA) is calculated (in this case 2.8) and this is used for producing league tables of departments and then universities. As we are only concerned with the outputs, i.e., papers, the environment and esteem profiles will not be considered.

The Panel was therefore required to produce a quality grading for every single piece of work submitted, in this case 12,575 papers. This was clearly a huge task and initially the B&amp;M Panel stated that it would look at 25% in detail although the actual number was never published. The results that were made public consisted of the grade profile (i.e., % of papers in each quality level) for each of the 90 institutions that submitted together with details of all the publications. However, the grades for individual outputs are not available.

Prior to the exercise, the Panel Chair (Professor Mike Pidd) made it clear on several occasions that the Panel did not intend to use journal ranking lists in making their judgements. He also stated that they expected to find a range of qualities within a single journal. What was not clear was how they would in fact grade outputs if they were only actually going to read a pro-portion of them. In the event, the Panel claimed that  X  X  X ost outputs were read in considerable detail X  X  ( RAE, 2009a , p. 5). It would seem to be a formidable task: 12,600 outputs to be read by 18 academics (700 each) in a few weeks, but nevertheless it does represent a major exercise in directly assessing the quality of research outputs. However, little is said in the review reports about precisely how the quality judgements were made, how the grade boundaries were determined, or the extent of consensus or dissensus among Panel members.

The purpose of this paper is to try to use the peer review quality judgements made by the Panel to evaluate journal qual-ity and journal ranking lists such as the Association of Business School X  X  one. Geary, Marriott, and Rowlinson (2004) per-formed a similar task after the 2001 RAE although their approach was somewhat indirect. They assumed that staff in higher quality departments will tend to publish in higher quality journals and that therefore one could assess a journal X  X  quality by the RAE grade of the departments that submitted it. Frequency counts were calculated for each journal and it was then awarded points on a 7-point scale corresponding to the RAE grades for each department that submitted it. An aver-age score for the journal could then be calculated using the mean, mode or median. This method has obvious drawbacks: it does not discriminate that well between journals; the score for a department depends on things other than the research out-puts; and it ignores the fact that there may be  X  X  X slands X  X  of excellence in otherwise weak departments.

A similar method could be used for 2008 based on the mean or GPA of the department X  X  profile, but we are proposing a more sophisticated approach that relates the outputs submitted and the quality profile awarded to them for each of the 90 departments. This is done using linear programming (LP). In brief, we create a set of decision variables for each journal that represent the five possible quality levels (including unclassified as zero). We then use LP to find the values of those variables that minimise the difference between the estimated quality profile (calculated from the variables) and the actual quality pro-file awarded to each department. The approach is analogous to least squares regression but with several thousand variables to be determined. The result is an estimate of the proportion of papers from a journal that were awarded the various levels of quality. In undertaking this analysis we are not suggesting that the Panel came to its decisions by using journal rankings. We are exploring the extent to which the Panel X  X  actual results, given only in aggregate form, can be replicated by an analytical method. We are not trying to replicate the process used in arriving at the results.

In the first section we compare the outputs submitted to the 2008 RAE with those from previous ones. We then describe the methodology including the mathematical model(s) developed, and the data cleaning and manipulation. Finally, we ex-plore the results obtained and comment on their validity. 2. Comparison of the 2008 and 2001 submissions
In this section we will just present the basic facts of the RAE submissions in comparison with previous ones without con-sidering the quality levels. Note that the journal data has been the subject of a cleaning process which is described in the next section. In particular, papers that were submitted as internet journal publications, i.e., they had been published elec-tronically but not in print, were allocated to the appropriate print journal.

Table 1 shows that since 1996 there has been a contraction of the number of submissions, and presumably departments, but an increase in the number of staff and publications submitted. Staff have increased by 43% but outputs by nearly 60%. It is also noticeable that journal papers have come to dominate the submissions reaching 90% in 2008. Other forms of research such as authored books, edited books and research reports are certainly not being submitted to the RAE, whether or not they are actually being produced. The number of different journal titles is also rising inexorably although it is not necessarily the case that they are all highly regarded as later results will show. The mean number of entries per journal and entries per insti-tution has also risen significantly.

The dominance of journals can be seen more clearly in Table 2 . Authored books and especially book chapters have fallen dramatically. This may well be attributable to comments by Panel members before the submission that one had to be careful with outputs that had not been peer reviewed, i.e., that were not in refereed journals. It will be interesting to see how the REF (the next RAE) tackles this problem with its increased emphasis on the external impact of research not merely its scholarly impact.

Looking within the journals, it has always been the case that submissions follow the Pareto rule  X  a small number of jour-nals account for a large proportion of the submitted outputs and vice versa.

We can see from Table 3 that around 50% of the journals only have a single entry in the RAE although this proportion reduced in 2008. Over 70% of journal titles have 4 or less entries. On the other hand, a relatively small number of journals account for a high proportion of total entries. The 105 most common journals between them account for 50% of the journal outputs submitted and as Fig. 1 shows, the top 20% of journal titles account for almost 80% of the submitted outputs.
Concentrating on these, we can compare the most popular journals with those from 2001. The 20 most frequently sub-mitted journals from 2008 are listed in Table 4 along with their relative positions on the Geary equivalent for 2001. These journals represented 22% of the 2008 outputs, compared to 20% of those in 2001. Fifteen journals have retained their place in the top 20, while five have slipped out  X  Human Resource Management Journal, Industrial Relations Journal, Personnel Review,
Applied Economics and Long Range Planning . The top 20 journals cover most of the spectrum of Business and Management and it is interesting that two top journals classified as social science have entered  X  Regional Studies and Research Policy . All but Service Industries Journal are 3  X  or 4  X  in the ABS rankings.

In the light of the discussion in the introduction about rankings lists leading to standardisation, it is interesting to see what proportion of the journals submitted are actually included in the ABS list (note that this research used version 9 of the list, the latest one available at the time). Fig. 2 shows that there were 825 journals in the RAE that are not in ABS; 224 journals in ABS that were not entered in the RAE; and 814 that were in both. The first figure shows that 50% of the
RAE journals are not actually included in ABS which is the most comprehensive listing of B&amp;M journals there is. There would seem to be two possible reasons: genuine Business and Management journals that ABS has not yet included, and journals that are not business ones. These would typically either be applications journals, e.g., health services or construction, or other relevant disciplines, e.g., philosophy or social science. The latter examples could be seen as healthy interdisciplinary and applicability, or they could be seen as business schools being somewhat of a  X  X  X umping ground X  X  for academics who do not fit well in other, more focussed, departments.

The issue of fragmentation within B&amp;M submissions is important as Bence and Oppenheim discuss ( 2004 ). First, it is very difficult for Panel members to genuinely have expertise across such a wide range of subjects although they are able to cross-refer papers to other panels. The 2001 Panel expressed quite strong concerns about this problem ( Bessant et al., 2003 ). This time the Panel overview was more sanguine but still concluded that  X  X  X ome submissions ... seemed to be of little or no rel-evance to Business and Management studies and ... some submissions were an over-eclectic mix of outputs X  X  ( RAE, 2009a ,p. 5). The second, related concern is that even if Panel members consider themselves competent to judge a paper there may be an unconscious bias towards papers published in core business journals rather than more peripheral ones. Some evidence relevant to this will be presented in the results section.

On the other side, we can see that 22% of the ABS journals were not actually submitted in the RAE. At first sight this seems quite high, especially given the large range of journals that were submitted. The main explanation is likely to be that they are the lower ranked journals which departments chose not to submit for fear of getting low quality gradings. This is confirmed there is a lower proportion of 1  X  journals in the submitted ABS journals than in the ABS list as a whole. 3. Modelling the aggregate RAE quality evaluations 3.1. The LP model
Linear programming (LP) is a mathematical method which determines the values of a set of decision variables so as to maximise or minimise a linear function of those variables (the objective function) subject to a set of linear constraints. In our situation we know the quality profiles for each institution and we also know how many entries for each journal the insti-tution submitted. We can then create two sets of variables  X  the grade profile for each journal and the estimated grade pro-file for each institution. The grade profile for a journal consists of five variables each of which represents the proportion of
The estimated grade profiles for institutions are formed from the journal grades, weighted by the number of articles an institution submitted from each journal. The estimated profile is, for each institution at each grade, the sum of each journal total number of articles in that department X  X  submission. The objective (function) is then to minimise the difference between the estimated profile and the actual profile for each institution by finding the best values for the journal grades. Initial model (QP1)
Let: j index the journals ( j =1 ... no. of journals) g index the grades 0  X   X 4  X  ( g =0 ... 4) i index the universities ( i =1 ... no. of institutions) u ig be the actual proportion of research at grade g for university i n ij be the number of entries of journal j submitted by university i e ig be the estimated proportion of research at grade g for university i p jg be the estimated proportion of the outputs of journal j graded at grade g s.t.
The objective function (1) minimises the squared differences between the actual and the estimated proportion of research outputs at each grade level for each department. Constraint (2) defines the estimated proportion in terms of the number of entries of a journal multiplied by the proportion of the journal at a particular grade and divided by the total number of en-tries for that department. Constraint (2) ensures that the grade proportions for each journal sum to 1. It is possible to for-mulate this model without explicitly using an estimated proportion variable, but we have done it this way for clarity.
As formulated, this is actually a quadratic program as the objective function is quadratic. Since solving large quadratic programs is generally computationally more expensive than linear ones an alternative model was produced with a linear objective minimising the absolute difference rather than the squared difference.
 Alternative model (AbsVal1)
Although the absolute value function is itself non-linear it can be easily linearised by generating a new variable ( err two new constraints: s.t. Here constraints (4) and (5) between them ensure that err takes on only the positive difference between actual and expected.
A third model was also developed with the idea of determining a single integer quality grade for each journal rather than a grade profile. This was easy to achieve in the formulation by simply restricting the journal grade variables ( p integers. The constraint that they must sum to 1 for each journal ensures that only one of the five possibilities will actually be 1 and so each journal will have only one grade level. This model (MIP1) proved very difficult to solve computationally using either the quadratic or the absolute value objective function.

We should discuss one possible concern with the model as formulated. The model is trying to estimate the proportions of show that, even if you know the actual proportion of different grades in the journal for a particular department (which in practice you never do), the model does not yield the correct estimate of the department X  X  overall quality. In other words, it would appear as though there is some built-in error in the model preventing it getting the right answer.
 the departments that submitted it, not just one. In other words, the overall proportions of grades within a journal are actually a kind of average across all the departments. It cannot therefore be expected to reflect the actual values for any one department unless, of course, the distribution of grades for the journal was identical in all departments, which is extremely unlikely.

In general, the greater the disparity of grade distribution for a particular journal across different departments, the greater will be the inaccuracy for any one department. We cannot actually know the real figures because the RAE did not release them but we do not think that there is any reason to assume it should be particularly extreme. We cannot see that, in prac-tice, papers from a particular journal submitted by one department would systematically receive very different gradings to those in the same journal submitted by another department.

Even if it did occur, perhaps by chance in particular examples, we do not think that it would produce a systematic bias  X  merely a degree of over-or under-estimation as one would expect with an averaging process. We believe that the overall validation of the results, as presented later in the paper, demonstrate the underlying soundness of the model. 3.2. Cleansing the data
Each submitting institution used a pro-forma to enter details of their outputs. After the publication of the RAE (2008) out-comes, the details of the individual outputs were released. The spreadsheet h RA2 i was downloaded from the Business and
Management sub panel section of the RAE website. There were 12575 records in the data set. Each output had to be placed within 1 of 20 categories (summarised in Table 2 ).

The RA2 data was used to derive a list of all journals, along with the number of outputs from each journal, cross-refer-enced with the institutions submitting those outputs. As journal nomenclature can be imprecise, journal titles were checked to ensure that no journal is listed more than once ( X  X  X he Journal of Example X  X  and  X  X  X ournal of Example X  X  must be resolved, is there an error in one of the titles? Are there two distinct journals?). We also used the journal ISSNs which were part of the
RAE data but again there was a good deal noise here: some were entered as text and some numeric; some were incorrect; and some journals actually have more than one ISSN. The online papers were treated as a different category in the RAE data (type H) but we amalgamated those with their printed equivalents. The journal titles adopted in the ABS list of journals were used in preference to any other variants found in the RA2 data.

The next issue was what to do with the outputs that were not journal papers, in the main books and book chapters. We could not include each item individually as if it were a journal because the model only works to the extent that the same journal occurs in a number of submissions. We could simply leave them out which would increase the residual variation in the results but lose information. So, what we did was to include each type (authored book, book chapter, etc.) as if it were a journal. Thus all 285 authored books were included as if they were a single journal. This increased the accuracy of the mod-el and also allowed us to see how these output types were treated by the RAE Panel. Were books rated highly or lowly? These categories (Book, Book Chapter, Edited Book, External Report and Other) represented 950 outputs (7.6% of the whole dataset).
We also had to decide what to do with all the journals that had only a small number of entries. The problem is that if the journal only occurs a small number of times it becomes essentially unconstrained and the model can use it simply to fill in unexplained variation. After some experimentation we decided to only include in the model those journals that had at least three entries. This meant excluding around 57% of the journal titles (see Table 3 ). We recognise that this may introduce some bias into the model but it was not apparent what this might be other than they were generally of a low quality.
The final output of this process was a 2-dimensional array indexed by journal name and institution. The full list of cleaned data is available from the authors. As noted by Geary et al. (2004) , the process of cleansing the RA2 data was the most inten-sive part of the project, and the results produced may not be identical to others attempting the same task. By using the ABS list as a standard and automating the search process errors are kept to a minimum. 3.3. Solving the models
With the data arrays prepared and the linear programme constructed, the programme was coded using the OPL Studio 4.1 modelling language and solved by the CPLEX 11.0 optimizer. Several runs of different versions and sizes of the model were conducted. The final version of the AbsVal1 included 701 journals and 89 institutions. It included roughly 4400 variables and 2050 constraints. It solved in about a minute and gave an objective function value of 23.6, i.e., the sum of all the 445 errors.
With a model like this where the variables are relatively under-constrained, there may be many solutions which differ mar-ginally and give broadly similar results. The sensitivity was explored and although there were many reduced costs with low values there were none with zero. In terms of the validity of the solution this is best evaluated in terms of concordance with other evidence, a task that is carried out in Section 4.1.

The integer version proved to be computationally very expensive. After running continuously for 35 days it had still not reached an optimal, fully integer solution. This is not unusual with models that have a large number of integer variables (3500 in our case). It had in fact converged to a near optimal which did not change significantly over 21 days but could not be shown to be the actual optimal.

This model gave a grade profile for each journal but for the purpose of constructing a journal ranking and comparing it with existing ones it is more appropriate for each journal to have a single grade. There are several ways of achieving this: take the modal grade, i.e., the one with the largest proportion; calculate the mean grade (i.e., the GPA) and then round this to the nearest integer; or get the LP to calculate the best value with the integer version of the model (MIP1).
After inspecting and comparing the results for the three different methods  X  mode, rounded mean, and MIP1 it was decided that the mode gave the fairest and most consistent results and so this has been used in the ranking comparisons, but the final table of results ( Supplementary Table ) includes the grade profile for each journal and the MIP1 results. 4. The results
The full results are too large to be printed in the paper but are referred to as Supplementary Table . This presents results for all the journals included in our model, i.e., those with at least three entries. We show the grade awarded based on the mode of the journal profile; the profile itself in terms of the proportions judged to be in each rank; the number of items sub-mitted, the ABS rank where available and the subject classification. The table is available as Supplementary material . 4.1. Assessing the validity of the reconstruction
Before presenting the results in detail, it is important to evaluate their degree of validity. The philosophy of the model is that, given the aggregate results from the Panel and knowing the papers that were submitted, it should be possible to recon-struct to some extent the grades that were awarded at the journal level. Clearly, if the results we obtain are wholly at odds with our preconceptions of journal quality we might conclude that they were not capturing anything meaningful. But, we would not expect them to be identical with the existing rankings, partly because of noise in the data resulting from the non-journal outputs being removed, and partly because the Panel were clear that their results did not mirror the existing lists ( RAE, 2009a , p. 1). So validity is a matter of degrees of concordance.
 We first consider the extent of concordance with existing journal rankings.

Table 5 shows the correlations between the reconstructed RAE grades and the ABS, Kent ( Mingers &amp; Harzing, 2007 ) and the Geary et al. (2004) rankings. Given the large numbers of observations (shown in parenthesis) all the correlations are highly significant. It is noticeable, however, that they are not as high as the correlations between the rankings themselves tend to be, as shown in the ABS documentation. We can see for example that the correlation between the Kent ranking and the ABS one is significantly higher. Nevertheless, it is clear that the RAE model is broadly in line with these rankings.
Some further evidence is shown by the treatment of non-journal outputs. As explained in the previous section, rather than totally ignore outputs such as books, book chapters and reports we included them as if they were a single journal. This generated a score for each of these categories so that we could see how the category was treated in comparison with the journals. The results are shown in Table 6 .

In the third column we can see the mean grade awarded to each output type. From a validation perspective the order of these types is what we would have expected, i.e., authored books were graded most highly, going down through book chap-ters to edited books. Other types (e.g., software) and external reports were least valued. This again gives us a degree of con-fidence in the overall method. In terms of the actual numbers, there was a concern before the RAE that books would be downgraded because they were not refereed. This does seem to have happened in that one might expect that a quality book would be regarded more highly than a single paper and so books should have achieved a high grading  X  at least 3  X  or more.
We can also see that external reports scored poorly which does not bode well for the REF trying to encourage the submission of work that has external impact.

Finally we look at the journals that come out top from our reconstruction in Table 7 . In terms of our estimation, the best and we have restricted it to those with at least 12 submissions. This results in 30 journals which are ordered in terms of the % lists including Geary X  X  analysis of the 2001 RAE and the citation impact (CI) factor. Interestingly, 14 of them are also included in the FT top-40 list of journals which is used to rank business schools worldwide. Of the rest of the FT-40 list, all but four were graded 3  X  , those being Human Resource Management (USA), International J. of HRM, J. of Business Ethics , and J. Interna-tional Business Studies which were only graded 2  X  . Table 7 also includes a sprinkling of the very top American journals such as AMR , Management Science, Organization Science, HBR and the American Economic Review . Given that these results have been generated purely by the model it does give us confidence that the results do reflect judgements about journal quality. 4.2. Comparing the RAE grades with the ABS ranking
As the ABS list has become the de facto standard for Business and Management in the UK, and is used extensively, for bet-ter or worse, in making decisions about appointments, promotions and submissions, it is important to see how it compares with the reconstructed RAE grades.

Table 8 shows the proportions of journals awarded different grades from the ABS ranking and our RAE reconstruction. The first column shows the proportions in the total ABS list, whether or not they were submitted in the RAE, with a GPA of 2.17.
Column 4 shows the proportions for all those journals in our RAE list (remembering that it excludes journals with less than three entries) with a GPA of 2.34. The proportions are significantly higher ( X our RAE list  X  columns 3 and 6. The two proportions are in fact very similar with GPAs of 2.43 and 2.42 although there are more of the extreme grades.

We can also look at the distribution of differences between the RAE and ABS. Table 9 shows, for each RAE grade, the num-
Accounting Review and Journal of Rural Studies . Although they had relatively small numbers of submissions. 4.3. Selectivity of journal submission
We now move to the issue of selectivity of journal submission. On the one hand, as we saw in Section 2 , there were an increased number of journals entered into RAE (2008) and a significant number of these are not even in the ABS list. This suggests a wide range of material. However, at the same time there is continual pressure on institutions to submit only the best work and this pressure will grow. There is currently concern that increasingly the top business schools will limit their academics to publishing only in the top A-rated journals. Indeed, the Parliamentary Select Committee on Science and Technology raised these very concerns in a report in 2004:  X  X  X he perception that the RAE rewards publication in journals with high impact factors is affecting decisions made by authors about where to publish. We urge HEFCE to remind RAE panels that they are obliged to assess the quality of the content of indi-vidual articles, not the reputation of the journal in which they are published. X  X  ( Select Committee on Science and Technology, 2004 ).

Guidelines recently issued concerning the 2013 RAE (the REF) ( HEFCE, 2009 ) say that they aim to support quality rather than quantity and the number of academics and papers is likely to reduce. This will lead to institutions focussing even more on those believed to be high quality journals.

The degree of selectivity can be seen from column 2 of Table 8 which shows the grade proportions in those ABS journals that were not submitted in the RAE. These are significantly different to the profile of ABS journals that were submitted ( X attention was focussed on those ABS journals that are at least 2  X  .

The possible results of this effect can also be seen in Fig. 3 which is a scattergram of the proportion of an institution X  X  submission in ABS journals on the x -axis and the GPA gained by institution on the y -axis. The correlation coefficient is highly significant (0.6) and it explains 36% of the variation in GPA by itself. Taken at face value, this shows that the greater the con-centration on ABS journals the better an institution did in its GPA. This might suggest that the RAE Panel grades papers from ABS (or at least mainstream Business and Management if not ABS per se ) journals more highly than others.

There are other possible interpretations of this association. One might suppose that high quality institutions produce more papers that are in the mainstream of B&amp;M anyway, and that there will be more papers available to be selected, so that the institution can choose mainly ABS ones. Whereas poor quality institutions have to make do with what papers they have, and may include more academics from the fringe areas. On this interpretation, the association would be indirect rather than causal  X  the high GPA and the high proportion of ABS both reflect underlying high quality rather than one causing the other.
Alternatively, one could interpret it as reverse causality and as evidence for the selectivity effect mentioned above  X  the bet-ter quality institutions are more rigorous in limiting their staff to ABS-only journals.
 We can get some more evidence directly from columns 5 and 6 of Table 8 . This shows the distribution of reconstructed
RAE grades for ABS and non-ABS journals. Did the RAE Panel actually grade ABS journals higher than non-ABS ones? They are significantly different ( X 2 4  X  39 : 9) but although there are fewer 4  X  and 3  X  than would be expected in the non-ABS journals, the biggest difference is that 13% of the non-ABS journals were allocated 0  X  as opposed to only 2% of the ABS ones. In other words, according to our estimates a significant proportion of the non-ABS papers were considered to be of no research merit.
This could be a legitimate response of the Panel to submissions that were not relevant to Business and Management. They state, p. 5) But it could also reflect a conscious or unconscious bias towards recognised journals regardless of paper quality.
Overall, we feel that there is evidence both of extensive selectivity in submissions and possible bias in judgements against non-mainstream B&amp;M journals. 4.4. Dispersion of grades for a journal
Another issue in connection with journal rankings is the extent to which the RAE Panel would award all papers in a par-ticular journal the same grade which would indicate that they simply went by the ranking of the journal. The Panel stated both before and after that they did not intend to do that, and the results do back them up to some extent.

Given that we are choosing the grade of a journal by its modal grade, i.e., the grade with the greatest proportion, we can measure the degree of dispersion by the percentage that is not in the modal grade. Journals with 100% in one grade will thus have zero dispersion. The greatest dispersion a journal could have is 66% with 34% being in the modal grade.
Table 10 shows the frequency distribution of dispersion. In fact the majority (62%) have been found to have 100% at a particular grade. Clearly this is only the estimate from our model and we do not know if this is the actual case but there is no reason for our model to choose 100% particularly and one would expect that it would do the best it could to match the grade profiles so as to minimise the squared deviations in the objective function. So this evidence would suggest that many journals were seen as having only one quality level (although not necessarily the same as their ABS grading of course). Table 11 shows those journals that are ranked as 4  X  in ABS but which have a high degree of dispersion in the RAE results.
As can be seen, there are some well known journals here and most have a large number of entries so the results should be reliable. In many cases the split is just between two adjacent grades, e.g., Organization Studies or J. of Marketing , but in some little dispersion were shown in Table 7 . 4.5. Differences between subject areas
It is of interest to look at the relative grading between subject areas. Reports from both the 2001 RAE ( Bessant et al., 2003 ) and the 2008 RAE ( RAE, 2009a ) make it clear that the subjects were seen to have different levels of quality. So, to what extent is that borne out by the ratings? Fig. 4 shows the mean journal grading by the ABS sector for the journal where it was in ABS. Those not in ABS have been given the title  X  X #N/A X  X .
 The data show a significant difference from the highest sector, Psychology, with a weighted average score of 2.8 down to
Tourism and Hospitality with a score of 1.3. The non-ABS journals have an average of 2.0. The ABS sectors are somewhat different to the subject groups that the RAE Panel report discusses. We can see that many of the long-established disciplines (e.g., Psychology) and management areas (e.g., Accounting and Finance, Operational Research and Organisational Studies) scored highly while newer and perhaps more applied areas (e.g., ethics, management development, innovation and tourism) did less well. Some surprises perhaps are the poor score for Strategy and perhaps the relatively high scores for General Man-agement (which is a bit of a catch-all category) and Public Sector.

We want to look in more detail at specific subject areas and have chosen Operational Research as that is where we have expertise. Note that some, more mathematical, OR groups were submitted to the Statistics and OR Panel so their contribu-tions are not included here. Table 12 shows all OR journals ranked in terms of the reconstructed RAE grade and then the number of entries. Those with a  X  X #N/A X  X  in the ABS Grade column were not classified in the ABS list but we have added them in as they all would be considered as OR journals. There are seven 4  X  journals (all comments in this section refer to grades reconstructed by the model) although all but Management Science have small numbers and three do not appear in ABS. Some of these are likely to be due to the small sample, but Decision Sciences, J. of Heuristics and the SIAM journal are generally con-sidered to be strong. In the 3  X  journals comes EJOR , with the second largest entry, and a wide range of other journals, many rather than 4  X  .
 highest in the whole RAE) and, together with EJOR , is the main publication outlet for UK academics who find it hard to pub-lish in the US journals. JORS figured highly in the Geary analysis of the 2001 RAE because of their methodology which rated journals in terms of the departments which published in them. It happens that the largest groups of OR academics are at
Lancaster and Warwick which were top rated in 2001 and so JORS secured a high grade. However, this was unrealistic in terms of the journal X  X  world rating ( Mingers &amp; Harzing, 2007 ) as the current result shows. It is a surprise, however, that it has gained no 4  X  work at all in our reconstruction. One other anomaly is Naval Research Logistics which is one of the ori-ginal OR journals and did have a strong reputation. However, in recent years its impact factor has fallen considerably, and in the recent COPIOR OR journal list it was only graded as 2  X  . 5. Discussion
In this section we wish to reflect on the extent to which our analyses can shed light on some of the theoretical debates surrounding journal rankings, the effects of the RAE on research, and peer review.

Journal rankings are a cause of great debate and controversy. Several issues emerge: the extent to which the quality of the which the quality of a journal can in any case be captured by a single ranking; and the extent to which departments and schools are using journal rankings inappropriately to make promotion and employment decisions, and to limit the types of journals in which their faculty can publish.

Many academic decisions, at all levels, depend on a judgement of the quality of a paper or other research output but this is far from straightforward. In principle, one might say that it should be done by competent experts in the subject, i.e., peer review, but even this approach is very fallible. There have been many studies of the process of reviewing papers for journals ( Starbuck (2003, 2005) provides a good overview) and the general conclusions are that there is a high level of disagreement between reviewers and many biases are prevalent. In a classic experiment, Peters and Ceci (1982 ) resubmitted 12 papers already published in high quality journals to the journals they were published in under assumed author names. Only three of the 12 were spotted, and eight of the nine were subsequently rejected.

Problems with obtaining valid peer judgements leads decision-makers to fall back on the quality of the journal as a proxy for the quality of the paper. This relies on two assumptions: that journals can be effectively ranked and that all papers within a journal are of the same quality. Considering the latter issue, it is commonly felt that there is not a uniform quality within journals. Certainly, as mentioned above, the RAE Panel took the view that there was not, and claimed strongly not to have based their judgements on journal rankings. The question has been approached theoretically by Starbuck (2005) who devel-oped a statistical model of the journal review process which showed that, based on divergences of opinion in referees X  re-ports, significant proportions of high quality work would be rejected from top journals and trickle-down into lower tier ones, and equally, less good work would be accepted by top tier ones. The results deduced in this paper also show a degree of dispersion of grades across a journal as reported in Section 4.5 but the results are equivocal. Sixty-two percentage of the journals were allocated to only one grade, thus 38% were given a range of quality levels. Moreover, as Table 11 shows, there were some ABS 4  X  journals that had a range of quality levels. Moreover, the dispersion on some journals was very wide
The second issue to be considered is the extent to which the RAE, and disciplinary processes such as journal ranking lists more generally, are having pernicious effects on the quality and direction of academic research ( Nkomo, 2009; Suchan, 2008 ). Such effects include: pressure on academics to publish only in a narrow range of top-ranked journals ( Truex, Cuellar, &amp; Takeda, 2008 ); pressure to concentrate on papers at the expense of other forms of output and a corresponding need to produce bite-sized pieces of research rather than more major and significant contributions ( de Rond &amp; Miller, 2005 ); pres-sure to avoid the more marginal areas of research or interdisciplinary research which are likely to be rated less highly. In turn, these pressures make it very difficult for new subjects ( Stewart, 2005 ) or areas of research ( Ozbilgin, 2009 ) and new journals to become established; lead to much game-playing in the publication process ( Dulek, 2008; Macdonald &amp; Kam, 2007 ); reduce the level of external engagement and impact  X  rigour at the expense of relevance ( Syed, Mingers, &amp; Murray, 2009 ); and generate a self-perpetuating hegemony of theories, methodologies, journals and institutions ( Adler &amp; Harzing, 2009 ).

There is a degree of empirical support for some of these arguments. Moed (2008) analysed publications patterns in UK science over 20 years, covering three RAEs (1992, 1996 and 2001), which showed changes in response to changing RAE requirements  X  increased quantity for 1992, increased quality for 1996, and a greater degree of collaboration in 2001. In terms of this RAE we can see that journal papers have come to dominate the submissions ( Tables 1 and 2 ), from 69% in 1996 to 92% in 2008 with authored books and book chapters falling dramatically. We can also see ( Table 6 ) that non-journal outputs (and particularly external reports) were not ranked highly by the Panel despite the general RAE guidelines which stated that all forms of output should be treated equally.

In terms of the pressures for journal selectivity there is mixed evidence. On the one hand, a greater range of journals were submitted ( Table 1 ) and half of them were not included in the ABS list ( Fig. 2 ), but we cannot tell from the data whether this was a matter of necessity, because they were the only publications that were available, or genuine choice. I have looked in detail at the submissions of four top-30 Schools and they all had very low proportions (&lt;10%) of non-ABS journals. On the other hand, there was clear evidence of selectivity in submission with a low proportion of ABS 1  X  journals being included ( Table 8 ), and there is possible evidence that journals included in ABS were rated more highly by the Panel. Fig. 3 shows an association between the proportion of a School X  X  submission that was in ABS and their overall GPA, and Table 8 shows that non-ABS journals were graded lower with a significant proportion being judged as 0  X  .

The final area to be discussed is the question of the peer review process itself. Again, this is the subject of strongly held opinions with some arguing that its inherent subjectivity and openness to bias means that more objective, bibliometric mea-way of getting a fair and rounded evaluation ( Jones, Brinn, &amp; Pendlebury, 1996a, 1996b ). As mentioned above, there have been studies of peer review in journal refereeing processes, and also in the awarding of grants ( Marsh, Jayasinghe, &amp; Bond, 2008 ) but nothing specifically on the RAE peer review process. Thomas and Watkins (1998) list the main problems with peer review in this context as: the influence of the author X  X  reputation on rating; rating journals that one publishes in oneself more highly; having only a limited range of expertise and down-grading work in other areas; and being overly influenced by input factors such as grants. To this could be added being affected (positively or negatively) by: the institution (the Russell Group effect), the journal, personal knowledge of the author, or the theoretical or methodological nature of the research.
We are not suggesting that such biases are conscious or intentional and we have to recognise that the context necessarily
RAE processes and the Panel as a whole, and loyalties to institutions and disciplines. Research suggests that experts often believe that they are behaving rationally and fairly even within a situation of conflicting interests when in fact they are gated how the auditors in major scandals such as Enron appeared unaware of the extent to which they had become com-promised.  X  X  X sychological research on the impact of motivated reasoning and self-serving biases questions the validity of this assumption {that an auditor is either independent or in collusion}. This evidence suggests that intentional corruption is probably the exception and that unconscious bias is far more pervasive. X  X  ( Moore et al., 2006, p. 16 ). They suggests mech-anisms such as selective perception, escalation of commitment, inaccuracies of self-perception and the effects of account-ability that underlie this motivated reasoning.
 Research into questions of conscious or unconscious bias could be of two types  X  X irect observation and analysis of the
RAE processes themselves, which seems infeasible given the secrecy imposed by HEFCE, or analysis of the results in compar-ison with the Panel membership. The reconstruction of journal grades here published could be used as part of such an analysis. 6. Conclusions
The 2008 RAE has been a huge exercise in peer review and the judgements that were made would have been extremely valuable in addressing some of the issues that surround the whole idea of journal rankings. Unfortunately, the gradings of individual outputs have been kept secret which was, in our view, both unnecessary and undesirable. What we have at-tempted to do in this paper is to reconstruct the judgements made by the RAE Panel at least at the level of individual journals although not at the level of papers. We have done this by developing a mathematical programming model that determines the best grade profiles to match the overall institutional profiles for all journals submitted that had at least three entries. We have shown, both in terms of internal reliability and in terms of correspondence with existing ranking lists such as the
ABS list, that the results we have generated have a high degree of plausibility. It is extremely unlikely that they do not rep-resent to a reasonable degree the actual judgements made by the Panel although clearly we can never actually assess the extent of the residual error.

With these results, we have been able to comment on several issues that have arisen concerning the conduct and effects of the RAE, as well as produce an RAE-based ranking for around 700 journals in Business and Management and related areas.
Many of these journals are not included in the ABS list. Care should be taken in interpreting the results, especially for journals that had few entries.

Comparing the grades given by the RAE with those in ABS, on those journals that are in common the overall results are very similar in terms of the average grade awarded. However, there are differences in the proportions of each grade with even three grades apart.

In terms of the RAE leading to selectivity, there is evidence in both directions. There were a very wide range of journals submitted, many of them not in ABS, but many of these non-management journals were given a low rank. It is clear that there was selectivity in the submissions with relatively few ABS 1  X  journals being submitted. There is also a clear asso-ciation between the GPA awarded to an institution and the proportion of its submission that was in ABS journals although the direction and nature of the causality is unclear.

The RAE Panel was clear that it was not grading papers on the basis of the journal they were published in. There is evi-dence that supports that since many journals, even top ones, had a degree of dispersion in their gradings. However, our results also produced 62% of journals with 100% in a single grade suggesting a considerable degree of uniformity in judgement.
 As expected, there were significant differences in the gradings given to different sectors with Psychology, Accounting and Finance, Management, and OR doing well and Management Development, Innovation, and Tourism faring worst.

For non-journal outputs, our results show that books (GPA 2.4) and book chapters (GPA 2.2) gained grades that were com-mensurate with journals, but reports (GPA 1.4) and other forms of output (GPA 1.3) were seen as poor. This does not bode well for the REF and its focus on external research impact.

In terms of wider significance, this research demonstrates an entirely new method for estimating underlying journal rankings from a set of peer reviewed data such as that produced by the RAE. To our knowledge the only similar attempt was by Geary et al. (2004) which used a fairly crude form of averaging. This represents a novel application of LP. Appendix A. Supplementary data
Supplementary data associated with this article can be found, in the online version, at doi:10.1016/j.ipm.2012.01.008 . References
