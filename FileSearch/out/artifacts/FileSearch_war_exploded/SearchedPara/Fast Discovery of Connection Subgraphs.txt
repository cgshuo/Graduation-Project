 We define a connection subgraph as a small subgraph of a large graph that best captures the relationship between two nodes. The primary motivation for this work is to provide a paradigm for exploration and knowledge discovery in large social networks graphs. We present a formal definition of this problem, and an ideal solution based on electricity ana-logues. We then show how to accelerate the computations, to produce approximate, but high-quality connection sub-graphs in real time on very large (disk resident )graphs.
We describe our operational prototype, and we demon-strate results on a social network graph derived from the World Wide Web. Our graph contains 15 million nodes and 96 million edges, and our system still produces quality re-sponses within seconds.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms, Human Factors
Suppose we are given a large graph and we are asked to find the relationship between two nodes  X  X  X  and  X  X  X . For il-lustration, we shall use a social network as an example of a graph. In the simplest case, the relationship is manifest as an edge in the graph. However, social network graphs are typically sparse, meaning that a vanishing fraction of node pairs actually have an edge between them. Nonethe-less, they may be related due to a composition of simple edges:  X  X  X  is related to  X  X  X , and  X  X  X  is related to  X  X  X . In this case, the relationship might be encapsulated as a path in the graph. In real life, however, the relationship between On sabbatical from Carnegie-Mellon University two people is often multi-faceted; for example,  X  X  X  and  X  X  X  might have the same manager and the same dentist. More-over, the paths connecting two people may not be vertex-disjoint; for instance, the dentist may also be the sister of  X  X  X , or may be dating the sister of  X  X  X . Representing the real-life relationship between two nodes in a graph using a single path is inherently limiting for two reasons: first, any automated mechanism to pick the most important path will make mistakes, and showing a subgraph will increase the probability that the critical path is present; and second, there may not be a critical path, as in the example of two people who have written papers together with a multitude of co-authors, rather than a single co-author. In this paper, we address the problem of extracting from very large graphs, a small (amenable to visual inspection )subgraph that best captures the connections between two nodes of the graph.
Connection subgraphs are useful in many domains. In a social network setting, connection subgraphs will help us identify the few people most likely to have been infected with a disease (or heard a rumor, or information-leak, or joke). They can also help us spot whether an individual has unexpected ties to any members of a list of individuals. In other domains, connection subgraphs will help us summarize the connection between two web sites using the hyper-link graph; or the connection between two proteins in a metabolic network; or between two genes in a regulatory network. More formally, the problem of interest is as follows: Connection Subgraph Problem Given: an edge-weighted undirected graph G ,vertices s and Find: a connected subgraph H containing s and t and at
The constraint on b is motivated by limitations on visual-ization of graphs ( e.g. , b  X  100). The function g represents the  X  X oodness X  of the solution H .If g is the negative sum of edge weights in H , for instance, then the resulting connec-tion subgraph will be the shortest path from s to t ;thisis a valid answer, but probably not the most illuminating one. Likewise, on the other extreme, if g is the number of edges of H then the connection subgraph will be the densest set of s -t paths; this is again probably not the best answer.
The Connection Subgraph Problem thus has two sub-problems:
In the following, the budget on vertices can be replaced with a budget on edges as required by the problem domain.
Sub-problem 1: What function g is an appropriate  X  X ood-
Sub-problem 2: How can the subgraph H maximizing g In this paper we propose a particular function g , tailored to produce connection subgraphs that capture salient aspects of relationships in social network graphs (though it also ap-plies to graphs arising from other applications). We also propose algorithms to compute the solution on very large graphs.

Our formulation and upcoming solutions are domain in-dependent, but we illustrate our techniques on a specific data set that we believe has a great deal of interest in itself. Specifically, we used  X  X amed-entity X  extraction algorithms to derive a name graph from the World Wide Web. In this graph, the nodes represent names of people, and there is an edge of weight w between two names if the names appear in close proximity on w different web pages. Our data set con-tains roughly 15 million distinct names, and about 96 million distinct edges, drawn from over 500 million web pages. The  X  X ame graph X  is a valuable resource, because it can help us find patterns, outliers, and connections. In Figure 1 we show three connection subgraphs that were produced by an interactive prototype system (described in Section 5.1 )that computes and displays good connection subgraphs in a few seconds.

Although the discussion in this paper focuses on connec-tion sub-graphs between persons, we also envision applying it to graphs that describe relationships between arbitrary pairs of entities, including persons, companies, products, or-ganizations, species, documents, web sites, phone numbers, etc. We expect that connection subgraphs will prove widely useful in interactive data exploration systems.

The structure of the rest of the paper is as follows. In Sec-tion 2 we describe some related work, and consider the prob-lem of what constitutes a good objective function g . The de-tails of our approach are presented in Sections 3 and 4, with experimental results on the name graph in Section 5. We de-scribe the interactive system in Section 5.1, and summarize our conclusions and suggest future directions in Section 6.
The heart of this paper is how to find  X  X ood X  connection subgraphs. To our surprise, we have not found any work that directly addresses this problem in the published literature. There are well studied areas in graph theory and networks that have some overlap on the surface, but none attacks this problem. We review this work next, show why some  X  X ea-sonable X  approaches perform poorly, and we briefly review other related work.

The two most natural measures for choosing  X  X ood X  paths would be the shortest distance, and the maximum flow cri-terion, in which the edge weights represent a maximum flow on the edge. Both of these fail to capture a natural no-tion of  X  X est path X  in social networks. Consider the graph of Figure 2 with unit weights. In this case, the shortest paths from s to t go through nodes 3 and 4, and both have length 2. Notice however that node 4 has many edges, as would be the case if the node represented a famous person with many incidental connections. Thus, one would intu-itively prefer the path through node 3, but this preference is not captured by the traditional shortest path computa-tion. Other distance functions in graphs[19, 22] match more closely with our intuition of which paths are best for con-veying a relationship, but it is important to stress that our measure g prefers subgraphs that contain multiple possibly overlapping paths where possible. Our goal is a good sub-graph rather than a path or collection of paths. Figure 2: A simple network where both shortest path and network flow fail to adequately model so-cial relationships. With all edges having weight 1, flow fails to distinguish between the paths s ,1,2, t and s ,3, t , even though the latter is shorter. Total path length fails to distinguish between the paths s ,3, t and s ,4, t , even though path through 4 is diluted by many extra connections.

If we instead treat connection subgraph generation as a maximum flow problem, we find that paths s ,1,2, t and s ,3, t both carry 1 unit of flow, although we would intuitively pre-fer the shorter path through node 3, since social relation-ships tend to blur with distance. Thus both shortest paths and network flow models fail to adequately capture the no-tion of a  X  X ood X  path in social networks, although both seem related.

There has been considerable work on community detec-tion [11, 9, 12]. However, reporting the  X  X ommunity X  of two remotely related nodes will force us to far exceed our bud-get b of allowable edges, and in cases where the two people belong to different communities, we are interested in rela-tionships between their communities.

The main problem of this paper is also related to pre-vious work on survivable networks (e.g., see [13]). There, the objective function g is usually expressed as the count of edge-disjoint or vertex-disjoint paths from the source to the destination. This measure also fails to adequately model social relationships, as the two paths in Figure 2 through nodes 3 and 4 have the same survivability: each path  X  X ies X  with the deletion of one node or edge.

Other related work includes the PageRank [21] and per-sonalized PageRank algorithms [15, 16]; graph clustering, partioning, and matrix reordering [2, 4, 17, 23, 24]; elec-trical circuits and random walks [3, 7, 22]; and influence propagation [18].  X  X ustomer value X  of a node [5]; and other topics on sparse graphs [1, 6, 8, 20].

However, again, in all these references, we have not found even the definition of the connection subgraph problem, let alone an attempt to solve it.
The approach that we propose is related to electrical cur-rents in a network of resistors [7]. Let G ( V , E )denote an undirected weighted graph, and let C ( e )denote the weight of the edge e . We then interpret this graph as an electri-cal network in which each edge e represents a resistor with conductance C ( e ). In a nutshell, we propose to choose as our connection subgraph the one that can deliver as many units of electrical current as possible. There are a few subtle traps, however, that can lead us astray. Let X  X  start with a review of electricicty laws. Table 1 lists the symbols and definitions used throughout the paper.

Suppose that we apply a voltage of +1 volt to the start node s , and ground (0 volts )on the destination node t .Let I ( u,v )be the current flow from u to v and let V ( u )denote the voltage at u .ThenwehaveOhm X  X law: and Kirchhoff X  X  current law:
These laws uniquely determine all the voltages and cur-rents, as the solution to a linear system: (where C ( u )= v C ( u,v )is the total conductance of edges incident to the node u ), with boundary conditions:
The voltages and currents of the resulting network have fascinating connections to quantities related to random walks along the graph. For example
Lemma 1. (See [7, p. 52]) Consider an electrical network defined by (3), (4). Consider also all random walks on the associated graph that (a) start from the destination node t (b) end on the source node s (c) following an edge ( u,v )with a probability that is proportional to its conductance ( C ( u,v ) ) (d) without revisiting the destination node. (Zero or more intermediate visits to the source node are permitted). Then, the electric current I ( u,v ) is proportional to the net number of times that such walks will traverse the edge ( u,v ) .
It is tempting to use this formulation of current flow as our measure of goodness for a connection graph, namely the subgraph of a given size that maximizes the total current v I ( v,t )flowing into the destination node. However, that has a flaw: consider the graph of Figure 2, and compare the two paths s  X  3  X  t and s  X  4  X  t . In the above setting, they will both carry the same current of 1/2 Amperes each, while we would like the path through node 3 to be more favorable. To compensate for this, we propose to follow [22], and introduce a universal sink node z that is grounded: and is connected to every node u of the graph with an edge of conductance for some parameter  X &gt; 0. We used  X  =1, but nearby choices make little difference. The universal sink absorbs a positive proportion of the current that flows into any given node, in a way reminiscent of  X  X ax X . Thus, it penalizes a node with high degree, because it taxes it not only directly, but multiple times as well, indirectly, through its neighbors. An extra fringe benefit is that it also heavily penalizes long paths, exactly because it taxes them repeatedly for every node that the path contains.

The intuition of Lemma 1 carries through, with just a few slight modifications, namely, that the random walks are also forbidden from reaching the universal sink. In any case, sub-graphs that carry much current are exactly the subgraphs we want to include. More accurately, we want a subgraph that, after the  X  X axation X  by the universal sink z ,isrespon-sible for delivering high current to the sink t . Thisisthe concept of Delivered Current , which we formalize next in subsection 4.1.
The goodness function g ( H )that we propose is exactly the total delivered current that the chosen subgraph H car-ries from source to sink, after the repeated taxations by the universal sink z . We are now faced with the problem of finding good connection subgraphs under that measure. We can reduce the problem to that of calculating the currents on the original graph, followed by a process that extracts a subgraph that carries high current to t . We refer to the latter problem as that of display generation , and we dis-cuss it in detail in Section 4.1. Calculating current flows with a universal sink is feasible even for very large graphs, but not in an interactive environment. In order to address this problem, we propose an optional preprocessing step, called candidate generation . The idea is to quickly produce a moderate-sized graph, by removing nodes and edges that are too remote from s and t to influence the solution. In our interactive system, this is what allows us to produce good connection subgraphs within a few seconds. We describe the candidate generation process in Section 4.2.
The display generator takes as input the weighted, undi-rected graph G and the flows I ( u,v )on all ( u,v )edges, and produces as output a small, unweighted, undirected graph G disp (  X H )suitable for display to the user. Typically, we expect G disp to have 20 X 30 nodes. Results showing how well this algorithm performs are given in Section 5.

As we mentioned earlier, the proposed goodness measure is the  X  X elivered current X  that the chosen subgraph G disp carries from source s to sink t . Notice that each atomic unit of flow (i.e., each electron )must travel along a single path; it is thus possible to decompose the flow into paths. This will allow us to formalize the notion of current delivered by a subgraph. We require the following sequence of definitions. Definition 1. Node v is downhill from u ( u  X  d v ), if I ( u,v ) &gt; 0 ,or,identically, V ( u ) &gt;V ( v ) . We can then define I out ( u ), the total flow leaving node u :
Definition 2. Total out-flow from node u : I out ( u )=
Definition 3 (Prefix path). A prefix path is any down-hill path P that starts from the source s , that is, P =( s = u ,...,u i ) where u j  X  d u j +1 Obviously, a prefix path has no loops, because of the down-hill requirement.

Definition 4 (Delivered current). The delivered cur-rent  X  I ( P ) over a prefix-path P =( s = u 1 ,...,u i ) is the vol-ume of electrons that arrive at u i from s , strictly through P . Formally, we define  X  I () inductively as follows, beginning with a single edge as base case: In words, to estimate the delivered current to node u i through path P , we are pro-rating the delivered current to node u proportionately to the outgoing current I ( u i  X  1 ,u i ).Weare now ready to define the current delivered by a subgraph; no-tice that this definition is intentionally quite different from the current delivered by applying voltages and computing current flows on the subgraph alone.

Definition 5 (Captured flow). We say the captured flow CF ( H ) of a subgraph H of G is the total delivered cur-rent, summed over all source-sink prefix paths that belong to H .

Consider the graph shown in Figure 3. For simplicity of exposition, and without loss of generality, we do not have a universal sink z (that is, we set  X  =0). After the voltages of the source and sink have been fixed to 1 and 0 respectively, the resulting voltages are shown for each other vertex. These voltages induce currents along each edge as shown. There are five downhill source-to-sink paths in the graph. These paths, with their delivered current are shown in Table 2. The path that delivers the most current (and the most cur-rent per vertex )is s  X  b  X  t . We can compute the 2/5A delivered by this path by observing that, of the 0.5A that arrive at vertex b on the s  X  b edge, 1/5 depart towards Figure 3: A sample network, showing voltages, cur-rent, and paths with delivered current. vertex c , while 4/5 departs towards vertex t .4 / 5  X  0 . 5A gives the 2/5A we seek.

Consider the { s,b,c,t } subgraph. We can compute its captured flow by adding the delivered current of all paths that travel exclusively through the subgraph; namely, s  X  b  X  c  X  t and s  X  b  X  t ; these paths together capture 2/5 + 1/10 = 0.5A of total current. We observe that this is one of two optimal 4-vertex subgraphs that could be produced.
Our optimization problem is now to find a subgraph that maximizes the captured flow over all subgraphs of its size. For this we apply a greedy heuristic, as follows. First, it initializes an output graph to be empty. Next, it iteratively adds end-to-end paths (i.e., from source s to sink t )to the output graph. Since the output graph is growing, a new path may include vertices that are already present in the output graph; the algorithm will favor such paths. Formally, at each step the algorithm adds the path with the highest marginal flow per capita. That is, it chooses the path P that maximizes the ratio of flow along the path, divided by the number of new vertices that must be added to the output graph.

Notice that the inductive definition of delivered current given above could easily be computed using dynamic pro-gramming. We will modify this computation in order to compute the path that maximizes our measure.

We begin with a definition of entries in our dynamic pro-gramming table D v,k (for  X  X elivery matrix X ), to be inter-preted in the context of a partially built output graph.
Definition 6. D v,k is the current delivered from s to v along the prefix path P =( s = u 1 ,...,u = v ) such that: 1. P has exactly k vertices not in the present output graph 2. P delivers the highest current to v among all such paths
To compute D we exploit the fact that the electric current flows I (*,* )form an acyclic graph. Formally, we arrange the vertices into a sequence u 1 = s,u 2 ,u 3 ,...,t = u n such that if node u j is downhill from u i ( u i  X  d u j )then u j u in our ordering ( i&lt;j ). That is, the nodes are sorted in descending order of voltage, and so electric current always flows from left to right in the ordering. We will fill in the table D in the order given by the topological sort above, guaranteeing that when we compute D v,k , we will already have computed D u,  X  for all u  X  d v .Theentriesof D are computed as follows: Algorithm 1 (Display Graph Generation).
 Initialize output graph G disp to be empty Let P be the maximum allowable path length (trivially, the target size of the display graph) While output graph is not big enough: For i  X  [1 .. |G| ] : Add the path maximizing D t,k /k,k =0
Intuitively, I ( u,v ) /I out ( u )represents the fraction of flow arriving at u that continues to v . Multiplying this by D gives the total flow that can be delivered to v through a sim-ple path. The path maximizing our measure is then the path that maximizes D t,k /k over all k = 0; it can be computed by tracing back the maximal value of D from t to s .
As mentioned earlier, computing the voltages and currents on a huge graph can be very expensive, and thus real-time responses are impossible. To create a real-time variant of the system, we propose an optional precursor step which we call candidate generation . This step extracts a subgraph of the original graph which we call the candidate graph .The extraction algorithm must quickly produce from the original graph a subgraph that contains the most important paths. This subgraph is then treated as the full graph for the re-mainder of the algorithm: current flows are computed as usual but on the candidate graph, and the display generator is applied to the result.

Formally, the candidate generation process takes a s and t vertex in the original graph G , and produces a much smaller graph ( G cand )by carefully growing neighborhoods around s and t . The focus of the expansion is on recall rather than precision; during display generation we will remove any spu-rious regions of the graph. As we will show, using candidate generation it is possible to attain performance close to opti-mal with a latency that is orders of magnitude smaller.
In the framework, candidate generation algorithms strate-gically expand the neighborhoods of s and t until there is a significant overlap. As the algorithm proceeds, it will ex-pand s , discovering other candidate vertices that it may choose to expand later. Our underlying assumption is that the graph is stored, say, in edge-list format, which makes node expansions inexpensive.

Let D ( s )be the set of vertices first discovered through a series of expansions beginning at s ;wesaythat s is the root of all vertices in D ( s ). We define E ( s )as the set of expanded vertices within D ( s ); that is, they have been accessed in a data structure, and their neighbors are now known. Like-wise, let P ( s )be the set of pending vertices within D ( s )that have not yet been expanded. Similarly, define D ( t ) ,E ( t ), and P ( t ).Notethat D ( s )is disjoint from D ( t )since each vertex is discovered only once, by expanding a vertex whose root is either s or t . Recall that for weighted graphs, we use C ( u,v )as the weight of the edge from u to v .Wede-fine deg( u )to be the degree (number of neighbors )of u . Algorithm 2 gives the high level pseudocode.
 Algorithm 2 (Candidate Generation). Given a graph G that is weighted and undirected, and two vertices s and t , find G cand  X  X  whichismuchsmallerthan G but contains most of the interesting connections between s and t . Set P ( s )= { s } and P ( t )= { t } .
 While not stoppingCondition(): // pick v , the most promising node of P ( s )  X  P ( t ) v  X  pickHeuristic() // and expand it Let r be the root of v Expand v ,movingitfrom P ( r ) to E ( r ) Add all new neighbors of v to P ( r )
Thus, the details of the algorithm lie in the process of deciding which node to expand next, and when to termi-nate expansion. Our algorithm repeatedly expands carefully selected unexpanded nodes, chosen by the pickHeuristic() , until a stopping condition stoppingCondition() is reached. Thesearethetwomajorroutines,andwedescribeaspecific heuristic for them in the Appendix. In effect, pickHeuristic() strives to suggest a node for expansion, estimating how much delivered current this node will carry. Thus, the heuristic favors nodes that are (a )close to the source s or the sink t (b )with strong connections (high conductance )and (c )low degree, to avoid the  X  X amous-node X  effect (recall node 4 of Figure 2).

The stoppingCondition() puts limits on the size of the output graph G cand (count of expansions, count of distinct nodes discovered, etc).
The calculation of currents on a network with a universal sink requires the solution of the linear system (3 )and (4 ). For a graph with N nodes and E edges, this can be done by direct methods in O ( N 3 )operations, but iterative meth-odswilloftenperformmuchbetteronsparsegraphs.Fora graph with E edges, we would expect to perform O ( E )op-erations per iteration, and the number of iterations depends on the gap between the largest eigenvalue and the second largest eigenvalue. In the case of the names graph we ob-served a fairly rapid rate of convergence, and this can be expected for many other social network graphs as well.
The display generator takes O ( ekb )time, and O ( vk )space, where v is the number of vertices in the input graph, e is the number of edges, k is the maximum length of any allowed s -t path, and b is the budget, or desired number of vertices in the display graph.

The candidate generator runs until its termination condi-tions are met, performing a single disk seek per expansion. Timings are provided in the appendix. Typically in our interactive system, the candidate generator requires 1-1.5 seconds to run, on a graph with  X  10 5 edges.
In this section we describe components of our prototype system and evaluate the performance of the various com-ponents. First, we give the experimental set up (data and queries), and then we describe our results. Our experiments were designed to answer the following questions:
We started from a text analytics system called WebFoun-tain[14] that has been under development at IBM Almaden Research Center. This system routinely crawls the web and collects documents, which are further processed ( X  X amed-entity X  extraction modules, cleanup of homonyms, synonyms etc.). Our  X  X ame graph X  was derived from approximately 500 million web pages, by spotting names, and adding an edge between them whenever two names occurred within a window of approximately ten words of one another. If the pair of names generates edges from w distinct web-pages, then this gets collapsed to a single edge of weight w . The resulting graph contains N =15,020,632 names, with E =96,689,078 unique edges between them.
 Software architecture. Our system was implemented with a web-based interface, in perl, python, C++, Apache, and php, and it runs on a Pentium class machine with a 2GHz processor running Linux 2.4. Users can submit pairs of names from our web interface. The system then runs the  X  X andidate generation X , the voltage computations and the  X  X isplay generation X , and displays the resulting connection graphs. We used the GraphViz system [10] for the graph-layouts. Moreover, our system provides click-able nodes and edges: on a click our system displays additional information (names of neighbors, web site for this person, etc) Query pairs. In order to test our algorithms, we selected a set of ten computer scientists and mathematicians, and a set of seven actors and actresses. We defined the query pairs shown in Table 3 for use in the experiments. We also Name Description #pairs CSM Both nodes are CS/mathematicians 21 Cross One node from each set 70
Table 3: Query pairs for experimental evaluation. considered three termination conditions for the candidate generator, C -small , C -medium ,and C -large ,whichresult in small, medium and large candidate graphs G cand .Their exact details are in the Appendix.
Figure 1 shows the graphs output by our system for three representative test cases. Figure 1(a )shows a small dis-play graph linking two movie actresses, Nicole Kidman and Cameron Diaz. In this example there are strong links, with high current, as expected.

Figure 1(b )shows a 10-node display graph for the con-nections from MIT Professor Nicholas Negroponte to IBM CEO Sam Palmisano. In this case the strongest connec-tion to Negroponte is through Esther Dyson, as evidenced by the fact that they have both published articles in Wired Magazine, have been mentioned together in the New York Times, and have authored books that are compared to each other in Amazon. The second node in the strongest path is Louis Gerstner, the IBM CEO that Palmisano replaced. The rest of the paths are weaker, involving the CEOs of most major computer companies: HP (Fiorina), Microsoft (Gates), Cisco (Chambers), and Dell (Dell). Notice that our goodness function led to results that make intuitive sense: the co-occurrences of Palmisano with the other CEOs is ex-pected, but not as strong and informative as the connection with Dyson.
 The final example 1(c )shows the network connecting Alan Turing to Sharon Stone, who are two people from largely disjoint communities. Without looking at the graph, one would expect to find weak paths, if any at all. The result is surprising: there is a connection, and there is even a fairly strong connection through the actress Kate Winslet, because she starred in a movie about the Engima cipher machine, in which Alan Turing played a part durign his lifetime. We also see a connection through Gillian Anderson because she stars in a science fiction television show that is popular among a technical audience. We note also that Alan Turing has di-rect connections to Alan Thicke, Alan Alda, and Bruce Lee (all of whom have direct connections to Sharon Stone), but these edges were discarded as carrying too little current. This example is also interesting in that they come from dis-tinct communities, and both have high degree (Turing has 1,249 neighbors and Stone has 6,014 neighbors). Again, our goodness function g ( )led to results that are sensible and revealing.
In this section, we evaluate the performance of the display generator by measuring the delivered current as a function of the budget b of allowable nodes.

Figure 4 shows the fraction of delivered current as a func-tion of the size of the display graph for four representative examples. The candidate generator for these examples was run using a stopping condition that resulting in 15 X 25K to-tal edges in the candidate graph. As the figure shows, the curve quickly flattens, and a reasonably-sized display graph delivers the vast majority of the total current. Figure 4: Fraction of amperage captured by display generator as a function of b (budget of allowed count of nodes). Notice the sharp rise, and the diminishing returns, in all cases.
 Table 4: Comparison of times and currents across three datasets and three stopping conditions. Values in parentheses represent percentage of total current delivered by display graph of size 20.

Table 4 shows aggregate results over a larger set of ex-periments and candidate generator stopping conditions. Ta-ble 4(b )shows total current delivered in the candidate graph, and then in parentheses the fraction of that current deliv-ered in a 20-node display graph. As stopping conditions change and the expansion algorithm is allowed to proceed further, the display generator captures a smaller fraction of the current. A more surprising bias arises from the choice of dataset. For Actors and Actresses, the display generator captures the vast majority of the flow, suggesting that while this neighborhood is extremely dense, nonetheless there are a few nodes that are responsible for most of the flow. Com-puter Scientists and Mathematicians fare slightly worse; for condition C -medium , 87% of the flow is captured on average, down from 99%. The Cross case is again slightly worse, val-idating the intuition that relationships between nodes that are not naturally related come about due to a larger num-ber of serendipitous low-flow paths. Nonetheless, overall, the display generator is able to capture the vast majority of current with a relatively small output graph.

Over all these experiments, the first b =20 nodes routinely carry most of the current. To conclude, the display genera-tor seems to be doing a good job of capturing flow from the much larger candidate graph.
In this section we summarize the results of experiments on different distance measures and stopping conditions for the candidate graph. Details are given in the appendix. The stoppingCondition() shows exactly the diminishing re-turns that we observed above; the heuristics for node expan-sion ( pickHeuristic() )usually perform about the same, with the surprising exception that one natural-sounding heuristic performs quite poorly X  X ee Section A.2 for details. Stopping Conditions. Table 4 shows the average runtime of the algorithm for each data set and each stopping con-dition. Most interestingly, more resources help for Actors &amp; Actresses, but not Computer Scientists &amp; Mathemati-cians or for relationships between the two groups ( X  X ross X ). Nodes in the AA region of the graph tend to have very high degrees, and may therefore require significantly more ex-pansion to find the good paths. For the other regions, it is possible to find the most important paths with signifi-cantly less computational effort. The timings given in the figure are for all three stages of the algorithm in aggregate. A deeper exploration of the timing details shows that can-didate generation typically requires more than 50% of the overall effort, with the remainder roughly split between volt-age computation and display generation. As a takeaway, it is quite feasible to find  X  X ood X  graphs in the most important CMS case (representing well-connected individuals without a massive media presence )over a 100M edge graph in un-der three seconds on a single machine without careful code tuning or optimization.
In this work, we defined and addressed the problem of  X  X onnection Graphs X , small graphs that convey much in-formation about the relationship of a pair of nodes. In addi-tion to posing the problem, additional contributions are the following:
Moreover, we implemented our algorithms in a working prototype, complete with an interactive web-based interface, on a real graph that we derived from the Web. The graph has 15 Million nodes and 96 Million edges.

Directions for future research include generalization to graphs that contain more than one type of entity (eg.,  X  X eo-ple X ,  X  X ompanies X  and  X  X roducts X ); and generalization to con-nection subgraphs between more than two nodes.
 Acknowledgements: We would like to thank Madhukar Korupolu for his help with the name graph dataset and his observations on the Display Generator, and Chris Palmer for his feedback on the voltage computation. [1] R. Albert, H. Jeong, and A.-L. Barabasi. Diameter of [2] U. Brandes, M. Gaertler, and D. Wagner.
 [3] A.K.Chandra,P.Raghavan,W.L.Ruzzo, [4] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [5] P. Domingos and M. Richardson. Mining the network [6] S. Dorogovtsev and J. Mendes. Evolution of networks. [7] P. Doyle and J. Snell. Random walks and electric [8] M. Faloutsos, P. Faloutsos, and C. Faloutsos. On [9] G. Flake, S. Lawrence, C. L. Giles, and F. Coetzee. [10] E. . Gansner and S. C. North. An open graph [11] D. Gibson, J. Kleinberg, and P. Raghavan. Inferring [12] M. Girvan and M. E. J. Newman. Community [13] M. Gr  X  otschel, C. L. Monma, and M. Stoer. Design of [14] D. Gruhl, L. Chavet, D. Gibson, J. Meyer, [15] T. H. Haveliwala. Topic-sensitive pagerank. Proc. [16] G. Jeh and J. Widom. Scaling personalized web [17] G. Karypis and V. Kumar. Parallel multilevel k-way [18] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing [19] D. Liben-Nowell and J. Kleinberg. The link prediction [20] M. E. J. Newman. The structure and function of [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The [22] C. R. Palmer and C. Faloutsos. Electricity based [23] S. van Dongen. Graph Clustering by Flow Simulation . [24] S. Virtanen. Clustering the chilean web. LA-WEB
We begin by describing the particular heuristics used in the candidate generator, after which we then report on some experiments comparing different parameter settings, and draw conclusions regarding the appropriate choice.
 A.1.1 pickHeuristic()
Recall that the pickHeuristic() function chooses the next node to expand during candidate generation. We do this within a framework based on a distance function on the in-process candidate graph. Among the pending nodes, we al-ways choose for expansion the one that is closest to its root, in some sense. There are several reasonable ways to define closeness. We introduce a (possibly asymmetric )length on edges, and define the distance between two nodes u and v as the minimum over all paths from u to v of the sum of the lengths of the edges along the path. Thus, the decision about what to expand next is encoded as a weighted, di-rected, graph distance. This formulation tends to focus on particular types of expansions, and does not allow others, but has the advantage that we know the exact shortest path from any expanded node to its root.

We considered eight definitions of the length of an edge from u to v , based on three flags that can each be set two ways. Generally, the distance is given by f ( n/d ), where the three flags control the values of f , n ,and d , as follows: Numerator: If the distance is degree-weighted then n = Denominator: If the distance is count-weighted then d = Multiplicative: If the distance is multiplicative then f ( x )=
Thus, the basic distance function is d ( u ) /C ( u,v ),and the degree-weighted, count-weighted, multiplicative distance function is log(deg 2 ( u ) /C ( u,v ) 2 ).

We give a brief intuition for these definitions. If the graph were not weighted then the basic distance function would be deg( u ). A more natural measure might be deg( u )+deg( v ), but we haven X  X  yet expanded v so we don X  X  have access to its degree; thus, we cannot include the deg( v )term in our definition, and we must instead employ an asymmetric dis-tance. The distance function as given treats lower-degree nodes as closer, so the expansion is designed to discover longer paths through low-degree nodes rather than shorter paths through high-degree nodes. Recall however that our graph is weighted, and that nodes with high weight edges should be considered close together because they have a rel-atively strong connection. This explains the presence of the term C ( u,v ), corresponding to the weight of the edge.
Finally, we motivate the notion of multiplicative distance rather than traditional additive distance. By taking the log-arithm of the edge weight and adding these values along a path, we compute the logarithm of the product; since the logarithm is monotonically increasing, comparisons of path lengths result as they would for multiplication of edge weights. Multiplication is a reasonable model here for the following reason. Consider a path in which all edges have weight 1. If the degrees of vertices along the path are d ,d 2 ,...,d k then the number of vertices reachable by ex-panding all paths of the given length in a tree with branch-ing factor d i at level i would be R = i d i . If the sink were uniformly located among all such nodes, the probabilty of reaching the sink would be proportional to R .Thus,in an idealized model, lower multiplicative distance represents nodes that are  X  X loser X  to the root in the sense that a se-quence of expansions with the given degree would reach a smaller set of vertices.
 Results for these various distance functions are shown in Section A.2.

Finally, we must discuss termination conditions. We de-fine three thresholds for termination; the algorithm will stop as soon as any threshold is exceeded. First, we adopt a threshold on total expansions, to limit the total number of disk accesses. Second, we adopt a larger threshold on dis-covered vertices, even if those vertices have not yet been expanded, to limit memory usage. And finally, we adopt a threshold on number of cut edges (edges between D ( s )and D ( t )), as a measure of the connectedness of the set of nodes with the src as a root with the set of nodes with the sink as aroot.

This completely characterizes the candidate generation al-gorithm.

As mentioned, we considered three termination condi-tions, C -small , C -medium ,and C -large , which result in w graph. small, medium and large candidate graphs G cand .Recall that the algorithm terminates when any of three thresh-olds is exceeded: the number of cut edges, the number of expanded vertices, and the number of discovered vertices. The termination conditions we considered are described in Table 6. Finally, we considered all eight distance measures; Table 6: Candidate Generation termination condi-tions the eight different measures are derived by turning on or off each of three different settings: degree-weighted, count-weighted, and multiplicative.

The number of cases in the overall experimental design is therefore 8  X  3  X  (45 + 21 + 70 )= 3264. For each case, we ran the candidate generator and measured wallclock time and number of edges in the resulting graph. We then ran the voltage computation on the candidate graph and measured wallclock time and total current. Finally, we ran the display generator and measured wallclock time and total captured current in the display graph.

Table 5 compares total current delivered across the eight different distance measures we employed.

First, we consider distance measures for candidate gener-ation, show in Table 5. We observe that as the algorithm is given more resources (ie, as the stopping condition changes), the best distance measure also changes. In fact, there are cases where normal or count-weighted, normal or degree-weighted, and additive or multiplicative distance measures are preferred. However, there are a few specific recommen-dations that we can make.

First, we consider our three data cases, which represent threecommontypesofqueries: CSM: Source and sink are connected, and live in a net-AA: Individuals are connected, and live in a very dense Cross: Source and sink have no  X  X atural X  connections. In
Overall, we observe that the simplest distance measure ( X  X ormal X  distance )never performs the best, and normal multiplicative distance performs well in all cases except for the somewhat unusual high-resource Cross condition in which count-weighting should be introduced.
