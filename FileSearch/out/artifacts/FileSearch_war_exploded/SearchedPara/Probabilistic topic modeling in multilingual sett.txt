  X  1. Introduction Probabilistic latent topic models such as probabilistic Latent Semantic Analysis (pLSA) ( Hofmann, 1999b, 1999a ) and models for representing the content of documents in large document collections. They provide a robust and unsupervised framework for performing shallow latent semantic analysis of themes (or topics) discussed in text. The families of these how words in documents have been generated. Fitting such a generative model actually denotes finding the best set of those latent variables in order to explain the observed data. With respect to that generative process, documents are seen as mix-document constitutes a high-level language-independent view of its content, unhindered by a specific word choice, and it
Probabilistic topic modeling constitutes a very general framework for unsupervised topic mining, and over the years it has been employed in miscellaneous tasks in a wide variety of research domains, e.g., for object recognition in computer and others.

Being originally proposed for textual data, probabilistic topic models have also organically found many applications in distributions of topics over documents (further per-document topic distributions ) can be directly employed to detect main themes 1 discussed in texts, and to provide gists or summaries for large text collections (see, e.g., Hofmann, 1999b; Blei observed as a low-dimensional latent semantic representation of text in a new topic-document space, potentially better than the original word-based representation in some applications. In an analogous manner, since the number of topics is usually much lower than the number of documents in a collection, per-topic word distributions also model a sort of dimensionality reduction, as the original word-document space is transferred to a lower-dimensional word-topic space. Apart from the 2010; Dinu &amp; Lapata, 2010 ), etc. Lu, Mei, and Zhai, 2011 examine task performance of pLSA and LDA as representative per-topic word distributions and per-document topic distributions.

However, all these models have been designed to work with monolingual data, and they have been applied in monolin-gual contexts only. Following the ongoing growth of the World Wide Web and its omnipresence in today X  X  increasingly con-nected world, users tend to abandon English as the lingua franca of the global network, since more and more content becomes available in their native languages or even dialects and different community languages (e.g., the idiomatic usage to determine the exact number of languages in the world, but the estimations vary between 6000 and 7000 languages and almost 40,000 unofficial languages and dialects. 2 It is extremely time-consuming and labor-intensive to build quality language-independent unsupervised corpus-based cross-lingual text mining from non-parallel corpora without any additional translation resources. High-quality parallel corpora where documents are sentence-aligned exact translations of each other
With the rapid development of Wikipedia and online social networks such as Facebook or Twitter, users have generated a huge volume of multilingual text resources. The user-generated data are often noisy and unstructured, and seldom well-paired across languages. However, unlike parallel corpora, such comparable corpora , where texts in one language are paired with texts in another language discussing the same themes or subjects, are abundant in various online sources (e.g.,
Wikipedia or news sites). Documents from comparable corpora do not necessarily share all their themes with their counter-same event contain a significant thematic overlap. We could say that such documents in different languages, although inher-ently non-parallel, are theme-aligned .

Multilingual probabilistic topic models (MuPTM-s) have recently emerged as a group of unsupervised, language-independent generative machine learning models that can be efficiently utilized on such large-volume non-parallel theme-aligned ure and the power of inference on unseen documents, these models have found many interesting applications. The knowledge Leveling, &amp; Jones, 2012 ) and others.

The main goal of this work is to provide an overview of the recently developed multilingual probabilistic topic modeling concept. It aims to model topic discovery from multilingual data in a conceptually sound way, taking into account thematic alignment between documents in document collections given in different languages. We have decided to provide a thorough analysis of the framework of multilingual probabilistic topic modeling because we feel that the current relevant literature lacks a systematic and complete overview of the subject. Moreover, during our tutorials at ECIR 2013 and WSDM 2014 on the subject we realized even more that, after being provided with feedback on our tutorials, it would be extremely ben-eficial for the IR community to have an extended written overview of the whole subject, along with its formalisms, defini-tions and modeling perspectives (both conceptual and mathematical), relevant state-of-the-art, a broad relevant references list, and also with standard evaluation procedures, an overview of applications, and suggestions for future work.
As a representativeexample, we choosebilingualLDA, whichhas been designedas abasic andnaturalextension of the stan-knowledge that is presupposed before training (e.g., document alignment, prior word matchings or bilingual dictionaries), per-documenttopic distributions. Finally,we also demonstrate howto utilizethe high-levelstructured text representations by means of per-topic word distributions and per-document topic distributions from any multilingual probabilistic topic model, andestablishknowledgetransferacrossdifferentlanguagesviathesharedspaceoflatentlanguage-independentconcepts,that the results obtained by BiLDA, but the presented solutions are completely topic model-independent.
The results reported across all these tasks show the validity of multilingual comparable data as training data, as well as the superiority of MuPTM over monolingual probabilistic topic modeling (MoPTM) and other data-driven modeling para-digms which do not rely on any expensive translation resources. We also expose and discuss an issue present in all current dependent number of topics in advance. We also report on a mismatch between the standard intrinsic evaluation measure of perplexity and the extrinsic evaluation in terms of final scores in the cross-lingual tasks. 2. Multilingual probabilistic topic modeling
This section presents and defines the basic concepts and modeling assumptions related to multilingual probabilistic topic modeling, with a special focus on learning from comparable theme-aligned corpora. We also draw an analogy to the broader training procedure for the model, the output of the model in terms of per-topic word distributions and per-document topic topic models. At the very end of the section, alternative multilingual topic models are also presented. 2.1. Definitions and assumptions l  X jLj languages, where L X f L 1 ; L 2 ; ... ; L l g is the set of languages, is a set of corresponding text collections fC
Each C i  X f d i 1 ; d i 2 ; ... ; d i dn fC ; C 2 ; ... ; C l g are said to be theme-aligned if they discuss at least a portion of similar themes. Here, dn number of documents in document collection C i , while wn document in document collection C i , and w i j denotes the j -th word in vocabulary V corpus C is a set of semantically coherent multinomial distributions of words with values P vocabulary V 1 ; ... ; V i ; ... ; V l associated with text collections C
P w i j j z k is calculated for each w i j 2 V i . The probabilities P and they constitute a language-specific representation (e.g., a probability value is assigned only for words from V guage-independent latent cross-lingual concept  X  topic z k cross-lingual topics from the set Z . This mixture for some document d topic z k has some probability to be found in a particular document (modeled by per-document topic distributions), and each such topic has a language-specific representation in each language (modeled by language-specific per-topic word distributions).

We can interpret Definition 2 in the following way: each cross-lingual topic from the set Z can be observed as a latent language-independent concept present in the multilingual corpus, but each language in the corpus uses only words from
P 2 V i P i w i j j z k  X  1, for each vocabulary V
Zhang, Mei, and Zhai (2010) provide an alternative, more general definition of a multilingual topic model, but we will show that their definition may be brought down to Definition 2 after a partition over the languages is performed. Namely, lingual topic z k 2Z as a semantically coherent multinomial distribution over all the words in all the vocabularies of lan-guages L 1 ; ... ; L i ; ... ; L l , and Pw j j z k gives the probability of any word w
In this case, we have P l i  X  1 P w obtained by retaining only probabilities for words which are present in its own vocabulary V butions. For a word w i j 2 V i , we have P i w i j j z k more present in the original corpus C might dominate the multinomial per-topic word distributions. By performing the partition and normalization over the languages, that imbalance is effectively removed.
 probabilistic topic modeling or latent cross-lingual topic extraction is to learn and extract a set Z of K latent language-independent concepts, that is, latent cross-lingual topics Z X f z document in the corpus, and discovering language-specific representations of these topics given by per-topic word distributions in each language (see Definition 2 ).

This shared and language-independent set of latent cross-lingual topics Z serves as the core of unsupervised cross-lingual text mining and cross-lingual knowledge linking and transfer by means of multilingual probabilistic topic models. It is the cross-lingual connection that bridges the gap across documents in different languages and transfers knowledge across lan-topic model may be further inferred on unseen documents.

Definition 4 ( Inference of a multilingual probabilistic topic model ). Given an unseen document collection C multilingual topic model on the collection C u denotes learning topical representations of the unseen documents d is, acquiring per-document topic distributions for the new documents based on the previous output of the model. acterized by the fact that corpora are present in more than one language.

Additionally, following the assumptions and general definitions provided in this section, monolingual probabilistic topic of multilingual probabilistic topic models where only one language is involved, and all the definitions and assumptions remain the same (see later the discussion in Section 2.3 ). 2.2. A more general framework: latent cross-lingual concepts (intermezzo)
The latent cross-lingual topics presented in Section 2.1 constitute only one possibility when the aim is to detect and the data. Latent cross-lingual concepts may be interpreted as language-independent semantic concepts present in a multi-razigravac  X  (playmaker), doigravanje (playoff), ... } in Croatian.

These K semantic concepts span a latent cross-lingual semantic space. Each word w may be represented in that latent semantic space as a K -dimensional vector, where each vector component is a conditional concept probability score
Pz tinomial probability distribution, a mixture over the same induced latent cross-lingual semantic concepts Pz
The description in this article relies on the multilingual probabilistic topic modeling framework, but we emphasize that all the work described in this article is independent of the actual method used to induce the latent cross-lingual concepts. The reader has to be aware of the fact that the description of how to utilize this latent knowledge in appli-cations is generic and model-independent as they allow the usage of all other models that compute probability scores
P  X  z k j w  X  and P  X  z k j d  X  (obtained from per-topic word distributions and per-document topic distributions). Besides
MuPTM, a number of other models may be employed to induce the latent cross-lingual concepts. For instance, one could document pairs. Other more recent models include matching canonical correlation analysis ( Haghighi, Liang, Berg-
Eguchi, &amp; Xing, 2012 ). 2.3. A representative example: Bilingual Latent Dirichlet Allocation (BiLDA) and present cross-lingual tasks in the bilingual setting . For bilingual corpora we introduce the source language L may be easily generalized to a setting where more than two languages are available. 2.3.1. Main modeling assumptions lored for modeling parallel or, even more importantly, comparable theme-aligned bilingual document collections. An example of such a document collection is Wikipedia in 2 languages with paired articles. BiLDA has been independently 2010 ). Unlike LDA, where each document is assumed to possess its own document-specific distribution over topics, the generative process for BiLDA assumes that each aligned document pair shares the same distribution of topics. Therefore, the model assumes that we already possess document alignments in a corpus, that is, links between paired documents dia data, where document alignment is established via cross-lingual links between articles written in different languages.
These links are provided by the nature of the Wikipedia structure. Cross-lingual document alignment for news crawled from the Web is also a well-studied problem. Since the establishing of cross-lingual links between similar documents 2006; Vu, Aw, &amp; Zhang, 2009 ).

Definition 6 ( Paired bilingual corpus ). A paired bilingual document corpus is defined as C X f d f X  d 1 ; d bilingual corpus a set of K latent cross-lingual topics Z , each of which defines an associated set of words in both L 2.3.2. The model BiLDA can be observed as a three-level Bayesian network that models document pairs using a latent layer of shared topics.
Fig. 2 shows the graphical representation of the BiLDA model in plate notation, while Algorithm 1 presents its generative story.

BiLDA takes advantage of the assumed topical alignment at the level of linked documents by introducing a single variable pair d j . For each document pair d j , a per-document topic distribution parameters a 1 ; ... ; a K . Then, with respect to h j , a cross-lingual topic z S document of the current document pair d j is then generated from a multinomial distribution / uments in a document pair need not be sampled from the same latent cross-lingual topic. The only constraint imposed by the model is that the overall distributions of topics over documents in a document pair modeled by h validity of this assumption/constraint is dependent on the actual degree of thematic alignment of two coupled documents, as well as on the chosen topic granularity (e.g., two Wikipedia articles about the same subject may have the same focus and topics). 2.3.3. Hyper-parameters
According to Griffiths et al. (2007) , each hyper-parameter a number of times topic z k is sampled in a document (or document pair) before having observed any actual words. If one is in possession of a certain prior or external knowledge (e.g., document metadata, main themes of a document collections) about the topic importance and the likelihood of its presence in the data, introducing asymmetric priors gives more prefer-prior knowledge about themes in a text collection, and then it is reasonable to assume that all topics are a priori equally likely. Therefore, it is convenient to use a symmetric Dirichlet distribution with a single hyper-parameter a such that  X  X  a K  X  a . Similarly, a symmetric Dirichlet prior is placed on / and w with a single hyper-parameter b . b may be interpreted as a prior observation count of the number of times words in each language are sampled from a topic before in smoothed per-topic word and per-document topic distributions, where the values for a and b determine the degree of smoothing. The influence of these hyper-parameters on the quality of learned latent topics is a well-studied problem in settings. 2.3.4. Extending BiLDA to more languages
A natural extension of BiLDA that operates with more than two languages, called polylingual topic model (PolyLDA) has been presented by Mimno et al. (2009) . A similar model has also been proposed by Ni et al. (2009, 2011) . Instead of docu-ment pairs, they deal with aligned document tuples (where links between documents in a tuple are given), but the assump-tions made by their model remain the same. Fig. 3 shows the graphical representation in plate notation of the BiLDA model generalized to l languages, l P 2, with document tuples d word distributions f / 1 ; ... ; / l g (see Section 2.1 ).
On the other hand, when operating with only one language, BiLDA or (more generally) PolyLDA is effectively reduced to lingual LDA model is only a degenerate special case of BiLDA and PolyLDA (see also Section 2.1 ). 2.3.5. Training: estimating the BiLDA model found by the training procedure. In simple words, we need to detect and learn which words are important for a particular
BiLDA is complex and cannot be solved by an exact learning procedure. There exist a few approximative training techniques which aim at converging to the correct distributions. Variational estimation for the monolingual LDA was used as the esti-mation technique in the seminal paper by Blei et al. (2003b) . Other estimation techniques for the monolingual case include Griffiths &amp; Steyvers, 2004 ).

An extension of the variational method to multilingual settings and its complete formulation for BiLDA was proposed and we opt for Gibbs sampling as the estimation technique for the BiLDA models in all applications described in this article. Therefore, we here provide an overview of Gibbs sampling for BiLDA.

Gibbs sampling is a Monte Carlo Markov chain (MCMC) estimation technique. MCMC is a random walk over a Markov state is repeatedly sampled randomly from the transition probabilities, and this is repeated until the equilibrium state is reached, in which case states are samples from the joint probability distribution. Gibbs sampling considers each word token in a text collection in turn and then samples a topic for that word token, where the probability of generating the current word by each topic is calculated conditioned given all other variables (including all other topics). For BiLDA in specific, the Gibbs sampling procedure follows the steps presented in Algorithm 2 .

After the convergence or the equilibrium state is reached, a standard practice is to provide estimates of the output dis-tributions as averages over several samples taken in the equilibrium state.
 assigned to a word position i that generated word w S ji in a document pair d each document pair d j and each word position i , the probability is calculated that z S possible topic indices (from a set of K topics), as indicated by variable z
In this formula, z T j refers to all target topic indices for document pair d formed in an analogous manner: pletely analogous manner. Starting from Eq. (1) , we can further write:
Both h and / have a prior Dirichlet distribution and their posterior distributions are updated with the counter variable n (which counts the number of assigned topics in a document) and the counter variable assigned topics in the corpus) respectively (see the explanations of the symbols after the derivation). The expected values
R xf  X  x  X  dx for h and / become: The counter variable n S j ; k denotes the number of times source words in the source document d are assigned to a latent cross-lingual topic z k (with index k ), while n S in the subscript of a counter variable, this means that the counts range over all values of the variable whose index all K topics in d S j .

The second counter variable, v S k ; w denotes all topic assignments for the source side of d j but excluding w S source language words from V S associated with the topic z guage words (a  X  X   X  X  appears instead of the w S ji ). Again, because of the : symbol in the superscript, the current w S (i.e., the count is then v S k ; 1). Finally, j V S j and j V respectively.

As can be seen from the first term of Eqs. (4) and (5) , the document pairs are linked by the counter variables n S both sets of assignments: z S ji and z T ji are drawn from the same h currently being considered. 2.3.6. Output: per-document topic and per-topic word distributions distributions are estimated as averages over these samples.

Language-independent per-document topic distributions provide distributions of latent cross-lingual topics for each docu-mula for per-document topic distributions for documents in an aligned document pair using Eqs. (4) and (5) : where the k -th dimension of the vector is exactly the probability of the latent cross-lingual topic z
We may detect from Eq. (6) that two documents from an aligned document pair are enforced to have exactly the same top-ical representations, that is, the two documents discussing the same themes will be presented as exactly the same mixtures over the induced latent cross-lingual topics. This property is achieved by making the computation of P  X  z uments in the pair explicitly dependent on the topic assignments counts from the source language document  X  n S tion 2.5 ) typically trained monolingual LDA on concatenated documents from an aligned document pair, where this property ficially created concatenated document, and does not enforce this property at all. Comparisons of monolingual LDA trained on concatenated documents forming aligned document pairs (further MixLDA ) and bilingual LDA reveal the superiority of the true multilingual approach modeled by BiLDA.

Language-specific per-topic word distributions measure the importance of each word in each language for a particular latent cross-lingual topic z k . Given the source language with vocabulary V and following Eq. (4) , a probability that some word w S i The same formula, but now derived from Eq. (5) is used for the per-topic word distributions ( w ) for the target language:
In summary, these per-document topic distributions and per-topic word distributions are in fact mathematical realizations concepts, we refer the reader to study that figure again. 2.3.7. Inference or  X  X  X hat with New Documents? X  X 
Since the model possesses a fully generative semantics, it is possible to train the model on one multilingual corpus (e.g., multilingual Wikipedia) and then infer it on some other, previously unseen corpus. Inferring a model on a new corpus means calculating per-document topic distributions for all the unseen documents in the unseen corpus based on the output of the trained model (i.e., we effectively learn the MuPTM-based representation of an unseen document, see Definition 4 and Eq. (7) ). Inference on the unseen documents is performed only one language at a time, e.g., if we train on English-Dutch Wiki-pedia, we can use the trained BiLDA model to learn document representations, that is, per-document topic distributions for Dutch news stories, and then separately for English news.

In short, we again randomly sample and then iteratively update topic assignments for each word position in an unseen document, but now start from the fixed v counters learned in training, and then cyclically update the probability distribu-tions from which the topic assignments are sampled. Since the inference is performed monolingually, dependencies on the topic assignments from another language are removed from the updating formulas. Hence, similar to Eq. (4) , the updating formula for the source language L S is:
Learning a multilingual topic model on one multilingual corpus and then inferring that model on previously unseen data property is extensively utilized in various cross-lingual applications. 2.4. Evaluation of multilingual probabilistic topic models
A simple way of looking at the output quality of a topic model is by simply inspecting top words associated with a 2010; Mimno, Wallach, Talley, Leenders, &amp; McCallum, 2011; Stevens, Kegelmeyer, Andrzejewski, &amp; Buttler, 2012; of cross-lingual topics and their alignment across languages when observing the actual words constituting a topic. These words provide a shallow qualitative representation of the latent topic space, and could be seen as direct and comprehen-sive word-based summaries of a large document collection. In other words, humans can get the first clue  X  X  X hat all this text is about in the first place X  X .

The desirable property of semantic coherence comprises both a strong intra-semantic coherence , that is, words from the
Samples of cross-lingual topics extracted by BiLDA trained on aligned Wikipedia articles are provided in Table 1 . We may human judges.

Besides this shallow qualitative analysis relying on the top words, there are other, theoretically well-founded evaluation by their perplexity, where the perplexity or  X  X  X onfusion X  X  of a model is a measure of its ability to explain a collection C unseen documents. The perplexity of a probabilistic topic model is expressed as follows: be well adapted to new documents and yield a good representation of those previously unseen documents. Since the per-plexity measure defines the quality of a topic model independently of any application, it is considered an intrinsic or in vitro evaluation metric.
 posed recently in ( Zhang et al., 2010 ), but that measure also presupposes an existing bilingual dictionary as a critical resource. Additionally, a number of intrinsic quantitative evaluation methods (but for the monolingual settings) are pro-found in the literature.

Finally, the best way to evaluate multilingual probabilistic topic models is to test how well they perform in practice for tion. We later investigate whether there exists a mismatch between the intrinsic and extrinsic evaluation in information retrieval (see Section 6 ).
 2.5. A short overview of other multilingual probabilistic topic models
Similarly to LDA in the monolingual setting (for which we have already shown that it is only a special case of BiLDA oper-topic modeling. Although MuPTM is a quite novel concept, several other models have emerged over the last years. All current state-of-the-art multilingual probabilistic topic models build upon the idea of standard monolingual pLSA and LDA and clo-sely resemble the described BiLDA model, but they differ in the assumptions they make in their generative processes, and in knowledge that is presupposed before training (e.g., document alignments, prior word matchings or bilingual dictionaries). lingual topics that has to be discovered in a multilingual text collection .
 ture in cross-lingual information retrieval. Artificial  X  X  X ross-lingual X  X  documents were formed by concatenating aligned parallel documents in two different languages, and then LSA on a word-by-document matrix of these newly built documents was used to learn the lower dimensional document representation. Documents across languages are then compared in that lower-dimensional space.

Another line of work Zhao and Xing (2006, 2007) focused on building topic models suitable for word alignment and sta-tistical machine translation operations. Again inspired by monolingual LDA, they have designed several variants of topic models that operate on parallel corpora aligned at sentence level. The topical structure at the level of aligned sentences or word pairs is used to re-estimate word translation probabilities and force alignments of words and phrases generated by the same topic.

However, the growth of the global network and increasing amounts of comparable theme-aligned texts have formed a need for constructing more generic models that are applicable to such large-volume, but less-structured text collections.
Standard monolingual probabilistic topic models coming from the families of pLSA and LDA cannot capture and accurately seldom co-occur in a monolingual text, and therefore these models are unable to group such pairs of words into a single issue, there have been some efforts that trained monolingual probabilistic topic models on concatenated document pairs in approaches also fail to build a shared latent cross-lingual topical space where the boundary between the topic representa-tions with words in two languages is firmly established. In other words, when training on concatenated English and Spanish Wikipedia articles, the learned topics contain both English and Spanish words. However, we would like to learn latent tation in Spanish.

Recently, several novel models have been proposed that remove such deficiency. These models are trained on the individual documents in different languages and their output are joint latent cross-lingual topics in an aligned latent Sections 3 X 6 ). These models require alignments at document level a priori before training, which is easily obtained for
Wikipedia or news articles. These document alignments provide hard links between topic-aligned semantically similar documents across languages.

Recently, there has been a growing interest in multilingual topic modeling from unaligned text, again inspired by mono-of pairs of words that link words from the source vocabulary to words from the target vocabulary. These matchings are ties together words with similar meanings across languages, where similarity is based on different features. Matchings are induced based on pointwise mutual information (PMI) from parallel texts, machine-readable dictionaries and orthographic and their evaluation has been performed on a parallel corpus. A similar idea of using matchings has been investigated in matchings (or word concepts , as they name them), where the matchings are acquired directly from a machine-readable bilin-gual dictionary. JointLDA uses Gibbs sampling for training and it is trained on Wikipedia data. Although these two models claim that they have removed the need for document alignment and are fit to mine latent cross-lingual topics from una-dictionaries have to be compiled from parallel data or hand-crafted, which is typically more expensive and time-consuming than obtaining alignments for Wikipedia or news data.
 Another work that aims to extract latent cross-lingual topics from unaligned datasets is presented by Zhang et al. (2010) .
Their Probabilistic Cross-lingual Latent Semantic Analysis (PCLSA) extends the standard pLSA model ( Hofmann, 1999b )by based constraints are the key to bridge the gap between languages by pushing related words in different vocabularies to lingual setting is also reflected between their multilingual extensions, PCLSA and BiLDA.

In this article, we will present a subset of cross-lingual applications in which any multilingual probabilistic topic model may be utilized. In specific, we show the results obtained by BiLDA and provide an overview of its task performance. The goal of this article is however not to provide a direct comparison of different multilingual probabilistic topic models in various cross-lingual tasks, but to provide a comprehensive and didactic description of a general model-independent framework for building systems that rely on such multilingual probabilistic topic models and MuPTM-based representa-tions of words and documents, and do not exploit any external expensive knowledge resource (e.g., parallel corpora, machine-readable dictionaries, extensive human annotations). Such data-driven unsupervised systems which exploit only internal evidence are essential for languages and language pairs with limited resources. We acknowledge that there exist numerous different techniques proposed for solving the presented tasks. However, our main focus is not to detect the best technique for each cross-lingual task, but to give a  X  X  X ookbook X  X  on how to exploit the latent cross-lingual topical knowl-edge as one source of evidence when dealing with these tasks in an unsupervised, language-independent and language pair independent manner.
 and Application IV (Section 6 ) contain already published work ( DeSmet &amp; Moens, 2009; De Smet et al., 2011; Vulic  X  ,
DeSmet, &amp; Moens, 2013 ), but we have decided to retain the essence and have rewritten the previously published work in a systematic and didactic manner in order to better stress the general applicability of text representations by means of latent cross-lingual topics in a variety of cross-lingual tasks, and to provide some new insights from observing all the applications together. Moreover, a significant portion of Application III (Section 5 ) is novel and previously unpublished. 3. Application I: Cross-lingual event-centered news clustering 3.1. Task description
The first task we have chosen to present is cross-lingual event-centered news clustering . In general, event-centered news clustering may be considered an information retrieval task in which it is necessary to group news stories into coherent clusters, where each item (i.e., each news story) in one cluster should report on the same event. A special case is cross-lingual event-centered news clustering where one has to perform the clustering of news stories now written in differ-ent languages into groups of stories that describe the same event. Implicitly, that also defines a method for linking news stories across languages. Due to the dynamic and ever-changing nature of news, one needs an unsupervised tool that can coherently capture such dynamics and provide a structured representation of news stories irrespective to their actual language. Such event-centered cross-lingual clustering of related news stories is highly desirable in systems for browsing, categorizing and summarizing large news archives given in multiple different languages ( Chen et al., 2000; Pouliquen, Steinberger, Ignat, K X sper, &amp; Temnikova, 2004; Evans, Klavans, &amp; McKeown, 2004; Kabadjov, Atkinson, Steinberger,
Steinberger, &amp; der Goot, 2010 ). Cross-lingual event-centered news clustering may be observed as a special case of the cross-lingual document clustering task (e.g., Montalvo, Mart X nez-Unanue, Casillas, &amp; Fresno, 2006; Wu &amp; Lu, 2007;
Tang, Xia, Zhang, Li, &amp; Zheng, 2011 ), with an extra constraint which specifies that documents  X  news stories should be clustered together if and only if they cover the same event.

An event is defined as a well-specified happening at a certain moment in time (e.g., a single day or a short period) and in different languages, cross-lingual event-related clustering of these stories is required.

An event can be observed as a mixture of different themes, where some themes are dominant, while others are only mar-ginally present. That phenomenon can be captured by probabilistic topic models  X  per-document topic distributions will be higher for topics closely related to the themes prominent in a news story. Two news stories s and are most likely discussing the same event if their per-document topic distributions are similar, that is, if the values
P  X  z by utilizing the language-independent set Z and per-document topic distributions as the news story representation, we are ture or document translation, or on the knowledge of shared or cognate named entities ( Montalvo et al., 2007 ). As already proven by DeSmet and Moens (2009) , here we stress that the cross-lingual topical knowledge and the representations by means of per-document topic distributions also prove beneficial for this task. 3.2. Methodology
Additionally, news stories reporting on same events usually involve many shared named entities. We can therefore rep-captured by our cross-lingual topics and (2) shared named entities. news story is the probability of the entity obtained by smoothed maximum likelihood estimation. Here, since we develop a vectors over shared and cognate named entities.

News stories are now represented by two probability distributions: (1) a probability distribution over cross-lingual topics (a per-document topic distribution) and (2) a probability distribution over shared named entities. In order to cluster the news stories based on the event they discuss, we need to choose a dissimilarity function. We use the symmetric representations, and some other constant when dealing with representations by means of shared named entities) x defined as: the topical aspect and the shared named entities aspect) obtained by Eq. (12) are combined by the maximum function, which events and locations are detected, we assume that we deal with different events. Vice versa, events that cover different themes (represented by different cross-lingual topics) that happen at the same location or performed by the same actors are also treated as different events. The final dissimilarity function is as follows: dis  X  s j A j denotes the number of aspects a news story is split into (i.e., j A j X  2 in this case), and A of story s i . We have also tried splitting news stories into more fine-grained named entity representations based on their semantic class ( A &gt; 2, person X  X ocation X  X rganization), but it has not improved the clustering performance. clustering with complete linkage ( Voorhees, 1986 ). This algorithm does not require the number of clusters to be chosen in advance. Its adapting ability is a very important property in the dynamic news environment. The algorithm iteratively merges clusters until a certain criterion is reached. To create a natural, unsupervised stopping criterion, a fitness-condition on the clustering is used. The consequence is that the data dictates the optimal number of clusters. For each story s i in the corpus, its fitness in cluster CL i is calculated as the normalized difference between the distance of s second best cluster CL j , and the average distance of s i g  X  s set f  X  s i  X  X  0. We search for the clustering that maximizes the average of f over all stories, over all possible stops in the hierarchy. 3.3. Experimental setup 3.3.1. Datasets ian, 18,911 English X  X rench and 13,696 English X  X panish articles. language, the statistics of the datasets in terms of the number of tokens and vocabulary words vary over different language mentioned.

For the event-centered news clustering, we train on the English X  X utch Wikipedia articles. The Dutch articles are usually iments. We have also trained standard LDA with K  X  100 in the monolingual context for both languages to measure the dif-ference between the monolingual and multilingual environment. In another experiment, we have trained BiLDA with K  X  20, 50, 100, 200, 300 topics and compare with LDA trained on concatenated documents (MixLDA) trained on the same data with the same number of topics K .

Following VanGael and Zhu (2007) , we have created our test set by compiling 18 events from Google news, forming 18 clusters of English and Dutch documents. In these 18 clusters, there are in total 50 English and 60 Dutch documents. A mul-transfer via the latent space of cross-lingual topics. 3.3.2. Evaluation metrics
Evaluation is done using the B-Cubed metric ( Bagga &amp; Baldwin, 1998 ). Let CL and G i its correct cluster from the ground truth. The B-Cubed metric then calculates Precision  X  lated one, but recall is very low in case of singleton clustering. Therefore, we present our results in terms of the F 3.4. Results and discussion
Table 2 shows the results of clustering the news stories according to the event discussed in the story. In the monolin-gual setting, the event-centered news clustering is quite accurate, and we can observe two interesting phenomena: (i) results for English are significantly higher than results for Dutch and (ii) there is a huge difference in favor of English between the results that rely on per-document topic distributions only. Wikipedia articles in Dutch are in general shorter and contain much less content, which disturbs the correct topic estimation, and effectively leads to learning per-document topic distributions of a lesser quality for Dutch news stories. Moreover, we can see that representations relying on named entities score better for Dutch than for English. That difference is attributed to the fact that Dutch news stories are on average more than 10 times shorter and contain less variation in wording.

As expected, when transferring the problem to the multilingual setting, results decrease, but it has to be noted that the lish, and then the cosine metric to measure distances between the news stories. The F topical representation outscores the representation which relies on shared named entities which was previously used in ( Montalvo et al., 2007 ).

In another experiment, we have varied the number of topics K which also influences the dimension of the representation of the test news articles. We have made a comparison of the BiLDA-based topical representation against the representation obtained by training the standard monolingual LDA model on concatenated documents from aligned document pairs and then inferring this model on test news articles ( MixLDA ). The results and the comparison are provided in Table 3 .We may observe that after a sufficient granularity of representation is used (given by the number of topics/dimensions K ), the clustering results are comparable over different K -s (from 50 to 300). On the other hand, the 20-dimensional represen-documents in aligned document pairs yields better topical representations than concatenating these documents into one artificial document as with MixLDA. By training on separate documents and imposing the constraint that two documents discussing the same themes need to have the same topical representations by means of per-document topic distributions, we are able to effectively able to remove an imbalance which occurs when two documents are mixed together into a single large document. This imbalance might occur because a document in one language may be much longer and more informative
The main strength of BiLDA lies in enforcing the topics of the two coupled documents to be sampled from the same distri-bution h (see Section 2.3 again).

In summary, the application of multilingual probabilistic topic models to cross-lingual event-centered news clustering pseudo-multilingual probabilistic modeling (as with MixLDA). These small experiments have shown (i) the validity of top-ical knowledge in cross-lingual event-centered news clustering and (ii) the better performance of an BiLDA-based cluster-ing model over the clustering model which relies on topical representations obtained by standard LDA trained on artificially created concatenated documents. The work on cross-lingual event-centered news clustering follows the previ-ous work in the monolingual event-centered news clustering ( DeSmet &amp; Moens, 2013 ), where it was already demonstrated that the topical representation of news stories by means of per-document topic distributions leads to improved clustering scores. Here, we have briefly described a similar cross-lingual framework where similarities between texts/news stories written in different languages can also be obtained by measuring similarities of cross-lingual topics X  distributions over those texts.

The goal of this section was to give the reader the first insight on how to exploit language-independent representations of documents by means of latent cross-lingual topics. This cross-lingual news clustering framework provides ample room for future work and building more advanced cross-lingual news clustering models. For instance, one might try to build event-centered clusters separately for each language, and then design a method to align these monolingual clusters and merge them into larger cross-lingual clusters discussing the same events. As another line of future work it is also worth investigating whether the topical knowledge might prove beneficial when combined with models that rely on readily avail-able bilingual dictionaries or machine translation tools.
 4. Application II: Cross-lingual document classification 4.1. Task description
Another task where representations of documents written in different languages as mixtures of cross-lingual topics (as document classification starts from a set of labeled documents in the source language L and then apply it to the classification of documents in the target language. The task obviously cannot be achieved by a method that only uses words from the labeled documents as features, since there is a minimal or no word overlap between the two languages. Hence, we have to find another solution. Here, we again deal with cross-lingual knowledge transfer, where the knowledge that is transferred across languages are text categories , that is, high-level labels that describe the content of a text.

Unlike in the previous application, where the similarity between two news stories or (more generally) documents has been established directly according to their respective per-document topic distributions, here we can observe each document as a data instance and use the probabilities P  X  z features. Again, by having the language independent set Z of K cross-lingual topics, we can operate in the same shared cross-lingual feature space regardless of the actual languages in which documents were written. 4.2. Methodology
A multilingual probabilistic topic model is first learned on a general multilingual corpus (e.g., Wikipedia). Then, given a cross-lingual document classification task, that is, a labeled document collection L in L S and U T . The methodology was proposed and described by De Smet et al. (2011) .

Each document from L S and U T is then taken as a data instance in the classification model, where its features are the inferred per-document topic distributions. The exact value of each classification feature of an instance, e.g., of document d is exactly the probability P  X  z k j d S i  X  , for all k  X  1 ; ... ; K . The same is valid for some target document d in both languages are represented by the language-independent features (the distributions of cross-lingual topics over the documents), supervision in only one language is needed and labels from the documents in the source language are then form cross-lingual knowledge transfer. For the classification model, one can choose any existing classifier such as Naive
Bayes, Perceptron, Maximum Entropy or Support Vector Machines (SVM). This choice is beyond the research reported in this per-document topic distributions as dimensions. 9 4.3. Experimental setup 4.3.1. Datasets
Training has been conducted on English X  X talian, English X  X rench and English X  X panish Wikipedia (see Section 3.3 ). English is considered the source language L S in all classification experiments.
 articles annotated with the category label were extracted. obtained as an average over classification results with each model. 4.3.2. Evaluation metrics
Again, we calculate the precision and recall scores and then combine them into the balanced F number of correctly labeled documents divided by the total number of documents labeled that way, and recall is the number of correctly labeled documents divided by the actual number of documents with that label as found in the ground truth. 4.4. Results and discussion
Table 4 displays the performance of the models in terms of their average F each language pair. Average perplexity scores after inferring the models on classification datasets are also presented in
Table 4 . We observe that the results vary over language pairs and over categories for each language pair. With respect to the fact that the approach is completely unsupervised and without any additional resource (e.g., a machine translation sys-exposes the mismatch between those scores and the actual classification results across categories for different languages. categories for English X  X talian and English X  X panish). 4.4.1. A comparison with a MixLDA-based classification model
As already mentioned in Section 3 , instead of utilizing a multilingual topic model, a common approach when dealing with cross-lingual tasks is to combine each bilingual pair of documents from training into a single document, mixing the words
LDA). The monolingual topic model may then again inferred on the test dataset. The two languages again share only one common topic space, and the learned per-document topic distributions are again used as the features for SVM to learn the classification model. In order to test the utility of MuPTM, we have compared the classification results of our BiLDA-based classification models with the classification results which rely on LDA trained on concatenated document pairs. Based in results between BiLDA and MixLDA have already been discussed in Section 3.4 . 4.4.2. Effect of K on the classification results
An additional experimental study tests how the classification results change related to the number of topics K set before per-document topic distributions as features will be distinctive enough to group this document with similar documents in a hyperplane and produce the correct label for the document.

The described classification framework also allows us to combine per-document topic distributions from different models models from the previous experiment are used as features. As the comparison of classification scores displayed in Fig. 7 reveals, we can observe a significant improvement in the classification results (+8.5% on average) by employing this proce-dure called topic smoothing .

In summary, we have shown how to utilize per-document topic distributions of a multilingual probabilistic topic model resentation of documents that is fine-grained enough to help us decide which category label to assign to documents. 5. Application III: Cross-lingual semantic similarity 5.1. Task description
So far, we have only used the per-document topic distributions in the previous applications. But, what about per-topic we have already detected that there ideally should exist a strong intra semantic and inter semantic coherence within cross-word w S 1 , we can build a ranked list RL  X  w S 1  X  which consists of all words w T scores sim  X  w S 1 ; w T j  X  . In the similar fashion, we can build a ranked list RL  X  w T scoring target words w T j for some source word w S 1 its M nearest neighbors . The ranked list for w S neighbors is called pruned ranked list (i.e., the ranked list is effectively pruned at position M ), and we denote it as RL semantically similar words across languages by exploiting the knowledge present in per-topic word distributions of a mul-tilingual topic model. The following section provides the answer. It introduces the complete framework for modeling cross-lingual semantic similarity by means of MuPTM. 5.2. Methodology
A straightforward approach to building one-to-one bilingual lexicons based on the knowledge from per-topic word dis-tributions was presented by Mimno et al. (2009) . For each topic z fashion, Boyd-Graber and Blei (2009) use learned matchings as one-to-one bilingual word lexicon entries (see Section 2.5 ). candidate translation pairs is limited to only the best scoring words, and the size heavily depends on the chosen number exploit the actual probability distributions of words over topics to mine semantically similar words across languages.
Inordertofullyexploitthoseper-topicworddistributions,wemakeaconnectionbetweenlatentcross-lingualtopicsandan idea knownas the distributional hypothesis ( Harris,1954 ).It states that wordswithsimilarmeaningsarelikely toappear insim-be regardedas context features . In other words, we consider that word w S lingual topics (extracted from per-topic word distributions for L setoflatenttopics(extractedfromper-topicworddistributionsfor L 5.2.1. Conditional topic distributions
After training, a multilingual topic model outputs per-topic word distributions with probability scores P  X  w S
P  X  w T 2 j z k  X  , for each w S 1 2 V S ; w T 2 2 V T and z has its own language-specific distribution over vocabulary words (see Sections 2.1 and 2.2 ).
 and the cross-lingual setting ( Ni et al., 2011 ) (see also Eq. (7) in Section 2.3 ). Since each document d ture of topics by means of per-document topic distributions given by the probability scores P  X  z two documents can be established by measuring the similarity of these probability distributions. When dealing with the by the probability scores P  X  z k j w S 1  X  and P  X  z k j w T latent semantic space. In other words, each word, irrespective to the language, is represented as a distribution over the K latent topics/concepts, where the K -dimensional vector representation of w S Using Bayes X  rule, we can compute these probability scores: where P  X  w S 1 j z k  X  is known directly from the per-topic word distributions. P  X  z in a typical setting where we do not possess any prior knowledge about the corpus and the likelihood of finding specific (i.e., that all topics/concepts are equally likely before we observe any training data). The probability scores P  X  z Eq. (14) for conditional topic distributions in that case may be further simplified: where we denote the normalization factor P K k  X  1 / S l ; 1 ilarity between two words may then be computed as the similarity between their conditional topic distributions as given by
Eq. (14) or Eq. (15) . We will use this property extensively in our models of cross-lingual similarity. 5.2.2. KL model and JS model
Now, once the conditional topic distributions are computed, any similarity metric may be used as a similarity function (SF) between word vectors to quantify the degree of similarity between the representations of words by means of these conditional topic distributions. We present and evaluate a series of models which employ the most popular SF-s reported in the relevant literature. Each SF in fact gives rise to a new model of cross-lingual semantic similarity!
The first model relies on the Kullback X  X eibler (KL) divergence which is a common measure of (dis) similarity between two probability distributions ( Lin, 1991 ). The KL divergence of conditional topic distributions for two words w S asymmetric measure computed as follows (our KL model ): output score implies a lower divergence between two words, and therefore, a closer semantic similarity. Both KL and JS are defined only if they deal with real probability distributions, that is, if probability scores sum up to 1, Additionally,
P  X  z k j w 1  X  &gt; 0 and P  X  z k j w 2  X  &gt; 0 have to hold for each z list RL  X  w S 1  X  may be obtained by sorting words w T 2 2 V scores computed by Eq. (16) (KL) or Eq. (17) (JS). 5.2.3. TCos model
The next distance measure is the cosine similarity which is one of the most popular choices for SF in distributional between two vectors of an inner product space that measures the cosine of the angle between them. The cosine similarity of conditional topic distributions (our TCos model ) is computed as follows: bound is 0 instead of 1), the higher the similarity between two words. A ranked list RL  X  w S words w T 2 2 V T in descending order based on their respective scores computed by Eq. (18) . 5.2.4. BC model
Another similarity measure is the Bhattacharyya coefficient (BC) ( Bhattacharyya, 1943; Kazama, Saeger, Kuroda, Murata, &amp; Torisawa, 2010 ). The similarity of two words based on this similarity measure is defined as follows (our BC model ): described by proper probability distributions. A higher score again implies a stronger semantic similarity between two iments will reveal its potential in identifying semantically similar words across languages. 5.2.5. Cue model
Another way of utilizing per-topic word distributions is to directly model the probability P  X  w T most similar target words should have the highest probability to be generated as a response to a cue source word. The prob-assumption of uniform topic prior, we can decompose the probability P  X  w T
The probability value directly provides the degree of semantic similarity and a ranked list RL  X  w S words w T 2 2 V T in descending order based on their respective probability scores computed by Eq. (20) . 5.2.6. TI model
The next model moves away from utilizing conditional topic distributions explicitly and aims to exploit latent cross-lin-gual topics in a different way. It builds a context vector
TTF-ITF ( term-topic frequency  X  inverse topic frequency ) is a novel weighting scheme which is analogous to and directly inspired by the TF-IDF ( term frequency  X  inverse document frequency ) weighting scheme in IR ( SparckJones, 1973; Salton, bility scores P  X  z k j w S 1  X  the context features sc 1 part of the complete score TTF-ITF  X  w S 1 ; z k  X  measures importance of w S assignments of the latent cross-lingual topic z k to the occurrences of w S exactly the Gibbs count variable v S k ; w S in Section 2.3 ). The ITF  X  w S 1  X  score measures global importance of w S frequency for the word w S 1 across the set of cross-lingual topics is computed as ITF  X  w S
ITF  X  w S 1 ; z k  X  score for the source language word w S as follows (our TI model): 5.2.7. TI + Cue model shared set of latent cross-lingual topics in different ways. Therefore, by combining the two models and capturing different (our TI + Cue model ):
Following Vulic  X  et al. (2011a) , the parameter c is set to 0.1. 5.2.8. Topic pruning over each cross-lingual topic z k 2Z and each calculation contributes to the overall sum. However, each word is usually thousands  X  2000  X  . Therefore, the procedure of topic pruning might lead to improvements in terms of overall quality and the speed of calculation.
 For the sake of simplicity, the description is valid for models that utilize conditional topic distributions with scores
P  X  z the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of the words in the shared latent semantic space spanned by cross-lingual topics as summarized in Algorithm 3 . 1: Obtain a subset Z K 0 # Z of K 0 6 K latent cross-lingual topics with the highest values P  X  z Calculating the similarity score sim  X  w S 1 ; w T 2  X  may be interpreted as:  X  X  X iven a word w 2: Retain conditional topic probability scores P  X  z 0 k j w 3: Retain conditional topic probability scores P  X  z 0 k j w
Both w S 1 and w T 2 are now represented by their K 0 -dimensional context vectors: for w S ec  X  w S 1  X  X  X  P  X  z 0 1 j w S 1  X  ; ... ; P  X  z 0 K 0 j w S vectors with the adjusted conditional topic probability scores to calculate similarity. 5.3. Experimental setup 5.3.1. Datasets We train on English-Italian Wikipedia. Following a common practice in relevant related work ( Koehn &amp; Knight, 2002; only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Therefore, our final vocabularies consist of 7160 Italian nouns and 9166 English nouns.
We designed a set of ground truth one-to-one translation pairs to measure the ability of these models of similarity to extract one-to-one word translations from the data (the bilingual lexicon extraction task). We randomly sampled a set of 1000 Italian nouns from our Wikipedia corpora (i.e., we conduct our experiments in the Italian-to-English direction) which late those words to English. The annotator manually revised the lists and retained only words that have their corresponding translation in the English vocabulary. Additionally, only one possible translation was annotated as correct. When more than occurs more frequently in the English part of our Wikipedia data.
For the multilingual topic model training, we have varied the number of topics K for BiLDA from 200 to 3500 with steps of topical space influences the quality of our models of similarity. 5.3.2. Evaluation metrics gual lexicons. In a more lenient evaluation setting, we also measure Acc among top 10 best scoring words in the ranked list for each word), and we also employ the mean reciprocal rank ( MRR ) 5.3.3. Models for comparison We evaluate all our MuPTM-based models of similarity in the BLE task (their codes are  X  -MuPTM , e.g., KL-MuPTM or JS-
MuPTM). We compare them against baseline models which also exploit document alignments when mining semantically similar words and translation candidates from comparable corpora: (2) Another baseline model is conceptually similar to our TI model. The model constructs feature vectors, but now in the 5.4. Results and discussion
We conduct two different batches of experiments: (1) We compare all our proposed models against the baseline models, and measure the influence of the number of topics K on the overall results in the BLE task and (2) we test and report the effect of topic pruning for a selection of models. 5.4.1. Experiment I: Comparison of all models
Acc along with the optimal number of topics K with which these results have been obtained. Based on all these results, we may derive a series of important conclusions: (ii) The choice of a similarity function matters. If we compare strictly SF-s operating with exactly the same representa-(iii) Based on these initial results, the TI model which relies on the representation with the new TTF-ITF weighting scheme (iv) We may observe that by combining the TI-MuPTM model and the Cue-MuPTM model, we are able to boost the overall (v) Our models of similarity reach their optimal performances with larger values of K (e.g., around the 2000 topics mark). 5.4.2. Experiment II: Analysis of topic pruning
In the next set of experiments, we analyze the influence of topic pruning on the behavior of our MuPTM-based models of cross-lingual similarity. All models use output per-topic word distributions from the BiLDA model trained with K  X  2000 topics. Table 7 displays the results over different values for the pruning parameter K we omit the results for: (1) the KL model whose behavior resembles the behavior of the JS model, only with lower overall scores, (2) the Cue model where we have not detected any major influence on the overall scores (i.e., the pruning is useful since it reduces execution time, but it does not lead to any improvements in scores), and (3) TI and TI + Cue models which play the change in overall scores for JS and BC over different values of K models. These time-related experiments were conducted on an Intel(R) Xeon(R) CPU E5-2667 2.9 GHz processor. We may notice several interesting phenomena: (i) Topic pruning helps to obtain higher results in the BLE task for the JS model (the increase is 7.5% even when proba-(iv) The best overall results are obtained with the BC model with K 6. Application IV: Cross-lingual information retrieval 6.1. Task description
Finally, we show how to employ both the per-topic word distributions and per-document topic distributions of a multilingual probabilistic topic model together, and how to exploit its inference power on an unseen collection in a language modeling translated using a machine-readable dictionary or a machine translation system, and then a myriad of techniques for the to build a CLIR system that does not rely on any external translation resource and can be trained on general-domain non-with the cross-lingual knowledge transfer.
 given by per-document topic distributions (with the values Pz distributionsmaybeusedtocalculatetheprobabilitythatcross-lingualtopic z links semantics of the query in the source language with semantics of the document written in the target language. 6.2. Methodology
Given a monolingual setting with only one language L S , the basic approach for using language models in information retrieval is the query likelihood model, where each document is scored by the likelihood of its model d Q of length m : P  X  Q S j d S j  X  X  Q m i  X  1 P  X  q S i j d S query terms, i.e., the unigram language model. Since we here deal with cross-lingual information retrieval, where documents are in the target language L T and query terms are in the source language L between them by means of a multilingual probabilistic topic model. The basic MuPTM-based language model for CLIR ( Vulic  X  et al., 2011b ) that uses only knowledge from the trained multilingual probabilistic topic model follows these steps: 1. Train the model on a (usually general-domain) training corpus and learn per-topic word distributions / and w , and per-document topic distributions. 2. Infer the trained model on the target collection given in the target language L tions h T for all documents in the collection. 3. For each term q S i 2 Q S do: (a) Obtain probabilities / k  X  1 ; ... ; K ; (b) Obtain probabilities h T j ; k  X  P  X  z d T j via the latent layer of cross-lingual topics: P muptm 4. Compute the final query likelihood for the entire query: Eq. (24) provides a query likelihood score for one document, so it has to be repeated for all documents in the collection. this CLIR technique that connects documents given in target language L displayed in Fig. 11 . There, each target document is represented as a mixture of latent language-independent cross-lingual topics (colored bars that denote per-document topic distributions) and assigns a probability value Pz (edges between documents and topics). Moreover, each cross-lingual topic may generate each query word by the probability Basic model.

When dealing with monolingual retrieval, Wei and Croft (2006) have detected that their model that relies on knowledge text and observed a major improvement in their results. We can follow the same principle in the cross-lingual setting, but with limited efficacy, since there is sometimes a minimum word overlap between languages. However, that model proves to be useful for languages from the same family, since, for instance, many named entities do not change across languages.
MuPTM-Unigram CLIR model combines the representation by means of a multilingual probabilistic topic model with the knowl-edge of the shared words across languages within the unified language modeling framework with the Jelinek X  X ercer and Dirichlet smoothing ( Zhai &amp; Lafferty, 2004 ). The model is as follows:
N is the length in words of the document d T j ; P mle q S i in the target document d T j ; P mle q S i j Coll T is the maximum likelihood estimate of q S
Dirichlet coefficient ( Zhai &amp; Lafferty, 2004 ), k is the interpolation parameter, and P
In this overview, we only report results obtained by these two basic MuPTM-based CLIR models. The language modeling framework for IR allows combining more various evidences in the query likelihood model. The framework is also topic model-independent and it allows experimentations and comparisons of different multilingual probabilistic topic models.
More complex LM CLIR models that rely on the representation by means of multilingual topic models are described and eval-based probabilistic dictionaries obtained by the methods from Section 5 instead of the shared words across languages and perform query expansion. The LM framework also allows blending knowledge from the external resources (e.g., machine-readable dictionaries) with the topical representation, but that research is beyond the scope of this work. 6.3. Experimental setup 6.3.1. Datasets For training , we use English X  X utch Wikipedia articles, but, to reduce data sparsity, we augment that dataset with 6206 Europarl ( Koehn, 2005 ) English X  X utch document pairs. 13
Experiments were conducted on three test datasets taken from the CLEF 2001 X 2003 CLIR campaigns: the LA Times 1994 ( LAT ), the LA Times 1994 plus the Glasgow Herald 1995 ( LAT + GH ) in English, and the NRC Handelsblad 1994 X 1995 plus the
Algemeen Dagblad 1994 X 1995 ( NC + AD ) in Dutch. Queries were extracted from the title and description fields of all CLEF themes for each year and queries without relevant documents were removed from the query sets. The overall statistics 6.3.2. Evaluation metrics
The performance of the CLIR models is reported in the mean average precision (MAP) scores and/or 11-pt precision-recall diagrams for all experiments. 6.4. Results and discussion
Table 9 a shows the perplexity scores after the inference of the BiLDA model on the CLEF test collections, while Table 9 b displays the MAP scores for all campaigns in both retrieval directions (i.e., English queries  X  Dutch documents, and vice an extra portion of information that can be easily embedded within the language modeling framework. The synergy between different evidences when performing retrieval leads to better unsupervised models. Due to a high percentage of shared words between English and Dutch, there is a possibility that the MuPTM-Unigram model draws its performance mainly from the part specified by the  X  X  X on-MuPTM X  X  part of the model (see Eq. (25) ). This model called Unigram-Basic which can be obtained by setting k  X  1 in Eq. (25) does not use any topical knowledge and relies only on the shared words. However,
Fig. 12 a and b clearly show that the final combined MuPTM-Unigram model clearly works as a positive synergy between the two simpler basic models, outperforming both of them. However, MuPTM-Basic provides higher scores than Unigram-
Basic and is therefore more important for the overall performance of the combined model. As in Sections 4 and 5 , we have also experimented with monolingual LDA trained on merged document pairs (MixLDA), but the reported retrieval results with the MoPTM-Basic retrieval model are in the range [0.01 X 0.03] in terms of MAP scores which is extremely lower than the MAP scores reported for our MuPTM-Basic model in Table 9 b.

The reported results also show the potential of training the multilingual probabilistic topic model on huge-volume out-of-domain data and its inference and usage on another collection, which is not necessarily completely theme-aligned to the document topic distributions of a multilingual probabilistic topic model in CLIR, we do not provide a thorough discussion here. That, along with many more experiments and comparisons (e.g., a comparison where it was shown that these results with MuPTM-s are comparable to results of the models which rely on translation tools such as Google Translate ) might again that is, MAP scores for CLIR. The comparison of the results in Table 9 a and b clearly shows that the theoretical in vitro measure of perplexity, often used to compare the quality of probabilistic topic models, does not guarantee a better in vivo performance in actual applications such as CLIR. The same general conclusion for language models in information from Section 4.4 where we have also stated that better perplexity scores of a probabilistic topic model do not necessarily reflect in a better  X  X  X eal-life X  X  classification task performance. 7. Conclusions and future perspectives in multilingual probabilistic topic modeling 7.1. Final discussion
In this article, we have conducted the first systematic and thorough overview of the current advances in multilingual prob-topic models. We have provided precise formal definitions of this modeling concept and drew analogies with its monolingual variants and a broader concept of inducing latent cross-lingual concepts from multilingual data. Additionally, we have probabilistic topic models which induce such a language-independent cross-lingual topical space in general comprise two sets of probability distributions: (1) per-document topic distributions that define topic importance in a document and (2) per-topic word distributions that define importance of vocabulary words in each language for each cross-lingual topic. As
In this article, we have shown that monolingual probabilistic topic models are only a special, degenerate case of multi-frameworks presented in this paper which tackle cross-lingual tasks are easily adapted to and functional in the monolingual settings.

A standard approach to induce topical knowledge when handling cross-lingual tasks in cases when an external translation resource is absent is to train a monolingual topic model such as LDA on merged/concatenated documents from an aligned document pair with documents in two different languages). However, our comparisons across different cross-lingual tasks (see Sections 3 X 6 ) clearly indicate that training a multilingual topic model on separate documents from a document pair even to extremely higher) scores in all these applications.

We also observe that the optimal setting of a priori parameters (e.g., the number of topics) is heavily application-depen-grained text representations, and use a lower number of topics. It seems that such coarse semantic representations are suf-ficient to successfully learn the correct category matchings. In Section 4 , we have also shown that the technique of topic smoothing which combines representations obtained by different K -s may lead to a more robust final model. On the other with the high number of topics topical representations lack sufficient detail.
 butions, the document representation is translated from the original high-dimensional word-document space to a lower-dimensional topic-document space. In a similar fashion, by learning per-topic word distributions, the word representation is effectively translated from the original high-dimensional word-document space to a lower-dimensional word-topic space.
In Section 5 , where a complete framework for modeling cross-lingual semantic similarity by means of MuPTM has been introduced and described, we have also introduced the paradigm of topic pruning, which selects only a subset of highly ments in both the quality of results and the speed of computations.

The results across these applications also reveal that the lower-dimensional representations (i.e., word-topic space and topic-document space) of the original word-document space alone might not be discriminative enough for the tasks that the latent semantic lower-dimensional representations with the original higher-dimensional representations leads to more effective and robust models. For instance, the MuPTM-Unigram model that combines unigrams shared across languages with topical knowledge and topical representation of documents leads to much better retrieval scores than MuPTM-Basic that uses only topical knowledge as the only representation for retrieval.

Finally, since all topic models in this article have been trained on comparable Wikipedia data, we have also implicitly shown the validity of training on such high-volume easily obtainable comparable datasets in a wide spectrum of NLP/IR tasks. The presented MuPTM framework is unsupervised and language pair independent in its design (since it does not rely on any external translation resource and induces knowledge directly from the given multilingual data). Consequently, that makes it potentially applicable to many language pairs. 7.2. Other applications
We have demonstrated how to make use of the output per-document topic distributions and per-topic word distributions information retrieval that have been tackled and perused here, MuPTM has been applied to a series of other NLP/IR cross-lingual tasks which we briefly list here:
Cross-lingual keyword recommendation ( Takasu, 2010 ). Both the text and the keywords are mapped into the same latent cross-lingual topical space (induced from parallel data), and a ranked list of keywords is provided based on the learned per-topic word distributions and per-document topic distributions.
 most similar document in the target language given a document in the source language. The knowledge of tightly coupled documents may be used to build high quality aligned multilingual datasets. Document representations by means of per-document topic distributions are again used to provide language independent representations of documents and allow their comparison in the shared latent cross-lingual space.
 tent, unified picture of sentiment across multiple languages.

Transliteration mining ( Richardson, Nakazawa, &amp; Kurohashi, 2013 ). A framework for cross-lingual semantic similarity described in Section 5 (i.e., the knowledge from per-topic word distributions) has been used to provide information about semantic similarity between potential transliteration pairs.
 tion of an entity to some given knowledge base (e.g., Wikipedia). The output of the BiLDA model is again used to compute the similarity between the context of the entity mention and the appropriate Wikipedia page to which the mention should be linked.

Cross-lingual word sense disambiguation ( Tan &amp; Bond, 2013 ). Given a sentence along with a polysemous word, the goal is to provide a correct sense for the polysemous word. A multilingual topic model may be used in this task to match the query sentence to a list of sentences in the other language based on the most probable topics of these sentences. The cor-rect sense is then extracted from the topically most similar sentence in the other language.

Models of MuPTM-based semantic similarity from Section 5 and the knowledge coming from language-specific per-topic word distributions may be used in various ways to extract word translation pairs from multilingual data. gual topic model may be embedded into the relevance modeling framework for cross-lingual information retrieval. This combination results in more effective and more robust models of cross-lingual IR.

All these tasks can be accomplished by means of the MuPTM-based representations of words and documents, that is, by the output per-document topic distributions and the per-topic word distributions. 7.3. Future work
A straightforward line of future work leads to investigating more applications of the MuPTM framework (e.g., cross-lin-gual document summarization, keyword and keyphrase extraction).

In order to improve the quality of the lower-dimensional topical representations of documents in the multilingual domain, there is a huge number of paths that could be followed. In the same manner as for the natural  X  X  X DA to BiLDA X  X  extension, other more sophisticated and application-oriented probabilistic topic models developed for the monolingual setting could be ported into the multilingual setting (e.g., Wallach, 2006; Blei &amp; McAuliffe, 2007; Gormley, Dredze, with single words ( Wallach, 2006 ), the use of sentence information or word ordering (using Hidden Markov Models) to yield 2008 ). The use of hierarchical topics (general super-topics connected with more focused sub-topics, see, e.g., Blei, comparable and more divergent and unstructured than Wikipedia or news stories, where only a subset of latent cross-lingual topics overlaps across documents written in different languages. Additionally, the more data-driven topic models should be able to learn the optimal number of topics dynamically according to the properties of training data itself (the so-called shared and non-shared topics in a multilingual corpus.

Additionally, besides being a multilingual environment, the Web and the world of information are also locales for multi-has already been made as it was proven that the topical knowledge combined with the (CL) IR framework from Section 6 is personal pages ( Zoghbi, Vulic  X  , &amp; Moens, 2013b, 2013a; Vulic  X  , Zoghbi, &amp; Moens, 2014 ).
Finally, as the probabilistic topic models have proven to work in the multilingual settings with comparable corpora, we multilingual probabilistic topic models that operate with comparable data should be transferred and adapted to the multi-parable nature of any dataset consisting of text and images or video. However, some initial studies have revealed that the multimodal context serves as a more complex setting, and additional expansions of the multimodal topic models are needed to effectively handle the existing gap between different modalities.
 Acknowledgments
The research presented in this article has been carried out in the framework of several research projects. It has been partially supported by the following projects: CLASS (EU FP6-027978) financed by the EU Sixth Framework Programme
ICT, AMASS++ (SBO-060051) financed by Instituut voor de Aanmoediging van Innovatie door Wetenschap en Technologie in Vlaanderen (IWT), WebInsight (BIL/08/08), and TermWise (IOF-KP/09/001) financed by the Flemish government. References
