 Language models are used to compute probabili-ties of sequences of words. They are crucial for good performance in tasks like machine translation, speech recognition, and spelling correction. They can be classified into two categories: count-based and continuous-space language models. The lan-guage modeling literature abounds with success-ful approaches for learning-count based language models: modified Kneser-Ney smoothing, Jelinek-Mercer smoothing, etc. In recent years, continuous-space language models such as feed-forward neu-ral probabilistic language models (NPLMs) and re-have outperformed their count-based counterparts (Chelba et al., 2013; Zaremba et al., 2014; Mikolov, 2012). RNNs are more powerful than n -gram lan-guage models, as they can exploit longer word con-texts to predict words. Long short-term memory lan-guage models (LSTMs) are a class of RNNs that have been designed to model long histories and are easier to train than standard RNNs. LSTMs are cur-rently the best performing language models on the Penn Treebank (PTB) dataset (Zaremba et al., 2014).
The most common method for training LSTMs, maximum likelihood estimation (MLE), is pro-hibitively expensive for large vocabularies, as it in-volves time-intensive matrix-matrix multiplications. Noise-contrastive estimation (NCE) has been a suc-cessful alternative to train continuous space lan-guage models with large vocabularies (Mnih and Teh, 2012; Vaswani et al., 2013). However, NCE in its standard form is not suitable for GPUs, as the computations are not amenable to dense ma-trix operations. In this paper, we present a natu-ral modification to the NCE objective function for language modeling that allows a very efficient GPU implementation. Using our new objective, we train large multi-layer LSTMs on the One Billion Word benchmark (Chelba et al., 2013), with its full 780k word vocabulary. We achieve significantly lower perplexities with a single model, while using only a sixth of the parameters of a very strong base-line model (Chelba et al., 2013). We release our large-vocabulary LSTMs with NCE. The contribu-tions in this paper are the following:  X  A fast and simple approach for handling large  X  Significantly improved perplexities ( 43 . 2 ) on  X  Extrinsic machine translation improvement  X  Fast decoding times because in practice there is In recent years, LSTMs (Hochreiter and Schmid-huber, 1997) have achieved state-of-the-art perfor-mance in many natural language tasks such as lan-guage modeling (Zaremba et al., 2014) and statis-tical machine translation (Sutskever et al., 2014; Luong et al., 2015). LSTMs were designed to have longer memories than standard RNNs, allow-ing them to exploit more context to make predic-tions. To compute word probabilities, the LSTM reads words left-to-right, updating its memory after each word and producing a hidden state h , which summarizes all of the history. For details on the architecture and computations of the LSTM, the reader can refer to (Zaremba et al., 2014). In this model the probability of word w given history u is where p ( w | u ) = exp D w h T + b w is an unnor-malized probability. D w and b w are the output word embedding and biases respectively, which are learned during training. The normalization constant is Z ( u ) = The standard approach for estimating neural lan-guage models is maximum liklelihood estimation (MLE), where we learn the parameters  X   X  that max-imize the likelihood of the training data, However, for each training instance, gradient-based approaches for MLE require a summation over all units in the output layer, one for each word in V . This makes MLE training infeasible for large vocab-ularies.

Noise-contrastive estimation (NCE) (Gutmann and Hyv  X  arinen, 2010) has been successfully adopted for training neural language models with large vo-cabularies (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014; Williams et al., 2015). NCE avoids repeated summations by train-ing the model to correctly classify between gener-ated noise samples and words observed in the train-ing data. For each training pair u i ,w i , we generate k noise samples,  X  w i 1 ...,  X  w ik from a noise distribu-tion q ( w ) , which is known. We label u i ,w i as true ( C = 1 ), and all u i ,  X  w ik as false ( C = 0) . We then train the parameters to maximize the binary classifi-cation log likelihood, where and P ( C = 0 | w, u ) = 1  X  P ( C = 1 | w, u ) .
We learn parameters to maximize this objective with gradient ascent. In NCE, we treat Z ( u ) as an-other parameter and learn its estimate along with the rest of the parameters. Following Mnih and Teh (2012) and Vaswani et al. (2013), we freeze Z ( u ) to 1 and the model learns to approximately satisfy the constraint. In practice, we find that our mean for Z ( u ) is very close to 1 and the variance is quite small (Section 6). For each training instance, we com-pute gradients for the observed word and each noise word, reducing the time complexity from O ( | V | ) for MLE to O ( k ) . However, unlike MLE, since we typ-ically sample different noise samples for each train-ing instance, our gradient computations for the NCE objective are not amenable to dense matrix opera-tions, making it difficult to benefit from fast dense matrix arithmetic on GPUs. In this paper, we present a simple solution to this problem: sharing the noise samples across all the training instances in a mini-batch. Sharing noise samples allows us to describe NCE gradients with dense matrix operations, and implement them easily on the GPU. In the next sec-tion, we describe our NCE implementation on the GPU with shared noise samples. In typical Noise-Contrastive Estimation, the objec-tive function requires noise samples coming from some distribution (in our case, the uniform distri-bution). The objective function for NCE is shown above in Equation 3, where we must calculate P ( C = 1 | w, u ) for the true word and the noise samples generated. There are three components to this calculation: p ( w | u ) , Z ( u ) , and k  X  q ( w ) . In NCE we fix Z ( u ) to be one, so we only need to cal-culate p ( w | u ) and k  X  q ( w ) . The term k  X  q ( w ) is simply an O (1) lookup, so the only costly operation is computing p ( w | u ) for all k noise samples and the true word. The operation to compute p ( w | u ) for a single word w is exp D w h T + b w where D w and b w represent the word embedding and bias cor-responding to the word we are computing it for.
The main cost in calculating the NCE objective function is computing exp D w h T + b w , where in normal NCE a dense matrix multiplication cannot be done. The reason is that the noise samples gen-erated per training example will be different. There-fore, when we parallelize our algorithm by running multiple training examples in parallel (a minibatch), the D w we need are different per h T that we are run-ning in parallel. If a constraint is set such that the noise samples must be the same across all training examples in the minibatch, then a matrix multipli-cation can be done to compute exp D w h T + b w for all words across that minibatch. This matrix mul-all the hidden states being used in parallel, whereas before h T was just a vector. Additionally, D is the matrix of word embedding for the samples that are being shared across a minibatch. Before, this was not possible as the D matrix would have to be much larger, being (minibatch size)  X  ( k + 1) in size, mak-ing the algorithm run much slower. An alternative is to not restrict the noise samples to be the same, but then we must use a sparse matrix multiplica-tion as in Williams et al. (2015), which is neither as fast nor as easy to implement. A comparison be-tween these two approaches is shown in Figure 1. We find that sharing noise samples across the mini-batch gives us significant speedups over a baseline of using different noise samples for each word in the minibatch. These timings were calculated for a sin-gle layer LSTM with 1000 hidden units, a vocab-ulary of 350 k , and a minibatch of 128 . Not sur-prisingly, MLE is quite expensive, limiting it X  X  use for large vocabularies. Additionally, the memory requirements for NCE are much lower than MLE, since we do not need to store the gradient which has the same size as the output embedding matrix. For this MLE run, we had to distribute the computation across two GPUs because of memory limitations. We conducted two series of experiments to validate the efficiency of our approach and the quality of the models we learned using it: An intrinsic study of language model perplexity using the standard One Billion Word benchmark (Chelba et al., 2013) and an extrinsic end-to-end statistical machine transla-tion task that uses an LSTM as one of several feature functions in re-ranking. Both experiments achieve excellent results. 5.1 Language Modeling For our language modeling experiment we use the One Billion Word benchmark proposed by Chelba et al. (2013). In this task there are roughly 0.8 billion words of training data. We use perplexity to evalu-ate the quality of language models we train on this data. We train an LSTM with 4 layers, where each layer has 2048 hidden units, with a target vocabulary size of 793,471. For training, we also use dropout to prevent overfitting. We follow Zaremba et al. (2014) for dropout locations, and we use a dropout rate of 0.2. The training is parallelized across 4 GPUs, such that each layer lies on its own GPU and communi-cates its activations to the next layer once it finishes its computation. 5.2 Statistical Machine Translation We incorporate our LSTM as a rescoring feature on top of the output of a strong Arabic-English syntax-based string-to-tree statistical machine trans-lation system (Galley et al., 2006; Galley et al., 2004). That system is trained on 208 million words of parallel Arabic-English data from a variety of sources, much of which is Egyptian discussion fo-rum content. The Arabic side is morphologically segmented with MADA-ARZ (Habash et al., 2013). We use a standard set of features, including two con-ventional count-based language models, as well as thousands of sparsely-occurring, lexicalized syntac-tic features (Chiang et al., 2009). The system addi-tionally uses a source-to-target feed-forward neural network translation model during decoding, analo-gous to the model of (Devlin et al., 2014). These features are tuned with MIRA (Chiang et al., 2009) on approximately 63,000 words of Arabic discus-sion forum text, along with three references. We evaluate this baseline on two test sets, each with ap-proximately 34,000 words from the same genre used in tuning.

We generate n -best lists ( n = 1000 ) of unique translations for each sentence in the tuning set and re-rank the translations using an approach based on MERT (Och, 2003). We collapse all features other than language models, text length, and derivation size into a single feature, formed by taking the dot product of the previously learned feature and weight vectors. We then run a single iteration of MERT on the n -best lists to determine optimal weights for the collapsed feature, the uncollapsed features, and an LSTM feature formed by taking the score of the hy-pothesis according to the LSTM described in Sec-tion 5.1. We use the weights to rerank hypotheses from the n -best lists of the two test sets. We re-peated this experiment, substituting instead a two-layer LSTM trained on the English side of the train-ing data. Our two experiments with LSTMs trained with our modification of NCE show strong results in their corresponding tasks.

Our perplexity results are shown in Table 1, where we get significantly lower perplexities than the best single model from Chelba et al. (2013), while having almost 6 times fewer parameters. We also compute the partition function values, log Z ( u ) , for our de-velopment set and we find that the mean is 0 . 058 and the variance is 0 . 139 , indicating that training has en-couraged self-normalization.
 Model Parameters Perplexity Chelba et al. (2013) 20m 51.3 NCE (ours) 3.4m 43.2
Recently, (J  X  ozefowicz et al., 2016) achieved state-of-the-art language modeling perplexities ( 30 . 0 ) on the billion word dataset with a single model, using importance sampling to approximate the normaliza-tion constant, Z ( u ).

Independent of our work, they also share noise samples across the minibatch. However, they use 8192 noise samples, while we achieve strong per-plexities with 100 noise samples. We also show significant improvements in machine translation, ex-ploiting self-normalization for fast decoding, in ad-dition to releasing a efficient toolkit that practition-ers can use.

Table 2 shows our re-scoring experiments. When we incorporate only the LSTM trained on the BOLT dataset we get a +1.1 BLEU improvement on Tune, +1.4 on Test 1, and +1.1 on Test 2. When we also incorporate the LSTM trained on the One Bil-lion Word dataset as a feature, we see another +0.2 BLEU increase on Tune and Test 2. In System Tune Test 1 Test 2 Baseline SMT 38.7 38.9 39.7 LSTM (BOLT) 39.8 40.3 40.8 LSTM (1b+BOLT) 40.0 40.3 41.0 these re-scoring experiments we simply use the un-normalized numerator p ( w | u ) as our word score, which means we never have to compute the costly partition function, Z ( u ) . This is because the parti-tion function is so close to 1 that the un-normalized scores are very close to the normalized ones. We describe a natural extension to NCE that achieves a large speedup on the GPU while also achieving good empirical results. LSTM models trained with our NCE modification achieve strong results on the One Billion Word dataset. Addition-ally, we get good BLEU gains when re-ranking n-best lists from a strong string-to-tree machine trans-lation system. We also release an efficient toolkit for training LSTM language models with NCE. This work was carried out with funding from DARPA (HR0011-15-C-0115) and ARL/ARO (W911NF-10-1-0533).

