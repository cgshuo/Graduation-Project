 In this paper, we propose a novel top-k learning to rank framework, which involves labeling strategy, ranking model and evaluation measure. The motivation comes from the dif-ficulty in obtaining reliable relevance judgments from human assessors when applying learning to rank in real search sys-tems. The traditional absolute relevance judgment method is difficult in both gradation specification and human assess-ing, resulting in high level of disagreement on judgments. While the pairwise preference judgment, as a good alter-native, is often criticized for increasing the complexity of judgment from O ( n )to O ( n log n ). Considering the fact that users mainly care about top ranked search results, we propose a novel top-k labeling strategy which adopts the pairwise preference judgment to generate the top k order-ing items from n documents (i.e. top-k ground-truth) in a manner similar to that of HeapSort. As a result, the com-plexity of judgment is reduced to O ( n log k ). With the top-k ground-truth, traditional ranking models (e.g. pairwise or listwise models) and evaluation measures (e.g. NDCG) no longer fit the data set. Therefore, we introduce a new rank-ing model, namely FocusedRank, which fully captures the characteristics of the top-k ground-truth. We also extend the widely used evaluation measures NDCG and ERR to be applicable to the top-k ground-truth, referred as  X  -NDCG and  X  -ERR, respectively. Finally, we conduct extensive ex-periments on benchmark data collections to demonstrate the efficiency and effectiveness of our top-k labeling strategy and ranking models.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Performance, Experimentation, Theory Learning to Rank, Top-k, Preference Judgment, Evaluation
In the past few years, learning to rank has been widely rec-ognized as an important technique for information retrieval (IR). A vital part to employ learning to rank in real search systems is the acquisition of reliable and high quality labeled datasets, both for training and evaluation. In traditional IR literature, assessors are requested to determine the relevance of a document under some pre-defined gradations, which is called absolute relevance judgment method. However, there are some significant drawbacks for this evaluation process. Firstly, the specifics of the gradations (i.e. how many grades to use and what those grades mean) must be defined, and it is not clear how these choices will affect relative performance measurements [26]. Secondly, the assessing burden increases with the complexity of the relevance gradations; the choice of label is not clear when there are more factors to consider, leading to high level of disagreement on judgments [4].
Recently pairwise preference judgment has been investi-gated as a good alternative [20, 26]. Instead of assigning a relevance grade to a document, an assessor looks at two pages and judges which one is better. Compared with ab-solute relevance judgment, the advantages lie in that: (1) There is no need to determine the gradation specifications as it is a binary decision. (2) It is easier for an assessor to ex-press a preference for one document over the other than to assign a pre-defined grade to each of them [7]. (3) Most state-of-the-art learning to rank models, pairwise or list-wise, are trained over preferences. As noted by Carterette et al. [7],  X  X y collecting preferences directly, some of the noise associated with difficulty in distinguishing between differ-ent levels of relevance may be reduced. X  Although prefer-ence judgment likely produce more reliable labeled data, it is often criticized for increasing the complexity of judgment (e.g. from O ( n )to O ( n log n ) [20]), which poses a big chal-lenge in wide use. Do we actually need to judge so many pairs for real search systems? If not, which pairs do we choose? How to choose? These questions become the origi-nal motivation of this paper.

As we know, in real Web search scenario, it is well ac-cepted that users mainly care about the top results [30]. In other words, the ordering of the top results (typically the results on the first one or two pages) is critical for users X  search experience. It indicates that a labeling strategy shall take effort to figure out the top results and judge the prefer-ence orders among them, but pay less attention to the exact preference orders among the rest results. Based on this ob-servation, we propose a novel top-k labeling strategy which adopts the pairwise preference judgment to generate the top k ordering items from a set of n items in a manner similar to that of HeapSort. The obtained ground-truth from this top-k labeling strategy is a mixture of the total order of the top k items, and the relative preferences between the set of top k items and the set of the rest n  X  k items, referred as top-k ground-truth. With this top-k labeling strategy, we can not only capture enough information for learning to rank [30], but also largely reduce the complexity of judgment to O ( n log k ).

With top-k ground-truth, we find that traditional ranking models, either pairwise or listwise, are no longer suitable for the labeled data set. It is natural to introduce a mixed rank-ing model, with the listwise model capturing the total order of the top k items and the pairwise model capturing the rel-ative preference between the set of top k items and the set of the rest n  X  k items. Such a mixed model can thus combine the advantages of both pairwise and listwise approaches to fully exploit the information in the top-k ground-truth. We refer such a mixed ranking model as FocusedRank, since it emphasizes more on the ordering of the top items. For evaluation, traditional IR evaluation measures (e.g., MAP, NDCG and ERR), which are mainly defined on the absolute judgment, cannot be directly applied to the top-k ground-truth. To address this problem, we extend NDCG and ERR to  X  -NDCG and  X  -ERR by taking a function of the position of items as the absolute relevance label. The pro-posed evaluation measures thus emphasize the importance of the ordering of the top k items. Unlike the evaluation measures based on preference judgments [6],  X  -NDCG and  X  -ERR keep the same form as NDCG and ERR thus enjoy all the merits of traditional IR evaluation measures.
Finally, we conduct extensive experiments on benchmark data collections. Major experimental findings include: (1) With top-k labeling strategy, the time cost on labeling one pair is much less than that on one item in absolute rele-vance judgment, and the overall time cost is comparable with that in absolute relevance judgment. (2) With top-k labeling strategy, the level of agreement on judgments is higher than that on absolute relevance judgment. (3) With FocusedRank, the ranking performance is significantly bet-ter than the state-of-the-art pairwise and listwise ranking models.

To sum up, we propose a top-k learning to rank frame-work 1 , a novel and complete framework including labeling strategy, ranking model and evaluation measures. Our main contributions are as follows: 1. We propose a novel top-k labeling strategy which adopts 2. We introduce a new ranking model named Focuse-
Note that Xia et al. also mentioned the top k ranking prob-lem in [30]. The difference is that they focus on the ranking models under the circumstances of traditional labeling strat-egy and evaluation measure. 3. We derive two new evaluation measures named  X  -NDCG
In this section, we briefly review some related work on labeling strategy, ranking model and evaluation measure in learning to rank literature.
In learning to rank, labeling strategies can be divided into two categories: absolute judgment and relative judgment [21, 26, 32, 7].

In absolute judgment, assessors are usually requested to assign a graded score to an item independent of the other items [15, 4, 27, 28, 29] under some pre-defined gradations. Such a labeling strategy has been widely adopted in both industry and academia to construct benchmark datasets in IR, e.g. TREC data sets (since 2000), Microsoft learning to rank datasets [18] and Yahoo! Learning to Rank chal-lenge 2010 data set. One difficult problem in using absolute judgment is to clearly define the specifics of the gradations, i.e. how many grades to use and what those grades mean. Some previous studies tried to figure out the proper num-ber of relevance gradations [22, 19]. However, as noted by Cox [12],  X  X here is no single number of response alternatives for a scale which is appropriate under all circumstances X . If the descriptions of each degree are not clearly defined, a multi-grade judgment method can be easily misused in the evaluation process [34]. Moreover, the assessing burden in absolute judgment increases with the complexity of the rele-vance gradations. When there are more factors to consider, the choice of label is not clear, resulting in high level of disagreement on judgments [4].

In contrast, relative judgement aims to directly judge the relative order of a set of items [26]. As a typical form of rel-ative judgment, pairwise preference judgment asks assessors to express a preference for one item over the other [6]. One concern of using this strategy is the complexity of judgment since the number of item pairs is polynomial in the num-ber of items. Carterette et al . [7] attempted to reduce the number of pairs for judging by using transitivity of relevance among documents. Nir Ailon [1] proposed a formal pairwise method based on QuickSort which can reduce the number of preference judgments from O ( n 2 )to O ( n log n ). Compared with O ( n ) in absolute judgment, this is still not affordable for assessors. To increase the efficiency of relative judgment, R. Song et al. [26] further proposed to select the best one each time from the remaining items, which is only applicable to small datasets. Different from the above related work, we propose a novel top-k labeling strategy to largely save the ef-fort of preference judgment, by exploiting what Web search users actually care about on ranking.
So far learning to rank has been mainly addressed by pointwise, pairwise, and listwise ranking models. In point-wise models [17], ranking is transformed to regression or classification on individual items to represent the absolute label on each item. In pairwise models [14, 11, 3], ranking is transformed to classification on item pairs to represent the preference between two items. In listwise models [33, 31, 5], instances as document lists are generated through the com-parison over item pairs, and it is superior in modeling more discriminative judgments. Therefore, we can conclude that, pointwise models is well suited for absolute relevance judg-ment; while both pairwise and listwise models are applicable in either absolute or relative judgment scenario.
In previous work [30], Xia et al. extended three listwise ranking models, namely top-k ListMLE, top-k ListNet and top-k RankCosine, to fit the top-k scenario. Note that they addressed this scenario under the circumstances of tradi-tional labeling strategy and evaluation measure. They con-ducted experiments on the top-k ListMLE, and claimed that the top-k ListMLE can outperform traditional pairwise and listwise ranking models. However, it cannot avoid the com-putational complexity on the entire permutation such as that in top-k ListNet.

In our paper, we take the above ranking models as the baselines to show the superiority of FocusedRank. To evaluate the effectiveness of a ranking model, many IR measures have been proposed. Here we give a brief in-troduction to several popular ones which are widely used in learning to rank. See also [16] for other measures.
Precision@ k [2] is a measure for evaluating top k positions of a ranked list using two grades (relevant and irrelevant) of relevance judgment. With Precision as the basis, Average Precision (AP) and Mean Average Precision (MAP) [2] are derived to evaluate the average performance of a ranking model.

While Precision considers only two graded relevance judg-ments, Discounted Cumulated Gain (DCG) [13] is an eval-uation measure that can leverage the relevance judgment in terms of multiple ordered categories, and has an explicit position discount factor in its definition. By normalizing DCG@ k with its maximum possible value, we will get an-other popular measure named Normalized Discounted Cu-mulated Gain (NDCG).

To relax the additive nature and the underlying indepen-dence assumption in NDCG, another evaluation measure, Expected Reciprocal Rank (ERR), is proposed in [8]. It implicitly discounts documents which are shown below very relevant documents, and is defined as the expected recipro-cal length of time that the user will take to find a relevant document.

Although MAP, NDCG and ERR are widely used in IR, they all adopt absolute relevance labels in their formulation, which imposes restrictions on direct application in the sce-nario of relative judgment. Therefore, new measures such as bpref , ppref ,and nwppref [6] have been proposed. How-ever, these measures have not been widely accepted by IR community. In this paper, we extend traditional IR evalua-tion measures to our relative judgment scenario with similar formulation.
In this section, we will introduce our top-k learning to rank framework in detail, which involves the labeling strategy, the ranking model and the evaluation measure.
According to previous work and the above discussions, pairwise preference judgment is superior to traditional ab-solute relevance judgment in the acquisition of reliable judg-ments from human assessors. However, it is often criticized for increasing the complexity of judgment. In this section, we propose a novel top-k labeling strategy by exploiting what Web search users actually care about on ranking. Our labeling strategy can not only capture enough information for learning to rank, but also largely save the effort of pref-erence judgment.
In real Web search applications, users usually pay more at-tention to the top-k items [9, 25, 10]. For example, according to a user study [30], in modern search engines, about 62% of search users only click on the results within the first pages, and 90% of search users click on the results within the first three pages. It shows that the ordering of the top k results is critical for users X  search experience. Two ranked lists of results will likely provide the same value to users (and thus suffer the same loss), if they have the same ranking results for the top positions [30]. Moreover, a good ranking on the top results is much more important than a good ranking on the others. Therefore, a labeling strategy shall take effort to figure out the top k results, judge the preference orders among them carefully, but pay less attention to the exact preference orders among the rest results.
Based on the above analysis, we propose a novel top-k labeling strategy using the pairwise preference judgment as the basis. The basic assumption for our labeling strategy is the transitivity of preference judgments of relevance [24, 7]. That is, if i is preferred to j and j is preferred to k ,the assessor will also prefers i to k . With this assumption, our labeling strategy generate the top k ordering items from a set of n items in a manner similar to that of HeapSort. It mainly takes the following three steps: Step1 Randomly select k items from the set of n items Step2 Randomly select an item r from the rest n  X  k items Step3 Sort the final k items in the min-heap in a descending The detailed labeling algorithm is shown in Algorithm1. With the above top-k labeling strategy, the obtained ground-truth is a mixture of the total order of the top k items and the relative preference between the set of top k items and the set of the rest n  X  k items, referred to as top-k ground-truth.
Here we consider the judgment complexity of each step in top-k labeling strategy: (1)The judgment complexity of building a min-heap with k items in Step 1 is O ( k ); (2)The judgment complexity in Step 2 is O (( n  X  k )log k ) according to the complexity analysis for HeapSort; (3)The judgment complexity in Step 3 is O ( klogk ).
Therefore, the total judgment complexity of top-k label-ing strategy is about O ( n log k ). Compared with QuickSort strategy adopted by Nir Ailon [1] for preference judgment, our top-k labeling strategy significantly reduces the com-plexity from O ( n log n )to O ( n log k ), where usually k n . The judgment complexity of our strategy is nearly compa-rable with that of the absolute judgment (i.e. O ( n )). Ex-perimental results in the following section also verify the efficiency of our labeling strategy which is consistent with the theoretical analysis.
With the top-k ground-truth obtained from our labeling method, traditional ranking models no longer fit the labeled dataset. On one hand, pairwise ranking models can cap-ture the information of the relative preference, but fail to model the total order of the top k items since they ignore the position information. On the other hand, listwise rank-ing models can capture the information of the total order, but suffer from the great computational complexity due to a large undifferentiated item set in top-k ground-truth. To ad-dress this problem, we propose FocusedRank, a mixed rank-ing model with listwise ranking model capturing the total order of the top k items and pairwise ranking model cap-turing the relative preference between the set of top k items and the set of the rest n  X  k items. Suchamixedmodel can thus combine the advantages of both pairwise and list-wise approaches to fully exploit the information in the top-k ground-truth.
Given m training queries { q i } m i =1 ,let x i = { x ( i ) be the items associated with q i ,where n i is the number of documents of this query, T i be the set of top k items and F i be the set of other n i  X  k items. Denote the total order of T i as a permutation  X  i ,where  X  i ( x ( i ) j ) stands for the position of item x ( i ) j  X  T i .Denote P i = { ( x ( i ) x u  X  T i ,x ( i ) v  X  F i } as the set of pairs constructed between the set of top k items and the set of the rest n  X  k items. We relate the top-k ground-truth to relevance labels by defining y our paper, we use y ( i ) j = k +1  X   X  i ( x ( i ) j ), if x y j = 0, otherwise. That is, suppose k = 10, the relevance labels for the top 10 items are defined in a descending order from 10 to 1, while the relevance labels for the rest items are defined as 0.
In FocusedRank, we adopt a listwise loss to model the total order of top k items, and a pairwise loss to model the preference of top k items to the other items. The general loss function of FocusedRank on a query q i is presented as follows 2 .

L ( f ; q i )=  X   X  L list ( f ; T i , y i )+(1  X   X  )  X  L pair where L list stands for a listwise ranking model and L pair stands for a pairwise ranking model,  X  is a trade-off coef-ficient to balance the two terms. As examples, we com-bine three popular listwise ranking models (i.e. SVM MAP , AdaRank and ListNet) with three popular pairwise ranking models (i.e. RankSVM, RankBoost and RankNet) respec-tively to get three specific forms of FocusedRank, namely FocusedSVM, FocusedBoost and FocusedNet accordingly. (1) FocusedSVM: RankSVM plus SVM MAP
Both RankSVM [14] and SVM MAP [33] apply the SVM technology to optimize the number of misclassified pairs and the average precision, respectively. Therefore, we combine these two ranking models together to get a new Focuse-dRank method, named FocusedSVM. Specifically, RankSVM is adopted to model the pairwise preference of P i , and SVM is employed to model the total order of T i . Therefore, L and L pair in Eq. (1) has the following specific form, respec-tively.
 L L
Like RankSVM and SVM MAP , FocusedSVM can then be formulated as an optimization problem as follows. where z T i stands for any incorrect label and  X  is the same as that in SVM MAP . Similar to SVM, 1 2 w 2 controls the complexity of the model w ,and C is a trade-off parameter between the model complexity and hinge loss relaxations. (2) FocusedBoost: RankBoost plus AdaRank
Both RankBoost [11] and AdaRank [31] adopt the boost-ing technology to output a ranking model by combining the week rankers, where the combination coefficients are deter-mined by the probability distribution on document pairs and ranked lists respectively. Hence we combine these two
In application, L list and L pair should be normalized to a comparable range, and we adopt this trick in our experi-ments. ranking models together to get a new FocusedRank method, named FocusedBoost. Specifically, RankBoost is adopted to model the preference of P i , and AdaBoost is to model the total order of T i . Therefore, L list and L pair in Eq. (1) has the following specific form, respectively.
 where E ( f,T i , y ( i ) i ) stands for the IR evaluation to optimize.
As in RankBoost and AdaRank, the detailed algorithm is shown in Algorithm2. (3) FocusedNet: RankNet plus ListNet
Both RankNet and ListNet aim to optimize a cross en-tropy between the target probability and the modeled prob-ability. The probability of the former is defined based on the exponential function of difference between the scores of any two documents in all document pairs given by the scoring function f . The probability of the latter is the permutation probability of a ranking list using Plackett-Luce model [23], which is also based on the exponential function. Hence we combine these two ranking models together to get a new Fo-cusedRank method, named FocusedNet. Specially, RankNet is adopted to model the pairwise preference of P i ,andList-Net is to model the total order of T i . Therefore, L list L pair in Eq. (1) has the following specific form, specifically. addition, we have that where s  X  ( j ) denotes the score of the object at position j of permutation  X  . As aforementioned, traditional IR evaluation measures (e.g., MAP 3 , NDCG and ERR) are mainly defined on the abso-lute judgment, thus cannot be directly applied to the top-k ground-truth. To address this problem, we extend NDCG and ERR to  X  -NDCG and  X  -ERR by taking the position of items as the absolute label using the way defined in Sec-tion 3.2.1. As a result, the derived evaluation measures ac-tually emphasize the importance of the ordering of the top k items. We first give the precise definition of NDCG as follows. where r j is the relevance label of the item with position j in the output ranked list, and N l is a constant which denotes the maximum value of NDCG@ l given the query.

Using the notations in Section 3.2.1, we can extend NDCG to  X  -NDCG with the following definition. where N l is a constant which denotes the maximum value of  X  -NDCG@ l given the query. We first give the precise definition of ERR as follows. where n is the document number of a query, and r max is the highest relevance label in this query.

Similar as  X  -NDCG, we can extend ERR to  X  -ERR with the following definition.  X   X  ERR = where y ( i ) max is the relevance label in the top position, as defined in Section 3.2.1.
In this section, we empirically evaluate our proposed top-k labeling strategy and ranking model. Firstly, we con-ducted user studies to compare the effectiveness and effi-ciency of our top-k labeling strategy with traditional abso-lute judgment based on the dataset from the Topic Distilla-tion task of TREC2003. Secondly, we compared Focuse-dRank with its corresponding traditional ranking models and top-k ListMLE [30] based on both the ground-truths from the absolute judgment and the top-k ground-truths. The experimental results show the superiority of our label-ing strategy and ranking model to previous work.
Since MAP is mainly designed for binary judgment sce-nario, we omit the modification on it.
Five-grade judgment 13.87 11.78 O ( n )5 0
To study whether top-k labeling is  X  X asier X  to make than absolute relevance judgments, we compare the Top-k label-ing strategy (i.e., k = 10) with the popular five-graded ( X  X ad X ,  X  X air X ,  X  X ood X ,  X  X xcellent X ,  X  X erfect X ) absolute rel-evance judgment method. We investigate which one is a better judgment method under two basic metrics, namely time efficiency [26], and agreement among assessors [7].
We describe our experimental design from the following four aspects: Data Set: We adopted all the 50 queries from the Topic Distillation task of TREC2003 as our query set. For each query, we then randomly sampled 50 documents from its associated documents for judgment. Existing Web pages in corpus of TREC2003 were employed in labeling to avoid the time delay in downloading content from Internet. In TREC topics, most of the queries have clear intent in the form of query descriptions, which are also exhibited along with the queries in labeling for better understanding.
 Labeling Tools: We designed labeling tools for two judg-ment methods separately, i.e., the top-k labeling tool T 1and the traditional five-graded relevance judgment tool T 2. In T 1, a query with its description is shown at the top, and two associated Web pages are placed at the main area. An assessor is then asked to decide which one is more relevant. In T 2, a query with its description is shown at the top fol-lowed with five grade buttons, and a Web page is placed at the main area. An assessor is asked to decide which grade should be assigned to that page. A timer is introduced to both tools for computing time per judgment. Assessors can click the clock button to stop the timer if they want to have a break or leave for a while. This will ensure the computing accuracy.
 Assessors: There are five assessors participating our user study. These assessors are all graduate students who are familiar with Web search. They all received a training in advance on how to use the tools and on the specifications of the five grades.
 Assignment: To make the comparison valid, the assign-ment should meet the following requirements. Firstly, for each method, all the selected documents should be judged at least once to obtain a complete data set. Secondly, for each assessor, he/she is most likely to memorize some information on the documents for a given query after judging. There-fore, to compare the methods independently, we shall ensure that each assessor will not see the same query under differ-ent tools to minimize the possible order effect. Finally, each tool has to be utilized by all the assessors to avoid the possi-ble differences between individuals. Therefore, we adopted the following assignment to satisfy all the requirements: (1) Table 2: Assessor agreement for preference judgments in top-k labeling results Table 3: Assessor agreement for inferred preference judg-ments in five-graded labeling results all the 50 queries are divided into five folds { Q i } 5 i =1 each fold has 10 queries; (2) for i =1 ,..., 4, each assessor U i judges Q i with T 1and Q i +1 with T 2, and the assessor U 5 judges Q 5 with T 1and Q 1 with T 2. At least such two different assignments are needed to compute the agreement among assessors.
Two basic metrics are utilized to evaluate the labeling strategies. (1) Time efficiency: Time efficiency [26] is used to mea-sure the cost of labeling. It is dependent on the time per judgment and the complexity of judgment (the total num-ber of judgments for each query). Statistically, less time per judgment suggests easier judgment. (2) Agreement: Here we measure the agreement be-tween two assessors over all judgments as [7] to average out differences in expertise, prior knowledge, or understanding of the query. For comparison, we inferred preferences from the absolute judgements: if the judgment on item A was greater than the judgment on item B, we inferred that A was preferred to B (denoted by A B). A tie between A and B is denoted by A  X  B.

As shown in Table 1, it is obvious that the average time per judgment in absolute judgments is longer than that of the preference judgments in top-k labeling strategy, e.g. about 2  X  3 times. The results verifies the common sense that the preference judgment is easier than absolute judgment. Meanwhile, from the average number of judgments conducted on each query, we can find that the top-k labeling strategy will take more judgments than the absolute judgments, at a scale around the theoretical value log k (i.e. k=10). Most importantly, we can see that the total judgment time spent on each query is comparable between the two methods. The results indicate that by adopting the top-k labeling strat-egy, the complexity of pairwise preference judgment becomes similar to that of the absolute judgment. Therefore, it is fea-sible to use top-k labeling in practice.

The agreement among assessors for preference judgment in top-k labeling and for inferred preference judgment in absolute judgment is shown in Table 2 and Table 3, respec-tively. Each cell ( X 1 , X 2 ) is the probability that one asses-sor would say X 2 (column) given that another assessor said X (row). Therefore, they are row normalized. From the results, we can see that by adopting preference judgment, top-k labeling can largely improve the agreement among assessors over the absolute judgment. The overall agree-ment among assessors reaches 74 . 5% under top-k labeling, while it is only 54 . 7% under absolute judgment. We con-ducted  X  2 test to compare the ratio of the number of pairs agreed on to the number disagreed on for both top-k la-beling and absolute judgment. The difference is significant (  X  2 = 420 . 7 ,df =1 ,p &lt; 0 . 001). We also investigate the agreement among assessors only on preference pairs (by ig-noring ties), where the agreement ratio is 89 . 7% under top-k labeling, and 83 . 1% under absolute judgment. We test the difference between the two methods using the ratio of agreed preference pairs to disagreed preference pairs, which is also significant (  X  2 =28 . 5 ,df =1 ,p&lt; 0 . 001).
From the above results, we can conclude that the top-k labeling strategy is both efficient and effective to obtain reliable judgments from human assessors, as compared with traditional absolute judgment.
In this section, we empirically evaluate the performance of our proposed ranking model, i.e. FocusedRank. Specially, we conducted extensive experiments to compare FocusedRank with different state-of-the-art ranking models based on both the ground-truths from the absolute judgment and the top-k ground-truths. Note that k is set to 10 in our experiments. Besides, we also investigated the impact of the balance factor  X  in our proposed FocusedRank.
For comparison, we constructed two datasets, each with both the absolute judgment and top-k labeling. One dataset comes from the benchmark LETOR4.0 collection. There are two homologous datasets with different labeling in LETOR4.0, one is referred to as MQ2007 with three-graded relevance judgments, and the other is MQ2007-list with the total or-der judgments as the ground-truth. The two datasets share the same queries. The only difference lies in that the docu-ments of a query in MQ2007 is the subset of the documents of the corresponding query in MQ2007-list. Thus the in-tersection of two document sets on each query is adopted. Those from MQ2007 comprise the ground-truth with abso-lute judgments, referred as Graded MQ2007. While those from MQ2007-list become the to p-k ground-truth by only preserving the total order of top k documents on each query, referred as top-k MQ2007.

The other dataset is the one manually constructed in pre-vious user study experiments with 50 queries from the TREC-2003 Topic Distillation task. The one with the five-graded absolute relevance judgments is denoted as Graded TD2003, and the one with top-k labeling is denoted as Top-k TD2003.
We divided each dataset into five subsets, and conducted 5-fold cross-validation. In each trial, three folds were used for training, one fold for validation, and one fold for testing. For RankSVM and SVM MAP the validation set in each trial was used to tune the coefficient C . For RankNet and List-Net it was used to determine the number of iterations. For our FocusedRank, the validation set was used to tune the balance factor  X  .
Besides, in our experiments, when applying traditional ranking models on top-k ground-truths, we relate the top-k ground-truth to absolute labels as defined in Section 3.2.1. While applying top-k ranking models (i.e. FocusedRank and top-k ListMLE) on absolute judgment datasets, we ran-domly generate a total order of the documents according to graded labels and preserve the top-k order for learning.
To measure the effectiveness of ranking performance, NDCG [13] and ERR [8] are used on ground-truths from absolute judgments, while  X  -NDCG and  X  -ERR are adopted on top-k ground-truths.
The performance comparison between different ranking models on the two datasets is shown in Table 4 and Table 5, respectively.

From the results on the Graded MQ2007 as shown in the upper part of Table 4, we can see that the overall perfor-mance of FocusedRank is comparable with traditional pair-wise and listwise ranking models in terms of both NDCG ( N @ j ) and ERR. It shows that even though FocusedRank is proposed for the top-k ground-truth, it can work quite well on traditional absolute judgment datasets under tradi-tional IR measures. Such results also reveals that, learning the ordering of the top items well is critical for the success of a learning to rank algorithm. Similar results can also been found on the Graded TD2003 datasets as shown in the upper part of Table 5.

From the results on top-k ground-truths in both tables (i.e. the bottom parts), we can find that FocusedRank can significantly outperform the corresponding pairwise and list-wise ranking models in terms of both  X  -NDCG (  X  -N @ j )and  X  -ERR. For example, considering FocusedBoost, the relative improvement over AdaRank and RankBoost is about 7 . 08% and 0 . 87% in terms of  X  -NDCG@10, respectively, and the relative improvements in terms of  X  -ERR is about 9 . 76% and 0 . 91%, respectively. Besides, we can also observe that un-der all the metrics, the best performance is almost reached by our FocusedRank (The best performance is denoted by number in bold). The results indicate that FocusedRank is particularly suitable for the top-k ground-truth. By combin-ing the advantages of both pairwise and listwise approaches, FocusedRank can fully exploit the information in the top-k ground-truth and thus outperforms each single model.
Moreover, when we compare FocusedRank with the state-of-the-art top-k ranking model, i.e., top-k ListMLE, we can see comparable performances on both absolute judgment datasets and top-k ground-truths. In fact, some of our Fo-cusedRank model, e.g. FocusedNet, can consistently outper-form top-k ListMLE on Top-k MQ2007 in terms of both  X  -NDCG and  X  -ERR. The results demonstrate that Focuse-dRank, as a mixed ranking model, can effectively cope with the top-k learning to rank problem.
Here we investigate the impact of the balance factor  X  in FocusedRank. By varying  X  from0to1withastepof 0 . 05, the curves of the ranking performance of FocusedRank in terms of  X  -NDCG 4 and  X  -ERR are shown in Figure 1 and Figure 2. Each performance value on test sets shown in the figures is averaged using five-fold cross validation as the same way used in LETOR.

In Figure 1, the performance variations on graded MQ2007 are represented as curves with open symbols while that on
For space limitation, we just show the results of @5,@10 for NDCG and  X  -NDCG. evaluation measures.
 Top-k MQ2007 are represented as curves with filled sym-bols. The results of FocusedSVM, FocusedBoost and Fo-cusedNet are shown in Figure 1(a), (b) and (c), respec-tively. To make the variation trend more clear, each figure adopts double y -axes, where the black curves of  X  -NDCG@5 (  X  -N@5),  X  -NDCG@10 (  X  -N@10) and  X  -ERR use the left y axis, while the other blue curves of NDCG@5 (N@5), NDCG@10 (N@10) and ERR utilize the right y axis. Simi-larly, in Figure 2 we also use double y -axes. The performance variations on graded TD2003 are represented as curves with open symbols while that on Top-k TD2003 are represented as curves with filled symbols. The results of FocusedSVM, FocusedBoost and FocusedNet are shown in Figure 2(a), (b) and (c), respectively.

From the results shown in Figure 1 and Figure 2, we find that: (1) There is a consistent trend 5 for the three types of FocusedRank on the two groups of different datasets. That is, as  X  increases, the ranking performance first grows to reach its maximum, and then drops. (2) The overall vari-ation of each performance curve is small. Take the most varied curves for example, the variance of the mean is 2.7% for the performance curve of  X  -NDCG@5 on Top-k MQ2007, and the variances of the mean is 6.2% for that on Top-k TD2003.
 Thus, we come to a conclusion that the performance of FocusedRank is relative stable with respect to  X  .
In this paper, we propose a novel top-k learning to rank framework, including labeling, ranking and evaluation, which can be effectively adopted for real search systems. Firstly, a top-k labeling strategy is proposed to obtain reliable rele-vance judgments from human assessors via pairwise prefer-ence judgment. With this labeling strategy, we can largely reduce the complexity of pairwise preference judgment to O ( n log k ). Secondly, a novel ranking model FocusedRank is
The small size of the datasets may be the reason for the non-smooth variation curves with 50 queries on Graded TD2003 and Top-k TD2003, compared with more than one thousand queries on Graded MQ2007 and Top-k MQ2007. presented to capture the characteristics of the top-k ground-truth. Thirdly, two new top-k evaluation measures are de-rived to fit the top-k ground-truth. We verify the efficiency and reliability of the proposed top-k labeling strategy through user studies, and demonstrate the effectiveness of top-k rank-ing model by comparing with state-of-the-art ranking mod-els.

There are many interesting issues for further investigation under our top-k learning to rank framework. (1) The top-k labeling strategy could be improved to further reduce the judgment complexity. For example, we may introduce the  X  X ad X  judgment like [7] for pages that are clearly irrelevant to further save labeling effort. (2) With top-k ground-truth, the design for new ranking models remains a valuable prob-lem to investigate. (3) It is also possible to find new top-k based evaluation measures for better comparison between different systems. This research work was funded by the National Natural Science Foundati on of China unde r Grant No. 60933005, No. 61173008, No. 61003166 and 973 Program of China under Grants N o. 2012CB316303. [1] N. Ailon and M. Mohri. An efficient reduction of [2] C.BuckleyandE.M.Voorhees. Retrieval system [3] C. Burges, T. Shaked, and et al. Learning to rank [4] R. Burgin. Variations in relevance judgments and the [5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. measures. [6] B. Carterette and P. N. Bennett. Evaluation measures [7] B. Carterette, P. N. Bennett, D. M. Chickering, and [8] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [9] S. Cl  X  emen  X  con and N. Vayatis. Ranking the best [10] D. Cossock and T. Zhang. Subset ranking using [11] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [12] E. P. C. III. The optimal number of response [13] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [14] T. Joachims. Optimizing search engines using [15] J. Kek  X  al  X  ainen. Binary and graded relevance in ir [16] A. Moffat and J. Zobel. Rank-biased precision for [17] L. P., B. C., and W. Q. Mcrank: learning to rank [18] T. Qin, T.-Y. Liu, and et al. Letor: A benchmark [19] T.R.,S.W.Jr.,andV.J.L.Towardsthe [20] K. Radinsky and N. Ailon. Ranking from pairs and [21] F. Radlinski and T. Joachims. Query chains: learning [22] F. G. Rebecca and N. Melisa. The neutral point on a [23] P. R.L. The analysis of permutations. Applied [24] M. Rorvig. The simple scalability of documents. [25] C. Rudin. Ranking with a p-norm push. In COLT , [26] R. Song, Q. Guo, R. Zhang, and et al.
 [27] E. M. Voorhees. Variations in relevance judgments and [28] E. M. Voorhees. Variations in relevance judgments and [29] E. M. Voorhees. Evaluation by highly relevant [30] F. Xia, T.-Y. Liu, and H. Li. Statistical consistency of [31] J. Xu and H. Li. Adarank: a boosting algorithm for [32] Yao. Measuring retrieval effectiveness based on user [33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [34] B. Zhou and Y. Yao. Evaluating information retrieval
