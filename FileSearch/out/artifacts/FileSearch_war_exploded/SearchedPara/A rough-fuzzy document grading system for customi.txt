 1. Introduction
The World Wide Web, with its large collection of documents, is a storehouse of information for any user. Search engines help users locate information. But these search engines usually return a huge list of url  X  s which are ordered according to a general relevance computation function. Most of the users find a large proportion of these documents to be irrelevant. However, since no two users usually have identical perspectives it is very difficult to find a general relevance computation function that can satisfy all users simultaneously. It is also not feasible to load a server with profiles of all the clients to serve them better. A viable way to provide relevant documents to every user is to use client side information filtering systems, which can learn a client  X  s perspective and grade documents according to a relevance function specific to the client.

In this paper, we have presented the design of a client-side text information filtering system based on a rough-fuzzy reasoning paradigm. This can pro-actively filter out irrelevant documents for a user, in his or her domain of long-term interest, after learning the user  X  s preferences. To begin with, the user rates a set of training documents retrieved as a result of posing a query to a standard search-engine. The user response is then analyzed to formulate a modified query which represents the user  X  s interests in a more focused way. This modified query is again fed to the search engine and has been found to retrieve better documents. However, since these documents are also ordered by the grading scheme of the search engine, their ordering do not still reflect the client  X  s preferences. A rough-fuzzy grading scheme is thereafter employed to re-evaluate these documents and order them according to the user preferences.

The most unique aspects of this work are:  X  The use of discernibility to represent the user  X  s relevance feedback.  X  We have presented how a client-side user relevance feedback based text retrieval system can be designed using rough-fuzzy reasoning. Most of the existing systems of this category use prob-abilistic reasoning (Intarka, Inc., 1999; Pazzani, Muramatsu, &amp; Billsus, 1996). Rough-Fuzzy reasoning paradigm helps in modeling natural language based information more elegantly through the use of equivalence relations.

The remaining paper is organized as follows. Section 2 presents a brief review of related work on text-filtering systems in general and also on application of rough-set theory to text-information retrieval. Sections 3 X 7 present the details of the various modules of our system. Section 8 provides some results and their analysis. 2. Review of related work
Significant work has been done towards building client side text retrieval systems based on user ratings. In this section, we first provide a brief overview of these. Later in this section we present some of the recent developments in applying rough sets for text information retrieval. 2.1. User preference based text information retrieval
User preference plays a major role in text information retrieval. Different schemes are used to store and analyze user preference. Currently, many interactive systems are being designed to provide better interfaces, simple interaction metaphors and learn user  X  s preferences. WebPlanner (Jochem, Ralph, &amp; Frank, 1999) is a system that guides users towards their needs by using structured domain-specific queries. However, it is not feasible to have domain-specific structured queries to be stored for all possible domains.

A better way to provide individual satisfaction is to use a dynamic representation for user  X  s preferences. One of the important advances in this area is the consideration of user profiles. User profiling aims at determining a representation of the user preferences so that the stored values may serve as input parameters for a filtering action operating on the available offer (Roldano, 1999). User profiling is often coupled with learning from user feedback. Relevance feedback by the user provides an assessment of the actual relevance of a document to a query. This can be utilized effectively to better the performances of retrieval mechanisms (Bodoff, Enache, Kambil, Simon, &amp;
Yukhimets, 2001; Korfhage, 1997). A relevance feedback method is either based on a document-oriented view or a query-oriented view. In the document-oriented view (Salton, 1971), the users  X  feedback are used to change the document  X  s internal representation to the search engine. This allows the return of similar documents to a set of similar queries that are posted by different users.
However most of the recent IR systems are built using the query-oriented view. Systems built around this model use the user feedback to modify the initial query posted by the user and tries to improve the retrieval performance. This method was initially proposed by Rocchio (1971). Salton,
Fox, and Voorhees (1985) proposed a query expansion technique based on the extended Boolean model. Crestani (1993) proposed a neural-network based method which learnt the user preference through adjustment of initial weights to get the desired performance. Allan, Ballesteros, Callar, and Croft (1995) presents an overview of TREC experiments related to query expansion. Fuhr and Buckley (1993) use a massive expansion of query using co-occurrence of words in good documents.  X  X  X rospectMiner X  X  (Intarka, Inc., 1999) is another retrieval system that learns user  X  s interests based on a user rating. The retrieval system suggests better queries that will fetch more relevant pages. The software agent also takes into account the co-occurrence and nearness of the words. Apart from the document rating, the retrieval system requires a term-feedback from the user and maintains a thesaurus with respect to the words present in the initial query.
A third approach uses the relevance feedback from the user to eliminate bad documents in future. These are usually installed at the client side and are client-specific.  X  X  X yskill &amp; Webert X  X  (Pazzani et al., 1996) is a client-side software agent that learns to rate web pages based on the user  X  s rating of a set of training pages. The system converts HTML source codes of a web page into a boolean feature vector of words, indicating the presence or absence of words. It then analyses the user  X  s feedback to determine the words to be used as features by finding the expected information gain that the presence or absence of a word W gives toward the classification of elements of a set of pages. A Bayesian classifier is used to determine the degree of interest of a new page to the user. Balabanovic  X  s Fab system (Balabanovic, 2000) recommends web sites to users based on a personal profile that has been adapted to the user over time. Individual user ratings of pages are used to generate the user  X  s profile adaptively, so that the recommendations gradually become more personalized. These systems have used probabilistic measures for judging the rele-vance of a document to a user. 2.2. Rough-set based text information retrieval
The systems presented above mostly worked with two-valued crisp logic to reason with user preferences. The chief problem with this approach is that it cannot handle complexities of natural language like synonymous words or polymeric words etc.

Rough-set based reasoning technique proposed by Pawlak (1982) provides a granular approach to reasoning. Rough sets are a tool to deal with inexact, uncertain or vague knowledge. Specifically, it provides a mechanism to represent the approximations of concepts in terms of overlapping concepts. Stefanowski and Tsoukias (2001) have shown how rough reasoning can be applied to classify imprecise information. Srinivasan, Ruiz, Kraft, and Chen (2001) and Das-Gupta (1988) have proposed the use of rough-approximation techniques for query expansion based on this model. Bao, Aoyama, Du, Yamada, and Ishii (2001) have developed a hybrid system for document categorization using latent semantic indexing and rough-set based methods. This system extracts a minimal set of co-ordinate keywords to distinguish between classes of documents. Chouchoulas et al. have shown the applicability of rough-set theory to the information filtering by categorizing e-mails in Chouchoulas and Shen (2001). Jensen et al. have used rough-set theory for automatic classification of WWW bookmarks in Jensen and Shen (2001). Menasalvas, Millan, and
Hochsztain (2002) have provided a rough-set based analysis of affability of web pages. They have also used rough-set based approaches to compute user interest parameters for web usage mining.
Since documents cannot be categorized uniquely on the basis of presence or absence of words, we argue that a rough-reasoning scheme is very appropriate to design text-retrieval systems. We have used the rough-theoretic concept of discernibility to analyze a set of user feedback. This analysis provides us with an enhanced query which better represents the user interest. Using rough sets allows us to handle synonymous words very efficiently. However, since the relationships among various concepts in the real world are vague, so a mechanism is needed to model the various degrees of equivalence (Srinivasan et al., 2001; Szczepaniak &amp; Gil, 2003). We have thus opted for a rough-fuzzy approach to design a text retrieval system which uses the user  X  s interests to rate a set of new documents effectively for the user. 3. A brief overview of rough-set based reasoning for text information retrieval Rough sets were introduced by Pawlak (1982). An information system can be defined as a pair
A  X  X  U ; A  X  where U is a non-empty finite set of objects called the universe and A is a non-empty information system is called a decision system if it has an additional decision attribute. The core of all rough-set based reasoning contains an equivalence relation called the indiscernibility relation. For any B A , the equivalence relation IND A  X  B  X  is defined as:
This relation is called a B -indiscernibility relation. We denote the equivalence classes of this re-used is the synonymy relation which establishes equivalence of two synonymous words. Thus two texts can be said to be roughly similar if they contain synonymous words but not necessarily the same words.

The equivalence classes obtained from the indiscernibility relation are used to define set ap-proximations.

Let B A and X U . Let U be represented by the collection of disjoint equivalence classes with respect to the relation R , i.e., U  X f C 1 ; C 2 ; ... ; C proximation space. The lower approximation of X , denoted by apr and the upper approximation of X , denoted by apr R  X  X  X  is defined by the set The objects in apr in B , while the objects in apr R  X  X  X  can only be classified as possible members of X . In crisp set theory, the similarity of two subsets can be defined as their degree of overlap. In rough-set theory, two subsets of the universe can be compared with respect to an indiscernibility relation using their approximations. 3.1. Rough similarity measures for text documents
We will now state some rough similarity measures introduced in (Srinivasan et al., 2001) to compute document overlaps. Two approximation spaces are first introduced to measure the de-gree of overlap between two subsets. Let S 1 and S 2 represent two subsets, which are collections of weighted words. Let S 1 represent the words in a retrieved document and S a query. One would be interested to find the similarity between the query and the document in order to judge the relevance of the document with respect to the query. Let R denote the syn-onymy relation and S be a set of weighted words. Let l S  X  y  X 2 X  0 ; 1 denote the degree of synonymy of the word y with x . The lower and upper fuzzy approximations of the word x are defined as follows:
Eq. (4) helps us in finding the word y which has the minimum/maximum degree of equivalence with x . Since the presence of a word can be related to the presence of its synonyms also, where the degree of equivalence can be judged by the degree of their synonymy, one can compute rough approximations for the entire set of words in a query S using the degree of synonymy of two words.

Let l R  X  x ; y  X  denote the degree of synonymy between words x and y . Then fuzzy lower and upper approximations for the word x is computed as
The two functions are now combined to give:
The approximations for a set S are computed as union of the fuzzy approximations of all the words occurring in it. These approximations are then used to find the similarity between two-weighted set of words S 1 and S 2 , where as defined earlier S and S 2 represents those in the query. We will explain in the next few sections how we have used these concepts of rough theoretic analysis and enhanced them to perform customized text in-formation filtering by our proposed system. 4. Architecture of the customized text filtering system
In our system text documents are represented as weighted vector of words like that used by google (Google Search Engine Optimization, 1999). The information system for classifying documents is constructed by taking words to represent attributes, and their weights in documents to represent the values of these attributes. The information system is converted to a decision system by including the user relevance feedback for each document.

This decision table is analyzed using rough-set based reasoning techniques to generate a user profile and provide a basis for more focused relevance computation, which can eliminate irrele-vant documents for the user effectively in future. Fig. 1 gives a schematic view of the complete system. The system works in co-operation with a backend search engine. Once the user specifies a query to the search engine, a training set is formed with a subset of the top graded documents to get the user feedback. The user is asked to rate each training document on a three-point scale in which 1 stands for bad, 2 for average, and 3 for good. The system then starts analysing the training set for generating the user profile and the grading scheme for future documents. Grossly, the functions of the various modules are as follows:
Query modifier  X  X  X his module generates the modified query that can retrieve better documents for the user. Every training document which is rated by the user is converted to a weighted vector of words. The most unique aspect of our system is the introduction of the rough-set theoretic concept of discernibility to identify words which help in distinguishing between relevant and ir-relevant documents. A discernibility matrix is constructed based on the user  X  s rating and the weighted word vector for each training document. The matrix is scanned to extract the set of most discerning words which yields the modified query . We propose to identify the most discerning words from a document as follows: i (i) Words which are present with a high relative importance in good documents and are not pres-(ii) Words which are present with a high relative importance in bad documents and have low im-
We use the positive and negative discerning words to formulate an improved search query for the user. The details of this are presented in Section 5. We will show that this can also imbibe the user  X  s preference. When this modified query is fed to the search engine, it is observed that the documents returned are in general better. However, the relative ordering of the documents is influenced by the relevance computing function of the underlying search engine. The documents are therefore graded according to user preferences.

User preference analyzer  X  X  X his module uses rough similarity measures to learn the user pref-erences for rating documents. Srinivasan et al. (2001) had introduced some similarity measures for queries and documents, though it was not indicated how these measures can be actually used to judge the relevance of a document. We observed that the values of similarities do not provide any indication about the actual relevance of a document, rather it is the range of the similarity values and their co-relations, that provide indication about the quality of documents in a domain specific way. Based on these observations, we have developed a complete grading scheme using these measures. The modified query is used as the focus of comparison. A new decision system is constructed using the ratings of the training documents and their lower and upper similarity measures with respect to the modified query. This table is then used to extract fuzzy grading rules, which relate the similarity measures to the user rating decisions. Section 6 presents the details of the scheme.

Grader  X  X  X his module grades the documents retrieved with the help of the modified query, using the fuzzy rules extracted by the user preference analyzer. Now rough similarity measures bet-ween the modified query and each new document is computed, keeping the query as the focus of comparison. The classification rules are then employed to assign a rating to the document.
The rating is expressed both as a crisp decision value and a fuzzy membership to a decision class.

We explain the detailed design of each of these modules in the next section. 5. Query modifier X  X  X orming modified query with most discerning words
As stated in the earlier section, every training document is converted to a weighted vector of words appearing in the document. To calculate the weights of the words, we use the HTML source code of the pages. Since each tag like  X  TITLE  X  ,  X  B  X  etc. in an HTML document has a special significance, we assign separate weights to each one of them. We have given tag weights in the range of 1 X 10, with 10 for  X  TITLE  X  , then 8 for  X  META  X  , 6 for  X  B  X  etc. The plain words have a weighing factor 1. The weight of a particular word s is then computed as follows: where w i represents the weight of tag i and n i represents the number of times the word s appears within tag i , and m is the total number of tags (including no tag) considered. The weights are normalized by dividing the weight of each word in a document by the maximum weight of a word in that document. We use a word vector of size n , where n P 30 to represent the document. All our results have been obtained with n  X  50, as no significant improvement has been observed with n &gt; 50. We now take a look at the decision table constructed with the weighted word vectors.
Suppose the user has rated the documents D 1 , D 2 , D 3 , and D
Further, suppose that D 1 , D 2 , D 3 , and D 4 have words W Table 1.

Table 1 shows that the word W 3 does not have the capacity to distinguish between  X  X  X ood X  X ,  X  X  X ad X  X  or an  X  average  X  document since it has a high weight in all of them. On the other hand, the word W 1 has the potential to distinguish a  X  bad  X  document from a  X  good  X  one. Thus we can say that
W 1 may be a negatively discerning word. Similarly it may be argued that W discerning word.

If we can extract the most discerning words using the decision table, these can be used to formulate a modified query as follows. The query is constructed as a Boolean function of all positive and negative discerning words. Positive discerning words are indicated as desirable words in the documents while negatively discerning words are indicated as undesirable. For example, a query  X  X  W 2  X  W 4 W 1  X  X  would indicate that we want documents which contain W not contain W 1 . Now we will explain how the most discerning words can be extracted from the decision table.

Let us suppose the number of distinct documents in the training set is N and the number of distinct words in the entire training set is k . We will now show how the discernibility table (Komorowski, Polkowski, &amp; Andrzej, 1999) for this set is constructed. For each distinct word in the domain, its weights (in different documents) are arranged in ascending order. An interval set P is then constructed for the word s , which is defined as s is thus associated with a set of cuts.
Since each word may not be present in all the documents, the number of intervals and therefore the number of cuts may be different for different words. Let us suppose word s the total number of cuts for the entire set of words is
Let D T denote the discernibility table. D T is constructed with help of decision Table 1 and the cuts. D T has one column for each cut induced over D T , and one row for each pair of documents ( D D
T is decided as follows: v both the documents are on the same side of the cut . v word in document j is less than the cut , and the documents have different decisions d spectively .

Thus, a non-zero entry corresponding to a word s in D T , indicates that the word has two different significance levels in two documents of different decisions. The absolute value of the entry determines the power of the word to distinguish between two different categories. A negative value indicates that the word has a higher weight in a bad document than in a good document, which means that the word may be a negatively discerning word. Table 2 denotes the discernibility table D T constructed from the decision table presented in Table 1.

Finally, we will now analyze D T , to get the most discerning words and the corresponding values of the cuts. Since, theoretically, there can be an infinite number of cuts possible, one can apply the
MD-Heuristic algorithm presented in Komorowski et al. (1999) to obtain the minimal set of maximal discerning cuts.

However, since the original MD-Heuristic algorithm works with a discernibility table in which all decision differences were considered as identical, we have modified this algorithm to find the most discerning words. For this, we first consider those columns which induce the highest degree of difference in decision, followed by the next highest and so on, till there are no more discerning words in the set.
 The steps in the modified MD-Heuristic algorithm followed by us are:
Step 1: Let W denote the set of most discerning words. Initialize W to NULL. Initialize T  X  r ,
Step 2: For each entry in D T consider the absolute value of the decision-difference stored there. If
Step 3: Considering the absolute values of decision difference, choose a column with the maximal
Step 4: Select the word w and cut c corresponding to this column. In case of a tie, the leftmost
Step 5: If majority of the decision differences for this column are negative, then the word is tagged Step 6: Add the tagged word w and cut c to W .
 Step 7: If there are more rows left, then go to step 2. Else stop.

This algorithm outputs a list of words along with their cut-values, which collectively discern all pairs of documents rated by the user. The presence of the positively discerning words and the absence of negatively discerning words are desirable in good documents. A modified query is constructed using these words and Boolean operators. The modified query is fed to the search engine again. It is observed that performance improves significantly. However, some irrelevant documents are still retrieved and the list is not ordered according to the user preference. In the next section, we will elaborate on how the irrelevant documents can be filtered out from this set. 6. User preference analyzer X  X  X earning the users basis for rating
To help the system rate the newly fetched documents and eliminate irrelevant ones, it is es-sential to learn the user  X  s rating paradigm. For this we make use of rough similarity measures between the modified query and the original documents that were rated by the user. Let S the set of words along with their weights, extracted from a document as explained in Section 5. Let
S 2 denote the set of most discerning words along with their discerning cut values. Using Eqs. (5) and (6) of Section 3.1, one can obtain the lower and upper approximations for each word. The equivalence relation R used is the  X  X  X ynonym X  X  relation. In general, the synonym dictionary is constructed using WordNet and each word occurring with a different sense is assigned the same weight. However, the dictionary does vary to some extent according to the domain. For example the word  X  X  X an X  X  which is usually a stop word for most of the domains, becomes a word synon-ymous to  X  X  X ustbin X  X  for the domain  X  X  X ir pollution X  X .
 Let apr spectively. These approximations can be computed using Eq. (5) as follows: Here l R  X  x ; y  X  is the degree of synonymy between words x and y , while l word x in the set S . The difference in the lower and upper approximations for the sets S senting the document, with respect to the lower and upper approximations of the set of most discerning words represented by S 2 are computed as follows: where jj represents the bounded difference. B l is called the lower approximation of subset S
S and B u is the upper approximation of subset S 2 with S 1
With these approximations, the similarity of two subsets S (2001) as where (10) denotes the lower similarity and upper similarity of S focus in the comparison. In both the cases the value will be 0 for no match and 1 for maximum match between S 1 and S 2 . We have used the set of most discerning words which also serve as the modified query, as the focus of comparison.

Using Eqs. (9) and (10), we first compute the lower and upper similarities between each of the old documents and the set of most discerning words. Since there is no apparent unique association between the similarity measures and the user  X  s rating, we decided to use a decision tree which can summarize the relationship as a set of rules. These rules typically relate the rating assigned to a document by the user to its similarity measures. The decision tree is constructed using the ID3 algorithm (Mitchel, 1997). Here are some typical rules generated by our system for the domain  X  X  X lcohol addiction X  X .

Rule 1 :If Lower similarity &gt; 0.027 and Upper similarity Rule 2 :If Lower similarity 6 0.01388 and Upper similarity &gt; 0.4305556 then Class  X  1 (bad) (4/2).
Rule 3 :If Lower similarity 6 0.01388 , Upper similarity &gt; 0.2777778 and
Rule 4 :If Lower similarity &gt; 0.01388 and Upper similarity
We found that around 10 or 11 rules were generated for each domain. 7. Fuzzy grading of documents
The rules generated by the preference analyzer are used to rate the new set of documents re-trieved using the modified query. For this we use a fuzzy reasoning scheme which provides both a crisp document grading as well as a fuzzy visualizer, to provide a qualitative idea about the relevance of a document.

Fuzzy reasoning consists of two core activities X  X  X diting the fuzzy input and output membership functions. To design the fuzzy input membership functions we have made use of the rules ob-tained earlier. The rules give us an idea about the cut-off values and the membership functions to be used for the input parameters i.e. the lower similarity and upper similarity and the class de-cisions. We have used the triangular function to represent the bad decision class, since the rate of change of quality of a document from bad to average or vice-versa is very steep. The average class is represented by the gaussian function, since it has a lower rate of change of quality. Finally we use the sigmoidal function to represent the good decision class since it is a right open function and indicates that once the quality of a document is judged good it remains so. Fuzzy Logic Toolbox also suggests use of similar functions for modeling linguistic variables like low, medium, high. The relationships of these functions with the input parameters are extracted from the rules generated by ID3.
 To plot the bad decision function we consider all the ID3 rules that yield the class decision Bad.
For each of the input parameters  X  X  X ower similarity X  X  and  X  X  X pper similarity X  X , we feed the range of these parameters for the Bad class as obtained from the rules. These ranges along with the type of the membership function used to represent the decision, generate the ultimate membership function curve for the class. For example the minimum and maximum values of lower similarity for the Bad decision class for the domain  X  X  X lcohol addiction X  X  were obtained as (0, 0.01388).
Similarly the least and maximum values for the upper similarity for the Bad decision class for the same domain are (0.027, 0.43). Since the membership function type for this is the triangular function, Fig. 2 shows the corresponding curve that was generated for the bad decision class.
Membership function curves for the other decision classes are also chosen accordingly. We have used the MATLAB Fuzzy Logic Toolbox to generate the fuzzy membership values for the doc-uments. Fig. 2 shows the example functions for the domain  X  X  X lcohol addiction X  X .

To rate a new document, it is first converted into a vector of weighted words. Using the modified query as the focus of comparison, we now compute the lower and upper similarity measures between the modified query and the new document using Eqs. (9) and (10). On feeding these values to the membership editor of the Fuzzy Tool Box, we get the membership of each document to all the three decision classes X  X  X ood, average and bad. The user is presented with a graphical representation of the fuzzy membership values of each document, which is also gen-erated by the Tool Box. This gives an intuitionist feel of the document to the user. 7.1. Crisp rating of documents
For each document, a crisp membership value to each individual category may also be ob-tained. The document may be awarded the class with the maximum membership value. The url  X  s are then re-arranged according to their crisp system ratings. Thus the good url  X  s are presented first followed by the average and the bad url  X  s. Using this approach, the system can filter out irrelevant documents by eliminating the ones which have maximum membership to the bad decision class altogether. This approach is also quite useful since the user does not have to take a look at ir-relevant documents at all.

To determine the efficacy of this approach we had requested the users to rate the new docu-ments also and then compared the user evaluations with our results. In the next section we present results obtained with different queries and present the success rate of the system grading scheme. 8. Results
In this section we will present some performance analysis of our system. We have worked with queries some of which like  X  X  X IV X  X  and  X  X  X lcohol addiction X  X  were chosen because they had been mentioned in TREC topics. TREC mentioned  X  X  X rain cancer X  X  as a topic. But we worked with  X  X  X lood cancer X  X  since we had less expertize in rating the other topic. Similarly, rather than  X  X  X hailand tourism X  X  as mentioned in Chakrabarti et al. (1998) we chose  X  X  X ndian Tourism X  X  as a domain. We chose a new query  X  X  X lternative medicine X  X  since many users have interest in this topic though from different perspectives. Table 3 shows the top 10 url  X  s obtained with the initial query  X  X  X lcohol addiction X  X . However, as we can see, the user has rated 5documents as bad and 5 documents as average out of 10. Using 50 documents retrieved from this query, we now find the most discerning words.

Table 4 shows these initial and modified queries along with ones we obtained for different other domains. Columns 2 and 4 of Table 4 show the percentage of bad documents among the top 50 the two queries need not be same. Usually, the total number of documents retrieved also decrease with the modified query as shown in Table 4. This is because the query is more focused now.
Certain documents which were not retrieved earlier may be retrieved now, and similarly certain documents which were obtained with the initial query may not be retrieved now. Thus the top 50 documents may not be identical. We see that while 50% of the top 50 documents were bad with the initial query  X  X  X lcohol addiction X  X , with the modified query containing the words shown in column 3, only 10% of the top 50 documents are bad. The reduction in bad documents is substantial in all the domains as the table indicates. The decrease in the percentage of bad documents with the modified query proves the effectiveness of the modified query.

Different groups of users were asked to evaluate different domains depending on their interest in the topics. For each domain, the same user(s) has been asked to rate the initial and final set of documents to maintain uniform standards of rating. In all the cases, authoritative pages con-taining good documentation about the topic were rated higher than hub pages containing links to other pages.

Next we present some results to show the working of the fuzzy grading scheme. Table 5presents the top ten retrieved documents using modified query from the domain alcohol addiction. Col-umns 3 and 4 of Table 5show the lower similarity and upper similarity of each document with respect to modified query. Columns 5 X 7 of Table 5 show the fuzzy membership values of each document to bad, average, and good decision classes respectively, using Fig. 2.
 Figs. 3 X  5present glimpses of some of the url  X  s and their fuzzy memberships are presented in
Table 5. These graphs give an idea about the quality of the documents to the user. Fig. 3 is corresponding to the url http://dmoz.org/Health/Addictions/Substance_Abuse/Treatment/
Alternative/ . This is a very informative url. We find that fuzzy membership value from the accompanying figure for Good category is also maximum. This url has a collection of links which give the information about alcohol addiction treatment, its prevention and important information about texts on alcohol addiction. Though this page is a collection of links, it provides adequate information about each link. Hence it gets a very high rating. Url numbers 2, 3, 6, 8, 9, and 10 in
Table 5, all have similar lower and upper similarity measures X  X  X ence all of them have identical fuzzy membership values.

Fig. 4 is corresponding to the url http://directory.google.com/Top/Health/Addictions/Sub-stance_Abuse/Resources/ . This url is an average document because it has collection of link which provides information mostly about drug abuse. Its fuzzy membership to different categories is also shown below it. The membership to the average category is maximum which corresponds to our judgement.

Fig. 5is corresponding to the url http://www.nada.org.au/links.asp . This url is a bad document because it provides information on education and training, funding, and government sites links on alcohol addiction. The membership values of this document to various categories also show that it is maximum for the bad category.
 To filter out irrelevant documents however, we need to get a single rating for each document. For that, we use the de-fuzzification technique. Table 6 shows the de-fuzzified value for the same documents. These values can be used to rate the documents. Column 4 of Table 6 shows the ratings assigned by the system to the documents using this technique.

In order to judge the accuracy of the grading, we compared the system-generated grades with feedback taken from the user for top 50 documents retrieved with the modified query. For this we requested the users to rate the newly retrieved documents also. Column 5of Table 6 shows the user assigned rating to the documents. We find that in most of the cases the system grading matches the user grading. However, the system is more harsh towards the average documents and in some cases average documents have been rated bad as Table 6 shows. It may be noted that this step is just for rating the system and is not an integral part of the system.
 Accuracy of system evaluation is defined as
Table 7 summarizes the accuracy of the system in various domains. The average accuracy of the rating scheme is around 80% for most of the domains. This establishes the effectiveness of the grading scheme. Since bad documents can be identified by the system, these can be eliminated from the list presented to the user. The accuracy was low for the domain alternative medicine since documents were very varied on different forms of alternative medicine like aromatherapy, yoga, acupuncture etc. in this domain.
 9. Conclusion
In this paper we have presented the design of a complete client-side filtering system for general text documents. The system uses the rough set theoretic concept of discernibility to find words that can discern between good documents and bad ones by analysing a set of training documents rated by the user. This scheme is more powerful than the usual techniques of computing term frequency and inverse document frequency, since it takes into consideration the synonymous words very elegantly. A modified query is built with the discerning words. This query is found to fetch documents, which are more relevant to the user. However, since the documents are still fetched by a traditional search engine, the ordering of the returned documents is still not cus-tomized for the user. Hence, we have proposed a rough-fuzzy reasoning scheme which grades the documents. The system first learns the user  X  s basis of rating by relating the grades to rough similarity measures between the training documents and the modified query. These associations are learnt using a decision tree and the classification knowledge is expressed as a set of rules. These rules are used to rate new documents and re-order them, on the basis of rough similarity measures between the new documents and the modified query. To obtain a performance analysis of the system, we requested the users to give their feedback about the retrieved documents also. The document grading scheme is found to work reasonably well.

The rough-set mechanism can be extended to automatic document classification also. The most discerning words for each category can be used as a signature for that category. Rough similarity measures can then be used for categorizing documents automatically. However, since the ranks in that case will not be a graded one, therefore the algorithm for finding words have to be modified.
We are also exploring the possibility of building domain specific question answering systems using rough-fuzzy reasoning.
 References
