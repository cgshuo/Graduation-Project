 Online ecommerce has been booming for a decade. For instance, as the largest online C2C marketplace (eBay), millions of new items are listed daily. Due to the overwhelming number of items, the process of finding the right items to buy is sometimes daunting. In order to address this problem, this paper describes the sold successfully. And adjust the item exposure chances proportional according to their conversion possibility. Hence, by ranking higher items that users are likely to buy, the chance that users make the purchases could be increased as well as their user satisfaction. For catalog products th at have been listed repeatedly, this probability can be measured empirically. However, on C2C sites like eBay, lots of items are not product-based. They are unique, and from different sellers. Th erefore, in order to predict whether a new listing will be sold, we collect a large scale item the average buyer shopping decision on C2C sites. Experimental results verified our system's feasibility and effectiveness. I.2.6 [ Artificial Intelligence ]: Learning. I.6.3 [ Simulation and Modeling ]: Applications.
 Algorithms, Measurement, Ec onomics, Experimentation, Performance. Online shopping, ecommerce, conve rsion, ranking, regression model. Online ecommerce has been booming for a decade. EBay, the World's Online Marketplace, enables trade on a local, national and international basis. Peopl e from all over the world trade goods worth billions of dollars every year. EBay X  X  Gross Merchandize Volume (GMV), the total value of all successfully closed items, totaled over 14.28 billion dollars in Q3 of 2008. Everyday, millions of new items are listed to eBay. Due to the recent policy of decreasing insertion fees, more and more items are listed on the site. Consequently , it is increasingly difficult for buyers to make purchase decision. To address this problem, our goal is to predict whether a new listing will be sold successfully, so that they can be ranked higher on the list of search results. The motivation is two fold.  X  Buyers are more likely to s ee the items they are willing  X  From the business perspective, revenue of ecommerce To estimate the conversion pr obability of catalog products is quite straightforward. It could be calculated from historical sales data. For example, if  X  X Phone X  demand is high over the last few months, it may remain relatively high for a period of time. However, this task raises a challenge for C2C sites because majority of items are not products. Items could be very unique, with no similar item on C2C sites, for example, a vintage stamp or specialty pottery. Moreover, even for some product-based items like  X  X Phone X , it is hard to predict that given one  X  X Phone X  sale, another  X  X Phone X  will also be sold successfully, since these two  X  X Phone X  may be from different sellers, and buyers may treat them completely different. Therefore, empirical measurement could not be directly used to estimate conversion probability for items on C2C sites. In this paper, we address the pr ediction problem by analyzing user behavior on the site and simu lating how buyers make shopping decisions on C2C sites. When buyers face a list of candidate items, we believe there are some shopping criteria in their mind, and they make an overall consideration of all the aspects of items and sellers, and finally decide which one to buy. For example, buyers usually care about the item price a nd sellers X  historical feedbacks, etc. We explore a significant set of features which are potentially helpful to determine the conversion probability of items. By using regression models, we could get the weight for each feature and the conversion probability. Finally, the score could be used to rank items in applications, such as browsing and searching. For example, in Figure 1, the items with higher conversion probability could be ranked in the top when users are browsing the category  X  X lassware X . In fact, if the item conversion probability could be predicted, not only browsing and searching could be benefited, some other applications like r ecommendation and adverting, etc on C2C sites could also be more targeted than before. In order to carry out this work, we collect a data corpus which consists of hundreds of thousands of ended items from eBay site during the past several mont hs. Thorough experiments are conducted in order to test the validity of the models. We also use correlation analysis to understand the relationships between any two features. Finally, we prove that the precision of our prediction model outperforms significantly over the traditional time ending soonest ranking method. The rest of the paper is organized as follows. Section 2 introduces the related work. The detailed algorithm and features are described in Section 3. The expe rimental results and analyses are presented in Section 4. In Sec tion 5, we discuss some existing problems and future work. Finally, we draw a conclusion in Section 6. To predict whether an item would be sold is quite similar to the task of binary classification. Support vector machine (SVM) (e.g., [3] [12]) and logistic regression [8] are two well known models, and they have been widely used in the binary classification area. Junk email filtering (e.g., [4][10][15]) is a representative application of binary categorizati on. Sentiment classification (e.g. categorize user opinions to in to positive and negative sets. In this paper, we used a list of features and combined them with regression models to rank items by their output scores (conversion probability). Similar algorithms have been studied in several other research works. Zeng et al. [19] re-formalized the search result clustering problem as a supervis ed salient phrase ranking problem. Several properties, as well as several regression models, were proposed to calculate salience score for salient phrase. Yih et al. [18] described a system for phras e extraction and matching ads by those relevant keywords. The syst em used a variety of features and logistic regression to determ ine the importance of keywords for contextual advertising. Recent years, predicting the click through rate (CTR) of online advertisement tends to be anothe r important research topic. To displayed on a given page. The aim of their work is quite similar to ours, but instead of online ads we hope to estimate the conversion probability of items on ecommerce site. Regelson and Fain [14] estimated the CTR by cl ustering ads by their bid phrases. The click through rate was averaged over each cluster, and the CTR estimate for new ads was obtained by finding the nearest cluster. More recently, Richardson et al. [13] estimated the click phrases, landing page, and title). The research work about incorpor ating user behavior in search ranking algorithm has been well studied recent years. Joachims et al. [9] presented an empirical evaluation of interpreting click through evidence. By performing eye tracking studies and correlating predictions of their stra tegies with explicit ratings, the authors showed that it is possible to accurately interpret click through in a controlled, laboratory setting. At the same time, while recent work (e.g., [17]) on using click through information for improving web search ranking is promising, it captures only one aspect of the user interac tions with web search engines. Agichtein et al [1] explored the utility of incorporating noisy implicit feedback obtained in a r eal web search setting to improve web search ranking. In contrast to their work, we also analyze historical user behavior by the ended item set, but the predicted conversion probability could not only be used in search, but also in browsing, advertising and recommendation on ecommerce site, etc. To our best knowledge, there are not many research papers about sales prediction recently. Thiesing et al. [16] applied neural networks trained with the back propagation algorithm to predict the future values of time series that consist of the weekly demand on items in a supermarket. The in fluencing indicators of prices, advertising campaigns and holidays are taken into consideration. Garber et al. [7] demonstrated th e ability of the spatial divergence approach to serve as a robust, early-period prediction tool, in both simulated and actual field studies . This demonstration added the potential contribution of spatial analysis to new product theory and practice. Bayus [2] disc ussed a practical method for estimating hardware and software sales of such products. Effects due to different market segment behaviors, pricing, awareness levels, and purchase intentions were incorporated into the model. Results from a study of the compact disc prerecorded audio market by RCAlAriola were presented in order to illustrate how the model can be applied and how the results are useful in making managerial decisions. However, none of these publications referred to online ecommerce market. User behavior on ecommerce could be significant different from traditional offline transactions. When users are shopping on C2C sites, usually they view all kinds of aspects of items and ma ke decisions based on the overall performance. Therefore, to predict whether a new listing could be sold, we hope to build a model which could simulate how people make shopping decisions. The core of our prediction system is regression models trained via machine learning algorithms. Given an item candidate, the models calculate the conversion probability by utilizing a vector of item features. Furthermore, a set of confidence scores or probabilities computed by the model can be used to rank items. Hence, once users are browsing one category, items could be sorted by the than before. Given a set of features of each item, our approach is to combine them into a single ranking score. In our system, we use historical ended item data to learn regressi on models. Regression is a classic statistical problem which tries to determine the relationship between two random variables x = ( x 1 , x 2 , ..., x case, variable x is a vector of features to be described in the below be sold or not respectively. For their simplicity and effectiven ess, we utilize linear regression [11] and logistic regression [8] for the ranking task. Linear regression attempts to m odel the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. Where the "residual" e is a random variable with mean zero, and the coefficients b j (0  X  j  X  n ) are determined by the condition that the sum of the square residuals is as small as possible. Therefore the linear combination with b j should be better than those with any other coefficients. binomially distributed dependent va riables. It is a generalized linear model that uses the logit as its link function. Logistic regression is used extensively in the medical and social sciences. It is sometimes referred to as a logistic model, logit model, or maximum-entropy classifier. Logistic regression attempts to find coefficients b j (0  X  j  X  n ) to fit x to a logistic transformation of the probability, which is also called logit. Instead of using a least-squared de viation criterion for the best fit, logistic regression uses a ma ximum likelihood method, which maximizes the probability of getting the observed results given the regression coefficients. Formally, we hope to predict an output variable Y , given a set of input of input features, X . Y would be 1, if a candidate item is sold, and 0 otherwise. X would be a vector of feature values associated with an item. For example, the vector might include relative price of the item and feedback score of the seller who sells this item. The model returns the estimated probability, P( Y = 1| X = logistic regression model learns a vector of weights, w, one for each input feature in X . The actual probability returned is, Besides machine learning models, we need a list of features. We experiment with a large set of features, which are potentially useful to predict item conversi on probability. These features can be divided in two groups, one group consists of features which are features clearly, we use eBay as a representative of C2C sites. But the selected features in this paper could also be applicable in any other C2C sites. EBay maintains a large single-parent category tree, and there are totally almost 30,000 leaf categor ies on US site. The conversion rate for each leaf category is totally different. For some hot categories, above 95% of items could be sold, while in some categories, only less than 1% items could be sold. Hence, items listed in hot categories are more likely to be sold, and in this paper, category hotness is calculate d by the historical conversion rate of each category. Now, lots of items are found through eBay X  X  search engine. Intuitively, items with higher expos ure chances in search results are more likely to be sold. The query log of an ecommerce site search engine reflects the distribution of the keywords that people are interested in. By leveraging daily query log and te rms in item titles, item hotness is calculated by equation (5), Where the title of item i has j terms, and Q (term popular in query log, the items could be exposed frequently in the search result set, and it is relativ ely easy to be viewed and sold. The number of similar items in the same category is calculated for each item, and it could be considered as the number of competitors. Intuitively, business would become very hard, if many competitors are around. Ideally, items with higher demand but fewer competitors are easy to be sold. In our system, similar items are simply calculated by items X  title distance. Although, on C2C sites, lots of items are not product-based or even unique, we still hope to take advantage of the historical conversion rate of similar items. This is a very straightforward feature in prediction. Sim * ( item i ) indicates a subset of items that were sold successfully in the history. Intuitively, users prefer cheap items when they are shopping. Comparing item price within different kinds of items is meaningless, so we calculate the relative price of each item in its by equation (7). Where, P ( item i ) is the current price of item i , and the average price of similar items. There are mainly two sale types on eBay, one is auction and the other is fix price. We consider it as a binary feature, and test whether sale type is useful to determine conversion. The value would be 0 if the sale type of an item is auction, otherwise the value is 1. The quantity of each new item is also collected. This feature is to verify whether users prefer to buy an item with multiple quantities. Intuitively, buyers would feel more trustable if there are multiple items from the seller. On eBay, each item should be associated with a return policy. For example, under what circumstances can buyers return the item? Do buyers have to return the item to the seller within a certain time period? What kind of refund does the seller provide? Who pays for return shipping and handling? Does the seller charge a restocking fee? Return policy level for each item is calculated as an overall score. Best offer is a mechanism on eBay that provides buyers opportunities to negotiate the price with the seller, and buy the item at a lower price than the Buy It Now price. The seller can accept, decline, or counteroffer buyer X  X  Best Offer. In our algorithm, this is also a binary feature. If seller turns the best offer function on, this value is 1, otherwise it is 0. This is a binary feature. If an item is a store inventory, the value is 1, otherwise it is 0. 3.2.2.1 Seller Feedback Score 1 Feedback is an important part of most C2C sites. It is used to evaluate a seller X  X  reputation. Usually, buyers like sellers with good reputation. Seller feedback score is the total number of feedback which consists of both of positive feedback and negative feedback. http://pages.ebay.com/help/fee dback/scores-reputation.html Besides feedback score, we also utilize the percentage of positive feedback of sellers. It could be simply calculated as, means he/she is an experienced seller or even power seller. Briefly, the whole prediction system is composed by four steps.  X  Collect training data.  X  Select features and regression models.  X  Train the regression model, a nd obtain the prediction engine.  X  Input new items and display items to users by the order of Figure 2 shows an overview of the system architecture. After we build up the prediction engine, it could be very easily used in many applications. In order to capture training data for our model and validate the effectiveness of our system, we collected a large set of items and conducted a very comprehensive evaluation process. In this section, we first describe the data set. Second, we validate the set of features introduced in the previous section and evaluate their effectiveness with the different regression models as well as the validity of the models. Third, we provide the performance of each feature and analyze the correlation between features. Finally, we introduce a preliminary study to show the impact on revenue. We collected 720,076 items whic h were ended during May 2008 and June 2008 from eBay site. Among them, 335,977 items were successfully sold, and 384,099 items were not sold in that period of time. In order to verify the algorithm cross different domains, the item set consists of items from 33 domains on eBay US site. item features were normalized in leaf category level. The detailed break down is shown in Table 1. Cameras &amp; Photo 24,279 43 Cell Phones &amp; Clothing, Shoes &amp; Coins &amp; Paper Computers &amp; Dolls &amp; Bears 30,000 48 DVDs &amp; Movies 5,626 7 Entertainment Memorabilia 25,473 38 Everything Else 30,000 43 Health &amp; Beauty 30,000 45 Home &amp; Garden 30,000 44 Pottery &amp; Glass 30,000 47 Specialty Services 6,213 8 Sporting Goods 30,000 51 Sports Men, Cards Toys &amp; Hobbies 30,000 47 
Video Games 15,133 20 In the Section 3, we provided a se t of features and two regression models. In order to validate the models and each single feature, we utilized statistical tests. Fo r linear regression, we did two-tailed t-test for each single feature. For logistic regression, we did z-test for each feature. In additi on, all experimental results were obtained by 10-fold cross validation. Table 2. P-value of each feature in two regression models Category Hotness &lt; 2e-16 &lt; 2e-16 Item Hotness &lt; 2e-16 &lt; 2e-16 
Conversion Rate of Similar items &lt; 2e-16 &lt; 2e-16 Item Quantity 0.267366 0.00539 Item Return Policy Store inventory 1.62e-15 &lt; 2e-16 Seller Feedback Percentage of Positive Feedback &lt; 2e-16 &lt; 2e-16 Top seller level 1.78e-09 5.65e-08 In Table 2, we list statistical re sults of the two regression models. The significant level used here is 95%. From the data, we can observe that most of the features X  p-values are greater than 0.05, except  X  X tem Quantity X  and  X  X tem Return Policy Level X . Besides each single feature, we also did statistical tests for the overall models, and both of linear regr ession and logistic regression passed the tests. The whole item set was split into training set and test set. By using training set, we got parame ters for regression models and the performance of the algorithm was tested by the rest of data set. All the experimental results disc ussed below were obtained by 10-fold cross validation. To predict whether an item could be sold by logistic regression is similar as traditional binary classification problem, some classic evaluation metrics could be used fo r our experiments, such as TP rate, FP rate, precision, recall and F1 score. However, for linear regression, evaluation methods of binary classification are not suitable. Since, the final purpose that we built the prediction model is when users browse and search on C2C sites, we can rank higher the items with high conversion possibility (scores outputted by regression models), precision @ N could be a good approach to measure how many top ranked items were really sold in the test data set. Where R is the set of top N items ranked by our system, and C is the set of sold items. In most of our experiments, we used P @1, P @10 and P @20 for evaluation. Firstly, we used classic binary classification evaluation methods to get the performance for logistic regression. As we mentioned above, the classification measurem ent method is not suitable for linear regression. Actual Not Sold FN = 108,525 TN = 275,574 In Table 3, we show the pre liminary prediction results. Where TP , TN , FP and FN mean true positive, true negative, false positive and false negative, respectively. TP rate and FP rate of  X  X old X  class could be calculated as, TP rate and FP rate of  X  X ot Sold  X  class could be calculated in the similar way. 
Class TP Sold 0.656 0.283 0.67 0.656 0.663 Not 
Sold The overall performance of logistic regression is shown in Table 4, and we could also get the accuracy of the logistic regression model by Equation 11, So, we got 495,973 correctly predicted items out of total 720,076 items, and the accuracy is 68.88 %. As we described above, we also utilized P @ N to calculate how many items ranked by the outputte d score of regression models were really sold. When users were browsing or searching on eBay, items sorted by time ending soonest was a default me thod, so with time passing, we could assume that each item has the equal chance to be exposed, namely, items are displayed ra ndomly. In our data set, 335,977 items were sold, and 384,099 items were not sold, therefore, if items are simply ranked by time ending soonest, P @ N could be estimated as 335977 / (384099+335977) = 0.467 by maximum likelihood estimation, i.e., average 4.67 items in top 10 items were sold. In our experiment, we considered time e nding soonest as the baseline, and regression. We can observe from Figure 3 that the precision of items ranked by regression models outperform nearly 30% over the baseline and the performance of linear regression and logistic regression are almost same. Hence, we proved that our al gorithm is very effective to rank higher the items which are more likely to be sold. Finally, both users and e-commerce sites will be benef ited. As we mentioned above, by ranking higher the items that users are likely to buy, we could greatly increase the chance that users make the purchases and improve user satisfaction as well. 
Figure 3. Performance comparison between two regression In Table 5, we list all the experimental results for each domain. P@N of the baseline method in each domain was calculated in the same way as Equation 12, The performance of the algorithm in some domains like  X  X ooks X  and  X  X ell Phones &amp; PDAs X  is much better than the performance in each other domain. For example, the precision of these two domains outperform nearly 50% over the baseline on P @1. However, the precision improvements in other dom ains, such as  X  X ollectibles X  and  X  X oins &amp; Paper Money X , etc are relatively low. The reason might be that in  X  X ooks X  and  X  X  ell phones X  domains, usually items help predicting, but for the  X  X oll ectibles X  domain, items could be very unique and the differences between items might be very significant. So predicting by histori cal data becomes more difficult. One interesting question is how good each feature is when it is regression model, and the performance of each single feature could be obtained in Table 6. It could be seen that the perform ance of  X  X onversion rate of similar item X ,  X  X ategory hotness X  and  X  X ercentage of positive feedback X  is better than others. However, some features like  X  X eller feedback score X  didn X  X  show its effect as we expected, which means users on eBay care more about the percentage of positive feedback of sellers, real world, all the features are not totally independent, and some features are even highly correlated. Therefore, the effect of one feature could be covered by others. Sports Men, Cards &amp; Fan Number of Competitors 0.537 0.491 0.478 Seller Feedback Score 0.517 0.483 0.480 Percentage of Positive In order to understand the correlati ons between features, we take advantage of the correlation coefficient to quantify the relation between two features. The correla tion of two features could be calculated as Equation 13. Where X i denotes the vector of values of feature i and X the vector of feature j . Due to the limited space, instead of the whole correlation matrix, we only list the correlation coefficients of some selected feature pairs in Table 7. It is not su rprising that  X  X tem hotness X  and  X  X umber of competitors X , are positive correlated. Usually, hotter items would bring in more competitors. The coefficient of  X  X ategory hotness X  and  X  X onversion ra te of similar items X  is also reasonable. It is quite interesting that  X  X eller feedback score X  and  X  X ercentage of positive feedback X  are negative correlated, because on eBay, new sellers star ts with 100% positive feedback, and after they do more and more business, it could be inevitable that their percentages of positive feedback drop down. Table 7. Correlation coefficients of selected feature pairs &lt;Item hotness, Number of competitors&gt; 0.128 In this section we hope to give a preliminary study that after we rank items by the predicated conversion probability, whether the new sorting order could help to increase revenue. In this paper, we use Revenue@N as an evaluation measurement. Revenue@N is calculated by the sum of sold items X  price among the top N items. As the example in Table 6, Revenue@3 = 10, and Revenue@5 = 10+12=22. In our test data, we sorted ite ms both by predicated conversion probability and by item X  X  end time (a baseline which was mentioned in Section 4.3.2.2), and calculated average Revenue@N for each ranked list. From Figure 4, we can observe that items ranked by predicated conversion probability can gain significant more revenue than the items ranked by end time. 
Figure 4. Revenue comparison between ranking items by Although this research work is very important for both business revenue and user experience, to simulate how people make shopping decision is very complicated. We acknowledge that our work in the ecommerce area is in a relative early stage. In the future work, it would be desirable to settle on some standard data sets to increase the ease of comp arison and involve more research work on this area. When users are comparing items and making shopping decisions, the process is quite complicated and lots of features could be potentially helpful in the prediction. In future, we also would like to incorporate more features into our model. From item side, we will study some features, such as whether sellers supply gallery images for their items, and whethe r sellers provide comprehensive description for items, etc. From seller X  X  side, we would like to merchant. People usually prefer to buy items from professional sellers who focus on only one or few domains, rather than sellers who sell different items across books, clothing and cell phones etc. Moreover, we also hope to include some features that could be useful to identify frauds. Another future direction could be cost-sensitive analysis. Just like classic junk mail filtering problem, cost-sensitive classification models were well studied in that area, e.g., [5][10]. Similar to email classification, in our scenario, it is worse that if we classify a good item into a bad item group, compared with classifying a bad item into a good item group, because that would induce lots of complaints from sellers. The research direction is quite promising, but from the sellers X  point of view, there would s till be a long debate. Obviously, compared with the approach that lists items by timing ending soonest, our model is more eff ective, however, a segment of sellers and items could be sacrificed because they may not have the equal exposure chances as before. Whereas, we are still confident in that the prediction model will benefit users and C2C sites greatly in long term, because the model makes it possible to control the growth of fraud that goes with more items being listed on the site. Poor quality items w ill not have equal chances as good items, and sellers will try them best to be responsible and competitive to run their business. As a result, C2C ecommerce business could continue to develop fast and safely. Online ecommerce has become more and more popular in people X  X  daily life. Given the exponential growth of items on C2C sites, users browse and search activity has become increasingly complex. In order to address this problem , this paper proposed a novel idea that ranks items by the conversion probability of listed item. Hence, by ranking higher the items that users are likely to buy, we could greatly increase the chances that users buy the items while improving their user experience. To predict the conversion probability, we collected a large set of ended items as the training data, and analyzed user behavior based on those historical items. A set of item features and regression algorithms were utilized to model user behavior and simu late how buyers make shopping decisions. Finally, the experimental results demonstrate that the prediction model could help ranking higher the right items. [1] Agichtein, E., Brill, E. and Dumais, S. Improving Web [2] Bayus, B.L. Forecasting Sales of New Contingent Products: [3] Burges, C.J., 1998. A tutorial on support vector machines for [4] Carreras, X. and Marquez, L. Boosting trees for anti-spam [5] Elkan, C. The Foundations of Cost-Sensitive Learning. [6] Gamon, M. 2004: Sentiment classification on customer [7] Garber, T., Goldenberg, J., Libai, B., Muller, E. From [8] Hastie, T. J. and Tibshirani , R. J. (1990). Generalized [9] Joachims, T., Granka, L., Pang, B., Hembrooke, H. and [10] Katirai, H. (1999),  X  X iltering junk e-mail: A performance [11] Montgomery, D. C. et al. Introduction to Linear Regression [12] Osuna, E., Freund, R. and Girosit, F. Training support vector [13] Richardson, M., Dominowska, E. and Ragno, R. Predicting [14] Regelson, M. and Fain, D. Predicting click-through rate [15] Sahami, M., Dumais, S., Heckerman, D., Horvitz, E. A [16] Thiesing, F., Vornberger, O. Sales forecasting using neural [17] Xue, G.R., Zeng, H.J., Chen, Z., Yu, Y., Ma, W.Y.  X  Xi, [18] Yih, W., Goodman, J. and Carvalho, V. R. Finding [19] Zeng, H.J., He, Q.C., Chen, Z., Ma, W.Y. and Ma, J.W. 
