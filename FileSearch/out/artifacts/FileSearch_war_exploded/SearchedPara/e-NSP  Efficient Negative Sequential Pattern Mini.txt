 Mining Negative Sequential Patterns (NSP) is much more challenging than mining Positive Sequential Patterns (PSP) due to the high computational complexity and huge search s-pace required in calculating Negative Sequential Candidates (NSC). Very few approaches are available for mining NSP, which mainly rely on re-scanning databases after identifying PSP. As a result, they are very inefficient. In this paper, we propose an efficient algorithm for mining NSP, called e-NSP, which mines for NSP by only involving the identified PSP, without re-scanning databases. First, negative containment is defined to determine whether or not a data sequence con-tains a negative sequence. Second, an efficient approach is proposed to convert the negative containment problem to a positive containment problem. The supports of NSC are then calculated based only on the corresponding PSP. Final-ly, a simple but efficient approach is proposed to generate NSC. With e-NSP, mining NSP does not require addition-al database scans, and the existing PSP mining algorithms can be integrated into e-NSP to mine for NSP efficiently. e-NSP is compared with two currently available NSP mining algorithms on 14 synthetic and real-life datasets. Intensive experiments show that e-NSP takes as little as 3% of the runtime of the baseline approaches and is applicable for ef-ficient mining of NSP in large datasets.
 H.2.8 [ Data-base Applications ]: Data Mining Algorithms, Performance, Experimentation Negative Sequential Pattern, Sequence Mining, Negative Con-tainment
Negative sequential patterns (NSP) refer to sequences with non-occurring items. For instance, assume p 1 = &lt; abcX &gt; is a positive sequential pattern (PSP); p 2 = &lt; ab  X  cY &gt; is a NSP. It is increasingly recognized [5][10] that such NSP, com-posed of both occurring and non-occurring items, can play an irreplaceable role in deeply understanding and tackling many business applications, such as the associations between treatment services and illnesses, and the detection of high impact occurring (positive behaviors) and non-occurring be-havior (negative behaviors) sequences [3, 2], which cannot be handled by mining PSP only. NSP cannot be described or discovered by classic PSP mining algorithms such as GSP, SPADE, PrefixSpan and SPAM. NSP mining has seen only very limited progress in recent years [6][5][10][11], and all existing methods are very inefficient in mining NSP. This is because NSP is much more difficult to mine than PSP, in particular because of two intrinsic complexities.
High computational complexity . The existing methods cal-culate the support of negative sequential candidates (NSC) by additionally scanning the database after identifying PSP. This leads to additional costs and results in low efficiency in NSP mining.

Large NSC search space . The existing approaches gener-ate k -size NSC by conducting a joining operation on ( k-1 )-size NSP. This leads to a huge number of NSC [6][5][7][10], which makes it difficult to search for meaningful outputs. Further, NSC does not satisfy the anti-monotony principle [10]. It is a challenge to prune the large proportion of mean-ingless and unnecessary NSC, and it is therefore important to develop efficient approaches for generating a limited num-ber of truly useful NSC.
To address the above critical challenges in NSP mining and make NSP mining workable in handling real-life appli-cations, this paper proposes a novel and efficient NSP mining approach called e-NSP . The main idea is as follows. To avoid additional database scanning, we convert the negative con-tainment problem to a positive containment problem. The NSC supports are then calculated by only using a NSC X  X  cor-responding PSP information. In this way, there is no need to re-scan the database after discovering PSP. To the best of our knowledge, this is the first approach to conduct efficient NSP mining by involving PSP only, without rescanning the database. e-NSP provides a new and promising strategy for efficient NSP mining which is workable in large datasets.
The remainder of the paper is organized as follows. Sec-tion 2 discusses the related work. In Section 3, we formalize the problem of mining PSP and NSP. The e-NSP algorithm is detailed in Section 4. Section 5 presents the experiment results, which are followed by conclusions and future work in Section 6.
Very limited research is available in the literature on min-ing NSP. [10] proposes a negative version of GSP algorithm NegGSP to mine for NSP. [5] proposes a PNSP approach for mining positive and negative sequential patterns in the form of &lt; (abc)  X  (de) (ijk) &gt; . [6] only handles NSP with the last element as negative. [11] proposes a genetic algo-rithm for mining NSP. [9] proposes an approach to mining event-oriented negative sequential rules. [7] identifies NSP in the form of (  X  a,b) , (a,  X  b) and (  X  a,  X  b) , which is similar to [9]. The above approaches either re-scan the database or do not directly address NSP mining. In addition, different researchers present inconsistent definitions and explanations about the basic concept of negative containment [4]. [5] con-siders that data sequence ds= &lt; dc &gt; cannot contain &lt; c tains them.
In association rule mining, a non-occurring item is called a negative item and an occurring item is called a positive item [8]. This tradition is followed in NSP mining. Those sequences consisting of at least one negative item are called negative sequences . Sequential pattern mining (as it is usu-ally called) mainly focuses on occurring items, namely posi-tive sequences [6][5][7][10]. The sequences in source data are called data sequences .
Let I = { i 1 ,i 2 ,...,i n } be a set of items .An itemset is a subset of I .A sequence is an ordered list of itemsets. A sequence s is denoted by &lt;s 1 s 2 ...s l &gt; ,where s s j is also called an element of the sequence, and denoted as ( x 1 x 2 ...x m ) ,where x k is an item, x k  X  I(1 k m) .For simplicity, the brackets are omitted if an element only has one item, i.e., element (x) is coded x . An item can occur at most once in an element, but can appear multiple times in different elements of a sequence.

The length of sequence s , denoted as length(s) , is the total number of items in all elements in s . s is a k -length sequence if length(s)=k .The size of sequence s , denoted as size ( s ), is the total number of elements in s . s is a k -size sequence if size(s)=k .

Sequence s  X  = &lt; X  1  X  2 ...  X  n &gt; is called a sub-sequence of s , denoted as s  X   X  s  X  , if there exists 1 j 1 &lt;j 2 &lt;...&lt;j s  X  contains s  X  .

A sequence database D is a set of tuples &lt; sid,ds &gt; ,where sid is the sequence id and ds is the data sequence .The number of tuples in D is denoted as | D | . The set of tuples containing sequence s is denoted as { &lt; s &gt; } . The support of s , denoted as sup ( s ), is the number of { &lt; s &gt; } { &lt; s &gt; }| = |{ &lt; sid,ds &gt; , &lt; sid,ds &gt;  X  a minimum support threshold which is predefined by users. Sequence s is called a frequent (positive) sequential pattern if sup(s) min sup .
In real applications, the number of negative sequences is large, and many of them are not meaningful. In order to reduce the number of NSC and discover meaningful NSP efficiently, constraints must be added to negative sequences [9][11][5][10][6]. This paper also refers to the constraints in these existing papers. The only difference is that we mine NSP only from frequent PSP. The reason is that PSP is most useful for users to make decisions so far.

In order to formalize the constraints, we provide a defini-tion as following.
 Definition 1. Positive Partner
The positive partner of a negative element  X  e is e , denoted as p(  X  e) , i.e., p(  X  e)=e . The positive partner of positive element e is e itself, i.e., p(e)=e . The positive partner of a negative sequence ns= &lt;s 1 ...s k &gt; is to change all negative elements in ns to their positive partners, denoted as p(ns) , i.e., p(ns)= { &lt;s 1 ...s k &gt; | s i =p( s i ), s i  X  p( &lt;  X  (ab) c  X  d &gt; )= &lt; (ab) c d &gt; .
Constraint 1. Frequency constraint. This paper only fo-cuses on the negative sequences ns whose positive partner is frequent, i.e., sup(p(ns)) min sup . While [5] and [10] only require the positive partner of each element in ns is frequent.
Constraint 2. Format constraint. Continuous negative elements in a NSC are not allowed. For example, &lt;  X  (ab)  X  cd &gt; is not allowed. This constraint is same as [5][10].
Constraint 3. Element negative constraint. The minimum negative unit in a NSC is an element [10]. For example, &lt; (  X  ab) c d &gt; does not satisfy this constraint, &lt;  X  (ab) c does.

In this paper, we assume that a negative sequence implic-itly satisfies the above three constraints.

Because of constraint 3, the definition of sub-sequence in positive sequence is not suitable for that in negative se-quence. Now we formally redefine it by the definitions of element-id set and order preserving sequence. Element id is the order number of an element in a sequence. Given a ment s i . Element-id set EidS s of s is the set that includes all elements and their ids in s , i.e., EidS s = { ( s i ,id( s } = { ( s 1 ,1), ( s 2 ,2), ... ,( s m ,m) } (1 i m) . The set including all positive/negative element-ids is called positive/negative element-id set of s , denoted as EidS + s , EidS  X  s respectively.
For any subset EidS s = { (  X  1 , id 1 ), (  X  2 , id 2 ), ... ,(  X  (1 &lt; p m) of EidS s ,  X  = &lt; X  1  X  2 ...  X  p &gt; ,if (1 i &lt; p) ,thereexist id i &lt;id i +1 ,then  X  is called an order-preserving sequence of EidS s , denoted as  X  =OPS( EidS s
Sequence s  X  is called a sub-sequence of negative sequence s ,and s  X  is a super-sequence of s  X  ,if  X  EidS s  X  , EidS is subset of EidS s  X  , s  X  =OPS( EidS s  X  ) , denoted as s If s  X  is a negative sequence, it is required to satisfy Con-straint 2. Specially, the sub-sequence containing all positive elements, OPS( EidS + s ) is called the Maximum Positive Sub-sequence of s , denoted as MPS(s) .

Example 1. Given s= &lt;  X  (ab) c d &gt; , EidS + = { (c,2), (d,3) MPS(s)= &lt; cd &gt; , OPS( { (c,2),(  X  (ab),1) } )= &lt; sub-sequence of s .
 Definition 2. Negative Sequential Pattern (NSP) A negative sequence s is a negative sequential pattern (N-SP) if its support is not less than the threshold min sup .
Figure 1 shows the framework and working mechanism of e-NSP. First, it mines all PSP by traditional PSP min-ing algorithm, then it generates NSC based on PSP, after that, it calculates supports of NSC by converting them to calculating support of corresponding PSP.
Because a sub-sequence (e.g., s 1 = &lt; d &gt; ) may occur more than one times in its super-sequence (e.g., s 2 = &lt; a(bc)d (cde) &gt; ), we need to know the positions that s 2 contains s 1 from left and right sides of s 2 . It is very important to our Negative Containment Definition. Therefore we give the following definition.
 Definition 3. First Sub-sequence Ending Position / Last Sub-sequence Beginning Position
Given a data sequence ds= &lt;d 1 d 2 ... d t &gt; and a positive sequence  X  ,(1)if  X  p(1 &lt; p t) ,  X   X  &lt;d 1 ... d p &gt; d p  X  1 &gt; ,then p is called the First Sub-sequence Ending Posi-(2) if  X  q(1 q &lt; t) ,  X   X  &lt;d q ...d t &gt;  X   X  &lt;d is called the Last Sub-sequence Beginning Position , denoted as LSB(  X  ,ds) ;if  X   X  &lt;d t &gt; then LSB(  X  ,ds)=t ;(3)if  X  ds , then FSE(  X  ,ds)=0 , LSB(  X  ,ds)=0 .

Our definition of a data sequence containing a negative se-quence is as follows. We use n-neg-size to denote a negative sequene containing n negative elements.
 Definition 4. Negative Containment Definition
Let ds= &lt;d 1 d 2 ... d t &gt; be a data sequence, ns= &lt;s ... s m &gt; be an m-size and n-neg-size negative sequence, (1) if m &gt; 2t+1 ,then ds does not contain ns ;(2)if m=1 and n=1 ,then ds contains ns when p(ns) ds ; (3) otherwise, ds contains ns if,  X  ( s i ,id( s i ))  X  EidS  X  ns (1 i m) ,oneofthe following three holds: (a) (lsb=1) or (lsb &gt; 1)  X  p( s 1 ) &lt;d 1 ... d lsb  X  1 (b) (fse=t) or (0 &lt; fse &lt; t)  X  p ( s m ) &lt;d fse +1 (c) (fse &gt; 0  X  lsb=fse+1) or (fse &gt; 0  X  lsb &gt; fse+1) &lt;d fse +1 ... d lsb  X  1 &gt; ,when 1 &lt; i &lt; m , where fse=FSE(MPS( &lt;s 1 s 2 ... s i  X  1 &gt; ),ds) , lsb=LSB( MPS( &lt;s i +1 ... s m &gt; ),ds) .

Example 3. Given ds= &lt; a(bc)d(cde) &gt; ,wehave ns . lsb =4 &gt; 0, but p( s 1 )= &lt; a &gt;  X  &lt;d 1 ... d (Case a). (2) ns= &lt;  X  aac &gt; . EidS  X  ns = { (  X  a,1) } . ds contains ns because lsb = 1 (Case a). (3) ns= &lt; (ab)  X  (cd) &gt; . EidS  X  ns = { (  X  (cd),2) contain ns because fse = 0 (Case b). (4) ns= &lt; (de)  X  (cd) &gt; . EidS  X  ns = { (  X  (cd),2) ns because fse =4( t = 4)(Case b). (5) ns= &lt; a  X  dd  X  d &gt; . EidS  X  ns = { (  X  d,2), (  X  not contain ns .For (  X  d,2) , fse=1 , lsb=4 , but p(  X  d) not satisfy the condition, we do not need to consider other negative elements. (6) ns= &lt; a  X  bb  X  a(cde) &gt; . EidS  X  ns = { (  X  b,2), ( contains ns .For (  X  b,1) , fse=1 , lsb=2 , fse &gt; 0  X  lsb=fse+1 (Case c).
In order to convert negative containment problems to pos-itive containment problems, we need to define a special sub-sequence as follows.
 Definition 5. 1-neg-size Maximum Sub-sequence For a negative sequence ns , its sub-sequence that includes MPS ( ns ) and one negative element e is called a 1-neg-size maximum sub-sequence, denoted as 1-negMS=OPS( EidS + ns , e) ,where e  X  EidS  X  ns . The sub-sequence set including al-l 1-neg-size maximum sub-sequences of ns is called 1-neg-size maximum sub-sequence set, denoted as 1-negMSS ns , 1-negMSS ns = { OPS( EidS + ns ,e) | X  e  X  EidS  X  ns } . Corollary 1. Negative Conversion Strategy
Given a data sequence ds= &lt;d 1 d 2 ... d t &gt; ,and ns= &lt;s s 2 ... s m &gt; ,whichisan m-size and n-neg-size negative se-quence, the negative containment definition can be convert-ed as follows: data sequence ds contains negative sequence ns if and only if the two conditions hold: (1) MPS ( ns ) and (2)  X  1-negMS  X  1-negMSS ns , p(1-negMS) ds .
Example 4. Given ds= &lt; a(bc)d(cde) &gt; ,1)if ns= &lt; a  X  dd  X  d &gt; , 1-negMSS ns = { &lt; a  X  dd &gt; , &lt; ad ds does not contain ns because p( &lt; a  X  dd &gt; )= &lt; add &gt; ds ;2)if ns X = &lt; a  X  bb  X  a(cde) &gt; ,1-negMSS ns = { b(cde) &gt; , &lt; ab  X  a(cde) &gt; } ,then ds contains ns because MPS(ns)= &lt; ab(cde) &gt;  X  ds  X  p( &lt; a  X  bb(cde) &gt; ) ds p( &lt; ab  X  a(cde) &gt; ) ds .

Corollary 1 proves that the problem of whether a data sequence contains a negative sequence is equivalent to the problem of whether the data sequence does not contain its corresponding positive sequences. The proof of Corollary 1 is omitted here because of limited space. Corollary 2.

Given a m-size and n-neg-size negative sequence ns ,for  X  1-negMS i  X  1-negMSS ns (1 i n) , the support of ns in sequence database D is: sup ( ns )= |{ ns }| = |{ MPS ( ns ) } X  X  X  n i =1 { p (1-negMS
This can be easily derived from Corollary 1. Because i =1 { p (1-negMS i ) } X  X  MPS ( ns ) } , equation 1 can be rewrit-ten as: sup ( ns )= |{ MPS ( ns ) }| X  X  X  n i =1 { p (1-negMS i ) }|
Example 5. sup( &lt;  X  a(bc)d  X  (cde) &gt; )=sup( &lt; (bc) d &gt; )-|{ &lt; a(bc)d &gt; } X  X  &lt; (bc) d (cde) &gt; }| ;
If ns only contains a negative element cd &gt; )
In particular, for negative sequence &lt;  X  e &gt; ,
From equation 2 we can see that sup ( ns )canbeeasi-ly calculated if we know sup ( MPS ( ns )) and | X  n i =1 { negMS i ) }| . According to Constraint (1) and the nega-tive candidate generation approach discussed in Section 4.4, MPS ( ns )and p(1-negMS i ) are frequent. So sup ( MPS ( ns )) can be easily obtained by traditional algorithms.
Now the problem is how to calculate | X  n i =1 { p (1-negMS Ourapproachisasfollows.

We use a data structure, which is called e-NSP data struc-ture, like Table 2 to store the corresponding data, includ-ing PSP, support and { sid } (containing all ids of the tuples that contain corresponding PSP). These data are stored in a hash table to identify PSP efficiently. In order to calculate the union set efficiently, we propose two other optimization methods as follows: (1) When we calculate support of a N-SC, we also utilize a hash table to accelerate search speed. Compared with the performance using common array, the search speed with hash table is far more efficient. (2) We assume that all data in Table 2 are stored in main memo-ry. This is feasible in practice since the mainstream memory can reach gigabytes and above. We do not record the sid of 1-size PSP because the equations do not need to calculate the union set of those sid of 1-size PSP.
The basic idea of generating a negative sequential candi-date is to change any non-contiguous elements (not items) in a PSP to their negative ones.
 Definition 6. e-NSP Candidate Generation
For a k -size PSP, its NSC are generated by changing any m non-contiguous element(s) to its (their) negative one(s), m=1,2, ... , k/2 ,where k/2 is a minimum integer that is not less than k/ 2.

Example 7. The NSC based on &lt; (ab) c d &gt; include: m=1 , &lt;  X  (ab) c d &gt; , &lt; (ab)  X  cd &gt; , &lt; (ab) c m=2 , &lt;  X  (ab) c  X  d &gt; .

Obviously, for all PSP in a sequence database, we can gen-erate all NSC that satisfy the three constraints, as described in Section 3.2.
The e-NSP algorithm, as shown in Algorithm 1, is pro-posed to mine for NSP using only identified PSP.
 Algorithm 1: e-NSP Algorithm
The sequence database is shown in Table 1 [5]. The pro-cess is as follows. (1) Mine PSP using one of the well-known algorithms, such as GSP, and fill in the e-NSP data structures, which are shown as Table 2. (2) Use the e-NSP Candidate Generation method to gen-erate all NSC. (3) Use Equations 2-4 to calculate the support of these NSC. The results are shown in Table 3, and the resulting NSP are marked in bold.

From this example, we can see that &lt; ac &gt; and &lt; a ly not all of them can be used to make decisions because they may be misleading. How to select the meaningful and workable patterns is one of our ongoing tasks. Table 3: Example Result -NSC and Support (min sup=2)
We conduct experiments on 14 synthetic and real dataset-s to compare the efficiency of e-NSP with two baseline ap-proaches PNSP [5] and NegGSP [10]. We select PNSP and NegGSP because they are the only available algorithms that are comparable to our algorithm. To compare their perfor-mance, we adapt PSNP and NegGSP to follow the same definitions and constraints as stated in Section 3. In the comparison, all positive patterns are identified by GSP. N-SP are further mined by e-NSP, PNSP and NegGSP. We conduct intensive experiments to compare the difference be-tween three algorithms in terms of computational costs on different data sizes and data characteristics.
 All algorithms are implemented in Java in a PC with Intel Core 2 CPU of 2.9GHz, 2GB memory and Windows XP Professional SP2.
To describe and observe the impact of data characteristics on algorithm performance, we use following data factors: C, T, S, I, DB and N , which are defined to describe character-istics of sequence data [1].

C : Average number of elements per sequence; T : Average number of items per element; S : Average length of maximal potentially large sequences; I : Average size of items per ele-ment in maximal potentially large sequences; DB :Number of sequences (= size of Database); and N :Numberofitems.
Four source datasets are used for the experiments. They include both real data and synthetic datasets generated by IBM data generator [1]. By partitioning the data, we obtain 14 datasets in total.

Dataset 1 (DS1) ,C8 T4 S6 I6 DB100k N100. We further adjust DS 1 to generate 10 additional datasets, labelled as DS 1 .x ( x =1 ,..., 10).
 Dataset 2 (DS2) ,C10 T8 S20 I10 DB10k N0.2k.

Dataset 3 (DS3) is from UCI Datasets. There are 989,818 records. The average number of elements in a sequence is 4, and each element only has one item.

Dataset 4 (DS4) is real-application data from financial ser-vice industry. It contains 5,269 customers/sequences. The average number of elements in a sequence is 21. The mini-mum number of elements in a sequence is 1, and the maxi-mum number is 144.
The runtime of mining NSP by the three approaches is shown in Figure 2. e-NSP takes mostly less than 3% of the runtime of PNSP and NegGSP on all datasets. For example, e-NSP spends 2.7% to 1.6% runtime of PNSP on DS 3when min sup decreased from 0.025 to 0.01. When min sup is reduced to 0.01, PNSP and NegGSP take around one hour, but e-NSP takes less than one minute, because e-NSP only needs to  X  X alculate X  NSP support based on the sid sets of corresponding positive patterns, while PNSP and NegGSP have to re-scan the whole dataset.

The results on the maximum length and number of neg-ative patterns are shown in Figure 3. It is difficult to draw a reliable conclusion from them, because the characteristics of the datasets are not comparable. Therefore, we conduct a dataset characteristics analysis in following section. Figure 3: Maximum Length and Number of Nega-tive Patterns Table 4: Dataset Characteristics Analysis Result
We analyze the dataset characteristics in terms of the above defined data factors to see the impact of the data factors (see Section 5.1) on the performance of e-NSP, com-pared to PNSP and NegGSP. We generate various types of synthetic datasets with different distributions. Dataset DS1 is extended to ten different datasets by tuning each factor, as shown in Table 4. For example, dataset DS1.1 (C4T4S6I6. DB10k.N100) is different to DS1 (C8T4S6I6.DB10k.N100) on C factor, which means they have different average num-bers of elements in a sequence. We mark the difference by underlining the distinct factor for each dataset in Table 4. In Table 4, t1 , t2 and t3 represent the runtime of NegGSP, PNSP and e-NSP correspondingly. We use t3/t2 to show e-NSP X  X  performance compared with PNSP. From the results (see Table 4), we can say that factors C , T and N seriously affect the performance of e-NSP, and factors S and I do not greatly affect it. When factor C is low, such as DS1.1 , e-NSP works better than on datasets with big C ,suchas DS1 and DS1.2 . Similar results hold for T ,suchas DS1 with small T ,comparedwith DS1.3 and DS1.4 with big T . When N is high, such as in DS1.9 and DS1.10 ,e-NSPworks better than that with small N ,suchasin DS1 .
Mining NSP is very challenging due to the large search space of negative candidates. Current NSP techniques rely on re-scaning databases after identifying positive pattern-s. This has been shown to be very inefficient, and little progress has been made in NSP mining. We have proposed a simple but very efficient NSP mining algorithm: e-NSP. e-NSP is based on a formal and consistent concept, negative containment, which defines how a data sequence contains a negative sequence. e-NSP encloses a negative conversion s-trategy to convert the problem of whether a data sequence contains a negative sequence to the problem of whether the data sequence contains some of the corresponding positive sequences. Supports of NSC are then calculated based on-ly on the corresponding PSP. Finally, a simple but efficient approach has been proposed to generate NSC. e-NSP has been tested on both synthetic and real-world datasets and compared with existing NSP mining algorithms. The exper-imental results and comparisons on 14 datasets from data characteristics perspectives have clearly shown that e-NSP is much more efficient than existing approaches. e-NSP offers a new strategy for efficiently mining large scale NSP.
We are currently working on effective approaches to se-lect the most meaningful patterns, and the application of negative sequence mining on complex behavior analysis. This work was supported by Projects of International Coop-eration Training for Shandong Province Higher Education-al Excellent Backbone Teacher, Shandong Provincial Natu-ral Science Foundation, China (No.ZR2011FM028), and by Australian Research Council Grants (DP1096218, DP0988016, LP100200774, LP0989721). [1] R. Agrawal and R. Srikant. Mining sequential [2] L. Cao. In-depth behavior understanding and use: the [3] L. Cao, Y. Zhao, and C. Zhang. Mining [4] X. Dong, L. Zhao, X. Han, and H. Jiang. Comparisons [5] S.-C. Hsueh, M.-Y. Lin, and C.-L. Chen. Mining [6] N. P. Lin, H.-J. Chen, and W.-H. Hao. Mining [7] W.-M. Ouyang and Q.-H. Huang. Mining negative [8] X. Wu, C. Zhang, and S. Zhang. Efficient mining of [9] Y. Zhao, H. Zhang, L. Cao, C. Zhang, and [10] Z. Zheng, Y. Zhao, Z. Zuo, and L. Cao. Negative-gsp: [11] Z. Zheng, Y. Zhao, Z. Zuo, and L. Cao. An efficient
