 Krikamol Muandet krikamol@tuebingen.mpg.de David Balduzzi david.balduzzi@inf.ethz.ch Bernhard Sch  X olkopf bs@tuebingen.mpg.de Domain generalization considers how to take knowl-edge acquired from an arbitrary number of related do-mains, and apply it to previously unseen domains. To illustrate the problem, consider an example taken from Blanchard et al. ( 2011 ) which studied automatic gat-ing of flow cytometry data. For each of N patients, a set of n i cells are obtained from peripheral blood sam-ples using a flow cytometer. The cells are then labeled by an expert into different subpopulations, e.g., as a lymphocyte or not. Correctly identifying cell subpop-ulations is vital for diagnosing the health of patients. However, manual gating is very time consuming. To automate gating, we need to construct a classifier that generalizes well to previously unseen patients, where the distribution of cell types may differ dramatically from the training data.
 Unfortunately, we cannot apply standard machine learning techniques directly because the data violates the basic assumption that training data and test data come from the same distribution. Moreover, the train-ing set consists of heterogeneous samples from several distributions, i.e., gated cells from several patients. In this case, the data exhibits covariate (or dataset) shift ( Widmer and Kurat , 1996 ; Quionero-Candela et al. , 2009 ; Bickel et al. , 2009 ): although the marginal dis-tributions P X on cell attributes vary due to biologi-cal or technical variations, the functional relationship P ( Y | X ) across different domains is largely stable (cell type is a stable function of a cell X  X  chemical attributes). A considerable effort has been made in domain adap-tation and transfer learning to remedy this problem, see Pan and Yang ( 2010 ); Ben-David et al. ( 2010 ) and references therein. Given a test domain, e.g., a cell population from a new patient, the idea of domain adaptation is to adapt a classifier trained on the train-ing domain, e.g., a cell population from another pa-tient, such that the generalization error on the test domain is minimized. The main drawback of this ap-proach is that one has to repeat this process for every new patient, which can be time-consuming  X  especially in medical diagnosis where time is a valuable asset. In this work, across-domain information, which may be more informative than the domain-specific infor-mation, is extracted from the training data and used to generalize the classifier to new patients without re-training.
 Overview. The goal of (supervised) domain gener-alization is to estimate a functional relationship that handles changes in the marginal P ( X ) or conditional P ( Y | X ) well, see Figure 1 . We assume that the condi-tional probability P ( Y | X ) is stable or varies smoothly with the marginal P ( X ). Even if the conditional is stable, learning algorithms may still suffer from model misspecification due to variation in the marginal P ( X ). That is, if the learning algorithm cannot find a solu-tion that perfectly captures the functional relationship between X and Y then its approximate solution will be sensitive to changes in P ( X ).
 In this paper, we introduce Domain Invariant Compo-nent Analysis (DICA), a kernel-based algorithm that finds a transformation of the data that (i) minimizes the difference between marginal distributions P X of domains as much as possible while (ii) preserving the functional relationship P ( Y | X ).
 The novelty of this work is twofold. First, DICA ex-tracts invariants : features that transfer across do-mains. It not only minimizes the divergence be-tween marginal distributions P ( X ), but also preserves the functional relationship encoded in the posterior P ( Y | X ). The resulting learning algorithm is very sim-ple. Second, while prior work in domain adaptation focused on using data from many different domains to specifically improve the performance on the target task, which is observed during the training time (the classifier is adapted to the specific target task), we assume access to abundant training data and are in-terested in the generalization ability of the invariant subspace to previously unseen domains (the classifier generalizes to new domains without retraining). Moreover, we show that DICA generalizes or is closely related to many well-known dimension reduction al-gorithms including kernel principal component analy-sis (KPCA) ( Sch  X olkopf et al. , 1998 ; Fukumizu et al. , 2004 ), transfer component analysis (TCA) ( Pan et al. , 2011 ), and covariance operator inverse regression (COIR) ( Kim and Pavlovic , 2011 ), see  X  2.4 . The per-formance of DICA is analyzed theoretically  X  2.5 and demonstrated empirically  X  3 .
 Related work. Domain generalization is a form of transfer learning, which applies expertise acquired in source domains to improve learning of target domains (cf. Pan and Yang ( 2010 ) and references therein). Most previous work assumes the availability of the tar-get domain to which the knowledge will be transferred. In contrast, domain generalization focuses on the gen-eralization ability on previously unseen domains. That is, the test data comes from domains that are not avail-able during training.
 Recently, Blanchard et al. ( 2011 ) proposed an aug-mented SVM that incorporates empirical marginal dis-tributions into the kernel. A detailed error analysis showed universal consistency of the approach. We ap-ply methods from Blanchard et al. ( 2011 ) to derive the-oretical guarantees on the finite sample performance of DICA.
 Learning a shared subspace is a common approach in settings where there is distribution mismatch. For ex-ample, a typical approach in multitask learning is to uncover a joint (latent) feature/subspace that bene-fits tasks individually ( Argyriou et al. , 2007 ; Gu and Zhou , 2009 ; Passos et al. , 2012 ). A similar idea has been adopted in domain adaptation, where the learned subspace reduces mismatch between source and tar-get domains ( Gretton et al. , 2009 ; Pan et al. , 2011 ). Although these approaches have proven successful in various applications, no previous work has fully in-vestigated the generalization ability of a subspace to unseen domains. Let X denote a nonempty input space and Y an arbi-trary output space. We define a domain to be a joint distribution P XY on X  X Y , and let P X X Y denote the set of all domains. Let P X and P Y|X denote the set of probability distributions P X on X and P Y | X on Y given X respectively.
 We assume domains are sampled from probability dis-tribution P on P X X Y which has a bounded second moment, i.e., the variance is well-defined. Domains are not observed directly. Instead, we observe N samples from P i XY and each P 1 XY , . . . , P N XY is sampled from P . Since in general P i XY 6 = P j XY , the samples in S are not with each sample S i . For brevity, we use P and P X interchangeably to denote the marginal distribution. Let H and F denote reproducing kernel Hilbert spaces (RKHSes) on X and Y with kernels k : X  X X  X  R and l : Y  X Y  X  R , respectively. Associated with H and F are mappings x  X   X  ( x )  X  X  and y  X   X  ( y )  X  X  induced by the kernels k (  X  ,  X  ) and l (  X  ,  X  ). Without loss of generality, we assume the feature maps of X and Y have zero means, i.e., Let  X  xx ,  X  yy ,  X  xy , and  X  yx be the covariance operators in and between the RKHSes of X and Y .
 Objective. Using the samples S , our goal is to pro-duce an estimate f : P X  X X  X  R that generalizes to some unknown distribution P t  X  P X ( Blanchard et al. , 2011 ). Since the performance of f depends in part on how dissimilar the test distribution P t is from those in the training samples, we propose to preprocess the data to actively reduce the dissimilarity between domains. Intuitively, we want to find transformation B in H that (i) minimizes the distance between em-pirical distributions of the transformed samples B ( S i ) and (ii) preserves the functional relationship between X and Y , i.e., Y  X  X |B ( X ). We formulate an opti-mization problem capturing these constraints below. 2.1. Distributional Variance First, we define the distributional variance, which mea-sures the dissimilarity across domains. It is convenient to represent distributions as elements in an RKHS ( Berlinet and Agnan , 2004 ; Smola et al. , 2007 ; Sripe-rumbudur et al. , 2010 ) using the mean map We assume that k ( x, x ) is bounded for any x  X  X  such is injective, i.e., all the information about the distribu-tion is preserved ( Sriperumbudur et al. , 2010 ). It also holds that E P [ f ] = h  X  P , f i H for all f  X  X  and any P . We decompose P into P X , which generates the marginal distribution P X , and P Y | X , which generates posteriors P Y | X . The data generating process begins by generating the marginal P X according to P X . Con-ditioned on P X , it then generate conditional P Y | X ac-cording to P Y | X . The data point ( x, y ) is generated according to P X and P Y | X , respectively. Given set of distributions P = { P 1 , P 2 . . . , P N } drawn according to P X , define N  X  N Gram matrix G with entries for i, j = 1 , . . . , N . Note that G ij is the inner product between kernel mean embeddings of P i and P j in H . Based on ( 2 ), we define the distributional variance, which estimates the variance of the distribution P X : Definition 1. Introduce probability distribution P on H with P (  X  P i ) = 1 variance operator of P , denoted as  X  := G  X  1 N G  X  G 1 N + 1 N G 1 N . The distributional variance is The following theorem shows that the distributional variance is suitable as a measure of divergence between domains.
 Theorem 1. Let  X  P = 1 N istic kernel, then V H ( P ) = 1 N and only if P 1 = P 2 =  X  X  X  = P N .
 To estimate V H ( P ) from N sample sets S = { S i } N i =1 drawn from P 1 , . . . , P N , we define block kernel and co-efficient matrices where n = Gram matrix evaluated between the sample S i and S j . Following ( 3 ), elements of the coefficient matrix Q  X  1 / ( N 2 n i n j ) otherwise. Hence, the empirical distri-butional variance is Theorem 2. The empirical estimator b V H ( S ) = tr( b  X ) = tr( KQ ) obtained from Gram matrix is a consistent estimator of V H ( P ) .
 2.2. Formulation of DICA DICA finds an orthogonal transform B onto a low-dimensional subspace ( m  X  n ) that minimizes the dis-tributional variance V H ( S ) between samples from S , i.e. the dissimilarity across domains . Simultaneously, we require that B preserves the functional relationship between X and Y , i.e. Y  X  X |B ( X ).
 Minimizing distributional variance. To sim-plify notation, we  X  X latten X  { ( x ( i ) k , y ( i ) k ) { ( x k , y k ) } n k =1 where n = P of B where  X  x = [  X  ( x 1 ) ,  X  ( x 2 ) , . . . ,  X  ( x n are n -dimensional coefficient vectors. Let B = [  X  1 ,  X  2 , . . . ,  X  m ] and onto b k , i.e.,  X   X  x = b  X  k  X  x =  X   X  k  X   X  x  X  x =  X  kernel on the B -projection of X is After applying transformation B , the empirical distri-butional variance between sample distributions is Preserving the functional relationship. The central subspace C is the minimal subspace that cap-tures the functional relationship between X and Y , i.e. Y  X  X | C  X  X . Note that in this work we generalize a linear transformation C  X  X to nonlinear one B ( X ). To find the central subspace we use the inverse regression framework, ( Li , 1991 ): Theorem 3. If there exists a central subspace C = [ c 1 , . . . , c m ] satisfying Y  X  X | C  X  X , and for any a  X  R d , E [ a  X  X | C  X  X ] is linear in { c  X  i X } m i =1 E [ X | Y ]  X  span {  X  xx c i } m i =1 .
 It follows that the bases C of the central subspace coincide with the m largest eigenvectors of V ( E [ X | Y ]) premultiplied by  X   X  1 xx . Thus, the basis c is the solution to the eigenvalue problem V ( E [ X | Y ]) X  xx c =  X   X  xx Alternatively, for each c k one may solve under the condition that c k is chosen to not be in the span of the previously chosen c k . In our case, x is mapped to  X  ( x )  X  H induced by the kernel k and B has nonlinear basis functions c k  X  H , k = 1 , . . . , m . This nonlinear extension implies that E [ X | Y ] lies on a function space spanned by {  X  xx c k } m k =1 , which coin-cide with the eigenfunctions of the operator V ( E [ X | Y ]) ( Wu , 2008 ; Kim and Pavlovic , 2011 ). Since we always work in H , we drop  X  from the notation below. To avoid slicing the output space explicitly ( Li , 1991 ; Wu , 2008 ), we exploit its kernel structure when esti-mating the covariance of the inverse regressor. The following result from Kim and Pavlovic ( 2011 ) states that, under a mild assumption, V ( E [ X | Y ]) can be ex-pressed in terms of covariance operators: Theorem 4. If for all f  X  X  , there exists g  X  X  such that E [ f ( X ) | y ] = g ( y ) for almost every y , then Let  X  y = [  X  ( y 1 ) , . . . ,  X  ( y n )] and L =  X   X  y  X  variance of inverse regressor ( 7 ) is estimated from the samples S as b V ( E [ X | Y ]) = b  X  xy b  X   X  1 yy b  X  yx Assuming inverses b  X   X  1 yy and b  X   X  1 xx exist, a straightfor-ward computation (see Supplementary) shows b where  X  smoothes the affinity structure of the output space Y , thus acting as a kernel regularizer. Since we are interested in the projection of  X  ( x ) onto the basis functions b k , we formulate the optimization in terms of  X  k . For a new test sample x t , the pro-jection onto basis function b k is k t  X  k , where k t = [ k ( x 1 , x t ) , . . . , k ( x n , x t )]. The optimization problem. Combining ( 6 ) and ( 8 ), DICA finds B = [  X  1 ,  X  2 , . . . ,  X  m ] that solves The numerator requires that B aligns with the bases of the central subspace. The denominator forces both dissimilarity across domains and the complexity of B to be small, thereby tightening generalization bounds, see  X  2.5 . Rewriting ( 9 ) as a constrained optimization (see Supplementary) yields Lagrangian where  X  is a diagonal matrix containing the Lagrange multipliers. Setting the derivative of ( 10 ) w.r.t. B to zero yields the generalized eigenvalue problem: Algorithm 1 Domain-Invariant Component Analysis Input: Parameters  X  ,  X  , and m  X  n .

Output: Projection B n  X  m and kernel e K n  X  n . 2: Supervised: C = L ( L + n X I )  X  1 K 2 . 3: Unsupervised: C = K 2 . 4: Solve 1 n CB = ( KQK + K +  X I ) B  X  for B . 5: Output B and e K  X  KBB  X  K . 6: The test kernel e K t  X  K t BB  X  K where K t n Transformation B corresponds to the m leading eigen-vectors of the generalized eigenvalue problem ( 11 ) 1 . The inverse regression framework based on covariance operators has two benefits. First, it avoids explicitly slicing the output space, which makes it suitable for high-dimensional output. Second, it allows for struc-tured outputs on which explicit slicing may be impos-sible, e.g., trees and sequences. Since our framework is based entirely on kernels, it is applicable to any type of input and output variables, as long as the correspond-ing kernels can be defined. 2.3. Unsupervised DICA In some application domains, such as image denois-ing, information about the target may not be available. We therefore derive an unsupervised version of DICA. Instead of preserving the central subspace, unsuper-vised DICA (UDICA) maximizes the variance of X in the feature space, which is estimated as 1 n tr( B  X  K 2 B ). Thus, UDICA solves Similar to DICA, the solution of ( 12 ) is obtained by solving the generalized eigenvalue problem UDICA is a special case of DICA where L = 1 n I and  X   X  0. Algorithm 1 summarizes supervised and unsu-pervised domain-invariant component analysis. 2.4. Relations to Other Methods The DICA and UDICA algorithms generalize many well-known dimension reduction techniques. In the su-pervised setting, if dataset S contains samples drawn from a single distribution P XY then we have KQK = 0 . Substituting  X  := KB gives the eigenvalue problem
L ( L + n X I )  X  1 K X  = K X   X , which corresponds to co-variance operator inverse regression (COIR) ( Kim and Pavlovic , 2011 ).
 If there is only a single distribution then unsupervised DICA reduces to KPCA since KQK = 0 and find-ing B requires solving the eigensystem KB = B  X  which recovers KPCA ( Sch  X olkopf et al. , 1998 ). If there are two domains, source P S and target P T , then UDICA is closely related  X  though not identical to  X  Transfer Component Analysis ( Pan et al. , 2011 ). This follows from the observation that V H ( { P S , P T } ) = k  X  2.5. A Learning-Theoretic Bound We bound the generalization error of a classifier trained after DICA-preprocessing. The main compli-cation is that samples are not identically distributed. We adapt an approach to this problem developed in Blanchard et al. ( 2011 ) to prove a generalization bound that applies after transforming the empirical sample using B . Recall that B =  X  x B .
 Define kernel  X  k on P  X  X as  X  k (( P , x ) , ( P  X  , x k P ( P , P  X  )  X  k X ( x, x  X  ). Here, k X is the kernel on H the kernel on distributions is k P ( P , P  X  ) :=  X  (  X  P where  X  is a positive definite kernel ( Christmann and Steinwart , 2010 ; Muandet et al. , 2012 ). Let  X  P denote the corresponding feature map.
 Theorem 5. Under reasonable technical assumptions, see Supplementary, it holds with probability at least 1  X   X  that, +tr( B  X  KB ) c 2 The LHS is the difference between the training error and expected error (with respect to the distribution on domains P  X  ) after applying B .
 The first term in the bound, involving tr( B  X  KQKB ), quantifies the distributional variance after applying the transform: the higher the distributional variance, the worse the guarantee, tying in with analogous re-sults in Ben-David et al. ( 2007 ; 2010 ). The second term in the bound depends on the size of the distortion tr( B  X  KB ) introduced by B : the more complicated the transform, the worse the guarantee.
 The bound reveals a tradeoff between reducing the dis-tributional variance and the complexity or size of the transform used to do so. The denominator of ( 9 ) is a sum of these terms, so that DICA tightens the bound in Theorem 5 .
 Preserving the functional relationship (i.e. central sub-space) by maximizing the numerator in ( 9 ) should re-rigorous demonstration has yet to be found. We illustrate the difference between the proposed al-gorithms and their single-domain counterparts using a synthetic dataset. Furthermore, we evaluate DICA in two tasks: a classification task on flow cytometry data and a regression task for Parkinson X  X  telemonitoring. 3.1. Toy Experiments We generate 10 collections of n i  X  Poisson (200) data points. The data in each collection is gener-ated according to a five-dimensional zero-mean Gaus-sian distribution. For each collection, the covariance of the distribution is generated from Wishart distribu-tion W (0 . 2  X  I 5 , 10). This step is to simulate differ-ent marginal distributions. The output value is y = sign( b  X  1 x +  X  1 )  X  log( | b  X  2 x + c +  X  2 | ), where b weight vectors, c is a constant, and  X  1 ,  X  2  X  N (0 , 1). Note that b 1 and b 2 form a low-dimensional subspace that captures the functional relationship between X and Y . We then apply the KPCA, UDICA, COIR, and DICA algorithms on the dataset with Gaussian RBF kernels for both X and Y with bandwidth pa-rameters  X  x =  X  y = 1,  X  = 0 . 1, and  X  = 10  X  4 . Fig. 2 shows projections of the training and three pre-viously unseen test datasets onto the first two eigenvec-tors. The subspaces obtained from UDICA and DICA are more stable than for KPCA and COIR. In par-ticular, COIR shows a substantial difference between training and test data, suggesting overfitting. 3.2. Gating of Flow Cytometry Data Graft-versus-host disease (GvHD) occurs in allogeneic hematopoietic stem cell transplant recipients when donor-immune cells in the graft recognize the recip-ient as  X  X oreign X  and initiate an attack on the skin, gut, liver, and other tissues. It is a significant clinical problem in the field of allogeneic blood and marrow transplantation. The GvHD dataset ( Brinkman et al. , 2007 ) consists of weekly peripheral blood samples ob-tained from 31 patients following allogenic blood and marrow transplant. The goal of gating is to identify CD3 + CD4 + CD8  X  + cells, which were found to have a high correlation with the development of GvHD ( Brinkman et al. , 2007 ). We expect to find a subspace of cells that is consistent to the biological variation be-tween patients, and is indicative of the GvHD develop-ment. For each patient, we select a dataset that con-tains sufficient numbers of the target cell populations. As a result, we omit one patient due to insufficient data. The corresponding flow cytometry datasets from 30 patients have sample sizes ranging from 1,000 to 10,000, and the proportion of the CD3 + CD4 + CD8  X  + cells in each dataset ranges from 10% to 30%, depend-ing on the development of the GvHD.
 To evaluate the performance of the proposed algo-
Methods Pooling SVM Distributional SVM n Methods Pooling Distributional Input 92.03  X  8.21 93.19  X  7.20 KPCA 91.99  X  9.02 93.11  X  6.83 COIR 92.40  X  8.63 92.92  X  8.20 UDICA 92.51  X  5.09 92.74  X  5.01
DICA 92.72  X  6.41 94.80  X  3.81 rithms, we took data from N = 10 patients for train-ing, and the remaining 20 patients for testing. We subsample the training sets and test sets to have 100, 500, and 1,000 data points (cells) each. We compare the SVM classifiers under two settings, namely, a pool-ing SVM and a distributional SVM. The pooling SVM disregards the inter-patient variation by combining all datasets from different patients, whereas the distribu-tional SVM also takes the inter-patient variation into account via the kernel function ( Blanchard et al. , 2011 ) tions. We use k 1 ( P i , P j ) = exp  X  X   X  P i  X   X  P j k 2  X 
P i is computed using k 2 . For pooling SVM, the ker-nel k 1 ( P i , P j ) is constant for any i and j . Moreover, we use the output kernel l ( y ( i ) k , y ( j ) l ) =  X  ( y  X  ( a, b ) is 1 if a = b , and 0 otherwise. We compare the performance of the SVMs trained on the prepro-cessed datasets using the KPCA, COIR, UDICA, and DICA algorithms. It is important to note that we are not defining another kernel on top of the prepro-cessed data. That is, the kernel k 2 for KPCA, COIR, UDICA, and DICA is exactly ( 5 ). We perform 10-fold cross validation on the parameter grids to optimize for accuracy.
 Table 1 reports average accuracies and their standard deviation over 30 repetitions of the experiments. For sufficiently large number of samples, DICA outper-forms other approaches. The pooling SVM and dis-tributional SVM achieve comparable accuracies. The average leave-one-out accuracies over 30 subjects are reported in Table 2 (see supplementary for more de-tail). 3.3. Parkinson X  X  Telemonitoring To evaluate DICA in a regression setting, we apply it to a Parkinson X  X  telemonitoring dataset 2 . The dataset consists of biomedical voice measurements from 42 people with early-stage Parkinson X  X  disease recruited for a six-month trial of a telemonitoring device for re-mote symptom progression monitoring. The aim is to predict the clinician X  X  motor and total UPDRS scoring of Parkinson X  X  disease symptoms from 16 voice mea-sures. There are around 200 recordings per patient. We adopt the same experimental settings as in  X  3.2 , except that we employ two independent Gaussian Pro-cess (GP) regression to predict motor and total UP-DRS scores. For COIR and DICA, we consider the to fully account for the affinity structure of the output variable. We set  X  3 to be the median of motor and to-tal UPDRS scores. The voice measurements from 30 patients are used for training and the rest for testing. Fig. 3 depicts the results. DICA consistently, though not statistically significantly, outperforms other ap-proaches, see Table 3 . Inter-patient (i.e. across do-main) variation worsens prediction accuracy on new patients. Reducing this variation with DICA improves the accuracy on new patients. Moreover, incorporat-ing the inter-subject variation via distributional GP regression further improves the generalization ability, see Fig. 3 .
Methods Pooling GP Regression Distributional GP Regression LLS 8.82  X  0.77 11.80  X  1.54 8.82  X  0.77 11.80  X  1.54 Input 9.58  X  1.06 12.67  X  1.40 8.57  X  0.77 11.50  X  1.56 KPCA 8.54  X  0.89 11.20  X  1.47 8.50  X  0.87 11.22  X  1.49 UDICA 8.67  X  0.83 11.36  X  1.43 8.75  X  0.97 11.55  X  1.52 COIR 9.25  X  0.75 12.41  X  1.63 9.23  X  0.90 11.97  X  2.09
DICA 8.40  X  0.76 11.05  X  1.50 8.35  X  0.82 10.02  X  1.01 To conclude, we proposed a simple algorithm called Domain-Invariant Component Analysis (DICA) for learning an invariant transformation of the data which has proven significant for domain generalization both theoretically and empirically. Theorem 5 shows the generalization error on previously unseen domains grows with the distributional variance. We also showed that DICA generalizes KPCA and COIR, and is closely related to TCA. Finally, experimental results on both synthetic and real-world datasets show DICA performs well in practice. Interestingly, the results also suggest that the distributional SVM, which takes into account inter-domain variation, outperforms the pooling SVM which ignores it.
 The motivating assumption in this work is that the functional relationship is stable or varies smoothly across domains. This is a reasonable assumption for automatic gating of flow cytometry data because the inter-subject variation of cell population makes it im-possible for domain expert to apply the same gating on all subjects, and similarly makes sense for Parkin-son X  X  telemonitoring data. Nevertheless, the assump-tion does not hold in many applications where the con-ditional distributions are substantially different. It re-mains unclear how to develop techniques that gener-alize to previously unseen domains in these scenarios. DICA can be adapted to novel applications by equip-ping the optimization problem with appropriate con-straints. For example, one can formulate a semi-supervised extension of DICA by forcing the invari-ant basis functions to lie on a manifold or preserve a neighborhood structure. Moreover, by incorporating the distributional variance as a regularizer in the ob-jective function, the invariant features and classifier can be optimized simultaneously.
 Acknowledgments We thank Samory Kpotufe and Kun Zhang for fruit-ful discussions and the three anonymous reviewers for insightful comments and suggestions that significantly improved the paper.
 A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Informa-tion Processing Systems 19 , pages 41 X 48. MIT Press, 2007.
 S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adaptation.
In Advances in Neural Information Processing Sys-tems 19 , pages 137 X 144. MIT Press, 2007.
 S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza,
F. Pereira, and J. W. Vaughan. A theory of learning from different domains. Machine Learning , 79:151 X  175, 2010.
 A. Berlinet and T. C. Agnan. Reproducing kernel Hilbert spaces in probability and statistics . Kluwer Academic Publishers, 2004.
 S. Bickel, M. Br  X uckner, and T. Scheffer. Discriminative learning under covariate shift. Journal of Machine Learning Research , pages 2137 X 2155, 2009.
 G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classification tasks to a new unla-beled sample. In Advances in Neural Information Processing Systems 24 , pages 2178 X 2186, 2011. R. R. Brinkman, M. Gasparetto, S.-J. J. Lee, A. J. Ribickas, J. Perkins, W. Janssen, R. Smiley, and
C. Smith. High-content flow cytometry and tempo-ral data analysis for defining a cellular signature of graft-versus-host disease. Biol Blood Marrow Trans-plant , 13(6):691 X 700, 2007. ISSN 1083-8791.
 A. Christmann and I. Steinwart. Universal kernels on Non-Standard input spaces. In Advances in Neural Information Processing Systems 23 , pages 406 X 414. MIT Press, 2010.
 K. Fukumizu, F. R. Bach, and M. I. Jordan. Kernel Dimensionality Reduction for Supervised Learning.
In Advances in Neural Information Processing Sys-tems 16 . MIT Press, Cambridge, MA, 2004.
 A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Sch  X olkopf. Dataset Shift in
Machine Learning , chapter Covariate Shift by Ker-nel Mean Matching, pages 131 X 160. MIT Press, 2009.
 Q. Gu and J. Zhou. Learning the shared subspace for multi-task clustering and transductive transfer clas-sification. In Proceedings of the 9th IEEE Interna-tional Conference on Data Mining , pages 159 X 168. IEEE Computer Society, 2009.
 M. Kim and V. Pavlovic. Central subspace dimension-ality reduction using covariance operators. IEEE
Transactions on Pattern Analysis and Machine In-telligence , 33(4):657 X 670, 2011.
 K.-C. Li. Sliced inverse regression for dimension re-duction. Journal of the American Statistical Asso-ciation , 86(414):316 X 327, 1991.
 K. Muandet, K. Fukumizu, F. Dinuzzo, and
B. Sch  X olkopf. Learning from distributions via sup-port measure machines. In Advances in Neural In-formation Processing Systems 25 , pages 10 X 18. MIT Press, 2012.
 S. J. Pan and Q. Yang. A survey on transfer learning.
IEEE Transactions on Knowledge and Data Engi-neering , 22(10):1345 X 1359, October 2010.
 S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis.
IEEE Transactions on Neural Networks , 22(2):199 X  210, 2011.
 A. Passos, P. Rai, J. Wainer, and H. D. III. Flexible modeling of latent task structures in multitask learn-ing. In Proceedings of the 29th international confer-ence on Machine learning , Edinburgh, UK, 2012. J. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in Machine Learning . MIT Press, 2009.
 B. Sch  X olkopf, A. Smola, and K.-R. M  X uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation , 10(5):1299 X 1319, July 1998. A. Smola, A. Gretton, L. Song, and B. Sch  X olkopf. A
Hilbert space embedding for distributions. In Pro-ceedings of the 18th International Conference In Al-gorithmic Learning Theory , pages 13 X 31. Springer-Verlag, 2007.
 B. K. Sriperumbudur, A. Gretton, K. Fukumizu,
B. Sch  X olkopf, and G. R. G. Lanckriet. Hilbert space embeddings and metrics on probability mea-sures. Journal of Machine Learning Research , 99: 1517 X 1561, 2010.
 G. Widmer and M. Kurat. Learning in the Presence of Concept Drift and Hidden Contexts. Machine Learning , 23:69 X 101, 1996.
 H.-M. Wu. Kernel sliced inverse regression with appli-cations to classification. Journal of Computational
