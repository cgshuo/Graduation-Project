 Recent advances in motion capture technology have fueled in terest in the analysis and synthesis of complex human motion for animation and tracking. Models b ased on the physics of masses and springs have produced some impressive results by using s ophisticated  X  X nergy-based X  learning methods[1] to estimate physical parameters from motion cap ture data[2]. But if we want to generate realistic human motion, we need to model all the complexitie s of the real dynamics and this is so difficult to do analytically that learning is likely to be ess ential. The simplest way to generate new motion sequences based on data is to concatenate parts of tra ining sequences [3]. Another method is to transform motion in the training data to new sequences by l earning to adjusting its style or other characteristics[4, 5, 6]. In this paper we focus on model dri ven analysis and synthesis but avoid the complexities involved in imposing physics-based constrai nts, relying instead on a  X  X ure X  learning approach in which all the knowledge in the model comes from th e data.
 Data from modern motion capture systems is high-dimensiona l and contains complex non-linear relationships between the components of the observation ve ctor, which usually represent joint angles with respect to some skeletal structure. Hidden Markov mode ls cannot model such data efficiently To model N bits of information about the past history they require 2 N hidden states. To avoid this exponential explosion, we need a model with distributed (i. e. componential) hidden state that has a representational capacity which is linear in the number of c omponents. Linear dynamical systems satisfy this requirement, but they cannot model the complex non-linear dynamics created by the non-linear properties of muscles, contact forces of the foo t on the ground and myriad other factors. In general, using distributed binary representations for h idden state in directed models of time series makes inference difficult. If, however, we use a Restricted B oltzmann Machine (RBM) to model the probability distribution of the observation vector at e ach time frame, the posterior over latent variables factorizes completely, making inference easy. T ypically, RBMs use binary logistic units for both the visible data and hidden variables, but in our app lication the data (comprised of joint angles) is continuous. We thus use a modified RBM in which the  X  visible units X  are linear, real-valued variables that have Gaussian noise[7, 8]. The graphi cal model has a layer of visible units v and a layer of hidden units h ; there are undirected connections between layers but no con nections within a layer. For any setting of the hidden units, the distr ibution of each visible unit is defined by where  X  data to have zero mean and unit variance. We have found that fix ing  X  well even though we would expect a good model to predict the da ta with much higher precision). The main advantage of using this undirected,  X  X nergy-based  X  model rather than a directed  X  X elief net X  is that inference is very easy because the hidden units b ecome conditionally independent when the states of the visible units are observed. The conditiona l distributions (assuming  X  where f ( ) is the logistic function, N (  X , V ) is a Gaussian, b j and visible unit i respectively, and w Maximum likelihood learning is slow in an RBM but learning st ill works well if we approximately follow the gradient of another function called the contrast ive divergence[9]. The learning rule is: where the first expectation (over hidden unit activations) i s with respect to the data distribution and the second expectation is with respect to the distribution o f  X  X econstructed X  data. The reconstructions are generated by starting a Markov chain at the data distribu tion, updating all the hidden units in parallel by sampling (Eq. 2) and then updating all the visibl e units in parallel by sampling (Eq. 3). not vice versa . The learning rule for the hidden biases is just a simplified v ersion of Eq. 4: 2.1 The conditional RBM model The RBM we have described above models static frames of data, but does not incorporate any tem-poral information. We can model temporal dependencies by tr eating the visible variables in the previous time slice as additional fixed inputs [10]. Fortuna tely, this does not complicate inference. We add two types of directed connections (Figure 2): autoreg ressive connections from the past n configurations (time steps) of the visible units to the curre nt visible configuration, and connections from the past m visibles to the current hidden configuration. The addition o f these directed con-nections turns the RBM into a conditional RBM (CRBM). In our e xperiments, we have chosen n = m = 3 . These are, however, tunable parameters and need not be the s ame for both types of directed connections. To simplify discussion, we will assu me n = m and refer to n as the order of the model. Figure 1: In a trained model, probabilities of each feature b eing  X  X n X  conditional on the data at the visible units. Shown is a 100-hidden unit model, and a sequen ce which contains (in order) walking, sitting/standing (three times), walking, crouching, and r unning. Rows represent features, columns represent sequential frames. Inference in the CRBM is no more difficult than in the standard RBM. Given the data at time t, t  X  1 , . . . , t  X  n , the hidden units at time t are conditionally independent. We can still use contrastiv e divergence for training the CRBM. The only change is that whe n we update the visible and hidden units, we implement the direct ed con-nections by treating data from previous time steps as a dynam ically changing bias. The contrastive divergence learning rule fo r hidden biases is given in Eq. 5 and the equivalent learning rule for t he tem-poral connections that determine the dynamically changing hidden unit biases is: unit i at time t  X  q to hidden unit j for q = 1 ..n . Similarly, the learning rule for the autoregressive connections that dete rmine the dynamically changing visible unit biases is: unit i .
 The autoregressive weights can model short-term temporal s tructure very well, leaving the hidden units to model longer-term, hi gher level structure. During training, the states of the hidden units a re deter-mined by both the input they receive from the observed data an d the input they receive from the previous time slices. The learni ng rule for W remains the same as a standard RBM, but has a different ef-fect because the states of the hidden units are now influenced by the previous visible units. We do not attempt to model the first n frames of each sequence.
 While learning a model of motion, we do not need to proceed seq uentially through the training data sequences. The updates are only conditional on the past n time steps, not the entire sequence. As long as we isolate  X  X hunks X  of frames (the size depending on t he order of the directed connections), these can be mixed and formed into mini-batches. To speed up t he learning, we assemble these chunks of frames into  X  X alanced X  mini-batches of size 100 .
 We randomly assign chunks to different mini-batches so that the chunks in each mini-batch are as uncorrelated as possible. To save computer memory, time f rames are not actually replicated in mini-batches; we simply use indexing to simulate the  X  X hunk ing X  of frames. 2.2 Approximations Our training procedure relies on several approximations, m ost of which are chosen based on ex-perience training similar networks. While training the CRB M, we replaced v by its expected value and we also used the expected value of v of activation of the hidden units. However, to compute the on e-step reconstructions of the data, we used stochastically chosen binary values of the hidden unit s. This prevents the hidden activities from transmitting an unbounded amount of information from the da ta to the reconstruction [11]. While updating the directed visible-to-hidden connection s (Eq. 6) and the symmetric undirected connections (Eq. 4) we used the stochastically chosen binar y values of the hidden units in the first term (under the data), but replaced h tion). We took this approach because the reconstruction of t he data depends on the binary choices made when selecting hidden state. Thus when we infer the hidd ens from the reconstructed data, the probabilities are highly correlated with the binary hid den states inferred from the data. On the other hand, we stop after one reconstruction, so the binary c hoice of hiddens from the reconstruction doesn X  X  correlate with any other terms, and there is no point including this extra noise. Lastly, we note that the fine-tuning procedure as a whole is ma king a crude approximation in addition to the one made by contrastive divergence. The inference ste p, conditional on past visible states, is approximate because it ignores the future (it does not do s moothing). Because of the directed connections, exact inference within the model should inclu de both a forward and backward pass through each sequence (we currently perform only a forward p ass). We have avoided a backward pass because missing values create problems in undirected m odels, so it is hard to perform learning efficiently using the full posterior. Compared with an HMM, t he lack of smoothing is a loss, but this is more than offset by the exponential gain in representatio nal power. We used data from the CMU Graphics Lab Motion Capture Databas e as well as from [12] (see acknowledgments). The processed data consists of 3D joint a ngles derived from 30 (CMU) or 17 (MIT) markers plus a root (coccyx, near the base of the back) o rientation and displacement. For both datasets, the original data was captured at 120Hz; we ha ve downsampled it to 30Hz. Six of the joint angle dimensions in the original CMU data had constant values, so they were elim-inated. Each of the remaining joint angles had between one an d three degrees of freedom. All of the joint angles and the root orientation were converted fro m Euler angles to the  X  X xponential map X  parameterization [13]. This was done to avoid  X  X imbal lock X  and discontinuities. (The MIT data was already expressed in exponential map form and did not nee d to be converted.) coordinate system. In order to respect physics, we wanted ou r final representation to be invariant to ground-plane translation and to rotation about the gravi tational vertical. We represented each ground-plane translation by an incremental  X  X orwards X  vec tor and an incremental  X  X ideways X  vector relative to the direction the person was currently facing, b ut we represented height non-incrementally by the distance above the ground plane. We represented orien tation around the gravitational vertical by the incremental change, but we represented the other two r otational degrees of freedom by the absolute pitch and roll relative to the direction the person was currently facing.
 The final dimensionality of our data vectors was 62 (for the CM U data) and 49 (for the MIT data). Note that we eliminated exponential map dimensions that wer e constant zero (corresponding to joints with a single degree of freedom). As mentioned in Sec. 2, each component of the data was normalized to have zero mean and unit variance.
 One advantage of our model is the fact that the data does not ne ed to be heavily preprocessed or dimensionality reduced. Brand and Hertzmann [4] apply PCA t o reduce noise and dimensionality. The autoregressive connections in our model can be thought o f as doing a kind of  X  X hitening X  of the data. Urtasun et al. [6] manually segment data into cycles an d sample at regular time intervals using quaternion spherical interpolation. Dimensionality redu ction becomes problematic when a wider range of motions is to be modeled. After training our model using the updates described above, we can demonstrate in several ways what it has learned about the structure of human motion. Perh aps the most direct demonstration, which exploits the fact that it is a probability density mode l of sequences, is to use the model to generate de-novo a number of synthetic motion sequences. Video files of these s equences are avail-able on the website mentioned in the abstract; these motions have not been retouched by hand in any motion editing software. Note that we also do not have to k eep a reservoir of training data sequences around for generation -we only need the weights of the model and a few valid frames for initialization.
 Causal generation from a learned model can be done on-line wi th no smoothing, just like the learning procedure. The visible units at the last few time steps deter mine the effective biases of the visible and hidden units at the current time step. We always keep the p revious visible states fixed and perform alternating Gibbs sampling to obtain a joint sample from the conditional RBM. This picks history. Generation requires initialization with n time steps of the visible units, which implicitly determine the  X  X ode X  of motion in which the synthetic sequen ce will start. We used randomly drawn consecutive frames from the training data as an initia l configuration. 4.1 Generation of walking and running sequences from a singl e model In our first demonstration, we train a single model on data con taining both walking and running motions; we then use the learned model to generate both types of motion, depending on how it is initialized. We trained 2 on 23 sequences of walking and 10 sequences of jogging (from s ubject 35 in the CMU database). After downsampling to 30Hz, the traini ng data consisted of 2813 frames. Figure 3 shows a walking sequence and a running sequence gene rated by the same model, using al-ternating Gibbs sampling (with the probability of hidden un its being  X  X n X  conditional on the current and previous three visible vectors). Since the training dat a does not contain any transitions between walking and running (and vice-versa ), the model will continue to generate walking or running mo-tions depending on where it is initialized. 4.2 Learning transitions between various styles In our second demonstration, we show that our model is capabl e of learning not only several homo-geneous motion styles but also the transitions between them , when the training data itself contains examples of such transitions. We trained on 9 sequences (fro m the MIT database, file Jog1 M ) con-taining long examples of running and jogging, as well as a few transitions between the two styles. After downsampling to 30Hz, this provided us with 2515 frame s. Training was done as before, ex-cept that after the model was trained, an identical 200 hidde n-unit model was trained on top of the first model (see Sec. 5). The resulting two-level model was us ed to generate data. A video available on the website demonstrates our model X  X  ability to stochast ically transition between various motion styles during a single generated sequence. 4.3 Introducing transitions using noise In our third demonstration, we show how transitions between motion styles can be generated even when such transitions are absent in the data. We use the same m odel and data as described in Sec. 4.1, where we have learned on separate sequences of walking a nd running. To generate, we use the same sampling procedure as before, except that at each time w e stochastically choose the hidden states (given the current and previous three visible vector s) we add a small amount of Gaussian noise to the hidden state biases. This encourages the model t o explore more of the hidden state space without deviating too far the current motion. Applyin g this  X  X oisy X  sampling approach, we see that the generated motion occasionally transitions bet ween learned styles. These transitions appear natural (see the video on the website). 4.4 Filling in missing data Due to the nature of the motion capture process, which can be a dversely affected by lighting and environmental effects, as well as noise during recording, m otion capture data often contains missing or unusable data. Some markers may disappear ( X  X ropout X ) fo r long periods of time due to sen-sor failure or occlusion. The majority of motion editing sof tware packages contain interpolation methods to fill in missing data, but this leaves the data unnat urally smooth. These methods also rely on the starting and end points of the missing data, so if a marker goes missing until the end of a sequence, na  X  X ve interpolation will not work. Such meth ods often only use the past and future data from the single missing marker to fill in that marker X  X  mi ssing values, but since joint angles are highly correlated, substantial information about the plac ement of one marker could be gained from the others. Our trained model has the ability to easily fill in such missing data, regardless of where the dropouts occur in a sequence. Due to its approximate infe rence method which does not rely on a backward pass through the sequence, it also has the ability to fill in such missing data on-line. Filling in missing data with our model is very similar to gene ration. We simply clamp the known data to the visible units, initialize the missing data to som ething reasonable (for example, the value at the previous frame), and alternate between stochastical ly updating the hidden and visible units, with the known visible states held fixed.
 To demonstrate filling in, we trained a model exactly as descr ibed in Sec. 4.1 except that one walking and one running sequence were left out of the training data to be used as test data. For each of these walking and running test sequences, we erased two different sets of joint angles, starting halfway through the test sequence. These sets were the joints in (1) t he left leg, and (2) the entire upper body. As seen in the video files on the website, the quality of t he filled-in data is excellent and is hardly distinguishable from the original ground truth of th e test sequence. Figure 4 demonstrates the model X  X  ability to predict the three angles of rotation of th e left hip.
 For the walking sequence (of length 124 frames), we compared our model X  X  performance to nearest neighbor interpolation, a simple method where for each fram e, the values on known dimensions are compared to each example in the training set to find the closes t match (measured by Euclidean dis-tance in the normalized angle space). The unknown dimension s are then filled in using the matched example. As reconstruction from our model is stochastic, we repeated the experiment 100 times and report the mean. For the missing leg, mean squared reconstru ction error per joint using our model was 8 . 78 , measured in normalized joint angle space, and summed over t he 62 frames of interest. Using nearest neighbor interpolation, the error was greate r: 11 . 68 . For the missing upper body, mean squared reconstruction error per joint using our model was 20 . 52 . Using nearest neighbor interpolation, again the error was greater: 22 . 20 . Figure 4: The model successfully fills in missing data using o nly the previous values of the joint angles (through the temporal connections) and the current a ngles of other joints (through the RBM is similar to the first). The original data is shown on a solid l ine, the model X  X  prediction is shown on a dashed line, and the results of nearest neighbor interpo lation are shown on a dotted line (see a video on the website). Once we have trained the model, we can add layers like in a Deep Belief Network [14]. The previous layer CRBM is kept, and the sequen ce of hidden state vectors, while driven by the data, is treated as a new ki nd of  X  X ully observed X  data. The next level CRBM has the same architectur e as the first (though we can alter the number of its units) and is trained in the exact same way. Upper levels of the network can then model higher-order structure.
 This greedy procedure is justified using a variational bound [14]. A two-level model is shown in Figure 5.
 We can also consider two special cases of the higher-level mo del. If we keep only the visible layer, and its n -th order directed connections, we have a standard AR( n ) model with Gaussian noise. If we take the two-hidden layer model and delete the first-level autoregressive conne ctions, as well as both sets of visible-to-hidden directed connections, we ha ve a simplified model that can be trained in 2 stages: first learning a static ( iid) model of pairs or triples of time frames, then using the inferred hidd en states to train a  X  X ully-observed X  sigmoid belief net that captures the tem poral structure of the hidden states. We have introduced a generative model for human motion based on the idea that local constraints and global dynamics can be learned efficiently by a condition al Restricted Boltzmann Machine. Once trained, our models are able to efficiently capture comp lex non-linearities in the data without sophisticated pre-processing or dimensionality reductio n. The model has been designed with human motion in mind, but should lend itself well to other high-dim ensional time series. In relatively low-dimensional or unstructured data (for ex ample if we were to model a single isolated joint) a single-layer model might be expected to have difficu lty since such cyclic time series contain cycle. It would be possible to preserve the global phase info rmation by using a much higher order model, but for higher dimensional data such as full body moti on capture this is unnecessary because the whole configuration of joint angles and angular velociti es never has any phase ambiguity. So the single-layer version of our model actually performs muc h better on higher-dimensional data. Models with more hidden layers are able to implicitly model l onger-term temporal information, and thus will mitigate this effect.
 We have demonstrated that our model can effectively learn di fferent styles of motion, as well as the transitions between these styles. This differentiates our approach from PCA-based approaches which only accurately model cyclic motion, and additionall y must build separate models for each type of motion. The ability of the model to transition smooth ly, however, is dependent on having sufficient examples of such transitions in the training data . We plan to train on larger datasets en-compassing such transitions between various styles of moti on. If we augment the data with some static skeletal and identity parameters (in essence mappin g a person X  X  unique identity to a set of fea-tures), we should be able to use the same generative model for many different people, and generalize individual characteristics from one type of motion to anoth er. Finally, our model is not limited to a single source of data. In the future, we hope to integrate low -level vision data captured at the same time as motion; we could then learn the correlations between the vision stream and the joint angles. Acknowledgments The first data set used in this project was obtained from mocap.cs.cmu.edu . This database was created with funding from NSF EIA-0196217. The second da ta set used in this project was obtained from http://people.csail.mit.edu/ehsu/work/sig05stf/ . For Matlab playback of motion and generation of videos, we have used Nei l Lawrence X  X  motion capture toolbox ( http://www.dcs.shef.ac.uk/  X  neil/mocap/ ).

