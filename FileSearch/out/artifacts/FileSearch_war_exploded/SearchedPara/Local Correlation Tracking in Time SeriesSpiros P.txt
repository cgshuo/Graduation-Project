
We address the problem of capturing and tracking lo-cal correlations among time evolving time series. Our ap-proach is based on comparing the local auto-covariance matrices (via their spectral decompositions) of each series and generalizes the notion of linear cross-correlation. In this way, it is possible to concisely capture a wide variety of local patterns or trends. Our method produces a gen-eral similarity score, which evolves over time, and accu-rately reflects the changing relationships. Finally, it can also be estimated incrementally, in a streaming setting. We demonstrate its usefulness, robustness and efficiency on a wide range of real datasets.
The notion of correlation (or, similarity) is important, since it allows us to discover groups of objects with sim-ilar behavior and, consequently, discover potential anoma-lies which may be revealed by a change in correlation. In this paper we consider correlation among time series which often exhibit two important properties.

First, their characteristics may change over time. In fact, this is a key property of semi-infinite streams, where data arrive continuously. The term time-evolving is often used in this context to imply the presence of non-stationarity. In this case, a single, static correlation score for the entire time se-ries is less useful. Instead, it is desirable to have a notion of correlation that also evolves with time and tracks the chang-ing relationships. On the other hand, a time-evolving cor-relation score should not be overly sensitive to transients; if the score changes wildly, then its usefulness is limited.
The second property is that many time series exhibit strong but fairly complex, non-linear correlations. Tradi-tional measures, such as the widely used cross-correlation coefficient (or, Pearson coefficient ), are less effective in cap-turing these complex relationships. From a general point of view, the estimation of a correlation score relies on an assumed joint model of the two sequences. For example, the cross-correlation coefficient assumes that pairs of values from each series follow a simple linear relationship. Con-sequently, we seek a concise but powerful model that can capture various trend or pattern types.

Data with such features arise in several application do-mains, such as:  X  Monitoring of network traffic flows or of system per- X  Financial applications, where prices may exhibit linear  X  Medical applications, such as EEGs (electroen-
Figure 1 shows the exchange rates for the French Franc (blue) and the Spanish Peseta (red) versus the US Dollar, over a period of about 10 years. An approximate timeline of major events in the European Monetary Union (EMU) is also included, which may help explain the behavior of each currency. The global cross-correlation coefficient of the two series is 0.30, which is statistically significant (ex-ceeding the 95% confidence interval of  X  0 . 04 ). The next local extremum of the cross-correlation function is 0.34, at a lag of 323 working days, meaning that the overall behav-ior of the Franc is similar to that of the Peseta 15 months ago, when compared over the entire decade of daily data.
Figure 1. Illustration of tracking time-evolving local correlations (see also Figure 6).
This information makes sense and is useful in its own right. However, it is not particularly enlightening about the relationship of the two currencies as they evolve over time. Similar techniques can be employed to characterize corre-lations or similarities over a period of, say, a few years. But what if we wish to track the evolving relationships over shorter periods, say a few weeks? The bottom part of Fig-ure 1 shows our local correlation score computed over a window of four weeks (or 20 values). It is worth noting that most major EMU events are closely accompanied by a correlation drop, and vice versa. Also, events related to anticipated regulatory changes are typically preceded , but not followed, by correlation breaks. Overall, our correla-tion score smoothly tracks the evolving correlations among the two currencies (cf. Figure 6).

To summarize, our goal is to define a powerful and con-cise model that can capture complex correlations between time series. Furthermore, the model should allow tracking the time-evolving nature of these correlations in a robust way, which is not susceptible to transients. In other words, the score should accurately reflect the time-varying relation-ships among the series.
 Contributions. Our main contributions are the following:  X  We introduce LoCo (LOcal COrrelation), a time- X  The model upon which our score is based can capture  X  Our approach is also amenable to robust streaming es-We illustrate our proposed method or real data, discussing its qualitative interpretation, comparing it against natural al-ternatives and demonstrating its robustness and efficiency. The rest of the paper is organized as follows: In Sec-tion 2 we briefly describe some of the necessary background and notation. In Section 3 we define some basic notions. Section 4 describes our proposed approach and Section 5 presents our experimental evaluation on real data. Finally, in Section 6 we describe some of the related work and Sec-tion 7 concludes.
In the following, we use lowercase bold letters for col-umn vectors ( u , v , . . . ) and uppercase bold for matrices ( U , V , . . . ). The inner product of two vectors is denoted by x T y and the outer product by x  X  y  X  xy T . The Eu-clidean norm of x is k x k . We denote a time series process as an indexed collection X of random variables X t , t  X  N , i.e., X = { X 1 , X 2 , . . . , X t , . . . }  X  { X t } t  X  N of generality, we will assume zero-mean time series, i.e., E[ X t ] = 0 for all t  X  N . The values of a particular realiza-tion of X are denoted by lower-case letters, x t  X  R , at time t  X  N .
 Covariance and autocovariance. The covariance of two random variables X , Y is defined as Cov[ X, Y ] = E[( X  X  the symmetric matrix defined by c ij := Cov[ X i , X j ] , for 1  X  i, j  X  m . If x 1 , x 2 , . . . , x n is a collection of n obser-
In the context of a time series process { X t } t  X  N are interested in the relationship between values at differ-ent times. To that end, the autocovariance is defined as  X  ity follows from the zero-mean assumption. By definition,  X  Spectral decomposition. Any real symmetric matrix is always equivalent to a diagonal matrix, in the following sense.
 it is always possible to find a column-orthonormal matrix A = U X U T .
 Thus, given any vector x , we can write U T ( Ax ) =  X  ( U T x ) , where pre-multiplication by U T amounts to a change of coordinates. Intuitively, if we use the coordinate system defined by U , then Ax can be calculated by simply scaling each coordinate independently of all the rest (i.e., multiplying by the diagonal matrix  X  ).
 note its eigenvectors by u i ( A ) and the corresponding eigen-values by  X  i ( A ) , in order of decreasing magnitude, where 1  X  i  X  n . The matrix U k ( A ) has the first k eigenvectors as its columns, where 1  X  k  X  n .

The covariance matrix C of m variables is symmetric by definition. Its spectral decomposition provides the di-rections in R m that  X  X xplain X  the most of the variance. If we project [ X 1 , X 2 , . . . , X m ] T onto the subspace spanned by U k ( C ) , we retain the largest fraction of variance among any other k -dimensional subspace [11]. Finally, the auto-covariance matrix of a finite-length time series is also sym-metric and its eigenvectors typically capture both the key oscillatory (e.g., sinusoidal) as well as aperiodic (e.g., in-creasing or decreasing) trends that are present [6, 7].
Our goal is to derive a time-evolving correlation scores that tracks the similarity of time-evolving time series. Thus, our method should have the following properties: (P1) Adapt to the time-varying nature of the data, (P2) Employ a simple, yet powerful and expressive joint (P3) The derived score should be robust, reflecting the (P4) It should be possible to estimate it efficiently. We will address most of these issues in Section 4, which de-scribes our proposed method. In this section, we introduce some basic definitions to facilitate our discussion. We also introduce localized versions of popular similarity measures for time series.

As a first step to deal with (P1), any correlation score at time instant t  X  N should be based on observations in the  X  X eighborhood X  of that instant. Therefore, we introduce the notation x t,w  X  R w for the subsequence of the series, starting at t and having length w , Furthermore, any correlation score should satisfy two ele-mentary and intuitive properties.
 Definition 1 (Local correlation score) . Given a pair of time series X and Y , a local correlation score is a sequence c ( X, Y ) of real numbers that satisfy the following prop-erties, for all t  X  N :
Before proceeding to describe our approach, we formally define a natural extension of a method that has been widely used for global correlation or similarity among  X  X tatic X  time series.
 Pearson coefficient. A natural local adaptation of cross-correlation is the following: Definition 2 (Local Pearson correlation) . The local Pearson correlation is the linear cross-correlation coefficient  X  t ( X, Y ) := where the last equality follows from E[ X t ] = E[ Y t ] = 0 .
It follows directly from the definition that  X  t satisfies the two requirements, 0  X   X  t ( X, Y )  X  1 and  X  t ( X, Y ) =  X  ( Y, X ) .
 Symbol Description
U , V Matrix (uppercase bold). u , v Column vector (lowercase bold). x t Time series, t  X  N . w Window size. x t,w Window starting at t , x t,w  X  R w . m Number of windows (typically, m = w ).  X  Exponential decay weight, 0  X   X   X  1 .  X 
 X  t Local autocorrelation matrix estimate. u i ( A ) , Eigenvectors and corresponding  X  i ( A ) eigenvalues of A .

U k ( A ) Matrix of k largest eigenvectors of A . ` t LoCo score.  X  t Pearson local correlation score.
 In this section we develop our proposed approach, the Local Correlation (LoCo) score. Returning to properties (P1) X (P4) listed in the beginning of Section 3, the next sec-tion addresses primarily (P1) and Section 4.2 continues to address (P2) and (P3). Next, Section 4.3 shows how (P4) can also be satisfied and, finally, Section 4.4 discusses the time and space complexity of the various alternatives.
The first step towards tracking local correlations at time t  X  N is restricting, in some way, the comparison to the  X  X eighborhood X  of t , which is the reason for introducing the notion of a window x t,w .

If we stop there, we can compare the two windows x t,w and y t,w directly . If, in addition, the comparison involves capturing any linear relationships between localized values of X and Y , this leads to the local Pearson correlation score  X  . However, this joint model of the series it is too simple, leading to two problems: (i) it cannot capture more complex relationships, and (ii) it is too sensitive to transient changes, often leading to widely fluctuating scores.

Intuitively, we address the first issue by estimating the full autocovariance matrix of values  X  X ear X  t , and avoid making any assumptions about stationarity (as will be ex-plained later). Any estimate of the local autocovariance at time t needs to be based on a  X  X ocalized X  sample set of win-dows with length w . We will consider two possibilities:  X  Sliding (a.k.a. boxcar) window (see Figure 2a): We
Figure 2. Local auto-covariance; shading cor-responds to weight.  X  Exponential window (see Figure 2b): We use all win-These two alternatives are illustrated in Figure 2, where the shading corresponds to the weight. We will explain how to  X  X ompare X  the local autocovariance matrices of two series in Section 4.2. Next, we formally define these estimators. Definition 3 (Local autocovariance, sliding window) . Given a time series X , the local autocovariance matrix es-timator  X   X  t using a sliding window is defined at time t  X  N as The sample set of m windows is  X  X entered X  around time t . We typically fix the number of windows to m = w , so that  X   X  ( X, w, m ) = factor of 1 /m is ignored, since it is irrelevant for the eigen-vectors of  X   X  t .
 Definition 4 (Local autocovariance, exponential window) . Given a time series X , the local autocovariance matrix es-timator  X   X  t at time t  X  N using an exponential window is Similar to the previous definition, we ignore the normaliza-tion factor (1  X   X  ) / (1  X   X  t +1 ) . In both cases, we may omit some or all of the arguments X , w , m ,  X  , when they are clear from the context.

Under certain assumptions, the equivalent window cor-responding to an exponential decay factor  X  is given by m = (1  X   X  )  X  1 [22]. However, one of the main benefits of the exponential window is based on the following simple observation.
 Property 1. The sliding window local autocovariance fol-lows the equation whereas for the exponential window it follows the equation An incremental update to the sliding window estimator has rank 2, whereas an update to the exponential window es-timator has rank 1, which can be handled more efficiently. Also, updating the sliding window estimator requires sub-cessity, the past w values of X need to be stored (or, in general, the past m values), in addition to the  X  X uture X  w values of x t,w that need to be buffered. Since, as we will see, the local correlation scores derived from these estima-tors are very close, using an exponential window is more desirable.

The next simple lemma will be useful later, to show that  X  t is included as a special case of the LoCo score. Intu-itively, if we use an instantaneous estimate of the local au-tocovariance  X   X  t , which is based on just the latest sample window x t,w , its eigenvector is the window itself. Lemma 1. If m = 1 or, equivalently,  X  = 0 , then Proof. In this case,  X   X  t = x t,w  X  x t,w with rank 1. Its row and column space are span x t,w , whose orthonormal basis is, trivially, x t,w / k x t,w k X  u 1 (  X   X  t ) . The fact that  X  k x u k x
Given the estimates  X   X  t ( X ) and  X   X  t ( Y ) for the two series, the next step is how to  X  X ompare X  them and extract a corre-lation score. Intuitively, we want to extract the  X  X ey infor-mation X  contained in the autocovariance matrices and mea-sure how close they are. This is precisely where the spectral decomposition helps. The eigenvectors capture the key ape-riodic and oscillatory trends, even in short, non-stationary series [6, 7]. These trends explain the largest fraction of the variance. Thus, we will use the subspaces spanned by the first few ( k ) eigenvectors of each local autocovariance ma-trix to locally characterize the behavior of each series. The following definition formalizes this notion.
 Definition 5 (LoCo score) . Given two series X and Y , their LoCo score is defined by where U X  X  U k (  X   X  t ( X )) and U Y  X  U k (  X   X  t ( Y )) are the eigenvector matrices of the local autocovariance ma-trices of X and Y , respectively, and u X  X  u 1 (  X   X  t ( X )) and u the largest eigenvalue.
In the above equation, U T X u Y is the projection of u Y onto the subspace spanned by the columns of the or-thonormal matrix U X . The absolute cosine of the angle  X   X   X  ( u Y , span U X ) =  X  ( u Y , U T X u Y ) is | cos  X  | = k U T X u Y k / k u Y k = k U T X u Y k , since k u Y k = 1 (see Figure 3). Thus, ` t is the average of the cosines | cos  X  ( u Y , span U X ) | and | cos  X  ( u X , span U Y this definition, it follows that 0  X  ` t ( X, Y )  X  1 and ` t ( X, Y ) = ` t ( Y, X ) . Furthermore, ` t ( X, Y ) = ` (  X  X, Y ) = ` t ( Y,  X  X ) = ` t (  X  X,  X  Y )  X  X s is also the case with  X  t ( X, Y ) .

Intuitively, if the two series X , Y are locally similar, then the principal eigenvector of each series should lie within the subspace spanned by the principal eigenvectors of the other series. Hence, the angles will be close to zero and the cosines will be close to one.

The next simple lemma reveals the relationship between  X  and ` t .
 Lemma 2. If m = 1 (whence, k = 1 necessarily), then ` =  X  t .
 Proof. From Lemma 1 it follows that U X = u X = x t,w / k x t,w k and U Y = u Y = y t,w / k y t,w k . From the definitions of ` t and  X  t , we have ` t = 1 2 Choosing k . As we shall see also see in Section 5, the directions of x t,w and y t,w may vary significantly, even at neighboring time instants. As a consequence, the Pearson score  X  t (which is essentially based on the instantaneous es-timate of the local autocovariance) is overly sensitive. How-ever, if we consider the low-dimensional subspace which is (mostly) occupied by the windows during a short period of time (as LoCo does), this is much more stable and less sus-ceptible to transients, while still able to track changes in local correlation.

One approach is to set k based on the fraction of variance to retain (similar to criteria used in PCA [11], as well as in spectral estimation [19]). A simpler practical choice is to fix k to a small value; we use k = 4 throughout all experi-ments. From another point of view, key aperiodic trends are captured by one eigenvector, whereas key oscillatory trends manifest themselves in a pair of eigenvectors with similar eigenvalues [6, 7]. The former (aperiodic trends) are mostly
Figure 4. First four eigenvectors ( w = 40 ) for (a) periodic series, x t = 2 sin (2  X t/ 40) + sin (2  X t/ 20) and, (b) polynomial trend, x t = t 3 . present during  X  X nstable X  periods of time, while the latter (periodic, or oscillatory trends) are mostly present during  X  X table X  periods. The eigen-decomposition can capture both and fixing k amounts to selecting a number of trends for our comparison. The fraction of variance captured in the real series of our experiments with k = 4 is typically between 90 X 95%.
 Choosing w . Windows are commonly used in stream and signal processing applications. The size w of each window x t,w (and, consequently, the size w  X  w of the autocovari-ance matrix  X   X  t ) essentially corresponds to the time scale we are interested in.

As we shall also see in Section 5, the LoCo score ` t de-rived from the local autocovariances changes gradually and smoothly with respect to w . Thus, if we set the window size to any of, say, 55, 60 or 65 seconds, we will qualita-tively get the same results, corresponding approximately to patterns in the minute scale. Of course, at widely different time scales, the correlation scores will be different. If desir-able, it is possible to track the correlation score at multiple scales, e.g., hour, day, month and year. If buffer space and processing time are a concern, either a simple decimated moving average filtering scheme or a more elaborate hier-archical SVD scheme (such as in [16]) can be employed X  these considerations are beyond the scope of this paper. Types of patterns. We next consider two characteristic special cases, which illustrate how the eigenvectors of the autocovariance matrix capture both aperiodic and oscilla-tory trends [7].
 We first consider the case of a weakly stationary series. In this case, it follows from the definition of stationarity that the autocorrelation depends only on the time distance, matrix is circulant , i.e., it it symmetric with constant diag-onals. Its estimate  X   X  t will have the same property, provided that the sample size m (i.e., number of windows used by the estimator) is sufficiently large. However, the eigenvec-tors of a circulant matrix are the Fourier basis vectors. If we additionally consider real-valued series, these observations lead to the following lemma.
 Lemma 3 (Stationary series) . If X is weakly stationary, then the eigenvectors of the local autocovariance matrix (as m  X   X  ) are sinusoids. The number of non-zero eigenval-ues is twice the number of frequencies present in X .
Figure 4a illustrates the four eigenvectors of the auto-covariance matrix for a series consisting of two frequen-cies. The eigenvectors are pairs of sinusoids with the same frequencies and phases different by  X / 2 . In practice, the estimates derived using the singular value decomposition (SVD) on a finite sample size of m = w windows have similar properties [19].

Next, we consider simple polynomial trends, x t = t k for a fixed k  X  N . In this case, the window vectors are always polynomials of degree k , x t,w = [ t k , ( t + 1) k , . . . , ( t + w  X  1) k ] T . In other words, they belong to the span of { 1 , t, t 2 , . . . , t k } , leading to the next simple lemma. Lemma 4 (Trends) . If X is a polynomial of degree k , then the eigenvectors of  X   X  t are polynomials of the same degree. The number of non-zero eigenvalues is k + 1 .

Figure 4b illustrates the four eigenvectors of the autoco-variance matrix for a cubic monomial. The eigenvectors are polynomials of degrees zero to three, which are similar to Chebyshev polynomials [3].

In practice, if a series consists locally of a mix of oscil-latory and aperiodic patterns, then the eigenvectors of the local autocovariance matrix will be linear combinations of the above types of functions (sinusoids at a few frequencies and low-degree polynomials). By construction, these mix-tures locally capture the maximum variance.
In this section we show how ` t can be incrementally up-dated in a streaming setting. We also briefly discuss how to update  X  t .
 LoCo score. The eigenvector estimates of the exponen-tial window local autocovariance matrix can be updated in-crementally, by employing eigenspace tracking algorithms. For completeness, we show above one such algorithm [22] which, among several alternatives, has very good accuracy with limited resource requirements.

This simple procedure will track the k -dimensional eigenspace of  X   X  t ( X, w,  X  ) . More specifically, the matrix V U 0 &lt;  X   X  1 : exponential decay factor g := h / (  X  + y T h )
C t  X  ( C t  X  1  X  g  X  h ) / X  return V t , C t be easily addressed by performing an orthonormalization step. The matrix C t is the covariance in the coordinate sys-tem defined by V t , which is not necessarily diagonal since the columns of V t do not have to be the individual eigen-vectors. The first eigenvector is simply the one-dimensional eigenspace and can also be estimated using E IGEN U PDATE The detailed pseudocode is shown below.
 Algorithm 1 S TREAM L O C O Eigenvector estimates  X  u X ,  X  u Y  X  R w Eigenvalue estimates  X   X  X ,  X   X  Y  X  R Eigenspace estimates  X  U x ,  X  U Y  X  R w  X  k Covariance (eigen-)estimates  X  C X ,  X  C Y  X  R k  X  k Initialize  X  U x ,  X  U Y ,  X  C X ,  X  C Y to unit matrices
Initialize  X  u X ,  X  u Y ,  X   X  X ,  X   X  Y to unit matrices for each arriving pair x t + w , y t + w do end for Local Pearson score. Updating the Pearson score  X  t re-quires an update of the inner product and norms. For the former, this can be done using the simple relationship Similar simple relationships hold for k x t,w k and k y t,w
The time and space complexity of each method is sum-marized in Table 2. Updating  X  t which requires O (1) time buffering w values. Estimating the LoCo score ` t using a sliding window requires O ( wmk ) = O ( w 2 k ) time (since we set m = w ) to compute the largest k eigenvectors of the covariance matrix for m windows of size w . We also need O ( wk ) space for these k eigenvectors and O ( w + m ) space for the series values, for a total of O ( wk + m ) = O ( wk ) . Using an exponential window still requires storing the w  X  k matrix V , so the space is again O ( wk ) . However, the eigenspace estimate V can be updated in O ( wk ) time (the most expensive operation in E IGEN U PDATE is V T t  X  1 x instead of O ( w 2 k ) for sliding window.

This section presents our experimental evaluation, with the following main goals: 1. Illustration of LoCo on real time series. 2. Comparison to local Pearson. 3. Demonstration of LoCo X  X  robustness. 4. Comparison of exponential and sliding windows for 5. Evaluation of LoCo X  X  efficiency in a streaming setting. Datasets. The first two datasets, MemCPU1 and MemCPU2 were collected from a set of Linux machines. They measure total free memory and idle CPU percentages, at 16 second intervals. Each pair comes from different machines, run-ning different applications, but the series within each pair are from the same machine. The last dataset, ExRates , was obtained from the UCR TSDMA [13]. and consists of daily foreign currency exchange rates, measured on working days (5 measurements per week) for a total period of about 10 years. Although the order is irrelevant for the scores since they are symmetric, the first series is always in blue and the second in red. For LoCo with sliding window we use ex-act, batch SVD on the sample set of windows X  X e do not explicitly construct  X   X  t . For exponential window LoCo, we use the incremental eigenspace tracking procedure. The raw scores are shown, without any smoothing, scaling or post-processing of any kind. 1. Qualitative interpretation. We should first point out that, although each score has one value per time instant t  X  N , these values should be interpreted as the similarity of a  X  X eighborhood X  or window around t (Figures 5 and 6). All scores are plotted so that each neighborhood is centered around t . The window size for MemCPU1 and MemCPU2 is w = 11 (about 3 minutes) and for ExRates it is w = 20 (4 weeks). Next, we discuss the LoCo scores for each dataset. Machine data. Figure 5a shows the first set of machine measurements, MemCPU1 . At time t  X  20  X  50 one series fluctuates (oscillatory patterns for CPU), while the other re-mains constant after a sharp linear drop (aperiodic patterns for memory). This discrepancy is captured by ` t , which gradually returns to one as both series approach constant-valued intervals. The situation at t  X  185  X  195 is similar. At t  X  100  X  110 , both resources exhibit large changes (ape-riodic trends) that are not perfectly synchronized. This is reflected by ` t , which exhibits three dips, corresponding to the first drop in CPU, followed by a jump in memory and then a jump in CPU. Toward the end of the series, both re-sources are fairly constant (but, at times, CPU utilization fluctuates slightly, which affects  X  t ). In summary, ` haves well across a wide range of joint patterns.

The second set of machine measurements, MemCPU2 , is shown in Figure 5b. Unlike MemCPU1 , memory and CPU utilization follow each other, exhibiting a very similar peri-odic pattern, with a period of about 30 values or 8 minutes. This is reflected by the LoCo score, which is mostly one. However, about in the middle of each period, CPU utiliza-tion drops for about 45 seconds, without a corresponding change in memory. At precisely those instants, the LoCo score also drops (in proportion to the discrepancy), clearly indicating the break of the otherwise strong correlation. Exchange rate data. Figure 6 shows the exchange rate ( ExRates ) data. The blue line is the French Franc and the red line is the Spanish Peseta. The plot is annotated with an approximate timeline of major events in the European Monetary Union (EMU). Even though one should always be very careful in suggesting any causality, it is still remark-able that most major EMU events are closely accompanied by a break in the correlation as measured by LoCo, and vice versa. Even in the cases when an accompanying break is ab-sent, it often turns out that at those events both currencies received similar pressures (thus leading to similar trends, such as, e.g., in the October 1992 events). It is also interest-ing to point out that events related to anticipated regulatory changes are typically preceded by correlation breaks. After regulations are in effect, ` t returns to one. Furthermore, af-ter the second stage of the EMU, both currencies proceed in lockstep, with negligible discrepancies.
 In summary, the LoCo score successfully and accurately tracks evolving local correlations, even when the series are widely different in nature. 2. LoCo versus Pearson. Figures 5 and 6 also show the local Pearson score (fourth row), along with the LoCo score. It is clear that it either fails to capture changes in the joint patterns among the two series, or exhibit high sensitivity to small transients. We also tried using a window size of 2 w  X  1 instead of w for  X  t (so as to include the same num-ber of points as ` t in the  X  X omparison X  of the two series). The results thus obtained where slightly different but sim-ilar, especially in terms of sensitivity and lack of accurate tracking of the evolving relationships among the series. 3. Robustness. This brings us to the next point in our discussion, the robustness of LoCo. We measure the  X  X ta-bility X  of any score c t , t  X  N by its smoothness. We employ a common measure of smoothness, the (discrete) total vari-ation V of c t , defined as V ( c t ) := the total  X  X ertical length X  of the score curve. Table 3 (top) shows the relative total variation, with respect to the base-line of the LoCo score, V (  X  t ) /V ( ` t ) . If we scale the total variations with respect to the range (i.e., use V ( c t ) /R ( c instead of just V ( c t )  X  X hich reflects how many times the vertical length  X  X raps around X  its full vertical range), then Pearson X  X  variation is consistently about 3 times larger, over all data sets.

Figure 7. Score vs. window size; LoCo is robust with respect to both time and scale, accurately tracking correlations at any scale, while Pearson performs poorly at all scales.
 Window size. Figure 7a shows the LoCo scores of MemCPU2 (see Figure 5b) for various windows w , in the range of 8 X 20 values (2 X 5 minutes). We chose the dataset with the highest total score variation and, for visual clar-ity, Figure 7 shows 1  X  ` t instead of ` t . As expected, ` varies smoothly with respect to w . Furthermore, it is worth pointing out that at about a 35-value (10-minute) resolu-tion (or coarser), both series the exhibit clearly the same behavior (a periodic increase then decrease, with a period of about 10 minutes X  X ee Figure 5b), hence they are per-fectly correlated and their LoCo score is almost constantly one (but not their Pearson score, which gets closer to one while still fluctuating noticeably). Only at much coarser resolutions (e.g., an hour or more) do both scores become one. This convergence to one is not generally the case and some time series may exhibit interesting relationships at all time scales. However, the LoCo score is robust and changes gracefully also with respect to resolution/scale, while ac-curately capturing any interesting relationship changes that may be present at any scale.
 4. Exponential vs. sliding window. Figures 5 and 6 show the LoCo scores based upon both sliding (second row) and exponential (third row) windows, computed using appro-priately chosen equivalent window sizes. Upon inspection, it is clear that both LoCo score estimates are remarkably close. In order to further quantify this similarity, we show the average variation  X  V of the two scores, which is defined as  X  SVD on sliding windows and ` 0 t uses eigenspace tracking on exponential windows. Table 4 shows the average score vari-ations for each dataset, which are remarkably small, even when compared to the mean score  X  ` := 1 t bottom line in the table is  X  V/  X  ` ). 5. Performance. Figure 8 shows wall clock times per in-coming measurement for our prototype implementations in Matlab 7, running on a Pentium M 2GHz. Using k = 4 and w = 10 , LoCo is in practice slightly less than 4  X  slower than the simplest alternative, i.e., the Pearson cor-relation. The additional processing time spent on updating the eigenvector estimates using an exponential window is small, while providing much more meaningful and robust scores. Finally, it is worth pointing out that, even using an interpreted language, the processing time required per pair of incoming measurements is merely 0.33 milliseconds or, equivalently, about 2  X  3000 values per second.
Even though, to the best of our knowledge, the prob-lem of local correlation tracking has not been explicitly ad-dressed, time series and streams have received much atten-tion and more broadly related previous work addresses other aspects of either  X  X lobal X  similarity among a collection of streams (e.g., [5]) or mining on time evolving streams (e.g., CluStream [1], StreamCube [8], and [2]). Change detection in discrete-valued streams has also been addressed [10, 23].
BRAID [18] addresses the problem of finding lag corre-lations on streams, i.e., of finding the first local maximum of the global cross-correlation (Pearson) coefficient with respect to an arbitrary lag. StatStream [24] addresses the problem of efficiently finding the largest cross-correlation coefficients (at zero lag) among all pairs from a collection of time series streams. EDS [12] address the problem of sepa-rating out the noise from the covariance matrix of a stream collection (or, equivalently, a multidimensional stream), but does not explicitly consider trends across time. Quantized representations have also been employed for dimensionality reduction, indexing and similarity search on static time se-ries, such as the Multiresolution Vector Quantized (MVQ) representation [15], and the Symbolic Aggregate approXi-mation (SAX) [14, 17].

The work in [20] addresses the problem of finding specifically burst correlations, by preprocessing the time series to extract a list of burst intervals, which are subse-quently indexed using an interval tree. This is used to find all intersections of bursty intervals of a given query time series versus another collection of time series. The work in [21] proposes a similarity metric for time series that is based on comparison of the Fourier coefficient magnitudes, but allows for phase shifts in each frequency independently.
In the field of signal processing, the eigen-decomposition of the autocovariance matrix is employed in the widely used MUSIC (MUltiple SIgnal Classification) algorithm for spectrum estimation [19], as well as in Singular Spectrum Analysis (SSA) [6, 7]. Applications and extensions of SSA have recently appeared in the field of data mining. The work in [9] employs similar ideas but for a different problem. In particular, it estimates a changepoint score which can subsequently be used to visualize relationships with respect to changepoints via multi-dimensional scaling (MDS). Finally, the work in [16] proposes a way to efficiently estimate a family of optimal orthonormal transforms for a single series at multiple scales (similar to wavelets). These transforms can capture arbitrary periodic patterns that may be present.
Time series correlation or similarity scores are useful in several applications. Beyond global scores, in the context of time-evolving time series it is desirable to track a time-evolving correlation score that captures their changing sim-ilarity. We propose such a measure, the Local Correlation (LoCo) score. It is based on a joint model of the series which, naturally, does not make any assumptions about sta-tionarity. The model may be viewed as a generalization of simple linear cross-correlation (which it includes as a special case), as well as of traditional frequency analysis [7, 6, 19]. The score is robust to transients, while accu-rately tracking the time-varying relationships among the se-ries. Furthermore, it lends itself to efficient estimation in a streaming setting. We demonstrate its qualitative interpreta-tion on real datasets, as well as its robustness and efficiency.
