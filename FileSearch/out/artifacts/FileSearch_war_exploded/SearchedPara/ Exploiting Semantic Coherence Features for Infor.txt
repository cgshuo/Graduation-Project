 Most of the existing information retrieval models assume that the terms of a text document are inde pendent of each other. These retrieval models integrate three major variables to determine the degree of importance of a term for a document: within document term frequency, document length and the specificity of the term in the collection. Intuitively, the importance of a term for a document is not only dependent on the three aspects mentioned above, but also dependent on the degree of semantic coherence between the term and the document. In this paper, we propose a heuristic approach, in which the degree of semantic coherence of the query terms with a document is adopted to improve the information retrieval performan ce. Experimental results on standard TREC collections show the proposed models consistently outperform the state-of-the-art models. Document ranking; Retrieval model; Term weighting Most of the existing information retrieval (IR) models assume that the terms of a text document are independent of each other. Generally, these retrieval models integrate three major variables to determine the degree of impor tance of a term for a document: within document term frequency, document length and the approaches are reasonably simple and easy-to-use, the coherence aspect of a term X  X  saliency in a document cannot be taken into account by the term independence assumption. The terms in a document can gene rally be classified into two groups: topical term and non-topica l term. The topical terms will be highly associated with each ot her, while the non-topical terms will have very low association with the other terms within the document. Let us consider two arbitrary documents  X  follows:  X   X  : So let's say you're out for a walk in the woods, with your iPhone handy, and you run into a grizzly bear ......  X   X  : Currently, Apple company has had to allocate massive resources to its iTunes and iPhone ...... The first document is concerned about  X  grizzly bear  X  and the second document is related to  X  Apple Company  X . Suppose that the two documents are equal in length. When we use  X  iPhone X  as query, all the retrieval models mentioned above will give approximately the same score fo r the two documents. However, the word  X  iPhone  X  is a topical term in document  X  topical term in document  X   X  . Apparently, the document  X  relevant to the query than document  X   X  . As we can see from the example, the degree of importance of a term for a document is dependent not only on the three aspects mentioned before, but also dependent on the degree of semantic coherence between the term and the document. Humans can easily identify the t opical terms and the non-topical terms of a document, because they have the knowledge of the  X  X ord association X . In  X   X  , the term  X  iPhone X  appears with many other terms such as  X  Apple X  ,  X  iTunes X , which are associated with the term  X  iPhone  X . In  X   X  , however, the term  X  iPhone  X  seems to be irrelevant to the other terms such as  X  woods X  X nd  X  bear X   X . For computational purposes, this knowledge can be discovered by analyzing the corpus. Semantic association measures based on corpus analysis have served th is purpose for many applications related to natural language processing and IR [6]. In this paper, we study how to efficiently use semantic coherence features to improve the inform ation retrieval performance. We propose an approach to use the degree of semantic coherence between term and document for doc ument ranking. First, each document is represented as a gra ph-of-word that corresponds to a weighted directed graph whose ve rtices represent unique terms, whose edges represent the transl ation probability between two terms. Then, a graph-based algorithm is adopted to calculate coherence-based weighting score of a term within a document. The coherence-based weight score of a term for a document is determined by the degree of se mantic relatedness between the term and the other terms within the document. Finally, the coherence-based document score and the frequency-based score obtained by the existing retrieval function are integrated into a linear feature-based model for document ranking. Experimental results on standard TREC collections show the proposed model consistently outperform the state-of-the-art models. Over the decades, many differen t retrieval models have been proposed and studied, including the vector space model [16, 17], the classic probabilistic model [7, 13, 14] and the language modeling approach [12, 19]. Most of the existing retrieval models assume a  X  X ag-of-words X  repres entation of both documents and queries. A typical effective retrieval function involves a TF part, an IDF part, and a document lengt h normalization part [4]. The TF part intents to give a higher score to a document that has more occurrences of a query term, while the IDF part is to penalize words that are popular in the whole collection. The document length normalization is to a void favoring long documents. Most of the traditional retrieval models employ a single term frequency normalization mechanism that does not take into account various aspects of a term sa liency in a document [11, 18]. Paik [11] proposes a novel TF -IDF term weighting schema (MATF) that employs two different within document term frequency normalizations to capture two different aspects of term saliency. The experimental resu lts show that MATF achieves significantly bette r precision than the start-of-the-art models, such as BM25 [14] and LMDir (the la nguage modeling approach with Dirichlet prior smoothing) [19]. Intuitively, the degree of semantic coherence between a term and a document is an importa nt factor to determ ine the importance of the term in the document. Howeve r, little work has been done around coherent-based term weighting. To the best of our knowledge, the model proposed in [6] is the only work closely related to ours. In the paper, the authors propose a neighborhood based document smoothing m odel by exploiting lexical association between terms. Differ ent from their work, we adopt a linear feature-based model for final document ranking instead of modifying the traditional retrieval functions. Inspired by the success of graph-based ranking algorithms like TextRank [9], some researches attempt to adopt graph-based document representation for improving information retrieval performance [1, 15]. Generally, a graph based ranking algorithm is a way of deciding on the importanc e of a vertex within a graph, by taking into account global information recursively computed from the entire graph rather than relying only on local vertex-specific information. Our work is different from the existing graph-based retrieval methods in two aspects: (1) Each edge is weighted by the mutual information between the two terms, instead of by the number of co-occurrences of the two term in sliding windows; (2) The final ranking score of a document is determined by the combination of the graph-based term weighting score and the frequency-based score instead of by the graph-based term weighting score alone. In this section, we introduce a graph-based model to calculate weighting score for terms and then use these score for final document ranking. The coherence-based weight score of a term for a document is determined by the degree of se mantic relatedness between the term and the other terms within the document. Let us consider the example mentioned in section 1 again. Once the semantic relatedness has been calculated,  X  iPhone  X  will have a higher relatedness with the terms such as  X  Apple, iTunes, company  X  than the terms such as  X  bear, woods, walk  X . Therefore,  X  iPhone  X  will get a higher coherence-based weight score in document d 2 document d 1 . When we use iPhone as query, document d 2 a higher coherence-based score. In this paper, we adopt a graph-based ranking algorithm to calculate the coherence-based wei ght score of a term within a document. Each textual document is represented as a graph-of-word that corresponds to a we ighted directed graph whose vertices represent unique term s, whose edges represent the translation probability between two terms. We prefer to use term rather than word because tokenization and feature selection algorithms (such as stop word removal) have already been applied. The coherence-based weight score of a term within a document is determined by the votes that are casting for it and the weight of the terms casting that votes. The weight of term t document d is initially set to 1 and the following PageRank function is run for several iterations where  X  is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph. In the context of the Web, this graph based ranking algorithm implements a random walk model, where a user clicks on links at random with a probability d, and jumps to a completely new page with probability  X  X  X  X 1 X  [10]. We set the damping factor to 0.85, the convergence threshold to 0.0001, following [9, 10]. Our experiments showed that only a small number of iterations (&lt; 50) is required to obtain convergence. In formula (1),  X  X  X  X  X  X   X   X ,  X   X  is the translation probability from term  X  to term  X   X  in the graph of document  X  . A simple way to estimate  X  X  X  X  X  X   X   X ,  X   X  is as follows:  X  and  X   X  . To calculate  X  X  X  X  X   X   X ,  X   X  in formula (2), we adopted a pairwise term similarity score, which is a combination of mutual information and a distance factor. The distance factor exponentially decreased as the di stance between terms increased. The assumption behind this work is that semantically related words are usually located in proximity, and the distance between two words could indicate the stre ngth of their association. The pairwise term similarity score of terms  X   X  and  X  as follows: where  X  X  X  X  X  X   X   X ,  X   X  is the average distance between terms  X  the exponential function. In our experiments, the decaying rate  X  is set to 0.8, following [5]. In formula (3),  X  X  X  X   X   X ,  X   X  denotes the mutual information between term  X   X  and  X   X  , which can be calculated as follows: where  X   X  and  X   X  are binary variables indicating whether term  X   X  is present or absent. The probabilities are estimated as follows: where  X   X   X   X   X 1  X  and  X  X  X   X   X 1 X  are the numbers of documents containing term  X   X  or  X   X  , respectively,  X  X  X   X   X , X 1 number of documents that contain both  X   X  or  X   X  , and  X  in the total number of documents in the collection. Coherence-based term weighting sc ore itself may be not effective to be used as the only scoring function for retrieval. It X  X  a potential way to incorporate c oherence-based term weighting score into the traditional retrieval functions for document ranking. However, it is not always pos sible to easily incorporate new feature into the traditional retrieval functions, which are built upon some underlying theoretical fr amework [8]. For example, it proved non-trivial to include query independent features such as PageRank into BM25 ranking formula [2]. Therefore we adopt a linear feature-based model for fi nal document ranking, instead of modifying the traditional retrieval functions. The linear feature-based model has been proven to be an effective way to incorporate different features into a united retrieval model [8]. The final retrieval function can be described as follows:  X  X  X  X  X  X  X   X   X , X   X   X  X  X  X  where  X  X , X  X  X  is the document score calculate by the existing retrieval functions, such as BM25, LMDir, and MATF;  X  X  X   X   X , X   X  is the coherence-based weight scor es of all query X  X  term within a document, which can be calculate as follows: where  X  X  X   X   X  can be calculated by formula (1);  X  X  X  X  X  inverse document frequency of term  X   X   X  Table 1 summarizes that statistics on test collections used in our experiments. These collections ar e different in size and genre. Each document is processed in a standard way for indexing. Words are stemmed (using porter-s temmer), and stop words are removed. In the experi ments, we only use titl e of the queries. In order to evaluate our model and compare it to other models, we use the MAP and P@10 measure, which are widely accepted measure for evaluating effectiveness of ranked retrieval systems. The methods used in our experiments are as listed in Table 2. An important issue that may affe ct the robustness of the proposed models is the sensitivity of their parameter  X  (in Equation 6). The parameter  X  controls weight of coherence-based weighting score in the final ranking function. In this section, we study how sensitive the parameter is to MAP measure. At the current stage of our work, the parameter is selected through grid search. In order to find the optimal value of parameter  X  , we sweep the values from 0 to 1 with an interval of 0.1. When  X  equals to 0, we reach the baseline. When  X  equals to 1, the retrieval model use coherence-based weighting score only. The value of  X  controls the influence of coherence-based wei ghting score. Figure 1 shows the sensitivity of the value of parameter  X  according to MAP measure. The experimental results show that parameter  X  can greatly impact the performance of the proposed models. Generally, the performance of all models increases at the beginning when the value of  X  grows up. The performance of each model starts to continually drop after a peak. Howe ver, this is no unique optimal 
ConRank-MATF The proposed model using MATF function to calculate  X   X   X , X   X  collection AP88-89 are 0.3, 0.4, and 0.3 for ConRank-BM25, ConRank-LMDir, and ConRank-MATF, respectively. For all models on all datasets, we can gain the best retrieval performance when 0.2  X   X   X  0.4. The proposed models constantly perform better than the corresponding baseline while  X  range from 0.1 to model, the performance is downward and even worse than that of the baseline. In order to make the comparison fair, we need to carefully choose suitable values for the parameters in all the models. To build strong baselines, we adopt a method proposed in [3] to find the optimal parameter settings for BM25 and LMDir. All parameters in BM25 and LMDir are respectively set to the optimal values in our experiments. As a parameter free model, MATF does not need parameter tuning. In the proposed models, parameter  X  and k are respectively set to 0.8 and 0.85 following [5, 10]. The previous research work [20] has demonstrated that when a new feature is integrated into a traditiona l retrieval function under linear interpolation, the best retrieva l performance can be obtained by assigning a relatively sm all weight (0.1-0.2) to the new feature. As we can see from the experiments presented in section 4.2, good performance can be archived with parameter  X  are set to 0.3. Therefore, the parameter  X  in the proposed models is set to 0.3 in the following experiments. Table 3 present the retrieval results of the six retrieval models on the four collections respectively. The results indicate that the proposed models constantly outperform the corresponding baseline on all collections. In mo st of the cases, the improvement of MAP and P@10 was statistica lly significant. The maximum average improvement is as high as 7.81% and 7.32% in terms of MAP and P@10, respectively. It is worth noting that MATF has obtained significant improvement s over BM25 and LMDir, and is therefore a strong baseline. The significant performance improvements from such a strong baseline are very encouraging. The results confirm that semantic co herent features can be used to improve the retrieval performance. 0.2693* (+5.53%) 0.2615* (+4.22%) 0.2752* (+3.03%) In this paper, a new term wei ghting approach is proposed to calculate the degree of semantic coherence of the query terms with a document. The coherence-based term weighting score can further be used in combined with the existing retrieval functions for document ranking. Experimental results on standard TREC collections show the proposed re trieval methods consistently outperform the corresponding st rong baselines. The results confirm that semantic coherent fe atures can be used to improve the retrieval performance. At the current stage of our work, only one type of distance measure is adopted to calculate semantic similarity between terms, and the values of the parameters in the proposed models are chosen empiri cally. For future work, we will investigate the effectiveness of other distance measures and study how to find the optimal parameter setting for further improving the retrieval performance. 
