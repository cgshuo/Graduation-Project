 Sander M. Boh te S.M.Bohte@cwi.nl Departmen t of Soft ware Engineering, CWI, The Netherlands Markus Breiten bac h Markus.Breitenba ch@colorado.edu Gregory Z. Grudic Greg.Gr udic@colorado.edu The rst goal of this pap er is to prop ose a compu-tationally ecien t class of nonparametric binary clas-si cation algorithms that generate nonlinear separat-ing boundaries, with minimal tuning of learning pa-rameters. We want to avoid the computational pit-falls of using extensiv e cross validation for mo del se-lection. For example, in Supp ort Vector Mac hines (SVMs) (Sc h X olkopf &amp; Smola, 2002), both the choice of kernels and corresp onding kernel parameters is based on extensiv e cross validation exp erimen ts, making gen-erating good SVM mo dels computationally very di-cult. Other algorithms, suc h as Minimax Probabil-ity Mac hine Classi cation (MPMC) (Lanc kriet et al., 2002), Neural Net works, and even ensem ble metho ds suc h as Boosting (Freund, 1999), can su er from the same computational pitfalls.
 The second goal of this pap er is to have the pro-posed class of algorithms give explicit estimates on the probabilit y of misclassi cation on future data, with-out resorting to unrealistic distribution assumptions or computationally exp ensiv e densit y estimation (An-derson &amp; Bahadur, 1962). The Minimax Probabilit y Mac hine for Classi cation (MPMC), due to Lanc kriet et al. (Lanc kriet et al., 2002), is a recen t algorithm that has this characteristic. Giv en the means and co-variance matrices of two classes, MPMC calculates a hyperplane that separates the data by minimizing the maxim um probabilit y of misclassi cation. As suc h, it generates both a classi cation and a bound on the ex-pected error for future data. In the same pap er, the MPMC is also extended to non-linear separating hy-persurfaces using kernel metho ds. However, MPMC then has similar complexit y as SVM algorithms. To address these two goals, we prop ose an ecien t, scalable, nonparametric approac h to generating non-linear classi ers based on the MPMC framew ork: the class of Polynomial MPMC Cascades (PMCs). PMCs are motiv ated by cascading algorithms like cascade-correlation (Fahlman &amp; Lebiere, 1990) and Tower (Gallan t, 1990) and others (Nadal, 1989), whic h se-quen tially add levels that impro ve performance. How-ever these algorithms applied to real world problems often su er from over tting and generally scale poorly to larger problems due to the increasing num ber of variables used in subsequen t levels of the cascades. In our cascading algorithm, for levels, instead of neural net works, we use low dimensional polynomials, after Grudic &amp; Lawrence's Polynomial Cascade algorithm for regression (Grudic &amp; Lawrence, 1997), whic h ef-cien tly builds very high dimensional nonlinear re-gression surfaces using cascades of suc h polynomi-als. In this manner, we avoid the gro wth in learn-ing complexit y in cascade-correlation type algorithms by alw ays pro jecting the intermediate outputs onto a two-dimensional state-space, and pro ceed from there. Since the rate of con vergence (as a function of the num-ber of training examples) of a basis function learn-ing algorithm dep ends on the size of the state-space (i.e. the num ber of basis functions) (Friedman, 1995), these low dimensional pro jections lead to stable cas-cade mo dels.
 The prop osed Polynomial MPMC Cascade algorithms generate a nonlinear hypersurface from a cascade of low-dimensional polynomial structures. The optimal choice for eac h level of the cascade is determined using MPMC to select the next most discriminating struc-ture. From one level to the next, these additional dis-criminating structures are added to the cascade using MPMC, suc h that at eac h step we obtain the next most discriminating polynomial cascade; we construct PMC varian ts that use di eren t ways of constructing the initial polynomial structures. By using MPMC to guide the addition of new cascade levels, we main-tain a curren t performance and curren t maxim um er-ror bound during construction. We stop the addition of new structures to the cascade when the error bound no longer impro ves.
 We sho w that the prop osed PMC algorithms yield comp etitiv e results on benc hmark problems, while pro-viding maxim um error bounds. The PMCs are ecien t in that their complexit y is 1) linear in the num ber of input-dimensions, 2) linear in the num ber of training examples, 3) linear in the num ber of levels of the cas-cade. Additionally , the PMC algorithm is ecien t in that there are no parameters that need ne-tuning for optimal performance. Unlik e approac hes like boosting (Freund, 1999), the PMC generates a single mo del, a polynomial, instead of an ensem ble of sev eral mo dels while still being fast and scalable to large datasets. To summarize, we believ e that the con tribution of this pap er lies in e ectiv eness and speed of the prop osed class of PMC algorithms: while being solidly rooted in the theory of MPMC, their linear time complexit y and nonparametric nature allo w them to essen tially be a \plug &amp; play" solution for classi cation prob-lems, yielding results comp etitiv e with algorithms like MPMC with Gaussian kernels and non-linear SVMs. A Matlab PMC implemen tation can be downloaded from http://www.cwi.nl/~sbohte/code/pmc . The nonparametric Polynomial Cascade Regression Algorithm (Grudic &amp; Lawrence, 1997) is based on the premise that very high dimensional nonlinear regres-sion can be done using a nite num ber of low dimen-sional structural units, whic h are added one at a time to the regression function. By keeping the structural units low dimensional, the algorithm is able to pro-duce stable, accurate, regression functions in very high dimensional, large problem domains (i.e. with tens of thousands of features and tens of thousands of training examples (Grudic &amp; Lawrence, 1997)). These regres-sion mo dels have good performance, both in terms of regression accuracy and scaling: the algorithms scale linearly with the dimensionalit y of the problem-space, and linearly with the num ber of examples.
 However, the Polynomial Cascade Regression Algo-rithm is not suitable for classi cation as it optimizes an error metric that typically doesn't create an e ectiv e classi cation mo del. Mainly , Polynomial Cascade Re-gression minimizes least squared error, treating clas-si cation as regression by tting a con tinuous regres-sion surface to class lab els (for example, -1 and +1 for binary classi cation). In con trast, algorithms that build e ectiv e classi ers, suc h as boosting (Freund, 1999), Supp ort Vector Mac hines (SVM's) (Sc h X olkopf &amp; Smola, 2002), and MPMC (Lanc kriet et al., 2002), t to metrics that only attempt to separate classes. In this section, we describ e an adaptation of the Poly-nomial Cascading algorithm to nonparametric binary classi cation using the MPMC framew ork.
 Problem De nition Let x and y denote the set of training samples available in a binary classi cation problem, with x [ y 2 R d N , for in total N samples, eac h of dimensionalit y d . The means and covariance matrices are denoted resp ectiv ely by ( x , x ) and ( y , y ). Let x i and y i denote the resp ectiv e vectors in di-mension i , f i = 1 : : : d g . The problem is to construct a classi er that ecien tly and accurately separates un-seen data from the same resp ectiv e classes. MPMC The Minimax Probabilit y Mac hine Classi-cation algorithm was designed as a generativ e clas-si cation metho d that is essen tially free of distribu-tional assumptions, and yields an estimate of bound of the accuracy of the mo del's performance on the fu-ture data. Compared to discriminativ e classi cation metho ds like SVM's, generativ e approac hes tend to be less sensitiv e to outliers. Additionally , MPMC's have been demonstrated to achiev e comparativ e per-formance with SVM's.
 The MPMC algorithm as dev elop ed in (Lanc kriet et al., 2002) determines a hyperplane H ( a ;b ) = f z j a T z = b g , where z ; a 2 R m , b 2 R (for some di-mension m ), whic h separates two classes of points, u and v , with maximal probabilit y with resp ect to all distributions having these means and covariance ma-trices: where 1 is then the estimate of the maxim um mis-classi cation probabilit y bound, and the MPMC algo-rithm of (Lanc kriet et al., 2002) minimizes this bound. 2.1. Polynomial MPMC Cascade The general idea behind the Polynomial MPMC Cas-cade algorithm is to start o with a low dimensional structure for the rst cascade level: this structure is deriv ed from a polynomial of just one input dimension (attribute) of the data vectors, where the particular input dimension is selected from all d input dimen-sions by computing the class separation power (i.e. the 1 error rate in (1)) of the corresp onding polyno-mial with MPMC. Then, the next level is constructed by com bining the output of this structure with a new input dimension, where again this input dimension is selected by trying all d input dimensions, i.e.: tak e dimension i = (1 : : : d ), create a polynomial of both the input from the previous level and the vector of in-put dimension i , and determine the usefulness of this polynomial structure for separating the classes with MPMC. Then, the best separating polynomial struc-ture is selected as an additional level to the cascade. The classi cation output of this level is a weigh ted sum of the output of the previous level and the new polyno-mial: we use MPMC to determine this weigh ting, thus at the same time obtaining a (decreasing) classi ca-tion error bound S i at every level as we construct the cascade. We keep adding levels until this classi cation error bound S i no longer impro ves. The pro cedure is depicted in Figure 1.
 In the remainder, we mak e the follo wing notational con ventions: Let X 2 R N d be a N d matrix. We will use x i as a vector ( x 2 R N 1 ) by taking the i th feature from eac h example out of the matrix. We de-ne x 2 to be the comp onen t wise square of x . We de ne xy to be the comp onen t-wise multiplication of the vector's entries (i.e. the result is a vector { this operation is not the dot-pro duct).
 Formally , the pro cedure works as follo ws: First the set of training samples z = x [ y is linearly scaled, that is, for eac h input dimension the maximal and minimal value of z i 2 R N are determined ( z i the vector of values in the training samples for input di-mension i ), and eac h input feature vector z i is lin-early scaled to the range [-1,1] with scaling vectors c , d i 2 R d (slop e, intercept).
 To build the rst cascade level, we de ne a second or-der candidate polynomial Z i 1 , for eac h input dimension i = (1 : : : d ), as: where z i = x i [ y i ; let Z i 1 + and Z i 1 denote the parts of Z i 1 from the resp ectiv e classes. For eac h candidate input dimension i , we compute the means and covari-1 , Z i 1 ). Plugging these values into MPMC, we obtain hyperplane coecien ts a i 1 ;b i 1 and error bound s . We select that input dimension that has the mini-mal error bound: S 1 = min ( s i ), where S 1 is the error bound for the rst level. With dimension i selected for the rst polynomial cascade level, the output vector of this structure is then calculated as G 1 = ( a i 1 ) T Z i Subsequen t levels j are then constructed as follo ws: of two inputs of the new level, one is the linearly scaled B j 1 (with scaling factors A j 1 and B j 1 ). Candi-date polynomials are computed by com bining an in-put dimension i = (1 : : : d ) with previous cascade level As before, we then use MPMC to nd the input di-mension i asso ciated with the polynomial Z i j with the minimal classi cation error bound. We compute the output vector G j of the new structure as: Crucially , the (intermediate) classi cation perfor-mance of the cascade is computed by com bining the output G j with the classi cation output of the previ-ous level, M j 1 . For the rst level, M 1 is set to the (unscaled) output G 1 . The classi cation output of subsequen t levels is computed by com bining the clas-si cation output of the previous level with the output 3 of the curren t level using MPMC, de ne: then we compute the MPMC hyperplane coecien ts of ( g j +, g error bound S j , where S j is the classi cation error bound for level j . The curren t classi cation output M j is thus computed as M j = T j g j j . Classi -cation on the training set thus nev er degrades when adding levels. The construction of new levels stops when the classi cation error bound S j no longer de-creases ( S j 1 S j &lt; ). The pseudo-co de for the PMC mo del generation is given in Algorithm 1.
 Impro ving error bound While constructing the Polynomial Minimax Cascade (PMC), the error-b ound on the classi cation, S i , monotonically decreases be-cause as another level j is added, the MPMC attempts to nd the best classi cation given previous classi ca-tion output M j 1 , and the new structure's output G j . At worst this classi cation will be as good as that ob-tained in M j 1 (whic h is our stopping criteria), and if there is any additional discriminatory information con tained in G j , the error-b ound will be better, i.e. decrease. The error-b ound S L , with L the nal level, is our estimate for the maxim um error bound on the test set.
 Summarily , let x = ( x 1 ; ; x n ) be the set of features available for generating the cascade. Let Z i be polyno-mials as de ned in section (2.1). Let M i be the MPMC classi cation output for level i .
 New data x t is evaluated by rst scaling by c 0 ; d 0 , and then clipping the range to [ 1 ; 1]. The classi cation function mapping a sample x t to a class lab el ^ y 2 f 1 ; 1 g as generated by the PMC algorithm is then: where where 1 L , 2 L and L are the classi cation output com-bination coecien ts for level L , with M 1 ( x t ) = G 1 ( x and where a T L and b L are the com bination coecien ts ob-tained from MPMC on the polynomial Z i L .
 Note that the PMC algorithm is similar to Boosting (Freund, 1999) by using weak classi ers at eac h level and summing their classi cation up in a weigh ted sum. Eac h level, however, uses the weigh ted output of the previous level as a feature, instead of re-w eigh ting ex-amples.
 Algorithm 1 Learn a cascade 1: Linearly scale inputs to be within [ 1 ; 1] . 2: Let x i denote the i -th feature column of 3: For each possible feature i , construct 4: Compute the output of the MPMC decision 5: Compute G 0 1 by linearly scaling the 6: repeat 7: j = j + 1 8: For each possible feature i , construct 9: Compute the output for level j : 10: Compute G 0 j by linearly scaling the 11: Compute (intermediate) classification 12: until j S j 1 S j j &lt; Complexit y It is easy to sea that the complex-ity of the algorithm is linear in the num ber of sam-ples N , the num ber of dimensions of the input d , and the num ber of levels L : c 3 N d L , where c 3 is a constan t related to computing the MPMC, with c being the order of the polynomial (for our 5) (Lanc kriet et al., 2002).
 Execution times Though an apples-to-oranges comparison, creating and evaluation one classi cation mo del for the Diab etes problem (see `Results') using our PMC algorithm in Matlab took 20s, whereas the same problem solv ed using libSVM (Chang &amp; Lin, 2003) (C-co de) took 1m30s (including cross-v alidation to nd optimal parameter settings, both runs on a P4 2.6Ghz).
 Pro jecting onto data-space Intuitiv ely, the use of only the input-dimensions as poten tial building blo cks for the low-dimensional structures of the cas-cade seems limiting. We prop ose a variation of PMC where instead of the actual input-dimensions, we use as \input dimensions" pro jections of the training-data onto single training samples: z i = z i T z =N , where i = (1 : : : N ), z i is the thus constructed \input dimen-sion", z i is training example i , and z is the set of all training examples. As in the PMC explained above, polynomials of every i th input, x i ; y i , i = (1 : : : N ) are evaluated, and the one polynomial most e ectiv ely separating the classes is added to the cascade using MPMC (as above).
 This pro jection pro cedure increases the num ber of available building blo cks for the cascade, alb eit at the cost of speed as the num ber of examples is typically much larger than the num ber of input dimensions. Al-though the datasets explored here are small enough for this not to be a problem, for very large datasets it migh t be useful to tak e a random sample from the data, instead of trying all N data vectors.
 Nonparametric The one parametric choice we mak e in the PMC is the complexit y of the polynomial: here, we chose a simple quadratic polynomial. More complex polynomials can be chosen, but may increase the risk of over tting the training samples. We studied the performance of the PMC algorithm for a num ber of benc hmark problems, mostly the benc h-marks used in (Lanc kriet et al., 2002): Wisconsin breast cancer dataset, Pima diab etes, Ionosphere and Sonar data (as obtained from the UCI rep ository). Ad-ditionally , we tested on the House-v oting dataset. As in (Lanc kriet et al., 2002), eac h dataset was randomly partitioned into 90% training and 10% test sets. The data for the Tw onorm problem was generated as spec-i ed by Breiman (Breiman, 1998). The results we re-port in Table 1 are the averages over 100 random par-titions.
 We sho w the results for three di eren t PMC vari-ants: PMC using the input dimensions only ( PMC Dim ), PMC using pro jections of individual data-vectors ( PMC Data ), and PMC using both input dimensions and data-pro jections as cascade building blo cks ( PMC Mixe d ). In Table 1, the results are com-pared to linear and kernel-based MPMC of (Lanc kriet et al., 2002). We note all PMC varian ts signi can tly outp erform the linear MPMC, and approac h the per-formance of kernel-based MPMC on all datasets except Sonar and Ionosphere. Additionally , we note that in general the maxim um error-b ound holds well for the PMC Dim and PMC Data varian ts (with the main exception being the Sonar data; this seems particular for the dataset as we note the same issue in (Lanc kriet et al., 2002)). For the PMC Mixe d varian t, almost all bounds are somewhat too optimistic, see the dis-cussion for possible solutions. The very low maxim um error bound on the Pima dataset suggests that the MPMC framew ork cannot give tigh t bounds for this small dataset (we note the same issue in (Lanc kriet et al., 2002)).
 The benc hmark results for the PMC algorithm are also comp etitiv e with state-of-the-art SVM metho ds: as sho wn in Table 2, the PMC varian ts clearly outp er-form the linear SVM, except for Sonar, and are close to the performance of Gaussian-k ernel based SVM's and boosting(-based) metho ds like Adab oost and Random Forests (latter tak en from (Breiman, 2001)). Giv en the general comp etitiv e performance of MPMC as demonstrated in (Lanc kriet et al., 2002), this con-rms the notion that our nonparametric MPMC-based approac h com bines the e ectiv eness of MPMC with the speed of nonparametric Polynomial Cascade algo-rithms.
 Learning and over tting: In the class of PMC al-gorithms, we have one free parameter: the order of the polynomial structure. By using the minimal { quadratic polynomial in the cascade, we attempt to minimize the possibilit y of over tting the training-samples. We studied this issue by trac king the perfor-mance of the algorithm varian ts during the construc-tion of the cascades (for all 100 runs): at every level, we noted the curren t error-b ound and we computed the accuracy on the training and the test set. The results for three benc hmarks for all algorithm varian ts are sho wn in gure 2. Sho wn are the averages over 100 runs, where the values for those cascades that are com-pleted (met stopping criteria) are tak en as constan t for computing performance for levels larger than the size of these cascades.
 The graphs clearly sho w that the performance of the cascades on the test samples is practically constan t af-ter initial learning (also observ ed in the other benc h-marks, not sho wn). Although some benc hmarks sho w sligh tly better performance early on, this seems to be within the variance of the nal results.
 The Polynomial MPMC Cascade (PMC) class of clas-si ers introduced in this pap er demonstrates good per-formance in the key areas that determine the usabilit y of a classi er: accuracy , speed, scalabilit y to high di-mensional problems, and a minim um of \tink ering" of the learning parameters. As we have sho wn, the pro-posed class of algorithms is nonparametric and highly accurate on the presen ted benc hmarks, and computa-tionally it is linear in complexit y in the dimension of the problem, in the num ber of training examples, and in the size of the cascade.
 We see sev eral areas where the prop osed class of al-gorithms could be extended: although all versions of the PMC framew ork studied here demonstrated good error rates on test data, the bounds for the version that used both input dimensions and data-pro jections as cascade building blo cks ( PMC Mixe d , see Results section), tended to be overoptimistic for some error bound predictions. Since the MPMC framew ork re-quires estimates of mean and covariance matrix, inac-curacies in these estimates lead to inaccuracies in error bounds. One solution to this problem we are curren tly investigating is to attempt to determine when these es-timates are poor, and to comp ensate for this using a metho d similar to the Robust MPMC framew ork de-ned in (Lanc kriet et al., 2002). Another approac h to this problem is to use robust covariance estimates suc h as in (Pena &amp; Prieto, 2001).
 We nd fast asymptotic con vergence of the test set ac-curacy as the cascade is constructed in all the datasets tested (i.e. g 2). This suggests that the num ber of levels in the cascades could be reduced, creating more compact (sparse) mo dels. We are investigating the use of other robust error bounds to attempt to iden-tify when further addition of cascade structure will not lead to signi can t impro vemen t in test set accuracy . While we succeeded in avoiding the computational costs of extensiv e cross-v alidation for mo del selection, kernel metho ds, like the data-pro jection metho d men-tioned, incur a time penalt y in that then most of the algorithm's run time is due to the exp ensiv e compu-tations of kernel matrices, resulting in a large mem-ory footprin t. Since we are extending the PMC with other kernels, like non-linear Gaussian kernels, this does become a concern. We are exp erimen ting with randomized versions of the PMC algorithm that seem to pro vide near-iden tical results using a smaller mem-ory footprin t as well as a speedup for these pro jection-type extensions. Additionally , we are working on nd-ing a statistically valid way to limit the searc h for the next feature to a small subset of suitable prosp ects. In conclusion, we nd that the prop osed class of Poly-nomial MPCM Cascade Classi er algorithms o er a \Plug &amp; Pla y" solution for sup ervised classi cation problems, and warran t further study . A Matlab imple-men tation of the PMC algorithm can be downloaded from http://www.cwi.nl/~sbohte/code/pmc .
 Ackno wledgemen t. We thank Gert Lanc kriet for
