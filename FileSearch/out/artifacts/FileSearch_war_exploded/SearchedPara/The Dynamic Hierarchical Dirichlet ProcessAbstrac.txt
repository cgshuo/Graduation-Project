 Lu Ren lr@ee.duke.edu David B. Dunson dunson@stat.duke.edu Lawrence Carin lcarin@ee.duke.edu The Dirichlet process (DP) mixture model (Escobar &amp; West, 1995) has been widely used to perform density estimation and clustering, by generalizing finite mix-ture models to (in principle) infinite mixtures. In order to  X  X hare statistical strength X  across different groups of data, the hierarchical Dirichlet process (HDP) (Teh et al., 2005) has been proposed to model the depen-dence among groups through sharing the same set of discrete parameters ( X  X toms X ), and the mixture weights associated with different atoms are varied as a function of the data group. In the HDP, it is assumed that the data groups are exchangeable. However, in many real applications, such as seasonal market anal-ysis and gene investigation for disease, data are mea-sured in a sequential manner, and there is information in this temporal character that should ideally be ex-ploited; this violates the aforementioned assumption of exchangeability.
 Developing models for time-evolving data has recently been the focus of significant interest, and researchers have proposed various solutions directed toward spe-cific applications. An early example is the order-based dependent DP (Griffin &amp; Steel, 2006), in which the model is time-reversible but is not Markovian, and it requires one to specify how the mixture weights change through time. Another related work is the time-varying Dirichlet process mixture model (Caron et al., 2007) based on a modified Polya urn scheme (Black-well &amp; MacQueen, 1973), implemented by changing the number and locations of clusters over time. This method is easy to understand intuitively but has com-putational challenges for large data sets. To exam-ine the temporal dynamics of scientific topics, latent Dirichlet allocation (Blei et al., 2003) (Griffiths &amp; Steyvers, 2004) has been used as a generative model for analysis of documents. In order to explicitly model the dynamics of the underlying topics, Blei (Blei &amp; Laf-ferty, 2006) proposed a dynamic topic model, in which the parameter at the previous time t  X  1 is the expec-tation for the distribution of the parameter at the next time t , and the correlation of the samples at adjacent times is controlled through adjusting the variance of the conditional distribution. Unfortunately, the non-conjugate form of the conditional distribution requires approximations in the model inference.
 Recently Dunson (Dunson, 2006) proposed a Bayesian dynamic model to learn the latent trait distribution through a mixture of DPs, in which the latent variable density changes dynamically in location and shape across levels of predictors. This dynamic structure is considered in this paper to extend HDP to incorpo-rate time dependence, and has the following features: ( i ) two data samples drawn at proximate times have a higher probability of sharing the same underlying model parameters (atoms) than parameters drawn at disparate times; and ( ii ) there is a possibility that tem-porally distant data samples may also share model pa-rameters, thereby accounting for possible distant rep-etition in the data. 2.1. Background A Dirichlet process is a measure on a measure G and is parameterized as G  X  DP (  X  0 , G 0 ), in which G 0 is a base measure and  X  0 is a positive  X  X recision X  param-eter. To provide an explicit form for a G drawn from DP (  X  0 , G 0 ), Sethuraman (Sethuraman, 1994) devel-oped a stick-breaking construction: where {  X   X  k }  X  k =1 represent a set of atoms drawn i.i.d. from G 0 and {  X  k }  X  k =1 represent a set of weights, with the constraint from Be (1 ,  X  0 ). According to the construction in (1), a draw G from a DP (  X  0 , G 0 ) is discrete with probability one. Based on this important property, Teh (Teh et al., 2005) proposed a hierarchical Dirichlet process (HDP) to link the group-specific Dirichlet processes, learning the models jointly across multiple data sets. Assume we have J groups of data and the j th data these data sets, x j,i is drawn from the model x j,i ind  X  F (  X  j,i ) with parameters  X  j,i iid  X  G j , and the parame-for which the associated sticks  X  j,k are large, as a con-sequence of the form of G j given by (1); for the J data sets, different group-specific G j are drawn from DP (  X  j 0 , G 0 ), in which G 0 is drawn from another DP. The generative model for HDP is represented as: where j = 1 , . . . , J and i = 1 , . . . , N j . Under this hierarchical structure, not only can differ-ent observations x j,i and x j,i 0 in the same group share the same parameters  X   X  based on the stick weights rep-resented by G j , but also the observations across differ-ent groups might share parameters as a consequence of the discrete form of G 0 (all G j are composed of the same set of atoms {  X   X  k }  X  k =1 ). The clusters in each group j , assumed by the set {  X  j,i } i =1 ,...,N via the posterior density function on the parameters, with the likelihood function selecting the set of discrete parameters {  X   X  k }  X  k =1 most consistent with the data { x ciated cluster parameters {  X   X  k }  X  k =1 ) are shared across multiple data sets, as appropriate.
 Although the HDP introduces a dependency between the J groups, the data sets are assumed exchangeable. However, in many applications, the data may be col-lected sequentially, and one may have a prior belief that sharing of data is more probable when the data sets are collected at similar points in time. The pur-pose of this paper is to extend the HDP to account for such temporal information.
 Before proceeding, it will prove useful to consider an alternative form of the HDP model, as derived in (Teh et al., 2005). Specifically, each draw G j may be ex-pressed as: where Stick (  X  ) stochastically generates an infinite set of the form in (1), here with parameter  X  , satisfying the constraint 2.2. Bayesian Dynamic Structure Similar to HDP, we again consider J data sets but now using an explicit assumption that the data sets are collected sequentially, with { x 1 ,i } i =1 ,...,N { x tion is that a time evolution exists between adja-cent data groups, the distribution G j  X  1 , from which {  X  To specify explicitly the dependence between G j  X  1 and G j , Dunson (Dunson, 2006) proposed a Bayesian dy-namic mixture DP (DMDP), in which G j shares fea-tures with G j  X  1 but some innovation may also occur. The DMDP has the drawback that mixture compo-nents can only be added over time, so that one ends up with more components at later times as an artifact of the model.
 In the dHDP, we have where G 1  X  DP (  X  01 , G 0 ), H j  X  1 is called an in-novation distribution drawn from DP (  X  0 j , G 0 ), and  X  w fied from G j  X  1 by introducing a new innovation distri-bution H j  X  1 , and the random variable  X  w j  X  1 controls the probability of innovation ( i.e. , it defines the mix-ture weights). As a result, the relevant atoms adjust with time, and it is probable that proximate data will share the same atoms, but with the potential for tran-sient innovation.
 Additionally, we assume that G 0  X  DP (  X , H ) as in the HDP to enforce that G 0 is discrete, which mani-fests another important aspect of the dynamic HDP: the same atoms are used for all G j , but with different time-evolving weights. Consequently, the model en-courages sharing between temporally proximate data, but it is also possible to share between data sets widely separated in time.
 Providing now more model details, the discrete base distribution drawn from DP (  X , H ) may be expressed as: ponents (atoms), drawn independently from the base distribution H and {  X  k } k =1 , 2 ,...,  X  are drawn from a stick-breaking process  X   X  Stick (  X  ), defined as: We also have J groups of data. G j represents the prior for the mixture distribution associated with the global components in group j , H j  X  1 represents the associated prior for the innovation mixture distribution, and this yields the explicit priors used in (4): where, analogous to the discussion at the end of Sec-tion 2.1, the different weights  X  j are independent given  X  since G 1 , H 1 , . . . , H J  X  1 are independent given G the relationship between  X  j and  X  is proven (Teh et al., 2005) to be To further develop the dynamic relationship from G 1 to G J , we extend the mixture structure in (4) from group to group: with  X  w 0 = 1. It can be easily verified that 1 for each w j , which is the prior probability that the data in group j will be drawn from the mixture dis-groups share the same mixture distribution G 1 and the model reduces to a Dirichlet mixture model, and if all  X  w j = 1 the model reduces to the HDP. Therefore, the dynamic HDP is more general than both DP and HDP, with each a special case. A visual representation of the model is depicted in Figure 1.
 According to (9), the observation x j,i will choose a mixture distribution from  X  1: j based on Mult ( w j ) to be drawn from the global parameter components k }  X  k =1 . We let r j,i be a variable to indicate which mixture distribution is taken from  X  1: j to draw the ob-servation x j,i ; z j,i is a parameter component indicator variable. An alternative form of the dHDP model is represented as:  X  w j | a wj , b wj  X  Be (  X  w j | a wj , b wj ) , r j,i and a graphical representation is shown in Figure 2, in which we add a gamma prior for  X  and for the com-ponents of the vector  X  0 : Pr (  X  ) = Ga (  X  ;  X  01 ,  X  02 Pr (  X  0 ) = metric model F (  X  ) may be varied depending on the application.
 2.3. Sharing Properties To see the mixture structure in a discrete partition space A = ( A 1 , . . . , A K ), we consider , G j  X  1 ( A 1 , . . . , A K ) + 4 j ( A 1 , . . . , A K where 4 j ( A 1 , . . . , A K ) =  X  w j  X  1 { H j  X  1 ( A G j  X  1 ( A 1 , . . . , A K ) } is the random deviation from G to G j .
 Theorem 1 . Given any discrete partition A , we have: According to Theorem 1, given the previous mixture distribution G j  X  1 , the expectation of the deviation from G j  X  1 to G j is controlled by  X  w j  X  1 . Meanwhile, the variance of the deviation is both related with  X  w j  X  1 the precision parameters  X  ,  X  0 j . To consider limiting cases, we observe the following: G  X  if  X   X  X  X  and  X  0 j  X  X  X  , V ( 4 j ( A ) | G j  X  1 ,  X  w j  X  1 , H,  X ,  X  0 j )  X  0. These limiting cases yield insights on the underlying dependence between adjacent groups.
 Theorem 2 . The correlation coefficient of the distri-butions between two adjacent groups G j  X  1 and G j for j = 2 , . . . , J is =
E { G j ( A ) G j  X  1 ( A ) } X  E { G j ( A ) } E { G j  X  1 = [ To compare the similarity of two data groups, the cor-relation coefficient defined in Theorem 2 can be calcu-lated from the posterior expectation of w ,  X  0 and  X  as a local similarity measure. 2.4. Posterior Computation A modification of the block Gibbs sampler (Ishwaran &amp; James, 2001) is proposed for dHDP inference. Since in practice the {  X  k }  X  k =1 in (1) diminish quickly with increasing k , a truncated stick-breaking process (Ish-waran &amp; James, 2001) is employed here, with a large truncation level K , to approximate the infinite stick breaking process. In the dHDP, the second level of DPs associated with the dynamic structure is the only part different from HDP (see Figure 2). Due to the limited space, we only give the conditional posterior distributions for  X  w ,  X   X  , r and z .
 has the simple form: (  X  w l | X  X  X  )  X  Be ( a w + where n jh = results that follow, for simplicity, the distributions Be ( a wj , b wj ) are set with fixed parameters a wj = a and b wj = b w for all time samples.
 The conditional distribution of  X   X  lk , for l = 1 , . . . , J and k = 1 , . . . , K , is updated under the conjugate prior:  X   X  lk  X  Be (  X  0 ,l  X  k ,  X  0 ,l (1  X  specified in (Teh et al., 2005). Then the conditional posterior of  X   X  lk has the form (  X   X  lk | X  X  X  )  X  Be (  X  0 l  X  k +  X  The update of the indicator variables r ji and z ji , for j = 1 , . . . , J and i = 1 , . . . , N j are completed by gen-erating samples from multinomial distributions with entries as follows:
Pr ( r ji = l | X  X  X  )  X   X  w where l = 1 , . . . , j . The posterior probability Pr ( r l ) is normalized so that
Pr ( z ji = k | X  X  X  )  X   X   X  r where k = 1 , . . . , K and the posterior is also normal-ized by a constant The remaining variables specified in (10) are sampled in the same ways as in HDP (Teh et al., 2005). The component parameters  X   X  k for k = 1 , . . . , K are consid-ered for different model forms depending on the spe-cific applications. For the results that follow, it is of interest to consider a hidden Markov model (HMM) mixture (Qi et al., 2007) and Gaussian mixture model (GMM), in which  X   X  k respectively represent the state-transition matrix, the observation matrix, the initial-state distribution for the HMM and the mean vec-tor and covariance matrix for GMM. For more details about sampling for such models, see (Qi et al., 2007) and (Escobar &amp; West, 1995). The Gibbs sampling al-gorithm was tested carefully under different initializa-tions and the diagnostic method in (Raftery &amp; Lewis, 1992) is used to demonstrate rapid convergence and good mixing (for the results considered, convergence based on this method was observed for a burn-in of 200 samples, followed by a subsequent 4000 samples). 3.1. Music Segmentation It is of interest to segment music, to infer inter-relationships between different parts of a given piece, as well as between different pieces. Here we consider segmentation of music, where a given piece is divided into contiguous subsequences, with each subsequence modeled via a hidden Markov model (HMM). The dHDP model is useful in this application in enforc-ing the idea that contiguous subsequences are likely to be within the same music segment, and therefore are likely to share HMM parameters. However, when the segment changes, these changes are detected via innovation within the dHDP.
 The music under consideration is the first movement  X  X argo -Allegro X  from the Beethoven piano Sonata No. 17 (Newman, 1972). As is widely employed for analysis of such audio data, MFCC features are ex-tracted and discretized with vector quatization (Qi et al., 2007); each of the aforementioned subsequences corresponds to a sequence of codewords (we here em-ploy a discrete HMM). The basic form of the Bayesian representation of a discrete HMM is as discussed in (Qi et al., 2007). The piece is transformed into 4980 dis-crete symbols, divided into 83 subsequences of equal length (the codebook has 16 codes, and 8 states are employed for each HMM); each subsequence corre-sponds to 6 secs in the music. To model the time dependence between adjacent subsequences, each sub-sequence corresponds to one group in the dHDP HMM mixture and will choose one set of HMM parameters according to the corresponding mixture weights. In the dHDP framework, one subsequence can share the old DP mixture distributions with the previous ones or it might be drawn from an innovation DP mixture, which may be also shared by the following time se-ries in a similar manner. To encourage that adjacent subsequences be shared, the prior for  X  w is specified as E (  X  w ) &lt; 0 . 5. The product of most interest here is the segmentation of the music, with the specific HMM parameters of secondary importance.
 To represent the time dependence of the piece, the similarity measure E ( z 0 z ) (see z in Eq. (18)) is com-puted across each pair of subsequences, as shown in Figure 3, in which larger values represent higher prob-ability of the two corresponding subsequences being shared during parameter inference. Based upon a discussion in (Newman, 1972), the movement alter-nates seeming peacefulness with sudden turmoil (1st-6th subsequences), after some time expanding into a haunting  X  X torm X  in which the peacefulness is lost (7th-21st subsequences). After the recurrence of the same pattern (22nd-42nd subsequences) and a small transition, the movement starts a long recitative sec-tion in a slow tone (53rd-69th subsequences). Then through the crescendo, previous disturbed tones come back again until the music goes to the peaceful epilogue (after the 70th subsequence). See (Newman, 1972) for more details on the Sonata. This is deemed to be an interesting piece for study because it is well charac-terized in the music literature, as briefly summarized above, and because it is anticipated to have repeated segments over the length of the piece. In Figures 3(a) and (b) we compare the dHDP and HDP, respectively, the latter computed by fixing all  X  w = 1 in the dHDP model. The dHDP and HDP yield related results, but the former yields a smoother segmentation, in good agreement with the music theory discussed above. Based on the results from the dHDP HMM, which ef-fectively yields a model with smoothly time-evolving statistics, we segment the music and present the asso-ciated auditory waveform in Figure 4. By examining the waveform and the results in Figure 3, we note that the dHDP segments the music into dominant auditory phenomena, but it is less sensitive to noticeable but temporally localized events in the music, yielding a segmentation that is consistent with the music theory. By contrast, the HDP results in Figure 3(b) are evi-dently more sensitive to these local temporal bursts in the waveform. 3.2. Gene Expression Data As a second example, we consider the time-evolving characteristics of gene-expression data, here for a Dengue virus study (Hibberd et al., 2006). Concern-ing a model for the gene-expression data at one time snapshot, Dunson (Dunson, 2006) proposed a latent response model based on a linear regression structure; we extend this model for time-evolving gene-expression data via dHDP (with comparison as well to HDP). Assume y ji is a feature vector with dimension p for time, i represents a particular cell from which a sample is collected, and p denotes the number of genes being modeled). Each y ji is represented as where  X  = (  X  1 , . . . ,  X  p ) 0 is the intercept vector and  X  = (  X  1 , . . . ,  X  p ) 0 represents factor loadings. We de-fine a hidden variable  X  ji underlying the observation y ji to be associated with the i th sample at time t j . The error term  X  ji is also a vector of dimension p and each coefficient  X  ji,d is independently drawn from a Student-t distribution. To eliminate the problem of model identifiability, we incorporate the constraints that  X  1 = 0 and  X  1 = 1, as (Dunson, 2006) discusses. In the present model, one cannot explicitly associate  X  exclusively with the virus; however, since these are cell data, it is anticipated that the virus represents the dominant phenomena.
 We have access to expressions of thousands of genes from each sample (cell) for multiple consecutive times t , t 2 , . . . , t J . For each time t j , there are N j samples measured from different cells (Hibberd et al., 2006). Although these samples may have different observa-tions in gene expressions at the same time, due to individual diversity, the hidden variable  X  (see (19)) underlying the observations may have similar charac-teristics. Based on this consideration, the  X  under-lying the observations in one group corresponding to one time are assumed to be drawn from a Gaussian mixture model. They may also share the same mix-ture distribution for proximate time points, under the assumption of the dHDP model.
 The Dengue gene expression data (Hibberd et al., 2006) are divided into six groups of samples measured at different times and the number of samples in each group are 10 , 12 , 12 , 10 , 12 , 9 (the specific time points associated with these data are respectively 3, 6, 12, 24, 48 and 72 hours); each sample has 19,143 genes. To deal with such high-dimensional data, the Fisher score (Duda &amp; Hart, 1973) is used to prelimi-narily select p = 5000 genes as being the most relevant (variable across time and cell), and then we use the dHDP mixture model discussed above to analyze the time evolution existing in these gene samples. Based on the samples collected from the Gibbs sam-pling after burn-in, the posterior distributions (includ-ing the minimum, median, maximum, 25th and 75th percentiles of the values) for all components of  X  un-derlying these samples at different times are shown in Figure 5. Time points 3hr, 6hr and 12hr appear to share a similar pattern, but the  X  t =12 seem to have smaller diversity among different samples. From 24hrs,  X  drops slightly to a new pattern and they drop signif-icantly again at 48hr. The posterior of indicator r is plotted in Figure 6(a) to show the mixture-distribution sharing relationship across different groups. Figure 6(b) shows the similarity measure E ( z 0 z ) across ev-ery pair of samples; here z ji is the indicator variable for the  X  ji associated with time t j (see Eq. (18)). Consider the factor loadings vector  X  , which has com-ponents linked to the p genes under consideration. The larger the value of |  X  d | , the more influence the pattern contained in  X  has on the corresponding gene at the mean of |  X  d | for all d from the Gibbs sampling we rank the genes based on their importance.
 In Figure 7 we plot the expression levels over time for the 10 most important and 10 least important genes. The red and blue curves show two different time pat-terns and their values have either an increasing or a decreasing trend with time, depending on whether the associated  X  is positive or negative. The green curves represent the genes with no apparent relation to the virus (as determined by the analysis) due to the lack of a systematic trend over time.
 As discussed in Section 2.2, if all  X  w j are set to one for j = 1 , . . . , J  X  1, the dHDP reduces to HDP and all the temporal groups are conditionally exchangeable. It is of interest to compare the dHDP with HDP both in the sharing mechanism and parameter estimation. In practice, acquisition of the gene-expression data is expensive, and it is desirable to reduce the number of samples required. To consider this issue, we reduced the samples size to four at each time point, and plot the data similarity matrix E [ z 0 z ] for HDP and dHDP respectively in Figures 8(a) and (b).
 Compared with HDP, dHDP has more sharing between the related groups (as expected from model construc-tion), and despite the reduced data samples the dHDP yields an inter-relationship between the different times that is consistent with that in Figure 6(b) which em-ploys all of the available data. In Figure 9 we compare dHDP and HDP estimation of  X  based on four sam-ples per time point. These results show that dHDP has a smaller estimation uncertainty for most  X  rela-tive to HDP, which is attributed to proper temporal sharing explicitly imposed by dHDP. As the sample size is increased, the differences between dHDP and HDP diminish.
 Finally, correlation coefficients between two groups are calculated from the samples drawn from the Gibbs sampler, according to (14) and plotted as a matrix in Figure 10; this representation is an additional bene-fit of the dynamic structure explicitly imposed within dHDP (of potential biological interest). The size of each small block at the i th row and j th column is pro-portional to the value of the correlation coefficient as-sociated with group i and group j . We note based on Figure 10 that such inference appears to be accurate (or at least consistent) even with diminished sample size. The proposed dynamic hierarchical Dirichlet process (dHDP) extends the HDP (Teh et al., 2005), imposing a dynamic time dependence so that the initial mix-ture model and the subsequent time-dependent mix-tures share the same set of components (atoms). The experiments indicate that the dHDP is an effective model for analysis of time-evolving data. Concern-ing future research, more efficient inference methods will be considered, such as collapsed sampling (Welling et al., 2007) and variational Bayesian inference (Blei &amp; Jordan, 2004).
 Blackwell, D., &amp; MacQueen, J. B. (1973). Ferguson distributions via polya urn schemes. Ann. Statist. , 1 , 353 X 355.
 Blei, D. M., &amp; Jordan, M. I. (2004). Variational meth-ods for the dirichlet process. Proceedings of the In-ternational Conference on Machine Learning .
 Blei, D. M., &amp; Lafferty, J. D. (2006). Dynamic topic models. Proceedings of the International Conference on Machine Learning .
 Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research , 3 , 993 X 1022.
 Caron, F., Davy, M., &amp; Doucet, A. (2007). Generalized poly urn for time-varying dirichlet process mixtures.
Proceedings of the International Conference on Un-certainty in Artificial Intelligence(UAI) .
 Duda, R. O., &amp; Hart, P. E. (1973). Pattern classifica-tion and scene analysis . Wiley.
 Dunson, D. B. (2006). Bayesian dynamic modeling of latent trait distributions. Biostatistics , 7 , 551 X 568. Escobar, M. D., &amp; West, M. (1995). Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association , 90 , 577 X 588. Griffin, J. E., &amp; Steel, M. F. J. (2006). Order-based dependent dirichlet processes. Journal of the Amer-ican Statistical Association , 101 , 179 X 194. Griffiths, T. L., &amp; Steyvers, M. (2004). Finding scien-tific topics. Proc Natl Acad Sci U S A , 101, Suppl 1 , 5228 X 5235.
 Hibberd, M. L., Vasudevan, S. G., Ling, L., &amp; George,
J. (2006). Time course expression data of human cell lines infected with dengue virus serotype2 ngc (Technical Report). Genome Institute of Singapore. Ishwaran, H., &amp; James, L. F. (2001). Gibbs sampling methods for stick-breaking priors. Journal of the American Statistical Association , 96 , 161 X 173. Newman, A. S. (1972). Sonata in the classic era (a history of the sonata idea) . W. W. Norton.
 Qi, Y., Paisley, J. W., &amp; Carin, L. (2007). Music analysis using hidden markov mixture models. IEEE Transactions on Signal Processing , 55 , 5209 X 5224. Raftery, A. E., &amp; Lewis, S. (1992). How many it-erations in the gibbs sampler? Bayesian Stat. , 4 , 763 X 773.
 Sethuraman, J. (1994). A constructive definition of dirichlet priors. Statistica Sinica , 2 , 639 X 650. Teh, Y. W., Jordan, M. I., Beal, M. J., &amp; Blei, D. M. (2005). Hierarchical dirichlet processes (Technical
Report). Dept. of Computer Science, National Uni-versity of Singapore.
 Welling, M., Porteous, I., &amp; Bart, E. (2007). Infinite state bayes-nets for structured domains. Proceedings of the International Conference on Neural Informa-
