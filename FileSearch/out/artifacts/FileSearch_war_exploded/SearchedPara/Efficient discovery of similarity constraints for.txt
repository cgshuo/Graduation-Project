 1. Introduction consider a Contacts relation with the schema: code, then these two tuples have the same City as well. Recently, functional dependencies ( well. detect violations in an instance of Contacts in Table 1 . The tuples t of City. Thus, these tuples are detected as violations of the above fd. equal values on Street. Obviously, this strict equality constraint limits the usage of has various representation formats. For instance, the tuples t cfd, since they have  X  different  X  Street values but agree on ZIP and CC = 44. However, indeed the  X  same  X  street in the real-world with different representation formats. proposed a new concept of data dependencies, called matching dependencies ( the Contacts example, we may have an MD as inconsistency on text attributes cannot be detected by using instance, according to their similarity on attribute Name is 0.7 which is less than 0.9. The dependency md similarities on Name and Street (i.e., both similarities are greater than 0.9). 1.1. Motivation discover such useful MD s. In fact, given a database instance, there are enormous traditional FD s. In other words, traditional FD s are special cases of ([ZIP]  X  [City], b 1.0, 1.0 N ). Clearly, not all the settings of thresholds for Following [11], we employ the confidence and support measures to evaluate the utility of
Specifically, let's consider an MD of a relation R , denoted by specifying different similarity thresholds on each attribute in X and Y . Let on the attributes X and Y respectively.  X  attributes of X and Y .  X 
The confidence is the ratio of tuple pairs whose matching similarities satisfy confidence. Otherwise, if users need high recall of object identification, then desirable to discover those MD s with high support and high confidence. In this work, we study the problem of discovering proper settings of similarity thresholds for in 1.2. Contributions dependencies on the given X  X  Y . Our main contributions are summarized as follows:  X  are formally defined, together with the computation of measures in the given relation instance.  X  Second, we study exact algorithms for discovering MD s. The pruning strategies by the minimum requirements of support and confidence.  X 
Third, we study approximation algorithms for discovering MD the approximation is developed. Moreover, we also develop a strategy of early termination.  X  Finally, we report an extensive experimental evaluation. The proposed algorithms on discovering confidence. In Section 4 , we develop the exact algorithm for discovering 2. Related work Recently, traditional data dependencies, such as functional dependencies ( conditional functional dependencies ( CFD s), as an extension of traditional conditions. Cong et al. [14] study the detecting and repairing methods of violations to propagation of CFD s for data integration. Bravo et al. [9] propose an extension of novel soft FD , which is also a generalization of the classical notion of a hard incorporating similarity metrics. 2.1. Dependencies with metrics matching or entity resolution, see [15] for a survey). The equality of values (i.e., having matching similarity equal to 1.0 exactly). Thus, Reasoning mechanism for deducing MD s from a set of given functional dependencies and matching dependencies. Motivated by the usefulness of discovering similarity constraints for matching dependencies from data. differential dependencies. Bass X e and Wijsen [3] propose neighborhood dependencies ( ordered data. A sequential dependency, in the form of X  X  the data that can be ordered. 2.2. Discovery of dependencies work targeted on generating a canonical cover of all FD s.
 with efficient pruning techniques for searching in the lattice of attributes. Remarkably, non-minimal dependencies, Flach and Savnik [22] discover FD The above three strategies are also adapted to improve the efficiency of discovering Unfortunately, the problem of discovering similarity thresholds for similarity thresholds for MD s. 2.3. Measures for dependencies A dependency can be measured in various ways. In measures of approximate instance, g 3 measure [32] is widely used to evaluate these approximate minimum description length principal. An encoding length of a table T is defined based on X
T is the degree to which T determines a function from  X  X support and confidence to evaluate MD s. 3. Utility measures In this section, we formally introduce the definitions of given database instance. 3.1. Matching dependencies in cosine similarity with word tokens [13] or q-grams [27] for text values. Consider a relation r with schema R A 1 ; ... ; A m  X  X  . Following similar syntax of dependency ( MD )  X  has the form ( X  X  Y ,  X  ), where X R
De fi nition 1. A Matching Dependency ( MD )  X  specifies a constraint that, for any two tuples t if  X 
A respectively. For each attribute A i  X  X  X  Y , the similarity matching operator t [ A ] satisfies the corresponding threshold  X  [ A i ].
 3.2. Measures tuple values. However, for the above similarity constraints of statistical distribution has a schema D A 1 ; ... ; A m ; in R ,and P is the statistical value. Let s be a statistical tuple in D ,and s [ A sim( A )with d elements.
 in the range of [0.0, 1.0] are mapped to elements in [0, d
For example, the cosine similarity between t 1 [Name] and t 1.0] is mapped to a unified space [0, 10], i.e., 0.7  X  ( d t , we can obtain a similarity vector b 10, 7, 10, 10, 10, 10 t and t 2 are 10, 7, ... , 10, respectively. Considering all the 6 having the same similarity vector. Consequently, the first tuple s Street (corresponding to A 1 , A 2 ,..., A 6 ), respectively.

Similarly, for the tuple pair ( t 1 , t 3 ), we compute a similarity vector 0.033). The next tuple s 3 (0,0,...,8,0.066)isgeneratedbythetuplepairs( t  X  and  X  Y be the projections of similarity threshold pattern that the similarity thresholds are also specified in terms of elements in sim( A ) of each A specified by  X  , i.e., Rn X  X  Y  X  X  . The definitions of support and confidence for the where  X  denotes the satisfiability relationship, i.e., X  X   X  values higher than the corresponding minimum threshold, i.e., s [ A ] calculate  X  Z P ( X  X   X  X , Y  X   X  Y , Z ) in formula (1) by attributes X and Y satisfy the corresponding thresholds  X  by
Consider any two tuples t 1 and t 2 from the original data relation R , the support( similarities of t 1 and t 2 on attributes X and Y satisfy the thresholds specified by confidence(  X  ) computes the conditional probability that the matching similarities between t specified by  X  Y (i.e., Y  X   X  Y ) given the condition that t means few instances of matching pairs that are similar on attributes X (i.e., X where  X  denotes the unsatisfiability relationship.
 confidence. On the other hand, if users need high recall of detection, then 4. Exact algorithms We now study the determination of similarity threshold pattern for the problem of determining similarity constraints is new and different from the previous Unlike FD s, we have various settings of matching similarity thresholds for processed from a relation, we discover the proper similarity thresholds for satisfied. 4.1. Problem statement
In order to discover an MD  X  with the minimum requirements of support should be given first: (I) what is Y ? and (II) what is matching quality requirement addressed by specific applications. For example, if we would like to use the discovered the discovered MD s will be meaningless. For example, an MD with  X  Y = 0 is useless and cannot detect any violation or identify objects correctly. Formally, consider a statistical distribution D . The threshold determination problem of
Problem 1. Given the attributes X and Y , the minimum requirements of support and confidence threshold pattern  X  Y , to find all the MD s  X  ( X  X  Y ,  X  support(  X  )  X   X  s , if exist; otherwise return infeasible.
 process can automatically remove those attributes that are not required in X for an 4.2. Exact algorithm We now present an algorithm to compute the similarity thresholds on attributes X for greater than  X  s and  X  c , respectively. Let A 1 ; ... ; A simplicity, we use  X  to denote the threshold pattern projection use the values from sim( A i ) as possible thresholds  X  [ A
Given a fixed scheme R , the total number of candidates is c  X C
P ( X ), which record P ( X  X   X  X , Y  X   X  Y ) and P ( X  X   X  increasing from 1 to c .

Finally, those  X  j can be returned if support(  X  j )= P n Algorithm 1. Exact Algorithm EA D ; C t ;  X  Y  X  X 
We can implement the exact algorithm (namely EA ) by considering all the statistical tuples s complexity O nc  X  X  . 4.3. Pruning strategies based on the given support and confidence, respectively. 4.3.1. Pruning by support candidates that have supports lower than  X  s .

De fi nition 2. Given two similarity threshold patterns  X  1 and dominates  X  2 , denoted as  X  1  X   X  2 .
 patterns.

Lemma 1. Given two MD s ,  X  1 =( X  X  Y ,  X  1 ) and  X  2 =( X then we have
According to the minimum similarity thresholds, for each attribute A , we have tuple s  X  cover(  X  2 ), we also have  X  1 [ A ]  X   X  2 [ A ] by  X  2 also satisfy the threshold of  X  1 , i.e.,
Referring to the definition of support, we have The conclusion is proved.  X 
According to Lemma 1 , given a candidate similarity threshold pattern requirement  X  s , i.e., P n j ( X , Y ) b  X  s , all the candidates that are dominated by safely pruned without computing their associated support and confidence. We present the implementation of pruning by support (namely
Algorithm 1 by introducing the pruning step in Lines 6  X  8. If the current dominated by  X  j can be safely pruned in Line 7.

In order to maximize the pruning, we can heuristically select an ordering of candidates in C dominant order of candidate patterns can be obtained by a Algorithm 2. Pruning by Support
EPS D ; C t ;  X  Y  X  X  4.3.2. Pruning by con fi dence tuples that have no improvement of confidence, when the confidence is already lower than
First, we divide the statistical tuples in D into two parts based on the preliminary n . For the first v tuples, we have while all the remaining n  X  v tuples have This partitioning of statistical tuples in D can be done in linear time.
Lemma 2. Consider a pre-partitioned statistical distribution D . For any 1 Moreover, for the remaining n  X  v tuples with s i [ Y ]  X   X  Meanwhile, the corresponding P ( X ) is non-decreasing, that is, for any v +1  X  i 1 b i 2  X  n . Consequently, we have Combining above two statements, we proved the lemma.  X  non-increasing. For a candidate  X  j , when processing the statistical tuple s Algorithm 3. Pruning by Support &amp; Con fi dence
EPSC D ; C t ;  X  Y  X  X  5. Approximation algorithms approximation algorithm which only traverses the first k ( k =1, confidence of returned MD s. 5.1. Preliminary confidence and support, C k and S k , by ignoring the statistical tuples from s
For any candidate threshold pattern  X  j  X  C t , let where  X  denotes P ( X  X   X  X ) for the candidate  X  j based on the first k tuples in D ,and remaining n  X  k tuples. The following Lemma indicates the error bounds of C range.

Lemma 3. If we have then the error of approximate con fi dence C k compared to the exact con and the error of approximate support S k compared to the exact S
Proof. Let
According to the computation of confidence, we have C k  X   X 
First, we have Note that  X  is equal to the approximate support S k of the
According to the minimum support requirement, to discover a valid to be greater than the minimum support  X  s , i.e.,  X  = S k  X   X  have
Moreover, according to the condition  X   X  min  X  s ;  X  s  X  c
Recall that  X   X   X  s and the confidence should be lower than or equal to 1, i.e.,
Since we have the condition, it follows  X   X   X  s  X  c 1  X   X   X 
Finally, based on the above two conditions, we conclude that
On the other hand, according to the computation of support, we have S Recall that we have  X   X   X  s and  X   X   X   X   X  s . It can be rewritten by
To sum up, the worst-case relative error is bounded by for both the confidence and support. 5.2. Approximation algorithm Now, we consider the last n  X  k tuples in D . Let  X   X 
Bk  X  X  . If there exists a k having Bk  X  X   X  min  X  s ;  X  s  X  according to Lemma 3 , given an error bound ,0 b b 1  X   X  c that is, the k in the range of [1, n ] such that Bk  X  X  attains the maximum value, having Bk  X  X  Theorem 1. Given an error bound ,0 b b 1  X   X  c , we can determine a minimum k, having The approximation by considering the first k statistical tuples in D finds approximate confidence and support compared with the exact one. The complexity is O kc  X  X  .
We present the approximation implementation in Algorithm 4 . Let B denote Bk  X  X  X  decreasing from n to 1, we can determine a minimum k where B  X  Bk  X  X  processing terminates. Here, the error bound is specified by user requirement with 0 5.3. Approximation Individual
Next, we study the approximation in each individual candidate
According to formula (3) in the proof of error bound, we find that for each specific candidate Algorithm 4. Approximation Algorithm
AP D ; C t ;  X  Y  X  X  then the error bound is already satis fi ed and the processing can be terminated for this for all the candidates, the bound of  X  can be determined dynamically for each candidate particular, Lines 14  X  16 accomplish the possible early termination for each individual formula (5).
 Corollary 1. The worst case complexity of the Approximation Individual is O kc  X  X  .
Proof. Note that with the increase of i from 1 to k , for a specific i.e.,  X  j is invalid currently, the bound condition cannot be satisfied having
When  X  j has  X   X   X  s as a valid threshold, the bound condition is relaxed from min condition may be satisfied by a smaller i than k , i.e.,
The worst case is that all candidates do not achieve their bounds until processing the tuple s must be satis fi ed. This is exactly the Algorithm 4 without individual approximation.
Algorithm 5. Approximation Individual 6. Experimental evaluation experiments run on a machine with Intel Core 2 CPU (2.13 GHz) and 2 GB of memory. 6.1. Experiment setting
We use three real data sets in the experimental evaluation. The Cora 150,000 respectively.
 We mainly study the efficiency of the proposed algorithms. Since our main task is to discover we study the runtime performance in various distributions with different the similarity threshold settings of attributes for MD s. Suppose that users want to discover data sets respectively: i) the dependencies on with the preliminary requirement of minimum similarity 0.6 on venue; ii) the dependencies on with preliminary minimum similarity 0.1 on subject, respectively. results returned by the experiments on Cora is: discovery process can automatically identify those attributes that are not required in X for an 6.2. Exact approach evaluation In the first experiment, we evaluate the performance of pruning by support ( ( ). Figs. 1, 2 and 3 report the time cost over three data sets respectively, given different s on large data.
 the pruning strategy, the EPS performance is only affected by support requirement effect on EPS . Thus, EPS has similar time costs in Fig. 5 (a) and (b) with the same in Fig. 6 as well.
 higher  X  s turns to the better pruning performance. Therefore, omitted on Cora and Restaurant . 6.3. Advanced approach evaluation by both support and confidence ( EPSC ), the approximation together with pruning by support ( individual together with pruning by support ( APSI ).

First, we study the influence of  X  c in different approaches. When the confidence requirement other hand, when  X  c is small, e.g.,  X  c = 0.15, we can have larger choices of Figs. 5 (a) and 6(a). Thus, the approximation approaches have lower time cost, especially the can choose EPSC in practical cases if the requirement  X  c costs.
 the program. Thus, approximation approaches show better performance in Fig. 4 (a), having whose  X  s =0.01.
 achieve the bound condition and terminate early, such as 50 k in Fig. 5 (a) with low time cost. Finally, we evaluate the approximate confidence and support of the returned in Figs.7,8and9 . First, we can observe that the exact approaches
Figs.7,8and9 , the approximate confidence and support of APS three data sets.
 ones. 6.4. Summary of discovering MD s.  X  discovering MD s on large data.  X   X 
If the minimum confidence requirement  X  c is high, the pruning by confidence works well.  X 
Otherwise, we can employ the approximation approaches to achieve low time cost. 7. Conclusions methods.
 the current approach can exclude the attributes that are not necessary to an attributes in the MD . However, the problem of determining attributes for similarity thresholds are not necessary t o be considered. Moreover, two different semantics, which leads us to the problem of generating MD also interesting. Most importantly, more exciting applications of promising.
 Acknowledgments
Grant and Huawei Noahs ark lab grant HWLB06-15C03212/13PN.
References
