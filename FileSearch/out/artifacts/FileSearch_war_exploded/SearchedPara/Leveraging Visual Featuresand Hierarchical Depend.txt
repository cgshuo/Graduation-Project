 As we all know, thousands of conferences, symposiums and workshops are held all around the world every year. The conference Web site is a main and official platform to share and post related conference information which can be accessed by people everywhere. With the increa se of conference Web pages, it becomes a cumbersome and time-consuming job for researchers to collect conference in-formation and keep track of the hot research topics. Furthermore, people some-times are more interested in discovering the future trends of the research field, analyzing the social networks of scholars and the inner relationships among these conferences. Building a repository of conf erence information can satisfy all the above requirements and many value-a dded services and applications can be de-veloped based on the repository. The ArnetMiner 1 system [1] is a good example of academic social networking based on the publication information repository. In or-der to automatically and effectively archive clean and high quality academic data, it is essential to extract useful academ ic information from the conference Web pages, which is more up-to-date, comprehensive and reliable than other sources.
Web information extraction is a classical problem, which aims at identifying interested information from unstructured or semi-structured data in Web pages, and translates it into a semantic clearer structure. During the past years, many techniques such as DOM structure analysis [2], visual based methods [3,4], ma-chine learning methods [3,4] have been devised for Web information extraction. Likewise, this paper studies the problem of automatically extracting structured academic information from conference Web pages.

In the area of conference Web page extraction, most traditional approaches focused on the use of visual features in the Web page. When people design conference Web pages, they usually follow some implicit rules about how they structure certain types of information on the page. So, it is easy to understand that visual features are in some sense more stable than content features. VIsion-based Page Segmentation (VIPS) algorithm [5] was proposed by Deng Cai et al to segment the Web page into text blocks. But the segmentation results of VIPS on conference Web page sometimes are not satisfied due to the ignorance of the hierarchical dependencies (see the example as shown in Figure 3), thus we aim to address this segmentation problem by proposing a new hybrid approach com-bines these two kinds of features to archive better segmentation results. In order to better organize the segmented text blocks, classification or block labeling pro-cedures are then employed, after which we can perform the refined information extraction on the annotated labels. The state-of-the-art models for sequence la-beling problems include Support Vector Machine (SVM) [6] and Conditional Random Fields (CRFs) [7], which mainly rely on the sequence structure. Look-ing into the conference websites, we obser ve that apparent hierarchical relations exist between different information blocks. For example, the submission dead-line information always appears directly after the phrase - X  X mportant Dates X  in a bold style. This observation gives us a hint that we should make full use of the hierarchical dependencies to archive a better annotation performance. Due to the fact that the hierarchical rela tion in conference Web page forms a tree structure, we therefore introduce a Tree-structured Conditional Random Fields model [8,9] to do the annotation work.

In summary, we have the following contributions: (1) we propose a hybrid page segmentation algorithm, which combines the vision-based segmentation algorithm with the DOM-based segmentation algorithm; (2) we introduce the Tree-structured Conditional Random Fields model [8,9] to annotate the text blocks, which making use of visual features, content features and hierarchical dependencies concurrently; (3) we refine the annotation quality by introducing some heuristic post-processing rules. Our experimental results on the real world data sets verify that the proposed integrated approach is highly effective for extracting academic information from conference Web pages.
 The rest of the paper is organized as follows. Related works are reviewed in Section 2. We overview our approach in Section 3. Page segmentation algorithm is discussed in Section 4. Text block annotation is described in Section 5. Ex-periment and evaluation results are reported in Section 6. Finally, in Section 7 we summarize our work and make some conclusions. In this section, we present a survey in tw o aspects: (1) Classification/sequence labeling problem and (2) Web information extraction.

First, our work is related to a classification or sequence labeling problem in some sense. Identifying the useful in formation from Web pages is sometimes equivalent to annotating the text blocks with different predefined labels. There have been a lot state-of-the-art approaches proposed for this problem, such as Bayes Network [10], SVM [6], Conditional Random Fields [7] and so on. But these approaches usually depend on individual features or linear-dependencies in a sequence of information, while in Web information extraction the information can be two-dimensional [4] or hierarchically depended [3].

Second, our work belongs to the area of Web information extraction, which receives a lot of attentions during past years. This w ork could be categorized into two types: (1) Template level wrapper induction systems. Several au-tomatic or semi-automatic wrapper learning approaches based on the templates have been proposed. Typical representatives are RoadRunner [11], EXALG [12] and so on. (2) Visual feature assisted techniques. In contrast, the differ-ent display styles of different parts in a Web page provides an additional means for segmenting content blocks. For example, [13,3] reported that visual features, such as width, height, font, etc. are useful in page segmentation.

The limitations of existing Web information extraction methods are:  X  Some rule-based Web information extraction techniques are not scalable for  X  Previous studies may be effective for sequence of text blocks as a linear  X  Traditional methods can only extract information from a single Web page, The academic information of a specific conference is distributed within a set of pages of the conference Web site. In general, we are primarily concerned with three types of academic information: (1) Information about conference events (e.g. conference name, time, location, submission deadline and submis-sion URL. (2) Information about conference topics (e.g. call for papers and topics of interests). (3) Information about related people and institutes (e.g. chairs, program committee, authors, companies and universities).
This paper proposes and implements a Web information extraction system which aims at extracting academic infor mation from conference Web pages. Fig-ure 1 shows the framework of our approach. We first use a hybrid algorithm to segment the page into text blocks, then construct a Tree-structured Conditional Random Fields [8,9] to annotate each text block based on visual features and content features. Further processing including verification and some heuristic rules are used to improve the initial annotation results. Finally, we integrate the whole conference information based on the annotation results. The first task for web information extraction is to find a good representation for Web pages. A good representation can make information extraction easier and more accurate. Through a survey on a large amount of conference Web pages, we find that although different conference We b sites adopt different design styles and layouts, most pages are composed of some common function blocks. Therefore, we first use the VIPS algorithm [5] to segment web pages into text blocks. VIPS algorithm makes use of page layout features such as font, color, and size to construct a vision-tree for a page.

Figure 2 illustrates the layout structure and the vision-based content structure of the Call for Papers page of VLDB 2011. At the first level, the original web page has three objects, namely, visual blocks VB 1  X  VB 3 and two separators  X  1  X   X  2 , as specified in Fig.2(b). Then we ca n further construct a sub-content structure for each sub-web page. For example, VB 2hastwooffspringobjects: VB 2 1  X  VB 2 2 and one separator  X  1 2 . It can be further analyzed recursively.
VIPS can obtain good segmentation results for most Web pages, but it may lose some important information when dealing with some conference Web pages. For example, the red blocks in Figure 3 shows parts of the segmentation results of the Important Dates page in VLDB 2011. We can find that it loses the blue blocks, which contains important submission deadline information. The block lost happens because VIPS algorithm is dependent on visual features of the page elements too much. From Figure 3, we can find that the display style of the red blocks adjacent to the lost part is the same (in this case, they are all in bold), while the lost blocks are displayed in a different style (in this case, they are in non-bold). In other words, VIPS ignores the adjacent blocks, which are not displayed consistently. The lost of these blocks is intolerable.
After investigating lots of conference Web pages, we find that the most useful while these nodes are always located at the same level of the DOM tree and adjacent to each other. Therefore, we define these six types of HTML nodes as Algorithm 1. Hybrid Page Segmentation Algorithm Content Nodes. A DOM-based page segmentation algorithm has two phases: (1) performing a depth-first traversal of the DOM tree, and saving useful Content
Nodes based on rules showed in Table 1. (2) re-dividing and reorganizing saved nodes into semantic blocks based on rules shown in Table 2.

According to the analysis above, we propose a hybrid page segmentation al-gorithm combining the VIPS and the DOM-based algorithm. Algorithm 1 shows the detail of the algorithm. First, it segments an input web page into visual blocks using the VIPS algorithm (line 1). Second, it removes the noise blocks (navigation blocks, copyright blocks, etc.) using some heuristic rules (line 2). Third, it segments the input web page using the DOM-based algorithm (line 3). Finally, it combines the text blocks produced by the VIPS and DOM-based algorithm to archive a more complete segmentation result (line 4). In this section, we use Tree-structured Conditional Random Fields (TCRFs) [8,9] to annotate the text blocks outputted by the segmentation algorithm described in Section 4. 5.1 Problem Definition The input of our problem is a set of Web pages related to a conference, each Web page can be parsed into a sequence of text blocks, namely B .Andthe output is the structured academic information (as defined in Section 3) about the conference. In order to achieve effect ive Web information extraction, we first annotate the sequence of text blocks according to the display style and content information of text block. The label space L is defined in Table 3. Definition 1. (Text block annotation): Given a sequence of text blocks b corresponding to a page, let b = { b 0 ,b 1 ,  X  X  X  ,b n } be the features of all the blocks and each b i is a feature vector of one block, let l = { l 0 ,l 1 ,  X  X  X  ,l n } be one possible label assignment of the corresponding blocks and each l i  X  L defined above. The goal of conference Web data extraction is to compute a maximum posteriori (MAP) probability of l and extract data from this assignment l  X  : Based on the above definition, the main goal is to calculate p ( l | b ) . Here, we introduce TCRFs [8,9] to compute it, which will be described in the following. 5.2 Tree-Structured Conditional Random Fields Conditional Random Fields (CRFs) [7] is a probabilistic model proposed by John Lafferty to segment and label sequence data. For understanding the formula, we define the following notations: X is a random variable over text blocks outputted by segmentation algorithm, and Y is a random variable over corresponding an-notations. All components Y i of Y are assumed to range over label space L . CRFs construct a conditional model p ( Y | X ) based on a given set of features from text blocks and corresponding annotations. The definition of CRFs is given below, for more details please refer to the original paper [7].
 Definition 2. (Conditional Random Fields [7]): Let G =( V,E ) be a graph and Y is indexed by the vertices of G , i.e. ( Y v ) vV .Then ( X, Y ) is a conditional random field, when conditioned on X , Y v obey the Markov property with respect to the graph: p ( Y v | X, Y w ,w = v ) = p ( Y v | X, Y w ,w  X  v ) ,where w  X  v means that there is an edge between w and v in G .
 Tree-structured Conditional Random Fields [8,9] is a particular case of CRFs, whose graphical structure is a t ree. TCRFs can model the parent-child and sibling dependencies, while the simplest Linear-chain CRFs [7] can X  X  do. We define ( y p ,y c ) as a parent-child dependency, ( y c ,y p ) as a child-parent de-pendency, and ( y s ,y s ) as a sibling dependency. A TCRFs model has the form: where x is a block sequence, y is annotation result, and y | e and y | v denotes y associated with edge e and vertex v in the tree; E pc is set of ( y p ,y c ) , E cp  X  j and  X  k are parameters corresponding to t j and s k respectively, and will be estimated from the training data; Z ( x ) is the normalization factor. 5.3 Conference Web Page Annotation Text Block Tree Representation. In order to apply TCRFs to conference Web page annotation, we construct a tree representation of the text blocks in a Web page. After investigating lots of typical top conference Web sites such as VLDB, CIKM, KDD etc. we find that almost all the Web pages follow the rule that the useful information displaying below titles are more obvious than other blocks. For example, the time of a confer ence event always follows an  X  X mportant Dates X  block which belongs to Title (TI) label. Based on this observation, we can construct a text block tree to represent a Web page. The root of the tree represents the whole page. Each inner node represents the block displayed in a more highlighted style (usually in a larger font and in bold style). Each leaf node represents the block display below the inner node. Figure 4 shows the text block tree representation of the Web page corresponding to Figure 2(a). In addition to the parent-child dependencies, we also model the sibling dependencies in the text block tree. For example, topics of a conference are always located close to each other, and always in the same level of the text block tree.
 Features in Annotation Model. After studying charact eristics of each la-bel defined in label space L , two kinds of features are chosen to represent a text block, e.g. visual features and content features. The visual features include FontSize, FontWeight, FontColor, Start WithH and so on. The content features specify whether the text block contains month, country, institute, topic keyword and other keywords. These features belong to the vertex features. We also take the current text block X  X  parent-child dependency, child-parent dependency and sibling dependency, respectively.
 Parameter Estimation. Parameter estimation is to d etermine the parameters  X  = {  X  1 , X  2 ,  X  X  X  ;  X  k , X  k +1 ,  X  X  X } from the training data D =( x ( i ) ,y ( i ) ) along with empirical distribution  X  p ( x, y ) . Actually, we optimize the log-likelihood function of the conditional model p ( y | x,  X  ) : We use f to represent both edge feature function and vertex feature function; c to represent both edge and vertex; and  X  to represent two types of parameters:  X  and  X  . Thus, the derivative of log-likelihood function with respect to parameter  X  j associated with clique index c is: marginal probabilities. We utilize the Tree Reparameterization (TRP) algorithm [14] to compute the approximate probabilities of the factors. The function L  X  can be optimized by several techniques; but in this paper, we adopt gradient based Limited-memory BFG S method [15], which outperforms other optimiza-tion techniques for linear-chain CRFs [16].
 Post-processing. The text blocks annotation results may directly affect the quality of the final conference academic information extraction result. In order to improve the quality of the annotation results, the post-processing is essential. Known from the conference Websites, t he text blocks can be divided into five groups according to their display characteristics and contents, i.e., Title, Data Item/Date/Item, Topic/List no text, Committee Person, Yes Text/No Text. We summarize some heuristic rules for post-processing for different label groups, and repair text blocks with wrongly annotated labels from the following two aspects automatically. (1) A block has special features but cannot be annotated correctly. For example, given a text bloc k Camera Ready Papers Due: Thursday, August 11, 2011, which have typical DI features. However, this block is classified as YT. For this situation, it can be repaired by identifying some typical combina-tion of features. This method is suitable for text blocks with clear features, such as DI/D/I, TO and CP. (2) A text block is classified as a category but it does not contain corresponding features. For example, text blocks about submission instruction are classified as YT. Although it contains a lot of words, it usually does not contain any people name, date or location. Therefore, we can check its feature values ContainMonth, ContainURL and Country Keyword to determine whether it is a YT category. Due to the space limitation, we omitted the technical details here. 6.1 System Implementation The system is mainly implemented in Java, and the Web page segmentation module is implemented in C#. We also use MALLET 2 , a Java-based package for machine learning applications. Our experiments are conducted on a PC with 2.93GHz CPU, 4GB RAM and Windows 7. Our system can process a conference Web page in average of 0.80 seconds, which contains 0.18 seconds for segmenting, 0.62 seconds for annotating and 0.002 seconds for post-processing. 6.2 Data Sets and Evaluation Measures We collect our conference Web page data by meta-search. First, we prepare a list of seed top conference names in computer science field. Then we throw each name as a query to Google and get a list of urls. Using some heuristic rules, we choose the url which is most likely to be the home page of the conference Web site, then we request all the Web pages of that conference Web site. Finally, we collect 50 academic conference Web sites in computer science field, which has 283 web pages and 10,028 labeled text blocks. In order to evaluate our approach, 10 students are invited to manually annotate all the text blocks as the ground truth of labeling. We use the five-fold cross-validation method to evaluate our system, that is, one fold of conference Web sites are training data and the rest sites are test data. We use Precision, Recall and F-Measure as evaluation measures. 6.3 Experimental Results and Discussions The object of the first experiment is to c ompare our hybrid segmentation al-gorithm with the baseline VIPS algorithm. To verify the effectiveness of our algorithm, we call these two algorithms for conference Web pages respectively, and take the manually segmented results as the ground truth. If a text block outputted by an algorithm is the same as the human segmented one, we call it a correct block. Table 4 shows the count number of correct text blocks of a conference Web site using the two segm entation algorithm respectively. From the results, we can find that our hybrid segmentation algorithm can get more correct text blocks than the VIPS segment ation algorithm. Especially for confer-ence DASFAA-11, VIPS get a poor result because the visual separators between two text blocks is not so obvious, while our hybrid segmentation algorithm gets a satisfactory result.
The aim of the second experiment is to demonstrate the effectiveness of combined TCRFs model. We have mentioned in Section 3, this work also ap-proaches classification or sequence labeling problem, so we introduce Bayes Net-work [10], SVM [6], CRFs [7] as baselines for comparison. Table 5 shows the P, R, F measures of different labels in the label space L .Wenameourap-proach as HY+TCRFs, which indicates the combination of hybrid segmentation algorithm and TCRFs model. From the numeric results in Table 5, we find that HY+TCRFs always performs much better than the other models, because it takes both the visual and dependent features into acco unt. It can also be found that by using HY+TCRFs, the TO/LNT, D/I/DI categories achieve a notable improvement than other labels. It is explainable because these labels have stronger dependencies in child-parent relation and sibling relation than other labels.

The last group of experiments as shown in Table 6 is the comparison between ini-tial annotation results and the results after post-processing. It has demonstrated that post-processing generally improves the quality of the annotation results. Looking close to the results, we observe that for label TI/TO/DI/D/CP/LNT they all have more than 0.95 F-Measure, and NT has average 0.90 F-Measure. The explanation is that they have apparent and specific features on not only display style but also semantic level. The post-processing can improve the initial anno-tation easier than other labels. We also observe that YT/NT/I have a relatively low F-Measure for two reasons: (1) it is h ard to effectively extract the features of YT/NT labels; (2) different conferences have different descriptions for an Item label, which has no constant format.
 This paper has proposed a hybrid approach to extract academic information from conference Web pages. Particularly , different from existing approaches, our method combined visual feature and hierarchical structure analysis. Meanwhile, we also developed heuristic-rule based post-processing techniques to further im-prove the annotation results. Our experimental results based on the real world data sets have shown that the proposed method is highly effective and accurate, and it is able to achieve average 94% precision and 93% recall.
 Acknowledgments. This work is partially supported by China National Sci-ence Foundation (Granted Nu mber 61073021, 61272438, 61272480), Research Funds of Science and Technology Commission of Shanghai Municipality (Granted Number 11511500102, 12511502704), Cross Research Fund of Biomedical Engi-neering of Shanghai Jiao Tong University (YG2011MS38).

