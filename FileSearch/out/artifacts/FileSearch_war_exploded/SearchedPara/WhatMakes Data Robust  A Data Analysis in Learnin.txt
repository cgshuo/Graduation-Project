 When applying learning to rank algorithms in real search ap-plications, noise in human labeled training data becomes an inevitable problem which will affect the performance of the algorithms. Previous work mainly focused on studying how noise affects ranking algorithms and how to design robust ranking algorithms. In our work, we investigate what inher-ent characteristics make training data robust to label noise. The motivation of our work comes from an interesting ob-servation that a same ranking algorithm may show very dif-ferent sensitivities to label noise over different data sets. We thus investigate the underlying reason for this observation based on two typical kinds of learning to rank algorithms (i.e. pairwise and listwise methods) and three different public data sets (i.e. OHSUMED, TD2003 and MSLR-WEB10K). We find that when label noise increases in training data, it is the document pair noise ratio (i.e. pNoise ) rather than document noise ratio (i.e. dNoise ) that can well explain the performance degradation of a ranking algorithm.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Experimentation Learning to Rank; Label Noise; Robust Data
Learning to rank has gained much attention in recent years, especially in information retrieval [11]. When ap-plying learning to rank algorithms in real Web search appli-cations, a collection of training data is usually constructed, where human judges assign relevance labels to a document with respect to a query under some pre-defined relevance judgment guideline. In real scenario, the ambiguity of query intent, the lack of domain knowledge and the vague defini-tion of relevance levels all make it difficult for human judges to give reliable relevance labels to some documents. There-fore, noise in human labeled training data becomes an in-evitable issue that will affect the performance of learning to rank algorithms.

An interesting observation is that the performance degra-dation of ranking algorithms may vary largely over differ-ent data sets with the increase of label noise. On some data sets, the performances of ranking algorithms decrease quickly, while on other data sets they may hardly be affected with the increase of noise. This motivated us to investigate the underlying reason why different training data sets show different sensitivities to label noise. Previous work either only observed how noise in training data affects ranking al-gorithms [21, 2], or focused on how to design robust ranking algorithms to reduce the effect of label noise [17, 6]. So far as we know, this is the first work talking about data robustness to label noise in learning to rank.

To investigate the underlying reasons for our observations, we conducted data analysis in learning to rank based on three public data sets, i.e. the OHSUMED and TD2003 data sets in LETOR3.0 [13], and the MSLR-WEB10K data set. There are multiple types of judging errors [9] and they leads to different noise distributions [19, 10], but a recent study shows that the learning performance of ranking algorithms is dependant on label noise quantities and has little rela-tionship with label noise distributions [10]. Therefore we randomly injected label errors (i.e. label noise) into training data (with fixed test data) to simulate human judgment er-rors, and investigated the performance variation of the same ranking algorithm over different data sets. In this study, we mainly focus on the widely used pairwise and listwise learn-ing to rank algorithms.

We find that it is the document pair noise ratio (i.e. pNoise ) rather than document noise ratio (i.e. dNoise ) that can well explain the performance degradation of a ranking algorithm along with the increase of label noise. Here dNoise denotes the proportion of noisy documents (i.e. documents with er-ror labels) to all the documents, while pNoise denotes the proportion of noisy document pairs (i.e. document pairs with wrong preference order) to all the document pairs. We show that the performance degradation of ranking algorithms over different data sets is quite consistent with respect to pNoise. It indicates that pNoise captures the intrinsic factor that de-termines the ranking performance. We also find the increase o f pNoise w.r.t. dNoise varies largely over different data sets. This explains the original observation that the performance degradation of ranking algorithms varies largely over differ-ent data sets with the increase of label noise.
Before we conduct data analysis in learning to rank, we first introduce related work and our experimental settings.
How noise affects ranking algorithms has been widely stud-ied in previous work. Voorhees [18] and Bailey et al. [2] show the relative order of ranking algorithms in terms of performance is actually quite stable despite of remarkable noise among human judgements. Recently various learning to rank algorithms have been proposed [7, 4, 22, 3]. Corre-spondingly, noise in training data has also attracted much attention in this area. Xu et al. [21] explored the effect of training data quality on learning to rank algorithms.
There are also various approaches proposed to learn ro-bust ranking models [6, 20]. Another solution to deal with noisy training set in learning to rank is to employ pre-processing mechanisms, such as  X  X airwise preference consistency X  [5] and repeated labeling techniques [16].

Similar to our work, there do exist a branch of studies on how to construct effective training data in learning to rank. They show us what characteristics of training data are needed to train a effective ranker [1, 8]. Especially, [12] makes an extensive analysis of the relation between the rank-ing performance and sample size. Different from existing work, our work focuses on investigating the inherent char-acteristics of training data related to noise sensitivity.
Here we present data sets and ranking algorithms used in our experiments.

Data Sets . In our experiments, we use three public data sets in learning to rank, i.e. OHSUMED, TD2003 and MSLR-WEB10K, for training and evaluation. Among the three data sets, OHSUMED and TD2003 in LETOR3.0 [13] are constructed based on two widely used data collections in information retrieval respectively, the OHSUMED collection and  X .gov X  collection for TREC 2003 topic distillation task respectively. MSLR-WEB10K is another data set released by Microsoft Research, where relevance judgments are ob-tained from a retired labeling set of a commercial Web search engine (i.e. Bing). We choose such three data sets for our experiments, since they have quite different properties de-scribed in Table 1. All the three data sets can be further divided into training set and test set with their  X  X tandard partition X . In our experiments, all the evaluations are con-ducted using the 5-fold cross validation.

Ranking Algorithms . In our work, we mainly focus on the widely applied pairwise and listwise learning to rank algorithms. We employ two typical pairwise learning to rank algorithms, namely RankSVM [7] and RankBoost [4], and two typical listwise learning to rank algorithms, namely ListNet [3] and AdaRank [22]. These algorithms are cho-sen not only because they belong to different categories of learning to ranking approaches (i.e. pairwise and listwise), but also because they adopt different kinds of ranking func-tions (i.e. linear and non-linear). Specifically, RankSVM and ListNet use a linear function for scoring, which can be repre-sented as f ( x ) = w  X  x: Meanwhile, RankBoost and AdaRank are both ensemble methods that combine many weak rank-ing functions. Their non-linear ranking function can be ex-pressed as f ( x ) = weak ranking function at the t -th iteration.
In this section, we first simulate the label noise in training data with the same noise injection method as [14, 21]. Then we show our basic observations on data sensitivities to label noise.
We take the original data sets as our ground truth (i.e. noise free data) in our experiments. To simulate label noise in real data sets, we randomly inject label errors into training data, while the test data is fixed. We randomly change some of the relevance labels to other labels uniformly. Since the noise is introduced at the document level, here we define the document noise ratio as the proportion of noisy documents (i.e. documents with error labels) to all the documents, re-ferred to as dNoise. Given a dNoise , each query-document pair keeps its relevance label with probability 1  X  , and changes its relevance label with probability . Note that with the simple noise injection method, we assume that hu-man judgment errors are independent and equally possible in different grades. In practice, the possibility of judgment errors in different grades are not equal [15], which can be considered in our future work.
Here we show our basic observations which motivated our work. We consider the ranking performance variation in terms of MAP and NDCG@10 with dNoise from 0 to 0 : 5 with a step of 0 : 05. For each fold with a given dNoise, we will apply our noise injection method to a training set for 10 times, and obtain the average performance over the corre-sponding noise free test set. Each point in the performance curve is derived from the average over 5 folds in Figure 1.
The performance degradation of ranking algorithms may vary largely over different data sets with the increase of dNoise. Take RankSVM for instance in Figure 1(a), its per-formance on TD2003 in terms of MAP (i.e. orange curve with up triangles) decreases quickly as the dNoise increases. Its performance on OHSUMED (i.e. black curve with up triangles) keeps stable for a long range of dNoise, and then drops. Its performance on MSLR-WEB10K (i.e. blue curve with up triangles) is hardly affected even though the dNoise reaches 0 : 5. The results are consistent in terms of NDCG@10 corresponding to three curves with circles in Figure 1(a). Be-sides, we can also observe very similar performance degrada-tion behavior with the other three algorithms in Figure 1(b), (c) and (d). In fact, similar results can also been found in previous work [21], but such observations are not the main concern in their work.

The above observations are actually contrary to the fol-lowing two intuitions. 1) Degradation Intuition : For a ma-chine learning algorithm, its performance usually would de-grade along with the deterioration of the training data qual-ity (i.e. increase of noise in the training data), no matter quickly or slowly. 2) Consistency Intuition : For a same ma-chine learning algorithm, the performance degradation be-havior against label noise usually would be similar across the data sets. A possible reason for the above results is that the label noise (i.e. dNoise) cannot properly characterize the deterioration of the training data quality in learning to rank. This brings us the following question: what is the true noise that affects the performances of ranking algorithms?
To answer the above question, we need to briefly re-visit the learning to rank algorithms. As we know, the pairwise ranking algorithms transform the ranking problem to a bi-nary classification problem, by constructing preference pairs of documents from human labeled data. The FIFA World Cup may help understand that a ranking list, a basic unit in listwise learning, is generated by pairwise contests. Thus, for both pairwise and listwise learning, the quality of pairs turns out to be the key to the ranking algorithms. In other words, errors (i.e. noise) in document pairs arise from the original document label noise might be the true reason for the performance degradation of ranking algorithms.
To verify our idea, we propose to evaluate the performance of ranking algorithms against the document pair noise. Sim-ilar to the definition of dNoise, here we define the document pair noise ratio as the proportion of noisy document pairs (i.e. document pairs with wrong preference order) to all the document pairs, referred to as pNoise. We would like to check whether the performance degradation of ranking al-gorithms against pNoise is consistent across the data sets.
For this purpose, we first investigate how document pair noise arises from document label noise and take a detailed look at pNoise. The document pairs  X  d i ;d j  X  (i.e. document d is more relevant than d j to the given query) generated from a noisy training data set can be divided into three Table 2: Proportions of Predicted Positive Document Pairs with a tie ca tegories according to the original relationship of the two documents in the noise free data set. (1) Correct-Order Pairs . Document d i is indeed more relevant than d j in the original noise free training data. It is clear that the correct-order pairs do not introduce any noise since they keep the same order as in the noise free case, even though the rele-vance labels of the two documents might have been altered. (2)
Inverse-Order Pairs . Document d j is more relevant than d i in the original noise free training data instead. Ob-viously, the inverse-order pairs are noisy pairs since they are opposite to the true preference order between two docu-ments. (3) New-Come Pairs . Document d i and d j are a tie in the original noise free training data instead, so we esti-mate its true preference order by the ranking model learned from the noise free training data. The probability being a noisy pair is estimated by the proportion that the predic-tion score of d j is higher than d i through experiments 1 experimental results in Table 2 show that there is approxi-mately half a chance of being a noisy pair.

According to the above analysis, the pNoise of a given data set with known noisy labels can be estimated as follows wh ere N inverse , N new and N all denote the number of inverse-order pairs, new-come pairs and all the document pairs, re-spectively.

With this definition, the performance variations of ranking algorithms against pNoise are depicted in Figure 2, where each sub figure corresponds to performances over three data sets using four ranking algorithms respectively. Surpris-ingly consistent behavior for the performance degradation is observed across different data sets. The performances (e.g. MAP and NDCG@10) of ranking algorithms keep quite stable as pNoise is low. When pNoise exceeds a certain point 2 (around 0 : 5 as indicated in our experiment), per-formances drop quickly. The results w.r.t. pNoise are in accordance with the degradation and consistency intuitions mentioned before, which are violated in the case of dNoise.
Therefore, the results indicate that pNoise captures the intrinsic factor that determines the performance of a rank-ing algorithm, and thus can well explain the consistency of performance degradation of various ranking algorithms.
Now we know that dNoise is a natural measure of label noise in training data as label noise is often introduced at document level in practice, while pNoise is an intrinsic mea-sure of noise that can reflect the true noise for ranking al-gorithms. Here we explore the variation of pNoise against dNoise across different data sets in Figure 3.

The increase of pNoise with respect to dNoise varies largely over different data sets, which actually well explains our ba-sic observations. Given a small dNoise (e.g. 0 : 1) on TD2003, pNoise reaches a very high value ( &gt; 0 : 4) in Figure 3, which is the turning point of the performance curve in Figure 2. This explains why the performances of ranking algorithms on TD2003 drop quickly along with the increase of dNoise. On the contrary, on OHSUMED and MSLR-WEB10K, the variations of pNoise with respect to dNoise are more gentle, and correspondingly the performances of ranking algorithms are quite stable. Even when dNoise reaches 0 : 5 on MSLR-WEB10K, the corresponding pNoise is still below a thresh-old (about 0 : 4), which means the pNoise has not reached the turning point of the performance curve on MSLR-WEB10K according to blue curves in Figure 2. This explains why the ranking performance is hardly affected by dNoise on MSLR-WEB10K in our basic observations. F igure 3: pNoise with respect to dNoise over Three Data Sets
In this paper, we conducted data analysis to first address the data robustness problem in learning to rank algorithms. In our study, we find that document pair noise captures the true noise of ranking algorithms, and can well explain the performance degradation of ranking algorithms. As a measure of labeling accuracy, it can be used for annotator filtering in a crowdsourced relevance labeling task in our next step. Additionally the current noise injection method is somehow simple and we may further improve the injection method for better analysis. In fact, the noise ratio could be different among queries due to various difficulty levels. This research work was funded by the 973 Program of China under Grants No. 2012CB316303, No. 2013CB329602, the 863 Program of China under Grants No. 2014AA015204, No. 2012AA011003, the National Natural Science of China under Grant No. 61232010, No. 61203298 and the National Key Technology R&amp;D Program of China under Grants No. 2012BAH39B02.
