 Twitter has attracted hundred millions of users to share and dissem-inate most up-to-date information. However, the noisy and short nature of tweets makes many applications in information retrieval (IR) and natural language processing (NLP) challenging. Recently, segment-based tweet representation has demonstrated e ff ectiveness in named entity recognition (NER) and event detection from tweet streams. To split tweets into meaningful phrases or segments, the previous work is purely based on external knowledge bases, which ignores the rich local context information embedded in the tweets. In this paper, we propose a novel framework for tweet segmentation in a batch mode, called HybridSeg . HybridSeg incorporates local context knowledge with global knowledge bases for better tweet segmentation. HybridSeg consists of two steps: learning from o ff -the-shelf weak NERs and learning from pseudo feedback. In the first step, the existing NER tools are applied to a batch of tweets. The named entities recognized by these NERs are then employed to guide the tweet segmentation process. In the second step, Hybrid-Seg adjusts the tweet segmentation results iteratively by exploiting all segments in the batch of tweets in a collective manner. Ex-periments on two tweet datasets show that HybridSeg significantly improves tweet segmentation quality compared with the state-of-the-art algorithm. We also conduct a case study by using tweet segments for the task of named entity recognition from tweets. The experimental results demonstrate that HybridSeg significantly ben-efits the downstream applications.
 H.3.1 [ Information Systems ]: Content Analysis and Indexing X  Linguistic processing Twitter, Tweet, Tweet segmentation, Named Entity Recognition
Twitter has become one of the most important channels for peo-ple to find, share, and disseminate timely information. As of March 2012, there are more than 140 million active Twitter users with over timely information generated by its millions of users, it is impera-tive to understand tweets X  language for the tremendous downstream applications like named entity recognition (NER) [12,14,21], event detection and summarization [5, 15, 22], opinion mining [16, 17], sentiment analysis [4,13,25], etc.

Given the limited length ( i.e., 140 characters) of a tweet and no restrictions on its writing styles, tweets often contain grammatical errors, misspellings, and informal abbreviations. The error-prone and short nature of tweets makes the word-level language models for tweets not reliable. For example, given a tweet  X  X  call her, no answer. Her phone in the bag, she dancin. X , there is no clue to guess its true theme with the word independent assumption. Very recently, a segment-based tweet representation model has been pro-posed to partially overcome the adverse features of tweets [12]. Af-ter splitting a tweet into a sequence of consecutive n-grams ( n  X  1), each of which is called a segment, the latent topics of the tweet can be better captured. For example, the segment  X  X he dancin X  in the previous example tweet is actually a key concept  X  it classifies this tweet into the family of tweets talking about the song  X  X he Dancin", a trend topic in Bay Area in Jan, 2013.

A segment can be a named entity ( e.g., a movie title  X  X inding nemo X ), a semantically meaningful information unit ( e.g.,  X  X  ffi -cially released X ), or any other type of phrase which appears  X  X ore than chance X  [12]. Because segments preserve more semantics of tweets than words do, they have been used as the language units for the tasks of NER and event detection [11, 12]. Segment-based representation has shown its e ff ectiveness over word-based repre-sentation particularly in the event detection task.

However, developing a high quality tweet segmentation is not trivial, because the prevalence of grammatical errors and unreli-able linguistic features makes classic Natural Language Processing (NLP) techniques including part-of-speech (POS) tagging [7] and NER less applicable to tweets. For example, classic POS taggers may fail in recognizing  X  X he dancin X  as a noun phrase in the pre-vious example tweet. The seminal tweet segmentation algorithm in [12] mainly relies on external knowledge bases ( e.g., Wikipedia and Microsoft Web N-Gram corpus). The main assumption is that the core semantic information is well preserved in tweets in the form of named entities or semantic phrases for information shar-ing, despite the noise nature of tweets, and those named entities and semantic phrases can be largely found in aforementioned external knowledge bases. We categorize this method into global context based solution, since it utilizes the language-generic information contained in the external knowledge bases.
Purely relying on external knowledge bases for tweet segmen-tation has two major weaknesses. First, tweets are highly time-sensitive so that many recently appeared terms like  X  X he Dancin X  can not be found in external knowledge bases. Another example is  X  X in Pei Ling X , a new politician who gained her reputation in Sin-ated in Wikipedia until 4th, April 2011, about a week later than the time her name appeared in tweets. Second, by defining other tweets published within the same short time window (i.e., an hour / a day) as the local context , a tween can only be well understood within its local context. For example,  X  X ancin X  could be a typo if it only appears in a single tweet. But it may refer to some named entity if it appears in lots of tweets in a short term.

In this paper, we propose a hybrid tweet segmentation framework incorporating local contexts into the existing external knowledge bases, and name our method HybridSeg . HybridSeg conducts tweet segmentation in batch mode. Following the same scope of [12], we only segment tweets from a targeted Twitter stream. A targeted Twitter stream receives tweets based on user defined criteria ( e.g., tweets containing some predefined hashtags or keywords, tweets published by a predefined list of users, or tweets published by users from a specific geographical region). Tweets from a targeted Twit-ter stream are grouped into batches by their publication time using a fixed time interval ( e.g., an hour or a day). Each batch of tweets are then segmented by HybridSeg collectively.
 HybridSeg conducts tweet segmentation in an iterative manner. At the first iteration, HybridSeg segments the tweet by utilizing the local linguistic features of the tweet itself. To avoid implementation from scratch, we simply apply a set of existing NER tools trained over general English languages on tweets. These existing NER tools provide an initial collection of confident segments by voting. Initializing HybridSeg with a set of o ff -the-shelf NER tools is based on the observation that some tweets from o ffi cial accounts of news agencies, organizations, advertisers, and celebrities are likely well written. A small set of named entities extracted from these tweets based on voting of classic NER tools can be a high precise yet low recall solution of tweet segmentation. For example, although  X  X in Pei Ling X  is a new figure, it can be recognized as people name by classic NERs with high confidence because there are many well-written tweets about her generated by news agencies.

After that, HybridSeg learns better segmentation iteratively based on the confident segments extracted from the batch of tweets in the last iteration. The iterative process stops when no more changes are observed from the resulted segments. The philosophy is the con-fident segments of one tweet can a ff ect the segmentation of other tweets in the same batch. External knowledge bases are also inte-grated here to better measuring the cohesiveness of segments. To the best of our knowledge, this paper proposes a novel framework by leveraging both global and local contexts in tweet segmentation for targeted Twitter streams.

We conduct extensive experimental analysis on two tweet datasets and compare HybridSeg with the approach purely relying on global knowledge bases [12], known as GlobalSeg in this paper. Our ex-perimental results show that HybridSeg achieved significant im-provement on tweet segmentation quality by comparing to the set of manually labeled tweets. We further split named entities from valid segments and only compared these named entities to human labels. Again, HybridSeg improved the NER quality compared to GlobalSeg .

The rest of the paper is organized as follows. Section 2 surveys the related works such as NLP, collocation measures, noun phrase extraction, and text segmentation. Section 3 gives an overview of the global knowledge guided segmentation process. The proposed hybrid-context based segmentation algorithm is presented in Sec-tion 4. In Section 5, we conduct experimental evaluation of the pro-posed HybridSeg framework and provide detailed algorithm analy-sis. Section 6 concludes this paper. Many conventional NLP techniques are designed for formal text. Many of these techniques are supervision based and heavily rely on the local linguistic features, such as POS tags, word capitalization, trigger words ( e.g., Mr., Dr.), and dictionary lookup like gazetteers, etc. These linguistic features, together with e ff ective supervised learning algorithms ( e.g., hidden Markov model (HMM) and con-ditional random field (CRF)), achieve state-of-the-art performance on formal text corpus [6,19,26]. However, these techniques cannot be directly applied to tweets because of the noisy and short nature of the latter. The error-prone and short nature of tweets (and other user-generated short text) has attracted renewed interests in con-ventional tasks in NLP including POS tagging [7], named entity recognition (NER) [12,14,21], etc.

To improve POS tagging on tweets, Gimple et al. incorporate tweet-specific features including at-mentions, hashtags, URLs, and emotions [7]. In their approach, they measure the confidence of capitalized words and apply phonetic normalization for ill-formed words to address possible peculiar writings in tweets. Normaliza-tion of ill-formed words in tweets has established itself as an impor-tant research problem. A supervised approach is employed in [8] to first identify the ill-formed words. Then, the correct normaliza-tion of the ill-formed word is selected based on a number of lexical similarity measures.

Both supervised and unsupervised approaches have been pro-posed for named entity recognition in tweets. T-NER is a part of the tweet-specific NLP framework in [21]. T-NER first segments named entities using a CRF model with orthographic, contextual, and dictionary features, and then labels the named entities using a LDA (Latent Dirichlet allocation) model. The NER solution pro-posed in [14] is also based on a CRF model. The unsupervised ap-proach named TwiNER recognizes named entities with two steps: tweet segmentation and segment ranking [12]. The tweets from a Twitter stream are processed in batch mode. Member tweets in a batch are segmented and each tweet segment is a candidate named entity (see Section 3). Collocation measure between words is uti-lized in tweet segmentation together with the global knowledge bases. Experimental results show that the collocation measure Sym-metric Conditional Probability (SCP) is much more e ff ective than Pointwise Mutual Information (PMI). To recognize named entities from the candidate segments, a segment graph is built based on segment co-occurrences. The segments are then ranked by apply-ing random walk on the segment graph (see Section 5.4.1). Chua et al. [3] propose to extract noun phrases from tweets using an un-supervised approach which is mainly based on POS tagging. Each extracted noun phrase is a candidate named entity.

Tweet segmentation is conceptually similar to the task of text segmentation despite the unit for segmentation (words vs sentences) is significantly di ff erent. Text segmentation divides a given text document into consecutive non-overlapping topically cohesive seg-ments [1]. Each segment consists of several consecutive sentences. Most existing works follow a similar idea that the boundaries of consecutive segments correspond to a change in word usage. The boundaries between segments are often detected by monitoring the drop of the lexical similarity. The proposed solutions di ff er widely in the way of calculating the sentence-pair similarity ( i.e., topical Figure 1: The framework of global knowledge bases guided tweet segmentation [12], named GlobalSeg . cohesiveness). Measures based on word co-occurrence [2, 9, 10] and generative models [1, 18, 20, 23] have been extensively stud-ied. The determination of the segment boundaries may not only be purely based on the local sentence-pair similarities but also be based on the global information derived from the distribution of the lexical similarities of the far neighboring sentences [2,10]. Concep-tually, the idea of incorporating the global information for bound-ary determination is similar to our idea of exploiting local context in tweets for tweet segmentation. However, the two problems (text segmentation and tweet segmentation) are di ff erent by definition and the techniques for text segmentation cannot be directly applied in tweet segmentation. Global knowledge bases like Wikipedia and Microsoft Web N-Gram corpus [24] have been successfully utilized to guide the tweet segmentation, which is named GlobalSeg by this paper. In this section, we briefly review its process, as illustrated by Figure 1.
The input of GlobalSeg is a tweet d = w 1 w 2 . . . w  X  , and its output is m ( m  X   X  ) non-overlapped segments, d = s 1 s 2 . . . s segment s i is a n -gram ( n  X  1). The optimal segmentation is to maximize the sum of stickiness scores of m segments: where C ( s ) denotes a stickiness function, defined as:
Eq. 2 encodes three factors for measuring the stickiness of each segment. They are: 1) the normalized segment length L ( s ) = 1 the penalty on long segments; 2) the probability that s is inside an anchor text in Wikipedia Q ( s ); 3) the Symmetric Conditional Probability (SCP) measure defined by Pr ( s ) is the n-gram probability provided by Microsoft Web N-Gram service. If s is a single word w , SCP ( s ) = 2 log Pr ( w ). If s contains more words, SCP tends to keep a cohesive s while all its possible binary partitions are not cohesive.
GlobalSeg defines a tweet segmentation method solely based on external knowledge bases ( e.g., Wikipedia and Microsoft Web N-Gram corpus). Its main assumption is that the external knowledge bases provide more robust segmentation features for tweets than their original natural languages which tend to be error-prone [12]. Figure 2: The framework of Hybrid-context based tweet seg-mentation, named HybridSeg .
 Despite its successful application in named entity recognition, it has two inherent limitations based on the below observations.
O bservation 1. Tweets are not topically independent to each other within a time window.
 In GlobalSeg , all tweets are segmented independently. This as-sumption is too strong as tweets published closely in time often talk about the same theme. These similar tweets largely share the same segments. For example, similar tweets have been grouped together to collectively detect events from tweets, and the event is usually represented by the common discriminative segments across tweets [11]. However, there is no mechanism in GlobalSeg to force similar tweets to be segmented consistently.

O bservation 2. Linguistic features of the tweets are not always useless.
 In GlobalSeg , the local linguistic features of a segment in its tweet has been abandoned completely. This assumption is mostly true especially when tweets contain many unreliable linguistic features like misspellings, informal abbreviations and unreliable capitaliza-tions [21]. However, there indeed exist tweets composed in proper English. For example, some tweets are published by o ffi cial ac-counts of news agencies, organizations, advertisers, and celebrities. For these tweets, the local linguistic features can be an informative source for tweet segmentation.

The two observations essentially reveal the same phenomenon: except for the external knowledge bases, local information like the other similar tweets within a time window or the linguistic features in the current tweet can also help in segmenting tweets. As both similar tweets and linguistic features refer to the local contexts of tweets, they lead to our novel idea of incorporating local contexts into the existing external knowledge bases for better tweet segmen-tation. Accordingly, we name our new method HybridSeg , short for a hybrid-context based segmentation for tweets.
The general framework of HybridSeg is given in Figure 2. Com-pared to Figure 1, the major novelty is that we would examine the segment likelihood in a batch of tweets (including the tweet to be segmented) based on local contexts. That is to say, given a batch of tweets T within a certain time window, we are going to utilize
Algorithm 1: Tweet Segmentation by HybridSeg their local linguistic features collectively for each member tweet X  X  segmentation.

However, how to separate valid segments from the cha ff s in a batch of tweets only based on local contexts is a challenging task itself. To tackle this problem, we design an iterative process in our framework, as shown in Figure 3.

Starting from a set of o ff -the-shelf NERs trained on classic En-glish texts, we generate a collection of seed segments from T only based on local linguistic features by voting. The seed segment collection can be small yet highly confident. Then, we combine the seed segment collection with the external knowledge bases in HybridSeg to segment each tweet member. After that, only those segments ranked high by HybridSeg are used to replace the seed segment collection. The same process is then repeated until the segmentation results of HybridSeg do not change any more. There are a couple of advantages in our design. First, existing NERs have already captured most known local linguistic features so that we do not need to implement from scratch. Second, most existing NERs are trained on formal texts. Applying them directly on tweets will easily cause the overfitting error. A voting algorithm can partially alleviate the error of training. Third, during each iter-ation, only top segments with high confidence scores are fed into HybridSeg in a pseudo feedback manner, further alleviating the er-rors caused by overfitting.

The tweet segmentation process of HybridSeg is outlined in Al-gorithm 1. In the following, we elaborate HybridSeg step by step.
In this section, we apply m weak NERs on T to generate the ini-tial collection of seed segments. Based on the truth that a named entity is always a valid segment, we directly select the seed seg-ments from the output of NERs.

Given a segment s , let f s be its total frequency in T . One NER r may recognize s as a named entity f r i , s times. Note that f since a NER may only recognize some of s  X  X  occurrences as named entity. Assuming there are multiple o ff -the-shelf NERs r we further denote f R s to be the number of NERs that have detected at least one occurrence of s as named entity, f R s = P m I ( f
We approximate the probability of s being a seed segment using a voting algorithm defined by Eq. 4: Our approximation contains two parts. The right part of Eq. 4 (rf. Eq. 6) is the average confidence that one weak NER recognizes s as named entity. A biased estimation of the right part is simply 1 / m  X  P However, such simple average ignores the absolute value of f which can also play an important role here. For example, for a party name in an election event, it can appear hundreds of times within a tweet batch. However, due to the free writing styles of tweets, only tens of the party name X  X  occurrences can be recognized by weak NERs as named entity. In this case, f r i , s / f s is relatively small yet f , s is relatively high. Thus, we design a function that can favor When  X  is large, our function is more sensitive to the change of f , s / f s ; when  X  is small, a reasonably large f r i , s be close to 1 despite of a relatively small value of f r i paper we empirically set  X  = 0 . 2 in experiments. A small constant  X  is set to avoid dividing by zero.

The left part of Eq. 4, w ( s , m ) (rf. Eq. 5) uses a sigmoid function to control the impact of the majority degree of m weak NERs on seed segments, which is tuned by a factor  X  . For example, in our paper we set  X  = 10 so that as long as more than half of weak NERs recognize s as named entity, w ( s , m ) is close to 1. With a small  X  , w ( s , m ) is getting closer to 1 only when more and more weak NERs recognize s as named entity.
 The learning from weak NERs is summarized in lines 1 -5 in Algorithm 1. Next, we discuss the combination of global knowl-edge bases and seed segments. The learning of  X  will be detailed in Section 4.5.
After computing the probability of s being a valid segment, we can select top confident segments to be included in the seed seg-ment collection. The next question is: how can we combine seed segments with knowledge bases for better tweet segmentation? Recall that GlobalSeg encodes three factors: segment length, Wikipedia knowledge, and SCP measure based on Microsoft Web N-gram corpus, among which the text partition mainly relies on SCP measure. Now, the seed segment collection naturally provides another information source to guide the text partition. The idea can be as simple as assigning a high stickiness score to a partition in the seed segment collection. Accordingly, we modify the partition likelihood Pr ( s ) in SCP measure using a linear combination: where  X  Pr ( s ) is defined by Eq. 4 with a coupling factor  X   X  [0 , 1), and Pr MS (  X  ) is the n-gram probability provided by Microsoft Web N-Gram service.
The last step of HybridSeg is to iteratively improve the seed seg-ments using the output of HybridSeg , with the final goal of improv-ing its output in the end. The whole process will terminate until no more changes are observed in the output of HybridSeg .

Suppose at iteration i , HybridSeg outputs a set of segments {h s , f i where f i s is the number of times s is a segment at iteration i . Then, f / f s relatively records the segmentation confidence of HybridSeg on s at iteration i (recall that f s denotes the frequency of s in current batch T ). Similar to Eq. 6, we denote
Following the same combination strategy defined by Eq. 7, we have the following iterative updating function: and change Eq. 7 to: where  X  Pr 0 ( s ) is the voting result on m weak NERs only. The it-eration process will stop when predefined criterion is met. Here, we define the stop criterion based on Jensen-Shannon divergence (JSD) of the frequency distributions of tweet segments in two con-may be wrongly segmented because of the errors introduced by the weak NERs. These errors could be further propagated at later itera-tions. For this reason, we do not consider the segments detected by
The learning from pseudo feedback is summarized in lines 7 -13 in Algorithm 1.
Eq. 8 defines the iterative learning process of HybridSeg . The coupling factor  X  is crucial to control the convergence of learning and the segment re-ranking performance of the next iteration. To guarantee the convergence and quality of our algorithm, we need a systematic way to learn  X  .

Our idea is: a good  X  must ensure that top confident segments from the previous iteration should be detected more times in the next iteration. Note that at each iteration we do not classify text partitions into binary classes ( i.e., segment and non-segment), but simply assign a stickiness score to each text partition and then treat segmentation as a ranking problem. Therefore, our idea is equiva-lent to maximizing the sum of detected frequency of top confident segments (weighted by their stickiness scores, rf. Eq. 2) extracted from the previous iteration. Accordingly, learning the parameter  X  is converted to an optimization problem as follows: C s ) is the stickiness score of s output by HybridSeg and used for segment ranking at the previous iteration. Based on it, top-k seg-ments can be retrieved. f i + 1 ( s ) is the detected frequency of s at the current iteration, which is an unknown function of the variable  X  . Dataset Collection period #Tweets #Annotations SGE Apr 13 -May 13, 2011 226,744 3,328 Therefore, the optimal  X  is intractable. In our experiments, we used brute-force search strategy to find the optimal  X  for each iteration and each tweet batch. Fortunately, as the size of each tweet batch is controllable, the e ffi ciency is not our concern in the current work.
Note that for the 0 th iteration,  X  must be learned di ff erently be-cause there do not exist top confident segments from the previous iteration. Since we use the voting results of m weak NERs as the seed segments in the 0 th iteration, a good  X  must ensure that the confident segments voted by m weak NERs can be detected more times by HybridSeg .

Let N  X  be the segments that are recognized by all m NER sys-tems ( i.e., N  X  = { s | f R s = m } ). For each segment s  X  N sider its confident frequency to be the minimum number of times that s is recognized by a NER as named entity among all m NERs. Let the confident frequency of s be f c , s . f c , s = min In this equation,  X  Pr 0 ( s ) is the value computed using Eq. 4; f learning  X  . If segment s is very likely to be a named entity ( i.e.,  X  Pr 0 ( s ) is high) and it has been detected many times by all NERs ( i.e., f c , s is large), then the number of times s is successfully seg-mented f 0 s has a big impact to the selection of  X  . On the other hand, if  X 
Pr 0 ( s ) is low, or f c , s is small, or both conditions hold, then f 0 less important to  X  setting.
 ments recognized by all weak NERs because of the noisy nature of tweets. This helps to reduce the possible oscillations resulted from di ff erent  X  settings, since  X  is a global factor ( i.e., not per-tweet dependent). On the other hand, we also assume that all the o ff -the-shelf NERs are reasonably good, e.g., when they are applied on formal text. If there is a large number of NERs, then the defini-tion of f c , s could be relaxed to avoid having too small values for all segments, due to one or two poor-performing NERs among them.
In this section, we evaluate HybridSeg against GlobalSeg as the baseline. We conducted experiments on two datasets, i.e., the SIN and SGE datasets that were used to evaluate GlobalSeg in [12]. Three weak NERs, namely, LBJ-NER, Standford-NER, and T-NER were used as input in HybridSeg for learning local context. We compare segmentation accuracy of HybridSeg against GlobalSeg . Because GlobalSeg was used to detect named entities in [12], we also report the accuracy of named entity recognition using Hybrid-Seg and GlobalSeg respectively. Tweet Datasets . The SIN and SGE datasets were originally col-lected for simulating two targeted Twitter streams in [12]. The for-mer was a stream consisting of tweets from users in a specific ge-ographical region ( i.e., Singapore in this case), and the latter was a stream consisting of tweets matching some predefined keywords and hashtags for a major event ( i.e., Singapore General Election 2011 in this case).

Reported in Table 1, named entities in 4 , 422 tweets from SIN and 3 , 328 tweets from SGE have been manually annotated to eval-uate tweet segmentation and named entity recognition performance in [12]. Table 2 reports the statistics of the annotated NEs in the two datasets where f g s denotes the number of occurrences (or fre-quency) of segment s in the annotated ground truth G . The fre-quency distributions of the NEs are plotted in Figure 4.
All the annotated tweets in each dataset were published on the same day, making it possible to evaluate batch-mode tweet seg-mentation using a day as the time interval. To fairly compare with the results reported in [12], we conduct our experiments using all the annotated tweets in each dataset as a batch so as not to take ad-ditional context from other unlabeled tweets published in the same day.

We also used the same Wikipedia dump (released on 30 Jan, 2010 3 ) as in [12]. This dump contains 3 , 246 , 821 articles and there are 4 , 342 , 732 distinct entities appeared as anchor texts in these articles.
 Evaluation Metric. Recall that the task of tweet segmentation is to split a tweet into semantically meaningful segments. Ideally, a tweet segmentation method shall be evaluated by comparing its segmentation result against manually segmented tweets. However, manual segmentation of a reasonably sized data collection is ex-tremely expensive and requires good understanding of the tweets. We therefore choose to follow [12] to evaluate a tweet segmenta-tion method based on whether the manually annotated named enti-ties are correctly segmented ( i.e., split as segments). Because each named entity is a valid segment, the annotated named entities in this case serve as partial ground truth for the evaluation. We use two measures, namely Recall and Frequency-biased Recall . Table 2: The annotated named entities in SIN and SGE datasets Dataset #NEs min f g s max f g s P f g s #NEs s.t. f g s &gt; 1 SIN 746 1 49 1234 136
SGE 413 1 1644 4073 161 Methods and Parameter Setting. We compare our HybridSeg method with GlobalSeg method in [12] as the baseline. Note that the method proposed in [12] is named TwiNER for named entity recognition and segmentation was one of its intermediate step. In this paper, GlobalSeg refers to the segmentation step in TwiNER .
The following three weak NERs are used in HybridSeg to de-rive the local context knowledge ( i.e., m = 3). Note that, we used the version downloaded from their corresponding websites. These NERs are not trained using our data.
For parameter settings,  X  in Eq. 6 is set to  X  = 0 . 2; the top-K segments in Eq. 9 for  X  adaption is set to K = 50. The search space for  X  is set to be [0 , 0 . 95] with a step 0 . 05. In this section we first report the accuracy of the three weak NERs in detecting named entities. We then compare HybridSeg and GlobalSeg on segmentation accuracy.
 Accuracy of three Weak NERs . The accuracy of the three weak NERs in recognizing named entities is evaluated by three standard Table 3: Accuracy of the three weak NERs; N  X  denotes the set of NEs detected by all three weak NERs. The best results are in boldface.
 Method LBJ-NER 0.335 0.357 0.346 0.674 0.402 0.504 T-NER 0.273 0.523 0.359 0.696 0.341 0.458 Stanford-NER 0.447 0.448 0.447 0.685 0.623 0.653
All NERs N  X  0.578 0.233 0.332 0.876 0.192 0.315 metrics: Precision ( Pr ), Recall ( Re ), and F 1 . Pr measures the per-centage of the recognized named entities that are true named enti-ties; Re measures the percentage of the true named entities that are correctly recognized; and F 1 = 2  X  Pr  X  Re / ( Pr + Re ) is the harmonic mean of Pr and Re . The type of the named entity ( e.g., person, location, and organization) is ignored as in [12]. Similar to the seg-mentation recall measure, each occurrence of a named entity in a specific position of a tweet is considered as one instance for the three measures Pr , Re , and F 1 .

Table 3 reports the performance of the three weak NERs on the two datasets SIN and SGE respectively. Observed from the table, all three weak NERs perform poorly on the two tweets collections. The results are consistent with that reported in [12]. However, if consider the set of NEs that are recognized by all three NERs N the precision is much higher than any of the individual weak NER. In particular, the precision on SGE dataset is 0.876. That is, the set of NEs detected by all the three NERs are of good precision and can be used as local context knowledge in improving tweet segmentation. On the other hand, the recall of N  X  is much lower than any of the three NERs as expected.
 Segmentation Accuracy . We report two sets of results for Hybrid-Seg and compare them with the baseline GlobalSeg . Specifically, in Table 4, we report the results of HybridSeg NER after learning from weak NERs, and the results of HybridSeg NER + Iter after the iterative learning converges in HybridSeg . We make three observations from the results: 1. Learning from the local context brought in by weak NERs, 2. HybridSeg NER + Iter further improves Re and Re F on both datasets. 3. Compared with the baseline method GlobalSeg , the proposed
Our experimental results show that, incorporating local context knowledge in a batch of tweets greatly improves tweet segmenta-tion accuracy. More importantly, the local context knowledge can be obtained with almost no cost with o ff -the-shelf NERs and lo-cally derived statistics. On the other hand, the improvement made by learning from pseudo feedback is relatively small compared to the improvement made by learning from weak NERs. In the next section, we conduct a detailed analysis of HybridSeg for possible reasons.
 Table 4: Performance of tweet segmentation algorithms, where  X  indicates the di ff erence against the baseline is statistically sig-nificant by paired t -test.
 Figure 5: Re , Re F and  X  NER (  X  ) (%) values of HybridSeg varying  X  in the range of [0 , 0 . 95] . Impact of  X  Adaption . We exploit the local context by using lin-ear combination in the calculation of SCP score (rf. Eq. 7 and 8). The choice of  X  largely a ff ects the performance of the tweet seg-mentation process. While a small  X  may not su ffi ciently exploit the local context, a very large  X  could make the local context dominate the segmentation process which may adversely a ff ect the segments with weak local context because of their limited number of occur-rences.
 Figure 5 demonstrates the impact of varying  X  on HybridSeg in terms of Re and Re F in the 0 th iteration (rf Eq. 10). For easy demonstration, we plot the normalized score obtained by Eq. 10 with di ff erent  X  , denoted by  X  NER (  X  ) in the figure. Observe that  X 
NER (  X  ) is positively correlated with the performance metrics Re and Re F on both datasets. In our experiments, we set the parameter  X  to be the smallest value leading to the best  X  NER (  X  ), i.e.,  X  = 0 . 5 on SIN and  X  = 0 . 7 on SGE . Because  X  is a global factor for all member tweets in the batch and  X  NER (  X  ) is computed based on a small set of seed segments. A larger  X  may not a ff ect the segmen-tation of the seed segments because of their confident local context, but may cause some other segments to be wrongly split due to their noisy local context. Observe there is minor degradation for both Re and Re F on SIN dataset when  X  &gt; 0 . 45 although  X  NER Table 5: The performance of HybridSeg NER + Iter up to 4 itera-tions.
 Iteration the maximum. The impact of varying  X  in the following iterations is not plotted for the interests of space. Similar observations hold. Analysis of the Iterative Learning . HybridSeg employs an itera-tive strategy to learn from the segments produced in the previous iteration. Table 5 reports the performance of HybridSeg and JSD between the frequency distributions of the segments from two iter-ations.

It is observed that HybridSeg converges after the second itera-tion. To understand the possible reasons for the quick convergence, we analyze the segments detected in each iteration. There are three categories of them:
Table 6 reports the number of segments and their number of oc-currences in each of the three sets (FS, MS, and PS). For partially detected segments, the number of detected and missed occurrences are reported.

As shown in the table, very few segments are partially detected after learning from weak NERs in 0 th iteration (19 for SIN and 24 for SGE ). The possible improvement can be made in the next itera-tion is to further detect the total 25 missed occurrences in SIN (resp. 67 in SGE ), which accounts for 2.03% (resp. 1.64%) of all anno-tated NEs in the dataset. That is, the room for further performance improvement by iterative learning is marginal.

Consider the SIN dataset, on average, there are about 6 detected occurrences to provide local context for each of the 19 partially detected segments. With the local context, HybridSeg manages to reduce the number of partially detected segments from 19 to 11 in the next iteration and the total number of their missed instances are reduced from 25 to 14. No changes are observed for the remain-ing 11 partially detected segments in iteration 2. Interestingly, the number of fully detected instances increased by 2 in the last iter-ation. The best segmentation of a tweet is the one maximizes the stickness of its member segments (rf Eq. 1). The change in the stickness C ( s ) of other segments leads to the detection of these two new segments in the fully detected category, each occurs once in the dataset.

In SGE dataset, the 24 partially detected segments reduce to 12 in the next iteration by learning from pseudo feedback. No change to these 12 partially detected segments are observed in the next it-eration, despite some of these segments have very strong local con-text based on their detected occurrences. A manual investigation show that the missed occurrences are wrongly detected as part of  X  X SP Election Rally X  and the latter is not annotated as a named en-tity. A further investigation shows that, probably based on its cap-italization,  X  X SP Election Rally X  is detected by weak NERs with strong confidence ( i.e., all its occurrences are detected). Because of its strong confidence,  X  X SP X  cannot be separated from this longer segment in the next iteration regardless the setting of  X  . We note that although  X  X SP Election Rally X  is not annotated in the ground truth as named entity, it is indeed a semantically meaningful phrase. On the other hand, a large portion of the occurrences for these 12 partially detected segments have been successfully detected from other tweets.

Compared to the baseline GlobalSeg which does not take local context, HybridSeg significantly reduces the number of missed seg-ments from 195 to 152, which is 22% reduction on SIN dataset. On SGE dataset, the reduction is 20% from 140 to 112. Many of these segments are fully detected in HybridSeg .
In this section, we use Named Entity Recognition as a down-stream application to evaluate the impact of tweet segmentation. We first briefly review the TwiNER method for NER [12] and then propose our own NER method taking tweet segment as input.
TwiNER recognizes named entities by applying Random Walk on the segments from a batch of tweets. The main assumption is that a named entity often co-occurs with other named entities in a batch of tweets while non-entity segments rarely co-occurs with named entities. The weight of the co-occurrence between two segments is measured by Jaccard Coe ffi cient. A random walk model is then applied to the segment graph. Let  X  s be the stationary probability of segment s by applying random walk, the segment is then weighted by: that a segment that frequently appears in Wikipedia as an anchor text is more likely to be a named entity. With the weighting y ( s ), the top K segments are considered as named entities.
Due to the short nature of tweets, the gregarious property could be very weak in tweets. As shown in Table 2, 82% of the annotated named entities appear only once in SIN (and 61% in SGE ). We choose to explore the part-of-speech tags in tweets for named entity recognition by considering noun phrases as named entities.
We estimate the likelihood of a segment being a noun phrase (NP) by considering the POS tags of its constituent words. Table 7 lists three POS tags that are considered as the indicators of a seg-its i -th occurrence, we calculate the probability of segment s being an noun phrase as follow: Dataset SIN dataset SGE dataset GlobalSeg 504 647 195 214 47 113 85 234 708 140 336 40 2850 179 Table 7: Three POS tags as the indicator of a segment being a noun phrase, reproduced from [7]
This equation considers two factors. The first factor estimates the probability as the percentage of the constituent words being labeled with an NP tag for all the occurrences of segment s , where [ w ] is 1 if w is labeled as one of the three POS tags in Table 7, and 0 oth-erwise; For example,  X  X hiam see tong X , the name of a Singaporean politician and lawyer 6 , is labeled as  X  X  X  (66 . 67%), NVV (3 . 70%),  X V X  (7 . 41%) and  X VN (22 . 22%) 7 . By considering the types of all words in a segment, we can obtain a high probability of 0 . 877 for  X  X hiam see tong X . The second factor of the equation introduces a scaling factor to give more preference to frequent segments, where  X  f and  X  ( f s ) are the mean and standard deviation of segment fre-quency.
 ing  X  s in Eq 12 by  X  P NP ( s ).
 With two named entity recognition approaches, namely Random Walk (RW) and POS tagging respectively, and two segmentation methods, namely GlobalSeg and HybridSeg , we have four com-binations to evaluate: GlobalSeg RW , HybridSeg RW , HybridSeg and GlobalSeg POS . Note that, GlobalSeg RW is the same method named TwiNER in [12].
 We introduce another baseline method Unigram POS , which use POS Tagging without segmentation. Similar to the work in [3], we extract noun phrases from the batch of tweets by using the follow-ing regular expression.
 The regular expression states that a noun phrase can be a combi-nation of common noun, proper noun and numeral, which begins with common or proper noun. The confidence of a noun phrase is computed using a modified version of Eq. 13 by removing its first component.

In summary, we have five methods to evaluate: GlobalSeg RW HybridSeg RW , HybridSeg POS , GlobalSeg POS , and Unigram
Table 8 reports the performance of the 5 methods. The results reported is the highest F 1 of each method achieved for varying K &gt; 50 following the same setting in [12]. Three observations are made. Table 8: The performance of GlobalSeg and HybridSeg with Random Walk and POS for NER Unigram POS 0 . 516 0 . 190 0 . 278 0 . 845 0 . 333 0 . 478 GlobalSeg RW 0 . 576 0 . 335 0 . 423 0.929 0 . 646 0 . 762 HybridSeg RW 0 . 618 0 . 343 0 . 441 0 . 907 0 . 683 0 . 779 GlobalSeg POS 0 . 647 0 . 306 0 . 415 0 . 903 0 . 657 0 . 760
HybridSeg POS 0.685 0.352 0.465 0 . 911 0.686 0.783 1. Tweet segmentation greatly improves NER. Unigram POS is 2. For a specific NER approach, either Random Walk or POS 3. Without local context in segmentation GlobalSeg POS is slightly
Figure 6 plots the Precision @ K for the 5 methods on the two datasets with varying K from 20 to 100. The Precision @ K reports the ratio of named entities among the top-K ranked segments by each method. Note that, Precision @ K measures the ranking of the segments detected from a batch of tweets; the occurrences of each segment in the ranking are not considered. This is di ff erent from the measures ( e.g., Pr ) reported in Table 8 where the occurrences of the named entities are considered ( i.e., whether a named entity is correctly detected at a specific position in a given tweet). We made the following observations from Figure 6.

On SIN dataset, it is clear that all methods using POS tagging for NER enjoy much better precision. RW based methods deliver much poorer precisions due to the lack of co-occurrences in the tweets. As reported in Table 2, 82% of the annotated named enti-ties appear only once in SIN . Among the three POS based methods, HybridSeg POS dominates the best precisions on all K values from 20 to 100. On SGE dataset, the di ff erences in precisions between POS based methods and RW based methods become smaller compared to those on SIN dataset. The reason is that in SGE dataset, about 39% of named entities appear more than once, which makes a much larger chance of co-occurrences. Between the two best performing methods HybridSeg POS and GlobalSeg POS , the former outperformed the latter on 6 K values plotted between 40 and 90. Without seg-mentation Unigram POS performed poorly in terms of Precison @ K measure on SGE dataset.
Recently, tweet segmentation has been proven to be e ff ective in the tasks of NER, event detection, and summarization. In this pa-per, we present a novel framework HybridSeg , which aggregates both the local context knowledge and the global knowledge bases in the process of tweet segmentation. First, HybridSeg exploits the local linguistic features in a collective manner by using the exist-ing NER tools. The recognized named entities with high confi-dence positively enhance the performance of tweet segmentation. Moreover, information that has not been well captured in the global knowledge bases is derived based on all segments from the tweets. Then, HybridSeg iteratively learns a better segmentation by consid-ering confident seed segments in the previous iteration. Through extensive experiments, we show that HybridSeg significantly out-performs the existing state-of-the-art algorithm on tweet segmenta-tion. We also show that the better segmentation benefits the named entity recognition application in tweets. In future, we plan to take more local factors into consideration for tweet segmentation, such as local word dependency. [1] D. Beeferman, A. Berger, and J. La ff erty. Statistical models [2] F. Y. Y. Choi. Advances in domain independent linear text [3] F. C. T. Chua, W. W. Cohen, J. Betteridge, and E.-P. Lim. [4] J. E. Chung and E. Mustafaraj. Can collective sentiment [5] A. Cui, M. Zhang, Y. Liu, S. Ma, and K. Zhang. Discover [6] J. R. Finkel, T. Grenager, and C. Manning. Incorporating [7] K. Gimpel, N. Schneider, B. O X  X onnor, D. Das, D. Mills, [8] B. Han and T. Baldwin. Lexical normalisation of short text [9] M. A. Hearst. Texttiling: segmenting text into [10] A. Kazantseva and S. Szpakowicz. Linear text segmentation [11] C. Li, A. Sun, and A. Datta. Twevent: segment-based event [12] C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and B.-S. [13] K.-L. Liu, W.-J. Li, and M. Guo. Emoticon smoothed [14] X. Liu, S. Zhang, F. Wei, and M. Zhou. Recognizing named [15] X. Liu, X. Zhou, Z. Fu, F. Wei, and M. Zhou. Exacting social [16] Z. Luo, M. Osborne, and T. Wang. Opinion retrieval in [17] X. Meng, F. Wei, X. Liu, M. Zhou, S. Li, and H. Wang. [18] H. Misra, F. Yvon, J. M. Jose, and O. Cappe. Text [19] L. Ratinov and D. Roth. Design challenges and [20] M. Riedl and C. Biemann. Topictiling: a text segmentation [21] A. Ritter, S. Clark, Mausam, and O. Etzioni. Named entity [22] A. Ritter, Mausam, O. Etzioni, and S. Clark. Open domain [23] M. Utiyama and H. Isahara. A statistical model for [24] K. Wang, C. Thrasher, E. Viegas, X. Li, and P. Hsu. An [25] X. Wang, F. Wei, X. Liu, M. Zhou, and M. Zhang. Topic [26] G. Zhou and J. Su. Named entity recognition using an
