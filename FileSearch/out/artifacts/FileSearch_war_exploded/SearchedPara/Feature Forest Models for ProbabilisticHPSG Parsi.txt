 University of Tokyo University of Tokyo University of Manchester plicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into sub-ture structures.
 provides a method for probabilistic modeling without the independence assumption when prob-any data structures is possible when they are represented by feature forests. empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. 1. Introduction
Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and Tsujii 2005), statistical modeling of these grammars is attracting considerable attention.
This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation.

HPSG parsing. Although previous studies have proposed maximum entropy mod-els (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003;
Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable.
 the problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002).
Our algorithm avoids exponential explosion by representing probabilistic events with feature forests, which are packed representations of tree structures. When complete structures are represented with feature forests of a tractable size, the parameters of maximum entropy models are efficiently estimated without unpacking the feature forests. This is due to dynamic programming similar to the algorithm for computing inside/outside probabilities in PCFG parsing.
 models to disambiguation in wide-coverage HPSG parsing. We describe methods for representing HPSG parse trees and predicate X  X rgument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing.
Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The model and the parameter estimation algorithm, which are substantially refined and extended from Miyao and Tsujii (2002). Another contribution is that this article thor-oughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text.
 grammars. Section 3 proposes feature forest models for solving this problem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Sec-tion 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem
Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now be-coming the de facto standard approach for disambiguation models for lexicalized or 36 feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and
Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002;
Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation.
 likelihood of training data under given feature functions. Given training data E = { x , y } , a maximum entropy model gives conditional probability p ( y Definition 1 (Maximum entrop ymodel)
A maximum entropy model is defined as the solution of the following optimization problem. where: feature function , which represents a characteristic of probabilistic events by mapping an event into a real value.  X  i is the model parameter of a corresponding feature function f , and is determined so as to maximize the likelihood of the training data (i.e., the a given sentence and Y ( x ) is a parse forest for x . An advantage of maximum entropy models is that feature functions can represent any characteristics of events. That is, independence assumptions are unnecessary for the design of feature functions. Hence, this method provides a principled solution for the estimation of consistent probabilistic distributions over feature structure grammars.
 such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved
Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limited-memory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright 1999), have been proposed for parameter estimation. Although the algorithm proposed in the present article is applicable to all of the above algorithms, we used L-BFGS for experiments.
 rithms. The size of Y ( x ) (i.e., the number of parse trees for a sentence) is generally very large. This is because local ambiguities in parse trees potentially cause exponential growth in the number of structures assigned to sub-sequences of words, resulting in billions of structures for whole sentences. For example, when we apply rewriting rule  X  NPVP , and the left NP and the right VP , respectively, have n and m ambiguous subtrees, the result of the rule application generates n  X  to the size of Y ( x ). The cost of the parameter estimation algorithms is bound by the computation of model expectation ,  X  i , given as (Malouf 2002):
As shown in this definition, the computation of model expectation requires the summa-algorithm is O (  X  | Y |  X  | F || E | ), where  X  | Y | features for an event, respectively, and | E | is the number of events. When Y ( x )grows exponentially, the parameter estimation becomes intractable.
 a dynamic programming algorithm for computing inside/outside probabilities (Baker 1979). With the algorithm, the computation becomes tractable. We can expect that the same approach would be effective for maximum entropy models as well.
 tropy models, as described in the next section. 3. Feature Forest Model
Our solution to the problem is a dynamic programming algorithm for computing inside/outside  X   X   X  -products . Inside/outside  X  -products roughly correspond to inside/ outside probabilities in PCFGs. In maximum entropy models, a probability is defined inside/outside probabilities, we can compute exp j  X  j f j cost, the model expectation  X  i is also computed at a tractable cost.
 of an exponential number of tree structures. Feature forests correspond to packed charts in CFG parsing. Because feature forests are generalized representations of forest structures, the notion is not only applicable to syntactic parsing but also to sequence tagging, such as POS tagging and named entity recognition (which will be discussed in
Section 6). We then define inside/outside  X  -products that represent the  X  -products of in PCFG, and represent the summation of  X  -products of the daughter sub-trees. Outside  X  -products correspond to outside probabilities in PCFG, and represent the summation of  X  -products in the upper part of the feature forest. Both can be computed incre-mentally by a dynamic programming algorithm similar to the algorithm for computing 38 inside/outside probabilities in PCFG. Given inside/outside  X  -products of all nodes in a feature forest, the model expectation  X  i is easily computed by multiplying them for each node. 3.1 Feature Forest
To describe the algorithm, we first define the notion of a feature forest, the generalized representation of features in a packed forest structure. Feature forests are used for Definition 2 (Feature forest) A feature forest  X  is a tuple C , D , r ,  X  ,  X  , where: forest is an acyclic graph, and unpacked structures extracted from a feature forest are trees. We also assume that terminal nodes of feature forests are conjunctive nodes. That is, disjunctive nodes must have daughters (i.e.,  X  ( d ) =
Conjunctive nodes correspond to entities such as states in Markov chains and nodes in CFG trees. Feature functions are assigned to conjunctive nodes and express their characteristics. Disjunctive nodes are for enumerating alternative choices. Conjunctive/ disjunctive daughter functions represent immediate relations of conjunctive and dis-junctive nodes. By selecting a conjunctive node as a child of each disjunctive node, we can extract a tree consisting of conjunctive nodes from a feature forest. alternative nodes, which are conjunctive nodes. Each conjunctive node has disjunctive nodes as its daughters. The feature forest in Figure 1 represents a set of 2 unpacked trees shown in Figure 2. For example, by selecting the left-most conjunctive node at each disjunctive node, we extract an unpacked tree ( c tree is represented as a set of conjunctive nodes. Generally, a feature forest represents an exponential number of trees with a polynomial number of nodes. Thus, complete structures, such as tag sequences and parse trees with ambiguities, can be represented in a tractable form.
 Definition 3 (Feature function for feature forests)
A feature function for a feature forest is:
Hence, together with feature functions, a feature forest represents a set of trees of features.
 forests have the same structure as PCFG parse forests, nodes in feature forests do not necessarily correspond to nodes in PCFG parse forests. In fact, in Sections 4.2 and 4.3, we will demonstrate that syntactic structures and predicate X  X rgument structures in HPSG can be represented with tractable-size feature forests. The actual interpretation of a node in a feature forest may thus be ignored in the following discussion. Our algorithm is applicable whenever feature forests are of a tractable size. The descriptive power of feature forests will be discussed again in Section 6.
 define model expectations,  X  i , on a set of unpacked trees, and then show that they can be computed without unpacking feature forests. We denote an unpacked tree as a set, c  X  C , of conjunctive nodes. Our concern is only the set of features associated with each conjunctive node, and the shape of the tree structure is irrelevant to the computation of probabilities of unpacked trees. Hence, we do not distinguish an unpacked tree from a set of conjunctive nodes.
 set of unpacked trees because we allow multiple occurrences of equivalent unpacked 40 trees in a feature forest. 2 Given multisets of unpacked trees, A , B , we define the union and the product as follows.
 tions of trees in A and B . It is trivial that they satisfy commutative, associative, and distributive laws.
 fined recursively. For a terminal node c  X  C , obviously  X  ( c ) = conjunctive node c  X  C , an unpacked tree is a combination of trees, each of which is selected from a disjunctive daughter. Hence, a set of all unpacked trees is represented as a product of trees from disjunctive daughters.

A disjunctive node d  X  D represents alternatives of packed trees, and obviously a set of its unpacked trees is represented as a union of the daughter trees, that is,  X  ( d ) = Definition 4 (Unpacked tree)
Given a feature forest  X = C , D , r ,  X  ,  X  ,aset  X  ( n ) of unpacked trees rooted at node n  X  C  X  D is defined recursively as follows.

Feature forests are directed acyclic graphs and, as such, this definition does not include a loop. Hence,  X  ( n ) is properly defined.
 forest in Figure 1. Following Definition 4, the first element of each set is the root node, c , and the rest are elements of the product of { c 2 , c 3 Figure 3 corresponds to a tree in Figure 2.
 follows.
 Definition 5 (Feature function for unpacked tree)
The feature function f i for an unpacked tree, c  X   X  (  X  ) is defined as:
Because c  X   X  (  X  ) corresponds to y of the conventional maximum entropy model, this an unpacked tree is given, a model expectation is defined as in the traditional model. Definition 6 (Model expectation of feature forests)
The model expectation  X  i for a set of feature forests {  X  ( x )
It is evident that the naive computation of model expectations requires exponential time complexity because the number of unpacked trees (i.e., related to the number of nodes in the feature forest  X  . We therefore need an algorithm for computing model expectations without unpacking a feature forest.
 42 3.2 Dynamic Programming
To efficiently compute model expectations, we incorporate an approach similar to the dynamic programming algorithm for computing inside/outside probabilities in PCFGs.
We first define the notion of inside/outside of a feature forest. Figure 4 illustrates this concept, which is similar to the analogous concept in PCFGs. partial trees (sets of conjunctive nodes) derived from node c of inside trees.
 Definition 7 (Inside trees) We define a set  X  ( n ) of inside trees rooted at node n  X  rooted at n . Definition 8 (Outside trees)
We define a set o ( n ) of outside trees rooted at node n
In the definition,  X   X  1 and  X   X  1 denote mothers of conjunctive and disjunctive nodes, respectively. Formally,
Theinside(oroutside)  X  -products are the summation of exp outside) trees c .
 Definition 9 (Inside/outside  X   X   X  -product)
An inside  X  -product at conjunctive node c  X  C is
An outside  X  -product is
Similarly, inside/outside  X  -products at disjunctive node d product of the inside and outside  X  -products.
 Theorem 1 (Model expectation of feature forests)
The model expectation  X  i of a feature forest  X  ( x ) = C product of inside and outside  X  -products as follows: 44 traversing conjunctive nodes without unpacking the forest, if the inside/outside  X  -products are given. The remaining issue is how to efficiently compute inside/outside  X  -products.
 programming without unpacking feature forests. Figure 5 shows the process of com-puting the inside  X  -product at a conjunctive node from the inside  X  -products of its daughter nodes. Because the inside of a conjunctive node is a set of the combinations of all of its descendants, the  X  -product is computed by multiplying the  X  -products of the daughter trees. The following equation is derived.
Hence, the inside  X  -product at disjunctive node d  X  D is computed as follows (Figure 6). Theorem 2 (Inside  X   X   X  -product)
The inside  X  -product  X  c at a conjunctive node c is computed by the following equation if  X  d is given for all daughter disjunctive nodes d  X   X  ( c ).
The inside  X  -product  X  d at a disjunctive node d is computed by the following equation if  X  c is given for all daughter conjunctive nodes c  X   X  ( d ).
Hence, the outside  X  -product of a disjunctive node is propagated to its daughter con-junctive nodes (Figure 7).
The computation of the outside  X  -product of a disjunctive node is somewhat com-plicated. As shown in Figure 8, the outside trees of a disjunctive node are all com-binations of 46
From this, we find: Theorem 3 (Outside  X   X   X  -product)
The outside  X  -product  X  c at conjunctive node c is computed by the following equation if  X  d is given for all mother disjunctive nodes, that is, all d such that c
The outside  X  -product  X  d at disjunctive node d is computed by the following equation if  X  c is given for all mother conjunctive nodes, that is, all c such that d for all sibling disjunctive nodes d . of feature forests. The key point of the algorithm is to compute inside  X  -products  X  and outside  X  -products  X  for each node in C , and not for all unpacked trees. The func-tions inside product and outside product compute  X  and  X  efficiently by dynamic programming.
 putation, although it is not shown in Figure 9. The computation for the daughter nodes and mother nodes must be completed before computing the inside and outside  X  -products, respectively. This constraint is easily solved using any topological sort algorithm. A topological sort is applied once at the beginning. The result of the sorting does not affect the cost and the result of estimation. In our implementation, we assume that conjunctive/disjunctive nodes are already ordered from the root node in input data. average numbers of conjunctive and disjunctive nodes, respectively. This is tractable 48 nodes in a feature forest is usually polynomial even when that of the unpacked trees is exponential. Thus we can efficiently compute model expectations with polynomial computational complexity. 4. Probabilistic HPSG Parsing
Following previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van
Noord 2004), we apply a maximum entropy model to HPSG parse disambiguation. The probability, p ( t | w ), of producing parse result t of a given sentence w is defined as where and T ( w ) is a set of parse candidates assigned to w . The feature function f resents the characteristics of t and w , and the corresponding model parameter  X  weight. Model parameters that maximize the log-likelihood of the training data are computed using a numerical optimization method (Malouf 2002).
 parse for a sentence w . Whereas t w is provided by a treebank, T ( w ) has to be computed by parsing each w in the treebank. Previous studies assumed T ( w ) could be enumer-ated; however, this assumption is impractical because the size of T ( w ) is exponentially related to the length of w .
 modeling of HPSG parsing. Section 4.1 briefly introduces HPSG. Section 4.2 and 4.3 describe how to represent HPSG parse trees and predicate X  X rgument structures by feature forests. Together with the parameter estimation algorithm in Section 3, these methods constitute a complete method for probabilistic disambiguation. We also ad-dress a method for accelerating the construction of feature forests for all treebank sentences in Section 4.4. The design of feature functions will be given in Section 4.5. 4.1 HPSG
HPSG (Pollard and Sag 1994; Sag, Wasow, and Bender 2003) is a syntactic theory that fol-lows the lexicalist framework. In HPSG, linguistic entities, such as words and phrases, are denoted by signs , which are represented by typed feature structures (Carpenter 1992). Signs are a formal representation of combinations of phonological forms and syntactic/semantic structures, and express which phonological form signifies which syntactic/semantic structure. Figure 10 shows the lexical sign for loves . The geometry of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word,
MOD denotes modifiee constraints, and SPR , SUBJ ,and COMPS describe constraints of a specifier, a syntactic subject, and complements, respectively. CONT denotes the predicate X  X rgument structure of a phrase/sentence. The notation of CONT in this article is borrowed from that of Minimal Recursion Semantics (Copestake et al. 2006): HOOK represents a structure accessed by other phrases, and RELS describes the remaining structure of the semantics. In what follows, we represent signs in a reduced form as shown in Figure 11, because of the large size of typical HPSG signs, which often include information not immediately relevant to the point being discussed. We will only show of suppressed attributes. 50 additional attributes that are not defined in the standard HPSG theory but are used by a disambiguation model. Examples include the surface form of lexical heads, and the type of lexical entry assigned to lexical heads, which are respectively used for computing the features WORD and LE introduced in Section 4.5. By incorporating ad-ditional attributes into signs, we can straightforwardly compute feature functions for each sign. This allows for a simple mapping between a parsing chart and a feature forest as described subsequently. However, this might increase the size of parse forests and therefore decrease parsing efficiency, because differences between additional attributes interfere with equivalence relations for ambiguity packing. 4.2 Packed Representation of HPSG Parse Trees
We represent an HPSG parse tree with a set of tuples m , l , r , where m , l ,and r are the signs of the mother, left daughter, and right daughter, respectively. partial parse candidates are stored in a chart , in which phrasal signs are identified and packed into equivalence classes if they are judged to be equivalent and dominate the same word sequences. A set of parse trees is then represented as a set of relations among equivalence classes. 5 and the arrows represent immediate-dominance relations. The phrase, saw a girl with a telescope , has two trees ( A in the figure). Because the signs of the top-most nodes are equivalent, they are packed into an equivalence class. The ambiguity is represented as the two pairs of arrows leaving the node A .
 set of equivalence classes, E r  X  E is a set of root nodes, and  X  : E to represent immediate-dominance relations.

Figure 13 shows (a part of) the HPSG parse trees in Figure 12 represented as a feature forest. Square boxes ( c i ) are conjunctive nodes, and d represents a disjunctive daughter function, and a dotted line expresses a conjunctive daughter function.
 52 mean the impossibility of incorporating features on nonlocal dependencies into the model. This is because a feature forest model does not assume probabilistic indepen-dence of conjunctive nodes. This means that we can unpack a part of the forest without changing the model. Actually, we successfully developed a probabilistic model includ-ing features on nonlocal predicate X  X rgument dependencies, as described subsequently. 4.3 Packed Representation of Predicate X  X rgument Structures
With the method previously described, we can represent an HPSG parsing chart with a feature forest. However, equivalence classes in a chart might increase exponentially because predicate X  X rgument structures in HPSG signs represent the semantic relations of all words that the phrase dominates. For example, Figure 14 shows phrasal signs with predicate X  X rgument structures for saw a girl with a telescope . In the chart in Figure 12, these signs are packed into an equivalence class. However, Figure 14 shows that the values of CONT , that is, predicate X  X rgument structures, have different values, and the signs as they are cannot be equivalent. As seen in this example, predicate X  X rgument structures prevent us from packing signs into equivalence classes.
 which may include reentrant structures and non-local dependencies. It is theoretically difficult to apply the feature forest model to predicate X  X rgument structures; a feature forest cannot represent graph structures that include reentrant structures in a straight-forward manner. However, if predicate X  X rgument structures are constructed as in the manner described subsequently, they can be represented by feature forests of a tracta-ble size.
 locality and monotonicity in the composition of predicate X  X rgument structures.
Locality: In each step of composition of a predicate X  X rgument structure, only a
Monotonicity: All relations in the daughters X  predicate X  X rgument structures include non-local dependencies. For example, Figure 15 shows HPSG lexical entries for the wh -extraction of the object of love (left) and for the control construction of try (right). The first condition is satisfied because both lexical entries refer to CONT of argument signs in SUBJ , COMPS ,and SLASH . None of the lexical entries directly access ARG X of the arguments. The second condition is also satisfied because the values of CONT | HOOK of all of the argument signs are percolated to ARG X of the mother. In addition, the elements in CONT | RELS are percolated to the mother by the Semantic Prin-ciple. Compositional semantics usually satisfies the above conditions, including MRS (Copestake et al. 1995, 2006). The composition of MRS refers to HOOK , and no internal structures of daughters. The Semantic Principle of MRS also assures that all semantic relations in RELS are percolated to the mother. When these conditions are satisfied, semantics may include any constraints, such as selectional restrictions, although the grammar we used in the experiments does not include semantic restrictions to constrain parse forests.
 coded into a conjunctive node when the values of all of its arguments have been instantiated. We introduce the notion of inactives to denote such local structures. Definition 10 (Inactives)
An inactive is a subset of predicate X  X rgument structures in which all arguments have been instantiated.

Because inactive parts will not change during the rest of the parsing process, they can be placed in a conjunctive node. By placing newly generated inactives into correspond-ing conjunctive nodes, a set of predicate X  X rgument structures can be represented in a feature forest by packing local ambiguities, and non-local dependencies are preserved. 54 and fact may optionally take a complementizer phrase. 7 The predicate X  X rgument struc-tures for dispute1 and dispute2 are shown in Figure 17. Curly braces express the am-biguities of partially constructed predicate X  X rgument structures. The resulting feature forest is shown in Figure 18. The boxes denote conjunctive nodes and d disjunctive nodes.
 corresponding to dispute1 (  X  in Figure 16) and the other corresponding to dispute2 (  X  in Figure 16). The nodes of the predicate X  X rgument structure  X  are all instantiated, that is, it contains only inactives. The corresponding conjunctive node (  X  in Figure 18) has two inactives, for want and dispute1 . The other structure  X  has an unfilled object in the argument ( ARG2 8 )of dispute2 , which will be filled by the non-local dependency. Hence, the corresponding conjunctive node  X  has only one inactive corresponding to want , and the remaining part that corresponds to dispute2 is passed on for further processing. by fact (  X  in Figure 16), and the predicate X  X rgument structure of dispute2 is then placed into a conjunctive node (  X  in Figure 18). resentation is isomorphic to the parsing process, that is, a chart. Hence, we can assign features of HPSG parse trees to a conjunctive node, together with features of predicate X  argument structures. In Section 5, we will investigate the contribution of features on parse trees and predicate X  X rgument structures to the disambiguation of HPSG parsing. 4.4 Filtering b yPreliminar yDistribution
The method just described is the essence of our solution for the tractable estimation of maximum entropy models on exponentially many HPSG parse trees. However, the problem of computational cost remains. Construction of feature forests requires parsing of all of the sentences in a treebank. Despite the development of methods to improve HPSG parsing efficiency (Oepen, Flickinger, et al. 2002), exhaustive parsing of all sentences is still expensive.
 in the estimation stage because T ( w ) can be approximated by parse trees with high probabilities. To achieve this, we first prepared a preliminary probabilistic model whose estimation did not require the parsing of a treebank. The preliminary model was used to reduce the search space for parsing a training treebank.
 where w  X  w is a word in the sentence w ,and l is a lexical entry assigned to w .This model is estimated by counting the relative frequencies of lexical entries used for w in the training data. Hence, the estimation does not require parsing of a treebank. Actually, we use a maximum entropy model to compute this probability as described in Section 5. 56
Given this model, we restrict the number of lexical entries used to parse a treebank. With a threshold n for the number of lexical entries and a threshold for the probability, lexical entries are assigned to a word in descending order of probability, until the number of assigned entries exceeds n , or the accumulated probability exceeds .Ifthis procedure does not assign a lexical entry necessary to produce a correct parse (i.e., an lexical entries are given by the HPSG treebank. This assures that the filtering method does not exclude correct parse trees from parse forests.
 such as a verbal entry taking a sentential complement (p = 0.01 in the figure), they are filtered out. Although this method reduces the time required for parsing a treebank, this approximation causes bias in the training data and results in lower accuracy. The trade-off between parsing cost and accuracy will be examined experimentally in Section 5.4. experiments, we will empirically compare the following methods in terms of accuracy and estimation time.
 Filtering only: The unigram probability  X  p is used only for filtering in training. Product: The probability is defined as the product of  X  p and the estimated model p . Reference distribution:  X  p is used as a reference distribution of p .

Feature function: log  X  p is used as a feature function of p . This method has been 4.5 Features
Feature functions in maximum entropy models are designed to capture the characteris-in Table 1. The following combinations are used for representing the characteristics of binary/unary schema applications. where subscripts l and r denote left and right daughters.
 the parse tree. follows: where subscripts p and a represent predicate and argument, respectively.
 is S and the surface form, part-of-speech, and lexical entry of the lexical head are saw , girl and with a telescope , in which the applied schema is the Head-Modifier Schema, the left daughter is VP headed by saw , and the right daughter is PP headed by with , whose part-of-speech is IN and whose lexical entry is a VP-modifying preposition. shows features assigned to the conjunctive node denoted as  X  in Figure 18. Because inactive structures in the node have three predicate X  X rgument relations, three features is ARG1 , the distance between the head words is 1, the surface string and the POS of 58 the predicate are want and VBD , and those of the argument are I and PRP . The second and the third features are for the other two relations. We may include features on more than two relations, such as the dependencies among want , I ,and dispute , although such features are not incorporated currently.
 smoothing. Tables 2, 3, and 4 show the full set of templates of combined features used in the experiments. Each row represents the template for a feature function. A check indi-cates the atomic feature is incorporated, and a hyphen indicates the feature is ignored.  X  X  X   X   X  X  X  X  X  X  X   X  X  X  X   X  X  X   X   X   X   X  X  X  X  X  X  60 5. Experiments
This section presents experimental results on the parsing accuracy attained by the feature forest models. In all of the following experiments, we use the HPSG grammar developed by the method of Miyao, Ninomiya, and Tsujii (2005). Section 5.1 describes how this grammar was developed. Section 5.2 explains other aspects of the experimental settings. In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing. 5.1 The HPSG Grammar
In the following experiments, we use Enju 2.1 (Tsujii Laboratory 2004), which is a wide-coverage HPSG grammar extracted from the Penn Treebank by the method of Miyao, Ninomiya, and Tsujii (2005). In this method, we convert the Penn Treebank into an
HPSG treebank, and collect HPSG lexical entries from terminal nodes of the HPSG treebank. Figure 22 illustrates the process of treebank conversion and lexicon collection.
We first convert and fertilize parse trees of the Penn Treebank. This step identifies syntactic constructions that require special treatment in HPSG, such as raising/control and long-distance dependencies. These constructions are then annotated with typed feature structures so that they conform to the HPSG analysis. Next, we apply HPSG schemas and principles, and obtain fully specified HPSG parse trees. This step solves Failures of schema/principle applications indicate that the annotated constraints do not conform to the HPSG analysis, and require revisions. Finally, we obtain lexical entries from the HPSG parse trees. The terminal nodes of HPSG parse trees are collected, and they are generalized by removing word-specific or context-specific constraints. because lexical entries are extracted from real-world sentences. Obtained lexical entries are guaranteed to construct well-formed HPSG parse trees because HPSG schemas and principles are successfully applied during the development of the HPSG treebank.
Another notable feature is that we can additionally obtain an HPSG treebank, which can be used as training data for disambiguation models. In the following experiments, this HPSG treebank is used for the training of maximum entropy models.
 of the Wall Street Journal portion of the Penn Treebank. This lexicon can assign correct lexical entries to 99.09% of words in the HPSG treebank converted from Penn Treebank Section 23. This number expresses  X  X exical coverage X  in the strong sense defined by
Hockenmaier and Steedman (2002). In this notion of  X  X overage, X  this lexicon has 84.1% sentential coverage, where this means that the lexicon can assign correct lexical entries to all of the words in a sentence. Although the parser might produce parse results for uncovered sentences, these parse results cannot be completely correct. 5.2 Experimental Settings
The data for the training of the disambiguation models was the HPSG treebank derived from Sections 02 X 21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses. The resulting training set consists of 33,604 sentences (when n = 10 and = 0 . 95; see Section 5.4 for details). The treebanks derived from Sections 22 and 23 were used as the development and final test sets, respectively. Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words. Table 5 shows the specifications of the test data.
 argument dependencies output by the parser. A predicate X  X rgument dependency is defined as a tuple w h , w n ,  X  ,  X  , where w h is the head word of the predicate, w head word of the argument,  X  is the type of the predicate (e.g., adjective, intransitive verb), and  X  is an argument label ( MODARG,ARG1, ... ,ARG4 ). For example, He tried running has three dependencies as follows: 62
Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser, and unlabeled precision/recall (UP/UR) is the ratio of w h regardless of  X  and  X  . F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate X  X rgument relations in a sentence.
These measures correspond to those used in other studies measuring the accuracy of predicate X  X rgument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact figures cannot be compared directly because the definitions of depen-dencies are different. All predicate X  X rgument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted.
 hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and
Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used. The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created during parsing exceeded 50,000. In such a case, the parser output nothing, and the recall was computed as zero.
 method of filtering lexical entries was applied to the parsing of training data (Sec-tion 4.4). Unless otherwise noted, parameters for filtering were n = 10 and = 0 . 95, and a reference distribution method was applied. The unigram model, p
The model includes 24,847 features. 5.3 Efficac yof Feature Forest Models
Tables 6 and 7 show parsing accuracy for the test set. In the tables,  X  X yntactic features X  denotes a model with syntactic features, that is, f binary in Section 4.5.  X  X emantic features X  represents a model with features on predicate X  argument structures, that is, f pa given in Table 4.  X  X ll X  is a model with both syntactic and semantic features. The  X  X aseline X  row shows the results for the reference model, p ( t | s ), used for lexical entry filtering in the estimation of the other models. This model for any rule r in the construction rules of the HPSG grammar.
 curacy than a baseline model. Comparing  X  X yntactic features X  with  X  X emantic fea-tures, X  we see that the former model attained significantly higher accuracy than the latter. This indicates that syntactic features are more important for overall accuracy. We will examine the contributions of each atomic feature of the syntactic features in Section 5.5.
 for the accurate disambiguation of syntactic structures. For example, PP-attachment ambiguity cannot be resolved with only syntactic preferences. However, the results show that a model with only semantic features performs significantly worse than one with syntactic features. Even when combined with syntactic features, semantic features do not improve accuracy. Obviously, semantic preferences are necessary for accurate parsing, but the features used in this work were not sufficient to capture semantic pref-erences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies may be too sparse to capture semantic preferences.
 ported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate X  X rgument dependencies differently. In contrast with the results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly lower than precision. This may have resulted from the HPSG grammar having stricter feature constraints and the parser not being able to produce parse results for around 1% of the sentences. To improve recall, we need techniques to deal with these 1% of sentences.
 indicates user times required for running the parameter estimation algorithm.  X  X o. of feature occurrences X  denotes the total number of occurrences of features in the training data, and  X  X ata size X  gives the sizes of the compressed files of training data. We can conclude that feature forest models are estimated at a tractable computational cost and a reasonable data size, even when a model includes semantic features including non-local dependencies. The results reveal that feature forest models essentially solve the problem of the estimation of probabilistic models of sentence structures. 64 5.4 Comparison of Filtering Methods
Table 9 compares the estimation methods introduced in Section 4.4. In all of the follow-that our method achieves significantly lower accuracy when it is used only for filtering in the training phrase. One reason is that the feature forest model prefers lexical entries that are filtered out in the training phase, because they are always oracle lexical entries in the training. This means that we must incorporate the preference of filtering into the final parse selection. As shown in Table 9, the models combined with a preliminary model achieved sufficient accuracy. The reference distribution method achieved higher accuracy and lower cost. The feature function method achieved lower accuracy in our experiments. A possible reason for this is that a hyper-parameter of the prior was set to the same value for all the features including the feature of the log-probability given by the preliminary distribution.
 determine the correlation between the estimation/parsing cost and accuracy. In our experiment, n  X  10 and  X  0 . 90 seem necessary to preserve the F-score over 86 . 0. 5.5 Contribution of Features
Table 12 shows the accuracy with different feature sets. Accuracy was measured for 15 models with some atomic features removed from the final model. The last row denotes the accuracy attained by the unigram model (i.e., the reference distribution). The num-bers in bold type represent a significant difference from the final model according to for 32 pairwise comparisons. The results indicate that DIST , COMMA , SPAN , WORD ,and contrast, RULE , SYM ,and LE features did not affect accuracy. However, when each was removed together with another feature, the accuracy decreased drastically. This implies that such features carry overlapping information. 5.6 Factors for Parsing Accuracy Table 13 shows parsing accuracy for covered and uncovered sentences. As defined in sentence. In other words, for covered sentences, exactly correct parse trees are obtained if the disambiguation model worked perfectly. The result reveals clear differences in accuracy between covered and uncovered sentences. The F-score for covered sentences is around 2.5 points higher than the overall F-score, whereas the F-score is more than 10 points lower for uncovered sentences. This result indicates improvement of lexicon quality is an important factor for higher accuracy. 66 the Gaussian prior was optimized for each model. High accuracy is attained even with a small training set, and the accuracy seems to be saturated. This indicates that we cannot further improve the accuracy simply by increasing the size of the training data set. The exploration of new types of features is necessary for higher accuracy. It should also be noted that the upper bound of the accuracy is not 100%, because the grammar cannot produce completely correct parse results for uncovered sentences.
 figure that the accuracy is significantly higher for sentences with less than 10 words.
This implies that experiments with only short sentences overestimate the performance of parsers. Sentences with at least 10 words are necessary to properly evaluate the performance of parsing real-world texts. The accuracies for the sentences with more than 10 words are not very different, although data points for sentences with more than 50 words are not reliable.
 of-speech tags are assigned automatically by a maximum-entropy-based parts-of-speech tagger (Tsuruoka and Tsujii 2005). The results indicate a drop of about three points in labeled precision/recall (a two-point drop in unlabeled precision/recall).
A reason why we observed larger accuracy drops in labeled precision/recall is that predicate X  X rgument relations are fragile with respect to parts-of-speech errors because predicate types (e.g., adjective, intransitive verb) are determined depending on the parts-of-speech of predicate words. Although our current parsing strategy assumes that parts-of-speech are given beforehand, for higher accuracy in real application contexts, we will need a method for determining parts-of-speech and parse trees jointly. 68 5.7 Analysis of Disambiguation Errors
Table 15 shows a manual classification of the causes of disambiguation errors in 100 sen-tences randomly chosen from Section 00. In our evaluation, one error source may cause multiple dependency errors. For example, if an incorrect lexical entry is assigned to a verb, all of the argument dependencies of the verb are counted as errors. The numbers in the table include such double-counting. Figure 25 shows examples of disambiguation errors. The figure shows output from the parser.
 modifier distinction, and lexical ambiguity. As attachment ambiguities are well-known error sources, PP-attachment is the largest source of errors in our evaluation. Our disambiguation model cannot accurately resolve PP-attachment ambiguities because it does not include dependencies among a modifiee and the argument of the preposition. Because previous studies revealed that such dependencies are effective features for
PP-attachment resolution, we should incorporate them into our model. Some of the attachment ambiguities, including adjective and adverb, should also be resolved with an extension of features. However, we cannot identify any effective features for the disambiguation of attachment of verbal phrases, including relative clauses, verb phrases, subordinate clauses, and to -infinitives. For example, Figure 25 shows an example error of the attachment of a relative clause. The correct answer is that the because arguments and modifiers are not explicitly distinguished in the evaluation of
CFG parsers. Figure 25 shows an example of the argument/modifier distinction of a subcategorization frame of tempts seems responsible for this problem. However, the disambiguation model wrongly assigned a lexical entry for a transitive verb because of the sparseness of the training data ( tempts occurred only once in the training data).
The resolution of this sort of ambiguity requires the refinement of a probabilistic model of lexical entries. Errors of verb phrases and subordinate clauses are similar to this example. Errors of argument/modifier distinction of noun phrases are mainly caused by temporal nouns and cardinal numbers. The resolution of these errors seems to require the identification of temporal expressions and usage of cardinal numbers. ure 25, compared with is a compound preposition, but the parser recognized it as a verb phrase. This indicates that the grammar or the disambiguation model requires the special treatment of idioms. Errors of verb subcategorization frames were mainly caused by difficult constructions such as insertions. Figure 25 shows that the parser could not identify the inserted clause ( says John Siegel ... ) and a lexical entry for a declarative transitive verb was chosen.
 were ignored in the evaluation of CFG parsers. We did not eliminate punctuation from the evaluation because punctuation sometimes contributes to semantics, as in coordination and insertion. In this error analysis, errors of commas representing coordination/insertion are classified into  X  X oordination/insertion, X  and  X  X omma X  in-dicates errors that do not contribute to the computation of semantics.
 phrases. These errors were mainly caused by the indirect effects of other errors. 70 catastrophic analyses. While accurate analysis of such constructions is indispensable, it is also known to be difficult because disambiguation of coordination/insertion requires the computation of preferences over global structures, such as the similarity of syntactic/semantic structure of coordinates. Incorporating features for representing the similarity of global structures is difficult for feature forest models. most were indirectly caused by errors of argument/modifier distinction in to -infinitive clauses.
 tures we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures
The model described in this article was first published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Appli-cations to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to repre-sentations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi,
Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic enough to be applied to natural language processing tasks other than parsing. gramming algorithm for maximum entropy models. The solution was similar to our approach, although their method was designed to traverse LFG parse results repre-sented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995).
The difference between the two approaches is that feature forests use a simpler generic data structure to represent packed forest structures. Therefore, without assuming what feature forests represent, our algorithm can be applied to various tasks, including theirs.
 of approximation. The work on whole sentence maximum entropy models (Rosenfeld 1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate parameters of maximum entropy models on whole sentence structures. However, the algorithm suffered from slow convergence, and the model was basically a sequence model. It could not produce a solution for complex structures as our model can. and Pereira 2001) for solving a similar problem in the context of maximum entropy
Markov models. Their solution was an algorithm similar to the computation of forward/backward probabilities of hidden Markov models (HMMs). Their algorithm is a special case of our algorithm in which each conjunctive node has only one daughter. This is obvious because feature forests can represent Markov chains. In an analogy,
CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs.
Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and
Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton,
Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obtain approximate so-lutions. A similar method may be developed to overcome a bottleneck of feature forest models, that is, the fact that feature functions are localized to conjunctive nodes. computational linguistics. As is easily seen, lattices, Markov chains, and CFG parse trees are represented by feature forests. Furthermore, because conjunctive nodes do not necessarily represent CFG nodes or rules and terminals of feature forests need not be words, feature forests can express any forest structures in which ambiguities are packed in local structures. Examples include the derivation trees of LTAG and
CCG. Chiang (2003) proved that feature forests could be considered as the derivation forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). LCFRSs define a wide variety of grammars, including LTAG and
CCG, while preserving polynomial-time complexity of parsing. This demonstrates that feature forest models are applicable to probabilistic models far beyond PCFGs. Feature
EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic pro-gramming language, PRISM (Sato and Kameya 1997), is converted into support graphs, and parameters of probabilistic models are automatically learned by an EM algorithm.
Support graphs have been proved to represent various statistical structural models, in-cluding HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato and Kameya 2001; Sato 2005). Taken together, these results imply the high applicability of feature forest models to various real tasks.
 might seem that they can represent only immediate dominance relations of CFG rules as in PCFG, resulting in only a slight, trivial extension of PCFG. As described herein, however, feature forests can represent structures beyond CFG parse trees. Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a node in a PCFG parse forest. That is, a node in a feature forest may represent any linguistic entity, including a fragment of a syntactic structure, a semantic relation, or other sentence-level information. learning methods. Taskar et al. (2004) proposed a dynamic programming algorithm for the learning of large-margin classifiers including support vector machines (Vapnik 1995), and presented its application to disambiguation in CFG parsing. Their algorithm resembles feature forest models; an optimization function is computed by a dynamic programing algorithm without unpacking packed forest structures. From the discussion in this article, it is evident that if the main part of an update formula is represented 72 with (the exponential of) linear combinations, a method similar to feature forest models should be applicable. 6.2 Probabilistic Parsing with Lexicalized Grammars
Before the advent of feature forest models, studies on probabilistic models of HPSG adopted conventional maximum entropy models to select the most probable parse from parse candidates given by HPSG grammars (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003). The difference between these studies and our work is that we used feature forests to avoid the exponential increase in the number of structures that results from unpacked parse results. These studies ignored the problem of exponential explosion; in fact, training sets in these studies were very small and consisted only of short sentences. A possible approach to avoid this problem is to develop a fully restrictive grammar that never causes an exponential explosion, al-though the development of such a grammar requires considerable effort and it cannot be acquired from treebanks using existing approaches. We think that exponential explosion is inevitable, particularly with the large-scale wide-coverage grammars required to an-alyze real-world texts. In such cases, these methods of model estimation are intractable. informative sample from the original set T ( w ) (Osborne 2000). The method was suc-cessfully applied to Dutch HPSG parsing (Malouf and van Noord 2004). A possible problem with this method is in the approximation of exponentially many parse trees by a polynomial-size sample. However, their method has an advantage in that any features on parse results can be incorporated into a model, whereas our method forces feature functions to be defined locally on conjunctive nodes. We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3.
HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector machines (Toutanova, Markova, and Manning 2004). However, the problem of expo-nential explosion is also inevitable using their methods. As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004). 2002) also proposed a maximum entropy model for probabilistic modeling of LFG pars-ing. However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results. As dis-cussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses.
 tures or predicate X  X rgument dependencies, which are essentially the same as the predicate X  X rgument structures described in the present article. Clark, Hockenmaier, and
Steedman (2002) attempted the modeling of dependency structures, but the model was inconsistent because of the violation of the independence assumption. Hockenmaier (2003) proposed a consistent generative model of predicate X  X rgument structures. The probability of a non-local dependency was conditioned on multiple words to preserve the consistency of the probability model; that is, probability p ( I tion 4.3 was directly estimated. The problem was that such probabilities could not be estimated directly from the data due to data sparseness, and a heuristic method had to be employed. Probabilities were therefore estimated as the average of individual probabilities conditioned on a single word. Another problem is that the model is no longer consistent when unification constraints such as those in HPSG are introduced. Our solution is free of these problems, and is applicable to various grammars, not only HPSG and CCG.
 adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al. 2004; Riezler and Vasserman 2004). Their methods of translating parse results into feature forests are basically the same as our method described in Section 4, and details differ because different grammar theories represent syntactic structures differently. They reported higher accuracy in parsing the Penn Treebank than the previous methods introduced herein, and these results attest the effectiveness of feature forest models in practical deep parsing. A remaining problem is that no studies could provide empirical compar-isons across grammar theories. The above studies and our research evaluated parsing accuracy on their own test sets. The construction of theory-independent standard test sets requires enormous effort because we must establish theory-independent criteria such as agreed definitions of phrases and headedness. Although this issue is beyond the scope of the present article, it is a fundamental obstacle to the transparency of these studies on parsing.
 training treebank without sacrificing accuracy in the context of CCG parsing. They first assigned each word a small number of supertags , corresponding to lexical entries in our case, and parsed supertagged sentences . Because they did not use the probabilities of supertags in a parsing stage, their method corresponds to our  X  X iltering only X  method.
The difference from our approach is that they also applied the supertagger in a parsing stage. We suppose that this was crucial for high accuracy in their approach, although empirical investigation is necessary. 6.3 Trade-Off between Dynamic Programming and Feature Locality
The proposed algorithm is an essential solution to the problem of estimating probabilis-tic models on exponentially many complete structures. However, the applicability of this algorithm relies on the constraint that features are defined locally in conjunctive nodes. As discussed in Section 6.1, this does not necessarily mean that features in our model can represent only the immediate-dominance relations of CFG rules, because conjunctive nodes may encode any fragments of complete structures. In fact, we demon-strated in Section 4.3 that certain assumptions allowed us to encode non-local predicate X  argument dependencies in tractable-size feature forests. In addition, although in the experiments we used only features on bilexical dependencies, the method described in
Section 4.3 allows us to define any features on a predicate and all of its arguments, such as a ternary relation among a subject, a verb, and a complement (e.g., the relation among
I , want ,and dispute1 in Figure 21), and a generalized relation among semantic classes of a predicate and its arguments. This is because a predicate and all of its arguments are included in a conjunctive node, and feature functions can represent any relations expressed within a conjunctive node.
 places in a sentence, conjunctive nodes must be expanded so that they include all structures that are necessary to define these features. However, this obviously increases the number of conjunctive nodes, and consequently, the cost of parameter estimation increases. In an extreme case, for example, if we define features on any co-occurrences of partial parse trees, the full unpacking of parse forests would be necessary, and pa-rameter estimation would be intractable. This indicates that there is a trade-off between the locality of features and the cost of estimation. That is, larger context features might 74 contribute to higher accuracy, while they inflate the size of feature forests and increase the cost of parameter estimation.

Malouf and van Noord 2004) allow us to define any features on complete structures without any constraints. However, they force us to employ approximation methods for tractable computation. The effectiveness of those techniques therefore relies on convergence speed and approximation errors, which may vary depending on the char-acteristics of target problems and features.
 deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective features can be represented locally in tractable-size feature forests, dynamic programming methods including ours are suitable.
However, when global context features are indispensable for high accuracy, sampling methods might be better. We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005). There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task. 7. Conclusion
A dynamic programming algorithm was presented for maximum entropy modeling and shown to provide a solution to the parameter estimation of probabilistic models of complete structures without the independence assumption. We first defined the notion of a feature forest, which is a packed representation of an exponential number of trees of features. When training data is represented with feature forests, model parameters are estimated at a tractable cost without unpacking the forests. The method provides a more flexible modeling scheme than previous methods of application of maximum entropy models to natural language processing. Furthermore, it is applicable to complex data structures where an event is difficult to decompose into independent sub-events. eling of linguistic structures such as the syntactic structures of HPSG and predicate X  argument structures including non-local dependencies. The presented approach can be regarded as a general solution to the probabilistic modeling of syntactic analysis with lexicalized grammars. Table 16 summarizes the best performance of the HPSG parser described in this article. The parser demonstrated impressively high coverage and accuracy for real-world texts. We therefore conclude that the HPSG parser for
English is moving toward a practical level of use in real-world applications. Recently, the applicability of the HPSG parser to practical applications, such as information extraction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al. 2006; Chun 2007).
 of new types of features is indispensable to further improvement of parsing accuracy.
A possible research direction is to encode larger contexts of parse trees, which has been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova, and Manning 2004). Future work includes not only the investigation of these features but also the abstraction of predicate X  X rgument dependencies using semantic classes.
Experimental results also suggest that an improvement in grammar coverage is crucial for higher accuracy. This indicates that an improvement in the quality of the grammar is a key factor for the improvement of parsing accuracy.
 linguistic structure and a unit of probability. Traditionally, a unit of probability was implicitly assumed to correspond to a meaningful linguistic structure; a tagging of a word or an application of a rewriting rule. One reason for the assumption is to enable dynamic programming algorithms, such as the Viterbi algorithm. The probability of a complete structure must be decomposed into atomic structures in which ambiguities are limited to a tractable size. Another reason is to estimate plausible probabilities.
Because a probability is defined over atomic structures, they should also be meaning-ful so as to be assigned a probability. In feature forest models, however, conjunctive nodes are responsible for the former, whereas feature functions are responsible for the latter. Although feature functions must be defined locally in conjunctive nodes, they are not necessarily equivalent. Conjunctive nodes may represent any fragments of a complete structure, which are not necessarily linguistically meaningful. They should be designed to pack ambiguities and enable us to define useful features. Meanwhile, feature functions indicate an atomic unit of probability, and are designed to capture statistical regularity of the target problem. We expect the separation of a unit of prob-ability from linguistic structures to open up a new framework for flexible probabilistic modeling.
 Acknowledgments References 76 78
