 In the recent Scottish Independence Referendum (hereafter, IndyRef), Twitter offered a broad platform for people to ex-press their opinions, with millions of IndyRef tweets posted over the campaign period. In this paper, we aim to classify people X  X  voting intentions by the content of their tweets X  their short messages communicated on Twitter. By observ-ing tweets related to the IndyRef, we find that people not only discussed the vote, but raised topics related to an in-dependent Scotland including oil reserves, currency, nuclear weapons, and national debt. We show that the views com-municated on these topics can inform us of the individuals X  voting intentions ( X  X es X  X  X n favour of Independence vs.  X  X o X  X  Opposed). In particular, we argue that an accurate classifier can be designed by leveraging the differences in the features X  usage across different topics related to voting intentions. We demonstrate improvements upon a Naive Bayesian classifier using the topics enrichment method. Our new classifier iden-tifies the closest topic for each unseen tweet, based on those topics identified in the training data. Our experiments show that our Topics-Based Naive Bayesian classifier improves ac-curacy by 7.8% over the classical Naive Bayesian baseline.
Both citizens and politicians are increasingly embracing social media to disseminate information, particularly during significant political events and campaigns. Twitter emerged as an especially popular platform during the recent IndyRef held in September 2014 X  X  vote that, if successful, would have created an independent Scotland X  X  secession of the 300 years union with Great Britain. This critical event at-tracted unprecedented levels of political activity both offline and online. Social media usage was essential to both cam-paigns, with more than 5.8 million tweets using the indyref hashtag within the year leading up to the vote [6]. Therefore, we propose here a technique to analyse the voting intentions of users, based on data mining and machine learning ap-proaches. Indeed, the general approach we advance could be used to understand vote intentions in other major elections.
To analyse voting intentions, we capture two months (Au-gust 1 to September 30, 2014) of Twitter data related to the IndyRef. To form a ground truth, we label users based upon hashtags appearing in their tweets, and we verify the reli-ability of this approach using the users X  followee networks. Those in favour of an independent Scotland used hashtags such as VoteYes and YesScotland ; those opposed used Bet-terTogether and VoteNo  X  X hich serves as our ground-truth. After removing the hashtags from these tweets, we then fo-cus on the remaining terms, treating each term as a feature. However, the referendum created an evolving discourse, with different topical themes (such as oil , currency , and debt ), which make the accurate classification of users X  voting in-tentions more challenging. For instance, the word (feature)  X  X hange X  is indicative of a  X  X o X  voter in the currency topic, and of a  X  X es X  voter in the nuclear weapons topic. That is, there was a significant discussion over whether Scotland would need to  X  X hange X  its currency if it obtained indepen-dence, while the  X  X es X  camp purported that the nuclear ar-senal base could  X  X hange X  in an independent Scotland. The dichotomy of the term  X  X hange X  in indicating voting inten-tions across different topics highlights the main benefit of our approach. Indeed, this paper contributes to the use of topical clusters to identify the topic of discussion in a tweet and subsequently to classify users X  voting intentions. Our approach, called Topics-Based Naive Bayesian (TBNB) demonstrates marked improvements over a classical Naive Bayes (NB) classification baseline.
Recently Al Zamal et al. [1] focused on the inference of la-tent attributes, such as age, gender and political orientation, based on textual and retweeting features. They achieved a high accuracy (90%); however, Raviv et al. [4] demonstrated that classification of political orientation was still a difficult problem and that the earlier result was exaggerated since it used easily classifiable political data. Conover et al. [5] showed that users X  tweeting behaviours X  X uch as the actions of re-tweeting, mentioning, and replying X  X an be used to clas-sify the political polarization of users. In contrast, instead of leveraging the users X  profile data or tweeting behaviours, we focus on the content of tweets to classify the users X  vot-ing intentions. We use as a starting point a classical Naive Bayesian (NB) classifier, which is an application of Bayes theorem within a probabilistic model to capture the con-ditional class probabilities of each feature. Since the num-ber of features can be very large, it is common to use fea-ture selection approaches to prune the features. For exam-ple, the following feature selection approaches are commonly used for the NB classifier: F requence ( word ) (denoted FR), LogP robRatio ( word ) (LR), ExpP robRatio ( word ) (ER), Od -dsRatio ( word ) (OR) and weightedOddsRatio ( word ) (WOR) (see [7]). OR is reported to obtain a high accuracy for the Naive Bayes classifier [7]. Each selection approach ranks and selects the F informative features (in our case we treat each term in a tweet as a feature) based on the training data (e.g., F = 1000). Of course, not every selected feature will appear in the unseen test tweets -we denote the number of such  X  X ctivated X  features as F test . For instance, a testing tweet containing  X  X cotland has remained in the media spot-light throughout 2014 X  has 9 terms. If only  X  X cotland X ,  X  X e-mained X ,  X  X edia X  and  X  X potlight X  were selected as features, the number of activated features is F test = 4.
The IndyRef discussions on Twitter revolved around a number of topics, for which people X  X  opinions usually re-flected their vote intentions. For example, many  X  X es X  voters believed that revenues derived from the North Sea oil fields belonged to Scotland and could sustain it. On the other hand, many  X  X o X  voters argued that these sources were in-sufficient in the long run. A feature X  X  dissimilarity repre-sents the usage difference of this feature across topics, and the difference in usage of  X  X il X  across different topics is there-fore high. For a given topic, a feature X  X  variance refers to the difference of the conditional probabilities of the occur-rence of such a feature in different categories. For example, the conditional probability of  X  X il X  in the  X  X es X  category is higher than in the  X  X o X  category. Typically, the feature se-lection approaches select features with higher variances be-tween categories. Thus if a feature differs between topics, it will be treated as different features in the TBNB model. Thus TBNB can capture term dependencies between topic and user voting intentions. On the other hand, since the essence of the NB classifier is to learn those features with high variance from the categories, the TBNB classifier is be-lieved to work better by leveraging both the features X  dissim-ilarities across topics and their variances in the categories.
We assume that a single tweet involves a single topic. In the training step, the topics are first detected by Latent Dirichlet Allocation (LDA), a probabilistic graphical model introduced in [3]. For each topic, a corresponding probabil-ity table is produced, where each feature has two associated conditional probabilities related to the two possible voting intentions ( X  X es X / X  X o X ). Consequently, during the training step, we produce as many feature tables as the number of used topics. In the testing step, we treat a user as a virtual document and this document contains the users X  tweets. For each tweet in the user X  X  virtual document, the topic that is closest to the tweet X  X  content is selected. As a topic can be represented by a mean vector of its tweets X  vectors, the clos-est topic can be selected by computing the cosine similarity between the vector representation of the unseen tweet and the vector representations of the topics. We use standard TF vectors to represent both the tweets and topics. Terms in an Algorithm 1 Topics-Based Naive Bayesian (TBNB). topic n , n = { 1 , 2 , .., N } X  topic detection ( tweets n  X  1 , c = {  X  Y es  X  ,  X  N o  X  } for n  X  N, n + + do end for
P robP roduct c i  X  1 for tweet in user testing do end for class ( user ) = argmax c i ( p ( c i )  X  P robP roduct c Table 1: Topics and associated terms in the IndyRef. unseen tweet are then examined using the probability table generated during the training step for the topic with which this tweet is associated. In this way, terms in different tweets are treated differently based on their associated topics, and the TNBN classifier applies, for each unseen tweet, those fea-tures that were learned from the corresponding topic. The detailed TBNB algorithm is presented in Algorithm 1. An overview of the whole TBNB classification process is shown in Figure 1. Note that the feature selection approaches can naturally be applied to the TBNB classifier. For example, if F (see Section 2) is set to 1000, the top 1000 features learned from each topic are selected.

We use the LDA implemented in Mallet 1 . We investigate various topic numbers ( T = { 5 , 10 , 20 , 30 } ). Table 1 shows the topic terms extracted using LDA for 10 topics. For readability purposes, the first column of Table 1 provides the general theme of the extracted topic 2 . For example, we can see that tweets related to currency and oil were com-mon. Other oft-used topics and features included references to Alex Salmond, who was both the leader of the Scottish National Party (SNP) and the  X  X es X  campaign.
The corpus pertaining to the Scottish Referendum event was collected from the Twitter network by searching for a number of referendum-specific hashtags (e.g. #IndyRef) and associated terms (e.g.  X  X ote X ,  X  X eferendum X ) using the Twitter Streaming API 3 . The (uncompressed) 33GB dataset http://mallet.cs.umass.edu/ 2 These themes are manu-ally annotated. 3 https://dev.twitter.com/ contains 6 million tweets from over 1 million users collected from August 1, 2014 to September 30, 2014.

In our dataset, 79.7% of users posting tweets with more than one. The most commonly used hashtags indicating the users X  voting intentions are listed in Sets 1 and 2 below. As can be seen, certain hashtags were associated with a  X  X es X  vote, and others with a  X  X o X  vote. To reduce sparsity, we retain only users with more than 30 tweets posted during the timeframe of the collection. To generate our ground truth, we assume that if a user X  X  tweets are only tagged by hashtags in Set 1, then this user is labeled as a  X  X o X  voter. Similarly, if a user X  X  tweets contain only hashtags in Set 2, then the user is labeled as a  X  X es X  supporter, favoring independence. Set 1: #NoBecause, #BetterTogther, #VoteNo, #NoThanks Set 2: #YesBecause, #YesScotland, #YesScot, #VoteYes
Using this method, we find 5326  X  X es X  users and 2011  X  X o X  users. Together these 7337 users account for more than 420K tweets. After labelling, all hashtags in Set 1 and Set 2 are removed from their original tweet text. The result-ing tweets constitute our classification dataset. We use this dataset to examine the usefulness of enriching the NB clas-sifier with the extracted topics. Without the hashtags, the classification task is naturally more challenging, but impor-tantly, the resulting generalisable classifier does not require the presence of hashtags.

Next, we verify our ground-truth X  X  reliability using the users X  followee networks. In particular, members of the Con-servative Party (CONV) were staunchly opposed to inde-pendence, with post-election surveys showing that 95% of Conservatives voted  X  X o X  4 . Thus, we argue that if a user mainly follows Conservative politicians, this person is likely to be a  X  X o X  voter. In contrast, 86% of SNP party voters favoured independence 4 , and hence if a user follows SNP politicians, their vote intention is more likely to be  X  X es X  X  X n like manner to a previously used method to classify users X  political orientation [2]. We then examined the networks of the 7337 users in our dataset, and used the Twitter REST API 3 to identify who these users follow among the 536 public Twitter accounts corresponding to Members of the British (MPs) or Scottish (MSPs) Parliaments. We use two veri-fication approaches, denoted c V 1 and c V 2 for verifying the reliability of our ground truth: c V 1 assumes an exclusive fol-lowee membership, while c V 2 assumes a marked tendency to follow politicians of a given political party, namely: where n p ( u ) is the number of times user u follows a politician (MPs/MSPs) of party p . We test our ground truth by com-paring a user X  X  label allocated using the hashtags versus that allocated using the two verification methods. If the two la-bels are concordant, then the user voting intention is said to be verified, i.e. it is likely to be correct. Table 2 reports the agreement statistics between our hashtag labelling method and the two verification methods. Comparing the hashtag labelling method with the two verifications, we find that c
V 1 verifies more users than c V 2 , but shows lower agreement (c.f. Cohen X  X  Kappa) than c V 2 . Overall, we find, of the 6332 users verified by c V 1 or c V 2 , 87% can be verified into  X  X es X  or  X  X o X  voters, demonstrating that our ground-truth produced by the hashtags labeling method is reasonable and reliable. http://lordashcroftpolls.com/ 2014/09/scotland-voted/
We use the dataset described in Section 4 to compare the performances of the NB and TBNB classifiers. To assess the impact of parameters used in both classifiers, we vary the number of selected features F and the deployed fea-ture selection approach for both NB and TBNB. We also vary the number of topics T in the TBNB classifier. Since the number of unique terms in our collection is 200K, we vary F = { 5 K, 10 K, 20 K, 50 K, 100 K, 120 K, 150 K, 180 K } for NB, while, for TBNB, as F depicts the number of fea-tures selected for each topic (i.e. the total number of features would be F  X  T ), we do not experiment with F &gt; 100 K 5
We use a 10-fold cross validation process over the 7337 users of our dataset to evaluate the performances of the NB and TBNB classifiers. In particular, we use the following performance indicators: Indicator 1: Accuracy , the standard classification accu-racy measure.
 Indicator 2: Average Number of Activated Features F test . For an unseen Twitter user, we concatenate their posted tweets into a virtual document and count the number of selected features activated in the virtual document. We average these numbers across the 10 folds to obtain F test Intuitively, the higher F test , the greater the confidence in the predicted category.
 Indicator 3: Average Rank of the Activated Feature R test . Each feature has a rank position ranked by the ap-plied feature selection approach. This indicator represents the average rank position of all testing features of all users in the 10 folds. Intuitively, it reflects the average effectiveness level of the activated features.

We use the three indicators to investigate and explain the performances of the NB and TBNB classifiers, as well as to validate our hypothesis that the TBNB classifier will outper-form NB on our IndyRef dataset. In particular, we answer the following related research questions: (i) how effective are the feature selection approaches on the used dataset?; (ii) what is the effect of F test and R test on the Accuracy performances of the classifiers? Figure 2(a)-(g) shows the performances of the NB and TBNB classifier when varying the parameters of the clas-sifiers. As a baseline, we use NB without feature selection (NB NO). This baseline has at least a comparable perfor-mance to both Support Vector Machine (SVM) and Deci-sion Tree-based (DT) classifiers. Both NB and TBNB per-form poorly when F is low. However, all TBNB classifiers markedly outperform the NB NO baseline when F ranges from 10K to 50K. The highest accuracy of TBNB (90.4%) is achieved when applying the WOR feature selection ap-proach (TBNB WOR) with T =10 and when the FR feature selection approach is deployed (TBNB FR) with T = 5. This is a 7.8% improvement over the NB baseline (82.6%). When varying the number of used topics ( T ), we note that the performance of the TBNB classifier generally increases as T increases. However, once T reaches 30 topics (see Figure
In our dataset, no topic has more than 100K features. 20 and 30 separately; (f ) and (g) show F test &amp; R test (h) shows an experiment on a 2nd dataset. 2(e)), the accuracy of TBNB starts decreasing while still out-performing the NB NO baseline around the IndyRef. This suggests that the tweets corpus reflects 10-20 main discus-sion themes. On the other hand, each NB or TBNB clas-sifier with feature selection approaches has an optimal F . For instance, the optimal F of NB OR is 150K while that of TBNB FR is 5K.
 We first contrast the feature selection approaches for the NB and TBNB classifiers. Figure 2(f) shows that the av-erage number of activated features ( F test ) is lower for the NB classifier across all feature selection approaches than for TBNB with the same feature selection. This demon-strates that the TBNB classifier activates more features in the virtual document of the user, thereby improving its con-fidence in the voting intention classification. Unlike in previ-ous work where the OR feature selection approach performs best (see Section 2), we found that the WOR and FR feature selection approaches are the most effective on our dataset.
Next, we consider the features selected and activated by each of the classifiers. Firstly, for NB, Figure 2(a) shows that increasing the number of features ( F ) increases the accuracy, until F reaches an optimal value, and decreases thereafter. The same conclusion is true for TBNB, e.g. for 10 topics (Figure 2(c)). However, contrasting Figures 2(a) &amp; (c), we see that TBNB exhibits higher accuracy than NB, despite using less features ( F ). Indeed, we observe from Figure 2 (f) that the number of features activated in the unseen tweets ( F test ) for a given F value is higher for TBNB than for NB -i.e. the classifier has more feature evidence to work with. Moreover, the average rank of those features selected ( R Figure 2(g)) increases as F increases. Hence, the relatively higher and stable F test and R test values observed for TBNB, in comparison to NB, are indicative of its higher accuracy. In summary, the advantage of TBNB over NB is that the topic-based features are more useful, leading to higher accuracies.
Finally, to show the generalisation of TBNB, we use a second dataset with 6234 labelled users, collected from an earlier period (i.e. July 25 to August 25 2014). For both NB and TBNB, we learn the parameters F and T from the first dataset with the 7337 users. We then use the learned pa-rameters in a 10-fold cross validation on the second dataset. From Figure 2(h), we observe that the TBNB classifier out-performs NB in terms of accuracy, with and without the feature selection approaches, by up to 10.3%. Overall, our experiments validate our hypothesis in Section 5, namely that TBNB will outperform NB on the IndyRef dataset.
We classified the users X  voting intentions on Twitter dur-ing the IndyRef. We noted that the users tended to focus their discussions on a specific set of topics, reflecting their voting intentions. As a consequence, we proposed to enrich the Naive Bayes classifier by leveraging the underlying top-ics covered in the tweets. Our proposed approach leverages the dissimilarity of the features across the topics, and their variance across the voting categories to increase the classifi-cation confidence. Our results demonstrate the effectiveness of our resulting TBNB classifier on two datasets with and without the use of feature selection approaches. In the fu-ture, we plan to analyse the effect of the evolving discussions on the users X  voting intentions over time.
