 While the Web is oriented to information sharing and human-machine inter-action, the emerging paradigm of web services promise to bring to distributed computation and services the flexibility that the web has brought to the sharing of documents (see [1]).
 (possibly offered by different providers) into value-added services that satisfy user needs. The same class of web ser vices for a task may be provided by one or more providers, any one of which can be invoked to execute the targeted task. Web services provided by different providers may be different in Quality of Service (QoS), so decision needs to b e made to determine which services are to participate in a given composite one.
 vices execution environment is not determin istic, it is inappropriate to select and compose web services statically. In thi s paper, Markov Decision Processes are used to model the process of dynamic web s ervices selection a nd composition. It is defined on the base of QoS description for web services. The process produces a policy that could optimally guide the composite web service towards its goal, which is based on Bellman X  X  Principle of Optimality(see [4]).
 first one is backward recursive value iteration. Because this approach can not handle the cases when web service invocation failure occurs, value iteration start-ing from beginning of the decision proces s is introduced. With forward iteration, the process of policy computation is inter woven with the process of service exe-cution tracking, which re-evaluates th e QoS data for invoked web services. It is assumed that the execution history of s ervices is recorded and QoS data about web services can be accessed in service registries. Experiments are conducted to evaluate the performance of MDP methods for service compos ition. The results show the methods are efficient.
 both elementary services and composi te ones; Section 3 gives Markov Decision Process method for composite web servi ce; Section 4 introduces two algorithms for computing MDP; Section 5 shows experimental results of the approaches introduced in section 4; Section 6 review s related works and Section 7 concludes this paper and discusses future work. In this paper, web services of the same kind are aggregated to be a service class (or called abstract service). The interfa ces of web services that belong to a service class are identical, though they ma y be different in quality of service. 2.1 Model for Single Web Service For an application to use a web service, its programmatic interfaces must be precisely described. WSDL [9] is an XML grammar for specifying properties of awebservicesuchas what it does, where it is located and how it is invoked. tion of composite services, QoS properti es of web service provide supports for dynamic service selectio n and composition. QoS model for Web service in this paper includes four criteria: reliability , cost, response time and availability. ability that a service request is responded successfully. Its value is computed using the expression Num(success) / Num(all) ,where Num(success) records the number of successful invocation and Num(all) the number of all invocation. Cost: The cost cost(s, op) is the cost that a service requester has to pay for invoking the operation op .
 Response time: response(s) measures the expected delay between the moment when a request is sent and the moment when the results are received. Availability: The availability availability(s) of a service s is the probability that the service is accessible.
 2.2 Web Service Composition The standards for web service compositio n have been proposed for years, includ-ing BPEL4WS[6], WS-Choreography[7], OWL-S(DAML-S)[8] and so on. The constructs and composition patterns in those standards or languages can be summarized as workflow patterns discussed by [10] in detail. The usually used patterns are sequential, conditional choice (or called switch), parallel and it-erative. This paper will only discuss how to represent these four constructs in Markov decision process.
 process using service class. The task nodes are defined independent of any con-crete web services, which means any concre te service that satisfies the interfaces description can be bound to the node. Fig. 1 gives the illustration of some web service composition scenarios. In this section, the concepts for Markov D ecision Process are first introduced, which are taken from [4, 13, 14]. Then MDP for web service composition is defined in the following tow subsections. 3.1 Preliminary Concepts Definition 1. (Markov Decision Process (MDP)) A Markov Decision Process is a 5-tuple, { T,S,A(i),p(  X | i,a),r(i,a) } ,whereTis the set of all decision time units.
 The set of all actions that may be taken at time t is denoted as A(i) which is also called action spaces. The union of all A(i) is denoted as A = assumed that state and action sets are finite and independent of time. (1) a reward r(i,a) (2) system state at next time unit is determined by probability distribution p(  X | i,a) when r(i,a) is positive, cost or penalties while negative. Generally speaking, the reward depends on state j at next time unit, i.e., r(i,a,j) . The expected reward of action a is r ( i, a )= bility that the system will transfer to state j from state i. It is assumed that the sum of the probability of all states the system will transfer to from some state i is 1.That is to say: Markov Decision Processes are special ca ses of decision processes where transi-tion probability and reward function depend only on current state and action taken, independent of history.
 Definition 2. (Decision rule and Policy) A decision rule  X  t , for time t, determines a probability distribution of actions over A(i). It is the principle that guides taking action at any state. form  X  =(  X  1 , X  2 ,..., X  t ,... ) .Symbol  X  is used for both infinite and finite horizon problems. To obtain a finite horizon policy from an infinite horizon policy, its operation must be restricted to a finite number of time units. A policy tells how to determine actions for any time unit of the process.
 some optimality criteria before the decisi on process begins. The criteria of Markov decision process used in this paper is finite horizon total expected reward. 3.2 MDP for Web Service Composition Markov Decision Processes for web servi ce composition are defined in the follows way.
 Decision Time Unit Because the number of nodes defined in a composite web service is usually finite, finite horizon MDP is used to represent the composition problem.
 States State is defined as a conjunction of status of each task node. Suppose there are M task nodes, then a system state of composite service is written as a M-tuple: &lt;s node is active and has been bound to a concrete web service while s i =0means that this node is not active.
 Action Actions may be take n at some state is the set of candidate web services that can be bound to the current active task node. For node i if there are M i candidate services, then the cardinality of the set A ( i )is M i . Transition Probability Matrix The state where node i is active is written as &lt;s 1 ...s i ... &gt; with s i =1, from which system will enter state &lt;s 1 ...s i ...s j ... &gt; (with s j =1)with of 1  X  reliability ( s i,j ).
 iteration approach for MDP. If the invocation of web service s i,j fails, state remains unchanged and the transition probability will be adjusted accordingly. After that, policy computation for MDP is resumed and new action might be picked up according to adjusted transition probability matrix.
 Reward Reward is defined as response time or cost associated with service s i,j that is selected to bind to node i . The goal of web service composition problem is usually to find the solution which minimizes response time. According to the definition of MDP, response time is negative reward, which means the policy is to minimize the total expected response time. 3.3 Discussion for Composition Constructs Definition of MDP for web service composition in last subsection is for sequential construct. Representation of conditional, parallel and iterative constructs using MDP (as shown in Fig. 1) will be discussed in this subsection.
 Conditional Case Suppose there are K choices in a conditional construct and the probability as-sociated with each branch is p 1 ,...,p K , respectively.
 active, which denotes the k -th branch of the conditional construct. for execution of node i .
 service.
 probability is in similar way. Suppose the k -th branch is entered from task t a , then system will enter state &lt;s 1 ...s i ...s t b ...s M &gt; from state S ( jk )with probability reliability ( s jk,t b ).
 Parallel Case Suppose current state is &lt;s 1 ...s i ...s j ...s M &gt; with s i corresponding to entry parallel construct, where s j 1 ...s j K are all active. The corresponding transition probability is executing node i . Reward is defined as the maximum of all these K web services. Iterative Case To represent iterative construct in MDP, it is not necessary to unfold it into a sequential construct as done by [11].
 Tasks within the loop can be modeled using sequential, conditional and parallel ones. Only the exit task needs s pecial discussion. After task t b finishes, the prob-ability that system will break out the loop is p , which means system will transfer from state for t b to states for tasks following the iterative with probability p . And system will remain at state for t b with probability 1  X  p . 4.1 Backward Recursive Value Iteration MDP can be computed by backward recursive value iteration. It starts from horizon N given final reward r N ( i N ) and iterates to horizon 0. When horizon 0 is reached, optimal criteria and optimal actions at each horizon are computed. The sequence of actions computed from this algorithm form the policy. Algorithm 1 outlines the computation procedure. More details about this algorithm can be found in [4, 13, 14].
 Algorithm 1. Value Iteration iteration algorithm re flects the concept of Principle of Optimality ,whichis first presented in [5]. Bellman X  X  Principle of Optimality is stated as:  X  X n optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. X (see [4]) 4.2 Forward Value Iteration The backward value iteration is based on final reward. During the computation, the transition probability matrix and reward function is assumed not to change. But in dynamic and stochastic web servic e environment, web services selected by the policy may fail. If the iteration is modified to start from the beginning, web service invocation failure can be a ccommodated in the policy computation process as mentioned in Subsection 3.3.
 subsection, with except that iteration equation is modified as: In order to evaluate the methods, experiments are performed to examine the per-formance of the methods discussed in this paper. The PC configuration: Pentium 4 1.5GHZ with 512M RAM, Windows 2000, jdk1.4.2.
 In this process, the number of task nodes is varied from 10 to 100 by adding tasks randomly and the number of candidate services per service class is varied from 10 to 50, both with steps of 10. All experiments are conducted 10 times and the average computation cost is computed.
 services that can be implemented by a co llection of concret e services by regis-tering concrete web services with each ab stract one. The deta iled description of abstract and concret e web services are stored in Xi ndice(http://xml.apache.org/ xindice/), while some of indexing column s such as service id, service name and quality parameters are stored in Mysql. This facilitates the process of identify-ing all candidate web services for a give n abstract service. The QoS data are generated according to random variables following Gaussian distribution. backward value iteration and forward value iteration(without failure). The com-putation cost is acceptable as far as the exp eriments are concerned. For example, when there are 100 tasks and average 50 ca ndidate services per class, the com-putation cost is about 1.07 seconds and 1.14 seconds for backward and forward iteration, respectively. Only in this case is the computation cost more than one second. This means the methods may be useful in dynamic service composition for such problem size.
 Fig. 3 shows the computation cost (in seconds) of MDP method using value iteration with service invocation failure. The average failure probability of service invocation is about 5% and 10%,respectively. Though the cost shown in Fig. 3 is higher than that in Fig. 2, it is still a cceptable. For example, when there are 100 tasks and average 50 candidates per class, the computation cost is about 1.42 seconds and 1.53 seconds with probability 5% and 10%,respectively. failure probability is set to 5% for forward value iteration. It can be seen from the figure that the performance of backward method and value iteration without failure is almost the same. While the computation cost for value iteration with failure is higher than the other two methods. This can be explained that the system will stay at some state when failure happens and the number of iteration increases.
 cation on computation cost, experiments are repeated for value iteration algo-rithm with different average failure probability. In experiments, the probability is set to 1%, 5% and 10%, respectively. Fig. 5 shows the results when the number of candidate services per service class is 2 0 and 50. From Fig. 5, it can be observed that computation cost increases when average failure probability increases. As the failure probability increases, the chances for the system to stay at some state increase as. So the average number of iter ation increases. And according to the algorithms, optimal action is computed for every state during each iteration. So the increased number of iteration incurs more computation. The emerging paradigm of web services promise to bring to distributed com-putation and services the flexibility that the web has brought to the sharing of documents. Related works on formal models of web services , automatic compo-sition, analysis and verification can be found in [1, 2, 15, 16].
 vices) [6] is the most popular standard for web service composition. However, performance and QoS are not covered in BPEL4WS. The topic of web service QoS and performance is discussed in [17 X 21]. Dynamic web service selection and composition is covered in [12, 11, 22]. Authors in [17] discussed a method for capacity planning for web services using Queueing Network-based models. QoS issues about web services are in [18, 19] covers quality computation and QoS negotiation. QoS requirements for web services are defined in detail in [20, 21]. Quality definition of web service in this paper is the same as previous works, which can be found in [11, 12].
 dynamically choreographing web services. The differences between this paper and [3] are the followings. (1)In contrast to [3], this work is a general frame-work for representing web service composition using MDP. While the stochastic nature shown by the motivating example in [3] is inherently a domain specific business logic. (2) Web service composition constructs are discussed in this pa-per, which makes this method flexible and can be easily extended to accommo-date other composition constructs. Some frequently used composition constructs are discussed, including sequential, parallel, conditional and iterative. (3) In ad-dition to forward value iteration method, backward value iteration algorithm for MDP is also investigated in this paper. (4) The performance of value it-eration algorithms is examined by experiments while the performance issue is not mentioned in [3]. (5) This paper expl ores the impact of service invocation failure probability on the performance of value iteration algorithm. (6) MDP model in this paper is based on QoS criteria, which is the promising base for web service publishing, brokering, billing, contracting and composing according to [2]. Markov decision process is used to mod el the process of selecting web services dynamically in this paper, where N horizo ns total expected reward is the crite-rion. It is defined on the base of QoS description for web services. Web service composition patterns including sequential, conditional, parallel and iterative are modeled in MDP. Two algorithms are introduced and implemented for comput-ing optimal policy. Experimental results show that these methods are efficient and it is acceptable of the computation cos t. In future work, other criteria for Markov decision process will be employed such as infinite horizon expected total discounted reward and average reward, etc. Composition constructs presented in BPEL4WS ([6]) will also be further investigated in future work. This work is supported by the National Natural Science Foundation of China under Grant No. 90412010 and Chi naGrid project of the Ministry of Education, China.

