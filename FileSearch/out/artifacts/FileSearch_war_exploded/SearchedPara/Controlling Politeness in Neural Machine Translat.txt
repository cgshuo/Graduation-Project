 Many languages use honorifics to express polite-ness, social distance, or the relative social status be-tween the speaker and their addressee(s). A wide-spread instance is the grammatical T-V distinction (Brown and Gilman, 1960), distinguishing between the familiar (Latin T u ) and the polite (Latin V os ) second person pronoun. In machine translation from a language without honorifics such as English, it is difficult to predict the appropriate honorific, but users may want to control the level of politeness in the output.

We propose a simple and effective method for in-cluding target-side T-V annotation in the training of a neural machine translation (NMT) system, which allows us to control the level of politeness at test time through what we call side constraints . It can be applied for translation between languages where the T-V distinction is missing from the source, or where the distribution differs. For instance, both Swedish and French make the T-V distinction, but reciprocal use of T pronouns is more widespread in Swedish than in French (Sch X pbach et al., 2006). Hence, the Swedish form is not a reliable signal for the appro-priate form in the French translation (or vice-versa).
Our basic approach of using side constraints to control target-side features that may be missing from the source, or are unreliable because of a cate-gory mismatch, is not limited to the T-V distinc-tion, but could be applied to various linguistic fea-tures. This includes grammatical features such as tense and the number/gender of discourse partici-pants, and more generally, features such as dialect and register choice.

This paper has the following contributions: Attentional neural machine translation (Bahdanau et al., 2015) is the current state of the art for English  X  German (Jean et al., 2015b; Luong and Manning, 2015). We follow the neural machine translation architecture by Bahdanau et al. (2015), which we will briefly summarize here. However, our approach is not specific to this architecture.
The neural machine translation system is imple-mented as an attentional encoder-decoder network. The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = ( x 1 ,...,x m ) and calculates a forward sequence of hidden states ( and a backward sequence ( states notation vector h j .

The decoder is a recurrent neural network that predicts a target sequence y = ( y 1 ,...,y n ) . Each word y i is predicted based on a recurrent hidden state s i , the previously predicted word y i  X  1 , and a context vector c i . c i is computed as a weighted sum of the annotations h j . The weight of each annota-tion h j is computed through an alignment model  X  ij , which models the probability that y i is aligned to x j . The alignment model is a single-layer feedforward neural network that is learned jointly with the rest of the network through backpropagation.

A detailed description can be found in (Bahdanau et al., 2015). Training is performed on a parallel cor-pus with stochastic gradient descent. For translation, a beam search with small beam size is employed. We are interested in machine translation for lan-guage pairs where politeness is not grammatically marked in the source text, but should be predicted in the target text. The basic idea is to provide the neu-ral network with additional input features that mark side constraints such as politeness.

At training time, the correct feature is extracted from the sentence pair as described in the following section. At test time, we assume that the side con-straint is provided by a user who selects the desired level of politeness of the translation.

We add side constraints as special tokens at the The attentional encoder-decoder framework is then able to learn to pay attention to the side constraints. One could envision alternative architectures to in-corporate side constraints, e.g. directly connecting them to all decoder hidden states, bypassing the attention model, or connecting them to the output layer (Mikolov and Zweig, 2012). Our approach is simple and applicable to a wide range of NMT ar-chitectures and our experiments suggest that the in-corporation of the side constraint as an extra source token is very effective. Our approach relies on annotating politeness in the training set to obtain the politeness feature which we discussed previously. We choose a sentence-level annotation because a target-side honorific may have no word-level correspondence in the source. We will discuss the annotation of German as an example, but our method could be applied to other languages, such as Japanese (Nariyama et al., 2005).

German has distinct pronoun forms for informal and polite address, as shown in Table 1. A further difference between informal and polite speech are imperative verbs, and the original imperative forms are considered informal. The polite alternative is to use 3rd person plural forms with subject in position 2:
We automatically annotate politeness on a sen-tence level with rules based on a morphosyntactic annotation by ParZu (Sennrich et al., 2013). Sen-tences containing imperative verbs are labelled in-formal. Sentences containing an informal or polite pronoun from Table 1 are labelled with the corre-sponding class.

Some pronouns are ambiguous. Polite pronouns are distinguished from (neutral) 3rd person plural forms by their capitalization, and are ambiguous in sentence-initial position. In sentence-initial posi-tion, we consider them polite pronouns if the English source side contains the pronoun you(r) . For Ihr and ihr , we use the morphological annotation by ParZu to distinguish between the informal 2nd person plu-ral nominative, the (neutral) 3rd person singular da-tive, and the possessive; for possessive pronouns, we distinguish between polite forms and (neutral) 3rd person forms by their capitalization.

If a sentence matches rules for both classes, we label it as informal  X  we found that our lowest-precision rule is the annotation of sentence-initial Sie . All sentences without a match are considered neutral. Our empirical research questions are as follows: 5.1 Data and Methods We perform English  X  German experiments on pus of movie subtitles. Machine translation is com-monly used in the professional translation of movie subtitles in a post-editing workflow, and politeness is considered an open problem for subtitle transla-tion (Etchegoyhen et al., 2014). We use OpenSub-titles2012 as training corpus, and random samples from OpenSubtitles2013 for testing. The training corpus consists of of 5.58 million sentence pairs, out of which we label 0.48 million sentence pairs as po-lite, and 1.09 million as informal.

We train an attentional encoder-decoder NMT Jean et al., 2015a). We follow the settings and train-ing procedure described by Sennrich et al. (2015), using BPE to represent the texts with a fixed vocab-ulary of subword units (vocabulary size 90000).
The training set is annotated as described in sec-tion 4, and the source side is marked with the po-liteness feature as described in section 3. Note that there are only two values for the politeness feature, and neutral sentences are left unmarked. This is to allow users to select a politeness level for the whole document, without having to predict which transla-tions should contain an address pronoun. Instead, we want the NMT model to ignore side constraints when they are irrelevant.

To ensure that the NMT model does not overly rely on the side constraints, and that performance does not degrade when no side constraint is provided at test time, only a subset of the labelled training in-stances are marked with a politeness feature at train-ing time. We set the probability that a labelled train-ing instance is marked,  X  , to 0.5 in our experiments. To ensure that the NMT model learns to ignore side constraints when they are irrelevant, and does not overproduce address pronouns when side constraints are active, we also mark neutral sentences with a random politeness feature with probability  X  . Keep-ing the mark-up probability  X  constant for all sen-tences in the training set prevents the introduction of unwanted biases. We re-mark the training set for each epoch of training. In preliminary experiments, we found no degradation in baseline performance when politeness features were included in this way during training.

The model is trained for approximately 9 epochs (7 days). At test time, all results are obtained with the same model, and the only variable is the side constraint used to control the production of hon-orifics. We test translation without side constraint, and translations that are constrained to be polite or informal. In an oracle experiment, we use the po-liteness label of the reference to determine the side constraint. This simulates a setting in which a user controls the desired politeness. 5.2 Results Our first test set is a random sample of 2000 sen-tences from OpenSubtitles2013 where the English source contains a 2nd person pronoun. Results are shown in Table 2. Side constraints very effectively control whether the NMT system produces polite or informal output. Translations constrained to be polite are overwhelmingly labelled polite or neutral by our automatic target-side annotation (96%), and analogously, translations constrained to be informal are almost exclusively informal or neutral (98%).
We also see that B LEU is strongly affected by the choice. An oracle experiment in which the side con-straint of every sentence is informed by the reference obtains an improvement of 3.2 B LEU over the base-line (20.7  X  23.9).

We note that the reference has a higher propor-tion of German sentences labelled neutral than the NMT systems. A close inspection shows that this is due to sentence alignment errors in OpenSubtitles, free translations as shown in Table 3, and sentences where you is generic and translated by the imper-sonal pronoun man in the reference.

The side constraints are only soft constraints, and are occasionally overridden by the NMT system. These cases tend to be sentences where the source text provides strong politeness clues, like the sen-tence You foolish boy . Neither the address boy nor the attribute foolish are likely in polite speech, and the sentence is translated with a T pronoun, regard-less of the side constraint.

While Table 2 only contains sentences with an address pronoun in the source text, Table 4 repre-sents a random sample. There are fewer address pronouns in the random sample, and thus more neu-tral sentences, but side constraints remain effective. This experiment also shows that we do not over-produce address pronouns when side constraints are provided, which validates our strategy of includ-ing side constraints with a constant probability  X  at training time.

The automatic evaluation with B LEU indicates that the T-V distinction is relevant for translation. We expect that the actual relevance for humans de-pends on the task. For gisting, we expect the T-V distinction to have little effect on comprehensibility. For professional translation that uses MT with post-editing, producing the desired honorifics is likely to improve post-editing speed and satisfaction. In an evaluation of MT for subtitle translation, Etchegoy-hen et al. (2014) highlight the production of the ap-propriate T-V form as  X  X  limitation of MT technol-ogy X  that was  X  X ften frustrat[ing] X  to post-editors. Faruqui and Pado (2012) have used a bilingual English X  X erman corpus to automatically annotate the T-V distinction, and train a classifier to predict the address from monolingual English text. Ap-plying a source-side classifier is potential future work, although we note that the baseline encoder X  decoder NMT system already has some disam-biguating power. Our T-V classification is more comprehensive, including more pronoun forms and imperative verbs.
Previous research on neural language models has proposed including various types of extra infor-mation, such as topic, genre or document context (Mikolov and Zweig, 2012; Aransa et al., 2015; Ji et al., 2015; Wang and Cho, 2015). Our method is somewhat similar, with the main novel idea be-ing that we can target specific phenomena, such as honorifics, via an automatic annotation of the target side of a parallel corpus. On the modelling side, our method is slightly different in that we pass the extra information to the encoder of an encoder X  X ecoder network, rather than the (decoder) hidden layer or output layer. We found this to be very effective, but trying different architectures is potential future work.

In rule-based machine translation, user options to control the level of politeness have been proposed in the 90s (Mima et al., 1997), and were adopted by commercial systems (SYSTRAN, 2004, 26). To our knowledge, controlling the level of politeness has not been explicitly addressed in statistical ma-chine translation. While one could use data selec-tion or weighting to control the honorifics produced by SMT, NMT allows us to very elegantly support multiple levels of politeness with a single model. Machine translation should not only produce seman-tically accurate translations, but should also consider pragmatic aspects, such as producing socially appro-priate forms of address. We show that by annotating the T-V distinction in the target text, and integrating the annotation as an additional input during train-ing of a neural translation model, we can apply side constraints at test time to control the production of honorifics in NMT.

We currently assume that the desired level of po-liteness is specified by the user. Future work could aim to automatically predict it from the English source text based on textual features such as titles and names, or meta-textual information about the discourse participants.

While this paper focuses on controlling polite-ness, side constraints could be applied to a wide range of phenomena. It is a general problem in translation that, depending on the language pair, the translator needs to specify features in the target text that cannot be predicted from the source text. Apart from from the T-V distinction, this includes gram-matical features such as clusivity, tense, and gender and number of the discourse participants, and more generally, features such as the desired dialect (e.g. when translating into Arabic) and text register. Side constraints can be applied to control these features. All that is required is that the feature can be anno-tated reliably, either using target-side information or metatextual information, at training time.
 The research presented in this publication was con-ducted in cooperation with Samsung Electronics Polska sp. z o.o. -Samsung R&amp;D Institute Poland. This project has received funding from the European Union X  X  Horizon 2020 research and innovation pro-gramme under grant agreement 644402 (HimL).

