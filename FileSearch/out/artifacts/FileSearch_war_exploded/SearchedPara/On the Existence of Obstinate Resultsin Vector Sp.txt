 The vector space model (VSM) is a popular and widely applied model in information retrieval (IR). VSM creates vector spaces whose dimensionality is usually high (e.g., tens of thousands of terms). This may cause various problems, such as susceptibility to noise and difficulty in capturing the underlying semantic struc-ture, which are commonly recognized as different aspects of the  X  X urse of dimensionality. X  In this paper, we investigate a novel as-pect of the dimensionality curse, which is referred to as hubness and manifested by the tendency of some documents (called hubs) to be included in unexpectedly many search result lists. Hubness may impact VSM considerably since hubs can become obstinate results, irrelevant to a large number of queries, thus harming the performance of an IR system and the experience of its users. We analyze the origins of hubness, showing it is primarily a conse-quence of high (intrinsic) dimensionality of data, and not a result of other factors such as sparsity and skewness of the distribution of term frequencies. We describe the mechanisms through which hubness emerges by exploring the behavior of similarity measures in high-dimensional vector spaces. Our consideration begins with the classical VSM (tf-idf term weighting and cosine similarity), but the conclusions generalize to more advanced variations, such as Okapi BM25. Moreover, we explain why hubness may not be easily mitigated by dimensiona lity reduction, and propose a sim-ilarity adjustment scheme that takes into account the existence of hubs. Experimental results over real data indicate that significant improvement can be obtained through consideration of hubness. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation, Measurement, Performance, Theory Text retrieval, vector space model, nearest neighbors, curse of di-mensionality, hubs, similarity con centration, cosine similarity
The vector space model (VSM) [13] is a popular and widely ap-plied information retrieval (IR) model that represents each docu-ment as a vector of weighted term counts. A similarity measure is used to retrieve a list of documents relevant to a query document. VSM allows for many variations in the choice of term weights and similarity measure used, with prominent representatives including tf-idf weighting and cosine similarity, as well as more recently pro-posed schemes Okapi BM25 [12] and pivoted cosine [14].

Typically, the number of terms used in VSM is large, producing a high-dimensional vector space (with, e.g., tens of thousands of dimensions). This high dimensionality has been identified as the source of several problems, such as susceptibility to noise and dif-ficulty in capturing the underlying semantic structure. Such prob-lems are commonly recognized as different aspects of the  X  X urse of dimensionality, X  and their amelioration has attracted significant research effort, mainly based on dimensionality reduction.
In this paper we investigate a novel aspect of the dimensionality curse, called hubness , which refers to the tendency of some vectors (the hubs) to be included in unexpectedly many k -nearest neighbor lists of other vectors in a high-dimensional data set, according to commonly used similarity/distance measures. Hubness has previ-ously been observed in various application fields, such as audio re-trieval [1, 2] and fingerprint identification [9], where it is described as a problematic situation. Nevertheless, none of the existing stud-ies provide full explanations of the mechanisms underpinning it. On the other hand, we have explored the hubness phenomenon for general vector-space data, mostly with Euclidean distance in the context of machine learning [11], and also conducted a prelimi-nary examination of the phenomenon for cosine and cosine-like similarity measures with respect to collaborative filtering applica-tions [10]. To our knowledge, hubness has not been thoroughly examined in connection to VSM and IR.

Hubness is worth studying in the context of IR, because it con-siderably impacts VSM by causing hub documents to become ob-stinate results, i.e., documents included in the search results of a large number of queries to which they are possibly irrelevant. This problem affects the performance of an IR system and the experi-ence of its users, who may consistently observe the appearance of the same irrelevant results even for very different queries.
We commence our investigation by demonstrating the emergence of hubness in the context of IR (Section 2). We continue with one of our main contributions, which is the explanation of the origins of the phenomenon (Section 3), describing that it is mainly a con-sequence of high intrinsic dimensionality of vector-space data and not of other factors, such as sparsity and skewness of the distri-bution of term frequencies (caused, e.g., by differences in docu-ment lengths [14]). We link hubness with the behavior of similar-ity/distance measures in high-dimensional vector spaces and their concentration , i.e., the tendency of all pair-wise similarity/distance values to become almost equal. Although the study of concen-tration has attracted significant research effort for l p Euclidean distance) [7], we analytically prove the emergence of concentration for the cosine similarity measure used in IR, and ex-press the differences compared to l p norms. To ease the presenta-tion of hubness, our discussion first considers the classical VSM based on tf-idf term weighting and cosine similarity, and then con-tinues by demonstrating its generality on the more advanced varia-tion Okapi BM25 [12], since hubness is an inherent characteristic of high-dimensional vector spaces that form the basis of various IR models. Moreover, it is explained why hubness is not easily miti-gated by dimensionality reduction techniques.

We next proceed to examine how hubness affects IR applica-tions (Section 4) by causing hubs to become frequently occurring but possibly irrelevant results to a large number of queries. For this purpose, we investigate the interaction between hubness and the notion of the cluster hypothesis [15], and propose a similarity adjustment scheme that takes into account the existence of hubs. The experimental evaluation (Section 4.2) of the proposed scheme over real data indicates that significant performance improvements can be obtained through consideration of hubness. Finally, we pro-vide the conclusions and directions for future work (Section 5).
This section will demonstrate the existence of the hubness phe-nomenon, initially on synthetic data (Section 2.1), and then on real text data (Section 2.2), focusing on the classical tf-idf weighting scheme and cosine similarity. A more advanced document repre-sentation, Okapi BM25, is discussed in Section 4.3.
To measure the existence of hubness, let D denote a set of vec-tors in a multidimensional vector space, and N k ( x ) the number of k -occurrences of each vector x  X  D , i.e., the number of times x occurs among the k nearest neighbors of all other vectors in D , with respect to some similarity measure. N k ( x ) can also be viewed as the in-degree of node x in the k -nearest neighbor directed graph of vectors from D .

We begin by considering an illustrative example, the purpose of which is to demonstrate the existence of hubness in vector-space data, and its dependence on dimensionality. Let us consider a ran-dom data set of 2,000 d -dimensional vectors (i.e., points) drawn uniformly from the unit hypercube [0 , 1] d , and standard cosine sim-ilarity between them (Eq. 1 in Section 3.4). Figure 1(a X  X ) shows the observed distribution of N k ( k =10 ) with increasing dimension-ality. For d =3 , the distribution of N k in Figure 1(a) is consistent with the binomial distribution. Such behavior of N k would also be expected if the graph was generated following a directed version of the Erd  X  os-R X nyi (ER) random graph model [5], where neighbors are randomly chosen instead of coordinates.

With increasing dimensionality, however, Figures 1(b) and (c) illustrate that the distribution of N k departs from the random graph model and becomes skewed to the right, producing vectors (called hubs) with N k values much higher than the expected value k .The same behavior can be observed with other values of k and data distributions. This simple example with dense and uniformly dis-tributed data is helpfu l to illustrate the connection between high di-mensionality and hubness, since uniformity may not be intuitively expected to generate hubness for reasons other than high dimen-sionality. To illustra te hubness in a setting more reminiscent of text data that have sparsity and skewed distribution of term fre-quencies, we randomly generate 2,000 vectors with the number of nonzero values for each coordinate ( X  X erm X ) being drawn from Lognormal(5; 1) distribution (rounded to the nearest integer), and random numbers (drawn uniformly from [0 , 1] ) spread accordingly throughout the data matrix. Figures 1(d X  X ) demonstrate the increase of hubness with increasing dim ensionality in this setting.
A commonly applied practice in IR research is to reduce the in-fluence of long documents (having many nonzero term frequencies and/or high values of term frequencies), by using various normal-ization schemes [14] to prevent them from being similar to many other documents. However, as observed above and as will be ana-lyzed in Section 3, the high dimensionality that is an inherent char-acteristic of VSM is the main cause of hubness, as opposed to other data characteristics, since it emerges even when such normalization (cosine) is applied to sparse-skewed data, and also in the case of dense-uniform data where  X  X ong documents X  are not expected.
Before elaborating on the mechanisms through which hubs form, we verify the existence of the phenomenon on real text data sets. Figure 2 shows the distribution of N k ( k =10 ) for tf-idf term weighting and cosine similarity on three text data sets selected with the criterion of having large difference in their dimensionality. Sim-ilarly to the synthetic data sets, it can be seen that hubness tends to become stronger as dimensionality increases, as observed in the longer  X  X ails X  of these distributions.

Table 1 summarizes the text data sets examined in this study. Be-sides basic statistics, such as the number of points ( n ), dimension-ality ( d ) and number of classes, the table also includes a column measuring the skewness of the distribution of N 10 ( S N 10 standardized third moment: where  X  N k and  X  N k are the mean and standard deviation of N respectively. 1 The S N 10 values in Table 1 indicate a high degree of hubness in all data sets. (The remaining columns will be explained in the sequel.)
To describe the mechanisms through which hubness emerges, we begin the discussion by considering again the random data in-troduced in Section 2, i.e., the dense data matrix with iid uniform coordinates, and the sparse data set that simulates skewed term fre-quencies. For the same data sets and dimensionalities, Fig. 3 shows the scatter plots of N 10 against the similarity of each vector to the data set mean, i.e., its center. In the chart titles, we also give the corresponding Spearman correlations. It can be seen that, as dimen-sionality increases, this correlatio n becomes significantly stronger, to the point of almost perfect correlation of hubness to the proxim-ity to the data center.

The existence of the described correlation provides the main rea-son for the formation of hubs: owing to the well-known property of vector spaces, vectors closer to the center tend to be closer, on average, to all other vectors. However, this tendency becomes am-plified as dimensionality increases, making vectors in the proximity to the data center become closer, in relative terms, to all other vec-tors, thus substantially raising their chances of being included in nearest-neighbor lists of other vectors.
To examine further the amplification caused by dimensionality, we compute separately for each of the two examined random data settings (dense-uniform and sparse-random) the distribution, similarities between all vectors in the data set to the center of the data set. From each data set we select two vectors: x 0 is selected to have similarity value to the data set center exactly equal to the expected value E( S ) of the computed distribution S (i.e., at 0 stan-dard deviations from E( S ) ), whereas x 2 is selected to have higher similarity to the data set center, being equal to 2 standard deviations added to E( S ) (we were able to select such vectors with negligi-ble error compared to the simila rities sought). Next, we compute the distributions of similarities of x 0 and x 2 to all other vectors, and the denote the means of these distributions  X  x 0 and  X  spectively. Figure 4 plots, separately for the two examined cases of random data sets, the difference between the two similarity means, normalized (as explained in next paragraph) by dividing with the standard deviation, denoted  X  all , of all pairwise similarities, i.e.: (  X  x 2  X   X  x 0 ) / X  all . These figures show that, with increasing di-mensionality, x 2 , which is more similar to the data center than x becomes progressively more similar (in relative terms) to all other vectors, a fact that demonstrates the aforementioned amplification.
One question that remains is: in high-dimensional spaces, why is it expected to have some vectors closer to the center and thus become hubs? In Section 3.4 we will analyze the property of the cosine similarity measure, referred to as concentration [7], which in this case states that, as dimensionality tends to infinity, the expec-tation of pairwise similarities between all vectors tends to become constant, whereas their standard deviation (denoted above as  X  shrinks to zero. This means that the majority of vectors become about equally similar to each other, thus to the data center as well. However, high but finite dimensionalities, typical in IR, will result in a small but non-negligible standard deviation, which causes the existence of some vectors, i.e., the hubs, that are closer to the center than other vectors. These facts also clarify the aforementioned nor-malization by  X  all , which comprises a way to account for concen-tration (shrinkage of  X  all ) and meaningfully compare  X  x 0 across dimensionalities.

Finally, we need to examine the relation between hubness and additional characteristics of text data sets, such as sparsity and the skewed distribution of term frequencies in  X  X ong X  documents (see Section 2.1). Since Figures 1 and 3 demonstrate hubness for both dense and sparse random data sets, sparsity on its own should Figure 4: Difference between the normalized means of two distribu-not be considered as a key factor. Regarding the skewness in the distribution of term frequencies, we can consider two cases [14]: (a) more (in number) distinct terms, and (b) higher (in value) term frequencies. For the sparse data set with d =2 , 000 dimensions (Fig. 3(f)) we measured the correlations of N 10 with the number of nonzero simulated  X  X erms X  of a vector and with the total sum of term weights of a vector, and found both to be weak, 0.142 for case (a) and 0.19 for case (b), in comparison with correlation 0.927 (see title of Fig. 3(f)) between N 10 and the similarity with the data set mean, which has been described as the main factor behind hub-ness. The weak correlations in cases (a) and (b), which will also be verified with real data (Section 3.2), are expected because nor-malization schemes (cosine in this example) are able to reduce the impact of long documents. What is, thus, important to note is that, even if the correlations of cases (a) and (b) are completely eliminated with another normalization scheme, the hubness phe-nomenon will still be present, since it is primarily caused by the inherent properties of high-dimensional vector space.
In the previous discussion we have used synthetic data that allow the control of important parameters. To verify the findings with real text data, we need to take into account two additional factors: (1) real data sets usually contain dependent attributes, and (2) real data sets are usually clustered, that is, documents are organized into groups produced by a mixture of distributions instead of originating from one single distribution.

To examine the first factor (dependent attributes), we adopt the approach from [7] used in the context of l p -norm concentration. For each data set we randomly permute the elements within every attribute. This way, attributes preserve their individual distribu-tions, but the dependencies between them are lost and the intrinsic dimensionality of data sets increases [7]. In Table 1 we give the skewness, denoted S S N 10 , of the modified data. In most cases S is considerably higher than S N 10 , implying that hubness depends on the intrinsic rather than embedding (full) dimensionality.
To examine the second factor (many groups), for every data set we measured: (i) Spearman correlation, denoted by C N 10 dm and the similarity with the data set center, and (ii) correlation, de-noted by C N 10 cm ,of N k and the similarity with the closest group center. Groups are determined using K -means clustering, where the number of clusters was set to the number of document cate-gories of the data set. 2 In most cases, C N 10 cm is much stronger than C dm . Thus, generalizing the conclusion of Section 3.1 to the case of real data, hubs are more similar, compared with other vectors, to their respective cluster centers.

Regarding long documents (see Section 3.1), for each data set we computed the correlation between N k and the number of nonzero term weights for a document, denoted by C N 10 len1 , and also the corre-lation of N k with the sum of term weights of a document, denoted by C N 10 len w . The corresponding columns of Table 1 signify that these correlations are weaker or nonexistent (on occasion even negative) compared to the correlation with the proximity to the closest cluster mean ( C N 10 cm ). The above observations are in accordance with the conclusions from the end of Section 3.1.
The attribute shuffling experiment in Section 3.2 suggested that hubness is actua lly related more to the in trinsic dimensionality of data. We elaborate further on the interplay of skewness and intrin-sic dimensionality by considering dimensionality reduction (DR) techniques. The main question is whether DR can alleviate the is-sue of hubness altogether.

We examined the singular value decomposition (SVD) dimen-sionality reduction method, which is widely used in IR through la-tent semantic indexing. Figure 5 depicts for several real data sets from Table 1 the relationship between the percentage of features (dimensions) maintained by SVD, and the skewness S N k ( k =10 ). All cases exhibit the same behavior: S N k stays relatively con-stant until a small percentage of features is left, after which it sud-denly drops. This is the point where the intrinsic dimensionality is reached, and further reduction may incur loss of information. This observation indicates that, when the number of maintained features is above the intrinsic dimensionality, dimensionality reduction can-not significantly alleviate the skewness of k -occurrences, and thus hubness. This result is useful in most practical cases, because mov-ing bellow the intrinsic dimensionality may cause loss of valuable information from the data.
Distance concentration, which has been examined mainly for l norms [7], refers to the tendency of the ratio between some notion of spread (e.g., standard deviation) and some notion of magnitude (e.g., the mean) of the distribution of all pairwise distances (or, equivalently, the norms) within a data set to converge to 0 as di-mensionality increases.

Hereby, we examine concentration in the context of cosine simi-larity that is widely used in IR. We will prove the concentration of cosine similarity by considering two random d -dimensional vectors p and q with iid components. Let cos( p, q ) denote the cosine simi-larity between p and q , defined in Eq. 1. 3 Note that our examination does not differentiate between sparse and dense data (concentration occurs in both cases).
From the extension of Pythagoras X  theorem we have Eq. 2 that relates cos( p, q ) with the Euclidean distance between p and q .
Define the following random variables: X = p , Y = q , and Z = p  X  q .Since p and q have iid components, we assume that X and Y are independent of each other, but not of Z .Let C be the random variable that denotes the value of cos( p, q ) .From Eq. 2, with simple algebraic manipulations and substitution of the norms with the corresponding random variables, we obtain Eq. 3.
Let E( C ) and V( C ) denote the expectation and variance of C , respectively. An established way [7] to demonstrate concentra-tion is by examining the asymptotic relation between E( C ) when dimensionality d tends to infinity. To express this asymptotic relation, we first need to express the asymptotic be-havior of E( C ) and V( C ) with regards to d . Since, from Eq. 3, C is related to functions of X , Y ,and Z , we start by studying the expectations and variances of these random variables.
 The same holds for random variable Y .

P ROOF . Follows directly from Theorem 1 and the fact that, since vec-lim d  X  X  X  (E( X 2 ) /d )= const , and lim d  X  X  X  (V( X 2 ) /d )= const . The same holds for random variables Y 2 and Z 2 .
 P ROOF . From Theorem 1 and the equation E( X 2 )=V( X )+E( Figure 6: Concentration of cosine similarity for uniform (left) and
Based on the above results, the following two theorems show that p
V( C ) reduces asymptotically to 0, while E( C ) asymptotically remains constant (proof sketches are given in the Appendix).
T HEOREM 2. lim
T HEOREM 3. lim
Figure 6 illustrates these findings for the uniform and sparse ran-dom data used in previous sections. With respect to the distribution of all pairwise similarities, the plots include, from top to bottom: maximal observed value, mean value plus one standard deviation, the mean value, mean value minus one standard deviation, and min-imal observed value. The figures illustrate that, with increasing di-mensionality, expectation becomes constant and variance shrinks.
It is worth noting that the concentration of cosine similarity re-sults from different reasons than the concentration of Euclidean ( l distance. For the latter, its standard deviation converges to a con-stant [7], whereas its expectation asymptotically increases with d . Nevertheless, in both cases the relative relationship between the standard deviation and the expectation is similar.
This section examines the ways that hubness affects VSM to-wards the main objective of IR, which is to return relevant results for a query document. We consider the commonly examined case of documents that belong to categories (e.g., news categories, like sport or finance). However, a similar approach can be followed for other sources of information about documents, such as indi-cation of their relevance to a set of predefined queries. In the presence of information about documents as in the form of cate-gories, k -occurrences can be distinguished based on whether cat-egory labels of neighbors match. We define the number of  X  X ad X  k -occurrences of document vector x  X  D , denoted BN k ( x ) ,asthe number of vectors from D for which x is among the first k near-est neighbors and the labels of x and the vectors in question do not match. Conversely, GN k ( x ) , the number of  X  X ood X  k -occurrences of x , is the number of such vectors where labels do match. Natu-rally, for every x  X  D , N k ( x )= BN k ( x )+ GN k ( x ) .
We define set normalized by dividing it with vation behind the measure is to express the total amount of  X  X ad X  k -occurrences within a data set. Table 1 includes hubs, i.e., documents with high BN k , are of particular interest to IR, since they affect the precision of retrieval more severely than other documents by being among the k nearest neighbors (i.e., in the result list) of many other documents with mismatching cate-gories. To understand the origins of  X  X ad X  hubs in real data, we rely on the notion of the cluster hypothesis [15]. This hypothesis will be approximated by the cluster assumption from semi-supervised learning [4], which roughly states that most pairs of vectors in a high density region (cluster) should belong to the same category.
To measure the degree to which the cluster assumption is vio-lated in a particular data set, we define a simple cluster assumption violation (CAV) coefficient as follows. Let a be the number of pairs of documents which are in different category but in the same clus-ter, and b the number of pairs of documents which are in the same category and cluster. Define CAV = a/ ( a + b ) , which gives a number in range [0 , 1] , higher if there is more violation. To reduce the sensitivity of CAV to the number of clusters (too low and it will be overly pessimistic, too high and it will be overly optimistic), we choose the number of clusters to be 3 times the number of cate-gories of a data set. As in Section 3.2, we use K -means clustering.
For all examined text data sets, we computed the Spearman cor-relation between contrast, N 10 (measured correlations are  X  0.03 and 0.109, respectively). The latter indicates that high in trinsic dimensionality and hubness are not sufficient to induce  X  X ad X  hubs. Instead, we can argue that there are two, mostly independent, factors at work: violation of the cluster assumption on one hand, and hubness induced by high intrinsic dimensiona lity on the other.  X  X ad X  hubs originate from putting the two together; i.e., the consequences of violating the cluster assumption can be more severe in high dimensions than in low dimensions, not in terms of the total amount of  X  X ad X  k -occurrences, but in terms of their distribution, since strong hubs are now more prone to  X  X ick up X  bad k -occurrences than non-hubs.
Based on the aforementioned conclusions about  X  X ad X  hubness, in this section we propose and evaluate a similarity adjustment scheme with the objective to show how its consideration can be used successfully for improving the precision of a VSM-based IR system. Our main goal is not to compete with the state-of-the art methods for improving the precision and relevance of results ob-tained using baseline methods, but rather to demonstrate the prac-tical significance of our findings in IR applications, and the need to account for hubness. Thus, the elaborate examination of more sophisticated methods is addressed as a point of future work.
Let D denote a set of documents, and Q a set of queries inde-pendent of D . We will also refer to D as the  X  X raining X  set, and to Q as the  X  X est X  set, and by default compute N k , BN k on D . We adjust the similarity measure used to compare document vector x  X  D with query vector q  X  Q by increasing the similarity in proportion with the  X  X oodness X  of x ( GN k ( x ) ), and reducing it in proportion with the  X  X adness X  of x ( BN k ( x ) ), both relative to the total hubness of x ( N k ( x ) ), for a given k : sim a ( x, q )=sim( x, q )+sim( x, q )( GN k ( x )  X  BN k The net effect of the adjustment is that strong  X  X ad X  hub documents become less similar to queries, reducing the chances of the docu-ment to be included in a list of retrieved results. To prevent doc-uments from being excluded from retrieval too rigorously, the ad-justment scheme also considers their  X  X ood X  side and awards the presence of  X  X ood X  k -occurrences in an analogous manner.
We experimentally evaluated the improvement gained by the pro-posed scheme compared to the standard tf-idf representation and cosine similarity (all computations involving hubness use k =10 ), through 10-fold cross-validation on data sets from Table 1. First, we focus on the impact of the adjustment scheme on the error intro-duced to the retrieval system by the strongest  X  X ad X  hubs. Let W be the set of the top p % of documents with highest BN k ,asde-termined from the training set, and let BN test k ( x ) and N test be the ( X  X ad X ) k -occurrences of document x from the training set, as determined from similarities with documents from the test set. Table 2: Retrieval  X  X adness X  of 5% of the strongest  X  X ad X  hubs ( We define the total  X  X adness X  of the strongest p % of  X  X ad X  hubs normalization with N test k is done to keep the measure in the [0 , 1] range. The B p % measure focuses on the contribution of  X  X ad X  hubs to erroneous retrieval of false positives.

Table 2 shows B p % on the same p =5% of  X  X ad X  hubs before and after applying similarity adjustment. It can be seen that for the majority of data sets, the adjustment scheme greatly reduces the amount of erroneous retrieval caused by  X  X ad X  hubs.

To illustrate the improving effect of the adjustment scheme on the precision of retrieval, Fig. 7 plots, for several data sets from Ta-ble 1, the precision of 10-fold cross-validation against the varying number ( m ) of documents retrieved as results.

Moreover, Table 2 also shows 10-fo ld cross-validation precision at 10 retrieved results, demonstrating the improvement of precision introduced by similarity adjustment on all data sets. 4
The issues examined in previous sections relate to characteristics of VSM that are existing in most of its variations, particularly the high dimensionality. To examine the generality of our findings, we consider the Okapi BM25 weighting scheme [12], which consists of separate weightings for terms in documents and terms in queries. The comparison between document and query can then be viewed as taking the (unnormalized) dot-product of the two vectors. We examine the following basic variant of the BM25 weighting. Pro-viding that n is the total number of documents in the collection, df the term X  X  document frequency, tf the term frequency, dl the document length (the total number of terms), and avdl the average document length, term weights of documents are given by while the term weights of queries are ( k 3 +1) tf / ( k 3 k , b ,and k 3 are parameters for which we take the default values k =1 . 2 , b =0 . 75 ,and k 3 =7 [12].

The existence of hubness within th e BM25 scheme is illustrated in Figure 8, which plots the distribution of N k ( k =10 )forseveral real text data sets from Table 1 represented with BM25. Figure 9 demonstrates the improvement of precision obtained through the similarity adjustment scheme described in Section 4.2, when BM25 representation is considered.
We have described the tendency, called hubness, of VSM-based models to produce some documents that are retrieved surprisingly more often than other documents in a collection. We have shown that the major factor for hubness is the high (intrinsic) dimensional-ity of vector spaces used by such models. We described the mech-anisms from which the phenomenon originates, investigated its in-teraction with dimensionality reduction, and demonstrated its im-pact on IR by exploring its relationship with the cluster hypothesis.
In order to simplify analysis by allowing quantification of the degree of violation of the cluster hypothesis, in this research we focused on data containing category labels. In future work we plan to extend our evaluation to larger data collections where rele-vance judgements are provided in a non-categorical fashion. Also, we will consider in more detail advanced models like BM25 [12] and pivoted cosine [14]. Finally, the similarity adjustment scheme described in this paper was proposed primarily with the intent of demonstrating that hubness should be considered for the purposes of IR. In future research we intend to explore other strategies for assessing and mitigating the influence of ( X  X ad X ) hubness in IR. Acknowledgments. The second author acknowledges the partial co-funding of his work through European Commission FP7 project MyMedia under grant agreement no. 215006.
