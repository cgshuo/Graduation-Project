 With the rapid development of Web 2.0, detecting topics and sentiments from the explosion of digitalized text stream of different media or different collections has become an important task. When some event happens, traditional media and social media spread the information about the event very fast. People wish to know what are the topics and their sentimental polarities, not only from the traditional media but also from many social media. There are also many product reviews collections. People want to know what are the topics and their polarities, which are valuable information when they buy some products.
 cluster, index and nd those underlying topics from large text collections. A document is assumed as a mixture of components or topics in these mixture models, whereas a topic is represented as a distribution over words. Sentiment analysis [11] aims to reveal what people think about an event or a product and so on. It focuses not only on a sentence, or a document, but also on a collection. uses the hierarchical Dirichlet process (HDP) [15] to mine topics and their cor-responding sentiment polarities from multiple collections. For example, suppose that there are two product review collections; one is about DVDs and the other is about electronic products. Using the proposed model, one can answer: { What are the main topics in these two collections? { What are the sentiment polarities towards these topics? { What are the differences among DVDs and Electronics reviews? { Determine automatically the number of underlying topics for different text { Reduce the dimensions and pre-allocations of model parameters and latent { Inference the topics and their sentiment polarities together. People can com-nally the experiments. Many researches focuses on topics and sentiments jointly, either mining or mod-eling [4, 7, 9, 10, 12, 13]. Among them, three kinds of models need to be mentioned here.
 Different Viewpoints Discovery. For any issue, there are positive and neg-ative viewpoints. Paul et al. [13] used a multi-faceted joint model to mine \for" and \against" viewpoints from opinionated text collections, such as editorials about the Israel-Palestine con ict. A document is assumed as a mixture of multi-dimensional components: a sentiment dimension and a topic (or aspect) dimen-sions. Fang et al. [5] proposed a model to regard topic and opinion as two aspects of multiple collections. Topics are the same across different text collections, while the opinions are speci c to each collection. They set the same number of topics and opinions on different collections, while it is not always the same in reality. Joint Sentiment Topic Model. Lin and He [9] proposed a joint sentiment topic model (JST) for unsupervised sentiment classi cation. It treats each com-ponent in the document as sentiment-topic-speci c. Each sentiment has the same number of topics. A model selection problem was raised whether the sentiment-topic distribution shall be modeled as per-document distribution or per-collection distribution in that paper. The subsequent works [4, 7, 10] have made different choices. In this paper, the document-level topic distributions are generated by the collection-level ones using the HDP and thus it automatically makes the suitable decision for each document.
 Topic Sentiment Mixture Model. Mei et al. [12] built a supervised model for mining sentiments associated with Weblogs. In the model a document is composed of several themes. Each theme consists of the positive, negative and neutral contents. A background component was also used to capture common words alongside these themes. They used a training collection with topic and sentiment labels for each document to learn the positive and negative sentiment models. Our model adopts the topic sentiment mixture with HDP, only uses sentiment lexicons as the sentiment prior, and treats the common words as a part of a component. A document is modeled as a mixture of several components. The number of com-ponents is determined by the underlying document collections. Instead of mod-eling a component as a Dirichlet-Multinomial for words like other topic models, our model treats the component itself as a mixture to be estimated. Each com-ponent consists of topic words, sentiment words, and some other (background) words in order to compose a human-readable document (named white noise). In our model each component consists of four parts: topics , positive words , negative words , and white noise . Each word is associated with a compo-nent , and a label to indicate a part of the component. A word in a document may serve as a topical word, a positive word, a negative word, or a white noise word. Sentiment words are recognized by a sentiment dictionary. In this case, the model can capture topics and sentiment polarity associated with topics. type of collections or things 1 . There are some assumptions for the model: { Different text collections share the same topic words with different propor-{ Different collections have their own positive and negative word distributions. 3.1 Topic-Sentiment Model for Text Collections Table 1 lists some important notations in this paper. Other notations for a model, such as the prior hyper-parameters, are omitted here for simplicity. They will be stated when they are used.
 ters. The generative process can be described as follows, 1. (a) draw a countable sequence of topics { f t g 2. draw the proportions f g for each collection and form the components f g 3. draw a Dirichlet process over the components as the top level mixture dis-4. draw a Dirichlet process over 0 for each collection c , emitting f c g 5. draw a Dirichlet process over c for each document in every collection, emit-6. for each word in every document, cabulary with hyperparameters f g . Positive and negative words speci ed by a sentiment lexicon f g . The positive words will not be generated by the negative sentimental topic and vice versa. Dirichlet prior is also applied to with hy-perparmeters &amp; . The hierarchical Dirichlet process is parameterized by ; 0 ; 1 respecting top, collection level and document level. The inference of this model consists of the posterior representation sampler of HDP [14, 15] and the resam-pling for z;l by their marginal distributions. 4.1 Experimental Setup Data Description. There are two kinds of datasets in our experiments. One dataset is the Multi-Domain Sentiment Dataset (MDS) 2 used for sentiment clas-si cation [3]. Each domain in the dataset is regarded as a document collection. DVD vs. Electronics domains and Games vs. Software domains are regarded as two multiple collections in the experiments. The detailed description is presented in Table 2. The other dataset is about two events from News and Sina Weibo in China. News collection was crawled from three Chinese news websites (Sina, iFeng, Tencent) during the end of 2011. Weibo collection was collected from Sina Weibo API using event keywords at the same time. There are two events; the rst one (\Occupy Wall Street") happened in USA in the year of 2011, whereas the second one (\Little Yue Yue") happened in the south of China in the year of 2011. The detailed information of the dataset is shown in Table 3. Both events attracted many Chinese microblog users as well as traditional news media and lasted about two weeks. This dataset will be referred as EVENT. We use it for result presentation.
 Aim of the Experiments: { whether the topics are correct for the text collections; { whether the sentiment analysis works on the text collections.
 Model Settings. The Dirichlet hyperparameter over words for each word dis-tribution is set to 0 : 01 as in [9, 10]. The Dirichlet hyperparameter &amp; is set as follows: the proportions among topic, sentiment, white noise have a uniform prior; the proportions between positive and negative have a Dirichlet( 0.5 ). This makes the model capture the positive and negative words/sentiment of a docu-ment. The HDP hyperparameters ; 0 ; 1 are chosen in range [1 ; 10] empirically. We use the emotion ontology constructed by DUTIR 3 [16] for Chinese (regarding \happy", \like" as positive sentiments and \angry", \dislike", \fear", \sad" as negative sentiments) and the HowNet's words for sentiment analysis 4 for English as the sentiment lexicons f g . 4.2 Evaluation Metrics In order to evaluate the correctness of topics mined from multiple collections, we design a human evaluation method. A topic is represented with the top 10 words. Each student was asked to give a score for each topic mined from the multiple collections: 2 for clear, valuable topics, 1 for somewhat meaningful topics, 0 for no meaning, but appear in these texts, -1 for incomprehensible ones and -2 for wrong ones. As stated in [6], negative values up to 0 suggest being a failure while positive scales suggest degree of success. They were asked to give a meaningful phrase for each topic as the golden standard. Seven students joined the evaluation process. The Krippendorff's -value[8] among the seven evaluators is 0 : 68. We use two metrics: Grade and Accuracy. Grade is the average score for each topic evaluated by students. The accuracy is calculated in the following: , where k indices topics and g k is the grade for each topic. 4.3 Topic Evaluation The evaluation results are shown in Table 4. The average grade is above zero and hence it means some degree of success. \Game vs. Software" and \Little Yue Yue" have a grade 1, which means meaningful topics. The low accuracy for \DVD vs. Electronics" is due to the fact that there are topics like \get, make, give, look, feel" and \one, time, two, little, never", which are considered incorrect by the human judges. The uniform prior among topic, sentiment, and white noise may be not suitable for that collection. For the event \Occupy Wall Street", since there are topics such as advertisements of losing weight (Table 5 No.5) , the grade is low.
 4.4 Topic-Sentiment Evaluation We use the EVENT dataset for topic-sentiment evaluation. As the golden stan-dard is not available, we represent topics and collection-speci c sentiment words using top-10 words. The results are listed in Tables 5, 6 with English translation via Google. Each row represents a topic, the right two columns are the proportion and the sentiment for each topic, where `+' for positive topics, `-' for negative topics and `X' for neutral ones for each collection. A topic k is considered to be positive (negative) in a collection c if the sum of the positive and negative proportion, c ( p + n ) ( k ) is not zero and the ratio of the positive (negative) pro-portion to c ( p + n ) ( k ) is greater than 0 : 6 (considering the estimation variance). Otherwise, the topic is considered as neutral.
 reports focused more on the factual topics, such as No.4 and No.6 with the proportion of 0.241 and 0.234 respectively, both with the neutral sentimental polarity. On the other collection, Microblogs had the positive polarity towards topics No. 3, No. 4, No. 6 and No. 7. Topics of No. 1, 5, 7 and 9 are not in the News reports since their topic proportions are 0. The last four rows are positive and negative topics mined from news and microblogs. The positive topic of Microblog is \revolution", \support" and so on. The positive topic of news reports is \support", \development", \response" and so on.
 cused on the event topic with No. 2(\incident"), No. 3(\candles") and No.7(\care") because this event was rst disseminated through the Microblogging platform. The news media focused more on topic No.9 which discussed the social legal problem. News reports had the positive polarity towards topic No. 3. The posi-tive topics of news and microblogs discussed the morality, i.e. people should help those ones who were injured in an accident. 4.5 Sentiment Evaluation In order to evaluate the sentiment detection of our model, we introduce a method to convert topic-sentiment proportions into document sentiment predictions and compare the predictions with the sentiment labels f s cj g given in the MDS. Us-ing our model, the positive proportion p cj ( pos : ) and the negative proportion p cj ( neg : ) of each document j in collection c can be calculated as: where cj ( k ) is the topic proportions and p cj ( z = k;l ) is the posterior topic-sentiment for the document. The sentiment distribution of a document is cal-culated as a summation on topics of the product of the topic proportion and the topic-sentiment. The prediction of the document sentiment label is based on ment ratio calculated by the sentimental assignments, the document is positive, otherwise it is negative.
 of negative documents is far less than the number of positive documents and the accuracy of the negative in \Game" is low. Experiments also show that the performance of \Software" domain is not affected when modeling together. The model does not propagate the label-bias problem in one collection to the other collection.
 4.6 Comparison with JST Model Both JST and our model extract topics with sentimental information; the JST in [10] nd topics under positive, negative and neutral labels, and our model nd topics with positive, negative proportions. We compare the results obtained from these two models based on the same MDS dataset.
 Sentiment Analysis Comparison. Table 8 gives the accuracy comparison between our method, a lexicon method and the JST in [10]. Our result is better than the method based on the lexicon. The number of topics in JST is always a multiple of three labels. However, in our model, the number of topics is varied. According to [10], the number of topics under each label with the best classi -cation is 15 on \DVD" but 1 on \Electronics". Our experimental result suggests the number of topics for both collections is 5. JST is designed for one collection and sets the number of topics for the sentiment classi cation while our work is proposed to choose the number of content topics in multi-collections and infer their sentiment polarities alongside.

Topic Comparison. Our model can mine topics with different sentimental polarity and positive and negative topics for each collection. JST model mined each topic with the positive and negative topics. Their topic words are mixed with sentimental words. The result of both models are quite different. However we made the experiments on the same dataset and select some mined topics for comparison according to [10]. Tables 9, 10 are some results mined from DVD and Electronics domain. The rst two lines are sentiment distributions found by our model, the third line is a topic found by our model with its sentiment polarity in brackets, and the fourth and fth lines are the topics of JST from [10]. JST models the same number of topics under the positive, negative and neutral sentiments. Our model has mined not only different positive and negative topics for them, but also 5 topics with different sentimental polarity. The advantage of our model is that the number of topics is generated automatically. In this paper, we present a topic-sentiment model for multiple text collections. A document is modeled as a mixture of components. Each component consists of topics, positive and negative proportions and white noise. The HDP is used in the model for automatic selection of the number of components (topics) and reducing the dimension of parameter space used for topic-sentiment joint mod-eling. According to the experiments on MDS and EVENT corpus: { The model can mine the topics and show different proportions of these topics { The model can mine the positive and negative topics and identify the senti-about the same event or the same type of topics. Each collection has only one positive and negative topic and many content topics. By setting different label proportion priors among topic words, sentimental words and white noise, we can get more ne-grained topics which is more suitable for the realistic world. The relationship between different priors and sentimental analysis will be explored in the future. We will do further experiments to analyze topics and sentiments from different media, different text collections about the same event.
