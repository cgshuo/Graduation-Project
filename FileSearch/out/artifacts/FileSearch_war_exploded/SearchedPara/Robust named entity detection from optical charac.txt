 ORIGINAL PAPER Krishna Subramanian  X  Rohit Prasad  X  Prem Natarajan Abstract In this paper, we focus on information extraction from optical character recognition (OCR) output. Since the content from OCR inherently has many errors, we present robust algorithms for information extraction from OCR lat-tices instead of merely looking them up in the top-choice (1-best) OCR output. Specifically, we address the challenge of named entity detection in noisy OCR output and show that searching for named entities in the recognition lattice signifi-cantly improves detection accuracy over 1-best search. While lattice-based named entity (NE) detection improves NE recall from OCR output, there are two problems with this approach: (1) the number of false alarms can be prohibitive for certain applications and (2) lattice-based search is computationally more expensive than 1-best NE lookup. To mitigate the above challenges, we present techniques for reducing false alarms using confidence measures and for reducing the amount of computation involved in performing the NE search. Fur-thermore, to demonstrate that our techniques are applicable across multiple domains and languages, we experiment with optical character recognition systems for videotext in English and scanned handwritten text in Arabic.
 Keywords Optical character recognition  X  Hidden Markov Model  X  Information extraction  X  Named entity detection Abbreviations OCR Optical character recognition HMM Hidden Markov model ROC Receiver operating characteristics 1 Introduction The confluence of large amounts of available media content in digitized form, like digitally scanned documents and dig-itized videotext images, as well as the advancements in opti-cal character recognition (OCR) technologies has provided unparalleled opportunities for information extraction from such media. At the lowest tiers of the information processing chain, the extracted information can be used to automatically index and retrieve media content. As we go higher up the chain, the indexed information can be used for any task that involves text processing, including classification and clus-tering [ 1 ], automatically finding entities [ 2 ], temporal text mining [ 3 ], and statistical machine translation [ 4 ].
Depending on the quality of the input media, variability in training and test conditions, and many other factors that affect OCR performance [ 5 , 6 ], information extraction from content generated by OCR systems can be quite inaccurate. In the presence of OCR errors, merely using the top-choice (1-best) hypothesis results in unreliable information extrac-tion, thereby limiting widespread adoption of such technol-ogy. Hence, there is a need for a robust set of tools to search for and extract information from multiple OCR hypotheses
In this paper, we focus on robust and efficient information extraction from content generated by OCR systems, such as the BBN Byblos OCR system [ 5 ]. The techniques developed in this paper can be used to extract content from any OCR system that provides a recognition lattice along with posterior scores for each word in the lattice. Specifically, we describe novel algorithms for detecting named entities (NEs) in OCR output that are more robust than simply looking up the NE of interest in the 1-best OCR output. We chose to focus on NEs because detecting NEs has use in many applications including search and real-time alerts for both intelligence and commercial domains. To demonstrate the efficacy of our NE detection approach, we process the OCR output from two different media sources  X  Arabic handwritten text and English videotext in broadcast news: 1. AHOCR (Arabic Handwritten OCR): The BBN Byblos 2. EVOCR (English Videotext OCR): The BBN Byblos The principal differences between these two systems are summarized in Table 1 . In both OCR systems, we show that we can improve recall by searching for named entities in the recognition lattice instead of searching for them in the 1-best hypothesis. Our work builds on the lattice-based methods that have been used extensively in speech recognition, used, for instance, to improve word error rates [ 7 ] and to combine outputs of multiple systems [ 8 ].

With lattice-based search, the improvement in recall comes at the cost of reduced precision; the increased number of true detects (TD) results in a larger number of false accepts (FA). To control the number of FAs, we develop posterior-based confidence estimates that result in an overall improve-ment in precision-recall characteristics.

Another aspect of searching for named entities in a lattice is the search speed. Specifically, performing a brute-force search for named entities in large lattice can be very time consuming. We explore two techniques for speeding up the search: (1) simplifying the lattice to make it easier to trace and (2) performing aggressive caching of previous search results. We show that these techniques significantly improve lattice lookup while not adversely affecting either the precision or the recall.

The structure of this paper is as follows. In Sect. 2 ,westart by describing the two corpora that were used for performing experiments using the AHOCR and EVOCR systems. We then briefly describe, in Sect. 3 , the architecture of the BBN Byblos OCR system, which forms the basis for building the AHOCR and EVOCR systems. In this section, we also high-light the differences between the AHOCR and EVOCR sys-tems. Section 4 describes the collections of named entities that were used to perform lookup. We then talk about related work done in speech recognition in Sect. 5 . The process of searching for named entities in OCR lattices is described in the Sect. 6 of this paper. In Sect. 7 , we describe techniques to reject falsely accepted NEs while retaining the NEs that were truly detected. Section 8 focuses on techniques for improving the speed of NE detection. Section 9 provides specific exam-ples where lattice-based search resulted in a true detect while the 1-best search failed to find a match due to OCR errors in the top-choice hypothesis. We conclude the paper by sum-marizing our work and mentioning some future directions for our work. 2 Corpus description As mentioned in the introduction, we evaluate our lattice-based named entity search algorithms on content produced by two OCR systems, AHOCR and EVOCR. For the AHOCR system, we use a large collection of Arabic handwritten pages [ 9 ] collected by the Linguistic Data Consortium (LDC). For EVOCR system, we used the TDT-2 corpus [ 10 ] that consists of broadcast news videos from different television channels. In both these systems, the corpus used is partitioned into three disjoint sets: training, development, and evaluation / test. The training set is used to train the hidden Markov model (HMM)-based Byblos OCR system. The development set is used to tune the system, and the test set is used to evaluate OCR per-formance. All results in this paper are reported on the test set. 2.1 LDC Arabic handwritten text corpus The corpus consists of scanned images of Arabic handwritten text from newswire articles, web log posts, and newsgroup posts, as well as their corresponding ground truth annota-tions. The scribes were chosen from different demographic backgrounds. Varied writing conditions such as the type of writing instrument (pen or pencil), type of paper (ruled or un-ruled), and writing speed (careful, normal, and fast) were introduced. The images were scanned through a high-quality scanner, at a resolution of 600 dpi. The ground truth annotations included the co-ordinates of bounding boxes around individual words and the corresponding tokenized transcriptions.

A total of 17,160 pages by 101 different scribes were used for training. Additionally, a set of 442 images gener-ated from a disjoint set of documents was used for validation purposes, which was split into development and test sets. Each of these sets contained documents written by scribes in the training set and by new scribes who were never seen in training. There was no overlap of either documents or scribes between the development and the test sets. The details of these sets are shown in Table 2 . Figure 1 shows examples of two images from the corpus. Note that the data exhibit sev-eral characteristics that make text recognition hard such as overlapping line/word boundaries, non-linear baseline within lines/words, slanted characters, scratches, and poor legibility. To the best of our knowledge, this corpus is the largest col-lection of Arabic handwritten documents with annotations. 2.2 TDT-2 videotext corpus The TDT-2 corpus is a collection of CNN and ABC news broadcasts recorded in 1998. We annotated text region boundaries and frame spans manually. Each text region con-sisted of a single line of text with possibly multiple words. A single transcription ground truth value was assigned to each text region. Approximately 7 hours of video each from CNN and ABC was manually annotated. All texts were annotated except for the moving text crawler in the CNN videos.
The text density in the TDT-2 corpus was significantly higher for CNN videos than for ABC videos: 6.6 text regions per frame versus 2.1 text regions per frame. The corpus there-fore contained significantly more CNN text data. Specifi-cally, CNN videos contained 16,719 annotated text regions and ABC videos contained 5,567 annotated text regions. We held out a fair development set containing 871 regions from CNN and 475 regions from ABC  X  none of the regions in the development set were included in the training set. The details are summarized in Table 3 . Two example images from the TDT-2 corpus are shown in Fig. 2 .
 3 Overview of BBN Byblos OCR system In this section, we describe the configuration of AHOCR and EVOCR that are built on the BBN Byblos OCR system [ 11 ], which was designed from the ground up to be script inde-pendent and language independent, thus making it possible to build both AHOCR and EVOCR with only minor mod-ifications, mostly in data pre-processing. The HMM-based Byblos OCR system can be subdivided into two basic func-tional components: training and recognition, that share a common pre-processing and feature extraction stage. In Fig. 3 , we show the block level overview of our BBN Byblos OCR system. We now proceed to describe the OCR system in more detail. 3.1 Pre-processing and feature extraction Feature extraction is the first step in both training and recog-nition. In order to convert the 2-dimensional images into a 1-dimensional sequence of observations modeled by HMMs, we determine the location of the top and bottom bound-aries of the lines of text, slice the region into a series of overlapping subimages (frames), and then compute fea-ture vectors for each subimages. If the lines are regularly spaced as in machine-printed data, HMM-based or connected component-based line-finding algorithms [ 12 ] achieve near-perfect accuracy. However, in free-flowing handwritten data, adjacent lines of text often overlap and there may be signifi-cant variations in the baseline and writing size within a text line. This hinders automatic text line segmentation and also estimation of the feature extraction frame rate, which is set to be proportional to the height of the line.

In our AHOCR experiments, we interpolate the annotated rectangular bounding boxes of individual word images to obtain a piecewise linear approximation of the envelope for the text line. For videotext data used in EVOCR, we assume that manually segmented videotext lines are available. Detec-tion of these lines from video frames would otherwise has been the first step in feature extraction.

Pre-processing of videotext images involves two key steps that are different from the processing of document images. The first step is to up-sample the videotext region by a fixed factor (typically a factor of 4). The up-sampling is performed to mitigate the effect of low resolution of videotext. We then binarize the image by thresholding pixel intensity. The threshold on intensity is determined empirically on a devel-opment set as a percentile of the intensity of each frame.
Following line-image extraction, the rest of the steps required for feature extraction follows identically in both AHOCR and EVOCR. The left and right boundaries of the word are not used for feature extraction. The line image is segmented into a sequence of thin, overlapping vertical windows called frames. A total of 129 of the following script-independent features were extracted from each frame: per-centiles of intensity values, angle, correlation, energy (PACE) and GSC (gradient, structure, and concavity) [ 13 ] features. GSC features are symbolic, multi-resolution features that combine three different attributes of the shape of a character  X  the gradient representing the local orientation of strokes; structural features that extend the gradient to longer dis-tances and provide information about stroke trajectories; and concavity that captures stroke relationships at long distances. For AHOCR, linear discriminant analysis (LDA) was applied to reduce the dimension of the feature space from 129 to 15. This set of transformed features is called PACE+GSC and is described in detail in [ 11 ]. For EVOCR, linear discrimi-nant analysis (LDA) was applied to only the 33-dimensional PACE features (no GSC) and the dimensionality of the fea-ture space was then reduced from 33 to 15 using LDA. 3.2 Training We used a 14-state, left-to-right HMM to model each individ-ual character, be it English characters (AHOCR) or Arabic characters (EVOCR). Each state of the HMM has an out-put probability distribution over the features modeled as a Gaussian mixture. The maximum likelihood estimate of the parameters of the HMM is obtained by iteratively aligning the sequence of feature vectors with the sequence of character models using the expectation maximization algorithm.
For handwritten Arabic, the shape of the character glyph often varies depending on the characters that precede and follow it. Such context dependence of glyphs is typical of cursive connected scripts, but can vary even more widely because of a writer X  X  personal style. Context-dependent HMMs offer a robust, data-driven approach for modeling contextual information. In [ 14 ], we showed the superiority of context-dependent models over context-independent models for Arabic text. We trained position-dependent tied mixture (PDTM) HMM models [ 14 ], where a separate set of Gaus-sians is estimated for each state of all the context-dependent HMMs associated with a particular character. In total, 339K Gaussians were trained for 176 unique characters (includ-ing Arabic characters, numerals, punctuations, and English characters).

For videotext in English (EVOCR), the shape of the char-acter glyph is independent of the characters that precede and follow it. As a result, for this system, we trained character-tied mixture (CTM) HMM models [ 14 ] in which all states of all context-dependent HMMs associated with a character share a single set of Gaussians. Including the punctuations and numerals, the recognition lexicon consisted of 86 char-acters. Each character HMM was associated with a mixture of 512 Gaussians for modeling the output feature distribution at each state. 3.3 Recognition The BBN Byblos recognition engine performs a two-pass search (see below for more details) using glyph HMMs and a language model (LM) [ 15 ]. A trigram language model trained on 90 million words of Arabic newswire data was used for recognition in AHOCR. For this system, the decoding lex-icon consisted of the 92K most frequently occurring words in our Arabic text corpus. The out-of-vocabulary (OOV) rate of the test set measured against the 92K lexicon is 7.5%. In EVOCR, a trigram character LM was trained using all the data available for training the glyph models. The reason we use character LMs for videotext is because it had limited data for training a word-based system and also to reduce the OOV. Also, higher order n -grams are not used for decoding due to the limitations of the decoder. Higher order n -grams are typically used for rescoring.

In the Byblos recognition engine, the forward pass uses a fast match beam search algorithm using the HMMs and an approximate bigram language model [ 16 ]. The output of the forward pass consists of the most likely word-ends per frame. The backward pass operates on the set of choices from the forward pass to restrict the search space and an approximate trigram language model to produce an n -best list or a lattice of hypotheses. The n -best list (or the lattice) is then re-ranked using a combination of the glyph and language model scores. The weights for re-ranking were tuned on the development set.

The word error rate (WER) for the AHOCR was 34.1% on the test set of the LDC Arabic Handwritten text cor-pus. For WER computation, all punctuations and digits were stripped from both the recognition results and the reference transcripts. The character error rate (CER) for EVOCR was 17.3% on the test set of the TDT-2 English videotext corpus. 4 Named entity data description For AHOCR, documents in the test set comprising of a total of 869 sentences were manually annotated for three types of named entities: person, organization, and geographical loca-tion. The details of the named entities that were annotated in these sentences are shown in Table 4 . In addition, the aver-age length of a named entity of interest was 1.5 words with a minimum length of 1 and a maximum length of 5.

For EVOCR, we did not categorize each NE but sim-ply identified 306 unique named entities in 1336 videotext images from the test set of the TDT-2 corpus. There were 415 NEs in total. 5 Lattices in speech recognition In this paper, we apply techniques from speech recognition research to improve named entity detection performance in OCR. The principle idea is to use decoder lattices instead of n -best hypothesis to expand the search space for named entity detection. Lattices are directed graphs that compactly represent a large set of decoder hypotheses. Each link in the lattice is labeled with HMM and language model likelihoods together with start and end times of a particular word hypoth-esis. Another idea is to use word confidences for improving detection performance. Using these scores and the forward-backward algorithm [ 17 ], we can compute the link posteriors for each link and then the word posteriors. In [ 17 ], the word posteriors were incorporated into the decoding process to improve recognition accuracy. In this paper, we use word (or character) posteriors to compute confidence scores for a detected named entity and use these confidence scores to reduce false alarms. For improving search speed, we collapse links that have the same start and end times. The result is a lattice that is smaller than the original lattice but still provides more search context than a fully collapsed lattice [ 18 ]. 6 Named entity detection using lattices Spotting the occurrence of known or pre-defined named enti-ties in electronic text data is a straightforward task. In con-trast, spotting such occurrences in automatically generated transcriptions of speech data or videotext data is compli-cated due to the inevitable presence of errors in the tran-scriptions. While n -best lists can increase the accuracy of name spotting, they suffer from the drawback that if one portion of a multi-word named entity is correctly recognized in one of the N hypotheses and the remainder is correctly recognized in a separate hypothesis, then the spotting algo-rithm does not have access to the entire correct hypothesis even though the required information is contained within the n -best hypothesis. Recognition lattices provide a way to over-come this limitation without limiting the vocabulary size.
While searching for a NE in a word or character lattice, we are essentially looking for a path in the lattice with the sequence of words or characters comprising of the NE that is being searched. We used the depth-first traversal technique to search for the NE in the lattice. Our first experiments for lookup used an exact string match criterion. In this proce-dure, we search for a path in the lattice whose arcs have the exact same sequence of words or characters as is present in the NE. In order to ensure that substrings do not result in a match (causing a FA), we begin and end our search at bound-ary tokens comprising of either the space token or the lattice start and end tokens.

However, analysis of the recognition errors revealed that a significant fraction of errors occurred due to punctuation insertions, effectively increasing the number of misses when performing NE search using the exact match criterion. To improve recall, we implemented a soft match criterion (also called  X  X pproximate X  match). In this procedure, we do not use, in other words ignore, punctuations tokens that were encountered during lattice traversal. The decision of a hit or a miss is solely based on the non-punctuation tokens encoun-tered during lattice traversal.

In Table 5 , we compare searching for NEs using the exact and approximate match criterion for the 1-best hypotheses (1-best), 100-best hypotheses (100-best), and the decoder lat-tice (lattice) using the EVOCR system. For each of these three hypotheses sets, we see that using the approximate match criteria improves recall while hurting precision. We see that the recall progressively increases while the precision rapidly decreases when using the 1-best, 100-best, and the lattice. Between the 1-best and the lattice, and using the approximate search criterion, while the recall increased by 31.4%, the pre-cision fell by an astounding 77.5%. For AHOCR, from the first two rows of Table 6 , we see similar precision-recall char-acteristics upon moving from searching for NEs in the 1-best to searching for them in the lattice X  X oth using the approxi-mate search criterion. The recall increased by 28.3%, while the precision fell by 48.1%. In the next section, we will exper-iment with different rejection schemes to retain the gains in recall while detecting fewer FAs, thus controlling the fall in precision. 7 Reducing false alarms From Table 5 and first two rows of Table 6 , we observe that as we search for NEs in larger search spaces, we not only increase the number of truly detected (TD) NEs but also increase the number of false accepts (FA).

In speech recognition, posterior probabilities have been shown to be effective in rejecting out-of-vocabulary (OOV) words [ 20 ]. We use the same principle for reducing false alarms in NE lookup. In our approach, we used word or char-acter (generically referred to as a token) posteriors as fea-tures for building a binary classifier to label NEs that are truly detected and falsely accepted. For each detected NE, we compute a confidence score that is based on the token posterior scores on the arcs of the lattice path where the NE was detected. The posterior scores are computed as described in [ 17 ]. Let the detected NE, d ,have N d tokens, and the pos-terior score for each token be p d i , i  X  N d . We consider three ways of computing the confidence score, c d , for a detected NE: 1. Minimum: c d = min ( { p d i | i  X  N d } ) 2. Average: c d = avg ( { p d i | i  X  N d } ) 3. Median: c d = med ( { p d i | i  X  N d } ) The confidence score is used to determine whether the detected NE was truly detected or falsely accepted. This is done by comparing the confidence score of a detected NE against a threshold,  X  . If the confidence score for a detected NE is higher than  X  , then the detected NE is classified as a truly detected, else falsely accepted. The choice of  X  is based on our willingness to trade-off reduction in recall for increase in precision. At one extreme, we may choose a very high threshold that is exceedingly successful in controlling FA but also rejects most or all of the TDs. At the other extreme, we may choose a very low threshold that does not do a good job at controlling the FAs but is able to find most of the NEs. The receiver operating characteristics (ROC) curves in Figs. 5 and 6 compare the performance of the three func-tions used to compute confidence scores for AHOCR and EVOCR, respectively. The performance was measured using lattice-based lookup with approximate match criterion. We see that the performance of classifiers using the median and average functions is essentially tied. We also see that these two classifiers perform better than the classifier using the minimum function. From comparing the first and third rows of Table 6 , we see that when we compare the performance of NE detection using the lattice-based search followed by rejection (LSR) scheme as opposed to simply using 1-best hypothesis lookup (OHL) scheme, the performance of the LSR scheme is better, resulting in 52 additional truly detected named entities for the same number of false accepts. From Table 7 , when we compare the performance of NE detection using the LSR scheme as opposed to simply using OHL, unlike for AHOCR, the NE detection perfor-mance of the two approaches is essentially tied with the LSR scheme resulting in 3 additional truly detected NEs. This indicates that if we want a very low FA, then the confidence scores of both the truly detected and falsely accepted NEs are high, and the objective function is ineffective at discriminat-ing between truly detected and falsely accepted NEs. From Table 7 , we also see that when we compare the performance of NE detection using the LSR scheme as opposed to sim-ply using 100-best hypothesis lookup, the performance of the LSR scheme is better and results in 11 additional truly detected NEs.

We believe that the difference in relative improve-ment obtained from lattice-based search over n -best search between AHOCR and EVOCR can be attributed to the quality of lattices produced for the NE detection task. Ideally, for robust NE detection, OCR lattices should contain the cor-rect sequence of characters/words and should have better posterior probabilities for the correct NE than incorrect NEs. One difference between AHOCR and EVOCR experiments which is likely to impact the desired quality of the OCR lat-tices is that AHOCR used a word lexicon (and a word n-gram LM) for decoding, whereas the EVOCR used only a charac-ter lexicon decoding. Furthermore, in our AHOCR exper-iments, all entities being searched for are in the decoding lexicon. While character-based decoding offers generaliza-tion to new terms, using a word lexicon that contains entities of interest is clearly better for generating OCR lattices with the characteristics described above. 8 Improving speed of lattice-based NE search Searching for NEs in lattices is computationally intensive and hence time consuming. In this section, we discuss some of the techniques that we have explored to speed-up search for NEs in dense lattices. Unless otherwise mentioned, these speed-ups are primarily focused toward improving NE search using approximate match criterion. We also present experi-mental results comparing the trade-offs in search speed ver-sus detection accuracy using these techniques. All speed-up improvements were developed for EVOCR, and we will only present results in this section for EVOCR. The algorithms used for speedup are generally applicable to other OCR sys-tems, including AHOCR.

In the following sections, we describe three techniques that we explored for speeding up lattice-based search. 8.1 Node clustering (NCl) As mentioned in Sect. 3.3 , we use a trigram LM in the decoder backward search. During the backward search, in order to ensure unique path histories, all merged nodes are expanded before traversing the lattice. Consequently, the lattice has many more copies of nodes and arcs than would be needed if we were to re-score using a bigram or unigram LM. Once we have computed token (character or word) posteriors using the glyph and language model scores with the forward-back-ward algorithm, we can collapse multiple copies of arcs and nodes for the purposes of looking up NEs. In doing so, we essentially perform three sequential operations on the lattice: merge nodes, merge arcs, and prune arcs that contain a char-acter which can be ignored during search. The first two oper-ations can be used to speed up the lookup with both exact and approximate match criteria, whereas the third operation is only useful for approximate match. After performing the three operations, we end up with a more compact lattice with significantly smaller number of arcs and nodes. We refer to this lattice as the node-clustered (NC) lattice. 8.1.1 Merging nodes The first step involves merging nodes that have the same position in the lattice. The algorithm to merge nodes is as follows. Let { n c } be a set of nodes in the lattice with the same start and end positions. We then perform the following operations for these set of nodes: 1. Create a new node n p . 2. For each node n  X  X  n c } ,let { a i }= set of incoming arcs 3. Re-route each { a i } through n p so that they are all incom-4. Re-route each { a o } through n p so that they are all outgo-5. Delete all nodes n  X  X  n c } . 8.1.2 Merging arcs The merging of nodes with the same position can create mul-tiple arcs between pairs of nodes with the same word label. In this step, we merge multiple arcs with the same word label. The algorithm used to merge arcs is as follows: 1. For each pair of connected nodes i and j : 2. End-for in Step 1. 8.1.3 Pruning  X  X o not care X  arcs If we are searching with the approximate match criteria, we can further simplify the lattice resulting from the transfor-mations described in Sects. 8.1.1 and 8.1.2 . When searching using the approximate match criteria, we ignore punctuations and other tokens, a set denoted by L , that we may encounter during lattice traversal. We can further speed-up search by replacing multiple arcs with tokens in L between the same pair of nodes with a single arc: 1. For each pair of connected nodes i and j : 2. End-for in Step 1. 8.2 Node caching (NCa) While describing the process of searching for NEs in the lat-tice, we mentioned that we use depth-first lattice traversal for our lattice-based search. In performing depth-first tra-versal with approximate match criterion, it is possible to tra-verse a node with the same partial match (of a particular NE) multiple times. However, the partial match at any given node is going to remain the same, irrespective of whether we reached that node in one single hop from a predeces-sor node or multiple hops, which included a sequence of  X  X o not care X  characters. Therefore, we maintain a record of the partial match at any given node in the lattice. Before traversing to a new node, we check to see if the partial match thus far has already been cached in the new node. We prop-agate to the new node only if the node has not been cached. After entering the node, we update the cache for future tra-versals. 8.3 Lattice pruning (Pr) The size of the lattice produced by the videotext OCR decoder directly impacts the lookup speed. Therefore, we also studied the impact of the size of the lattice produced by the decoder on speed and accuracy of NE lookup. The Byblos OCR decoder has the capability to reduce the size of generated lattice either during creation or after computing the posteriors using the forward-backward algorithm [ 21 ] and eliminating arcs with posteriors below a prescribed threshold. 8.4 Experimental results In Table 8 , we list different speed-up configuration options that we tried and for each option, we sampled the recall at three different points along the ROC curve, denoted by FA = 4, FA = 10, and FA = 131. For each configuration, we also list the amount of time it took to perform lattice-based search for NEs as a fraction ( f ) of the time it took to per-form lattice-based search without any speed improvements. We used EVOCR to perform these experiments. The approx-imate search criterion was used with each configuration.
Performing node clustering alone without node caching or significant lattice pruning does not degrade recall perfor-mance but significantly improves search speed by a factor of 87%. Performing node caching alone again does not degrade recall performance but gives a more modest speedup of 20%. When performing lattice pruning alone slightly improves recall performance while increasing search speed by 50%. By using all the three speed-up techniques, we get a slight hit in recall, especially for high FA (FA = 131) regions, but improve the search speed by 94%. 9 Specific examples of improved NE detection performance In this section, we show some examples in which search-ing for a NE in the lattice resulted in a true detect but the same search failed when looking up the NE in the 1-best hypothesis. In Fig. 7 , for EVOCR, the named entity  X  X ICH-ARD BORRECA X  could not be detected because the 1-best hypothesis had recognition errors. Three errors were made by the recognition engine: 1) a deleted space between  X  X  X  and  X  X  X , 2) substitution of  X  X  X  in BORRECA with  X  X  X , and 3) deletion of one of the  X  X  X  X  in BORRECA. But this named entity was successfully detected upon searching for it in the recognition lattice. The lattice was searched using the approximate search criterion and using the average confi-dence score. The rejection threshold level was chosen so that after rejection, we have the same FA for the lattice lookup as we had with the 1-best hypothesis.

Similarly, in Fig. 8 , we show three instances where a NE lookup using the 1-best hypothesis failed to result in a match but the NE was truly detected upon looking for it in the recognition hypothesis. The lattice results were obtained after filtering the detected NEs using the average confidence score at a threshold point, which results in the same precision for the lattice lookup as the 1-best lookup. 10 Summary and future work In this paper, we motivated the need to develop robust algo-rithms to perform information extraction from the content that was generated by an OCR system. We presented novel algorithms for lattice-based search for a pre-defined set of named entities as a robust alternative over simply looking up NEs in the top-choice OCR output. Since lattice-based search usually improves NE recall at the cost of increasing false alarms, we developed techniques to control the number of false accepts by filtering detected NEs based on confidence scores. Furthermore, to reduce the number of paths traversed in lattice-based NE search, we presented three different tech-niques, two of which simplify the lattice by reducing the number of nodes and arcs and the third that improves search performance by aggressively caching previous search states. Together, the three techniques helped to reduce lookup time by 94% relative, without significantly affecting precision-recall characteristics.

To demonstrate the efficacy of the algorithms described in this paper, we experimented with two different media types: (1) document images containing Arabic handwritten text and (2) overlaid text in English broadcast news. On both media types, searching for NEs in the lattice resulted in a larger number of matches than was possible when looking up NEs in the top-choice OCR output alone. However, the cost of finding larger number of NEs, in terms of the larger number of FAs that remain even after rejection, was lesser for the Arabic handwritten text system than for the English video-text system. We believe this difference can be attributed to the difference in the quality of the OCR lattices between the two media types.

This paper described algorithms for detecting known or pre-defined named entities in OCR output. Our future work will focus on using OCR lattices for improved detection of new named entities, which requires integrating linguistic and other contextual information in the search process. References
