 1. Introduction In the arts, as in other spheres of creative and sporting endeavor, popularity should not be confused with prestige. Topping the bestseller lists will not greatly affect an author X  X  chances of winning the Nobel Prize for literature, nor is a
Hollywood blockbuster that breaks box office records likely to land the Palme d X  X r at Cannes. Similarly, impressive auc-tion house sale prices are no guarantee that MoMA or Tate Modern will acquire an artist X  X  work. Popular appeal and peer esteem are not synonymous, as sociologists of culture and others have noted (e.g., English, 2005 ). Things, of course, are not that different in the symbolic capital markets of academia ( Bourdieu, 1988; Cronin, 1999; Cronin &amp; Shaw, 2002 ).

Bollen, Rodriguez, and Van de Sompel (2006) distinguished between scholarly popularity and prestige. They compared journal rankings resulting from a weighted PageRank metric (prestige) with those obtained using the impact factor (pop-ularity) (see also Franceschet, 2010 ). In this paper we focus primarily on authors rather than journals. The popularity of a social actor (artist, pianist, scholar) can be defined as the total number of endorsements (acclaim, applause, citation) re-ceived from all other actors and prestige as the number of endorsements coming specifically from experts (see Bollen et al., 2006, p. 2 ). Bibliometrically, popularity can be operationalized as the number of times an author is cited (en-dorsed) in total, and prestige as the number of times an author is cited by highly cited papers. A scholar may be popular but popularity does not necessarily equate with prestige, though on occasion there may well be a strong positive cor-relation between the two measures. For a thoroughgoing review of the concepts of prestige, prestige hierarchies and prestige scales, as well as related notions such as esteem, charisma, hierarchy and status, the reader is referred to Wegener (1992) .

In the vernacular, it is not how often one is cited but by whom; that is to say, a citation from a Fellow of the Royal Society would for most of us carry more weight than one from a doctoral student. Likewise, a citation coming from an obscure paper probably would not be granted the same weight as a citation from a groundbreaking article ( Bollen et al., 2006; Maslov &amp;
Redner, 2008 ). Here we take the quality of citing articles into consideration in assessing the standing of researchers, using information retrieval as our test site.

In the present study, the popularity of a researcher is measured by the number of times he is cited by all papers in the same dataset; the prestige of a researcher by the number of times he is cited by highly cited papers in that dataset. Popularity and prestige are differentiated on the basis of the presumptive quality of citations. We show how scholars X  popularity and prestige rankings change over time. We also explore the relationship between popularity and prestige and variables such as date of Ph.D. degree award, receipt of honors/prizes, number of key publications, organizational affiliation, and gender. The paper is organized as follows. Section 2 discusses related work on citation analysis and research evaluation. Section 3 de-scribes the methods we used to calculate popularity and prestige. Section 4 analyzes changes in scholars X  popularity and prestige rankings over time. Section 5 links popularity and prestige with other variables. In Section 6 we summarize our find-ings and suggest possible future work. 2. Related work
Quantitative measures of research impact have been used since the early 20th century ( Garfield, 1999 ). Cason and Lubot-skyt (1936) employed journal-to-journal citation analysis to measure the dependence of journals on each other. Pinski and
Narin (1976) developed a citation-based technique to measure the influence of scientific journals, subfields, and fields. They calculated the eigenvalue of a journal cross-citing matrix as a size-independent influence weight for journals. Impact factors have been used to determine the standing of journals ( Bordons, Fernandez, &amp; Gomez, 2002; Garfield, 1999; Harter &amp; Nison-ger, 1997; Nederhof, Luwel, &amp; Moed, 2001 ), and the same principle has been used to measure the impact of web pages ( Smith, 1999; Thelwall, 2001 ). The h-index and variants thereon have been employed to assess the performance of research-ers ( Hirsch, 2005; Cronin &amp; Meho, 2006; Sorensen, 2009 ). Other more or less novel approaches to citation analysis continue to emerge (e.g., Jin, Liang, Rousseau, &amp; Egghe, 2007; Redner, 1998; Sidiropoulos, Katsaros, &amp; Manolopoulos, 2007 ).
Straightforward counting  X  the number of times a particular author, paper, journal, institution, country has been cited  X  is the most basic approach. Riikonen and Vihinen (2008) stress the importance of simple citation counting having examined the effects of assigning differential weights to citations. There are also more advanced techniques to determine a scholar X  X  influence on a particular field or intellectual community, for example, author co-citation analysis (e.g., White &amp; McCain, 1998 ), social network analysis ( Newman, 2001; Yan &amp; Ding, 2009 ), and PageRank ( Ding, Yan, Frazho, &amp; Caverlee, 2009 ).
Recently, for instance, Sorensen (2009) applied citation analysis to post-1984 research on Alzheimer X  X  disease. Based on data extracted from PubMed and Thomson Reuters X  Web of Science, the top 100 Alzheimer X  X  investigators were identified and their h-indexes calculated. Sorensen then highlighted those scientists on his list who had won either or both of the two most prestigious Alzheimer X  X  research awards. Riikonen and Vihinen (2008) examined the productivity and impact of more than 700 biomedical researchers in Finland from 1966 to 2000. Their study showed that actual publication and citation counts were better indicators of the scientific contribution of researchers, disciplines, or nations than impact factors. Cronin and Meho (2007) explored the relationship between researchers X  creativity (production of key papers) and professional age in the field of information science, but they, like others, did not take into account the quality of citing articles in their analysis.

Pinski and Narin (1976) proposed giving greater weight to citations coming from a prestigious journal than to citations from a peripheral one, an approach also suggested by Kochen (1974) . Habibzadeh and Yadollahie (2008) granted greater weight to citations if the citing journal had a higher impact factor than that of the cited journal and then calculated the weighted impact factor to better measure the quality of journals. Bollen et al. (2006) proposed a weighted PageRank algo-rithm to obtain a metric of prestige for journals, and found significant discrepancies between PageRank and impact factor.
They defined popular journals as those cited frequently by journals with little prestige, and prestigious journals as those with citations coming from highly influential journals. Popular journals normally have a high impact factor but a low weighted
PageRank, while prestigious journals have a low impact factor but a high weighted PageRank. Bollen et al. argue that the impact factor is a measure of popularity not of prestige and in so doing they have challenged the status quo ( Al-Awqati, 2007 ). It is also worth noting that although researchers have begun to take account of the differential coverage of databases
In an effort to address this deficiency we here use weighted citation counts as a means of distinguishing between scholarly popularity and prestige.

The basic units of measurement in bibliometrics are authors, papers, and journals. Straightforward citation analysis is a very convenient but also somewhat crude method: the strengths and limitations of the journal Impact Factor, for in-stance, have been debated extensively and reviewed thoroughly by Bensman (2007) . Most studies do not distinguish be-tween scholarly popularity (reflected in raw citation counts) and prestige (reflected in weighted citation counts). The difference between prestige and popularity at the journal level has been little addressed in the literature; notable excep-tions are an early paper by Pinski and Narin (1976) and more recently a detailed proposal by Bollen et al. (2006) . Very few researchers have applied these kinds of approach to the author and paper levels. Here, we describe in detail how weighted citation counting at the author level can be applied in order to differentiate between scholarly prestige and popularity.
 3. Methods 3.1. Data collection
We chose information retrieval as our test field as both of us have some familiarity with the domain and the actors. This is an interdisciplinary field, one that brings together scholars from information science and computer science in particular. It is also a field that draws upon techniques and tools from a number of other areas. Our sample contains many individuals who are recognizably mainstream researchers in IR (e.g., Harman, Robertson, Saracevic) and others who are associated with more or less cognate fields (e.g., Chen, Kohonen, Stonebraker).

Papers and their cited references were harvested from Web of Science (WoS) for the period 1956 X 2008. Search strategies were based on the following terms (including plurals and variants) which were determined by checking Library of Congress Subject Headings and consulting several domain experts: INFORMATION RETRIEVAL, INFORMATION STORAGE and RETRIE-VAL, QUERY PROCESSING, DOCUMENT RETRIEVAL, DATA RETRIEVAL, IMAGE RETRIEVAL, TEXT RETRIEVAL, CONTENT BASED
RETRIEVAL, CONTENT-BASED RETRIEVAL, DATABASE QUERY, DATABASE QUERIES, QUERY LANGUAGE, QUERY LANGUAGES, and RELEVANCE FEEDBACK. In total, 15,370 papers (henceforth the IR paper dataset) with 341,871 cited references (hence-forth the IR cited references dataset) were collected. The citation records comprised first author, year, source, volume, and page number. The dataset is split into four time periods: phase 1 (1956 X 1980), phase 2 (1981 X 1990), phase 3 (1991 X 2000), and phase 4 (2001 X 2008). 3.2. Measures of popularity and prestige
We measured the popularity of a researcher by the number of citations he received over time. For example, if researcher A was cited 50 times by papers published prior to 1980, his popularity for that period was 50. We measured a researcher X  X  prestige by the number of citations he received from highly cited papers. For example, if researcher A received five citations from highly cited papers published prior to 1980, his prestige score for that period was five (see Fig. 1 ). 3.3. Prestige calculation 3.3.1. Step 1: identify highly cited papers from the IR cited references dataset
We identified a subset of highly cited papers from the IR cited references dataset for each time period. The subset contains roughly 20% of the total citations for each period: 2379 highly cited papers (papers cited more than once) for 1956 X 1980, 4243 (papers cited more than twice) for 1981 X 1990, 24,487 (papers cited four or more times) for 1991 X 2000, and 46,209 (papers cited five or more times) for 2001 X 2008. We sought to maintain the same ratio (roughly 20%) for each period for the sake of comparability, given that the time periods contain very different numbers of citations. For example, if we had defined highly cited papers as papers cited four or more times, we would have ended up with only 75 records for 1956 X  1980 but 23,487 for 1991 X 2000. Moreover, papers cited four or more times in 1956 X 1980 may be qualitatively different than those cited equivalently in 1991 X 2000; as the number of publications grows exponentially, the probability of citation increases. 3.3.2. Step 2: match highly cited papers against the IR paper dataset
The first author name, publication year, volume and beginning page fields were used to match the highly cited papers against the IR paper dataset. Ultimately, 85 matches were recorded for 1956 X 1980, 136 for 1981 X 1990, 478 for 1991 X  2000, and 875 for 2001 X 2008.
 3.3.3. Step 3: collect cited references in the matched papers and store them in the core cited references datasets
We collected 1603 cited references from the 85 highly cited papers for 1956 X 1980; 3388 from the 136 papers for 1981 X  1990; 18,928 from the 478 papers for 1991 X 2000, and 35,305 from the 875 papers for 2001 X 2008. 3.3.4. Step 4: calculate the number of times each author has been cited in the core cited references datasets
The prestige rankings of authors for the period 2001 X 2008 were calculated based on 35,305 cited references in the core cited references dataset  X  the number of times authors have been cited by the highly cited papers in this period. The process was identical for the three earlier time periods. Fig. 2 illustrates the steps involved in generating measures of prestige. 4. Results and discussion 4.1. Dynamics of popularity The left side of Table 1 shows the top 40 ranked IR authors (in bold) in terms of popularity for each of the four time bands.
Unsurprisingly, it is hard to maintain a continuous presence in the top 40 for 50 plus years. Many authors appeared once (e.g., Ingwersen P [NA-104-19 -14193], Tahani V [232-37 -939-2793]), several twice (e.g., Date CJ [389-14 -33 -389], Stoneb-44 -431], Bates MJ [901-39 -9 -38 ]). Four were continuously present (marked in bold in Table 1 ): Salton G (1927 X 1995),
Van Rijsbergen CJ, Robertson SE, and Jones KS (1935 X 2007). Each of these authors has made fundamental contributions to the field; at the risk of over-simplifying, the SMART system, theoretical models of IR, probabilistic searching model, and in-verse document frequency, respectively. Moreover, three won the Gerard Salton Award, named after the doyen of the field who, coincidentally, ranked top in both prestige and popularity across all four time periods.
 Turnover is only to be expected. There are new entrants such as Spink A (NA-NA-22 -6 ), Flickner M (NA-NA-37 -11 ), Chen
HC (NA-NA-37 -36 ), Rui Y (NA-NA-194-2 ), Baeza-Yates R (NA-NA-593-4 ), and Smith JR (NA-NA-49-5 ). Flickner M, Rui Y, Bae-za-Yates R and Smith JR currently work in industry (Yahoo!, IBM, Microsoft, respectively), while Spink A and Chen HC are in academia. Most received their Ph.D. in the 1990s (three did not have a terminal degree), and typically spent 10 X 20 years working in the area before reaching the upper echelons (see Cronin and Meho (2007) on timelines of creativity in informa-tion science). Some authors X  rankings are declining, for example, McCarn DB ( 22 -175-2559-19223) and Doyle LB ( 40 -196-577-4805). Some have left the field, retired, or died: Frome J ( 9 -NA-NA-NA), Kent A ( 4 -610-875-4822), Williams ME ( 5 -46-555-2188) and Janda K ( 32 -NA-NA-NA), for example.
 Information retrieval is a dynamic field. Only four authors were ranked in the top 40 for the entire period (Salton G, Jones
KS, Van Rijsbergen CJ and Robertson SE). Among the top 40 ranked authors in phase 1, 16 kept their ranking in phase 2, 10 in phase 3, and five in phase 4. Among the top 40 in phase 2, 19 maintained their ranking in phase 3, and eight in phase 4. In the 40% of the authors in the top 40 were new entrants in each phase. 4.2. Shifting measures of esteem
The right side of Table 1 shows the top 40 authors ranked in terms of prestige. Ten had a continuous presence. This group included the four authors who were continuously ranked in the top 40 for popularity. The six other individuals (and their broadly defined areas of expertise) were: Lancaster FW for IR evaluation, Cooper WS for IR evaluation, Bookstein A for index-ing theory, Swanson DR for medical IR, Cleverdon CW for IR evaluation, and Harter SP for probabilistic indexing. Some authors maintained their membership of the top 40 cohort for 10 years (e.g., Hillman DJ [ 22 -349-459-920], Harper DJ [NA-16 -74-141], Tahani V [570-24 -219-674]), some for 20 years (e.g., Marcus RS [ 22 -42-82-340], Luhn HP [ 19-38 -153-303], Ra-9 ], Fuhr N [NA-NA-30 -17 ]), while others were fading (e.g., Summit RK [ 22 -410-1294-3612], Hawkins DT [ 13 -464-569-2153], Padin ME [ 36 -275-1359-NA]). Some names disappeared from the rankings (e.g., Bello F [ 31 -NA-NA-NA], Rubinoff M [ 36 -NA-NA-NA], Standera O [ 22 -NA-NA-NA]).

Overall, the prestige rankings were more stable than the popularity rankings. Ten authors were continuously ranked within the top 40 for prestige (see the right side of Table 1 , names in bold). Of the top 40 ranked authors in phase 1, 18 fea-tured in phase 2, 14 in phase 3, and 10 in phase 4. Of the top 40 authors in phase 2, 21 maintained a presence in phase 3, and 15 in phase 4. Of the top 40 in phase 3, 26 maintained a presence in phase 4 (see the right sides of Table 2 and Fig. 3 ). As a general rule, once an author is ranked high on prestige, i.e., is highly cited by important IR researchers, he tends to maintain his ranking for some time. 4.3. Popularity vs. prestige Popularity and prestige exist in the following possible relations: High popularity and high prestige.
 High popularity and low prestige.
 Low popularity and high prestige.
 Low popularity and low prestige.

Gerard Salton is a singularity in that he is consistently ranked highest in terms of both prestige and popularity. (The Feb-ruary 1996 issue of the Journal of the American Society for Information Science contains an In Memoriam that captures the nat-ure of the man and his contributions.) Most of the top 10 ranked authors score highly in both the popularity and prestige atively low popularity and low prestige (within the top 40 ranked authors), such as Martin TH (popularity rank: 40 -315-3464-NA vs. prestige rank: 36 -69-395-1345). There are those whose rankings diverge. For example, people with high pres-tige rank but low popularity rank or the converse. For the period 2001 X 2008 there are many such cases: Croft WB (prestige rank 7 , popularity rank 39), Borgman CL (prestige rank 14 , popularity rank 88), Ingwersen P (prestige rank 19 , popularity rank 46), Marchionini G (prestige rank 33 , popularity rank 62); Maron ME (prestige rank 80, popularity rank 417); and Yu
CT (prestige rank 52, popularity rank 431). These authors attract a relatively high number of citations from highly cited pa-pers and a relatively low number of citations from non-highly cited papers. Conversely, some authors attract a relatively high number of citations from non-highly cited papers and a relatively small number of citations from highly cited papers. A large number of citations coming from non-highly cited papers will boost an author X  X  popularity rank. There are several such cases for the years 2001 X 2008 (see Table 1 ).

Table 3 shows the number of authors ranked within the top 40 for both popularity and prestige across the four time peri-ods. Many leading researchers were found among the top 10 in both categories across all four time periods. However, the popularity and prestige rankings of the researchers in ranks 11 X 40 differ appreciably. For example, the number of authors who were ranked high on both categories and across all time periods dropped from approximately 65% in ranks 1 X 10 to 20% in ranks 11 X 20, to 5% in ranks 21 X 30, and to 25% in ranks 31 X 40. 4.4. Validity
We tested the validity of the popularity and prestige ranks by comparing them with the rankings obtained by adding the impact factors of the journals in which the citing articles were published as weights to the raw citation counts. We limited our examination to 2001 X 2008, as this period contained the largest number of papers and citations (see Table 4 ). The Spear-
Bollen et al. (2006) , namely, that the journal impact factor measures popularity rather than prestige. It can be inferred that prestige and popularity ranks measure slightly different dimensions of peer esteem. Fig. 4 shows the scatter plots of these three different rankings, which underscores the point. 5. Popularity, prestige, and other indicators of esteem
Table 6 shows the top 40 most highly cited/most popular authors from 1956 to 2008 along with related professional information: date of Ph.D. award, degree granting institution, institutional affiliation, major awards, service to the ACM SIGIR conferences, and an indication of authors X  key contributions to the field. Almost all of the top 40 authors either work or have worked at leading universities (e.g., the University of California at Berkeley, University of Chicago, Stanford University) or research labs (e.g., IBM, Microsoft, Yahoo!). Twenty-five of these organizations are in the USA, 6 in the UK, and one each in Denmark, France, Germany, Spain, the Netherlands, Finland, China and Australia. Of the top 40 authors, 6 (15%) are female.
The top 10 individuals received their Ph.D. from illustrious institutions, five in the USA and five in the UK: Harvard Univer-sity, University College London, University of Cambridge (3), University of Illinois, University of Southern California, Case Western University, City University, and Rutgers University. The full list of degree granting institutions includes Columbia University, MIT, Princeton University and Stanford University. Five of the top 40 received their Ph.D. from the University of California at Berkeley.

Many of these authors X  work has had a significant impact on the IR field (e.g., Salton G [the SMART system], Roberston SE [probabilistic retrieval model], Van Rijsbergen CJ [IR models], Belkin NJ [IR evaluation], and Jones SK [TF/IDF X  X nverse doc-ument frequency]) or related fields (Abiteboul S [database management systems], Smith JR [multimedia retrieval, MPEG],
Codd EF [OLAP relational model], Ullman JD [database management systems], Zadeh LA [fuzzy logic], Borgman CL [scholarly communication], and Kohonen T [neural networks]). Many also served as program committee members for the SIGIR con-ferences at some point during the period 1997 X 2008: Robertson SE, Van Rjisbergen CJ, Spink A, Harman D (chair of TREC), and Voorhees EM (chair of TREC). Some of those coming from related fields served as program committee members for SIGIR (e.g., Smith JR and Ellis D) or related conferences (e.g., SIGMOD [Stonebraker M], and VLDB [Abiteboul S]).

Table 7 displays several of the major awards in information retrieval and the broader information science field: the Ger-ard Salton Award, the Tony Kent Strix Award, the ASIS&amp;T Award of Merit, the ASIS&amp;T Research Award and the ASIS&amp;T Best
Book Award. For the period 2001 X 2008, researchers ranked high in prestige have a stronger presence among the award win-list of the most popular authors for the period 2001 X 2008, while seven are featured on the list of the most prestigious authors. All the Gerard Salton Award winners, with the exception of Cleverdon CW and Dumais S, are included in Table 6 .
If Table 6 had listed the most prestigious rather than the most popular authors, Cleverdon would have been included because
Susan Dumais from Microsoft Research. She is ranked 80th on prestige and 121st on popularity for the years 2001 X 2008. Her relatively low ranking may have to do with the fact that she works in industry, with the result that her work may not appear so often in the open literature. She has a higher prestige than popularity ranking, which suggests that domain experts are cognizant of her work. The Gerard Salton Award has nine winners to date, six of whom (67%) were among the top 10 most prestigious authors and only two (22%) among the top 10 most popular authors for the period 2001 X 2008. This seems to suggest that an author X  X  prestige ranking is a better reflection of perceived scholarly significance than his popularity ranking.
We gathered data on when authors produced their most important works (see Fig. 5 ). As mentioned earlier, we defined key publications as those that had been cited at least 40 times. We also determined the date when authors were awarded their doctorate (three did not have a terminal degree). Fig. 5 shows that the majority of key publications were produced 10 X  20 years post-Ph.D., a finding that is congruent with Cronin and Meho X  X  (2007) results. Three of these were books and all three appeared in the popularity column. A comparison of the 10 most highly cited publications for the period 2001 X  2008 based on popularity and prestige found that only three articles were the same (see Table 8 ). This further suggests that measures of popularity and prestige are not interchangeable. 6. Summary and concluding remarks
Citation analysis is an established means of assessing the relative impact of a scholar X  X  research. We have described here a novel approach to citation-based evaluation of individuals that factors into account the quality of the papers that cite an author X  X  oeuvre. We measured the prestige of a scholar X  X  work in terms of citations coming from relatively highly cited pa-pers and popularity in terms of citations from all other papers. We used information retrieval as our test site and gathered all
IR papers for the years 1956 X 2008 to create our corpus. We broke the analysis down into four time bands and calculated the top 40 authors based on popularity and prestige for each period. We also gathered biographical (e.g., gender) and profes-sional (e.g., organizational affiliation) data on our sample.

The popularity rankings changed over time. Only four scholars managed to maintain a presence in the top 40 rankings for the entire period. The churn rate from one phase to the next was very roughly 40%. Most authors ranked within the top 40 for a single phase; a few for two or three. Rankings based on prestige were more stable than those for popularity. Ten authors ranked in the top 40 for prestige across all four phases. Authors who ranked high on prestige tended to keep their status for 20 or 30 years. We found that authors can rank high on prestige but not on popularity, and vice versa.

Many of the 40 highly ranked authors were affiliated with prestigious organizations  X  universities and corporate labs in the main  X  and had received their Ph.D. degrees from leading universities. They were likely to have received awards and hon-ors from the professional community. Six of the nine Gerard Salton Award winners belonged to the top 10 most prestigious authors, and only two were among the top 10 most popular authors for the years 2001 X 2008. Six females featured among the top 40 ranking authors. Typically, the top-ranked IR scholars produced their key publications approximately 10 X 20 years after completing their doctorate.

Simple citation counting has been a standard approach in first generation bibliometric research. But authors X  behaviors (e.g., citing each other or publishing together) generate various kinds of scholarly networks, for example, a paper-citation network, co-authorship network, or author co-citation network. The topology of these social network graphs should not be ignored in assessing the impact of a scholar X  X  research. For example, in a co-authorship network, authors with direct or indirect links to author A will transfer their weight to this author. Simple citation counting only calculates the number of nodes with direct links without considering the weights transferred by indirect nodes.

Both HITS (viewed as a precursor of PageRank) and PageRank use link analysis algorithms that take the link graph topol-ogy into consideration when rating web pages. When ranking one node in a graph, they consider the weights coming from link to page q, has in some measure conferred authority on q X  ( Kleinberg, 1998, p. 2 ). HITS takes into account both hub and authority; for example, the web page www.harvard.edu should have the highest authority for Harvard University. Hubs are those web pages linking to related authorities, such as web pages with large directories, that led users to other authorized pages, for example, www.dmoz.org (the Open Directory Project). PageRank is very similar to HITS and uses random surfer theory to predict the possibility of any given web page being visited. The PageRank formula consists of two parts: simple counting of nodes (similar to simple citation counting) and weight transfer based on graph topology. A damping factor is used in the formula to balance these two parts. By tuning the damping factor, emphasis can be placed on either of the two parts. For example, if the damping factor is set at low, simple node counting will play a major role in determining the PageRank score, and vice versa ( Ding et al., 2009 ).

The weighted citation counting approach being proposed here demonstrates the value of adding weights to citations so that papers cited by highly cited papers receive more weight than those cited by non-highly cited papers. However, it does not consider the graph topology of citation networks. Several researchers have shown that PageRank can capture the prestige tested this at either the author or paper level. We plan to apply the model described here to the paper level and further test the PageRank and HITS algorithms to identify novel methods for measuring popularity and prestige.
 Acknowledgment
The authors would like to thank Johan Bollen and two anonymous referees for their insightful comments on an earlier draft.
 References
