 Massachusetts Institute of Technology
A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the contain sentences not found in any of the input documents and can synthesize information across sources. 1. Introduction
Redundancy in large text collections, such as the Web, creates both problems and opportunities for natural language systems. On the one hand, the presence of numer-ous sources conveying the same information causes difficulties for end users of search engines and news providers; they must read the same information over and over again.
On the other hand, redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke,
Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). Clearly, it would be highly desirable to have a mechanism that could identify common information among multiple related documents and fuse it into a coherent text. In this article, we present a method for sentence fusion that exploits redundancy to achieve this task in the context of multidocument summarization.
 use of sentence extraction for multidocument summarization (Carbonell and Goldstein 1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy by clustering), one of these sentences is selected to represent the set. This is a robust approach that is always guaranteed to output a grammatical sentence. However, ex-traction is only a coarse approximation of fusion. An extracted sentence may include not only common information, but additional information specific to the article from which it came, leading to source bias and aggravating fluency problems in the extracted summary. Attempting to solve this problem by including more sentences to restore the original context might lead to a verbose and repetitive summary.
 sentences that are common. Language generation offers an appealing approach to the problem, but the use of generation in this context raises significant research challenges.
In particular, generation for sentence fusion must be able to operate in a domain-independent fashion, scalable to handle a large variety of input documents with various degrees of overlap. In the past, generation systems were developed for limited domains and required a rich semantic representation as input. In contrast, for this task we require text-to-text generation, the ability to produce a new text given a set of related texts as input. If language generation can be scaled to take fully formed text as input without semantic interpretation, selecting content and producing well-formed English sentences as output, then generation has a large potential payoff.
 eration technique which, given a set of similar sentences, produces a new sentence containing the information common to most sentences in the set. The research chal-lenges in developing such an algorithm lie in two areas: identification of the fragments conveying common information and combination of the fragments into a sentence.
To identify common information, we have developed a method for aligning syntac-tic trees of input sentences, incorporating paraphrasing information. Our alignment problem poses unique challenges: We only want to match a subset of the subtrees in each sentence and are given few constraints on permissible alignments (e.g., arising from constituent ordering, start or end points). Our algorithm meets these challenges through bottom-up local multisequence alignment, using words and paraphrases as anchors. Combination of fragments is addressed through construction of a fusion lattice encompassing the resulting alignment and linearization of the lattice into a sentence using a language model. Our approach to sentence fusion thus features the integration of robust statistical techniques, such as local, multisequence alignment and language modeling, with linguistic representations automatically derived from input documents. opposed to extracts (Borko and Bernier 1975), for multidocument summarization. Un-like extraction methods (used by the vast majority of summarization researchers), sen-tence fusion allows for the true synthesis of information from a set of input documents.
It has been shown that combining information from several sources is a natural strat-egy for multidocument summarization. Analysis of human-written summaries reveals that most sentences combine information drawn from multiple documents (Banko and
Vanderwende 2004). Sentence fusion achieves this goal automatically. Our evaluation shows that our approach is promising, with sentence fusion outperforming sentence extraction for the task of content selection.
 sion method within the multidocument summarization system MultiGen, which daily summarizes multiple news articles on the same event as part browsing system Newsblaster (http:/ /newsblaster.cs.columbia.edu/). In the next sec-tion, we provide an overview of MultiGen, focusing on components that produce input or operate over output of sentence fusion. In Section 3, we provide an overview of 298 our fusion algorithm and detail on its main steps: identification of common infor-mation (Section 3.1), fusion lattice computation (Section 3.2), and lattice linearization (Section 3.3). Evaluation results and their analysis are presented in Section 4. Analy-sis of the system X  X  output reveals the capabilities and the weaknesses of our text-to-text generation method and identifies interesting challenges that will require new insights. An overview of related work and a discussion of future directions conclude the article. 2. Framework for Sentence Fusion: MultiGen
Sentence fusion is the central technique used within the MultiGen summarization system. MultiGen takes as input a cluster of news stories on the same event and produces a summary which synthesizes common information across input stories. An example of a MultiGen summary is shown in Figure 1. The input clusters are automati-cally produced from a large quantity of news articles that are retrieved by Newsblaster from 30 news sites each day.
 overview the MultiGen architecture, providing details on the processes that precede sentence fusion and thus, the input that the fusion component requires. Fusion itself is discussed in the subsequent sections of the article.
 ponent of the system, Simfinder (Hatzivassiloglou, Klavans, and Eskin 1999) clusters sentences of input documents into themes, groups of sentences that convey similar information (Section 2.1). Once themes are constructed, the system selects a subset of the groups to be included in the summary, depending on the desired compression length (Section 2.2). The selected groups are passed to the ordering component, which selects a complete order among themes (Section 2.3). 2.1 Theme Construction
The analysis component of MultiGen, Simfinder, identifies themes, groups of sen-tences from different documents that each say roughly the same thing. Each theme will ultimately correspond to at most one sentence in the output summary, generated by the fusion component, and there may be many themes for a set of articles. An example of a theme is shown in Table 1. As the set of sentences in the table illustrates, sentences expressing information that is not common to all sentences in the theme. Information that is common across sentences is shown in the table in boldface; other portions of the sentence are specific to individual articles. If one of these sentences were used as is to represent the theme, the summary would contain extraneous information. Also, errors in clustering might result in the inclusion of some unrelated sentences. Evalua-tion involving human judges revealed that Simfinder identifies similar sentences with 49.3% precision at 52.9% recall (Hatzivassiloglou, Klavans, and Eskin 1999). We will discuss later how this error rate influences sentence fusion.
 sentence, including WordNet synsets (Miller et al. 1990) and syntactic dependencies, such as subject X  X erb and verb X  X bject relations. A log-linear regression model is used to combine the evidence from the various features into a single similarity value. The model was trained on a large set of sentences which were manually marked for similar-ity. The output of the model is a listing of real-valued similarity values on sentence pairs.
These similarity values are fed into a clustering algorithm that partitions the sentences into closely related groups.
 300 2.2 Theme Selection
To generate a summary of predetermined length, we induce a ranking on the themes measured as the number of sentences, similarity of sentences in a theme, and salience score. The first two of these scores are produced by Simfinder, and the salience score is computed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) as described below. Combining different rankings further filters common information in terms of salience. Since each of these scores has a different range of values, we perform ranking based on each score separately, then induce total ranking by summing ranks from individual categories: the lexical cohesive structure of the text and have been shown to be useful for determin-ing which sentences are important for single-document summarization (Barzilay and
Elhadad 1997; Silber and McCoy 2002). In the multidocument scenario, lexical chains can be adapted for theme ranking based on the salience of theme sentences within their original documents. Specifically, a theme that has many sentences ranked high by lexical chains as important for a single-document summary is, in turn, given a higher salience score for the multidocument summary. In our implementation, a salience score for a theme is computed as the sum of lexical chain scores of each sentence in a theme. 2.3 Theme Ordering
Once we filter out the themes that have a low rank, the next task is to order the selected themes into coherent text. Our ordering strategy aims to capture chronological order of the main events and ensure coherence. To implement this strategy in MultiGen, we select for each theme the sentence which has the earliest publication time ( theme time related themes and then apply chronological ordering on blocks of themes using theme time stamps (Barzilay, Elhadad, and McKeown 2002). These stages produce a sorted set of themes which are passed as input to the sentence fusion component, described in the next section. 3. Sentence Fusion
Given a group of similar sentences X  X  theme X  X he problem is to create a concise and fluent fusion of information, reflecting facts common to all sentences. (An example of a fusion sentence is shown in Table 1.) To achieve this goal we need to identify phrases common to most theme sentences, then combine them into a new sentence.
 adapting the  X  X ag of words X  approach. However, sentence intersection in a set-theoretic being ungrammatical, it is impossible to understand what event this intersection de-scribes. The inadequacy of the bag-of-words method to the fusion task demonstrates the need for a more linguistically motivated approach. At the other extreme, previous ap-proaches (Radev and McKeown 1998) have demonstrated that this task is feasible when a detailed semantic representation of the input sentences is available. However, these approaches operate in a limited domain (e.g., terrorist events), where information ex-traction systems can be used to interpret the source text. The task of mapping input text into a semantic representation in a domain-independent setting extends well beyond the ability of current analysis methods. These considerations suggest that we need a new method for the sentence fusion task. Ideally, such a method would not require a full semantic representation. Rather, it would rely on input texts and shallow linguistic knowledge (such as parse trees) that can be automatically derived from a corpus to generate a fusion sentence.
 that involved in traditional generation systems in which a content selection component chooses content from semantic units, our task is complicated by the lack of semantics in the textual input. At the same time, we can benefit from the textual information given in the input sentences for the tasks of syntactic realization, phrasing, and ordering; in many cases, constraints on text realization are already present in the input.
Content selection occurs primarily in the first phase, in which our algorithm uses local alignment across pairs of parsed sentences, from which we select fragments to be included in the fusion sentence. Instead of examining all possible ways to combine these fragments, we select a sentence in the input which contains most of the fragments and transform its parsed tree into the fusion lattice by eliminating nonessential information and augmenting it with information from other input sentences. This construction of the fusion lattice targets content selection, but in the process, alternative verbalizations are selected, and thus some aspects of realization are also carried out in this phase. Finally, we generate a sentence from this representation based on a language model derived from a large body of texts. 3.1 Identification of Common Information
Our task is to identify information shared between sentences. We do this by aligning constituents in the syntactic parse trees for the input sentences. Our alignment process differs considerably from alignment for other NL tasks, such as machine translation, because we cannot expect a complete alignment. Rather, a subset of the subtrees in one sentence will match different subsets of the subtrees in the others. Furthermore, order across trees is not preserved, there is no natural starting point for alignment, and there are no constraints on crosses. For these reasons we have developed a bottom-up local multisequence alignment algorithm that uses words and phrases as anchors for matching. This algorithm operates on the dependency trees for pairs of input sen-302 tences. We use a dependency-based representation because it abstracts over features irrelevant for comparison such as constituent ordering. In the subsections that follow, we describe first how this representation is computed, then how dependency subtrees are aligned, and finally how we choose between constituents conveying overlapping information.
 determines which sentence constituents convey information appearing in both sentences. This algorithm will be applied to pairwise combinations of sentences in the input set of related sentences.
 to those of another and select the most similar ones. Of course, how this comparison is performed depends on the particular sentence representation used. A good sentence representation will emphasize sentence features that are relevant for comparison, such as dependencies between sentence constituents, while ignoring irrelevant features, such as constituent ordering. A representation which fits these requirements is a dependency-based representation (Melcuk 1988). We first detail how this representation is computed, then describe a method for aligning dependency subtrees. 3.1.1 Sentence Representation. Our sentence representation is based on a dependency tree, which describes the sentence structure in terms of dependencies between words.
The similarity of the dependency tree to a predicate X  X rgument structure makes it a natural representation for our comparison. 3 This representation can be constructed from the output of a traditional parser. In fact, we have developed a rule-based component that transforms the phrase structure output of Collins X  X  (2003) parser into a representation in which a node has a direct link to its dependents. We also mark verb X  subject and verb X  X ode dependencies in the tree.
 abstracted to a canonical form which eliminates features irrelevant to the comparison.
We hypothesize that the difference in grammatical features such as auxiliaries, number, and tense has a secondary effect when the meaning of sentences is being compared.
Therefore, we represent in the dependency tree only nonauxiliary words with their associated grammatical features. For nouns, we record their number, articles, and class (common or proper). For verbs, we record tense, mood (indicative, conditional, or
The eliminated auxiliary words can be re-created using these recorded features. We also transform all passive-voice sentences to the active voice, changing the order of affected children.
 mappings, in practice some paraphrases are not decomposable to words, forming one-to-many or many-to-many paraphrases. Our manual analysis of paraphrased sen-tences (Barzilay 2003) revealed that such alignments most frequently occur in pairs of noun phrases (e.g., faculty member and professor ) and pairs including verbs with parti-cles (e.g., stand up , rise ). To correctly align such phrases, we flatten subtrees containing noun phrases and verbs with particles into one node. We subsequently determine matches between flattened sentences using statistical metrics. shown in Figure 3. (In figures of dependency trees hereafter, node features are omitted for clarity.) 3.1.2 Alignment. Our alignment of dependency trees is driven by two sources of in-formation: the similarity between the structure of the dependency trees and the similar-ity between lexical items. In determining the structural similarity between two trees, we take into account the types of edges (which indicate the relationships between nodes).
An edge is labeled by the syntactic function of the two nodes it connects (e.g., subject X  example, corresponds to an edge connecting a verb and an adjective in another sentence. also identify pairs of paraphrases, using WordNet and a paraphrasing dictionary. We automatically constructed the paraphrasing dictionary from a large comparable news corpus using the co-training method described in Barzilay and McKeown (2001). The dictionary contains pairs of word-level paraphrases as well as phrase-level para-phrases. 4 Several examples of automatically extracted paraphrases are given in Table 2.
During alignment, each pair of nonidentical words that do not comprise a synset in 304
WordNet is looked up in the paraphrasing dictionary; in the case of a match, the pair is considered to be a paraphrase.
 by Sim , is computed. If the optimal alignment of two trees is known, then the value of the similarity function is the sum of the similarity scores of aligned nodes and aligned edges. Since the best alignment of given trees is not known a priori, we select the max-imal score among plausible alignments of the trees. Instead of exhaustively traversing trees of given depths, assuming that we know how to find an optimal alignment for trees of shorter depths. More specifically, at each point of the traversal we consider two cases, shown in Figure 4. In the first case, two top nodes are aligned with each other, and their children are aligned in an optimal way by applying the algorithm to shorter trees. In the second case, one tree is aligned with one of the children of the top node of the other tree; again we can apply our algorithm for this computation, since we decrease the height of one of the trees.

For a tree T containing a node s ,thesubtreeof T which has s as its root node is denoted by T s .
 sions NodeCompare ( T , T ), max s  X  c ( T ) Sim ( T s , T ), and max part of Figure 4 depicts the computation of NodeCompare ( T , T ), in which two top nodes are aligned with each other. The remaining expressions, max with one of the children of the top node of the other tree (the bottom of Figure 4). alignment for the child nodes of the given pair of nodes and is defined by
NodeCompare ( T , T ) = NodeSimilarity ( v , v ) where M ( A , A ) is the set of all possible matchings between A and A , and a matching (between A and A )isasubset m of A  X  A such that for any two distinct elements depth one, NodeCompare ( T , T ) is defined to be NodeSimilarity ( v , v ).
 corresponding words are identical, paraphrases, or unrelated. The similarity scores for pairs of identical words, pairs of synonyms, pairs of paraphrases, and edges (given in
Table 3) are manually derived using a small development corpus. While learning of the similarity scores automatically is an appealing alternative, its application in the fu-sion context is challenging because of the absence of a large training corpus and the lack of an automatic evaluation function. 5 The similarity of nodes containing flattened noun phrases antitank missile and machine gun and antitank missile is computed as a ratio between the score of their intersection antitank missile (2), divided by the length of the latter phrase (5).
 in which the shortest subtrees are processed first. The alignment algorithm returns the similarity score of the trees as well as the optimal mapping between the subtrees of input trees. The pseudocode of this function is presented in the Appendix. In the resulting tree mapping, the pairs of nodes whose NodeSimilarity positively contributed to the alignment are considered parallel. Figure 5 shows two dependency trees and their alignment.
  X  X atchings X : Every node in one tree is mapped to at most one node in another tree. This restriction is necessary because the problem of optimizing many-to-many alignments 306 is NP-hard. 7 The subtree flattening performed during the preprocessing stage aims to minimize the negative effect of the restriction on alignment granularity.

Local alignment maps local regions with high similarity to each other rather than creating an overall optimal global alignment of the entire tree. This strategy is more meaningful when only partial meaning overlap is expected between input sentences, as in typical sentence fusion input. Only these high-similarity regions, which we call intersection subtrees, are included in the fusion sentence. 3.2 Fusion Lattice Computation
Fusion lattice computation is concerned with combining intersection subtrees. During this process, the system will remove phrases from a selected sentence, add phrases from other sentences, and replace words with the paraphrases that annotate each node. Among the many possible combinations of subtrees, we are interested only in those combinations which yield semantically sound sentences and do not distort the information presented in the input sentences. We cannot explore every possible combination, since the lack of semantic information in the trees prohibits us from assessing the quality of the resulting sentences. In fact, our early experimentation with generation from constituent phrases (e.g., NPs, VPs) demonstrated that it was difficult to ensure that semantically anomalous or ungrammatical sentences would not be generated. Instead, we select a combination already present in the input sentences as a basis and transform it into a fusion sentence by removing extraneous informa-tion and augmenting the fusion sentence with information from other sentences. The and the applied transformations aim to preserve semantic correctness, the resulting sentence is a semantically correct one. Our generation strategy is reminiscent of Robin and McKeown X  X  (1996) earlier work on revision for summarization, although Robin and
McKeown used a three-tiered representation of each sentence, including its semantics and its deep and surface syntax, all of which were used as triggers for revision. basis tree, augmentation of the tree with alternative verbalizations, and pruning of basis tree is guided by the number of intersection subtrees it includes; in the best case, the sentence which is the most similar to the other sentences in the input. Using the alignment-based similarity score described in Section 3.1.2, we identify the centroid by computing for each sentence the average similarity score between the sentence and the rest of the input sentences, then selecting the sentence with the highest score. sentences. More specifically, we add alternative verbalizations for the nodes in the basis tree and the intersection subtrees which are not part of the basis tree. The alternative verbalizations are readily available from the pairwise alignments of the basis tree with other trees in the input computed in the previous section. For each node of the basis tree, we record all verbalizations from the nodes of the other input trees aligned with a given node. A verbalization can be a single word, or it can be a phrase, if a node represents a noun compound or a verb with a particle. An example of a fusion lattice, augmented 308 with alternative verbalizations, is given in Figure 6. Even after this augmentation, the fu-sion lattice may not include all of the intersection subtrees. The main difficulty in subtree insertion is finding an acceptable placement; this is often determined by syntactic, se-mantic, and idiosyncratic knowledge. Therefore, we follow a conservative insertion pol-icy. Among all the possible aligned sentences, we insert only subtrees whose top node aligns with one of the nodes in a basis tree. 8 We further constrain the insertion procedure by inserting only trees that appear in at least half of the sentences of a theme. These two constituent-level restrictions prevent the algorithm from generating overly long, un-readable sentences. 9 tree. However, removing all such subtrees may result in an ungrammatical or seman-tically flawed sentence; for example, we might create a sentence without a subject.
This overpruning may happen if either the input to the fusion algorithm is noisy or the alignment has failed to recognize similar subtrees. Therefore, we perform a more conservative pruning, deleting only the self-contained components which can be removed without leaving ungrammatical sentences. As previously observed in the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such com-ponents include a clause in the clause conjunction, relative clauses, and some ele-ments within a clause (such as adverbs and prepositions). For example, this procedure
These phrases are eliminated because they do not appear in the other sentences of the theme and at the same time their removal does not interfere with the well-formedness tion is completed. 3.3 Generation
The final stage in sentence fusion is linearization of the fusion lattice. Sentence generation includes selection of a tree traversal order, lexical choice among avail-able alternatives, and placement of auxiliaries, such as determiners. Our generation and then chooses among remaining alternatives using a language model derived from a large text collection. We first motivate the need for reordering and rephrasing, then discuss our implementation.
 als, since the number of valid traversals is limited by ordering constraints encoded in the fusion lattice. However, the basis lattice does not uniquely determine the tences is not restricted by the original basis tree. While the ordering of many sentence constituents is determined by their syntactic roles, some constituents, such as time, location and manner circumstantials, are free to move (Elhadad et al. 2001). Therefore, the algorithm still has to select an appropriate order from among different orders of the inserted trees.
 tracted sentence; although the basis sentences provides guidance for the generation process, constituents may be removed, added in, or reordered. Wording can also be modified during this process. Although the selection of words and phrases which appear in the basis tree is a safe choice, enriching the fusion sentence with alternative verbalizations has several benefits. In applications such as summarization, in which the length of the produced sentence is a factor, a shorter alternative is desirable. This goal can be achieved by selecting the shortest paraphrase among available alternatives.
Alternate verbalizations can also be used to replace anaphoric expressions, for instance, 310 when the basis tree contains a noun phrase with anaphoric expressions (e.g., his visit ) and one of the other verbalizations is anaphora-free. Substitution of the latter for the anaphoric expression may increase the clarity of the produced sentence, since frequently the antecedent of the anaphoric expression is not present in a summary. Moreover, mations, and the best verbalization might be achieved by using a paraphrase of them from another theme sentence. As an example, consider the case of two paraphras-our correspondent is removed from the sentence Sharon told our correspondent that the elections were delayed . . . , a replacement of the verb told with said yields a more readable sentence.
 in the input nodes. In most cases, aligned words stored in the same node have the same feature values, which uniquely determine an auxiliary selection and con-jugation. However, in some cases, aligned words have different grammatical features, in which case the linearization algorithm needs to select among avail-able alternatives. and placement of auxiliaries as well as the determination of optimal ordering. Since we do not have sufficient semantic information to perform such selection, our algo-rithm is driven by corpus-derived knowledge. We generate all possible sentences statistics derived from a corpus. This approach, originally proposed by Knight and
Hatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method used in statistical generation. We trained a trigram model with Good X  X uring smoothing over 60 megabytes of news articles collected by Newsblaster using the second version CMU X  X ambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997).
The sentence with the lowest length-normalized entropy (the best score) is selected as the verbalization of the fusion lattice. Table 4 shows several verbalizations produced by our algorithm from the central tree in Figure 7. Here, we can see that the lowest-scoring sentence is both grammatical and concise.
 the quality of the generated sentence. For example, the fifth sentence in Table 4 X 
Palestinians fired antitank missile at a bulldozer to build a new embankment in the area X  is not a well-formed sentence; however, our language model gave it a better score than its well-formed alternatives, the second and the third sentences (see Section 4 for further discussion). Despite these shortcomings, we preferred entropy-based scoring to symbolic linearization. In the next section, we motivate our choice. system (Barzilay, McKeown, and Elhadad 1999), we performed linearization of a fusion dependency structure using the language generator FUF/SURGE (Elhadad and Robin 1996). As a large-scale linearizer used in many traditional semantic-to-text generation systems, FUF/SURGE could be an appealing solution to the task of surface realization. Because the input structure and the requirements on the linearizer are quite different in text-to-text generation, we had to design rules for mapping between dependency structures produced by the fusion component and FUF/SURGE input. For instance, FUF/SURGE requires that the input contain a semantic role for prepositional phrases, such as manner , purpose ,or location , which is not present in our dependency representation; thus we had to augment the dependency representation with this information. In the case of inaccurate prediction or the lack of relevant semantic information, the linearizer scrambles the order of sentence constituents, selects wrong prepositions, or even fails to generate an output. Another feature of the FUF/SURGE system that negatively influences system performance is its limited ability to reuse phrases readily available in the input, instead of generating every phrase from scratch. This makes the generation process more complex and thus prone to error.
 seemed promising, the system performance deteriorated significantly when it was applied to automatically constructed themes. Our experience led us to believe that transformation of an arbitrary sentence into a FUF/SURGE input representation is similar in its complexity to semantic parsing, a challenging problem in its own right.
Rather than refining the mapping mechanism, we modified MultiGen to use a statis-312 tical linearization component, which handles uncertainty and noise in the input in a more robust way. 4. Sentence Fusion Evaluation
In our previous work, we evaluated the overall summarization strategy of MultiGen in multiple experiments, including comparisons with human-written summaries in the Document Understanding Conference (DUC) 11 evaluation (McKeown et al. 2001;
McKeown et al. 2002) and quality assessment in the context of a particular informa-tion access task in the Newsblaster framework (McKeown et al. 2002).
 other system components; we analyze the algorithm performance in terms of content selection and the grammaticality of the produced sentences. We first present our eval-uation methodology (Section 4.1), then we describe our data (Section 4.2), the results (Section 4.3), and our analysis of them (Section 4.4). 4.1 Methods 4.1.1 Construction of a Reference Sentence. We evaluated content selection by com-paring an automatically generated sentence with a reference sentence. The reference sentence was produced by a human (hereafter the RFA), who was instructed to gener-ate a sentence conveying information common to many sentences in a theme. The RFA was not familiar with the fusion algorithm. The RFA was provided with the list of theme sentences; the original documents were not included. The instructions given to the RFA included several examples of themes with fusion sentences generated by the authors. Even though the RFA was not instructed to use phrases from input sentences, the sentences presented as examples reused many phrases from the input sentences.
We believe that phrase reuse elucidates the connection between input sentences and a resulting fusion sentence. Two examples of themes, reference sentences, and system outputs are shown in Table 5. 4.1.2 Data Selection. We wanted to test the performance of the fusion component on automatically computed inputs which reflect the accuracy of the existing preprocessing tools. For this reason, the test data were selected randomly from material collected by
Newsblaster. To remove themes irrelevant for fusion evaluation, we introduced two additional filters. First, we excluded themes that contained identical or nearly identical sentences (with cosine similarity higher than 0.8). When processing such sentences, our algorithm reduces to sentence extraction, which does not allow us to evaluate the generation abilities of our algorithm. Second, themes for which the RFA was unable to create a reference sentence were also removed from the test set. As mentioned above,
Simfinder does not always produce accurate themes, 12 common. An example of a theme for which no sentence was generated is shown in
Table 6. As a result of this filtering, 34% of the sentences were removed. 4.1.3 Baselines. In addition to the system-generated sentence, we also included in the evaluation a fusion sentence generated by another human (hereafter, RFA2) and three baselines. (Following the DUC terminology, we refer to the baselines, our system, sentences, which is obviously grammatical, and it also has a good chance of being rep-resentative of common topics conveyed in the input. The second baseline is produced by a simplification of our algorithm, where paraphrase information is omitted during phrase information to the performance of the fusion algorithm. The third baseline bution of the insertion and deletion stages in the fusion algorithm. The comparison against an RFA2 sentence provides an upper bound on the performance of the system and baselines. In addition, this comparison sheds light on the human agreement on this task. 314 4.1.4 Comparison against the Reference Sentence. One judge was given a peer sen-tence along with the corresponding reference sentence. The judge also had access to the original theme from which these sentences were generated. The order of the presentation was randomized across themes and peer systems. Reference and peer sentences were divided into clauses by the authors. The judges assessed overlap on the clause level between reference and peer sentences. The wording of the instructions was inspired by the DUC instructions for clause comparison. For each clause in the reference sentence, the judge decided whether the meaning of a corresponding for full overlap, this framework allows for partial overlap with a score of 0.5. From the overlap data, we computed weighted recall and precision based on fractional count (Hatzivassiloglou and McKeown 1993). Recall is a ratio of weighted clause overlap between a peer and a reference sentence, and the number of clauses in a reference sentence. Precision is a ratio of weighted clause overlap between a peer and a reference sentence, and the number of clauses in a peer sentence. 4.1.5 Grammaticality Assessment. Grammaticality was rated in three categories: grammatical (3), partially grammatical (2), and not grammatical (1). The judge was in-structed to rate a sentence in the grammatical category if it contained no grammatical mistakes. Partially grammatical included sentences that contained at most one mistake in agreement, articles, and tense realization. The not grammatical category included sentences that were corrupted by multiple mistakes of the former type, by erroneous component order or by the omission of important components (e.g., subject).
 punctuation is a limitation of our implementation of the sentence fusion algorithm that we are well aware of. 13 Therefore, in our grammaticality evaluation (following the
DUC procedure), the judge was asked to ignore punctuation. 4.2 Data
To evaluate our sentence fusion algorithm, we selected 100 themes following the proce-dure described in the previous section. Each set varied from three to seven sentences, with 4.22 sentences on average. The generated fusion sentences consisted of 1.91 clauses on average. None of the sentences in the test set were fully extracted; on average, each sentence fused fragments from 2.14 theme sentences. Out of 100 sentence, 57 sentences produced by the algorithm combined phrases from several sentences, while the rest of the sentences comprised subsequences of one of the theme sentences. (Note that compression is different from sentence extraction.) We included these sentences in the evaluation, because they reflect both content selection and realization capacities of the algorithm.
 examples are chosen so as to reflect good-and bad-performance cases. Note that the first example results in inclusion of the essential information (the fact that bodies were found, along with time and place) and leaves out details (that it was a remote location or how many miles west it was, a piece of information that is in dispute in any case). The problematic example incorrectly selects the number of people killed as six, even though this number is not repeated and different numbers are referred to in the text. This mistake is caused by a noisy entry in our paraphrasing dictionary which erroneously identifies  X  X ive X  and  X  X ix X  as paraphrases of each other. 4.3 Results
Table 7 shows the length ratio, precision, recall, F -measure, and grammaticality score output length to the average length of the theme input sentences. 4.4 Discussion
The results in Table 7 demonstrate that sentences manually generated by the second human participant (RFA2) not only are the shortest, but are also closest to the reference sentence in terms of selected information. The tight connection generated by the RFAs establishes a high upper bound for the fusion task. While neither our system nor the baselines were able to reach this level of performance, the fusion algorithm clearly outperforms all the baselines in terms of content selection, at a reasonable level of compression. The performance of baseline 1 and baseline 2 demonstrates that neither the shortest sentence nor the basis sentence is an adequate system and baseline 3 confirms our hypothesis about the importance of paraphrasing information for the fusion process. Omission of paraphrases causes an 8% drop in recall due to the inability to match equivalent phrases with different wording. sentences contain grammatical errors, unlike fully extracted, human-written sentences.
Given the high sensitivity of humans to processing ungrammatical sentences, one readability of the generated sentences. Sentence fusion may not be a worthy direction to pursue if low grammaticality is intrinsic to the algorithm and its correction requires 316 knowledge which cannot be automatically acquired. In the remainder of the section, we show that this is not the case. Our manual analysis of generated sentences revealed that most of the grammatical mistakes are caused by the linearization component, or more specifically, by suboptimal scoring of the language model. Language model-be able to dramatically boost the linearization capacity of our algorithm. 4.4.1 Error Analysis. In this section, we discuss the results of our manual analysis of mistakes in content selection and surface realization. Note that in some cases multiple errors are entwined in one sentence, which makes it hard to distinguish between a sequence of independent mistakes and a cause-and-effect chain. Therefore, the pre-sented counts should be viewed as approximations, rather than precise numbers. interesting mistakes that we encountered during system development.

Mistakes in Content Selection. Most of the mistakes in content selection can be attributed to problems with alignment. In most cases (17), erroneous alignments missed relevant word mappings as a result of the lack of a corresponding entry in our paraphrasing resources. At the same time, mapping of unrelated words (as shown in Table 5) was quite rare (two cases). This performance level is quite predictable given the accuracy of an automatically constructed dictionary and limited coverage of WordNet. Even in the presence of accurate lexical information, the algorithm occasionally produced suboptimal alignments (four cases) because of the simplicity of our weighting scheme, which supports limited forms of mapping typology and also uses manually assigned weights.
 many-to-many alignments. Namely, two trees conveying the same meaning may not be decomposable into the node-level mappings which our algorithm aims to compute. For example, the mapping between the sentences in Table 8 expressed by the rule
X denied claims by Y  X  X said that Y  X  X  claim was untrue cannot be decomposed into smaller matching units. At least two mistakes resulted from noisy preprocessing (tokenization and parsing).
 three clauses that were present in the corresponding reference sentences. The sentence
Conservatives were cheering language is an example of an incomplete sentence derived from the following input sentence: Conservatives were cheering language in the final version that ensures that one-third of all funds for prevention programs be used to promote abstinence.
The omission of a relative clause was possible because some sentences in the input theme contained the noun language without any relative clauses.

Mistakes in Surface Realization. Grammatical mistakes included incorrect selection of determiners, erroneous word ordering, omission of essential sentence constituents, and incorrect realization of negation constructions and tense. These mistakes (42) originated during linearization of the lattice and were caused either by incompleteness of the linearizer or by suboptimal scoring of the language model. Mistakes of the first type are caused by missing rules for generating auxiliaries given node features. An exam-completeness of existing application-independent linearizers, such as the unification-based FUF/SURGE (Elhadad and Robin 1996) and the probabilistic Fergus (Bangalore and Rambow 2000). Unfortunately, we were unable to reuse any of the existing large-scale linearizers because of significant structural differences between input expected adapting Fergus for the sentence fusion task.
 in these cases, a language model selected ill-formed sentences, assigning a worse the language model picked the incorrect verbalization. We found that in 27 cases the optimal verbalizations (in the authors X  view) were ranked below the top-10 sentences ranked by the language model. We believe that more powerful language models that incorporate linguistic knowledge (such as syntax-based models) can improve the quality of generated sentences. 4.4.2 Further Analysis. In addition to analyzing errors found in this particular study, we also regularly track the quality of generated summaries on Newsblaster X  X  Web page. We have noted a number of interesting errors that crop up from time to time that seem to require information about the full syntactic parse, semantics, or even discourse. Consider, for example, the last sentence from a summary entitled Estrogen-
Progestin Supplements Now Linked to Dementia, which is shown in Table 9. This sentence was created by sentence fusion and clearly, there is a problem. Certainly, there was a study finding the risk of dementia in women who took one type of combined hormone pill, but it was not the government study which was abruptly halted last summer. In looking at the two sentences from which this summary sentence was drawn, we can see that there is a good amount of overlap between the two, but the component does not have enough information about the referents of the different terms to know that two different 318 studies are involved and that fusion should not take place. One topic of our future work (Section 6) is the problem of reference and summarization.
 first error is in the references to the segments .Thetwousesof segments in the first source document sentence do not refer to the same entity and thus, when the modifier is dropped, we get an anomaly. The second, more unusual problem is in the equation of Clinton/Dole, Dole/Clinton ,and Clinton and Dole . 5. Related Work 5.1 Text-to-Text Generation
Unlike traditional concept-to-text generation approaches, text-to-text generation methods take text as input and transform it into a new text satisfying some constraints algorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003) and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003) are other instances of such methods.
 and they aim to reduce a sentence by eliminating constituents which are not crucial for understanding the sentence and not salient enough to include in the summary.
These approaches are based on the observation that the  X  X mportance X  of a sentence constituent can often be determined based on shallow features, such as its syntactic peripheral to the central point of the document can be removed from a sentence without significantly distorting its meaning. While earlier approaches for text compression were based on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999), more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced (Knight and Marcu 2002;
Jing and McKeown 2000; Reizler et al. 2003). The summary sentences, which have been manually compressed, are aligned with the original sentences from which they were drawn.
 channel model (Brown et al. 1993). In this model, a short (compressed) string is treated as a source, and additions to this string are considered to be noise. The probability of a source string s is computed by combining a standard probabilistic context-free grammar score, which is derived from the grammar rules that yielded tree s , and a word-bigram score, computed over the leaves of the tree. The stochastic channel model creates a large tree t from a smaller tree s by choosing an extension template for each node based on the labels of the node and its children. In the decoding stage, the system searches for the short string s that maximizes P ( s | t ), which (for fixed t ) is equivalent to maximizing P ( s )  X  P ( t | s ).

McKeown (2000) also rely on cohesion information, derived from word distribution in a text: Phrases that are linked to a local context are retained, while phrases that have no such links are dropped. Another difference between these two methods is the extensive which components of the sentence are obligatory to keep it grammatically correct. The corpus in this approach is used to estimate the degree to which a fragment is extraneous and can be omitted from a summary. A phrase is removed only if it is not grammati-being removed by humans. In addition to reducing the original sentences, Jing and
McKeown (2000) use a number of manually compiled rules to aggregate reduced sentences; for example, reduced clauses might be conjoined with and .
 shallow analysis of the input and statistics derived from a corpus. Clearly, the difference in the nature of both tasks and in the type of input they expect (single sentence versus multiple sentences) dictates the use of different methods. Having multiple sentences in the input poses new challenges X  X uch as a need for sentence comparison X  X ut at the compression algorithms is always a substring of the original sentence, sentence fusion may generate a new sentence which is not a substring of any of the input sentences. This is achieved by arranging fragments of several input sentences into one sentence. that of Pang, Knight, and Marcu (2003). Their method operates over multiple English translations of the same foreign sentence and is intended to generate novel paraphrases of the input sentences. Like sentence fusion, their method aligns parse trees of the input sentences and then uses a language model to linearize the derived lattice. The main difference between the two methods is in the type of the alignment: Our algorithm performs local alignment, while the algorithm of Pang, Knight, and Marcu (2003) performs global alignment. The differences in alignment are caused by differences in input: Pang, Knight, and Marcu X  X  method expects semantically equivalent sentences, while our algorithm operates over sentences with only partial meaning overlap. The 320 presence of deletions and insertions in input sentences makes alignment of comparable trees a new and particularly significant challenge. 5.2 Computation of an Agreement Tree
The alignment method described in Section 3 falls into a class of tree comparison algorithms extensively studied in theoretical computer science (Sankoff 1975; Finden and Gordon 1985; Amir and Keselman 1994; Farach, Przytycka, and Thorup 1995) and widely applied in many areas of computer science, primarily computational bi-ology (Gusfield 1997). These algorithms aim to find an overlap subtree that captures structural commonality across a set of related trees. A typical tree similarity measure considers the proximity, at both the node and the edge levels, between input trees.
In addition, some algorithms constrain the topology of the resulting alignment based on the domain-specific knowledge. These constraints not only narrow the search space but also increase the robustness of the algorithm in the presence of a weak similarity function.
 based machine translation, in which the goal is to find an optimal alignment between the source and the target sentences (Meyers, Yangarber, and Grishman 1996). The algorithm operates over pairs of parallel sentences, where each sentence is represented driven by lexical mapping between tree nodes and is derived from a bilingual dictio-nary. The search procedure is greedy and is subject to a number of constraints needed for alignment of parallel sentences.
 over syntactic dependency representations and employs recursive computation to find an optimal solution. However, our method is different in two key aspects. First, our algorithm looks for local regions with high similarity in nonparallel data, rather than for full alignment, expected in the case of parallel trees. The change in optimization criteria introduces differences in the similarity measure X  X pecifically, the relaxation of certain constraints X  X nd the search procedure, which in our work uses dynamic programming.
Second, our method is an instance of a multisequence alignment, pairwise alignment described in Meyers, Yangarber, and Grishman (1996). Combining evidence from multiple trees is an essential step of our algorithm X  X airwise comparison of nonparallel trees may not provide enough information regarding their underlying correspondences. In fact, previous applications of multisequence alignment have been shown to increase the accuracy of the comparison in other NLP tasks (Barzilay and
Lee 2002; Bangalore, Murdock, and Riccardi 2002; Lacatusu, Maiorano, and Harabagiu 2004); unlike our work these approaches operate on strings, not trees, and with the exception of (Lacatusu, Maiorano, and Harabagiu 2004), they apply alignment to paral-lel data, not comparable texts. 6. Conclusions and Future Work
In this article, we have presented sentence fusion, a novel method for text-to-text generation which, given a set of similar sentences, produces a new sentence contain-ing the information common to most sentences. Unlike traditional generation methods, sentence fusion does not require an elaborate semantic representation of the input but instead relies on the shallow linguistic representation automatically derived from the input documents and knowledge acquired from a large text corpus. Generation is performed by reusing and altering phrases from input sentences.
 common information and in most cases generates a well-formed fusion sentence. Our algorithm outperforms the shortest-sentence baseline in terms of content selection, without a significant drop in grammaticality. We also show that augmenting the fu-sion process with paraphrasing knowledge improves the output by both measures.
However, there is still a gap between the performance of our system and human performance.
 of content selection and realization. We believe that the process of aligning theme sentences can be greatly improved by having the system learn the similarity function, instead of using manually assigned weights. An interesting question is how such a similarity function can be induced in an unsupervised fashion. In addition, we can improve the flexibility of the fusion algorithm by using a more powerful language model. Recent research (Daume et al. 2002) has show that syntax-based language models are more suitable for language generation tasks; the study of such models is a promising direction to explore.
 multiple verbalizations of a given fusion lattice. In our implementation, this property is utilized only to produce grammatical texts in the changed syntactic context, but it can also be used to increase coherence of the text at the discourse level by taking context into account. In our current system, each sentence is generated in isolation, inde-pendently from what is said before and what will be said after. Clear evidence of the limitation of this approach is found in the selection of referring expressions. For example, all summary sentences may contain the full description of a named entity (e.g., President of Columbia University Lee Bollinger ), while the use of shorter descriptions such as Bollinger or anaphoric expressions in some summary sentences would in-crease the summary X  X  readability (Schiffman, Nenkova, and McKeown 2002; Nenkova and McKeown 2003). These constraints can be incorporated into the sentence fusion algorithm, since our alignment-based representation of themes often contains several alternative descriptions of the same object.
 appropriate paraphrases of each summary sentence, we can significantly improve the coherence of an output summary. An important research direction for future work is to develop a probabilistic text model that can capture properties of well-formed texts, just as a language model captures properties of sentence grammaticality. Ideally, such a model would be able to discriminate between cohesive fluent texts and ill-formed texts, guiding the selection of sentence paraphrases to achieve an optimal sentence sequence.
 Appendix. Alignment Pseudocode Function: EdgeSim ( edge 1 , edge 2 )
Returns: The similarity score of two input edges based on their type begin if type of (edge 1 )= type of (edge 2 ) =  X  X ubject-verb X  then 322 end Function: NodeSim ( node 1 , node 2 )
Returns: The similarity score of two words or flattened noun phrases based on their begin end
All the comparison functions employ memoization, implemented by hash table wrappers.
 Function: MapChildren ( tree 1 ,tree 2 ) memoized
Returns: Given two dependency trees, MapChildren finds the optimal alignment of tree begin end Function: NodeCompare ( tree 1 ,tree 2 ) memoized
Returns: Given two dependency trees, NodeCompare finds their optimal alignment that begin node-sim  X  NodeSim ( tree 1 .top, tree 2 .top ); /*If one of the trees is of height one, return the NodeSim score between two tops */ if is leaf (tree 1 ) or is leaf (tree 2 ) then else end end Function: NodeCompare ( tree 1 ,tree 2 ) memoized
Returns: Given two dependency trees, Sim finds their optimal alignment. The function begin best  X   X  1, void ; /*find an optimal alignment between one of the children of tree foreach sintree 1 .children do end /*find an optimal alignment between one of the children of tree foreach sintree 2 .children do end /*find an optimal alignment that include the two top nodes */ res  X  NodeCompare ( tree 1 ,tree 2 ); if res.score &gt; best.score then best  X  res ; return best end Acknowledgments 324 326
