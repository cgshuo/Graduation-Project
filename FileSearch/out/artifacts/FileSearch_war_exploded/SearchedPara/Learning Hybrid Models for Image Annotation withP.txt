 vision communities [7, 14]. State-of-the-art methods form ulate image annotation as a structured prediction problem, and utilize methods such as Conditiona l Random Fields [8, 4], which output ing model parameters. It is widely acknowledged that consis tently-labeled images are tedious and segmentation performance and relax the constraint of discr iminative labeling methods. A wide range of learning methods have been developed for usin g partially-labeled image data. One Others take a semi-supervised learning approach by viewing unlabeled image regions as unlabeled diction tasks [1, 10]. However, the common assumption about the smoothness of the label distri-variation of object appearance. Other semi-supervised met hods adopt a hybrid approach, combining a generative model of the input data with a discriminative mo del for image labeling, in which the simple probabilistic models are considered in these approa ches, without capturing the contextual information in images.
 Our approach described in this paper extends the hybrid mode ling strategy by incorporating a more task. First, we extend the Latent Dirichlet Allocation mode l (LDA) [3] to include not only input our model integrates a generative model of image appearance and a discriminative model of region labels. Second, the original LDA structure does not impose a ny spatial smoothness constraint to approaches have introduced lateral connections between la tent topic variables [17, 15]. However, can be non-smooth over the image plane in general. In this pap er, we model the spatial dependency to accurate label predictions.
 The remainder of this paper is organized as follows. The next section presents the base model, section we discuss the model limitations and future directi ons. The structured prediction problem in image labeling can be f ormulated as follows. Let an image x be represented as a set of subregions { x categorical set L . For instance, subregion x object classes. Denote the set of labels for x as l = { l concerns how to capture the interactions between labels in l given the input image. Model I. We first introduce our base model for capturing individual pa tterns in image appearance and label space. Assume each subregion x describes its appearance (including color, texture, etc.) in some appearance feature space A and t and subregion appearances given positions by modeling co-o ccurred patterns in the joint space of appearance.
 More specifically, we assume each observation pair ( a of K hidden  X  X opic X  components shared across the whole dataset, given the position information t i . Following the LDA notation, the mixture proportion is denot ed as  X  , which is image-specific and shares a common Dirichlet prior parameterized by  X  . Also, z specify from which hidden topic component the pair ( a given the position t as follows, where P (  X  |  X  ) is the Dirichlet distribution. We specify the appearance mo del P ( a invariant but the label predictor P ( l left panel of Figure 1. (a) Label prediction module P ( l probabilistic classifier that takes ( a tion for l MLP with one hidden layer in our experiments, although other strong classifiers are also feasible. (b) Image appearance module P ( a eters  X  to the image features { a A can be represented as a visual word v , and we have P ( a Figure 1: Left:A graphical representation of the base topic prediction model (Model I). Middle: Model II. Right: Model III. Circular nodes are random variab les, and shaded nodes are observed. N is the number of image features in each image, and D denotes all the training data. usually strongly correlated. To incorporate spatial infor mation, we extend our base model in two different ways as follows.
 Model II. We introduce a dependency between each label variable and it s neighboring topic vari-a neighborhood. More specifically, we change the label predi ction model into the following form: where N ( i ) is a predefined neighborhood for site i , and w z . We set w representation is shown in the middle panel of Figure 1. This model variant can be viewed as an extension to the supervised LDA [2]. Here, however, rather t han a single label applying to each input example instead there are multiple labels, one for eac h element of x .
 Model III. We add lateral connections between label variables to build a Conditional Random Field of labels. The joint label distribution given input image is defined as where Z is the partition function. The pairwise potential f ( l the Kronecker delta function. Note that P is shown in the right panel of Figure 1.
 Note that the base model (Model I) obtains spatially smooth l abels simply through the topics cap-a conventional form of spatial dependency by directly incor porating local smoothing in the label complicated learning methods. Posterior Marginals (MPM) criterion: We consider the label inference procedure for three models s eparately as follows. Models I&amp;II : The marginal label distribution P ( l Denote v form of posterior distribution. Both v be written as P ( l P ( l i | a i , t i , v i ) around v i,q using Taylor expansion: Taking expectation on both sides of Equation 6 w.r.t. P ( z v Model III : We first compute the unary potential of the CRF model from the base topic prediction model, i.e., P computed by applying loopy belief propagation to the condit ional random field. data components to compute the label prediction. We take a Gi bbs sampling approach by integrating given other variables, which is required by Gibbs sampling: where z samples of the topic variables, we estimate their posterior marginal distribution P ( z computing their normalized histograms. Here we consider estimating the parameters of both extended models from a partially labeled image set D = { x n , l n } . For an image x n , its label l n = ( l n and l n with different components, we treat them separately.
 Models I&amp;II. We use the Maximum Likelihood criterion to estimate the mode l parameters. Let  X  be the parameter set of the model, We maximize the log data likelihood by Monte Carlo EM. The low er bound of the likelihood can be written as procedure similar to Equation 7. It uses the following condi tional probability: the updating equation for parameters of the appearance modu le P ( a | z ) can be derived from the stationary point of Q : where the approximation takes the same form as in Equation 6. We use a gradient ascent algorithm which reduces training time.
 Model III. We estimate the parameters of Model III in two stages: (1). Th e parameters of the base on Maximum Likelihood. (2). Given the base topic prediction model, we compute the marginal label probability P tion 3). We then estimate the parameters in the CRF by maximiz ing conditional pseudo-likelihood as follows: where Z n pseudo-likelihood. Data sets and representation. Our experiments are based on three image datasets. The first i s a subset of the Microsoft Research Cambridge (MSRC) Image Dat abase [14] as in [16]. This subset as in [5] (referred therein as Corel-B). It includes 305 manu ally labeled images with 11 classes, focusing on animals and natural scenes.
 images, in which the segmentation algorithm is tuned to gene rate approximately 1000 segments for we used 20 bins for edge information and 50 bins for texture in formation. We also augment each feature by a SIFT descriptor extracted from a 30  X  30 image patch centered at the super-pixel. The The cluster centers are used as visual words and each descrip tor is encoded by its word index. Comparison methods. We compare our approach directly with two baseline systems: a super-pixel-wise classifier and a basic CRF model. We also report th e experimental results from [16], each super-pixel independently. The MLP has 30 hidden units , a number chosen based on validation its conditional pseudo-likelihood, and label the image bas ed on the marginal distribution of each label variable, computed by the loopy belief propagation al gorithm.
 Performance on MSRC-9. Following the setting in [16], we randomly split the dataset into training The average classification accuracy is at the pixel level.
 we set the vocabulary size to 500, the number of hidden topics to 50, and each symmetric Dirichlet parameter  X  learning of the classifiers per iteration.
 CRF model. Table 1 shows the average classification accuracy rates of our model and the baselines than the S Class. Also, Model II and III improve the accuracy further by incorporating the label good performance of the simple CRF.
 representation, we evaluate our models using data with diff erent amount of labeling information. labeled pixels to 62.9%, 52.1%, 44.1%, 36.4%, 30.5%, 24.9% a nd 20.3%, respectively. The original previous work (cf. [16]). We also note that the performance o f Model III is more robust than the other two variants, which may derive from stronger smoothin g.
 Table 2: A comparison of classification accuracy of our three model variants with other methods on the full MSRC dataset and Corel-B dataset.
 Performance on other sets. We further evaluate our models on two larger datasets to see w hether into 175 training images and 130 testing images randomly. We use the same setting of the models models as well as some previous methods. For the full MSRC set , the two extended versions of our Right top: Examples of an image and its super-pixelization. Right bottom: Examples of original labeling and labeling after dilation (the ratio is 36.4). provides useful cues. Also, our models have the same accurac y as reported in [5] on the Corel-B note that the topics and spatial smoothness play less roles i n the labeling performance on Corel-models handle the extended regions better than those fine obj ect structures, due to the tendency of (over)smoothing caused by super-pixelization and the tw o spatial dependency structures. tent representation of an image then provides an additional input to the label predictor. We also native approaches, such as a standard classifier and a standa rd CRF. Its performance also matches possible. A final issue concerns the reliance on visual words formed by clustering features in a complicated appearance space. Using a stronger appearance model may help us understand the role of different visual cues, as well as construct a more powerfu l generative model. Figure 3: Some labeling results for the Corel-B (bottom pane l) and MSRC-9 (top panel) datasets, based on the best performance of our models. The  X  X oid X  regio n is annotated by color  X  X lack X .
