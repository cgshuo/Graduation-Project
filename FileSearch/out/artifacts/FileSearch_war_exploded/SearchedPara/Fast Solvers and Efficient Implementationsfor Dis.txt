 Kilian Q. Weinberger kilian@yahoo-inc.com Yahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 9505 Lawrence K. Saul saul@cs.ucsd.edu Many algorithms for pattern classification and ma-chine learning depend on computing distances in a multidimensional input space. Often, these distances are computed using a Euclidean distance metric X  X  choice which has both the advantages of simplicity and generality. Notwithstanding these advantages, though, the Euclidean distance metric is not very well adapted to most problems in pattern classification.
 Viewing the Euclidean distance metric as overly sim-plistic, many researchers have begun to ask how to learn or adapt the distance metric itself in order to achieve better results (Xing et al., 2002; Chopra et al., 2005; Frome et al., 2007). Distance metric learning is an emerging area of statistical learning in which the goal is to induce a more powerful distance met-ric from labeled examples. The simplest instance of this problem arises in the context of k -nearest neigh-bor (kNN) classification using Mahalanobis distances. Mahalanobis distances are computed by linearly trans-forming the input space, then computing Euclidean distances in the transformed space. A well-chosen lin-ear transformation can improve kNN classification by decorrelating and reweighting elements of the feature vector. In fact, significant improvements have been observed within several different frameworks for this problem, including neighborhood components analy-sis (Goldberger et al., 2005), large margin kNN clas-sification (Weinberger et al., 2006), and information-theoretic metric learning (Davis et al., 2007). These studies have established the general utility of distance metric learning for kNN classification. How-ever, further work is required to explore its promise in more difficult regimes. In particular, larger data sets raise new and important challenges in scalability. They also present the opportunity to learn more adap-tive and sophisticated distance metrics.
 In this paper, we study these issues as they arise in the recently proposed framework of large margin near-est neighbor (LMNN) classification (Weinberger et al., 2006). In this framework, a Mahalanobis distance met-ric is trained with the goal that the k -nearest neigh-bors of each example belong to the same class while examples from different classes are separated by a large margin. Simple in concept, useful in practice, the ideas behind LMNN classification have also inspired other related work in machine learning and computer vision (Torresani &amp; Lee, 2007; Frome et al., 2007). The role of the margin in LMNN classification is in-spired by its role in support vector machines (SVMs). Not surprisingly, given these roots, LMNN classifica-tion also inherits various strengths and weaknesses of SVMs (Sch  X olkopf &amp; Smola, 2002). For example, as in SVMs, the training procedure in LMNN classification reduces to a convex optimization based on the hinge loss. However, as described in section 2, na  X  X ve imple-mentations of this optimization do not scale well to larger data sets.
 Addressing the challenges and opportunities raised by larger data sets, this paper makes three contributions. First, we describe how to optimize the training pro-cedure for LMNN classification so that it can readily handle data sets with tens of thousands of training examples. In order to scale to this regime, we have implemented a special-purpose solver for the particu-lar instance of semidefinite programming that arises in LMNN classification. In section 3, we describe the details of this solver, which we have used to tackle problems involving billions of large margin constraints. To our knowledge, problems of this size have yet to be tackled by other recently proposed methods (Gold-berger et al., 2005; Davis et al., 2007) for learning Mahalanobis distance metrics.
 As the second contribution of this paper, we explore the use of metric ball trees (Liu et al., 2005) for LMNN classification. These data structures have been widely used to accelerate nearest neighbor search. In sec-tion 4, we show how similar data structures can be used for faster training and testing in LMNN classi-fication. Ball trees are known to work best in input spaces of low to moderate dimensionality. Mindful of this regime, we also show how to modify the optimiza-tion in LMNN so that it learns a low-rank Mahalanobis distance metric. With this modification, the metric can be viewed as projecting the original inputs into a lower dimensional space, yielding further speedups. As the third contribution of this paper, we describe an important extension to the original framework for LMNN classification. Specifically, in section 5, we show how to learn different Mahalanobis distance met-rics for different parts of the input space. The novelty of our approach lies in learning a collection of different local metrics to maximize the margin of correct kNN classification. The promise of this approach is sug-gested by recent, related work in computer vision that has achieved state-of-the-art results on image classifi-cation (Frome et al., 2007). Our particular approach begins by partitioning the training data into disjoint clusters using class labels or unsupervised methods. We then learn a Mahalanobis distance metric for each cluster. While the training procedure couples the dis-tance metrics in different clusters, the optimization re-mains a convex problem in semidefinite programming. The globally coupled training of these metrics also distinguishes our approach from earlier work in adap-tive distance metrics for kNN classification (Hastie &amp; Tibshirani, 1996). To our knowledge, our approach yields the best kNN test error rate on the extensively benchmarked MNIST data set of handwritten digits that does not incorporate domain-specific prior knowl-edge (LeCun et al., 1998; Simard et al., 1993). Thus, our results show that we can exploit large data sets to learn more powerful and adaptive distance metrics for kNN classification. Of the many settings for distance metric learning, the simplest instance of the problem arises in the con-text of kNN classification using Mahalanobis distances. A Mahalanobis distance metric computes the squared distances between two points ~x i and ~x j as: where M 0 is a positive semidefinite matrix. When M is equal to the identity matrix, eq. (1) reduces to the Euclidean distance metric. In distance metric learning, the goal is to discover a matrix M that leads to lower kNN error rates than the Euclidean distance metric. Here we briefly review how Mahalanobis distance met-rics are learned for LMNN classification (Weinberger et al., 2006). Let the training data consist of n la-beled examples { ( ~x i , y i ) } n i =1 where ~x i  X  X  d and y { 1 , . . . , c } , where c is the number of classes. For LMNN classification, the training procedure has two steps. The first step identifies a set of k similarly labeled target neighbors for each input ~x i . Target neighbors are selected by using prior knowledge (if available) or by simply computing the k nearest (similarly labeled) neighbors using Euclidean distance. We use the nota-tion j i to indicate that ~x j is a target neighbor of ~x The second step adapts the Mahalanobis distance met-ric so that these target neighbors are closer to ~x i than all other differently labeled inputs. The Mahalanobis distance metric is estimated by solving a problem in semidefinite programming. Distance metrics obtained in this way were observed to yield consistent and sig-nificant improvements in kNN error rates.
 The semidefinite program in LMNN classification arises from an objective function which balances two terms. The first term penalizes large distances be-tween inputs and their target neighbors. The second term penalizes small distances between differently la-beled inputs; specifically, a penalty is incurred if these distances do not exceed (by a finite margin) the dis-tances to the target neighbors of these inputs. The terms in the objective function can be made precise with further notation. Let y ij  X  X  0 , 1 } indicate whether the inputs ~x i and ~x j have the same class label. Also, let  X  ijl  X  0 denote the amount by which a differently labeled input ~x l invades the  X  X erimeter X  around input ~x defined by its target neighbor ~x j . The Mahalanobis distance metric M is obtained by solving the following semidefinite program:
Minimize P j i d 2 M ( ~x i , ~x j ) + P l (1  X  y il )  X  ijl subject to: The constant defines the trade-off between the two terms in the objective function; in our experiments, we set = 1. The constraints of type (a) encourage in-puts ( ~x i ) to be at least one unit closer to their k target neighbors ( ~x j ) than to any other differently labeled in-put ( ~x l ). When differently labeled inputs ~x l invade the local neighborhood of ~x i , we refer to them as impos-tors . Impostors generate positive slack variables  X  ijl which are penalized in the second term of the objective function. The constraints of type (b) enforce nonneg-ativity of the slack variables, and the constraint (c) enforces that M is positive semidefinite, thus defining a valid (pseudo)metric. Noting that the squared Ma-halanobis distances d 2 M ( ~x i , ~x j ) are linear in the matrix M , the above optimization is easily recognized as an semidefinite program. The semidefinite program in the previous section grows in complexity with the number of training examples ( n ), the number of target neighbors ( k ), and the di-mensionality of the input space ( d ). In particular, the objective function is optimized with respect to O ( kn 2 ) large margin constraints of type (a) and (b) , while the Mahalanobis distance metric M itself is a d  X  d matrix. Thus, for even moderately large and/or high dimensional data sets, the required optimization (though convex) cannot be solved by standard off-the-shelf packages (Borchers, 1999).
 In order to tackle larger problems in LMNN classifica-tion, we implemented our own special-purpose solver. Our solver was designed to exploit the particular struc-ture of the semidefinite program in the previous sec-tion. The solver iteratively re-estimates the Maha-lanobis distance metric M to minimize the objective function for LMNN classification. The amount of com-putation is minimized by careful book-keeping from one iteration to the next. The speed-ups from these optimizations enabled us to work comfortably on data sets with up to n =60 , 000 training examples. Our solver works by eliminating the slack variables  X  ijl from the semidefinite program for LMNN classifica-tion, then minimizing the resulting objective function by sub-gradient methods. The slack variables are elim-inated by folding the constraints (a) and (b) into the objective function as a sum of  X  X inge X  losses. The hinge function is defined as [ z ] + = z if z &gt; 0 and [ z ] + = 0 if z &lt; 0. In terms of this hinge function, we can express  X  ijl as a function of the matrix M : Finally, writing the objective function only in terms of the matrix M , we obtain:  X  ( M ) = X This objective function is not differentiable due to the hinge losses that appear in eq. (2). Nevertheless, be-cause it is convex, we can compute its sub-gradient and use standard descent algorithms to find its minimum. At each iteration of our solver, the optimization takes a step along the sub-gradient to reduce the objective function, then projects the matrix M back onto the cone of positive semidefinite matrices. Iterative meth-ods of this form are known to converge to the correct solution, provided that the gradient step-size is suffi-ciently small (Boyd &amp; Vandenberghe, 2004). The gradient computation can be done most efficiently by careful book-keeping from one iteration to the next. As simplifying notation, let C ij =( ~x i  X  ~x j )( ~x i  X  ~x In terms of this notation, we can express the squared Mahalanobis distances in eq. (8) as: To evaluate the gradient, we denote the matrix M at the t th iteration as M t . At each iteration, we also define a set N t of triplet indices such that ( i, j, l )  X  X  t if and only if the triplet X  X  corresponding slack variable exceeds zero:  X  ijl ( M t ) &gt; 0. With this notation, we can Computing the gradient requires computing the outer products in C ij ; it thus scales quadratically in the input dimensionality. As the set N t is potentially large, a na  X  X ve computation of the gradient would be extremely expensive. However, we can exploit the fact that the gradient contribution from each active triplet ( i, j, l ) does not depend on the degree of its margin violation. Thus, the changes in the gradient from one iteration to the next are determined entirely by the differences between the sets N t and N t +1 . Using this fact, we can derive an extremely efficient update that relates the gradient at one iteration to the gradient at the previous one. The update subtracts the contribu-tions from triples that are no longer active and adds the contributions from those that just became active: For small gradient step sizes, the set N t changes very little from one iteration to the next. In this case, the right hand side of eq. (6) can be computed very fast. To further accelerate the solver, we adopt an active set method. This method is used to monitor the large margin constraints that are actually violated. Note that computing the set N t at each iteration requires checking every triplet ( i, j, l ) with j i for a po-tential margin violation. This computation scales as O ( nd 2 + kn 2 d ), making it impractical for large data sets. To avoid this computational burden, we observe that the great majority of triples do not incur mar-gin violations: in particular, for each training exam-ple, only a very small fraction of differently labeled examples typically lie nearby in the input space. Con-sequently, a useful approximation is to check only a subset of likely triples for margin violations per gra-dient computation and only occasionally perform the full computation. We set this active subset to the list of all triples that have ever violated the margin, ie S that the working set N t does contain all active triples that incur margin violations. This final check is needed to ensure convergence to the correct minimum. If the check is not satisfied, the optimization continues with the newly expanded active set.
 Table 1 shows how quickly the solver works on prob-lems of different sizes. The results in this table were generated by learning a Mahalanobis distance metric on the MNIST data set of 28  X  28 grayscale handwrit-ten digits (LeCun et al., 1998). The digits were pre-processed by principal component analysis (PCA) to reduce their dimensionality from d = 784 to d = 169. We experimented by learning a distance metric from different subsets of the training examples. The experi-ments were performed on a standard desktop machine with a 2.0 GHz dual core 2 processor. For each ex-periment, the table shows the number of training ex-amples, the CPU time to converge, the number of ac-tive constraints, the total number of constraints, and the kNN test error (with k = 3). Note that for the full MNIST training set, the semidefinite program has over three billion large margin constraints. Neverthe-less, the active set method converges in less than four hours X  X rom a Euclidean distance metric with 2 . 33% test error to a Mahalanobis distance metric with 1 . 72% test error. Nearest neighbor search can be accelerated by storing training examples in hierarchical data structures (Liu et al., 2005). These data structures can also be used to reduce the training and test times for LMNN classifi-cation. In this section, we describe how these speedups are obtained using metric ball trees. 4.1. Ball trees We begin by reviewing the use of ball trees (Liu et al., 2005) for fast kNN search. Ball trees recursively par-tition the training inputs by projecting them onto di-rections of large variance, then splitting the data on the mean or median of these projected values. Each subset of data obtained in this way defines a hyper-sphere (or  X  X all X ) in the multidimensional input space that encloses its training inputs. The distance to such a hypersphere can be easily computed for any test in-put; moreover, this distance provides a lower bound on the test input X  X  distance to any of the enclosed train-ing inputs. This bound is illustrated in Fig. 1. Let S be the set of training inputs inside a specific ball with radius r . The distance from a test input ~x t to any training input ~x i  X  S is bounded from below by: These bounds can be exploited in a tree-based search for nearest neighbors. In particular, if the distance to the currently k th closest input ~x j is smaller than the bound from eq. (7), then all inputs inside the ball S can be pruned away. This pruning of unexplored subtrees can significantly accelerate kNN search. The same basic strategy can also be applied to kNN search under a Mahalanobis distance metric. 4.2. Speedups We first experimented with ball trees to reduce the test times for LMNN classification. In our experiments, we observed a factor of 3 x speed-up for 40-dimensional data and a factor of 15 x speedup for 15-dimensional data. Note that these speedups were measured rel-ative to a highly optimized baseline implementation of kNN search. In particular, our baseline implemen-tation rotated the input space to align its coordinate axes with the principal components of the data; the coordinate axes were also sorted in decreasing order of variance. In this rotated space, distance computations were terminated as soon as any partially accumulated results (from leading principal components) exceeded the currently smallest k distances from the kNN search in progress.
 We also experimented with ball trees to reduce the training times for LMNN classification. To reduce training times, we integrated ball trees into our special-purpose solver. Specifically, ball trees were used to accelerate the search for so-called  X  X mpostors X . Recall that for each training example ~x i and for each of its similarly labeled target neighbors ~x j , the im-postors consist of all differently labeled examples ~x l postors dominates the computation time in the train-ing procedure for LMNN classification. To reduce the amount of computation, the solver described in sec-tion 3 maintains an active list of previous margin viola-tions. Nevertheless, the overall computation still scales as O ( n 2 d ), which can be quite expensive. Note that we only need to search for impostors among training examples with different class labels. To speed up train-ing, we built one ball tree for the training examples in each class and used them to search for impostors (as the ball-tree creation time is negligible in comparison with the impostor search, we re-built it in every iter-ation). We observed the ball trees to yield speedups ranging from a factor of 1 . 9 x with 10-dimensional data to a factor of 1 . 2 x with 100 dimensional data. 4.3. Dimensionality reduction Across all our experiments, we observed that the gains from ball trees diminished rapidly with the dimen-sionality of the input space. This observation is con-sistent with previous studies of ball trees and NN search. When the data is high dimensional, NN search is plagued by the so-called  X  X urse of dimensionality X . In particular, distances in high dimensions tend to be more uniform, thereby reducing the opportunities for pruning large subtrees.
 The curse of dimensionality is often addressed in ball trees by projecting the stored training inputs into a lower dimensional space. The most commonly used methods for dimensionality reduction are random pro-jections and PCA. Despite their widespread use, how-ever, neither of these methods is especially geared to preserve (or improve) the accuracy of kNN classifica-tion.
 We experimented with two methods for dimensionality reduction in the particular context of LMNN classifica-tion. Both methods were based on learning a low-rank Mahalanobis distance metric. Such a metric can be viewed as projecting the original inputs into a lower di-mensional space. In our first approach, we performed a singular value decomposition (SVD) on the full rank solution to the semidefinite program in section 2. The full rank solution for the distance metric was then re-placed by a low rank approximation based on its lead-ing eigenvectors. We call this approach LMNN-SVD. In our second approach, we followed a suggestion from previous work on LMNN classification (Torresani &amp; Lee, 2007). In this approach, we explicitly parame-terized the Mahalanobis distance metric as a low-rank matrix, writing M = L  X  L , where L is a rectangular matrix. To obtain the distance metric, we optimized the same objective function as before, but now in terms of the explicitly low-rank linear transformation L . The optimization over L is not convex unlike the original optimization over M , but a (possibly local) minimum can be computed by standard gradient-based methods. We call this approach LMNN-RECT.
 Fig. 2 shows the results of k NN classification from both these methods on the MNIST data set of handwritten digits. For these experiments, the raw MNIST im-ages (of size 28  X  28) were first projected onto their 350 leading principal components. The training pro-cedure for LMNN-SVD optimized a full-rank distance metric in this 350 dimensional space, then extracted a low-rank distance metric from its leading eigenvectors. The training procedures for LMNN-RECT optimized a low-rank rectangular matrix of size r  X  350, where r varied from 15 to 40. Also shown in the figure are the results from further dimensionality reduction us-ing PCA, as well as the baseline k NN error rate in the original (high dimensional) space of raw images. The speedup from ball trees is shown at the top of the graph. The amount of speedup depends significantly on the amount of dimensionality reduction, but very little on the particular method of dimensionality re-duction. Of the three methods compared in the figure, LMNN-RECT is the most effective, improving signif-icantly over baseline kNN classification while operat-ing in a much lower dimensional space. Overall, these results show that aggressive dimensionality reduction can be combined with distance metric learning. The originally proposed framework for LMNN clas-sification has one clear limitation: the same Maha-lanobis distance metric is used to compute distances everywhere in the input space. Writing the metric as M = L  X  L , we see that Mahalanobis distances are equivalent to Euclidean distances after a global lin-ear transformation ~x  X  L ~x of the input space. Such a transformation cannot adapt to nonlinear variabilities in the training data.
 In this section, we describe how to learn different Ma-halanobis distance metrics in different parts of the in-put space. We begin by simply describing how such a collection of local distance metrics is used at test time. Assume that the data set is divided into p disjoint par-titions { P  X  } p  X  =1 , such that P  X   X  P  X  = {} for any  X  6 =  X  and S  X  P  X  = { ~x i } n i =1 . Also assume that each parti-tion P  X  has its own Mahalanobis distance metric M  X  for use in kNN classification. Given a test vector ~x t , we compute its squared distance to a training input ~x i in partition  X  i as: These distances are then sorted as usual to determine nearest neighbors and label the test input. Note, how-ever, how different distance metrics are used for train-ing inputs in different partitions.
 We can also use these metrics to compute distances between training inputs, with one important caveat. Note that for inputs belonging to different partitions, the distance between them will depend on the par-ticular metric used to compute it. This asymmetry does not present any inherent difficulty since, in fact, the dissimilarity measure in kNN classification is not required to be symmetric. Thus, even on the train-ing set, we can use multiple metrics to measure dis-tances and compute meaningful leave-one-out kNN er-ror rates. 5.1. Learning algorithm In this section we describe how to learn multiple Ma-halanobis distance metrics for LMNN classification. Each of these metrics is associated with a particular cluster of training examples. To derive these clusters, we experimented with both unsupervised methods, such as the k -means algorithm, and fully supervised methods, in which each cluster contains the training examples belonging to a particular class.
 Before providing details of the learning algorithm, we make the following important observation. Multiple Mahalanobis distance metrics for LMNN classification cannot be learned in a decoupled fashion X  X hat is, by solving a collection of simpler, independent problems of the type already considered (e.g., one within each partition of training examples). Rather, the metrics must be learned in a coordinated fashion so that the distances from different metrics can be meaningfully compared for kNN classification. In our framework, such comparisons arise whenever an unlabeled test ex-ample has potential nearest neighbors in two or more different clusters of training examples.
 Our learning algorithm for multiple local distance met-rics { M  X  } p  X  =1 generalizes the semidefinite program for ordinary LMNN classification in section 2. First, we modify the objective function so that the distances to target neighbors ~x j are measured under the met-ric M  X  in (a) so that the distances to potential impostors ~x l are measured under the metric M  X  place the single positive semidefinite constraint in (c) by multiple such constraints, one for each local met-ric M  X  . Taken together, these steps lead to the new semidefinite program:
Minimize P j i h d 2 M subject to: Note how the new constraints in (a) couple the dif-ferent Mahalanobis distance metrics. By jointly op-timizing these metrics to minimize a single objective function, we ensure that the distances they compute can be meaningfully compared for kNN classification. 5.2. Results We evaluated the performance of this approach on five publicly available data sets: the MNIST data set 1 of handwritten digits ( n = 60000, c = 10), the 20-Newsgroups data set 2 of text messages ( n = 18827, c = 20), the Letters data set 3 of distorted computer fonts ( n =14000, c =26), the Isolet data set 4 of spoken letters ( n =6238, c =26), and the YaleFaces 5 data set of face images ( n = 1690, c = 38). The data sets were preprocessed by PCA to reduce their dimensionality. The amount of dimensionality reduction varied with each experiment, as discussed below.
 To start, we sought to visualize the multiple metrics learned in a simple experiment on MNIST handwritten digits of zeros, ones, twos, and fours. For ease of visu-alization, we worked with only the leading two princi-pal components of the MNIST data set. Fig. 3 shows these two dimensional inputs, color-coded by class la-bel. With these easily visualized inputs, we minimized the objective function in section 5.1 to learn a special-ized distance metric for each type of handwritten digit. The ellipsoids in the plot reveal the directions ampli-fied by the local distance metric of each digit class. Notably, each distance metric learns to amplify the di-rection perpendicular to the decision boundary for the nearest, competing class of digits.
 Our next experiments examined the performance of LMNN classification as a function of the number of dis-tance metrics. In these experiments, we used PCA to reduce the input dimensionality to d =50; we also only worked with a subset of n = 10000 training examples of MNIST handwritten digits. To avoid overfitting, we used an  X  X arly stopping X  approach while monitor-ing the kNN error rates on a held-out validation set consisting of 30% of the training data.
 Fig. 4 shows the test kNN error rates on the Isolet and MNIST data sets as a function of the number of distance metrics. In these experiments, we explored both unsupervised and supervised methods for parti-tioning the training inputs as a precursor to learning local distance metrics. In the unsupervised setting, the training examples were partitioned by k -means cluster-ing, with the number of clusters ranging from 1 to 30 (just 1 cluster is identical to single-matrix LMNN). As k-means clustering is prone to local minima, we aver-aged these results over 100 runs. The figure shows the average test error rates in red, as well as their stan-dard deviations (via error bars). In the supervised set-ting, the training examples were partitioned by their class labels, resulting in the same number of clusters as classes. The test error rates in these experiments are shown as blue crosses. In both the unsupervised and supervised settings, the test error rates decreased with the use of multiple metrics. However, the im-provements were far greater in the supervised setting. Finally, our last experiments explored the improve-ment in kNN error rates when one distance metric was learned for the training examples in each class. In these experiments, we used the full number of train-ing examples for each data set. In addition, we used PCA to project the training inputs into a lower di-mensional subspace accounting for at least 95% of the data X  X  total variance. Fig. 5 shows generally consis-tent improvement in training and test kNN error rates, though overfitting is an issue, especially on the 20-NewsGroups and YaleFaces data sets. This overfitting is to be expected from the relatively large number of classes and high input dimensionality of these data sets: the number of model parameters in these exper-iments grows linearly in the former and quadratically in the latter. On these data sets, only the use of a validation set prevents the training error from vanish-ing completely while the test error skyrockets. On the other hand, a significant improvement in the test er-ror rate is observed on the largest data set, that of MNIST handwritten digits. On this data set, multiple distance metrics yield a 1 . 18% test error rate X  X  highly competitive result for a method that does not take into account prior domain knowledge (LeCun et al., 1998). In this paper, we have extended the original framework for LMNN classification in several important ways: by describing a solver that scales well to larger data sets, by integrating metric ball trees into the training and testing procedures, by exploring the use of dimension-ality reduction for further speedups, and by showing how to train different Mahalanobis distance metrics in different parts of the input space. These exten-sions should prove useful in many applications of kNN classification. More generally, we also hope they spur further work on problems in distance metric learning and large-scale semidefinite programming, both areas of great interest in the larger field of machine learning. This research is based upon work supported by the Na-tional Science Foundation under Grant No. 0238323.
