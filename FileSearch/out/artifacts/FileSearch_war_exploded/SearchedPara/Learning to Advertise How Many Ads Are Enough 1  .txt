 Sponsored search places ads on the result pages of web search engines for dif-ferent queries. All major web search engin es (Google, Microsoft, Yahoo!) derive significant revenue from such ads. Howev er, the advertisement problem is often treated as the same problem as traditional web search, i.e., to find the most relevant ads for a given query. One different and also usually ignored problem is  X  X ow many ads are enough for a sponso red search X . Recently, a few research works have been conducted on this problem [5,6,8,17]. For example, Broder et al. study the problem of  X  X hether to swing X , that is, whether to show ads for an incoming query [3]; Zhu et al. propose a m ethod to directly optimize the revenue in sponsored search [22]. In most existi ng search engines, the problem has been treated as an engineering issue. For example, some search engine always displays a fixed number of ads and some search eng ine uses heuristic rules to determine the number of displayed ads. However, the key question is still open, i.e., how to optimize the number of displayed ads for an incoming query? Motivation Example. Figure 1 (a) illustrates an example of sponsored search. The query is  X  X ouse X  and the first one is a suggested ad with yellow background, and the search results are listed in the bottom of the page. Our goal is to predict the number of displayed ads for a given query. The problem is not easy, as it is usually difficult to accurately define the relevance between an ad and the query. We conduct several statistical studies on the log data of a commercial search engine, the procedure is in two-stage: First, for each query, we obtain all the returned ads by the search engine; Second, we use some method to remove several unnecessarily displayed ads (detailed in Section 4). Figure 1 (b) and (c) are the statistical results on a large click-through data (DS BroadMatch dataset in section 3). The number of  X  X emoved ads X  refers to the total number of ads cut off in the second stage for all the queries. Figure 1(b) shows how #clicks and Click-Through-Rate (CTR) vary with the number of removed ads. We see that with the number of removed ads increasing, #clicks decreases, while CTR clearly increases. This matches our intuition well, displaying more ads will gain more clicks, but if many of them are irrelevant, it will hurt CTR. Figure 1(c) further shows how CTR increases as #clicks decreases. This is very interesting. It is also reported that many clicks on the first displayed ad are done before the users realize that it is not the first sea rch result. A basic idea here is that we can remove some displayed ads to ach ieve a better performance on CTR.
Thus, the problem becomes how to predict the number of displayed ads for an incoming query which is non-trivial and poses two unique challenges:  X  Ad ranking. For a given query, a list of related ads will be returned. Ads  X  Ad Number Prediction. After we get the ranking list of ads, it is necessary Contributions. To address the above two challenges, we propose a learning-based framework. To summarize, our contributions are three-fold:  X  We performed a deep analysis of the click-through data and found that when  X  We developed a method to determine the number of displayed ads for a given  X  We conducted experiments on a commercial search engine and experimental Suppose we have the click-through data collected from a search engine, each ad q ( p ) is the ad at the position p returned by the search engine and c q ( p )isa binary indicator which is 1 if this ad is clicked under this query, otherwise 0. For each ad ad q ( p ), there is an associated feature vector x q ( p ) extracted from a query-ad pair ( q, ad q ( p )) and can be utilized for ranking model learning. Ad Ranking: Given the training data denoted by L = { q, AD q ,C q } q  X  X  in which Q is the query collection, for each q  X  X  , AD q = { ad q (1) ,  X  X  X  ,ad q ( n q ) } n q is the total number of displayed ads. Similarly, the test data can be denoted by T = { q ,AD q } q  X  X  where Q is the query collection. In this task, we try to learn a ranking function for displaying the query-related ads by relevance. For each query q  X  X  , the output of this task is the ranked ad list R q = { Ad Number Prediction: Given the ranked ad list R q for query q ,inthis task we try to determine the number of displayed ads k and then display the top-k ads. The output of this task can be denoted by a tuple O = { q ,R k q } q  X  X  where R k q are the top-k ads from R q .

Our problem is quite different from exi sting works on advertisement recom-mendation. Zhu et al. propose a method to directly optimize the revenue in sponsored search [22]. However, they o nly consider how to maximize the rev-enue, but ignore the experience of users. Actually, when no ads are relevant to the users X  interests, displaying irrelevant ads may lead to much complains from the users and even train the user to ignore ads. Broder et al. study the problem of  X  X hether to swing X , that is, whether to show ads for an incoming query [3]. However, they simplify the problem as a binary classification problem, while in most real cases, the problem is more complex and often requires a dynamic number for the displayed ads. Few works have been done about dynamically predicting the number of displayed ads for a given query.
 3.1 Data Set In this paper, we use one month click-through data collected from the log of a famous Chinese search engine Sogou 1 , the search department of Sohu company which is a premier online brand in China and indispensable to the daily life of millions of Chinese. In the data set, each record consists of user X  X  query, ad X  X  keyword, ad X  X  title, ad X  X  description, displayed position of ad, and ad X  X  bidding price . For the training dataset DS BroadMatch, the total size is around 3.5GB which contains about 4 million queries, 60k keywords and 80k ads with 25 million records and 400k clicks. In this dataset, the ad is triggered when there are common words between keyword and user X  X  search query. We also have another little training dataset DS ExactMatch which a subset of DS BroadMatch and contains about 28k queries, 29k keywords and 53k ads with 4.4 million records and 150k clicks. In DS ExactMatch, the ad is triggered only when there are exactly matched words between keywords and user X  X  search query. For the test set, the total size is about 90MB with 430k records and 1k clicks. 3.2 Position vs. Click-Through Rate (CTR) Figure 2 illustrates how CTR varies according to the positions on the dataset DS ExactMatch. We can see that the actions of clicks mainly fall into the top three positions of ad list for a query, so the clicks are position-dependent. 3.3 Click Entropy In this section we conduct several data analyses based on the measure called click entropy. For a given query q , the click entropy is defined as follows [11]: is the ratio of the number of clicks on ad to the number of clicks on query q .
A smaller click entropy means that the majorities of users agree with each other on a small number of ads while a larger click entropy indicates a bigger query diversity, that is, many different ads are clicked for the same query. Click Entropy vs. #Removed ads. Figure 3 shows how the number of removed ads varies with the click entropy of a query on the dataset DS BroadMatch. By this distribution, for a query, if we want to remove a given number of ads, we can automatically obtain the threshold of the click entropy which can be utilized for helping determine the number of displayed ads.
 Click Entropy vs. Max-Clicked-Position. For a query, Max-Clicked-Position is the last position of clicked ad. Figure 4 shows how the Max-Clicked-Position varies with the click entropy on the two datasets. The observations are as follows:  X  As the click entropy increases, the Max-Clicked-Position will be larger.  X  The values of click entropy on the dataset DS ExactMatch are in a smaller  X  On the dataset DS ExactMatch, the clicked positions vary from 1 to 10, while Click Entropy vs. QueryCTR. Figure 5 shows how QueryCTR varies with the click entropy of a query. QueryCTR is the ratio of the number of clicks of a query to the number of impressions of this query. We can conclude that when the click entropy of a query is greater than 3, the QueryCTR will be very near zero. This observation is very interes ting, the QueryCTR is the summation of the ads X  click entropy, so we can utilize this observation to help determine the number of displayed ads for a given query.
 4.1 Basic Idea We propose a two-stage approach corresponding to the two challenges of our problem. First, we learn a function for predicting CTR based on the click-through data by which the ads can be ranked. Second, we propose a heuristic method to determine the number of displayed ads based on the click entropy of query. For a query, the click entropy is the summation of entropy of each clicked ads, so we consider the ads in a top-down mode, once the addition of one ad leads to the excess of a predefined threshold by the click entropy, we then cut off the rest ads. By this way, we can automatically determine the number of displayed ads. 4.2 Learning Algorithm Ad Ranking: In this task, we aim to rank all the related ads of a given query by R d  X { 0 , 1 } from the click-through data where d is the number of features.
Let ( x, c )  X  L be an instance from the training data where x  X  R d is the feature vector and c  X  X  0 , 1 } is the associated click indicator. In order to predict the CTR of an ad, we can learn a logistic regression model as follows whose output is the probability of that ad being clicked: is the predicted CTR of that ad whose feature vector is x .

For training, we can use the maximum likelihood method for parameter learn-ing; for test, given a query, we can use the learnt logistic regression model for predicting the CTR of one ad. Ad Number Prediction: Given a query q , we can incrementally add one ad to the set of displayed ads in the top-down mode, and the clicked ads will contribute to the click entropy. We can repeat this process until the click entropy exceeds a predefined threshold, and then stop. By then, the size of that set is exactly the number of the displayed ads for that query.

It is also worth noting that how to automatically determine the threshold of click entropy. Figure 3 demonstrates that when the click entropy of a query exceeds 3, the QueryCTR of that query w ill be very near zero. According to this relationship, we can learn a fitting model(eg. regression model) from the statistics of data, then for a given number of ads to be cut down, we can use the learned model to predict the threshold of click entropy.

The method can also be applied to a new query. Based on the learned logistic model, we can first predict the CTR for each ad related to the new query [17], then predict the number of ads based on the click entropy for the new query. 4.3 Feature Definition Table 1 lists all the 30 features extracted from query-ad title pair, query-ad pair, and query-keyword pair which can be divided into three categories: Relevance-related, CTR-related and Ads-related.
 Relevance-related features. The relevance-related features consist of low-level and high-level ones. The low-level features include highlight, TF, TF*IDF and the overlap, which can be used to measure the relevance based on keyword matching. The high-level features include cosine similarity, BM25 and LMIR, which can be used to measure the relevance beyond keyword matching. CTR-related features. AdCTR can be defined as the ratio of the number of ad clicks to the total number of ad impressions. Similarly, we can define keyCTR and titleCTR. KeyCTR corresponds to the multiple advertising for the specific keyword. And titleCTR corresponds to multiple advertising with the same ad title. We also introduce features key TitleCTR and keyAdCTR, because usually the assignment of a keyword to an ad is determined by the sponsors and the search engine company, the quality of this assignment will affect the ad CTR. Ads-related features. We introduce some features for ads themself, such as the length of ad title, the bidding price, the match type and the position. 5.1 Evaluation, Baselines and Experiment Setting Evaluation. We qualitatively evaluate all the methods by the total number of clicks for all queries in the test dataset: # click ( q )= n q p =1 c q ( p ).
For evaluation, we first remove a certain number of ads for a query in the test dataset by different ways, and then find the way which leads to the least reduction of the number of clicks.
 Baselines. In order to quantitatively evaluate our approach, we compare our method with two other baselines. Assume that we want to cut down N ads in total. For the first baseline LR CTR, for each query in the test dataset, we predict the CTRs for the query-related ads, and then pool the returned ads for all the queries and re-rank them by the predicted CTRs, finally remove the last N ads with lowest CTRs. The major problem for LR CTR is that it cannot be updated in an online manner, that is, we need to know all the predicted CTRs for all the queries in the test dataset in advance. This is impossible for determining the removed ads for a given query. For the second baseline LR RANDOM, we predict the CTRs of the query-related ads for each query in the test dataset, and then only remove the last ad with some probability for each query. We can tune the probability for removing a certain number of ads, the disadvantage is that there is no explicit correspondence between these two. For our proposed approach LR CE, we first automatically determine the threshold of click entropy for a query and then use Algorithm 1 to remove the ads. Our approach does not suffer from the disadvantages of the above two baselines.
 Experiment Setting. All the experiments are carried out on a PC running Windows XP with AMD Athlon 64 X2 Processor(2GHz) and 2G RAM.

We use the predicted CTRs from the ad ranking task to approximate the term P ( ad | q )inEq.1inthisway: P ( ad | q )= CTR ( ad ) CTR ( ad i ) are the predicted CTRs of the current ad and the i -th related ad for query q respectively. For the training, we use the feature  X  X osition X ; while for testing, we set the feature  X  X osition X  as zero for all instances. 5.2 Results and Analysis #Removed ads vs. #Clicks. Figure 6(a) shows all the results of two baselines and our approach. From that, the main observations are as follows:  X  Performance. The method LR CTR obtains the optimal solution by the  X  User specification. From the viewpoint of the search engines, they may #Removed ads vs. CTR and #Clicks. Figure 6(b) shows how the total number of clicks and the number of removed ads vary with the threshold of click entropy. As the threshold of click entro py increases, the total number of clicks increases while the number of removed ads decreases. 5.3 Feature Contribution Analysis All the following analyses are conducted on the dataset DS ExactMatch. Features vs. keyTitleCTR. Figure 7 shows some statistics of the ad click-through data. When the values of features in 7 (a) and (c) increase, the keyTi-tleCTRs also increase, while the feature in 7 (b) increases, the keyTitleCTR first increases and then decreases. Feature Ranking. Recursive feature eliminatio n(RFE) uses greedy strategy for feature selection [22]. At each step, the algorithm tries to find the most useless feature and eliminate it. In this analysis, we use the measure Akaike Information Criterion (AIC) to select us eful features. After excluding one feature, the lower the increase of AIC is, the more useless the removed feature is. The process will be repeated until only one feature left. Finally, we can get a ranking list of our features, and the top three are keyTitleCTR, position and cos sim of title. CTR-based advertisement. In this category, people try to predict CTRs, by which the query-related ads can be ran ked. These methods can be divided into two main categories: click model [12,23] and regression model [15].
Regarding click model, Agarwal et al. propose a spatio-temporal model to estimate CTR by a dynamic Gamma-Poisson model [1]. Craswell et al. propose four simple hypotheses for explaining the position bias, and find that the cas-cade model is the best one [9]. Chapelle and Zhang propose a dynamic Bayesian network to provide an unbiased estimation of relevance from the log data [5]. Guo et al. propose the click chain model based on Bayesian modeling[13].
Regarding regression model, Richardson et al. propose a positional model and leverage logistic regression to predict the CTR for new ads [17]. Chen et al. de-sign and implement a highly scalable and efficient algorithm based on a linear Poisson regression model for behavioral targeting in MapReduce framework [6].
There are also many other works [14]. For example, Dembczy  X  nski et al. pro-pose a method based on decision rule [10].
 Revenue-based advertisement. In this category, people try to take relevance or revenue into consideration rather than CTR while displaying ads.
Radlinski et al. propose a two-stage approach to select ads which are both relevant and profitable by rewriting queries [16]. Zhu et al. propose two novel learning-to-rank methods to maximize search engine revenue while preserving high quality of displayed ads [22]. Ciaramita et al. propose three online learning algorithms to maximize the number of clicks based on preference blocks [8]. Streeter et al. formalize the sponsored search problem as an assignment of items to positions which can be efficiently solved in the no-regret model [19]. Carterette and Jones try to predict document relevance from the click data [4]. Threshold-based methods. In this category, people try to utilize thresholds for determining whether to display ads or where to cut off the ranking list.
Broder et al. propose a method based on global threshold to determine whether to show ads for a query because showing irrelevant ads will annoy the user [3]. Shanahan et al. propose a parameter free threshold relaxation algorithm to ensure that support vector machine will have exce llent precision and relatively high recall [18]. Arampatzis et al. propose a threshold optimization approach for determining where to cut off a ranking list based on score distribution [2]. In this paper, we study an interesting problem that how many ads should be displayed for a given query. There are two challenges: ad ranking and ad number prediction. First, we conduct extensive analyses on real click-through data of ads and the two main observations are 1) wh en the click entropy of a query exceeds a threshold the CTR of that query will be very near zero; 2) the threshold of click entropy can be automatically dete rmined when the number of removed ads is given. Second, we propose a learning approach to rank the ads and to predict the number of displayed ads for a given query. Finally, the experimental results on a commercial search engine valid ate the effectiveness of our approach.
Learning to recommend ads in sponsored search presents a new and interesting research direction. One interesting issue is how to predict the user intention before recommending ads [7]. Another interesting issue is how to exploit click-through data in different domains where the click distributions may be different for refining ad ranking [21]. It would also be interesting to study how collective intelligence (social influence between users for sentiment opinions on an ad) can help improve the accuracy of ad number prediction [20].
 Acknowledgments. Songcan Chen and Bo Wang are supported by NSFC (60773061), Key NSFC(61035003). Jie Tang is supported by NSFC(61073073, 60703059, 60973102), Chinese National Ke y Foundation Re search (60933013, 61035004) and National High-tech R&amp;D Program(2009AA01Z138).

