 Document clustering is a powerful technique to detect topics and their relations for information analysis and organization. However, unlike document categorization where a set of labels or terms is predefined for each category, clustered documents require post-assignment of concise and descriptive titles to help analysts interpret the results. Most existing work selects the title words from the terms contained in the documents themselves. Although this is justifiable, this may not be sufficient. It would be desirable to further suggest gene ric topic terms for ease of analysis, especially in the applications where documents cover a wide range of domain knowledge. Examples of this need are often found in topic analysis for patent or scientific publications [1-3]. 
In this work, we attempt to automatically create generic labels which do not necessarily exist in the clustered documents for easier cluster interpretation. As an example, if documents in a cluster were talking about tables , chairs , and beds , then a title labeled  X  furniture  X  would be perfect for this cluster, especially when this hypernym does not occur in it. This kind of problem was often solved by human experts, such as those in [4-5], where cluster titles were given manually. To make our automatic approach feasible, external resources such as WordNet or other hierarchical knowledge structures are used. Our method first selects content-indicative terms for each cluster. A hypernym search algorithm is then applied to map these terms into generic titles. The rest of the paper is organized as follows: Section 2 reviews some related work. Section 3 introduces our method for content-indicative term extraction. Section 4 describes the hypernym search algorithm based on WordNet. Section 5 details the experiments that evaluate our method. Section 6 discusses the results and suggests possible improvement. Section 7 concludes this work and shows its implications. Labeling a clustered set of documents is an inevitable task in text clustering applications. Automatic labeling methods mainly rely on extracting significant terms from clustered documents, where the term significance can be calculated very differently from clustering algorithms to algorithms. 
For example, in the vector space model, where clusters are represented as weighted sums or centroids of the document vectors, terms with heaviest weights in the cluster TF x IDF (Inverse Document Frequency) weighting scheme. As to its effectiveness, the authors in [9] pointed out (although without experiments) that the simple centroid-based approach outperformed the probabilistic odds scheme which computes the ratio conditional probabilities of the term in other clusters. 
In the Self-Organization Map (SOM) method [10], where clusters are organized in This measure is the square of the relative term frequency in the cluster normalized by the sum of the relative term frequencies in other distant clusters. 
In an application to group terms for detecting events over time [11], the cluster title consists of the highest ranked named entity followed by the highest ranked noun phrase. The ranks of these terms were obtained by sorting the maximum chi-square values of the terms occurring in a time interval. 
In clustering web search results, the longest phrases occurred in most documents in a cluster were used as its title [12]. 
In other related fields, such as document summarization and translation, there were tasks in the Document Understanding Conference (DUC) [13] to generate very short summaries. These short 10-words summaries have the potential to serve as cluster titles. However, most participants use extraction-based methods [14]. Even there were studies that generate document titles not from the document themselves, a large corpus of documents with human-assigned titles is required to train the  X  X ranslation model X  so as to map document words into human-assigned titles [15]. Besides, these summaries tend to be event-descriptive rather than topic-indicative for a set of documents. 
As can be seen, despite there are various techniques to label a set of documents, there are few studies that attempted to deal with the problem that we propose. The methods to extract cluster descriptors in the above-mentioned studies are mostly related to their clustering algorithms. In our application, we would like to have a solutions from text categorization where selecting the best content-revealing or category-predictive features has been widely studied. 
Yang et al [16] had compared five different methods for selecting category-specific categorization performance. The chi-square method computes the relatedness of term T with respect to category C in the manner: where TP (True Positive), FP (False Positive), FN (False Negative), and TN (True Negative) denote the number of documents that belong or not belong to C while containing or not containing T, respectively. With content terms sorted by chi-square values in descending order, top-ranked terms can be selected as the cluster title. 
However, chi-square does not distinguish negatively related terms from positively ones. A remedy to this problem is the use of the correlation coefficient ( CC ), which is just the square root of the chi-square: 
As pointed out by Ng et al [17], correlation coefficient selects exactly those words that are highly indicative of membership in a category, whereas the chi-square method membership in that category. This is especially true when the selected terms are in small number. As an example, in a small real-world collection of 116 documents with only two exclusive categories: construction vs. non-construction in civil engineering tasks, some of the best and worst terms that are computed by both methods are shown in Table 1. As can be seen,  X  X ngineering X  is a lowest-ranked term (-0.7880) based on correlation coefficient in the non-construction category, while it is a top-ranked term in both categories based on chi-square (0.6210 is the square of -0.7880). Therefore, instead of chi-square, correlation coefficient is used as our basic word selection method. 
A further analysis of the correlation coefficient method reveals that it may be effective for large number of short documents. But without considering the occurring frequency of the term in each document (i.e., TF ), it tends to select category-specific terms that are not generic enough for long documents. Therefore, we choose only those terms whose document frequency in a cluster exceeds a ratio r of the number of documents in that cluster. We denote this revised method as CC r , where r is a tunable parameter and is 0.5 in our implementation. Another remedy is to multiply the term X  X  CC with its total term frequency in the cluster (TFC), denoted as CC x TFC , where TFC is the sum of a term X  X  TF over all documents in the cluster. The cluster descriptors generated in the above may not be topic-indicative enough to well summarize the contents of the clusters. One might need to map the identified [18]). If the categories have existing data for training, this mapping can be recast into a standard text categorization problem, to which many solutions can be applied. Another need arises from the situation that there is no suitable classification system at often solved by human experts, where cluster titles are given manually. Below we propose an automatic solution by use of an extra resource, i.e., WordNet. 
WordNet is a digital lexical reference system [19]. English nouns, verbs, adjectives and adverbs are organized into synonym sets. Different relations, such as hypernym, hyponym, meronym, or holonym, are defined to link the synonym sets. With these structures, one can look up in WordNet all the hypernyms of a set of given terms and then choose the best among them with some heuristic rules. Since the hypernyms were organized hierarchically, the higher is the level, the more generic are the hypernyms. To maintain the specificity of the set of terms while revealing their general topics, the heuristics have to choose as low-level common hypernyms as possible. When there are multiple choices, ranks should be given to order the hypernyms in priority. 
In our implementation, we look up for each given term all its hypernyms alone the path up to the root in the hierarchical tree. The number of occurrence ( f ) and the depth in the hierarchy ( d ) of an encountered hypernym are recorded. With the root being given a depth value of 0, a weight proportional to the normalized f and d is calculated for each hypernym as follows: where nt is the number of given terms to normalize the occurrence f to a value ranges from 0 to 1 and c (0.125 in our implementation) is a constant to control the steepness of the sigmoid function 1/(1+exp(-c x d)) whose value approaches 1 (-1) for large range of the sigmoid is from 0.5 to 1. It is thus subtracted with 0.5 and then multiplied by 2 to map the value of d into the normalized range: 0 to 1. Note that a term having no hypernym or not in WordNet is omitted from being counted in nt . Also note that a term can have multiple hypernyms and thus multiple paths to the root. A hypernym is counted only once for each given term, no matter how many times the multiple paths of this term pass this hypernym. This weight is finally used to sort the hypernyms in decreasing order to suggest priority. 
In the example mentioned at the beginning, where the 3 terms were given: table , chair , and bed , their hypernym:  X  furniture  X  did result from the above calculation with a highest weight 0.3584 based on WordNet version 1.6. 5.1 Document Collections Two collections were used to evaluate the proposed method. One contains 612 patent documents downloaded on 2005/06/15 from the USPTO X  X  website [20] with  X  X ational Science Council X  (NSC) as the search term in the assignee field. NSC is the major government agency that sponsors research activities in Taiwan. Institutes, NSC. Once the research results have been used to apply for US patents, the intellectual property rights belong to NSC. In other words, NSC becomes the assignee of the patents. (However, this policy has been changed since year 2000 so that the number of NSC patents per year declines since then.) Due to this background, these documents constitute a knowledge-diversified collection with relative long texts (about 2000 words per document) describing various advanced technical details. Analysis of the topics in this collection becomes a non-trivial task as very few analysts know of such diversified technologies and fields. Although each patent has pre-assigned International Patent Classification (IPC) or US Patent Classification (UPC) codes, many of these labels are either too general or too specific such that they do not fit the intended knowledge structures for topic interpretation. Therefore, using them alone do not meet the requirement of the analysis. As such, computer-suggested labels play important roles in helping humans understand this collection. This work is in fact motivated by such a need. 
The other collection is a subset of the widely used Reuters-21578 collection for text categorization. It contains brief stories (about 133 words per story) in the financial domain. Each document was assigned at least one category to denote its topics and there are a total of 90 categories based on Yang X  X  preprocessing rules [21]. In this work we used the largest 10 categories of them to evaluate our method. 5.2 Document Clustering Since it would be hard to have a consensus on the number of clusters to best organize the NSC collection, we clustered it using different options and parameters to get various views on it. Specifically, we analyzed the collection based on document clustering and term clustering as well. The 612 US patent documents were parsed, filtered, segmented, and summarized. Structured information in the documents such as assignees, inventors, and IPC codes were removed. The remaining textual parts were segmented into several main sections (Abstract, Claims, Field of the Invention, Background of the Invention, Summary of the Invention, and Detailed Descriptions) based on the regularity of the patent summarized using extraction-based methods [22] to select at most 6 best sentences from each section. They were then concatenated to yield the document surrogates for key term extraction, term co-occurrence analysis, indexing, and clustering. These document surrogates did not include the Claims section due to its legal-oriented contents. 
From the document surrogates, 19,343 key terms (terms occur at least twice in a document) were extracted. They were used in the term co-occurrence analysis to see which terms are often co-occurred with each other in the same sentence [23]. Only 2714 of the key terms occur in at least two documents and have at least one term co-occurred with them. These 2714 terms were clustered by a complete-link method into 353 small-size clusters, based on how many co-occurred terms they share. These clusters were then repeatedly clustered, again based on the common co-occurred terms ( co-words ), into 101 medium-size clusters, then into 33 large-size clusters, and finally into 10 topic clusters. The reason that we performed this multi-stage clustering is due to the fact that even we set a lowest threshold, we could not obtain as low as 10 similarity threshold set to 0.0. In other words, among the 2714 terms, no two terms in different small-size clusters share the same co-words. Through this term clustering, the topics of the collection were revealed as the key terms were merge into concepts, which in turn were merge into topics or domains. This approach has the merit that it can be efficiently applied to very large collections due to the fast co-occurrence analysis [23]. 
As another way for topic analysis, th e 612 NSC patents were clustered directly based on their summary surrogates. As with the above term clustering, they were first clustered into 91 topics, which in turn were grouped into 21 sub-domains, from which 6 major domains were found. 
We did not cluster the Reuters collection since it already has been organized by its predefined labels. These labels were used to compare with our cluster titles. 5.3 Evaluation of Descriptor Selection For comparison, we used three methods to rank cluster terms for descriptor selection, namely TFC , CC 0.5 , and CC x TFC , as defined in Section 3. These ranking methods clustering as described above. The second is the second-stage document clustering. The third is the third-stage term clustering based on their co-words. For each cluster, at most 5 best terms were selected as its descriptors. 
Two master students majored in library science were invited to compare the relative quality of these descriptors under the same clusters. For each ranking method, counted. Multiple best choices for a cluster are allowed and those cluster titles that are hard to assess can be omitted from being considered. The ranking methods are coded examined titles. The assessment results are shown in Table 2. Note hierarchical clustering structures were shown to the assessors. They were free to examine whatever clusters or sub-clusters they were interested in. This is why the numbers of examined clusters differ between them. 
In spite of this difference, this preliminary experiment shows that cluster descriptors selected by CC 0.5 or CCxTFC are favorable by one assessor, while those generated by TFC are not by either. This is somewhat useful to know, since most past studies use TFC or a variation of it to generate cluster titles. 5.4 Evaluation of Generic Title Generation The proposed title mapping algorithm was applied to the final-stage results of the document and term clustering described above. The first set has 6 clusters and the second has 10. Their best 5 descriptors selected by CCxTFC are shown in the second column of Table 3. 
The proposed method was compared to a similar tool called InfoMap [24] which is developed by the Computational Semantics Laboratory at Stanford University. This WordNet is also used as its reference system, because the output classes are mostly WordNet X  X  terms. Since no technical details about InfoMap were found on the Web, we cannot implement the InfoMap X  X  algorithm by ourselves. Therefore, an agent program was written to send the descriptors to InfoMap and collect the results that it returns. Only the top-three candidates from both methods are compared. They are listed in the last two columns in Table 3, with their weights appended. 
The reasonable classes are marked in bold face in the table. As can be seen, the two methods perform similarly. Both achieve a level of 50% accuracy in either set. 
The 10 largest categories in the Reuters collection constitute 6018 stories. Since method was used to select the cluster descriptors. As can be seen from the third column in Table 4, 8 (those in boldface) out of 10 machine-selected descriptors clearly coincide with the human labels, showing that the effectiveness of the CC method in this collection. When compared to the results made by [25], as shown in Table 5, where mutual information (MI) was used to rank statistically significant features for text categorization, the CC method yields better topical labels than the MI method. 
ID Cluster X  X  Descriptors WordNet InfoMap 
As to the generic titles shown in Table 4, about 70% of the cases lead to reasonable results for both the WordNet and the InfoMap methods. It is known that the Reuters wheat , and corn can be merged into a more generic category, such as foodstuff . As can be seen from Table 4, this situation was automatically captured by the proposed method. 
ID Category Descriptors WordNet InfoMap The application of our generic title generation algorithm to the NSC patent collection leads to barely 50% of the clusters to have reasonable results. However, this is due to the limit of WordNet in this case. WordNet does not cover all the terms extracted from the patent documents. Also WordNet X  X  hypernym structures may not reflect the knowledge domains desired for analyzing these patents. On one hand, if the texts of the documents better fit the vocabulary of WordNet, the performance improves, as is suitable classification system can be found for the document collection to be analyzed, the hypernym search algorithm may lead to more desirable labels. We have tried Google X  X  directory for the NSC collection, but the categories returned from the NSC clusters. However, even with the current level of performance using WordNet, this method, as well as the method to select the cluster descriptors, are still helpful, as noted by some NSC analysts. 
As an example, Figure 1 shows a topic map of the 612 NSC patents. In this figure, ID. With the Multi-Dimensional Scaling (MDS) method [27] to map these clustered (while the absolute position and orientation of each topic does not matter). Details of dashed lines in the figure were those derived from the final-stage document clustering and were labeled by hand with the help of the proposed method. Cluster labeling is important for ease of human analysis. In the attempt to produce generic cluster labels, a hypernym search algorithm based on WordNet is devised. Our method takes two steps: the content-indicative terms are first extracted from the documents; these terms are then map to their common hypernyms. Because the algorithm uses only the depth and occurrence information of the hypernyms, it is general enough to be able to adopt other hierarchical knowledge systems without much revision. Real-case experiments using two very different collections justify our anticipation on the performance of the proposed method. Possible improvements were suggested and have been tried. The equality in performance with other similar systems such as InfoMap suggests that our well-described method are among the very up-to-date approaches to solve the problem like this. Acknowledgments. This work is supported in part by NSC under the grants: NSC 93-2213-E-030-007-and NSC 94-2524-S-003-014-. 
