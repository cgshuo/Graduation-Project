 In this paper, we propose a method to rank and assign weights to query terms according to their impact on the top-ic of the query. We use Search Result Overlap Ratio (SROR) to quantify the overlap of the search results of the full query and a shorten query after removing one term. Intuitively, if the overlap is small, it indicates a big topic shift and the removed term should be discriminative and important. The SROR could be used for measuring query term importance with a search engine automatically. By this way, learning based models could be trained based on a large number of automatically labeled instances and make predictions for fu-ture queries efficiently.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Algorithm, Experimentation, Performance Query Term Ranking, Query Reformulation, Search Results Overlap
Query term ranking aims to assess query terms X  impor-tance and effectiveness. It has many potential applications. For example, it helps for formulating better queries by tun-ing term weights in retrieval models or removing ineffective terms. The important query terms or concepts also help for understanding a query X  X  topic.

Recently, machine learning techniques have been adopt-ed for similar tasks such as classification for key concepts identification [1], regression models for term ranking [3] and learning to rank for concept importance weighting [2]. Most of these approaches need manually labeled training data, either labeled key concepts or document relevance. This paper proposes a simple approach based on Search Result Overlap Ratio (SROR) . The basic idea is to quantify a term X  X  impact by considering the overlap of the search re-sults of the full query and the shorten query leaving out the term. We assume the search results of a query represent its topic. If the overlap is small, it indicates the query X  X  topic shifts a lot after removing the term. So the removed term plays an important role for determining the query X  X  top-ic and should be discriminative. The experimental results on manually labeled queries showed that SROR based ter-m ranking method had acceptable consistence with human judgement. This approach requires searching multiple times for a given query but the whole process can be done auto-matically with a search engine and does not need any human assistance. As a result, it could be used for automatically computing term weights for a large number of queries with a search engine beforehand. Learning based models could be trained using the statistical features extracted from these  X  X abeled X  instances. The learned models can make predic-tions for future queries efficiently. The experimental results on real web queries indicated the reformulated queries with SROR based weighting schemes improved the retrieval per-formance. For a given query q = f t i g , we define the Search Result Overlap Ratio (SROR) for term t i 2 q as where  X  N ( q ) denotes the top N search results of query q returned by a search engine. q : t i refers to the query after removing term t i from query q . The sign # is used to count the size of a set. Heuristically, if SROR ( t i ) is large, it means t is not so important, because this term has little impact on the topic of query q . Otherwise, it indicates this term is discriminative for the full query. Based on SROR ( t i assign weights to terms according to
W eight can be used for both ranking and assigning weight-s to query terms. We demonstrate a query in Table 1. We can see the query term importance presented by normalized W eight ( W eight ( t i ) / human intuition. Similar idea has been used for predicting query performance [5] by measuring the overlap of search re-sults between using the full query and the individual query term. However, individual term may be not enough to indi-cate the query topic sometime. Instead, we borrow the idea from [3] they assume leaving out an important term results in decrease retrieval effectiveness. In our case, we assume removing an important term may lead to topic shift. T able 1: Terms X  SRORs and normalized Weights of the query  X  X ovie free download website X .

The experiments were conducted on real web queries and a web corpus. We used one month query log collected by So-gou 1 during June 2008 consisting of 5604251 unique queries and associated clicks. For 99.67% queries, the query length l ( q ) 10. The web corpus containing 5 TB web pages was also crawled by Sogou during 2008. We used Indri [4] for in-dexing and searching this corpus. The top 30 search results of a input query returned by a search engine were used for computing SROR .
 Term Ranking . We computed SROR using Sogou and Baidu 2 search engines respectively. To evaluate the ability of ranking terms using SROR , we accessed the agreement between the automatically generated rankings and human labelers. We sampled 120 queries, 3 l ( q ) 10. Two label-ers were asked to rank terms in each query according to the term importance for the full query. We computed Kendal-l X  X  W coefficient 3 , which ranges from 0 (no agreement) to 1 (complete agreement), between different rankings for each query. The average score over all queries is reported.
The agreement between using two search engines is con-sistent (0.77). It indicates SROR is stable across different search engines. The agreement between two labelers is 0.75. The average agreement between one SROR based ranking (using Sogou or Baidu) and one labeled ranking (by labeler 1 or labeler 2) is 0.64. The average agreement among one SROR based ranking and two labeled rankings is 0.57. Due to the difficulty of this task, there is a positive correlation.
Retrieval Effectiveness . For evaluating the retrieval ef-fectiveness of the SROR based weighting schemes, we sam-pled test queries from the query log. Each query must have at least 5 associated clicked pages in the indexed corpus and 3 l ( q ) 10. Finally, we collected 1538 test queries, 113 of which contained at least 7 terms. We viewed user clicked pages as relevant. The average MAP and p@10 over all queries were computed.

We could directly apply W eight for assigning weights to terms, noted as WT . This strategy has to search many times for a given query. For efficiency, we used W eight to auto-matically weight query terms for a large number of queries beforehand with a search engine. Then we trained a regres-sion model based on statistical features extracted from the labeled instances. The learned model could predict weights for future queries efficiently at querying time. We trained this regression model based on 2000 queries automatical-ly labeled by W eight using Baidu search engine, noted as REG . These queries contain 4 to 6 terms each. We do not use long queries for training, because the current search en-gines do not perform so well on such queries compared to shorter queries. The extracted features include term index within a query, times the term occurs in query log as a whole h ttp://www.sogou.com/ http://www.baidu.com/ http://en.wikipedia.org/wiki/Kendall X  X  Table 2: The retrieval effectiveness comparison of different weighting strategies on queries with differ-ent length. que ry, average rank of associated clicks, corpus tf, corpus idf and mutual information among query terms .

Table 2 shows the retrieval effectiveness using different weighting methods on queries with different length. UNF denotes giving terms uniform weights. IDF denotes the cor-pus based IDF approach which solely depends on the term distribution within the corpus. We can see WT which di-rectly uses W eight for term weighting outperforms other methods in most cases. It means SROR is a useful signal for query term ranking. The regression model REG per-forms better than UNF as well. It indicates the potential to use SROR for automatically labeling. IDF achieves good results when dealing with long queries but performs poorly on queries with moderate length in our experiments.
In this paper, we have proposed a simple approach to mea-sure query term importance by examining the search results overlap ratio. The experiments showed SROR based weight-ing schemes improved the retrieval performance. We believe our method could be improved further if noun phrases or multi-term concepts are identified and used, because they explain the query topics better than individual terms. We also plan to incorporate more features for learning based on automatically labeled instances using SROR .
 This research is supported by NSFC under Grant No. 60736044, 61073126 and 61073129, by HIT.KLOF.2009020. [1] M. Bendersky and W. B. Croft. Discovering key [2] M. Bendersky and W. B. Croft. Learning concept [3] S.-H. K. Chia-Jung Lee, Ruey-Cheng Chen and P.-J. [4] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. [5] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.
