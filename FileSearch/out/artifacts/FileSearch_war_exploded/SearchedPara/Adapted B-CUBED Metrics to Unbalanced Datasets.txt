 B-CUBED metrics have recently been adopted in the evaluation of clustering results as well as in many other related tasks. However, this family of metrics is not well adapted when datasets are un-balanced. This issue is extremely frequent in Web results, where classes are distributed following a strong unbalanced pattern. In this paper, we present a modified version of B-CUBED metrics to overcome this situation. Results in toy and real datasets indicate that the proposed adaptation correctly considers the particularities of unbalanced cases.
 H.3.3 [ Information Storage and Retrieval ]: Information search and retrieval X  clustering Evaluation, Search results clustering, Unbalanced datasets
Evaluation of partitions obtained as a result of clustering algo-rithms is a challenging task. Two main kinds of metrics can be identified: supervised and unsupervised metrics. In this paper, we will deal with the former. In the information retrieval area, a recent study proposed the use of a family of metrics known as B-CUBED [1], which is used when clusters of documents are evaluated. These new metrics successfully satisfy a set of formal constraints that in-clude problematic situations such as Cluster Homogeneity , Clus-ter completeness , Rag Bag , and finally, Cluster size vs. quantity . Each of these constraints evaluate a different situation that must be solved with a good evaluation metric. However, in the particular case of unbalanced datasets, these metrics fail to identify the cor-rect solution [4]. The particularity of an unbalanced dataset is that one of the classes covers most of the document collection. Namely, this is the case when the set of documents to be clustered is domi-nated by one class, e.g., one of the classes covers a high percentage of documents and the remaining documents belong to many small classes. This is not a strange situation. Indeed, this is a recurrent c  X  2015 ACM ISBN 978-1-4503-3621-5/15/08 ...$15.00 case when the Web Search Results Clustering (SRC) problem is studied. SRC consists in grouping Web results in meaningful clus-ters where each cluster should  X  X opefully X  correspond to a unique topic. Moreover, it is often the case that topics are not equally dis-tributed in Web results. For example, consider the results obtained with a search engine and presented in Figure 1. Note that mainly two topics can be found in the results, the animal and the car. The total number of images related to the animal is almost 5 times the number of images related to the car 1 . This example clearly illus-trates the existence of unbalance between the two classes eral, this behaviour can be observed in several Web SRC datasets including ODP239 [3], MORESQUE [7] and WEBSRC401 [6].
 For this reason, the use of clustering evaluation metrics must be verified in unbalanced cases. This is recurrently present in the SRC problem as well as in other clustering problems.
 Figure 1: Commercial search engine results for the query  X  jaguar X . 57 Web image results visualized, 47 of which are re-lated to the animal and only 10 to the car.

In this paper, we present an evaluation of the B-CUBED met-rics family using SRC datasets. Our results support the idea that B-CUBED give high scores to algorithms that follow similar distri-butions to the topics and otherwise, low scores even when cluster are randomly assigned. This can be explained by saying that B-CUBED metrics were also designed to penalize the erroneous links created between two classes more than putting documents in the wrong class [2]. Finally, we show how B-CUBED metrics can be modified to consider the evaluation of datasets that present the un-balanced issue. The remainder of this paper includes a description of B-CUBED clustering metrics and their modifications in Section 2. Experiments and results are presented in Section 3 and finally, discussion and conclusions are presented in Sections 4 and 5.
S urrounded by the dotted blue rectangle.
Many reasons could explain this distribution, however, how it af-fects user interaction with Web results is out of the scope of this paper.
S RC algorithms have been evaluated with several supervised and unsupervised clustering metrics. In the former category, B-CUBED metrics have received a lot of attention in recent years. Similarly, SRC has also privileged these metrics but their impact in this par-ticular problem is not clearly discussed. The particularities of the SRC problem motivate our efforts to develop an adapted version of these metrics.
B-CUBED metrics were originally proposed in [2], but exhaus-tively studied in [1] where it is shown that they can successfully evaluate partitions in situations included in defined formal con-straints. Full comparison with illustrated examples can be found in [1]. B-CUBED F-measure ( F b 3 ), Precision ( P b 3 ) and Recall ( R are defined in Equations 1, 2 and 3. where  X  i is the cluster solution i and  X   X  i is the gold standard of the category i and N is the total number of documents. Two main parameters of B-CUBED metrics can be modified.
 First, the  X  parameter in Equation 1 can vary to alter the impor-tance of P b 3 and R b 3 . This issue will be discussed in section 3. Sec-ond, the number of elements considered to calculate the Precision or Recall, i.e., the number of inputs received by g 0 (  X  ,  X  ) and g can be extended to three or more elements. The new formulation to allow the use of several elements 3 is presented in Equation 4.
Note that because more possible combinations are considered, the normalization factors in Equation 2, 1 |  X  ified. After mathematical factorization, the normalization value is cancelled by the modified Precision ( P mod b 3 ) and it is factorized in terms of R b 3 by the modified Recall ( R mod b 3 ). Factorized versions of the adapted B-CUBED metrics are presented in Equation 5. T he number of elements will be determined by the size of ~ x .
Note that, when the number of elements considered by the g functions is equal to 2, i.e. | ~ x | = 2, then the F mod ular, the new F mod b 3 tends to give less importance to partitions with high Recall and benefits Precision preserving the  X  parameter.
In our experiments, we use a total of five toy examples that in-clude four classical clustering situations as well as one situation that represents the unbalanced case. These toy examples are in-cluded in the first row of Table 1. Note that for each example, a left and right partition is included. In all cases, the right partition is considered a more adequate solution. Finally, in order to show the impact in real situations, we perform experiments in three SRC datasets: ODP239 [3], MORESQUE [7] and WEBSRC401 [6].
SRC ALGORITHMS : A host of classical and recent algorithms were used: LINGO, STC and CascadeSRC. LINGO is based on the spectral decomposition of a term-document matrix to define the respective clusters. Finally, labels are assigned by choosing the best representative for each found cluster. STC clusters documents based on a suffix tree. Clusters are determined from the tree by selecting the longest set of strings which are used as labels. Cas-cadeSRC [5] is a two level combination algorithm that preserves the quality in terms of intra-document similarity offered by LINGO and the compactness offered by STC.

R ANDOM ALGORITHMS : Two random algorithms are studied to verify the impact of document distribution in the obtained par-titions. First, the UniformRand algorithm assigns documents to each cluster in such a way that, in the end, each partition contains equally sized clusters. Secondly, the UltraShapedRand algorithm imitates the unbalanced SRC distribution. In this case, if k clusters are required, then for the clusters c 1 ,.., c k  X  1 only one document is randomly assigned and the remaining documents are assigned to cluster c k . Note that this distribution is an extreme case of SRC but allows to show the difference from a uniform distribution.
The formal constraints are listed as Cluster Homogeneity , Clus-ter completeness , Rag Bag and Cluster size vs. quantity . Cluster Homogeneity consists in giving a higher score to partitions where clusters contain elements of only one class, Cluster completeness gives higher scores to partitions where classes are represented by few clusters, Rag Bag gives higher scores to partitions where only one cluster contains different classes than to several clusters con-taining different classes. Finally, Cluster size vs. quantity gives higher scores to partitions where few clusters are provided but sep-arates most classes. In addition to these formal constraints, the Unbalanced constraint was recently added by [4] and evaluates if a misclassification is present in a big class or in a small one. This constraint gives better scores when the incorrect classified element is from the biggest class. Results using the examples proposed by [1] and [4] 4 are shown in Table 1 5 . For each example, the first col-umn shows the value obtained with the metric for the left partition, the second column shows the result for the right partition and the third column indicates if the formal constraint is satisfied (  X  ) or
T he original example was slightly modified to put only one mis-classified document in each evaluated partition.
All metrics can be found in [7] and [1]. not (  X  ). Finally, the column  X 4+1 F.C. X  indicates if the five formal constraints are satisfied simultaneously.

Note that none of current metrics can satisfy all constraints. In-deed, F b 3 satisfies the first 4 F.C., but misses the correct identifica-tion of the best partition for the unbalanced case as reported by [4]. However, the proposed modifications F mod &amp;0 . 9 b 3 (with | ~ x | = 3) and F b 3 manage to correctly classify all the formal constraints using the parameter  X  = 0 . 9. Indeed, positive values are obtained start-ing from  X  = 0 . 7, but to achieve a more general solution  X  = 0 . 9 was selected. Our choice is motivated by the reduction of the bias generated by unbalanced datasets namely for the SRC task. It is important to remark that when  X  &gt; 0 . 5, Precision receives more importance than Recall. A total of 10 runs were performed for each random algorithm. F are presented in Table 2 for different k values (from 2 to 20) and using the three SRC datasets. The UltraShapedRand algorithm be-haves better than the UniformRand when evaluated with both met-rics using the mentionned datasets 6 . Although when k = 2 both algorithms score similarly, the differences get larger as the num-ber of k partitions grows. This was observed for both metrics in the three datasets. However, when k = 20, the differences are larger for F cision allowing to get good performance by just getting more clus-ters. Indeed, when the number of clusters is increasing, the number of elements by cluster must be reduced. This situation reduces the chances of putting together elements from different classes which implicitly increases Precision.

When using MORESQUE and ODP239, F 0 . 9 b 3 gives better scores to the UltraShapedRand algorithm as the number of k partitions increases. Again, this situation is given by the parameter  X  = 0 . 9, which gives higher importance to Precision than Recall. However, this situation is not the same for F mod &amp;0 . 9 b 3 . This metric does not This situation was also observed for the F b 3 metric.
 MORESQUE ODP239 WEBSRC401 with STC, LINGO and CascadeSRC using real datasets. In bold the best score by metric and dataset. always give better scores to this situation and partitions with higher numbers of clusters may not be preferred. This is an important issue, because results suggest that F 0 . 9 b 3 will prefer partitions with clusters that contain a unique document which is not the case in any of the used datasets.

A summary of three SRC algorithms (LINGO, STC, CascadeSRC) the SRC algorithms than to the random strategies, as it is expected for a good evaluation metric 7 . Unfortunately, the behaviour is dif-ferent for WEBSRC401, where none of the metrics manages to cor-rectly assign the scores when compared with the random strategies. However, as shown in [6], WEBSRC401 is a hard SRC dataset. But this still raises discussion. This is not an evident situation. Remember that, as shown by [4], F b 3 can not select the correct partition in unbalanced datasets.
Although many clustering evaluation metrics exist, none of them can consider all possible situations. Indeed, new metrics could be
F
F
Although many clustering evaluation metrics exist, none of them can consider all possible situations. Indeed, new metrics could be proposed to simultaneously deal with the formal constraints as well as adapt to the specific task. However, as shown in Table 1, this is a hard task. Moreover, we have presented F mod &amp;0 . 9 b 3 ified version of the B-CUBED metrics. Our proposal manages to correctly classify the examples used to validate the initial 4 for-mal constraints and the case for unbalanced datasets. Note that a simple  X  parameter modification (the F 0 . 9 b 3 metric) also manages to correctly classify the examples, but fails when it is evaluated in real datasets. It is mainly due to the fact that too much importance to Precision is given thus privileging partitions with many clusters formed by few documents. On the other hand, the F mod &amp;0 . 9 only deals with the formal constraints but is also not disoriented by the random algorithms. Note from Table 2 that for the Uni-formRandom, F mod &amp;0 . 9 b 3 reduces the assigned score as the number of clusters increases. On contrary, for the UltraShapedRand, it in-creases until a certain point from which it starts to decrease. These behaviours were observed for both algorithms in the three datasets.
These results could inspire the development of (1) new analysis to identify more cases (such as the unbalanced) that must be consid-ered in the SRC problem, (2) new metrics or adaptations of exist-ing ones to satisfy the 4+1 studied formal constraints and (3) SRC strategies that consider adapted optimization functions to obtain the satisfaction of the formal constraints. Regarding the last one, some of the existing algorithms implicitly capture these characteristics, i.e., classical SRC algorithms, such as LINGO and STC, generate shapes similar to UltraShapedRand without explicitly including it in their algorithm. This situation could explain why these are hard to beat algorithms. Indeed, the classical K-means algorithm gen-erates partition shapes similar to the UniformRand algorithm and usually its performance is under what is obtained with LINGO or STC.
This paper presents a study about B-CUBED metrics and pro-poses an non-trivial adaptation of the F b 3 to be used in the SRC problem. Unbalanced datasets are implicitly used in the SRC prob-lem and it is a frequently ignored issue in recent studies. Several ex-periments were performed in toy examples and real datasets. Main findings indicate that our proposed metric ( F mod &amp;0 . 9 the only one to correctly classify the toy examples in the evaluation of the formal constraints including the unbalanced case, and at the same time, able to give adequate scores when comparing SRC algo-rithms against random algorithms with unbalanced shapes. New re-search in SRC must consider the effect of using unbalanced datasets by using adapted metrics to achieved more adequate results. Simi-larly, existing metrics based on F b 3 must reconsider the unbalanced effect in the datasets. Our immediate work consists in the explo-ration of bigger sizes for | ~ x | that will help in the understanding of this parameter. [1] E. Amig X , J. Gonzalo, J. Artiles, and F. Verdejo. A comparison [2] A. Bagga and B. Baldwin. Entity-based cross-document [3] C. Carpineto and G. Romano. Optimal meta search results [4] M. C. P. de Souto, A. L. V. Coelho, K. Faceli, T. C. Sakata, [5] J. G. Moreno and G. Dias. Easy web search results clustering: [6] J. G. Moreno, G. Dias, and G. Cleuziou. Query log driven web [7] R. Navigli and D. Vannella. Semeval-2013 task 11: Word
