 The World Wide Web is growing and changing at an as-tonishing rate. Web information systems such as search engines have to keep up with the growth and change of the Web. Due to resource constraints, search engines usually have difficulties keeping the local database completely syn-chronized with the Web. In this paper, we study how to make good use of the limited system resource and detect as many changes as possible. Towards this goal, a crawler for the Web search engine should be able to predict the change behavior of the webpages. We propose applying clustering-based sampling approach. Specifically, we first group all the local webpages into different c lusters such that each cluster contains webpages with similar change pattern. We then sample webpages from each cluster to estimate the change frequency of all the webpages in that cluster. Finally, we let the crawler re-visit the cluster containing webpages with higher change frequency with a higher probability. To eval-uate the performance of an incremental crawler for a Web search engine, we measure both the freshness and the quality of the query results provided by the search engine. We run extensive experiments on a real Web data set of about 300,000 distinct URLs distributed among 210 websites. The results demonstrate that our clustering algorithm effectively clusters the pages with similar change patterns, and our solution significantly outperforms the existing methods in that it can detect more changed webpages and improve the quality of the user experience for those who query the search engine.
 H.3.5 [ Information Storage and Retrieval ]: Online In-formation Services X  Web-based services  X  This work was partially supported by NSF grant 0535656. Algorithms, Design, Experimentation.
 Incremental crawler, Web search engine, refresh policy, sam-pling, clustering.
The world-wide-web is huge. In reality, a crawler can-not always crawl a webpage immediately after the page is changed. Even within a crawl cycle (in which a crawler does not crawl the same page twice) a crawler cannot crawl all the webpages on the world-wide-web. Therefore, in each crawl cycle, the crawler must decide which pages to crawl.
Anticipating when a webpage will change is a hard prob-lem. Publishers of webpages, often, do not change the web-pages based on a fixed frequency. Whenever the publisher has new information to disseminate, when there is a (busi-ness or personal) need to change a webpage, when he or she finds the time and resources to make the change, the web-page is changed. It is also important to note that a crawler is an intermediary between the publisher of a webpage and a search-engine user. That is, if a webpage is never retrieved, a crawler should not waste precious resources to crawl the webpage even if it knows exactly when the webpage is going to be changed. Even if a webpage is very important, if the change to the webpage is cosmetic, like a typo being fixed, the color of the background being changed, the updated web-page should not be crawled. Note that the primary reason a crawler crawls webpages is so that the search engine can index the webpage and its (updated) contents. Typically, although the visual presentation of a webpage may be very important to the end-user, such presentation details are not indexed. Even if the crawler has not crawled a webpage, if the webpage is returned as a result of the query, upon clicking the link to the webpage, an end-user can see the latest page. Given these various desired characteristics, it is hard to design a crawling policy that respects all of them.
In this work, we investigate a typical scenario in which due to resource constraints, a crawler is only allowed to periodically download a fixed number of webpages and up-date the corresponding local copies when a change has been found. We define this fixed number of pages as the down-load resources and the periodical interval as the download cycle . We show that clustering webpages and then crawling based on clusters is a better crawling strategy than exist-ing crawling policies. We investigate and determine which features of webpages can predict the change frequency of a webpage, and the importance of a webpage. Based on these features, we cluster the entire set of webpages and rank the clusters based on their change frequency and importance. The crawler crawlers clusters based on this ranked order. The importance of clusters can change from one crawl cycle to another.

Cho and Ntoulas proposed a sampling-based algorithm which is widely adopted [13]. In their approach, a small number of webpages are first downloaded from each website as samples, then these samples are used to decide which websites contain more changed webpages so that more re-sources are assigned to those websites. More specifically, their sampling is at the level of a website with a greedy sampling policy. In other words a few webpages from each website are chosen as samples and then all the webpages in the website with the largest number of changed samples are re-downloaded. We argue that the level of a website may not be a good granularity for sampling because the update patterns of various webpages on the same website could be quite different, while webpages with similar update patterns may distribute across different websites [17]. We propose using clustering techniques to group webpages with similar update pattern together and using the cluster as the downloading granularity for the sampling approach. Our major contributions include:
The rest of the paper is organized as follows. In Section 2 we describe some existing work related to our approach. We propose our clustering process and the features used for clustering in Section 3. In Section 4 we investigate the sampling process on the cluster level. In Section 5 we present a new evaluation model for incremental crawlers. In Section 6 we discuss the empirical results. Finally, we outline our conclusion in Section 7.
Existing studies have shown that webpages change at vary-ing rates from a few hours, a few weeks, to a year [7, 9, 23]. Different aspects of the webpages X  changes have been studied, such as the change frequency, the degree of change, and the factors affecting the change. The implication from these studies for the crawlers has also been investigated. It is suggested that by following some refresh policies, the crawler should be able to maximize some pre-defined eval-uation metrics by incrementally downloading and updating the webpages in the local repository. In this section, we will give a systematic overview of the existing refresh policies and evaluation metrics. In order for search engines to keep up with the dynamic Web, some server-side approaches are proposed. One of the existing approaches requires that each Web server keeps a file with a list of URLs and their respective modification dates [6]. Before visiting a webpage, a crawler downloads the URL list, identifies the URLs that were modified since its last visit, and retrieves only the modified pages. This approach is very efficient but requires modifications to the server-side implementation. Most current web servers do not support this.

On the other hand, client-side techniques can be cate-gorized into different groups. First, probabilistic models have been proposed to approximate the observed update history of a webpage and to predict its change in the fu-ture [11, 16]. Most of these change-frequency X  X ased refresh policies assume that the webpages are modified by Poisson processes [18]. Edwards et al. [16] argue that crawling frequency should be increased for more frequently-changing pages. Besides a page X  X  change frequency, several metrics have been proposed to guide the crawler to choose webpages for re-downloading, such as freshness and age [11],  X  X mbar-rassment X  metric [31], and user-centric metric [25]. However, a limitation common to all these approaches is that they need to gather enough accurate historical information of each webpage so that they can use the historical information to predict its future changes.

To address this problem, sampling-based approaches are proposed to detect webpage changes by analyzing the update patterns of a small sample of webpages [13]. Their sampling method samples and downloads data in the unit of a website. Different from their approach, we analyze whether sampling based on the unit of a cluster, which is based on some change related features, can be more effective.

Recently, a classification-based algorithm [4] takes into account both the history and the content of pages to predict their behavior. Their experimental results based on real web data indicate that their solution had better performance than the change-frequency X  X ased policy [9]. Similar to our approach, they also extracted features from each webpage related to its update and used them to group webpages into different categories, each of which represents one specific change frequency. The key difference between their algo-rithm and ours is that they just put all webpages into 4 classes whereas we create a lot more clusters. Therefore, in our case each cluster can come with a prescribed crawl cycle of k days (i.e., the cluster is recrawled after k days) where k can take numbers of values but with 4 classes they are limited to 4 values only. Moreover, they assume initially the crawler knows the change history of a set of webpages. And these webpages are used as training data to learn the change pattern of other new pages. While in our approach, we do not assume the crawler has any existing historical information at the very beginning. We made this choice because 1) it is not realistic for a crawler to have a set of webpages with their change behaviors, which can be used as the training data set to predict the change behavior of other webpages, 2) new webpages are being created all the time; their change history is not known, yet, they must be crawled at some frequency, and 3) even if the crawler can obtain the past change patterns of webpages in a period of time, they are not accurate enough for predicting the webpages X  future change behavior. Therefore, our clustering-based approach, which needs no historical information, is more practical than the classification-based method.
Several evaluation metrics have been used to compare the performance of various refresh policies. They can be classified into three categories as staleness, change rate, and query-based metrics.
The most straightforward way to measure the staleness of a set of webpages is to count the number of obsolete webpages in the local repository. In [10], the freshness of webpage p i at time t is defined as Here, up-to-date means that the locally stored image of the webpage is exactly the same as the counterpart at the source. Then the freshness of the entire local copy at time t is Here, U is the set of all locally stored webpages. Informally, the freshness metric represents the fraction of webpages that are up-to-date. For example, if we maintain 100 pages and if 80 pages are up-to-date at t , the freshness is 0.8. To measure the refresh policy X  X  ability to detect updates, ChangeRatio [15] is defined as the fraction of downloaded and changed webpages D c i over the total number of down-loaded webpages D i in the i th download cycle. The average ChangeRatio C is the mean C i over all download cycles.
The above general ChangeRatio metric treats every web-page as equally important. However, in practice some web-pages may be more popular than others so that once they have changed, such change has a higher priority to be de-tected by the crawler. Motivated by this observation, a Weighted ChangeRatio [13] is introduced by giving different weights w p to the changed pages p , formally defined as in which I 1 ( p ) is an indicator function:
Query-based metrics evaluate the quality of a search en-gine X  X  local repository of webpages with respect to the qual-ity of the users X  experience when they use the search engine.
In [31], Wolf et al. proposed a query-based metric to evaluate the badness of the search engine X  X  local copies of the webpages, called Embarrassment Level Metric . It is defined as the probability that a client issues a query, clicks on a URL returned by the search engine, and then finds that the resulting page is inconsistent with respect to the query. Under such a metric, the crawler X  X  job is to minimize the Embarrassment Level Metric .

Another query-based metric proposed in [25] argues that minimizing embarrassment cannot guarantee a good search experience. Consider that the search results returned by the search engine generate no embarrassment, while the low quality of the relevant documents can still substantially de-grade the quality of the user X  X  search experience. In this context, the average quality of search repository can be expressed as Avg. Quality  X  Here, V ( w i ), the likelihood of viewing w i , is computed using a scoring function over q and the local repository, which is possibly stale. While the relevance rel ( q, w i ) is obtained using the same scoring function over q and the  X  X ive X  copy of w i .Both freq q and V ( w i ) can be obtained from the user logs from the search engine. The user-centric metric is the change in such an average quality before and after the crawler updates each w i in the local repository. The proposed user-centric Web crawling is driven directly by this user-centric metric based on the search repository quality.
The ideal evaluation metric should be one that compares the search results of two different crawling policies. The crawling policy that generates the better search result should be preferred. How two ranked lists of search results can be compared in the absence of a gold standard that ranks and lists the ideal search results is an active area of research [3]. In reality, because users seldom examine search results beyond the first page, perhaps, the two top-10 search re-sults resulting from the two policies should be compared. If needed, top-10 can be generalized to top-k.
We cast the problem of finding webpages that have similar change patterns into a clustering problem, with the assump-tion that webpages have specific features related to their update behavior. In this section, we discuss how to cluster all the webpages in the local database. The first step of the clustering process is to extract two types of intrinsic features (static and dynamic) that are correlated with the webpages X  change patterns.
Recent studies have found that some characteristics of webpages are strongly correlated with their change frequen-cies. For example, Douglis et al. [15] noted that actively-changing webpages are often larger in size and have more im-ages. Fetterly et al. [17] observed that the top-level domains of the webpages are correlated with their change patterns. Based on the previous findings and our observations, the following static features are used in our clustering process.
Content features There are 17 content features in to-tal. First, we construct a word-level vector to represent the content of the webpage, which is used as one feature vector for clustering. To avoid its dominance over other features, we first cluster all the webpages based on the tf  X  idf [27] vectors of each page where the dimensions of the vector are all the words that appear across all webpages. The IDs of the 10 largest clusters are chosen as the labels of 10 dimensions in the feature vector. For each webpage p , the values of these 10 dimensions are computed as follows. Suppose the webpage belongs to the cluster C p , the feature X  X  cluster label is C f . Using the distance of the centroids of each cluster to represent the distance of two clusters, the value of the feature, f C f , is computed as follows. f C for the webpages in C p = C f , f C f = 1 for the webpages in C p that have the shortest distance to C f , f C f =2forthe webpages in C p that have the second shortest distance to C , and so on. To speed up the computation, we use the 1000 most frequently appeared words(after filtering the stop words) in all the webpages to calculate the distance between clusters.

In addition to the 10 content-related features, we consider seven other features: number of images, number of tables, number of non-markup words, file size in bytes (excluding the HTML tags), and file types: whether the file is an HTML file, a TXT file, or others (i.e., each file type is represented as one dimension in the feature vector and they are all in binary form, e.g. for an HTML file the corresponding dimension has a value of 1 and a value of 0 for the other dimensions). URL features: There are 17 URL features in total. First, we compute 10 features for the words in the URLs similar to how we generated the word features in the web-pages. Second, we compute the depth of the webpage in its domain, i.e. the number of  X / X  in the URL. We also consider the name of the top-level domain in the URL, and reserve six dimensions in the feature vector for the domains, .com, .edu, .gov, .org, .net., and .mil. Each of these six dimensions is a binary feature indicating whether the webpage belongs to the corresponding domain or not. We chose these six domains as the features because most webpages fall in these six domains. We can also include some other domains, such as .biz, if there are a significant number of webpages in the repository belonging to that domain.
 Linkage features There are four linkage features in total. We use PageRank [24] of the webpage as an indicator for the popularity of a webpage. The assumption is that the more popular a webpage is, the more up-to-date it needs to be kept. PageRank is a probability distribution to indicate the likelihood that a person randomly traversing the Web will eventually arrive at any particular page. The PageRank of a webpage, p i , is calculated as follows: where p 1 ,p 2 , ..., p N are the webpages, IL ( p i )isthesetof pages that link to p i , OL ( p j ) is the number of out-going links on page p j , d is a damping factor, and N is the total number of webpages in the local repository. PageRank has been proven in previous studies [12] as an excellent ordering metric when crawling for pages with many in-links.
We also consider three other features related to linkage: number of incoming links within the local repository, num-ber of outgoing links, and number of email addresses on the webpage.
Ali and Williams have indicated that the significance of the past changes in document content is an effective measure for estimating whether the document will change again and should be re-crawled [2]. We extend this idea to some of the aforementioned static features as follows. Note that it is impossible to calculate these dynamic features until after the second download cycle, due to the fact that they are based on the comparison between two consecutive snapshots of a webpage.

Dynamic Content Features We use the following five features: Change in the webpage content, which is com-puted by the cosine similarity between the webpages X  content vectors in two consecutive download cycles; change in the number of images; change in the number of tables; change in the file size (in bytes, without the HTML tags); and change in the number of non-markup words on the webpage.
Dynamic Linkage Features We use the following four features: Change in PageRank, change in the number of incoming links within the local repository; change in the number of outgoing links, and change in the number of email addresses on the webpage.

We also include the webpage X  X  change frequency as one dyanmic feature for clustering. This feature can help the crawler adapt to the observed change history. Note that when computing the change frequency for a webpage, we consider only the changes detected by the incremental crawler while ignoring those changes that are not detected.
With each webpage represented by a feature vector, we apply the Repeated Bisection Clustering (RBC) algorithm [19] to construct hierarchical clusters. With the RBC al-gorithm, a k -way clustering solution is obtained via a se-quence of cluster bisections: first the entire set of webpages is bisected, then one of the two clusters is selected and further bisected; such process of selecting and bisecting a particular cluster continues until k clusters are obtained. Note that the two partitions generated by each bisection are not necessary equal in size, and each bisection tries to ensure that a criterion function is locally optimized. We do not choose the bottom-up clustering algorithm for our clustering process because with RBC ,whichisatop-down hierachical clustering algorithm, we do not need initial seeds at the beginning.

An important question that needs to be answered before applying the clustering algorithm is how many clusters there are in the data set. Since clustering analysis is an unsuper-vised learning technique, we do not have prior knowledge about the number of clusters. However, we can estimate this number by using the method of v-fold cross-validation . The general idea of the v-fold cross-validation is to divide the overall data set into a number of v folds. The clustering process is then successively applied to the data belonging to the v  X  1 folds (training samples) to get k clusters. For the v th fold (test sample), the centroids of these k clusters are applied to them and the data in the v th fold are put into k different clusters. A single measure is used to evaluate the quality of the clustering results in the v th fold. This measure is based on the goal of clustering, which is to minimize the similarity of data within the same cluster while maximize the similarity of data between different clusters. In general, we try a range of numbers, k s, in clustering. For each k , we apply the v-fold cross-validation method and observe a score function used in the clustering process. Specifically, we focus on the criterion function used by most vector-space based clustering algorithms. Let the k clusters be S r , r  X  [1 ,k ], and their centroids be C r .Ifweusethe cosine function to measure the similarity between a webpage p i and a centroid C r , the criterion function becomes the following: where cos ( p i ,C r ) is obtained as
To maximize  X  in (3) is to maximize the similarity be-tween each webpage and the centroid of the cluster that is assigned to. Generally speaking, the larger k is, the higher  X  is. However, a very large k may impact the efficiency of the clustering algorithm and result in a large number of unnecessary clusters. Thus we need to try different values of k and select the one that can balance both. The results of v-fold cross-validation are best reviewed in a simple line graph. The line showing the score function should first quickly increase as the number of clusters increases, but then levels off. As such, the optimal number of clusters can be found at the point switching from increasing to leveling off.
In this section, we describe a sampling-based update de-tection algorithm that works at the cluster level, under the assumption that each cluster contains webpages with similar change patterns. However, as we have no prior knowledge about which cluster is more likely to change, we adopt a sampling approach to determine which cluster contains web-pages with the highest change probability. The mechanism of this sampling approach is as follows. The crawler first chooses samples from each cluster instead of each website, and then checks whether the samples are up-to-date. Finally the crawler selects the clusters with high change probabili-ties to re-visit.
We first address two challenging questions in sampling webpages: which webpages in the cluster, and how many of them should be selected as samples.

To select representative webpages for the whole cluster as samples, those that are near the cluster X  X  centroid are the most suitable candidates. The distance between a webpage and the centroid can generally be represented using the cosine function given by (4). This measure ranges in [0 , 1] and becomes larger when the webpage is more similar to the centroid.

A sufficient sample for a cluster is defined as the sam-ple that produces a valid mean score for the cluster. This valid mean score should represent the population mean  X  within  X   X   X  at a confidence level (e.g. 80%) in a standard t -test. Here  X  is the average change frequency of the samples (explained later). The sample X  X  valid mean score is used to represent the mean score of the whole cluster. In some cases, a sample of reasonable size is unable to produce a valid mean score. We set an upper bound for the sample size in each cluster to avoid analyzing too many samples. In our setting, we set the upper bound of the sample size as the total number of download resources over the number of clusters.
The sampled webpages are used to predict the change pat-terns of the cluster. For each sampled webpage, we estimate its change frequency based on its update history. Then for each cluster, we compute its download probability  X  as the average change frequency of all sampled pages in that clus-ter. The crawler can then download webpages from clusters in the descending order of their download probabilities, until the download resources are exhausted.

The basic idea is that pages with many recent changes are more likely to change again. We also model the update of the webpages using a Poisson process [18], which has been shown effective in experiments with real webpages [8, 11]. It is reasonable to assume that the update of a webpage p follows a Poisson process with its own change rate  X  p . This means a webpage changes on its own, instead of depending on other pages. This assumption may not be strictly true but it has been proved to be the first workable approximation for real applications. Note that the change rate may differ from page to page. For each webpage p ,let T denote the time when the next event occurs as a Poisson process with change rate  X  . Then, we obtain the probability that p changes in the interval ( o, t ] by integrating the probability density function:
Pr { T  X  t } =
We set the parameter download probability  X  to be Pr { T  X  t } where t = 1, which means one download cycle. Therefore,
Clearly,  X  depends on the parameter  X  p . We compute  X  p based on the change history of the webpage p within n download cycles: where I ( p i ) is an indicator function defined as follows: And w i is the weight assigned to changes occurred in differ-ent download cycles so that the distribution of change events is also taken into account.

Typically, w a  X  w b is satisfied when a&lt;b , indicating that changes in the more recent download cycles are more important. We propose four settings for w i that we will experiment with later in Section 6: 1. Non-adaptive: w 1 = w 2 = ... = w n = 1 2. Shortsighted adaptive: w 1 = w 2 = ... = w n  X  1 = 3. Arithmetically adaptive: w i = i n 4. Geometrically adaptive: w i = 2
In our work, we focus on the quality of the final rank-ings returned to the search engine  X  Ss users. The difference between our metric and the exi sting user-centric metrics is that we measure the quality of the returned webpages according to the users X  queries, instead of evaluating the quality of all the webpages in the local repository. This means we only consider a subset of the local repository that is of the users X  interest. For those webpages not of interest to the users, their updates have negligible importance to the search engine, hence they are not considered in the evaluation process.

A search engine uses its indexes and ranking function on its local repository to assign scores to the webpages in re-sponse to a query. Two essential factors govern the quality of the ranking generated from the search engine: (1) feasibility of the indexing and ranking function, and (2) freshness of the local repository [25]. Clearly, if the indexing and ranking function is appropriate for estimating the actual goodness of each webpage, and the local repository is closely synchro-nized with the Web, the users can acquire satisfying results. Based on this principle, we may fix the first factor, namely the indexing and the ranking function, and then evaluate the ranked results returned to the users. Note that these rankings are generated from different sets of documents, which are maintained by following different refresh policies. To detect the quality of the local repository, we evaluate the ranking generated from it by applying the same query and the same scoring function. Based on the rankings, we further propose two following evaluation metrics to measure the quality of the return ed ranking: the top-k mean average precision (MAP) and the top-k freshness .
To generate the ranking on the repositories, we apply two scoring functions: (1) the TF-IDF metric [27] and (2) the PageRank [24], both are widely adopted in the information retrieval community. The first one, TF-IDF metric (term frequency inverse document frequency), is a content-based scoring function that assigns importance to words where the importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the whole document collection. The second metric is a linkage-based scoring function for searching webpages. It is a probability distribution used to represent the probability that a person randomly clicking on links will arrive at any particular page. We have shown the way to compute this metric from a set of webpages in Formula 2.

We also combine these two scoring functions to generate a final ranking score for each webpage as follows. where  X  +  X  =1. Weset  X  =  X  = 0.5 in our experiment.
The next step is to use our metrics to evaluate the re-turned ranking from different dimensions. We adopt two metrics in our experiments: 1. Top-k Freshness : This metric reflects the percentage of 2. Top-k MAP (Mean Average Precision) :Theterm Av-
Typically, we consider only the top-10 results in the re-turned ranking because the probability that a user clicks on a result at rank 10.000 is virtually zero even if the result is rel-evant to the query [30]. We manually judged the relevance of each result in the returned ranking to create a gold standard. Note that in this process there are two steps to simulate the users X  browse experience. That is, we need to check the relevance of 1) the result X  X  title and summary, which are generated from the local copies in the search engine, in the returned ranking, and then 2 ) the current webpage to which the result is linking, which is the real copy on the Web. If the title and summary of the result in the first step are judged to be irrelevant, it is not necessary to execute the second step. By executing the above two steps, we simulate the users X  searching behavior and get a reasonably accurate judgment of the ranking quality. For example, when a user inputs a faculty member X  X  name to the search engine to get his homepage, it still works if there are updates on the faculty X  X  homepage while the crawler does not catch such an update. This is because, when the search engine returns the faculty X  X  stale homepage in its local repository, the user can decide whether this is what he wants by examining the title and summary. When the user clicks on the link of the result, he will be directed to the updated version of the homepage. This is called the  X  X ucky X  case for the search engine. However, in another scenario, the change on the webpages could be so significant that when the user clicks on the link he is given a totally irrelevant webpage or even a dead link. Wolf, et al., [31] discussed this  X  X mbarrassing X  case was and the crawler should definitely try to avoid. An effective evaluation process should be able to test whether the crawler can catch the changes that may lead to these cases. A collection of real URLs were first obtained from the WebArchive project 1 and we implemented a spider to crawl the Internet Archive 2 . Excluding the webpages from We-bArchive that were not available in the Internet Archive, we eventually constructed a dataset containing approximately 300,000 distinct URLs that belong to more than 210 web-sites. For each URL, we downloaded from the Internet Archive their historical snapshots and their change history for a period of one year. The distribution by different top-level domains is shown in Table 1. Table 1: Distribution of top-level domains in the collected data; .com and .edu together account for more than 70% of the webpages.

We then let the crawler maintain the above data set as its local repository. To verify the effectiveness of our clustering-based sampling approach, we applied our clustering-based sampling approach with different download cycles (2 weeks, http://webarchive.cs.ucla.edu/ http://www.archive.org/ 4 weeks, and 8 weeks) and download resources (25K, 50K, and 100K). Note that our work is modulo the sampling frequency of the Internet Archive. That is if the pages X  change at a faster rate than their crawl frequency of the Internet Archive crawler, i.e., there is more than one change between two consecutive crawls, then only one change is detected.
 To evaluate our proposals, we used three kinds of metrics, ChangeRatio , top-k freshness ,and top-k MAP , which have been discussed above. For the first metric, the webpage X  X  local copy is compared with the historical snapshot with thesametime-stampfromtheInternetArchivetocheck whether it has changed or not; if there is no exact match in the archive, the version with the closest time-stamp is used instead. For the latter two metrics, ranking is generated from user queries. We randomly selected 100 queries from the query log of AOL search engine [29]. Ranking is obtained based on Equation (7). The popular Lucene software [1] is used to generate the TF-IDF value. MatLab codes for PageRank computation are downloaded from [22] for our evaluation process.
We now present our findings in applying the sampling-based algorithm on the aforementioned data collection.
We employ the 10-fold cross-validation method to cluster the content of webpages. Figure 1 shows the values of  X  under different numbers of clusters, k .Thevalueof  X  goes up as k increases. Its gradient is sharp when k is smaller. For URL clustering, there is a turning point for  X  when k reaches 100: as k passes 100,  X  increases much slower than before. This indicates that once k&gt; 100, increasing k does not have significant impact on the clustering output. The similar trend also appears in content-based clustering. Therefore, we use k = 100 for all the clustering processes in the following experiments.
Criterion Function (  X  ) Figure 1: Cluster quality for URL and content clustering over different numbers of clusters. k = 100 is the optimal setting because it can balance the quality of the clusters and the overhead for clustering.
We now take a close look at some statistics on the number of webpages in each cluster. The comparison between our cluster-level sampling and the site-level sampling is shown in Table 2. From this table we can see that, our cluster-level sampling has a larger sampling unit compared to the site-level sampling. In our data set, some websites contain too few webpages to be used for the sampling process. For example, the minimal number of webpages in one website is 1. In the following experiments, we will show that our cluster-level sampling is more effective in grouping webpages with similar change pattern than the site-level sampling. Table 2: Some statistics of webpages in different sampling units.
We partition all the features into different categories and apply the clustering process to each category. These cate-gories include: (1) URL: All words in the webpages X  URLs; (2) Content: All words on the webpages; (3) URL+content; (4) Static: All the static features listed in Section 3.1 except the 10 features related to URL words and the 10 features related to webpage content. There are 28 features in total; (5) Dynamic: All the dynamic features listed in Section 3.2. There are 10 features in total; (6) Combined: All the features we have discussed so far. There are 48 features in total.

We show the comparison results in Figure 2. For this set of experiments, the download cycle is set to be one month and the download resources R is set incrementally from 25K, 50K, to 100K. For the sampling process, we set the change frequency to be the non-adaptive one and confidence level at 80%. The figure gives us the following illustrations: First, when the download resource is scarce, the choice of feature sets for clustering has a huge impact on the average ChangeRatio of the clusters. While the combined feature set performs the best, using only the URL words for clus-tering has the lowest ChangeRatio because it ignores other important features. Second, when the download resource is abundant, the performance of all feature settings becomes similar. Using words in both URL and content for clustering has similar performance as using words in content only. This is because, compared with content, URL is relative short and much less informative. The performance of using all features for clustering is better than using any of the subsets, as such we use all features discussed for clustering in the following experiments.
We experimented with the four different settings for w i discussed in Section 4. They are non-adaptive ( nad ), short-sighted-adaptive ( sad ), arithmetically-adaptive ( aad ), and geometrically-adaptive ( gad ) policies. For this set of exper-iments, the download cycle is set to be two weeks and the download resources R is set to be 25K. Figure 3 shows the results from this experiment. At the beginning, the policy with shortsighted-adaptive w i , which focuses on only the current change status, outperforms all the other because others (except the non-adaptive policy) have no knowledge Average ChangeRatio Figure 2: Comparison of different clustering feature sets. Dynamic features can predict change patterns better than static features. about the change history. On the contrary, using the non-adaptive w i has the lowest ChangeRatio at the beginning while gradually outperforms the shortsighted-adaptive one. The other two adaptive policies have very similar perfor-mances, and eventually their ChangeRatio sarethehighest among all. In this figure, there are variabilities with the download cycles because most webpages have similar change patterns. For example, most webpages change at the end of the week or at the end of the month. This reinforces our proposal that clustering webpages with similar change patterns can help to detect more webpage updates in the same cluster.
Average ChangeRatio Figure 3: Comparison of adaptive and non-adaptive w i . Using adaptive w i can improve the average ChangeRatio in the long term.
We compare the effectiveness of our cluster-level sampling approach against that of four existing algorithms:
For our cluster level sampling algorithm (CLS), we show the average ChangeRatio of four different settings for w i and set the confidence level to 0.8. Figure 4(a) gives the ChangeRatio for all download policies. Clearly, the perfor-mance of our cluster-level sampling is better than that of the site-level sampling. This is because in CLS, webpages are grouped based on the features most relevant to their update patterns, while in SLS webpages are grouped based only on their top-level domains. Thus a crawler implementing the CLS scheme can easily capture webpages with similar change patterns. In this set of experiments, using sad has higher average ChangeRatio than nad , aad ,and gad . However, as we observe in Figure 3, the two adaptive strategies aad and gad will outperform sad in the long term.
 Figure 4(b) gives the f reshness @10 computed based on Formula (8) for all download policies. This time we focus on the freshness of the webpages in which users are interested. Again, the performance of our cluster-level sampling is bet-ter than that of the site-level sampling. This is because in CLS, we use not only static but also dynamic features for clustering webpages. In this case, our approach is good at catching the significant changes that impact the search engine X  X  final ranking. In terms of f reshness @10, there is no significant difference between the three adaptive download policies.

Figure 4(c) gives the MAP @10 computed based on For-mula (9) for all download policies. In this set of experiments, we added one more bar, the last one in the figure, to show the performance of the X  X deal X  scenario, in which the crawler is so powerful that it keeps every we bpage in its local repository completely synchronized with the real copy on the Web. From the figure we can clearly see that keeping the local repository fresh can lead to a significantly higher MAP than others. This means that when the search engine X  X  index is built on top of the most up-to-date webpages, users can get accurate answers to their queries. For the other download policies, their MAP s are very similar as shown in the figure.
To justify the effectiveness of the download policies in terms of MAP @10, we apply a two-sample hypothesis testing about means , which is a commonly used statistical method in large sample tests [28]. Specifically, we are going to check if there is a significant statistical difference between the means of the MAP @10 values of the download policies. We set the null hypothesis H 0 : MAP 1 = MAP 2 for two populations with the means MAP 1 and MAP 2 ,the alternative hypoth-esis H A : MAP 1 = MAP 2 ,andthe level of significance  X  . Recall that we have a sample of 100 MAP @10 values for each cycle and there are totally 24 cycles. Therefore, we have 2400 MAP @10 values for each download policy. Since the sample size is large enough (  X  30),accordingtothe Central Limit Theorem [5], we can assume that the sampling MAP distribution would be approximately normal. The statistic to be tested is the difference between the sample means. We can locate this statistical value on the distribution and see whether it lies close to zero. If it does, H Otherwise, if its absolute value is larger than a critical value, we will reject H 0 . In this case, a critical value is required to reject H 0 at the  X  level. Technically, we need to compute the t -value for a pair of samples (i.e., two MAP @10 data sets) and check if t exceeds the critical value, which is given regarding to a specific  X  level. If it does, then we are able to reject H 0 with the confidence level as (1  X   X / 2)  X  100%. We present only the numerical results of the tests as in Table 3.
Table 3 lists all the t -values for the comparisons between pairs of MAP @10 samples. From Table 3, we can see that most of the t -values, except two ( CLS-sad vs. Freq and CLS-sad vs. SLS ), are larger than the critical value of 1.960 (  X  =0 . 05), which implies that we are able to reject H 0 yond the 0.05 level. These results suggest that, with 97.5% confidence, CLS-nad , CLS-aad ,and CLS-gad achieve signif-icantly better MAP s than the existing approaches, such as Rand , RR , Freq ,and SLS .For CLS-sad , it can signif-Table 3: The t -values for pairs of MAP @10 samples. Our cluster-l evel sampling approach outperforms the other existing approaches beyond the significance level of 0.05. icantly outperform Rand and RR with 97.5% confidence, and significantly outperform Freq and SLS with 95% con-fidence.
In this paper, we studied how to design effective download policies for a search engine crawler to detect webpage up-dates with a limited amount of available download resources. Motivated by the observation that webpages sharing simi-lar change patterns tend to have certain similar features, we proposed a sampling-based synchronization algorithm which works at the cluster level. The algorithm first clusters the webpages in the local repository based on the features closely correlated with their update patterns, then samples the clusters that are more likely to change, and eventually uses the changed samples in each cluster as the seeds to discover more changed webpages. Furthermore, we develop an evaluation framework for the incremental crawler that measures the quality of the final ranking generated from the local repository maintained by the crawler. Experimental results show that our approach outperforms various existing sampling-based update detection algorithms.

There are several promising di rections that deserve fur-ther study in order to improve the update detection for an incremental crawler. First, it is noted that a crawling algorithm that only cares to maximize the ChangeRatio may lead to a fairness problem where some webpages are never downloaded. Because it is necessary to guarantee that pages are downloaded within a reasonalbe period of time, we plan to adjust this issue by letting the crawler periodically re-visit some pages in the clusters with low change frequency. Sec-ond, we consider that different clustering algorithms may have different impacts on the crawler X  X  performance. We will investigate different clustering algorithms and find out which is the most suitable for our application. Finally, to estimate the appropriate number of clusters before the clus-tering phase, we can apply some newly proposed clustering methods, e.g. X-means [26] in addition to the v-fold cross validation .
