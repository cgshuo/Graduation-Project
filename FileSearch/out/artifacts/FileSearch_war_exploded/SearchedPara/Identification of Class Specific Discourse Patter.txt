
In this paper we address the problem of extracting important (and unimportant) discourse patterns from call center conversa-tions. Call centers provide dialog based calling-in support for cus-tomers to address their queries, requests and complaints. A Call center is the direct interface between an organization and its cus-tomers and it is important to capture the voice-of-customer by gath-ering insights into the customer experience. We have observed that the calls received at a call center contain segments within them that follow specific patterns that are typical of the issue being addressed in the call. We present methods to extract such patterns from the calls. We show that by aggregating over a few hundred calls, spe-cific discourse patterns begin to emerge for each class of calls. Fur-ther, we show that such discourse patterns are useful for classifying calls and for identifying parts of the calls that provide insights into customer behaviour.

H.2.8 [ Database Applications ]: Data Mining ; H.4.0 [ Information Systems Applications ]: General Algorithms, Experimentation
Call Center Analytics and Applications, Classification and Clus-tering, Information Extraction, Text Mining, Unsupervised Learn-ing
Many companies today maintain call centers to present a sin-gle point of contact to their customers. At these call centers, cus-tomers interact with professional agents who address their queries, requests and complaints. Call centers handle hundreds of calls de-pending on the nature of the business. They range from techni-cal support (help desk, customer care) to promotional (marketing, sales) to transactional (booking, rental).

There is a wealth of information hidden in the calls that could be useful to the organizations. Text analytics can play an impor-tant role in performing deep and insightful analysis of conversa-tional transcripts. In this paper we address the problem of extract-ing important (and unimportant) discourse patterns from call center conversations. We show that by aggregating over a few hundred calls, specific discourse patterns begin to emerge for each class of calls. We also demonstrate that these discourse patterns also serve as useful features for call classification and clustering required in the tasks of call routing, obtaining call log summaries, agent assist-ing and monitoring, automatic domain model generation, system evaluation and modeling, business insight generation, etc.
Today X  X  call centers handle a wide variety of domains such as computer sales and support, mobile phones, car rentals, apparels, and so on. It has been observed that within a domain, or within an instance of a domain, the interactions in a contact center follow specific, repetitive patterns. This is mainly because of the similar nature of the queries, requests, and complaints received from cus-tomers. For example, in a call center that handles car bookings, the call flow remains unchanged between calls. The agent starts out by introducing herself, gathers the customer X  X  needs, suggests possible car options and finally makes a booking if the customer is satisfied. So a lot of phrases and discourse patterns get repeated.
We exploit this repetitive nature of the calls to extract key dis-course patterns within each class of calls. We show that such pat-terns are class specific and identifying them result in useful busi-ness knowledge as well as extremely useful features for call clas-sification and clustering. We argue that patterns consisting of se-quences of non-consecutive phrases capturing contextual correla-tions are vital features for information extraction from natural lan-guage text. We show that intra sentence (phrasal) and inter sentence (discourse) long range patterns are present in the calls. We discuss methods to extract these discourse patterns that capture key knowl-edge about the calls.

But what are the patterns of interest and what is the value of extracting them? A snippet of an example interaction between a customer and an agent is given in Figure 1. The greeting segment and the conversation relating to the agent asking for the pick up and car details are repeated across most of the calls. Consequently, this discourse pattern is present in most calls and is possibly uninter-esting. However, the discourse relating to the agent presenting the rate and the customer raising an objection to it would not be present in all the calls. In this case it is also interesting to capture how the agent overcomes the objection to make the sale. Thus, while conversations common to all calls may be uninteresting, spe cific portions of conversations that differ from others provide v aluable knowledge. Identifying such discourse patterns helps in ca pturing specific customer objections and agent best practices for ha ndling customer objections. Also this serves as a summary of the hun dreds of calls to a call center. Breaking up calls into familiar por tions that are common to all calls and portions typical to a specific clas s of calls, allows for easier analysis. It is important to captur e the busi-ness knowledge present in the calls as it allows the manageme nt to understand reasons for sale (no sale), evolve better agent t raining methods, understand customer needs, and so on.
Call center analytics is a relatively new area. There is need to an-alyze huge amounts of customer agent interactions to derive deeper insight into the business processes, customer needs and age nt capa-bilities. In this section we present some of the work we have b uilt upon and extended in this paper.

Call Center Analytics: Call transcripts have been analyzed for topic classification [6], quality classification [20] and fo r estimat-ing domain specific importance of call fragments [14]. It has also been shown that useful business intelligence can be obtaine d from customer agent conversations [16].

Extraction of Discourse Patterns: A call center conversation typically proceeds in the form of questions and answers. In a pro-cess like car rental booking, the questions are mostly asked by the agent as the call is agent driven in this case. The agent asks q ues-tions like  X  what is the pick up location  X ,  X  what is the pick up date  X , etc. The first task in learning the discourse model of conversa-tions of call center data is to identify the questions and the ir an-swers accurately. Question answer pairs can be identified in emails using lexical similarity and based on writing styles [15]. I denti-fying questions in conversations is difficult because featu res such as question marks are absent in spoken language. We use certa in keyword based methods to identify questions. Identifying u seful discourse patterns in conversations is typically based on c luster-ing speaker turns [13]. Question-answer extraction and clu stering them based on speaker turns helps in finding discourse patter ns ef-fectively.

Automatic Call-type Classification: A lot of work on automatic call type classification for the purpose of call routing ( [10 ], [7]), obtaining call log summaries [5], agent assisting and monit oring [11] has appeared in the past. In most cases, authors have mod -eled this as a text classification problem. These approaches rely on finding key phrases, which are used as features. For manu-ally transcribed calls, which do not have any noise, [11] a ph rase level significance estimate is obtained by combining word le vel es-timates that were computed by comparing the frequency of a wo rd in a domain-specific corpus to its frequency in an open-domai n cor-pus. In [18], phrase level significance was obtained for nois y tran-scribed data where the phrases are clustered and combined in to fi-nite state machines. Other approaches use n-gram features w ith stop word removal and minimum support ( [10] [5]).

Unsupervised Clustering of Calls: Clustering call records for automatic domain model generation [14], system evaluation and modelling [2] and business insight generation is fairly com mon in literature related to Call-Center Analytics. Call centers typically handle queries from various domains such as computer sales a nd support, billing, car rental, etc. Each such domain general ly has a domain model which contains common problem categories, typ ical customer issues and their solutions. These domain models, w hich are essential to handle customer complaints, are manually c reated over time. In the work [14], they propose an unsupervised tec h-nique based on call-record clustering to generate domain mo dels automatically from call transcriptions. The TAKMI (Text An alysis and Knowledge Mining) [12] project, which has been successf ully applied in the Call-Center domain to derive valuable busine ss in-sights from call transcripts/logs, relies heavily on the qu ality of the call-clustering.
We present a method to identify useful discourse patterns by ex-ploiting the redundancy in call center conversations. To do this, we first identify questions in conversations using a rule-base d method (Section 3). Next, we cluster the questions using features t hat are frequently occurring patterns of non-consecutive words an d named entities (henceforth called phrasal or horizontal patterns ) extracted from the question collection. Section 4 is dedicated to disc ussing details of the algorithm employed for determining frequent generic patterns of non-consecutive items. The motivation for name d en-tity identification and an overview of the technique employe d by us for this task is discussed in Section 5.1. It will be pointe d out later, that unigram and n -gram features are just special cases of the more generic patterns of non-consecutive words. Cluste ring of questions gives us a way of canonically representing each qu estion using the corresponding unique cluster label (or identifier ). Based on this representation of questions, we mine frequent disco urse pat-terns in the form of sequences of non-consecutive question c luster labels, named-entity annotations, and content words in the utter-ances (henceforth called discourse or vertical patterns ). This pro-cess again makes use of the algorithm discussed in Section 4. Fig-ure 2 illustrates the phrasal and discourse patterns . Note that the question cluster labels are assigned arbitrarily in the figu re.
Experimental results (Section 7) demonstrate three advant ages of discovering the generic discourse patterns: 1. These discourse patterns, when employed as binary featur es 2. These discourse patterns can also be effectively used as b i-3. We show that the discourse patterns that we extract contai n
Finally, in Section 8, we present our conclusions and scope f or future work.
Most business-oriented call center conversations proceed more or less in an inquisitive fashion unlike other interactive d ialogs. Both parties, i.e. , the agent and the customer, exchange information in the form of asking and answering questions relevant to com plet-ing the task at hand. Hence the discourse fragments in a dialo g are generally centered around questions. Consequently, id entifying questions becomes important. Here we describe a naive appro ach to question extraction from call center conversation data.
Unlike normal question answers from web pages like FAQs, etc. , identifying questions and answers in speech data is not stra ight for-ward. Punctuations are often missing and case information i s not present. Also informal language is used by the speakers in ex press-ing their inquiries. For example, agents may ask your last name, please? instead of asking the same question in a more formal man-ner. There can also be questions in declarative form, e.g. a c us-tomer says I would like to inquire if I can get a 12 passenger car for rent. The agents are usually trained to ask questions in an inter-rogative form. So agent questions are easier to extract.
The crux of any question extraction system is to identify the ask-ing point (how, what, when, where, etc.) in the text. Our ques tion extraction method uses a similar approach, except for the fa ct that our a sking point dictionary consists of not only key phrases (how , what, when, have you, what X  X  your etc.) but also special phra ses which are specific to the domain. Examples of domain specific k ey phrases include which location, which car, for how long . Any utter-ance that begins with a key phrase would be recognized as a que s-tion. To extract queries which do not contain an asking point , we use an additional set of words which indicate that a sentence could be a question. E.g. , words such as,  X  inquire  X ,  X  inquiry  X ,  X  please  X  in a normal sentence indicate a question. This method performs quite well in extracting questions from the data we have experimen ted with (see Section 7.2).
In this section we describe a variation of apriori algorithm for association rule mining applied on natural language text. T his al-gorithm is the backbone of our method for extracting discour se patterns. Patterns that capture non-consecutive item (ite ms could be words, entities, canonical question labels, etc. ) sequences are important for natural language text. Though we will discuss the approach in the context of mining frequent word sequences fr om sentences ( horizontal patterns ), the approach applies equally well to the mining of frequent vertical patterns consisting of question labels, named entities and content words in utterances from calls.
Natural language expressions allow inclusion of complex mo d-ifiers. The intervening modifiers improve the richness of exp res-sions and are thus important to natural languages. However, these variations make certain highly correlated words non-conse cutive. Figure 3, presents a set of example sentences. All are questi ons asking the same thing. However, they differ on the surface du e to the use of modifiers and other intervening words. Patterns in volv-ing non-consecutive word and entity sequences are required as fea-tures or clues to detect  X  X imilar" questions and  X  X imilar" a nswers, so that they can be respectively grouped into clusters repre sentative of their types.

We implemented an efficient algorithm described in [8] (an ex -tension of the apriori algorithm described in [1]) for findin g fre-quent patterns of non-consecutive tokens. The minSup value in the apriori corresponds to the minimum number of times a non-consecutive n-gram should occur across all the sentences. S o, the most frequent non-consecutive n-grams which exceed the thr esh-old value minSup are output by the system. Among all the non-consecutive patterns that can be extracted, the system gene rates the longest sequences. For example, from among the patterns #re nted #car#before# and #rented#car# and #rented#before#, the sy stem considers #rented#car#before# and leaves out the other two . While generating such patterns over text sentences we consider on ly the content words. Two parameters which control the generation of these patterns are the minimum threshold value ( minSup ) and the maximum token gap (  X  ) with which non-consecutive n-grams needs to be considered. We use the non-consecutive pattern m in-ing approach in two places in our method, one to cluster the se n-tences based on non-consecutive N-grams, and other to gener ate discourse patterns over the entire conversations. We will r eproduce here, some details of the pattern mining technique from [8] t o set the context for the discussions that follow. A pattern can be equivalently represented as a token-sequence . .* to.* &lt; company &gt;  X  can be represented as the following token se-We will denote a token-sequence comprising n tokens by s n
D EFINITION 1. A document d is said to contain an instance of token-sequence s n ; d  X  s n ;iff all tokens in s n appear in d in the same order, with a fixed upper bound,  X  , on the number of intervening words between every pair of successive tokens i n s Note that patterns with  X  = 0 correspond to the commonly used n  X  gram features.

Let  X  = 4 . The document D in Figure 4 has an instance of not have any instance of  X  [ toward, unit ]  X , because there are more than  X  intervening words. Similarly, D does not have any instance of  X  X ompany X  type.

Let D be a set of training documents of size |D| . freq ( s |{ d | d  X  X  , d  X  s n }| is the number of documents in D that contain instances of s n . freq ( s n ) can also be counted at a granularity finer than document, such as at the sentence level. However, freq ( s considers overlapping occurrences of s n to be a single occurrence
We define the support of s n as sup ( s n ) = freq ( s n ) be a threshold on support. We define S n as the set of all token se-quence of length n that have support greater than or equal to the minimum support minSup , i.e. , S n = { s n | sup ( s n )  X  minSup } . Let S  X  be the set of token-sequences s  X  such that each s between 1 and a threshold N and sup ( s  X  ) is above the threshold minSup . S  X  is identified using the algorithm outlined in Fig-ure 5. The algorithm is inspired by the a-priori [1] algorith m. The a-priori algorithm optimally and efficiently yields all ite m-sets or item-sequences, that have their support value above a given thresh-old value. We next prove that the algorithm given in Figure 5 o pti-mally discovers all non-consecutive token sequences.

D EFINITION 2. Let s i and s j be two token sequences where j &lt; i . We say s i  X  s j ; iff s j is a contiguous subsequence of s E.g. Token sequences &lt;t 1 , t 2 &gt; and &lt;t 2 , t 3 t &gt; is not a contiguous subsequence of s 4 . Note that every docu-ment that has an instance of a sequence also has an instance of each of its contiguous subsequences, i.e.  X  d  X  X  , d  X  s n  X  X  X  s s , d  X  s i . This implies  X  s n  X  s i , sup ( s n )  X  sup ( s
T HEOREM 1. If s n  X  X   X  , then  X  s i | s n  X  s i , s i  X  X  Proof: We prove the theorem by contradiction. Let there be an s , i &lt; n , s.t. s n  X  s i and s i /  X  X   X  . This implies sup ( s minSup . However, sup ( s n )  X  sup ( s i ) . Therefore, sup ( s minSup which implies s n /  X  X   X  . This is a contradiction.
Using the result of Theorem 1, we iteratively build token se-quences of length n + 1 from token sequences of length n .
The following corollary is important, as it implies that a se t of patterns obtained with particular values of  X  and N subsumes all sets obtained using lower values of the respective paramete rs.
If overlapping occurrences of a token sequence are consider ed as multiple occurrences, the monotonicity property of freq ( s not hold [19].
 Figure 5: The algorithm for generating the set S  X  of token-sequences with high support
C OROLLARY 1. Let S  X  (  X , N, minSup ) be the set of all non consecutive token sequences up to a length of N such that a max-imum of  X  intervening words are permitted between any two suc-cessive tokens. Let the minimum support count be minSup . Then,  X  N  X   X  N,  X   X   X   X , minSup  X   X  minSup , we have S (  X   X  , N  X  , minSup  X  )  X  S  X  (  X , N, minSup ) .
 Corollary 1 implies that larger values of  X  and N and smaller val-ues of minSup should be preferred. There is however a trade off; large values of N and  X  and small values of minSup can result in a large number of patterns which might not be very significa nt. Moreover, the time required for mining patterns grows almos t ex-ponentially with increasing values of  X  and N and decreasing val-ues of minSup . Given this trade off, we experimented with sample data to decide on reasonable values of these parameters. The se val-ues will be reported in the experimental section.
In this section we describe how we extract the sentence level patterns and use the patterns for clustering questions.
In this section we show how the task of named entity annota-tion is performed and why it is important in the perspective o f ex-tracting discourse patterns. We assert that named entity an notation improves the quality of phrasal patterns , thereby improving the quality of clustering of sentences based on these features. Effec-tive sentence clustering in turn improves the extraction of discourse patterns from calls. Sequences of word tokens annotated as named entities are represented using the canonical form of their n amed en-tity types, so that the text segments which differ only in the named entity instances can be easily grouped together. As an examp le, the two sentences  X  are you picking up the car at Cleveland?  X  and  X  are you picking up the car at Orlando?  X  will look similar after annotation as  X  are you picking up the car at LOCATION_CITY  X . Thus, named entity annotation is an important preprocessin g step in generating discourse patterns.

We handle a few domain specific named-entities that are most frequently used in particular type of conversations, for ex ample in a car rental process named-entities such as, CAR-MAKE (Chev ro-let, Toyota, Mercedes), CAR-SIZE (mid-size, full-size, mi ni van), VEHICLE-TYPE (car, van, suv), LOCATION-NAME (e.g. Chicago Cleveland, Orlando, Los Angeles ), DATE and TIME (e.g., Febru-ary 28th, monday the 28th of feb, morning of 28th, 10 o clock, 1 0 p.m. ), AMOUNT (e.g. $320.0, 344 dollars and 35 cents, 320.45 dollars ), CAR-TYPE (e.g. luxury car, economy car ), DISCOUNT-TYPE (e.g. AAA discount, military discount, sams club, AARP dis-count ) are very common.

We used a rule-based named entity annotator which has been de -veloped in-house for annotating unstructured text. The ann otator is written using Apache X  X  open source annotator framework UIM A (Unstructured Information Management Architecture). The work-ing of the annotator is as follows. The input text is first pass ed through a sentence chunker which identifies sentence bounda ries. Each sentence is tokenized based on a set of token-separator char-acters ( e.g., space,tab,-,*, etc. ). Tokens are then tagged with their dictionary attributes based on dictionary lookups.

Regular expressions over tokens and their dictionary and or tho-graphic properties are extensively used for named entity an nota-tions. The Common Pattern Specification Language (CPSL) 3 ifies a standard for describing Annotators that can be implem ented by a series of cascading regular expression matches. Our rul es for entity identification were composed using a subset of CPSL an d are similar to the syntax of rules used for named entity annotati ons in GATE [3]. The GATE architecture for text engineering uses Ja va Annotations Pattern Engine (JAPE) [4] for its information e xtrac-tion task. JAPE is a pattern matching language. Our rules sup port two classes of properties for tokens that are required by gra mmars such as JAPE: (1) orthographic properties such as an upperca se character followed by lower case characters, and (2) gazett eer (dic-tionary) containment properties of tokens and token sequen ces such as  X  X ocation X  and  X  X erson name X . The algorithm in the core an no-tator engine is independent of rules and dictionaries and is helpful in annotating different types of entities such as EMail, URL , Dates, etc , if corresponding rules and dictionaries exist.
In this step we try to capture questions that have similar mea n-ing but different surface forms. In call centers the same que stions get asked again and again in slightly different forms across calls. So for example, the questions in Figure 6, need to be identifie d as questions that have similar meaning. For this we perform clu ster-ing on the collection of all questions, using phrasal patter ns (mined from the question collection) as features. It is necessary t o cluster the questions across the conversations in an unsupervised w ay. A bag of words approach does not work because the vocabulary fo r a given call center task is limited. In a car rental task, for ex ample, there are less than 500 content words. The difference is in ho w the words come together in a given sentence which changes what is be-ing said. So,  X  do you have a valid credit card  X ,  X  do you have a valid aaa member card  X  and  X  do you have your credit card please tell me the number  X  may be difficult to distinguish using bag of words. Also approaches based on consecutive n-grams fail because o f the complex modifiers that natural language gives.

To make clustering most effective we annotate the questions . We observed that annotations result in better phrasal pattern s. This is shown in Figure 6. In this example we see that by annotating past and before as TIME the extracted feature X  X  count across all sen-tences increases and makes it an important feature for clust ering. http://incubator.apache.org/uima/ http://www.ai.sri.com/~appelt/TextPro Figure 7: Another Example Illustrating Importance of Anno-tations
The essence of clustering is to increase the distance betwee n dis-similar sentences and decrease the distance between simila r sen-tences. Hence, it is also imperative to avoid dissimilar sen tences getting clustered together based on words alone. Figure 7 sh ows examples of car type and place annotations. Such an annotati on re-duces the first question to  X  Can I know the cost of a CARTYPE  X  and second question to  X  How much does it cost from PLACE to PLACE  X . So the word  X  cost  X  even though common in both sentences, because of the presence of canonical tokens like CARTYPE and PLACE th e clustering algorithm performs well in grouping first senten ce sepa-rately from the second sentence. Every question is now repre sented using the following features for clustering, viz. , (i) words and (ii) phrasal patterns. In generating the phrasal patterns using apriori, we take a small value for the token gap parameter and high valu e for minimum support count, the K value parameter. This is under-standable since the sentences are small and we need good repr e-sentative features for clustering. Next, the questions are clustered using K-Means clustering algorithm. After extracting and c luster-ing the questions we represent each occurrence of a question in a call by its canonical cluster label as determined by the clus tering algorithm. An example of a call with labels assigned to its co ntents is shown in Figure 8 4 .
Extracting discourse patterns involves searching for freq uently occurring discourse fragments in the conversations. We defi ne what is called discourse features in a conversation as text with a sequence containing content words, named-entities canonicalized b y their types, questions canonicalized by their cluster-labels. T he task is to extract the discourse patterns from all conversations. To perform this task, we apply the algorithm we described in Section 4 ov er a larger sequence of tokens comprising the whole conversatio n text with canonicalizations. Refer to Figure 2 which illustrate s this pic-torially.

As shown in Figure 8, many calls have #GreetingQuestion# fol -lowed by #CostQuestion# from a customer, which in turn is fol -lowed by #LocationQuestion# from the agent. This is an exam-ple of a frequent discourse pattern. The advantage of using n on-consecutive pattern mining is that it makes our extraction r obust to noise. In spontaneous conversations people do not follow a r igid format. In the above example the agent could say  X  can you wait for a second sir  X  before the #LocationQuestion#. Additionally, since there are many different irrelevant utterances like  X  give me a moment sir  X , used in conversations, we need to pick the most
For simplicity and effectiveness of understanding in the pa per we manually renamed some arbitrary question-cluster-labels given by clustering tool weka to meaningful names in the figure 8. relevant sequences which capture the discourse informatio n. Non-consecutive pattern mining allows us to handle all these var iations. We choose a large value for the token gap parameter  X  . This choice can be justified, since we had already shown that there can be i rrel-evant discourse fragments in between some important ones. H ence we need to relax the value of token gap to capture sequences of im-portant discourse fragments separated by several other tex ts in be-tween. In the following sections we illustrate how these ext racted discourse patterns can be used as features for better classi fication of calls in call center domain. Also we explained some class spe cific discourse patterns which emerge as top features through cla ssifica-tion.
In this section, we present the experimental study of our tec h-nique. We start by describing the experimental setup and the data set. To bring out the value of the discourse patterns in call c en-ter conversations we present three sets of results. First, w e show that the extracted discourse patterns when used for call cla ssifica-tion result in improved performance compared to bag of words , unigram, bigram and trigram based techniques. This alludes to the fact that there are class specific discourse patterns presen t in the calls. Next we show unsupervised clustering of call-record s using the discourse features which resulted in better entropy com pared to using uni-grams and bi-grams. Next we show that we are indeed able to extract the key discourse patterns for a given class.
We collected 935 calls from a car rental help desk. We obtaine d automatic transcriptions of the dialogs using an Automatic Speech Recognition (ASR) system. The transcription server, used f or tran-scribing the call center data, is an IBM research prototype. The speech recognition system was trained on 300 hours of data co m-prising of call center calls sampled at 8KHz. These calls wer e of three types, viz. , calls that resulted in booking ( booked ), calls that did not result in booking ( unbooked ) and service calls where cus-tomers were seeking information and not trying to make a book ing ( service ). The calls that did not result in a booking ( unbooked ) were further divided into sub classes based on the reason for thei r not re-sulting in a successful booking. A call can become unsuccess ful when the agent and customer do not reach common terms based on the customer X  X  requirement. Specifically, we concentrated on the classes  X  X ates too high X ,  X  X navailability of car X  and  X  X ot m eeting requirements X . The first class corresponds to customers not mak-ing bookings because they thought the rate being quoted by th e agent was too high. In the second, the car being asked by the cu s-tomer was not available. And in the third, the customer did no t meet one or more requirements for renting a car, such as payme nt option requirements, driving license requirements, etc.

The question extraction step extracted around 7000 questio ns from the data of which around 5000 are questions asked by the agent and 2000 are questions asked by the customer from a tota l of 935 conversations. By randomly sampling some calls, we es -timated that the average total number of questions asked by t he customer and agent within a  X  X ooked X  conversation is 10, wit hin an  X  X nbooked X  conversation is 8, and within a  X  X ervice X  conv ersa-tion is 5. Based on these sampled numbers, we estimated that t here should be around 7152 questions in the entire data. On the sam -pled calls, our question extraction method yielded precisi on, recall and F1 measures tabulated in Table 1. The definitions for prec ision, recall and F1-measure are given below.
 P recision = Number of instances correctly classified in a class F 1 Measure = 2 1
As can be seen from Table 1, the F1 measure for each category is above 0 . 9 . Recall from Section 3, that question extraction is not an end in itself, but is only a technique used for feature c on-struction for the tasks of finding frequent discourse patter ns, call classification and clustering. We report the representativ e num-bers in Table 1 only to give an idea about the quality of our que s-tion extraction technique. The phrasal patterns for questi ons were derived by employing the algorithm (Figure 5) explained in S ec-tion 4, with the following parameter settings: (maximum tok en gap)  X  = 5 , (minimum support threshold) minSup = 5 and (max-imum number of items in a pattern) N = 5 . We generated the ques-tion clusters for agents X  and customers X  questions separat ely using k = 30 in the k-means implementation in Weka [17]. We used the Weka toolkit [17] for clustering the questions. Subsequ ently, frequent discourse patterns were mined using the algorithm (Fig-ure 5) explained in Section 4 with the following parameter se t-tings: (maximum token gap)  X  = 15 , (minimum support threshold) minSup = 5 and (maximum number of items in a pattern) N = 7 .
We show that classification of call-records, which is the und er-lying theme in applications such as call routing, call log su mmary generation, agent assisting and monitoring can significant ly benefit from the use of discourse patterns as proposed in our work. Th e following tables show the results obtained on classificatio n of the calls into the three classes discussed previously. We rando mly split the data into test and train set. 80% of the data is used for tra ining the classifier and the rest 20% is used for testing. Results ar e re-ported as averages over 5 random train-test splits. We illustrate the results using two types of classification methods X  the Naive -Bayes Classifier and Support Vector machines implemented in Weka [ 17]. We measure the results in terms of precision, recall and F1-m easure of the classifier.

Classification of  X  X nbooked" calls is important from the cal l cen-ter company point of view, for understanding the reasons beh ind calls being unsuccessful or ending abruptly. In this task we achieve significant improvement in the results when our approach is e m-ployed compared to classification using unigrams, bi-grams and tri-grams. Table 2 shows the results by using unigrams and bi -grams. Table 3 shows the results using a combination of unigr ams, bi-grams and tri-grams. Tables 5 and 4 shows the results for o ur method; using the discourse patterns as features for classi fication along with unigrams and bi-grams.

We asserted in the beginning that there are discourse segmen ts in the conversation which are common across all the calls such a s the greeting part, the agent asking for details of car reservati on, date and time of picking, etc. These are the  X  X nimportant" discou rse patterns that are identified by our algorithm. These are redu ndant as far as classification is concerned. Further, they occur th e most number of times since they are present in all the calls irresp ective of class. We can find the top-K most frequently occurring unimpo rtant patterns and remove them, K value can is dependent on the data. This is known as feature selection. After doing this, the top dis-tinguishing features in the classification (Table 5) begin t o emerge; the precision, recall and F1-measure of the classes improve d sig-nificantly as shown in the Table 5 compared to the results in Ta ble 4. The summary of classification results is shown in 6. All the se results are when using Naive Bayes classifier.

The results for support vector classification for the same da ta are given in the tables 7 and 8. Table 7 gives results for unigr ams and bigram features using support vector classification. Ta ble 8 gives results for the combination of unigram, bigram and dis course features with feature selection using support vector class ification. We find that the precision, recall and F-measure using SVM are slightly better compared to those generated by Naive Bayes C las-sifier. With SVM, the precision, recall and F-measure on comb i-nation of unigrams, bigrams and discourse features with fea ture selection is much better compared to using unigrams with big rams.
Precision Recall F-Measure Class 0.48 0.65 0.55 rates 0.37 0.21 0.267 unavailability of car 0 0 0 not meeting requirements Table 2: Classification Results with Unigram and Bigram Fea-tures
Precision Recall F-Measure Class 0.48 0.65 0.55 rates 0.34 0.26 0.29 unavailability of car 0 0 0 not meeting requirements Table 3: Classification Results with uni-grams, bi-grams, a nd tri-gram Features
Precision Recall F-Measure Class 0.533 1 0.696 rates 1 0.273 0.429 unavailability of car 0 0 0 not meeting requirements Table 4: Classification Results with uni-grams, bi-grams an d discourse pattern Features
Precision Recall F-Measure Class 0.7 1 0.82 rates 0.857 0.6 0.71 unavailability of car 0.6 0.39 0.47 not meeting requirements Table 5: Classification Results using Naive-Bayes Classifie r with uni-grams, bi-grams and discourse pattern features af ter feature selection by removing unimportant discourse patte rns
Precision Recall F-Measure Class 0.7 1 0.82 rates 0.77 0.9 0.83 unavailability of car 1 0.222 0.364 not meeting requirements Table 7: Classification Results with only Unigrams and Bi-grams using Support Vector Machines
To validate the claim that clustering applications can sign ificantly benefit from our method of call record representation using p hrasal and discourse patterns, we conducted clustering experimen ts on a call-transcript corpus from the car-rental domain. The cor pus con-tains 233 transcriptions from 4 business categories explai ned ear-lier. ( X  X ates too high",  X  X navailability of cars",  X  X ot mee ting re-quirements",  X  X pecific car requirements not met"). The clus tering results are as shown in Figures 9 and 10. The clustering was pe r-formed using CLUTO [9] and evaluated using its entropy and pu -rity functions. Cluster purity indicates the degree to whic h a cluster contains concepts from one class only (perfect purity would be 1). Cluster entropy indicates whether concepts of different cl asses are represented in the cluster (perfect entropy would be 0). The math-ematical formulas for entropy and purity are given in the Fig ure 9. We find that the results on using discourse features are bette r com-pared to using the uni-grams and bi-grams (Figures 9 and 10). The summarized results are shown in 10.
In this section we illustrate the class specific and unimport ant discourse patterns extracted by our algorithm. Figure 11 sh ows an example of a discourse that is found commonly in all calls. Th is discourse fragment is an instance corresponding to the most com-mon discourse pattern that our algorithm extracts from the d ata. The extracted pattern looks like this: #GREETING-QUESTION#VEHICLE-QUERY# PICKUP-DATE-QUERY#DATE-ENTITY# where tokens ending with  X  X UERY" are question clusters and D ATE-ENTITY is the annotation for date ( 3rd November ) 5 . This pattern
For simplicity and effectiveness of understanding in the pa per we renamed the arbitrary question-cluster-labels generated by cluster-
Precision Recall F-Measure Class 0.774 1 0.873 rates 0.818 0.818 0.818 unavailability of car 1 0.444 0.615 not meeting requirements Table 8: Classification Results using Support Vector Classi fica-tion with uni-grams, bi-grams and discourse pattern featur es after feature selection Table 9: Entropy and Purity in CLUTO. S r is a cluster, n is the size of the cluster, q is the number of classes, n number of concepts from the i t h class that were assigned to the r h cluster, n is the number of concepts, and k is the number of clusters.

Entropy Purity Representation 0.782 0.536 uni-grams+bi-grams 0.441 0.777 uni-grams+bi-grams+discourse patterns Table 10: Summary of clustering results from tables 9 and 10. Higher the purity or lower the entropy measure, better is the clustering. The best measures for each representation is re -ported in bold font; it can be easily seen that  X  uni-grams+bi-grams+discourse pattern features  X  give far better performance than just  X  uni-grams+bi-grams  X  . defines an instance of a discourse in which GREETING-QUESTIO N (e.g. how may i help you ) of the agent is followed, with some to-ken skips, by VEHICLE-QUERY (e.g. do you have a 12 passenger van ) by customer, which is followed by PICKUP-DATE-QUERY (e.g. what date and time you want to pick the car ) followed by the DATE-ENTITY. This is an example of an unimportant discourse pattern since it covers a common discourse found in most call s.
Now we show some important discourse patterns. Figure 12 shows an example of a discourse from the  X  X ates too high" call . The prominent class-specific discourse feature extracted b y our al-gorithm is: #AMOUNT-ENTITY#NAME-QUERY#better#price# , where AMOUNT-ENTITY indicates the amount annotation of $421.07 in the text, NAME-QUERY indicates query of agent ask-ing for customer X  X  name (e.g. can i have your last name ) which is followed, with some skips, by two content words better and price . This indicates that the extracted feature covers the discou rses from data which indicate the reason behind unbooked nature of the call because of customer not being satisfied with the price quoted by agent.

Figure 13 shows an example of a discourse from the  X  X navail-ability of car" class. The pattern extracted here is: #CARTYPE-QUERY#CARTYPE-ENTITY#not#available#, where, CARTYPE-ENTITYis annotation type for economy car , CARTYPE-ing tool weka, to meaningful names in the above representati on of discourse feature Figure 9: Clustering Results with text features (uni-grams and bi-grams) QUERY indicates the agent X  X  query What type of car would you like to go for .

Figure 14 shows an example of a discourse from a call belong-ing to  X  X ot meeting requirements" class. The top distinguis hing discourse feature extracted by our algorithm for this class is: #CC-QUERY#RTT-QUERY#valid#credit#card# where CC-QUERY indicates the credit card query ( e.g. ,  X  Do you have a valid credit card  X ) and RTT-QUERY indicates the query  X  do you have round trip travel ticket  X . A domain expert independently identified, by manually going through the calls, the discour se snip-pet of figure 14 as important to identify the reason for unsucc essful nature of the call in this class.

In this section we showed how some of the most common dis-course patterns automatically extracted by our algorithm a nd clas-sification cover relevant discourse fragments from the call s. These results also show that extraction of relevant class-specifi c discourse patterns has been made possible by accurate question extrac tion and clustering.
In this paper we proposed a method for automatic extraction o f important discourse patterns from call center conversatio ns using frequent sequences X  pattern mining approach. We presented a uni-fied approach for speech data mining by doing question-answe r extraction, named entity recognition, extracting pattern s of non-Figure 10: Clustering Results with discourse patterns, uni -grams and bi-grams consecutive items (such as words, named entities, question labels, etc. ), clustering and classification using these features. The e xtrac-tion of the patterns of non-consecutive items is done along t wo di-mensions, (i) on the words at the sentence level and other (ii ) on the sequence of speech utterances in conversations. This en ables us to extract important discourse patterns from the calls. In most of the previous works [13] the generation of such patterns wa s performed on the whole transcript, without much discrimina tion between speech utterances. Because of the noise encountere d in this form of data, the results on classification using discou rse fea-tures have not been found to be very promising. After abstrac ting portions of text with question-clustering and named-entit y-labeling we were able to achieve better class accuracy as well as speci fically identify discourse patterns in the calls which are typical o f an issue being addressed such as  X  X ates being too high",  X  X navailabi lity of car",  X  X ot meeting requirements".

In all the stages, care is taken such that our methods are doma in independently applicable. For the question extraction pha se, the domain specific key phrases dictionary is a plug in. This ensu res that the question extraction technique is applicable to cal l center conversational data which proceeds mainly by questions and an-swers. Our methods though are not applicable to call convers ations which are not significantly question-driven. But fortunate ly this method is still powerful enough for a huge class of contact ce nter data. The annotator framework expects a set of dictionaries specific to the domain and a set of rules. Also the technique we employe d is time-effective since it does not use parsing, natural lan guage dis-course analysis techniques and does not require large corpo ra for training. [1] R. Agrawal and R. Srikant. Fast algorithms for mining [2] F. Bechet, G. Riccardi, and D. Hakkani-Tur. Mining spoke n [3] H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. [4] H. Cunningham, D. Maynard, and V. Tablan. JAPE: a Java [5] S. Douglas, D. Agarwal, T. Alonso, R. Bell, M. Gilbert, [6] A. Gilman, B. Narayanan, and S. Paul. Mining call center [7] P. Haffner, G. Tur, and J. Wright. Optimizing svms for [8] S. Joshi, G. Ramakrishnan, S. Balakrishnan, and [9] G. Karypis. CLUTO -a clustering toolkit. Technical Repo rt [10] H.-K. J. Kuo and C.-H. Lee. Discriminative training in [11] G. Mishne, D. Carmel, R. Hoory, A. Roytman, and A. Soffer .
Figure 14: Snippet of a  X  X ot Meeting Requirements" class [12] T. Nasukawa and T. Nagano. Text analysis and knowledge [13] D. Padmanabhan and K. Kummamuru. Mining [14] S. Roy and L. V. Subramaniam. Automatic generation of [15] L. Shrestha and K. McKeown. Detection of question-answ er [16] H. Takeuchi, L. V. Subramaniam, T. Nasukawa, S. Roy, and [17] I. H. Witten and E. Frank. Weka -a machine learning [18] J. H. Wright, A. L. Gorin, and G. Riccardi. Automatic [19] M. Zhang, B. Kao, D. W. Cheung, and K. Y. Yip. Mining [20] G. Zweig, O. Siohan, G. Saon, B. Ramabhadran, D. Povey,
