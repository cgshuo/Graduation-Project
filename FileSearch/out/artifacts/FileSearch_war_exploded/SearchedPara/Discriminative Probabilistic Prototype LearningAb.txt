 Edwin V. Bonilla edwin.bonilla@nicta.com.au Antonio Robles-Kelly Antonio.Robles-Kelly@nicta.com.au A fundamental problem in machine learning is that of coming up with useful characterizations of the in-put so that we can achieve better generalization ca-pabilities with our learning algorithms. We refer to this problem as that of learning representations . This has been a long standing goal in machine learning and has been addressed throughout the years from differ-ent perspectives. In fact, one of the simplest and oldest attempts to tackle this problem was given by Rosen-blatt (1962) with the Perceptron algorithm for classifi-cation problems. In such an approach it was suggested that we can have non-linear mappings of the inputs so that the obtained representation allows us to discrimi-nate between the classes with a simple linear function. However, the mapping (or  X  X eatures X ) should have been engineered beforehand instead of being learned from the available data. Neural networks and their back-propagation algorithm (Rumelhart et al., 1986) became popular because they offered an automatic way of learning flexible representations by introduc-ing the so-called hidden layers and hidden units into multilayer Perceptron architectures. Kernel-based al-gorithms and, in particular, support vector machines (SVMs, Scholkopf &amp; Smola, 2001) offered a clever al-ternative to neural nets, circumventing the problem of learning representations by using kernels to map the input into feature spaces where the patterns are likely to be linearly separable. However, SVMs are inher-ently non-probabilistic and unsuitable for applications where one requires uncertainty measures around their predictions.
 In this paper we propose a simple yet powerful ap-proach to learning representations for classification problems where an original input datapoint is de-scribed by a set of feature vectors and its associated output may be given by soft labels indicating, for ex-ample, class probabilities, degrees of membership or noisy labels. Our approach to this problem is to rep-resent an input datapoint as a K-dimensional vector, where each component is a mixture of probabilities over its corresponding set of feature vectors. Each probability indicates how likely a feature vector is to belong to one-out-of-K unknown prototype patterns. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional ap-proaches based on likelihood maximization. More im-portantly, both the model parameters and the proto-type patterns can be learned from data in a discrim-inative way. To our knowledge, previous approaches have not addressed the problem of discriminative pro-totype learning within a consistent multi-class proba-bilistic framework (see section 5 for details). In this paper we are interested in multi-class classifi-cation problems for which an input point is character-ized by a set of feature vectors S = { x (1) ,..., x ( M ) } where each feature vector may describe, for example, some local characteristics of the input point. Addi-tionally, we consider the general case where the out-puts may be given by soft labels indicating, for exam-ple, class probabilities, degrees of membership or noisy class labels. Hence, our goal is to build a probabilistic classifier based on given training data, which is com-where S ( n ) is the set of feature vectors in the n th tu-ple. In general, the number of feature vectors is dif-ferent for each input and therefore we shall describe the C-dimensional vector of soft labels (e.g. empirical probabilities) associated with the C output classes of the n th training instance. Obviously, these probabili-ties are constrained by P C j =1  X  P ( y n = j ) = 1, where y denotes the latent class assignment of datapoint n . A common approach to prototype-based learning de-scribes an input by a histogram of words from a vocab-ulary of size K . This histogram is commonly known as the bag-of-words representation. In order to spec-ify the vocabulary it is customary to use clustering methods such as K -means or generative models such as Gaussian Mixtures, which are often used as a disjoint step before training a specific classifier. The model for extracting such representations is given by: f k ( x ) = where z is a K -dimensional vector to be used as the input representation for a specific classifier; {  X  k } K are usually referred to as the centers; and  X  k is set to 1 /M n . We will refer to each f k ( x ) as a prototype func-tion as it performs the encoding of each D -dimensional vector x into its corresponding (binary) K -dimensional representation. It is important to realize that this method is a winner-takes-all approach where each fea-ture vector x is assigned to only one centre. This is the main motivation for our method where we will re-lax this assumption and propose a fully discriminative probabilistic model for learning such representations. Our model assumes a bag-of-words representation as given by equation (2). However, here we consider that the prototypes are probabilities and are given by: where  X  is a rate parameter (or inverse temperature). Note that when  X   X   X  Equation (3) becomes equiv-alent to the hard limit in Equation (1). Therefore, f ( x ) is the probability of feature vector x belonging to  X  X luster X  k and z k is a mixture of these probabili-ties.
 In addition to defining how to map the set of input vectors into parameterized probabilistic prototype rep-resentations, our method requires the definition of a discriminative probabilistic classifier. In principle, this could be any classifier that focuses on defining the con-ditional probability: directly in terms of our prototype representation z . Here we have used X def = { x ( j ) } M j =1 and made explicit the dependency of z on its corresponding feature vec-tors. In the sequel, for simplicity in the notation, we will drop this dependency . Note that our model is a (conditional) directed probabilistic model. This con-trasts with other approaches such as latent-variable CRFs which are undirected graphical models. As we shall see later, we will focus on a softmax classifier due to the simplicity and efficacy of this parametric model. However, it is clear that we can also incorporate non-parametric classifiers. We expect such approaches to be more effective than their parametric counterparts and we postpone their study to future work. In this section we are interested in learning the pa-rameters of our discriminative probabilistic prototype framework. These parameters are: the rate parameter  X  , the vocabulary or centers {  X  k } K k =1 and the param-eters of the discriminative classifier  X  . This could be effected using a number of optimization methods in-cluding simulated annealing and Markov Chain Monte Carlo. Here we propose direct gradient-based opti-mization of the data log-likelihood.
 3.1. Direct Likelihood Maximization with Assuming iid data, the log-likelihood of the model pa-rameters given the data can be expressed as: where  X  P ( y n ) refers to the soft labels associated with input n , e.g. the empirical class probabilities. In order to optimize the data log-likelihood we use a Quasi-Newton method (BFGS) to which we provide the fol-lowing gradient information: and where: for k,` = 1 ,...K and d = 1 ,...D . The advantage of the above formulation is that the partial derivatives wrt the cluster centers can be computed in closed-form. Note that although these updates are straight-forward to derive, we specify all the details here for completeness and for their later use when showing the relation between our method and LVQ in Section 3.3. The derivatives are given as follows: Moreover, by using equations (2) and (3) we have that: = A similar approach can be followed to compute the partial derivatives wrt  X  . Hence we have that:  X  L n  X  X   X  X  3.2. Discriminative Parametric Model: For the case of a parametric model such as the softmax classifier:
P ( y = i | z ,  X  ) = the local log-likelihood terms can be written as: The corresponding derivatives wrt the prototype rep-resentations are given by: Finally, we also need the gradient information wrt the model parameters (  X  ): Equations (5) and (19) can be modified so as to in-clude a regularization term  X   X  tr (  X  T  X  ), where  X  is a regularization parameter; tr (  X  ) is the trace opera-tor; and  X  is the matrix of weights with columns given by each  X  i , i = 1 ...,C . It is well known that the so-lution for the weights in this case corresponds to the MAP solution when considering a Gaussian prior over the weights  X  i  X  N (  X  i | 0 , X  I ). We can have a similar prior or regularization for  X  .
 We note here that optimization of the objective func-tion in Equation (5) wrt to all the parameters is a non-convex problem. However, as we shall see in sec-tion 4, we use coordinate ascent and optimization of the model parameters  X  , given all others fixed, is a convex problem.
 3.3. Relation to LVQ Learning Vector Quantization (LVQ, Kohonen, 1990) is a prototype-based learning algorithm that uses the class label information to adapt the prototypes (i.e. cluster centers). The idea is that the prototypes should move towards the training examples in their corresponding class and away from training examples with different labels. As in the k-means algorithm, the assignment of datapoints to prototypes corresponds to the rule in Equation (1). If we were going to update the prototypes in our model with gradient ascent, this update would become: where  X  is the learning rate.
 By expanding Equation (11), substituting Equations (12) and (18), and assuming hard labels, i.e.  X  P ( y is 1 for only one label and zero for all the others, we should get:  X   X   X   X  + X By taking the hard limit for the assignment of the sam-ples to a single prototype (and assuming that all sam-ples corresponding to a single data-point are assigned to the same cluster): f ` ( x ) = 1 and, consequently, f ( x ) = 0 for k 6 = ` we have:  X  If | S n | = 1 then the factor premultiplying (  X  `  X  can be absorbed into the learning rate and, therefore, we obtain the LVQ update. In our model, this factor is interpreted as the difference between the parameter corresponding to the prototype ` for the label of the current datapoint , i.e (  X  y n ` ) and the average (wrt the posterior probabilities) of the parameters of the other classes. As a result, we can view the gradient-ascent updates for our method as a relaxed version of LVQ. In this section we present results on synthetic data, shape classification, hyperspectral image classification and people X  X  work class categorization. For all our ex-periments, we employ our prototype learning approach where we iterate the learning of the prototype param-eters {  X  ` } K ` =1 ,  X  and the model parameters  X  via co-ordinate ascent on the model likelihood. We have ini-tialized the cluster centers making use of k -means and varied the size of the vocabulary, i.e. K , according to the experimental vehicle. To illustrate the behavior of our algorithm and to show its performance with re-spect to competitive benchmarks, the first three prob-lems studied (synthetic data, shape classification, hy-perspectral image classification) consider the common case of hard labels and our last experiment on people X  X  work class categorization investigates the use of class probabilities as soft labels. 4.1. Illustration on Synthetic Data These experiments are based on a set of 20 datapoints each comprised by a set of M n  X  { 1 ,..., 20 } feature vectors in a 2-D space. Here we compare the baseline model yielded by k -means clustering (with K = 2) against our discriminative model. Figure 1 shows the original datapoints in the two-dimensional space along with the cluster centers (top). We see that the cluster centers learned by our discriminative approach (Panel b) are quite different even though the clusters obtained by k-means (Panel a) are very close to those used to generate the data. At the bottom panel we show the prototype representation given by both methods and we observe that the representation learned by our method is much more discriminative as it takes into consideration the class labels. 4.2. MPEG DataSet Our first real dataset is given by the MPEG-7 CE-1 Part B database (Latecki et al., 2000), which we will refer to as the mpeg-7 dataset. This contains 1400 binary shapes organized in 70 classes, each comprised of 20 images. We have sampled 1 in every 10 pixels on the shape contours and we have built a fully con-nected graph whose edge-weights are given by the Eu-clidean distances between each pair of pixel locations. These weights are then normalized to be in the interval [0 , 1]. The feature vectors are given by the frequency histograms of these distances for every node. In our experiments, we have used 10 bins for the frequency histogram computation and set K = 200.
 For purposes of shape categorization, we compare our method to three alternatives. The first one is a prototype-based baseline akin to the bag of words ap-proaches in computer vision (Fei-Fei &amp; Perona, 2005). Our baseline recovers prototypes via k -means cluster-ing. The shapes under study are then described by the prototype-based representation which we then employ as input to a classifier. The classifier used for this base-line is also a softmax classifier as in our method. In other words, the only difference between this baseline method and our approach (probabilistic prototype) is how these prototypes are learned.
 The other two alternatives are specifically designed for purposes of shape classification. These are the shape and skeletal contexts in Belongie et al. (2002) and Xie et al. (2008), respectively. Once the shape and skeletal contexts are at hand, we train one-versus-all SVM clas-sifiers whose parameters have been selected through ten-fold cross validation. For all our shape categoriza-tion experiments, we have divided the graphs in the dataset into a training and a testing set. The train-ing set comprises 50% randomly selected graphs, i.e. 700 and the other 50% of the data was used for test-ing. For these partitions we have effected five trials. The categorization results are shown in Table 1. The table shows the mean percentage of correctly classi-fied shapes and the corresponding variance. Despite the basic strategy taken for the construction of our graphs, which contrasts with the specialized nature of the skeletal and shape contexts, our probabilistic pro-totype method outperforms the alternatives. 4.3. SPECTRAL Dataset This application considers hyperspectral imagery from real-world scenes that include various types of materi-als. We have annotated these images at a pixel-level considering 10 different classes (C=10): tree trunk, light poles, shadow on grass, grass, road, white line on road, shadow on road, leaves, sky, and white regions on sky. We considered 24 different images from which we have extracted 1 , 746 , 708 data points with their corre-sponding labels. In order to characterize an input data point with a set of vectors ( S n ) we have considered neighborhood information according to 7  X  7 windows. Hence, for each data point we have 49 feature vectors. We have subsampled the data so as to include 1000 training instances and 16643 test instances across 10 different partitions.
 Note that our method is effectively learning the pro-totypes that can be used to represent the spectra for the objects in the scene as a linear combination of the spectral signatures of its material constituents. In geo-sciences and process control this is known as unmixing (Bergman, 2006). Current unmixing methods assume availability of the end-member spectra, which usually involves a cumbersome labeling of the end member data, effected through expert intervention.
 For comparison we use the standard prototype ap-proach (based on k-means for learning the cen-ters), and the InfoLoss (Lazebnik &amp; Raginsky, 2009) method. InfoLoss adapts the prototypes based on the optimization of an information-theoretical crite-rion. For this method, we have used an in-house im-plementation whose parameter settings have been set as described in Lazebnik &amp; Raginsky (2009). For the standard prototype approach and InfoLoss, as in our method, we have used a softmax classifier so as to al-low a direct representation-quality comparison via the classification rates for our approach and those yielded by the alternatives. In Table 2, we show the accuracy of the methods evaluated along with two standard er-rors. As before, probabilistic prototype refers to our method and standard prototype refers to the represen-tation computed via the direct application of k-means for learning the centers. From the table, we can con-clude that our method outperforms the alternatives by delivering a mean categorization rate of 81.25%. 4.4. Test Likelihoods on MPEG-7 and Now, we turn our attention to the evaluation of the methods from a probabilistic point of view by report-ing the likelihood of the models on the test data. In Figure 2(a), we show the test data log-likelihood for our probabilistic prototype approach and the baseline. The bar corresponds to the mean across the five tri-als and the segments account for two standard errors. Note that the log-likelihood for our approach is signifi-cantly greater than the one yielded by the standard ap-proach. This is since the performance measures above account for the correctness of the labels yielded by the classifier devoid of their probability of error. Thus, our method not only delivers better performance than the alternatives, but also provides better probability esti-mates. In Figure 2(b) we show similar results for the spectral dataset. As with the mpeg-7 dataset, the test data log-likelihood for the spectral dataset de-livered by our approach is greater than the one yielded by the alternatives and shows less variability. 4.5. ADULT Dataset Finally, we applied our method to the adult dataset from the UCI machine learning repository (Frank &amp; Asuncion, 2010). This dataset has been extracted from census-based data and the original task was to predict whether a person makes over $50K a year. It contains continuous and discrete input variables including edu-cation , age , work class (with eight different categories), etc. In order to use this dataset as a test benchmark for our method, we have grouped people X  X  records ac-cording to the values of the discrete variables except native country and work class and we have selected the latter variable as our target labels. Hence, we have an 8-dimensional output variable. We represented an in-put instance by the set of vectors that belong to the same group (i.e. with the same values for all the dis-crete features excluding native country). Similarly, we have computed soft labels (  X  P ) by calculating for each group the proportion of records that have the same value for the corresponding label. Finally, we have sub-sampled the data to have 4146 different instances and considered 50% for training and the other 50% for testing. The regularization parameters for our model and the baseline have been set-up via cross-validation. The (average) KL -divergence between the true (empir-ical) class distribution and the predictive distribution is 1.12 bits and if the soft labels were thresholded to provide a hard class assignment the accuracy of the model would be 77%. One interesting application of our model on this dataset is the automatic  X  X iscovery X  of the latent population prototypes and how these re-late to the class labels under consideration. Due to space limitations, we have not included these results here and postpone their analysis to future work. Generative models have been proposed as probabilistic generalizations of ad-hoc learning methods mostly for unsupervised learning scenarios (see e.g. Bishop et al., 1998). The work by Seo &amp; Obermayer (2002) is re-lated to ours in their attempt to generalize LVQ but their probabilistic model is inherently generative (see their equation 4). Their objective function is based on likelihood ratios and does not arise naturally from their original model.
 Neural networks and, more recently, deep belief net-works (DBNs) have proved popular as methods for learning latent representations with the goal of tack-Salakhutdinov, 2006; Bengio &amp; LeCun, 2007). In par-ticular, our method can be seen as a generalization of radial basis function (RBF) networks that considers multiple probabilistic observations; applies a pooling operator over the set of vectors belonging to the same instance; and optimizes the parameters via a coordi-nate ascent mechanism. There is also an interesting relation between our proposed learning algorithm and how deep architectures are currently trained as we ini-tialize our method with k-means, which can been seen as a pre-training stage, which is followed by a fine-tuning stage in DBNs.
 In the computer vision community, summarization into a codebook has been used by a number of ap-proaches in the literature. For example, Gemert et al. (2008) replace histograms with kernel density estima-tion (KDE) in the construction of codebooks and use the extracted features in conjunction with SVMs (non-probabilistic approach) for classification. It is interest-ing to mention the different between InfoLoss (Lazeb-nik &amp; Raginsky, 2009) and our approach. The meth-ods are similar in spirit but their difference is anal-ogous to the difference between filter and wrapper methods in feature selection. While InfoLoss focuses on a filter metric (an information-theoretical objec-tive function), our method directly includes the spe-cific classifier in the learning process (wrapper). On a similar vein, Boureau et al. (2010) propose the learn-ing of mid-level features for recognition in computer vision. Their work focuses on (multiple) binary classi-fiers and does not provide consistent probabilistic out-puts across all classes. It is well-known in the machine learning theoretical literature that simple approaches to combining these classifiers such as one-against-all are inconsistent in that given optimal binary classi-fiers this  X  X eduction X  may not yield optimal multi-class classifiers (see e.g. Beygelzimer et al., 2009). Finally, recent work by Bergamo et al. (2011) has pro-posed an approach to learning compact representa-tions for novel category recognition. Their focus is on learning compact descriptions that can be used, for example, in image retrieval. We have presented a probabilistic model for the dis-criminative learning of latent representations, which corresponds to a relaxed version of the popular ap-proach of prototype-based classification. From the ap-plication viewpoint, our method can be viewed as a discriminative technique that can be used for unmixing in geosciences and remote sensing (Bergman, 2006). It can also be applied to other problems, such as popu-lation modeling where labels and proportions of these can be associated to groups of instances.
 Our method requires, in general, a model for a prob-abilistic discriminative classifier and for purposes of illustrating the utility of our approach we have used a softmax classifier. However, our approach can, in principle, be used with any discriminative and prob-abilistic classifier including non-parametric methods. In the future we aim to explore the combination of non-parametric methods with our prototype learning approach and also investigate better optimization al-gorithms for parameter learning, e.g. based on mean field approximations.
 We thank Sarah Namin and Lars Petersson for provid-ing us with the spectral dataset. NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.

