 This paper proposes multi-task copula (MTC) that can handle a much wider class of tasks than mean regression with Gaussian noise in most former multi-task learning (MTL). While former MTL em-phasizes shared structure among models, MTC aims at joint predic-tion to exploit inter-output correlation. Given input, the outputs of MTC are allowed to follow arbitrary joint continuous distribution. MTC captures the joint likelihood of multi-output by learning the marginal of each output firstly and then a sparse and smooth output dependency graph function. While the former can be achieved by classical MTL, learning graphs dynamically varying with input is quite a challenge. We address this issue by developing sparse graph regression (SpaGraphR), a non-parametric estimator incorporating kernel smoothing, maximum likelihood, and sparse graph structure to gain fast learning algorithm. It starts from a few seed graphs on a few input points, and then updates the graphs on other input points by a fast operator via coarse-to-fine propagation. Due to the power of copula in modeling semi-parametric distributions, SpaGraphR can model a rich class of dynamic non-Gaussian correlations. We show that MTC can address more flexible and difficult tasks that do not fit the assumptions of former MTL nicely, and can fully exploit their relatedness. Experiments on robotic control and stock price prediction justify its appealing performance in challenging MTL problems.
 K.4.1.4 [ Computing Methodologies ]: Machine Learning X  Learn-ing Paradigms, Multi-task Learning ; K.4.3.5.3 [ Computing Method-ologies ]: Machine Learning Approaches X  Learning in Probabilis-tic Graphical Models Machine Learning, Statistics Multi-task learning; Copula; Semi-parametric model; Sparse Struc-tured learning; Covariance Regression
Effectiveness of multi-task learning (MTL) has been broadly proved in massive real applications such as gene expression analysis [41][24], brain activity prediction [22], collaborative filtering [38], informa-tion retrieval [25] and economic forecasting [13]. MTL studies how to exploit the underlying task relatedness in learning multiple tasks X  models so that tasks can benefit from each other. Precisely, let x  X  X  be input features in domain X  X  R p and y = { y i } k be outputs of k tasks in domain Y = Q k i =1 Y i  X  R k . Given as-sociated k training sets { X i ,Y i } k i =1 with X i  X  R R i  X  1 , in machine learning it is of general interest to learn a joint conditional probability model p ( y | x ) , which contains all predictive information of outputs y given x . Classical MTL methods study the case when p ( y | x ) is a product distribution Q p ( y component a mean regression model f i ( x ) : X  X  X  i plus an i.i.d. Gaussian noise i  X  X  (0 , X  2 i ) where y i = f i ( x ) + i . Note the above model reduces the model complexity of p ( y | x ) on the expense of ignoring the dependency among outputs. Two families of MTL methods have been devel-oped based on (1) and exploit task relatedness in different ways.
We first consider the case when f i ( x ) is a parametric function f ( x ; w i ) with parameter w i . While maximizing the likelihood p ( y | x ; W ) with W = { w i } k i =1 leads to k independent M-estimators learning regardless of task-relatedness; maximizing the posterior of W with a joint prior p ( W ) has motivated a line of MTL methods, which relate different tasks by endowing shared structures across { w i } k i =1 . MAP estimator  X  W = arg max W log[ p ( W ) Q yields a regularization based learning framework d where f i ( X i ; w i ) extends f i ( x ; w i ) by applying f in X i , and R ( W ) is a regularization term resulting from p ( W ) and encoding the shared structure. In previous MTL methods, a linear function f i ( x ; w i ) = xw i is usually adopted, whereas their used regularizers R ( W ) are of different types. For instance, an ` ( q &gt; 1 ) norm regularizer R ( W ) = P k i =1 k w i k q nonzero patterns among different w i and thus yields joint feature selection across tasks [2, 23, 29]; a trace norm regularizer R ( W ) = k W k  X  restricts vectors { w i } k i =1 lying in a low-dimensional space that indicates each w i is generated from a few shared basis vectors [16, 30]; the vector fields [3] of predictor functions span a low-dimensional subspace, a graph regularizer R ( W ) = tr( WLW can equip { w i } k i =1 with clustered [39], tree [17], manifold or graph weighted fusion structure [6]. Moreover, sum mixture of two regu-larizers for W = P + Q is proposed, where P encodes shared struc-ture and Q captures task-specific structure [1, 5, 15, 14], which in-cludes alternating structure optimization (ASO), incoherent sparse low-rank (ISLR) formulation, robust multi-task feature learning (rMTFL), robust multi-task learning (rMTL), and so forth. In these methods, the task relatedness refers to shared model structures that lead to smaller hypothesis parameter space. When the prior p ( W ) agrees with the true models, a better generalization performance is prov-ably achieved.

The model (1) also gives rise to a non-parametric MTL scheme named  X  X ulti-task Gaussian process (GP) regression (MTGP) X  [4], which admits more flexible form of f i ( x ) and assumes identical in-put X  X  X i  X  [ k ] in training set (so n  X  n i  X  [ k ] ). However, rather than imposing a GP prior to each single f i ( x ) , a task-related GP prior is placed over all latent functions { f i ( x ) } k ance is the Kronecker product of a covariance function k x over input x and a  X  X ree form X  positive semi-definite task similar-ity matrix K f  X  R k  X  k . The key assumption added to (1) here tween different predictive function values on different input data points. Learning hyper-parameters  X  x in k x and K f is conducted by likelihood maximization rather than MAP. In MTGP, it is the stationary hyperparameter K f rather than the structure shared by task-wise parameters { w i } k i =1 encodes the task relatedness. In-stead of reducing the hypothesis parameter space to gain a better generalization performance, it abridges the output space.
So the inference of one task output on a new data point requires weighted averaging of kn outputs from the whole training set. In-verse of an kn  X  kn (assume identical input X  X  X i  X  [ k ] n i  X  [ k ] ) covariance matrix is needed in weight computing and might cause computational burden. Learning hyperparameters  X  x and K f is conducted by likelihood maximization rather than MAP, n  X   X  which can be solved by EM algorithm that requires kn  X  kn matrix inverse per iterate.
Although the model in (1) has long been favored and thoroughly studied in former MTL research, its effectiveness is only limited to tasks that are mean regressions with independent Gaussian noise.
Firstly, in a wider range of MTL problems, the k tasks may vary in diverse types and in different combinations. This implies the conditional marginal p ( y i | x ) may be non-Gaussian, or the marginals for different outputs y i are from different classes of distributions. In the following, we discuss some non-Gaussian examples of p ( y Comparing to mean regression with Gaussian noise, median regres-sion with symmetric heavy-tailed noise such as Cauchy or Laplace is more robust to outliers in practice; quantile regression with asym-metric heavy-tailed noise is commonly used in ecology and econo-metrics to discover input-output relationship when input has weak ties with output mean; Poisson distribution is naturally used in count data regression; Gamma distribution is the best model for studying the emission mechanism of gamma-ray burst data; mix-ture model is better to fit a multi-mode distribution that can cap-ture p ( y i | x ) where y i is the prediction from multi-model or multi-expert [27], which could be multiple user groups in recommenda-tion system [19], multiple mechanisms resulting in brain activities, or multiple systems for economics forecasting. Thus a flexible and expressive multi-task learning model should allow the combination of such different choices of p ( y i | x ) . However, most previous MTL models cannot provide such rich possible options of p ( y
Secondly, correlations among output variables { y i } k i =1 nored in (1) to obtain a decomposable model in learning. However, they are natural task-relatedness that are possible to be directly es-timated from data and improve prediction. Instead of employing a joint likelihood to capture such relatedness, exploration of task-relatedness in previous MTL relies on the joint prior of parame-is required either to know them (e.g., joint prior p ( W ) or K advance, or to learn them indirectly from data in a complicated fashion, like EM algorithm. Both are difficult.

In contrast, if we consider about learning the joint likelihood as a multivariate Gaussian p ( y | x ) , a direct idea embedding output cor-relations in p ( y | x ) = p ( = y  X  f ( x )) is to posit dependent noises { } k i =1 . We also face difficulty because this demands learning a covariance function  X ( x ) : X  X  R k  X  k , which is a difficult open problem in former studies [10, 20]. It becomes even harder ous densities to express more general tasks, because in this case the joint density p ( y | x ) falls into a broad category of general distribu-tions, in which the output correlations are usually hard to be even parametrized.

In this paper, we propose multi-task copula (MTC) that can choose to build a joint predictive distribution p ( y | x ) , which also encodes the output correlations as a decomposable parametric part c (  X | x ) in its density function. Thanks to the flexibility of copula model, MTC overcomes the aforementioned two main drawbacks of pre-vious MTL methods, and can handle more general task types as two-stage learning scheme for MTC will be advocated, in which we first learn the marginal p ( y i | x ) for each task independently, and then learn the copula density c (  X | x ) encoding outputs dependency. One merit of this scheme is that previous MTL methods can be seamlessly incorporated into the first stage, so the task relatedness can be encoded and fully exploited by both joint prior from previ-ous MTL and joint likelihood in MTC. Another merit is that, in the special case of Gaussian copula, learning the non-Gaussian correla-tions among outputs encoded in c (  X | x ) can be reduced to estimating a Gaussian covariance on each given x .

In order to provide a reliable estimation of the Gaussian covari-ance function of x that defines the Gaussian copula, we develop an efficient non-parametric estimator  X  X parse graph regression X  (Spa-GraphR) to predict input-dependent Gaussian covariance, whose inverse (i.e., precision matrix) is sparse and encodes a structured dependency graph. The zero entries correspond to the conditional independence between outputs on the graph. In MTC, rather than estimating a stationary dependency graph by  X  X ovariance selection X  [7], SpaGraphR enables the graph of y to dynamically vary with input x and thus provides a more expressive and flexible model for describing output correlations. Such local covariance has been broadly observed in various real problems, and verified to be a help-ful information for prediction. For example, the correlations of dif-ferent factors describing climate or market behaviors are always changing with time and locations. The prediction of these factors essentially relies on these changes in their correlations.
Similar problem of learning covariance function has rarely been studied before. SpaGraphR updates graphs on all given input data points in a hierarchical order and is a result of kernel smoothing in conjunction with ` 1 regularized likelihood maximization. In learn-ing algorithm, it starts with sparse precision matrices on a small set of representative points, and then updates the precision matri-ces on other points for several rounds by a fast proximal operator. In each round, a few new points joined in the set of points with up-dating graphs, and thus the graph estimator is refined in a coarse-to-fine multi-resolution style. Each update merely requires small matrix multiplications and entry-wise soft-thresholding. Hence, SpaGraphR has promising efficiency on big data. Experiments on challenging real problems, i.e., robotic control and stock predic-tion, rigidly demonstrate the effectiveness of MTC and SpaGraphR in MTL problems. MTL Gaussian/Logistic Allow -MTC Any continuous dist. Allow Allow
We propose MTC that tailors the copula model [35] to the MTL problem as p ( y | x ) = c ( F 1 ( y 1 ) ,F 2 ( y 2 ) ,  X  X  X  ,F k ( y where F i ( y i ) is the cumulative distribution function (CDF) of out-put y i associated with density g i ( y i ) = p ( y i | x ) so that F takes all marginal CDFs { F i ( y i ) } k i =1 as its augments, and main-tains the output correlations in a parametric form conditioned on x . As a semi-parametric model for joint prediction, MTC is easier to learn and less prone to over-fitting than a fully non-parametric model, while more expressive and flexible than a fully parameter-ized model. In theory, let F ( y ) be the joint CDF, (1) can be derived from the k th order partial derivative of a copula function C (  X  ) .
D EFINITION 1. Let Y 1 ,...,Y k be real random variables marginally uniformly distributed on [0 , 1] . A copula function C : [0 , 1] [0 , 1] is a joint (cumulative) distribution By Sklar X  X  seminal theorem [35], any joint CDF F ( y ) can be repre-sented as a copula function C (  X  ) of its univariate marginals { F .
 Moreover, C (  X  ) is unique in the case of continuous marginals. Fur-thermore, the converse is also true, i.e., any copula function defined on a combination of any marginal functions gives a valid joint dis-tribution with the same marginals.

This property is critical to MTC. Firstly, it provides a theoret-ical guarantee that MTC is able to model the multi-task learning model p ( y | x ) by arbitrary joint joint distribution with continuous marginals. So it overcomes the two limitations of previous MTL models mentioned in the beginning of Section 1.2. Secondly, it in-dicates that the model is decomposable, so we can separately learn the marginal prediction model p ( y i | x ) for each task and the then learn the outputs dependency conditional on x encoded in C (  X  ) . In addition, we do not need to estimate the partition function for normalization in MTC. The copula model p ( y | x ) (4) automatically turns into a legal joint prediction model taking output correlations into account.

Theoretically, we can construct any legal copula C (  X  ) by a cop-ula trick derived from inverting Sklar X  X  theorem. For example, Gaussian copula [9, 21] where  X  is the standard normal distribution and  X   X  is the zero mean multivariate Gaussian with covariance matrix  X  , and Archimedean copula [26] where  X  is the so called generator function with a single parameter are two families of copulas widely applied in practice. Both have the strength to model dependency in high dimensions (i.e., with large k ). While Archimedean copula is easier to estimate due to its single parameter, Gaussian copula is more powerful for expressing the sophisticated dependency graph structure.

When the structure of a graphical model G = ( V,E ) (where V is the vertex set and E is the edge set) is known in advance, we can decompose the joint density into the product of several components based on local copulas. Two recent examples are tree-structured copula [18] with density where c ij is a bivariate copula density associated with y and copula Bayesian network (CBN) [8] with density g ( y ) = where { pa ij } k i j =1 is the parents of node i and R c  X  X opula ratio X  computed from a local copula associated with y its parents.

It is not hard to verify that the density functions of all the above copula models can be written in the same form as p ( y | x ) in MTC (4), i.e., a parametric component 1 encoding the conditional depen-dency of y multiplied by the product of all marginals { p ( y In addition, since all of them are capable of handling an arbitrary number of outputs, after they have been learned from training data, they can be freely plugged into MTC (4) and results in a joint pre-diction model.

In Table 1, we provide a comparison between classical MTL (1) and MTC (4) from the perspective of model. In addition, an illus-tration of MTC is also given in Figure 2.
Note the parameters in both c (  X | x ) and the marginals { p ( y of copula models introduced before are assumed to depend on x in MTC. Therefore, before producing a valid prediction model, these
In CBN it is not always a legal copula density c (  X | x ) , but can still be treated as a parametric component conditioned on x in MTC, and thus causes no difference in learning and inference. parameters have to be learned as parametric or non-parametric func-tions of x from training data. According to the  X  X istribution gen-eration X  mechanism of copula suggested by Sklar X  X  theorem, we adopt a two-stage learning scheme that estimates the marginal dis-functions of x to build the joint prediction model.

A primary advantage of this scheme is that its first stage is ex-actly equal to likelihood or posterior maximization ignoring out-puts dependency, and thus can be accomplished by off-the-shelf classical MTL methods in Section 1.1 or their trivial variants. For example, in the case of robust regression, we consider a general task with output y i = f i ( x ) + i , where noise i is a random vari-able obeying symmetric (for mean regression) or asymmetric (for quantile regression) distribution, and function f i ( x ) is a constant computed from x . When f i ( x ) is assumed to have a parametric form f i ( x ; w i ) like linear or log-linear functions, the parameters { w i } k i =1 sharing structures can be immediately learned by the reg-ularization based MTL (2). When a more flexible non-parametric function f i ( x ) is preferred, the multi-task GP regression can be ap-plied to achieve k predictive functions with related GP priors.
In addition, in the case of multi-model or multi-expert, we can consider a more general task whose output y i obeys a conditional 1-D Gaussian mixture model such that When f ij ( x ) = xw ij , each task is exactly the  X  X aussian mixture regression X  (GMR) [12], and an MAP estimator of W with any joint prior p ( W ) mentioned in Section 1.1 can result in an EM al-gorithm, whose M-step invokes the regularized MTL (2). When f ( x ) is selected as a non-parametric function, each task collapses to the  X  X ixture of Gaussian processes X  (MGP) model [33]. The GPs associated with different tasks can be related by the same trick used in multi-task GP regression via positing a task-similarity ma-trix K f . An EM algorithm is also required in learning K kernel parameters. Since the above EM algorithms can be directly derived by standard procedures, we will not go into their details in this paper and leave them to a longer version.

Therefore, the first stage allows lots of flexible choices of tasks and their corresponding marginal prediction models. Moreover, the first stage enables MTC to take the advantage of the task related-ness exploited by joint prior in classical MTL methods, whose mer-its thus can be fully inherited in MTC. Together with the task re-latedness complied in the output dependency graph, which will be learned in the second stage, MTC can achieve a considerable per-formance boost. This is because the output dependency encoded by c (  X | x ) and the task-relatedness encoded by joint prior in classi-cal MTL play independent roles in MTC model, and thus cannot be transferred to each other. To see this, we investigate the two terms on the right hand side of (4). While the outputs dependency is encoded by the first term c (  X | x ) , the coherent latent structures exploited by classical MTL are embedded in the second term, i.e., the product of marginals. It is worthy noting that the marginals can also be trained independently as single tasks, and the MTC can still give promising prediction. The benefit of doing this is we can enjoy faster speed from divide-and-conquer or distributed compu-tation naturally in this stage.

After the first stage, we obtain the estimates of all the marginal densities, as well as those of their associated cumulative functions {  X 
F in the form of c { F i ( y i ) } k i =1  X ( x ) from the training set where the copula parameter  X ( x ) is a function of x . For an Archimedean copula with only one parameter, and copula models built from local copulas defined on a given graphical model, the regular maximum likelihood estimate is able to achieve an accurate estimate
In this paper, an primary challenge is to learn a locally smooth or piece-wise linear covariance function  X ( x ) in accompany with sparse precision S ( x ) = ( X ( x ))  X  1 for Gaussian copula (6) ap-plied to MTC. The main reason for choosing Gaussian copula here is its capability of modeling high dimensional distributions. Learn-ing Gaussian copula in MTC requires a simultaneous learning of the graph structure and the corresponding edge weights, both vary-ing with x . This problem is extremely challenging because: 1) a parametric  X ( x ) easily suffers from over-fitting in large k and p case, for example, k 2 p parameters are required to be estimated even when each entry of  X ( x ) is a linear function of x ; 2) a non-parametric  X ( x ) by kernel smoothing of local covariance matri-ces is unlikely to also be sparse; 3) point-wise covariance selection based on only one sample of y on each point x is seriously ill-posed and computationally intractable.
In this section, we propose an efficient algorithm called  X  X parse graph regression X  (SpaGraphR), which learns a non-parametric co-variance function  X ( x ) that has sparse inverse S ( x ) on any x in the dataset and are locally smooth. SpaGraphR leverages the lo-cal smoothness to avoid a great amount of costly computations, and achieves sparse . From the perspective of non-parametric re-gression, we need to estimate a sparse precision S ( x ) for multi-variate Gaussian variable z = {  X   X  1 (  X  F i ( y i )) } samples on each point x . However, we usually have only one out-put sample y i  X  R k on the i th input sample x i in training set X = { x i } n i =1 , which indicates only one sample z will firstly discuss two possible classes of covariance estimator that have been investigated recently.

Covariance selection (CS), first proposed in [7] and drawing tremen-dous interests recently [11, 34], aims to estimate a sparse precision matrix from insufficient samples in a high-dimensional space. In this case, maximum likelihood estimate  X  S =  X   X   X  1 cannot offer a legal  X  S , because the sample covariance matrix  X   X  is singular and thus does not have an inverse. Additional ` 1 regularization is ap-plied to S in CS for two purposes: 1) to obtain a valid S , and 2) to make the entries of S sparse, where the zeros indicate conditional independence and suggest a structured Gaussian Markov random field. CS is formulated as  X  The estimate  X  S  X   X  , X  has been proved to converge to the true S at dif-ferent rates in different norms by recent papers. These convergence rates differs in the order of dimension p yet share the same order O (1 / each x i , the variance of CS estimate (12) with  X   X  = z i corresponding to x i ) is unbearable. In addition, applying expensive CS algorithm to each data point is impractical on computation.
Different from CS, another strategy that is able to achieve a with lower variance is to take the local smoothness of  X ( x ) into account, which results in Nadaraya-Watson (NW) estimator [28, 36] based on kernel smoothing (with kernel function K ( x,  X  ) ) of covariance, i.e., NW estimator involves the neighbors of x i in the covariance es-timation on x i , and thus could provide a more precise estimator  X  K ( x i ) than z i z i function of x . However, in practice, the resulting  X  S ( x ) = ( X  does not hold a guarantee of sparsity, so we usually cannot gain a structured graph from directly taking the inverse. A straightforward approach to obtain a sparse graph regression in this case is to plug a NW estimator into CS by replacing  X   X  in (12) with  X  K ( x ) , i.e.,  X  when applied to a great number of data points, because we have to apply CS algorithm to each point.

According to the density function of multivariate Gaussian, it is the entries in precision matrix S that encode the weights of all edges on the dependency graph. Therefore, it is more preferable to seek a smooth precision function S ( x ) rather than a smooth covari-ance function  X ( x ) , when the generated sparse graphs are expected to exhibit structures and edge weights smoothly varying with x . In contrast to (13), we define an NW estimator for S ( x ) as Note S K ( x,X ) is defined as the kernel smoothing of the preci-sion matrices on training data points within the -ball B ( x,X )  X  i : k x  X  x i k 2  X  ,x i  X  X of x . If the neighboring precision negative kernel weights, S K ( x,X ) is a symmetric positive definite (PSD) matrix if all the involved S ( x i ) i : i  X  B ( x ) S
K ( x,X ) is a legal precision matrix estimator encoding underly-ing graph structure, and will be used for outputs dependency graph prediction in SpaGraphR.

Unfortunately, { S ( x i ) } i  X  B ( x,X ) in (14) are unknown and need to be estimated in advance. We cannot obtain them by applying S
K ( x,X ) again because it turns out to be the  X  X hicken or the egg X  dilemma. We also cannot apply the aforementioned CS estimate  X  S
K ( x ) , X  because the training set could be huge and thus causes heavy computational burden. Nevertheless, it is possible to start from a few graphs by applying  X  S  X  K ( x ) , X  to a small subset of train-ing points  X   X  X , and then develop an efficient operator to update graphs on the other points from the known ones in an incremental or propagation manner. By incorporating the likelihood maximiza-tion on point x , ` 1 regularization in (12), and local smoothness of S K ( x,X ) , we develop an operator S + ( x,  X ) for updating S ( x ) in SpaGraphR where  X   X  (  X  ) is an matrix element-wise soft-thresholding operator [  X   X  ( X )] i,j = sgn ( X i,j ) max ( | X i,j | X   X , 0) that results in sparse update S + ( x,  X ) , and  X  is a parameter adjusting the trade-off be-tween likelihood maximization and local smoothness. Kailath vari-ant of Woodbury identity is used to obtain the equality between the two representations in (15). As an important consequence, the second representation merely requires simple matrix-vector mul-tiplication and entry-wise substraction, and thus makes S considerably efficient to compute.

Let X  X  look at some motivations behind (15). Equivalently, it can be derived by applying a proximal operator [31] to PSD matrix S
V ( x,  X ) = (1  X   X  ) zz T +  X  ( S K ( x,  X ))  X  1 ization, i.e., According to the property of proximal operator, S + ( x,  X ) finds a sparse precision matrix (whose sparsity is controlled by  X  ) close to S V ( x,  X ) , whose inverse is a  X  X seudo sample covariance X  (1  X   X  ) zz T +  X  ( S K ( x,  X ))  X  1 comprised of a rank-1 sample covari-ance zz T and a covariance matrix estimated by inverting the NW estimator S K ( x,  X ) . The weight  X   X  [0 , 1] adjusts the contribu-tions of the two terms in building the pseudo sample covariance. When the neighboring graphs used in S K ( x,  X ) are precise with preferred sparsity, a large  X  is applied so that zz T is a minor max-imum likelihood correction term to ( S K ( x,  X ))  X  1 . Hence the re-sulting S + ( x,  X ) has sparse graph structure and edge weights close to those of S K ( x,  X ) and fitting training data z well. This can be verified more clearly in the second row of (15), in which S is comprised of the NW estimator S K ( x,  X ) and a rank-1 correc-tion term caused by z . With a large  X  , the resulting S codes a similar dependency graph as S K ( x,  X ) , while the likeli-hood on the unique sample z is properly maximized and the spar-sity is maintained by soft-thresholding. Hence, the proximal oper-ator (15) allows us to update sparse precision matrices on arbitrary points given those on a small subset  X  .

In SpaGraphR, we start from a small subset  X  0 storing the most representative points whose S ( x ) have been estimated by CS as  X  X eeds X , and a sequence of disjoint subsets {  X  i } M i =1 are ordered by their capability to represent the whole training set X . When the representative capability of a subset is measured by the squared error, the subset sequence can be built by top-down hierar-chical k -means clustering. In this case, cluster centers of the second layer constitute  X  0 , and  X  t  X  [ M ] is comprised of the training points closest to the cluster centers at the ( t + 2) th layer. In iterations, S ( x ) on points of each subset is updated given S ( x ) estimated on all the prior subsets by (15). For example, S ( x ) on points in subset  X  t is updated as S ( x j ) := S + ( x j ,  X  t  X  1 l =1  X  l
In SpaGraphR, graph regression S ( x ) on a smaller  X  places a warm start for the regression on a larger  X  , and thus the depen-dency graphs encoded by S ( x ) in X are updated in a coarse to fine multi-resolution manner by incrementing  X  of S K S ( x,  X ) . In addition, SpaGraphR is equal to applying a spe-cial proximal Newton-like method [31] to a CS problem (12) with  X   X  = S V ( x,X ) for each involved x . Since we do not know S
V ( x,X ) , in the t th iterate, a quadratic approximation of CS ob-ject function (12) on S = S V ( x,  X  t  X  1 l =1  X  l ) and with ) is minimized. We summarize SpaGraphR in Algorithm 4.
 Algorithm 1 Sparse Graph Regression (SpaGraphR) Input: X, { Y i } k i =1 , {  X  F i ( y i ) } k i =1 ,  X  ,  X  , M Output: precision matrices S ( x ) on all points
Compute { Z i } k i =1 by applying z i =  X   X  1  X  F i ( y i
Generate subset sequence {  X  i } M i =0 by ( M + 2) -layer top-down hierarchical k -means on X ;
Estimate S ( x ) on x  X   X  0 by  X  S ( x ) =  X  S  X  K ( x ) , X  algorithm can be applied; for t = 1  X  M do end for
Given a new instance x , the predictive distribution of y can be obtained by plugging x into the learned MTC model (4). Specif-ically, the precision matrix S in Gaussian copula density is esti-mated by S K ( x,X ) , while the marginals are achieved by plugging x into the classical MTL model learned in the first stage. If the goal is to achieve a distribution of y or its clustering structure, the above procedure directly provides the results. However, when exact val-ues for outputs y are required for prediction, an additional MAP inference needs to be conducted to p ( y | x ) in (4)  X  y = arg min Without loss of generality, we study the case of Gaussian copula, the above optimization is equal to  X  y = arg min which can be solved by alternating direction method of multipliers (ADMM) [31]  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X  where the subscript  X  t denotes the t th iterate, p/ 2 is the weight of quadratic penalty in augmented Lagrangian, q  X  R k is the La-grangian multiplier, and the diagonal of S is set to zero. In the above inference algorithm, the k outputs in y are updated indepen-dently as in classical MTL except an additional regularizer linking y with z i . The k elements in z are jointly updated according to their dependency graph encoded in S and relationship to y . Dual variable q is updated by gradient ascent. In practice, we usually initialize y by classical MTL such that So only a few subsequent iterates are required to gain an appealing joint prediction. Moreover, linear approximation could be applied to F i ( y i )  X   X ( z i ) in the updates of y and z for obtaining ana-lytical solutions to the minimizations. Hence, the joint prediction conducted by (19) is efficient.

Another optimization algorithm for solving (18) can be derived by computing the finite difference approximation of Jacobi matrix in computing the first order gradient. In particular, if we substi-to an unconstrained optimization, which can be efficiently solved by gradient descent method or accelerated first order optimization like Nesterov X  X  method. By using chain rule, the gradient of the objective function G ( y ) in (18) can be approximated by  X  X  ( y )  X  X  where h is a difference vector for y . GraphR on representative points selected by hierarchical k -means.
In the first experiment, we apply MTC and MTL algorithms to inverse dynamics dataset Sarcos 2 collected from a 7 degrees-of-freedom robot arm. It contains 48933 samples, each has 21 input features including 7 joint positions, 7 joint velocities plus 7 joint accelerations, and 7 joint torques as task outputs. In experiments, 400 samples are randomly selected for training and the rest are for test, in which averaged normalized MSE (nMSE) is measured for evaluation. We firstly apply single-task regression and regulariza-tion based MTL methods embedded in [40], and obtain 7 mean regression models. However, the Q-Q plot of the model noises { i = y i  X  xw i } 7 i =1 in Figure 2(a) shows that 3 outputs tend to have non-Gaussian noises, and thus implies the contradiction to as-sumed model in (1). This may cause sensitiveness to outliers and weaken the prediction performance.

We then apply MTC with Gaussian copula for achieving a joint conditional model p ( y | x ) that can fit data better. In particular, we change the predictive models for the 3 outputs to a median regres-sion with symmetric Laplacian noise and two quantile regression with asymmetric Laplacian noise, while keep the mean regression models for the rest outputs. In learning, these marginal models are estimated at first and then SpaGraphR is invoked to obtain a sparse precision function of x depicting the correlations among the Gaussian and non-Gaussian marginals. In Figure 2(b), we show the resultant sparse precision matrices on representative points identi-fied by hierarchical k -means ( 2 points per cluster and 2 on t th layer). It could be seem that both the graph sparse struc-ture and the edge weights vary smoothly in local area of X . Hence SpaGraphR provides a flexible model for outputs conditional cor-relations without losing robustness. In prediction, MTC has com-petitive performance comparing ro most classical MTL methods. The nMSE of different methods are listed in Table 2. In this case when the number of tasks is closed to the number of features, MTC exploiting outputs dependency is more effective in reducing model complexity. http://gaussianprocess.org/gpml/data/
In this experiment, we establish an MTC model for stock price prediction by learning from historical quote data collected from Ya-hoo! Finance 3 . Then we test and compare the prediction perfor-mance of MTC and other MTL methods for recently stock prices. In particular, we collect the weekly closing price of 35 famous companies X  stocks from their historical quotes on NASDAQ be-tween 01/01/2000 and 09/23/2013, which are 35 time series of stock prices in 716 weeks. For each stock, given historical prices in the pasting v weeks { q t  X  i } v i =1 , the price q t in t dicted by linear autoregression (AR) model  X  q t = P v i =1 w . Hence, we sample 600 weeks with equal time intervals, and let the stock prices in these 600 weeks as responses. For each week out of the 600 , we let the stock prices in the pasting 10 weeks as its input features. Therefore, for s th stock, we have a dataset with Table 2: Comparison of single-task regression (STR), dirty model for MTL [15], clustered MTL (CMTL) [39] and MTC on Sarcos dataset (nMSE).

However, when predicting multiple stocks, we expect the histor-ical prices of other stocks can also help the prediction. Therefore, we expand the input features and the AR model predicting the s stock X  X  price in the t th week turns to where s  X  [35] indexes the k = 35 stocks and t  X  [600] indexes the 600 weeks, and v = 10 . The inputs of the dataset for each stock changes to the same matrix [ X 1 ,...,X 35 ]  X  R 600  X  350 the experiment, we split the dataset by choosing the prices in the http://finance.yahoo.com/ first 400 weeks out of the 600 weeks as training set, and the prices in the rest 200 weeks for test. Note this setting is more challenging than random split and is closer to real problem, in which we expect to gain a reliable model for future prices only from historical prices.
Our goal here is to build a joint prediction model for all 35 stocks tured by the AR model (22), but also by leveraging the dynami-cally varying correlations among different stocks. When modeling model (22), an interesting and broadly verified phenomenon is that  X  q s,t in (22) performs more robust and reliable when assigned to the  X  th quantile (e.g., median, 25% , 75% ) of the conditional marginal rather than the mean. This indicates that the conditional marginals are non-Gaussian, possibly asymmetric and heave tailed, and the conditional correlations among outputs are also non-Gaussian. The former fact leads to the failure of most MTL methods based on mean regression with Gaussian noise, while the latter fact leads to the failure of most covariance regression/selection methods based on Gaussian covariance assumption.

Fortunately, MTC is able to precisely model the non-Gaussian conditional marginals and correlations by learning the marginal model and copula density model separately. In particular, we adopt a  X  th quantile regression with skewed Laplace noise for each marginal model, i.e., In the first learning stage, we impose additional ` 1 regularization to the coefficients w as a sparse prior. This is essentially helpful to our problem because 1) the current price of one stock is usually related to the prices of a few other stocks at a few historical time intervals rather than all stocks over all intervals; and 2) the number of training samples 400 is very close to the feature dimensions 350 and thus may introduce large variance. Given w , b can be easily obtained via MLE. The quantile  X  (reported in Table 3) and the weight of ` 1 regularization (either 0 . 2 or 0 . 3 ) for each stock are achieved by hypothesis testing and cross-validation, respectively. In the second learning stage, we compute 400 Gaussian samples Z from Y and the CDF of marginal models obtained in the first stage as training set for SpaGraphR, which learns the sparse and smooth graph function describing the dynamic graph over stocks. The sparse prior of the graph function also plays an important role here, because one stock currently is only related to a subset of other stocks in the same or related areas, and the influence of one area to another varies over time, which makes the edges in the graph changing over time.

We report both the ` 1 relative error |  X  q  X  q | / | q | averaged over all 200 testing weeks for each stock in Table 3. The performance of marginal models before joint inference (STR) is also reported in order to make the improvement solely brought by the sparse graph function more clear. For comparison, we also apply existing MTL methods to this problem and report their performance in the same ways. Different from modeling  X  q s,t in (22) by quantile regression, all of them model it by least square regression yet with different joint priors/regularization over all w .

The results in Table 3 show that MTC successfully and precisely predict most of the stock prices over time, while the other classic MTL fail on most of them (their training errors are between [0 . 5 , 2] but the test errors are much worse, indicating their joint likelihood is very far from the truth, which can be accurately modeled by MTC). This is because the mean regression with Gaussian noise has large variance on stock data, and classical MTL cannot cor-rectly capture and leverage the conditional non-Gaussian correla-tions by joint prior. Comparing to the prediction by single quantile regression models, the additional improvement of MTC prediction proves the essential role of SpaGraphR in relating multiple tasks to help prediction.
Multi-task learning (MTL) covers a rich class of machine learn-ing and statistical problems expecting to let different tasks benefit each other by exploring their relatedness [32][37]. However, in order to reduce MTL to a easier regularization problem, most pre-vious MTL methods limit the types of tasks, ignore the conditional dependency among outputs and merely rely on the pre-defined joint prior distribution of models to capture the relatedness. So they can hardly handle more complex tasks in practice, and usually achieve limited improvement over single task learning.

In this paper, we adapt copula model from semi-parametric statis-tics to the joint likelihood function p ( y | x ) in MTL. The capabil-ity of copula in generating arbitrary continuous joint distribution results in an expressive model that allows combination of differ-unified parametric description of outputs dependency. This struc-ture enables us to develop a two-stage learning scheme for the proposed  X  X ulti-task copula (MTC) X . While the first stage learns the marginals by using single task learning or any previous MTL method, the second stage aims at learning a conditional covariance function  X ( x ) (or precision function  X   X  1 ( x ) encoding a sparse de-pendency graph of y varying with inputs x .

Although this covariance regression is an open challenging prob-lem in recent machine learning community, we proposes an effi-cient nonparametric estimator for  X ( x ) called "sparse graph regres-sion (SpaGraphR)" that incorporates local likelihood maximiza-tion, kernel smoothing, and sparse structure of graph. SpaGraphR enables both the edge weighs and the sparse graph structure smoothly changing with varying x , which can be rarely achieved by previous methods. In addition, SpaGraphR starts from a few seed graphs on a small number of representative x and updates the graphs on other points from the known graphs by cheap matrix multiplication and entry-wise soft-thresholding. So it can refine the estimation accu-racy of  X   X  1 ( x ) in a coarse-to-fine grained space of x efficiently for big data.

Different from previous MTL methods relying on given joint prior applied to task models, MTC automatically learns the con-ditional dependency of task outputs which directly serves the joint prediction. The joint prediction can be obtained by solving an easy optimization problem.

We applied MTC to robotic control and stock price prediction problems, in which there exist strong correlations between task out-puts that cannot be correctly captured by previous MTL methods due to the non-Gaussian and asymmetric properties of their data distribution. In experimental comparison, we verify that the capa-bility of both integrating different types of marginal distributions and capturing their conditional dependency lends MTC the power to outperform other MTL methods.

It is worth noting that the models presented in this paper and used in experiments are merely special instances of MTC. MTC allows rich combination of marginal models from different distri-bution classes or even non-parametric distributions, and arbitrary copula density functions that describe the output dependency. Sim-ilar to copula which is a  X  X istribution generator X , MTC plays a role of  X  X odel generator X  for multi-task learning that can fit each task with the suitable marginal model and simultaneously fully leverage the task-relatedness encoded by both output dependency and joint prior of marginal models. Therefore, we believe lots of possible future contributions can be made to improve either MTC model for specified MTL problems or the efficiency of its learning/inference algorithms.
 We would like to thank Prof. Carlos Guestrin, Prof. Ben Taskar and Prof. Emily Fox for their important suggestions in improving this draft. This work is supported by Australian Research Council Projects FT-130101457 and DP-140102164.
 [1] R. Ando and T. Zhang. A framework for learning predictive [2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature [3] C. Z. J. Y. B. Lin, S. Yang and X. He. Multi-task vector field [4] E. Bonilla, K. Chai, and C. Williams. Multi-task gaussian [5] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse and [6] X. Chen, Q. Lin, S. Kim, J. Carbonell, and E. Xing. [7] A. Dempster. Covariance selection. Biometrics , 28:157 X 175, [8] G. Elidan. Copula bayesian networks. In Advances in Neural [9] P. Embrechts, F. Lindskog, and A. McNeil. Modeling [10] E. Fox and D. Dunson. Bayesian nonparametric covariance [11] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse [12] Z. Ghahramani and M. Jordan. Supervised learning from [13] J. Ghosn and Y. Bengio. Multi-task learning for stock [14] P. Gong, J. Ye, and C. Zhang. Robust multi-task feature [15] A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A dirty [16] S. Ji and J. Ye. An accelerated gradient method for trace [17] S. Kim and E. Xing. Tree-guided group lasso for multi-task [18] S. Kirshner. Learning with tree-averaged densities and [19] B. Liu, Y. Fu, Z. Yao, and H. Xiong. Learning geographical [20] H. Liu, X. Chen, J. Lafferty, and L. Wasserman.
 [21] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: [22] H. Liu, M. Palatucci, and J. Zhang. Blockwise coordinate [23] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [24] Q. Liu, Q. Xu, V. W. Zheng, H. Xue, Z. Cao, and Q. Yang. [25] X. Mao, B. Lin, D. Cai, X. He, and J. Pei. Parallel field [26] A. McNeil and J. Ne X lehov X . Multivariate archimedean [27] K. Mo, E. Zhong, and Q. Yang. Cross-task crowdsourcing. [28] E. Nadaraya. On estimating regression. Theory of [29] G. Obozinski, B. Taskar, and M. Jordan. High-dimensional [30] G. Obozinski, B. Taskar, and M. Jordan. Joint covariate [31] N. Parikh and S. Boyd. Proximal algorithms. Foundations [32] N. Quadrianto, A. J. Smola, T. S. Caetano, S. V. N. [33] C. Rasmussen and Z. Ghahramani. Infinite mixtures of [34] B. Rolfs, B. Rajaratnam, D. Guillot, I. Wong, and A. Maleki. [35] A. Sklar. Fonctions de repartition a n dimensions et leurs [36] G. Watson. Smooth regression analysis. Sankhya: The [37] S. Yang, Y. Jiang, and Z.-H. Zhou. Multi-instance [38] S. Yu, K. Yu, V. Tresp, and H. Kriegel. Collaborative ordinal [39] J. Zhou, J. Chen, and J. Ye. Clustered multi-task learning via [40] J. Zhou, J. Chen, and J. Ye. MALSAR: Multi-tAsk Learning [41] J. Zhu, B. Zhang, E. Smith, B. Drees, R. Brem, L. Kruglyak,
