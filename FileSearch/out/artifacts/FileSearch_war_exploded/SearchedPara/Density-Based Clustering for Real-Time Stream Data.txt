 Existing data-stream clustering algorithms such as CluS-tream are based on k -means. These clustering algorithms are incompetent to find clusters of arbitrary shapes and can-not handle outliers. Further, they require the knowledge of k and user-specified time window. To address these issues, this paper proposes D-Stream , a framework for cluster-ing stream data using a density-based approach. The algo-rithm uses an online component which maps each input data record into a grid and an offline component which computes the grid density and clusters the grids based on the den-sity. The algorithm adopts a density decaying technique to capture the dynamic changes of a data stream. Exploiting the intricate relationships between the decay factor, data density and cluster structure, our algorithm can efficiently and effectively generate and ad just the clusters in real time. Further, a theoretically sound technique is developed to de-tect and remove sporadic grids mapped to by outliers in order to dramatically improve the space and time efficiency of the system. The technique makes high-speed data stream clustering feasible without degrading the clustering quality. The experimental results show that our algorithm has su-perior quality and efficiency, can find clusters of arbitrary shapes, and can accurately recognize the evolving behaviors of real-time data streams.
 H.2.8 [ Database Management ]: Database Applications X  data mining Algorithms, Experimentation, Performance, Theory Stream data mining, density-based clustering, D-Stream, sporadic grids Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
Clustering high-dimensional stream data in real time is a difficult and important problem with ample applications such as network intrusion detection, weather monitoring, emergency response systems, stock trading, electronic busi-ness, telecommunication, planetary remote sensing, and web site analysis. In these applications, large volume of multi-dimensional data streams arrive at the data collection center in real time. Examples such as the transactions in a super-market and the phone records of a mobile phone company illustrate that, the raw data typically have massive volume and can only be scanned once following the temporal or-der [7, 8]. Recently, there has been active research on how to store, query and analyze data streams.

Clustering is a key data mining task. In this paper, we consider clustering multi-dimensional data in the form of a stream, i.e. a sequence of data records stamped and ordered by time. Stream data clustering analysis causes unprece-dented difficulty for traditional clustering algorithms. There are several key challenges. First, the data can only be ex-amined in one pass. Second, viewing a data stream as a long vector of data is not adequate in many applications. In fact, in many applications of data stream clustering, users are more interested in the evolving behaviors of clusters.
Recently, there have been different views and approaches to stream data clustering. Earlier clustering algorithms for data stream uses a single-phase model which treats data stream clustering as a continuous version of static data clus-tering [9]. These algorithms uses divide and conquer schemes that partition data streams into segments and discover clus-ters in data streams based on a k -means algorithm in finite space [10, 12]. A limitation of such schemes is that they put equal weights to outdated and recent data and cannot cap-ture the evolving characteristics of stream data. Moving-window techniques are proposed to partially address this problem [2, 4].

Another recent data stream clu stering paradigm proposed by Aggarwal et al. uses a two-phase scheme [1] which con-sists of an online component that processes raw data stream and produces summary statistics and an offline component that uses the summary data to generate clusters. Strate-gies for dividing the time horizon and manage the statistics are studied. The design leads to the CluStream system [1]. Many recent data stream clustering algorithms are based on CluStream X  X  two-phase framework. Wang et al. proposed an improved offline component using an incomplete partition-ing strategy [17]. Extensions of this work including cluster-ing multiple data streams [6], parallel data streams [5], and distributed data steams [3], and applications of data stream mining [11, 16, 13].

A number of limitations of CluStream and other related work lie in the k -means algorithm used in their offline com-ponent. First, a fundamental drawback of k -means is that it aims at identifying spherical clusters but is incapable of revealing clusters of arbitrary shapes. However, nonconvex and interwoven clusters are seen in many applications. Sec-ond, the k -means algorithm is unable to detect noise and outliers. Third, the k -means algorithm requires multiple scans of the data, making it not directly applicable to large-volume data stream. For this reason, the CluStream ar-chitecture uses an online processing which compresses raw data stream in micro-clusters, which are used as the basic elements in the offline phase.

Density-based clustering has been long proposed as an-other major clustering algorithm [14, 15]. We find the density-based method a natural and attractive basic clustering al-gorithm for data streams, because it can find arbitrarily shaped clusters, it can handle noises and is an one-scan al-gorithm that needs to examine the raw data only once. Fur-ther, it does not demand a prior knowledge of the number of clusters k as the k -means algorithm does.

In this paper, we propose D-Stream , a density-based clustering framework for data streams. It is not a simple switch-over to use density-based instead of k -means algo-rithms for data streams. There are two main technical chal-lenges.

First, it is not desirable to treat the data stream as a long sequence of static data since we are interested in the evolving temporal feature of the data stream. To capture the dynamic changing of clusters, we propose an innovative scheme that associates a decay factor to the density of each data point. Unlike the CluStream architecture which asks the users to input the target time duration for clustering, the decay factor provides a novel mechanism for the system to dynamically and automatically form the clusters by placing more weights on the most recent data without totally dis-carding the historical information. In addition, D-Stream does not require the user to specify the number of clusters k . Thus, D-Stream is particularly suitable for users with little domain knowledge on the application data.

Second, due to the large volume of stream data, it is im-possible to retain the density information for every data record. Therefore, we propose to partition the data space into discretized fine grids and map new data records into the corresponding grid. Thus, we do not need to retain the raw data and only need to operate on the grids. How-ever, for high-dimensional data, the number of grids can be large. Therefore, how to handle with high dimensionality and improve scalability is a critical issue. Fortunately, in practice, most grids are empty or only contain few records and a memory-efficient technique for managing such a sparse grid space is developed in D-Stream.

By addressing the above issues, we propose D-Stream, a density-based stream data clustering framework. We study in depth the relationship between time horizon, decay fac-tor, and data density to ensure the generation of high qual-ity clusters, and develop novel strategies for controlling the decay factor and detecting outliers. D-Stream automati-cally and dynamically adjusts the clusters without requir-ing user specification of target time horizon and number of clusters. The experimental results show that D-Stream can find clusters of arbitrary shapes. Comparing to CluStream, D-Stream is better in terms of both clustering quality and efficiency and it exhibits high scalability for large-scale and high-dimensional stream data.

The rest of the paper is organized as follows. In Section 2, we overview the overall architecture of D-Stream. In Section 3, we present the concept and theory on the proposed density grid and decay factor. In Section 4, we give the algorithmic details and theoretical analysis for D-Stream. We conduct experimental study of D-Stream and compare D-Stream to CluStream on real-world and synthetic data sets in Section 5 and conclude the paper in Section 6.
We overview the overall architecture of D-Stream, which assumes a discrete time step model, where the time stamp is labelled by integers 0 , 1 , 2 ,  X  X  X  ,n,  X  X  X  . Like CluStream [1], D-Stream has an online component and an offline component. The overall algorithm is outlined in Figure 1.

For a data stream, at each time step, the online com-ponent of D-Stream continuously reads a new data record, place the multi-dimensional data into a corresponding dis-cretized density grid in the multi-dimensional space, and update the characteristic vector of the density grid (Lines 5-8 of Figure 1). The density grid and characteristic vec-tor are to be described in detail in Section 3. The offline component dynamically adjusts the clusters every gap time steps, where gap is an integer parameter. After the first gap , the algorithm generates the initial cluster (Lines 9-11). Then, the algorithm periodica lly removes sporadic grids and regulates the clusters (Lines 12-15).
In this section, we introduce the concept of density grid and other associated definitions, which form the basis for the D-Stream algorithm.

Since it is impossible to retain the raw data, D-Stream partitions the multi-dimensional data space into many den-sity grids and forms clusters of these grids. This concept is schematically illustrated in Figure 2.
In this paper, we assume that the input data has d di-mensions, and each input data record is defined within the
Figure 2: Illustration of the use of density grid. space where S i is the definition space for the i th dimension.
In D-Stream, we partition the d  X  dimensional space S into density grids . Suppose for each dimension, its space S i i =1 ,  X  X  X  ,d is divided into p i partitions as then the data space S is partitioned into N = d i =1 density grids. For a density grid g that is composed of S
A data record x =( x 1 ,x 2 ,  X  X  X  ,x d )canbe mapped to a density grid g ( x ) as follows:
For each data record x , we assign it a density coefficient which decreases with as x ages. In fact, if x arrives at time t , we define its time stamp T ( x )= t c ,anditsdensity coefficient D ( x, t )attime t is where  X   X  (0 , 1) is a constant called the decay factor .
Definition 3.1. (Grid Density) For a grid g ,atagiven time t ,let E ( g, t ) be the set of data records that are map to g at or before time t , its density D ( g, t ) is defined as the sum of the density coefficients of all data records that mapped to g . Namely, the density of g at t is:
The density of any grid is constantly changing. However, we have found that it is unnecessary to update the density values of all data records and grids at every time step. In-stead, it is possible to update the density of a grid only when a new data record is mapped to that grid. For each grid, the time when it receives the last data record should be recorded so that the density of the grid can be updated according to the following result when a n ew data record arrives at the grid.

Proposition 3.1. Suppose a grid g receives a new data record at time t n , and suppose the time when g receives the last data record is t l ( t n &gt;t l ), then the density of g can be updated as follows: Proof. Let X = { x 1 ,  X  X  X  ,x m } be the set of all data records in g at time t l ,wehave: Accordingto(4),wehavethat: Therefore, we have:
D ( g, t n )= Proposition 3.1 saves huge amount of computation time. To update all grids at each time step requires  X ( N ) comput-ing time for density update at each time step. In contrast, using Proposition 3.1 allows us to update only one grid, lead-ing to a  X (1) running time. The efficiency improvement is significant since the number of grids N is typically large.
Moreover, Proposition 3.1 saves memory space. We find that we do not need to save the time stamps and densities of all the data records in a grid. Instead, for each grid, it suffices to store a characteristic vector defined as follows. We will explain the use of each element in the vector later.
Definition 3.2. (Characteristic Vector) The charac-teristic vector of a grid g is a tuple ( t g ,t m ,D,label,status ), where t g is the last time when g is updated, t m is the last time when g is removed from grid list as a sporadic grid (if ever), D is the grid density at the last update, label is the class label of the grid, and status = { SPORADIC, NORMAL } is a label used for removing sporadic grids.
We now need to decide how to derive clusters based on the density information. Our method is based on the following observation.

Proposition 3.2. Let X ( t ) be the set of all data records that arrive from time 0 to t ,wehave: 1) x  X  X ( t ) D ( x, t )  X  1 1  X   X  , for any t =1 , 2 ,  X  X  X  Proof. For a given time t , x  X  X ( t ) D ( x, t )isthesumof density coefficient of the t +1 data records t hat arrive at time steps 0 , 1 ,  X  X  X  ,t , respectively. For a data record x arriving at time t ,0  X  t  X  t ( T ( x )= t ), its density is D ( x, t )=  X  Therefore, the sum over all the data records is: Also, it is clear that: lim
Proposition 3.2 shows that the sum of the density of all data records in the system will never exceed 1 1  X   X  .Since there are N = d i =1 p i grids, the average density of each grid is no more than but approaching 1 N (1  X   X  ) . This obser-vation motivates the following definitions.
 At time t ,foragrid g ,wecallita dense grid if where C m &gt; 1 is a parameter controlling the threshold. For example, we set C m =3. Werequire N&gt;C m since D ( g, t ) cannot exceed 1 1  X   X  .
 At time t ,foragrid g ,wecallita sparse grid if where 0 &lt;C l &lt; 1. For example, we set C l =0 . 8. At time t ,foragrid g ,wecallita transitional grid if
In the multi-dimensional space, we consider connecting neighboring grids, defined below, in order to form clusters.
Definition 3.3. (Neighboring Grids) Consider two den-sity grids g 1 =( j 1 1 ,j 1 2 ,  X  X  X  ,j 1 d )and g 2 =( j there exists k ,1  X  k  X  d , such that: 1) j 1 i = j 2 i ,i =1 ,  X  X  X  ,k  X  1 ,k +1 ,  X  X  X  ,d ;and 2) | j 1 then g 1 and g 2 are neighboring grids in the k th dimension, denoted as g 1  X  g 2 .
 Definition 3.4. (Grid Group) A set of density grids G =( g 1 ,  X  X  X  ,g m ) is a grid group if for any two grids g G , there exist a sequence of grids g k 1 ,  X  X  X  ,g k g g
Definition 3.5. (Inside and Outside Grids) Consider a grid group G and a grid g  X  G , suppose g =( j 1 ,  X  X  X  ,j if g has neighboring grids in every dimension i =1 ,  X  X  X  ,d , then g is an inside grid in G .Otherwise g is an outside grid in G .

Now we are ready to define how to form clusters based on the density of grids.

Definition 3.6. (Grid Cluster) Let G =( g 1 ,  X  X  X  ,g m ) be a grid group, if every inside grid of G is a dense grid and every outside grid is either a dense grid or a transitional grid, then G is a grid cluster.

Intuitively, a grid cluster is a connected grid group which has higher density than the surrounding grids. Note that we always try to merge clusters whenever possible, so the resulting clusters are surrounded by sparse grids.
We now describe in detail the key components of D-Stream outline in Figure 1. As we have discuss in the last section, for each new data record x ,wemapittoagrid g and use (5) to update the density of g (Lines 5-8 of Figure 1). We then periodically (every gap time steps) form clusters and remove sporadic grids. In the following, we describe our strategies for determining gap , managing the list of active grids, and generating clusters.
To mine the dynamic characteristics of data streams, our density grid scheme developed in Section 3 gradually reduces the density of each data reco rd and grid. A dense grid may degenerate to a transitional or sparse grid if it does not receive no new data for a long time. On the other hand, a sparse grid can be upgraded to a transitional or dense grid after it receives some new data records. Therefore, after a period of time, the density of each grid should be inspected and the clusters adjusted.

A key decision is the length of the time interval for grid inspection. It is interesting to note that the value of the time interval gap cannot be too large or too small. If gap is too large, dynamical changes of data streams will not be adequately recognized. If gap is too small, it will result in frequent computation by the offline component and increase the workload. When such computation load is too heavy, the processing speed of the offline component may not match the speed of the input data stream.

We propose the following strategy to determine the suit-able value of gap . We consider the minimum time needed for a dense grid to degenerate to a sparse grid as well as the minimum time needed for a sparse grid to become a dense grid. Then we set gap to be minimum of these two mini-mum times in order to ensure that the inspection is frequent enough to detect the density changes of any grid.
Proposition 4.1. For any dense grid g , the minimum time needed for g to become a sparse g rid from being a dense grid is Proof. According to (8), if at time t ,agrid g is a dense grid, then we have: Suppose after  X  t time, g becomes a sparse grid, then we have:
On the other hand, let E ( g, t )bethesetofdatarecords in g at time t ,wehave E ( g, t )  X  E ( g, t +  X  t ) and: Combining (13) and (14) we get: Combining (12) and (15) we get: which yields:
Proposition 4.2. For any sparse grid g , the minimum time needed for g to become a dense grid from being a sparse grid is Proof. According to (9), if at time t ,agrid g is a sparse grid, then we have: Suppose after  X  t time, g becomes a dense grid, then we have: We also know that: E ( g, t +  X  t ) can be divided into those points in E ( g, t )and those come after t . The least time for a sparse grid g to become dense is achieved when all the new data records are mapped to g . In this case, there is a new data record mapped to g for any of the time steps from t +1 until t +  X  t .The sum of the density of all these new records at time t +  X  Now we plug (20) and (19) into (22) to obtain: Solving (23) yields: which results in: Note N  X  C m &gt; 0since C m &lt;N according to (8).
Based on the two propositions above, we choose gap to be small enough so that any change of a grid from dense to sparse or from sparse to dense can be recognized. Thus, in D-Stream we set:
A serious challenge for the density grid scheme is the large number of grids, especially for high-dimensional data. For example, if each dimension is divided into 20 regions, there will be 20 d possible grids.

A key observation is that most of the grids in the space are empty or receive data very infrequently. In our implementa-tion, we allocate memory to store the characteristic vectors for those grids that are not empty, which form a very small subset in the grid space. Unfortunately, in practice, this is still not efficient enough due to the appearance of out-lier data that are made from errors, which lead to continual increase of non-empty grids that will be processed during clustering. We call such grids sporadic grids since they contain very few data. Since a data stream flows in by mas-sive volume in high speed and it could run for a very long time, sporadic grids accumulate and their number can be-come exceedingly large, causing the system to operate more and more slowly. Therefore, it is imperative to detect and remove such sporadic grids periodically. This is done in Line 13 of the D-Stream algorithm in Figure 1.
 Sparse grid with D  X  D l are candidates for sporadic grids. However, there are two reasons for the density of a grid to be less than D l . The first cause is that it has received very few data, while the second cause is that the grid has previ-ously received many data but the density is reduced by the effect of decay factor. Only the grids in the former case are true sporadic grids that we aim to remove. The sparse grids in the latter case should not be removed since they contain many data records and are often upgraded to transitional or dense grids. We have found through extensive experimen-tation that wrongly removing these grids in the latter case can significantly deteriorate the clustering quality.
We define a density threshold function to differentiate thesetwoclassesofsparsegrids.

Definition 4.1. (Density Threshold Function) Sup-pose the last update time of a grid g is t g ,thenattime t ( t&gt;t g ), the density threshold function is
Proposition 4.3. There are the following properties of the function  X  ( t g ,t ). (1) If t 1  X  t 2  X  t 3 ,then (2) If t 1  X  t 2 ,then  X  ( t 1 ,t )  X   X  ( t 2 ,t ) for any t&gt;t Proof. (1) We see that: (2) Let  X  t = t 2  X  t 1 ,wehave
We use  X  ( t g ,t ) to detect sporadic grids from all sparse grids. In the periodic inspection in Line 13 of Figure 1, at time t , we judge that a sparse grid is a sporadic grid if: (S1) D ( g, t ) &lt; X  ( t g ,t ); and (S2) t  X  (1 +  X  ) t m if g has been delete before (at time t Note that t m and t g are stored in the characteristic vector.
In D-Stream, we maintain a grid list which includes the grids that are under consideration for clustering analysis. The grid list is implemented as a hash table using doubly-linked lists to resolve collision. The hash table allows for fast lookup, update, and deletion. The key of the hash table are the grid coordinates, while the associated data for each grid entry is the characteristic vector.

We use the following rules to delete sporadic grids from grid list . (D1) During the periodic inspection in Line 13 of Figure 1, (D2) In the next periodic inspection, if a grid g marked as
It should be noted that once a sporadic grid is deleted, its density is in effect reset to zero since its characteristic vector is deleted. A deleted grid may be added back to grid list if there are new data records mapped to it later, but its previ-ous records are discarded and its density restarts from zero. Such a dynamic mechanism maintains a moderate size of the grids in memory, saves computing time for clustering, and prevents infinite accumulation of sporadic grids in memory.
Although deleting sporadic grids is critical for the effi-cient performance of D-Stream, an important issue for the correctness of this scheme is whether the deletions affect the clustering results. In particular, since a sporadic grid may receive data later and become a transitional or dense grid, we need to know if it is possible that the deletion prevents this grid from being correctly labelled as a transitional or dense grid. We have designed the density threshold function  X  ( t g ,t ) and the deletion rules in such a way that a transi-tional or dense grid will not be falsely deleted due to the removal of sporadic grids.

Consider a grid g , whose density at time t is D ( g, t ). Sup-pose that it has been deleted several times before t (the density is reset to zero each time) because its density is less than the density threshold function at various times. Sup-pose these density values are not cleared and suppose all data are kept, the density of grid g would be D a ( g, t ). We call D a ( g, t )the complete density function of the grid g .
Now we present several strong theoretical properties of the  X  ( t g ,t ) which ensure the proper functioning of the D-Stream system. We will show that, if a grid can later become a transitional or dense grid, deleting it as a sporadic grid will not affect its later upgrades.

Thefirstquestionweinvestigateis,ifagrid g is detected as a sporadic grid, is it possible that g can be non-sporadic if it has not been previously deleted from grid list ?Itis answered in the following result.

Proposition 4.4. Suppose the last time a grid g is deleted as a sporadic grid is t m and the last time g receives a data record is t g . If at current time t ,wehave D ( g, t ) &lt; X  ( t then we also have D a ( g, t ) &lt; X  (0 ,t ) &lt;D l . Proof. Suppose the grid g has been previously deleted for the periods of (0 ,t 1 ), ( t 1 +1 ,t 2 ),  X  X  X  ,( t m  X  1 the density value D ( g, t i ) ,i =1 ..m satisfies (let t Thus, if all these previous data are not deleted, the complete density function satisfies:
Since t g  X  t m + 1, by property (2) in Proposition 4.3, we know
D a ( g, t ) &lt; The last equalities are based on successive applications of property (1) in Proposition 4.3.

Proposition 4.4 is important since it shows that deleting a sporadic grid will not cause transitional or dense grid be falsely deleted. It shows that, if g is deleted as a sporadic deletions have not occured, it is still sporadic and cannot be a transitional or dense grid since D a ( g, t ) &lt;D l .
Proposition 4.5. Suppose the density of a grid g at time t is D ( g, t ), and g receives no data from t +1 to t + gap , then there exist t 0 &gt; 0and t 1 &gt; 0 such that: (a) If D ( g, t ) &lt;D l ,then D a ( g, t + gap ) &lt;D l (b) If D ( g, t ) &lt;D m ,then D a ( g, t + gap ) &lt;D m Proof. We prove (a). (b) can be proved similarly. Suppose the grid g has been previously deleted for the periods of (0 ,t 1 ), ( t 1 +1 ,t 2 ),  X  X  X  ,( t m  X  1 +1 ,t m ), then:
D a ( g, t + gap )= Since we assume that g receives no data from t +1 to t + gap , (according to (S2)) &lt; X  (0 ,t m )  X   X t/ (1+  X  )  X  gap + D In order to ensure D a ( g, t + gap ) &lt;D l ,werequire: Thus, (a) is true for t 0 satisfying:
Proposition 4.5 is a key result showing that (S1), (S2), (D1) and (D2) work together correctly. It implies that, as time extends for long enough, we will never delete a potential transitional or dense grid due to the previous removals of data. If a grid is sparse ( resp. not dense), then when it is deleted, it must be sparse ( resp. not dense) even considering those deleted data. Note that D a ( g, t + gap ) is the density of the grid upon deletion assuming no previous deletion has ever occurred. The result shows that, after an initial phase, deleting sporadic grids does not affect the clustering results.
We describe the algorithms for generating the initial clus-ter and for adjusting the clusters every gap steps. The pro-cedure initial clustering (used in Line 10 of Figure 1) is il-lustrated in Figure 3. The procedure adjust clustering (used in Line 14 of Figure 1) is illustrated in Figure 4. They first update the density of all active grids to the current time. Once the density of grids are determined at the given time, the clustering procedure is similar to the standard method used by density-based clustering.

It should be noted that, during the computation, when-ever we update grids or find neighboring grids, we only con-sider those grids that are maintained in grid list .There-fore, although the number of possible grids is huge for high-dimensional data, most empty or infrequent grids are dis-carded, which saves computing time and makes our algo-rithm very fast without deteriorating clustering quality.
We evaluate the quality and efficiency of D-Stream and compare it with CluStream [1]. All of our experiments are conducted on a PC wi th 1.7GHz CPU and 256M memory. We have implemented D-Stream in VC++ 6.0 with a Matlab Figure 4: The procedure for dynamically adjusting clusters. graphical interface. In all experiments, we use C m =3 . 0, C l =0 . 8,  X  =0 . 998, and  X  =0 . 3.

We use two testing sets. The first testing set is a real data set used by the KDD CUP-99. It contains network in-trusion detection stream data collected by the MIT Lincoln laboratory [1]. This data set contains a total of five clus-ters and each connection record contains 42 attributes. As in [1], all the 34 continuous attributes are used for cluster-ing. In addition, we also use some synthetic data sets to test the scalability of D-Stream. The synthetic data sets have a varying base size from 30K to 85K, the number of clusters is set to 4, and the number of dimensions is in the range of 2 to 40. In the experiments below, we normalize all the at-tributes of the data sets to [0, 1]. Each dimension is evenly partitioned into multiple segments, each with length len .
We find that the sequence order of data stream can make great effect on the clustering results. In order to validate the effectiveness of D-Stream, we generate the synthetic data sets according to two different orders.
First, we randomly generate 30K 2-dimensional data set in 4 clusters, including 5K outlier data that are scattered in the space. The distribution of the original data set is shown in Figure 5. These clusters have nonconvex shapes and some are interwoven. We generate the data sequentially at each time step. At each time, any data point that has not been generated is equally likely to be picked as the new data record. Therefore, data p oints from different clusters and those outliers alternately appear in the data stream. The final test result by D-Stream is shown in Figure 6. we set len =0 . 05. From Figure 6, we can see that the algo-rithm can discover the four clusters without user supply on the number of clusters. It is much more effective than the k -means algorithm used by CluStream since k -means will fail on such data sets with many outliers. We can also see that our scheme for detecting sporadic grids can effectively remove most outliers.
 Figure 5: Original distribution of the 30K data.
 Figure 6: Final clustering results on the 30K data.
In the second test, we aim to show that D-Stream can capture the dynamic evolution of data clusters and can re-move real outlier data during such an adaptive process. To this end, we order the four classes and generate them se-quentially one by one. In this test, we generate 85K data points including 10K random outlier data. The data distri-bution is shown in Figure 7. The speed of the data stream is 1K/second, which means that there are 1K input data points coming evenly in one second and the whole stream is processed in 85 seconds. We check the clustering results at three different times, including t 1 = 25, t 2 = 55, and t = 85. The clustering results are shown from Figure 8 to 10. It clearly illustrates that D-Stream can adapt timely to the dynamic evolution of stream data and is immune to the outliers.
 Figure 7: Original distribution of the 85K data.
 We test D-Stream on the synthetic data set and KDD CUP-99 data set described above under different grid gran-ularity. The correct rates of c lustering results at different times are shown in Figure 11 and 12. In the figures, len indicates the size of each partitioned segment in the nor-malized dimensions. For example, when len =0 . 02, there are 50 segments in each dimension. From Figure 11, the average correct rates on the synthetic data set by D-Stream is above 96.5%. From Figure 12, the average correct rate on KDD CUP-99 is above 92.5%.

We also compare the qualities of the clustering results by D-Stream and those by CluStream. Due to the non-convexity of the synthetic data sets, CluStream can not get a correct result. Thus, its quality can not be compared to that of D-Stream. Therefore, we only compare the sum of squared distance (SSQ) of the two algorithms on the net-work intrusion data from KDD CUP-99. Figure 13 shows the results. We can see that the average SSQ values of D-Stream at various times are always less than those of CluStream, which implies that data in each of the cluster obtained by D-Stream are much more similar than that ob-tained by CluStream.
 Figure 11: Correct rates of D-Stream on synthetic data. We test and compare the clustering speed of D-Stream and CluStream. First, both algorithms are tested on the KDD Figure 12: Correct rates of D-Stream on KDD CUP-99 data.
 Figure 13: Comparison of D-Stream and CluStream on KDD CUP-99 data.
 CUP-99 data with different sizes. The results are shown in Figure 14. We can see that CluStream requires four to six times more clustering time than D-Stream. D-Stream is efficient since it only puts each n ew data record to the corre-sponding grid by the online component without computing distances as CluStream does. Furthermore, the dynamic de-tection and deletion of sporadic grids save tremendous time. It can also be seen that D-Stream has better scalability since its clustering time grows slower with an increasing data size.
Next, both algorithms are tested on the KDD CUP-99 data with different dimensionality. We set the size of data set as 100K and vary the dimensionality from 2 to 40. We list the time costs under different dimensionality by the two algorithms in Figure 15. D-Stream is 3.5 to 11 times faster than CluStream and scales better. For example, when the dimensionality is increased from 2 to 40, the time of D-Stream only increases by 15 seconds while the time of CluS-tream increases by 40 seconds.
In this paper, we propose D-Stream, a new framework for clustering stream data. The algorithm maps each input data into a grid, computes the density of each grid, and clusters Figure 14: Efficiency comparison with varying sizes of data sets.
 Figure 15: Efficiency comparison with varying di-mensionality. the grids using a density-based algorithm. In contrast to previous algorithms based on k -means, the proposed algo-rithm can find clusters of arbitrary shapes. The algorithm also proposes a density decaying scheme that can effectively adjust the clusters in real time and capture the evolving be-haviors of the data stream. Further, a sophisticated and theoretically sound technique is developed to detect and re-move the sporadic grids in order to dramatically improve the space and time efficiency without affecting the cluster-ing results. The technique makes high-speed data stream clustering feasible without degrading the clustering quality.
This work is supported by Microsoft Research New Fac-ulty Fellowship and National Natural Science Foundation of China Grant 60673060. [1] C. C. Aggarwal, J. Han, J. Wang, and P. S. Yu. A [2] B. Babcock, M. Datar, R. Motwani, and [3] S. Bandyopadhyay, C. Giannella, U. Maulik, [4] D. Barbar  X  a. Requirements for clustering data streams. [5] J. Beringer and E. H  X  ullermeier. Online-clustering of [6] B.R. Dai, J.W. Huang, M.Y. Yeh, and M.S. Chen. [7] M. Garofalakis, J. Gehrke, and R. Rastogi. Querying [8] L. Golab and M. T.  X  Ozsu. Issues in Data Stream [9] S. Guha, A. Meyerson, N. Mishra, R. Motwani, and [10] S. Guha, N. Mishra, R. Motwani, and L. O X  X allaghan. [11] O. Nasraoui, C. Rojas, and C. Cardona. A framework [12] L. O X  X allaghan, N. Mishra, A. Meyerson, S. Guha, [13] S. Oh, J. Kang, Y. Byun, G. Park, and S. Byun. [14] J. Sander, M. Ester, H. Kriegel, and X. Xu. [15] S. Subramaniam, T. Palpanas, D. Papadopoulos, [16] H. Sun, G. Yu, Y. Bao, F. Zhao, and D. Wang. S-tree: [17] Z. Wang, B. Wang, C. Zhou, , and X. Xu. Clustering
