 We present a new solution to the  X  X cological inference X  prob-lem, of learning individual-level associations from aggregate data. This problem has a long history and has attracted much attention, debate, claims that it is unsolvable, and purported solutions. Unlike other ecological inference tech-niques, our method makes use of unlabeled individual-level data by embedding the distribution over these predictors into a vector in Hilbert space. Our approach relies on re-cent learning theory results for distribution regression, us-ing kernel embeddings of distributions. Our novel approach to distribution regression exploits the connection between Gaussian process regression and kernel ridge regression, giv-ing us a coherent, Bayesian approach to learning and infer-ence and a convenient way to include prior information in the form of a spatial covariance function. Our approach is highly scalable as it relies on FastFood, a randomized ex-plicit feature representation for kernel embeddings. We ap-ply our approach to the challenging political science prob-lem of modeling the voting behavior of demographic groups based on aggregate voting data. We consider the 2012 US Presidential election, and ask: what was the probability that members of various demographic groups supported Barack Obama, and how did this vary spatially across the country? Our results match standard survey-based exit polling data for the small number of states for which it is available, and serve to fill in the large gaps in this data, at a much higher degree of granularity.
 Machine learning; supervised learning; kernel methods; Gaus-sian processes; distribution regression
The name ecological inference refers to the idea of ecolog-ical correlations [28], that is correlations between variables observed for a group of individuals, as opposed to individ-ual correlations, where the individuals are the unit of anal-ysis. The ecological inference problem has much in common with the  X  X odifiable areal unit problem X  [20] and Simpson X  X  paradox. Simply put, it is the problem of inferring individ-ual correlations from ecological correlations. This challenge arises in computational advertising, healthcare data, opin-ion survey data, and population health data, because in each case for privacy or cost reasons, we are missing individual-level data, we have access to aggregate-level data, and we want to make individual-level predictions. One way to un-derstand the reason it is called a  X  X roblem X  is to consider a two-by-two contingency table, with unknown entries inside the table, and known marginals. As shown in the contin-gency table below, we might know that a certain electoral district X  X  voting population is 43% men and 57% women and that in the last election, the outcome was 63% in favor of the Democratic candidate and 37% in favor of the Repub-lican candidate. These percentages correspond to the num-bers of individuals shown below: Is it possible to infer the joint and thus conditional probabilities, for example can we ask, what was the Democratic candidate X  X  vote share among women voters? It is clear that only very loose bounds can be placed on these probabilities without any more information. Based on the fact that rows and columns must sum to their marginals, we know, e.g., that the number of Democrats who are men is between 0 and 1,500. These types of determin-istic bounds have been around since the 1950 X  X , under the name the method of bounds [4].

What if we are given a set of electoral districts, where for each we know the marginals of the two-by-two contingency table, but none of the inner entries? Then, thinking statis-tically, we might be tempted to run a regression, predict-ing the electoral outcomes based on the gender breakdowns of the districts. But this approach, formalized as Good-man X  X  method [8] a few years after the method of bounds was proposed, can easily lead us astray X  X here is not even a guarantee that outcomes be bounded between 0 and 1, and it ignores potentially useful information provided by deter-ministic bounds.

We review related work in Section 2 and provide the nec-essary background on kernel embeddings of distributions, distribution regression, and GP regression in Section 3. We formalize the ecological inference problem in Section 4 and propose our method in Section 5. We apply it to the case of the 2012 US presidential election in Section 6, comparing our results to survey-based exit polls.
The ecological inference problem has a long history of solu-tions, counter-solutions, and it is often taught with a note of grave caution and stark warnings that ecological inference is to be avoided at all costs, usually in favor of individual-level surveys. As with Simpson X  X  paradox, it should come as no surprise that correlations at one level of aggregation can and do flip signs at other levels of aggregation. But abandoning all attempts at ecological inference in favor of surveys is not feasible or appropriate in many circumstances X  X elevant re-spondents are no longer alive to answer historical questions of interest; subjects are reluctant to answer questions about sensitive topics like drug usage or cheating X  X eaning social scientists have been hard-pressed and even discouraged from studying many interesting and important questions. Eco-logical inference problems appear in demography, sociology, geography, and political science, and X  X s discussed in [13] X  landmark legislation in the US such as the Voting Rights Act requires a solution to the ecological inference problem to understand racial voting patterns 1 .

This problem has attracted a variety of approaches over the years as summarized in [13], which also proposes a Bayesian statistical modeling framework incorporating the method of bounds (thus uniting the deterministic and probabilistic ap-proaches). [13] sparked a renewed interest in ecological in-ference, much of which is summarized in [14]. A parametric Bayesian approach to this setting was proposed in [12] and a semiparametric approach was proposed in [23].
 Our method differs from existing methods in fours ways. First, it uses more information than is typically considered in a standard ecological regression setting: we assume that we have access to representative unlabeled individual-level data. In the voting example, this means having a sample of individual-level census records ( X  X icrodata X ) about each electoral district. Second, our method incorporates spatial variation. Spatial data is a common feature of ecological regressions (which, after all, usually have much to do with geography) but it is only very recently that ecological in-ference methods have begun to address spatial variation ex-plicitly [14]. Third, while our method may be applied to the classic ecological inference problem of inferring individual level correlations from aggregate data, we propose that it is most well-suited to a related ecological problem, common in political science: inferring the unobserved behavior of sub-groups based on the aggregate behavior of groups of which they are part. For our application, this means inferring the voting behavior of men and women separately by electoral district, given aggregate voting information by district. Fi-nally, our work is nonparametric. Kernel embeddings are used to capture all moments of the probability distribution over covariates, and Gaussian process regression is used to non-parametrically model the dependence between predic-tors and labels.

A related line of work, termed  X  X earning from label propor-tions X  by some authors [24, 15, 30, 21], has the individual-level goal in mind, and aims to build a classifier for individ-ual instances based only on group level label proportions. While in principle, this approach could be used in our set-
Long-standing solutions have proved quite inadequate: in one court case involving the Voting Rights Act, a qualified expert testified, based on Goodman X  X  method, that the per-centage of blacks who were registered to vote in a certain electoral district exceeded 100% [13]. This evidently false claim was apparently made earnestly. ting, since we are only interested in subgroup level predic-tions the extra task of estimating individual level predictions is probably not worth the effort considering we are working with n = 10 million individuals.

Our method is based on recent advances in distribution re-gression [6, 33], which we generalize to address the ecological inference case. Previous work on distribution regression has relied on kernel ridge regression, but we use Gaussian pro-cess (GP) regression instead, thus enabling us to incorporate spatial variation, learn kernel hyperparameters, and provide posterior uncertainty intervals, all in a fully Bayesian set-ting. For scalability (our experiments use n = 10 million individuals), we use a randomized explicit feature represen-tation ( X  X astFood X ) [16] rather than the kernel trick.
In this section we review kernel embeddings for proba-bility distributions, distribution regression, FastFood, and Gaussian process (GP) regression.
Kernel embeddings of distributions, e.g. [31, 32, 5], are a powerful class of reproducing kernel Hilbert space (RKHS) techniques that map joint, marginal and conditional proba-bility distributions to vectors in a high (or infinite) dimen-sional feature space. Let  X  : R n  X  H . It has been shown that if a kernel map  X  is universal/characteristic (e.g. a Gaussian RBF kernel), then for iid samples x  X  X , the mean embedding in feature space, denoted: completely characterizes the distribution in the sense that any two distributions with a difference in any moment will be mapped to a different point in the Hilbert space. This result has been used in a variety of kernel-based statistical tests, including tests of independence and two-sample tests [10]. It is a key feature of our method, because it will allow us to link aggregate labels to individual-level data without throwing out any information.

In this work, we use the simple empirical mean estimator for the kernel mean: It is shown in Smola et al. [31] that this plug-in estimator is a consistent, and it converges to  X  X with rate O ( R n ( H ) + 1 /  X  RKHS. As long as R n ( H ) = O ( n  X  1 / 2 ) we have the (optimal) parametric rate. Recent work has focused on improving this estimator using James-Stein shrinkage [17].
In this section, we formalize distribution regression, the task of learning a classifier or a regression function that maps probability distributions to labels. The problem is funda-mentally challenging because we only observe the probability distributions through groups of samples from these distribu-tions. Specifically, our dataset is structured as follows: where group i has a single real-valued label y i and N i indi-vidual observations (e.g. demographic covariates for N i in-dividuals) denoted x j i  X  R d .
To admit a theoretical analysis, it is assumed that the probability distributions themselves are drawn randomly from some unknown meta distribution of probability distributions. The intuition behind why distribution regression is possible is that if each group of samples are iid draws from a distribu-tion which is itself an iid drawn from the meta distribution, then we will be able to learn.

Recently, this  X  X wo-stage sampled X  structure was analyzed, showing that a ridge regression estimator is consistent [33] with polynomial rate of convergence for almost any meta-distribution of distributions that are sufficiently smooth. The basic approach is as follows: use the kernel mean es-timator of Eq. (2) for each group separately to estimate: Next, use kernel ridge regression [29] to learn a function f : where the objective is to minimize the L 2 loss subject to a  X  X idge X  complexity penalty weighted by a positive constant  X  : In [33] a variety of kernels for f corresponding to the Hilbert space H f are considered. We follow the simplest choice of the linear kernel k ( the fact that we are already working in Hilbert space over the  X  i . Following the standard derivation of kernel ridge re-gression [29], we can find the function f in closed form for a new test group  X   X  : where k  X  = [  X  practice, it is hard to know whether the conditions under which the proofs in these papers hold are met. As a partial remedy, our Bayesian approach allows us to quantify the degree of uncertainty in our posterior predictions. Also, as shown in the experiments, a useful diagnostic is to measure the distance between training and test distributions.
Naively implementing distribution regression using the ker-nel trick is not scalable in the setting we consider: to com-pute just one entry in K requires computing K ab =  X  we assume for simplicity N i = N,  X  i ) so computing K is O ( n 2 N 2 ). In our application, N  X  10 4 , so we need a much more scalable approach. Since we ultimately only need to work with the mean embeddings  X  i rather than the indi-vidual observations x j i , an explicit feature representation, even if it is very high-dimensional, will drastically reduce our computational costs.
 We use an approximate kernel transformation called Fast-Food [16], which finds a d -dimensional approximation  X   X  ( x )  X  R d of  X  ( x ) for every x . Here  X  can be any radial basis func-tion (RBF) kernel. Take Gaussian RBF kernel as an ex-ample, FastFood boils down to the following transformation due to Rahimi and Recht [25]: where i is the imaginary unit of a complex number and V is a appropriately scaled p  X  d Gaussian random matrix with p &gt; d . FastFood allows us to approximately com-pute V x without explicitly construct V . In particular, Fast-Food transformation takes V = [ V T 1 ,V T 2 ,...,V T d p/d e each square d  X  d matrix is given by: Here S,B,G are diagonal random matrices (nonnegative scaling, Rademacher and Gaussian respectively),  X  is a ran-dom permutation, and H is the Walsh-Hadamard matrix. Every single one of these transformation can be computed in almost linear time. The whole transformation  X  ( x ) can be therefore computed in O ( p log d ) time. This is orders of magnitude faster than random kitchen sinks [25] which costs O ( pd ) per transformation or the kernel trick which needs to do an O ( N 3 ) inversion of a dense N  X  N kernel matrix.
It is shown in [16, Theorem 6] that for any x,x 0 ,  X   X   X  ( x ) , converges to  X   X  ( x ) , X  ( x 0 )  X  with rate O ( log(2 / X  ) the failure probability. This can be viewed as a Johnson-Lindenstrauss transformation of an infinite dimensional space to a finite dimensional Euclidean space while preserving the angles and distances in the original space. While this is not a uniform convergence bound as with the random features in [25], the exponential tail enables us to simultaneously guarantee an exponential number of kernel evaluations via the union bound. Not surprisingly, it has been empirically shown to be comparable in accuracy and to approximate the kernel transformation for all data points quite well. For simplicity, from here onwards we will overload notation and refer to  X  ( x )  X  R p as our feature mapping which is under-stood to be approximated with FastFood.
In this section, we briefly state the main results we need from Gaussian process regression [26], reviewing the well-known connection between the posterior mean in GP regres-sion and the kernel ridge regression estimator of Eq. (7).
Given observations ( s 1 ,y 1 ) ,... ( s n ,y n ) a Gaussian process prior on a function f where our model is y = f ( s ) + is written: with mean 0 and covariance function k . This implies that for a finite set of locations X = { s 1 ,...,s n } , the distribution of f = [ f ( s 1 ) ,...,f ( s n )] &gt; is multivariate Gaussian: where K ij = k ( s i ,s j ). Notice that we have switched from a function f ( s ) to a vector f . This is because it is only for-mally correct to consider a probability distribution over the finite-dimensional vector f , not over the infinite dimensional function f ( s ). For a formal discussion see [35]. Conditional on the latent variable f , we have a Gaussian observation model: for variance parameter  X  2 which can be thought of as mea-surement error (known as the  X  X ugget X  in geostatistics). For a fixed set of locations X , it is straightforward to sample f from its prior distribution in Eq. (8). Due to conjugacy, we can marginalize out f in closed form to find the distribution: If we wish to make a prediction at a new location s  X  , the standard predictive equations for GP regression [26], derived by conditioning a multivariate Gaussian distribution, tell us: y | s  X  ,X, y  X  X  ( k  X  ( K +  X  2 I )  X  1 y ,k  X  X  X   X  k  X  ( K +  X  where K ij = k ( s i ,s j ) and k  X  = [ k ( s 1 ,s  X  ) ...k ( s k  X  X  X  = k ( s  X  ,s  X  ). Thus we have a way of combining a prior over f , parametrized by k ( s,s 0 ), with observed data to ob-tain a posterior distribution over a new prediction y  X  at a new location s  X  . This is a very powerful method, as it en-ables a fully Bayesian treatment of regression, a coherent approach to kernel learning through the marginal likelihood (for details see [26]), and posterior uncertainty intervals.
We can immediately see the connection between the ker-nel ridge regression estimator in Eq. (7) and the posterior mean of the GP in Eq. (11). (A superficial difference is that in Eq. (7) our predictors are generic locations s i , but this difference will go away in Sec-tion 5 when we propose using GP regression for distribution regression.) The predictive mean of GP regression is ex-actly equal to the kernel ridge regression estimator, with  X  corresponding to  X  . In ridge regression, a larger penalty  X  leads to a smoother fit (equivalently, less overfitting), while in GP regression a larger  X  2 favors a smoother GP poste-rior because it implies more measurement error. For a full discussion of the connections see [2, Sections 6.2.2-6.2.3].
In this section we state the ecological inference problem that we intend to solve. We use the motivating example of inferring Barack Obama X  X  vote share by demographic sub-group (e.g. men versus women) in the 2012 US presidential election, without access to any individual-level labels. Vote totals by electoral precinct are publicly available, and these provide the labels in our problem. Predictors are in the form of demographic covariates about individuals (e.g. from a survey with individual level data like the census). The challenge is that the labels are aggregate, so it is impossi-ble to know which candidate was selected by any particular individual. This explains the terminology:  X  X cological cor-relations X  are correlations between variables which are only available as aggregates at the group level [28]
We use the same notation as in Section 3.2. Let x j i  X  R be a vector of covariates for individual i in region j . Let w i be survey weights dimensional vectors ( k i ,n i ) where k i is the number of votes received by Obama out of n i total votes in region i . Then our dataset is: We will typically have a rich set of covariates available, in addition to the demographic variables we are interested in stratifying on, so the x j i will be high-dimensional vectors denoting gender, age, income, education, etc.

Our task is to learn a function f from a demographic sub-group (which could be everyone) within region i to the prob-ability that this demographic subgroup supported Obama,
Covariates usually come from a survey based on a random sample of individuals. Typically, surveys are reported with survey weights w j i for each individual to correct for oversam-pling and non-response, which must be taken into account for any valid inference (e.g. summary statistics, regression coefficients, standard errors, etc.). i.e. the number of votes this group gave Obama divided by the total number of votes in this group.
In this section we propose our new ecological inference method. Our approach is illustrated in a schematic in Figure 1 and formally stated in Algorithm 1. Figure 1: Illustration of our approach. Labels y ,y 2 and y 3 are available at the group level giving Obama X  X  vote share in regions 1, 2, and 3. Co-variates are available at the individual level giv-ing the demographic characteristics of a sample of individuals in regions 1, 2, and 3. We project the individuals from each group into feature space using a feature map  X  ( x ) and take the mean by group to find high-dimensional vectors  X  1 , X  2 and  X  3 , lem is reduced to supervised learning, where we want to learn a function f :  X   X  y . Once we have learned f we make subgroup predictions for men and women in region 3 by calculating mean embed-dings for the men  X  m 3 = 1 2 (  X  ( x 3 3 ) +  X  ( x 4 3 )) and women  X  f (  X  m 3 ) and f (  X  w 3 ) . For a more rigorous description of our algorithm see Algorithm 1.

Recall the two-stage distribution regression approach in-troduced in Section 3.2. Our method has a similar approach. To begin, we use FastFood as introduced in Section 3.3 with an RBF kernel to produce an explicit feature map  X  and calculate the mean embeddings 3 , one for each region i , of Eq. (4) with survey weights:
Distribution regression with explicit random features was previously considered in Oliva et al. [19] using Rahimi and Recht [25] to speed up an earlier distribution regression method based on kernel density estimation [22]. This ap-proach has comparable statistical guarantees to distribution regression using RKHS-mean embeddings but inferior em-pirical performance [33]. As far as we are aware, using Fast-Food kernel mean embeddings for distribution regression is a novel approach. Algorithm 1 Ecological inference algorithm Input: { ( x j 1 ,w j 1 ) } N 1 j =1 ,s 1 ,y 1 ,..., { ( x 1: for i = 1 ...n do 2: Calculate 3: Calculate  X  m i using Eq. (17) with FastFood. 4: end for 5: Learn hyperparameters  X   X  = (  X  2 x , X  2 s ,` ) of the GP model 6: Make posterior predictions using  X   X  at locations Output: Posterior means and variances for y m 1 ,...,y m n Next, instead of kernel ridge regression, we use GP regres-sion. Recall that unlike in distribution regression our labels y are given by vote counts ( k i ,n i ). We use a Binomial like-lihood as the observation model in GP regression (this is sometimes known as a logistic Gaussian process [27]). We transform each component of the latent real-valued vector f of Section 3.4 by the logistic link function  X  ( f ) = 1 we replace Eq. (9) with the following: where we use the formulation for the Binomial distribu-tion of n i trials and probability of success  X  ( f ( x is the generalized linear model (GLM) specification for bi-nary data, combining a Binomial distribution with logistic link function [3, Ch. 7].

The predictors in our GP are the mean embeddings .
 We also include spatial information in the form of 2-dimensional spatial coordinates s i giving the centroid of region i . Putting these predictors together we adopt an additive covariance structure: Where we have used a linear kernel between mean embed-dings weighted by a variance parameter  X  2 x . Since the mean embeddings are already in feature space using the FastFood approximation to the RBF kernel, we are approximately us-ing the RBF kernel. For the spatial coordinates we use the Mat  X ern covariance function which is a popular choice in spa-tial statistics [11], with  X  = 3 / 2, length-scale ` and variance parameter  X  2 s : k ( s,s 0 ) =  X  2 s 1 +
By adding together the linear kernel between mean em-beddings and the spatial covariance function, we allow for a smoothly varying surface over space and demographics. The intuition is that this additive covariance encourages predic-tions for regions which are nearby in space and have simi-lar demographic compositions to be similar; predictions for regions which are far away or have different demographics are allowed to be less similar. GP regression with a spa-tial covariance function is equivalent to the spatial statistics technique of kriging X  X e are effectively smoothly interpolat-ing y values over a very high dimensional space of predic-tors. Another way to think about additivity is that we are accounting for a spatially autocorrelated error structure in the predictions we get from covariates alone. (We also con-sidered a multiplicative structure, which had slightly worse performance.)
Eq.s (14)-(15) complete our hierarchical model specifica-tion. For non-Gaussian observation models like Eq. (14), the posterior prediction in Eq. (11) is no longer available in closed form due to non-conjugacy. We follow the stan-dard approach for GP classification and logistic Gaussian processes and use the Laplace approximation [36, 27]. The Laplace approximation gives an approximate posterior dis-tribution for f , from which we can calculate a posterior dis-tribution over the k i of Eq. (14) as explained in detail in [26, Section 3.4.2]. The Laplace approximation also allows us to calculate the marginal likelihood, which is the probability of the observed data, integrating out f . To learn  X  and ` , we use gradient ascent to maximize the log marginal likelihood.

Once we have learned the best set of hyperparameters for our model we can make predictions for any demographic subgroup of interest. To predict the fraction of men who voted for Obama, we create new mean embedding vectors by gender and region, modifying Eq. (13): where j m are the indices of the observations of men in region i and c  X  m i is the mean embedding of the covariates for the men in region i . We then make posterior predictions using the Laplace approximation as above at these new gender-region predictors. Notice that for a new  X   X  this requires calculating k  X  = [ k 1  X  ,k 2  X  ,...,k n  X  ] of Eq. (11) where k  X   X  will be similar to existing predictions in regions with similar covariates and they will be similar to existing predictions at the same (and nearby) locations.

Our algorithm is stated in Algorithm 1. We now ana-lyze its complexity. Lines 2 X 3 are calculated by streaming through the data for individuals. For each individual, cal-culating the FastFood feature transformation  X  ( x j i O ( p log d ) where x j i  X  R d and  X  ( x j i )  X  R p . To save memory, there X  X  no need to store each  X  ( x j i ). We simply update the weighted average demographic subgroup considered in line 3 is simply a subset of the observations calculated in line 2, so there is no added cost to calculate the  X  m i or indeed a set of  X  m 1 i ,..., X  q different demographic subgroups of interest. Overall, if we have N individuals the for loop takes time O ( Np log d ). Usually p N and d N so this is practically linear and trivially parallelizable.

On line 5 to learn the hyperparameters in the GP re-gression requires calculations involving the covariance ma-trix K  X  R n  X  n . Each entry in K requires computing a dot product  X  the Mat  X ern kernel for the spatial locations, which is a fast arithmetic calculation. Once we have K , the Laplace ap-proximation is usually implemented with Cholesky decom-positions for numerical reasons. The runtime of computing the marginal likelihood and relevant gradients is O ( n 3 and gradient ascent usually takes less than a hundred steps to converge. Posterior predictions on line 6 require calcu-lating k  X   X  R 1  X  n for each  X  m i so this is O ( n 2 the Cholesky decompositions above means predictions can be made in O ( n 2 ). GP regression requires O ( n 2 ) storage. Overall, we expect n N , so our algorithm is practically O ( N ), with little extra computational cost arising from the GP regression as compared to the work of streaming through all the observations. The N observations do not need to be stored in memory, so the overall memory complexity is only O ( n 2 ).
In this section, we describe our experimental evaluation, using data from the 2012 US Presidential election, and com-pare our results to survey-based exit polls, which are only available for the 18 states for which large enough samples were obtained. Our method enables us to fill in the full pic-ture, with much finer-grained spatial estimation and results for a much richer variety of demographic variables. This demonstration shows the applicability of our new method to a large body of political science literature (see, e.g. [7]) on voting patterns by demographics and geography. Because voting behavior is unobservable and due to the ecological inference problem, previous work has been mostly based on exit polls or opinion polls.

We obtained vote totals for the 2012 US Presidential Elec-tion at the county level 4 . Most voters chose to either re-elect President Barack Obama or vote for the Republican party candidate, Mitt Romney. A small fraction of voters ( &lt; 2% across the country) chose a third party candidate. Separately, we obtained data from the US Census, specifi-cally the 2006-2010 American Community Survey X  X  Public Use Microdata Sample (PUMS). The American Community Survey is an ongoing survey that supplements the decen-nial US census and provides demographically representatives individual-level observations. PUMS data is coded by pub-lic use microdata areas (PUMAs), contiguous geographic re-gions of at least 100,000 people, nested within states. We used the 5-year PUMS file (rather than a 1-year or 3-year sample) because it contains a larger sample and thus there is less censoring for privacy reasons. To merge the PUMS data with the 2012 election results, we created a mapping between counties and PUMAs 5 , merging individual-level census data and aggregating vote totals as necessary to create larger geo-graphic regions for which the census data and electoral data coincided. The mapping between PUMAs and counties is many-to-many, so we were effectively finding the connected components. Since counties and PUMAs do not cross state borders, none of the geographic regions we created cross state borders. An example is shown in Figure 2.

In total, we ended up with 837 geographic regions rang-ing from Orleans Parish in New Orleans, which voted 91% for Barack Obama to Davis County, a suburb of Salt Lake City, Utah which voted 84% for Mitt Romney. For the cen-sus data, we excluded individuals under the age of 18 (vot-ing age in the US) and non-citizens (only citizens can vote in presidential elections). There were a total of 10,787,907 individual-level observations, or in other words, almost 11 million people included in the survey. The mean number of people per geographic region was 12,812 with standard deviation 21,939.

There were 223 variables in the census data, including both categorical variables such as race, occupation, and ed-ucational attainment and real valued variables such as in-https://github.com/huffpostdata/election-2012-results using the PUMA 2000 codes and the tool at http://mcdc.missouri.edu/websas/geocorr12.html Figure 2: Election outcomes were available for the 67 counties in Florida shown in (a). Demo-graphic data from the American Community Sur-vey was available for 127 public use microdata areas (PUMAs) in Florida, which sometimes overlapped parts of multiple counties and sometimes contained multiple counties. We merged counties and PUMAs as described in text to create a set of disjoint regions with the result of 37 electoral regions as shown in (b). come in past 12 months (in dollars) and travel time to work (in minutes). We divided the real-valued variables by their standard deviation to put them all on the same scale. For the categorical variables with D categories, we converted them into D dimensional 0/1 indicator variables, i.e. for the variable  X  X hen last worked X  with categories 1 =  X  X ithin the past 12 months, X  2 =  X 1-5 years ago, X  and 3 =  X  X ver 5 years ago or never worked X  we mapped 1 to [1 0 0] T , 2 to [0 1 0] and 3 to [0 0 1].

Putting together the indicator variables and real-valued variables, we ended up with 3,251 variables total. For every single individual-level observation, we used FastFood with an RBF kernel to generate a 4,096-dimensional feature rep-resentation. Using Eq. (13) we calculated the weighted mean embedding for each region. The result was a set of 837 vec-tors which were 4,096-dimensional.

We treated the vote totals for Obama and Romney as is, discarded the remaining third party votes as the exit polls we use for validation did not report third party votes. Thus for each region, we had a positive integer valued 2-dimensional label giving the number of votes for Obama and the total number of votes.

We focused on the ecological inference problem of pre-dicting Obama X  X  vote share by the following demographic groups: women, men, income  X  US$50,000 per year, income between $50,000 and $100,000 per year, income  X  100,000 per year, ages 18-29, 30-44, 45-64, and 64 plus. For each region, we used the strategy outlined above, restricting our census sample to only those observations matching the sub-group of interest and creating new mean embedding predic-tors as in Eq. (17),  X  subgroup i . We made predictions for each region-demographic pair. Note that we have made our task harder than necessary to demonstrate our method; we could have trained our model using the exit polling data, where available, and we would certainly recommend practitioners use all available data to get the best possible estimates.
All of our models were fit using the GPstuff package with scaled conjugate gradient optimization and the Laplace ap-proximation [34]. Since n N , the time required to fit the GP model and make predictions is much less than the time regression method. required to preprocess the data to create the mean embed-dings at the beginning of Algorithm 1.
We learned the following hyperparameters for our GP:  X  s = 0 . 18, ` = 7 . 92, and  X  2 x = 4 . 56. The  X  2 parameters can be roughly interpreted as the  X  X raction of variance explained X  so the fact that  X  2 x is much larger than  X  2 s means that the demographic covariates encoded in the mean embedding are much more important to the model than the spatial coordi-nates. The length-scale for the Mat  X ern kernel is a little more than half the median distance between locations, which indi-cates that it is performing a reasonable degree of smoothing. We used 10-fold crossvalidation to evaluate our model and ensure that it was not overfitting, an important considera-tion as generalization performance is critical. The root mean squared error of the model was 2.5 and the mean log predic-tive density was -1.9. Predictive density is a useful measure because it takes posterior uncertainty intervals into account. For comparison, predicting the national average of Obama receiving 51.1% of the vote in every location has a root mean squared error of 8.3. As a sensitivity analysis, we also con-sidered a multiplicative model, for which the performance was comparable.

To validate our models, we compared to the 2012 exit polls, conducted by Edison Research for a consortium of news organizations. National results were based on inter-views with voters in 350 randomly chosen precincts, and state results in 18 states were based on interviews in 11 to 50 random precincts. In these interviews, conducted as vot-ers left polling stations, voters were asked who they voted for and a variety of demographic questions about themselves. Bias due to factors such as unrepresentativeness of the sam-pled precincts and inadequate coverage of early or absentee voters could be an issue [1]. The national results had a mar-gin of error (corresponding to a 95% uncertainty interval) of 4 percentage points 6 and the state results had a margin of error of between 4 and 5 percentage points [18]. For com-paring to the 18 state-level exit polls, we aggregated our geographic regions by state, weighting by subgroup popula-tion.

As a preview of our results by gender, income, and age, and to get an idea of the power of our method, Figure 3 shows four maps visualizing Obama X  X  support among women and men. In Figures 3a X 3b, we show the results from the exit polls, at the state level, for only 18 states. In Figures 3c X  3d we fill in the missing picture, providing estimates for 837 different regions. We compare to competing methods below for national-level gender estimates. In the supplementary materials, we consider the non-binary demographic covari-ates age and income and the case of regional-level estimates, which present a difficulties for the competing methods.
Voting by gender is shown in Figure 4, where we compare our results to the exit poll results. The fit is quite good, with correlations equal to 0.96 for men and 0.94 for women. The inference that we are most interested in is the gender
This presumably corresponds to a sample size of only n = 600 individuals, since the usual margin of error reported by news organizations is 1 . 96 q . 5 2 n  X  1 [2] N. Cristianini and J. Shawe-Taylor. An introduction to [3] A. Dobson. An introduction to generalized linear [4] O. B. Duncan and D. B. An alternative to ecological [5] K. Fukumizu, L. Song, and A. Gretton. Kernel bayes X  [6] T. G  X  artner, P. A. Flach, A. Kowalczyk, and A. J. [7] A. Gelman, D. Park, B. Shor, J. Bafumi, and [8] L. A. Goodman. Some alternatives to ecological [9] A. Gretton, A. Smola, J. Huang, M. Schmittfull, [10] A. Gretton, K. M. Borgwardt, M. J. Rasch, [11] M. S. Handcock and M. L. Stein. A bayesian analysis [12] C. Jackson, N. Best, and S. Richardson. Improving [13] G. King. A Solution to the Ecological Inference [14] G. King, M. A. Tanner, and O. Rosen. Ecological [15] H. Kueck and N. de Freitas. Learning about [16] Q. Le, T. Sarlos, and A. Smola. Fastfood: [17] K. Muandet, K. Fukumizu, B. Sriperumbudur, [18] New York Times. President exit polls -election 2012. [19] J. B. Oliva, W. Neiswanger, B. P  X oczos, J. G. [20] S. Openshaw. Ecological fallacies and the analysis of [21] G. Patrini, R. Nock, T. Caetano, and P. Rivera. [22] B. Poczos, A. Singh, A. Rinaldo, and L. Wasserman. [23] R. L. Prentice and L. Sheppard. Aggregate data [24] N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. [25] A. Rahimi and B. Recht. Weighted sums of random [26] C. E. Rasmussen and C. K. Williams. Gaussian [27] J. Riihim  X  aki and A. Vehtari. Laplace approximation [28] W. S. Robinson. Ecological correlations and the [29] C. Saunders, A. Gammerman, and V. Vovk. Ridge [30] D. R. Sheldon and T. G. Dietterich. Collective [31] A. Smola, A. Gretton, L. Song, and B. Sch  X  olkopf. A [32] L. Song, J. Huang, A. Smola, and K. Fukumizu. [33] Z. Szabo, A. Gretton, B. Poczos, and [34] J. Vanhatalo, J. Riihim  X  aki, J. Hartikainen, P. Jyl  X  anki, [35] G. Wahba. Spline models for observational data , [36] C. K. Williams and D. Barber. Bayesian classification
