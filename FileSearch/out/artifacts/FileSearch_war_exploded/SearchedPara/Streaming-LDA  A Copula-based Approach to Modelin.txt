 We propose in this paper two new models for modeling topic and word-topic dependencies between consecutive doc-uments in document streams. The first model is a direct extension of Latent Dirichlet Allocation model ( LDA ) and makes use of a Dirichlet distribution to balance the influ-ence of the LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second exten-sion makes use of copulas, which constitute a generic tools to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copulas, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our exper-iments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal LDA ), both in terms of perplexity and for tracking similar topics in a document stream.
  X  Computing methodologies  X  Latent Dirichlet alloca-tion;  X  Mathematics of computing  X  Bayesian compu-tation; Gibbs sampling; Metropolis-Hastings algorithm; Latent Dirichlet allocation, Copulas, Document Streams, Topic Dependencies The recent proliferation of temporal textual data on the Internet such as Tweets or comments on Youtube has brought new challenges for learning with interdependent data. Thou-gh important progress has been made in some directions [8], popular approaches for most of these tasks are designed to deal with static collections of documents. This is specially the case for latent topic modeling, albeit analyzes of social content have gained much attention in recent years for dif-ferent aspects of daily life, such as latent health-related topic analysis [19] or buzz detection [20].

Although the main goal of probabilistic modeling is to find word topics, an equally interesting objective is to examine topic evolutions and transitions. The seminal work of [4] proposed to model the dynamic evolution of topics by first grouping documents into time slices and then to chain the evolution of both the word-topic and topic mixture distribu-tions via a Gaussian process. In some cases, the Gaussian distribution was not found to be the appropriate distribu-tion in modeling the topic shifts and some studies considered other probability distributions for capturing the evolution of topics over time [22]. However, the idea of grouping docu-ments into epochs for modeling topic evolution was echoed in a number of studies. For example, [24] estimated a transi-tion matrix over topic vectors between two predefined epochs and they showed that the LDA model [5] can be enhanced by considering directly the evolution of the topics over time.
In this paper we propose two extensions of LDA for model-ing the dependency between two consecutive documents in a stream. In our first model, we suppose that the dependency between topic distributions of two consecutive documents follows a Dirichlet distribution controlled by an hyperpa-rameter. This model is similar to the one of [4] with time slices equal to 1, but it offers a more precise mechanism for controlling the dependencies and is based on a framework en-compassing all the situations (from complete independence to plain equality). This first study paves the way for a more general topic model in which the dependencies between the topics of two consecutive documents are captured by cop-ulas which constitute generic tools to model dependencies between random variables [6]. Among the several families of copulas that have been defined in the literature, our choice fell on Archimedean copulas [13, 14] as they are symmetric and associative, necessary conditions when dealing with ex-changeable random variables [18]. More particularly, we use Franck copulas, a special case of Archimedean copulas that rely on a single parameter, easier to estimate and more ro-bust to sparse data. Using three collections with different characteristics, we show that our approaches are faster and improve over state-of-the-art topic models. We also analyze the precision of our models to track the topics on a labeled dataset.

The outline of this paper is as follows. In the next sec-tion, we present our models. In Section 3, we introduce an efficient procedure to estimate the most important, in terms of size, parameters. We then describe in Section 4 the ex-perimental results obtained with our approaches on three distinct datasets. In Section 5, we position our work with respect to the state of the art. Finally, Section 6 concludes our study by summarizing its main results and by giving some pointers to future research. Latent Dirichlet Allocation ( LDA , [5]) is a probabilistic Bayesian model used to describe a corpus of D documents, associated with a vocabulary of size V . In this model, la-tent variables, indexed in { 1 ,  X  X  X  ,K } , are used to represent the hidden (in the sense non-observed) topics underlying each document. LDA is associated to the following genera-tive model 1 : where N is the length of each document and  X  k,v is the v coordinate of  X  k .  X  and  X  correspond to the priors of the model. They are usually fixed, following [5]. Furthermore, in almost all previous studies on LDA , the priors are consid-ered to be symmetric, each coordinate of the vector being equal:  X  1 =  X  X  X  =  X  K . If one assumes a broad Gamma prior for both  X  and  X  , then their value can be easily learned from data by maximum a posteriori [1] or Markov Chain Monte Carlo [15] methods. One can also envisage learning asym-metric Dirichlet priors [21], which raises no particular diffi-culties for the models we are considering. For clarity sake, we however assume here fixed, symmetric priors; the ex-tension to their learning through Gamma priors or through asymmetric priors is purely technical. In the remainder, we will denote by  X  and  X  the priors for the Dirichlet distribu-tions as well the constant value taken by each coordinate of these priors, the context being sufficient to determine which element is referred to.

An important characteristic of LDA is that each document is generated independently from the previous ones. This is not a realistic assumption in different settings, as document streams, and we introduce below two extensions of LDA that model such dependencies.
We introduce here a first extension of LDA , that we refer to as ST-LDA-D .
For simplification and following standard practice, we do not model here the length of each document, assumed to be fixed and equal to N . In this first model, we rely on a direct extension of the LDA model to take into account dependencies between the document-specific topic distributions of two sequential doc-uments, denoted ( d  X  1) and d (2  X  d  X  D ). This extension uses, as the standard LDA model, Dirichlet distributions for the document-specific topic distributions, the parameters of which are linear combination of the standard prior  X  and the topic distribution estimated in the previous document: where  X  d is a uniformly distributed parameter that controls the influence of the topics of document ( d  X  1) on the top-ics of document d (see Figure 1). The expectation of each component of  X  d is given by: Hence, if  X  d is high, i.e. if document d covers the same topics as document ( d  X  1), then E [  X  d i |  X  d  X  1 i ]  X   X  d  X  1
We furthermore assume that the previous document, ( d  X  1), can influence the word-topic distributions of the cur-rent document d . This assumption, also made in dynamic topic models [4] and topic tracking models [11], is motivated by the fact that, within a given topic, if word distributions evolve over time, they tend to do so in a smooth way. As before, one can use a direct extension of the LDA model to account for dependencies between word-topic distributions in sequential documents: Here  X  d is again a uniformly distributed parameter that con-trols the tradeoff between the prior  X  and the learned topic-word distributions  X  d  X  1 . As usual  X  d  X  1 k is the word distri-bution of topic k . The conditional mean of each component of  X  d k is given by: and is approximately the value of the same component of document ( d  X  1) when the two documents are strongly de-pendent.

Lastly, as one can note, by setting  X  d =  X  d = 0 ,  X  d, 2  X  d  X  D , one  X  X orgets X  the dependencies between consecutive documents. The streaming model is in this case identical to the standard LDA model.
As mentioned before, the parameters  X  and  X  are consid-ered fixed. The other parameters can be estimated through Gibbs sampling, with Metropolis-Hasting updates for the parameters  X  d and  X  d . We give here the update formulas of each parameter.

For  X  , one has: where  X  d is defined as in [23] and represents the d th row of the D  X  K count matrix  X , with  X  d,k being the number of times that topic k is assigned to words in document d .
The update for  X  d k , 1  X  k  X  K is similar: where  X  k is again defined as in [23] and represents the k row of a K  X  V count matrix,  X  k,v being the number of times that topic k is assigned to word v in the documents seen so far.

The Gibbs update for z is the same as the one for the standard LDA model: Finally, for  X  d and  X  d , one can not directly compute Gibbs updates as the normalizing factor for the distribution of  X  given all the other parameters can not be computed exactly. One can nevertheless rely on a Metropolis-Hasting proce-dure, detailed in Appendix A.
Model ST-LDA-D captures topic and word-topic dependen-cies through Dirichlet distributions, which allow one to bal-ance the influence of the priors (  X  and  X  ) and of the topic and topic-word distributions of the previous document. We introduce now another extension of LDA in which the de-pendencies between the topics of consecutive documents are modeled through copulas, which constitute a generic tool to model dependencies and do not rely on a specific distribu-tion. We first provide a brief overview of copulas, prior to describe our model.
For every p  X  2, a p  X  X imensional copula is a p  X  X ariate density function on [0 , 1] p , whose univariate marginals are uniformly distributed on [0 , 1]. Copulas are particularly use-ful when modeling dependencies between random variables. Indeed, the joint cumulative distribution function (CDF) F 1 ,  X  X  X  ,X p of any random vector X = ( X 1 ,  X  X  X  ,X p ) can be written as a function of its marginals, as follows: Theorem 1 ( Sklar X  X  theorem Theorem 2.3.3 of [16]) Let F 1 ,  X  X  X  ,X p be a p  X  X imensional distribution function with marginals F X 1 ,  X  X  X  ,F X p . Then there exists a copula C with uniform marginals such that: Furthermore, when the CDF F X 1 ,  X  X  X  ,X p is continuous, the copula is unique.

Copulas represent a general way of modeling the depen-dencies between random variables, from complete indepen-dence to equality. If the random variables X 1 ,  X  X  X  ,X p pairwise independent, their copula is the so X  X alled indepen-dency copula : whereas in the case X 1 =  X  X  X  = X d , one gets the comono-tonicity copula :
Several copula families have been defined in the literature, among which the Archimedean copulas ([16, Ch. 4]), partic-ularly interesting in our case. A p  X  X imensional Archimedean copula C with generator  X  is defined as: where  X  is a continuous, decreasing function, from [0 ,  X  ] satisfying: Archimedean copulas have the following interesting proper-ties:
Th e LDA model is based on the assumption that topics are infinitely exchangeable within a document. In this study, we further consider a particular case of the Archimedean copulas, namely the one X  X arameter family of Franck copula, defined, for any  X   X  R \{ 0 } , as: When  X   X  0, one approaches the independency copula, whereas  X  =  X  yields the comonotonicity copula. Lastly, for any  X   X  R \{ 0 } , C  X  is twice differentiable on [0 , 1] that the copula function admits a density, denoted in the se-quel c  X  . By varying  X  from 0 to  X  , Franck copula allows one to model all the possible dependencies between two random variables, from complete independency to equality. Depen-dency/independency is furthermore controlled by a single parameter,  X  , which makes parameter estimation both eas-ier and more robust.
Instead of generating the topic distribution of each docu-ment  X  d independently, as is done in standard LDA we bind, as for our first model, ST-LDA-D , the topic distributions  X  and  X  d of consecutive documents, this time by using copulas, and more precisely Franck copula.

One can not however directly use Sklar X  X  theorem as it does not extend to joint distributions over random vectors. This means that if we are given two random vectors X 1 , X one can not claim that there exists a copula C such that, for any ( x 1 , x 2 )  X  [0 , 1] p 1  X  [0 , 1] p 2 : except in very specific situation as when X 1 and X 2 are in-dependent for example. One can nevertheless relate latent topics  X  d  X  1 and  X  d through their components. Indeed, the topic Dirichlet distribution can be decomposed into univari-ate Gamma distributions with parameters (  X , 1), denoted Ga (  X  ): Theorem 2 (from Theorem 2.1 of [17]) A random vector  X  follows a Dirichlet distribution Dir (  X  ) iff there exists a random vector T  X  Ga (  X  )  X  X  X  X  X  X  Ga (  X  ) such that: where ( L ) = means  X  X quality in distribution X . In addition, if we are given  X   X  Dir (  X  ) and R  X  Ga ( K X  ) independent, then T = R X   X  Ga (  X  )  X  X  X  X  X  X  Ga (  X  ).

To bind the topic distributions  X  d  X  1 and  X  d of two con-secutive documents, we thus consider the associated vectors T d  X  1 and T d , and bind them coordinate per coordinate us-ing Franck copula. For the word-topic distributions, we use the same coupling between consecutive documents as the one used in model ST-LDA-D , as a tighter coupling through copulas would be too costly. We will come back to this issue in Section 3.

In the sequel for any  X  &gt; 0, f  X  (resp. F  X  ) denotes the pdf (resp. cdf) of the Gamma distribution with parameters (  X , 1). The global generative model is thus as follows: 1. Generate the first document according to the standard 2. For each document d , 2  X  d  X  D : where T d k represents the k th coordinate of the vector T and follows a distribution Ga (  X  ) according to Theorem 9. We refer to the corresponding model as ST-LDA-C . Figure 1 provides a graphical representation of this model, together with the ones of previous models.
The updates for z d ,  X  d and  X  d are identical to the ones for model ST-LDA-D . For  X  d , one gets: The same Metropolis-Hasting procedure as the one used for model ST-LDA-D and detailed in Appendix A can then be used.

For  X  d , one needs first to estimate the conditional prob-ability of the random vector T d with respect to the other parameters. This expression can be factored as follows: As in the classical context of LDA , one has P ( z d |  X  ) = B ( X   X  ) /B ( X  d ) where  X  d is defined as before. By assumption on the distribution of the random vectors ( T d  X  1 , T d ):
Developing P ( z d |T d ) as detailed in Appendix B, finally leads to: Each T d k can then be estimated through the Metropolis-Hasting procedure presented in Appendix A;  X  d is finally obtained from T d through Eq. 9.
For model ST-LDA-C , the word-topic distributions  X  d k (1  X  k  X  K ) could be estimated in the same way as  X  d is esti-mated, as mentioned in Section 2.2. However, this would entail running K  X  V Metropolis-Hasting procedures, which is problematic as soon as the collections considered are rel-atively large. We thus proposed in Section 2.2 to estimate it through Eq. 6, as done for ST-LDA-D . This time, K  X  V Gibbs sampling updates are required. If this estimation pro-cedure is faster, it may still be too slow for really large col-lections. Theorem 2 nevertheless suggests a way to approxi-mate  X  d k (1  X  k  X  K, 2  X  d  X  D ) through Gamma updates, as follows: 1. For each word v in d , generate t k,v  X  Ga (  X  +  X  d  X  1 2. For each word v in the vocabulary V ,  X  d k,v  X  t k,v where  X  corresponds to the real parameter ( i.e. , the constant value that makes up the V dimensional vector of priors). The quantities t k,v are first initialized through t k,v  X  Ga (  X  ), and updated each time a new document is encountered. As one can note, this update primarily concerns the words present in the current document (step 1), the components for the other words being just renormalized (step 2). This contrasts with Eq. 6 in which the contribution of all words is resampled for each document via a multivariate Dirichlet distribution. The above procedure simplifies this by relying on the univariate equivalent of the Dirichlet distribution, namely the Gamma distribution, and by binding the vari-ables through the renormalization step. It is faster as it involves only K  X  N samplings from a Gamma distribution instead of K samplings from a multivariate, V ( V &gt;&gt; N ) dimensional Dirichlet distribution (the K  X  V renormaliza-tions in step 2 do not really harm the procedure and are negligible compared to the Dirichlet samplings). We have Algorithm 1: Inference process for ST-LDA-[D|C]
Input : Stream of D documents of length N ; number of
Output : For each document d , topic distribution  X  d , // Initialization for k = 1 to K , v  X  X  do 2 t k,v  X  Ga (  X  ) for d = 1 to D do 4 Random initialization of  X  d ,  X  d and z d n , 1  X  n  X  N  X  1 =  X  1 = 0 // Document processing for d = 1 to D do 7 repeat 8 For ST-LDA-D : update  X  d acc. to Eq. 5 9 For ST-LDA-C : 10 (a) update T d (Metropolis-Hasting) 11 (b) obtain  X  d from T d through Eq. 9 12 Update  X  d k acc.  X  -procedure 13 Update  X  d and  X  d (Metropolis-Hasting), d &gt; 2 14 Update z d n acc. to Eq. 7, 1  X  k  X  K, 1  X  n  X  N 15 until estimates are stable observed in practice no difference, in terms of performance measures we consider (see Section 4), between this procedure and the more complex ones mentioned before, and make use of it in the remainder of the paper. In terms of speed, this procedure performed 1.5 times faster on the NIPS collec-tion, which contains long documents and a relatively small vocabulary ( ca. 12,000 words), and 2 times faster for the TDT4 and Tweets collections, which contain shorter docu-ments with a larger vocabulary (up to 42,000 words). Algorithm 1 summarizes the inference process we rely on. It makes use of the above procedure to estimate  X  , referred to as  X  -procedure.
We conducted a number of experiments aimed at evaluat-ing how the proposed models behave on different collections by analyzing their stability, convergence time and perfor-mance.

Datasets. We performed experiments on three datasets with different characteristics. The NIPS dataset contains 1,500 scientific papers with no time dependency between them. The size of the vocabulary is 12,375 and documents contain 500 unique words in average. The collection was collected from the NIPS proceedings and is relatively homo-geneous in terms of the topics covered. It allows us to assess whether topic dependencies are still useful in a  X  X oose X  con-text in which there is no more temporal dependency. It is available at the UCI ML Repository [12].

The Multilingual Text and Annotations data set ( TDT4 ) 3 proposed for topic detection and tracking, has 3,190 original documents in English and a vocabulary size of 22,965. Doc-uments here are newswires extracted from different broad-casts and the number of unique words per document is 100 in average. Even though newswires are not extracted from the same source, they are ranked by the time.
 The Tweets dataset is collected using Twitter X  X  streaming API during 20 days from 8/10/2014 to 27/10/2014. The collection contains 72,592 tweets and a vocabulary of size 42,336. Tweets have been sequenced by time and are filtered over health issues using an SVM classifier trained over MeSH categories 4 .
 Each dataset was separated into training and test sets. The NIPS collection was randomly splitted into training (90% of the collection) and test (10% of the collection) sets. For TDT4 , we used the first 2800 newswires released in time for training, and the last 390 ones for testing. For the Tweets dataset, we used the tweets issued in the first 17 days for training (60,000 documents) and those of the last 3 days (12,000 documents) for testing. Table 1 summarizes the characteristics of these collections.

Evaluation. Results are evaluated over the test set using the widely used perplexity measure that can be ap-proximated by [5]. perplexity ( C test ) = exp Linguistic Data Consortium, The Trustees of the University of Pennsylvania https://catalog.ldc.upenn.edu/ LDC2005T16. https://www.nlm.nih.gov/mesh/
P erplexity
P erplexity Table 1: Datasets used in our experiments along with their properties.
 Documents in Train set 1,350 2,800 60,000 Documents in Test set 150 390 12,000
Vocabulary size 12,375 22,965 42,336 # of unique words per doc. 500 100 15
Words in total 1,900,000 779,000 904,262 where C test denotes the test collection, D test is its size and v n represents the word at position n in document d . The parameters  X  d k and  X  d k are estimated on the training set. Fur-thermore, for the TDT4 collection we use the available se-mantic labels of newswires in the test set in order to evaluate the ability of the models to find documents of the same se-mantic labels using only their predicted topic distributions (Section 4.2). To this aim, we measure ROC curves and AUC of different topic models on TDT4 .

Settings and comparisons. For all models, both hyperparameters  X  and  X  were fixed to 0 . 5. Documents of the NIPS dataset are initially stoplisted, we did not perform further preprocessing of the data nor removed stop words from the TDT4 and Tweets documents as for all methods best results are obtained when collections are not filtered.
To validate the streaming LDA models described in the previous section, we test the following six methods. The first two are LDA models [5]: (a) LDA 1 , which consists in training an LDA model on the whole training data, then fix-ing  X  and updating  X  for each document in the test set, (b) LDA all , which consists in training an LDA model on the whole on training data and updating both  X  and  X  for each document in the test set. In addition, we consider two state-of-the-art latent models that take into account dependencies between topics: Dynamic Topic Model ( DTM ) [4] and Tem-poral LDA ( TM-LDA ) [24]. DTM is certainly the most popular model to take into account topic dependencies. It is fur-thermore complete in the sense that it integrates both topic and word-topic distributions. TM-LDA is a very recent pro-posal with nice features. Lastly, we also consider the two streaming LDA models we have introduced ( ST-LDA-D and ST-LDA-C ). For these last two models,  X   X  (see Appendix A) is set to 30,000 5 . All the algorithms were implemented in Python with Numpy and Scipy 6 except DTM that is a C++ implementation tool from [3]. For both training and test, DTM is used considering that each document corresponds to a time slice.
We start our evaluation by analyzing the gains provided by modeling dependencies between topics by streaming (as with ST-LDA-D and ST-LDA-C ) compared to other approaches on the different datasets. Figure 2 shows the evolution of perplexities of different models over the test set with re-spect to the training time of each model on NIPS and TDT4 datasets. The code program of DTM (in C++) generally ex-ecutes faster than the other code programs (written in pyh-ton), nevertheless we ignore this detail and consider all the curves identically.
 To measure the perplexity for each model, we estimate  X  and  X  over respectively all documents and all words of the training set. These estimates are then used to evaluate iter-atively new  X  and  X  distributions for each document in the test set. This iterative update of  X  and  X  is done for all of the methods except LDA 1 which updates the distributions  X  and  X  over the whole documents in the test set with the last parameters that were obtained from the training set.
As expected, all perplexity curves decrease monotonically with respect to time. On both datasets, perplexity curves
This value, upper bounding  X  d , corresponds to a regime of the Franck copula close to comonotonicity.
We are working to release all the programs developed in this study publicly available for research purpose. ST-LDA-D and ST-LDA-C lower-bound the other curves on all iterations. On the NIPS dataset, DTM becomes competitive with the two others, at the end of the iterations, while on TDT4 , where test documents come in a stream, ST-LDA-C stands clearly as the best model. These results show the ability of ST-LDA-C to capture dependencies between topics in document streams. Further, we note that at the begin-ning of iterations where dependencies are not yet apparent, the perplexity curves of both models are very similar to the one of LDA all . This is in line with our assertion of the pre-vious section supporting that both models reduce to LDA in the case where topics are independent. TM-LDA is not com-petitive in this setting as it does really not make advantage of the fact that the words in the new, arriving documents are known. Its ability to predict future topics is not exploited in this setting.

The evolution of perplexity on Tweets from the three last consecutive days considered in our experiments is shown in Figure 3. The behavior of perplexity curves here are accen-tuated with the total stream characteristics of Tweets ; the curve of LDA all gets away from those of ST-LDA-C and ST-LDA-D , while DTM comes close. In order to see if the number of topics, that we fixed for all models to 80, have an im-pact on these results or not, we repeated the experiments by varying the number of topics in the set { 20 , 40 , 60 } .
P erplexity F igure 3: Perplexity of each method by number of tweets that are added to the test set (80 topics).

Table 2 depicts the perplexities of all models on the three collections at the end when the parameters  X  and  X  have been estimated over all the test documents. In all exper-iments, best results are obtained with ST-LDA-C and ST-LDA-D , followed by DTM on NIPS and TDT4 and by LDA all on Tweets . These results are consistent with those of the figures 2 and 3. Again, TM-LDA does not perform well (as explained before); LDA all which is a standard LDA model, performs relatively well; however, both DTM and the ST-LDA-[D|C] models outperform it by taking into account dependencies between topics. We see here that the extra flexibility of the ST-LDA-[D|C] models allow them to outperform DTM .
We further investigate on the ability of models to find top-ics that can detect documents of the same semantic class. For doing so, we used the TDT4 collection for which some documents are assigned semantic classes by experts. We hence use the cosine measure or the  X  d parameter of ST-LDA-C , to detect consecutive documents in the test set of this collection that are found similar on the basis of their topic distributions; two consecutive documents are consid-ered as similar if the cosine measure of their topic distribu-tions (resp. estimated  X  d -line 13 Algorithm 1) is higher than a given threshold. If two consecutive and similar doc-uments share the same semantic label, we count them as a true positive; if they do not share the same semantic label, we count them as false positive. By changing the threshold, we can plot the ROC curves for the corresponding method. Figure 4 depicts ROC curves of DTM , TM-LDA and ST-LDA-C defined over 8 different thresholds taken in the set [0.2 0.5 0.7 0.86 0.89 0.92 0.95 0.98] for the cosine measure and [0.5 1 2 5 10 15 20 50] for  X  d when the number of topics is fixed to 20 and to 80.

In order to compare between the different ROC curves, we estimated the area under them, shown in Table 3. From these results it comes clear, that topic distributions found by ST-LDA-C are more able to detect these semantic classes than topic distributions of DTM and TM-LDA .
 M ethods 20 (Fig. 4, left) 80 (Fig. 4, right) S T-LDA-C with  X  d 0.7982 0.8306 ST-LDA-C with cosine 0.8004 0.7755 TM-LDA with cosine 0.7652 0.7349 DTM with cosine 0.7357 0.6301
F inally, to further illustrate the role of  X  d , we pictorially illustrate the correlation between the estimated  X  d and the topic distributions of three consecutive documents (Figure 5) with identical labels in the TDT4 collection. As one can
T rue Positive Rate (Recall)
T rue Positive Rate (Recall) ST-LDA-C , for the number of topics fixed to 20 (left) and 80 (right). see, the distributions of topics in the three pairs of consec-utive documents with high  X  d are similar. In addition, the two most probable topics of the document pairs retained in Figure 6, also taken from TDT4 , do not share any word when  X  d is small and are almost identical when  X  d is high. These examples illustrate the fact that  X  d is a good indicator of the topic dependencies between documents.
Some studies have considered the possibility to model dif-ferent streams of documents, as in [10], trying to leverage standard models (as LDA ) by considering topics common to the different streams. In such studies the evolution of topics over time is not considered. The study presented in [22] aims at modeling, through an extension of LDA , the timestamp associated with each token in a document. If dependen-cies between topics are not explicitly modeled, topics tend to specialize over different time periods through the joint dependence of each word and timestamp on the topic vari-able ( z in LDA ). Other studies have addressed the problem of topic evolution and dependencies within a single docu-ment, as the recent sequential LDA model described in [7]. We rather focus in this study on explicitly modeling topic dependencies across documents, for both topic and word-topic distributions. Several studies have addressed a similar problem. One of the first proposals corresponds to the Dy-namic Topic Model ( DTM ), introduced in [4] and illustrated in Figure 1. An interesting feature of DTM is its use of time slices; we have not considered time slices in this study, but our models (as most dynamic models) can be extended to deal with them. DTM captures dependencies for both topic and word-topic distributions. These dependencies are how-ever captured through Gaussian distributions, the expecta-tion of which corresponds to the previous parameters. This entails that new parameter values are constrained to be dis-tributed around the values observed previously. In contrast, even in model ST-LDA-D , the expectations of the new topic and word-topic distributions (Eqs. 2 and 4) can be uncor-related to the previous distributions in the absence of de-pendencies. Our models thus offer additional flexibility over the presence or absence of dependencies between consecu-tive documents in a stream. The Dynamic Mixture Model ( DMM , see Fig.1) introduced in [25] is similar to DTM except that topic dependencies are directly considered at the topic level (as is the case for ST-LDA-D and ST-LDA-C but not for DTM which operates at the prior level) and that word-topic dependencies are dropped. As for DTM , the expectation of a new topic distribution is given by the values obtained in the previous document. This again contrasts with our proposal that introduces additional flexibility, as mentioned before. The Topic Tracking Model ( TTM , see Fig.1) introduced in [11] is similar to our models in the sense that both topic and word-topic (more precisely interest-topic) dependencies are considered. However, as for DTM and DMM , the mean of the current topics and interests are the same as the ones of the previous topics and interests. The model is thus again limited in its ability to model the presence or absence of de-pendencies between consecutive documents. A more recent proposal, called Temporal LDA ( TM-LDA , see Fig.1), was in-troduced in [24]. TM-LDA differs from the previous models as it also aims at predicting future topics even in the situa-tion where future documents are not seen. It thus assumes a strong dependency between consecutive documents, which is not always realistic, even on such collections as Tweets . Furthermore, TM-LDA does not consider dependencies for the word-topic distributions.
We have proposed in this paper two new models for mod-eling topic and word-topic dependencies between consecu-tive documents in document streams. The first model is a direct extension of Latent Dirichlet Allocation model ( LDA ) and makes use of a Dirichlet distribution to balance the in-fluence of the LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second extension makes use of copulas, which constitute a generic Sport -right) and subject labels in TDT4 dataset (20 topics). Figure 6: 5 most frequent words of the most probable topic (20 topics). tool to model dependencies between random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal LDA ), both in terms of perplex-ity and for tracking similar topics in a document streams. Compared to previous proposals, our models have extra flex-ibility and can adapt to situations where there is in fact no dependencies between the documents.

In the future, we plan to develop non-parametric exten-sions as well as versions of these models that scale well, fol-lowing the improvements on the inference methods for LDA , proposed in streams [26] or in online settings [9, 2].
We thank the anonymous reviewers for their useful com-ments. This work was partly supported by the LabEx PERSYVAL-Lab ANR-11-LABX-0025. [1] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. [2] A. Banerjee and S. Basu. Topic models over text [3] D. M. Blei. Free C++ implementation for dtm. https: [4] D. M. Blei and J. D. Lafferty. Dynamic topic models. [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] S. Derrode and W. Pieczynski. Unsupervised data [7] L. Du, W. L. Buntine, and H. Jin. Sequential latent [8] M. M. Gaber, A. Zaslavsky, and S. Krishnaswamy. [9] M. D. Hoffman, D. M. Blei, and F. Bach. Online [10] L. Hong, B. Dom, S. Gurumurthy, and [11] T. Iwata, S. Watanabe, T. Yamada, and N. Ueda. [12] M. Lichman. UCI machine learning repository, 2013. [13] A. J. McNeil. Sampling nested Archimedean copulas. [14] A. J. McNeil and J. Ne X slehov`a. Multivariate [15] R. Neal. Slice sampling. Annals of Statistics , 2000. [16] R. B. Nelsen. An introduction to copulas . Springer [17] K. W. Ng, G.-L. Tian, and M.-L. Tang. Dirichlet and [18] O. Ostap, O. Yarema, and S. Wolfgang. Properties of [19] M. J. Paul and M. Dredze. You are what you tweet: [20] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [21] H. M. Wallach, D. M. Mimno, and A. McCallum. [22] X. Wang and A. McCallum. Topics over time: A [23] Y. Wang. Distributed gibbs sampling of latent topic [24] Y. Wang, E. Agichtein, and M. Benzi. TM-LDA: [25] X. Wei, J. Sun, and X. Wang. Dynamic mixture [26] L. Yao, D. Mimno, and A. McCallum. Efficient
The Metropolis-Hasting procedure is based on the follow-ing steps: 1. Generate an initial value of x : draw x 1  X  P prior ( x ) 2. Initialize j = 1 3. Repeat till sequence is stable For x =  X  d , one has: where P prior (  X  d )  X  U [0 , X   X  ]. As  X  d should be higher when  X  d  X  1 and  X  d are more similar (as in such a case the influence of  X  d  X  1 on  X  d is more important), we make use of the follow-ing jump function, based on the exponential distribution: For x =  X  d , the same distribution is used for the jump func-tion, the cosine being taken between the vectors that corre-spond to the column-wise concatenation of the columns of each matrix  X  d  X  1 and  X  d . The prior this time is P (  X  function corresponds to Franck copula, and  X ( T d k sponds to the k th contribution in Eq. 10.

We provide here the complete derivation of Eq. 10. For any d  X  2, one has:
T Let F  X  (resp f  X  ) denote the cdf (resp pdf) of the Gamma distribution with parameters (  X , 1). By assumption: and, since  X  d = T d / P K k =1 T d k , : Further, as usual [23]: Hence: p ( T d |T d  X  1 ,z d ,  X  X  X  ) = leading to the desired result.
