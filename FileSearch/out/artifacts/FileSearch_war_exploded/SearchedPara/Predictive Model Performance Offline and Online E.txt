 We study the accuracy of evaluation metrics used to estimate the efficacy of predictive models. Offline evaluation metrics are indicators of the expected model performance on real data. However, in practice we often experience substantial discrepancy between the offline and online performance of the models.

We investigate the characteristics and behaviors of the evaluation metrics on offline and online testing both ana-lytically and empirically by experimenting them on online advertising data from the Bing search engine. One of our findings is that some offline metrics like AUC (the Area Un-der the Receiver Operating Characteristic Curve) and RIG (Relative Information Gain) that summarize the model per-formance on the entire spectrum of operating points could be quite misleading sometimes and result in significant dis-crepancy in offline and online metrics. For example, for click prediction models for search advertising, errors in predic-tions in the very low range of predicted click scores impact the online performance much more negatively than errors in other regions. Most of the offline metrics we studied includ-ing AUC and RIG, however, are insensitive to such model behavior.

We designed a new model evaluation paradigm that sim-ulates the online behavior of predictive models. For a set of ads selected by a new prediction model, the online user behavior is estimated from the historic user behavior in the search logs. The experimental results on click prediction model for search advertising are highly promising. H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Software Engineering ]: Metrics X  complexity mea-sures, performance measures  X  Contact author evaluation metric, offline evaluation, online evaluation, AUC, RIG, log-likelihood, prediction error, simulated metric, on-line advertising, sponsored search, click prediction
In the field of machine learning, evaluation metrics are of-ten used to judge and compare the performance of predictive models on benchmark datasets. It is quite clear that good quantitative assessments of their accuracies are essential to build successful predictive systems. Though a large array of evaluation metrics are already available [5, 13] and de facto standard metrics may exist for specific prediction problems, they do not come without limitations and drawbacks. Previ-ous research has shown that some metrics may overestimate the model performances for skewed samples [9, 10, 14], and there exist variations of a metric that lead to different results under certain circumstances like cross validation [14].
For a typical machine learning problem, training and eval-uation (or test) samples are selected randomly from the pop-ulation the model needs to be built for, and predictive mod-els are built on the training samples. Then the learned mod-els are applied on the evaluation data, and the qualities of the models are measured using selected evaluation metrics. This is called offline evaluation .

In addition, highly complex modern applications, such as search engines like Google and Bing, and online shopping engines like Amazon and eBay, often conduct online evalu-ations of best performing offline models on a controlled AB testing platform ( online evaluation ). The online AB test-ing platform may set up two isolated testing environments that are identical except one is set up with the baseline (or control) model, and the other one with the new model to be tested. They send a predefined amount of live traffic to each environment for the same time period. The differences in online user behaviors, such as clicks and the number of searches per user, and some other performance metrics, such as revenue per search, are evaluated to determine whether the difference is statistically significant before making a final launch decision of the new model. The assumption here is that the online performance metric would be better, if the new model delivered better quality results.

One problem with the model evaluations in reality is that sometimes the improvement of model performance in offline evaluation does not get realized as much, or sometimes gets reversed in online evaluation. Unlike static offline evalua-tion, online testing even under the controlled environment is highly dynamic, of course, and many factors not consid-ered during the offline modeling play a role in the results. Nevertheless, these observations raise a question if there ex-ist fundamental biases or limitations of the offline evaluation metrics that lead to such discrepancies.

Another problem is comparing performance of predictive models built with different kinds of data, especially data with rare events. Rare events occur in disproportionately lower frequency than the counterparts, thus result in skewed sample distributions between the classes. This is a quite common phenomenon in real world problems. Examples of rare events include clicks on web search result links, clicks on display ads, and making a purchase after clicking on prod-uct ads. Previous research has shown that some metrics may overestimate the model performance for skewed samples[9]. The observations lead into the following questions. With the bias, how can we interpret and compare the model per-formance applied to different kinds of data ? For example, when we build prediction models for text ads and display ads, can we use the offline metrics as comparative measures to predict their true performance ? Suppose we know the true performance of a model, and we get equivalent offline metrics of the other model. Can we estimate the true per-formance of the other model ? If we can X  X , what kind of metrics should we use instead ?
We propose a new model evaluation paradigm: simulated metrics . We implemented auction simulation for offline sim-ulation of online behaviors and used the simulated metrics to estimate the online model performance of click predic-tion models. Since simulated metrics are designed to simu-late online behaviors, we expect they would suffer less from the performance discrepancy problem. Also, since simulated metrics directly estimates the online metrics such as user CTR (Click-Through Rate), they can be directly compara-ble even if they are for models built on different kinds of data.

The contributions of this paper are four-fold: The remaining parts of this paper are organized as follows. In the next section, we briefly review online advertising and binary classification error measurement. In Section 3, we survey predictive model evaluation metrics in open litera-ture. We then review some of the metrics frequently used in the surveyed literature in Section 4. In Section 5, we de-scribe the problems and limitations of the AUC and RIG measures on large scale click prediction models for spon-sored search. In Section 6, we discuss the discrepancy of the offline and online performance of the models deployed on real-time production traffic of the Bing search engine. Fi-nally, we summarize our findings and suggest best practice guidelines based on our analysis and lessons from real world experiences on online advertising data. The target application of our study is online advertising. Some of the problem areas discussed in this study might be specific to the domain. In this section, we briefly review major areas of online advertising, and those who are inter-ested may find excellent tutorials on the references provided below. Sponsored (or paid ) search [11, 21, 28, 34] such as Google AdWords and Bing X  X  Paid Search, is search advertising that shows ads alongside algorithmic search results on search en-gine results pages (SERPs). Sponsored search reaches out to people actively looking for information about products and services online, thus has relatively higher click-through rate (CTR) compared to other types of advertising. Advertisers bid on keywords through a Generalized Second-Price (GSP) auction [11]. Bidders with highest rank scores ( r ) win the auction: where b is a bid amount, p is estimated position-unbiased CTR, and  X  is a parameter, called click investment power . If  X &gt; 1, the auction prefers ads with higher estimated CTRs, otherwise, ads with higher bids. Rank score is estimated CTR weighted by cost per click bid.

Ads are allocated in the descending order of estimated rank scores, and the auction winners pay price per click (a.k.a. cost per click, or CPC) for their ad impression only when people click on their ads. In a GSP auction, CPC depends on the next higher bidder X  X  bid amount, c i :
User clicks are highly dependent on the position of the ads[7, 15]. Typically ads shown on the section above algo-rithmic search results (called mainline ) get higher CTR than those shown to the right of the algorithmic results (called side bar ). Within the same section, the higher the ad loca-tion, the more clicks it gets for the same ad.

Display ads [32] are graphical ads that appears on web-sites, content pages, or applications such as instant messag-ing, email, etc. Contextual ads [7], such as Google AdSence or Bing X  X  Contextual Search are contextually optimized ads placed on publisher X  X  sites often with customized look and feel of the publisher X  X  site.

Accurate estimation of the probabilities of user clicks is critical for the efficiency of ad exchange [25]. The problem of estimating click probabilities has been studied extensively both for algorithmic search [24, 30, 31, 36] and for ads[6, 8, 16, 27].
Consider a feature vector x , and observed binary responses, y  X  X  0 , 1 } . x is considered as a realization of a random vec-tor X , and y as a Bernoulli random variable Y . The class 1 probability  X  = P [ Y = 1 ] is a function of x :  X  ( x ) = P [ Y = 1 | X = x ]. A binary classifier predicts samples with  X  ( x ) &gt; c as class 1, where c is a parameter: otherwise, pre-dicts as class 0.

The efficacy of the predictions is estimated using vari-ous criteria including the primary criteria such as prediction error, and surrogate criteria such as log-loss and squared er-ror loss [4]. Primary criteria are used to estimate the class directly, and surrogate criteria are to estimate the class pre-diction probability. Prediction (or misclassification) error is intrinsically unstable for estimating model performance. In-stead, log-loss and square error loss are often used for prob-ability estimation and boosting, and defined as follows:. where p is the estimated probability of  X  ( x ). The equality of squared error loss holds only for binary classifiers: i.e., y  X  X  0 , 1 } .

Log-loss is the negative log-likelihood of the Bernoulli model. Its expected value, - X log ( p ) -(1  X   X  ) log (1  X  p ), is called Kullback-Leibler loss [19] or cross-entropy .
Throughout the paper we show motivating examples and the analyses of the click prediction model performance on Microsoft Bing search engine. We sampled data from Bing X  X  sponsored search logs during the time period of Jun. thru Aug., 2012. We used two sets of data: one sampled from paid search data on Bing, and another from the contextual ads on partner websites on Microsoft publisher network.
We studied papers from the proceedings of the Interna-tional World Wide Web Conference (WWW), the ACM In-ternational conference on Web Search and Data Mining Con-ference (WSDM), and the ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining Confer-ence (SIGKDD) in years 2011 and 2012 in the area of al-gorithmic search and online advertising. We manually cat-egorized the topic areas of the papers and the evaluation metrics they used. Table 1 summarizes the results.
There are four major topical categories we found: recom-mendation, search, online advertising, and CTR estimation. Search and online advertising are further divided into sub-categories. The count of higher level category is sum of the counts of its sub-categories.

The categories of metrics are divided into offline and on-line metrics. Online metrics include model performance statistics such as ad impression yield, ad coverage, and user reaction metrics, such as CTR and the length of user ses-sions. Offline metrics are categorized into the following six types [18, 1, 26, 35]:
NDCG is a de facto standard metric of choice for search ranking algorithms. Even though probability based met-rics are relatively popular for advertising domain, there still doesn X  X  exist a single metric that dominates the domain like NDCG for search ranking problems,. Despite previous re-search that suggest AUC is much more reliable[3, 29, 22], there were only 2 papers we found that measured AUC. We applied AUC on the click prediction ( pClick ) problem on advertising domain, and found that it was one of the most reliable metrics, but not without problems. We will discuss about individual metrics in detail in the next section.
We focus our review on the metrics primarily used for click prediction problems. A click prediction model esti-mates position-unbiased CTRs of ads for the given query. We treat it as a binary classification problem.

We exclude NDCG from our review because it is designed to prefer a ranking algorithm that places more relevant re-sults at earlier ranks. As discussed in section 2.1, in search advertising, the ranks are determined not by the pClick (i.e., the estimated click) scores, but by the rank scores. There-fore, measuring the performance of pClick by the rank orders using NDCG is inappropriate.

We also exclude Precision-Recall (PR) analysis on our re-view because there is a connection between PR curve and ROC (Receiver Operator Characteristic) curve, thus a con-nection between PR curve and AUC [9]. Davis and Goadrich show that a curve dominates in ROC space if and only if it dominates in PR space [9].
Consider a binary classifier that produces the probability of an event, p . p and 1-p , the probability the event does not occur, represent the degree to which each case is a member of one of the two events. A threshold is necessary in order to predict the class membership. AUC, or the Area under the ROC (Receiver Operating Characteristic) Curve [12, 33], provides a discriminative measure across all possible range of thresholds applied to the classifier.

Comparing the probabilities involves the computation of four different fractions in a confusion matrix: the true posi-tive rate (TPR) or sensitivity , the true negative rate (TNR) or specificity , the false positive rate (FPR) or commission errors , and false negative rate (FNR) or omission errors . These four scores and other measures of accuracy derived from the confusion matrix such as precision , recall , or accu-racy all depend on the threshold. Offline Metrics The ROC curve is a graphical depiction of sensitivity (or TPR) as a function of commission error (or FPR) of a binary classifier as its threshold varies. AUC is computed as follows:
Empirically, AUC is a good and reliable indicator of the predictive power of any scoring model. For sponsored search, AUC, especially AUC measured only on mainline ads, is one of the most reliable indicators of the predictive power of the models. A good model (AUC &gt; 0.8) usually has statistically significant improvement if AUC improves by 1 point (0.01).
The benefits of using the AUC for predictive modeling include:
RIG ( Relative Information Gain ) is a linear transforma-tion of log-loss [15, 36]: where c and p represent observed click and pClick , respec-tively.  X  represents the CTR of the evaluation data.
Log-loss represents the expected probability of click. Min-imizing log-loss means that pClick should converge to the expected click rate and the RIG score increases.
MSE (Mean Squared Error) measures the average of squared loss: where p i and c i are pClick and the observed click, respec-tively, of sample i .
 NMSE (Normalized MSE) is MSE normalized by CTR,  X  : Mean Absolute Error (MAE) is given by: where e i = | p i  X  c i | is an absolute error.

MAE weighs the distance between the prediction and ob-servation equally regardless of the distance to the critical operating points. MAE is commonly used to measure fore-cast error in time series analysis.
Empirically it also has a good performance on estimating the pClick model efficacy for sponsored search. It is one of the most reliable metrics together with AUC.
Prediction Error (PE) measures average pClick normal-ized by CTR: PE becomes zero where the average pClick score exactly estimates the CTR. On the other hand, PE could be still very close to zero even when the estimated pClick scores are quite inaccurate with mix of under-and over-estimation of the probability as long as the average is quite similar to the underlying CTR. This makes prediction error quite unstable, and it can not be used to estimate the classification accuracy reliably.
Although online experiments on controlled AB testing en-vironment provides the real performance metrics of models under comparison by user engagement, AB testing environ-ments are pre-set with a fixed set of parameter values, thus the model performance metrics on the testing enviroment is only for the given set of operating points. Conducting online experiments over numerous sets of operating points is not practical because online experiment is not only very time consuming, but also could be very expensive in terms of both user experience and revenue, if the new model under-performs.

Instead of using expensive and time consuming online evaluation, the performance of a model over the entire span of feasible operating points can be simulated using the his-toric online user engagement data. Kumar, et. al. devel-oped an online performance simulation methods for feder-ated search [20].

Auction simulation, first, reruns ad auctions offline for the given query and selects a set of ads based on new model prediction scores and/or various sets of operating points.
We implemented auction simulation [15] using sponsored search click logs data and produced various simulated met-rics. Auction simulation, first, reruns ad auctions offline for the given query and selects a set of ads based on the new model prediction scores. During the simulation, user clicks are estimated using historic user clicks of the given (query, ad) pair available in the logs as follows:
Sidebar ads are ads shown on the ad block at the right side of algorithmic search results. Click curve and reference CTR are derived from the historic user responses in the search advertising logs.

Empirically, auction simulation produces highly accurate set of ads selected by the new model for the given set of op-erating points. Simulated metric often turns out to be one of the strongest offline estimators of online model performance.
In this section we analyze the behaviors, limitations and drawbacks of various metrics in detail in the context of click prediction for search advertising. Note that we do not mean to suggest these metrics be dismissed all together due to the limitations and drawbacks. We rather suggest the met-rics be carefully applied and interpreted, especially on the circumstances where the metrics may produce misleading estimations.
While AUC is a quite reliable method to assess the perfor-mance of predictive models, it still suffers from drawbacks under certain conditions of sample data. The assumption that AUC is a sufficient test metric of model performance needs to be re-examined [23].

First, it ignores the predicted probability values. This makes it insensitive to the transformation of the predicted probabilities that preserve their ranks. On one hand, this could be an advantage as it enables comparing tests that yield numerical results on different measurement scales. On the other hand it also is quite possible for two tests to pro-duce dramatically different prediction output, but with sim-ilar AUC scores. It is possible that a poorly fitted model (overestimating or underestimating all the predictions) has a good discrimination power [17], while a well-fitted model has poor discrimination if probabilities for presences are only moderately higher than those for absences, for example.
Table 2 shows an example of a poorly fitted model that has even higher AUC score where a large number of negative samples have very low pClick scores, thus lower CTR. This has an effect of lowering the FPR in the relatively higher range of pClick scores, thus raising the AUC score.
Second, it summarizes the test performance over the en-tire spectrum of the ROC space including the area one would rarely operate on. For example, for sponsored search, plac-ing an ad in mainline impacts the CTR significantly, while it is not as much of a concern how the predicted CTR fits to the actual CTR once it is shown on mainline or where it is not shown at all. In other words, the extreme right and left side of the ROC space are generally less useful. Baker and Pinsky proposed partial ROC curves as an alternative to entire ROC curves[2].

It has been observed that higher AUC does not necessarily mean better ranking always. As shown in Table 3, changes in the sample distribution on either end of FPR impacts the AUC score quite substantially. Nevertheless the impact on the performance of the model in terms of CTR could be the same especially at the practical operating point of model.) point.
 table shows a better-fitted model.) Figure 1: ROC curves of sponsored search and con-textual ads the threshold. Because AUC does not discriminate the var-ious regions of the ROC space, a model may be trained to maximize the AUC score just by optimizing the model per-formance on the either end of data. This may lead to lower than expected performance gain on the real online traffic. Third, it weighs omission and commission errors equally. For example, in the context of sponsored search, the penalty of not placing the optimal ads in mainline (omission error) far exceeds the penalty of placing a sub-optimal ads (com-mission error). When the misclassification cost are unequal, summarizing over all possible threshold values is flawed.
Lastly, AUC is highly dependent on the underlying dis-tribution of data. The AUC measures computed for two datasets with different rate of negative samples would be quite different. See Table 4. A poorly fitted model with lower intrinsic CTR has the same AUC as a well-fitted model. This also implies that higher AUC score for a model trained with higher rate of negative samples does not necessarily imply the model has better predictive performance. Fig-ure 1 plots the ROC curves of pClick models for sponsored search and contextual ads. As indicated on the figure the AUC score of contextual ads model is about 3% higher than AUC of sponsored search, even though the former is less ac-contextual ads.
One problem with RIG is, like AUC, it also is highly sensi-tive to the underlying distribution of evaluation data. Since the range of the RIG scores of evaluation data vary quite widely depending on the data distribution, one may not be able to judge how good a prediction model is just by having the RIG scores.

Figure 2 illustrates how RIG (solid curve) and PE (dot-ted curve) varies over a typical CTR range of interest. We observe the RIG scores drop as the CTR of the dataset in-creases even with the same prediction model. The prediction error plotted on Figure 2 roughly indicates how close the prediction score is to the true CTR. As expected, the click prediction error is higher with the low pClick score range.
This behavior coincides with our earlier observations on various click prediction data sets with varying level of the intrinsic CTR. The observations suggest the followings in practice: Figure 2: The RIG and PE scores over varying CTR of sample data: The RIG scores drop with increasing CTR.
 Table 5: Offline and online metrics of a new model (model-2) compared to the baseline model. model-2 metrics 8.6% 19.5% -9.96% -8.07%
A more significant problem with the offline evaluation metrics in practice is the discrepancy in performance be-tween the offline and online testing. There are cases where a predictive model that achieved significant gain on offline metrics does not perform as well or sometimes even under-perform when deployed on the online testing environment.
Table 5 summarizes offline and online metrics of a click prediction model built with sponsored search data from the Bing search engine, and tested on online AB testing environ-ments on Bing with real-time user traffic. Click yield (CY) is a metric of online user clicks that measures the number of clicks on ads per search page views. Mainline CY is the number of clicks on mainline ads per search page views. The new model experienced significant drop in user clicks over the baseline model on online environment even though both Figure 3: Relative contribution of log-loss over the typical range of pClick of interest.
 AUC and RIG exhibited significant gain on offline evaluation data.

Figure 3 compares log-loss [4] of two click prediction mod-els (model-1 for baseline and model-2 for test) of each quan-tile within the typical range of pClick scores of interest. Model-2 substantially overestimates the pClick scores on the quantiles in lower pClick score range, and for the quantiles with higher pClick scores with much less degree of over-estimation. Figure 4 plots the prediction error of the same data with similar pattern.

Over-estimation of click probability on higher range of pClick scores, in practice, makes less impact on online per-formance than over-estimation on low pClick score range, because ads in the high pClick score range would have been most likely selected by either model. And once shown to the user, user clicks are mostly determined by the ad-position and relevances of the ads, rather than the assigned pClick scores.

On the other hand, over-estimation of pClick scores on the low pClick range could make significant negative impact on online metrics by giving low quality ads higher chance to be selected compared to the base model. The lower quality ads selected due to over-estimated pClick scores would result in lower rate of user clicks, thus hurting the online metric.
Most of the offline metrics including RIG and AUC are not able to capture thess behaviors, as the metrics cumulates the impact throughout the entire range of pClick scores.
We computed the simulated metric by auction simulation as described in Section 4.6. The experimental results of the simulated click metrics along with the offline and on-line metrics are summarized in Table 6. We first trained a new model and optimized parameter settings that offer the best expected user click metric by auction simulation based on historic logs data. The click metrics with the best performing operating points of the models are reported as simulated metric in the table. We then set up AB testing environments with the best settings and ran the online AB testing experiments to get the online metrics. You can see that the online metrics highly coincides with the simulated metrics, while the improvements on AUC and RIG metrics differ drastically.
We reviewed and investigated the behaviors of various of-fline metrics for predictive models, especially in the context of click prediction for search advertising. To summarize: [1] A. Ashkan and C. L.A. Alarke. On the informativeness [2] S. G. Baker and P. F. Pinsky. A proposed design and [3] J. R. Beck and E.K. Shultz. The use of relative [4] A. Buja, W. Stuetzle, and Y. Shen. Loss functions for [5] R. Caruana and A. Niculescu-Mizil. Data mining in [6] D. Chakrabarti, D. Agarwal, and V. Josifovski. [7] Y. Chen, P. Berkhin, J. Li, S. Wan, and T. W. Yan. [8] Y. Chen, D. Pavlov, M. Kapralov, and J. F. Canny. [9] J. Davis and M. Goadrich. The relationship between [10] C. Drummond and R. Holte. Explicitly representing [11] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet [12] T. Fawcett. Roc graphs: Notes and practical [13] C. Ferry, J. Hernandez-Orallo, and R. Modriou. An [14] G. Forman and M. Scholz. Apples-to-apples in [15] T. Graepel, J.Q. Candela, T. Borchert, and [16] N. Gupta, U. Khurana, T. Lee, and S. Nawathe. [17] D.W. Hosmer and S. Lemeshow. Applied logistic [18] K. Jarvelin and J. Kekalainen. Ir evaluation methods [19] S. Kullback and R.A. Leibler. On information and [20] A. Kumar, K. Pattabiraman, D. Brand, and [21] S. Lahaie, D. M. Pennock, A. Saberi, and R. V. [22] P. Langley. Crafting papers on machine learning. In [23] J. M. Lobo, A. Jimenez-Valverde, and R. Real. Auc: a [24] V. Murdock M. Ciaramita and V. Plachouras. Online [25] R. McAfee. The design of advertising exchanges. [26] A. Moffat and J. Zobel. Rank-biased precision for [27] V. Murdock, M. Ciaramita, and V. Plachouras. A [28] P. Papadimitriou and H. Garcia-Molina. Sponsored [29] F. Provost, T. Fawcett, and R. Kohavi. The case [30] M. Regelson and D. C. Fain. Predicting click-through [31] M. Richardson, E. Dominowska, and R. Ragno.
 [32] R. Rosales, H. Cheng, and E. Manavoglu. Post-click [33] John A. Swets. Measuring the accuracy of diagnostic [34] H. R. Varian. Position auctions. Int X  X  Journal of [35] E. M. Voorhees. The trec-8 question answering track [36] C. Xiong, T. Wang, W. Ding, Y. Shen, and T. Liu.
