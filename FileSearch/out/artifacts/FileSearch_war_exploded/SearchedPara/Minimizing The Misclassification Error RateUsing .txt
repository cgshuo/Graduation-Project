 Shai Ben-David shai@cs.uwaterloo.ca University of Waterloo David Loker dloker@cs.uwaterloo.ca University of Waterloo Nathan Srebro nati@ttic.edu Toyota Technological Institute at Chicago Karthik Sridharan skarthik@wharton.upenn.edu University of Pennsylvania Perhaps the most fundamental question studied in the theory of machine learning is that of binary classifi-cation with half-spaces. However the problem of ag-nostically learning half-spaces is known to be NP-hard in general ( Kearns et al. , 1994 ). Even when one only wants to learn a half-space relative to the best possible M -margin error, Ben-david &amp; Simon ( 2000 ) show that (subject to P 6 = NP ) there exists no proper learning algorithm (i.e. returning a linear predictor) that runs in time polynomial in both 1 /M and the desired ac-curacy. Under a cryptographic hardness assumption, ( Shalev-Shwartz et al. , 2010 )extendthisresulttoim-proper learning (i.e. when the algorithm may output any predictor, as long as it generalizes well). In practice, one typically reverts to minimizing a con-vex surrogate, such as the hinge-loss, squared loss, logistic loss, exp-loss etc. Minimizing such a convex loss is usually easy, and can be done in polynomial time, but the question then is how well does min-imizing such a convex surrogate perform relative to minimizing the actual classification error. An impor-tant line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of  X  X lassification calibrated X  loss functions ( Zhang , 2004 ; Bartlett et al. , 2006 ). As discussed in detail in Section 4 , this notion is relevant only when the Bayes predictor is in our hypothesis class X  X .e. for linear prediction, when the Bayes predictor is exactly linear. Here we consider the more realistic agnostic setting, where we make no such assumption. Instead, we only rely on the existence of some linear predictor with small error rate at some margin, and ask the ques-tion of what misclassification error can be guaranteed by minimizing a convex loss. We obtain guarantees for specific loss functions, which allow us to compare between them, as well as a lower bound that holds for any convex loss. We now proceed to discuss and compare our work with other related work. 1.1. More Related Work Several other authors, beyond those discussed in the previous section, also address the question of choosing appropriate surrogate loss functions for binary classi-fication. Masnadi-Shirazi &amp; Vasconcelos ( 2009 ) and Nock &amp; Nielsen ( 2008 ) study various choices of sur-rogate losses for the classification problem and argue that the  X  X ight X  loss depends on the underlying un-known distribution. In the agnostic setting, which we consider here, we do not know the underlying distribu-tion, and so seek a distribution free guarantee. Christ-mann &amp; Steinwart ( 2004 ) investigate robustness prop-erties of learning algorithms that are based on convex risk minimization in terms of the so called influence function corresponding to the loss. They argue that most statistical models are intended to be approxi-mations of true model generating data. Robustness of the method basically implies that when the true underlying distributions deviate from the model only slightly then the performance of the method is not af-fected much. Hence, they use this to argue about the e cacy of algorithms based on convex losses. While robustness property tells us that deviating from the model by a small amount doesn X  X  a  X  ect performance much, it does not address the issue of agnostic learning where no assumptions are made about the underlying distribution (except that a hypothesis in the hypoth-esis class has low risk). In contrast, our work directly looks at how minimizing a surrogate loss corresponds to minimizing the misclassification error rate without any assumptions about the model generating the data or assuming even that the approximate Bayes optimal, w.r.t. the surrogate loss, belongs to the hypothesis class.
 In terms of directly minimizing the misclassification er-ror rate, without using a convex surrogate loss, Kalai et al. ( 2008 ) provide non-asymptotic finite sample bounds for e cient binary prediction with half spaces. However, to do so they assume the inputs are uni-formly distributed on the surface of a sphere, which is an extremely strong an unrealistic assumption. Simi-larly, Kalai &amp; Sastry ( 2009 ) also provide an e cient al-gorithm for minimizing the misclassification error, but only under the assumption that the conditional distri-bution of the label given the input x is some mono-tonic function of w  X  x for some w  X  X gain significantly departing from the agnostic setting.
 As mentioned earlier, although they focus on boosting (coordinate descent optimization), Long &amp; Servedio ( 2008 ) essentially establish that if one does not assume that margin error,  X  , of the optimal linear classifier is small enough then any algorithm minimizing any con-vex loss (which they think of as a  X  X otential X ) can be forced to su  X  er a large misclassification error. They do not, however, consider the bounded-margin case, that is when the margin error  X  of the best linear classifier is small compared to the margin M . In concurrent work, ( Long &amp; Servedio , 2011 ) do show that using convex surrogate losses for learning can at best only guaran-tee that the zero-one is bounded by O (  X  /M ), which is a result very similar to our Theorem 4 , though this a much higher constant. They do not however show lower bounds for specific loss functions which gives one the tool to compare various convex surrogate losses in a precise manor. More interestingly, they provide a randomized improper learning algorithm whose zero-one loss is bounded by  X  / ( M log( 1 M ))+  X  in time poly-nomial in 1 /M and 1 /  X  (where  X  is the misclassifica-tion error rate at margin M ). This shows that, at least when improper learning is allowed (i.e. we are allowed to return a non-linear predictor, as long as it generalizes well), it is possible to do (slightly) better than minimizing a convex surrogate. The question of whether this is possible with proper learning remains open. Let D ( x ,y ) be a distribution over U  X  { +1 , 1 } ,where, for some d , U = { x 2 R d : k x k X  1 } is the d -dimensional unit sphere. We will often actually con-sider finite samples, in which case D should be un-derstood as a uniform distribution over points in the sample.
 A linear predictor is described by a vector and a bias term: ( w ,w 0 ), w 2 R d , w 0 2 R . For a loss function : R ! R ,the -risk is give by: When the distribution D is understood from the con-text, we will simply use R ( w ,w 0 ). We will be par-ticularly interested in the 0-1 loss 01 ( z )= 1 { z  X  0 } and the margin-loss m ( z )= 1 { z&lt; 1 } , and the cor-responding risks:
R 01 ( w ,w 0 )= R 01 ( w ,w 0 )=Pr( y ( h w , x i + w 0 )  X  0)
R m ( w ,w 0 )= R m ( w ,w 0 )=Pr( y ( h w , x i + w 0 ) &lt; 1) Note that we are considering a prediction of zero as an error both for the positive class and the negative class, thus always predicting zero yields an error rate of one. We are also using m and R m to denote the error relative to a margin of 1, and so we will actually encode the margin through the norm of w ,i.e. the actual margin is 1 / k w k . We begin by showing that for any convex loss func-tion, and any arbitrarily low misclassification error rate  X  &gt; 0, there exists a sample which is linearly separable with error rate  X  , but for which the pre-dictor minimizing the surrogate loss would have error rate arbitrarily close to 1. In other words, there are training samples over which an algorithm that mini-mizes the surrogate loss will output a classifier whose actual training error is close to 1, in spite of the fact that these samples can be classified with small error by another half-space (which is missed by the loss mini-mization algorithm, due to having high surrogate loss). Furthermore, such examples exist even if the domain space is just the real interval. A similar result was also essentially shown by Long &amp; Servedio ( 2008 ) X  they discuss  X  X oosting X , i.e. loss minimization by co-ordinate descent, here we refer more directly to the loss minimizer.
 For a loss (  X  ), define the Misclassification E rror G uarantee relative to the zero-one loss as: EG ( ,  X  )= sup That is, we are asking: what is the largest misclassifi-cation error su  X  ered by the linear predictor minimizing -risk when the underlying distribution is such that the misclassification error rate of the best half-space is bounded by  X  .
 When  X  = 0, i.e. in the separable case, this is essen-tially a question about  X  X lassification calibration X , and we have that for convex , EG ( , 0) = 0 if and only if is di  X  erentiable at zero and 0 (0) &lt; 0( Bartlett et al. , 2006 ) X  X ee Section 4 . This is a mild condition that holds for most common loss functions, but here we are interested in EG ( ,  X  ) for  X  &gt; 0. Our initial claim can be state as follows: Proposition 1. For any convex function ,andany  X  &gt; 0 , EG ( ,  X  )=1 .
 This follows from the one-dimensional source distribu-tion below: Here, the optimal linear predictor is one that labels points to the right of 0 as positive and left of 0 as negative, yielding misclassification error  X  . However as M ! 0, the minimizer of any classification calibrated convex surrogate loss will label points to the right of 0 as negative and left of 0 as positive because the points close to 0 ( M close) su  X  er small loss under the convex loss. This simple example shows that EG ( ,  X  )=1 for any convex loss. 3.1. Surrogate loss minimization when We have just shown that the existence of a linear pre-dictor with low misclassification error is not enough to ensure the success of surrogate loss minimization. Our next step is to analyze the success of this paradigm under stronger assumptions -the existence of a good linear classifier with respect to some positive margin. Our next definition considers the worst possible error of a surrogate loss minimizer when the data allows a low error classifier with some margin. Our main result in this section, Theorem 4 , implies that, up to a factor of 2, the hinge loss is optimal among all convex loss functions in that respect.

EG ( ,  X  , B ) Here, B specifies the margin, and we will sometimes refer to it directly as M =1 /B . We have that EG ( ,  X  )= EG ( ,  X  , 1 ). A more careful look at the lower bound on EG ( ,  X  ) in the previous section re-veals that for  X  1 / ( B + 1) = M/ ( M + 1), we have EG ( ,  X  , B ) = 1 for any convex loss . However for smaller values of  X  , it is possible to get meaningful bounds on EG ( ,  X  , B ). In particular, for the hinge loss the simple observation that B + 1 times the mar-gin loss upper bounds the hinge loss in the interval [ B, B ] gives the below upper bound on EG ( ,  X  , B ). Proposition 2. For the hinge loss hinge ( z )= max(0 , 1 z ) , we have that In order to prove our main result, it would be useful to generalize the above result a parametric family of  X  X caled X  hinge losses give by hinge ( z ) = max(0 , 1 ), with a parameter &gt; 0. Thus, if = 1, hinge Theorem 3. For al l &gt; 0 ,if  X  ( B + 1) &lt; 1 , then Using the above theorem, we prove our main result : Theorem 4. For any convex loss function , we have The above theorem and Proposition 2 together show that hinge loss is optimal up to constant factor 2. We would also like to compare the error guarantees of specific losses or loss families. To this end, we de-rive the following generic  X  X ecipe X  for obtaining lower bounds specific to loss functions (See Appendix, Sec-tion A for a proof): Lemma 3.1. For any non-negative convex loss , EG ( ,  X  , B ) (3) Based on the above lemma, we show a lower bound for any strongly convex loss function, which shows that choosing strongly convex loss functions is in fact qual-itatively worse in the worst case sense (See Appendix, Section B for a proof).
 Corollary 5. For any -strongly convex surrogate loss that is L -Lipschitz in the interval [ 1 , 1] , we have that 3.2. Bounds for Specific Losses Before we proceed we would like to give an alternate bound to Equation ( 3 ) which is often easier to get a handle on. To this end note that by ( 3.1 ):
For the first term in the max, note that for some decreasing in x 1 and so,
Further note that for &gt;  X  , 2  X  4 1 4 and so we can conclude that, Example 3.1 (Hinge Loss) . The hinge loss is give by ( z ) = max(1 z, 0) . Simply using Theorem 4 we get Example 3.2 (Squared Hinge Loss) . The squared hinge loss is given by ( z ) = max(1 z, 0) 2 .Using Equation 4 with = 1 2 , we get Example 3.3 (Exponential Loss) . Exponential loss is given by ( z )= e z . For Exponential loss using Equation 4 with =1 / 2 we get Example 3.4 (Logistic Loss) . For Logistic loss is given by ( z ) = log(1 + e z ) . For logistic loss, us-ing Equation 4 with =1 / 2 we get, EG ( ,  X  , B ) min Notice that for large B this behaves similar to hinge loss. Also notice that the squared hinge loss (and sim-ilarly square loss) behave quadratically in B and ex-ponential loss has exponential dependence on B .Thus we see that for large B hinge loss gives qualitatively better bound that squared loss of exponential loss. 3.3. General Hypothesis Classes The misclassification error guarantee ( EG ) was spe-cific to linear predictors (with norm bounded by B ). One can easily generalize this definition of misclassifi-cation error guarantee w.r.t. an arbitrary hypothesis class H as follows :
EG H ( ,  X  , B ) Since the linear hypothesis with norm bounded by B is a particular case of a hypothesis class that satisfies sup x , x 0 | h w , x i h w , x 0 i |  X  2 B , we have that for any loss : On the other hand, note that Proposition 2 was only based on the fact that hinge loss can be bounded by the margin loss times the maximum value of the loss (given maximal value of predictor is B ). Hence the proposition directly extends to any hypothesis class and so we can conclude that And so, all our upper and lower bounds can also be interpreted as upper and lower bounds on sup H EG H ( hinge ,  X  ,B ), i.e. on the best misclassifica-tion error guarantee that is possible based only on the loss function, and is required to hold independent of the hypothesis class.
 Furthermore, by Theorem 4 , the right hand side in ( 6 ) is in turn bounded by 2 EG ( hinge ,  X  ,B ), and we see that the  X  X xtreme X  hypothesis class for the hinge-loss is the linear class. This can also be extended to the other commonly used losses referred to in Section 3.2 . Hence, even if we want misclassification error guaran-tees that focus only on the loss and are required to hold regardless of the hypothesis class, studying the linear class, i.e. EG ( ,  X  , B ), is often su cient. The main reason we focus our presentation on linear predictors is that learning linear predictors (possibly linear in a feature space, which includes kernel meth-ods) combined with convex losses is essentially the only situation that yields a convex optimization problem, which is one of our goals when using convex surro-gates. An important line of work focused on relating the ex-cess loss to the excess misclassification error, and in-troducing the notion of  X  X lassification calibrated X  loss functions ( Zhang , 2004 ; Bartlett et al. , 2006 ). A basic notion here is that of a loss being  X  X lassification cali-brated X , i.e. ensuring that zero excess loss (beyond the Bayes optimal) translates to zero excess misclassifica-tion error (beyond the Bayes optimal). This ensures that if we consider a class rich enough to include the Bayes optimal predictor then minimizing the expected loss indeed also minimizes the misclassification error. Classification calibration can be seen as an extreme point of the misclassification error guarantee ( EG )in two ways: First, for any convex loss , EG ( , 0) = 0 if and only if is classification calibrated (both are equivalent to the derivative at zero being defined and negative). Dis-cussing  X  = 0 corresponds to considering only the sep-arable case, which is in a sense the point of intersection of our study and that of Zhang ( 2004 ); Bartlett et al. ( 2006 ).
 Second, we can think of classification calibration as re-ferring to EG M ,where M is the set of all measurable functions. That is, a surrogate loss is classification calibrated if and only if EG M ( , nu )=  X  .
 Analyzing either EG ( , 0) or EG M ( ,  X  ) is not satis-factory as they don X  X  correspond to the agnostic learn-ing case where we are interested in doing as well as the best hypothesis in the function class of interest. Typically, classification calibration based results are used in conjunction with approximation theory to ar-gue that as the number of training samples increase one can consider richer and richer hypothesis classes, and hence eventually converge to the set of all measur-able functions M ,where EG M ( ,  X  ), and hence the notion of classification calibration, is relevant. How-ever, such an analysis typically only establishes asymp-totic behavior (unless strong assumptions are made). The analysis in this work neither needs to assume that data is linearly separable nor assume that the Bayes optimal predictor under the surrogate loss function is linear (with norm bounded by B ).
 For example, based on Zhang ( 2004 ), Rosasco et al. ( 2004 ) argue that the hinge loss (and also logistic loss) enjoy better rates than other losses like squared loss. However these results are also based on convergence to the Bayes optimal and so we either need to take very rich hypothesis classes or assume that the Bayes optimal predictor under surrogate loss is contained in the hypothesis class used. It is interesting to consider how misclassification error guarantee ( EG ) combines with estimation error rate. In practice, we get a finite training sample and when picking the hypothesis that minimizes some empirical loss, the estimation error involved in minimizing em-pirical objective, rather than the true expected objec-tive, comes into the picture. While choosing the loss one should take into account both the misclassification error guarantee ( EG ) associated with the loss and also the associated estimation error for the problem. For example, thinking of only estimation error, one might think that squared error is better as one might expect a1 /n rate where n is the sample size. However, EG for squared loss is large as we argued in Section 3.2 . For high dimensional cases, one can argue that hinge loss is the loss of choice even when we take estima-tion error into account. For the conservative update algorithm w.r.t. the hinge loss with its corresponding analysis by ( Shalev-Shwartz , 2007 ), or for the exact minimizer (which corresponds to the SVM) of empiri-cal hinge loss using results in ( Srebro et al. , 2010 ) (and noticing that hinge loss upper bounds a smooth ver-sion of margin loss which in turn upper bounds the zero-one loss) one can show that if b w n is the linear predictor returned by one of these algorithms, then, in expectation over training sample where  X  =inf w : k w k X  B R 01 ( w ). In order to com-pare this with the squared loss, we first note that the best misclassification error guarantee one can give for an algorithm that minimizes the expected squared loss is bounded by EG ( squared ,  X  ,B ) min {  X  ( B 1) 2 / 128 , 1 / 8 } . Further, when the dimensionality is large (compared to the sample size) then the estima-tion error rate for the squared loss (with linear pre-dictors) can be lower bounded by B 2 /n (see for in-stance ( Srebro et al. , 2010 )) and so the best guarantee that can be provided on the classification risk of es-timator obtained by minimizing squared loss scales is  X  ( B 1) 2 + B 2 /n . Comparing this with the upper bound in equation 7 shows that the hinge loss is qual-itatively superior (in the worst case) even if one takes into account the estimation error rates.
 A similar analysis can be repeated w.r.t. other losses where e  X  ectively hinge loss (and also logistic loss) can be shown to have qualitatively better performance than, for instance, squared loss or exponential loss, or any strongly convex loss. We would like to point out that the low-dimensional analysis requires a bit more care, as the estimation error for the squared loss might be much lower than for methods based on other loss functions. Proof of Theorem 3 . The distribution for this theorem is as follows:  X  There are  X  points at x = 1 labelled +1  X  There are points at x = M labelled 1  X  There are 1  X  points at x = M labelled +1 Although the data lies on the real line, we consider the example to be in R 2 . Now consider the classifier found when minimizing the convex surrogate loss M hinge . We aim to show that the vector w ? =(0 , 1) with w 0 = M is the optimal classifier. It has R ( w 2 . The margin loss of ( w ? , w ? 0 ) is clearly . Any other classifier that misclassifies fewer than points, must cross the x -axis.
 Case 1 Assume that we have a classifier that misclassifies fewer than points, and that this classifier intersects the x -axis between [ M, 0] and mislabels only (and all)  X  points at x = 1. Assume that it crosses at c 2 [ M, 0]. Assume also that it crosses the x -axis with some angle  X  .Thus, w =( sin (  X  ) , cos (  X  )) and w 0 = sin (  X  ) c .
 Consider the case where ( w ,w 0 ) is further than M away from the 1  X  points at x = M .Then, By taking the derivative, we can see this is increasing as  X  increases, thus taking  X  to be as small as possible while maintaining the M distance from the points at x = M is best. This gives, sin (  X  )= M M + c ,whichyields R ( w ,w 0 )= 1 M + c  X  (  X  (1 + M )+2 c ).
 Finally, taking the derivative with respect to c we find that if &lt;  X  1+ M 2 M then R ( w ,w 0 is minimized at c = M with a cost of R ( w ,w 0 )=  X  1+ M 2 M + . We do not need to consider similar classifiers that in-tersect the x -axis between (0 ,M ], as they will only increase the cost of points at x = 1 and add cost of points from x = M . This will never beat the classifier mentioned above when c = 0, which is already beaten by the classifier with c = M .
 Case 2 Now assume that we have a classifier that misclassi-fies fewer than points and that this classifier inter-sects the x -axis between [ 1+ M 2 , M ], and that the classifier mislabels only (and all) 1  X  points at x = M . Assume the classifier crosses the x -axis at po-sition c , and that it crosses with some angle  X  .Thus, w =( sin (  X  ) ,cos (  X  )) and w 0 = sin (  X  ) c . If the classifier is at least distance M from all of the points, then R ( w ,w 0 ) (1  X  ) 2 c c M ,whichis minimized when c is largest. Thus, R ( w ,w 0 ) 2(1 1+ M , which is one of our assumptions. So, for this case, ( w ,w 0 ) is not optimal.
 Finally, for this case, we have where the points at x =
M are within the margin of the classifier, and so they contribute to the -loss. Let c 0 = c M .This Using the derivative, we find that it is minimal when sin (  X  )= M 1 c 0 M .Thisyields R ( w ,w 0 )= derivative with respect to c 0 ,yieldstheminimumwhen c = 0 (which means c = M ). Therefore, the cost is R ( w ,w 0 )= 1 1 M ( 2 M +(1  X  )(1 + M )). Again, we find that this cost is larger than 2 when  X  &lt; M 1+ M Provided 1  X  &gt;  X  , we do not need to con-sider similar classifiers that intersect the x -axis be-tween [ 1 , 1+ M 2 ), as they always have higher cost. Remaining cases Any classifiers outside of those discussed in Case 1 and Case 2 misclassify at least points, as long as &lt; 1 2 . Therefore, since R ( w ? , w ? 0 )=2 we merely need that 2 &lt;  X  1+ M 2 M + to handle Case 1, which is true when &lt;  X  1+ M 2 M . This was our assumption.
 Finally, the EG (  X  ) found here applies to all hinge losses because M hinge ( x )= hinge ( M x ), and thus they are equivalent losses with respect to their er-ror guarantee. Thus what we have shown is that EG ( hinge ,  X  ,B ) for any such that  X  &lt; &lt; such  X  X  gives the final form of the bound.
 Proof of Theorem 4 . There exists an  X  ,with0 &lt;  X  such that the horizontal line above the x-axis, labelling all points as +1, has cost  X  (  X  )+(1 )  X  (  X  ) &lt; 1, as long as &lt; 1 2 , which holds for this theorem since For any convex function ,thereexistsa &gt; 0such that ( x ) hinge ( x ), for all x . Thus, any classifier ( w, w 0 ) that misclassifies fewer than points must sat-isfy R ( w, w 0 ) +  X  2  X  (1+ B ). This follows from the proof of Theorem 3 , because it is the minimum value of R than points.
 However, for any classifiers ( w, w 0 ) that misclassifies fewer than points, it is some distance c away from at least  X  points that it misclassifies, where c&gt; 0. This follows from our distribution having three groups of points: a group of size  X  , a group of size , and a group of size 1  X  . The smallest group size is  X  because &gt;  X  and 1 &gt; 2  X  and that ( w, w 0 ) must misclassify at least one of the groups of points. Therefore, R ( w, w 0 ) +  X  2  X  (1 + B )+  X  ( ( c ) Note that ( c ) hinge ( c ) 0. From here, we break the analysis into two cases. First, for all x  X  0, ( x )= hinge ( x ). Second, is where there exists x 0 &lt; 0 such that ( x 0 ) &gt; hinge ( x 0 ).
 For both cases, we will make use of the cost of a hor-izontal line above the x-axis that labels all points as +1. Recall from above that the cost of this classi-fier, which is (0 ,w 0 0 ) for some w 0 0 &gt; 0, is given by  X  (  X  )+(1 )  X  (  X  ) &lt; 1. Also, this inequality holds for any (  X  ). We refer to this classifier as h  X  and its risk is R ( h  X  ).
 Case 1: In this case, ( c ) hinge ( c ) = 0 for all c&gt; 0. Thus, we are back to R ( w, w 0 ) +  X  2  X  (1 + B ). However, we can simplify R ( h  X  ) in this case. Now, since R ( w, w 0 ) +  X  2  X  (1 + B ) for any convex loss function (  X  ), we can replace ( x )with 0 ( x )= ( kx ) for any k&gt; 0 and the same inequality must hold for 0 ( x ). Further, 0 ( x )= hinge ( kx ), for all x  X  0 and k&gt; 0. Recall that as x !1 , 0 ( x ) ! 0. As k !1 ,  X  0 ! 0. Finally, we have from Theorem 3 , that, for all k&gt; 0, R 0 This implies that there exists an  X  &gt; 0 such that large enough such that  X  0  X   X  . This implies that Therefore, there exists a k&gt; 0 such that for 0 ( x )= ( kx ), EG ( 0 ( x ) ,  X  ,B ) .
 Finally, because 0 ( x )= ( kx ), where k&gt; 0, they are equivalent losses. Therefore, EG ( ( x ) ,  X  ,B ) . Case 2: Consider loss 0 ( x )= ( kx ), where k&gt; 0 is large enough such that ( kc ) hinge ( kc ) &gt; 1  X  (1 + B ). This is possible since (  X  ) is convex and we assumed that for some x&lt; 0, ( x ) &gt; hinge ( x ). Therefore, R 0 ( w, w 0 ) &gt; 1. But we know that there exists  X  &gt; 0 such that (0 ,w 0 0 ), with w 0 0 &gt; 0(i.e. a horizontal line above the x-axis that labels all points as +1) has cost  X  0 (  X  )+(1 )  X  0 (  X  ) &lt; 1. Thus, for 0 (  X  ), EG ( 0 (  X  ) ,  X  ,B ) .
 Finally, because 0 ( x )= ( kx ), where k&gt; 0, they are equivalent losses. Therefore, if 0 &lt;  X  &lt; M M +1 = 1 then EG ( (  X  ) ,  X  ,B ) , for all such that  X  &lt; &lt;  X  (1 + B ) and 1 &gt; 2  X  .
 To conclude the proof note that as already mentioned, when  X  ( B + 1) 1 we anyway get that EG ( ,  X  , B ) 1 and what we showed in the proof is that when  X  ( B + 1) &lt; 1, then EG ( ,  X  , B ) for any s.t. &lt; which we conclude the proof. In this paper, we provide lower bounds on the best misclassification error achievable by algorithms min-imizing convex surrogate losses in terms of the M -margin error. Specifically, we show that the misclas-sification error rate of the linear predictor minimizing expected hinge loss is bounded by  X  ( B +1), where  X  is the bound on the M -margin error and B =1 /M .Fur-ther, by showing that when using linear predictors any algorithm minimizing any convex loss has a misclassi-fication error of at least  X  ( B +1) 2 , we conclude that the hinge loss is optimal up to factor 2. We also show lower bounds for specific convex losses and that any strongly convex loss has a qualitatively worse guarantee when compared to hinge loss. We argue that the analysis can be used to qualitatively compare convex surrogate losses used for binary classification, and show that the hinge loss is the loss of choice for classification prob-lems. The relationship of the misclassification error guarantee term, which we introduce in this paper, with the notion of classification calibration of loss function is also explored. Specifically, we show how classifica-tion calibration can be seen as arising from an extreme case of our misclassification error guarantee term. As an example of the implications of our results, we ar-gue that even when one takes estimation error rates into consideration, hinge loss is optimal up to con-stant factor (in the worst case sense, at least for high dimensional problems).

