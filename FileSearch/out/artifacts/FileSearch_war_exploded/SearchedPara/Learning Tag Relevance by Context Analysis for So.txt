 With the rapid growth of Internet and mobile devices , users are able to share social images more easily than before, and this le a d s to massive social images that are avai l-able online. Therefore, how to retrieval social images more efficiently and accurately has been an important research topic [1] . At present, the existing social image retrie v-al technology can be classified two categories , that is, content -based and annotation -based [2, 3] . Compare to the content -based approach, the annotation -based social image retrieval can supply better retrieval performance and more convenient user interface. H owever, this approach needs a lot of human -effort to annotate the images, which limits the application because of the high cost.

More rece ntly, with the development of Web, there are some image sharing we b-sites like Flickr , which allow users to upload im ages with some associated tags and these social tags can help solve the high -cost problem in annotation -based social i m-age retrieval to some extent [4, 5] . Nevertheless , compare d with the standard image annotations, the social image tags has some spec ific features and problems: 1) t agging accuracy, because the annotators are not experts, so inevitably there exists irreleva nt tag, weakly -relevant tag, misspelling tags and so on . Fig . 1 -A shows a social image of tiger with the associated tag set, obviously the tag  X  tiger  X  is the most relevant tag for this image, next  X  wild  X , and the other tags are either weakl y relevant or irrelevant at all; 2) t ag length and content, which means for the similar images, the length of tag sets and tag content may be very different. For example, when compar ing Fig . 1 -A with Fig . 1 -B , we can see although these two images are similar, both the length s and content s of these two tag sets are dissimilar . These characteristics make it difficult to apply traditional text retrieval model to image retrieval based on social tags. It can be also observed from Fig . 1 that for the query  X  tiger  X  , if we utilize the classical tf -idf as the retrieval model, the final rank list will be &lt;C, B, A&gt; , which means for all the s o-cial image tag sets with  X  tiger  X , the less tags the image contains, the better rank the social image ha s, obviously this do esn X  X  meet the actual situ ation. For example, al t-hough Fig . 1 -C has the least tags, it is the least relevant image for  X  tiger  X .
To solve all these problem s above , the researchers have done a lot of related r e-search work. Overall, these work mainly concerns with two aspects of tag set revise and tag ranking. Tag set revise contains tag enrichment and tag refinement, for exa m-ple, Qian et al . built a graph for each image with its initial tags, and implemented the tag enrichment by a graph -cut approach [6] ; and Xu et al . proposed to do tag refin e-ment from the perspective of topic modeling and presented a graphical model called regularized Latent Dirichlet Allocation to expl oit both the statistics of tags and visual affinities of images in the corpus [7] . Xia et al . and Sang et al . use concept ontology and user information to do the tag refinement [8, 9] . However, these approaches have the disadvantages that the noisy information may be introduced to tag set or remove useful information from tag set. And for tag ranking, which means rank the tag a c-cording their relevance to the image, for example, [10] is the first paper aiming to attack the tag ranking problem, they es timated initial relevance scores through prob a-bility density estimation, and then refine d the initial scores by performing a random walk algori thm over a tag similarity graph. Zhuang et al . propose d a novel two -view learning approach to compute the tag rel evance using both textual and visual contents of social images, and they map ped the learning into an optimization task and presen t-ed an efficient algorithm to solve it [11] . Xiao et al . also use d both semantic and vis u-al information to get the tag -tag simi larity matrix, and then discover ed the signif i-cance of each tag [12] . The exist ing tag ranking methods focus ed on the inner rank ing in the tag sets, while for social image retrieval the emphasis should be put on the global comparison for the same query tag in different image tag sets . These methods only use d rough ranking information f or image retrieval . O bviously they ignore d some useful information which can be used to improve retrieval performance.

Based on these observations above, a novel scheme is dev eloped in this paper for facilitating social image retrieval by learning the relevance of the image tags. In our approach, a multi -model association network is constructed to model the relationship between tags; then we build a context -analysis model to co mpute the tag relevance and finally refine it using an approximate random -walk algorithm. Our scheme signi f-icantly differs from other work in: 1. our approach assume that a tag has high relevance when it has close relationship with other tags in the same ima ge, and we make this assumption from the inspir a-tion of the user tagging process for social images, generally users are less likely to tag two close -related tags as noises simultaneously, in other words, if user chose a tag for an image, the higher related this tag with other tags, the higher relevance this tag for the image. Our model well embodies this characteristic. 2. The relevance of tag can be compared globally, which means we use richer info r-mation to improve the performance of i mage retrieval and o ur experiment s on a large number of public data from Flickr have obtained very positive results . 3. For multi -tag query, our model is more robu st than other tag ranking approache s using only ranking information. For tags in the same tag set of a social image, the tag relevance is determined by the related context, that is, the relationship between a tag and the other tags in the same tag set and the relationship between a tag and the associated image. For the tag -tag inter -related relationship, we consider two aspects of the co -occurrence frequency and the visual similarity, because if two tags often co -occur in the same image tag set, or the visual feature of two tags are similar, we may infer the relationship betwee n these two tags is close. For the tag -image inter -related relationship, we consider the visual similarity between the tag and the associated image. Thus a multi -model association network is utilized as an important knowledge source for tag relevance measu rement.
Our multimodal association network consists of two key components, i.e., the co -occurrence association network and the visual association network. As shown in Fig . 2, the left part is our co -occurrence association network, in which to better express what we mean, not all the tags are listed in the figure. Taking the tag  X  tiger  X  as an example, { X  bite  X ,  X  cage  X ,  X  grass  X ,  X  zoo  X  ...} are all the tags co -occurring with  X  tiger  X  in at least one image, and all these tags and their neighbors constitute the entire ass o-ciation network. This network is formalized with a four -tuple &lt;T, E, W t , W e &gt; , where T represents a tag set that contains all the tags occur in the image database , and each tag can be seen as a node on the network, E represents a set of edges, which means if two tags co -occur in at least one image, we assume that there exist an edge between these two tag nodes. W t r epresents the weight of the tag nod e t i n T, and i ts corr e-sponding value is set as the occurrence times of th e t ag t . S ince each tag usually o c-curs only once in one image, th is weight can be explain ed as the number of images which contain the tag t, as described in Formula (1) . W e re present s the weight of the ed ge e in E , and its corresponding value can be set as the occurrence time s of the ed ge e o r the tag -tag pair . This weight can also be explain ed as the number of images that contain both of these two tags, as shown in Formula (2) .
T he right pa rt of Fig . 2 is our visual association network, which has the same stru c-ture with the co -occurrence association network, that is, t hese two networks have the same tag nodes and edges. The four -tuple &lt;T X , E X , W X  t , W X  e &gt; is aslo used to represent the visual association network, where T X  and E X  ha ve the same meaning as in the co -occurrence association network and represent the tag set and the tag -tag pair set r e-spectively ; the value of W X  t i s set as the visual feature vector of the t ag t . which is calculted by using the mean histogram information of the tag ; and the value of the W X  t is calculated by Formula (3) . w here HIST(img) is the histogram feature of the image img ; and IMG(t ) is the image set , in which each image has the tag t . Here we d o n o t use the complex visual feature , because the images from Flickr website are flexible and complex and the complex visual features may not be sui table . T he value of W X  e is the Euclidean d istance b e-tween two tags with the e d ge e, whic h is calculated by Formiula (4) . w here the function EU_DIS returns the Euclidean d istance between the visual feature vectors of two tag s of t i and t j . Our tag relevance measure model consists of two step s, first ly an initial relevance score is obtained through the context analysis for each tag on the basis of our mult i-mod a l association network, and then a random -walk algorithm is adopted to make a refinement for the initial score of tag relevance. 3.1 Initial Measure based on Context Analysis When we try to understa nd a certain word in a text, except for the original meaning of th is word, the word  X  s context is often utilized to help us and play s an important role for our understanding. Similarly, when we try to measure the tag relevance, the co n-text around the tag can be utilized . F or a given tag, the context include s the other tags in the same tag set for the corresponding social image and the image itself . Conside r-ing the first image in Fig . 1 as an example, the context of the tag  X  tiger  X  is shown in Fig . 3, thus we can calculate the initial tag relevance according to the context analysis . Here the  X  object tag  X  and  X  context tags  X  are used to represent the object tag and other tags in the same tag set for an image. F irstly, for the relationship between the object t ag and the context tags, we assume that each tag of the context tags can provide its own contribut ion s for the relevance of the object tag, and the contribut ion can be measured from two aspects of the co -occurrence frequency and the visual similarity . I f two tags often co -occur in the same image, it means that users tend to annotate these two tags simultaneously for the same image. G enerally users won X  X  annotate two close -related tags as noise s simultaneously , and in fact i f the object tag has the higher co -occurrence frequency with the context tags, its relevance will be higher . F or the object tag otag , the contribut ion of a tag t in the context tags can be calculated by Formlual (5).
 w here W otag and W e are defined in our co -occurrence association network, and can be calculated by Formulae (1) and (2). Since the length s of different  X  context tags  X  are different, we set a normalized weight for each tag in the context tags to avoid the influence of the length, and the value of weight can be calculated by Formula (6). T hus we can measure the relevance of the object tag from the view of co -occurrence association by Formula (7). S imilarly, we measure the contribut ion of context tags from the view of visual association in a similar manner . S omething different is that we replace the co -occurrence property with the visual similarity, and the e xponential function of the Euclidean distance is used to represent the visual similarity, as shown in Formula (8) . where img means the corresponding image to the object tag. Expect the relationship between the object tag and context tags, the relevance of object tag is also affected by the relationship with the image itself , and the visual similarity between them c an be calculated by Formula (9). where EU _ DIS is the Euclidean distance between the object tag and the image. Since Formulae (8) and (9) are all about the visual simi larity, for simplify ing our model we combine Formulae (8) and (9) as Formula (10). is f ( otag,img ) .  X  Weight ( t, otag ) W e ( t, otag ) t ontextTags . E ( tag,img ) F inally we combine Formulae (7) and (10) and get the final initial relevance measure for the object tag , as shown in Formula (11). where  X  is a weight parameter ranging from to 1. 3.2 Tag Relevance Refinement Through Random Walk After acquiring the initial relevance score for each tag in the image annotation , we make a further refinement for the tag relevance using an approximate random -walk algorithm. Here, w e refer the method implemen ted in [10] , and accomplish some change in our work for the purpose of getting globally -comparable tag relevance. First, we see the tag set in each image as a full -connected graph, and the tag is the node in the graph , then a random -walk process is run on the graph based on the sim i-larity between the tags, and finally more refined tag relevance can be obtained . We use the exponential function of the Google Distance [13] to compute the similarity between tags, as shown in Formula (12) . whe re G is t he total image numbers in the database. Based on this similarity measure , we can give the transition probabilities used in the process of random walk, as shown in Formula (13) . w her e p ij is the transition probability from the tag t i to the tag t j ; k is the total number of the neighbor tags of the tag t i . Hence this formula can be seen a normalization pr o-cess. We th e n run the random walk process on each image individually, as shown in Formula (14). wh ere r k (j) represent s the relevance of the tag t j in the k th iteration ; p ij is the transition probability ; V j is the initial relevance of the tag t j , and m is the total number of context tags for tag t j . T he reasons for using parameter m is that we want to exclude the i m-pact of the tag set length, and it is fair to the images have different tag set length. For the same reason, we don X  X  consider the convergence of Formula (13), and just co n-ducted a limited number of iterations . A nother reason for t his is that our initial rel e-vance is relatively accurate, thus just a small number of iterations are needed to refine the relevance . O n the contrary, too much iteration will reduce the accuracy. 4.1 Dataset and Query Definition Our data set is established based on MIR Flickr Data constructed by [14] and further development by [15] which have been used in ImageCLEF . It consists of 1,0 00,000 annotated images collected from Flickr with unconstrained contents , and these images have all the original user annotations. In our experiment, we use a subset dataset with about 200,000 images for retrieval just as in ImageCLEF 2012 , which contains about 37863 tags after filtration. W e choose 26 queries as the test query in our experiment, as shown in table 1 , these queries cover various categories like Animal, Building, Human, Event, Object, and so on. Except 20 single -word queries, we also introduce 6 multi -word queries to see the performance of our model on different kinds of queries. As fo r ground truth collection, due to the large collection of 200 thousand images, we don X  X  obtain all the ground truth relevance assessments for each query. W e first get the retrieval results for each query using all the models we use in our experiment, and then we form a pool for each query by aggregating all these returned images and o b-tained the relevance assessments by annotating manually. Each image was labeled  X  X  elevant X  or  X  X rrelevant X  , which we then used to evaluate the results of each model. 4.2 E valuation Metrics and Model Selection In our work, we choose two evaluation m etrics to assess the performance of social image retrieval via more effective tag relevance measurement.  X  AP@X : T his evaluation measure the average precision when a certain n umber of images are encountered. I n our work, we use AP@5 , AP@10, AP@50, AP@100 respectively to measure the experiment results.  X  MAP : H ere we focus on the non -interpolated MAP, that is, each time a relevant image is encountered from top to bottom, we compute the precision, and finally get the mean of these precis ion values. 
For the model selection, there are two parameters we need to set:  X  in Formula (12) and the iteration numbers for Formula (13), When we set  X  .7 and iter , the best retrieval performance can be acquired, as shown in Fig . 4 . 4.3 Experimental Result To investigate the effect of each part on the whole retrieval performance, we intr o-duce four evaluation patterns .  X  TEXT : use only textual information, which means we set the parameter  X  = 1 in
Formula (11).  X  VISUAL : using only the visual information, which means we set the parameter  X  = 0 in Formula (11).  X  TEXT+VISUAL : using both textual and visual information, which means we choose the optimal value of  X  . As Fig . 4 shows, here we set  X  = 0.7 .  X  TEXT+VISUAL+RANDOM : Making further refinement by using our random -walk algorithm based on the result of Formula (11).

Fig . 5 shows the result s of above four patterns. It can be seen that the performance is 4) &gt; 3) &gt; 1) &gt; 2) and this result can illustrate that each part in our model can co n-tr ibute for the final per formance. W e can find that the performance is poor by only using the visual information. However, when combining with the text ual information, the result is much better than only using the text ual information. It can be also o b-served that the random -walk p rocess is able to further improve the final result.
To give full exhibition to the superiority of our alignment model, we have also pe r-formed a comparison between our and the other classical approaches in recent years. S ince our work is mainly for social image re trieval, we choose two categories of existing approache s as our comparative experiments: one is traditional text retrieval model for annotation -based image retrieval , the other is tag ranking approaches for social image retrieval. For the first category, w e choose two retrieval model, one is a language model implemented in [16] , the other is a information -based model implemented in [17] . As for the tag ranking method, we also choose two methods, one is the tag ranking model in [ 10 ] , for the other method, we combi n e our method with the method in [ 10 ] .  X  Language Model (LM): T he tags for each image can be viewed as a document, the basic idea is to estimate a language model for each document, and documents are rank ed by the likelihood of the query according to the estimated language model.  X  Information -Based Model (IBM): T his model is based on a hypothesis that the significance of a tag in the document can benefit from the difference of the behaviors of this tag at the document and collection levels.  X  Tag Ranking Model (TRM): T his model first get s local relevance scores using the probability density estimation and random walk algorithm and rank s the tags using these scores, and then use the rank information to get the gl obal relevance for image retrieval by F o rmula (15) . where rank t denotes the rank value of tag t , and N t denotes the number of tags in the corresponding tag set .  X  Joint Model (JM) : W e first get the tag relevance by using our method, however, just use these relevance value s to rank the tags and then refer Formula (15) to compute the new relevance . W e want to show the advantage of our approach in using more rich information to improve the retrieval performance.

In addition, method 3 and 4 both use the vector space model as the retrieval model, just as our approach. This means for single -word queries, we just rank the images according to the relevance of the word; for multi -word queries, we use the sum of the relevances to rank the images. Since the work in [ Liu et al., 2009 ] use d only single -word queries, for fairness, we first use the twenty single -wo rd queries in T able 1 in our experiment, and the final results can be seen i n Table 2 . W e can see that our approach outperforms other approaches obviously, and the comparison result between JM and our approach confirm s the global comparability of our relevance and the advantage of our model in capturing the appropriate information to compu te the relevance. W e then implement the experiments on each query in Table 1 . For the purpose of clarity, we just list the results of LM, TRM an d our approach in Fig . 7, here we choose AP@10 as our evaluation metrics since we us u ally pay attention to the top images of the return list. It can be seen that for single -word queries, our approach outperforms other approaches obviously, and o ur approach  X  s performance is the best for almost all the queries. As for multi -word queries, our approach is more robust than tag ranking method TRM and has roughly the same results with LM . the reason for this is probably because that the presence of all query word s in a tagged image largely guarantees the high relevance for the image, and the poor peformance of TRM once again verified the coarse of the rank information for im age retrieval. At last, we show the top 5 retrieval results of these m odels in Fig . 6 for the query  X  lion  X ,  X  sheep  X  . It can be see n that generally our approach can exactly return the better retrieval results. In this paper, we build a relevance learning model for the tags associated with the social images, and use the tag relevance to improve the performance of image retrieval. Our model consider s the user tagging habits and try to compute the tag relevance by ana lyzing the context of the tag. For computing the relevance, a multi -model association network is constructed as our knowledge source and a random -walk algorithm is adopted to refine the relevance. The experiment results demonstrate that our approach can ma ke use of rich information to compute the relevance and our tag relevance can help improving the retrieval performance significantly. For the future work, we will put more focus on multi -word queries, since different query words are not independent, simply consider the sum of the relevances of the query words are not enough, so our future work will consider the semantic relationships between them to further improve the performance.
 Acknowledgments. This work is supported by National Science &amp; Technology Pi llar Program of China (No. 2012BAH59F04 ) , National Natural Science Fund of China (No. 61170095 ; No. 71171126 ), and Shanghai Municipal R&amp;D Foundation (No. 12dz1500203, 12511505300) .

