 Signal and Image Processing Laboratory, Electronics and Computer Science Faculty, University of Science and Technology of Houari Boumedienne, Algiers, Algeria School of Computing, Engineering and Information Sciences, Northumbria University, Newcastle Upon Tyne, UK 1. Introduction
For high dimensional multivariate data, density estimation has received less attention than regression and classification. Nevertheless, there are many complex problems that can be efficiently solved if the sampling and the estimation of the density function of the high dimensional multivariate data are carried out with high accuracies.
 Nonparametric density estimation methods such as kernel estimator, also commonly referred to as the Parzen window estimator [35], work well for low dimensional problems (dim &lt; 4), but are not effective for high dimensional ones. The main difficulty is due to the intractable computational cost of the cross validation when the bandwidths need to be selected for each dimension [16]. On the other hand, finite-mixture approaches can be used for density estimation if only a few efficient statistical estimators and mixing weights are required with respect that the assumed model is correct.

Attempts to achieve High Dimens ion Density Esti mation (HDDE) require pe rforming a dimension-ality reduction in an explicit or an implicit manne r based on some assumptions. Linear methods, such as Principal Component Analysis (PCA) [40], project all data points onto an uncorrelated lower di-mensional hyper-plane and therefore are inappropriate to describe nonlinear data. Suzan et al. present a nonlinear approach which utilizes Diffusion Map (DM) technique in order to project the data onto a lower space [7]. More recent methods attempt to exploit the sparsity, which is omnipresent in high dimensional data, with the aim of reducing the complexity.

Sparsity is a generic concept that can be viewed in several ways. For example, Lui et al. [28] assumed that only a small subset of the variables is involv ed in the nonlinear comple x variations in HDDE. With the aim of reducing the computational cost when using a Parzen window estimator, Girolami et al. [16] proposed a Reduced Set Density Estimator (RSDE) which provides a sparse representation and reduces the estimation computational cost. In this paper, we suggest to exploit the  X  X tructural sparsity X  in the sense that the observed variables are not completely conditionally and statistically dependent. In other words, this means that, formally the set of variables contains many disjointed subsets X , Y , Z such as X is statistically independent to Y given Z ,i.e. X  X  Y/Z .

The framework of Bayesian Networks (BNs), which were introduced in 1988 by Pearl [36], is geared towards learning such a structure of sparsity and relies on a directed graph structure that encodes the conditional independencies. The conditional independencies are assumed to be invariant in the appli-cation domain. BNs are based on the Markov property which claims that each variable in the graph is independent to its non-descendent variables given the value of their direct parents. The application of the Junction Tree (JT) algorithm [23,26,30] on the BN graph results in a decomposition of the whole joint the BN). The local densities obtained are mainly characterized by their relative low dimensions and also by the full dependency among the variables within; thus allowing us to use standard estimation methods. In this paper, we propose a new method for the local density estimation using Independent Component Analysis learning-based methods.
 Independent Component Analysis (ICA) [21] is an unsupervised technique conceived initially for Blind Source Separation (BSS) problems where the goal is to recover mutually independent signals. Unlike the conventional ICA, which uses a linear model for the global description of the data, Karhunen et al. [25] proposed a new structure for general nonlinear distributions where local ICA models are used in conjunction with a suitable clustering algorithm of the data. The clustering part is responsible for taking into account the nonlinearity, while a linear ICA model of each cluster is used for describing local features of the data. It seems obvious that spherical clusters, such as in k-means, are inappropriate for local ICA, but rather clusters having a high likelihood to be derived from linear ICA models are more suitable. Clustering is a delicate task; it influences severely the results and it is known to be an ill-posed problem. Therefore, fuzzy clustering methods, which are characterized by a high degree of freedom, may be more convenient than hard clustering for describing the data. In this context, Honda et al. [18 X 20] proposed a local ICA method using Fuzzy c-Va rieties (FCV) that consists of partitioning the data into linear c-varieties clusters.
 For a multivariate density estimation purpose, we propose to use local ICA method based on the fuzzy Gustafson Kessel (GK) [17] algorithm. The GK is a generalization of the Fuzzy C-Means (FCM) [12] algorithm and, unlike FCM, it possesses the ability to find elliptical clusters. We have chosen to inves-tigate the GK algorithm in order to exploit the idea that linear ICA mixture increases the Gaussianty. Therefore, elliptical shapes may be promising for representing data with distributions that tend towards Gaussian ones.

The remainder of this paper is organized as follows. In the next section, a discussion of some related works is given. In Section 3, we briefly consider the linear ICA case where we introduce a framework for the multivariate density estimation for the data that can be approximated by a linear ICA mixture. Sec-tion 4 gives a generalization of the estimation framework for the nonlinear data using local ICA method based on fuzzy clustering, where we present the technique used for the connection between the defuzzifi-cation process and the linear ICA inputs. In Section 5, we point out two specific weaknesses of the model described in Section 4 in the case of spaces with high dimensionality. First, even with the presence of the structural sparsity, ICA performances decrease in terms of accuracy and computational complexity. Second, clustering algorithms suffer from some drawbacks in high dimensional spaces. In order to ex-ploit the structural sparsity, we propose in Section 6 to learn the BN structure of the observed variables with the aim of decomposing the joint density, then applying ICA and fuzzy clustering efficiently on small subsets of variables. Section 7 discusses the experiments carried out on artificial datasets drawn from known distributions for density estimation where the proposed method is compared to the Gaussian Mixture Models (GMM) estimation method based on Expectation Maximization (EM) algorithm. The proposed approach is also used for solving the supervised classification problem. The results obtained including a comparative study with the known classifiers such as Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) clearly demonstrate the usefulness of the proposed method. Conclusions of the paper are given in Section 8. 2. Related work
Graphical models as Markov networks and BNs aim to represent densities function in a graphical way where the conditional independence among variables is the key aspect. In many real problems, a variable might closely interact with just a few other variables, and affect the remaining variables indirectly. Markov properties in graphical models [36] lead to the factorization of the whole joint density in order to reduce the dimensionality of density functions. A rich discussion in given by Deane [9] about the application of graphical models for dimension reduction in HDDE.

Ammar et al. [2] proposed mixtures of tree-structured Bayesian networks and Schnitzler et al. [39] proposed mixtures of bagged Markov trees for the HDDE purpose. In both models, the skeleton of each network within the mixture is restricted to a tree (a variable having only one parent). The tree graph structure is chosen in order to allow the factorization into small conditional densities.
ICA model has nowadays many applications beyond the traditional BSS problem. Let us briefly review some salient connections between ICA and BNs. ICA attempts to recover independent sources that explain the dependencies between the observed variables. The sparsity of the mixing ICA matrix, the number and location of zeros cells, corresponds to the graph structure in BNs. The values of the non-zeros cells in the mixing matrix and the univariate densities of the independent components correspond ICA models which can be useful for graph structure learning in BNs.

The linear ICA form is limited to a restricted class of multivariate distributions and therefore it can-not be used for density estimation, but rather nonlinear ICA is more suitable due to their capacity of generalization. Karhunen et al. [25] proposed a nonlinear ICA basis for data representation where linear ICA models are applied on local clusters. Palmieri et al. [33] proposed a generalized ICA framework in the form of a multi-layer perceptron as a density estimator. We used local ICA method in this work for density estimation; both clustering algorithms and lineal ICA models deteriorate with dimensionality increasing and this is the reason why we used BNs with conjunction of local ICA in order to reduce the dimensionality. 3. Density estimation for linear mixture ICA model
Multivariate density distributions aim to describe existing conditional distributions between the marginal distributions. Thus, studying the statistical dependencies among the observed variables is one of the main approaches used for multivariate density estimation. Statistical dependencies are usually explained by the existence of physical causal relationships which are not always restricted among the observed variables only, but they often involve latent variables. According to Reichenbach X  X  Common Cause Principle (PCC), in its simplified statement form, if two random variables A and B are statistically dependent, then either A causes B , B causes A or A and B are the joint effects of common causes Z [38]. still used to handle theories in probabilistic models as discussed in [3].

Let U , V be two variables such that U causes V ,theeffect V can be expressed as a mathematical function of the cause U , V = F ( U ) [42]. Hence, we can formulate the three causality cases of PCC by a function, [ A, B ] T = F ( C ) ,where C may include either A or B , union the other common causes Z ,(see Fig. 1). In most real situations, the main difficulty results from the complexity of the causal function F which is generally nonlinear and involves a lot of latent causes Z  X  C . In this section, we first consider the case where F can be simulated by a linear mixture of a set of independent variables.
Let X =( X 1 ,X 2 ,...,X K ) T be a K -dimensional random vector where the variables X i  X  X are continuous with one at most allowed to be Gaussian. First, we consider the case where the multivariate variable X is effectively derived from, or can be approximated by, an instantaneous linear and non-a set of Independent Components (ICs). For the sake of simplicity, we consider the number of variables in S to be equal to K .Thevariables S i  X  S can be viewed as common causes which quantify the statistical dependencies among the observed variables X . For example, S can represent the sources in the BSS problem [21].

Since the transformation X = A  X  S is continuous, it has continuous partial derivatives and defines a one-to-one mapping, so that the joint density distribution of X can be simplified using the well-known Jacobian formulation as follows: where Det ( Jac ( X )) = Det ( Jac ( A  X  S )) is the Jacobian of the transformation, defined as: and A i  X  S is the row i of the mixture matrix A multiplied by the column vector S , thus the following equation is obtained: Using the independence assumption of the independent components S , the joint density P S ( S ) can be factorized into a product Therefore, the estimation of the joint P X can obtained by learning the ICA mixture matrix A and by estimation of the univariate densities P S i ,i =1 ...K . 4. Density estimation for nonlinear mixture ICA model 4.1. Nonlinear ICA method
In the conventional ICA method, a linear mixture model is used for the representation of data. Al-though, it produces significant results in many applications, it might be inappropriate for more general data distributions. Usually, the observed variables nonlinearly depends on their causes. As explained in the previous section, the observed vector X results from a nonlinear mixture function of causes C that may include a part of variables within X and other eventual hidden causes. Our goal in this work is not to recover the real causal variables as is required with sources in the BSS problem, but rather to quantify the dependency relationships among the observed variables in order to construct a density estimator. Therefore, we propose to approximate the observed vector X by an instantaneous mixture as follows: of a set of independent components S =[ S 1 ,...,S K ] ,where G : R K  X  R K is an unknown nonlinear mixture function which can be estimated using an adequate nonlinear ICA method. It is shown that the nonlinear ICA problem is highly non-unique [22]. Consequently, additional assumptions such as external criteria or prior information are incorporated within the optimization of the nonlinear ICA model in order that it converges to unique solutions [18,19]. This noted drawback does not represent a to find a representation as can be seen in Eq. (6) of X where the independence between the estimated components S is as strong as possible.

The most commonly nonlinear ICA methods treat the nonlinearity into two different ways: neuronal networks approach [1,13], or a combination of a set of linear ICA models [18,19]. The second approach firstly consists in partitioning data into clusters in such a way that each cluster should have a high likelihood to be approximated by a linear ICA model. Then, a linear ICA model is used to describe the local features; such nonlinear methods are referred to as Local ICA methods [18 X 20,25]. We have opted for the second alternative to avoid the difficulties related to nonlinear ICA. Moreover, extending the density estimation model shown in Eq. (4) to the general case is less complicated. 4.2. Local ICA
Local ICA method is a nonlinear ICA method which attempts to partition the observed data vectors The clustering part is responsible for dealing with the nonlinear mixtures and therefore it is crucial for the other nonlinear ICA approach. Indeed, the clustering depends on the initial parameters and it is clear that changes in the clusters affect the independent components calculus [25].

Maximization of the non-Gaussianty is used successfully by the Fast ICA [21] linear method where the foundation relies on the central limit theorem; each observed variable X i  X  X converges to a Gaussian distribution. However, there is no guaranty that the joint variable X =( X 1 ,X 2 ,...,X k ) converges to a multivariate Gaussian distribution because the observed marginal variables X i ( i =1 ,...,K ) are not statistically independent; they are supposed to be different mixtures of the same ICs. Moreover, in practical situations, it is expected that the observed data vectors will often compact and usually scatter into spherical and elliptical shapes.

Kurhunen et al. proposed in [25] local ICA models for use with a suitable clustering algorithm such as the K-means algorithm using non-Euclidean Minkowski distances and the Self-Organizing Map (SOM) algorithm. Locally, the observed data is assumed to be a linear mixture of independent components. Therefore, clustering algorithms that partition the data into some spherical clusters like K-means are not suitable for the extraction of lo cal independent components, but rat her the data should be partitioned et al. enhanced the Fast ICA algorithm to Fuzzy Fast ICA by integrating the Fuzzy c-Varieties (FCV) [6] clustering, which is able to partition data into linear clusters using linear varieties as prototypes. The FCV algorithm detects perfectly line shape clusters in dimension 2 and hyper-planes shapes in higher dimensional spaces. However, for local ICA models, one needs to find clusters that have a high likelihood to be approximated by linear ICA models.

In this work, we propose to study the fuzzy Gustafson-Kessel (GK) clustering algorithm on a set of real databases. The GK algorithm provides clusters having elliptical shapes. GK algorithm has a high Therefore, GK algorithm appears to be more interesting to approximate the observed data with linear ICA models. 4.3. Fuzzy clustering
Data clustering is the process of dividing data elements into classes so that the vectors in the same class are as similar as possible while the vectors belonging to different classes are as dissimilar as possible. Clustering algorithms can be classified into two approaches: crisp and fuzzy clustering (also referred to as hard and soft, respectively). Boundaries between clusters in hard clustering are fully defined such that each vector is assigned to only one cluster while vectors in fuzzy clustering belong to all clusters with different membership degrees. In many real life cases, the boundaries between natural clusters may be overlapping, so that some vectors may belong to more than one cluster. In such cases, fuzzy clustering may be useful to classify this kind of patterns. In Local ICA method, for obtaining a reliable representa-tion, the number of data vectors falling into each cluster should be large enough [25]. This noted problem is lessened with fuzzy clustering algorithms because each cluster involves different membership levels for all the vectors.
 and let 2 &lt;L&lt;N be an integer (cluster X  X  number), the fuzzy partitioning space for X is the set
The i t h row of U contains values of the membership function of the observation vector x i which constrains the sum of each row to 1. 4.3.1. Fuzzy c-means clustering algorithm
Fuzzy c-means (FCM) algorithm, which is one of the most widely used fuzzy clustering methods, was developed by Dunn [12] and improved by Bezdek [6]. FCM is based on the minimization of the following objective function: R
N is a vector of the clusters centers, which have to be determined, and  X  A =  X  T A X  is the norm based on the matrix A expressing the similarity between observations and the centers 4.3.2. Gustafson-Kessel clustering algorithm
The Gustafson-Kessel algorithm is an extension of the FCM algorithm and uses an adaptive distance norm in order to find clusters of different geometrical shapes [17]. Therefore, each cluster has its own matrix norm A j which produces the following inner product distance norm: The matrices A j are considered as variables in the optimization functional in Eq. (10), thus allowing each be the norm inducing matrices of clusters, then the objective functional in the GK algorithm is as follows: In this work we have used the numerically robust implementation of the GK algorithm described in [4]. Figures 2 and 3 show a 2-dimensional clustering example of the GK and the FCM algorithms, the number of clusters L is fixed to 4. 4.3.3. Validity measure for fuzzy clustering
Finding the optimal number of clusters is known to be a hard problem and until now there are no exact reliable methods to address this issue. In this work, we have adopted the Xie and Beni index ( XB ) proposed in [44] for the fuzzy clustering validation. XB relies on a fuzzy clustering validity function which measures the overall compactness and septation of a fuzzy partition. Let  X  is called the compactness of the fuzzy partition of a dataset. The more compact the clusters are, the smallest  X  is. Let d min = min i,j v i  X  v j 2 i = j :1 ..L be the minimum distance between cluster centroids. A larger d min indicates that all the clusters are separated.

The ratio of compactness to separation XB =  X /d min indicates how much clusters are overall compact and separate to each other. The goal is to find a fuzzy partition with the smallest ratio XB . XB can be writhen as The clustering algorithm is run for different values of the number of clusters L and the best solution having the smallest ratio XB is retained. In our experiments, we have investigated the value range L =2 .. 15 . The clusters obtained may not correspond to  X  X hysical X  clusters, we are only interested to find a partition of few but well separated clusters such that the overall intra-cluster variation is lower.
We have used the XB measure (11) for FCM and GK validation, the only difference being that the matrix norm A is identical for all clusters in FCM and it is specific to each cluster in GK. 4.4. Proposed method for nonlinear case
In order to deal with nonlinearity of distributed data better than when using a single global ICA model, we propose to employ fuzzy local ICA models. Our method consists of the following steps: 1. Group the observed data vectors into L more homogenous clusters using fuzzy clustering methods; 2. Construct the fuzzy clusters where the defuzzification process is performed using a duplication 3. Subtract the cluster mean vector from vectors belonging to each cluster; 4. Estimate linear ICA separating matrix W for each zero-mean cluster using a suitable ICA algo-5. Estimate the univariate density distribution for each independent component using the standard For some linear ICA algorithms, the fuzziness membership can be integrated in the objective function. The fuzzy Fast ICA algorithm [20] is an excellent illustrating algorithm following this approach. Assume u ic to be the membership of a data vector x i in the c th fuzzy cluster, Honda et al. [20] defined the fuzzy Kurtosis and denoted the ICA objective function as: The integration of the fuzziness membership at the objective function level may be so complex for most available ICA algorithms. In this work, we aim to design a density estimation framework which allows us to test different ICA algorithms without any restrictions. Therefore, to apply ICA to fuzzy partitioned data, we propose the following steps: 1. Depending on the number of clusters L , we choose a sufficient duplication number D , (for example 2. Each fuzzy cluster c will contain the integer round number of the value u ic  X  D copies of the vector Compared against the first approach, the proposed method implies an additional time complexity related to the duplication process and also a loss of information due to the discretization of the value u ic  X  D . However, it has the advantage to be simple and it can be used with all the available ICA algorithms.
Let X =( X 1 ,X 2 ,...,X K ) be a K -dimensional variable where its observed vectors are partitioned into L fuzzy clusters. The joint density for the nonlinear case can be computed by marginalization over clusters C j as follows: By using Bayes X  X  theorem, we obtain where P C ( C j ) is the prior probability of the cluster C j . Hence, Eq. (4) can be rewritten as: where A j is the mixing matrix and S i,j , i =1 ,...,K are the independent components of the linear ICA model associated to cluster C j . 5. Local ICA, high dimension and sparsity 5.1. ICA and high dimension
The performance of the conventional ICA methods tends to deteriorate significantly with different speeds, depending on the characteristics of the method, especially when the number of sources is too large. At the same time, the complexity increases importantly.

Several experimental works studying the decreasing ability of ICA methods to recover the original sources in high-dimensional spaces have been carried out. A comparison between some ICA algorithms on super-Gaussian sources has shown that the separation error increases with an increase of the number of sources [15]. An experimental separation comparison on Rayleigh distributed sources have been car-ried out between the Pearson-ICA and JADE ICA algorithms and the results show a diminution of the Signal-to-Noise Ratio (SNR) for high dimensions [24]. It has also been shown in [29] that the SNR of both SHIBBS/SJAD ICA algorithm based on the joint approximate diagonalization and FastICA algo-rithm decreases exponentially as the number of sources increases where half of the considered sources are uniformly distributed and the other half are super-Gaussian. 5.2. Clustering and high dimension
Many applications such as physics, medicine, and bioinformatics have led to a vast amount of data of high dimension to be analyzed. However, high-dimensional data poses new challenges for clustering algorithms in terms of cost, over fitting and the validity of the similarity measures.

Popular clustering algorithms become unreliable when the size of the data is too small compared to the number of parameters to estimate. This well-known phenomenon is referred to as the curse of dimensionality and was introduced by Bellman [5], we refer to [45] for an illustrating example.
Even with the abundance of data, clustering in high dimensions suffers from another problem which is more serious than the curse of dimensionality. This problem relates to the non-equal relevance for dimensions between clusters. As it is known, clustering topology depends on the adopted measure of similarity. However, not all the dimensions are important to determine a cluster and many of them are subsets of important dimensions. All of these facts cause traditional distance/similarity measures, which use all dimensions with equal relevance, ineffective [10].

Unfortunately, the feature selection or dimensionality reduction cannot be applied to clustering prob-lems because they are global transformations in the sense that they compute only one subspace of the original data considering the complete set of points [27]. In contrast, multiple subspaces are needed be-cause each cluster may exist in a different subspace. To overcome the problem of relevance, a commonly recent approach consists of finding clusters in subspaces as described in [27,34,37]. 5.3. ICA and sparsity
Sparsity investigation has a great interest in both ICA and BSS and it has been the focus of numerous works recently. For example, the sparsity prior assumption of the signal sources is employed to improve the results of the BSS problem in the so-called Sparse Component Analysis SCA [14], sparsity in the SCA context means that at each instant the majority of sources are almost null (inactive sources).
Another important useful type of sparsity may be viewed at the level of the mixing matrix A ,where some parameters are zero. The number and the location of non-zero parameters of the mixing matrix A define the structure of the observed variables X . In this sense, ICA has been used to learn the structure of the data by following a suitable pruning strategy [32].
 It is obvious that the ICA separation accuracy can be improved if the mixing matrix is too sparse. This can be explained by the fact that sparsity leads to a weak mixing; each observed variable in X is a mixture of only a small subset of sources. Also as shown in the previous section, the ICA methods accuracy increases as the number of sources decreases.

Figures 4 and 5 show the SNR of JADE and FastICA algorithms when we modify both the number of sources and the sparsity degree of A . The number on the top-right associated with each curve represents the portion of zeros in A . For example, 1/6 parameters of A are zero for the green-dashed curve. It can be noticed that the SNR decreases exponentially when the ICs number increases and it is generally more important when the mixing matrix A is too sparse.

A summary of some limitations of the ICA algorithms for high dimensional data is as follows: 1. It is clear that the separation accuracy of all existing ICA methods deteriorates with an increase of 2. In the majority real world data, structural sparsity is frequently observed when the dimension is 3. The computational complexity often soars exponentially with an increase of the dimension.
In Section 4, we have proposed an estimator for the nonlinear case where the fuzzy clustering is performed ahead of applying ICA. To overcome the above mentioned problems of the standard ICA methods and clustering in high dimensional spaces, we propose to take advantage of the sparsity charac-teristics which is omnipresent in such cases by making use of methods conceived especially for learning the data structure. Then, the obtained structure will be employed to reorganize the variables into con-nected small groups, so that the variables within each group are highly dependent. Finally, a fuzzy local ICA estimator as shown in (12) can be applied separately on each group.

One of the most widely used data structure representation is BN models [36]. The data structure in BNs by a Directed Acyclic Graph (DAG). In this work, we propose to use the well-known structure learning algorithms of BNs. The next section is devoted to introduce our main contribution which consists of combining BNs and fuzzy local ICA methods in a framework for the HDDE purpose. 6. BNs and ICA for density estimation 6.1. Bayesian networks BNs are probabilistic models useful for reasoning with, or representing knowledge under uncertainty. Essentially, a BN can be defined as a pair ( G, P ) where G is a DAG G =( V,E ) with the vertices V as the nodes in the network. Each node V i corresponds to a random variable X i relevant to the problem domain. The dependencies among the variables are encoded by the set of edges E in the underlying DAG. The second parameter P is a set of conditional probability distributions of each variable (node) conditioned only by its direct parents used to quantify the BN.

Let us denote by  X  i the direct set parents of the node X i in the graph. One of the interesting properties The factorization is possible only if Markov property is satisfied, thus meaning that the structure of the graph is determined so that each node is conditionally independent of its non-descendants given its parents. 6.2. Structure learning
The structure of a BN is defined by a set of edges which represent the underlying statistical depen-dencies among the variables. Given a data training set D =( x 1 ,...,x i ,...,x N ) in the k-dimensional D . Several algorithms have been developed for this purpose [31,41] and most of them can be classified into two categories: constraint-based and score-based approaches.

Methods of the first approach establish a set of conditional independence statements among the vari-ables and use the obtained results to build the BN. On the other hand, methods of the second approach define a quality metric (called also the score function) which measures how much the BN graph matches the training data. Such methods often require an iterative search algorithm to move from a structure to another having a high score.

In general, algorithms from both approaches are heuristic; the exhaustive search is impossible due to the huge size of the DAG search space. The number of the possible structures of K nodes is super-exponential. 6.3. Junction tree (JT)
A junction tree representation of a Bayesian Network BN =( G, P ) is constructed by a triangulation of the moralized graph G . The nodes of the junction tree correspond to cliques (a clique is a fully-connected subset of nodes in a graph) of the triangulated graph. The cliques of the junction tree are connected by separators such that the so-called junction tree property holds [30]. Each separator S between two adjacent cliques C i and C j is equal to the intersection C i C j . The junction tree property ensures that whenever two cliques C i and C j are connected by a path, the intersection I = C i C j is a subset of every clique and separator on the path. A junction tree also has the property that each variable and its parents are contained in at least one clique.

A junction tree construction follows the following steps: (i) moralization by suppression of the edges orientation and marrying parents, (ii) triangulation by adding undirected links in order to make the moralized graph triangular (iii) finding a maximal spanning tree where the weight of an edge in the clique graph is equal to the cardinality of the separator, see example in Fig. 6.
The junction tree is traditionally used for the belief propagation over BNs. The key-idea is that the underlying joint probability space may be decomposed into a set of subspaces corresponding to a de-composable (hyper-graph) cover of the moralized graph. Jensen in [23] has shown that any maximal spanning tree of a decomposable cover of the moralized graph, can be used as the basis for a simple inward/outward message-passing scheme for the propagation of evidence (belief updating) in BNs.
To each clique and each separator is associated a (belief) potential  X ( A ) ,where  X ( A ) is a non-negative function for each set A in the complete subsets of X .

The joint probability distribution P of a Bayesian network with a corresponding junction tree T ( C, S ) is proportional to the joint potential belief  X = X  X [26] given by A potential  X  A is normalized if X junction tree are normalized, then the joint potential  X  X is normalized (i.e. P X ( X )= X  X ) separator s . 6.4. Density estimation by local ICA and BNs
The proposed multivariate density estimation method for continuous high dimensional data consists of the followings stages: 1. Learn the BN structure G of the observed variables using the available BN structure learning algo-2. Construct a correspondent junction tree T ; 3. Associate a fuzzy local ICA model to each junction tree element, clique q  X  T or separator p  X  T ,
By combining the result obtained in Eq. (12) with the joint probability decomposition of Eq. (14), one can compute the joint probability using the following equation: where S i,j,q and A j,q are the i th independent component and the estimated mixing matrix of the cluster number of cliques while Kq is the number of the independent components which is equal to the clique size (the same notation is used for separators p  X  Sep in the denominator)
The number of clusters Lq | Lp should be specific to each clique q or separator p in the junction tree.Determining the optimal number of clusters is achieved by minimizing the XB validity index [44]. XB aims to measure the ratio of compactness to separation of clusters of a fuzzy partition, see Sec-tion 4.3.3 for more details. 7. Experimental results 7.1. Application for density estimation 7.1.1. Test distribution and performance measure
We have conducted a number of density estimation experiments to compare the performance of the proposed method and the standard GMM which is often used for HDDE. The expectation-maximization algorithm is used for learning the GMM estimator.

In the experiments, we have generated 20 GMM target densities for each dimension with half of them by using 40 components and the remaining half by using 80 components. Therefore, for each configu-ration defined by the dimension and the density complexity (40 or 80 components), we have generated a total of 10 GMM density functions. The mean and the variance matrix values of each component are chosen uniformly and randomly within the intervals [0 10] and [0 5], respectively. The weights of the components are also fixed randomly. Then, for each target density a dataset of 8000 observations are generated by sampling where 4000 are used for learning and 4000 are used for prediction.
For the purpose of performance comparison of the estimators, we have used the normalized mean square error (NMSE) metric which is useful when comparing results between different datasets. NMSE is obtained by dividing the Mean Square Error (MSE) by the variance as follows: where p x i and p x i are respectively the true and the estimated probabilities of the test vector x i . 7.1.2. Results and discussion
The GMM density estimation technique is compared to our proposed method for the two following combinations: FCM with FastICA and GK with FastICA. In both cases, the constraint-based BN struc-ture learning algorithm Grow-Shrink [31] is employed. The number of components in GMM is selected by using the Bayesian Information Criterion (BIC) which is often applied for model selection in GMM.
Figures 7 and 8 depict the NMSE averaged over 10 trials of datasets sampled from 40 (i.e., less complicated data) and 80 (i.e., more complicated data) components GMM targets, respectively. For each even dimensionality dim =2 , 4 ,..., 38 , the mean NMSE is calculated, the NMSE of one dataset is computed using Eq. (16).

In both figures, the mean NMSE of GMM and FCM&amp;FastICA estimators tend to grow more rapidly than GK&amp;FastICA as the dimensionality increases. The curves can be divided into three distinct parts. For low dimensions ( dim &lt; 14 ), NMSE of GMM is much smaller than that of FCM&amp;FastICA for 40-components datasets and it is quite close to that of FCM&amp;FastICA for 80-components datasets. On the contrary, NMSE of GK&amp;FastICA is significantly higher. In the middle region ( 14 &lt; dim &lt; 24 ), NMSE of GMM and FCM&amp;FastICA increase rapidly with a relatively high speed for GMM than FCM&amp;FastICA. In this region, the FCM&amp;FastICA is slightly better than other estimators, especially for 80-components datasets. In the case of high dimension region ( dim &gt; 24 ), GK&amp;FastICA provides the smallest NMSE. We have also noticed that gaps between the three NMSE curves are much narrower on 80-components datasets than on 40-components datasets.

Performance degradation of GMM in the high dimensional setting occurs because there is a significant number of free parameters to estimate in the covariance matrices of the components distributions. Thus, in the learning stage, the Expectation-Maximization algorithm typically converges to an incorrect set of parameters for the Gaussians.

Another interesting point to discuss relates as to why FCM outperforms GK for low dimensions and the inverse for high dimensions. As the number of components in the target density is fixed (40 or 80), by increasing the dimensionality, the simulated components are further separated from each other. The multivariate Gaussians are generated randomly having generally a different variance on each dimen-sion. Therefore, the shapes of the Gaussian components are generally much closer to an ellipse than to a sphere; this may be the reason why GK outperforms FCM in high dimensions. In contrast, low dimensions results in overlapped components. In addition, a large number of overlapped components tends to take the form of a sphere. This may be the probable reason why FCM outperforms GK in low dimensions. 7.2. Application for classification 7.2.1. Resolving the multiclass classification problem
In order to assess the performance of the proposed estimation model, a supervised classification ap-plication has been chosen. Tests are performed on a set of heterogeneous real datasets and the results are compared to some well-known classifiers such as SVM (Support Vector Machine) and KNN (K-Nearest Neighbor).

First, for each class  X  i , i =1 ,...,Nc , a density estimator  X  i is built using training data following the stages described in the previous Section 6.4. The parameters set of the model  X  i includes the junc-tion tree, the clustering parameters, the ICA mixture matrix of each cluster, and the univariate density estimation parameters of each independent component.
 is said to assign a feature vector x to class  X  c if c = argmax i g i ( x ) , i =1 ,...,N c where g i ( x ) corresponds to the a posteriori likelihood of the class  X  i and it can be computed as follows: j of the clique q equal to its frequency, and since the value of the test vector is known,we can make a x is within the cluster j . Indeed, the obtained clusters are low dimensional data and have spherical or elliptical shapes. For example, a Gaussian Mixt ure Model (GMM) with a small number of components can fit better the cluster distribution. Hence, the prior probability of the cluster j may be computed as follows: of cluster j in separator p . 7.2.2. Data description
Some known benchmarks in the classification area are employed for the evaluation phase. Basically, the chosen datasets are heterogonous, real, and where the dimension is high (dim &gt; 20). Our choice of real datasets is motivated by the fact that high dimensional real data are often characterized by an important sparsity, and in such cases the structure Bayesian network learning leads to a junction tree of types based on acoustic or on seismic features, and steel plate faults.

In general, acoustic or seismic based vehicle classification, and mechanical fault detection are difficult signal processing problems as their features depend on many uncontrolled hidden factors, which are regarded as noise. Many of the parametric models fail to deal with such kind of complex data. For example, the acoustic waveforms are rather complicated functions as they depend on the vehicle speed, the wind force and direction, the sensor-to-vehicle angle, the temperature and the humidity of the air, etc. The description of the employed dataset is given in the following sections. SatImage
The SatImage dataset is taken from the StatLog collection and consists of multi-spectral values of pixels in 3  X  3 neighborhoods of satellite images. The aim of the classification is to predict the class of the central pixel in each neighborhood. In the sample database, the class of a pixel is coded as a number. SensIT vehicle (acoustic)
This dataset is taken from the third SensIT situational experiment (SITEX02) where acoustic vibra-tions caused by running military vehicles are collected by a wireless sensor network deployed on a road intersection [11]. The data consists of three classes: noise and two types of vehicles, namely Assault Amphibian Vehicles (AAV) and Dragon Wagon (DW).
 SensIT vehicle (seismic)
The same description and source as the acoustic dataset, introduced above, with the difference that seismic vibrations are collected instead of acoustic ones [11].
 Steel fault faults. The fault description is defined by 27 indicators representing the geometric shape of the fault and its contour. Details about the dataset can be found at http://archive.ics.uci.edu/ml/datasets/Steel+Plates+ Faults.

The SatImage and SensIT Vehicle (acoustic and seismic) are downloaded from LibSVM benchmarks at http://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets/. See Table 1 for more details. 7.2.3. Results
We have used the BNLearn toolbox [41] for learning the BN structure. A summary of the employed algorithms is presented in Table 2. For the constraint and hybrid based algorithms, the following condi-(aict), mutual information based test (mi), and a mutual information test based on shrinkage estimator (mi-sh), see [41] for more details.

For each dataset, we have experimented all the combinations of the modules constituting the proposed estimation framework; GK and FCM fuzzy-clustering algorithms, JADE and FastICA ICA methods, the introduced conditional statistical tests and some of BN structure learning algorithms.

The classification rate results are depicted in Tables (3 X 6). From the results, it can be noticed that the classification results of our method with the GK clustering algorithm are commonly and slightly better than the corresponding FCM results. These findings are in agreement with the theoretical results, which confirm that elliptical clusters are more suitable t han spherical clusters when using local ICA methods. Regarding the separating ICA module, it is clear that ICA performances in terms of the statistical inde-pendence strength among the CIs depend on how much the linear ICA method fits with the statistical properties of the used dataset variables. For the datasets used, it can be seen that the FastICA results outperform slightly those of the JADE algorithm.

The BN structure learning affects seriously the performances since the classification rate fluctuates clearly from one structure to another. As such one can observe that the BN structure learning algorithms are not stable; each algorithm that yields good results for one dataset, yields modest results for the other datasets. This may be explained by the fact the all the tested BN structure leaning algorithms are heuristics in nature and applied on discretized data. Therefore, there is no guaranty to obtain the optimal BN structure using the tested algorithms. For example, structure S1 (where the junction tree is built manually by making the adjacent frequency coefficients in the same cliques and separators) of the SensIT Vehicle acoustic dataset outperforms the employed automatic BN learning algorithms (See Table 4).

For the Steel fault dataset results depicted in Table 6, we notice that the classification rates obtained by the proposed method change substantially from a configuration to another with the rates varying between 0.5243 and 0.6990. This instability is due to the fact that the number of instances per class (which is in average 168 instances per class) compared to the dimension is not quite enough to build a reliable density estimator.

The best results obtained by the proposed classifier ICA-BN are compared with the results obtained by the popular classifiers, SVM and KNN. The KNN parameter K is empirically optimized using a data sample. The sigma parameter  X  of the Gaussian kernel and the cost parameter C for SVM are fixed empirically by searching on a wide range of values. Table 7 shows the results obtained of the three classifiers. The classification accuracy obtained by the proposed classifier ICA-BN is slightly better than the SVM counterpart. For example, among the four datasets, ICA-BN yields better accuracy on SensIT Vehicle (acoustic) and Steel fault datasets than SVM ones.
 In the experiments, we considered only JADE and FastICA methods. For the SatImage ad SensIT Vehicle (seismic), it could be that these two methods cannot effectively separate the dependencies of tools. Indeed, one of the main advantages of the proposed method is its generic quality. For example, if the dataset variables have a parsimonious representation (variables are often nearly zeros), then sparse ICA tools as SCA [14] may be more accurate. 8. Conclusion
The purpose of this study was to develop a semi-non parametric estimation framework for high di-mensional multivariate densities. The technique is based on a combination of BNs structure learning algorithms and local ICA models using fuzzy clustering. The fuzzy local ICA model is used for the representation of data; where the fuzzy clustering part is responsible to tackle the overall nonlinearity, while linear local ICA of each cluster is used to describe the local features. It is well-known that both clustering algorithms and ICA methods do not perform well if the data dimension is high. In addition, in the most high dimensional applications, the structure sparsity is omnipresent; each variable is condition-ally dependent only to a small subset of the variables of the domain. This means that all the information about a given variable is contained into a small subset of variables and the rest of variables do not bring any additional or useful information. Furthermore, BNs have enjoyed an increasing popularity especially to encode this kind of sparsity such as a DAG. In this paper, we have proposed to learn the data structure by using the available BN structure learning algorithms and then to construct the junction tree of the obtained DAG. Each element of the junction tree involves only a small number of highly dependent variables. This has resulted in a reduction of dimensionality of the vectors allowing us to use efficiently a fuzzy local ICA model to estimate the density of each tree element separately. The dimension being equal to the number of variables contained in the tree element.

Basically, the experiment was divided into two parts, firstly a comparison to the GMM estimator on simulated datasets was conducted and secondly the supervised classification on a set of real heteroge-neous datasets was carried out. We have experimented many combinations of the algorithms: the FCM and GK fuzzy clustering algorithms, a set of constraint-based and score-based BNs structure leaning algorithms, the JADE and the Fast ICA algorithms. Generally, the proposed method is highly dependent on the performances of the BNs structure learning, which is responsible for the dimensionality reduction. The idea proposed in this paper can be further improved by the integration of the defuzzification process in the optimization function of ICA methods. A second interesting point can be a study of the link that exists between the linear ICA performances and the clusters shapes. We have used GK and FCM for illustration purposes; it may be possible that other clustering algorithms are more accurate when com-bined with FastICA and JADE. The progress recorded in BSS and ICA domain may be useful for the enhancement of the proposed estimation framework, for example, the use of convoluting ICA mixtures seems to be promising for estimation/classification of data having convoluting nature.
 References
