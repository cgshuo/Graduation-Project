 Clustering offers significant insights in data analysis. Density-based algorithms have emerged as flexible and ef-ficient techniques, able to discover high-quality  X  X nd po-tentially irregularly shaped X  clusters. We present two fast density-based clustering algorithms based on random pro-jections. Both algorithms demonstrate one to two orders of magnitude speedup compared to equivalent state-of-art density based techniques, even for modest-size datasets. We give a comprehensive analysis of both our algorithms and show runtime of O ( dN log 2 N ), for a d -dimensional dataset. Our first algorithm can be viewed as a fast variant of the OP-TICS density-based algorithm, but using a softer definition of density combined with sampling. The second algorithm is parameter-less, and identifies areas separating clusters. F.2.0 [ Analysis of Algorithms and Problem Complex-ity ]: General; I.5.3 [ Clustering ]: Algorithms Clustering, Data Mining, Random Projections
Data doubles about every two years. This makes the anal-ysis of Big Data a necessity and it also drives the design of more efficient algorithms for data analytics. Clustering is an important operation for knowledge extraction. Its objective is to assign objects into groups such that objects within a group are more similar than objects across different groups. Subsequent inspection of the realized groups can provide important insights, with applications to pattern discovery [13], data summarization/compression [10] as well as data classification [4]. Density-based clustering algorithms have emerged both as high-quality and efficient clustering tech-niques with solid theoretical foundations on density estima-tion [8]. They can discover clusters with irregular shapes and require easy to set parameters (e.g., minimum number of points per cluster). They also can help assess important dataset characteristics, such as the intrinsic density of data, which can be visualized via reachability plots.

In this work we extend the state-of-art in density-based clustering techniques by presenting algorithms that signifi-cantly improve runtime, while providing analytical guaran-tees on the preservation of cluster quality. We achieve this through the use of random projections. A key theoretical re-sult of random projections is that, in expectation, distances are preserved. We exploit this in a pre-processing phase, to partition objects into sets that should be examined together. The resulting sets are used to compute a new type of density estimate through sampling.

Our first algorithm, requires only the setting of a single pa-rameter, the minimum number of points of a cluster, which is customarily required as input in density-based techniques. Our second algorithm lifts this requirement and presents a parameterless density-based clustering algorithm by creat-ing a sequence of clusters for varying density parameters. In general, we make the following contributions : For points and lines we use capital letters, e.g. P, T, Q, L . For a set of points or a sequence of lines we use calligraphic font, e.g. P , S , L . For a set of sets or sequences we use Fraktur letters, e.g. S , L , W . We are given a set of N points P in the d -dimensional Euclidean space, i.e., for a point P  X  P holds P  X  R d . We use the term whp , i.e., with high probability, to denote probability 1  X  1 /N c for an ar-bitrarily large constant c . The constant c (generally) also occurs as a factor hidden in the big O -notation. Define the distance D ( A, B ) := || B  X  A || 2 for two points A, B  X  P to be the L 2-norm. Assume or norm the shortest distance min A,B  X  X  ,A 6 = B D ( A, B ) to be 1. We often use the following Chernoff bound:
Theorem 2.1. The probability that the number X of oc-curred independent events X i  X  { 0 , 1 } , i.e. X := P i can be bounded by p ( X  X  (1  X  c 0 ) E [ X ]  X  X  X  (1+ c 1 2 e If an event occurs whp for a point (or edge) it occurs for all whp. The proof uses a standard union bound.

Theorem 2.2. For n c 0 (dependent) events E i with i  X  [0 , n c 0  X  1] and constant c 0 s.t. each event E i occurs with that all events occur is at least 1  X  1 /n c  X  c 0  X  2 .
Our density based clustering algorithms consist of two phases: the first partitions the data so that nearby points are placed in the same partition. The second phase uses these partitions to compute distances or densities only within pairs of the same set. This allows for much faster execution.
The partitioning phase splits the dataset into smaller sets (Partition algorithm). We perform multiple of these parti-tions by using different random projections (MultiPartition algorithm). Intuitively, if the projection P  X  L and Q  X  L of two points P, Q onto line L is of similar value then the points should be close. Thus, they are likely kept together whenever the point set is divided.
 For a single partition, we start with the entire point set. We split it recursively into two parts until the size of the point set is at most minSize +1, where minSize is a pa-rameter of the algorithm. To split the points, the points are projected onto a random line, and a point that has been pro-jected on the line is chosen uniformly at random. All points with a projected value smaller than that of the chosen point constitute one part and the remainder the other part. More formally, MultiPartition algorithm chooses the set L := {L 0 , L 1 , ... } of c 0 log N sequences (for a constant c where L i := ( L 0 , L 1 , ... ) is a sequence of c 1 log N random lines for a constant c 1 with L j  X  R d . The Partition algo-rithm is called for a sequence L i . The points S are projected onto each line L j  X  L i . First, after the projection onto L the points S are split into two disjoint sets S 1 0  X  P and S using the value r s := L  X  A of a randomly chosen point A  X  S . The set S 1 0 contains all points P  X  P with smaller projected value than the chosen number r s , i.e., Q  X  L 0  X  r s , and the other points P \ S 1 0 end up in S 1 1 . Afterwards, recurse on sets S 1 0 and S 1 1 , that is to say, for line L 1 we first consider set S 1 0 and split it into sets S 2 0 and S 2 1 . Then, the process is repeated for S 1 1 to obtain sets S 2 2 and S 2 3 . For line L we consider all four sets S 2 0 , S 2 1 , S 2 2 and S 2 3 ends once a set S contains fewer than minSize +1 points. We compute the union of all sets having at most minSize points of all projection sequences L i  X  L . An instance of the above process is illustrated in Figure 1. Similar techniques to algorithm Partition have been used in the RP-tree [5]. However, here we use a much simpler splitting rule. Figure 1: A single partitioning of points using ran-dom pro jections. The splitting points are chosen uniformly at random among all projected points.
 Algorithm 1 Mult iPartition(points P , minimum set size minSize ) return set of point sets S 4: S := S  X  W 5: end for 6: if |S| &gt; minSize then 12: else 13: S := S  X  {S} 14: end if The following hold, but we omit the proofs for brevity. Theore m 3.1. For a d -dimensional dataset, Algorithm Partition runs in O ( dN log N ) time whp.

Algorithm MultiPartition calls Algorithm Partition c log N times, thus using Theorem 2.2: Corollary 3.2. Algorithm MultiPartition runs in O ( dN log 2 N ) time whp.
U sing the previous data partitioning we compute for each point a probabilistic neighborhood and an estimate of den-sity.
 Sampled Neighbors: For each point A we compute a sam-ple of close neighbors using a set of sequences of points S for a parameter dP ts . A sequence is an ordering of points projected onto a random line (see Figure 1). For each se-quence S  X  S and every point A  X  S we choose randomly a point B being at most dP ts points after point A in the sequence S . All the selected points around A form the sam-pled neighbors N ( A ). See Algorithm 2 and for an example consider Figure 2.
 Algorithm 2 Samp ledNeighbors(set of sequences of points S , distance in points dP ts , return for each point A neighbor set N ( A )) 3: Sort S according to values of projected points 4: for i = 1 to | S |  X  dP ts do 5: j := Random integer in [1 , dP ts ] 7: end for 8: end for Figure 2: Two partitionings using random projec-tions. For dP ts = 1 a density estimate for point P 3 is obtained as follows: For the first partition-ing either P 2 or P 0 is added to the sampled neigh-bors N ( P 3 ) . For the second, P 1 or P 2 . Say P 0 and P 2 are chosen, i.e. N ( P 3 ) = { P 0 , P 2 } . Then, for f = 1 the average distance Davg ( P 3 ) becomes ( D ( P 3 , P 0 ) + D ( P 3 , P 2 )) / 2 . The candidate(s) for P only be P 2 , since D ( P 3 , P 0 ) &gt; m ( P 3 , P 0 )  X  Davg ( P Density Estimate: The density of a point A is the average distance D avg ( A ) of a subset of all sampled neighbors N ( A ) for a parameter dP ts . More precisely, we only consider the points from N f ( A )  X  N ( A ) that are among the fraction of f closest points in N ( A ) for some constant f . Mathematically speaking, let C be the |N ( A ) |  X  f -th closest point in N ( A ) to A then N f ( A ) := { B  X  N ( A ) | D ( A, B )  X  D ( A, C ) } and D D avg ( A ) is just the average of all sampled neighbors. Note, the smaller the average distance, the larger the density.
Assume that there are just dP ts +1 points. Then D avg ( A ) is an approximation of the average distance to the f  X  dP ts -th (closest) neighbors. If we add a point to the dataset that is closer than the f  X  dP ts -th nearest neighbor N  X  then the average distance will decrease (in expectation). If we add a point that is farther away than the f  X  dP ts -th nearest neigh-bor B then the average distance increases. So, if we add many points that are somewhat further away than B then the added points may cause the average distance to increase significantly beyond the average distance of the f  X  dP ts -th closest neighbors B . However, as we shall see due to our partition process (Algorithm MultiPartition) points distant from A only appear in a set S  X  S with low probability. However, to ensure that the average is not significantly dis-torted with high probability, we do not compute only the average of all sampled neighbors but restrict ourselves to a subset dependent on the parameter f .
 Candidate Mergers: Two points A, B are candidates to be merged if their distance is within merging distance. The merging distance m ( A, B ) is just the minimum of the average distances of the two points, i.e. m ( A, B ) := min( D avg ( A ) , D avg ( B )). A point A may merge with any sampled neighbor B  X  N ( A ), if and only if their distance is less than the merging distance m ( A, B ) &lt; D ( A, B ). Thus, we restrict the pairs of points that can be merged by any clustering algorithm according to some criterion to the pair of points A, B with m ( A, B ) &lt; D ( A, B ) with B  X  N ( A ) (or A  X  N ( A )), i.e. we define the candidates for point A as N is captured in Algorithm 3. For an example, the reader is directed to Figure 2.
 Algorithm 3 Can didateMergers(points P , distance in points dP ts , return for each point A candidates N C ( A ) for potential mergers) 3: Compute SampledNeighbors ( S  X  , dP ts ) 5: for all A  X  P do 6: for all B  X  N ( A ) do 9: end for 10: end for
Assume that to estimate the density of a point A we mea -sure a volume V ( A ) containing dP ts points. If the volumes V ( A ) and V ( B ) of two points intersect significantly then the density at a point contained in the intersection is likely to be of similar density of either A or B (or both). However, in case the two volumes do not intersect this does not hold. For density-based clustering we want to form clusters of points of similar density -at least all nearby points must have sim-ilar density. Therefore, it suffices to consider nearby points, i.e. points with intersecting volumes. Note, that for merging candidates A and B th e volumes V ( A ) and V ( B ) used for the computation of the density intersects by definition since m ( A, B ) := min( D avg ( A ) , D avg ( B )) &lt; D ( A, B ). Algorithm Complexity: Now we state our main theorems regarding the complexity of the presented techniques. Theorem 4.1. Algorithm 3 runs in O ( dN log 2 N ) whp.
Next, we state a bound on D avg ( A ) for a point in R d , i.e. we relate D avg ( A ) and the average of the distance to the dP ts -nearest neighbors of a point A .

Theorem 4.2 states that for the smallest set S A returned by the Partition algorithm containing A , a sufficient fraction of all points are not substantially far from A than D avg Therefore, if we sample points from S A at least some of these points are close to A (Theorem 4.3).

For a point A , define D ( A, dP ts ) to be the distance to the dP ts -th nearest point from A . Define N ( A, r ) to be all points C  X  S within radius r := D ( A, dP ts ) from A , i.e. D ( A, C ) &lt; r . Denote the average distance of the dP ts nearest points to a point A as D ( A, dP ts ) := P Theorem 4.2. For a point A , the probability that |S A \ N ( A, c 7 r ) | / |S A | &gt; 1 /c 7 1 / 4 is at least 1 / 2 dP ts &gt; c 9 for some constants c 7 , c 9 .

Theorem 4.3. For every point A  X  R d holds D ( A, dP ts  X  c ) &lt; D a vg ( A ) &lt; 2
As an example, assume that D ( A, dP ts  X  c 10 )  X  D av g c , which holds in general, if the number of points from A do not increase much faster with distance than to the dP ts  X  c 10 nearest point. In this case, we compute an O (1)-approximation of D ( A, dP ts ). A ssuming that D ( A, dP ts ) is a n equivalently valid density measure as the distance to the minP ts -th neighbor used by OPTICS we compute a O (1)-approximation of the density, i.e. core-distance, used by OPTICS.
We apply our ideas to speed up the computation of the popular OPTICS algorithm (Ordering points to identify the clustering structure) [2]. OPTICS defines a sequence of all points and a distance for each point. This allows for an easy visualization to identify clusters. Similarity between two points A, B is measured by computing a reachability dis-tance. Figure 3 provides an illustration. This distance is the maximum of the Euclidean distance between A and B and the core-distance (or density of a point), i.e. the distance of A to the minP ts -th points, where minP ts corresponds to the minimum size of a cluster. Any point A is equally close (or equally dense) to B , if A is among the minP ts -nearest neighbors of B . If this is not the case then the distance be-tween the two points matters. The algorithm maintains a list of point pairs sorted by their reachability distance. It chooses a point A with minimum reachability distance (if the list is non-empty, otherwise an arbitrary point) and up-dates the list by computing the reachability distance from A to each neighbor. The algorithm investigates each point X  X  neighbors only once. For performance reasons OPTICS re-quires a parameter  X  that denotes an upper bound on the distance between any two points that are considered to com-pute the reachability distance. The parameter  X  should be at least the distance to the minP ts -nearest point of any point.
We compute a similar ordering as for OPTICS but use a different definition of reachability. As core-distance of a point A we use the average distance D avg ( A ) as proposed in Section 4. Whereas for the core-distance in the original OPTICS algorithm only the distance to a single point, i.e. the minP ts -nearest point matters. Using our probabilistic approach we compute a smoother estimate: points that are closer (or further) than the minP ts -nearest point matter, too. Two points are reachable if they are within merging distance. In other words, we dynamically adjust the param-eter  X  for each point A . Figure 3: Reachability plots of FOPTICS and OP-TICS for the Compound dataset. Note that both ex-hibit the same hills and valleys and hence discover the same clusters.
 More precisely,  X  ( A ) states the maximal distance of a point B that might be merged with A , i.e.  X  ( A ) := D avg ( A ). Point A can reach point B , if B is a candidate to be merged, i.e. B  X  N C ( A ), see Section 4. The definition of the reachability distance for a point A and a reachable point B from A is the same as for OPTICS, it is the maximum of the core-distance of A and the distance of A and B . However, for a point A we only compute the reachability distance to all sampled neighbors B  X  N ( A ).

In Figure 3 we provide a visual illustration for the reacha-bility plot computed on one dataset for OPTICS and FOP-TICS. It is apparent that both techniques can reveal the same cluster structure.
 Theorem 5.1. FOPTICS runs in O ( dN log 2 N ) whp. Proof. Computing all candidate neighbors takes time O ( dN log 2 N ) whp according to Theorem 4.1. For each point we consider all candidate mergers at most once. This takes time O ( N log N ).
Dens ity-based clustering algorithms require as input a pa-rameter that captures some notion of density, e.g. the min-imum number of points which constitute a cluster or the volume used to estimate density. Different parameters may lead to different clustering outcomes. Figure 4 shows such an exa mple, consisting of two elliptical clusters connected by a thin line of points.

The thin line is denser when the density is estimated us-ing small volumes (smaller  X  values). For larger volumes the left and right elliptical areas become denser. The outcome of OPTICS for different parameters is also shown in Fig. 4. Notice that the cluster structure may change, leading to clusters merging or separating. We propose an efficient al-gorithm without data-dependent parameters to capture the variability of cluster structure. Figure 4: The choice of parameters, i.e. data den-sity , greatly affects the resulting cluster structure. The outcome of OPTICS for different parameters minP ts = 7 , 14 and 28 , suggesting three, one and three clusters, respectively.
Algorithm DeBaRa ( Density Based Clustering via Ran-dom Projections ) gradually increases the average distance of a point, i.e. parameter dP ts for computing the sampled neighbors (Algorithm 2). For a fixed value of dP ts it iden-tifies points that potentially split clusters, i.e. points that belong to volumes of lower density than their surrounding. Any point that is not classified as low density point merges with all nearby points, whereas low density points greed-ily merge with the point of maximum density within some distance.
 Two points A and B are adjacent (or nearby) if their Euclidean distance D ( A, B ) is less than their merging dis-tance: D ( A, B )  X  m ( A, B ). Each point A identifies the point B  X  N C ( A ) among the merging candidates of maxi-mum density (that might also be itself). Any such identified point B is called non-separating . Any non-separating point A merges with all clusters that have a point B within half the merging distance and any non-separating point C within the merging distance.

The assumption behind this rule is that two clusters are separated by points of low density. Points of low density have large average distance D avg and it is more likely that they merge with a distant point A . Such a point A has larger density, i.e. smaller D avg . Furthermore, point A is likely to be somewhat distant from the cluster border. Therefore, it can merge with all the points within (half the) merging distance without merging with another cluster separated by points of low density.
 Algorithm 4 DeBa Ra(points P ) return C i 14: end for 17: end for 18: end for
Theorem 6.1. A lgorithm DeBaRa runs in O ( dN log 2 N ) time whp.
 Proof. Computing all candidate neighbors requires time O ( dN log 2 N ) whp based on Theorem 4.1. For a fixed dP ts for each point we consider all candidate mergers at most once. This takes time O ( N log N ). A merger of clusters Cl ( P ) and Cl ( Q ) takes time proportional to the size of the smaller cluster. There are at most N  X  1 mergers. The run-ning time is maximal if both merged clusters are of the same size. Therefore, we have N/ 2 mergers of clusters of size 1, N/ 4 of clusters of size 2 and so on. Thus all merger oper-P i  X  [0 , log N  X  1] N/ 2 = N/ 2 log N  X  1 time. We consider log N distinct values for dP ts yielding time O ( N log 2 N ).
We ev aluate the runtime and clustering quality of the pro-posed random projection based techniques. The FOPTICS and DeBaRa algorithms have been implemented in Java. We compare their performance with OPTICS and DeLi-Clu, from the Elki Java Framework 1 . DeLi-Clu represents an im-provement of OPTICS leveraging indexing structures, such as R*-trees, to improve performance.
 Parameter setting: OPTICS requires parameters  X  , and minP ts .  X  is set to infinity, which provides the most accurate results. minP ts depends on the dataset. DeLi-Clu requires elki.dbs.ifi.lmu.de/ only the minP ts parameter. FOPTICS uses the same pa-rameter value for dP ts as minP ts for OPTICS (and DeLi-Clu): dP ts = minP ts . DeBaRa uses no data-dependent parameters. We performed 100 partitionings, that is, calls to algorithm Partition from MultiPartition, of the entire dataset for FOptics and DeBaRa and used f c = 1 for the computation of D avg .
 Cluster Quality: The original motivation of our work was to provide faster versions of existing density-based tech-niques while not compromising accuracy. To compare the clustering we use the Rand index [11], which returns a value between 0 and 1, where 1 indicates identical cluster results. The results for various datasets are summarized in Table 1. FOPTICS provides almost perfect cluster preservation with OPTICS. As shown in previous examples the reachability plots for FOPTICS and OPTICS are very similar. For the chosen parameter dP ts = minP ts the reachability plot of FOPTICS is typically smoother , following from the defini-tion of the average distance D avg that computes a smoother estimate of the density than OPTICS.
 Runtime : Our performance comparison in Figure 5 sug-gests a drastic improvement of FOPTICS and DeBaRa com-pared to both OPTICS and DeLi-Clu. FOPTICS is more than 500 times faster than OPTICS and more than 20 times faster than DeLi-Clu. Note, that DeLi-Clu is using an R*-tree structure to speedup various operations. Our approach bases its runtime improvements on random projections, thus is much simpler to implement and maintain. Figure 5: FOPTICS runs faster than OPTICS or DeL i-Clu.

Figure 6 highlights the runtimes for increasing data di-mensionalities for a synthetic dataset of Gaussian clusters. Note that the performance gap between OPTICS and DeLi-Clu diminishes for higher dimensions. In fact for more than 500 dimensions OPTICS is faster than DeLi-Clu. This is Figure 6: Performance of DeLi-Clu diminishes for higher dimensions, due to its use of indexing tech-niques. a by-product of the use of indexing techniques by DeLi-Clu. It is well understood that the performance of space-partitioning indexing structures like R-trees, diminishes for increasing dimensionalities. The performance improvements of FOPTICS compared to OPTICS range from 47x (at low dimensions) to 32x (for high dimensions). A different trend is suggested in the runtime improvement against DeLi-Clu ranging from 17x (at low dimensions) to 38x (at high di-mensions). Therefore, when dealing with high-dimensional datasets it is best to resort to techniques based on random projections.

In general, DeBaRa is slower than FOPTICS but does not require to set any parameters, making it an excellent candidate for exploratory data analysis.
