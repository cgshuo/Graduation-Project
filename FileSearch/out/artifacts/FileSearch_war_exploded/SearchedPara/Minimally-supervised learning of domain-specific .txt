 1. Introduction as shown in sentence S1.

Causality plays an important role in many disciplines. In philosophy, several studies have been dedicated towards of causality in different genres of texts.
 they have been predominantly applied over well-written and massive corpora in the general (open) [6 relation extraction.
 between the cause  X  broken cable  X  and the effect  X  voltage loss into higher economic gains.

However, despite their importance in specialized domains, such as PD annotated training data, and have been applied with promising results in several relation extraction endeavors [12 initialized only with a set of pairs (e.g.  X  hiv  X  aids  X  to recursively identify new pairs (e.g.  X  smoking  X  cancer relation triple.
 challenge is the issue of data sparsity, which is detrimental to their performance [19 question we address in this article. Specifically, as our main contribution, we show that:  X  they enable substantial gains in performance.
 described below:  X  based on the Latent Relational Hypothesis [27] .  X 
Implicit relations are hard to detect since they are not always synonymous with explicit causal verbs (e.g.  X   X  on more sophisticated lexico-syntactic patterns.
 These challenges will be elaborated in Sections 2.3 and 2.4 .
 domain-specific corpus provided by our industrial partners. and discuss areas for future work and limitations in Section 5 . 2. Related work 2.1. Causality subtypes Tables 1 and 2 (borrowed from Barriere's taxonomy [32] ).
 Tables 1 and 2 .
 notion of causality in RST is different from ours. 2.2. Extracting causal relations from text  X  [ effect ] is the result of [ cause ]  X  . Each pattern consisted of a causality marker, such as recall of 0.76.

In the supervised technique of Girju [37] , 429 concept pairs that participated in a causal relation, such as  X   X  examples of causal relations. Their technique achieved an accuracy of 0.78 in identifying cause 0.78.
 to the task of causal relation extraction. 2.3. Causal relation extraction challenges 2.3.1. Challenge 1: Lack of domain-speci fi c resources domain-specific (causal) relation extraction. (A detailed review of the applications of Wikipedia in NLP can be found in [48] .) 2.3.2. Challenge 2: Data sparsity extraction is yet to be investigated. 2.3.3. Challenge 3: Seed selection Conversely, with 20 seeds, the precision and recall dropped respectively to 0.48 and 0.34. others, and with some subtypes not represented by any seeds at all. which offer an abundance of archetypical causal pairs that can be used as seeds. 2.3.4. Challenge 4: Surface patterns: Morphological variations and continuous elements techniques will extract 2 distinct surface patterns, namely, and S4
Such a formalization is clearly inefficient since in both S3 and S4 the causal relation between the pair and  X  damage  X  can be adequately expressed using the single pattern aggregated.
 dependency is the causal relation between the pair  X  hurricanes-damage 2.3.5. Challenge 5: Implicit causal relations we concentrate on 3 types of implicit causal relations.  X 
Relations of the first type, T1 , are expressed by resultative verbal patterns [51] . These include verbs suchas or  X  become  X  . Relations of this type inherently describe (part of) the resulting situation, as in  X  is the verb  X  mar ( by )  X  as in  X  white spots mar the x-ray image component of the result  X  marred x-ray image  X  .  X   X  rise in  X  and  X  due to  X  , such as  X  there was a rise in the voltage level 2.3.6. Challenge 6: Semantic drift
The pattern  X  lead to  X  , for example, is ambiguous. It establishes a causal relation when it connects  X  smoking leads to cancer  X  , but not when it relates  X  roads iterations ( i + 1) by promoting the extraction of invalid causal pairs (e.g. extraction of other unreliable patterns, like  X  found in  X  semantic drift. But as will be discussed in Section 2.4 , these techniques also exhibit several limitations. 2.4. Negative seeds and counter-training for semantic drift causal and part  X  whole relations, seeds like  X  hiv  X  aids relation r i , such as part  X  whole, are treated as negative seeds for the other relations r the patterns (e.g.  X  consist of  X  ) harvested by its negative seeds (e.g. by its corresponding (positive) seeds (e.g.  X  hiv  X  aids  X  overlap between the semantic space of the multiple relations [16] . pair e when estimating the similarity.

A more critical limitation of counter-training with negative seeds is that it helps only in discarding wrong ( which do not express a particular target relation, such as the pattern techniques like counter-training offer only a partial solution to the issue of semantic drift.
Our approach for addressing the aforementioned challenges will be next described. The corresponding experiments to empirically validate its performance are subsequently presented in Section 4 . 3. Extracting domain-speci fi c causal relations 3.1. Architecture overview domain-specific corpus. This will be achieved as described in Sections 3.3 framework. 3.2. Wikipedia as a knowledge base
Open-domain resources are attractive alternatives to compensate for the lack of domain-specific resources. One such
However, although WordNet contains relevant noun pairs, such as types of causal relations.

Both of them exhibit certain desirable features, namely:  X  would be involved had resources of similar scale been manually constructed from scratch.  X 
Representative: Wikipedia and the BNC consist of well-written and grammatically sound documents. Their contents are knowledge base for our task. These desiderata include:  X   X 
Wikipedia was used as a knowledge base was higher than the performance with the BNC.  X  the Dutch language, simply by switching the current English Wikipedia collection with the Dutch version.  X   X  3.3. Pattern Induction and Formalization
Wikipedia articles into a structured representation. 3.3.1. Linguistic pre-processing and term identi fi cation in Wikipedia's sentences.
 from POS-tags with the pipe (  X  |  X  ) delimiter. POS-tags are as defined in the Penn Treebank [61] . For example  X 
NN  X  represents (common) nouns, and  X  DT  X  determiners.
 literature, and has been found to achieve high performance levels on different types of texts [63,64] . 3.3.2. Pattern formalization syntactically parsing the sentence S5 is depicted in Fig. 3 . from the Wikipedia sentences, we use the Stanford syntactic parser [66] . Examples of lexico-syntactic patterns, which we derive from the sentences S5 , S6 and S7 are shown in Fig. 4 . pairs. A relation triple is given by a pair and its corresponding lexico-syntactic pattern. (For better readability, we will use the patterns' surface form, e.g. patterns are shown in our illustrations.)
Our lexico-syntactic patterns overcome the shortcomings of the traditional surface-strings. As can be seen from the the single, most general pattern,  X  cause  X  , to represent the relations between the pairs capture the relation between  X  hurricane  X  ,  X  cause  X  and extracted in Pattern Induction and Formalization encode different semantic relations. For example, the triple derived from sentence S6 , expresses a causal relation between the pair the triple  X  poem consists of stanza  X  , with the pattern between the pair  X  stanza  X  poem  X  .

To identify which of the pairs (e.g.  X  hiv  X  aids  X  ,  X  hurricane
We will next describe in details how we identify which of the harvested patterns (and pairs) express causality. 3.4. Seed selection using taxonomy of causal relations strategy, as described next. Details will be presented in the Experimental evaluation section ( Section 4 ).  X  conciseness and to avoid repetition, we will refer to the specialized seed sets per causality subtypes simply as 3.5. Pattern Selection until a suitable number of patterns are collected.
 of connecting valid causal pairs. For example, the pattern pairs, r ( e ). The reliability of the initializing seeds is set to 1, i.e. r ( e )=1. pattern p subcategorizes the pair x  X  y The  X  *  X  symbol is a wildcard representing any pattern or any pair. we observed that the discounting factor did not improve the performance.
The 1st (top-most) pattern in this illustration explicitly expresses causality with the causal verb expresses causality with the resultative verb  X  increase ( by ) experimentally in Section 4.4 . 3.6. Pair Selection back to the next iteration of the Pattern Selection stage (for harvesting new patterns a pair reliability measure. It comprises of two components, namely pair explained next. 3.6.1. Pair  X  pattern association the number of patterns, and the value of pmi ( e , p ) is as defined in Eqs. (1) and (2) . 3.6.2. Pair_purity unambiguous (e.g.  X  cause  X  ,  X  generate  X  ) and ambiguous (e.g. overcome it, we consider as example, two sentences, S10 and S11 :
Both sentences contain an ambiguous causal pattern, namely p  X  ( Section 3.5 ). Then, they will cause other invalid patterns to be selected. For example, the invalid pair e to the extraction of the pattern  X  locate in  X  ,asin  X  path located in garden pattern  X  locate ( in )  X  will lead to the extraction of the invalid causal pair [26] .
 is achieved by our pair_purity measure. It differs from the pair pairs are awarded much lower scores than valid ones and are subsequently rejected.
To calculate the pair_purity, we consider an archetypal pattern, p seeds regardless of their causality subtypes. Given a pair e = x pair_purity score is to search our Wikipedia corpus for the phrase q = we can deduce that e is a valid causal pair since it occurs with p that not all causal pairs in Wikipedia were subcategorized by archetypal patterns, such as p size. For example, in our Wikipedia collection, the valid causal pair of 0 since the phrase  X  death caused by attack  X  will not be found. querying the Bing search engine 3 with q =  X  yp unambiguous top-50 search results that contains phrase q in their summaries. In this way, invalid pairs, e.g. like q =  X  path caused by garden  X  or  X  garden caused by path our performance are presented in Section 4.7 .
 (parsing) and statistical computations, is extremely difficult on a web-scale. 3.6.3. Overall pair reliability
To compute the (overall) reliability of a pair, e , we combine the pair in Eq. (4) . number of iterations will be determined experimentally in Section 4.4 . 3.7. Extracting domain-speci fi c causal relations noun phrases like  X  loose connection  X  , and verbal events, such as verb phrases like are extracted are presented in Fig. 7 . 4. Experimental evaluation they enable substantial performance gains. 4.1. Corpora statistics 4.1.1. Knowledge base of texts (uncompressed) [24] . 4.1.2. Domain-speci fi c (target) corpus repair actions of engineers on high-end, electronic equipment in the Product Development
This target corpus was compiled over a 5-year span (2005  X 
Statistics on our Wikipedia knowledge base and on the target corpus are presented in Table 3 . 4.2. Pattern Induction The entire operation was completed 5 in about two days.
 connected by these patterns from the syntactic trees. Statistics on the pair given in Table 4 .

Sample patterns, the pairs they subcategorized and statistics on the pair in the 4th, 3rd, 2nd, and 1st columns of Figs. 5 and 8 (bottom row). expressed causality. 4.3. Seed selection causality subtypes. Example patterns are underlined in the sentences in the 2nd column of Table 5 . were collected into different seed sets for each causality subtype. We refer to these individual sets as Table 5 .

At this stage, a few points deserve further elaboration:  X  conveyed similar meanings, corroborating with the findings of Barriere [32] .  X  patterns and pairs.  X  in our minimally-supervised framework. They were performed automatically. 4.4. Discovering causal patterns and pairs from Wikipedia repeated for each specialized seed set (per causality subtype).
 seed sets used to initialize our algorithm.
 reliable ones are harvested in the later phases. 4.4.1. Statistical signi fi cance for optimal iteration halted in too late iterations, the extracted relations will be of dubious quality. as can be seen from Fig. 9 . The next issue is how to detect a significant performance drop.
H0. Performance drop between (iterations) itr i and itr i + 1
Ha. Performance drop between itr i and itr i + 1 is significant. (Performance refers to the average reliability scores of the pairs harvested in itr significant when it is not (i.e. a Type 1 error). We chose the value of practice [74  X  76] .

The t-test is a commonly employed technique for assessing whether the performance (accuracy) of algorithms differs significance due to outliers. Therefore, in our experiments, we relied on the Mann
The MWW statistics was computed 7 between the pairs harvested in successive iterations, itr iterating as soon as the MWW statistics between these pairs dropped below the critical value seed sets (per causality subtypes). The corresponding optimal iterations thus obtained are presented in Table 6 . remaining experiments. 4.5. Effect of seeds on performance extracted in the optimal iteration, which was found to be itr =7. 4.5.1. Precision of specialized vs. general seeds seeds of different subtypes.
 The precision of the pairs extracted per seed set was then computed using Eq. (5) were true _ positives or false _ positives , are also given.
 the higher scores of the pairs extracted by the creation and modification seeds (in the general set).
Based on these findings, we conclude that 1) especially when learning non-primitive relations, the performance of a practice of using a single set that mixes seeds of different subtypes yields, at best, moderately high performance. 4.5.2. Distinct patterns and pairs patterns extracted per seed set to determine whether they were peculiar to specific causality subtypes. common patterns that were extracted by the creation and increase seeds, such as discovered by the maintenance seeds were also highly specific, such as by mixing seeds from the different specialized sets ( Section 4.3 ).
We also observed that the general seeds could extract prototypical causal patterns, e.g. are presented in Table 8 .

This compromises the overall precision and recall (coverage). 4.6. Explicit and implicit causal patterns iteration, itr ( Section 4.4.1 , Table 6 ).
 explicit indicators of causality  X  realized by causal verbs, such as of resultative verbs, e.g.  X  increase in  X  , and adjectives, e.g. the result (effect) were least frequent, accounting for 3% (T2). These patterns, such as implicit expressions of causality.

These results indicate that our approach accurately extracts explicit as well as implicit causal relations. 4.7. Reducing semantic drift with pair purity in mitigating the negative impact of semantic drift on performance.
We re-implemented our minimally-supervised approach by removing the pair_purity component from the pair reliability causal patterns. The extracted results were then analyzed.
 number of ambiguous causal patterns, e.g.  X  produce in / by and  X  link-page  X  respectively extracted the incorrect causal patterns links  X  ). Then, these patterns introduced other invalid causal pairs, such as the pairs and results for Implementation_Org (Org), from Sections 4.4.1 and 4.5.1 , are also depicted. specified the causal-agent as part of the effect (e.g.  X  mar by constructs (e.g.  X  due to  X  , column T3 in Table 9 ).
 our algorithm to discover both explicit and implicit causal relations. 4.8. Domain-speci fi c causal relation extraction drift and of the initializing seeds on the performance.
 causal patterns and the nominal (e.g.  X  loose connection  X  target corpus.
 pattern/relation group. 4.8.1. Discussion of results
The most frequent patterns, participating in around 55% of the extracted relations, were resultative verbs like  X  their common usage for describing product failures as in  X  reporting observations on product behavior as in  X  breaker voltage exceeded allowable limit Customer Service (PD  X  CS) domain, these relations provide useful information for product quality improvement. verbs, such as  X  cause by  X  and  X  induce  X  , which participated in causal relations as in cable extension might have induced the motion problem  X  respectively. In the PD facilitate the diagnosis procedure of engineers.
 included constructs like  X  due to  X  and  X  source of  X  , which participated in causal relations as in cable due to wear and tear  X  and  X  the chip board was the source of noise information on repair actions of engineers.
 causal-agent inseparable from the result, as in  X  cluttered options mars (the) console menu can be exploited to better understand the causes of customer complaints/dissatisfaction. like  X  spark  X  ; implicit resultative verbs, like  X  end  X  and complaints in the PD  X  CS domain. 4.8.2. Performance evaluation precision, recall and F1-scores.
 they were too sparse and precise, and could positively bias our evaluation results. were considered as false _ positives . They were realized by causal patterns, e.g., e.g.  X  screen-today  X  , in the target corpus, as in  X  screen due to arrive today failed to discover remaining 20 ( false _ negatives ). The recall (R) was computed as 0.96 using Eq. (6) . express causality. Example patterns included:  X  X 
Shift  X  ,asin  X  the circuit shifted the frequency  X   X  X 
Move  X  ,asin  X  the rotating arm moved the table  X  Eq. (7) .
 iteration. Nevertheless, our scores compare favorably with those of heavily supervised techniques, such as [6 manually-annotated training data, and that only discover explicit causal relations. 4.9. Comparison with baseline and Wikipedia contribution contribution of Wikipedia as a knowledge base for overcoming the data sparsity issue posed by the target corpus. various semantic relations, such as hypernymy, part  X  whole and successor-of (e.g. between monarchs). the target corpus to discover domain-specific causal relations.
 causality. For example, the seed  X  pixel  X  image  X  frequently instantiated both a causal relation, as in image  X  , and a part  X  whole relation, as in  X  pixels found in image patterns that connected these domain-specific seeds, e.g. giving rise to semantic-drift.

In addition, we noticed that reliable patterns, which unequivocally express causality, such as
For example,  X  cause  X  and  X  cause a number of  X  , which are variations with the verb more likely to be selected in subsequent iterations.
 approach, Implementation_Org, which exploits Wikipedia as a knowledge base, are also presented. 5. Conclusion and future work which is beneficial to the precision and recall of our technique.
 relations from a real-life collection of domain-specific texts.
  X  cause  X  , for .e.g.  X  mar by  X  and  X  rise in  X  .
 applied for detecting causality from domains other than PD to discover inter-clausal causality, such as those realized by markers like between larger textual units, such as sentences and paragraphs.
 Acknowledgement
References
